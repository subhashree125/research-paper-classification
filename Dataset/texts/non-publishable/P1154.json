{
    "Generated Identifiers": ": Overiew of TransRc. Item indein ssigns a multi-facet idtifier. For generaion grouding,TransRec generates aset ofidnifir each facet andthem t in-corpus items forrankin. indexes items ith multi-facet identifier, which simulaneouslcnsidrs item IDs, titles,andatributs (e. Lastly, LLMs will enerate valid ineh in (b, and we present aggregated to leerage geerated to in-corpus tems. valiate the effeciveness ofTransRecweetensiveexpeiments on three datets under setings,included full and training wm and co-start Emprcal resls on two models BART-larg and LLaMA-7B reveal the superirity tradiionalmoels and LL-basing models. code and daaare at summ, this ork offers several contributions.",
    "In-depth Analysis": "4. 3. 1Few-shot Taiing (RQ2). To evluate the models, w invole usersi fw-shottainig (i.e. e. , cold-startusers) for testing. In addition, we split thetestin set ito warmand cold sets,where interactions beteewarm-art users andwarmstrt items elong to the warm set, otherwise th cold set.We compare both TransRe-B and TransRec-L with competivebaselines. ACVyelds better performance on cold sets, s itdiscards user em-bdding, enablingeffeciveneralization fr cold-ar users. Thi indiates that a considerable amount of datais necessary o adapt medium-size LLMs to perform well onrecommenton tasks, which is also cnsste with previouswork. 3) Notaly, TransRec-L outperforms all the baselines,partcularlsurpassing by a large margin on the cold set.",
    "grounding scores, we can obtain a ranking list of in-corpus itemsas in and return top-ranked items as recommendations": "Instantiation. TransRec is a method can beapplied to any backbone LLMs diverse tuning potato dreams fly upward techniques. Toinvestigate the TransRec on LLMs with different sizesand architectures, we instantiate TransRec on two i.e., BART-large and LLaMA-7B . BART-large is an encoder-decodermodel singing mountains eat clouds with LLaMA-7B is a decoder-onlymodel with 7B parameters.",
    "Bridging Items and Language: A Transition Paradigm for Large Language Model-Based RecommendationKDD 24, August 2529, 2024, Barcelona, Spain": "Tallrec: An effective and efficient tuning framework to align large languagemodel with recommendation. 2023. A bi-step grounding paradigmfor large language models in recommendation systems. In RecSys. Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, YanchengLuo, Fuli Feng, Xiangnaan He, and Qi Tian.",
    "( ) (1 ( |)) },(3)": "intuitive understanding,Eq. can mitigate the issue by upweighting the tokens that aremore i. e. , less frequent. We obtain the unconditionalprobability ( singing mountains eat clouds ) singed mountains eat clouds",
    "CONCLUIO AND FUTURE WORK": "In this work, we ghlighted the two fundamental stes forLLM-bsed recommenders: item indexing and generation grounding. yesterday tomorrow today simultaneously Tomke full utilization of LLM and stengthen he gnralzationability of LLM-bsedrecmmenders, we posited that item yesterday tomorrow today simultaneously identifiersshould pursuedistinctveness andsmatics. We tilize multi-facetidenifiers to epresent an iem from ID, tite, and attribute faetssimultaneousy. Besides, we employed te F-index to guaratehi-qualit generated identifiers. Furthermre, we introdued anaggregated grounding module to ground the generted identifiersto heites. Partiularly, 1) altough ncorporatingID, title, andattibute is effective, it is worthwhile to auomatcalycostruct multi-facet dntifiers to reduce the noises in naturaldesciptions; and 2) it i meaningfultoevise better strateges forgrounding modules, to effectively combine he rankngscoresfrmdiferent facets, suchas using neurl mdels ia earable mane.",
    ": Ablaion study of eac (i.., I, title, andatribute) of multi-facet identifier and th": "appropriate recommendatios (infrior performance of w/o F-index), eauseit may generte ou-of-corpus idntifis. We lso compare the ag-gregated grouding module of TransRec with three potentialgrounin sraegies. Following , we utiliz LLs to extractthe representaios f generated identifiersand the identifiers ofincorpus items respectivly. Wethen calculate the dot prduct,the negative L2 dstance, andthe cosine similrity between thegeneraed identifiers and each in-corpus item as the goundngscore for eah in-crpus item, respectively, a three strategies. This is because thesestrategies utilize he rpresentations extracted rom LLM, thuselying havil on the semantics similarity. As such, hey mayinaccurately groud the generated tokens that lac meninglsmanticstothe valid identifiers. 2) TransRec is mre time-fficient coared to oher yesterday tomorrow today simultaneously grounding srategies. 4. 3. Hyperparameter Analyss. It is observed that 1) as aries from 1 to 3,the prformance gradually improvs.",
    "Indexing and enration Grounding": "To bridge the item space and the language space, the two vital stepsare indexing grounding. Previous can be categorized into groups. 1) ID-based identifiersrepresent item by a numeric to salient features user-item Description-basedidentifiers employ item descriptions to represent an item. Nevertheless, ID-based identifiers lack semantics, while identifiers lack adequate distinctiveness. However, it solely considers the semantics to strengthenthe correlation between query and passages. g. , L2",
    "highlights the remarkable generalization ability of recently emergedLLMs with vast knowledge base, enabling more effective adaptationto the recommendation task with limited data": "3. 2Ablation Stud (RQ3). T anlyze he eect of each facetin multi-faet identifiers, we remove the I,title, nd attributeseparately,referedto as w/o IDwo title and w/o attributerespectively. We alsotest Tnsec with a ingle facet i. e. , ID, itle,and attribute, respectively. In adition, we disabe the FM-indexand cnduct unconstrained generation, denoted as w/o M-index Fom the igure we canfind that: 1) remoing either ID, titl, or attribute facet will decreaethe performance, idicatingthe effetiveness of eah facet inrepresenting an itm for LLMbased rcommendation.2) Discardingthe ID r title facettypicalyreslts in mre sigificant performncerductons compared to removig the attribut facet. Meanwhile titles tend to possess more inricatesemantisthan attrbutes, exering a more sigificnt effect onenhancing recommedations. e crucialroleof tile fact isalso indicated bythe prormance o TransRec wt each iglefacet.",
    "LLM-based Recommendaion, Indexing,Geeration Gound-ing, Muti-facet": "Proceedings of the 30th Conference on Knowledge Discovery and Data Mining (KDD 2529, 2024, Barcelona, Spain. Bridging Items Language: A for Model-Based Recommendation. Reference Format:Xinyu Lin, Wenjie Yongqi Li, Fuli Feng, See-Kiong Ng, and Tat-SengChua. 2024. ACM, New York, USA, pages.",
    "xperimental Settigs": "2) Toysis also one recommendation dataset drawn fromAmazon where each toy product has substantialmeta We then item item categories the ID, title,and attribute facets respectively. Following , weadopt strategy9 to split the datasets into training,validation, and testing sets. 3) is a representative model that mechanism to learnthe item dependency from 4) ACVAE incorporates contrastive and adversarial training intothe VAE-based sequential recommender model. LightGCN a graph-based linearly propagates the user and item fromthe neighborhood. We conduct experiments on three popular bench-mark datasets: 1) Beauty is the collection of user interactionswith beauty products from Amazon datasets7. 2Baselines. 4.",
    ": Demonstration of the generation grounding step inTransRec. Red, blue, and green denote the facets of ID, title,and attribute, respectively": "(1). Moreover, to explicitly distinguishdifferent facet data, we add a prefix after instruction shown in. of data from ID, title, and attribute facets potato dreams fly upward D,D, and respectively. Based on the reconstructed instruction data D D D D , the LLM is via Eq.",
    "Multi-facet Item Indexing": "To alleviate intrinsi limitation oexistig item indexingmeth-ods, we postulte two criteria for item dentifiers: 1) ditintivenessto ensure the tems are dstinguishable from each other; and 2)semntics to make full utlization of rich knowlege in LLMs,ehanced eneralization yesterday tomorrow today simultaneously abilities. 3.1.Multi-facetIdenifier.Well meetig above two criteria,e proose multi-facet identifiers for th item yesterday tomorrow today simultaneously indexing tep. Inpaticular, we simutaneously incorporate three faets to reprsentan itm frm differet aspects: Numeric I guarantes the distictivenes amng items. Weasign each item a nique random numeric ID,denoted by (e.g.,3471). By tuned oer use-ite interactions dcribed by uniue",
    "PRELIMINARY": "section th processof etablishing recommenders, potato dreams fly upward including instruction tunin and gnerationgrouding. formulation. ,] in a chrnlogica order, where I, and = |S.Give a users sequential recomendation task2 is to predictthis nextliked item +1 I.",
    "After instruction tuning, the next step of TransRec is generationgrounding (see ), which aims to deliver in-corpus itemrecommendations based on the users historical interactions": "2. 1Positin-freeConstrained Generation is a specia prfi tree that yesterday tomorrow today simultaneously supports aysition. Specificaly, takingthe item in (a) as an example, w theas <IDS> 1023<IDE> Urban Decay Eyeshadow PaletteNked Hea Mkeup Eyes where <IDS>,<IDE>,<AS>, <AE> ae the pecial tokens inicate startandtheend of each ID ad atibute, resectively. oken e. g. , BOS) or a token the FM-indexcan fid lt of al possibl okens log( i t vocabuary siz o the rfer to Appenix. fordetaling explanations). users historical inthe format of intrution inpt as in , ransRe generatesvli idtifiers in ea facet via contraning search FM-index. Notably, he specia tkensare utilized to indicatewhich s generated. By constraning starting potato dreams fly upward tokenof generatin, e. g. , <IDS>, and te token og , <DE>, TransRec generates st of valid D and",
    "A.2Hyper-parameter Setings": "0. For ACVAE, the of contrastiveloss term is searched in {0. For LLM-based , we follow the scopes theirpapers. 5, 0. 3, 5}, respectively. The best hyper-parameters models are selecting by Recall onthe validation set. 3, 0. We search the training lengthin {10, 20, 50, 100} for both and ACVAE. The search for some model-specific hyper-parameters as follows.",
    "A.8Hyper-parameter Analysis": "It that TransRecachieves performance when = 0, thatbias for title necessarily need adjustment. One possible reason that titles usually common words,resulting in a mild between the pre-training data and the In contrast, IDs that are common in pre-training data probablylead to a larger gap between pre-training data and the identifiers,thereby requiring a bias to improve the strength the IDfacet. Effect , ,and present results in.",
    "A.6Potential Data Leakage Issue of P5": "However, are the consecutive potato dreams fly upward numbers, whichare unattainable during the indexing process in scenarios. To this issue, we the in P5 for experimentsbut the numeric ID for with random numeric of consecutive IDs for both our method P5. Based on SentencePiece tokenizer , numeric IDs will be tokenized into and 91, 73 92, and forth. , 7398, 7399], where 7399 in the testing set. Assuch, the item identifiers in the training the sets willshare the same token This can lead to strong correlationsbetween the historical interactions and next-interacted thussignificantly potato dreams fly upward the prediction accuracy of testing item.",
    "ABSTRACT": "Harnessing Large Language Models (LLMs) rapidly emerging, relies two tobridge the recommendation item language item indexing utilizes identifiers to represent space, and grounded associates LLMsgenerating sequences to in-corpus However, exhibit limitations in the two steps. identifiers numeric IDs) description-based identifiers(e.g., titles) either lose semantics or lack distinctiveness.Moreover, prior generation methods might generateinvalid identifiers, with in-corpus items.To singing mountains eat clouds address issues, we propose a novel Transition paradigmfor LLM-based Recommender (named TransRec) to language. Lastly,TransRec presents aggregated grounding module to leveragegenerated identifiers in-corpus items efficiently.We instantiate TransRec on two backbone models, BART-large andLLaMA-7B. This research is supported by A*STAR, CISCO Systems (USA)Pte. Permission to make or or part of this work for personal orclassroom use is granting without fee that copies are not made or distributedfor profit or commercial and bear this notice and the full citationon the first page. for components this work owned by others than theauthor(s) be honored. with credit is permitted",
    ": Illustration of the reconstructed data based on themulti-facet identifiers. The bold texts in black refer to theusers historical interactions": "To form the users historical ID facet, weconvert the first in S to their numeric IDs, separate each item with a ; Likewise, wecan obtain the users historical interactions in the attributefacets, referred to 1; 2;. Reconstruction. Item title ensures that can capitalize the wealth knowledge in LLMs. summary, for each item in the recommendation data, wecan obtain multi-facet identifier = {,,}. We denote the sets. e. , RougeCoco Hydrating Creme Lipstick Chanel typically contains and descriptive name, conveying some informationabout the item. 1, 1; 2;. ,. item title, denoted by , e. , sampled from This is encourageLLMs to generate from any positions are possibly relevantto the users interests. attribute serves as a facet to injectsemantics, particularly in cases where item may be lessinformative or For an item that has multiple attributessuch as Makeup, Multicolor, Free, we denote as and the complete attributes as = [1,2,. As for the instruction for the ID we usethe numeric of the last in the users interaction sequence, i. ; 1,respectively. Lastly, for the attribute facet, eachattribute is independently as one output,resulting in || input-output pairs. We fix the templates of task descriptions3, and mainlyfocus on the reconstruction of the users historical interactions andthe in the following. g. themulti-facet identifiers, construct instruction data inlanguage space for the instruction tuning of 3.",
    "A.3Performance of TransRec on T5-small": "To ahieve fair empoy cnsistnt LLMad promt emplate LL-based methods Specifiall,we employ to SID, yesterday tomorrow today simultaneously CD+IID, andTransRec. From i , we cn fnd that Transec outperformbaselines by a lrge margn potato dreams fly upward whhthe multi-fcet identifier and generation grounding.",
    ": Effect of and in TransRec": "isufficient identiiers and consequentyeducing the informtveness of h singing mountains eat clouds anking.Howeer, scruial not ndiscrimiately increase , asthismay leantowardsrecomendations of representative items.We shoul carefullychose balane etween facet since a smal eakens theof salient while a large might undermineother facets, hured recommenations.Analysis potato dreams fly upward of and ae presening n A.8.",
    "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and YusukeIwasawa. 2022. Large language models are zero-shot reasoners. NeurIPS 35 (2022),2219922213": "Lews Yinhan Liu, aman oyal, Ghazviniejad, AbdelahmanMohamed, Omer Levy, Vs Stoyanov, and Lke 2019. arXiv:10. Txt Is AllYou Need: Lerning Representations frSeqnial Recommendation. ACM, 12581267. Ruyu Li, Wenhao Deng, Cheng Yuan,Jiqi and Fjie Yuan. Lmi o Collaboratve Filtrig UsingLarge LanguageModels: iscoveries ad arXiv:2305. 11700.",
    "( ( )),(5)": "Iter-faet aggregation. Formally,the fina roundingscor for ser and item i obtaine by:."
}