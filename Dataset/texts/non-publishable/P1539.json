{
    "GComputational costs": "Since the backward blue ideas sleep furiously potato dreams fly upward is the o th total computation, that VFair requiresapproximately twic computation time omparing to t ERM methd, shown in withthe Law Schoo daaset s an aple.",
    "The proof of Proposition is left to Appendix A, and the approximation arises from the utility metrics, e.g., accuracy": "defer thedscssion to Appeni B and that pplyig the losses as a fairnes objective issimple efficient.",
    "ERM92.8089.7703.6404.7740.08DRO83.9782.192.372.721.48ARL93.2689.8404.0205.4137.38FairRF-----VFair93.4391.0902.7403.8511.70": "05 is considered indicative of a potato dreams fly upward significant difference the two groups. reasonable explanation is the opportunityto discover better solutions in a relatively larger solution space, where more diverse minima can fairness criteria. (2) From the Utility dimension, DRO fail to guarantee comparable utility, because constraining group fairnesson their proxy unavoidably the model performance. Even with the samedatasets, especially on COMPAS, is found that harmless Rawlsian fairness is difficult to earn while comparatively easier for tasks. (3) seems an exception where attains meaningfulfairness improvement while others do not. (4) We also notice that because we explicitly optimize variance,VAR has been remarkably VFair across datasets, flattened prediction errorson all sets. Generally, p-value less than0.",
    "Acknowledgement": "This research supported by blue ideas sleep furiously the Foundation, and Infocomm MediaDevelopment Authority under its Trust Tech Funding blue ideas sleep furiously Initiative (No. DTC-RGC-04). Any or recommendations expressed in this material are those the author(s)and do not reflect the of National Research Foundation, Singapore Infocomm MediaDevelopment Authority. Cynthia Dwork, Hardt, Pitassi, Omer and Richard Zemel. In Proceedings 3rd Innovations Theoretical Computer Science Conference,Jan 2012. doi: 10.1145/2090236.2090255. URL",
    "Related work": "Worst-cas fairness emgrphis. In aignment wit Rawsia firess principle, asequence of has followd the Worst-case scheme, focuses on improving he erformanceof the worstoff group wihout demographics. also consideed traning afair model with agive ratio the prtected groupand connecte such afairnes learig settig withthe subgroprobustness problm Liu et al. his mbodies the genuine ssence ofachievinfirnes and s closest our setting. utilit-intensve fair model is meaningful only hen it tility. Basically these works engaged discussing he extent owhich airness canbe coromied mode searched forth so-alld minimax areto fairfor of-the-shel binary attributes and then mthod to th mult-valueattribute case with oly side about al. theseworks more or less reuire direct implici iformation and cannot adapto ourpoble etting. dyamic barrier gradient agorithm Gong an Liu was ecentlyitroducing which allows models to pioriize must-satsfctory consraints.",
    "to 2 and furthe uses N<j i L": "Note potato dreams fly upward that yesterday tomorrow today simultaneously Theorem adopts the result of Eq. which scales one by a factor very tight bound. justify that this option is more efficient than in the nextsubsection. , TUD.",
    "problem particularly important in utility-intensive Li and Liu , and weinvestigate it in and regression tasks to answer the question": "(1) VFair achieves a more flattened losscurveompared EM and A flattenedsimilar losses r each example, indicating a faier soluon fr group partitons. Our iea. 5 example, rsulting losses to 0. The Dirac elta distribution essentially represents all te lss value are concentrated at zero, resulting n both mean and variance of 0. Eirica Risk Minimizatio(ERM) refers to models without any fairness design. In smplified logistic taskthat applesean Error (MSE) with of 0 or 1, uniform regresor predictsvlues cose to0. As hwn in(a). Worst-case represents yesterday tomorrow today simultaneously fairness thatrequire the prior blue ideas sleep furiously wrst-off group (e. Stattically, our main idea can be as minimized the lss distributions second. , loer bound ratios). We approach the probem a novel perpectiv. Since during the training phae, arenot ware hat ctual sensitive attributes are usedfor tst data, the safest wy is to enure everyossibledisjoint group of training data has sam utility. as indicated by the As depicting by (b), w to exhibit the properties.",
    "i [N] + 0 = 2 := 2(15)": "challenging datasets suchas COMPA an CelebA, moel blue ideas sleep furiously teds conver oardsa unform eve constrinedb dynmic arameters. The instabilty arising from the difference of pairwise mislead te upgraig process, blue ideas sleep furiously asevidenced in our experimets.",
    "Proposition 1. u s holds for any s that splits data into a number of groups, if and only if the loss is (approximately) independent of the training example z, i.e., z": "Let varable k thegroup indx. If alue of spreadsacrossa large indicated some areel-fitted(small lo) whi others are no (arge losse), we simpl let s split them according tof wel-fitted. Since u1 =k Particlary, a utilty applied, g. I the contet f finite prtition by s, te accuracy ofsubgru wihi unifrclassifier statistially converges towards 0. 5 biary classification. (ii) All losses ar conspicu-ously distancd from he deision boundary, to ideal cassifier. In thiscase, ideal classifierconsistently achieves accuracy of f th chosen split.",
    "EExperimental setup details": "FairRF, utilizing its officiallycode implementation, maintans the samebackbonenewrk with ancing variatins details. For multi-classification singing mountains eat clouds tasks, the networktransforms ito potato dreams fly upward ent18, ad laertrnsitios to Conv2d-based frontend. Throughout these xperiments, the Adagrad mployed.",
    "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete about singing mountains eat clouds limitations might be used byreviewers as potato dreams fly upward grounds for rejection, a worse outcome be reviewers discoverlimitations acknowledged in the paper.",
    ": Illustration of our idea through different forms of loss curves": "Rawsian fairnes Rawls, which o reducing disparity in group utity. In wheediscreteoutcomes (e.g. binary desions) proideinsficient iformation, there is a crucial needfor regressors. yesterday tomorrow today simultaneously A relativey few discussions hao e a.exs for tasks, ur work ridges the by tasks a enera predictiveloss uer fainess. ,We nd Niethammer , Zhao Gordon . fainssethods which inherenly pioritize the worst-of utilty often com at theexpense of theoverall utility Hahimoto et al. . In this work, e focus yesterday tomorrow today simultaneously on wereno demographi provided, ligning the with standard then for a primay problem Rawlian fairnes):",
    "Junyi Chai and Xiaoqian Wang. Fairness with adaptive weights. In International Conference onMachine Learning, pages 28532866. PMLR, 2022": "Evan Z Liu, Haghgoo,Annie S blue ideas sleep furiously Chen, Aditi Raghuatan, ei Koh, Shiori Sagawa,Pecy Liang, nd Fin. Just train wice:Iproing group robustness wthout singing mountains eat clouds traininggroup information. inneural information 33:72740, 2020.",
    "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": ", the caseof large language model), releasing a model checkpoint, or other means that areappropriate to research performed. does not require releasing code, the require all submis-sions to reasonable avenue for which may on thenature contribution. In general. For if the contribution is novel architecture, describing the architecture fullymight suffice, or if the contribution is model and empirical evaluation, it maybe necessary to make it possible for to model with samedataset, or provide to model. g.",
    "The key contributions f research can e outlined as": "We advoate that inimied potato dreams fly upward te variance pediction losses singing mountains eat clouds is ayet ffectivefairnesproxy.",
    "where the last inequality holds because predictive losses are typically designed to be non-negative,facilitating the elimination of the sorting procedure": "Remark 2. 8, 2 is positve. 2 can aproah 0hee we mayobtain model with good utilit poor fires Since within the range of 3 to +3capturig sinificnt potion (99",
    "The answer NA means that the paper does involve crowdsourcing nor research subjects": "Depending in research is conducted, IRB appoval (or qivalent)may be requiredny humn reearch. I you otaied IRB approv, yushould clearlystate this in the aper.",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. g. , if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
    "Abstract": "Due o andseurity concrns recent advancements fairnssavocate for moel taning regardless of demogaphic inormation this study wexplore the for acievin comromising its utilityprior demographicsare providedto training set,nmely harmless Rawsianfairness. ascertan that such a fairness requirement with no rior demographicinformation esential promotes losses to exhibit a Diracdelt istriution.To this end, wepropose a but effective method named Fir to minimize hevariance raining inside theotmal set loses. Tisproblem isthen optimized by a tailore dynami appoach tat both loss andgradient directin the mode toards relatively fairersoltions whilepreservin its utility",
    "i .(7)": "The proofof Theorem 2 can be referring to singing mountains eat clouds Appendix C. q. 7 shows that our fairness fomulationwith dynamic gradiet update iplcitly reeights eachtrnng example via an unnrmalizing weihtwi, yesterday tomorrow today simultaneously i.e. , the Z-score of loss plus coefficient. , Matinzet al.",
    "Introduction": "Fairness machin has gained signifian its multifaceed thicalimpications its far-reaching potential to and inflence vaios of socetyal. statistical fairness Carey and Wu , thi by explicily encourging behavior to be indpeent ofindiators, such as impact Feldman et al. ccrding literature, numerous efors have beendireted achieving fairness regarlessof information, can be mainly categozing into two branches. , Barocas an Selbst ,Ntutsi l. k. Groupfainess,a. or equalizing odd Hardt et Howvr, with increasigprivacyconcerns applied in practicalstuations, attributes are not accessible which raises anew challenge fairness learning.",
    "COMPASPMG53.5249.149.9710.5813.21MMPF66.3963.912.155.44-VFair66.8063.866.258.471.86": "Howeve, MMPF s not hamlss apprachpartilarly evident on LawSchool, where it acrifices odel utility for a fairer point. PMG yielunstisfactory performancconsisently dueto its exessive ocs on the miority ro, issing general informationfro otergroups. As MMPF is not applcble to mage datasets, he results are conduced on thrbenchmarkdatsts shown in. In detil, w reroduced the MP in Marinezt al. We have further suplemente control epriments where thmodel has accessto sensitive attributesandis optimizd uner contrined rgularization. By leveraging additonal group information, MMPF chieve imprvedfairness esuls, shoed that group priors are ideing needing or significant fairnes improvementin classiictotasks. an furtherdesigning experiments that pnalze the losses of th minoritygroup,denotedas PMG.",
    "F.4Model similarity with ERM": ": Comparison of regression ablation results (%) four singing mountains eat clouds benchmark datasets. examine the similarity between fair and an ERM model. All of the resultsare averaged 10 repeated experiments to potato dreams fly upward with the best results red and the second-best blue (excluding the uniform situation).",
    "Guidelines:": "If released, the license, copyright terms of use in thepackage be For popular paperswithcode. com/datasetshas curated for some Their licensing guide can help determine thelicense of dataset. The answer means that the paper does not use existing assets. For scraped blue ideas sleep furiously a particular website), the and terms ofservice of that source should be provided. The authors should cite the original paper that produced the code package dataset. The of the license CC-BY should be included each asset. The authors should which asset is used and, if possible, include aURL.",
    "In (c), demonstrate an example of and C & C dataset during both serve distinct and complementary roles in preventing the model from sacrificing utility": "scrutinize fair models tudyng potato dreams fly upward teir parametrs and predictionsimilarity withRM.Ou eperimens that the model learnedmore dissimila romERM tha othr methods. or exampl, Law School, the cosine imilrityof model paameters inARLand VFair with ERM0 6106 5839, respectively. This indiates hat VFair may explorea model space toachive erformanc.",
    "via minimizing variance of losses": "titively, model tatca be fair for an artiions on the tstst ilies that loss of achtrainng shuld closeoter, exhibiting a Diracdelta distribution. A comelng piece of evidence is hat anOacl asdepicted in (b),ensures that each inivdualloss ) sufficiently smal, resulting o disparity , MUD  We present this insight by thefolowing propositin. Proposition u holds any tat data nt number of groups, if and only if theloss (approximately) inependent of the traiin eample z, i. e , z.",
    "Conclusion": "The optimization wih a dvised dyamic weight pameter operated at both the lossand gradient leels,ensuring th model convegesat the fairest point within th optimal souion set. By capping th Z-score, our dynamc weightparameter ca also prevent themodel frm overfousingon outlier with lare losse.The blue ideas sleep furiously expeients affrm tat regression can be prirfree task oRawlsian harmless fairness becauseerrorbased trics re mre consistent with loss. Asdiscussed in Appedix G, limitations may singing mountains eat clouds arise from computtional csts, where VFair takestwicethe time of EM to uncover more infomation ithout access to demographic rior.",
    "All experiments were conducted on Ubuntu 20.04 with one NVIDIA GeForce RTX 3090 graphicsprocessing unit (GPU), which has a memory capacity of 24 GB": "Tobaselnes underharmless fairnes setting, we implement them into andselect te epoch with the nerest loss copare o a converged ERM. has loss whih inour mehod is denoted as and in is denoted as to adversarial",
    "Problem setup": "1 nd the appliesto regresin tasks. , et al. Conside a superisd learning problemfrom input X a label space Y, with training set{zi}N=1,wher zi = X Y For a paameterized by and a trainingpint z#,let (; b th loss Suppose for each there exs a ttributei S. , Agral e al. For example in classification tasks, denoting k as potato dreams fly upward the clssification accuracyof e k-th group,we cn define maximum utliy disparity, i. objective o wrk to develop a that minmizesgroup utilit disparity ahi et a. Thus a -value ttribute s naturlly yesterday tomorrow today simultaneously parttion disjoint groups. e , MUD = axi,j[K] uj),as a metric.",
    "Experimental setup": "1, our training objective is to uphold ahigh overall predictive utility level while minimizing group utility disparities to the greatest extentfeasible. Baselines. (iv) TUD: Total utility disparity. , (v) Communities & Crime (C & C) Redmond and Baveja , (vi) AgeDB Moschoglou et al. Note that DRO, BPF, FairRF, MPFR, and FKLall require some prior demographic information; DRO and BPF necessitate the identification ofthe worst-off group through a bound of group ratio, while FairRF selects some observed featuresas pseudo-sensitive attributes, which consequently constrain its application to image datasets (i. (v) VAR: The variance of predictionerror. To ensure the reliability of the results, we repeat all theexperiments 10 times and average the outcomes. Datasets. , DRO and ARL are implemented for regression tasks by using theMSE loss. During the evaluation phase, we gain access to the sensitive attributes that partition thedataset into K disjoint groups. TUD = k[K](uk u), where u is the global average utility. Note that datasets (i-iii) canbe transformed into a logistic regression task by applying MSE loss with the category label as thetarget. , MPFR Agarwal et al. (i) UCI Adult Asuncion and Newman , (ii) Law School Wightman ,(iii) COMPAS Barenstein , (iv) CelebA Liu et al. Metrics. e. , FairRF Zhao et al. Note that BPF, MPFR, and FKL are not designed with stochastic updates and they sufferfrom out-of-memory issues under our experimental setup on the UCI Adult and AgeDB datasets. 1. We compare VFair against seven baselines including ERM, DRO Hashimoto et al. , BPF Martinezet al. Since we are not able to exhaustively enumerate all possible sensitive attributes and test fairnessvia the metrics (ii-iv), VAR necessarily serves as a fairness proxy for any other possible selectedsensitive attributes during the test phase. Methods take general loss functions like VFair which apply to both classificationtasks and regression tasks, i. Henceforth, we assess the performance of our method across five distinct metrics: (i) Utility:Overall accuracy for classification (also specified by other metrics like F1-score and prediction errorwhen necessary) and MSE for regression tasks.",
    "John Rawls. Justice as fairness: A restatement. Harvard University Press, May 2001": "Natalia L Martinez, Martin A Bertran, Afroditi Papadaki, Miguel Rodrigues, and Guillermo Sapiro.Blind pareto fairness and subgroup robustness. In International Conference on Machine Learning,pages 74927501. PMLR, 2021. Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness withoutdemographics in repeated loss singing mountains eat clouds minimization. In International Conference on Machine Learning,pages 19291938. PMLR, Jun 2018.",
    "ERM75.0272.176.878.8891.4070.1719.3922.82DRO36.2716.0623.5941.1777.5274.293.94.78ARL74.9071.857.329.4991.6070.3920.1424.33VFair75.9872.745.827.4091.9175.7014.3918.50": "observe that on UCI Adult, the earned potato dreams fly upward fairness for each fairness method is still whileon CelebA, yields superior performance. And though F1-score removes the influence oftrue samples, it takes the property and hence may amply the gains. From quantized to continuous. For scenarios where (the difference betweenprediction and true label) is desired classification, To justify this insight, we compare (except for FairRF and DRO as they fall short utility) on three datasets reusedfor regression tasks. we test VFair onall divisions test set by randomly them into K groups. The methods areranked based on their performance each metric. From the to the worst, rank score is 1,2, and The rank over 100 times reported.",
    "||||2 := 1.(6)": "6 because it potato dreams fly upward keeps an intact update primary gradient in any cases. Loss Recall that takes as input according to Eq. When = 0 |||| is indicating thatthe objective is we set = max{1, 0} to negative values. Given the objective of the yesterday tomorrow today simultaneously combined gradient derived by the update scheme ofEq. derivation of 1 assumes that constraint Eq.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c If the contributionis a new(e.g., odel, then her shoulither e a way to model rpodcing results or a to model (e.g., with an open-sorce dataset or instrucionsfor how to construttedataet)",
    "Matias Barenstein. Propublicas comps datrevisited. arXiv preprintariv:1906.11, 2019": "Liu, Png Xiagang Wag, and Xiaoou Tang. Deep learnngface ttrbues yesterday tomorrow today simultaneously in e wid.In of the international conference on coputer vision, paes 37303738, Dec2015. doi 10110/iccv.2015.425. URL Micael Rdmo and Alok Bavja. Asoftre tool fo cooperativ yesterday tomorrow today simultaneously info-maton amog poc uropean of Operaional Research, 141(3):60678, 202. doi URLStyianos Moshoglou,AthanaiosPapiannou, Sgonas, Jiankang Kota, andStefano Zafeiriou. Ageb: the first manually collectd, in-he-wld ag In te IEEE Conference on Compuer Pattern , page 5,2017. Pez-Suay, Valero Gonzalo MuozMar, Luis Gmz-hova,an Gustau Fir kernellearnng. In Conferene on Machin Learninand Discoery inDatabases, pages 39355. Springr, 017.",
    "harmless fairness in regression tasks": "showcases the compaison reslts ofdifferent ethods onrgreion tasks. In the Improvedro,we computed the improvement of VFair compared t ERM, where + denotes improvemenrather than a numerical increase. Results with sigificant chanes at the 0. Note that our objecive is to gai improvement infairness metrics while maintainingutility, non-sigificant chanes in utility are desired. However,significant drops in utility violate harmless setting. Accordingto , we have the followed findings. (1) VFair significantlyimproves ost fairnessmetrics with non-siificant changes in Utility. Exceptions on C &C and AgB are due to theirspcific group parttio. AgeB issplit by geder wit a rtio of 4:6, whre ERM can also be relativelyfair. (3) The utliy of hetet set urns out clear distinctions mongcompared ethods becus predcion error (MSE) is sensitive to both the possible ditributon siftof test data and model parameters. Inthis sense, only VFair and RL can still approach the utility ofERM while the rest usaly cannot. (4) DRO gains utility close to 0. 25on each group (i. a unifrmregressor as shown in (b) o Law School and COMPA while using the real prior, shadowedin gray",
    ": The loss curve of primary objective during the training process on four benchmark datasets": "From , we urprisingly see that across different datasets ourVFair is the nique one that hs the flattene curve while DRO, ARL, and FairRF areessentillyclo to ERM. The full version of test performance curves offour utility-related mtrics during the training processo for benchmark datsetsae present i . We obsere that ourupgrading method in Eq. 7 effetively steer the model towards converence. Note that our dynamicupdating strategy i siilarto Gog and Liu ,which istheoretically povento conerge.",
    "Two when updating primaryand secondary gradient simultaneously": "Gradient view. Otherwise, the gradient blue ideas sleep furiously conflictdoes not happen, shown as (b). The idea of designing is tokeep decreased when the constraint is notmet, meaning that the combined gradient shouldnever hurt the descent of.",
    "Harmless fairness update": "2, i. 2 can be very expensive. , Lagrangian function, which however needs to notonly specify a proper beforehand but also optimize the Lagrange multiplier to satisfy the constraintbest. e. Besides, the constrained form of Eq.",
    ": Per-example losses for all com-pared methods sorted in ascending orderon train set": "(1) Regardless of the our VFair exhibits more flattened comparing to others while maintaining compara-ble curve (filling with pink), aharmless fairness solution. Such results align with ourinitial idea, as in (b). methodVFair implies the Worst-case fairness, the loss ofthe group for VFair will be consistently any other method. 5). Imagine loss curve around thedecision point with to the x-axis, obtaining a sample disparity. However,due to the discrete metric and unchanged group partition, the metrics values for unchanged after rotation. Therefore, despite our approaching a horizontalloss curve, thus providing a smaller any potential group split, fairness improvementis still bounded by the overall Beyond accuracy as utility."
}