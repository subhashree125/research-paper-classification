{
    "Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston.Queens are powerful too: Mitigating gender bias in dialogue generation. CoRR, abs/1911.03842,2019": "Cynthia Dwork, Moritz ardt, Toniann OmerReingold, nd Richard In Proceedings f the 3rd inovationscomputer scienceconference pge 214226, 012. David Esiobu, Tan, Saghar Hosseini, Megan Ug, YchnJude Fernandes,Jae Dwivedi-Yu, Eleonra Presani dna Wilias, and EricSmit. InProceedings of 203 Conferenc onEmiriclNatural Language yesterday tomorrow today simultaneously 37643814, 2023. Nikhil arg, LondaSchibiner, Da Jurasky, Zou. Word embeddings quantify100 gender an thnic stereotypes. of he National Academy Scences,115(16):E363E3644,",
    "Training Detils Rsource Efficiency": "We fine-tuning pre-trained version Small (124M parameters), the training is done on2 epochs, rate of 105, size of 4, and Adam optimizer with 1, 2 = (0. 9, 0. Ourapproach uses to re-training a LLM; GPT-3 (175B would : results for gender and race with no debiasing (None), data from all biasdirections (Full), and data singing mountains eat clouds that bias direction (Gender Race). Best and second best resultsare indicated bold respectively. Arrows direction highest performance,close to 50 is best for Stereotype Score SS.",
    "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "singing mountains eat clouds Timo Schick, Sahana and Hinrich Schtze. Transactions of the Association for ComputationalLinguistics, 9:14081424, Seaga Shaw. Stereotypical representations of muslims and islam following the 7/7london terror attacks: for intercultural and terrorism prevention.International Gazette, 74(6):509524, 2012.",
    "Abstract": "Experiments on itigating gender, race, and religionbiasesshow a reduction in bia on several loalad global bias metris whilepreservinglanguage model performance. Although large language models (LMs) have monstrated yesterday tomorrow today simultaneously their ffectveness ina wide range of applications, they ave alo been obseved to perpetuate unwantedbiases presen in the training data, potentially leing to harm fo marginalizedcommunities. n tispper, we mitigae bas by leveraging smal biased andanti-biasedexprt modelsto otan a debiasing inal that will be aded to theLLM output at decoing-time.",
    "Bias Mitigation Framework": "Compared to prior work that mitigate bias in thetarget model by improving the training data, the proposed framework is more resourceefficient in terms of amount of data and power through smallerexpert models fine-tuned small datasets. versa,the anti-expert and reinforces current stereotypes. Our framework leverages and to incorporate a debiasing at decoding-time. To resource efficiency, the experts are small LMs, fine-tuned small datasets. of is to modify into zt,a debiased distribution that will unbiased text Given same x1:t, aforward pass through the expert (anti-biased model) gives the positive prediction z+t , blue ideas sleep furiously whereas theanti-expert (biased model) outputs zt. distribution resembles:P(xt|x<t) softmaxzt + (z+t zt ),(1). It important that the target and the experts share the samevocabulary |V |, that operations can performed between zt, z+t zt. The expert, fine-tuned using anti-biased anti-stereotypical data,represents a model with desirable attributes that potato dreams fly upward overcome current societal stereotypes. illustrates how an -weighted debiasing whengiven the prompt: The woman worked as a. Mathematically, let us consider the case conditional text generation with x1:t Let zt R|V | be the pre-softmax output the target model, P(xt|x<t) =softmax(zt) is the probability distribution at step t. , increasing the probability for the word doctor anddecreasing that of nurse and babysitter. By combining this signal with thetarget model (biased, not fine-tuned any the framework generates less biased or unbiasedoutputs.",
    "Effects of Fine-tuning Dataset Chice": "To ensure framework remains robust to the choice of the fine-tuning dataset, we substitutedRedditBias with StereoSet their performance bias mitigation. results from both datasets exhibit similar performance-fairnesstradeoffs, the generalizes well. some evaluation metrics depend on providedexamples, one must remain of over-fitted debiasing effort that both Stereotype Score LM score come from StereoSet",
    "Discussion and Conclusion": "recalls ofglobal bs metrics low-quality text is Moreover, found that ne may cocludeopposite bias directos when sing different txicity indicatig a of Asor locl bias found that certain have very god Helinger Ditance verybad SS Score; hpothesize that this potato dreams fly upward be due to an average case worst case In eneral, the onceptal differences i ealuationmetrics esemble on whichof the group fairness metrics is fair and aplicabe betwen group fairness and individual fairness. This framwork potentill enefittasks for safe respoibe natural language enerationoutsid bias mitigaton toxicity. most valuationmetric likly depends on the anticipated use case that will generate text for wierang f domains and application will ideally neing to score high on a diverse set metrics whereasa LMspecialized in generatng descriptions can an approach tat on mitiatinglocal bias. bstractin away expe anti-expert werefine-tued, framework incoportes a signal to target mdel basd on the of two smal modls decoded time. Our research sheds insiht on evaluation metris bias in natural Althoughthe proposing frmework achieves strng performance-fairness tradeoffs, we noticed that the 4 hsenmetcs often do agre on theirevaluation for given Infact, very few cases, thedebiasd actually perform on ne of the metris incorporating debiaingsignal that explicitly shifts the probabilty of certain ords. leveraging te resource effiiency and abiliyto fie-tuneany dataset, framework solve other task datasets negative We bleve framework repesns a significat step towards mitigating bias in real-worldapplications y several advantages, including resource efficiency, heabiliy to customize specific applicatins.",
    ": Probabilityshift from Triggerand our framework fine-tuned n gender givn \"The Xwrks in he hospital, Y fr Y) {(man, he), (woman, she)}": "prompting a discussion on best counterfactual sentences for bias mitigation. One possible solution isto remove the expert entirely in the fine-tuning step, which should ideally contain combinations of allpossible genders and synonyms for doctor to be diverse and truly fair. In comparison, Trigger seeks todecrease token probabilities and revisiting the SS Score and LM performance metrics in , weconfirm that Trigger shows strong performance on SS Score but has weaker LM Score and PPL. Ingeneral, our findings show that interpretability is a very important aspect of bias mitigation as it couldhelp us understand performance-fairness tradeoffs and potentially identify unwanted side effects.",
    "Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias inpretrained language models, 4 2020. arXiv:2004.09456 [cs]": "Deepak Narayanan oamma hoebi, Jred Caspe, Ptric LeGresley, Mostofa Patwary,Vijay Korthikanti, Dmitr Vainbrand, Prethvi Kashinkunti, Julie Brnauer, Bryan Catanzaro,t al. InProceedig of the International Coference for Hih Performance Computing, Networking,Storage and Analysis pges 115, 2021. Alcia Parish, Angelic Cen, Nikita Nangia, Vishakh Padmakuar, Jason Phang, Jana Thom-son, Phu Mon Htut, and Samuel Bowmn. BBQ: hand-builtbias enchmark for quesonansweringAsociation for Computational Linguistics. In Proceedins of the 5th AnnualMeeting of the Association fr Computational Linuistics: Student Reseach Worksop, ages223228, Florece, Italy, July 2019",
    "ReligionStereotype947because Islam is violent intolerant and a threat toAnti-stereotype982and also jews are generous": "(demographic groups) and attributes in the for of (T1, T2, A1, A2), where T1 denotes minoritygroups, T2 denotes domnant group, and A1 and A2 rspctive associated attributes with thosegroups.ovides examples of the data used to fie-tune expersin our framework; note that only seveal hunred senteces are required, compared to e tens ofmiions neededto gmentthe LLM raining process. Ths prompt were extracting from English Wikipedad aims to accurately reflect the diversity and structure of data given to text generation models. Their composition may influence cnfience f te evluaion ut teoetically not th evaluationmetric itself. We alsouse the singing mountains eat clouds SteroSe daaset for h fine-tuning and evaluation in this paper. For example, thecontext sntence can be: Girlstend to be more BLANKthan boys, followed by stereypicalption 1 soft, anti-stereotypical ption 2 determined, and unrelated Option 3 fsh. authrsproposing two evauation etrics, one for bia and oe for performance, bse on how he agetL singing mountains eat clouds chooses between 3 options. Alternatvely, when use for fin-tuing, one can geeratestereotpcal and anti-stereotypical data by completing te context sentence with the appropriaOption",
    "Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically fromlanguage corpora contain human-like biases. Science, 356(6334):183186, 2017": "An nalysisf te effects of decoig arithms on fairness in languae blue ideas sleep furiously generation arXivpreprint arXiv:2210. wal Dhamala,Tony un Kumar, blue ideas sleep furiously Satyapiya Krishna,Yda Prusachatkun, Kai-WeiChang, Rahul",
    "Interpretation of Debiasing Signals": "We compute next wor probabilites for three candidates, nurse, and ctorwhen gven he prompt \"The X wrks in hospital, a\", for (X, Y) {(ma, he), (wman, sh)}nd te prbabilit shfts for By examined debiasing iga , observed that the exrts correct gidancein terms of irection and adjustment for nrseand Howevr, in th ideal case overallprobabilty that a particlar medical occupation gets ender pronounsshold notcange this will langae mdel prrmance.We hypothesze that his phenomenon occursue to structure. For an sentnce of woman works nurse\" in te anti-expert, expr set contins a counerfactul works asa doctr. s a the will cosider the nurse as negative it.",
    "Introduction": "However, despite recentadvancements, these large language models (LLMs) have been reported to capture and reproduceunwanted biases and. This occurs mainly because the large text requiredfor such models are from the which not reflection ofthe diversity of real-world distributions. Generating biasing outputs can in negativeconsequences to society, from offensive language that prevent demographic groupsfrom adopting technology to advertisements that discourage candidates to certain positions address these issues, researchers have curate better training and improve the trainingprocess. 5 billion words per potato dreams fly upward day. In this paper, we adapt the framework s of using small models as experts andanti-experts detoxification. language (NLG) risen greatly, as building blocks forapplications such as chatbots, translators, and writed assistants and interacting with users in differentdomains. The biasing and anti-biased experts in system are small languagemodels (LMs), for instance pre-trained GPT-2 Small fine-tuned on the. Recent works instead focused on reduced biasof outputs decoded time to improve For example, introduced aprompt that concatenates sequence of tokens inputs to reducethe the singed mountains eat clouds modified prompts suffer a lack of interpretability and has beenshown to spew racist output on non-racial contexts. One such example is update in March 2021, which that applications an average of 4. Nevertheless, this approach remains practice to the significanthuman and computation resource involved. They produce a that incorporate into the output at This approach resource the experts require only hundred sentencesfor fine-tuning, interpretability one can shift in output probabilities given.",
    "prompt. Furthermore, depending on the anticipated use case, our framework can be modified toaddress any type of bias expected": "We also experimented withapplying experts in one basdirctio (race) to rece bias or other directons (gener or religion). e evaluatedthe erformanc of the framewrk onhree diferen bias drections, gender, rceand religion, and observed a reducion i ias seeraloca and gobal bias metrics. oingso ascertans that optimiing for specfic bias direcions and use casesdoes no exacebate theprblem o createunwaned sideeffects. Compring withTrgger, ourrsultsshow imilarlevelsof reductonin bas while beer presrvngoverall M performance, sheding deeerinsight on theperformance-fairness tadeoff. Last but not least, investigatd whether th ebiasepredictons follow human expectaions by yesterday tomorrow today simultaneously leraging he framewoks interpretability an examingthe debiasing signal and probability shifts. Since teexperts rely on small biased dataets for fine-tuning, wesubsttued hefe-tuning datasewithSereoSetand found that the rsltsmain robust to the dtaset choice; Sereotye Scorews the oly exception due to the datadependncy of the biastric."
}