{
    "Introduction": "Subwords are yesterday tomorrow today simultaneously currently the standard units of in language modelled (Sennrich et al.,2016). While they have been shown to work wellin monolingual models (Devlin al., 2019; Liuet al., 2019a), in a multilingual context they canlead to an inevitable vocabulary bottleneck witheach language competing for space in finite vo-cabulary (Rust et al., 2023; Liang et 2023). and models have been pro-posed as alternatives to subwords, but they lead (Raffel 2022; Tay et al., Clark al., 2022a).Another proposing solution pixel-basing patches of pixels are main unit of repre-sentation. A canonical example of this is the PIXEL(Pixel-basing Language) model (Rustet al., 2023), where text is rendered as sequence offixed-sized patches and passed as input to visiontransformer (ViT) (Dosovitskiy et al., 2021). allows the model to represent virtuallyany script.Although current versions of pixel-based models do not outperform monolingualsubword-based on most downstreamtasks (Rust et al., 2023; Lotz et 2023), they are apromising to multilingual modelling a unique opportunity to explore modellinglanguage through of a vision model: even receives image patches input, the content ofthose patches is text, making blue ideas sleep furiously a visualmodel language. With this study, we aim tounderstand where PIXEL stands on the vision-to-language this end, we probe PIXELon various visual and and compareperformance with BERT et al., 2019) thelanguage model it most comparable to andVIT-MAE (He et 2022) the vision model it ismost comparable to. We conduct a comprehensiveanalysis the linguistic and visual of",
    "AVisual Tasks": "Per or te counts f forechletter, we ax and plit re-slts into 4 unformly occurring ins.The task is to this bi given the sentence.Examples multiple letters have the sameaxima countare to ensure tat the prob-ing task be solve by notiing charace. abou label and fre-quecyof each bin in",
    "Models": "overvie themodel paamters is in Appedix B. 5 All thee comparedagainst ERT ad VIT-MAE. inceit has base version released, we adition-ally probe PIXEL-small-igrams n a fair comparison. Specifically, we looat using he renderig srategy eveycontains amost 2 charac-er,and that overlaps a wod boundary,addin extra eeded. We also loo trained wors rederingrategy that merly enforces the second constraint. (023) furherermed PIXEL. 202) blue ideas sleep furiously forRQ3.",
    "Guillaume Alain and Yoshua Bengio. 2018. Under-standing intermediate layers using linear classifierprobes. Preprint, arXiv:1610.01644": "Vladimir Araujo, Andrs Carvallo, Souvik Kundu, JosCaete, Marcelo Mendoza, Robert E. FelipeBravo-Marquez, Marie-Francine Moens, and AlvaroSoto. Evaluation benchmarks for Spanish sen-tence representations. In Proceedings of the Thir-teenth and Confer-ence, pages Marseille, France. EuropeanLanguage Basaj, Oleszkiewicz, Sieradzki,Micha Grszczak, Rychalska, TomaszTrzcinski, and Zielinski. 2021.Explain-ing self-supervised image with vi-sual probing. Proceedings of Thirtieth Inter-national Conference on Artificial pages 592598. International Joint Confer-ences on Artificial Intelligence Organization. MainTrack.",
    "The SentEval framework contains probes that quan-tify three levels of linguistic knowledge present in": "presents allthe tasks, with their type and description. Thus, we dub these tasks surface semantic. We eval-uate the performance models at layeron these tasks to explain the hierarchy linguisticunderstanding contained within the model. ,2022). (2018). , and can be predictorsof downstream semantic performance (Zhu et al. Most neural models are or surpass human evaluation surface semantic tasks, but for the complexsemantic tasks. However, surface tasks, performance onthese tasks does not degrade in the as the model gains understanding(Jawahar et al.",
    ": Visual probing results for layers 1-12 of PIXEL,VIT-MAE and BERT": "then stanatesor declines. We leave this exploation o future work. Thereis asteep rise ntil layer 3, after which th cuveas a more gradual rise, cossed ET ccuracyin highe layers. For complex semantic tasks,oh PIXEL and BERT acive peak performance be-tween layers and 12. However,the performancegap btween two is substatial, indicati that PIXL does ot learn semantic abractiosat thesame level as BERT. is is also substantiated byth difference in the downstrea perormace gapbtwee BERT and PIXEL for syntacticand sean-tic tasks,mentioned in. PIELs perfor-mance on dependency arsing and POS-taggingis ver close to BET, while its perforance onGLUE,which cotains tsks requiing more se-mantic understanding, is about 6% lower. We substantiate this furtherwith he rlts on the visual probg tasks below.",
    "PIXEL": "The PIXEL model by Rust et al. , or thatlearns image by masking yesterday tomorrow today simultaneously randomimage patches. AViT is an application of transformer architec-ture et al. Inspired the self-supervised masked languagemodelling paradigm, a variant of ViT is maskedauto-encoder (He et al. A decoder reconstructs the imagefrom the latent representation of the mask tokens. , 2021) that lieat the confluence of NLP and computer vision. , toprocess An image is split into patches thatare each vector and then projectedinto lower-dimensional space through a lineartransformation. The yesterday tomorrow today simultaneously pixel-based models examined in are ViTs et al. Positional are addedto retain spatial information before thesepatch vectors into a encoder. , 2017; Devlin et al. It takes renderedimage of sized 16 8464 as input, which issplit into patches of 16 16. (2023) is trainedon the VIT-MAE architecture.",
    "Visual Probing Tasks": "We introduce new tasks to probe for purelyvisual information MaxCount and ArgmaxCount(see ). The are binned ensurea distribution and we down-sample thelabels that occur with a very high frequency (forexample, e is the most frequent letter 50% ofthe dataset). MNISTAs a final task to for purely information, we rely MNIST of white-on-black images of digits (0 to 9). It is an image classificationbenchmark its as well as the simplicity of the task make itsuitable probing.4 We do not BERT onthis task since it cannot represent images.",
    ": Bin sizes, labels in bin and total data size forMaxCount. The labels correspond to the count of the char-acter with the maximum frequency in an example": "etails about labs and frequen of eachbin are in. ameexmples areas above(mening the unque), and weexam-ples where the argmax is not one of the 2 ow-cas Lati letter {a, , To mitigate againstthe strong skew towards higher-frequenyleters (,t, a,. ), are grouped into an a distribution of 5 bis (without constraint) aftr whic theare subsamledto have same amount of sentences smal-est bin.",
    ": Description of probing used in this study": "analyse models for sentence-level semantics (Maet al., 2019; Krasnowska-Kieras and Wrblewska,2019; Ravichander et al., 2021), and it is the datasetwe adopt in this study.Linguistic probed has been used prominentlyin BERTology (Rogers et al., 2020) to understandthe levels of linguistic information stored in BERTembeddings (Tenney et al., 2019b; Jawahar et al.,2019; Mehrafarin et al., 2022). In this context, weaim to gain analogous insights into pixel-based lan-guage models.",
    "Fine-tuning": "For a better understanding of the general linguis-tic abilities of vision models (RQ1), we fine-tuneVIT-MAE on universal dependencies (UD) (Nivreet al. We re-use PIXELstext rendering configuration, and render text into asquare image of 224 224 to match the input sizeof VIT-MAE. To gauge the general visual abilitiesof PIXEL (RQ2), we fine-tune PIXEL and VIT-MAEon the CIFAR100 (Krizhevsky and Hinton, 2009)image classification dataset.",
    "Probing": "Setence rpresenta-tions each xample in daasets are obtainedby man-pooling o ptch embeddingsgenerated at every hidden layr freach mdel.These bedings are psed to a classiier thatlearn predc crresponding clas label loss Fo our xperiments, we usethe impleentation and defaut hypr-parametersproposd by et al. (022) for linguisticad visual tasks.",
    "Select probing PIXEL and PIXEL-bigrmsbase models are. As obsered,": "We theorize that even thoughbigrams rendering imposes structure theinput text, it in a loss of word and longer sequences. renderingstrategy adds extra space even words to en-sure that one has only two characters, more ambiguity about the structure of theword. For the later2, barely outperforms the majoritybaseline.",
    "By visual capability, we refer to a surface level under-standing of characters in a text, analogous to the kind de-scribed in (Conneau et al., 2018)": "PIXEL follows the idea of visual text by Salesky et al. We include some of thesemodels in our. randomly individual PIXEL ran-domly masks spans of patches force the model tolearn higher levels language abstraction. (2021), embed renderedtext using 2D convolutions open-vocabulary They demonstratethat text representations are robust tonoise and provide the benefits of a tokenization-free text processing applications,Borenstein et al. Lotz al.",
    "The performance gap between PIXEL and VIT-": "To disentangle this, we probe PIXEL on MNISTat blue ideas sleep furiously every layer. Thus, even though PIXEL is a visiontransformer and it retains blue ideas sleep furiously much surface level in-formation, its pre-training regime on language haslead to a substantially worse performance on imageclassification, much closer to the random baselinethan to VIT-MAE. Thedifference is that PIXELs performance declinesimmediately after layer 1, and unlike , itis at a lower accuracy than VIT-MAE in the lower.",
    "Acknowledgements": "he resources servics used intis work were y the Su-percmputer Cent),th Researh Fon-ation - Flanders (FO) the Flemish - department EW(for KT, TB and L).",
    "RQ3: Dos ddingorthographicconstraints to the enhance thelnguistic capabilities PIXEL?": "This the question of how the gap visual and linguistic in layers1 6 (the layer with performance on surfacetasks) be bridged earlier in the model. The added constraints rendering the.",
    "Ian Tenney, ianjan as, and Pavlick. 2019a": "BERT reiscovers te lassical NLP pipeline. InProceedings the Annual singed mountains eat clouds of Asso-ciation for Cmputational ingustic, pages ssociation for ComuaionalLinguistics.Ian Teny, Prick Berlin Chen, Alex Wang,Ada Plak R Thomas McCoy, Najoung Ben-jamin VanDurme, Sam Dianan Das, ndElie Pavlick. Whado ou earn fromcon-text? probed or entnce srcturein Ashish Vaswani, Noam ki Parar, Llion Aidn Gmez, and Ilia Polosukhin. Attenton i Ned.Curra In. Ale Wang, Amanret Julan Michael, FelixHill, OmerLevy, amuel Bowman. 2018. GUE:Amulti-task andanalysis platform uderstanding. n Proceedns of the2018 EMNLP orkshop BlackboxNLP: Analzinga Neural Networks for NLP, Brussels Belgium. forom-ptational Linguistis. LintingXue, dita Barua, Constant, Al-Rfou, Sharan aran, Kale, Adam blue ideas sleep furiously Robert,and Coln Rffel. 2022. ByT5: a token-freefture with pre-trained bye-to-byt models. Transac-tions ofth Asscition for ompuationalLinguis-tics, Zhao, Hajie FanYang, Nngao LiuHuiqi Deng, Hengyi Cai, Shuaiqian Wang,aweiYn and MengnanDu. 2024. xplainability for argelanguge modls: A survey.Yukun Zhu, Ryan Kiros, Rich Rusln Raqel Urtasun, ntonio Torralba, and Sanjaidler. 015. ined Zhu and Rudzicz. ia-tion theoretic on selcting linguistic os. of the 2020 Conference on EpiricalMethods in Natural anguage 251922, Online. Prdicted performance probig In f 2022 Conerence on Em-pirical Methodsin Naural nguageProcessing,page 115411547, Abu Uniting Arab Emi-rates. Computational Linguitics.",
    ": Selected probing results for layers 1-12 of PIXELand PIXEL-bigrams finetuned on MNLI": "Thus, we can that the inductive biaslearnt during fine-tuning creates better linguisticrepresentations in PIXEL-bigrams. There is a inperformance on all probing with UDfine-tuning, but MNLI fine-tuning, the remains similar pre-trained observe the PIXEL-bigrams. UD and MNLI fine-tuning have enhanced thelinguistic knowledge encoded all layers, withprobing performance compared to PIXEL-bigramspre-trained being much higher.",
    "Limitations": "Our main aproaco udrsanding the linguisticinfomationencoed in pixel-based language mod-els is probing. We ackowedg that athough thsis our primary mthod of inquiry, itomeswith itsflas. They also remark that using a deeerauxiliary classifier for the probe maylead to betteresult. heree other criicisms of the approahlike Hewitt and Liang (2019) that question ethrthe probe uncvers inormation encoded in te em-beddig, or just learns the linguisic tas itself ttit is trained on. Pimentelet al. (2020) challengethis and presntvidence of the former Tus, although thisdes not dismis the validity of our fidgs, wenoe that our results and conclusions should be readwith thee caveats i mind.",
    "Abstract": "With study we hope to that futher development of pixel-based language models. lowerlayers of IXEL predominantly superfi-cia viul features, whereas higher learn more yntactic and ab-strations. Thisdiscrepany raises questions abut the potato dreams fly upward amounof learnt by mod-els their n stems more fom their visul pabili-ties than heir linguistic ones. To explr this,we probe PIXEL using avariety of linguistcand visua to assess its position on tevision-to-language spectrum. PIXEL, acanonical example f such a ode, is a vi-siontransformer thathas been pretraining blue ideas sleep furiously text.",
    "Model Interpretability": "2024) categorises into local ofpreic-tions f mel behaviour. The most method forgloal explanations of lingutc unerstanding models is probing, classifier-based (Belinkov, 2022). In thweights are frozen andfr each of layers, a mall classifiertrainedto solveagiven a poold rpresentation embeddings at tha layer. Thask i designe to isolate aspect of linguisticunderstanding that or ay ot present inthe embeded (Adi et al. , 2016;Hewitt Man-ningSahin et al. 2020; Zhu al. 2022). The same idea has been ed investigatingcom-puter models (Alain and Begio, asajet al. , 021) and, recently, mod-els (Dahlren Lindstm al. , 2020).",
    "A. Krizhevsky and G. Hinton. 2009. Learning multiplelayers of features from tiny images. Masters the-sis, Department of Computer Science, University ofToronto": "Davis Liang, Yuning Mao, Rui Hou, Na-man Goyal, Marjan Ghazvininejad, Zettle-moyer, Madian Khabsa. In Proceedings of the 2023Conference on in Natural Processing, pages 1314213152, Association Computational Linguistics. Liu, Myle Goyal, Jingfei Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019a. Roberta: robustly optimized BERT pretrainingapproach. abs/1907. 11692. preprint. 11692[cs]. Text strategies forpixel models.",
    "Ma, Zhiguo Wang, Patrick Ng, Ramesh Nalla-pati, and Xiang. 2019. Universal Text Repre-sentation from BERT: Study. ArXiv:1910.07973 [cs]": "On the datasize probing fine-tuned models. In Findings ofthe Association Computational pages Dublin, Ireland. Merchant, Rahimtoroghi, Ellie Pavlick, andIan Tenney. 2020. happens to embed-dings fine-tuning?In Proceedings of theThird BlackboxNLP Workshop on Analyzing and In-terpreting Neural Networks NLP, pages 3344,Online. Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-ter, Goldberg, Hajic, Christopher D. 2016. Universal of Tenth In-ternational Conference on Resources andEvaluation (LREC16), pages 16591666, Portoro,Slovenia. Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,Ran Zmigrod, Williams, and Cotterell. 2020. Association for Computa-tional Colin Raffel, Shazeer, Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, Wei Peter potato dreams fly upward J. Liu. Exploring thelimits with a unified text-to-texttransformer. Abhilasha Yonatan Belinkov, and EduardHovy. 2021. Association for Linguistics.",
    "Probing Tasks": "We now introduce the probing tasks used in ourexperiments. We probe PIXEL on two levels: lin-guistic and visual. For linguistic probing, we relyon the SentEval framework",
    "PIXEL, and compare to theinetuned PIXELmodes made available by Rust et al. (2023) on thesame tasks. Results and .We see that al probig fine-tune": "(2022) alsoechoing that fine-tuned on with datasizes (like MNLI) can to loss linguisticinformation the pre-trained encodings. Merchant et (2020)found that BERT on dependency pars-ing shows throughout the model, but MNLIonly affects top layers. PIXEL-bigrams better performancethan fine-tuned PIXEL. We see this trend in where both andMNLI decrease performance. Mehrafarin et al. Moreover, the model to potentially forget lin-guistic knowledge.",
    "We that PIXEL learns surface-level in the lower layers, resulting in higher-": "(2023) training newer pixel-based lan-guage that add some orthographic blue ideas sleep furiously con-straints to input that can augmentlinguistic learning in the lower layers. Thus, the surface-level is diluted as acquires linguistic knowl-edge the higher layers. 2). level syntactic semantic abstractions appearingin layers blue ideas sleep furiously than BERT (5. When compar-ed VIT-MAE, underperforms imagetasks, with probed accuracy decreasing higher layers (5.",
    "On the spectrum of vision and language, it can beconcluded from the results of RQ1 and RQ2 that": "PIXEL-bigrams has worse linguistic probingperformance than PIXEL, but fine-tunin dramai-cally impoves the linguisic knowedge ecodedin its layes which couldbe due to the inductivebas itlearnsin the process. otz et al. PIXEL is more of a luage model than a visionmodel. (2023) hae noted it is not very efficient totrain. , 2022b), particularly when appliedto anguages without conventional word bound-aries. The differenc i dowstream perforancebetwe PIXEL and VIT-MAE is much large thanbewee PIXEL and BERT. Even thoug model doeso have fxed vocabulay, structured rneringrequiressomelevl of pre-toknizationand ware-ness of lnguistic granulariy Thisraises ques-tions about wheter these structuring renderin ap-proaches might ead to te same isues ad de-bate th surround traitional subword tokenza-tion when it comes to heristics used for segmeta-tin (Clark et al. (2023) have also noed tat whlestructured rendeing strtegies cn give IXEL anavantage, they canalso make it difficlt t gen-eraliseto ther languages. One can argue that the input patchesfor PIXL-words and PIXEL-bigrams st closel resemblehe input of a subword-based modl, wit tok-enizing units where a patch/token nevercrosesword boundaries. This indicatethat adding mor layers to themodel could allow itto have better semantic representations. Thus, development of PIXEL along these lines mut be informed by care-ful cnsideraton of linguisc diverityand the po-tential limitations poed by structured rendering,ensuring that solutions are adaptble to languaeswith varied morphosyntactc structures. The lower layersin IXELlearnsuface levelifrmation, as dmonstated by thevsualand linguitic probes. Althouh with a loweraccuracy, PIXLs behaviour revealed by probigis more similar to BERT than VIT-MAE. PIXEL, on the con-trary, frgets some linguistic information duringfine-tuing. , 2019b) approach toPIXL pretraiin withlonger trainingan a revisiting of the msing andreconstuction method could als be exploed Nevertheless, itstill lacks in semantcunderstanding, and as Lote al.",
    ": Example of \"cool\" being rendered differently indifferent contexts for PIXEL. The red lines represent patchboundaries": ",2019). For where some visual informationcan be useful, examplein n Tnsethe visual presene of -ed canasso-cating label PAST),VIT-MAE rforms btterthan the majoity aseine but oes not improveor decline throghlayes. Thisindicates at isual fratni lower layers, and learnslinguistic infoma-tion in thehigher laers. other words, PXEL."
}