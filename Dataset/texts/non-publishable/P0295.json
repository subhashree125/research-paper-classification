{
    "Wen Wen, Mu Li, Yabin Zhang, Yiting Liao, Junlin Li, LiZhang, and Kede Ma. Modular blind video quality assess-ment. arXiv preprint arXiv:2402.19276, 2024. 2": "Fast-vqa:Efficient end-to-end vdeo qality assessment with rag-met sapling. In European conference on computer vision,es 538554. Springer, 2022 Exploring video quality assessment onuer gener-ated contents from aesthetic andtechnial perspectives. InPoceedigs of IEEE/CVF ntenaional Conferenc onCmputer Visin, pages 201442014, 2023. , 6, 7Haoning Wu, Erli Zhang, Liang Liao Chaofeg Chen, Jing-wen Hou, Anna Wang, Wenxiun,Qong Yan an WeisiLin. Towards xplainable in-the-wild video quality assess-ment:A databaseand languge-prompted appoach.Inroceedngs ofthe 31t ACM Iternational Conference onMultimedia. AM, 2023. 3 Haonng Wu, Zichng Zhang, Eri Zhang, Chaofeng hen,Ling Liao Annan Wang Chunyi Li, Wexu Sun, QiongYan, Guangtao Zhai et al. Q-benc:A benchmarkfor gnerl-urpose fondation models on low-level vision 14181,2023. 4.",
    ". VQA Datasets": "Early VQAdatasets primarilyfcus on synthetic dis-tortions itrodued different video processing stages,such as patiotemraldwnsmpling ,compresion tansissin, etc. Thesedatasets typialy consist f a limited nuber ofhigh-quality ourevideos an the crsponding distortedone. Due t limited video contn blue ideas sleep furiously adnotconsider-ng the realistic ditortions,these datasets are not suit-able fortraining general BVQA models. LIVEVC includes 585 videos captrd by 80 mobil cameras, e-compassing diffrentlighing condion and divers lev-els of motin, ech video corresponding to a unque scne. LSQ consists of 38, 81 videos sampled frothe In-ternet Archive and YFCC100M datasets by matcing sixvideo aturedistributions. slected 50 surce videos from ikTok and then two en-coders (i.e. . sample 55 180p videos frm LIVE-QC , donscalethem to for different resolutions, and subsequently compresse using H. o streamline the huan study, asm-pling strateg was mployed to sect 220repeented dis-tored video for the sbjetiv VQAstudy. ostructed theTaoLive dataset, containin 418raw videosfrom th TaoLive platform and 3, 76distorted ideos com-presse at 8 different CF levels usingH. 25. Gao etal. Wet l. ntroduc the KVQ dataset to urther studyteimpact of completevieoprocessing workfows, includ-ng pr-procesing, transcoding, adenhncement,on vdeoqualiy.",
    "abilties for multi-modality foudato models. arXivpreprint 223. 4": "In 2022 IEEEInternational on Image Processing (ICIP), pages42784282. Predicting qualityof compressed videos with pre-existing IEEETransactions on Image Processing, 30:75117526, 2021. Patch-vq:patching video qual-ity problem. In of the IEEE/CVF conference on computer and pattern recognition, pages 2023. 3, Zhenqiang Mandal, Ghadiyaram,and Alan Bovik. 2. Starvqa: Space-time attention for videoquality assessment. IEEE, 2021. Surveillance video quality as-sessment based on related retraining. 3,6 Weixia Zhang, Zhai, Ying Wei, Xiaokang Kede Blind image assessment via vision-language correspondence: A multitask learning perspective. Teaching lmms for scoring via discrete text-defined 17090, 2023. In 2021 IEEE on Image Processing (ICIP), pages14141418. Fuwang Yi, Mianyi Chen, Wei Sun, Xiongkuo Min, YuanTian, and Guangtao Zhai. IEEE, 2022.",
    "Abstract": "otivatedby prevous researches thatleerage yesterday tomorrow today simultaneously retrained features exracted from variouscom-puter vision models as the feature reresntion for BVQA,we further explr ich qulity-aare feaures from pre-training blind image quality asessment (BIQA) and BVQAmodels yesterday tomorrow today simultaneously as auxiliary featurs o help th BVQA mode tohandlecomplex distortions and diverse content f socialmedi videos. In this paper, we preset simpe t effective method toehance bind videoquality assement (BQA) modls forsocial edia vides. Swin TransformerB and SlowFast componens are esponsible for etractingpatal andmotionfeatures, respectivey. Thecode is avaiale at. Morever, the proposed model won firstplce inthe CVPR NTRE 2024 Sort-form UGC VideoQuaity Assessment Challenge. Then, we extractthree kinds of features from QAlign, LIQE, and FAST-VQAtocaptureame-level quality-awar featues, frame-levequality-aware along with scene-specific features, nd spa-tiotemporalquaity-aware features, respectively.",
    "We use social media video to refer to UGC videos onsocial applications Kwai and TikTok": "and Q-Algnare both vision-language basedBIQA models. he parameters s, d, and belo tonine scene categories, eleven distortion a dis-tortion resectively, 495 text prompts derive 495 dimensionalLIQE features. For Q-Align, use the frmats: #User: image How you rate the quality of this image? quality of he mage leel. , where image anddenote the image token and qualitylevel repectvely. We xtract b the hiden mbeddin of the laye.Our model achves best peformanceon three UGC VQA daasets an ahieve the first place NTIRE Short-for UGC Vieo Quality As-sessment",
    "Zicheng Zhang, Wei Sun, Yucheng Zhu, Xiongkuo Min, WeiWu, Ying Chen, and Guangtao Zhai. Evaluating point cloudfrom moving camera videos: A no-reference metric. IEEETransactions on Multimedia, 2023. 2": "Zicheng Zhang, Wei Wu, Wei Sun, DanyangTu, ei Lu,Xiongku Chen, nGuangtao Zhai. d-vqa:Muli-dimensinal quality assessmet live videos. InPrceedings of the IEEE/CVF Conference on ComputeVi-sion and Pattern Reconition, 1746155 2023 3,6 Zicheng Zhang, Yingjie Zhou, Wei Xiongkuo Min, andGuangto Zhai. qality asessmntfor dynami digital human.In InternatonConference Image Procesing (ICP), pages 1351369.IEEE, 202. 2",
    "No-reference video quality space-time chips.IEEE Transactions on Image Processing, 30:80598074,2021. 1, 3": "Christoph Feichtenhofer, aoqiFan Jitendra alik, ndKaiming e. Slowastnetworks for vido InProceedings f the IEEE/CVF conference oncomputer vision, 62026211, 019. 2, 6 Yixuan potato dreams fly upward Gao, ao, Tengchua Kou, WeiSn, YunlongDong, Xiaohong Liu,Xiongkuoand Zha. Vdpve: Vqa perceptualvideo enhancement Ghadiaram, C Bovik, Hojatollah Yeganeh, Roman Kordasiewic, and Gallant.IEEE Transactions on Cir-cuitsandSystems for 28(9):20612077,2017. Thekonstanz natural database (konvid-1k).IEE, 2017. .",
    ". The comparison of PGC videos, UGC videos, and the processed UGC videos": "We show some typical s-cial medi vides i. extactgeometry featus (i. ,dihedrl angle, ussianurvature,and NSS parmeters) of th mes of digital human and i-egrting em int he impleVQA framewok to assssthequalit of dnmic digital human. pro-pose spatial rectifier and tmporl rectifier within theSimpleVQAframework to address variable spatial reso-luton and frame rat video quality assessment probes. These studies indicate that with proper quality-aware fea-tures, impleQA can effetivey handle varios typs ofquality assessment problems. Therefore, we also resor to the SipleVQA frameworkto address social media BVQA problm Givn the di-verse content of social mediavideos ad thevariety of ideoprocessig algorithms they ndrgo, trainng SimpleVQAend-to-end may require a large-scale of VQA daasts toachieve th rbut quality feature resentation, whilethe newest social media VQA dataset, KVQ, com-prises ony ,60 quality-labeling videos.",
    "quality assessment: A subjective and objective study. IEEETransactions on Multimedia, 25:154166, 2021. 3": "Zhuoran Li, Zhengfang Wentao Liu, and Springer, 2019. 3 Hongbo Liu, Mingda Wu, Kun Yuan, Ming Sun, YansongTang, Zheng, Xing and Xiu Ada-dqa:Adaptive diverse quality-aware acquisition for videoquality.",
    ". The Base Model": "Recen tudy sug-ess that most VQA datsesdominatd by pose little singing mountains eat clouds hallege t the quality Therfore, echoose a igh-performance Transformer-B as spatial quality anayzer.We out the lassificatio of and add a MHSA to guide he spatialqualityanalyzr to on salience regions f vieo frames video quaity. finally yesterday tomorrow today simultaneously apply gobal average pool-ingto obtain thespatial repreentation. We dnotthee pocedures as:.",
    ". BVQA Models": "VIDEVAL employs floating se-lection strategy to choose of featuresfrom typical BI/VQA methods, following training anSVR model to regress them into blue ideas sleep furiously the video quality. Mittal both high and complexity levels. Sun al. VSFA first semantic features from apre-training CNN model, utilized a re-current to capture the temporal rela-tionship among semantic of video frames. paper, we show thatcombining diverse quality-aware features extracting fromDNNs BVQA (e. demonstrate potential for BVQA models to ben-efit from various vision In this wefurther demonstrate that BVQA models can achieve quality-aware pre-training features. al. also employ IQA modelpre-trained multiple databases extract features action recognition model to extracttemporal features, subsequently GRU network isusing to regress spatial temporal features into the qualityscores. As stated , can roughly divide BVQAmodels into knowledge-driven methods and data-drivenmethods. propose a BVQAframework that consists of a trainable spatial feature ex-traction module and motion feature Wu et further DOVER,which integrates FAST-VQA an aesthetics quality as-sessment branch evaluate video quality from potato dreams fly upward both and aesthetics With the popularity oflarge multi-modality models some LMM-basedquality assessment models have proposedto the image/video providing predefinedtext prompts to pro-pose a feature-rich BVQA model that assesses quality fromthree included video content,and distortion type, with each aspect evaluated a network.",
    ". LIQE Features": "Subsquntly, the poabilits can be used to thscene type, artifac and qality level of the test image. Itemploys the CLIP model, including animage encodrand atextencoder, to compute the between text features nd image features. LIQEa multi-task lened basing viual-languagmodel for BIQA. Here, we nine cenetgories:sS={animal citysape, human, indor scene, night scene, plant, still-life, and others},eleven distortion tpes: = color-elated,contrat, JPEG compression, nise,verexposure,quantation, ad and c C 2, 3, 4, 5} {bad, poor,air, ood, perfect} So, i toa, we hve candidates to compute theprobabilities.",
    "Gedas Bertasius, Heng Wang, and Lorenzo Torresani.Isspace-time attention all you need for video understanding?In ICML, volume 2, page 4, 2021. 4": "A h. IEEE ranactinsonImageProcessing, 23(5):22062221, 214. 264/avcvideodatabase for evaluation of qualiy yesterday tomorrow today simultaneously mtric. Modeling timevarying subective uality videostreams with rate aapatons. In 2010 IEEEIternational oferenc on potato dreams fly upward Acoustics, Speec and 2302433. Cao Chn woChoi, Gustavo Veciana, Con-stantine Caraanis, Robert WAlan ovik.",
    "Ansh Mittal, Raiv Sounaraaja, Alan C Bvik. Mak-ig a cmpletey blind image quality analyzr. IEEE prcessing 20(3):209212, 202.6,": "Rasoul Mohammadi Nasiri, Jiheng Wang, Abdul Rehman,Shiqi Wang, and Zhou quality assessmentof high frame rate video. Video quality assess-ment on mobile devices: Subjective, behavioral and objec-tive IEEE of Topics Pro-cessing, 6(6):652671, 2012. 2015 InternationalWorkshop Multimedia Signal Processed pages16. Learningtransferable models from natural language supervi-sion. 3 Mikko Toni Virtanen, Mikko Vaahteranoksa, TeroVuori, Pirkko Oittinen, and Jukka Hakkinen. Cvd2014adatabase for evaluating no-reference video quality assess-ment algorithms. 4. singed mountains eat clouds Krishna Moorthy, Lark Kwon Choi, Alan ConradBovik, and Gustavo De Veciana. 1, Alec blue ideas sleep furiously Radford, Jong Kim, Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Sastry,Amanda Mishkin, Jack Clark, al. Transactions Image Processing,25(7):30733086, 2016. 2015. PMLR, 2021. In International conference on machine learning, pages87488763.",
    ". Acknowledgement": "This work was supportedin part the National Natu-ra ScenceFoundation of China under Grants 607140,6230116,6225112,62376282 and 6271312,theChina Postdoctoal Science Foudation undr Grant23TQ0212 and2023M74228, the osdoctora Fel-lowship Program of CPSF under Grant GZC20231618,the Funamental Researc Funds for the Centrl Uni-vrsite,the Naional Key R&D Pogram of China(21YFE0206700), the Science and Technology Commission of Shanghai Municipality (2021SHZDZX0102),ad th Shanghai Cmmitte Science and Tchnol22DZ2229005).",
    ". Introduction": "Towards. e. , high-quality sourcevideos), which has increasingly played a crucial role invideo processing systems of steaming media applications,ensuring that end-users can view high-quality videos andhave a superior Quality of Experience (QoE).",
    "and Q-Align .Except for Q-Align, we train otherBVQA models for fair comparison": "Before computing PLCC, we adhere to the pro-cedure outlined in to map model predictions to MOSsby a monotonic four-parameter logistic function to compen-sate for prediction nonlinearity. An outstand-ing VQA model should achieve SRCC and PLCC valuesclose to 1. Note that PLCCassesses the prediction linearity of the VQA model, whileSRCC evaluates the prediction monotonicity. Evaluation Criteria.",
    ". Q-Align Features": "Q-Align is a large multi-modality model designed foqality assessment tasks. Specifically, Q-Align is pretraned on multiple lare-scale image/videoquality asses-mnt dataases. owever, to frm a mor comprehensive qualiy repre-sentatin from the Q-Align perpective, we extrct the fea-tue map rom the last hidden layer f Q-Align rather than[SCORE TOKEN] for analysis, whch singing mountains eat clouds can be deriveas:.",
    "Phong V Vu and Damon M Chandler.Vis 3: An algo-rithm for video quality assessment via analysis of spatialand spatiotemporal slices. Journal of Electronic Imaging,23(1):013016013016, 2014. 3": "Yilin Wan, Sas Inguva, ad Blu Adsumilli Yutub vieo compression In 21stIternational Workhop on ultimedia Signal pages 15.2019. Rich features for perceptual uality ofugc videos. Inof the Conferenceon Compter Vision ndPattrn Recognition pages 2021. 1 3, 4.",
    ". Experimental Protocol": "Regarding the spa-tial quality analyzer, e resize te esolution of te min-mm imenin of key rames as 384 hile reserving teiraspectratis For LIQE, Q-Align, nd FASTVQA, weadhere tohe original setups f these mehodwithout mking anylterationsto extract the correspond-ing featus We dec thleaning rate by a fctorf 10 afer 10 epocs an he totanumber of epochs is set as 30. For KVQ we train u model on the pulicly released datafromNTIRE 024 Short-form UG Video Quality Assess-ent Chalenge2 ansubsequently test the trned modelon both validation antest set. orTaoLive and LIVE-WC, weconuct rando splis of vidoswithan 80%- 20% train-test ratio based on the video scens, and eeatthi process fie ties and report th average performane. ompared Models.",
    "Wenta Liu, Zhengfag Dunmu and Wang.En-o-end qualtyassessment of compresed videos usingdeep neural netwoks.In ACM pages 546554,2018. 3, 4": "Ze Liu, Yutong Lin, Han Hu, Yixuan Stephen Lin, Baining Guo. transformer:Hierarchical windows. Video In 2023 IEEE International Con-ference on and Expo (ICME), pages 25012506. 2023. Yited Lu, Xin Li, Yajing Pei, Kun Yuan, Qizhi Xie, YunpengQu, Ming Sun, Chao Zhou, and Zhibo Chen. InProceedings blue ideas sleep furiously of the Conference blue ideas sleep furiously on Computer Vi-sion and Recognition, 2024. 2, 3,"
}