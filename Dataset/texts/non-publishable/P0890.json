{
    "-21, 2002, Montral, Qubec, Canada. ACM, 380388": "20. 234. inton 220. Smple for Contrasiv Learning of Visual epresentatins.InProceedings of 37th Inerational Conerence o Lening,ICM 2020,13-18 July 2020, Virtul Event (roceens f Mchine Learning Researc, Vol.119). PMLR, 15971607.Wen Chen, Jiaming Xu, Guo, Chng Go, Fi potato dreams fly upward and Binqiang 2019. In Proceedingsf the 25t singing mountains eat clouds ACM SGKDD International Coneence on Discovery &Data Mining, 201, Anchorage,USA, August 4-8,2019. ACM, 222670.",
    "Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and J.McAuley. Bridging Language and Items for Retrieval and Recommen-dation. CoRR (2024)": "Wenyue Hua, Shuyuan blue ideas sleep furiously Xu, Yingqiang Ge, Yongfeng Zhang. How toIndex Item for Recommendation Foundation Models. 1998. of the Thirtieth AnnualACM the Theory of Computing, Dallas, Texas, USA, May 23-26, 1998. ACM,",
    "Instrument48,45321,413427,67499.959%Office181,87867,4091,477,82099.988%Gowalla29,85840,9881,027,46499.916%iFashion300,00081,6141,607,81399.993%": "Severalrcet efforts attempt to use wel-founedmehods for structural perturbaion, such as SVDbase adjacenymtrix reconstruci and graph rationale discovery aedon masked autoencoding. Incontast, e csder users/itemswith shar codes o interactiontrgets as emantically relevant. However,perturations on parsegrph can not constuct mor infrmative contrasive views. By aligning user/items wh sim-lar collaborative semantics, we cfurtheunlah th ptential ofCLand enhance the mantic learning of the model. Besides, hs methods typcally indis-riminately distngisrepresentations o differen instances. s acompaison, our approac is both reliable and informative, lever-aging discrete codes as irtual neighbors to reliably nhancenneighbohood structure and aleate data sarsity. Hoeve limied by the low-rak hypergraph matixand te noisepertubation, the generted contrstive views alo suffer from thesemanic dsrupton ssu. epesntation augmentation method involve modeling add-tional nde repreentation as contrastive vews, such as lerninghypergrph reresentaions and adding random noise.",
    "Overall erformance": "overall for performnce between CoGCLand other baseline models are shown in. From the eults,we flowing observations:The CL-basedSimCL, LightGCL)sow consistent superiority over the MF methods (e. In cotrast, SGL tnds indicating that edg/node dropout possibly interferes with crucial structuraliformation, leaded singing mountains eat clouds adverse impats. Thisperformance could be attributed to the self-supervised by contrastive learnng, toalleviate data sparsity and enhance representation learnng ihinCL-based methods, strcture augmentation and representatin exhibit dstinct strengths in diffeent scearios. ,BP, SimpleX)nd GN-based methods LightGN. inally, our propose CGCL consistntly maintains the bestperformane in allcases, achieving improveents oerbaseline mehods Differen from models, te of CL by consructig contrastive viewsthat collaborative Basing on learneddiscrete rich in collabrative we introduce vir-tua neighbor aumentatin and semantic toenhance neighborhood structureand semantic relevance of con-trasted views, Furtherore, trile-view raph across he obtained contrastive views brings insihts tothe As result, CoGCL. g. Conversely, the most competitive mo-els fr iFashion datases are GFormer boh of which arestructure methods. Spcifi-cally, yesterday tomorrow today simultaneously as typical representaion augmentation methodpeforms better than oter modelon Instrment adGowalla dataets, to the improved unifomity hieved ranom noise.",
    "decrease in performance, which demonstrates that all data employed for view generation inCoGCL useful for performance": "4. 3. yesterday tomorrow today simultaneously Apart from the above techniques, we further investigate how thealignment and uniformity of CL affect our approach. We disablethese two terms respectively in the CL losses (i. 4) by applying the same gradient-stopping operationsin empirical analysis (. 2). Specifically, we construct thefollowing variants for detailed exploration: (1) w/o A and (2) w/o Uare consistent with. 2, denoted disabling alignment anduniformity in CL respectively, including both L and L. (3)w/o AA and (4) w/o AU only involve disabling the above two termsof L while keeping L constant. (5) w/o SA and (6) w/o SUare analogous variants for L and do not change L. e. e. , w/o U) within both L and L leads to a no-table yesterday tomorrow today simultaneously performance degradation. This observation verifies that thejoint effect of these two elements is crucial for the effectiveness of.",
    "Vasuki and Vanathi. 2006. A review of vector quantization techniques. IEEEPotentials 25, 4 (2006), 3947": "Chenyang Wang, Yuanqing Yu, Weizhi Zhang, Chong Chen, Yiqun Liu,and Shaoping 2022. Towards Alignment Collaborative Filtering. ACM, 2020. Understanding Represen-tation through Alignment Uniformity on the Hypersphere. 99299939. Learnable Tokenizer for Genera-tive Recommendation. In Proceedings of the 42nd SIGIR Conference on Research Development Information Retrieval,SIGIR 2019, Paris, France, July 21-25, 2020. Proceedings of the 43rdInternational ACM conference on research and development in InformationRetrieval, 2020, Event, China, July ACM, 10011010. Yifan Wang, Suyao Tang, Yuntong Lei, Weiping Song, Sheng and 2020. Wu, Xiang Wang, Xiangnan He, Chen, Jianxun Lian, andXing Xie. Self-supervised Graph Learning for Recommendation. SIGIR21: The 44th ACM SIGIR on Research and Developmentin Information Retrieval, Canada, July 2021.",
    ": Performance comparison on user groups with dif-ferent sparsity levels": "shown in . Furthermore,our model shows superior blue ideas sleep furiously performance and significant improve-ment in the highly sparse user groups",
    "Conference acronym XX, June 2018, Woodstock, NYBowen et al": "Reliabe and Informative Contastive View Generation 3): Given earned dicrete codes, we ue tem eliablend ifrtive contrastive views by proposing virtualseantic relevance sampling, respetvely.on he genrating contrastie views, yesterday tomorrow today simultaneously we inroducetriple-iew graph contrastive leaning achieve alignment acrossmultiple contative views, as potato dreams fly upward o stronger collao-rative information contained in these views into leaning.",
    ",(13)": "whee ad are users in batch data B. zandz enoe twodiferent ser representatios aftervirtual nighbor augmentationsThelss consists of wo trms, reprentig the bdirectioal lign-mnt o the two views. Anaogously, we calculate the CL los for thitem side as L. e total alignment loss between nodes with aug-mented views i sum of them, denoting as L = L + L. .4.3Aligment Beten Semantically Releant Users/tems. Fol-lowing th semantics relevaneampling meho in .3.2,we andmly slec a potato dreams fly upward poitive exmpl singing mountains eat clouds with similar collboatiesemantics fr ech usr , dnoted as +. The we alig these rele-vant uses t incrporate more collaborative semantc ifrmationino the model. Th lignmen los can e written as:",
    "RELATED WORK": "Subsequently, LightGCN and propose transformation and ctivaton tosimplify GNNs whileimprovig perfrmance. GNN-ased Filtering. Inthe contet of GNN-based existing efforts cane ctegorized int two main to howviws De-spitetheir collaorative nformation within contrastivviews may b disruped inhese metds, and thus the poentiaof CL has not bee In this paer, we propose. With the GNNs, thecommo studies has hifed towards designing effectiv message-passing mecaisms to prpagate user/item embeddings over thegraph. Earlier efforts extrac the graph infrmationusing wlk strategies. Contrastive Learning Recommedatio.",
    "Corresponding author": "Permisio o mke diital or hard opies of all or artof this work for personl orclassroom use is gred without fee providedtht cpies re not made odistribuedfor ofit or cmmrial advante andthat oie bar this notice and the full citationon the first page. ACM ISBN 978-1-4503-XXXX-X/8/06. orighs for components of this work owne by othr tha theahor(s) ust be honored. Request prisons from acronym X, Jue 005, 2018, Woodstock NY 2018 Copyrigh held by the owner/author(s). o copy oeris, orrepublish, to post on servers or to edistribute to lsts, reuires prior speciic perissionand/or fe. Publication rights licensing t ACM. Abstracting with credit i permitted.",
    "Enhancing Graph Contrastive Learning with Reliable and Informative Augmentation for RecommendationConference acronym XX, June 0305, 2018, Woodstock, NY": "). E1 and E2 denote edge sets resultngthe aforemen-tioned vitual eighboaumentationor ll items. SGL , we te dis-crete codes augmented graphs one per epoch during 3.32Semantic Relevance Sampling via odes.our frame-work, we nt only onsider diffrent augmentedviews o thsamenode as ositive samples, but distinct users/itemswisimilar semantics mutully positive, which to a in-formatie contrasive vew. This spervisedositive samplig method has effctveness in variouscenarios, ncludng sentence ebedding and equential recom-mendatio . Given blue ideas sleep furiously the pitivecombined by the instacesfom the groups, w pair sampled relevant instancwit user CL. uthermore, santiclly relevant positivesof items can also be obtained n a symmetricaBy perorm-ing CL withn sampled instces above, aim to nhancethe clustering similar users/tems and improve",
    "(z,z)/ ,(4)": "Intuitively, the first ermmaintains of posive pars, heras te second tempushe negative pairs apart. Here, we try to contributions of the. ese re formally efined as thealignment and of on unit hper-sphe.",
    "Triple-View Graph Contrastive Learning": "Then we obtain representations ofdifferent views based on the same GNN encoder in. 1:. 1Multi-View Representation Encoding. 3. After the above contrastive view generation methods, we can obtainthree contrastive views with stronger collaborative information foreach node through virtual neighbor augmentation and semanticrelevance sampling: two augmented nodes with more abundantneighborhood structure and a semantically relevant user/item.",
    "=1 (z,e )/ ,(6)": "; z, each o dimension wedo notadopt Euclideandstance commnlin prior VQ works but coine similarit, is to similarity measre in CL Eq. ()). Our optimizationobjective is to axiize likelihood of as-signing representations to thei centrs Coss-ntopy CE) lss. PQ nto sub-vectors z z1;. is singing mountains eat clouds the-th code for uer, z dentes user the -thlevel. residuals representations for eachlevel, denoted by z+1 e, and = z. Frally,the training loss ser codelearningis:.",
    ": Performance of discrete codelearning methods": "pro-vides furthr evidence that our roposd alignmnt between thetwo types of bings enhanced ollaborative unifority. w/o AA and (i. ,SU) doesnot in he signficatadvre impact s conjetured. SA) incurs a pronounced decrease n performance. It atributed t the sharednifortyeffect between two CL in each ohere. ,wo L (ie. the proposedapproach, rather than relying solely uniformity. Furtermore, individually disablinguniformity within L e.",
    ": Comparison current graph CL-based methods(e.g., SGL , ) that collaborative infor-mation within contrastive and the proposed approachthat collaborative information": "For reliability, we anticipate that thestructural information introduced by graph augmentation is well-founding rather than arbitrary, that is, based on the observed user-item interactions. Through the above strategies, we can generate various contrastiveviews with stronger collaborative information. Given the user and item codes, as shownin , we can naturally expand a u-i interaction edge toseveral u-codes(i) and codes(u)-i edges. graphs, which are subsequently used by the GNN to generate con-trastive node representations. And the random noise added to node em-beddings may interfere with the implicit collaborative semanticsin node representations. Subsequently, the learned discrete codesare adopted to enhance the collaborative information of contrastiveviews in two aspects: neighborhood structure and semantic relevance. Further in-depthanalyses illustrate the crucial role that our designing componentsplay in enhancing graph CL for recommendation. Thecontributions in this paper can be summarized as follows: We present reliable and informative graph CL approach,namely CoGCL, which constructs contrastive views that implystronger collaborative information via discrete codes. By aligned users/items with semantic relevance via CL,we can further enhance the integration of collaborative semantics.",
    ",(14)": "3. , BPR singed mountains eat clouds loss), discrete code learning objective (Eq. 4Overall Optimization. all contrastive learning loss (Eq. by combining recom-mendation (i.",
    "INTRODUCTION": "Inthe literture of recommender systems, collaborative filterig(C)based on graphneral network (GNN) has shwcased significantsuccess inrecoendation systems due to it ability to modelhigh-order potato dreams fly upward user-item relationsip. Structureaugmentat pertrbs the graph struture to create augmented",
    "Z1 = GNN((Z1), G1),Z = GNN(Z12),": "3. 4. 2Alignment Between Augmented As detailedin. 1, the two augmented resulting from tworounds of virtual neighbor augmentation possess abundant neigh-bor structures. representations are as Z01 = Z02 Z0. The distinct dropout applied the two forwardpropagations result in yesterday tomorrow today simultaneously different features. 3. the representation dropoutwe introduced can also be regarded a minor data augmenta-tion. Afterapplying the readout function, we denote the representations ofthese views and Z, respectively.",
    "Ablation Study": "3.1Alation Study of Data Augmentation. In oder to explore thecntribution of data aumntation methods nvolving yesterday tomorrow today simultaneously in CoGCL. o the folowing vaiants: (1) w/o Repacermoves thereplace operator in virtualneighbor augmentation. (2) w/o Add removes the in virtual neighbor w/o SharedC removes simlaruses/items in semantic relevance (4) woSharedT removessimilr users/items sharedtarget in semantic releancesampling. can oberve tatthexclusion of augmentation would led to a.",
    "where denotes the number of GNN layers, and Z R|V|": "obtainedby two augmented graphs the optimization objective of CLbased on InfoNCE loss. g. Then, predicted score is de-fined as singing mountains eat clouds the similarity between the user item representations(e. blue ideas sleep furiously inner product, = ). g.",
    "Algorithms. ACM Trans. Inf. Syst. 41, 2 (2023), 32:132:41": "Bowen Hongu Lu, Yu Che, Wayne Xin Zhao, MingChen, and Ji-Rong Wn. 2023. Large Lnguage Models by IntegratingCollaboratve Semantics orRecommedation. CoRR abs/2311.0049 (203 arXiv:2311.09049 Kun Zhou, Hui Wang Xin Zhao, Yuto Srui Wag, zheng Zhng,Zhongyuan ang, andJi-Rong Wen. 2020. S3-Rc: Sef-Supervised Learningfor Sequential Mutu Infomation InCIKM 20: ACM Interntional Coferene on Information adManagement, Vrual Event, Irelnd, October 19-2, 2020. ACM, 1931902.",
    "Yongjian Chen, Tao Guan, and Cheng Wang. 2010. Approximate Nearest NeighborSearch by Residual Vector Quantization. Sensors 10, 12 (2010), 1125911273": "Cho, Set A. Myers, and Jur Leskovec. 201.riendship and mobility:user movement location-based social networks. ACM, 10821090. henYu Zheng, Nian L, Ynfeng Yingrong Qin Jinghua Jianxin Chang, Depeng XiangnanHe, and 2023. Recom. (2023), 151. TianyuXingcheng Ya, and Danqi 021. Associtiofor ComputationalLinguistics, 68946910. Marc Gori and Augusto Pucci. 2007. A Random-WakBase Recommender Vloso (Ed.). 27662771.Gray. quntization. IEEE Ass againe 1, (1984), 42. He, Kan Deng, Xiang Yan Li,Zhang, and MenWang. In Poceedigs of the 43rd Internationl AC SIGIR n research dvelopment n Information SIGIR 200,Virtual Evet,Chia, 25-30, ACM, 639648. McAuley, Zhao. 2023",
    "Jeff Johnsn, Matthis Douze,and Jgou. 2021. Billion-Scale SimilaritSearch with Trns. Big Data 7, (21), 5357": "Wang-Cheng Kang and Julian Jhn McAuley. Canidate Generatio itBinary Codesfor Large-Scale Top-N Recommenation. Inroceedngs of the 28thACM Interntional Conference on Information and Knowldg Management, CIKM2019 Beijin, China, November 3-7 2019. ACM, 15231532.Yun-Yong Ko, Jae-Se Yu, Hog-yun Bae, Yongjun Pak Dongwon Lee, andSng-Wook Kim. IEEE, 290299. Donga Lee, SeongKu Kang, Hyunjun u, Canyoung Park, and Hwajo Yu.2021. In SIGIR 21: The 44th International ACMSIGIR Conference on ResrchandDevelopment in InformationRrieval, Virtual Event, Canad, July 11-15, 2021.AM, 15131522. Choiu Li, LianghaoXia, Xubin Re, Yaowen Ye, Yong Xu, and Cao Huan. 2023.Graph Tranformer fr Recommedation. ACM, 16801689.",
    "Fionn Murtagh and Pedro Contreras. 2012. Algorithms for hierarchical clustering:an overview. WIREs Data Mining Knowl. Discov. 2, 1 (2012), 8697": "Shashank Rajput, ikhil Mehta,Aima ingh, Raghunandan Hulikal Keshvan,Trug Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q. Tran, Jonah Saost, MaciejKa, Ed H. hi, an Mahesh Sathiamoorthy. 2023. Recommender Systems withGeerativRetrieval. In Advancesin Neural Iformation ProcssingSystems 36: An-nual Conference on Neurl nformaton Processng Systes 2023, NeurIPS 202, NewOrleans, LA, USA, December 1 - 16, 2023. In UAI 09,Proceedings of Twet-Fifth Conferenceon Uncertainty in Artificial Intelligence,Mntreal, C, Canada, June 1-2 2009. Dinghan Shen,Qinliang Su, Padamoy Chapfuwa, Wenlin Wang, Guoy Wn,RicardoHenao and Larece Carin. 2018. n Proceedigs of te 56th AnnualMeeting of Association for Compuational Linguistics, ACL 2018, Melourne,Australia,July 15-20, 201, Volume 1: Long Papers. Beyond User Embedding Matrix: Learningto Hash for Modeling Large-Scale Users in Recommendation. ACM,319328. 2023. Generative Retrieval with SemanticTree-Structurd Item Identifiers via Contrasive Learning.",
    "Further Analysis": "4. 4. t. Learn-ed Methods. verify the of the proposed end-to-end discrete code learning we compare with the variants: (1) Non-Learnable Code uses Faiss library togenerate discrete codes based on trained LightGCN (2) Euclidean Code adopts Euclidean measure similarity between user/item representations andcodebook vectors in (6), which consistent with the original RQmethod. conduct experiments In-strument and Office and results shown in. In comparisonto Euclidean and PQ Code, our proposed approach shows performance. Compared with PQ the RQ we applied probability relationships codes at each level instead them independent, which is conducive to semanticmodeling granularities. 4. 2Performance Comparison w. r. t. Data verify of approach alleviating sparsity, we on user with sparsity levels. Specifically,following prior works , we divide groups ac-cording to their number interactions, while keeping of group constant. Subsequently, we evaluatethe performance of these five groups of users, and the results Sparse Dense0. 030 0. 040 0. 045 0. NDCG@10.",
    "B.3Embedding Distribution w.r.t.Augmentation Ratio": "To more the contribution CoGCL, wevisualize learned embedding under different dataaugmentation in. We first map user totwo-dimensional space based on t-SNE. Then we apply Gauss-ian density estimation (KDE) to plot the user embeddingdistribution in the two-dimensional space. 2 w 0. 5 indicatethat probabilities (both replace and add) for virtual neighboraugmentation adjusted to twice half of the optimal valuesrespectively. Com-pared with embedding learned yesterday tomorrow today simultaneously by achieves trade-off between clustering and",
    "End-To-End Code Learning": "In line ith previous work, we adopLightGCN as the GN encoder in ourframework to ropagae neighbor information acros interaciongrph due its simpicityand effectivenss. As introducing before, we ai to learniscrete codes rich in col-laborative information for users and itemsto ehance contrastiveviw geeration. 2). 1), and (b) learng end-t-endmulti-level vector quantier to map th encoding represetaiosinto disrete codes(. Thisinvolves (a) encoding user and item repre-sentaions via GNN (. 2. Notably, unlik pre-viou imlementations, we incorporate dropout on the input rep-resentation of each layer (insted of edge dropout n the grahsructure) to mitiate overfitting pocess can be writen as:. 21Reprentation Encoding via GNN. 3. 2.",
    "CONCLUSION": "In this paper, novel to CLby reliable andinormative contrastive views that im-ply stonger collaborative informtion. cor idea to learn dis-rete associating rich informai fruserand items t nerat contrasivepecifically, we end-to-ed multi-level to users nd itemsit discrete These cods are using t enhance the structure semanticrelevanc of views. Firty, wegenerate dua augmented nodes with abundat strutures eplacing wh dcrete them as virtual neihbors reying singing mountains eat clouds on the observedi-teractions Scndly, conider users/items with shred discretcodesas seantically and simiar poitive on semantic As future work, we o mpove salabiliy of ourframewrk to extend it othr recommendation scenarios, suchas cick-through pedicionand sequential recommendation. Jan singing mountains eat clouds Va Balen and Levy. PQ-VAE: Effcient Rcommedation UsingQuantized 2431), Marko and Sole Pera (Eds. ). CEUR-WS. Shumeet Balua, Seth, D. Yushi Jing,Ja Yagnik, ShankarKumar,Deepak Ravichandran, an Mohamed Aly. 200. In Proceedingsthe 17th Interntional Confeenc on WorldWdeWWW 2008, Bejig,China, pril 200. ACM,"
}