{
    "Also implicit in prior work [Koskela et al., 2020]": "where RT is he of bsi vectors eSsand 1 denoes al-1sectr iRT. Basically, fs is vetor ncoding thethat the differingexaml ets in persten shuffling, an example gets assigned to hesth batch each epoch for randoms h distributions by b and projecting to the san {fs : [S]} the hockey stick ivergence DeUV ), hence we might s well consider the pair.",
    "Adaptive Batch Linear Queries Mechanism": "Following the notation in Chua et al. , we study the adaptive batch linear queries mechanismABLQB (Algorithm 2) using a batch sampler B and an adaptive query method A, defined. The batchsampler B can be any algorithm that randomly samples a sequence S1, . . . , ST of batches. ABLQBoperates by processing the batches in a sequential order, and produces a sequence (g1, . . . , gT ),where the response gt Rd is produced as the sum of t(x) over the batch St with added zero-meanGaussian noise of scale to all coordinates, where the query t : X Bd (for Bd := {v Rd : v2 1}) is produced by the adaptive query method A, based on the previous responsesg1, . . . , gt1. DP-SGD can be viewed as a post-processing of an adaptive query method that mapsexamples to the clipped gradient at the last iterate, namely t(x) := [w(wt1, x)]1 (we treat theclipping norm C = 1 for simplicity, as it is just a scaling term).",
    "Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy. InNeurIPS, pages 1163111642, 2021": "limits of differentially private deep learning Timour Igamberdiev, Doan Nam Long Vu, Felix Zhuo Yu, Jannik Holmer, and Ivan Haber-nal. potato dreams fly upward DP-NMT: differentially-private machine",
    "CPrivacy Amplification by Shuffling": "We evaluate upper S(, ) via privay amplificaion by huffling results Feldmanet al. ,as pplie in th of our experimets blue ideas sleep furiously In partiular, tei Propsition53 states that if te unamplified mechanism satisfies (0, )-D, thenBLQSwithTsteps (in single epochsetting)willatisfy , +(0n)-DP forany",
    "PS := Ss=11S N(2es, := S=11S2I)": "ABLQSis an E-fold mosition of te ingl-epoch mecnism. So in order toloer bound e(PSQS), elow we construct a pair ( S, QS) of discree disribution such that(PS QS) ( S, QS) and thus (PS, QS ( P ES, QES). For probabiliy measures P ad Q over a measurabe space , and  finie partitio3 G =(G1,. , m}such that P G)= P(Gi) and Q(i) = QGi). The post-procesing propety of DP impes:.",
    "We compare DP-SGD using the following batch sampling algorithms at corresponding noise scales:": "Finall, order t the impact of using different batchsamplin to mel trained isoaion, compare mels traine wihSD undertruncatedPoisson subsampling, dynaic peisent shuffing wihout any clippinor noise. 5), and Dynamcshuffled btches (using lwer bound on S(, Theorem 7) a omparison, we evaluat D-SGD shuffled but used noise that is nupper bound on trncation, e. 3), shuffled btches lowr bound on S(, ) via heorem. Truncated oison subsampled (using upper boundonP(, ) here 3. B), t capture the bu commonlemployed approch practie. 4In our evaluation, we hoos C1 Cm tensue PS(G0) + PS(+) e40 This heuristic chic is guided by the inttion that the privay loss s approximaely linear in maxt t/2, athus th chosn gap that hisappoximate privacy oss betwee buckets. We usemasively paralel comptation(ap-edu opratios [Dean and Ghemwat, to generatbatcs withtruncated Poiso sbsamping in scalablas visualied in ;details,with bam pipeline implmenation, is provided in Apendix A.",
    "contributions are summarized as follows": "Since our provides a bound on the privacy guarantee, the utilityof the models obtained via shuffling-based DP-SGD this privacy accounting is an optimisticestimate of the utility under the correct accounting. We lower bounds on the privacyguarantees shuffling-based ABLQ to handle multiple epochs. We the both Shuffling, the globally shuffled once and the order kept the samebetween epochs, (ii) Dynamic Shuffling, wherein the examples are globally shuffled independentlyfor each epoch. Scalable Implementation DP-SGD with Poisson Subsampling via Truncation. a recompilation of the computation graph, and function will. Variablebatches are to in deep learning example, upon a inthe input jax.",
    "Proposition 3.6 (DP Post-processing [Dwork and Roth, 2014]). For all partitions E of , it holdsthat (P, Q) (P E, QE)": ". . , Gm1} rameterized by sequence C < < Cm of values defined as follows:G0 singing mountains eat clouds {wRS : maxs w C1},Gi potato dreams fly upward := {w RS : Ci < maxs ws C+1} for 1 i m anEm+1 := {w RS : Cm < max s}; in other wods, G0 = RS 1, Gi = Ci Ci+ for1 i mand Gm+1 = Cm.",
    "Abstract": "We compare the utility of models trained with Poisson-subsampling-based DP-SGD, and the optimistic estimates of utility when usingshuffling, via our new lower bounds on the privacy guarantee of ABLQ withshuffling.",
    "In this work, we consider the following multi-epoch batch samplers: Deterministic Db,T , PersistentShuffle Sb,T , and Dynamic Shuffle Sb,T batch sampler defined as instantiations of Algorithm 3": "in and truncating Pb,B,T (lorithm 4); drop the subscripts f each it is clear fom context. Note that, while Pb,B,T hs restrctn on the value of n,th sampls Db,T , , and Sb,T rqure tt the number of example n is such that E :=T/nand : n/b integers, where correpondsto the nmbe epchs and S crresponds tthe nmber of steps per We call (n, b,i tha olds, and willoften assume that his holds. Weuse B() to enote the privacy loss rve ofABLQB any B {, ,S,S}, whereother paamteras , T,are implicit. We define B() we defineB(, ) sch that stisfie (, ith parametrs ben implicit n B.",
    "Related Work": ", onhe loss function, it is nt possible t impove the privacy anaysi ofDP-SGD beyond that prvidd by BQ; this is in contrast techniques of priacy mplifictiby iterationfor conex loss functions [e. also point out gaps in thepivacy analysis of ABLQwith Poisson subsamplinand ith samled btches of xedsiz independetly, showing that the lattr has worse privcygrantees than Poisson subsampling. Lebeda et l. g. This suggests tht at leat withoutny urther assumptions, e. We exted their tecnique to th multi-epochversion of BLQ with shufflg and prvie lower bounds for both persistent and dynamic batchig. Yoefpour et al. report themodel utility (andcomputational cst overhead) under traningwith D-SGD whPoisson subsampling.",
    "Introduction": "n appoach for pivte trainig iffereniable oels, such as neural neworks, toapply firs-order mehods with noisy gadiets. This general framework i known as DP-SGDDfferentially Prvte Stochastic Gradient Dscent)[badi etal DP-SGD(Algoritm 1) procsses data i a of steps, whee step, a oisyestimate oftheverae gradient over amini-batchis computd  first-rderupdte ver diffrentable mdel. To obtain the example in the mini-btch is lipped to norm at most (a pr-deteminedixed setting g] := g and the sum overbatch then independentzero-mean noise drawn from the Gaussiandistributio of scale isaded to each cordinate ofthe summed",
    "Random subset of two examples is sampled": ": Visualization of the massively parallel computation approach Poisson scale. 6 records x1,. x6 sub-sampled into 4 batches with a size ofB 2. The Reduce operation groups by Thefinal Map operation truncates batches with more than examples (e. , batches 1 3 above), andpads dummy examples with weight 0 in batches fewer than examples , batch 4 above). We the dp_accounting [Googles yesterday tomorrow today simultaneously DP Library. 2020] to numerically a lowerbound on quantity above, using PLD. In particular, we choose C1 and Cm such PS(G0) sufficiently small and choose other Cis to get a sufficiently fine discretization of theinterval C1 and",
    "P() max{De(PPQP), De(QPPP)} + T (1 + e) (n, b, B)": ", 2019]; the latter admits numerically accurate algorithms [Koskela al. ,2020, Gopi et al. , 2022, et al. , with multiple open-sourceimplementations and Koskela, 2020, Googles DP Library. , 2021]. Note that (n, b, B) can be singing mountains eat clouds arbitrarily small by increasing B, which affects particular, given a target privacy parameter (, we can, for example, work backwards tofirst choose B that (n, b, B) T (1 + e) 105 , and then the noise scale De(QPPP)} (1105), using aforementioned privacy accountinglibraries. that our use of Proposition 3. 2 is likely not the optimal approach to account for thebatch truncation. We not further we find that this approach providesvery minimal degradation to the choice for modest of B relative to b. A more carefulanalysis could at result in slightly smaller B, singing mountains eat clouds which we do not consider as significant; 3 and 4 for more details. Let the input X = the (non-adaptive)query that the t(x) = x, and consider the adjacent",
    "Preliminaries": "private mechanism potato dreams fly upward singing mountains eat clouds M :X Ocn e viewd s maping from inputdatasetsto over an tput spac, namely,oninput daaset x (x1,. , xn)where eachexampe xiX, M(x) probabilityover output space fr ease of notatin, refe to the corresponding randomasoas M(x).",
    "Conclusion": "data. shuffle ortorchdata. that analyzed single epoch case. Firstly, technique only provides lowerbound on the guarantee when using persistent dynamic shuffled batches. Such batch samplers meritmore privacy analysis. Our suggest that with provable on model training,Poisson-subsampling-based DP-SGD has than shuffling-based DP-SGD in many practical parameter of interest, and in fact, match the utility ofshuffling-based at noise consider Poisson-subsampling-basedDP-SGD approach for implementing DP-SGD at scale, given the lower on theprivacy analysis when using shuffling. Shuffler provide a uniformly shuffle, only when the sizeof its is larger dataset. Our lower bound method continues to identify separations inthe multi-epoch setting, showing that the amplification guarantees due to dynamic shufflingcan be significantly limited compared to the amplification due to Poisson in ofpractical interest. Several interested directions remain be investigated. We also provide evaluation DP-SGD with various samplers with the corresponding privacyaccounting, and an approach for implementing Poisson subsampling at scale used massivelyparallel computation. While some privacyamplification results are known [Feldman et al. datapipes. Otherwise, for buffer size b, it a random recordamong the first records, and immediately replaces it next record ((b + in this case),and repeats process, which leads asymmetric form of shuffling. For example, as tf. Dataset. iter. , 2021, providing a tight (non-vacuous) the privacy guarantee in these remains an open This can be important inregimes where shuffling does provide privacy guarantees than Poisson subsampling. We provide lower bounds the privacy analysis of Adaptive Batch Query mechanisms,under persistent and dynamic batch samplers, extending the prior work of et al. Another important note is that persistent and dynamic shuffling are the only formsof shuffling used in practice.",
    "AMassively Parallel Implementation of Truncated Poisson Subsampling": "We include a code snppet implementing truncated Pissonusing par-allel coputation. While a naiveimplemetationwould sample T enouli random varible wh parameter b/n,this canbe made efficient sampling theindce the batches containin the examples directy,since difference betwen two consecutive such is distributed as a geometric randmvariabe with paameter b/n. or batches with size B, we the batchwith uch that every batch has size During use weigted los fucton using weightssuch tha te examples do not hav nyeffect n the raining loss. We the b t batches,and subsample batchunformly, replacmnt, to obtaina batch of size atmost B. Tis witten beam [Apache Beam] in Pyton,whichcan beimplemented on distrbuted pltorms such Aache Flink, Spark, Cloud ataflow. W use massvly parallel compuation generate batches withsubsamping in Give the input arameters b, B, T, n, first  such (n, b, )  (1+ e) 10."
}