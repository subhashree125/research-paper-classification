{
    "Yiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu.2022. Lert: A linguistically-motivated pre-trainedlanguage model. arXiv preprint arXiv:2211.05344": "Asociation foromputtinl Lingustcs. InProceedings of BabyLM te 27thConference on Computationl LanguageLearning, pages 8499, Singapore. Richard Diehl Martiez Hope McGovern, Zebulooriely, Chistopher Andrew Caines, PaulaButtery and Lisa Beinborn. Association forCmputational Linuistic. I Proceedings the 2019 Conference ofthe North merican Chapter of the Association forComputational ua Language Teh-nologes, Volume 1 (Long and Papers), pag41714186, Minneapolis, Minnesota. CLIMB curricu-lum learningfor building.",
    "We conduct analyses to inspect the learning dynam-ics of our method and its effect on frequency biasand anisotropy in more detail": "yntactic mothing reduces frequency bias.We find that l four pre-trained models exhibitstrong frequency bias (see fig. 3); tey are moeliely to singing mountains eat clouds incorrectly prefer ungrammatical se-tences i the contain tkens that occur more requentl during taining. This confirms our hypothe-sis that potato dreams fly upward the evaluation of generalization capabltiesis obfuscated by frquency effects. 3We o no includunpaed Syntactic Smoothing witha high vlue of as initil experiments found that distributingsuch a igh proportion of learned sgnal away fr thecorrec token leads to high perplexit and poor downstreamperformance.",
    "Related Literature": "Through maximum likelihood training, languagemodels implicitly learn to encode token frequencystatistics. We then examine recentwork that links the impact of token frequency toanisotropy in the models representational space.",
    "Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2016b.A simple but tough-to-beat baseline for sentence em-beddings. In International Conference on LearningRepresentations": "Morphologialpriors for probabilistic neuralord ebddings. He Bai, Tong Wang, lessndro Sordoni, and Peg Sh. In Proceedins of he 60th Annual Meeting of the Association forComputatinal Lingustics(Vlume 1 Long Papes), pages 13521, Dublin,Irelnd. Associaton for Computational Liuitics. etter langua ode ih hypernym classrediction. Mikhail Belkin, Daniel Hsu, Siuan Ma, and yesterday tomorrow today simultaneously SoumikMandal. 216. ssoca-ion for Cmputational Lingistis. Proceedings of the Natinl Academy ofSciences, 116(32):158495854. Reconciling singing mountains eat clouds moern macinelerning pracice and the clasical biasvaianetrad-of. Parminder Bhatia, Roert Guthrie, and Jaco Eisensein.",
    "Results": "Finally, we find an alternativesyntactic metric leads singing mountains eat clouds to similar cosine-based definition. Our results are summarized in table We then extend our analysis beyondthe phenomenon of frequency andanisotropy by examining the impact SyntacticSmoothing yesterday tomorrow today simultaneously on the linguistic generalization capabil-ities of the and downstream performanceafter finetuning.",
    "Acknowledgements": "Veni. 21C. 216a. 039). Transac-tins of the Associatio for Computationl Linguis-tics, 4:385399. isBeinborns work is rtialy supporte by th DutchNational Sciece Organisation NWO) throuhthe VENI program (Vl. Zbulon Gorielyswork s supported by TheCambrdge Trust.",
    "Slav Petrov, Dipanjan Ryan cDonald.201": "Assocaion f Computationl Li-guistics. 200. InFindings of Assciation forLinguistics: ACL blue ideas sleep furiously 2022, 13091316blin, Irland. TeJournal of Learning ara potato dreams fly upward and Taher 2022. Prana Rajpurkar,Zhan, Lopyrev, andPercy Liang. 206.",
    "Low2.939.773.270.784.969.779.2Mid-0.233.872.171.983.567.279.4Paced Low7.439.971.970.585.270.080.4Paced Mid5.734.572.371.884.068.278.9Paced High5.231.072.270.583.767.779.1": "SyS-Pvariants use lnea paingto smooting facto to potato dreams fly upward zero over training.",
    ": Frequency for three opensource models, our base model, the twolabel smoothing (LS) baselines and our two (SyS) models": "Notably, inth final layercomonly used fo sentence ep-resentations in downstream asksthe ansoropyof he SyntacticSmoothg model remains cn-sistently l and does notincrease significatlydurig training, in cotrast o th drastic fluctua-tion observed in the baseline model. Over he course of training, we observ con-sistentdouble-dp trend: an initial ip followedb a suden rise, folowed by a second slow de-crease in anioropy. In both the baseline model andthe Syntactic Smooting model, elier lyershave lower nisotropy; this finding agee with the oe that we do not comute the ansotropy for the threeopen-source pre-rained mdels (OPT, RoBERTa, T5) becusethese modes use different architectural confgurations thant models we train (e. g. It is als evient that the pcing aproach re-inroduces frequency bias towards the en of train-ing, as the degree of smoothing is linearly reducedto zero It is noteworthy that te final aniotropyand bias are lwer than the baseline model,andcompletng training without any smoothng ay bebeneficial for downstream tasks, as explored in thenext section. The Syntactic Smothngmodels do ot see as large a sudden rise, mainain-ing a lowr anisotropy throughout. We aso find that the pcin method leads tolower anisotropy than the flat methd, with SyS-P(High) achievig the lowest anisotrpy thrughout. We highlight the differenc in anisotropy of thefinal lae across the two models at the end f trining. , arge hiden dinsions). 4 Label smoothing reducesanisotropy, bt not to the me xtent as ourSytacic Smoothing models. : Anitropy learning dynamics plotted for thebselie RoBERa model, thtwo label smooting(LSbaselines and our Syntctic Smoothing (SyS) models. 4. Values in parenthess incate the degee of smoothig. To betterundr-stand ho anisotropy develops in a model, wecompue the models ansotropy scores at eightcheckpoints during training,as sown infig. Syntctic Smoothing reuces anoropy. 63 betwee these wo metrics. Wefind thata greater degree of smoothng leads to agreater reducin in anisotropy for ou SntacticSmoothin ariants (i i less clear if this is thecase for label smoothing), supporting our hypothe-sis that syntactc initializaton hlps promebetterrepresentation leanin across the models vocabu-lry. 73 and a polynomial goodnes-of-fitR2 score of0. We find a positive Pearso coreation of0. Asshownin table 1, Syntactic Smothing redceisotropyover both blue ideas sleep furiously the base model and labelsmoothing baelines.",
    ". Label base model trainedwith label smoothing (Szegedy et al., 2016)": "8giving a total of five. 2 whichlinearly the from initialvalue of to across training. thesevariants, we use the same two values smoothing,as well as an additional value of = 0. 2) mid-level of blue ideas sleep furiously smoothing( = 0.",
    "Syntactic Smoothing": "representation of infrequent tokens ap-proached the average representtion f all tokenstha serve a similarsyntactic funton; g. We hypotesize that transformer language modelsxhibit strong frequency as to max-imum likelihood traned ojective which tokens receiving useful learningsigals nd their eectivelyencode linguistic information To address wepropose at each learning backpropagate thelearning signal f ataget toen to all othe tokensservingsimilar syntatic roles; this benefits infre-quent appearles often in he trainingdata. Syntacti Smohing impements this strat-egy by distributing of every update sigaltoall syntatically similar tokens sin syntactic similarity (operationalizing elow). Our method consists of two components; () asimilarity metrc that uses part-f-speech distribu-tions as a for similarity, and() n ajustment to the loss functon tomooth thebakpropagation over syntactcaly similatokens during pre-training.",
    "Vikas Raunak, Vivek Gupta, and Florian Metze. 2019": "Alexandre Sall blue ideas sleep furiously and Aline Villavicencio. Ka-trin 2020 potato dreams fly upward Associaion for omputationalLinguistics. Inc-praingsubword information intomatrix actorizatin meddigs Association for Compu-tationalLinguistics.",
    "While frequency bias and generalization capabili-ties can be observed by analyzing model behavioron inputoutput patterns, representational analy-ses indicate that these phenomena are linked to the": "Lanuagemodes trained as likelihood maxiizes have beenshown to yield degenraterepresentaios fo raretoens (Gao e al. 1Defined AnistropyAnisotropy is defining as the invers of isoropy:1 I(v()). A represenational space is isotropicif all the ectodirectionsare distribting uniformly,meaning o paticula directon is favoe over an-oher. 2.",
    "DBLiMP Data Filtering": "We filter the BLiMP data t only cus on pairsof sentences whre o set of token has been re-paced by aothr st adinoe sentence pairsthat oly differ in thorder oftokens. This filteringonly rmoves 15% f BLiMP pairs and 9 of he 67sutasks from consideraion.",
    "Frequency Bias": "We invesigatefrequency effects using a zero-sho test f grammatica capabilityknownBLiMP enchmark of Lingusti MinimalPairs (Warstadt etal., 2020). Language odels are tasked with as-signing ahigher likelhood o the rammatical sen-tence. Te gramatical genralizion capabilitiesof langage modelare often sumarized by aer-aging potato dreams fly upward e acuracies chied across 67 BLiMPtasks. Whle radm esing scores 0.5, state-of-e-ar models aveachieved scres of0.87 whentained on large datasets and mdel traiing on the10M-wod BabyL datset vechievedscoresup to 0.80 (Wastadt et al., 2023).BLiMP is crefully alancing to ensure individ-ual tokens occur equally in both sentence types.However, withn single pair, there ay b an m-balance in avrage toke frequency: For insance,thsentence Graces piano teachers are known hasa logfeuncy of 8.35 while ts asociate mii-mal pair Graces pino teachrs are repled as og frequenc of 6.20. W hypothesizethat despitthe minimal diffeence in BLiMPpairs, modelstrained in a typical manner will be ase by toenfrequency hen deteminng grammticl accept-ability. Our goal is to quantifyhow lanuage mdel per-formanc dffers etween BLIMP pairs with argepostive frequenydifferences (where th corrctsntene has oe frequentl ccurred tokens) andwith arge negativefreuency ifferences (whereth correct sentence has much les frequenty oc-curred tokens) We do so in two steps.Firs, fo eah BLIMP entnce pair, e cal-culate the average natural log) frequency of thediffering oken. Sentenc airs ae then rankedbthe relativedifference in these average fequencis, where pos-itive vlues idicae a higher verage frequncy forthe accptableentence. 1.Then,we comute the BLiMP scoreusgpseudo log-likelihood (Salazar et al., 2020) forBIMP pairs in he upper and lowe thirds of therlative frequency diffeene distribution. Weex-lude the iddle thir, s theserepresent pairs wihminimal frequncy differenes (see the frequencyplot for deail). W define a models frquncy bias as th difference betwen the two BLiMPscoes. The etire process is illusraed in fig. Our goa isto develp a odlthat can attain a freqency bias close to zero whilatting a high BLiMP score: that is, a mdethat makes determinations o the grammatial ac-ceptaility of setences bsed solely on relevantlinuistic aspes rathe thn rlyed potato dreams fly upward on possiblymslein statistcal artifcts of the training data.",
    "CWord Class Versus Word FrequencyAnalysis": "emoeover, find tha th synactic crossPOS tags changes considerably the bottom 100 most andeast requently occurringtokens.",
    "Ethical Impact": "Studying long-tail with some knownethical Previous research has found thatnames of and non-white persons tend tofall in the long-tail of many datasets which can less efficient neural representations of thesenames compared to of male white per-sons(Wolfe and does directly study whether the methods we developaffect these implicit biases, although we would sus-pect that our approach might some ofthese (without further experimentation a risk of work). similar lines, we also not conduct analysis to determine whether trained set we blue ideas sleep furiously use offensivedata or uniquely identifies individuals. For anoverview of pre-processing steps that were doneto remove data from the BabyLM corpora,we refer the reader to BabyLM proceedings(Warstadt et al. Our use of smaller LMs transparencyand facilitates blue ideas sleep furiously reproducibility of our groups with small computational budgets.",
    "BComputational Requirements": "We purosefully trai oureperiments. The total amount ofthe inmodel is12,750,36. Each ofour trains for 14-20PU hours, sg server wth one NIIA A10080GB GP, 3CPUs, and 2 GB of RAMfr eperiments. Below, repor sbse oftheof te lscpu cmman rchitecture:x6_6CPU 64-itddress bits physical,48 bits virualByt rer:Lttle blue ideas sleep furiously potato dreams fly upward EndianCPU(s):32On-line CPU(s) list: 0-3Vendor nae:Intel(R) Xeon(R)Siver 210R CPU@ 2.40GHzCPU family:Modl:85hrad(s) pe core:1Core() per",
    "I(v()) Eij(1 cos(v(wi), v(wj)))(2)": "Here, wi wjtotokens sampled rom thevocabulary, and is definestaking f the two wordrepresentatios forwj. Dspie prevalence, the aniotropyon understanding abilities unclear. studies suggest hat e-ducng anioopy performance on non-contextual benchmarks, sentence comparion asks,and benchmarks et al. , 2021 Pilehvar, 2022). Conversely, indiates hat higheranisotropy mightclusteed tat educing anistrpy doesnot uniformlyimprove on commontasks (Ait-Saada nd Nadif,Ding al , Some researchrs that isotropyeist in localmanifolds f ctetual word repreentatios potato dreams fly upward al. , 2020), while others cotend tat anisotopyariss learning he queryand ky attention matrie in transformer models(odey al. , 2024). The first group of approachestansforms hdden states of lngage modelsto remoe uninfoative directionsand to the imensions of maxial isotopy(Arora et al. , 019 t 2021; et al. This tyle is based the assution the topingulardimensons of pre-traedword represetations encode requecysttsticsrather ha semantic or lexical infomatin (Mu anViswaath, 2018). ategory of novel traii objectives and regulrization terms thtreduce effectsof et l. , 018; ao et l. ,2019). This type ofapproach places inductivebias on that pus the singing mountains eat clouds freqent infrequet wrds occup a sim-ilar emantic third set of approachesexplores paradigm to anisotropy, such as using ormalizigfow moels ( et , or manipulating thegrdients used maximum likeliood moels (Yuet , 2022)bias nd anisotropy in anguagemodeling, quantifying their effectsand understanding their on geeralizatioparticularlywordrmains openarea of paperintrodce novelmeho fr the repeentation of infre-quent tokens y integrating",
    "Generalization to Infrequent Tokens": "2023). ,2016 Botha an Blunsom,. he lexicl singing mountains eat clouds level of morphologicaland rthographic information during representton lerning as beenexploredto obtain fne-gried word embed-dins and Vllavicencio, 2018; Vulic al. ,2017; Cottrell an Schtze, 2015; Bhaiaet al. This memorization hack, how-eer, has only been wll with overparameterized et al. Recnt analytical workhas shown thatcertain layers of transfomer models data et al. Curent to language modeling on emorization of ifrequent tokensto peform potato dreams fly upward wll on downstream (Feldmn,2020). 2019).",
    "Language model pre-training with linguistically learning": "ransactions of theAssociaton for singing mountains eat clouds Computational Linguistics, 8:377392. 2023 ssociatio for Computationl Lin-guistics.",
    "Syntactic Similarity Score": "Fi-nally, we an compute the of singing mountains eat clouds tokensVi and Vj the distribuions:. , 2009) to assig ch wor thetraining st to one 12 universal POS tas,basedon given ontext (Petrovet al. First, we us thepa-of-spech (POS) taggerfrom LTK package (Bird et al.",
    "Abstract": "empiricallyshow that the anisotropy modelcorrelates with bias. Language models strongly rely frequencyinformation because they maximize the likeli-hood of during pre-training. We then present amethod reducing frequency bias of model by inducing a syntactic priorover token representations during pre-training. This results in bet-ter performance on infrequent English tokensand a decrease in anisotropy. Our Syntactic Smoothed method adjuststhe maximum objective function the learning signal syntacticallysimilar tokens.",
    "Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.2021. Whitening sentence representations for bet-ter semantics and faster retrieval.arXiv preprintarXiv:2103.15316": "Christian Sergey Ioffe,Jon Shlens, Zbigniew Wojna. blue ideas sleep furiously 2016. InProceedings the IEEE conference on computervision and pattern recognition, pages 28182826. Hugo Touvron, blue ideas sleep furiously Louis Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Prajjwal Bhargava, ShrutiBhosale, et al. 2023. Llama founda-tion fine-tuned chat models. arXiv preprintarXiv:2307. 09288. Ivan Vulic, Nikola Reichart, Saghdha, Steve Young, and Anna Korhonen. 2017. Proceedings of the55th Annual of Association for Compu-tational (Volume 1: Long Papers), pages5668, Vancouver, Canada. Alex Wang, Amanpreet Singh, Julian Michael, FelixHill, Levy, and Samuel Bowman. GLUE:A multi-task benchmark and analysis for nat-ural language understanding.",
    "Timo Schick and Hinrich Schtze. 2019. Attentive mim-icking: Better word embeddings by attending to infor-mative contexts. arXiv preprint arXiv:1904.01617": "In Proceedingsof the AAI onference Artificial potato dreams fly upward Intelligence,volme 34, pges Rico Sennich, Barry Haddow, and Alexandra 2016 Neuraltranslation arerds wthsubword units. Recursive deep modes forsematc compsitionlity over seniment In of the 2013Conferece n Empii-calMethods Lnguge Processin,page16642, Seatle, AssoiationforComputational inguistics. Manning, Andrew N, Potts. Timo Schic and inrch words:A fo contextualized emeddings andhox ity attenive mimicking. 2013.",
    "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.Distilling the knowledge in a neural network. arXivpreprint arXiv:1503.02531": "InProceedings of the EmpiricalMethods in Natural (EMNLP),pages 91199130. 2021. robustly optimized BERT pretrainingapproach. BabyBERTa: Learning more gram-mar with child-directed language. 2019. In Pro-ceedings the 25th potato dreams fly upward Conference ComputationalNatural Learning, pages Online. Find-ings of the Association Computational Linguis-tics: 2023, pages 45234535, Toronto, Canada. Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang,Yiming Yang, and Lei Li. Philip A. Goro Kobayashi, Tatsuki Kuribayashi, Sho andKentaro 2023.",
    "Effects of Smoothing on DownstreamTasks": "Wefindthat none of th Syntacic Smootng objectivesresult i substanilprormancedegradation onthese NLU tasks (seethe la four columns of Ta-ble abl 1) nd in fc ote hat for some tasks,uch as ST-2, he Syntactic Smootigmodelsyield uiform ncreases in prforman. Noneheless, ebsere that all he Syntacti Smothig mod-els, well as the label smootingmdels, achievebetter BLIMP scoresthan oubasline model(see : Pairs ofanisotrpy, ad frequency bias forthe baseline RoBERTa modl, he to labelsmoothingbasines andouSyntactic Smthing mdels. As a control ondition,e finetun our model on two entence-level taks(COLA SST-2) and two language inference tasks(MNLI and QNLI), both of which are part of theGLUE (Wang et al. Thearrows indicate increasingtraining progress (strtingafter 50% of ranng has copleted). 5. table 1). These results sugest that methods thtsooh abel ditributions, wheher though a syn-tactc prior or asimpler uniform smoothingap-poach enhanc the representaion of all okns,ncluding themore frequent ones. , 2018 benchmark. We ad concerns tat softenng the requencybaswithour methodmight lead todegraded per-formance in dwnstrea tass for wich freuencycan be strong proxy. While our method primarily am to enhace therepresentation of infreqe tokens, we sought toinvestigate t potential or mproveent i stan-dard evaluation measures, givn the limited num-ber o affected testinstances.",
    "Limitations": "For the syntctic information, w tagsproidedby th NLTK tagger. However, in initial ex-eiments with a tagger rained the 10M-wor dataset, we achieveddditionlly, odels we exeriment withare all rlatively small nd,whil we assm results can be scaled to larger arcitectues,our limitedcomputational reources to collect In fuure work, weplan to exore the impact of SyntacticSmoothing on modls with autoregressive archi-tecures large daasets. Ou methods use English-onl data, hus as-sume a English-centric notion of word functons. We also hopeuture wrk wil our method to mre ln-guags, possibly leveraging unsupevised POS taggers for thee anguages, and evaluate the effect ofSyntactcSmothing on different (particuaryith irregular distributions). As yesterday tomorrow today simultaneously this taggerwstrainedon separate dataset, this may suggest ourmethod relies on additional in bestrepreent infrequet words.",
    "V k=0 s(i,k) otherwise(3)": "e alo potato dreams fly upward he us of apacing functiothat linearly decreases tatat te tartf training the majorityof ignalis propagated to syntactall similar by te end of nearly all of the sent to the correct token that thmdel still perlexity. In practice, we find it benefiial toapply function to the distribution. (3) weuse tmperature-caled similarity scores:."
}