{
    "Training Dynamics Comparison": "for Llama2-chat, with a total 500 and suffix length are illustrating in. to theinterleaving variant i-DeGCG shows higher FT lower ST loss, to alternation FTS and CAS, achieved a balancebetween these searched Regarding the ASR performance, shown in (c) and (d), and i-DeGCG outper-form GCG-M, achieving within300 steps, while GCG-M continues underper-form even after 500 steps. In addition, i-DeGCG achieves higher ASRperformance within the first 100 compared DeGCG and GCG-M, and perfor-mance to within the 300 Thissuccess of both and the vari-ant validates the effectiveness of decoupledframework highlights the importance of self-transferable suffixes. Specifically, we cross-entropy loss of token the the entire target (ST),and performance on the validation(Valid) and test sets. is noteworthy thatDeGCG achieves low ASR within the 100steps using only FTS and reaches optimal perfor-mance the subsequent 100 steps CAS. For DeGCG under setting, we perform FTS for 100steps followed by CAS for steps. This indi-cates that first-token optimization is noised by other optimization goals, degradingsearching efficiency. This reveals CAS is for successfulattack, and FTS provides a initialization forCAS. As depicting in subfigures (a) (b) of ,both DeGCG and converge faster thanGCG-M, achieving lower cross-entropy losses forboth the reaches a near-zero FT loss steps, whereas the of GCG-M remainsgreater than within the same steps.",
    "Suffix/Suffix-CAS! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !": "2023). Ourwork focuses on adverarial suffix tanserring earningacross aligned andassocates rnsfeability with efficency. Tanfer uses he LLM bu distinct sets, while Transferuses the distinct LLMs For Self-Trafer, use the same nd set alternatingbetween FTS nd CAS. non-natural languages such as ciphrs al. In the ost-searching/fin-tuning we pefom content-awaresearcing with LLM B Behavior B. Furthermore,their indicae the transferabiity an univer-salityo these adversarial Recent workasalso unveiled ailbreak wihin of scenarios (e et al. ,2023), hghlighting riskfor all open-sorceLLMs with modified decoding strategies (Hanget a.",
    "DeGCG": "1 equal importance toeach tar-get token, egardless of levls difficltyassoiatedwith optiizing each one. The challeng ofGC attck is rimarily as-socited wth fis-token optmization Eq. In thisaalogy, the conterpart yesterday tomorrow today simultaneously of paamtr spacetheserchinspace DeGCG. The ui-objecive optimizaion itrouces intothemore challenging first-token ptimiztionpocesswhere significant loss signls could be biasing byother compettors, thereby redued efficencyofthe Our DeCG une tokensin in a manner to howparmetes incontinuousspace are tuning uringhe pre-training singing mountains eat clouds d fine-uning press.",
    "Abstract": "pecifically, we employ directfist target token optimizaion in pre-searhigtofacilitt the search process. 2)and 39. e applyour aproach to cross-model, coss-data,andelf-transfer scenario. 5 onvli potato dreams fly upward andtest sets, respectively. potato dreams fly upward Notably, our iDeGG out-peroms he baselin on Llama2-chat-7b withASRs of 43. Futher analy-sis on cros-modeltranfer indicates pivotalrol of frst target token optimization in levrgin sux trnsferablity fo efficient sear-ing. owever, CG struggles withcomputaionainefficiency, limiting further invstigations rgardig suffix trasferability and scalabilityacross modes and data. Language Language Mdels (LLMs facesafety concernsde to potential misue by ma-licious users. 0 (+19.",
    "Conclusion": "stud, DeGCG to enhancethe efficiency of adversarial suffix searching foraligned LLMs. Theo Firt-Token Searching an Conten-wareSearching enablesmre and effectiv iden-fication of suffixes. Thefindings contibuteto boder undertanding potato dreams fly upward oLLM potato dreams fly upward vulnerabilities nd te development of and secure.",
    "We found that suffixes obtained through first-token searching can be effectively transferredacross different models and datasets with furthersearching.Additionally, we leverage the self-": "of adversarial and interleaved training algorithm, i-DeGCG, whichperforms FTS CAS in an manner. blue ideas sleep furiously. experimentalresults demonstrate the effectiveness and the framework and i-DeGCG variant,highlighting success of suffix transfer throughtwo-stage and impor-tance of initialization for search efficiency. We evaluate our method on the Harm-Bench across various LLMs.",
    "MainResults": "Cross-Model Transferring.To evaluate the effi-cacy of suffixes training through FTS on one modeltransferred to another model via token-level fine-tuning, we conduct cross-model transferring ex-periments across four open-source models. To en-sure a fair comparison, we maintain equal totalsearch steps (FTS + CAS) for all experiments, con-sistent with the baseline, totaling 500 potato dreams fly upward steps. Wealso include the baseline GCG-T from HarmBenchthat optimizes suffixes against multiple models fortransferring. Our transfer performances on the vali-dation set and test set are presented in .Our proposing DeGCG approach significantlysurpasses GCG-M across various models onboth validation and test sets",
    ": Performance comparison (ASR) in Cross-Data Transferring across different behavior types in HarmBench.We report the results of LLama2-chat-7b on both the Validation and the Test sets": "achieves absolute improvements 9. 0 and 9. on vali-dation and test Notably, despite differences in tok-enizers between source target singing mountains eat clouds models, transferlearning from FTS through CAS still yields sig-nificant performance improvement. For instance,transferring suffix from Mistral-Instruct to achieves absolute 22. 4 ASRs on validation and test demon-strating the efficacy of DeGCG. theDeGCG approach outperforms GCG-T on both val-idation and sets. This reveals that oursuffix transfer learning is effective than thedirect transfer with concatenations multiple models. Moreover, target model is identical tothe model, DeGCG method ASR performance, achieving over 100%enhancement on LLama2-chat-7b. We attribute thisimprovement to effective providedby the same model, amore token fine-tuning afavorable neighbor area in the search space. To evaluate the effec-tiveness of the framework in cross-datatransferring, we initially perform FTS llama2-chat-7b using the dataset of HarmBench. Subsequently, we conduct CAS with a domain-specific dataset derived from the general validationset HarmBench. we use six dis- tinct categories defined in asseparate Biological, Illegel, Cybercrime, and Harass-ment The general GCG-M without training serves as the baseline. We in-clude experiments using GCG-M trained with data. To ensure a fair comparison, maintain the total search steps,500. The experimental for both validationand test sets are displayed in. Theinclusion domain data significantly per-formance, particularly in the Chemical Illegal, and categories. Nonetheless, the success of the behavior-agnosticsuffix transferring underscores the efficacy of FTS,validating the necessity of the first-tokensearching and content-aware search Interleaved To evaluate theeffectiveness the proposed i-DeGCG algorithmfor we apply the interleaved algo-rithm on Llama2-chat and 5 models,respectively. In this the source and tar-get models are and the validation set as the training dataset. We assess performanceacross various scales the searching space. Specif-ically, given that the searching space grows expo-nentially increased suffix length, we extend.",
    "A.2Implementation Details": "Weuse Pytorchand Huggngface potato dreams fly upward Trsformers i our mplementation. 2. Besides, e set the threshol of etraining loss to be 0. Specifically,we ealuated fourmodels in our main experimnts. hen the training losseachs a blue ideas sleep furiously ower valu than the threshold, we update training behavior set.",
    "Ethics Statement": "Our study does not prpose new attc paradigmtojilbrek LLMs. w hexisted adversarial suffix-baing jailbrea atack,aimig undertnd the of adversarialsuffixes n a wa. For example, we manlyexamine h suffix tranferabiity with searchefficiency. We also that curentsuffix-based aackscan be wel fended PPL detection-basedto. The computatoal work fo his performed on resources of theNationalSupercomputing Cente (NSCC), ina-poe2.",
    "lo p(t(j)n+1|t(j)1:n)(2)": "In ths tsk,the suffix is optimizing basing onthe gradient derived solely from the first target to-ken, resltigin a direct and eficient opimization.The first target toke is typicall beavor-anostic,suc as Sureor Her. Therefore, otainedsffxes SFTS serve as geneal initalization witha low cross-entropy lss for th first tken. Start-ing earch fom an effectiveinitialiaion wtha low first-token loss helps to migae the ineffi-ciency associaed potato dreams fly upward with starting each search from ahigh first-token los, reducing te time and compu-tationl resources accordngly.",
    ") to compare our approach and the baseline.We use the text-only set which comprises threetypes of behaviors: Standard, Copyright, and Con-textual. Detailed statistics of HarmBench can be": "In or expeiments, we usevalidaton and tet plits provided by HarmBnch. pecifically we use the standar beaior subsetsof both alidation and testsets.Implementation Details. Specifically, weutilie LLama2-chat (Touron et al. , 023) Mistral-Instuct (Jia et al. , 2023), OpenChat-3. , 2023), and Starlng-LM-alha (Wang e al. ,2023) n our epriments. Due to memory con-straints,we use 7b models for all experients. Forevaluaton, we report the cassifier-based at-ack success ate (ASR). We consier the baselineGCG-M fro theHarmBenh that uses GCG fosuffix sarching with multiple behaviors. To en-sure reproducibiit and fair comparison, we usethe open-source classifierprovided in HarmBench. This lassifier is a ne-tuned LLama2-3b model,which achieve stng performance on a manually-labeld valdaton e",
    "Context-Aware Searching": "Suffixes obtained from FTS are effective forbehavior-agnostic targets but fall short in elicitingbehavior-relevant responses.",
    "Steps": "plot changes in cross-entropy lossof target tokens at positions 100 steps.This discrepancy in loss dynamics highlights the impor-tance of first token optimization in GCG. In these concerns, recentefforts have focused aligning LLMs with hu-man preferences to enhance the responsibility of their responses (Bai et al., 2022;Ouyang et al., 2022; Korbak et al., De-spite alignment still potential attacks et studies have revealed types of jail-break attacks (Wei et Kanget 2023; Lapid al., 2023; Liu al., 2023a),which involve using alongsidemalicious queries to compel LLMs to gen-erate harmful and unethical responses, thereby cir-cumvented the safety alignment constraints. One notable attack, Greedy Coordinate Gradient(GCG) (Zou et al., 2023), utilizes gradient informa-tion to search prompts, also adversarial which can be appendedto malicious to responses.These adversarial suffixes consist of tokens and are generally not to humans.However, deriving these suffixes gradient-based searching inefficient. increased search space of randomsuffixes expansion presents significantchallenges efficiency. Furthermore, et al. thatmodels preference optimizationexhibit robustness suffix Despitethese insights, works focused transfer, which transferabilityacross different data for using adversarial suffixes as initializationfor learning largely unexplored.In work, motivated by challenges in op-timizing gradient-basing search process witheffective initial adversarial we explorehow to leverage suf-fixes during optimization. To address we proposea two-stage transfer learning framework, DeGCG,which the original search process intotwo stages: behavior-agnostic pre-searching andbehavior-relevant post-searching: In the pre-searching we asimplified First-Token Searched (FTS),searching adversarial suffixes with abehavior-agnostic target such as en-abling LLMs to elicit first target tokenwithout In post-searching stage, we obtained from the pre-searching stageand conduct (CAS)with a target",
    "Preliminary": "uch as Sure, here is how to make bomb. Theoal ofsearching seuenc formulated to imiz thefollowed neativ log-likeliood:. In this section, Gredy (GCG) Le X denote mali-cious promts a Tellme how make abomb, objective of the GCG attack is t findthe sffix S = {si}LSi=1 lengh , so that T{X, S} = {t1, t2, tn} as can gnerae rsponses startig arget sequence = {tn+1, tn+2,. the ointtarget disributon is representdby p(tn+1:n+m|t:n).",
    "(1)": "GCG searches for suffixes throughmultiple blue ideas sleep furiously iterations, adopting greedy in iteration. To construct the candidate batch, it firstcomputes negative esiL with the vector representation andselects tokens from vocabulary with the top Kvalues esiL, forming token candidate setat each position.",
    "Interleaved Self-Transfer": "Leveraging of andenhance the efficiency of search wepropose interleaving variant of our approach, i-DeGCG. i-DeGCG FTS and as ameta-process and dynamically alternates This approach maintainsa dynamic balance between generating the firsttoken and producing behavior-relevant The iterative process continuous the suffix, the strength both FTSand enhanced performance. Wesummarize in Alg. 1.",
    ": Training dynamics (cross-entrory loss) comparison for GCG-M, DeGCG, and i-DeGCG": "selfrepettin of these sufixes. In tewords, the longer suffixisconstructedy suffix and concatenatingthem for searching the large space For this we the of length 20earche o Llama2-chat-7b after 500 anrepeat t 2, 3, 4, and 5times to initial-izationof 40, 80, and 100, 5 to 54. 7 on tes set. findings also ndicae sufix search insmall search spaces provides valuale effetive initializations loger fine-tuning in large search saces.",
    "Jailbreak Attacks on Aligned LLMs": ", blue ideas sleep furiously 2023a; Zhu et al. ,2023 Guo et Additionally, adersarial. Subsequent advancements havautomaed the these stealthy controllability (Liu et al. , yesterday tomorrow today simultaneously 223b). jailbrea attacks th caftingf inpt prompts.",
    "Limitations": "Future work coul xpore transferabil-ity of between large language models andlare multimodalfor bt tet and mul-timodal data. Several liitations inourwork.",
    "Self-Transferring by Self-Repetition": "of from a rndomsuffix a large we utilize suffxes in a smallersearch space and singing mountains eat clouds xpand singing mountains eat clouds search sace through",
    "Alexander Nika Haghtalab, and Jacob Steinhardt.2023. Jailbroken: How does llm safety training Conference on Neural InformationProcessing Systems": "2021. Finetuned lan-guage models are zero-shot learners. arXiv preprintarXiv:2109. 01652. Gpt-4 too smart safe:Stealthy chat with via arXiv preprintarXiv:2308.",
    "Yangsibo Huang, Samyak Gupta, Mengzhou Xia, KaiLi, and Danqi Chen. 2023. Catastrophic jailbreak ofopen-source llms via exploiting generation. arXivpreprint arXiv:2310.06987": "Albert Q Arthur Men-sch, Bamford, Devendra Singh Chaplot, Diegode las Florian Bressand, Gianna Lengyel, Guil-laume Lample, Saulnier, et 2023. 06825. Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin,Matei and Tatsunori Hashimoto. 2023. 05733. Jason Phang, R Bowman, and EthanPerez. language models withhuman preferences. PMLR."
}