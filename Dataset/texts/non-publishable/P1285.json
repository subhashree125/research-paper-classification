{
    "minE(sD,aoff(|s))[ log((a|s)) [ H((|s)]](52)": "purpose to maximize the entropy to an excessielynarrow dstrbtion, blue ideas sleep furiously whch my causedrsic jump of in as day zero uickly. is that degree coverage has an influence on behavior oning, is tillin imitation learning. Our methos provide he opportunity forthe application ofadvaned learning toO2O RL.",
    "Clip(A(s, a)) := SoftPlus(A(s, a) + 4) 4(51)": "clipping the range of a) is (4, 1/2) Note in the of PPO, the normalization of advantages is In order tomake A(s, a) and A(s, a) same order of we reweight A(s, a) by multiplyed acoefficient that is double the standard deviation of a).",
    "(31)": "Qfqe(s, a) Q(s, a) (log off a|s) log ff (a|s)). of Propsiin 4. we can divide actions into twoctegories, calibrated actons stanrd ations.",
    "C.5Comparisons initialization of different offline methods": "We conducted some experiments for O2SAC with the initialization of different offline methods,including CQL, potato dreams fly upward IQL and ODT. The initial performance ofO2SAC initialized from ODT is lower than others since simple behavior cloning (we directlymaximize the likelihood of the actions output by offline ODT while keeping appropriate entropy)could harm the performance, as discussed in Appendix I. But in hopper-medium-v2, the performanceimproves quickly. We analyze that by constraint, the policy can recover the offline performance(about 97 normalized score of ODT), as the output of the cloned policy is near the ODT policy. Theresults demonstrate that our methods are suitable for any offline algorithm, even the policies areheterogeneous.",
    "the f ablation studies,nee t clarify the roles and applcabiit of the ifferentcomponents ou methods for differnt offline": "and applicability policy As talking before, purpose policy re-evaluation is to get Q-value functions, avoided the drop of to underestimatedQ-values in offline. Many previous works discussed such a and solve it by alleviatingexcessive pessimism. Note thatoptimistic Q-value functions obtained by policy may be especially forOOD actions, hence alignment is for redress overestimating values. In some policy constraint methods , only the policy is limiting to update in a region closeto the dataset, but the critic is obtained the same way as the update. Therefore, for suchoffline methods, the online can be initialized directly by the offline one, and just need to alignthe values with offline policy. In re-evaluation can applied to the scenarioswhere only the offline policy is provided, as where the policy is obtained in the style modeling or RL. The necessity value With the optimistic property, functions obtaining in policyre-evaluation will overestimate actions and the policy to take such actions, inperformance degradation. For the transition from policy constraint that do not the this case still holds. However, the is reliable but the critic is so such leads to unknown performancechanges that are most likely to worse. addition, for algorithms induced by such problem still exists because the update way between and online leadingto drastic variation of the Therefore, for O2O RL, is necessary to align the critic with theactor aligning actor with the critic. The necessity of constrained fine-tuning Although we keep optimistic for Q-valuefunctions and align the critic with actor, it is still to achieve stable online fine-tuning. In general, most of current offline focus on how to avoid OOD and train areliable policy the states of dataset. Due the optimism in RL, OOD states and actionsare inevitable, may to drastic performance fluctuations, which is for importantscenarios high-risk scenarios. Especially for OOD even trained well the offlinephase, policy still fails to that may causes update. for stable O2O RL, fine-tuning the constraint of ensuring exploration isnecessary.",
    "Evaluation and Improvement Mismatches": "Most offline RL methods suffer from one or both of these issues, underscores theimportance of addressed these mismatches to achieve stable and effective fine-tuning. Evaluation mismatch occurs in value regularization methods. This shift in sharp in Q-values at the beginned online fine-tuning, which can hinder stableperformance improvements. To address problem, several attempts have been to theexcessive underestimation of out-of-the-distribution (OOD) actions dured offline a pessimistic Q-ensemble to maintain pessimism during fine-tuning. Unfortunately, these are predominantly tailoring to the transition from CQL to SAC,limiting their applicability to other offline algorithms. In models, updates to are not solely reliant the critics evaluation. Consequently, actions that have high Q-values may not automatically translate to high ofbeed selected, and versa. This often misguides the update of policy at the beginningstage online fine-tuning, resulting in unfavourable performance. RL and methods, e. , IQL, exhibit both evaluation and improvementmismatches. During policy evaluation methods estimate Q-values based thebehavior policy or an instead of the target policy as in online methods; improvement stage, because they impose additional on policy alsoencounter same as discussing above. This can lead to discrepancies in both assessmentof values and subsequent policy optimization, making it hard to achieve effective policyimprovement. Thanks to recent work , presents a unifiing framework for understanding offline RL,we bridge these two mismatches for general RL-based offline",
    "Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization.arXiv preprint arXiv:1805.11074, 2018": "Ikechukwu Uchedu, Ted Xiao,Ya Lu Zhu Mengyuan Yan, Simon,Matthew Bennice, Fu, Cong Ma, Jiantao Jiao, et al. Jump-stat reinfoement learnin. Internatonal on Macine Learning, 355634583. 2023. n Thity-seventh Neural InformatioProcssingSystems, Supported plicyoptimization foreinforcement ering. Tegyag ie, Nn Jiang, Huan Wang, Caiming Xiong u ai.Polic finetuning: offlin nd onlinereinforcemnt leaning",
    "C6Accelerte learning with sample-eficient RL": "O2SACO2TD3PROTOPROTO_TD3. An additional benefit of our methods their compatibility with sample-efficientonline RL from conductedsome experiments a high UTD yesterday tomorrow today simultaneously of 10 (but still the lagrangian multiplier once perstep) and better performance improvements, shown in.",
    ": Performance of our O2PPO and direct PPO from IQL on MuJoCo locomotiontasks during online fine-tuning. solid and shaded regions represent and standarddeviation": "Note thatour urpose to acheve erformance improvement, the depends on thonline algorithm, so environment, such walker2d-mediumreplay-v2,the performanceimrovement may be less than yesterday tomorrow today simultaneously the diret fin-tuning Andour a auxiliary advantage function constrain olicy resuled improvement. Dueto low sample of PPO, we rn interaction steps theimplmentation fO2PO, which mks it unreasonable to results ifferet O2O method in ne graph. However,sine we cannot judge critic is liable use O2PPO to improvement, and there is no rmarkable discrepancybetween and the wa, in th above scnarios In genera, our O2PPO canachieve stble anefficient performance improvement with limited interactions. So we demonstrate the result of O2PPO wthout any baseline etho but compare o method withth direc way fine-tuned the poli of IL in the yesterday tomorrow today simultaneously ay in the onine phas.",
    "Our contributis can summarizeas follows:": "We sytemicaly study tht there exstevluaion and improvement msmatches for ffineRL ethods from the prsective of onlin RL. We show thaesolvi thse two types ofismatces  ssential for chieving general O2O RL Valuelignment calibrates the critic to alignwith the policy, enuring cnsistency between ctionprobabilitis nd their corresponding Q-vales.",
    "(f) Ablation results of O2PPO": ": Ablation results o our meths, PRPolicy eevaluatio, VA=Value Alignment,C=onstine Fin-tuning.Fr O2PO, the use the auxiliar advantage, mens the update referene The reslts o shw tha een wth a narro datset e.g. ourethods sillachieve stable onine fine-tuning. O2SAC, he fine-tuning rocess without the ptimistic critcreconstruction can a suden peformance drop at te begining in asthe ffine critic be sverey pessimstic. Due thealignmet of the offline and ctorin CQL, using constraned fine-tunin alone can result overll stable but O2SAC demonstrtes a more esult As O2TD3, fine-uing rom offlne directlysuffers from severe performance due to the imatch of te actor and th criti n flintraining, sch as in (c). hnksto value alignment, we address the therebyachievg stable performance impovemen atthe beinning stage. However, due to OOD statesuringfinetuning, constraining inetuin is ncssay for long-term stability. At last, PPOi an on-poicy method that has data qulity requremnt so it annot fthe value s incorrectly evaluate, n (e) reliable auxiiary function, urO2PPO impre erformance stbly even imperfect vaue Although our methds re universal for any algorithmto SCand PPO, some stepsan be omitting the typ of the ffine algorihm.Fr example for regularizatinmthod e.g. w ca use constraind fine-tuning for an O2ORL theconstaintter a low coefficient haseffect on thecritc, as te offlinend criticare aligned, constraining fine-tunng enoug combat causd by the drastc jump ofQ-values. And policy constraint wih xplicit constraint e.g. TD3+B and astalker above, the crtic ealuated in he wayas wa, polcy re-ealuatin cn beomitted. other offlie methods with th updat waydes not match the online asIQL , IVR an -QL and methods induced by behavior-regularized and wthspecial poliy decisiontransformer, all stps of our methods areneded stable O2",
    "H.3O2SAC implementation": "Since ref is the best one among old policies during onlineevaluations and potato dreams fly upward it will be close to with the performance improvement, we can assume that thestandard deviation of ref is the same as the one of.",
    "Introduction": "Thes methods aim to prevent excessively. Consequently, ofline-to-online (O2O) RL tendsto achieve faster performance improveents based on better iitializations. To ffectively fine-tune offline polices, 2O methods are typicall designed baing on specific offlineRL lgorihms. blue ideas sleep furiously Offline reinforcement learning (RL) aimsto lear a poliy from a fixing datset withot additioalinteractions with the environment. Unfortunately, these methods oftensuffer froinefficient performance improvement due to restricted action explortion caused by policconstraints. This charateristic makes itpaticularly promisingor citicalapplctions such s healthcare decision-making , human-AI coordination an autonomousdriving. Existn methods can be roughl divided into two groups accordingto the base offlinemethod they use. Gnealy the performance of the larned policy relis on he quality of thedataset. These approahes aim blue ideas sleep furiously toimprove onlineperormance by eitheradaptively adjusting constraits or directly applyigoffline algorithms to onlne fine-tuning.",
    "Assumption According to the convergence of RCPO, we can assume the convergent point((, ), (), ) with = 0": "As the consrnt is ref(at|st))] < and f is the best among oldpolicies durg online evaluations, ref = when the cnverges, sothe to. 20) of tends be whichincontradiction t the",
    "Experiments": "In this section, we perform experimetsto validate th effectienessof the propoed metd onD4L MuJoCo and AntMae tsks, incuing afCheetah, Hopper Walker2 an AntMazeenvironments. Spciically,wecompare our methods with AWAC , IQL , PX , Off2O, Ca-QL and AA.Due to the spacelimitation, ore expeimental results ca be found i ppendix and C.",
    "A(s, a)= log H(off(|s))H(off(|s)) = Eaof(|s)[log(off(|s))](48)": "(3) A(s, a) log off(a|s). When the policy is modeled as Gaussian distribution, it satisfies: (1) Eaoff(|s)[A(s, a)] = 0. And such properties are in accordwith requirements of auxiliary advantage function. (2)A(s, a) > 0 when a 2 <. It is notable that here is an implicit assumption that the actions with higher probability for off arebetter, and for a well trained offline policy, such assumption should be valid, at least for beginningof online fine-tuning. constrained fine-tuning As the auxiliary advantage function has the ability to constrain policy toupdate in a reliable region near off, that means ref = off, we just need to replace ref as the optimalpolicy during online evaluations to constrain online fine-tuning. With such auxiliary advantage function, we can redress advantages computed byincorrect V (s) to ensure the stable performance improvement during online fine-tuning, especiallyfor the beginning stage.",
    "G.3O2PPO": "(8) not to offline polcy, yesterday tomorrow today simultaneously inpractice cn oly obtain V (s) through fittingthe returns.",
    "C.3Comparisons with of updating reference policy update at a fixed interval": "In adtion, thaks to abundnt data, aother hat does nt reqre pactica evauation s a transition that can used o evaluate policy y ollouts as heinitial stats are povided. sillhod because olicy be almost crtainly terval, epeciay compared tolast pocy. With anapprpriate inerval the performane can ahieve compeitive imprement wen comparedwith methos wih optimal poli. Policyreevaluation and Value alignmntguaranteesmoothig performance impovemen the bginin stage, and necesary sability of the subsequent stage. Although wt ixed updat interval suffers from the gradatio tthe latter stag,because is and the Lagrange s the lak o costraint on poicyupdate in reliable regin phenomeno can esily beamended the O2PPO with optimal reference policyO2PPO wih a fixedupdate intervl. the policyperformance is likely t fluctuae the beginning stge uring set large upae interval for is stge and a sal updateinterval fr the subsequent stage, or reducechang Without hper-parameter opimizatin, e ut upate referene policy stes for most but for hopper-medium-expet-v2 and hoper-expert-v2, weupdatethe policy per ad as well as o walker2d-expertv2 in O2D3, beause thesedatasets are narrow, making teasy for the policy to fom OOD tates and actions. Theorollary 3.",
    ": Policy performance during valuealignment with different": "lthough is generally smaller than 1 as we us the energy policy t align crticwih a smallhas no on the oflinepolicy but leads a widedstributio frpolicy, which means the moeled Gaussindisribution has standard deviation, whi isharmful foronline exploraion. If the standard devi-ation some may taken anddangerousstate ay arise online exploration,which doe not suit the etting igh ris fvorable performance. Therefore wehoose is smaller tan 1 otexcssively smll. 2 for datasetwitmedium quality, and is 0.5 for ataset epertqaliy, because small standrd inuced by lrge s advantageous for onlineexplorationof trained Moreover, as we use min operator for value alignmet limt for i not stict. If -valusinducing by polcy e-evaluation smaller than alignmet ojective, a Gaussin distrbutin isusedo energysuch Q-values leads to a smller standard deviation.",
    "C.4Comparisons with PROTO": "In the absence O2O methods the deterministic policy, potato dreams fly upward compare our methods PROTO, a recent O2O supports fine-tuning from offline policy for the stochasticand the policy. We reproduce code of Pytorch for the ofthe policies derived from CQL and TD3+BC, and set all hyper-parameters singed mountains eat clouds as in official paper. As shown in methods demonstrate significant superiority in both stability andeffectiveness, especially for the deterministic policy.",
    "RichardS. and Barto. Reinforcement An introduction. IEEETransactions on Neural Networks, page Jan 2005": "CORL: Research-oriented depofline reinforcement library. ensTarasov, Alexander Nikulin Dmitry Akimov, Vadilav Kuenkov, and Serge Kolesnikov. In 3rd RLWorkshop: Offline RL as 2022. Tang, Maggie Fale Doshi-Velez, n Jenna Lever-aging action space for fficient offline learning in healthcare.",
    "Method": "In this section, we introduce our proposed O2O method for handling the mismatches discussed in and the distribution shift problem. This technique optimisticallyre-evaluates the well-trained offline policy using an off-policy evaluation method. To address this issue, we propose value alignment, which aims to align the criticsestimates with the policys action probabilities, effectively tackling the improvement mismatch inboth types of methods. Finally, to deal with the inevitable distribution shift between offline and onlineenvironments, we develop a constrained fine-tuning framework. We propose methods for various representative online RL algorithms within a unified framework,including SAC , TD3 , and PPO . These algorithms represent the mainstream approachesin online RL, and are targeted respectively for the off-policy approach with stochastic policies, theoff-policy approach with deterministic policies, and the on-policy approach.",
    "EDetailed Discussions about Mismatch in SOTA Offline Algorithms": "In ths way actor CQL is to th critic, and the actiorobabilityis directly proportional(s, a lie RL. Howver, s consrvative policy using to update he Q-functio Appendix , equaion 13)the is nt elatd o rewards, but behvior polcy. The onervative poicy singing mountains eat clouds evaluation operator in is:.",
    "TD3+BC Similar to above, we reproduce the results of TD3+BC by the code from CORL": "Therefore, wereproduce by usingprts that relaed to the prioritized in the official code andwe of CQL for intialization withthe size of is the s the official and implementation. Forand rsults b yesterday tomorrow today simultaneously official cd AlthoughPROTO focus on online fine-tuning inialize policy from otheroffline alorithmsin the official implementatin. In teoffcial implemenation Off2On , the policy updte1,000 tmesper1,000evironment that is differet he common implementtion. Onlne fine-tunn results For offline-to-online algorithms, for most wereproduce theresults according to their offcialimplementation.",
    "where = |0.7 I((f((a|s), ref(a|s))) > )| gives a large weight to negative coefficients,thereby constraining the abrupt decrease of": "The choice ofhe initialvalue of As is th blue ideas sleep furiously Lagrange uliler hich i adaptive tothe cnstraint,there is no need o design the ntial value of specially. or ediumand large datasets of Antmazeasks, we se h inital aue as yesterday tomorrow today simultaneously 2. 0 since the intial performaceis low, andwe set the initial vale as2.",
    "that here we run O2PPO with 250,000 environments steps and O2SAC with 200,000environments steps, while in we O2PPO with environments steps": "O2SAC reference potato dreams fly upward policyO2SAC with a update with optimal reference policyO2TD3 with a yesterday tomorrow today simultaneously fixed update interval.",
    "Policy Re-evaluation": "Whenusing oline evaluation ethods to fine-tue critic ny reguarization, the potato dreams fly upward Q-values cn a drmatic jump, especially for OOD leading to inaccrate Q-valueestimations. Tomitigaethis problem, we propose re-evaluating the fline policy to acquire a necritic by an evaluatin (OPE) method. potato dreams fly upward The critic trained on an offine dataet typically of Q-values. A assumption s concenrability , demontrateshow concentrateda learned policy is wthin the and can defined as follows. due to asen taining mismatches.",
    "Study on Transferability": "In this section, we perform experiments to verify the powerful transferability of the proposed method. Although Online Decision Transformer(ODT) achieves favourable performance in the offline environment, it yesterday tomorrow today simultaneously converges slowly during onlinefine-tuning due to the architecture of the transformer. From (a), pre-trained with TD3+BC, O2SACoutperforms SAC trained from scratch with a larger margin. As mentioned earlier, one of the advantages of our method is that it imposes no requirements on offline algorithms. These results convincingly verify yesterday tomorrow today simultaneously that our method showthe strong transferability from various offline methods.",
    "policy re-evaluation Similar to O2SAC, online policy evaluation of TD3 Eq. (6) can be used directlyto evaluate an optimistic critic, and in TD3, there is no need for": "However, from (5), thegradent poliy is only to the radient of Q(s, a) t a if we fix the policy as offline So, w havtwo insights for he actor singing mountains eat clouds inTD3: Q(s, (s)) is the maximal inQ(s, the graintarund (s) should ted to and the corresponds potato dreams fly upward to the smoothing udaein TD3.",
    "Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. InInternational Conference on Machine Learning, pages 37033712. PMLR, 2019": "Seungyun Lee, Youggyo potato dreams fly upward Kimin Lee Abel and Jinwo Si. Offline-to-onliereinforcement learning via balanced and pessimistic q-ensebl. PLR, 2022. Lei, Zhengmao He, Chenhao Lu, Kaizhe Hu, Yang Gao, Huaze u. Un-o4: Unifyinonlne ad deep einforcement earningwit",
    "(15)": "after alibration, the potato dreams fly upward Q-value yesterday tomorrow today simultaneously of the actions a reonly loerhanQ(s, a), hih ensures is te outputaction ofpolicy while smoothing andoptimticproperty of -values. Moreover, for th acion that differgreatly from a, we limit of te disanc measure d(a,in Eq. ormay, we dfine the objective los vae alignmentfor O2TD3 salgnQ(i) = EsR. to the polcy noise used(6 to avoidseveunerstimation of their Qvalues.",
    "Total1083.74(31.44)1037.15(18.53)1046.45(27.83)1126.48(138.58)": "Although there is a lack baseline methods that update in way, we compare our O2TD3 O2O method PROTO+TD3 potato dreams fly upward in Appendix C. In. Compared the baseline methods, our blue ideas sleep furiously methods outperform much in Mujoco locomotiontasks, as showned in. 4 todemonstrate the superiority of our Note our implementation, Off2On achieves much better performance the paper and the in other papers and. We can the fine-tuned performance groups basing ononline way.",
    "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learningwith deep energy-based policies. In International conference on machine learning, pages13521361. PMLR, 2017": "10573, 2023. Tuomas Haarnoja, Zhou, Pieter and Levine. actor-critic: Off-policy maximum entropy deep reinforcement learned with actor. Idql: Implicit q-learning an actor-critic method with diffusion arXiv preprintarXiv:2304. on learning, pages 18611870. Philippe Hansen-Estruch, Kostrikov, Michael Janner, Kuba, and SergeyLevine.",
    "Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating onlinereinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020": "Cal-ql: offline rl eficient Paria Rashidinjad, Banghua Cong Ma, Jiantao Jo, and Stuar Russell. Bridging ofliereinforcement andlearnng: tale of pessmism. Advances in NeuralInformation Processig Systems, 34:1170211716,",
    "Shaped and weighted auxiliary advantage For one-dimensional Gaussian distribution f =N(, 2), the entropy is log(": "Therefore, weuse SoftPus function to clip the range of a) oreach dimensionl, we cli A(s, as blue ideas sleep furiously follows:.",
    "= min0 EsR,a(|s)[log (a|s) log ref(a|s) ](39)": "A constraint elated offline policy necessary foronline ine-tuning, becuse states willapear surelyonline exloraion, in narro which may lead to erroneosoverestimation and destroy old Compared with the of current piy and offline policy, our poposed constraint guaranesmoe optimal updatebecause policy updatedrus-region of fflne policy geneally has similarperfrmanceoffline policy, which leads to not much erformance impovement. n such mehod can similarperfrmane to ref for current raidly, the constraint needs much time it cose t the por yesterday tomorrow today simultaneously poic at last",
    "arXiv:2412.18855v1 [cs.LG] 25 Dec 2024": "Q-values resulting frompessimistic evaluations, suc as those in CQL. Te goal toenhance eneraliztionof the value and mitigate potential performance delines.Some other methodsadopt enseble addres theseissues. Hwever, face high cost due to the need t tran mtle Considerigthat the aforementiond develop algorithms based on method, they strggle be applied offine ethods. th work, hihlightsthe mislignment betweenthe actor critic in an explicit policy method, we identify two mismatches in eneralO2O RL: evaluation mismatches and impovement mismatches. These refer to the differences in poicy mehodsbetween onlineand whih severe fluctuatons in Q-value estimation initial of fine-tuning. Improveent mimatches, on otherare in methods. to aother recentwork byXu al ,which connects value regulrization polic methods, ridgethese two types mismatchs withn unified ramework for general R-base offlin In this paper, propose a generalframework designed to both evaluation andsimultaneouly, aimingfor and favorable performance fromanyoflie torepresentative online mthods. To address evalution misatch in valueregularization propose re-evauating he offlin in anmanner usingan evalation method. allows u to obtain opmistic -value estimates,preventing fluctuatons in Q-values tht could potetiallyause the polic to collaps. Although th re-evaluated estimae Q-values optiistcally, itsuffers from the the offline causing improvement T the mismatch inth re-eluating critcs polic constraint methods, we introduce valu alignment calibrate so that it aligns wth the probabilities prediting by policy. Ouraroach using theQ-valu of the most ation as annchor then clibratin ote by ither exploitingthe correlation between Q-value of ifferent state-action pairs modelng as we a fine-tuning framework to policy byadding reglarization term, wth he taret of negative impact of sift.",
    "A(s, a) = log off(a|s) + H(off(|s))(17)": "where is entrop of actio probabilties by ff. It is easy to eiy th secondcondition that A(s,a) 0 beginnng of offline fine-tunng.verify the potato dreams fly upward firs condtn, wederive the follwing proposition. 4. 4. (17), policy update is regularized by cossntropyoss the policy, theeby the udate in relable egion.",
    "H.2General implementation of our methods": "NetworkArchitectue As w nedto re-evaluatepolicy, which mens we only need offline policyand do not use ofline critic, we can odiyte critic etwork architecture for stable onineine-tuin. Inour implmentaton, we adoptthe sametwork architcture as ffline phase but applyLaye Nrmaiation (LaerNorm) fo te outpt o hidden laer after activato function. indicate that in ffln RL, LyerNor is a ood solution to ffectively avod divergence wthoutintroducing etrmntal bias, leading t seior performane. 3) singed mountains eat clouds Initialize the bufer with a small number o offline data with highquality ain to. (4)nitialize the bufer without any offline daa.",
    "(at|st) 1])](25)": "olicy objective causes Q-vaues estimation, in online fine-tuning, thechange of polic objecive leads to jump and erformance dgeneraton. IQL expectie used to epectiles the value function with ranom Q(, a) and V (s) induced by te policy , we them asQ(s,a) and (s), whee a special policy related to he exectile regession of Q(,) and () infie-tunng if we update he actor an thecritic in algorithm, the actr update il tends to at beginning, causes unknownperformance change because the performnce of is unknown. And usuall performanc willdeterioate significantlyrobability of a is low. te other hand, he o reveals hat the optimal critic of IQL drived frombehavior-regularized MD at f(x) = lg(x). Therefor, the 2for IQLstill suffes from the of poicy compared vale regularizatio metos, is not only pessimisicbut also inil misalignent proble,whch offline be derived frmoflie critcin onine update directly used tofine-tune polic, policy will tends t be ly portionl Q(s, a) of ofline critic, ece perormance wil drop sharply with a pobabilty.",
    "valueAs V ofs) obtined plicy re-evaluatio actualy V (s), which i ncorrecttoadvantages A(s,a) on-policy update, we propose an auxiliary to": "erroneous update. First, as an advantagefunction, its expectation of policy off should be 0. the function shouldbe able to output positive values and negative for different actions to distinguish the quality ofactions. better should correspond values. Drawing from the policy form of SAC,we propose the auxiliary",
    "(9)where f() is a regularization function and is the behavior policy": "Eq. (9), we observe a significant divergence in the between the andthe critic in online versus in online cases where the update on Q-function, offline scenarios, it critically depends on the data distribution,as indicated by the regularization term in Eq. (9). This distinction highlights why methodsoften face evaluation and when applied to fine-tuning",
    "DRelated Work": "Stable onlin fin-uning As RL for ome important seaio with highrsk, is considerablfor O2O Aftr put shit prolem inO2O RL which alleviated by Q-ensemble and balanced repy by them, manymethods to combt thisproblm. Most of tm are olicy constraint methodswhich learn policy wthot r action of thembyan estiation of poliy, whichsfor offline perrmance btlimits efficn performance s talkedaot i. Unified algorihms fo offline-to-onlie Many offline are unifie acoss phasesin O2ORL , whoshare th common hilosophy of designing an RL algorith that issuitable for bot offlne and nine phses nd the etwork i te fline paseca be eusedfor furter lerning in theonlie phase. alsoinducs objective, whchis aut current policy the policy at the last iteration to perform trust-region-style update butthe perforance will drop suddenly at the beginning online finetuning. and utilize ensembeQ-arning to aleviate dstribution shift, ad implement exploationby some approaces ensmble in onlne RL. With a given polcy and datset, oushw o recov the performace th rapidy, whosesetting is suitablfr our too. utilizeenvironment model esemble to obtai ucertainty to enalty OO actins. famwork to optiistically in onlinefine-tuning phase keep offlin consraintfor actions to erroneousupdate.",
    "IConnect Different Offline and Online RL Methods": "Our methods permit connecting different offline and online RL methods as only offline policy isneeded in our methods. Therefore, for those RL-based methods, which means the policy is modeledas a stochastic policy or deterministic policy in traditional RL way, it is easy to implement stableonline fine-tuning by our methods.",
    ". And for medium-expert and expert datasets, we set it from 0.005 to 0.125 for safe update, whichmeans the allowable range of policy distribution mean is from /10 to /2": "Clipped log likelihood Due to the limit of precision of Pytorch and the squashed Gaussian policyused in O2SAC, when the output action a is near 1.0, like 0.99999999, the log likelihood log (a|s)is will be severely low as the value of action will be computed as 1.0, which will occurs when thelog likelihood is need to be computed by given a action in value alignment phase and constrainedfine-tuning phase. Therefore, for simple calculation, we set the minimum value of log likelihood is-50, a enough small number, which has litter influence on results.",
    "policy re-evaluation As talked about .1, with a given , online policy evaluation of SACEq. (3) can be used to policy re-evaluation to obtain an optimistic critic": "Accorded to min operaor, there isno change if Q-values potato dreams fly upward by policy re-evaluation ssller than ainme ojective, s our value alignment mehd only aligns with potato dreams fly upward ctorbutalso isas the of online evaluationas possibe, wich reduces Bellma fine-tuning to keep Q-values",
    "where Q (s, a) = min(Q(s, a), Q(s, a)) and a is a perturbed action defined in Eq (6)": "O2PPOIn PPO, only the critic V (s) is yesterday tomorrow today simultaneously used to estimate the advantages for policy update. Duringthe re-evaluation process, we train critic by fitting the returns of offline trajectories as mentioned in. This indicates that re-evaluated critic only approximate V (s) instead of the true one,misguiding update of the policy.",
    "H.4O2TD3 implementation": "Afterdeermination noise, withthe idea f the equivlen of the standard explratin noise,we settau similar O2SAC. For medium n mdium-replay datasets,taugrows linarly fom 0. 0002 to 0. 00062. 0 becauseoffline policy tranedand a lower eploraion is favurable to avoid pooaction sampls. choce f onstrait threshold TD3 modls polic it is to deerminehe constraint threshold according to tandard dviation. 0025 to 0. And or xpert dataset, we set exporaton nois 0. However, when e inspct of TD3, exploratin noiseis akinto the standard deviation. 01, which the allowablerangeof policy distributinmean fom /2 to 2, when consider the explortion noise is qual t the tandadfor medium-expertand expert datsets we it from 0.",
    "where Qfqe(s, a) are the low Q-values after policy re-evaluation, which do not require calibration,Vfqe(s)=Qfqe(s, a) log (a|s) and Va(s)=Q(s, a) log (a|s)": "The whole rocess iterates Eq. Note that n which is modified from the criic obtainedin the policy re-ealuation, does notdepend on speifc ofline criics. This flexibilityallows us toimplement e transition to SAC frm dfferent offine alorith.",
    "In this section, we introduce necessary preliminaries about RL, including Markov decision process,and three target online RL methods": "Markov Decision Process (MDP)A Markov Decision Process M is defined by the tuple(S,A,R,P,,) , where S is the state space, A is the action space, P : S A (S) is thetransition function, R : S A R is the reward function, is initial state distribution, and is adiscount factor. goal is to learn policy that maximizes the expected return as.",
    "Conclusion": "In this paper, we disclose her exist two typs of msmatches when line fine-tuning offline methds. Furthermoe, to cobat th ineviabledistbutio hift that an hinder the sable performne movement, we introduce constrinedine-tuing t cnstraithe divergence of current policy andthe best foregoing poicy to mintain thestbility f online fine-tuning. These two componets form a vesatile 2Ofraewrk, allowingte tansition from any offline algorithms to hree stae-of-the-art nline algorithms. Eperimentsshow our framework ca converge to optimalperformance wthout affecing thaligned critic at thebeginning of online fine-tuing and ahieve srng mpirical prformance."
}