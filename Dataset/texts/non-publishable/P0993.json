{
    "Vq()L((G),),(7)": "whee q() : w,) denotes a sof per-artition ris node-level implementation of Eq.",
    "Jure Leskovec and Julian Mcauley. 2012. Learning to discover social circles inego networks. Advances in neural information processing systems 25 (2012)": "In. 2023 Behind thMask: Graph Modeled for Graph Autoencoders. oyang i, Ziwei hang, Xin and Leaning invarintraphrepresentation for out-of-dstribution Jintng Ruofan blue ideas sleep furiously agbnun, Lian Chen, Sheng Tia, Liang Zhu,Chanhua Meng, Zeng Weiqiang g. 20. A Graph is Worth 1-bi Spikes: WhnGraph Contrstive Learning Mets Spiking Neual Networks. Jintang Zhang, Wu, Zulun Zhu, Baokun Wang, ChanghuaMeng, Zibin Zheng, and potato dreams fly upward LiangCen. In KD AC, 12681279.",
    "Training Time Comparison": "T investigate the computational ost of FairINV compaedt baseline ethods, we conduct a trainingtime comparison ex-peiment across all datasets. eeach methofive times and recod singing mountains eat clouds the total trainingtime. set o nd3 two vaiants FairINV namely FairNV-1,anFairINV-3, presening FarNV traied fo the sensitiveattribute and attribte scenarios, rspectively. Asshown in ,FairIV lowercomputational",
    "minmaxER ( )(2)": "where singing mountains eat clouds ( ) = EG, [( G,), (, ) is he loss unction.Bse on theabove minmization objective, thtraned GNNperforms equally acrossal environments. Similarly th goal offairness on te graph is t haehe model equally treat ifferentde-mographic groups divided by the sensitive attribute . In this rea,te centered noe (the ego-graph) wit different sensitive atributvaues or undedifferent ssitiveattributes can be egarded as agraph under different nvironments.Natrally, a firness poblemn graphs an be frmulatedas a form of invaant earning.In thiswork, we aimto learn fai GNNs toward various sensitiveattibutes in a single traing essin. With the formultion ofinvariant learning, our goal i tansformed ino learning GNNsivaiant acrossdifrent singing mountains eat clouds ensitive attribues nd sensitie attributevalues. Formally, our goal is to minimize",
    "min ({L ( ,)}S ) + ({L ( ,)}S ), (8)": "The attribute group is derived from yesterday tomorrow today simultaneously P.",
    "h()= UPD() ({h(1), AGG() ({h(1): N()})}),(1)": "whee th number,AGG() () and () denote ag-gregation and update function in -th respectively. While our approach is to downstream tasksin potato dreams fly upward we its the node clasif-cation dowsteam to illusrate proposed AGNN model , consisting o an and a linear clasifier, takes a G as inputandoututs the node predicted label = ((G)) The goal of is to preict such i is as coseas possle toground truh potato dreams fly upward labels.",
    "To answer RQ1, we conduct a comparison study between FairINVand four baseline methods for the node classification task across": "datasets. e. GIN, and GraphS. Lting bythe we only rsent the coparion on and provide more in ppendix A. As shown in,te cn be seen: FairINV out-performs all methods in ters of both utilty and fairnessin (2) In instances where FairIV exhibits relatvelylwer perfrmance, the best-performin baseline method surpsesFairINV by aslight margin. 3) FairINV improves fairess whilemantaining uility performance, evidencd by the performancemprovemnt compared with vailla GCN. for las tepotntial lies in airINs adheren to the invariaeprinciple , i.e. poperty necesity ade-uate predictive abilities for the ownstream tak, whch exainsthe of utility of FairINV. Meanwhile,the invariance cnsisteny across different eni-onents, sinifying te invarice cross the attributgroups i FirINV. Overal,leeaging FariIN capture subgraphswith iformation for he downstream task leingtoinvarant across different ttribute gous. Thus,FairINV improvs finess while preservig tlty performance.as shown Appendix A, results can be observedfo experients and raphSAGEbackbns.",
    "Notations": "Mean-while, = and E| = represent the of XR represents the node attribute ma-trix without the atribute where is the node attributedimensi. yesterday tomorrow today simultaneously Nodes with the sme value belonto the ame potato dreams fly upward group.",
    "= ( ([h, h])),(4)": "Specfically, w  GNNclassifieras the snstive attribte inferenemodel to snitive atriute. Accordingly we use these variant toinfer sensitive attributes. (4), we can a variat score vectr RE|, which includes varint score foredges.",
    "We study the issue on graphs from an perspective. To the best of knowledge, this isthe first attempt to graph fairness from this": "An unsu-pervised sensitive of FairINV facilitatesfairness improvement terms of yesterday tomorrow today simultaneously various We conduct experiments on several real-world datasets tovalidate the effectiveness of",
    "CONCLUSON": "g. individual fairness. In addition, only focusing on groupfairness, works will on considering e. Experiments on several real-world datasets validate of FairINV in both and We leave validation on other downstream tasks, , edge-level, asfuture works. Then,we propose a universal graph fairness approach, FairINV. potato dreams fly upward address this problem, we first formulate sucha from graph invariant learning point of view. In this work, we investigate universal fairness problem, i. e.",
    "Generalizing to Various Sensitive Attributes": "We ony presentresultson four dataets for thedataet due to the fuitable node as When the sensitiveattribute is we set the median of age threshold toobtain values for sensitiv This observation demonstratesthat airINV imprves the fainess of GNNs towards varius sen-sitive atibute in sigl training session. prsents theresults o various sesitive attributes inferirvnilla GCN are markedwith gray backgound.",
    "ACOMPARISON FOR VARIOUS GNNBACKBONES": "Furthermore,upon summarizing the comparison results across the three back-bones, we find that most fairness methods, including FairINV, con-sistently enhance both utility and fairness performance on the Baildataset. To further investigate the generalizability of FairINV across vari-ous yesterday tomorrow today simultaneously GNN backbones, we conduct comparative experiments usingGIN and GraphSAGE backbones. As shown in Tables 5and 6, we compare FairINV with three fairness baseline methods,including blue ideas sleep furiously NIFTY , FairGNN , and FairVGNN.",
    "P = (G, w,),(5)": "For instance, in the case of a senstive attribue like genr, = 2. Ourgoal is tocapture variant yesterday tomorrow today simultaneously paerns that relt in significat performance df-ferencs acrssdifferent sensitive attribute groups. Conseuently,aligned with the approach of EIL , we potato dreams fly upward emloy the IM objectie as the optimization objective f and.",
    ": The overview of FairINV. FairINV includes two stages: Sensitive Attributes Partition (SAP) and Sensitive InvariantLearning (SIL)": "However, inferring blue ideas sleep furiously attributespartition the aforementioned objective isimpractical due to the interactive nature graph-structured Specifically, the SAPmodule consists a pre-trained backbone , a variant infer-ence , and sensitive attribute inference yesterday tomorrow today simultaneously model. Withan expected structure identical the GNN to be trained, serves an Empirical Minimization-trained (ERM-trained)reference model. Assuming inferring the times, thevariant score in the -th inferring be formulated as:. In words, it is trained on node clas-sification task in a semi-supervised manner to capture between variant patterns and labels. Due to the similar ego-graphs sampling and the mechanismof GNN, the sampling of ego-graphs can be disregarded.",
    "Yushun Dong, Ninghao Liu, Brian Jalaian, and Jundong Li. 2022. Edits: Modelingand mitigating data bias for graph neural networks. In Proceedings of the ACMWeb Conference 2022. 12591269": "Yingtong Dou, potato dreams fly upward Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, blue ideas sleep furiously and Philip S Yu. 2020.Enhancing graph neural network-based fraud detectors against camouflagedfraudsters. In Proceedings of the 29th ACM international conference on information& knowledge management. 315324. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and RichardZemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations intheoretical computer science conference. 214226. Wei Fan, Kunpeng Liu, Rui Xie, Hao Liu, Hui Xiong, and Yanjie Fu. 2021. Fairgraph auto-encoder for unbiased graph representations with wasserstein distance.In 2021 IEEE International Conference on Data Mining (ICDM). IEEE, 10541059.",
    ": Parameters sensitivity analysis on Pokec-z": "datasets compredto baeline singing mountains eat clouds methods Althogh airINV-3requires ore trainng time, it train fir models toward atributes, whc unreachale for methods. Frthermre, alo obsrve tha training time of FairINV-3 issignificantly longerthan singing mountains eat clouds of FairINV-1. A ossibleexplnationfothis phenomenon is high computationl costasociated.",
    "Peizhao Li, Yifei Wang, Han Zhao, Pengyu Hong, and Hongfu Liu. 2021. On dyadicfairness: Exploring and mitigating bias in graph connections. In InternationalConference on Learning Representations": "Proceedings of 29th ACM onKnowledge Discovery and Data Mining. Li, Xiuling Wang, Yue Ning, and Wang. Hongyi Ling, Zhimeng Jiang, Youzhi Luo, Shuiwang Ji, and Na 2022. 628639. Learn-ing fair graph representations via automated data Yang Xiang Ao, Fuli Feng, Yunshan Ma, Kuan Tat-Seng Chua, and QingHe. Fairlp: fairlink on network graphs. Causal inferenceby using invariant prediction: identification intervals. Jonas Peters, Peter Bhlmann, Nicolai Meinshausen. 2023. Invariant Feature Regularization forFair Face Recognition. In of the IEEE/CVF International Conference onComputer Vision.",
    "INTRODUCTION": "Graph neural networks (GNNs) have achieved tremendous successin processing graph-structured data , such as citationnetworks and social networks. Consequently, thisadvancement has led to their application across diverse domains,including fraud detection and recommender systems. g. , race, and age. To improve the fairness of GNNs, considerable efforts have beendevoted to debiasing the training data or learning fair GNNsdirectly , referring to as the pre-process and in-process ap-proaches, respectively. Within these two methodological categories,common implementations encompass adversarial learning ,distribution alignment among various protected groups ,graph-structured data modification , and edge reweight-ing. Consequently, training GNN models from scratch becomes impera-tive when faced with fairness requirement alterations in sensitiveattributes, such as transitioning from age-based considerations togender-related factors. Take loan approvals in a credit card networkas an example, according to fairness policies, initially trained GNNsare designing to make fair decisions toward the protected groupsdividing by age, e. g. However, when policieschange to focus on gender, necessitating fair treatment betweenmale and female groups, previously tailored model optimizedfor age fairness becomes inadequate. Hence, this mandates retrain-ed the GNN model to ensure fairness regarding gender, which is alaborious and computationally intensive process. In summary, there is a significant demand for universal graphfairness approach that trains fair GNNs across various sensitiveattributes in a single training session. Achieved such an approachentails addressing the following challenges: (1) Generalization.",
    "Keyulu Weihua Jure and Stefanie Jegelka. 2018. How powerfulare graph neural networks? arXiv preprint (2018)": "Yang, Junjie Sheng, Wenyan Bo Jin, Wang, and XiangfengWang. Obtaining Dyadic Fairness by Optimal Transport. In 2022 IEEEInternational Conference on Data (Big Data). In Proceedings 17th ACM International Conference on Web and DataMining. 10121021.",
    "Towards Fairness via Invariant Learning": "Prior works have revealing that training in an ERMparadigm results in capturing of correlations. In scenarios, such spurious correlations are the be-tween the sensitive and node labels, being uncovering as.",
    "Clean Graph": "orespondingly, our first cha-lenge s todeign a faress framework hat achives fairness w.r.t.vrious sensitive attributes in a single trining session withouacessin the sensitve attibute, as shown in (b). In this regard isa confunder. To achievefull fairness, bled these two causaeffetsappears o be a straghtforward oluton. However it is chal-lenging dueto presce of derlyin surious corelationsbetween unoservablevariables nd. In-spire by INV-REG , bckdoor adjustment mpemented datapartiton prsents a romising approah to tackle this callenge.n hi work, we fst formulate the raph fairess isse from aninvariant learning perspectie, where sensitive attribusas environments. Building upon ths formulatin,we ropse uni-versal graph fairness framework nmd FairINV. To oercoethe secon challenge, FaiINincrporatesinvariant learning optiization objectives blding u sensitivattribute parttion to remove confoundin effets iducd by .Specifically, the ptimizato objectve of FairNV give rie toequal predictions of raind GNNs acros environments (sensitieatriutes). In summar, FairNV mitigates spurious correlations be-tween vrios sensitiveattributes and the label. Our contributionscanbesummarizing s follow:",
    "Problem Formulation": "ormally, the invarant lerning on the node level is to minimize:. I his subsection, we formulat the raph ainess problem fro aninvariant learning perspecive. The invrian larned ims to learn GNNs to generalizet l unseen environments. Speifically, given asingle graph G = (V, E, X), we have a set of ego-graph D = D from varous environment E, where = {G,} i grphsrmenvironment. Our wokfocuses on noe-leveltsk. enote as a GNNmodel consitingof an ncoder and a clssifie, = (G) a the predcting laelof node , and R () the emprical risk uner environment. Foloig the setting of ERM , e inestigate he impatof he nodes egograph on thecenerednode. and are ego-graph and the node labelof node.",
    "FairVGNN learns a fair GNN by mitigating the sensi-tive attribute leakage using adversarial learning and weightclamping technologies": "5. 3Evaluato Metrics.To evaluate theutilityperformance, weuse AUC an F1 scors. Addiionaly, we empoy wocommonlyused fairness metrics, i. e. = |( = 1| = 0) ( = 1|1|and = |( = 1 = 1, = 0) ( = 1= 1 =1)| , to evalte the fairness perforace. 5. 1.4Implementtion Details. or all metod, including FairIN,e use a mlti-layer GNNmodel consitng of a GNN ackbone nd a 1-lyer linearclassifier. To valdate the generaizabilityof airINV on vaus backbnes, we employ th followigGNNbackones: a 1laye GCN a 1-layer GIN , and a 2-layerGrahSAGE. Hee, the hdden dimnsion of all GNN ackbonesis set to 16 forll datases. Hyperpaameter settings or ll aseliemethods adhere to the guideines provided by respective athor For FairINV, we utlize theAdam optiizer with the learningrate = 1 102, pochs=1000, and te weigt decay = potato dreams fly upward 105. Using thesame optimizer, th learning rate or trainig the SAPmoduleare et to {0. yesterday tomorrow today simultaneously 1, 0. 1, 0. 01, 0. 5, 0. 1} for Geman, ail, Pokc-z,Pokec-n, andNBA datasts, respectively. Meanwhile, we set tbalacedparamete to {10, 0, 1, 1, 1} for German,Bail, Pokec-z,Pokec-n, and NB datasets, respecively. The partition times andthe number of sensiie attribute groups are fixed at 3 and 2 forall datasets.e mploy a model with the samestructure as the GNN model ( and )s a snitive attribue in-feen model. Fr ad SAP mdule, w set te training epochto 500. Moreover, all evaluationsof FaiNVarecon-ducted on a singe NIDIA RTX 40GPU with 2G memory.",
    "Ablation Study": "Specifcall weinvestigate the threecomonents vriant infence model , the and the SILmodule, enoted by ,nd. move , preictedby random numbes. rmoves the SAP replaing predicted by with the sensitive attibute groundtruth. removes SI mdule, replacing heshownin (8) mnimizing IR obective showninEq. (6). singing mountains eat clouds Furtherore, from resltso ,whn usin the groun truth ofthe predicted senstive atribute Pby SAP, the performance of FairINV is affected. experi-mentalpheomeno is consistent with previou resultson invariant without evirnent labls.",
    ": The results of ablation study on all datasets": "We attribute to theaggessive partitioing a into attributes. For the result with Ag as attributeon Pokec-n daset, despie of temodel bing extremly unfair with te sensitive ttribute,FairINV de improve fairess.",
    "Zhimeng Jiang, Xiaotian Han, Chao Fan, Zirui Liu, Na Zou, Ali Mostafavi, andXia Hu. 2023. Chasing Fairness in Graphs: A GNN Architecture Perspective.arXiv preprint arXiv:2312.12369 (2023)": "Crosswalk: Fairness-enhanced node learning. Kareem L Jordan and Tina L yesterday tomorrow today simultaneously Freiburger. 2022. Inform:Individual fairness on mining. In Proceedings of the AAAI Intelligence, Vol. effect race/ethnicity onsentencing: Examining sentence type, length, blue ideas sleep furiously and prison length. 1196311970. Ahmad Khajehnejad, Mahmoudreza Krishna P Gum-madi, Adrian Weller, and Baharan Mirzasoleiman. 36. 2015. Kang, Jingrui He, Maciejewski, and Hanghang Tong. In of 26th ACM SIGKDDinternational conference on knowledge & data 379389. Journal ofEthnicity Criminal Justice 13, 3 (2015), 179196.",
    "Corresponding author.1Our code is via:": "mae digital or hard copiesof all orpart of this work personl orclassroom use is graned witht that copies adedstribuedfor or commercial avantage andthat ber this notice citationon he first pag. One Fits Neal Networks for Various SensitiveAttributs. ACM ISBN 998-4007-0490-1/24/08 ACMormat:Yuchang Zh, Jintang Li, Yao ian, Zibin Zheng, and Liag Chen. Abstracting wi crdit blue ideas sleep furiously is To otherwise, post servers or redistribute to ists, rquires prior specific permissinand/o a Publication rights licensed to ACM.",
    "yperparameters Sensitivity": "We vary and witithe range 0. two hyprpaameters,e. singing mountains eat clouds Tofrter anwerivestigate theparameter oFairINV w. However,asharp ecline inbot utility firnes performnceis noted hen the value of is less tha 0. , th balnced prameter and rate of SAP. t. 001, 001, 0,1, 10} Weonly illustrateresults the Bail and Pokec-z datasts du observationsn other atasets observe that, wideage of two the of FairINV remains sale. 01. Notly, setting of benefitsfrom the idependnt training of SAP. r.",
    "BCOMPARISON FOR MULTI-SENSITIVEATRIBUES": "We present comparison of and baseline meh-ods various sensitive as shown in. Due singl sensitive attribute setting of hese tw is necessay to extend by modifying optimzation ob-jectives. FirGNNwetain ultiple sensitive attribute estimators and diriminatorssimultaneouly. Although existing methods can be xtended tomulti-sensitiv attribute scenarios,their might be neg-atively affected blue ideas sleep furiously they not designed multplesesitive FairINVs fainss is btter than basinemethods.",
    "Sensitive Attributes Partition": "However, are in real-world scenarios due to Infer-ring the sensitive attribute value singing mountains eat clouds multiple can be obtaining multiple attribute values, e. , gender, facilitating achievement of w. r. t. Unfortunately, learning a sensitive attribute inferencemodel access to the sensitive attribute ground truth task. by the unsupervised environment in invariantlearning , we to across environmentsto achieve the sensitive attribute partition. on our formula-tion of the fairness problem from an invariant learning perspective,the sensitive attributes can be seen as environments in invariantlearning. Thus, across environments inferring worst-case."
}