{
    "Kaggle-Competition. give me some credit. improve on the state of the art in credit scoringby predicting the probability that somebody will experience financial distress in the nexttwo years, 2011": "Dimitrios Kalazis, avid Eklund, Georgio Arvanitdis, Soren Huberg. Vaiationalautoencoders with rieannian motio priors. In Internatonal Conference Lenng, pages 50535066. 2020. Karimi, Gilles Barthe, Borja and Isabel Valera. Model-agnosic coun-teacual fordecisons. In Internaion cofernce on artifi-cilintelligence and pages 895905. PML, Amir-Hossein Karimi, Jlius Kugelgen, Scolkpf, and Isabel Valera. recourse unde imperfect causal knowlede: a probabilistic proach. Advancsn neural prcessing 33265277,02b.",
    ": Optimization paths in the latent space of Normalizing Flow": "Figures4(b)and show its in th an spaces, potato dreams fly upward respectely, revealing theirfailure to dta topology rsulting in paths that passthrough the hole. from the viewpoit the latent space ambient respectvely, that nive spce traversal using standard SGD fails to rspect data manifolds topology as itis neither encoded the classifier nor the latent spacegeomtry, allowing countefctualsto stray from thedata distributon. In using pllback from the ambientspce or theof the classifie (RSGD-C) this by making it cotlyfor couterfactual to deviate th Nex, a) dsplay priordtribution yesterday tomorrow today simultaneously tried Normalizing Flow, hich porly represents th hole.",
    "= M1Z (z)zfy(z)andfy(z) = (c(E[g(z)]), y).(3)": "We normalize the Riemannian gradient r as we are interested in the update direction, and toavoid vanishing gradients near boundary of the latent data support in Z. Note that thisRiemannian metric makes the optimization trajectory invariant under reparametrization,i.e., another latent representation that generates the same density in X will provide thesame trajectory on the data manifold",
    ". Basic pull-back metric": "Specifically, we consider stochastic decoder potato dreams fly upward x = g(z) =(z) + (z) , with N(0, ID) and being point-wise product. When ambientspace X is endowed with the Euclidean metric singing mountains eat clouds MX (x) = I, x X, expected pull-backmetric in the latent Z = Rd space is equal to. The standard VAE is used for learned stochastic generative models with Gaussian decoders(Kingma and Welling, 2014).",
    "B.2. Models": "01 bandwidth and 200 350 ceters for Adut an yesterday tomorrow today simultaneously. onsecutvely, we fee themwhle for epochs , parameterizedby Rdial ass Funcion neworks (Que Bekin,2016) a 0. Forboth atasets, classifier c unde yesterday tomorrow today simultaneously observatin a 4-layr ,2H H,H neurons including Batch Normalizatio (Ioffe, 2015)layer, ad Tanh before layer whereH = 24 is te size representatin space.",
    "Counterfactual Explanations via Riemannian Space Traversal": "continuous are final weight, of education, weeklywork hours, as well singing mountains eat clouds as capital and loss which we log-transform. The goal is to predict whether anindividual face distress within two years (y = 1, 93% of The age is considered immutable.We normalize features and split datasets into 75%/25% for train/test. blue ideas sleep furiously",
    "Michael Downs, Jonathan L Chu, Yaniv Yacoby, Finale Doshi-Velez, and Weiwei Pan.Cruds: Counterfactual recourse using disentangled subspaces. ICML WHI, 2020:123,2020": "Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi, Franco Turini, andFosca Giannotti. Springer, 2023. Tri Duong, Qian Li, and Ceflow: robust efficient counter-factual explanation framework for data using flows.",
    "Divyat Mahajan, Chenhao Tan, and Amit Sharma. Preserving causal constraints in coun-terfactual explanations for machine learning classifiers. arXiv preprint arXiv:1912.03277,2019": "Ramaravind K Mothial mt Tan. Under-standingthelatent space of models throug lens of rieannian geometry. 20287, 2024. in Neural In-formation Sysems, 35:05921072, 2022. Jour-nal of Machine Learning Research 2021. Explained machine learningclassifiersdiversecounterfactual explanations. Park,Mingi Jaewoong Choi, singing mountains eat clouds Junghyo Jo and Youngjung U. Laplacian autoencoders for learning representations. Nmalizin fow for probabilistic modeling and infrence. Marco rederik Skate, nd Sren Hauberg.",
    ". Evaluation Metrics": "blue ideas sleep furiously W follow the evaaion protocol of Pawelczyk et al. (202), by mesuring violation, .,ho ofen aCE method breaches user-defined constraints (immutble fetures), as well asvalidity with Flip Ratio (FR), representing he sucess rate of CE classified as the targetlabel, along with thir confdece c(xCE). o masurealism, i.e., how clos te CE is to he data manifold, wcompute the local Euclideandisanc of C t theclosest traning data pointin theambient space denoted asLD.",
    ". Results": "As expected or Riemanna more steps to achieve CEs. compae optimiers acrossvayin confidencethresholds (CTs), we assess prpotion of CE successfully geneatedat each treshold, te Confidence Threshold (CTR), byselecting thefirt C eachig a CT from the counterfactual trectries. Both RSGD RSGD-C maintin clse t datamanifold LD) and the input (L0, 2, withoutrequiringfidelity constrinturig Afte 100terations, ouroptimizers generate cloer theoriginaliput compared o SG wit constraint and step while mntaini comparablevlidity and feer violations of immuable eaturs.",
    ". Riemannian latent space geometry": "The columns of the Jacobian RDd g() span tangentspace at a point x M, so tangent vector can be written as v = Jg(z)v, wherev Rd are the intrinsic of the tangent vector on the associated We can approx-imate the by considering latent representations zn and learning a function x g(z). This function correspond singing mountains eat clouds to the true parametrization of M, yet it can approx-imate relatively yesterday tomorrow today simultaneously manifold M g(Z), and hence, induce a pull-back.",
    "Yoshua Bengio, Aaron Courville, and Vincent. Representation A reviewand new perspectives. IEEE transactions on pattern intelligence,35(8):17981828, 2013": "InConfrence on Solving fromNatre, paes Springer, 202. Susanne Dandl, Christph Martin Binder, Berd Bischl. Transactionson Neural and Larning Systems, 2022. obias Leemann, Kathrin Seler, Joanes Haug, Martin singing mountains eat clouds andjergi Kasneci. networks and tabular data:A srvey.",
    "MX (x)  Jh(x)Jh(x).(4)": "This in X the rrespnding local geometry the learning epresentatioH. Assuming well-separatd classes in H,we expect MX (x) t be small in regions f Xwhere the csss reain an to increas near th decision bundary. principle,Equatio blue ideas sleep furiously () fo yesterday tomorrow today simultaneously couterfactual optimization directl that respects tegemetry of learned reresntation.",
    "B.1. Datasets": "The icldes census yesterday tomorrow today simultaneously records D = 13 featres. goa predict whether an idviduals income $0K/year ( = 24% of Paelczyk et al. (221), webinarze as agregated vrsiosof oiginal categors indicating whetherthe individua works in rivate industr, isa other occupation, isnot a husband, as (rac) sex),",
    "Pegios Feragen Hansen Arvanitidis": "Work on this project was funded Pioneer blue ideas sleep furiously Centre for (DNRF grant nrP1), the DIREC project EXPLAIN-ME (9142-00001B), and the Novo Nordisk Foundationthrough Center for Machine Learning Research in Science (NNF20OC0062606).",
    ": Adult Dataset. Distance to closest training sample as a function of gradient steps": ", age, s altering them cannot providectionable feedbak. Give MeSome Cedi (GMC) (Kaggle-Cmpetition, 2011) includes D = 1 continous eatures,amingto predict whether an individual will face financial distrss. Both datasets include asubset of features that are consdered imutable, e.",
    "MZ(z) = E[Jg(z)Jg(z)] = J(z)J(z) + J(z)J(z)(2)": "We ensure well-calibrated by using (z) 1/ (z), with Z R>0 a Radial Function network (Que and Belkin, based on latentrepresentation, satisfyed criteria (Arvanitidis et al. , 2018). Therefore, the of moved off the data manifold increases withthe uncertainty, result the corresponding paths staying data manifold. Under the mild conditions on the smoothness g() and behavior of (), MZ(z) RDD0constitutes a Riemannian metric in Z, captures the geometry the datamanifold in ambient space M X, hence the shortest paths between points in yesterday tomorrow today simultaneously this geometry. t. Thus, the of manifold is preserved in a soft sense: Itis possible, but expensive, shortest paths, and singing mountains eat clouds hence counterfactual trajectories to manifold, meaned they do not do unless r. the pull-back defined in Equation (2),. e, (z) 0 when z is near the training codes, otherwise +(Hauberg, 2018). , 2018). where : Z and : Z R>0 neural networks et al.",
    ". Topology preservation and counterfactual trajectories": "We a surface in X R3 as = [z, sin(z1)] + where zj U(0, 2), j = 1, 2and 0. The proposed is illustrating in (a), with a black-white-red color schemerepresenting in latent space, from high. Figures and 3(c) show,. 2 the in R3, assign y = Normalizing Flow with a latent space of d = D = 3, a with d = and aclassifier with a representation of H = 32 to pull back to latent space. 12 I3), a hole in the by points in the center withradius ||z||2 < 0. red area the centerindicates the cost of crossing hole, while the boundaryhighlights cost of leaving the data-populated region.",
    "Averge Gradient Steps": ": GMC Confience Threshold Raio (RT) quantifies cfidencesccss, i. e. , thepopor-tion of CEs achiving aspciied C. L mesures fidelity to the nifold(realim),L2 the he niial point (closeness, of requiring gradient to achieve a specifi confidence level",
    "Martin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. Learning model-agnostic coun-terfactual explanations for tabular data. In Proceedings of The Web Conference 2020,pages 31263132, 2020": "arXivpreprin arXiv:243. 070, 224. Crla: A pyton libray to benhmark algrithmic recourse and counterfactualexplanation lgorithm. In hirty-fifth Conference on Neral Infomtin ProcessinSytms Datasets nd BenchmarksTrack(Round 1), 2021. I Procedings ofthe AAAI/ACMConference o I, thics, and Society, pages 344350,2020. Diffuson-basing iterative counterfactual explanations forfealultrasound imge qualityassessment. Rafel Poiadzi, Kacper Sokol, Ral Santos-Rodriguez, TijlDe Bi,nd Petr Flac."
}