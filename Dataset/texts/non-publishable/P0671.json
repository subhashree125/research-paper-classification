{
    "Tsendsuren Munkhdalai, Manaal and Sid-dharth 2024.Leave no behind:Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143": "Bo Peng yesterday tomorrow today simultaneously Eric lcaie, Quentin Anthony,Alon Albalak,Sauel Aradinho, Stella Biderman, Huanqi Cao,Xin Cheng, Michael Matteo Grella,et al.2023. Rwkv: Reinventing for arXiv preprint arXiv:2305.3048. Peng, Jeffrey Qeselle,Honglu potato dreams fly upward Fan, and EicoShippole. 2024. YaRN: window ex-tesion large language In he TwelfthIntenational Confeece Learning epesenta-tons.",
    "Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Lon-glora: Efficient fine-tuning of long-context large lan-guage models. arXiv preprint arXiv:2309.12307": "Zihang Zhilin Yang, Yang, Jaime Car-bonell, yesterday tomorrow today simultaneously Quoc V Le, and Ruslan Salakhutdinov.2019.Transformer-xl: Attentive language mod-els a fixed-length context. arXiv preprintarXiv:1901.02860. Dong, Tianyi Tang, Junyi Li, Xin Zhao,and Ji-Rong Wen. 2023. Bamboo: comprehen-sive benchmark for evaluating long text of large language models. arXiv",
    "Anecdotally, Mistral v0.1 that uses sliding window at-tention (a technique that improves perplexity at long range)does not perform well on Needle in a Haystack at longrange (v_JULY_v, 2024)": "to Rulers easurement, dos rquire uderstaning ca-pabilty allowing us to measure mdel of anysize(e. to effctive length Rle, the mea-sred apability ees to exceedLlama-2base/chatbaseline. g.",
    "Treating all prompts as i.i.d experiments allows for theo-retical comparison of two models using statistical tests. How-ever, large variance makes achieving statistical significance": ", 2024; et al. For instance, in the in aHaystack task model is prompted tolocate needle, failure can be difficult to as it could be due to a in themodels capability orits natural capability. Inference Factors (IF)The third limitation weidentify is current long-context evaluations of-ten fail to distinguish the influence of inherent such as natural understand-ing etc. Some benchmarks et al. Secondly, it challenged these metricsto unaligning and due theirlimited to follow instructions to find This that new architectures scaled up and aligned before metricscan be applied, prevented rapid ex-perimental validation of a models long-contextcapability. benchmarks such asRuler (Hsieh et al. , 2023) can force a to produce answersthrough carefully prompts, approachis still cumbersome and model-specific. Sim-ilarly, failure in tasks such as long context questionanswering or mathematical expression calcu-lation may not reflect shortcomings in modelslong-term rather, deficiencies in itsspecific abilities. , StreamEval (Xiaoet al. , conduct testswithin a confining which restricts theirability to evaluate model performance in longercontexts.",
    "Limitations": "Notably, when theimoses astrct limit onits length (e. g. Secondly, alhough we e-tensively teted robustness various szes(83M, 7B), using diferentsettings (baseand chat) and uder various corps settings, weremark that the tests performed within theLlama transferability of r-gettig to models unexplored. This work par by the Nationalcenc Fondtion ofChin (No. 62276056), theNatural ScienceFoundation blue ideas sleep furiously of Liaoned Provinceo Cina (202-KF-16-01), yesterday tomorrow today simultaneously the Funamental Re-search the Cental Universities(Nos.N226016 N2316002), Yunan Fundamen-tal Research Projects No. andthe Program of Introducing Talets of Disciplnto Uniersities, Plan (o. B16009). Josua insie, ames Michiel de Jong, Federico and Sumit Saghai. 2023. multi-query rans-forme models multi-head checkpoints. arXivpreprint arXiv:2305.13245. Shansan Gong, Med Zhong, Mukai, Jun hang, Lingpeng Kong, L-eval: Instituti standrdized evaluationfor long context languagearXiv prerintarXiv:2307. 1108.",
    "n LWM with 1Mclamedcontext length an show forgettingcurve partially atAppendix ) are very slw and suffers from out-f-memory (OOM) ssues in our settings": "This highlights current limitationsin RNN development for accurate token recall. grained Memory Length, indicating an inability toperfectly memorize or to retain memory at any sig-nificant length.",
    "Abstract": "Weprovide an extensive survey for limitations inthis work and propose a new method calledforgetting curve to measure the memorizationcapability of long-context models. We also examine the difference betweenour measurement and existing benchmarksas well as popular metrics for various mod-els. Numerous recent works target to extend effec-tive context length for language models andvarious methods, tasks and benchmarks exist tomeasure models effective memorization length. However, through thorough investigations, wefind limitations for currently existing evalua-tions on models memorization capability.",
    "v_JULY_v. 2024.The paper-review dataset wasusedtofine-tuneseparatelymistral,gemma": "2019. 03771. Zekun Moore Wang, Zhongyuan Peng, Haoran Que,Jiaheng Liu,Wangchunshu Zhou,Yuhan Wu,Hongcheng Guo, Ruitong Gan, Zehao Ni, ManZhang, et al. arXiv preprintarXiv:1910. 2023. Huggingfaces transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz,et al. Rolellm: Benchmarking, elic-iting, and enhancing role-played abilities of largelanguage models. 00746. arXiv preprint arXiv:2310.",
    "(b)": "(a) Vrious irrelevant text sources, and copytext is sourcd se. (b) Vaious copy extsources, and irrelevanttext is suced from",
    "D.1Llama-83M": "9, donot employ in our model. We train the Llama-83M model for one epoch on the training set, with the of probability of the next token, which is the pre-training objective language models. 2024a)shown in. optimization performed using optimizer Hutter,2017) with beta2) = (0. We use the scheduler (Hu et al. and training hyperparameters listing in.",
    ": The curve and perplexity (bot-tom) for the Llama-XL, is trained and tested PG-19 dataset": "Comparisn of cure an other con-text length measurement. conextlength measrd by curves as well asby Rule al. , 2024) (i. e. h ffectiveLegth in. Mamba RWKV), the effectivelength measured by Ruler is low; hweve,th inverse does not hold,s shownthe on CaGL3, YarnLlama-2 and LWwhere we observ cntex while Ruereffctive lngth reans think thisiscrepancy are ue to two interleaved ctrs: Rler usesmainly syntheic fulfilin whichrequires modelunderstndng capability,and.",
    "*These authors equally to this author": "Thecurve f Llama-2-base-32k x-axis denots length. blue, ad red areas respectively indicate fine-grained memory where the model achieves accuracy (excep very short sequences),coarse-grained memory were copyaccurac surpassesLM and the amsia area wher te modelcompletely ignores the prefix. approache caim significantlyt con-text window coeponding models withempirical validations.owever, to bestknowlede, the cmmunty has notestablished (andtus has noagreed on) a mtric that odels inherent memory with respect tothecontex, i other singing mountains eat clouds wors, mdels effecivememy length. most used mtric for evaluating a mdels ntural generation abilityis 2024b) This questions aoutte vlidity f using an of amodels lon-context cpability. yesterday tomorrow today simultaneously",
    "An lustrative Example": "te text P-19 Rae al. Eachacurayis calculated 10 times,allowing to potato dreams fly upward plot the meancuacy and the vari-ance in the curv. a forgetting curve (). 6In. the ccuacy data point(1K), we obsere hat Llama-2-base-32k copy task pefectly instruc- 5Oe notice this is he sme issu e hghlighten regardig perpleity. 4, e demonstrate that distibutin usedo sample the radom string impact the langugemodel accuracy.",
    "Albert Gu and Tri Dao. 2023. Mamba: Linear-timesequence modeling with selective state spaces. arXivpreprint arXiv:2312.00752": "Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shan-tanu Acharya, Dima Rekesh, Fei Jia, and Boris Gins-burg. 2024a.",
    "Experiments": "Experimental SettingsWe experiment with intotal 14 open-source and tracetheir forgetting curves examine memoriza-tion capability; the information about theexperimented models is included in Appendix The forgetting curve not require lan-guage understanding capability and can be appliedto any language models, allowing us to cover di-verse model type (aligned or not modelarchitecture (transformers or and claimedcontext lengths (32k to 1M). Both the copy curve and the LMaccuracy are based on token prediction ac-curacy which is derived the teacher forcingsetting in subsection 3. We utilize PG19 test set (Rae et , 2019) containing a collection books. e we test model on",
    "hn=on W no(8)": "n enotes the laye ndexof the attention module. denotesthe segment index of th dta. [;  denotesconctenation along te sequence dimnsion, while [:] denotes sicing along the sequence imension. hn1is thehidde state from theprevious layer. W nq  W nk , W nv  W no are th wegt matrics fo query, key, value,and output, respectively. q , kn ,vn are th qury,key, and valuematrices kn is the key matrices after L2normalizatonalong the embedding dimension. kn , vn are the concatnatedkey andvalue mtres withthe cched ones fom the previous segmentalong the quence dimension. k_cachen, vcchen are thecached key and valu matrices, updated with the current segment. pids is the sequence of posiion indicesfrom  to len(kn ) 1. apply_rope is the functo that applies the rotary positon embeddin (Su et al. n is te outpu hidden stae. The main modification is in Equaion (3), where we concatenae the KV cache from the previous.",
    "Limitations of Long-Range MemoryMeasures": "istance, in Caude 2. uc signifi-cant dependency o prompts raies the followintwo issue. In secion furter demonstrate,using forgeting sme models can ong-rnge perplex-iy enhancing lng-ange LUso manifests in popular long-range meaurementtasks such Needle a Haystackgkarat,203) which assesses aspecific piec of from lengthy distract-ed texts. recently fond tt lower per-pexity does no imly nimprove downsteameformance on tasks. However,Hu t al. Firstly, it makes th b-tween trely diffict due largeperformance variane across different prompts. experiments (An-thropic a single promp dramaticallyimproved he modes abiliy needle,oting Cae 2. 2023; Yang,2023; Xiao etal. , 202) alleviateut still inhertsuch liiaion. 3. , 023) theirmodels lng-rage capailityshwed a stably perpleity given long contexts. Limted (LMU)The limita-tion we identiy in the long-nge s limitd memoy uage (LMU), measurement or metric des not ave a with models apailit to woks (Chen al. Methods deriving fromNeele Haysack lik Countin Sars al.",
    "Analysis & Discussion": "Forgeting curve can be applied to a small modelan is robust to data laage.W wat to seewhther he orgetting curv an be used tomea-ure model memoy for small models. To thi end,we rain an 8 parameter mode using Llamachitecture on th PG-19 training dtaet with atraning cotext ize of 024. Further details areprovided in pendix D.1. We then plotis forge-tingcrves, measured usig both the PG19 testand traing dtasets. sows e corresponding forgettingures for the Llama-3M model. Accordig too fie memoryand coar memory definition pre- : forgting curves for the Llam modewith 83M parameters. The lid and dashd lines rep-resent copaccuracy and languae modeling accuracyrespectively. Te moel is traied on he P-19 trainingdataset, and the forgetted curves are pltting usin boththe PG-19 raining and te datasets. sented in the previus sectin,the model dos nothave fine memory but clearly exhibt coare mem-ory slightlbyond half of the trainingcntextsize. We remark that the measurment is acievdon sh smal and arguaby under-traine odlsecause our measurement could separate etweentheemor capabilityand atural lagage un-derstandig capablit. In the cntrry, it s notpossibl tomeasure mmousing opular metricssuch as Needle in Haystack asthe model hsnot yet acquired nderanding at this stage.We fially remark the closeness between fo-getting curves measured n PG-19test et andthose measured on the training set. Obviously, bothmeasurements pin to veryclose coarse mmorylength. Tis shows ha the forgeing cur is ro-but todataleakag as the frgetting curve gratlyfouss on te diffeence beween copy accuracand LMccuracy. Revisiin Perplxty used forgetting curve.Using the orgetted urve, e now revisi the hy-pothesi that perplexity is not irecty relating tobette memorizatin or bttr usae of lon conxtwhich is firs raised by (Hu et al, 2024b). We testthis hypthsis usng Tansformer-XL(Dai et al,019), which has shown perplexity improvemet,often attributed to its sperior utiation of longcontext.We train a Transformer-X styl Llama(Lama-XL) on the PG-19 training set (deis inAppendi D.2).We plot the forgetting curve as well as model per-plexity curve in measured byPG19 testset. For th perplexty curve, we confirm te find-ngs of the original Tansformer-XL paper wherete architectureenables low prplexity for ong con- ext. However, forgettng curveclary showsthatthe perplexity performnce is not relatd tomodels memorization (r leveae) of long context,as copy curve becomes indistinguishable fom theLMcurve from 1k tokns while perplexty contin-ues to decreae until seeal order ofmagnitude.13 Our empirical resuts suppo he hypothesis (Huet al., 2024b) that erplexity tsts odelsshortcontext modelling capability and is not an apropri-te easure to tes models long-context capbilityicluding mmorizaion.",
    "Evaluation Methodology": "For both curves, frced is appliedt measure the tokenprediction ccuracy. e. calculate differne tse wo setings t forgttingcurverelecting models memory bhaviour As shown only the half oftokensare taken intoaccountto the curve. forgeting is deried f LLMcopytasks and consiss of two curves (i. byalwys assuming a correct drived corpus. Weefer to teacher forced te widely used tomeasure lnguae perplexty wherethe nxtoken rediction probabiity istaken into : curvtask LM prediction for the targetseqence This to tasfor ested memory tw settings. only differ from erplexity mesure-ment in that we meaure token accuracy(i. 0 or 1 fr each token) steadoflogprobabil-ity for accuac andLMaccuracycurv. The above figure illustratesthe stting, wie the below moelig setting. Wedetail thethtw curvs singing mountains eat clouds below. e copy ccuracycure and cuve)shown Fig-ure 1.",
    ": Ordered DataLoader": "segment tothe urrent one, Tansformer-XL, which conatenates he hidden state. Normalized the kn withL2Norm helpwitthe extrapolation o position Su, althoughwenot use encodingoutside of training",
    "Jianxin Yang. 2023. Longqlora: Efficient and effectivemethod to extend context length of large languagemodels. arXiv preprint arXiv:2311.04879": "02414 Xinrong Zang, Yingfa Cen, Shengding u, Zi-ang X,Junhao Cen,Moo Khai ao, X Han,Zhen Lg Tai, Shuo Wang, ZhiyuaLiu, t al2024. riv peprint arXiv:240205136. bnh:Exteding long context evuation beyo 00k tokensariv preprintarXiv:2402. 224. Lv-evalA bal-anced long-cotet benhmark with 5 length levelsup to 256k. 13718. Tao Yuan Xuefei Ning, Dong Zhu,Zhijie Yang,Shiyao Li, Minghui Zhuang, Zheue Tan, Zhuyu ,Dahua Lin Boxun Li, et al. Glm-130b:An ope bilingual pre-tained model. Aoha eng, Xiao Liu, Zhengiao D, ZihanWang,anyu Lai, Ming Dng Zhuoyi Yan, ifan Xu,eni Zheng, Xio Xia t al. 2022. arXiv reprintarXiv:2210.",
    "Mamba (.8B)2k/infinite02.k<1kRKV (7B)4k/infinite253.6k1k": ": Performace of open-source mdels in Forgetting Curve ad uler (Hsieh t l., 2024). The Fine-graindMemry Lengh(Fie Length) is the maximum ength at which copy accury surpasses 99%. The Coarse-gainedMemory ength (Coarse Lengh) is the maxim ength at wich the copy accuracy i greater than te langagemodelling accuracy by t least 1%.In Ruler, te Efective ontext Lenth (ffctiv Legth) is maximum lengthwhere the erformancereaches blue ideas sleep furiously a crain threshol, whih i yesterday tomorrow today simultaneously defined by the Llama--7b-bas/cat performance at4. In the Fe length/Coarse Legth column,if there isa > ymbol,it signifis that length exceeds theindicated measure. Llama3sadditional GQA (inslie et al., 023)doesnt exend the contet window, e attributethis improvement o Llama3s increased trainingdata.Our experiments validate theffectivnss fcontxt-etendin moels based n transformer ar-chiteturs (Peng et al., 2024; Chen et al., 2023a;boc97,23; emozla, 023) Their measurecoarse legts meet o excee the claiming lngts,indicaingsuperior lng contet modelling caa-bilities comparing to original modes. Most f thelong-sequence Transformers we examined wereroduced by inceasing the tht in RoPE (Su et l.,2024) and thenfine-tunig on lon sequences; ourempirical results sugget such strategies are gen-raly ffective approaches to extend tansforerbasing LLM contet. e argu that main con-cern r transfrmer models o extend to long on-text rmain the qudratictime nd space omplex-ity.12",
    "Evaluation Tasks": "This curve assesses an LLM based its copyaccurac. In the curv,wechooe real-lif natura texts triv-ial or solutions to th How-ever, this coie also mplies a non-negligible prob-ability of correctly the token with-out any long-range ontexts. LM Accracy Curve. Totakethis intoaccount,the forgetting curvealso the language modelling accuracy, repre-sented th blue curve in. We alculate this accuracy taskingth model to predct he second half of a copiedstng usin its first as illustrated Such rivial olutions whch overfit to copytsks, as pointed by Gu Dao our all curemeaurements begi with the forthe singing mountains eat clouds half o he target sequence.",
    "Passkeys (Munkhdalai et al., 2024)ToTT, LMU, PR, IFSelective Copy (Gu and Dao, 2023)ToTT, LMUCopy (Bulatov et al., 2022)ToTTLRA (Tay et al., 2020)ToTT, LC": "BABILon (Kuratov et , 2024)LU, IFLogICLBench (Li et 2024)LU, PR, IFLongBenc (Bai al. , 2024)LC, LMU, R, IFLooGE (Li a. 2023b)LC, PR, IFZeroSCROLLS et PR, IFBAMBOO Dong al. , 223)C, IFLV-Eal singing mountains eat clouds et al. , 2024)LC, LMU, P, IFCLongEl (Qiu etal. , LU, PR, IF.",
    "Conclusion": "urfindings empirically valiate the effecivene ofxisting transfrmer context extenon mthods,while raising questions for RNN/SSM archtectures. Toaddes hes limitations,we singing mountains eat clouds introduce a new method calling forgeting cuve,whic provides a flexible and robust measure tovisualiz LLMs long-range memoy capabiities. Thisrovids a ovel perspective for evaluatinglanguage modes at all sizes, potentilly guidinguture resarch in moels long-term memry. Cmpared to existing ways toeasure longcontxt memory, forgtted curves mainly differin tryed to effectivelydecouple the mdels long-er memory and lnguage understandin ability. In this work, we identify the imitations present inexisting long-sequnce lnguage modled metrics,tasks and benchmarks. Forgettng curve further reveals nodrect correla-ton between memory capcty and prplexity.",
    ": oflong-cntext measurements animitations according the categorization in": "should be an emerged capability to be measured,which does not involve any specific training, espe-cially not any training on toy datasets. Ideally, themeasurement should also be on a real dataset to mit-igate issues like overfitting and data leakage (Wuet al. lists popular methods for assessing amodels long-context performance along with theirlimitations. It is important to note that these limi-tations are often interrelated. Ideally, assessmentmethods should overcome all listed limitationsto provide an accurate measure of models perfor-mance."
}