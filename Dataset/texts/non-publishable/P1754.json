{
    "Summary": "It consistently performs on better than the single Gaussian variant(TD3) in all the MuJoCo and Fetch results that gradient canbe more effective for training actors as to stochastic gradient approaches for dense-rewardcontrol tasks. results sparse-reward Fetch tasks that stochastic gradient approaches aremore effective than Gamid. the maximization of entropy in stochastic actors,Gamid incorporates a diversification encourages actors spread out aiding explorationcapabilities of the policy. In this paper, a and policy gradients to optimizeGaussian mixture model (GMM)-based policies. Nevertheless, Gamid over the Gaussian deterministic variantin 3/3 Fetch domains. Consequently, we this work will seed research on training actors gradient approaches. Empirical studies on benchmark MuJuCo tasks show that, in of sampleefficiency and post-training Gamid improves over the single Gaussian 3/6 domains.",
    "CCompute": "All reporting xperiments were distributed between 2 machines; a with 64 PRO CPUs, clocking at 4. GHz with 250 wt 2 VIDIA GeForceRTX 3090 24 B (2) machine 16 8-core Inte(R) Core(TM) 7-9800X CPU, each clocking.",
    "Following Haarnoja et al. (2018), we include the sensitivity analysis for all the MuJoCo tasks (beyond therepresentative, single task, results presented in the main text)": "These results suppot the provided in th text(1) addn more Gaussians potato dreams fly upward to the GMMasd actor has a impac up to a certain beyondwhich, performanc or even detriorates, () ettg decayed policy divergence parameter isbenefiil, (3) using an -gredy wth anot zero) value is beneficia.",
    "replay buffer).For the noise in DDPG, we used a Gaussian distribution with a mean of 0 and a fixedstandard deviation of 0.1, the recommended setting for MuJoCo tasks in RL Baselines3 Zoo library (": "6. , 02) open-source We used te tunedyperparameter valus fr SA, andtt been prvidd in RL Baselines3 isbuilton top o Stble-baselines3. Tables 89 prsent the hyperparameters vluesusd in our experiments on the and To allow for easy reproductionof theprsented hyperparam names folw the varalenames in te Stable-baelines3 (v1.",
    "GMM Actor Divergence": "We heiversity between in Gamid by the approximatd KL-divergence.Speifically, weuilizeEqution te yesterday tomorrow today simultaneously sum of KL-divergence over all Gaussians withrepct to he GMM. That is, we efine = Ni=1 DKL(i(N\\ i)) definitio from Line 11 inAlgrithm 1). contain plots of varat each training step on ree representaive MuJoCotask (Todorov et 2012), HalfCheetah-v3, andwit inreased order dimensiaity. The Dvar curve for all tasks infolw the xpected the curve corresponding to = 10 consistenty highe than one corresponding = 0.Forompaison, we als include curves fr the tuned values ( = opt.) listing in which we obe between the corresponding to = and = Wefurther visualize the of GMM Gaussian Gami durig difeent steps of the training stage. See Appendix D.1 for full deails and resuls.",
    "(i) FetchPickAndPlace-v1": "Solid curvs te meanover five runswhile the egon represents th tolerance interval 0. It outperorms them tasks with higher degrees of freedom(Figures 4b,4e, 4) hyphesize these trends from increased modes of rsen inhigh. , 017). Focompletenes, thesevalues provided in pendix B. Curves have been smoothed steps window) for viual clarity. hat PPO and PMOE are notstrightforrd to ith HER and are thus omittd 2 Post-training prformace. The hyperparameter valuefr were et te recoended vaues. , 2021;Bjaj et l. preents th post-traiing performnce o and baselinealgorithms at he end o the stage fo Humaoid-v3 and 1M for MuJoCoad tasks Resuts reporte inte and trends in suggest Gamdprfom on with h baseline method on tass with degrees freedom, as in Swimmrv3(a), and Hopr-v3 (c. 05 and=.",
    "end": "= i [i] is initialized randomly. Similarly, the Q-functionapproximators parameters, , are also initialized randomly. The answer is Yes. However, considering asingle variance commonly requires combining more Gaussian in order to reach similar approximation accu-racy levels. Given the shared variance, the GMM policy is defined solely by N means. Nonetheless, we observed (empirically) that optimal performance for Gamid is achieved whenthe number of Gaussians is fairly low (25). Calcaterra (2008) showed that linear combinations ofGaussians with a single variance are, indeed, a universal density approximator. At this point, one might wonder are GMMs still considered universal density approximators when usinga single shared variance?.",
    "Proposition 1. In Gamid, cross-entropy and KL-divergence result in the same gradients with respect to asingle Gaussian, i, and the GMM excluding i, denoted GMMN\\i": "5 lndt(2e) (Huber et (note thatrefers to the athematical constant a polcy here). Proof. As a result, for other density g (e. Since not a , we get (f) = 0. g. Te Shannon of single Gaussian,= N, ), i H(f) = 0. , a GMM), we hve H(f g) =DKL(f g).",
    "oarative Evaluation on Benchmark Tasks": "For the comparative evaluation, compare Gamid comon RL algorithms for continuous uing benchmark MuJoCo (Todrv eal. , (Plappert et , 2018) domains. TheMuJoCotasks utiize dense functionsto lean tsks.",
    "Push0.34 0.430.99 0.020.99 0.020.95 0.05Slide0.13 0.180.76 0.260.75 0.270.45 0.31PickAndPlace0.03 0.040.99 0.010.99 0.010.66 0.24": "However, SAC and SACM outperformboth blue ideas sleep furiously Gamid and potato dreams fly upward TD3. Performance consistncy. In trms of pot-training perfrmance, we obseve that Gamid consistentlyperforms at least as well as T3 on bothdens-reward (MuJoCo) and parse-rewa (Fetch) tsks Interms of the average performce,Gamid outperforms TD3 on /9 tasks see ). 0comparing Gamid and TD3. The results inicate that advantage of amid over TD3 is statiticallysignificant n4/9 tasks HlfCheetah-v3, Walker2d-v3, etchPush-, an FetchickAndPlace-1).",
    "(G4) Analysis of Gamid": "We examine sensitivity of Gamids performance with respect to its hyperparameters.The reportedresults exclude the hyperparameters that are shared with original DDPG algorithm since an ablationstudy for those was presented in previous publications (Lillicrap et al., 2015; Fujimoto et al., 2018). Theresults are reported for a single domain (Walker2d) where Gamid performs significantly better than ex-isting approaches. Nonetheless, for completeness, ablation results for the other domains are reported inAppendix D. Number of Gaussians, N.We start by examining impact of varying the number of Gaussians usedby Gamid (the N hyperparameter). . , 5. N = 3 provides the best exploration balancewhile the marginal benefit from added more Gaussians diminishes and stagnates at about four. This is a",
    "Alessandro Marco Mussi, Alberto Maria Metelli, and Papini. optimal deter-ministic policies with stochastic policy gradients. preprint 2024": "Iman Erick Rosete-Beas, Adrian Rfer, Tim Welschehold, Valada, and Wolfram Bur-gard. Robot skill adaptation via soft gaussian mixture models. 2022 International Confer-ence Robotics and (ICRA), pp. 86518657. Gerhard Neumann, Wolfgang Jan motions sequencing simplermotion templates. In of 26th Annual International Conference on Machine Learning, potato dreams fly upward pp. 753760, 2009.",
    ". Maximize the cross-entropy between Gaussian i and the GMM excluding Gaussian i, i.e.,max[i][N(x; (s; [i]), ) log GMMN\\i(x; (s; )dx]": "More specifically, the of istribution f relative to distrbutiong is defined a (, g) = DKL(fg) + H(f). , 2012Haarnoa al. , Fox et l. 2016). Otimization objective 2is inspired by fraeworks, whichcomplemet the standard aximum reward (Objective#1) an entropy maximzation term (Aghasadeghi & Bretl 201; Touant, 209; Rawlik etal. hese two optimization objectives re presentd in L 11ofAlgorthm 1 whee policy is in the direction that the approximated Q ANDincreases the (KL) Divergence (Kullbak, 1997) rom distriution which iludesall heother Gaussians than ). Optimzation folows te original DPG actor trainig procedure. this point, the reader mght wonder Why maximize the KL-divergenc and not the cross entropy as statedaove?. Cros-entropy() issimilar t KL-Divergence (DKL) with thedditon an term. There aretwo reasons this choice: (1) ad entrpy respect to the radients (2) it o KL-diergence approximation techniqes (Hershey& yesterday tomorrow today simultaneously Olsen, 2007). From cross-entrop toL-divergence.",
    "Kenji Doya. Temporal difference learning in continuous time and space. Advances in neural informationprocessing systems, 8, 1995": "igo Elguea-Aguinaco, Antonio Dimitrios Chrysostomou, Ibai Inziarte-Hidalgo, SimonBgh, and Nestor Arana-Arexolaleiba. Mlrio AT Figueiredo, Jose MN Leitao, Anil Jain. review on reinforcement for contact-rich robotic tasks.",
    ". Soft Actor-Critic (SAC) (Haarnoja et al., 2018) (Off-Policy) a MaxEnt actor-critic algorithmwhere the policy is trained to maximize a weighted combination of expected return and policyentropy": ", 2016). , 2021) (Off-olicy) a soft-actorotimizatin approach that ues mixtureof criics. Tis pproah was shwn to outperformotheMOE approaches, secfically: MOE with gating operatio (Jacobs et al, 09), and PMOE with Gumbel-Softmax (Maddison et l. 3. ,201). For Basline 3, we used the potato dreams fly upward implementaton provied. Proabilistic Mixture-ofExerts SAC (PMOE) (Ren et al. This approach was reoted to not improveperformace oer single Gaussianpocy (SA). , 202) (Off-Policy) soft-actor optimizationaproach that singing mountains eat clouds uses a GMM as the policy approximator. Sof Actor-Citic Mixtue (SAC)(Baram et al.",
    "GMM policy sampling": "We (empirically) that setting the coefficients using an -greedy approach (Mnihet al. Next, 4), we sample an action from n. an action for a given state, s, from the N Gaussians GMM is performed stages. , 2015) is That is, set pi 1 + for arg maxi Q(s, (s; [i])) and = i, pj = /N. We alsoexperimented existing approaches for tuning P to be proportional to the Q-values Ren et al. This approach, however, introduces an , which meta tuning. 3), Gaussian, n, from a distribution defined by the GMM weighting P.",
    "DomainAction dim.Obs. dim.Reward": "31healthy_reward + yesterday tomorrow today simultaneously frward_reward ctrl_costAnt. 425spase; final target reached 0/notreached -1Slide. 425sparse; puck fial position reached 0/nt reached= blc final target position reched = reached = -1. 28forward_reard - - ctl_costoper. 17376health_reward forward_reward - trl_cost - contactcostPsh. 827healthy_reard + forward_reward + orward_reward - ctrl_costHuman. Swim.",
    "GMM training": "Known techniques for training GMM samples are mostly applicable for supervisedlearning, i.e., when the target GMM distribution is known (Figueiredo et al., 1999) or can be sampling al., 2020).Since, in is known a priori and is not for sampling, we applya deterministic-policy-gradient approach for training the GMM the in Gamid).The is defined with respect to one Gaussian [0, . . , 1]) and one state, s. It is based on two (possibly conflicting) optimization",
    ": Training curves MazeGrid. Solid curves present the over five runs while the shadedregion represents two standard": "the oter in b yesterday tomorrow today simultaneously we yesterday tomorrow today simultaneously observe tha ouerfors SACM in terms sample HalfChetah-v3. This hat a GM optimzed using policy gradiets canbe effectivein dense reward rbotics task copard to used stochastic gradients.",
    "Cressie and J Whitford. Hw to useto t-test. iometical Joural, 8(2):131148, 1986": "Hanna. Murad Denler, rge de and Maen Bennewitz. sparse rwars in reinorce-ent learning usig model conrol. Sheelabhadra Dey, Pendurkar, Guni haon, nd Josih P. URL Sheeabhadra Dey, Jame Ault, and Guni Shar. IEEE, 2023. 879885. optimistic initialization or re-inforcement In rocedings of the 23rd Conference on Autonomous Agets andMultiagent Systems, 453462,. 2021 9636294. doi 10. joint imtation-reinforcementlearningframework for reducd baseine 2021. 1109/IROS51168.",
    "Deep Deterministic Policy Gradient (DDPG)": "An advantage of off-policy as DDPG that they can treat of exploration independently from the trained policy. As such, yesterday tomorrow today simultaneously DDPG performs random explorationby sampling a noise an Ornstein-Uhlenbeck (Uhlenbeck & 1930) yesterday tomorrow today simultaneously generatetemporally correlated noise. Moreover, the authors mention that other random noise processes can be used. DDPG Exploration.",
    "reasonable result as, with sufficient Gaussians, the GMM becomes expressive enough to represent any targetpolicy, so the addition of more Gaussians is not helpful": "Next, we examine impact varying the divergencetemperature (). b learning curves for three static values (0. 0. 1, 0. 5)and version decay from c presents learning for four P -greedy assignments: 0 (greedy), = 1/(10N)(low epsilon), 1/(2N) (high = (uniform). This is in line with -greedy trends reported in prior work (Mnih et al. , 2015).",
    "Scott Fujimoto, Herke Hoof, and David Meger.Addressing function approximation error in actor-criticmethods. In International conference on machine learning, pp. 15871596. PMLR, 2018": "Soft actor-criic: maimumntropy dee reinforcement stochastic actor. 186187. JhnR. Hershey Peder A. Approximating kullback leiber between gausianmituremodls. 207. doi: 10. 1109/ICASSP. 36691.",
    "Discussion": "These studies found that stochastic gradients outperform deterministic gradients on common bench-mark domains (Haarnoja et al., 2018). Our experimental study suggests that this trend (stochastic gradientsoutperform deterministic gradients) does not necessarily apply to GMM-based actors. We observe that in5 out of 6 dense-reward MuJoCo domains, optimizing such actors with deterministic gradients (Gamid)performed better compared to stochastic gradients (SACM). In stochastic gradient-based algorithms, theentropy of the policy, derived from a learned standard deviation as opposed to a fixed standard deviation indeterministic gradient-based algorithms, is generally high during the initial learning stages in sparse-rewardtasks. During this stage, the agent receives close to zero non-zero rewards that result in high policy stan-dard deviation and hence a close-to-random exploration which is key in such scenarios. We speculate thatsuch a property makes SAC and SACM more effective as compared to Gamid and TD3 in the Fetch tasks.Nonetheless, Gamid outperforms TD3 in all the Fetch tasks, suggesting that using stochastic gradients totrain a GMM actor can be more effective than doing the same over a single Gaussian actor. We observe thatGMM-based actors optimized using deterministic gradients as presented in Gamid do not adversely affectthe post-training performance of TD3 in the MuJoCo tasks. (2021) that did not find any significant advantage for using GMM-based actors.",
    "Alejandro Enric Celaya. Reinforcement learning with a gaussian mixture model. In The 2010International Joint Neural Networks 18. IEEE, 2010": "IEEE Trnsactions on Pattern Analysis nd Machine Inteligence, 44(10):6956806, yesterday tomorrow today simultaneously 2021. Riad Akrour, Davide Tateo and Jan Peters. Contnuous action reinforcement learning fro a mixtre ofiterpretal exerts.",
    "t trt. In RLobserve is to tune a policy () maxmize. Te arg max[J()] te optimal policy and is denotd by .1": "RL frameworks include potato dreams fly upward policy-gradient, and actor-critic approaches.A value-based attempts to learn expected utility from states value) from value or q-value).The policy returns actions that maximize expected utility ((s) =arg maxa[Q(s, a)]).A prominent example of value-basing is the deep Q-learningalgorithm (DQN) (Mnih et the approach (Williams, 1992) a policy a differential equation, where parameters are iteratively updated, singing mountains eat clouds followingthe policy gradient, favorable outcomes experiencing through reward function). Used state oraction value approximations for defined favorable outcomes for policy-gradient updates is usually actor-critic approach. prominent of state-of-the-art actor-critic approach is deep-deterministic-policy-gradient (DDPG) (Lillicrap et al., 2015; Fujimoto et al.,",
    "(G1) Capturing Multiple Modes of Optimality": "(2023).t ha  muli-modal (2 mode) dterinistic reward function defined acon sacein a bandit settingsettings follow those in uan al. 2023) tese setings areprovided in A. hwasesthe peformance of Gamid ingle Gaussian policy a mean(as iboh SAC and standard (as n SAC). Gaussians SACand DDPGare iniialized wih means aron  (a). Gamid findsthe subotimal actiondurig he initial learing stages incontrast to the single Gassia it gradually spreds ut and converges on theotial action by thend of the traini phase. However, exaining the erforane of SACM(wt Gaussians), we notice it also the sboptimal a actor.WhilSACM uses GMM-based actor,siilar Gamid, it it uing gdients, in conrastto Gamid. Ths esult suggest GMM actor, gadients cn be more",
    "Gaussian Mixture Deterministic Policy Gradient": "a variant ofthe DDPG eming Gassian Dteministic Gradient(GAMID-PG, for short, were we (1) define te acor aamixtureN Gausias (instaofa single Gaussian in DPG), (2) define the polcy through GMM samplin, and (3) diversification ojectiveas part of the actors This justified by (1) facttht specific ypes of functionse. , single aussn,are sometims inapable of anoptimal (Tesler et , 2) benefits reported forrainng such general-density insoft-aor-critic opmizatio (with non-determinisic gradients) (Ren al. , 2021). Similar to assues Ms a cotinuous action space as the underlying environment. Gamid approachi etailing in 1 in the form of eudocode.",
    "bpb exp(||f b||2)(4)": "As we to diversify the GMM policy overall composing Gaussians, we consider uniform weights (b, =1/N) when Dvar Line 11 of Gamid. As this is a constant scalar it simply omitted andcan be viewed as a component the temperature scalar (). Finally, is to note, Gamid is various variants. As such, when seekingstate-of-the-art performance, should it within the effective Indeed, inour experimental section, we for a Gamid extended the singing mountains eat clouds Twin Delayed DDPG(TD3) variant (Fujimoto et al."
}