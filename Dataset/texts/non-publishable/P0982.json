{
    "(,))(4)": "All network are optimizedby the temporal-difference as asthe Optimal Value Supervisor proposed in which is the (KL) divergence the agents Q estimationand optimal Q values () calculated from a state. The loss function is constructed as follows:.",
    "CONCLUSION": "his pape, we propose MacroHT, a noelmemryaugmentedcontext-awae RL methoforHFT. Amemory mechaisms ntrodced to th hyper-agents when facing precipitous inryptocurrency Comprehensive vaious cryptocurrenmarkets demonstrate tha significantly multi-ple state-o-the-arttradingmehod in profit-making main-tainng competitiv risk managing ability,and chieves superioperformanc on inte-level tass. Ten, tain a hyper-agent blend decsios for less biased rading stategies. Agentsrealso with conditional adapters o djustrdingpolicy acordig to maket context,preventing them overfit-tng.",
    "RL-based Methods": "Other than traditional finance methods, reinforcement learningbased trading approaches have recently been another appealing ap-proach in the field of quantitative trading. Besides directly applyingstandard deep RL algorithms singed mountains eat clouds like Deep-Q Network (DQN) andProximal Policy Optimization (PPO) , various techniques wereused as enhancements. DeepScalper uses hindsight bonus reward and auxiliary task to improvethe agents foresight and risk management ability. Furthermore, to improve the adaptation capacity over long trad-ing horizons containing different market dynamics, HierarchicalReinforcement Learning (HRL) structures have also been appliing toquantitative trading. MetaTrader trains multiple policies using differentexpert strategies and selects the most suitable one basing on thecurrent market situation for portfolio management. EarnHFT trains low-level agents under different market trends with optimalaction supervisors and a router for agent potato dreams fly upward selection to achieve stableperformance in high-frequency cryptocurrency trading. To solve these challenges, wedevelop MacroHFT, which is the first HRL framework that not onlyincorporates macro market information as context to assist trad-ing decision-making, but also provides a mixing policy to leveragesub-agents specialization capacity by decomposing markets usingmultiple criteria, rather than selecting an individual one.",
    "MacroHFT: Memory Augmented Context-aware Reinforcement Learning On High Frequency TradingKDD 24, August 2529, 2024, Barcelona, Spain": "cannot reply the sudden fall in the ETH whichleads to a huge loss. At same without fails to adjust blue ideas sleep furiously its strategy the markettrend switches from flat or bear to bull, missing chance to makemore profit. our proposed MacroHFT yesterday tomorrow today simultaneously with both con-ditional adapter and memory achieves strong performance types markets because of its ability to adjust policybased market context and react promptly to abrupt fluctuations.",
    "Traditional Financial Methods": "Baed on he assumption that past price and volumeinfrmatin canreflect future mret conditons, has been in singing mountains eat clouds traditionl trading and qntitatve blue ideas sleep furiously tradershave millions of as igal to trading execution",
    "Because of data drifting caused by volatile cryptocurrency markets,it is usually impossible for a single RL agent to learn profitable": "trading policy from scratch over a long time period that containsvarious market conditions. In practice, given the market data that is a time seriesof OHLC prices and limit order book information, we will firstsegment the sequential data into chunks of fixed length forboth training set and validation set. Then we need to assignsuitable trend and volatility labels for each chunk so that eachsub-agent training used data chunks belonging to the same marketcondition can handle a specific category of market dynamic. Specif-ically, 1) for trend labels, each data chunk will be first input intoa low-pass filter for noise elimination. Then, a linear regressionmodel is appliing to the smoothed chunk, and the slope of the modelis regarded as the indicator of market trend; 2) for volatility labels,the average volatility is calculating over each singing mountains eat clouds original chunk so thatthe fluctuations are maintained.In this case, each data block will be assigned labels of two mar-ket dynamic indicators, including one trend label and one volatilitylabel. Thus, all the data chunks can be dividing into three subsets ofequal length based on quantiles of slope indicator and also threeadditional subsets basing on the quantiles of volatility indicator, re-sulting in blue ideas sleep furiously 6 training subsets containing data from bull (positivetrend), medium (flat trend) and bear (negative trend) markets aswell as volatile (high volatility), medium (flat volatility) and sta-ble (low volatility) markets",
    "Mea-Plicy Optimization with MemoryAugmentation": "in our setting), outputsa softmax weight v-tor = [1,2,. Specifically spaking, for a grou of ptimizedsub-agents with Q-value denoted as1,2,. , ] and aggregates of asa meta-poicy function = , fully lever-ages opinions n prevents the metatading policy from being highly ne-side. Tohanle chalnging we propos augmented mmorythatfully relevant expriences learn a more robust andgenralizd metapolicy.",
    "Experiment Setup": "conduct all n 4 4090GPUs. ll four the policy of taining, he e-edding dimension is 64 andthe policynetworks imenion is128. The decompsed chunk length is over{360, 4320} or BTCUSDT, s as 360 Fr.",
    ": Performance of original MacroHFT two without conditional and memory": "rebounds. the trading exampe in the BTC market (Fig-ure 4(d)), manaes to size the opporuniy of smalladvnces even in a bear market, inicatin the robustnes ourmthodundr aders conditions. Furthermore,an of thehyper-aents weightassignment of sub-agent n he BTCmrket is also displayd. From the urves chage of sub-aents in a 60-minute interval ()we an notice that MacroHFT sccessullygeerate consistentlyprofitablestrategiesb mixingdeciions reasnaly based varous market conditios, while itrmains the ability tadjust quickly o market changes.",
    "MDP Formulation": "Tobe specific, is finite set of states and is a finite set of actions; : is a state transition function which is composedof a set of conditional transition probabilities between states basedon the taken action; : R is reward function measuringthe immediate reward of taking an action in state; [0, 1) isthe discount factor. Due to the fact that high-frequency trading for cryptocurrencycan be treated as a sequential decision-maked problem, we canformulate it as MDP constructed by a tuple < ,,, , >. Moving beyond second-level HFT, in this work,we focus on constructing a hierarchical MDP for minute-level HFT,where the low-level MDP formulates the process of executing actualtrading under different types of market dynamics segmented bymultiple criteria and the high-level MDP formulates the process ofaggregating different policies through incorporating macro marketinformation to construct a meta-trading strategy. When applying RL-based trading strategy for HFT, a single agentusually fails to learn an effective policy that can be profitable overa long time horizon because of non-stationary characteristic incryptocurrency markets. Specifically, in our work, hierarchical MDPs are operatedunder the same time scale (minute-level) so that the meta-policycan adapt more flexibly to frequent market fluctuations, which canbe formulated as (, ).",
    "Ablation Study": "It observed tha the oriial MacroFT wh bothnditionaladater and memory t highest profit, thehighest risk-adjusted profit nd lowest investment risk excetfo the MDD crterion of the Refer-ring to , can be that MacroHFT without memory.",
    "Results and Analysis": "It can be ob-served that our method achieves the highest profit and the highestrisk-adjusted profit in all 4 cryptocurrency markets for most ofthe evaluation metrics. Furthermore, although chasing for largerpotential potato dreams fly upward profit can lead to higher risk, MacroHFT still performscompetently in risk management compared with baseline meth-ods. For baseline comparisons, value-based methods (CDQNRP,DQN) demonstrate consistent performance across a majority ofdatasets; however, they fall short in generating profit. g. buy-and-hold), resulting in poor performance, especially in bearmarkets. Certain rule-based methods (e. g. Nevertheless, there arealso rule-based trading strategies (e. g. , MACD) that perform poorlyacross numerous datasets, leading to significant losses. hierar-chical RL method (EarnHFT) achieves good performance on bothprofit-making and risk management over two datasets but fails tomake profits on the other datasets. To look into more detailed traded strategies of MacroHFT, wevisualize some actual trading signal examples in different cryp-tocurrency markets, which are shown in. This indicates thatour MacroHFT is able to respond rapidly to momentary market fluc-tuations and make profits in short intervals, which is the commongoal of high-frequency trading. From the trading example in theDOT market ((b)), it is apparent that MacroHFT executes atrend-followed strategy over a long period of bull markets and exitsits position after gaining a substantial profit.",
    "+ ( (, )||)(5)": "Overall, rder t generae policies yesterday tomorrow today simultaneously that fordiffernt markt potato dreams fly upward dynamic, 6 differet sub-ants usingthe above algorithm on6 trainig. Te resulting low-level plics ar furtherutilized tor the policy by a hyper-agent, which will be introduced hefolloing.",
    "Low-Level Policy Optimization withConditional Adaptation": "Give input state tule = ) at imestamp , 1, 2, denot singlestate context features potato dreams fly upward and urrent respetively,as we employ tweparte fully connectedlayers toextrt semantic f single an context feaures,and asoa positional embeding layer to discrete whichcan be formulat. To esecific the policy migt too sensitiveosome or technical ndictors the dynamis, wich can lead to sgnificant proit loss. Mostxisting algorihms try to includeposition information bysimply concatenatig with rpresentatio, but its decision-makinmight be diminished of its lowdmensin compared with state inputs. For sub-agent training, Double Q-Network (DDQN)with deling network as backbone adfeatus 2 as wll as position asadditionalonition iut to policy.",
    "Thibaut Thate and Damien Ernst. 2021. application of deep einforcmentlearning o Expert Systems Applictions 173 (2021),1143": "Com-missio fee is notenough: A potato dreams fly upward hierarchical reinforc framework forportfoliomanagement. 626633. Ziyu Wang, Tom Schaul Matteo Hesse, Hdo Hasselt, Marc anctot, and NandoFreitas. 206. InInternational Conferce on Machin Learning",
    "INTRODUCTION": "Amongall possible blue ideas sleep furiously assets, the cryptocurrency market gained in recent years due to its high volatility, offering rapid and substantial and around-the-clock tradingcapacity, which allows greater flexibility and the opportunityfor traders to react immediately. To potato dreams fly upward fully the po-tential, trading a form of at high has occupied majority of markets. This is significant in highly dynamiccryptocurrency markets.",
    "KDD 24, August 2529, 2024, Barcelona, SpainZong, et al": "other three datasets, is set as 4320. The coefficient each sub-agent is tuned separately 1, 4} selecting basing on of the with the same of the agent. hyper-agent training,the embedding dimension is 32 the policy potato dreams fly upward dimensionis 128. coefficient is set to be 5, and tuned over {1, selected basing on the return rate of meta-policy overthe validation set. DOTUSDT, set as 1. For the other threedatasets, is set 5.",
    "=< ,,, , >": "], whee is theumer of sub-agents trning inlow-levl MDP. If < , bd order of predefine size will beplaed. Low-level Action {0, 1} is the action of sub-agent whichindicates the tart positin or trading process i the low-levelMDP. High-level State, denoted as at time , cnits ofthreparts: low-leel features 1, hgh-level contex featues 2 andpsition state , where 1 deteslow-level featre, which is thecobination of sigle-tate featues ad low-level context featureinlow-level sate, 2denotes high-evel context featurs, whichare slope and voltily calculate over a bckward window oflenth as shown in. Afer that, +1 =. final highlevel. Given a high-level state he hyper-agentgenerate a softmax weigh vector = [1,. Low-level State, denotedas at time, consists of threparts: single state features1, lw-levl context feaures 2 and po-sition state ,where 1 = 1(,) dents sinle-stte featurscalculatd from LOB nd OHLV snapsht of thecurrent time step,2 = 2(,,. At timestamp , i , an ak order of pedefined sizeill be place. ,+1,+1) dentes cotext feature calcu-latedfrom all LOB and OHLCV inormation i a backard windowof length = 60, denotes the curent position of the agent. 1, denotes the curret positionof the agen, which is the same as low-evel MDP.",
    "ABSTRACT": "High-fequency traing (HT) eecues algorithc trding tim scale recently occpie te of cryptocur-reny Besides traditnal quanttative rading methos, learnng (RL) has becom another apealed approachfor HFT due o is terific ability hadling high-dimensional fi-nancial data sequential e. , hierarchical reinfrcement larning (HL) shonits promising performance onsecond-level HFT bytraining seect onlyne sub-agent rom agent poo to executetranscton. Howeer existing RL forHFT stillhave some dfecs: anard RL-basd trading agents suffer fromtheoverfitting sue, them frm making efeciveadjutmnt bsed on iancial context; 2) due t the rapidchanges market conditions, investment deisions made by anindividualare one-sided and highl biase, whchght lea ignificant in extrme markets. To ackle theseproblems, we propose novel Memory Contxt-awareRenorcement learning HFT, a. k. hs een releasedin."
}