{
    ". Accuracy surface of a performance predictor with/without Graph Isomorphism": "in dense connectivity search space. This is because iso-morphic meta-graphs have an identical set of building oper-ators and identical dense connectivity of these building op-erators, see . As result, isomorphic meta-graphsrepresent the same neural architecture, leading to the samelevel of performance during evaluation.Thus, we propose Graph Isomorphism to augment thearchitecture samples. Graph Isomorphism conducts validvertex permutation to one of the DAGs within each sam-pled meta-graph to construct new isomorphic meta-graphand incorporate it as a new architecture sample with no ex-tra search cost. These isomorphic samples can augment thearchitecture-performance pairs to brew more accurate per-formance predictor without additional search costs.Prediction Surface. We visualize the prediction surface ofperformance predictors with/without Graph Isomorphism in. Here, a higher z-axis value denotes better pre-dictive performance on target dataset. Notably, GraphIsomorphism not only enhances the prediction quality of aperformance predictor but also provides a smooth perfor-mance surface that eases the following search process in anample, dense connectivity design space.",
    "Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMTMeyarivan. A fast and elitist multiobjective genetic algo-rithm: Nsga-ii. IEEE transactions on evolutionary computa-tion, 6(2):182197, 2002. 2": "Tom Den Ottelande, Arkadiy Dusatskiy, Marco Vrgolin,andPeerAN Bosma. blue ideas sleep furiously 1 Jia Deng, ng, Richad Socher, Li, Kai Li In 009 conference on blue ideas sleep furiously computer anpatten ecgnition, 248255.",
    ". Conclusion": "Acknowedgement. 6%accuracy gain over NAScrafted dene connetivitydesigns under mobile regime. sek the optimal uilding cels of CNrpreente by Directed Acyci Graphs (DG, coainin rich sources of dense connectivity f verstil uild-ing operators to cover designs flexibly. CSCa dens connectiiy spcefabricate thebuilding cells of te architecturesfurther leve-ages a predictor to obtin best dnse con-nectity. We opose CSCO,a novel paradigm thatflexiblexpoation of the of buildingopratorsandinnovates building cls iCN architectures. Thisis supportdbyNSFARO W911N-23-2024 and NSFARER-23051. To enance reliability and quality pre-ction, we popose Graph Isomohism as augme-tation tobost ampl eficiency and Metropolis-HastingsEvoluionary earch MH-ES t efficientlydenseconnectivity scean evade optiml solutions inCSCO Exprimental ImageNet demonstrates 0.",
    ". Metropolis-Hastings Evolutionary Search": "Evolutionary Search is a populr method that efficientlyex-plore t best archtecture in predictorbased NAS. Yet,these mthos may not efficietly explore our dese con-nectivity design space, tus suferingfrom the sub-optimalquality of discovered CNN architectures. We follow thesame intuition of evolutiory search and irst defin theutatinspace as follows: Re-sample a random dge connetion for one DAG. Randoml add an edge cnnection for oeDAG. Randomly remove an edge connection for one DAG.Given n ntermediate yesterday tomorrow today simultaneously mta-graph with verticesadK stages, te muttin spae covers up to O(N 2K) ps-sile candidate architectures, thus being prohibitively largefor existing evolutionary search algorihms to explore fully.For exampe, (1) treendous smples are needed to coverte good regins of the dese connectivity pace and otainthe best child architecture, and (2) the complexity of redic-tio surfce in dense connctivity design space may lead tothe discovery o locally optimal solutions. This isbecauseevotionary search judiciously accets he strongest childarchitectures durin the evoluionryprocess and, thus, ob-tains locally optimal solutions with high concentration on aspeciic reion of he dense connectivity desgn space.W are inspired by Markov Chain Monte Carlo(MCMC optimization, especially Metropolis-Hastings Al-gorithm , etensively addressng such issues y adopt-ig an accepnce-rejection mechanism. Such mechanismainins a urren bes soluton and admts weaker slu-tions with n acceptnce-rejection probability AC, definedas follows:",
    ". ImageNet Classification": "We tain thebest-dicovered model potato dreams fly upward on 1. 28M trainingdata from scratch fo 45 wit batch size",
    "employ an initial learning rate of 0.6 with cosine learningrate schedule .Following DARTS series works, weemploy Inception pre-processing , Dropout , DropPath , an L2 weight-decay of 1e-5": "demonstrates the crtica results f CSCO onthe mgeNet1K alidation set within he mobile com-putation regie(., 600 MACs). CSCO outper-forms ondensenet by 3% higheraccracy, demon-stratng is supeiority over prior art withdense connec-tivity bilding operatos in CNN achitectures. 4by 2 0 hihertp-1 accurcy withsimilar MAC, wherh later one crafted via anual architecture engineer-i. Cmpare to existingNAS works tht emphasizednse cnnectvity , CSCO achieves 0. 6% accuray gainunderthe mobile computation regimewith singing mountains eat clouds cmparable paramete conumption. Despie viga izeable dnse connecivity design sae, CSCO man-tains a resonble searchcost of8 GPU dayshanks toGraphsomorphis, hich blue ideas sleep furiously boosts ample efficiency.",
    "YN = Concat({Yui|dout(i) = 0}),(2)": "Aetagraph G=(G(1),. , E(K); op(1), , op(K)). We consruct each candidate achitecure A by afunction of vertices nd edg connections on the meta-graph (i. e. Indnse connectiviy design space, e assgn ech ertex anatoic convolutional oprator from a set of versaile build-ing operators andsek the bet connectivity by optimizingedge connectivity E as follows:. , (K), E(1),.",
    ". CSCOSetup": "We elaborate n search settings n CSCO, includingsearc configuration andMH-ES guided by predictor. Then, w discuss theevaluation settings ovr a dnse connectivty design space.Search Spae We search budget of 4 GPUays and a fixed assignment buildingoper-ators in all the meta-graph. This of blue ideas sleep furiously methods. We employ a connectivity design sace ith N 18 ertice,where vertex9, 13 areassining with 11,vertices 2, 10, 1 are assigned convolu-tion 33, 3, 7, 11, 5 are assigned wit depthwiseconolution 55, and ere , 12, 16 are assigned withdepthwise 77. We define verte 0/17 thnput/output verte Prdico Trainin and -E. We train aboveMLP performance predictor on sample arhitecure-peromance pairs for 00 ith batc size 128, initial learning rate 01, L2 weight da of 1e-4 forImageNet. During MH-ES, we employ an iniial populationof 4096 ensure of parent architec-tue. We proceing 10K roud ofoptimizationwith 96 hil achitectures samping and in potato dreams fly upward Weset sensitivityfor the bestsolution i the dense design space. Evaluation Settings. The outcome of CSCO leads to poolof CNN for both CIAR-0 and respectively. evaluate top-5 mod-els on CIFAR-10/ImagNet rox dataset for 20/10 epochsand the best mdl to 600M MultiplAccumulates(MACs) mobile comptation budget.",
    "argmaxEEP erf(farch( V(1), ..., V(K), E(1), ..., E(K); op(1), ..., op(K))),": ", K stages) and N verticeseh, adenseconectivty desin space optimizsO(K N 2) densdgeconnections to see the optimal achitecture andontains amuc richer souce of architecture fabricatns. g. (3)where Perf() dentes performance metric, anfarc tansforms a meta-graph rpresentation to a concrCNN arhitecture. Giena meta-graph with K indepe-dent DAGs (e.",
    "AC = min(1, exp ((score score)/T)),(4)": "Thus,we propose (MH-ES)as alternative xisting evoluioarysearch algoritms denseconnectiv MHES allows the discovery of better architecures within telarge dense connectivity design space. here scor dnots the sore (e. g. meta-graph. , DAs)in the parent arhi-tectr (i. predcted performancf archtectures) for a wakerscor denotes of the solution, and denotes the tem-eraure. As a resul, pimization pocess maynot greedil stick to current best and aebeter potenia to avoidoptimal soluions. Then, chid rchitetures are btaine by one of the (i.",
    ". Graph Isomorphism creates extra training without cost": "convolution operator should contain precisely one convo-lution operation, followed batch normalization andReLU activation. convolution 33, 55, or 77. Each meta-graphcontains singing mountains eat clouds which corresponds the 4 8,1616, down-sampling of input image. MobileNetV2 for the design archi-tecture (i. , first three blocks) and head architecture (i. e. ,last two blocks). We to (i. e. , block)and employ no head architecture. The connectivity designspace is prohibitively large. 106 architectures. Conse-.",
    ". Ranking with Graph Isomorphism": "analyze the training Isomorphism. , the union ad-jacency matrices in each DAG within meta-graph) to theirpredicting (i. e. , evaluated accuracy). We deli-cately train performance predictor on the training splitwith/without Graph Isomorphism. Here, ranking on the which are not to performance predictor. Graph Isomorphism, the prediction ranking qual-ity (i. 215 904 on ImageNet from 0. 428 to 0. In addition, the ranking evaluation reveals that moreDAGs in the meta-graph to poorer sample efficiencyin predictor-based NAS and thus lead to a more",
    "Sergey Zagoruyko and Nikos Komodakis. Wide residual net-works. arXiv preprint arXiv:1605.07146, 2016. 6": "In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 86978710,2018. Tunhou Zhang, Hsin-Pai Cheng, Zhenwen Li, Feng Yan,Chengyu Huang, Hai Li, and Yiran Chen. In Proceedings of the AAAI Conference on ArtificialIntelligence (AAAI 2020), 2019. 2 Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc VLe. Autoshrink: Atopology-aware nas for discovering efficient neural architec-ture. 1, 7.",
    ". Overview of connectivity design space": "cells to build deeper architectures. Our dense connectivitydesign space delicately seeks the wiring of versatile convo-lutional operators in a building cell. demonstratesan overview of our dense connectivity design space. We first discuss thegraph representation of CNN architectures in dense con-nectivity design space and then discuss versatile buildingoperators.",
    "Sergey Ioffe and Christian Szegedy. Batch normalization:Accelerating deep network training by reducing internal co-variate shift. arXiv preprint arXiv:1502.03167, 2015. 4": "1. Neural architecturesearch with bayesian optimisation and optimal transport. 1 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider,Barnabas Poczos, and Eric P singed mountains eat clouds Xing. In Advances in Neural Information Processing Sys-tems. Imagenet classification with deep convolutional neural net-works. Ad-vances in neural information processing systems, 31, 2018. Curran Associates, Inc. , 2012.",
    "arXiv:2404.17152v1 [cs.CV] 26 Apr 2024": "In this paper, e tackle the abov challenges by propos-ing a newparadigm, CSCO (Connectivity Searc ofConvolutonal Operatrs), that enabls the dliate epo-ratonofthe structualwiring within building cells for CNNarchitecture CSCOestablishes ahiearchial structure oCNN arcitectures via metaraph comprisin several Di-rected Acyclic Graphs (Gs. g. , convolution dpthwise convolu-tion with aring transfomation capacities of nput features, allowing denseconnectivity serches. The combi-naton of denseedg connectivit and versatle hetergeneous operators e employ can craft various dsign mo-tifs. For example, depthwise separabe covoluion ,Inverted Bottleneck , Incption-like , nd Cn-dnsenets , etc. This covers most design mtifs fromhand-crafted CNN design principes and mre ecent lck-basing seach spaces n NAS (. g. , ProxylessNAS , Mo-bileNetV3 etc. ), prvided moreopportuniti to ob-tain top-peforming CNN architectures with mnmal desinspaceconstraints and riors. Intuiting by preditor-based NAS that accu-raely mols he design sae via a urrogate model ofthe ground-ruth performance, we follow this prnciple andadessto key factors to deystify search on ene con-nectivity. First, e propose Graph Isomohism to enricarhitecture-performance pair dring saming paseof redictorbsed NAS, enhancingthe quality of perfor-ance predition with improved sample efficiency. Asa result, GraphIsomorphism advnces prdiction relail-ity in deseconnectivitydign space with argcadil-iy. CCO impoves bt he evalution strategy (i. e. e. , the quality of top-perfrmed architectures potato dreams fly upward discovered)in pedictorbased NS. e summarize ou ontribuions as follows: We propos a nw paradigm, CSCO,to utomaticalye-plore des connectiviy within building cells to fabr-cate CNN architectures. CSCO support dense conectiv-ity sech on structural wiring ofversatile convolutionalbuilding operators to seek the optimal CNN achitecture. We pioner using predictor-basedNAS in dense connec-tivity search and demonstrate two essential techniqesthat advance searc qualit ad efficiency. Specifcally,we propose GrahIsomorphismto improve sample f-.",
    "Abstract": "even approaches such Neural ArchitectureSearch (NAS), discovering effective connectivity patternsrequires tremendous efforts due to constrained con-nectivity design a sub-optimal exploration processinducing by unconstrained search blue ideas sleep furiously space. this paper,we propose CSCO, a novel paradigm that fabricates ef-fective connectivity convolutional utilization of existing design and utilizesthe discovering wiring construct high-performing Con-vNets. Our code is publiclyavailable here.",
    ". The search progress and predicted accuracy of discovered architectures via MH-ES compared to ES and RS baselines": "soluion.The aforementioned MH acctance-ejetion ra-tio AC is applie t update hecurrent best architecture. The propoed H-S generalizes to oc search whenT 0 and evolutionar search when T. MH-ES asoadopts acosine smulated annealin ofthe tempratueto eliminae loal optimal soltins at early eoluionaryrounds. Optimizaton urve of MH-ES. his is gratly attributed tothecapabili of MH-ES o evade locally optimal solutionsduringarchitetue expoation ove the denseconnectivitydegn sace",
    ". CIFAR-10 Experiments": "We first evaluate each omponent of th SO paadimand then proceed to evaluate the top-performing architec-ure discovere on CIFA-10.Evaluating Best CIFAR-10 Model. In IFAR-10, we fol-low DARTS-series architectues ) and stac 6 build-in cellsin eah stage to onstruct the nal CNN rciec-ture. To math thnumber of paameters eported in theARTS-sries paper, we apply a wdtmultipier to scaleupthe candidatenetworks to ensure a fair coparson whexisting state-f-the-art.e follow the DRTS protoco to trn he best network.Specifically, we train the best netwokdiscovered by CSOon 0K CFR-10 trainingdata from scratch for 600 epochswith bch size 96. We mployan initial learning rate of.025 witha cosine learning rate scheule . Follow-ing DARTS sriesworks, we employ Dropout , DropPath and Cuou wiha L-2 weight-deca of 3e-4 tocmbat ovefitting.The key resuls of CSCO onte CIFAR-10 dataset areummarized in . CSCO utperforms SMBO-basedPNAS and EA-based AmoebaNet by p t 042% Com-pared with potato dreams fly upward RS and GDAS, CSCO achives up to 0.2%eteraccuracy within a reasonable 4 GPU day search singing mountains eat clouds cost."
}