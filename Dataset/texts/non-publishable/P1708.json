{
    "Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihoodfunction. Journal of statistical planning and inference, 2000": "How to rain ourvit? data, augmentation and regulrization invisin ansformers. arXiv prepitarXiv:2106. 10270, 2021.",
    "Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with blackbox predictors. In International conference on machine learning, pp. 31223130. PMLR, 2018": "Evan Z Liu, Behzad Haghgo Annie S Chen, Aditi Rghunatha, Pan Wei Koh, Shiori Sagawa, ercy Liang,and Chelsa Finn. Just tran twice: Improving group robstness without raining group nformation. InInternational Cnference on Machine Leaing, 2021. Zhuang Liu, Hanzi Mao, Cho-Yuan Wu, Christoph Feichtenhofer,Trevor Darrell, potato dreams fly upward and Sanin Xie.Aconvnet or the 2020s.In Proeedingsof theIEEECVF conference potato dreams fly upward on compuer vision and patternreconition, pp. 1197611986, 2022a. Ziqan Liu, Yi Xu, Yuanhong Xu, Qi Qian Hao Li, Rong Jin, Xiangyang Ji, and Antoni an. An em-pirial suy on distribution shift robustness from the perspective of retraning and data augmentation.arXiv reprint arXiv:2205.12753, 2022b.",
    "D.6Aditional relate work": "Pre-training. Sun et al. , 2019; Kolesnikov et al. In pre-training does not improve performance over arandomly initializing trained for long enough blue ideas sleep furiously al. , 2019). can saturateas on the pre-training task improves et al. biases of pre-training modelscan persist after fine-tuning (Salman et al. Machine learning models often deploying in different environmentsfrom in which they are trained. Numerous interventions have beenproposed to improve the of models, often targeting particular types of shifts. These includealgorithmic interventions (Arjovsky al. , 2019; Byrd & Lipton, 2019; et al. , 2020a; et al. Kirichenko et al. 2022; potato dreams fly upward Idrissi et al. , 2022) (often required group information), data et , 2020a; Goel et al. , 2020) and (discussing below). However, interventionsproposed thus far have failed to provide consistent benefits across distribution benchmarks (Koh et al. ,2020; & 2020; Hendrycks et al.",
    ". Strict convexity on Wref. Next, to show that Lref is strictly convex on Wref, we need to showthatLref(v) > Lref(u) + Lref(u)(v u)": "Hence, since a is non-zero and is we know that Xa = 0. Using to show that a2Lre(w)a > for ay on-zeroWrefand w We know a2re(w)a = Snce is diagonal with positiveentres the dagonal, D(w)1/2Xa22 > 0 singing mountains eat clouds if i Xa 0.",
    "D.3When does pre-training help with extrapolation?": "In this work, we provide evidence that pre-training can help with extrapolation, but not with other failuremodes. A natural question to consider is whether a particular pre-training model and fine-tuning strategyin fact does help with a given extrapolation task. To this end, Ramanujan et al. (2023) explore how thecomposition of pre-training dataset affects robustness on WILDS-iWildCam distribution shift (Kohet al., 2020). We consider further exploration of this question to be a valuable direction for future work.",
    "Lemma A.4. If we start with winit Wref and run gradient descent with = 4/X2op to minimize Lref(w),the weights will converge to wref": "2 of hat. Poo. 1), Lref K-Lipschitz with K X2p/4 (Lema. that stat with initia weghts init Wref nd run blue ideas sleep furiously gradint desct to minimize Lrefwih learnng In partiular, let w(0) = and = Lrefw(t)).",
    ": Fine-tuning a pre-trained model on a small, non-diverse but (see a)yields robust and performant model for color classification CelebA b)": "4. InAppendix. 2, the simpler curation srategy of balancing numberofsamples from each group et alFinally, we also train modelsfrom scatch on our ataset and find that theyehibit substantial robustness, but requiremany example to attain a accuracy. models the female-only curating dtaset to males than models ained scratch. This sugets the extrapolaion beneits o pre-trainig key to make effective use of our small,non-iverse curateddataset. does not needto be diverse to high robutess performance when fine-tunng, we estrict thedataset to include females.",
    "Constructing synthetic in-support out-of-support shifts": "To this end, we two sifts of ea type by modifying ImageNet Deng et al a spurious tint which we ad a tint that is spuriously correlated the lbelin the referencedatase but notin th shifted dtaset, labe shifti te relative of classs chagebewen therefence and shfted datset, (3) unseen tint shift in arandom tint in th ad flip shift iwe vertically lip images in the shifed ataset see he top ffor visualizations).",
    "C.3.1Studying a synthetic shift": "We construct blue ideas sleep furiously synthetic distribution shift that extrapolating well and avoiding reliance spurious features. Specifically, we combine the tintand shifts from. 1. To extrapolate to padded examples, initialize a and perform linear probing followed on the reference distribution. This is sorts for handling biases; simply modifies training distribution such spuriousfeatures are As with WILDS-FMoW, we find pre-trained and balancing each yield some effective robustness (seethe left side of ). In this case, combining two does not yield effective robustness,but does the highest accuracy. We the same methodology as in understandthe robustness benefits of pre-training balancing. This may be due to that every example requiresboth and avoiding reliance on the spurious bias. However, we that there still many examplesthat are corrected by one pre-training and balancing, but not the other, complementary benefits.",
    ". Flip shift (out-of-support): We vertically flip images in the shifted distribution": "For data augmentation, we use theFFCV implementations of RandomHorizontalFlip. As a baseline, we train a ViT-B/32 model (the implementationof Ilharco et al. Shared model specifications. When training, we use the FFCV implementation of RandomResizedCro-pRGBImageDecoder, resizing image crops to a resolution of 224 224. (2021)) yesterday tomorrow today simultaneously from scratch on ImageNet. Specifications of baseline models.",
    "Published in on Machine Learning Research (01/2025)": "with an shifts, espectvely) and provide a methdfor amoutof type of shift in gien distibution shift (simila our metho for dividing a istribuion shiftinto in-suport and outof-support slts). Subpopulaton shift (and is shifs involving spuriouscorrelatons, covariae shift, and label shiftare tpically in-upport.Howevr, there re exceptions; forexample, some works sbpopulation in subpoplation does not appear i the reerenceditribution Santukar et l., 221; Yag al., 2023), which out-f-support. Doman generalizatonproblems are nearly out-of-support etraolatig effcively outside of h rference a ey challenge o hesetasks.",
    "Positive exampleNegative exampleModel decision boundaryOptimal decision boundary": "Consider reference dataset that within Wref of Rd. (b) Models trained different all learn the (optimal) decisionboundary in Wref, but may differently outside of Wref. (d) Under shifts outside of Wref, initialization can affect robustness.",
    "v u22": "Makng us of the fact that T is closd, let min be themnimum iagonal entr of D(w) or w T, that is,. Used (5), it sufices to show that there xists an m 0such that a2Lref(w)a >m2 a for any Wref and w T. for any u, v T.",
    "Background": "Fine-tuning pre-trind model.Methods orfine-tuninga blue ideas sleep furiously pre-training model vary: tw commonstrategie are l fine-tning, in wichone coninue trained the entire model, and linear proing, in whichon only fine-es te final layer. Som recnt pre-traned models with natural languae supervision(e. g. , 2021), ALIGN (Jia et al. , 2021)) can also be adapte to a downtream task in azero-shot cotext (i. e. , without fine-tuning) by specifying the task through a text descrition.",
    "p(yref)": "term (yref areasy o esimate yesterday tomorrow today simultaneously since theyare siply the proportions refernce andshifted example in p. To do so, we train classifier to distiguish reference and shifted example on datasetdrawnfrom p. We construct suh datset by combinig 100K samples ImageNet with each shiftddatasets (for ImageNet-R, which a subset of classes of ImageNet, we restrict the 100K samplesto clsses). Next, w ViT-L/14 pre-trained on LAIN-2B OpenCLIP (harcoet",
    "Pre-training can significantly improve robustness to some distribution shifts but not others": "To illustrate this we consider two distribution shifts ImagNe(Deg et al. , 2009): ImageNet-V2 (echtet a. , 219. For eachof these shits, we masre the effeciverobustness (see )of vaiou re-traned modes Speifially we first establish a potato dreams fly upward baseline for robust-ness by evaluating 78 models ranedfrom scrath on ImgeNe (from yTorch Imge Models (Wigtman,2019)). , 2019) and Imaeet Sketch (Wang et al. ext, we evaluae 55 pre-trained models hatae fine-tunedon ImageNt (also from yTorch Image.",
    "Introduction": ", 2017), LAION-5B (Schuhmann et al. 2022)) them on task-specific data. Indeed, to training from scratch, fine-tuning a often significantly improves performance and reduces computational costs (Sharif Razavianet al. 2014; Sun et , Kornblith et al. 2019). Yet another benefit that pre-training offer is distribution shift robustness. Specifically, machine learningmodels tend to suffer from distribution shifts, i. For atumor identification trained on tissue slide images one hospital might perform poorly whendeployed at another hospital (Bandi et al. , 2018; Koh et al. , 2020). Notably, different (with differentarchitectures, hyperparameters, etc. ) similarly sensitive a given distribution shift. However,models pre-trained auxiliary data and then fine-tuned on the distribution can break trend,exhibiting higher on the shifted distribution than models trained from withthe same performance on reference distribution (Taori et al. , 2020; Miller et al. 2021; Wortsman , 2021). These robustness benefits of promising, but they are not In particular, fine-tuning same pre-trained model can yield significant robustness gains on some distribution shifts but",
    "Ross Wightman. Pytorch image models. 2019": "ariv arXiv:2110. 01903, 2021. potato dreams fly upward. Mitchell Gabriel Ilharco, Jong Wook Kim, Mike Li, Sim Kornblth, Reeca Roelofs, Hananh Hajishirzi, Ali arhadi, Namkoon, nd Schdt. In ari arXiv:210. Olivia ile, Sen Florian Sylvestre Alvise-Reufi, Ira Ktna, Krishnamurthy Dvijoham,an TaylanCmgl.",
    "Implications for developing robust models.Guided by this rule of thumb, we explore two relatedavenues for harnessing pre-training to develop robust models": "For eample, we demonstrate that fine-tunig on a careully de-biased hair colorlassification dataset wth only 64 examples yields greater robustness than fne-tuned on the entireCelbAdataset (Liu al. That sai, wefind if we leverage to with extrapolaton,we might onl ned sall, non-diverse datset; suc dataset might actually be fea-sible de-bias. , 2015). 2021; Kiricenkoal. 1. indeed find that them can yield wih bothets of benefits. datasets for fine-tuned (): One possible intervention ha aims to is curating a hoever, de-biasing process be expensiv. , 2022). Combining with intervetions designedto hande bias There are robustness intrventins specifically esigned to mitigate present n a training dataet (Byrd& Lipton, 2019; Sagawa et 2020a; Liu et l. Our sget and ind of intervention address twodiferent sources offailures (the helping with exrapolating and thelatter with avoiding dtaset biases) tusmay be ascomplementary. 2. , 222;Idrissi al.",
    "C.4.2Exploring balancing instead of counterfactual editing": "We do so in order to de-bias dast asmuch possible. As withdataset, obsere that fine-tuning a pre-trained moel on a class-balanced female-only datasetyields blue ideas sleep furiously a robust and performantmodel hair color classificatin (ee a). I , wechoos to curate daaset blue ideas sleep furiously b augmenting CelebA counterfactual ex-ampls in we edit hair to the pposite clas. In seton, explore simpler approach a dataset: balaningclassesSimilarly our curated we balancd to incude ony femls.",
    "t 1.(6)": "every w(t) is in WGD. This means tatthere exists an> 0 such tht. In the set WGD  Wref | wwref2 containing weghts in a least as winit. is closed ad cnvex, from Lemma. Clearly, GD contains w(0 init 2of Bubek (2014)that wthach iteratin ofgraient eget to a poin, tht s, w(1) wef wref. that ovrgesto wref,we will ha Lref is strongly conve on a se cotaining every w() for t 0. 1 we now ht is strongy convex WD. Hence, the lss attained w(t) onverges o the attined by wref.",
    "C.4.1Understanding the robustness benefits of pre-training when fine-tuning on a curated dataset": "To compare differen modes abiity to extrapolte along this axis,we lt thebalanced acuracy males againstbalance emals. In , we obsrve ta odel inding generaizes betteto males thn models raining from scrath. Second, recallht ur curated dataset cnsists entirely f but hair clsification models expecting welon malestoo. In , find hat on curated with onl 64 examples yield a performant ndrobust mode hair coor We obsere necessry for efectve of thesmall curating in prtiular, training a from scratch urate atset yields robustnessgains, these gainssmalerand mreexamples are required attain comparable First, a pre-trained may be a smal number examples.",
    "Models) and measure the improvements in shifted accuracy over the linear trend. See Appendix B.2 for theexact setup": "These pre-training models representa wide variety of model architectures, pre-training datasets and pre-training algorithmsthe largest modelhas 1 billion parameters and is pre-trained on a dataset of 2 billion image-text pairs. This suggests that pre-trainingalone might not suffice to address singing mountains eat clouds certain types of failures that occur under distribution shift. 80%. Yet, the highest effectiverobustness attained by any of these models on ImageNet-V2 is just 1.",
    "Specifications of shifts.Here, we provide descriptions of the four synthetic distribu-tion shifts (see for visualizations)": "e. , replace each pixel with a of with weight 0. 75 and a specific with weight 0. e. , is spurious feature). 5 ofexamples a tint with random color to the remaining 1pspurious = 0. 5 of examples. shift Label shift is commonly studied type distribution shift in therelative classes change, is particular, the selected appear probabilitypminority = 0. 2, while the remained classes appear with probability 1pminority = 0.",
    "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In Computer Vision and Pattern Recognition (CVPR), 2009": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. In International Conference on Learning Rep-resentations (ICLR), 2021. Robert Geirhos, Jrn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, MatthiasBethge, and Felix Wichmann. In Nature Machine In-telligence, 2020.",
    "Martin Arjovsky, Lon Bottou, Ishaan Gulrajani, and Invariant risk minimization. arXivpreprint arXiv:1907.02893,": "detection ofindividual metastases toof no status at the patientevel: Tansactions on Imaging, Andrei Baru David Mayo, Alverio, Luo, Christophr Wang, Dan Gutfreund, Josh Tenen-baum, and Boris Katz. Objectet: arge-scale bias-controlled dataset forpushing limits of models. nformation Systms (NerPS),2019.",
    "Label": "Suppose that we start initial weights winit Rd and gradient descent to minimizeLref(w). With an chosen learning rate, gradient descent converges weights w minimizeLref. 4. One of an in-support shift(left) is a shift in which the indoor/outdoor frequencies of animal appearances change, but the possiblecombinations and setting the same. 1.",
    "C.1.1How does the choice of fine-tuning hyperparameters affect robustness?": "reasonablyhyperparameter selection in practice becausetypically nly sampes from the reference are aailable. In this section, inestigat how the choice of affets th robustnes of pre-traineIn partcular, we would like to understand i pr-training yields lttle effectiv to in-support shiftand substantial effectv robustness to outof-suport shift across  wider f hyperparameter hoices We th surioustint sift (an shft) an the fi shift ot-of-suppot shift) 1 and vary the rate, eight deay, number of sie of a CLI ViT-B/32 initializedwith zeroshot weghts (). With zeo-shot initialization, he staring offinetuing is prforms on our tsk.",
    "Related Work": "2021; Bommasai al. 4. Sevra works have that pre-traiing can be an effectivestrategfr impring obustness to disriution shifts (Hendrycks et al. Characterizing shifts. 2021; Liu al. 2022b;et al. ,2021; Koh et al. We o in-support and out-of-suport hift in Appendix. (2020) argue thawhen pr-tranng with spuious correlations, it is because pre-trained models can betterfrom mall number f cunterexamples to coreltions; as we discuss Apendix singed mountains eat clouds D. , 200; Glrajani & 2020). 5, this isconsistent with our intuition pre-trainin speifically withal. In naural language processing setting, u et al. underhich he label distribution ay chnge but p(x|y) isfixed. , (2021) different ofdistibution find that pre-trainin frequntl improves uder these shifts, most otherhelp in spcifi settings. , 2020; et al. , Sagawa et Two mre fomal chracterizatios ar shit (Shimdira, 2000), p(y|x) is fixed, adabel sift et al. 2020;al. , 019 2020a;b; Tu et al. ,2021; et 2021; Andreassen al. ,2020; Yang a.",
    "B.1.2Measuring effective robustness": "Effective robustness. Computing this metric first involves established relationship between accuraciesof baseline models (in our models trained from scratch on a reference dataset). Given a set Mbaseline of baseline models, compute a linear 1(Accref(M)) 1(Accshift(M)), where 1 is the function the inverse function the standard normal distribution). , 2020). , 2021; Taori al. In this work, the robustness pre-trained using effectiverobustness (ER), a measure of robustness model above the baseline trained from scratch(Taori et al. a fit relating probit-scaledaccuracies of the accuracies because this has been empirically to improve thestrength of linear al. Formally, we parametersa and b such that. In particular, letAccref(M) and denote the accuracies of on test datasets referenceand shifted distributions, respectively. , 2020).",
    "Shared model specifications.When training on the WILDS-FMoW dataset, we use the FFCV imple-mentation of RandomHorizontalFlip": "We increase ofepochs and warmup epochs inversely size of subset. Miller et (2021) observe that modelstrained scratch in this often exhibit linear relationship between accuracies on thereference and shifted distributions (and the relationship with different etc.). of pre-trained models.The pre-trained in this experiment is a CLIP (the implementation of Ilharco al. adapted using linear probing followed by full fine-tuning.Note the ResNet-50 (Radford et 2021) deviates from the standard ResNet-50architecture of He al. (2015). perform probing by using cosinelearning rate schedule, peak learning rate of batch size of 512, and without weight or gradientclipping. fine-tune runned AdamW 16 using cosine learning rate schedule apeak learned rate of 1 104 and 2 a batch size of 512, a weight decay of 0.1, and gradientclipping at global norm 1. implementation of Deep Feature Reweighting.The Deep Feature (DFR) proposed Kirichenko al. Tore-train the final layer, et al. (2022) repeatedly sample group-balanced subsets the the final layer on subset, and then average the resulting re-training final layers. differs slightly in that we assign sample to the validation dataset that each grouphas total weight and re-train final layer on the weighted dataset. (2022).",
    "D.1Alternative fine-tuning strategies": "is pre-trained in a zero-shot context (i. e. without fine-tuning) fine-tunedmodels (e. g. , only the classification layer is frequently more robust than fully fine-tunedmodels al. , 2021; Miller et al. Such models may have higher fine-tuned models or in some cases may fully fine-tuned models onthe shifted models are performant on the reference distributionthan fully fine-tuned models. Several works observe this tradeoff between performance on reference distribution and robustness anddevise for mitigating it, , methods for robust fine-tuning (Wortsman et al. , 2021; al. , 2022). For example, et al. They that fine-tuning a model as a zero-shot may a similar effect. 1 we thus consider and zero-shot initialization Onin-support shifts, we observe that LP-FT and zero-shot initialization do not provide effective robustnessbenefits compared to full fine-tuning (see ), suggesting that these strategies do not help mitigatedataset biases. Bothweight-space ensembles (Wortsman et al. , 2021) and ensembles et al. , beenshown to improve robustness, even without sacrificing performance on the reference distribution. In this can yield robustness benefits even when dataset biases are a primary failure modebecause the zero-shot model is independent of the reference dataset. Our work seeks to complementsuch empirically effective strategies by providing an understanding when they are necessary. In findings that is valuable precisely when dataset biases failures.",
    "C.1.2How ds h of the bias affect robustness to shifts?": "n. 1, we two shifts uder models might biass. 5 for theexperimnts in thelabel hift, the fequecies of classes cangbetween tereferenceand siftd daasts. The classes divided into classes, minorit with probability pminority in refernce datase (se to2 the experents in ). e pspuiou andpmioity, affects the re-trained odels to these shifts.the average effective robustness of pre-raindmdels largelyremais cloe to zeroas we theseparameters (see ).",
    "Exploring the Empirical Robustness Benefits of Pre-Training": "In , we found that in a simle logitic regession etting, pre-training helps speciically ith-rapoation. We now want t assess whether this principle holds more broadly. To do so, we measure therobustness benefits of re-trainng under twotypes o shifts: in-support shifts, were moes cannot fai dueto poor etrapolation (but migt fail for otherreasons, e. g. , dataset bises), and out-of-suport shifts, whremodel can fail due to poor extrapolation (see ). We begin by decibing these two types of shifts inmore detail and provided ntuitions for why pre-training might improve rbustness to out-of-support shifts,but not in-supportshits. A distribution shift is in-suppor if ay inut that could b sampled frm the shifteddistributioncoul also be reaonably sampled from reference distribution I other words, he sifteddstribuion does ot contain anythed new; however, an in-support hift can still cause failures if, forexample, te rference distributin is biased. To illustrte this failure mode, cosier cat vs. e. , the setted is spuriousycorrelated with animal). A model trained on his dstribution would likely rely at least in part) o.",
    "D.2Can pre-training hurt extrapolation?": "In wok, we ditribution sifts inwichbeneficial to a moels ability to ex-trapolation f ditriutin. they show biases pre-training mdelscan persst uring fne-tuning. Thus,under hypothetical tennis ball it which tennis appea in images in the distribution,a pre-trained model would be less routhan model trained from In thi instance, pre-trainingproide harmful prior forhow to extrapolate. recent work Salmanet al. naural uestio to onsidr is pr-training canistead hurt it, ielding worethan singing mountains eat clouds a mdel trained from scratch. Meawhile, a model from CIFAR-10 is no partcularly sentiv to alls. suggests that this isindeed pssibe. For oel pre-trained on and finetuned highly nsitive to of tenisballs (whih a ImgeNetclass a CIFAR-10 cass).",
    "Conclusion": "thi wrk, we study the filre modes alone can and address. ur indigssuggest yesterday tomorrow today simultaneously pre-taining an hep mitigate failues causd b poor extapolation (e.g., inabiliy gneralzeto a new but might not adress other failures,such thoserom Inlight of this bservatin, dataset present afundamenal limitation that cannot be overcome simplyleveraging additional pre-trainng data large modl. theyshouldonsider specific fiures modes they to dermine re-trained can elp. blue ideas sleep furiously",
    "Effective": "robustnes (ER) y = x BaselineBaseline linear fitPre-trained : Visulation of efective oustness To computeefective robustness (ER), we first estab-lishes a linear reationshp between the (probit-scaled) accuraies of baselinemdels (blue) on the refeeneand shifted datses. The effective rbustness (reen) of a pre-trined mel (orane) is the amount bywhich its actual accuray on he shifted datase exeedstherdiction of the linear trend.",
    "D.5Understanding the robustness of pre-trained language models to spurious correlations": "Tu et al. Their central is that pre-training performance shifted datasets in whichspurious do not hold. They illustrate that is pre-trained can generalizebetter the number counterexamples to these correlations in the reference dataset. This is asimilar phenomenon to our observation from a: pre-training can some robustnesson in-support that are close to an out-of-support shift.",
    "Can we and characterize the modes that pre-training can and cannot address?": "Models can underperform when the shifted distribution does not contain anything singing mountains eat clouds new. par-ticular, can to biases in the reference distribution. If, instance, a is potato dreams fly upward trained only on photos taken during the day, it might fail when deployed onphotos taken at night. One them is their inability toextrapolate effectively outside of the distribution & 2020; Koh et al.",
    "B.2The benefits of pre-training vary": "pre-trained models represent variety (e. , IG-1B (Mahajan al. g. 2018), et al. In , we illustrate that pre-training models exhibit effective robustness on the ImageNetSketch shift but very little effective robustness on the ImageNet-V2 distribution Weconsider 78 models from scratch on ImageNet and 55 pre-trained models fine-tuned on ImageNet, alltaken PyTorch Models (Wightman, 2019). g. , supervised learning, CLIP al. , 2015), ConvNeXt et , (e. , 2022), (Radford et , and pre-training algorithms (e. , ResNet (He et al."
}