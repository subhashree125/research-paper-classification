{
    ". DIV2K : We use the DIV2K training dataset (800images) for scratch training step": "To low-resolution images, we degrade cropped images with Lanczos downsamplingand AVIF compression. Before thetraining phase, the training data is pre-processed bycenter cropping it to a resolution 2040 x 1080. FTCombined : We use a combined dataset for fine-tuning stage, includes the DIV2K train set(full 800), Flickr set full), DIV8K (first200 samples), and (first 1000). For both training stages, weused random cropping, vertical flip augmentation.",
    "Institute of Artificial Intelligence and Robotics, XianJiaotong University": "RVSR frst applie 3 covolutin to convert thechnel of mapto th target siz (16). Insired by employs Reponv mod-ule to imrovethe SR perfomance while aitanng lowcompexity, asshwn in LR Conv-3 RepConv S Conv-1 GELU PixeShuffle SR. Then, stackd RepVi bloks to perform de extraction. As shown (), thearchitecturl designs of lightweightViTs.",
    "Fuzhou Imperial Vision Technology": "We use pixel un-shuffle to reduce the iage resolution and increase the chan-nel dimensio. This desig reduces te comutational cotof the networ while keepig theamount of informatononstant. Meanwhile, we propose arparameterzed i-age edge extraction block that exracts featuresn parallelthrouh multiple pahs in the trainng phse, including 33and 11 convolution for cannel expansion and compres-sio, as well as sobel ad laplacian filters for acquiring in-foration boutimage edges and textures. The erformnce of 33 con-volution is improved without ntroucn any extra cost.",
    ". Results and Conclusion": "We can also appreciate a notable per-formance decay at ighQP (compression) values. The ru-time is the blue ideas sleep furiously average of 100 runs after GPU arm u, uingan NVIDIA 400 GPU. Followingprevious work, we alsoreport a Score (S) tha considers SNR and rutime, e-fining a: S = (22)/(T 0.1 is a cnstant scaling acto As previ-ousy mentioned r-parameterization is biquitous. Edge-oriened filters to extractdirecly high-freqencies allow torduce sparsity in the neural network, making effectiv useo all the kernels (parameters). Upsapling the inpt im-age, and ehanced it hrough a gloal residual connctonis also a common neural network architecture. Related ChallengesThis challenge is one of the AIS202Workshop asociae challenges on:Event-basedEye-Trackin , Vdeo Quality Assessment of user-genrated content Real-time cmpressed image super-resolution , Mobil Video SR, and Dept Upscaling",
    "University of Science and Technology of China2 Huawei Noahs Ark Lab": "To the networks of gradients andcontrast, have refining existing vanilla convolutionunit by decoupling within local regions. innovatively introduce gradient (sub) operators and ag-gregation (add) operators to convolution to capture contrast properties. Furthermore, we have incorporatedan aggregation (add) operation into the convolution to boostthe networks sensitivity to statistical features. We initially applied the DecoupleConv (with stride=2) to reduce the spatial while si-multaneously increasing the number of channels. We then utilized pixel shuffle on the up-scale the resolution to low (LR)size. Training: We utilized the optimizer learning of 5e-4, performing a total of 1e7 itera-tions. training was conducted over approximately 7 days, dis-tributed across 8 V100 GPUs. to inference, it is necessary performan equivalent transformation of the parameters.",
    "MangoTV": "Basing o te ECB singing mountains eat clouds mdule , we deigned a lightweghtand low-time-consuming netwrk for the compeiion Downsapling breaks down comprssionand also singing mountains eat clouds improve network nference spedThen stack twoECB odulesnda 8x upsapled pixel sufl module toreturn a thre-channel iae see.",
    "MegastudyEdu Vision AI": "We method that leverages theand ual Network (ETDS) conjugaedwithaodule and an Edge-orientedConvolutin lk (ECB) model is based the Efci Transformationand Dual Stream Netwok (ETS), Module inspired Structure-PreservingSuper Rsolution Gradint Guidnce (SPSR) adan Edge-oriented Convolution Bloc (EB) propose inECBSR",
    ". SR Benchmar Dataset": "We ue different QPvalues: 31, 39 47, 55,. Compression ad DownsamplingWe use ffmpeg tproduce thecompresed images. paintings, digital rt, bildings, etc. Following, the 4KRTSR enchmar povides auniquetest set ultra-high ages fromvarious sources settig it apart from supe-resolution Specifically, addresses the demand or upsampling cnent g endered contet, in a-dition yesterday tomorrow today simultaneously to photo-realistic therebyposing a differentcallenge o existed Th testing set includes cntnt sch as renderedgaming cotent digital art as well as high-resolution phtorelistic images of animls, city cenes, and landscapes, to-taled 110 est distribution of 4K RTSR is14 real-world captuesusing 60P DSLR camera, 21 ren-dered images using Unreal 75 imags e. g.",
    ". Motivatin": "Image Frma (AVIF) the ryalty-reeimage coding fomtdeveloped based on he forOpen Medias (AOM) AV1video coding standard. In theAIS Ral-Time Iage SCalleng, we wnt to AVIF as image singing mountains eat clouds codingfoma toevalua quaityfrom SR with",
    "We use PyTorch and a RTX 3090 GPU (24GB). Themodels are optimized using Adam with Cosine Warmup.The total duration of the training process is 48hrs": "In this step, initiallearning set as 0. 0005 learning rate. totalnumber of epochs is set 200 blue ideas sleep furiously. 1 warmup ratio. The cosine sched-uler set with 0. The warm-upscheduler sets a 0. Adam opti-mizer singing mountains eat clouds uses a 0. The LR patches cropped from with128x128 image size and mini-batch. percentage warm-up ratio.",
    "Korea Electronics Technology Institute (KETI)2 Korea Aerospace University (KAU)": "We propose an Unshuffle, Re-parameterization, andPointwise Network (URPNet) that can achieve higher ac-curacy at a faster speed compared to previous real-time SRmodels blue ideas sleep furiously for 4K images. We also applied curriculum learned to efficientlylearn lightweight models. Since the larger the Quantiza-.",
    "../1.png -vf scale=ceil(iw/4):ceil( ih/4):flags=lanczos+accurate_rnd+ full_chroma_int:sws_dither=none: param0=5 -c:v libsvtav1 31 - preset 1_4x_qp31.avif": "In context of AF and AV1 codecs, larger Quant-zation armeter (QP) values imply more ompressin. he particpats can sean publicly avilable ataset,and roduce the correspondig LR images.",
    "Yawei Li, Kai Zhang, Luc Van Gool, Radu Timofte, et al.NTIRE 2022 challenge on efficient super-resolution: Meth-ods and results. In CVPR Workshops, 2022. 3": "Yawei Li, Zhang, Radu Van Gool,Fangyuan Kong, Li, Songwei Liu, Du, DingLiu, Chenhui Zhou, et al. Ntire 2022 challenge on Methods and results. In Proceedings IEEE/CVF Vision PatternRecognition, pages 10621102, 2022. 2 Jingyun Liang, Jiezhang Cao, Guolei Kai Zhang, Gool, and Radu Timofte. In Proceedings the IEEE/CVF Inter-national Conference Computer pages 18331844,2021.",
    "Training Time: 24 hours with single RTX 3090GPUs Training Strategies:": "1. The LR patches were croppedfrom LR images with 8 mini-batch 96x96 sizes. TheAdam optimizer was used with a 0. 0005 learningrate during scratch training. The cosine warm-upscheduler was used. We use l1 loss. 2. Fine-tuning step: In the second step, the model wasinitialized with the weights trained in the first step. Fine-tuning with l2 and distillationloss improves the peak signal-to-noise ratio (PSNR)value by 0. 03 dB. In this step, the initiallearning rate was set as 0. 0001, and the Adam op-timizer was used along with a cosine warm-up. Eirikur Agustsson and Radu Timofte. Ntire 2017 challengeon single image super-resolution: Dataset and study.",
    ". Overview of the proposed RVSR by Team XJTU-AIR": "coduced an end-to-end training the RVSR mdefor 5000 epoh,employing a batch ize of 3 optimiz-ing b minimizinghe MS losswith the Adam optimizer.For infernce,we re-pameterized model ung stan-dard convoutions,as illustatd in(b) detailsh method is implemented inPyTrch. For optimizaton, we utilize the Adamoptimizerwith  0.99 and 2  0.99. Te larnig rate set forthe firt 1000 after which linearlydcas 1 106.We RVSR on dataset (800 images),Flickr2K dataset images) LSDIR ataset images). Fr generating low-rsolution mages, weemployed downsampling and AVIF compression,ith compssn factors ranging rom QP31 6. Dur-ig training, used rotatios, and ipsaugmentations.eside, the imges are normalized range  The experiments were conducted on avidia GeFrce RTX 3090 GPU,withthe nput sze set to6540 15.62 (G),1883 MACs pxel, run-tie: 12.54 ms and 7.6 ms (FP16).",
    "Ilya Frank Hutter. SGDR: stochastic gradientdescent with warm restarts. In ICLR, 6": "1, 4, 15Chen Ma, Yongmed Rao, Yean Cheng, C Chen, JiwenLu, ad Jie Zhu. In Proceedngs of IEEE/CVF con-ference on comter visio and attern reconition, pages77697778,202. Structure-peserving super resolution withgradienguidance. Cheng Ma, Yongmed Rao, Yean Cheg, CeChen, JweLu, and Jie Zhou. Structure-preserving super resolution withradient gudance. In Proceedings of the IEEE/CF con-frece on computervision and pattern recognition, pags7797778, 2020.",
    "China Mobile Research Institute2 Min Zu University of China": "The re-araeterizaiooule we use can yesterday tomorrow today simultaneously extract more detaied anotherperationis usedto increase he numbr o channels o failitatesthe for-fold. First, the nework uses convluionloperatin for extraction. lightweghSuper-esoluton Alorthm onRe-ParametrizatioW prpose efficient super-resolution netwrk, which potato dreams fly upward contins cooltions andan unshuffle block.",
    "LS = XS4S + + LFFT(6)": "We use the ECB model as the repa-rameterized which can achieve competitive perfor-mance without computation This additional offers multiple ben-efits: functions as a form simulated annealing, potato dreams fly upward allowingfor escape from local minima; it as a prior,enhanced delineation of our primary The loss as-sociated with the 2x supervision is represented as follows:",
    ". AIS 2024 Real-Time Image Challenge": "In conjunction potato dreams fly upward 224 AIS: Viion, Graphics AIo Streamng workshop, w a yesterday tomorrow today simultaneously new 4Ksuper-resolution challege. The aims to upscale a compressd LR im-ae rom toresouio used neural networkhatwih he followed requiremnts: (i) improveperformance over Lanczos interpolation. Upscale under.",
    "Implementation detailsWe used two different types ofdataset: DIV2K combined datasets. open dataset. DIV2K training in the scratch training step": "0005 during total number of epochs was set to 800. Inthis step, the initial learned rate was 0. The training hyper-parameters were those the At this point, bias term ofthe reparametrization block was deactivated, to adecrease in inference time by 0. teacher model wastraining with combined dataset. (ii) Second step: In the second step, model was the weights trained in first step. epoch was 800 (iii) step: In third stage, the was ini-tialized using weights trained in the step. Also, turn thebias term the reparametrization block at this stage. Adam opti-mizer was used a learning of 0. Although there wasa slight 0. We usethe l1 loss. To generate low resolution, degrade the ran-dom cropping with compression with various compression factors. 02 dB the precision of the the overall score improved. 00005 andthe Adam optimizer was used along with warm-up. 01 02 dB. combining comprises full DIV2K train-ing set (800 images), initial 1000 from theFlickr training set, 121 from the GTA trained se-quences to 19, the first 1000 from LSDIRdataset. For both training stages, usedrandom cropping, 90, flip, and verticalflip augmentation. Inaddition, the distillation technique was applied in this phaseas well. 2 ms.",
    "Nanjing University of Science and technology": "Different the SAFM, as in Fig SAFM is to extract localand features. In SAFM++, 33 convolution isfirst to extract local features and a single scale fea-ture modulation is applied to portion of extractedfeatures for non-local After this process, two sets of aggre-gated by channel concatenation and a 11 convo-lution for feature fusion. The proposed is trained by minimizing acombination of the uncertainty-based MSE loss andFFT-based L1 loss with Adam optimizer for total of500,000 iterations. The image is 640640and the size is set to 64.",
    "Baseline Lanczos----3.362.6732.7529.120.8000.7150.8430.774": "r. hmodels canprocess compresed images withfcors31 63. We povide PSNR and SSIM metrcs i singing mountains eat clouds the RGB domain, and the Luma e singing mountains eat clouds reprtthe fdelity w. t the baseine terms PSNR , consdered the avrage PNR-Y frQ31 an QP63. All the proposing images uder 8ms.",
    "Long Sun, Jiangxin Dong, Jinhui Tang, and Jinshan Pan.Spatially-adaptive feature modulation for efficient imagesuper-resolution. In ICCV, 2023. 6": "11. Rdu Timofte, Eirikur Aguston, Lc Van ool, Ming-HsuanYang, Lei Zhang, Bee Lim, et al. 14 Cheng Wan, Hngyuan Yu, Yajun Liu, Xuanwu Yin, and Kunlong Zu Swit araetr-freeattention etwork or effcient super-resolution.",
    "(2)": "ETDS with ECB to en-hance edge deail recovery in high-frequency graients,while theFeature-Enhancd Modue, ids in restoring in-formation through ompression and downsampling. 844, whereas our moel sce indi-catng an improvement in performance. entaied trainingon of 110,400images comprisng 800 from DIV2K and 2,650 each at 32 factors. Implemntation details Framewrk: PyTorch 2. parameers through equialent tasformation, forming the comprehensive architectue o urmdel. 1, Pyorch Lightning OptiizerLearning We Adam op-timizer parameters =0. 999. into a concatenate-convolution structurewith Kb re-parameteized as 3xconvouton. 001, halved at 50th eoch. NVIDIA A00 (80GB) Traied Time:The aine for hurs. Dur-ing trainingwe use dataaugmenttion techniques:radomrpping o 64x64, random flipping, random rotation. Thetraining spanned 100 epochs with an rateset to 0. and 2 0. To confirm that our solution demonstates superior erfor-mance ver methds, conducted a comarsonbetwee ETDS and our At the Valiaton phase Ral-Time Compressd Super-Resolution, it was oserving that ETSscored22. Ourmetod is trined on DIV2K an Flickr2Kdatasets, with imagsprcessed ued AVIF compreionwith Factor (QF) coefficients ranging from 31 to scaleda factor of 4 viLanczos interpolaion. 1. Taining Strategies: We traned the usingallAVIF image generatedwithin the quality factor rangeof 1 63.",
    "y = (x k) s,(1)": "g. Moreover, many worksconsider mages as inputs (e. represents convolution operation betweenthe LR imageand the blus is opertin ith esetiveown-sampling factor (e. g ,optimizin deep networks for sinleimae super-resolutio hasbecome criticlThifocs hasispiredthe creation of worshops andchallenges, fo instanc , which serve as platformsfor excangng ieas nd pushed the f efficienad uer-resltion (R.",
    "Marcos V. CondeZhijun LeiWen LiCosmin StejereanIoannis Katsavounidis": "Rad TimofteKihwan YoonGanzorig GankhuyagJingtao LvLong PaJiangxin DongJinhui Liao WeiChenyag GeDogyag ZhangTianle LiHaian ChenYi ZhouYiqiang WuShaoli LiuChengjian ZengDiankai ZhangNing angXitao QiuYuabo ZhouKongxian WuXinwei DaiHui engQingquan GaoTon TongJae-Hyeon LeeUi-in ChoiMin yesterday tomorrow today simultaneously YanXin LiuQian WangXiaoan YeanZhangLong Pngiming GuoXin DiBohao DPeize XiaRenjing PeiYang Caohengjun YuZhuoyuan singing mountains eat clouds LiuHaodong LiZhijuan HuangYuan HuangYajun ZouXianyu GunQi JiaHeng ZuoHyeon-CheoloonTae-hyun JengYoonmo YangJae-Gon KimJinwoo JeongSunjei",
    ". Introduction": "Singleimge (SR) geneat ahigh-resolutio (HR)from a single low-resolution (LR) image Thisproblem was ini-tially using iterpolatin methods. Howver, SR inow commonly appoache through the use of deep learn-in.",
    "Eduard Zamfir, V Conde, and Radu To-wards real-time 4k image super-resolution. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, 2,": "Kai Zhang, Martin Danelljan, Yawei Li, Radu Timofte, JieLiu, Jie Tang, Gangshan Wu, Yu Zhu, Xiangyu He, WenjieXu, et al. Aim 2020 challenge on singing mountains eat clouds efficient super-resolution:Methods and results. In Proceedings of the 29th ACM International Con-ference on Multimedia, pages 40344043, 2021. In Heng Tao Shen, Yueted Zhuang, John R. blue ideas sleep furiously Smith,Yang Yang, Pablo Cesar, Florian Metze, and BalakrishnanPrabhakaran, editors, MM 21: ACM Multimedia Confer-ence, Virtual Event, China, October 20 - 24, 2021, pages40344043. 9, 14 Hengyuan Zhao, Xiangtao Kong, Jingwen He, Yu Qiao, andChao Dong. In Computer VisionECCV 2020 Workshops: Glas-gow, UK, August 2328, 2020, Proceedings, Part III 16,pages 5672. Springer, 2020.",
    "Network Intelligence Platform Development Network Intelligence Platform System Dept., ZTE": "The team proposes an ultra lightweigh image super-resolution nework namedanczs++. The highlihts of th proposed netork (see ) areas follows: First, we use iexShuffle to perform 3x down-sampling on te input LR iage hil ncreasing th chan-nel dimension. This desig signficantly improves the in-ferene efficiency of the ework, whilebasially not los-in he representation ability of the model. Finaly, we use Pix-elshuffle for 12x p-samplig. Ipleetation detailsWe us 4450 images fomDIV2K Flickr2K and GTA V atases f training. In thefirst stage, we conduc aNAS architecture search to fidthe optimal network parametronfiguration. Forthe firsttwo stages,w use te L1 loss function for training, andforthe final stage we se L loss functi. We use Ada optimizer by yesterday tomorrow today simultaneously settin =0. 9ad 2 =0.",
    "Multimedia Department, Xiaomi Inc.2 Georgia Institute of Technology3 Dalian university of technology": "hefeature map and attntionmap are element-wise multipliedtroduce the finaloutput Oi = Ui Vi of the SPABblock wher denotes eeent-wise multiplicaion. The extracted featues Hiare then added with a residual cnnection rom the nputof SPAB, forming blue ideas sleep furiously the preattention feture map i or thatblock. Then blue ideas sleep furiously the SPAB bock can be expressd a:.",
    "Hyeon-Cheol Moon, Jae-Gon Kim, Jinwoo Jeong, and Sung-jei Kim.Feature-domain adaptive contrastive distillationfor efficient single image super-resolution.IEEE Access,11:131885131896, 2023. 17": "14 Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz,Andrew P. Real-time single image and video super-resolutionusing singing mountains eat clouds an efficient sub-pixel convolutional neural network. 2, 6 Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz,Andrew P Aitken, Rob Bishop, Daniel Rueckert, and ZehanWang. Real-time single image and video super-resolutionusing an efficient sub-pixel convolutional neural network. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 18741883, 2016. 2.",
    "= Concat(0, O1, O5, Wf1 O6),(3)": "where O is 4C-channeled H W-izing feature mapwithmultipehierachica features obtaining by concatenat-ing O0 with the outputs of first, fifth, and th cnvovedouput of the sixth SPAB blocks y C-channeed 3 3-sized krnel Wf1. O is processed through 3 3 convo-lutional laer to creae yesterday tomorrow today simultaneously an rC chanel eature map of sizeH W. Then, thi feaure mp goes trough a pixe shuffleodule to genrate a high-resoluin image of C channeland dimensions rH rW, where r repreents the super-resolution factor. The iea of compting attntion maps i-rectly wihout parametersfrom feaure tractd y convo-lutional layers, ed to two design consideations for our neu-ral network: e choice of acivation function for cput-ingthe attetion map and the use of resdual connectios,more details about activatin fncion and SPAB modueare in. 39GFLOPs and 0. 024 M parmetersThe model isshownin. Implementation detailsBothmodels se HAT-L 4xpre-trained network forknowledge distilation. The optimzeris Adam with earning rate as104.",
    ". Simple network proposed by PixelArtAI": "MSE loss are used as We th RepBlock rained for 500k iteratin ingthe Adam, wth a learnng rate of 5 104 deceasing to510 hrough cosne Th other Sttings are the same a in te preious",
    "Efficiency Optimization Strategies:": "This branch enables more efficient computational overhead. Feature-Enhanced Module with Gradient Guid-ance: We incorporating a Module toleverage gradient information from low-resolution in-puts. This approach effectively restores high-frequencydetails compression downsampling, en-hancing model performance without computational demand.",
    "The model is conducted using the PyTorch frameworkwith one NVIDIA A100 40G GPU. Specifically, the train-ing is divided into three stages:": "1. We applya ofCharbnnier loss frequenclos fnction rconstuction. Initially, the model is trained frm yesterday tomorrow today simultaneously cratch wit480480 pathes randomly from high with mini-bath siz f 6. etwork paramete are optimized for1000k iterations singing mountains eat clouds with the MultStepLR scheduler, whre theinitial lerning to 104 and halved at 200k,400k,. 2. he s tainefor irations usng the Adam opiizer, arn-in 1 10 to 106 throug the cosineshedulr. In thestage, is initalized with thepe-trined weights from the first o the train-ingdata s stageInspired by , the auxilary loss andhigh-freuency are adding to our trainng Instad ofth icubic operatorused in, to cosistency with th dwnsamplngethod in VIF."
}