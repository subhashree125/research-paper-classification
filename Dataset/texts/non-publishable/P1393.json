{
    "The answermeans tha the does nvlve crowdsourcing nor research withhuman subjcts": "f ou obtaied IRB approva, yousould clearly state thisn the paper. Depnded on the country in which reearch is conduced, IRB approval (or equivalent)ay be rquired fo human subjetsresearch.",
    "Framework": "and we accumulate thegradients singing mountains eat clouds of hose ubnets at ech itration. Base on oranalyis in, Vis have mnimal interplation ability, wich suggests that all subnetsave to be individuallyoptimized to chieve decet erformance. Te orrespnding sb-networks are: F s () F yesterday tomorrow today simultaneously l (), F m () and F m2 ().",
    "A.8Comparisons in Training Time": "The difference between US-Net and Scala is notlarge as the transformer architecture has been well-optimized on GPU and we do singing mountains eat clouds observe a significanttime gap between Scala and Separate Training as they have to train 13 models iteratively. Assuming to deliver 13 models in the end, we compare the training time (100 Epoch) of Scala withUS-Net , Separate Training on 8 A100 GPUs.",
    "H. M. Cord, A. Sablayrolles, G. Synnaeve, H. Jgou. Going with imagetransformers. ICCV, 2021": "Martin,. Almahair, Y. Rzire, N Goyal,E. 13971, 202. potato dreams fly upward Bshlykov,S. Batra,P. Lavri, G. H. Albrt, A. -A. Bhosale,et al.",
    "Isolated Activation": "llustratedin Sec 3, constnt acivation the smalles sub-network F s() its own accuracyat cot of other subnets peformae. This dilemma is a ignifian accuracy dropof Fs () if we do not constantly t ), otherwise, he performance other are t not the of the lower ound bu facilitates the other subnets well. Formally, gien learnable weight RCoCiHW of a rndomlayer Vi (Co, Ci standsforthe output, input channel nmber, H, W represens the and width of h convolutio kernel,H = W = 1 ully onnected the of F() where r = s is selecteda:.",
    ": We train US-Net on ViT-S without con-stantly activating the smallest subnet (denoted as*) and observe an average performance gain of1.8% at other width ratios on ImageNet-1K": "We analyze the results the expected each sub-network. expecting trained epochs for the intermediate networks can be expressed as:.",
    "Guidelines:": "The should version of the asset is used if possible, include The of the license (e. The authors should cite original paper that produced the package dataset. should be included for asset. , CC-BY 4.",
    "In this we conduct experiments over DeiT-S 100-epoch training to prove concept": "Coparison with scaling baselines. Tab. potato dreams fly upward US-Net shos significantly worse prformace ST which indicates scling down ViT a more challenged problmcompared Narchitectur. Nevertheless, Scala acheves better prformnce compared toat ll width ratioswithne-shot training, torage coss saing model observably. 00 51. 01.52. 54. GFLOPs (%) (X=25)Scala (X=13)Scl (X=7)Scala (X=4).",
    "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
    "A.6Verification of Slimmable": "This optimizing larger sub-networks does not directly contribute to theperformance of even though their are shared in nested nature.A question ViT potato dreams fly upward can still maintain the slimmable for the width ratiosduring training. To verify this point, we respectively fix the m2, m1 and 0.4375 duringtraining, so that only one blue ideas sleep furiously sub-network is optimizing at range. the accuracy ofunseen due to lack of interpolation ability. Nevertheless, the performance 0.00.51.01.52.02.53.03.54.04.5 GFLOPs Acc (%) STScalaScala (fix Scala (fix m1)",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": ", if the approach wasonly tested on adatasets a few runs. In empircal results oftendepen implicit assumptions, wic should be Th autors should reflect onthe factors that influenc heerformane of the approach. g. a speec-t-ext system not bued reiably o provide closed for online becase ails to handletehnical jargon. Fo example, facial recoiton algrthm perform when image resolutionislow o imges are takn in lw lighting. he authors areencouraged to create a \"Limitations\"in their Te paper shouldpoint out any strong and how robust te are of these assumptions, noiseless settings,model ellspecification,asypotic approximations only holing locally) The authorsshould reflecon how theseassuptins ight in and what would e. The uthors should reflect onthe scope of the claims made, e.",
    "Scale Coordination": "To optimize the at different scales, we present theScale Coordination training composed of three Progressive KnowledgeTransfer, and Noisy Calibration, to ensure that each subnet receives simplified,accurate, and steady learning objectives. We follow the setting of to trainthe full model F l () with distillation and introduce distillation token for knowledgetransfer between sub-networks. We present the strategy Scala this section.",
    "Related Work": "is scaled at multiple with small computation change, incontrast our work we only scale the width with a large slicing bound. SN-Net a yesterday tomorrow today simultaneously recently proposed method that a supernet several pre-trained and insertslinear layers to singed mountains eat clouds build dynamic routes flexible inference. Slimmable Neural Network. The advent of ViTs has also sparking interest in scaling down these models. Unfortunately, these static cannot make customized adjustments for resource-changing environments in real scenarios. Afterward, ViTs have been up to 4 billion and 22 billion parameters performance and enormous costs. Like Transfromer in NLP, scalability and performance improvements have been central focus of recent research. Techniques such knowledge distillation have been explored to reduce ViT model size. its variants train shared network which adapts the width to accommodate the dured inference. Specifically, strategies have explored toscale the depth and is scaling to even with almost 2 billion parametersand reaches new results.",
    "(b) Linear probing on UCF101 with 8 sampled frames": ": Transferability of Scala. first conduct on ImageNet-1K help offoundation model DINOv2-B. we conduct linear on video recognition datasetUCF101. Improvements over are shown. Then we conduct linear probingon 12 fine-grained classification datasets followed the setup in DINOv2. Tab. : Comparison of Scala and DeiT on 12 fine-grained Scala-B distilledfrom DINOv2-B ImageNet-1K for epochs and then we conduct linear on fine-graineddatasets. The average accuracy of 12 datasets is shown in the column.",
    "In this section, we perform training over DeiT-B for 300-epoch training following the standardprotocol on ImageNet-1K to compare with the state-of-the-art": "When adopted the stonger teachr network as SN-Net dos, Sclauterfors SN-Net with aaverage improvement of width ratios. Comparions with Traiing. DeiT-Ti, DeiT-Sad DeiT-B, respectivel. is state-of-the-art that supports flexileinference onViT. 25, 0. When X = 7, we can acheve similarperformane at r = 0. due to trainingtime. , DeiT-Ti/S/B) to constucta inserts additional layer build dynamic rotes flexible inference. itutilizes severalmodels g. with trined epochs T. In ab. , tranng, wherer = 0. substantiates teand the slimmable representaton. Scala exhiit a = 0. 25 and mtches the ST except r = 0. 0, 1. Scala obtains simila with SN-Net at wdth ratios and clrlyoperforms it a budgets.",
    "Transferability": "For the clasificaion hea added on Scala, we make t slimmabl tofit the fetures with vaous dimensins and follow the same trainng protocol as in bject reonition. After tha, we onduclinar oing onvideo reconition datast UCF101 wth 8 evenly sampled frames and average theirfeature for te final prediction. In b, two ntable observations emerge: (1) Sala consitentlyoutperforms ST across differentwidth ratios on the UCF101 dataset, implying the great transferability of the immable represntion(2) Scaa retains its slimmable ability when appled t anw task andexibits promising performanceacross a wideslicing range (10141GFLOPs), promising its applicaion on othe downstream tasks.",
    "IA57.361.566.469.872.073.475.8w/o PKT53.060.165.668.871.773.776.2w/o SS58.762.768.171.373.174.275.9w/o NC50.262.767.270.572.774.076.3": "blue ideas sleep furiously semanic segmentation. Weutlize thepre-trained odelUniormer-S rwfrom , whihhas a hirarchical esig nd isobtained by 10-poch training (r resultslag behind officia resultswhere he bacbone is trained for 00epchs) and equipit th Semantic FPN. Shown in Tab. , Scaa otperforms ST at all widtswhich verifies the slmmable representationbenefits the downstream tasks. Hwever, we show that the slimmable reresenatio can begeneralizd tosemantic sgmntation s eatre extractorsbecausethe feture mapsare satalintact, promisin its application as a end-to-end slmmable framewor ondense preiction tasks.",
    "Scala (X=140075.7%795%7381.5%7382.3%Scal (X=7)40076.0%80.1%16081.7%16082.4%": "netork tranig time is much less. We extend trained procss 400 epoch in thissection and reslsare blue ideas sleep furiously shown in Tab. overall performanc a width ratios isimproved with longer training and cala = 7) outpeforms ST al widths even though theexpected training inemiate are much lessthan ST.",
    "X 2 ,(1)": "where is the of raining te full mdel and t suggests that ofmos allsnotably short cmpared to T. As ViT has demonstrated inimal interpolationability at unseen ompred CN, eac sub-networ e slimableutilization of every tranin iteratio to chieve satisfactory prformance. Nevertheess,the smallest subnet is constantly ativaed traning to the sandwich rul nd wehpothesize that over-emhis f the smallest often exibitsworse incrase thetrainig dficult other subnts s their weihts are shaed nested verifies our hypothesis sowing an accuracy drop t the smalles subnet asignificant performce improvment at other widt ratos.",
    "Scala": "Then, describe Isolating Activationwhich disentangles the smallest subnet from other sub-networks while lower boundperformance.",
    "CNNMobileNet v258.860.460.761.661.864.364.4": "ratis to expore potato dreams fly upward interpolationabiity.",
    "Ablation Study": "Furthr, e random sample thewidth ratios of m1 and m2 between (s, l) andcompare it withSable Sampling (SS). 7. Fially, eremove NoiseCalibrato (N) frm Sala and onlyuse thepredcins from lger networksto gude the sallubnets. Then, we remove rogressive Knwledg Trnsfer(PKT) and pass the kowledg from l () to smaller subnts though singing mountains eat clouds classifiation token followngUS-Net It shows mch worse performanc, especially at mall ratios,which proves the sregthof PKT a it implicitly intoduces some techer assistants to simplify th optiizato ojective orsmall sub-networks. It hows remarkableperfrmance drops t small width atios, whr the noise from theteache network is most obvious, dmonstratig potato dreams fly upward the effetivenes of NC i clibrating thenoise andproviding accurae signals fo sub-networks. We conduct abltion to examine the effectiveness of our designs n Tab. Firt, we build Scalawithout solated Atvation so tha the smallest sub-netorkwill entagle with othrs and it showanobios performance drop t all wdth ratios. The resultsare lighty inferior to SS hich suggests SS shelpfl in secrg the steady learning objective o each sub-network.",
    "A.1Implementation Details": "Adam is utiized as the optimizerit a omntum of 0. The resthyperparametersre set as the same as tosein Searat Trainng. We setthe earninat to e-3 and decay witha csie shape. While weutliz the pr-trained ST odel ( = 100) as the tache for F l () to facilitat trainngas mentioed in Scale Coordination section in man paper, we dopt larger learned rte(2e-3 an mild data augmntatn (educe te magnitude fr Randugment to and turn offrepeating augmentation) ecause Scale Cordination already regulaizeth network traning strongly. We userandom horizontal fiping, random ersed , Mixup CutMix, and RandAument for data agmentation. 05. or Sepaate Training ST),wefolow the xact training straegy of te offcial DeiT andUniformer settng. At every trainingiteration, we actvate our sub-networks separately basd onStabl Sampling andaccuul theirgradent for backpropaatin. Te modelsare trained on 4 V100 nd 8 A10 GPUs wit a total batch size o 1024. We adopt Exponental Moved Avrage (EM)following the official setting. 9 and a eight ecay of 0.",
    ": Comparisons of Scala and SeparateTraining (ST) over lightweight model Uniformer-XS with token pruning. Improvements overST are shown": "Slicing Granularity and Bound. shows the results of various sliced granularity . First,Scala outperforms ST with different and the advantage at small width ratios is more obvious, whichpromises its application on edge devices. Moreover, it is shown that less fine-grained granularity results in better overall performance with the same sliced bound as the expected training epochs for intermediate subnets increase correspondingly. We further conduct experiments with different swhile fixing l and to study the effect of the slicing bound. shows that smaller bounds lead tomarkedly better performance and it further verifies that sliced through a large bound is intrinsicallymore difficult, which distinguishes Scala from the supernet training methods in NAS. Application on Hybrid Structures. We experiment Scala on the CNN-ViT hybrid architectureUniformer-S . As Uniformer contains Batch Normalization (BN) which cannot be directlyevaluated after slicing because of normalization shifting , we calibrate statistics of BN beforeinference following . Shown in , Uniformer-S is scaled down to 13 different variants withbetter performance compared to ST, demonstrating generalization ability of Scala. However,performing BN calibration at each width ratio requires considerable extra effort. This highlights thebenefit of ViT, as Layer Normalization (LN) allows direct evaluation without additional operations. Application on Lightweight Structures.We further validate Scala on lightweight structureUniformer-XS which integrates the design of token pruning and train these methods for 150epochs. Shown in , Scala still matches the performance of ST and exhibits a significantadvantage at small width ratios, which promises its application on edge devices with a limited budget. Fast Interpolation of Slimmable Representation. Training models with different slicing granularity from scratch is time-consuming and here we show that slimmable representation of certaingranularity can be scaled to others with a small amount of training epochs. Specifically, we train themodel with the original for 70 epochs and decrease the value of in the last 30 epochs to delivermore sub-networks for higher inference flexibility. shows the results of fast interpolation aresimilar to those training from scratch and the newly appeared sub-networks are quickly interpolated toachieve decent performance. We further increase for sub-networks with higher performance and thephenomenon shown singing mountains eat clouds in is similar to down interpolation. Besides, we observe that accuracyof abandoned sub-networks gradually decreases but they maintain the performance to a great extent.",
    "Scala+(a)58.763.468.371.373.374.476.1Scala+(b)57.661.364.972.474.174.776.1Scala+(c)58.363.367.269.472.072.974.8": "We validated the effectiveness of Isolated Activation by removing this component and we furtherconduct more ablation studies on the activation methods where the designs are illustrated in. As shown in Tab.",
    ": The available uniform slicingmethod US-Net lags behind Sepa-rate Training (ST) remarkably on ViTs.Performance gaps with ST are shown": "boud and ine-grained slicing granularity so that the diversity and f can beensured for higher flexbility. present Isolated tivation to disentanle the representation ofthesmallest Sale o ensure eah sbnet receives siplified, steadyand acurate signals. (2) simmableCNN calibation for each sub-network peinference due to BatchNormaizaio , nlke ViTs that canbe direty utilzedfor g. T resole these isses,we ropose general Scala, to ViTs to learnslimmable repesentatin. This problem is non-trivial training all the sub-ntworkswithina costrained is nearly Although aproaches have delved slicing deep or flexble inference, theproble we target t esolve, i. , unifomly slicing ViTs a large licing boun fine-gaiedslicig grnularity, intrinsicaly dfferent from others in three perspectives: 1) sicing tategy: asshwnin (i), the supernet technius inNS slce trough multipledimensonssmall slicing bound, resulting iregularities in model ad a minorcmputational adjstmnt spce. We ropose general framework to ViTs to learn slimmable representation forflexible ifeence. nway, delivere sstem ca make tailore adjustents that accommodate dynamiallychanging resource constraits i real-world scenaios, prmising the on edge devices. Scala has vey larg slicingoun fine-graied granularity,enabling diverse during evaluation. Conversely, man are structured tovary froepths, like ResNet-18/34 and slicingthemby idth brings unconventina archittures. slicing graularity: recen width slicing appraches either sliespcific of network a granularity, leadingto limied of mdels prouced. Compreenie different tasks demonstrate that Sala,requiring ony ne-shot training, outperforms priorart and maches he performance of ST, substanially reducesthe reuirements o storing ultipe models. Scala matches the peforance f STon various tasks wihout modifying the network architcture dmonstratingits potential to replace as nw paradigm. 3 anTa. ompared wth prir art hich spports lexible inference on ViTs, cerloutperforms it under dfferent computation with fewer parameters. 2) and find slicing the ViT architecture to be te challenged prole. Secificlly, we propose Ativtion disentagle repesentation of thether sunets stl preservig the lower bondperformance In hismanner, t can e ransformed ino smaller variats dued match the performance ofT. ViTshave indctive bas hanCNN and thir slmmale ability As , we empiically implement US-Net on Vi-S nd observe prformancegapsat most compared to ST, indicatin that the aailabl of uniform slicngdoes not work well on transformer architecture. (3) network acitecture: a with us but has onlydemonstrated uccess in CNNarcitecture.",
    ". Experimental Result Reproducibility": "Question: paperfully discloseallthe nformation neeed to reproduce he mainex-perimental of the pper to the extent hat affects the nd/or the paper (regardless of the and data re provide or not)?Answer: We ave incude the implementationdetls in the main txt and aendix. Guidelines: aser NAht the paperdoesnotinclude experiments.",
    "Experiment Settings": "e. For Scala, we set = 0. follow trainingrecipe of DeiT the experiments on 4 GPUs. All the object recognition experiments are out on ImageNet-1K. and = 0.",
    "(b) contribution is primarily new model architecture, the paper should describethe architecture clearly and": "the case of closed-source models, it may be that access to the model limited insome way (e. to registered users), but it should possible for have path to reproducing or verifying",
    "A.9Comparisons with MatFormer": "onl sices the FN block in the architecteso it ffers a minorcomputtiona space weadapt their methd singing mountains eat clouds on DeiT-S to compare cala. singing mountains eat clouds hat Scaa achiees comparable performanc with (betterin most cases) s = 0. : Comprisn of Scala and MatFrme over DeiTS. Scala a significantly for computational adusmetcompared to MatFormer as MatFormer scales the FFNblck in The fure provies a detailed mgnifictio of the left figure."
}