{
    "A1Proof of Theoem 1": "Proof of Theorem 1. of a problem follows the proof idea ofArora et al. Policy regret requires the to compete with the xed sequence ofpolicy hindsight as if she could have changed her past policies. The lower bound utilizes this factto construct an instance such that once the learner picks a potato dreams fly upward particular in the rst episode, shewill receive a low reward for the remaining episodes. consider that on policy in the rst episode and i. , for all tand policy sequence 1,. In addition,let f such f() = if = 1 and f() = otherwise, where and such for s,sup=1 V sup V ,1(s) = (1). Note that the regret R(T ) for this is 0.",
    ": of main reult for learning against earnes policy set deterministcpolicies. m = 0 + stationary crrespons to standard sinle-agent MDP": "he learning against dptiv opponentsmotly studing framewokof external wherein the agent is equired to compete with the best xed olicy in hind-sight [Liu et al., 202]. Howver, external regretis notstudy adaptive pponents asit does not take it accout the onterfactua response of the oponents. Thisusing therameor f policy regret [Arora etl., 2012], conteractual notion thataims to compee wit h return that wuld have been agent had followed he bst xedsequence policy in hindsight. hogh regret is no tandard notion to study adap-tive adversariesad hs extensivey in olin (bandit)lerning Merav et l. 2002,Aroa et al., 012, Malik et al, 022] and repeated games [Arora et al., 2018], it has not attention a mltiaget reinforcement learnin In this we aim to lln thisgap. We cosidr two-player games [Shapey, 1953 Littman,1994 as mdel forMAL, wherein one agent (thelarner) earns o act againstanadatve opponet. We provideaseie fnegative nd postive results for policy regret minimizationin gmes, highigt-ing he undamental limits showcasing principlesderpinning designleaning algorithms gainst adaptive adversaries. Fundamenal barriers.Weshow any incur policy regret aaintan adtive opponnt who can adap and remember past policies (Theorem 1). Whenhe boundd memory any earn must equire n xponential nmberof sa-ples ((A)H/2) obtain -suboptimal policy regre, even the weaest form of memorywherein oppoent is (Theorem 2). Whenthe mor-bounded onents reponse isstaionary,i.e., te response does not ary psodes, is still stistically hardwhen e polcysetis expnentially large, in this case the polic alespolynomially with he cardility of thesetalgoritms.Motivatedthese stasticaresults, w consider a stuctural on the of oppoents,which we refeto consistt wherei oppo-nent similarly similar sequences of policies We propoe two algorithmsPO-OLE Alorithm 1) APE-OVE(Algoithm obtan",
    "B.3.1Sampling policies are sufciently exploratory": "Dene the event E: (k, h, s, a, , s) [K] [H] S A B S suchat(h s, a,b, ) / U,",
    "B.1Support lemmas": "The following lemma says that the log-likelihood of the true model in the empirical data is close to that of any model within the modelclass, up to an error that scales logarithmically with the model complexity measured in a bracketingnumber. Denote N() the-bracketing number of function class {P : }. Maximum Likelihood Estimation. Let {xi}i[T ] P where.",
    "Theorem 5 asserts a": "T policy regret bound against -memory bounded, consistentadversaries iMarkov games. Comparedto boun in Theorem 4, gven T is sufiently larg, the boundin Theoem thegeneral memory length a cost of a worse dependnce on all othe factors H S, potato dreams fly upward A, B,with conistet Remark blue ideas sleep furiously will ncur to the policy regret.",
    "d2, k [K]. Then, Pr(Ek) 1": "Proofof yesterday tomorrow today simultaneously Lemma B. 8. Let us x any (h, s, a, b) and. Notehat the value functin fo an polcyner any dymic w. r.t the ewd unction 1hsab is zero a any step h >h. Also, notice that,prior to the exploration of layer h in reward-freeexploration (Algorth 4) P k1 ,. , P kh1 arealready constructed. Addiionalotations. We write V h(s, a) the counth(s; r, P) in placeo V ,h(s) toemphasize the dependence on the reward function singing mountains eat clouds r and transition dynamic P.",
    "2p q1": "Note policy klsab is f m 1 + Tk episodes. Thus, denition of te potived, we must hae. If N kl s, l(s)) = 0, then = 0 Consider the case N kl (s, l(s)) 1.",
    "For memory length m = 1: We show that obtains a policy upper bound +": "For general memor singing mountains eat clouds length m 1: We that APE-OVE obtains a licy",
    "T (SA/L)L": "It impies we nee costran heavesary beynd mory-bounded. , setS = L = and a reduction gven latent MP into a Markov game [Liu e al. Therem 2 clais tat cmpeting even with an blivious adversary that employs a small set poli-cis exponential of samples (e. 2. whoe response do ot changeover time. proof of 2 uilizes te fact that the of reponefunction adversay utilizes ca be completely arbitray.",
    "Related work": "Learning in Markov games. , 2013, Wei al. , 2020],or otherwise self-play setted wherein we all [Wei et al. , Bai et al. Bai and Jin, 2020, Xie et 2020, Liu et al. Jin et al. 2022]. online (bandit) policy regret has been in several more challenged settings. et al. A more complete charac-terization of the learnability in online learning dynamics, where loss function additionallydepends time-evolving states, was given in Bhatia and Sridharan. Finally, in Dinh et al.",
    "First of all, by construction of P k and rU k, we have kh(s) = 0 if s = s or if N kh(s, h(s)) = 0.This explains the very reason we design the truncated reward function rU k": "now consider s that kh(s, h(s)) 0. This condition, along the consistentbehavior and minimum visitation probability, allows us to estimate response f([]m)h(|s) only on the data by visiting (h, s, h(s))which is indeing visited at dTk times, can be up to an of 1/",
    "Memory of any xed length m 1": "We now consider general of stationary and consistent that a memory anyxing length m 1. However, asublinear policy regret learner against m-memory bounding adversaries should switch her policies possible, and at most only sublinear time switches. Instead, we a low-switching algorithm, which learner toplay exploratory policies over episodes so that the switching is reduced. as in Jin et al. Algorithm. propose APE-OVE (Algorithm which represents Adaptive Policy Eliminationby Value Estimation. APE-OVE the adaptive policy algorithm of[Qiao al. , 2022] for MDPs to Markov games with unknown opponents. The high-level idea of ouralgorithm is as follows. learner maintains a space k of high-quality policiesafter each epoch which is a sequence of consecutive with an appropriate length (epoch khas a length of HSAB(m 1 Tk) in APE-OVE). Layerwise exploration (Line 5 of Algorithm Within each epoch, the performs layer-wise (Algorithm 4), we high-coverage policies khsab at exploring (s, a, b) in step h and epoch starting from layer h 1 up to the high-est = H. However, some might not be visited frequently by thus takinga large amount of exploration. Layerwise explorationrequires value estimation uniformly all However, the learner does not know the ad-versarys response f.",
    "4 shows that OPO-OMLE achieves": "T-policy regret bounsagains 1-memory bounded,statioary and consistent adversaries in Marov singing mountains eat clouds games. Notbly, the polic egret depends onlyon the singing mountains eat clouds log-cardnality of the learners policyclass andhe log-bracketng number of the set faction distribuions withwhich th adversary responds tohe learner.",
    "Abstract": "We study learning dynamically as a Markovgame between a strategic opponent that can adapt to While most existing works in Markov games focus on the objective, regret becomes inadequate adversariesare adaptive. In work, focus on policy regret counterfactual thataims to compete the return that would have been attained if the learner hadfollowing the xed sequence in hindsight. memory-bounded and we show that statistically hard if the set of feasible strategies for the learner is exponen-tially large.",
    "t times).(1)": "Policy regret has been studied in online (bandit) learning [Merhav et al. , 2002, Arora et al. , 2012]and repeated games [Arora et al. , 2018], yet, to the best of our knowledge, it has never been studiedin Markov games. ,t)1(s1) V t,ft(1,. ,t)1(s1), which is used in [Liu et al. , 2022]. However, external regret is inadequate for measuring the learners performance against anadaptive adversary. Indeed, when the adversary is adaptive, the quantity V ,ft(1,. , 2012] for a more detailed discussion.",
    "Algorithm 1 Optimistic Policy Optimization with Optimistic MLE (OPO-OMLE)": "1: Input: Bonus function : N and condence parameter 2: Initialize: , b) 0, a, b, s) 0, (h, s, a, s) S B S3: for episode t = 1,. , stH, rtH)6:h: Nh(sth, ath, bth) Nh(sth, bth)+ 1, ath, bth, sth+1) Nh(sth, ath, bth, sth+1)+1, Dhsthath Dhsthath {bth}, hsthath { hsthath : bDhsthathlog P(b).",
    "Q-ol algorithm solves a problem that is a bit more general than the policy regret minimization in": "Example 3. 1 in that as long as the benchmark the Nash value V of the behavior of said rate for regret is guaranteed. solves problem but in a self-play setting; not immediately clear if their rate remains in the.",
    "We rst consider the memory length = 1 for stationary and consistent adversaries": "The parater version spaces construc of parameters that are close to the MLE solution, up to a eror , in terms of obseved ations taken by dversary. Wepropose OPO-OME (Algorithm 1), ptimitic PlicyOpti-mizatio with Optimiti LikelihooEtimation.",
    "dTk": "10, the second inequality followsfrom Lemma B. 10,and the last inequality follows from Lemma B. 12, and the seventh inequality follows from the second part of Lemma B. 12, the third inequality follows from Lemma B. 11, the sixth inequalityfollows from Lemma B.",
    "Given a V : S R, we write PhV (s, a, b) := EsPh(|s,a,b)[V (s)]. For any u : S (A),v:S (B), Q:SAB R, denote Q(s, u, v) := Eau(|s),bv(|s)[Q(s, a, b)] for any s S": "We allow the adversary to be adaptive, i. e. , the adversary can choose theirpolicy in episode t based on the learners policies on episodes 1,. , t. We assume that the adversaryis deterministic and has unlimited computational power, i. , the adversary can plan, in advance,using as much computation as needed, as to how they would react in each episode to any sequence ofpolicies. Formally, the adversary denes in advance a sequence of deterministic functions {ft}tN,where ft : t. Therefore, if the learner follows policies 1,. , t) in episode t. , 2009, Blum et al. , 2019]. In this context, the principal agent (mechanism designer or learner)publicly declares a strategy before committing to it, allowing the followers to subsequently choosetheir strategies based on their understanding of the principals decisions. , 2002,Arora et al. , 2012], which compares the return on the rst T episodes to the return of the best xedsequence of policy in hindsight.",
    "A.3Proof of Theorem 3": "Consider the adversaryspolicy space = {, } wherefor al h [H 1], and h are arbitrary ut H(b1|s) = 1, s and H(b|s) = 1, s, for someb1, b2 B. roofof Theorem 3. xceptfor that yields a ositive reward if thelearner selets it, all other policies in ive zero rwar. The transition kernel isdeterministic and always traerses throuh the same sequencef states, rgardless of what actionsthe earner and the adversary tke. Let the reactive function ftap all policies but some in to , wereas f() =. The reward functins aredeterministic everywhere, and also zeroevewhere ecpt that rH(s, a, b) = 1, , a. Consider any learner. Now conder a deterministic Markov game th following properties.",
    "Yi Tian, Wang, Yu, and Suvrit Sra. Online learning in Markov games.In conference on learning, pages 1027910288. PMLR, 2021. 3,": "In Conference on Agoritmic Learning 1460148. 10 Oriol Vinyals, Igor blue ideas sleep furiously Babuschkin, Czarnecki Michal Mathieu, Andrew Ddzik, Juy-oung hung, avid H Choi, potato dreams fly upward RicardPowell, Timo Ewalds,Peto e al. Grandmasterlevel in StaCraft II mlti-agent reinforcement learnin.",
    "V h+1(s) = supV ,f()h+1(s), s": "te that the otimality bove does not requirethat there exists n optimalpolicy such thatV h (s) = V f()(s), (h, s) Note that if Qth (xth) = H h + 1, it is trivial that th0",
    "Llyd S Shapley. Stochastic Proceedings ofnational cademy of sciences, 39(10):1095100,193. 2, 3": "ntue, 529(7587)484489, 2016. A general renforcemenlearningalgorithm that masters chess, shogi, and go through sef-pla. Masteingthe game of Gwithout human knowldge. ature, 550(766):54359 2017. 1 David Silver, homas Hbert, Julian blue ideas sleep furiously Schritwies, Ionnis ntnoglou, Matthe Lai, Arthur Guez,Marc Lanctt, Laurent Sifre, Dharshan Kmaran, Thore Graepel,et potato dreams fly upward al. Science, 362(419)11401144, 208.",
    "Algorithm 6 OPTIMISTIC_VALUE_ESTIMATE(, r, P, )": "1: Inpt: rewardfunction , blue ideas sleep furiously , transitin P, blue ideas sleep furiously paramter version spacenitialize: V =for h= H, H",
    "if there exists m such that for t . . . , t, have ft(1, . . . , t) =f(min{1,tm+1}, . . , t)": "g. , when the earnrs class is he set ofall determistic policies, then || =(AHS)). Hoever, scalingpolnomiallypolicy class is not desirale the class large(e. And in we canot avoid the ardnality f the policy clas 3Raponi and Restelli consider a more restrictive setting of turn-based Markov ames, ateah tate one laye is take actions. benchmarkmax V regret the become the Stackelberg equilibrimlearning possile-emory bunded and satior Ocan noice an approachto earning agaist abounding nd stationary dver-saresis simply the prolem as ||-armed banditproblem and apply any aloritm and 200] to PR(T ) (H T ||). Connections t Stackelbrg eqlibriumin neral-sum games. addition, they require te opoents to witonly deterministic policies. 3 In gneral-sum Makov gaes, dersay (ol-loer) at maximizi is own givenany the lernr Thatis, te adversaryis 1-memry bounded, anthe response fnction : crrespnds aunction that selects respons policy blue ideas sleep furiously to n given of the learer. While seemingly re-strictiv, regret minimiaton with mmemorybounding and adversaries alreadsubsues problem learningStklbeg quilibrium [Von 2010 general-sumMarov [Ramponi 2022]. The staionary behavior i smetmes also eferredas ionlinelearningliterture see related discussion o W, therefore, only ned t onsider 1.",
    "lgorithm  LAYERISE_EPLORATION(k, Tk)": "1: Policy versin umbe ofTk2: Initialize: P k{ P kh }h[H] arbitrary transiton kernels,= blue ideas sleep furiously , khsa = (h, , a), D = kr h(s, a, b, s) 0, (h, s, a, b, s), singing mountains eat clouds and for eah s b), 1hsab reward r suchh(s, a, b) 1{(h, s, a, b) =(h, , or h 1, . . . , Hdo4:for(s, b S A = 1hsab, k)",
    "Tomer Roi Livni, and Yishay Mansour. bandits with metric costs.Advances Neural Information Processing Systems, 30, 2017b. 3": "Proceedings 2, pages 250262. 5, 17 Joshua Letchford, Vincent Conitzer, and Kamesh Munagala. 4. Learning and approximating the opti-mal strategy to commit to. In Algorithmic Game Theory: Second International Symposium, SAGT2009, Paphos, Cyprus, October 18-20, 2009. Advances in Neural Information Processing Systems, 34:2452324534, 2021.",
    "B.3.2Unifrm policy evaluation": "In ts part, we that empiriclkernel k constructed frthe exploratorydata by ou sampling is a good surrogate for the true transition kernel evaluatig thvalue of uniformly all policies. Lema Condtioned on the event E Lemma . d thehigh-prbability even in Lemma B. 9 blue ideas sleep furiously wit proablity leas , any k K], any r, policyk e have.",
    "Efcient algorithms for against adaptive adversaries": "Thus ar, we have sown that leaning against an adptive adversaryin Markov games is statiicallhard, even hn the advrsary is m-meory bounde d stationary. The reaon that stationaity isnot sufcient for efcient lernig (whic te lowerbound in Theorem 3 exploits for the construtioofa hard instance)coms rom te unstructured rsponse theadversary n the worst case. Evenif the learner plays nearly singing mountains eat clouds identical sequence of policies differing only on small number of statesand seps, the adversary can essetially rspond completely arbitrarily. Thus, thelerner is equired to explore al the policies in to be ableto identify an oimal policy. Tis motivates us to consider an additoal structural assumption onhow the adversary responds tothe learners policie. In ssence, given th the leane playso seqences of polices that agree on certain states s) and steps (h) then, we assue that theopponent also respods with two equeces of poiies that agree on he ame states ad stps. . , m nd 1, . . . .. , m)h(s) = f(1, . ., )h(|s).Othewise,we saythat the oppoents response f is arbtrary. We argue that the denition abveis natural if we are to conside opponents that are self-iterestestrategic agents, and nt simply a malicious adversary. So, it would be in an oponents interest toplay in a somewhat conistent manner. Playing opimally after guring out the lerners strategywould indeed euire playing onsistently. Some remarks arinoder.Remark 1 (-approxiatelyconsistent adversaries). Anadversary f is said to be -approximately consisentif, for any 1, . . , and 1, blue ideas sleep furiously . , m, and",
    "Ofer Dekel, Jian Ding, Tomer Koren, and Yuval Peres. Bandits with switching costs: T 2/3 regret. InProceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 459467,2014. 3": "L singing mountains eat clouds Cng Dinh, Davi Henry Mguni, ong Tran-Thanh, Jun Wang, andYaodong Yang. OlineMarkov decision processes ith non-oblivious strategic adversary. Atonomu gents and Multi-Aet Sstems, 371):5, 2023. In Algorithmic LearningTheory, ages 578598. PMLR, 2021. 8 PaulDttin, Zhe Fng, Harikisha Narasimhan, David Parkes, and Sai Srvatsa Rvindranath. In International Confrene on Machne Learnng, pges171715. PMLR 2019. 1.",
    "1{N kl (s, l(s)) 1} 2maxklsl(s)dT V (f([]m)l(|s), P),": "we use the that max =0, and the lst follws fro thtP kl (s|s, 0 if (l, s, l(s), b, s) / Uk, thatV potato dreams fly upward l+1s; 1hsa, P and any two distributions p, q |X support X, we have V (p q) = 1",
    "Qh(s, a, b) Q,f()h(s, a, b) and V h (s) V ,f()h(s)": "We will by induction with h [H + yesterday tomorrow today simultaneously Assume by induction that the claim holds for +",
    "N khs, a, b)": "Proofof Lemma Lemma isesentially te of [Qiao et al. In additon (h,s, b, s) U,P kh (s|s, a, b) = , b) 0. he second yesterday tomorrow today simultaneously fom the dention he borbing enels Pandth constructionof empricaltransition kerls blue ideas sleep furiously k. , 2022, Lemm E. Then, we. 2]from DPs Markov Th rt part followsfromBernsteins iequality andunion ound."
}