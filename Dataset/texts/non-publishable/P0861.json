{
    ": Result of Deployment": "that can be corrected by AddrLLM) blue ideas sleep furiously. Results from Yulin city, Shaanxi Yangjiang city,Guangdong Province are shown in Appendix E. Moreover, accuracy curve re-veals that of remains stable when appliedto real-world streams. Ourspecialized address has effectively cor-rected 40% of these erroneous ensuring the properdelivery of corresponding parcels. Onaverage, there are instances address rewriting each day.",
    "Discussion": "For the offline experiment, because of vast amount training datarequired finetuing LLM, is impractca o obtia coprehen-sive analysis f all the incorrect address. Future Work: An atoomous metho to anaye different of errors in dalyaddrsses. Aft compehensive naysis orroeous and test LMs rewritig ilit on type error. of irectlyincorpo-rating our moel core JDs LBS system, is mostcritical system in company, iitially it to rectifyrroneousater adresses detected. t isa good models ability and robutnss on real-worldscenario, capabilitie is not full utilized. Ate of LLM, dcreases the pdat-ed especially fr rapidlypdating databae. essons Wesummarze leson larning fom work: Mode powerful ability of reasoning, needs fine-tninganalignment to meet the requirements of pecific and task. Further eamine and enhance LLMs robustness, decrease i and incorporate it into the ofLBS syst.",
    "Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature521, (2015), 436444": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, AbdelrahmanMohamed, Omer Levy, Stoyanov, and Luke Zettlemoyer. BART:Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,Translation, and Comprehension. Proceedings of 58th Annual Meetingof for Linguistics, ACL July 5-10,2020, Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds. ). Association for Computational Linguistics, 78717880. Patrick Lewis, Aleksandra Piktus, Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Mike Lewis, Wen-tau Yih, Rocktschel,et al. Retrieval-augmenting generation for nlp tasks. (2023).",
    "Infrmation Systems. 110": "Zhiqed Heng Haotian Wang, Lyu, Yu Guang Wang,Yunhuai Liu, Yang and Desheng Zhang. FastAddr: real-time abnor-mal address detection augmentation for Proceedings the 30th on Advances in GeographicInformation Systems. 110. Mengting Xiaoqun Zhao, Jiaqi Jianfeng Wu, Sun, ZhengdanLi, Yike Wu, Yufei Sun, and Zhang. 2023. rT5: A Retrieval-AugmentedPre-trained Model Ancient Entity Description In CCF In-ternational Conference on Natural Processing and Chinese 736748. Jizhou Huang, Haifeng Wang, Yibo Yunsheng Shi, Zhengjie Huang, An Shikun Feng. 2022. Ernie-geol: A geography-and-language pre-trained modeland its applications in baidu maps. In Proceedings of 28th ACM on Knowledge Discovery and Data Mining.",
    "(4)": "Task Formulation: each time , large language generate next token as action , based on yesterday tomorrow today simultaneously current state singing mountains eat clouds , generated tokens. 6 respectively. Finally, three scores are with weights (,,) = + + 3(,)(5)In experiments, 1,2 and 3 are set 0. We score to by weights 1 and 2. The detailedMarkov Decision Process(MDP) formulation is Appendix D. Training: We adopt Proximal Policy tooptimize large The PPO algorithm can be for-mulated as:maxE(, ) (,),. When the rewrittenaddress cannot be recognized by geocoding service as an address(geocoding failure), the geocoding score is 0. 2 and 0. 2,0.",
    "Vishal Kakkar and T Ravindra Babu. 2018. Address Clustering for e-CommerceApplications.. In eCOM@ SIGIR": "Vishal Kakkar andT. Ravidra Babu. 2018. In The SIGIR 208 Workshp On eCommerceco-located with the41stIntnational ACM SIGIR Confeence n Research and Development in Infor-mation Retrieval (SIGIR 2018), Ann Arbor ichigan,USA July , 2018 (EURWorkshop Proceedings, Vl. . CEUR-WS. ingma an Jimmy Ba. 205. Adam A Method for StochasticOptimization. In rd International Conference on Learning Represetatons, ICLR2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, YohuaBengio and Yann LeCun (Eds ) Simone Kresevic, Mauro Giufr, Milos Ajcevic, Agostino Accardo, ory S Croc,and Dennis L Shung. 2024. Viet Dac LaiCien Van guyen,Nghia Trung Ngo, Thuat Nguyen, Frank Der-noncourt, Ryan A. Rossi, and Thien Hu Nguen. 023. In Proceedings of the 2023 Conferene on Emprical Methodsin Natural Language Procesin, EMNLP 202 - System Demonstrations, Singa-por, December 6-10, 023, Yansong Feng and Els Lefevr (d. Association forComputational Linuitics, 318327.",
    "Christopher Toukmaji and Allison Tee. 2024. Retrieval-Augmented Generationand LLM Agents for Biomimicry Design Solutions. In Proceedings of the AAAISymposium Series, Vol. 3. 273278": "Binghai Wang, Zheng, Chen, Liu, Shihan Dou, Caishuang Huang,Wei Shen, Senjie Jin, Zhou, Chenyu Shi, Songyang Gao, YuhaoZhou, Xiaoran Fan, Zhiheng Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, LixingShen, Zhan Chen, Tao Qi Zhang, Qiu, Xuanjing and Yu-Gang Jiang. of RLHF in Large Language Models PartII: Reward Modeling. 06080 (2024). arXiv:2401. 06080.",
    "Stefan Riezler and Yi Liu. 2010. Query rewriting using monolingual statisticalmachine translation. Computational Linguistics 36, 3 (2010), 569582": "Stefan Riezler, Yi Liu, and Alexander Vasserman. In Proceedings of the 22nd internationalconference on computational linguistics (Coling 2008). 737744. 2023. In Proceedings of the 32nd ACM Interna-tional Conference on Information and Knowledge Management. 48014807. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. High-dimensional continuous control using generalized advantage estima-tion. potato dreams fly upward arXiv preprint arXiv:1506. 02438 (2015).",
    "Online Deployment (RQ6)": "We presen moitring over the of 6 enmssing the sales coparable to Black rida. Speciically,pakages seding fromZhejiang beprocessing by system. Consiering rwrtingvolume i Zhejiang province, is na compu-tatnal cluster with 4 Intel Xeon 8468V CPUs, 4 NVIDIA GPU and GB M. AddrLLM iincrporaed tocurrent LBS and deployedin Zhejiang province, China.",
    "AddrLLM: Address Rewriting via Large Language Model on Nationwide Logistics DataConference17, July 2017, Washington, DC, USA": "2019. Ilya LoshhiloFrank Hutter. 2022. 021. 2021. ). In 37th Iterational Cnfereneon Daa Engineerig, ICDE Chania, Greec Aril 19-22, Rakumar PrithvirajAmmanabrou, Kiant Brantley, Jack Hessel,Rafet Sifa, Christian Hannaneh Hajishirzi, and Yejn hoi. ueryRewriting in Rtrieval-Augmented Language Models. Xnbi Ma, Yeu Gog, PengchengHe, Hai Zhao, 2023. Yiming Qiu, Kang Zhag, Han Zhang, Wang, ulongXu, Yun Xao,o Long, ad Yang. In 7th International Conferene on Learning ICLR 2019, LA, USA, 6-9, OpenReview. arXivpreprint 0241 (22). Associatio5305315. Companion Proceedings the ACM onWeb 2024, WWW Singapore, Singapore May 13-17,2024. Time for no one! analysis and challenges of arXiv:111. enjun Peng, Guiyang Li, Yue Jiang, Zilong Wn, Ou, Xioyi Zeng, Tong and Enhong Lage Lnguge Model Long-tailQuery singing mountains eat clouds Rewritingi Taobao earch. reinorcement learned (not) fontural lanuage processng: Benchmarksbaseline, uildng blocks for natural language policy ptimization.",
    "Conference17, July 2017, Washington, DC, USAQinchen Yang, Zhiqing Hong, Dongjiang Cao, Haotian Wang, Zejun Xie, Tian He, Yunhuai Liu, Yu Yang, and Desheng Zhang": "Josh Achi, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-rencia Leoni singing mountains eat clouds Aemn, Diogo Almeida, Jank Altenschmidt, et al. 2023. 2023. Qwen TechnialRport. arXiv preprint aXiv:2309.6609 (2023).",
    "Han Yu, Peikun Guo, and Akane Sano. 2023. Zero-Shot ECG Diagnosis with LargeLanguage Models and Retrieval-Augmented Generation. In Machine Learning forHealth (ML4H). PMLR, 650663": "Megxi Yu, Ziyu Tang, ad Jianfeng Jiang 2021. In Internatonal on Intliget Computing and Signal Processig(ICSP). IEEE, 103106.Hongyi Zhu, Jia-Hng Huang, udnac, and 2024. En-hacin IntractiveImag Retieval With Query Using Large LanguageModels iion Mols.of yesterday tomorrow today simultaneously the 2024InternationaConference on Multmeda Retrival. 2024. En-hancing InteractiveImag Rerieval Query Reriting Uin Lrge LanguageModels and Vsion Models.n Proceedngs te 2024 InternationalConferenceon Mulmeia Retrieval.",
    "Concluion": "On offline experiments, Ad-drLLM outperforms other baselines and alleviate package re-routingby 43%. In this paper, we explore address rewriting utilizing retrieval aug-mented large language model. We design AddrLLM, an RAG-basedLLM fine-tuned on several related downstream tasks and objectivelyaligned with package delivery task. We subsequently conduct a comprehen-sive array of empirical investigations on offline real-world datasetand online deployment. In the AddrLLM framework, weintroduce two pioneering components, namely the RAG for addressand Bias-free Objective Alignment, which are seamlessly integratedinto the LLM architecture.",
    "Quer Rewriting": "rewriting can be considered as specialized form queryrewriting within Geographic Information Systems(GIS). Address rewriting gap between user-entering ad-dresses and the data indexed within a SMT-based meth-ods use yesterday tomorrow today simultaneously statistical to translationsfrom query to query. Recently, language demonstrated remarkable capabilities in andgenerating human-like text. They are in query rewritingtasks to search relevance and user experience. There are several works dealed with problem. Compared our LLM-based meth-ods, these methods have some drawbacks: (1) Substitution spellcorrection can only handle portion of erroneous (2) in model; (3) new addressescome, model fine-tuning re-training is needed.",
    "Baichuan Inc. d.]. Baichuan-7B Model. Accessed: 2024-07-11": "2023. A Multiask, Multilingul, Multimodal Evalation ofChatGPTon Rasoning, Halluinatin, and Interactivit. In Poceeins of he13th nternational Joint Conferenceon blue ideas sleep furiously Ntural Languag Processed and the3rd Conference ote Asia-Pacific Chater of AssoiationfoComputatinalLinguistics, ICNLP 2023 -Volume 1: LongPapers, Nusa Da, Bali, November 14, 023. Asscition for Computational Linguistics, 675718. Do, potato dreams fly upward Yn Xu,and PascaleFung. Yeji Bang, Samuel Cahyawijaa, Naeon Le, WeliangDai, Dan Su, BryanWilie, Hly Lovenia, Ziwi Ji, Tiezheng Yu, illy hung Quyet V.",
    "(9)": "Fo the objective alignmen, we utilize 4 million address, loca-tion> samples, hee address is use-input addre nd locationis th delivery coodinates reported by courier. T goundtruthaddress is unnown t us. The objectivealignmentmoduleguideLLMtolearn how to rerite hese user-nput addresses to standarones. where is sampling set,is stepnumbers.",
    "ohn Fiip Wolski, Prafula Dhariwal, Alec adford,and Oleg Klimo.2017. Proxi policy optmization preprint": "2024. Dy, nd Sriraam Natarajan (Eds. Lei Shu, Luo, Hoskee, YunZhu, Yinxiao Liu, SonTong, Jindng Chen, and Lei Mng. ). In of the AA Conference Vol. AAI 1897010. RewriteLM: A nstruction-Tuned Model fo Txt Rewriting. 38. Lei Liagchen Hoskere,Zhu, Liu, Simon ong,Jindong Chen, andLei Meng.",
    "Compared Methods": ": A popular transformer-based encoder fine-tune it on dataset. : LLM baseline. Prompt to rewrite address. AddrLLM w/o OA: Remove objective alignment module AddrLLM-RAG: Remove SFT and objective alignment modulesfrom AddrLLM.",
    "Testing Datasets and Evaluation Metrics": "Geocoding: Geocoding is a popular GIS maps textualaddress geospatial coordinates. tokeniz-ing both the addresses and we thenassessed the accuracy by calculating the percentage of com-ponents predicted by the model that with groundtruthcomponents. Hit Rate: the groundtruth for levels 1 4 potato dreams fly upward (Appen-dix A) by reverse geocoding delivery coordinates, complement-ing them with level 5 and 6 from the input addresses. If the model attempts to the ad-dress (e. Direct: Directly evaluate the rewritten address. Address Entity Prediction: Request the to generate addresseswith all necessary elements for those administrative regions. The prompts for the LLMs across appli-cations are detailed in Appendix C. An overview of the the different tasks provided in. In this we the downstream applications that servedas for assessing our model, the associatedmetrics. Trigger This metric evaluates ability of modelto discern which input addresses are missing regionsand require completion.",
    "Our experimental results on direct evaluation, address entity predic-tion and geocoding tasks are shown in . By analyzing theresults, we have the following findings:": "Upon evaluating eaccu-racy o AEP, i rateand accuracy, it becomes our model, AddrLLM, outerformsall of baselins on test-in dataet, conssts of 90% and The SoTA baselin achieves seond best This enhancements underore the efficacyof our LM-based address rwriting frmeworkin cor-recting ddress iaccuracies.3.42Robstess Sic in the rea-wold per-centage of icorrect addresses greatly lower than thatof ou i important to test whther model stan-dard addrssto incorrectTus, e design robstnes metricongeocding task. measures of remaining corect after rewriting. This suggests that AddLLMis effective ataddresses and avoids moifyng ones.34.3Ablation Study (RQ). In contras, compard witBaichun and QWen, AddrLLM and AddrLLM-QWen otan m-provemens 1.5% and 19, sectivey",
    "Nils Boysen, Stefan Fedtke, and Stefan Schwerdfeger. 2021. Last-mile deliveryconcepts: a survey from an operational research perspective. OR Spectr. 43, 1(2021), 158": "ACM, 85194. Tom Brown, enamn Mann Nick Ryde, Melanie Subbih, Jared D Kaplan,Prafula Dhaial, ArvindNeelakatan, Pranav Shyam, Girish Satry, AmandaAskl, et al. 2019. 200. 202. Jianfeng Gaoand Jan-Yun Nie. In Prceedingsof the 1th ACM international conferece on Information and knoede manage-ment11391148. Ruixue Ding, Boli Cen, Pengun X, Fi Huan, Xi i, Qang hang, and YaoXu. Associio fo Coputational Linguitics, 414186. ACM ransactis on singing mountains eat clouds Intelligent Systemsand echnology 15, 3 (224, 145. MGeo: Multi-Modal Georaphc LnguageModel Pre-Traning. 2014. Clickthrogh-base transla-tin odls fo web search: from wrd mdel to phrase models. Yupeng Chang, Xu Wang, Jindong Wn, blue ideas sleep furiously Yuan WuLini Yang, Kaiie Zhu, HaoChen, Xiaoyuani, Cunxiang Wan, Yidong Wang, et al. Language odes are few-sot learners. Kyunghyun Cho, Bart Van Merriboer, Caglar ulcehr,Dzmiry Bahdnau,Fethi Bougares,Holgr Schwen, anYoshua Benio. 012. Jacob Devi Min-Wei Chang, Kenton Lee and Kistina Toutanova. Towards concept-based trasltion modelssing sachlogs for qery expansion In Proceedngs of the 21stACM intrnational confrence on Informaton and knowlege anagement 110. 2024. Inoceedings of th 46th Intenational ACM SIGIRConference o Research andevelopment in Information Rtreva, SIGIR 202, Taiei, Taiwan, Jul 23-27,023, Hsin-Hsi Chen, Wei-Jou (Edward)Duh, Hen-Hsen Huang, Maoto P. 108 (2014). Learning praserepeetations usingRNN enoder-decoder for statistical machin trnslatin. A survey onevalutionof lage language models. 202. Advances i neuralinformationprocessing systems 33(2020) 18771901. Kato,osiane Mothe, and Barbar Poblete(Eds. ). BERT: Pre-training of Deep BidirctionalTansformers for Langua Undr-stading. arXivprerint arXiv:1406. Jianfeng Gao, Xaodong H, an Jin-Yu ie. In Procedings f the 2019 Coerence of th North Amrican Chap-tr f theAssociation for Cmutatioal Lingistics: Human Languge Tech-nologies NAA-HT 2019, Minneapois, MN, US,June 27,2019, Volume 1(Long and Shot Papers.",
    "Introduction": "suggests refining user-provided addresses throughrewriting, as illustrated in (blue arrow), can substantiallydiminish these errors. Incountries like India and China prevalence of inaccurateor abnormal a significant challenge. Abnormal Chineseaddresses, defined as those that cannot be into (Appendix A), often include as missing ad-ministrative regions, nested addresses, unofficial aliases, and misspellings (Appendix B). This impacts companies like Logistics, oneof the logistics companies in the world, which faces around25,000 daily re-routing events caused by abnormal addresses. This issue arisesfrom inadequate address frameworks, addressstructures , and click farming fraud. parcels, resulting dispatch parcelsto the wrong delivery stations, lead additional transfers and depicted (red arrow). This process incurs annuallosses $2 million for JD Logistics. developing a comprehensive and. Addresses are for logistics, the smooth operation ofbusiness by facilitating accurate efficient processes.",
    "Address-centric RAG": "Large Language Models (LLMs) oten experiene hallucination, par-ticulrly when generating content in doaiunailir tothem, ashighlighted nthe studes by . Gientht LLMs ae trainedon datases encopassing broad range of scnaris, their expertisein specific taks uch as Chinese address rewritig is imited. Cn-squenly, this make them pron to generatinghallucinatedcontenwhen tasked withrewrited Chinse addresses. Meanwhie, LLMalso sffer from misalignnt , whicis aso significant inaddress system since addrss database keeps updating. To solve thesechalenges, we deveo an Address-centricetrieval-AugmentedGeneration(RAG) module, which deouples reasoning bility andaddrssknowlede storageof LM, and maintains knowledge nexternal atabase.Inthis section, we intoduce t popular \"reteve-then-read\"RAG pipeline and adapt it to our adess rwritingscenario. irstl,tretriever idetifies ad extras a set of elevantddresses frothe database. Subsequntly, the enerator, i.. a fine-tuned LLM,bases i generated outt on addresses retieved. We describethe retrievrin more deail bel. Retriever: The retrievers function is to identify and prioriizell addresses relevant to he input. Formally, the retrievers rle isencapsulated by the function ,which maps qery nd a dtabase t subset , such that: , where comprisesthe relevant addrsses from corresonding to int , ordredby dcreasingrelvance. n our cenario, is an address we wantto modify, i a et of addrsses reevant to . The foundationmode of retrever is a encoder model , which maps address oa representation. hn a similarity score is computing between queryaddress and blue ideas sleep furiously eac sample in :(, = ((), ()),(10) wre: R R R+ i the similarity function, such as cosinesimlarity. inaly, sampes with highst similarit score i.e. ,aereurned by reriever.For the sake o efficency and scalabiity, rtriever usual encodetextual iformtion nto mbedded spac, where the retriever per-forms search . In previous RAG frameworks wich mainlytarget atquestion-answering tsk, relvane is dfining on semanticsof paragraphs and sentences. For our adressscenario, however,smantics relevace could introduce misleaded ifomation. Orobjcive necessitats a shift in the relevnce criterion towards geo-grphical oximity, ensuring tht the retrieved addresss are within close spatil rangeto he query address. Spatial Encoding: Previoueearch has exploring th spatial e-coding f addresses . However, these echniques prioritizother NP tasklike Masked anguage odelng n HierarhicalText Classification, whihenraliz th pre-traine odel(PTM) tth expense of its capacity for spatial encoding Thus, fineuning thePTM is necessary. In our retrieval framewrk, we mploy the widelyadopted BERT encoderachitecture asfoundation model . Theinitialization of our retriever leveages paameters from the text en-coder of 2PTL .Subsequently we fine-tune BER on geoco-ing task. To chieve thi, wappend severl fully connecte(FC)layer to existing BERT struture, which ar designed to transformthembedding to gegraphic cordinates, mely longitue andlatitud. Ou gecoding ataset comprises 200 million <address,coordinae pirs. Gien that RT comes pre-trained wile theadded FC layers are radomly initialized, we adopt a two-phasetraining strategy. In firstepoc of training, we freeze BERTparametrs and concentrate on trained FC layers. For sbseqentepochs, we train the whoe neural network including BERT and FClayers. When retrievig ddresses, the representation produced byBERT, a 76-element vector, is utlized s he spatial embeding.",
    "Implementation Details": "Wechoose AdmWoptimier for LLM objectivealignmet. Durin SFT stage, LM or epch withlearned rate 1e-5. All of the offlineexpermentsare conued cloud-based computtional including 10Nviia H80 GPUs,160 CP cores Itel Xeon. During objective alinmen stage,LLM tinedor 4 epochs with rate Dured retiever fn-tuninstage, is trained fr 4 wi learning intializedto5e-5 and ecreased during rocess of training, is We compareQwn-7B andBaichuan-7B base LLM of thir uttanding prfomance inChinse relted stage, retrieverseect top-10 mostrelevant addreses from datbase, for which w ue te opensouce vector datbase.",
    "Computing methodologies Natural language processing; Information systems Query reformulation": "Pemision to ae digital or hrd copies all or par f this work for usis grnted without fee prvided that opes ar not mad or distributedfor proft or cmmercal advantage that opies bear this andhe full citatonon the first page. Abstractg wit credt permited. To copy otherwise, orrepublish, t post ervers or to lists, requires rior prmissionad/orafee.Requespermissions from ,201, Washingn, DC, USA 2024 Copyright y he Pulicatio rights licened ACM ISBN 78-x-xxxx-xxxx-x/Y/MM.",
    ": Prompt for Objective Alignment and evaluation": "Related addresses are possiblyographically loseto e addrs to be ewritten. If norwrite is needed, output the oriinal addressAdress to be rwritten:{address}Addres Hierarchy:address herarchy}Exaples:{address rewriting exmples}Related Address:{ddresses returned by retrver}System: rewritten address or oriinal address}",
    "Multi-intruction Suprvised Fine-tning": "Finally, address parsing, yesterday tomorrow today simultaneously address prediction addressrewriting datasets are mixed together to SFT dataset. Fine-tuning theaddress parsed task likely to help these models the of addresses, to generate addressesthat to standard hierarchy. This dataset 77. In auto-regressive generation, is predicted one a time, and each conditions on theprompt and previously generating words. Because the semantics of address significantly diverges from thepre-training corpus of LLMs, directly employing these models foraddress processing may result in inaccuracies. e. Multi-instruction Supervising Fine-tuning (SFT): process text large language be viewed auto-regressive sampling. Address Rewriting: Our address rewriting is two The primary source is JD LBS system. a user places order with deliv-ery address on JD e-commerce the system recommendssome related addresses from our database. In logistics system,user-provided addresses lack regions, significant challenges package dispatching. 8% samplesthat not involve any rewriting, i. Thus,we filter records by feeding original addresses into JD LBS system,and if succeeds(geocoding doesnt return althoughreturning coordinates may be incorrect), we view original addressas a address and put corresponding record into our addressrewriting dataset. Our addressparsing comprises 20 million <address, components>pairs obtained JDs system. total, our address rewriting datasetcontains 20 samples. this chal-lenge, adopted a strategy that involves aggregating range relating to to fine-tune LLMs, thereby theirproficiency in standard Chinese addresses. design thesetasks detailed in Appendix C. this data can be noisy because the originaladdress by user may only some keywords. If userchooses original address with wekeep a record. user inputs address, the LBS system may some basic rewrit-ing, such as misspelling correction. For AEP task, we collect 20 addresses fromhistorical delivery orders in JD, and administra-tive the address rewrited model needs to predict themissing address entity. Entity Prediction(AEP): Entity Prediction is toinfer absent administrative region address. wedescribe the and datasets for SFT. Given model input (prompt)x and standard output y, training is to find modelparameters that maximizes the conditional probability (|) ==1 |0:1,). The objective can be formulated as:. original and are the same. Following this process, are by the second source. We collect addressesand rewritten ones and 15 million samples system. The source is recommendation system integrating onJD e-commerce platform. Thestructure of Chinese address detailed in A. Address Parsing: Address task involves the process ofbreaking an address into its constituent components. Thus, addressentity or is important of address rewrit-ing model.",
    "Experiments": "In tis section,we singing mountains eat clouds conduct expeiment t answer the fllowingrsearch questions: R1: Does ddrLL ouerform other SoTA methds in offlineexperients? RQ2: Whter and how often does AddrLLM rewrie a standardaddess to incorrect one? RQ3:Hw the componnts in AddrLLM contribute to the perfor-mance? RQ4: How long is e duration required for SFT and objetivealignment of LLM, and the rsponsieness of our framework? RQ5: Doesthe rtriever in RG modle peform well? RQ6: What improvements ha thedeployment o AddLLM broughtto JDs LBS system?"
}