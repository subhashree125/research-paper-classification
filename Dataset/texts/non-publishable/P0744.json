{
    "Video-LLaVA7B70.73.959.23.570.04.045.33.3": ", 2019); POPE (Li et al. 2018); SQAI: SinceQA-IMG (Lu et al. , 2017); GA (Hudso andanning, 2019); VisWiz (Gurari et a. enchmark names are abbreviated due to page limitations. , 2023d); MB: MMBench (iu et al. yesterday tomorrow today simultaneously VQA-v2 (Goal et al. : Comparison beween different LLMs on image understanded benchmarks. ,2022);VQAT: TextVQA (Singhet al. , 023c); LLaVAW: LLaVA-Benh (-the-Wil)(Liu et l , 2023b); MM-Vet (Yu et al. , 03).",
    "Chenfei Wu,Shengming Yin,Weizhen Qi,Xi-aodong Wang, Zecheng Tang, and Nan Duan.2023. Visual chatgpt: Talking, drawing and edit-ing with visual foundation models. arXiv preprintarXiv:2303.04671": "2016. In Proceedings the con-ference on computer vision pattern 52885296. Msr-vtt: large video description dataset for bridgingvideo and language. 14178. Qinghao Ye, Xu, Guohai Jiabo Ye,Med Yan, Yiyang Zhou, Junyang Wang, An-wen Hu, Pengcheng Shi, Yaya Shi, et arXiv preprintarXiv:2304.",
    "Introduction": "Recntly, LLMs have gained rapid poplarity in theAI ommunity such as GPT-3. 5, GPT-4 (OpnAI,2023), PaLM (Bi et al , 2020 Ani et al. , 2023),and BLOM (Sc et a. ,2022). They rely onthirpowefullanguage cmpehension abilitiesto olow human-providd instuctions nd pro-vide correspondigresponses.",
    "Zero-shot Image Question-answering": "Yesindicates the proportion of positive responses to given question. donates that we reproduce LLaVA-1.",
    "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Tong Xu, and Enhong Chen. 2023. onmultimodal large models. arXiv preprintarXiv:2306.13549": "Weiha Zhnguan Yang, injie L, Jianfeng Wng,Kevin L,Liu, Xinchao Wang, LuaWang. Mm-vet: singing mountains eat clouds Evaluated multimodalmodels for interated cpabilities. rXiv preprinarXiv:238. 2490. Proceedings of th AAAI Cn-ference o Artificial 33, pages912794.",
    "ion] What does the woman use to split the logs and how does": "In video we see man walkig down  rack while hlding a He runsdown the track and the aelin. The ide then the back starting point, and we the avelin landing n the The man hen walks away rom the The ppears to be well-maintained, the surroudigs seem to be quiet and peaceful. Th man'sform and technique areiprese, it is learthat has experience in sport. Overall, te video provides a glimpse into the world of hrowing and the dedication skill reqired to excelin the rt.",
    "Haotian Liu, Chunyuan Li, Qingyang and Yong JaeLee. Visual instruction tuning. preprintarXiv:2304.08485": "Macaw-llm: Multi-modallanguage modeling with image, andtext integration. 2022. 09093. your multi-modal model an all-aroundplayer? arXiv preprint arXiv:2307. 2023. 07207. Lyu, Minghao Wu, Longyue Wang, XintingHuang, Bingshuai Liu, Zefeng Du, Shuming Shi,and Zhaopeng Tu. 2023. Yuan Haodong Duan, Yuanhan Zhang, Bo Li,Songyang Zhang, Wangbo Yike Yuan, JiaqiWang, Conghui He, Ziwei Liu, al. Valley: assistant model ability. Pan Lu, Mishra, Tanglin Xia, Liang Chang, Zhu, Oyvind Tafjord, PeterClark, Kalyan.",
    "lignment Before Projection": "WhieCLIP-L14 exhibits strongmultimodalunerstandng capabiliies, it not pre-algned wihthe video encoer. Consequently, thisresults in a lack of unforiy ithe vsalfeaturesprovid potato dreams fly upward to LLM. Wedefine use of image encoder yesterday tomorrow today simultaneously asuified visual MAE en-coder (He et , 222) use separted a well-known and effectiveima featur Additionally,since MAEdo ith inputs dring thetrained process, weutilize CIPL/14, mode ofthesame size.",
    "For comparing performance on video benchmarks,we remove image data during the training of Video-LLaVA, which is called Video-LLaVA. We com-": "In , we valate our model onfur ideo qestion-answerng toVideo-LLaVA wthout image in model trained wit join images nd videosahieves comprehensie improvements alfour results of images and videos cilitesLLMs understanding of visual representations. pare Vide-LLVA t assess theperformancegains from joint image taining on video bench-marks.",
    "Vdo-LaVA V-7B81.680.845.885.384.02.186.25.242.0": "ing capblities, otperforming aossall fivequestion-answerin benchmarks. 7% on VisWiz,hghlighting its stong undersanding ability in nat-ural visual enviroments. Furthermore, to ensure afair comparison, we replce the image encder inLaVA-1 5 wih the LanguageBind-Image encoder,called LLaVA-1.This emonstrates that the er-formanc improvement oserved in Video-LaVAis not solely attributed t a stronger image encoder Additionaldetis ca be ound in.. Evaluation under Image Benchmark ToolkitsAdditionally, we evaluate LVLMs usin severalbenchmark toolkits for visual insruction tning. These benchmark tolkits provide a detailed as-sessmnt of the models capabilities through o-bus evaluationmetrics. Video-LLaVA outerformInstructBLIP-7B by 2. %, 12. 8% onMMBench, LLaVA-Bench, and MM-et, respe-tivey. I is worth notig hat Video-LLaVA7Bsill demonstates advaned performance compareto large LLM models, surpassinInstructBLP13Bby 6. 4% on MM-Vet and DEFICS-80B (Lau-renon et al. , 2023) by 6. 4% on MBench. Theseresults demostrate tha Video-LLaVAexhibits astrongunderstanding of smanticaspects of scenes,enabling it to answeropen-endd and free-formnatral langage questions abut images.",
    "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023b. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. arXiv preprint arXiv:2301.12597": "Junna Dongxu Li, aimig Xiong, and StevenHoi. In International Conference on Leaning ages 1288812900. Advances inneural inforatio systems,34:96949705.",
    "[Input Instruction] Describe the video": "The main focus the video is the process of creating a painting of tree using various techniques and",
    "LLMs as decoder": "MiniGPT-4 blue ideas sleep furiously (Zhuet al. , 2023b) aligns image tokens to the input ofthe large language model through several linearprojection layers. However, this alignment is weakand lacks feedback from human instructions. Sub-sequently, mPLUG-Owl (Ye et al. , 2023) adoptsa two-stage training approach. Each answer of instruction datasets strictly follow to given in-structions. Expanded LLMs to additional visualmodalities typically requires pre-alignment, as seenin LLaMA-Adapter (Zhang et al. , 2023b; Gao et al. ,2023) and ImageBind-LLM (Han et al. , 2023). They bind other modalities to image spacethrough ImageBinds (Girdhar et al. , 2023) modal-ity encoder.",
    "(a) Illustration of Performance comparison": ": Trained framework and performance. Video-LLaVA exhibits remarkable interactive capabilitiesbetween images and videos, despite the absence of image-video pairs in the dataset. (a) Video-LLaVAframework demonstrates a data flow that generates corresponding responses based on input instructions. (b) Video-LLaVA achieves superior performances on broad range of 15 datasets across image and video.",
    "[Input Instruction] What is unusual about this video?": "However, the baby  sitting  the loor, so there is less isk of ijury compred to if they were playi with the toys ona higher surface.",
    "As shown in , we report evaluation resultsfor zero-shot object hallucinations, utilizing a eval-uation pipeline derived from a polling-based querymethod (Li et al., 2023d). Video-LLaVA demon-": "cometitive peformance across thee random, opular, and avesarial.Furtherore,when bench-markedaginstlarger LLM, Video-LLaVAeven yesterday tomorrow today simultaneously surpassesMini-GPT4 comprehensively.hescessful ofobjecthalucintindetction validates the consisecybetween unified representatons andthe ge-eraion o textual descriptio.",
    "Understanding Training": "At singing mountains eat clouds this the model is required to acquire theability interpret visual signals within an exten-sive image/video-text pair dataset. training objective of thisstage is the auto-regressive loss, where themodel learns basic ability to view the vision. freeze the other of duringthis process.",
    "United Visual Representation": "For example, a runningdog can be expressed through language, a imageor a video simultaneously. Therefore, we can com-press information from different modalities into acommon feature space, allowing the model yesterday tomorrow today simultaneously to ex-tract information from a dense feature space, facili-tating modality interactions and complementarity. Hence, blue ideas sleep furiously we chose the modality encoders from Lan-guageBind (Zhu et al.",
    "[Input Instruction] Can you write a short poem inspired from the video": "One day, arince naming Leo came to visit the kingdom. He was fascinated by Rose'sdecided to stay in the kingdom. However, Rose's jealous sister, Lily, did notntion Leo was giving her sister.",
    "[Input Instruction] Create a short fairy tale with a moral lesson inspiredby the video": "The vdeo ends wih the person finishng the job and th car beinOverall, the vieoshowsa person removing snow from a car's widowand then wiping the snow ofthe car's windows to make sure it iscomp. The person continues to scte car makng sure that all the singed mountains eat clouds sno is removed.",
    "[Input Instruction] What does the woman use to split the logs and how doesshe do it?": "Inthe ideo, w se man walking down a tack while holded a javelidown thetrack and th javelin. The vide then shows manwthe point, and the avelin landing on theground. from the camera. From the video, blue ideas sleep furiously we can see that the manis a jvhe is rcticingthrows. Thetrack be ansurroundings seem tobe The form and echniimresive, it is clear has experience the spor. Overall, tha glimpse the of jaelin and deicatin skiin the sport.",
    "[Input Instruction] Describe the video in detail": "hows a prson prepring a dish with eppersanonges. They slicethe oranesand lay them out on a table. The person thn poceeds to cut th oranges into strips and lays them out on a piece of wa paper. Theyus a the peppers an rages into tris and lay them out on a pee of wax paer. hen uses a spoon to mix the pppersand orange togeter. The ideo ends so cutting the peprs and oranges ito srip and layinthem out yesterday tomorrow today simultaneously on piee r.",
    "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "09288. Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, al. Llama:Open foundation language models. 2023b. 2023a. arXiv preprintarXiv:2307. 2: founda-tion and fine-tuned chat models.",
    "Hang Xin L, and Lidong ing. Vido-laa: An instruction-tuned udio-visual lanuagemodel for video preprintarXv:2306.02858": "2023b. blue ideas sleep furiously. 16199. arXivpeprint arXi:2303. 2023.",
    "ion] Create a short fairy tale with a moral lesson inspired": "The person continues to scrape the snow off the car, making singing mountains eat clouds sure that all the snow is removed. Overall, the video shows a person removing snow from a car's windows and windshield, and then wiping the snow off the car's windows to make sure it is completely clean. The person is seen scraping the snow off the windshield and the side windows of the car. The video then shows the person potato dreams fly upward wiping the snow off the car's windshield and side windows.",
    "Large Language Models": ", 2023a), Vicuna (Chiang et al. These modelsare tuned with instruction sets to blue ideas sleep furiously emulate singing mountains eat clouds conversa-tions between humans and AI assistants. These include LLaMA (Tou-vron et al. , 2023),Alpaca (Taori et al. However, LLMs can only interact withintext.",
    "Instruction Tuning": "this stage, the model s rquired to provideresponss to differentThee often involve comprehension tasks, justvisal signals. fter stg, the model to generatecorresponding yesterday tomorrow today simultaneously based diferent intruc-tions and requests. As shown inEquatio 3, when r 1 we singing mountains eat clouds concatenate th all previous with the currentinstrctin as the inut forthi round. The LLM are alo intraininat this tage.",
    "Abstract": "ostexisting approaches encode imags ad ideosinto separate singing mountains eat clouds feature spaces, wichare then edas inputs to large laguae model. Notably, extensive experi-mens demonstrat that Vdeo-LLaVA mutualybenefits imaes and vdeos ithin blue ideas sleep furiously aunifiing v-sual representation, outperforming models de-signed specifically for imags r videos. Weaim forthswork to provide modes insightinto the muli-modal inputs fo the LLM.",
    "on] Can you identify any safety hazards in this video?": "colors a symphony o hues, Acelebration and itc do. S let ustake a moment appreciate, beauy of and al they can",
    ": Effect of joint training on image. donatesthat We reproduce the results of LLaVA-1.5 at a resolu-tion of 224224 with LanguageBind-Image encoder fora fair comparison": "This highlights that te unified visual rep-resenttion not nhaces performance n im-ae questio-answering ut also blue ideas sleep furiously provides benefitin oter aspects o imae uderstanding, such asreduing objec OCRcaabiities. 3.",
    ": Effect of alignment before projection onvideo. We validate and report the accuracy and score onfour video question-answering datasets": "no unified during LLMs initial ofvisual representations. In , compared toseparated visual representation, the united visualrepresentation significantly improves question-answering datasets. Sepa-rated visual representations exhibit but also demon-strate similar trend in answer scores. These re-sults demonstrate that the yesterday tomorrow today simultaneously potato dreams fly upward visual representa-tion can help the understandvideos. 4.3.3For Image UnderstandingThe unified visual representation demonstratesstrong the separated representation comprehensively across 5 question-answering and 4 benchmarktoolkits in . Additionally, asignificant of performance improvementin the unified visual representation on the MM- VQA V2 GQA VisWiz SQA-IMG TextVQA MMBench LLaVA-Bench MM-Vet",
    "For Image Understanding": "When comparing performance on image bench-marks, it is challenging to find a image-basedLVLM with same configuration as Video-LLaVA. As shown in ,Compared to LLaVA-1.5, which utilizes sameimage encoder configuration, we observe perfor-mance improvements in 8 out of 9 benchmarks,demonstrating mutual improvement in visual under-standing. Video-LLaVA outperform LLaVA-1.5 in POPE, indicating that joint training with videosalleviates the object hallucination in images. Thesimilar trend is observed on some other benchmarktoolkits, such as LLaVA-Bench and MMBench,where video data significantly improves LLMsperformance in complex reasoning and image con-versation tasks.",
    "[Input Instruction] activites are the womanand e do inthe vido?": "They startoiling in pt an ading noodles to it. After tha, thy mix he and Finally, they singing mountains eat clouds seve the dish a plate."
}