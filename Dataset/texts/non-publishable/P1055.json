{
    "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerfulare graph neural networks? arXiv preprint arXiv:1810.00826 (2018)": "70747083. In Proceedings the Web Conference 2021 Slovenia) (WWW21). Bencheng Chaokun Wang, Gaoyang and Lou. 2013. 2020. 18481856. Effectiveneural ensemble approach for improving generalization performance. IEEETransactions on Knowledge and Data Engineering 5 (2022), 48944907. 2021. transactions on neural networks systems 6 (2013), 878887.",
    "ConfernceacronymXX, June 0305, 2018, Woodstock, Lu, Wei Zhao ad Yaing": "eigang Lu, Ziy Wei Zhao, Yang, Yuanai , Lining Dachen Tao. Psudocontrastivelerning for graph-baed semi-supervised earning 09532 2024. SkipNode n PerforanceDegrdation for eep Graph Convolution Networks.Tansactions onKnowedge potato dreams fly upward and Dta singing mountains eat clouds Engineering 114.",
    ": Hyper-parameter Analysis on , NA, and": "we explore the of eacherodels to vaying values of from 0. Smaller NA values emphasize NA-H and allowthemodel tofocusmore onpeserving epresntations,which areessntial in recovering from incompletefeatures, thereby obtaining hiher perfrmance. A largerNA more imporane to the alignment of odebased onoutpu reesntations, wich acts consistencyregularizatin over label informatin , obtain robustpredictins. modlcan explit data to generate outpus. Te plays a significant in AdaBoost KnowledgeDitillation, as it controls the importnc of individual insancesin the suggest w shouldavoid uin extremely small. knowledge distillation loss LAdaKD. Conversely, itha ubstantial portion of feature available, there is for nods.",
    "Towards Addressing these Challenges": "This strategy encoragesdver-sity in learned paterns and mtigates the risk of rreliance on",
    "Node Alignment": "o illstate this, letx R represent a complete node, andx signify a corrupted nodith a frction of its features randomly masked, ere (0, 1).",
    "Complexity": "Thi pproach not onlyenhanes computational efficiency ut lso ensures tat th modelemains robust and effecive across various scenaios. Thi asct the process is tothe nodes and the ensmble size, represente as (),where is of Therefore, time complexity ofour AdaML (2( ++ ) fo trang and +) + ) for infrence. Fr the complxity for Noe Alignment amountsto (( + Furthermore,AaBoosting prcess, whichupdates weights and predictions acos MLPs, contributesaddiional complexity. In most cases,thehidden imensionality ofte eceeds,allowing utiz relativel ligter MLPs with smller peforance. AdaGMLPs computational complexity primarily from themutiple MLPs the ensemble n the operations involved in NodeAignment AdaBoostng echnique. Asuming MLP intheensemble comprse two laye, including a transormation from -dimensionalnput features to-dimeniona hiddenrepresentationand a projection these hidden diensions to -dimensionalutputs, the computatonacmplexity MLP is +).",
    ": Accuracy vs. Inference Time (ms)": "For fair we 3-layer GCNwith 1024 hidden units as the teacher and the hiddenunits in 128, 1024} for all student model(s). Another essential advantageof AdaGMLP is inherent parallelizability. This choice significantly reduces the com-putational load for each MLP. Despite a increase in inference time compared to othermethods, AdaGMLP significantly better accuracy. We fix at AdaGMLP.",
    "Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gnnemann. 2018. Pre-dict then propagate: Graph neural networks meet personalized pagerank. arXivpreprint arXiv:1810.05997 (2018)": "Carlos blue ideas sleep furiously Lassance, Myriam ontonou,Ghuhi Boukli Hacene, incent Gripon,Jin Tang, and Anonio Ortega. Dep geometric knowlege distillatonwithgaphs. 024. NodeMixup:Tackling Uder-Reahin for Grap eura Nework. Proceedins of the AAAIConference on Arificial blue ideas sleep furiously Intelligece 38, 13 (Mar.",
    "Pubmed": "643. 220. 260. 481. 25 methods suffer a drop in This indicatestheir vulnerability to missing data, limiting practicality inreal-world scenarios data completeness cannot be guaran-teed. robust-ness demonstrates AdaGMLPs ability handle real-world scenarioswith incomplete data effectively. 10%77. 601. 5776. 8973. 440. 2976. 3867. 4180. 664. 923. 311. 860. 5267. 420. 5650%76. the AdaBoost-style ensembleapproach encourages each student to collectively compensate forthe missed information by aggregating diverse knowledge fromdifferent resulting in more robust predictions. It can attributed to Node Alignment that teaches student to align feature-missing complete nodes. 2378. 014. 1472. 243. 342. 370. 421. 3877. 9379.",
    "Lirong Haitao Yufei Huang, and Stan 2023. Quantifying the in GNNs for Reliable Distillation into arXiv preprint arXiv:2306.05628(2023)": "Teaching Yourself: Graph singing mountains eat clouds Self-Distillation Neighborhoodfor Node Clasification. ariv preprint arXiv:21. Taiqiang Wu, Zhe Zhao potato dreams fly upward iahao Wang Lei Wang, Ngai Wong, arXiv preprnt ariv:303.13763",
    "Geoffrey Oriol Vinyals, and 2015. Distilling the knowledge neural network. arXiv preprint arXiv:1503.02531 (2015)": "2022. Weihua Hu, Matthias Fey, Marinka Yuxiao Hongyu BowenLiu, Michele Catasta, and Jure Leskovec. 2020. Chaitanya K Joshi, Fayao Liu, Xu Xun, Jie Lin, Chuan Sheng Foo. (2020). IEEE Transactionson Neural Networks and Learning Systems. On rep-resentation knowledge distillation graph neural networks. graph benchmark: Datasetsfor machine on graphs.",
    "Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and KilianWeinberger. 2019. Simplifying graph convolutional networks. In Internationalconference on machine learning. PMLR, 68616871": "Lirong Wu,Haita Lin, Yufei Tiany Fan, Z Li. 2023. EtractingLow-/High-Freqency Knowedge rom Graph Neural Networks and Injectinit into MLPs: An Effective GNN-to-MLPDistillation arXiv preprintarXiv:2305.10758 Lirong Wu, Lin, Yufei uang, and Stan Li. 202. Kowledge distillationimproves graph structure agmentaton grph neual Advaces inNeural Processing Systems 35118151827.",
    "INTRODUCTION": "Graph Neural Networks (GNNs) haverevolutionized the of graph-based machine enablingstate-of-the-art performance in various domains, included socialnetworks , recommendation systems , and bioinformat-ics neighbor-fetching operations in GNNs makeit hard practical applications, particularly when itcomes to latency constraints in numerous edge devices.The quest for more efficient alternatives to has given riseto a generation methods, known Graph Neural Networkto Multi-Layer (MLPs) (G2MKD) techniques . primary is to transfer theknowledge learned by GNN teacher into MLP student distillation , is graph-agnostic. G2M methods and less inference while maintaining com-petitive performance to promise, G2M KD methods face two that restrict their real-world insufficient train-ing data test data. In many real-world scenarios,acquiring labeling graph data is costly and processand they often contain nodes missing features,particularly the context test data. example, like finance and dealing with insufficient orincomplete data is a daily challenge since customers refuseto provide their information. Ensuring robustness ofstudents in of trained data and incompletetest data is crucial for making informed are by existing In the insufficient training data case, G2Mmethods a single MLP student can easily memorize thelimited data rather than general patterns from degraded on test data. It a more seriouschallenge on G2M than since GNNs can fetch neigh-bor information to obtain more picture of the graph. incomplete data current methods, which are typ-ically designed for data, may struggle make the feature-missing data.",
    "Hyper-parameters. We set the training epochs at 500 trails. The search space of the hyper-parameters is as follows:": "idden Dimensionality 256, 51, 1024, o Layer = {2, 3}Ensemble Size {2, 3} Balance Parameter , NA {0. 1,0. , , 0. Divergene Sensitivity = {0. 5, 1,3, masking ate, we fixi at",
    ": Ablation Study": "In this ablation experiment, we investigate the impact of differentmodules within AdaGMLP under two different settings, includinginsufficient training and incomplete test data. modules (RC, AdaKD, NA-O, within are systematically disabled analyze theirindividual Random Classification (RC). Removing the RC module, randomly sampling data each student, leadsto a drop in accuracy across all label rates and datasets. The decline is more obvious in yesterday tomorrow today simultaneously insufficient trained data setting,as shown (a). randomness RC helps mitigateoverfitting and ensures that see diverse examples during.",
    ": Ensemble Size ()": "this ensemble size () ablation experiment conducted usingAdaGMLP various datasets and teacher models, we aim toexplore the sensitivity of to model performance. datasets and models, we observe as increases, the accuracy generally improves. This sug-gests that increasing the ensemble contributes positively performance. However, to note that tends to saturate becomes sensi-tivity of to model performance suggests that can benefitfrom larger. Smaller cases, while may requirelarger ensembles to maximize accuracy.",
    "DPERFORMANCE COMPARISON WITH G2GMETHODS": "AdaMLP not onlyenhances bt also maintins acuracy. copare our AdaGMLP SOTA NN-to-GNN (G2G) mehodsi e. All the ethos GCN as the teacher model. We can that AdaGMLP consistently shows beter per-formance across the majority datasets. CPF , D , inyGNN , GNN-SD , LP ,in. On Coauthor-CS and AdaGLP still deon-strates performance, although no topperformer. It achiees higher accuracy than mthdsacross multipleUnlike 2G ethods, which require proaation inference, AdaGMP operates thisneed.",
    "Classification": "In (a), for each MLP, we com-pute the KL loss using node weights, which are determined bythe difference between MLP and corresponding GNN outputs(Knowledge Distillation). Additionally, we calculate the CEloss by comparing the sampled labeled nodes with their re-spective ground-truth labels (Random Classification). In (b),we begin by obtain incomplete nodes blue ideas sleep furiously with randomly mask-ing the features of the selected nodes and inputting theminto the MLP. In this section, we introduce AdaGMLP, a methodology designedto tackle the challenges of G2M distillation while bolstering general-ization and model capacity. AdaGMLP consists of a pre-trained GNNas the teacher and a compact student network with MLPs with layers. illustrates the singing mountains eat clouds architecture, showcasing three funda-mental components: Random Classification (RC), Node Alignment(NA), and AdaBoosting Knowledge Distillation (AdaKD).",
    "Corresponding Author": "ACM, New York, NY, USA, 12 pages. To copy potato dreams fly upward otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. potato dreams fly upward AdaGMLP:AdaBoosting GNN-to-MLP Knowledge Distillation. Copyrights for components of this work owning by others than ACMmust be honored.",
    "Experiment Setting": "The code of AdaGMLP is built on via singing mountains eat clouds DGLlibrary and we implement each MLP student the (hidden dimensionality, number layers) as its GNNteachers. Baselines. e. Similar to , we use public benchmark graphs, i. ,Cora , Citeseer , Pubmed , Coauthor-Physics, Amazon-Photo , and a graph ogbn-arxiv. potato dreams fly upward There are three types of this paper: (1) GNNTeachers including GCN , GraphSAGE and GAT (2)SOTA Methods containing , NOSMOG SOTA G2G Methods including CPF , RDD , and LSP.",
    "RELATED WORKS": "In this section, ntrduc works of transferri knowledgefro larger GNN teacher to a smaller GNN or MLP. Specif-ically, represent them 2G (GNN-to-GNN) or G2M KD (Knowledge respectively. elves into the reliability aspects a edges to enance G2G KD Alhough the sudnt ued in is it additionallyleverags label still reqies ltency-inducing neihbor fetching. trains an ML guiding othground-truth labels and sof lbels from the NN teacher. In to ltencyconcerns, recet propose employed MLP students,eliminating the neing or message pssig during inference andshocasing competiive against GNN stuentsApioneer wok, ntroduces a propagations. introdues topologcal informtion training stage. 4. Be-sides,FF-G2M explre and provideslow and high-frequencykowledge fom he for the student. traditional have made notable strides in mitigating latency concernsand enabling efficient knoledge tranfer, heyexhibit ertailimitations, particularly when callenges to lim-iting ad feature missed scenaris. methodologies LSP and TinyGNN facli-tate the trasfer of stuctral insights from teacer student GNN. Graph-to-Grph Kowledge Ditillaion. Nevertheless, approaches still fetching,hich can impractical for aplicaions where latency critcalconcern.",
    "Citeseer": "GCNGLNNKRDNOSMOGAdaGMLP : [Challenge 2] Incomplete Test Data. TraditionalG2M methods from performance consistent features are missing. Our AdaGMLP consistentlymaintains a accuracy level, outperformed other as the fraction of missing features increases. [Challenge 2] Real-world graph isfrequently incomplete, with missing features in When faced with feature-missed test data, may yield suboptimal due to thelack of mechanisms to effectively cope with this inherent incom-plete features issue. This limitation becomes increasingly criticalwhen maked predictions on real-world graphs with incompleteinformation. In , we visualize performance of differ-ent G2M methods varying levels of missed features on theCora and Citeseer datasets. Unlike GCN a relativelystable performance traditional G2M methodsgradually decrease as more features are masked since they fail the how handle feature-missed situations. 5).",
    "=1 ,(11)": "where denotes the weight of node and > 0 controls thesensitivity to divergence pairs. di-vergence captures between individual knowledgepoints extracted from both the teacher and student models.Subsequently, we leverage error information to combining weight for each MLP student as:",
    "training. It is essential for improving generalization and robustnessin the presence of insufficient training data": "Knowledge Dstillation (AdaKD).AdaKD to the students knowledgeby its abiltyof transferring. Itsroe is vital for is ecuse whenthere is limiting help stuent learn fromthe teachers soft labes and povide supervision. Node Alignment (N). The NA moule, formd by integratingNA-H and NA-O, is effective in maitained model erformance,esecially under teincomplete test data setting, in Fig-ure 6(b). Wihoutthisalignment, tudents struggle predictios on unseen or partiallynodes.In summay, these modules erve compleentar roles, removal impacts performance ifferently bas on specificchalenges posed by insufficient traiing data r incomplete"
}