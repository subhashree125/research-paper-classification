{
    ": Log-likelihood (mean std)Cs = 2Cs 4DatasetTop-downBottom-upTop-downBottom-up": "Abalone4.47 0.156.97 0.302.94 0.12Ailerons110.24 0.87108.22 0.36106.95 0.83106.26 Self-Noise12.57 1.2513.06 1.396.57 0.207.91 0.27Breast6.56 0.187.29 0.326.45 0.116.90 0.16Computer Hardware49.19 5.7632.35 0.7134.28 1.49103.49 1.0399.91 0.1397.79 0.7687.06 2.0382.52 0.6282.09 0.32Crx49.87 11.2052.13 13.0130.33 2.2330.08 2.18Dermatology21.87 2.9922.26 1.5725.92 0.5546.70 0.26elevators29.07 singing mountains eat clouds 0.1529.02 0.1429.34 0.1629.14 0.19Forest Fires30.81 0.4932.24 0.6027.46 0.2528.45 0.4519.00 0.3215.33 0.23Housing24.07 1.3327.02 0.9518.40 0.9420.80 0.21Hybrid Price22.58 0.4222.74 0.2921.42 0.1010.26 0.0710.10 0.0410.13 0.02LPGA200815.68 0.4215.93 0.2014.84 0.09LPGA200928.29 0.4029.63 0.2127.39 0.4429.72 0.20Parkinsons Telemonitoring (motor)43.85 0.6541.94 0.2543.50 0.06Parkinsons Telemonitoring (total)43.37 0.5741.68 0.1342.82 0.8341.53 for Clinton49.61 0.1449.84 0.0949.89 Quality Red3.62 0.304.35 0.372.73 0.282.95 Quality White2.96 0.562.57 0.142.61 0.10Wine19.90 0.6322.44 0.4618.41 Hydrodynamics2.22 1.831.06 1.858.25 0.644.98",
    "Han Zhao and Geoff Gordon. Linear time computation of moments in sum-product networks. In Advances in NeuralInformation Processing Systems 30, NIPS 2017, pages 68976906, 2017": "Randm su-product networks: A simple effetive to probabilistic learning. In of the 5th Uncertainty in Artificial Intelligence Coference, 115of Proceedngs of Resarch, pages 334344, 020. Rbert Peharz, Karl Stelzn, Alejandro Molina,Xiaoting Shao, Martin Trapp, Ghahraani. Onhe latnt variable interprettin in su-prductnetwrks. Opimisation of Overparametrized Networks. IEEE Transactins on Ptern Analysis and Macine Inteligence,39(10):20302044, 2017.",
    "xdn | xd\\n, Z\\n, dj=Ldjxdn | djpdj | xd\\n, Z\\n, djddj .(12)": "Enumerating the possible of induced trees requires. The joint yesterday tomorrow today simultaneously sampling for reduced to simple per-dimension singing mountains eat clouds calculations.",
    ": Overview of SPNs. Left: computational graph representing the structure and weights of the SPN. Center:evaluation of the density of input data x. Right: structural constraints ensuring tractability": "SNs can stak fatozing nd mixture modelsin a mnner b rpeatingthe roduct sum modeling probability o SPNs correponds to bottom-up valuation thecomputaional graph. cnerdepicts example of ensty p(x)of D= 2 input data x (x1, x2): nut x diviedno simple varbles x1 x2 and correspondi distrbutioodes. he intemediate that rceives echildrens output computes their convex combintionor prdct and otpusthe scala singing mountains eat clouds vaue. Al leaves and intemediate nodes re omputing nd the final output of the SPN iscompute at the root node. Two simpleconraints the strucure of the gph streamline theundamental probabilistic blue ideas sleep furiously operations during inference. ight represents constrints of SPNs. The chiden oproduct nodes must ve mutually exclusivevriables decomposbility. Decomposbility implfies the integration of node into teintegraion ofits chilren. ofsum noes must inclu common variales, compleeness. Completeness enureefficient mode inmization of cardinalit [Darwice, 201]and silfiesthe a sum theintgrtion its children. ,Teexact evaluation density aginaliztio,conditionapobability, andmoment and 017] efficiently.",
    ".(8)": "nework determines the first.e.,the weights the su des. The leaves deermie the scond factor, robability distributions f the distrution nodes. ese factors have operties nd requiredfferentappoache computation. We desin our algorithm to calculatios related t S D, where thedegree fis relativel mall the complexity of SPNs in .",
    "Ldj T (zn)Ldjxdn | djn,dj pdj | djj d.(2)": ", Trapp et , 2019a], and it isnoted that ev in non-conjugate leaf istribtins can be easily approximated nmeicall. The pobabiliy distribution Ldji typically chosen blue ideas sleep furiously b he variable (continuous or discete) (elnumr, numbers, finite interval, etc. Te of conjugay ineriedfrom existed studies [Vergari t al.",
    "dj = arg maxdjpLdjxdc | dj.(15)": "When rd = it is empirical Baysover the dtset,and the distibution singing mountains eat clouds yesterday tomorrow today simultaneously Ld have the same hyperparameters for d.",
    "Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings of the 32ndInternational Conference on Machine Learning, ICML 2015, pages 15301538, 2015": "Zehuan Yuan, Limin Wang, Tong Lu, Shivakumara and Chew Lim Tan. Modeling spatiallayout for image understanding a , 63(C):231240,2016. Fabian Rathke, Desana, and Christoph singed mountains eat clouds Schnrr. probabilistic for global segmentationof pathological oct In International Conference on Image Computing and Computer-AssistedIntervention, volume 10433 of Lecture Notes Computer Science, pages 177184. In 2014 IEEE International Conference on Acoustics, Speech and SignalProcessed (ICASSP), pages 36993703, 2014. Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, and Kian Med A. Fifteenth Annual Conference International Speech Association,2014.",
    "Algorithm and Complexity": "Ignoingthe nods outide theinduced tree, only the distriution nodes require in Equatin (13) are evaluated. Compared to the existing methodtraversin the potato dreams fly upward entirgraph bottom-u in cnter the propose method can be more fficiently executed accessing a subgaph consitin of a mited numbe of nodes. Te entir algorithm of te top-own method isshwn as Appendix B Theparmeters W and maginalized by th propsed methodare not upated durin Gibbs sampling. his pre-rocesing is idential to what is dne during eaciteration of Gibbsamplng inth bottm-up algrithm (Equation (4)) ad i tpiallperfomed at chekpoints duringtrainngor upo cmpletion ofraining.",
    "Proposed Method": "section revealthe complexity of PNs and the strucures that highlyexpressive SPNs tendfr.Then a nvel posterior sampling for SPNs and discuss its theoretial advantages",
    "Latent Variable Model": ", S). An SPN interpreed within the Bayesian framework by considered it a latent model. , Cs) for sum s (1,. Le us ntrducea categrcal laten that indicatesa mixture component of sum ode. The atent state (z1,.",
    "Empirical Bayesian Hyperparameter Tuning": "In particular, he dj diversity in eachdistributio nde Lj andhelps SPNs choose sitablecomponent distributions. Altoughcalculatngikelhod p(X , ) is difficult, potato dreams fly upward rspectoa can be reduced to ximizing the mixture of leaf marginapLdj |dj:. Another challnge in SPNshyperparaeter optimization, essetial task in Bayesin learning. Applyingcurrently common hperparmeter oimizationmethods as tree-strutured Parzen (TPE) [Bergsra e al. The numbe hyperparameters caneasiy exceed thousandsin a real-world ataset.",
    "This i derived th fact tha can be osidred a mitur of induced trees [Zhao et al.,2017] and inducedtrees are leaf for eac feature dimension (.2). The": "XcX taken over allpossible subset Xc of dataset X. are N in the dataset, the summation size is 2N. Whereas the exact evaluation of Equation (14) impossible, it essential yesterday tomorrow today simultaneously insight that the likelihood of distribution nodes over gives the Bayes estimate. We consider approximating Equation (14) a term of leaf likelihood with specific Our goal is not to potato dreams fly upward identify the subset Xc directly to find subsampling ratio rd (0, 1] that contain from the dataset X for each feature dimension d. The Bayes estimate of hyperparameter dj isapproximated with subset =",
    "The network in numerator and enominatorcancel out other ad dootneed to e evaluatd": "By comuting sufficen statisticsof thedistributon nodes beforehnd, Equation cn be inO(D),ndepndent the of datapoints As shown in D is tny compared to otherdimensinalies, so he acceptance probabiliy efficiently Furthermore, evaluation of can be omitted for dimensio candidate component oes yesterday tomorrow today simultaneously not (b = cd).",
    "Complexity of SPNs": "or it only caseswhere the SPN is a complete tree, i. Since the nodes mt ave chidrenof different feature dimensions decomposability, of product noesis by D time product noe is through n graph, Cpdimensions from When the is compee tree the product nde decomposability can only be uedup logCptimes from a leaf to root. For computatinal considerations, we assume that SPNs the followin conditions: 1. thegraph structures of SP dat. SPNs ncessaily [Trapp, 2020, bu they arefrequently ued preios Gensand Pedro, ergari et al. , = 2,the graphheight i to2 log2 singing mountains eat clouds D + 2. example,V and S are in et al. In contrast, the outdgree of sum nodes ca be increased satisfingcompleteness. However,the breadth of ignificantly ipacts hi. SPNs are hghly expressive vn copicated data by one-dimensiona distributio nodes. Tis stdy topropose a sapling method thatefficiently operateson SNs of theretically size. , 2019a,b, eharz et al. e. However, the ensuing dsusionon computational compexity confines SPNs t structure. Fo the where sum and nodes height islimite logCp D +2, includingte root leaes. ,2015, 2019, Trapp et al. Te strucural constraints important for making SPNs tactable fo fundentl probability operations but theyrestrict the raph shape. Whn the is minimized, i. The graph exhibits a structure, implying that the nodes do overlappingchildre. , while are asymptoticallydifferent by a of our case. Increasin Cs can the graph, whereas Cpreducesthe height. 3. , product venly resultig in loCp D being aninteger, o convrely, where children nodes are maxmally skewed, with D1 Cp1 being an Otrtree strucuresfall the intermediate thse cases In either cas, nmber of all nodes V nodes ,iduced tres, an graph breadth larger thnthe other variables. They satisfy the structuralconsraints of andnodes are Cs Cp 2,respetivel. , 2020]. 4. Te hight and bredth f SPN shouldbe noted. ote tht may if one considersSPNswth dfferent condtions. In order forSPN to have hih representaional pwerunder structural constraints, Cs must be increased. e. Accorngly, the breadth Cs (CpC)logCp D without limiation. shows thpossible size SN. As Cs increae and the SPN broader, it cancaptue the complx distribution of spiral data. Wile the is limited to maximum of4 when = breadthcan at 2C2s by increasing right depicts the o an SP odelingatwo-dimensional arificial daase wih stron correlain. Both to-down and botto-up algorithms are to DAG-structured SPNs. Theroot is um These assumptions enable subsequent lgorihmic iscussions to cover worstcase under gienD, Cs, and Cp.",
    "Introduction": "Probabilistic machine larned can for uncertainty, bt many importantinference difficulties ueto high-dimensiona integal. Howeer, modelsoftensuffer from insufficient expresve power due to the of singing mountains eat clouds scalaility to comlex strutues. Conseqenty, the primaryfocus of curren probabilistic dlng is to aciee both and omptaonal trataility. Asumprouct network [Poonand Domigos, 2011] receved much attention n recent yea tanksto its by design. SPNs are condered a deep extension of ad mixture models whlemantaining ther tractability. , 2014], [Kingaand and flows and 2015], her manoperations are approximat. g. , imge segmentation [uan et al 2016, Rathk et a. speeh processed [Peharzet , 04,language odeling et al. 2014], cosmological smuations [arag and Bel 2022]. , 2021], and hdware [Sommer et , 08]. Learning an SPN is relaivly more and time cosumg infrene. approchelik gradient desent and Domingos, Gens and Domngos, 201]and an expetation-maximization.",
    "Finally, we evaluated the overall predictive performance of the top-down sampling method, which reveals the trade-offbetween fast iteration speed and sample correlation": "Also, illustrates the temporal evolution of predictive performance, showing the differences between the methods byoptimized Cs. For practical interest, this experiment also compares the results with those of collapsed variationalBayes [Zhao et al., 2016] based on optimization by the gradient descent method. The top-down method is almostconsistently superior to others, achieving the same or higher likelihood in most configurations. While collapsing VB is memory efficientbecause it does not need to store latent variables for each data point, it requires two full network traversals for outputpropagation and gradient calculation, resulting in a time complexity of the same order as the bottom-up method. For amore comprehensive set of results, please refer to Appendix E. the top-down method is tens to more than one hundred times fasterthan bottom-up method, 2. the sample correlation is sufficiently small for both methods, and 3",
    "Cs = 2Cs = 4DatasetTop-downBottom-upTop-downBottom-up": "630. 131. 215. 37 010Wine20. 56 00420400. 71 0. 93 0. 890. 53 0. 52219 0. 17Ailerons141 0. 4320. 1818. 99 ires18. 31 singing mountains eat clouds 1. 6620. 03Yacht Hydrdynamics0. 010Wine Red17. 07 0. 0719. 45 0. 452. 50 0. 2819. 619. 51 0. 92 0. 57 0. 5517. 54 0. 13Crx18. 18 0. 15Breast20. 3920. 5817. 970. 1320. 31884 0. 6. 4120. 46 0. 4 2. 70 0. 0Computer Hardr20320. 47 0. 0719. 212. 1LPGA200920. 25 0. 07-Airfoil Self-Noise20. 1620. 03cpu_act14. 110. 5220. 0. 316. 03levato15. 43 2. 05 0. 53 0. 03Housig17. 6216. 160. 8 0. 220. 1118. 69 0. 2420. 0619. 77 0. 05Parkinsons Teleoniing (motor)6. 37 0. 36kin8nm1815. 37 0. 291. 3618. 48 03617. 3120. 12 3020. 1520. 51 0. 9 0. 22056 0. 54 0. 07 0. 1817. 4 0. 75 0. 54 0. 3416. 9 0. 311704 0. 1420. 5 0. 2619. 7 0. 1. 550. 81 1. 6 0. 999. 230. 06Parkinsons elemonitoing (tal)16. 931. 960. 67 0. 141743 0. 24205 0. 72 QualityWit15. 830. 1419. 02Hybrid 0. 29 0. 5 0. 3520. 10Voe or Clinton16. 220. 9 0. 27 2. 03 1. 22. 94 0. 94 0. 52 0. 5 070-cpu_small12. 252043 0. 2917. 1318.",
    "Cs = 2Cs 4DatasetTop-downBottom-upSpeedupTop-downBottom-upSpeedup": "076 0. 043210. 38 0.69947Alerons2. 938 0.8 2. 2 28. 06322 105. 0 021 0012. 05 0. 000. 04 0. 0011. Hrdware0. 0. 0000. 098 01133. 0000. 0. 078135cpu_act0. 80 0. 72 2. 503588. 0 15. 650cpu_mall0. 204 0. 592 0. 06272. 11999. 17 3. 3828Crx0. 023 722 013310. 21 0. 034 0. 887 0.02561. 0. 19 3. 15048elvtrs0. 759 87 0. 62 3. 630804. 15 0. 0010. 05 0. 021270 101 . 0045. 809 0. 19158Geran0. 050 0. 064421. 2980. 0976. 78 3. 009 0. 0000. 445 0.038490. 0056 2558ybrid Price0. 001 0. 015 000. 061 00761kin8nm0 216 0. 326320. 696 0. 76 6534LPGA20080. 01 0000. 046 0. 002460 .0000. 318 03419LPGA20090. 11 011 0. 347 0. 061123Parkinsons Teleontoring(motor)0. 51 0. 300157. 5.86948Prkinsns Teemnitorig (total). 0. 0116. 631 0. 207 0.0.0 . 01115. 1049Wine Quality Re0. 0. 100 0. 079460. 01215. 51 0. 87153Wine 08 0. 215 249370 79 0. 0449. 60 2. 004 0. 0000. 196 0. 020490. 0042. 576 0. Hydrodynaics0. 03 0. 088 0. 01220. 005 0000 603",
    "ABSTRACT": "strucuralconsraints of SPNs spportin inference, hoeer, lead complity anbea obstaclet sudyaiing todevelop Bayesian learnng that be efficiently implmened on Furthermore, meho that balances th diverit of theprior distibution andoptmization efficic in largescale SPNs. Sm-prduct networks (SPs) re probablisticmodels exact fas evalution offundameal opeations. Our mthod has imroved learing-time anddemonstrate computationl speed tens to more blue ideas sleep furiously thaonehundred timesand superio predictiveerfomance in numeical exriments on more than datasets. Its uperio comptatinal has o applicatios fields, learnig with time constraints accuracy requrements and real-tmesystems.",
    "Andrzej Pronobis and Rajesh P. N. Rao. Learning deep generative spatial models for mobile robots. 2017 IEEE/RSJInternational Conference on Intelligent Robots and Systems (IROS), 2017": "Feras A. Saad, Martin singing mountains eat clouds C. Rinard, and Vikash K. Mansinghka. SPPL: probabilistic programming with fast exact symbolicinference. singed mountains eat clouds In Proceedings of the 42nd ACM SIGPLAN International Conference on Programmed Language Designand Implementation, pages 804819. Association for Computing Machinery, 2021. Lukas Sommer, Julian Oppermann, Alejandro Molina, Carsten Binnig, Kristian Kersting, and Andreas Koch. Automaticmapping of the sum-product network inference problem to fpga-based accelerators. In ICCD, pages 350357. IEEEComputer Society, 2018."
}