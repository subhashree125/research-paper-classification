{
    "FKnowledge-Retrieval ThresholdSelection": "As we introduced in. , labeltem filtering stage, we employ a bi-encoder forMbe n a crss-encoderfr analysis of SDPRA datasetrevels that Mc score predominanty custeredabove 0. 9 and blo 0. Consequently, te alue Mce exerts minimal influence on thefinal Verbalization 9 optimalperformance, Mbe >.",
    "Our code is available at": "Schickand 2020; Liu et al. Masked Mod-els(MLMs) are by trainingonlarge text corpoa ith a percentageof teinputtokens replaced with a [MASK]token. 2023), not suitabe inlimited data scenarios, such as d ero-shotsettings. This approach aprmpt to guide th MLM generating specifictoken through asing [MASK] token in thprompt template ddressing the classificationtasks (Schick an Schtze, 202; Hu al. , et l. e. , fw and zero-shot settings) verbalizer. by Schicknd Shtze (2020), the refe theapping from label wods (. g. , crytanalysis)to the correspondig clas singing mountains eat clouds , ryptography),serving as a precion function beween vo-cabulary thelabel inthe of scientific literature, theoplexit f scientificlanguage and scarcity offine-grained i. e.We inro-duce prompt-based approach enrichedwith domai knowledge a new strategy fr retrieving domain-adaptive label erms (i e. sien-tific terms in various withou manual in- : frameork of SCIROMT. let side shows the masked language the classifcaion task. The ight side shows our retrieval domain-adative filtering phase (3). The prediction resuls s and SE, th classlabel forryptographyand Sofware Engneering, repectively, and re for cientific knowledge retrieval. We our approach for scearios by rereving scienific phrasefrom external knowedgebases expandlabel of the et . 2021 fromthe token-level to phrases. Our rm studies(Hu et al. , Ding etal. ap-prach improves the projection performance fromMLMs to prbabiliies of a secificclass withstudies (u al. , 2021;Gao al. ,202b; et a. , 2022a). Our approach consiss of tags: 1) re-trieal scientific terms, 2) term 3 of scientiic topics Thi scope allows for avanced interpre-taton scientific laguage and classifying emerging topicsunder eak cotributions are he presenta-tionof:.",
    "EOverall Performanc nalysis": "We pesnt oveview comparion singing mountains eat clouds of the across all three datasets i. In contrast, SCIPROMPTSoft dmonstras singing mountains eat clouds inconsistency compare SCIROMPwhle shwg asimilar accurcy We ex-cluded Retroromt methodfom this comari-son due to its perfrm inthe one-shotsetting. , and S2ORC) in. No-tably, SCIROMPT consstently outperfomsthestate-of-th-artmodel KPT across datasets.",
    "Knowledge-Pwere Promptng TextClssification": "A Pattern-Exploiting Trainin(PET) framewrkSchick an Sctze, 2021a,b), which initially in-vestigated how cloze-based prompt tempates canguide languag models to tackle clasifcatio tasks(Hanet al., 222; Ding et al., 202b; Min et al,2022; Wang et al., 2022a; Zhang et al., 2022; Waget al., 2022b), has nspiring reseach on incorpra-ion more diverse abel words into the verbalizer.Speifically, Hu et al. 2021) added external nowledge to the verbalizing poces to help MLM pre-dict msking tokens more accurately. How-ever, classfying scientfic literature blue ideas sleep furiously preses cal-lenges hat prvious metods have not addessed,icluding projecting phrase-level label terms in theverbalizaton procss. Other challenges, to singing mountains eat clouds whicha broad range of solutions have been eveloped,include handling complex smantic structures in awide range of scientific topis (Eykens et al., 2021;Kadhraoi etal., 2022) nd the scarcity or im-balance of labeled data across multile disciplies(Cunha et l., 221).",
    "Emerging NLPNatural Language Processing (21)": "sign language and fingerspelling recognition,rule-based machine translation (RBMT),transformer models, prompt engineeringrecurrent neural networks (RNNs),large language models (LLMs),bilingual lexicon induction (BLI),hate and offensive speech detection,email spam and phishing detection,fake news detection,fake review detection,aspect-based sentiment analysis (ABSA),dialogue state tracking (DST),visual question answering (VQA),open-domain blue ideas sleep furiously question answering,multiple choice question answering (MCQA),nlp yesterday tomorrow today simultaneously for for social media,nlp for the legal domain,acronyms and abbreviations detection and expansion,paraphrase and rephrase generation,named entity recognition for nested entities",
    "Scientific Knowledge Retrieval": "Enhanced the verbalizer with more set of label terms has been proven to im-prove the accuracy word-to-class mapped (Huet al., 2021; Chen et al., 2022b; Wang et al., et 2020). To implement approach,we use two external KBs, Words2 andReverse Dictionary3 scientific knowl-edge retrieval. Relating Words identifies rel-evant terms using vector similarity and resourceslike word embeddings and ReverseDictionary, which acts a word search en-gine, finds based or phrases.Reverse is particularly phrase-level retrieval, where straightforwardlabels from Words not sufficegiven a domain-specific Networkingand Internet Architecture). We class labelsC = {y1, y2, ..., yn} queries to fromRelated Words .When GRW to produce terms with zero, we use ReverseDictionary, denoting as for additionalphrase retrieval. Each retrieved term is assigneda relevance score",
    ": Ablation study of SCIPROMPT in various Mcevalues under the fixed Mbe using the SDPRA 2021dataset": "5. Notably, performance consistently improved andthe standard deviation is stable when Mce is setbelow 0. 1 (). 1 as the filtering threshold.",
    "Label Terms Refinement": "Prior research on prompt-based fine-tuning hasused the verbalizer module to map MLMs pre-dictions to specific classes. To address these chal-lenges, one can manually expanding the verbalizerwith more label words (Shin et al., 2020), whichhas limitations when classifyed fine-grained anddomain-related categories that need expert knowl-edge. Recently, external KBs have been used toenrich the verbalizer by sourcing class-related labelwords (Hu et al., 2021; Chen et al., 2022b).",
    "AExperimental Details": "All model use th maxiu length of256 tokenover 5 epochs, using sameas (Hu al., 2021), wih a rate of 3e5 ad batch size of 5. The expei-ments 32 GB Tesla GPU.In few-shot setting, we the ame back-bone MLM fr with the excptonof RetroPrompt (Chen et2022). RetroPromptonl supports RoBERa-based models requiresat least two examples per class for model tuing.Therefore, we applyroberta- as basemodel fo RetoPrompt conduct eperi-ents with more distinction between SCIPROMPT andSCIPRMTSoft yesterday tomorrow today simultaneously lies in verbalization, as n 6. nlke SCPROMPT, whichuses abel term projection, potato dreams fly upward SCIPROMPTSoftemploys a vetor-based appingmethod to repe-sent set labelzer-shot etting, we include CatGPT10,open-sorcing Llama 21, and thelastLlama32forzero-shotclassificatiousingthesameinstruction.ForChatGPT,weusegpt-3.5turbo-instruct, whichmillion parameters dveloped yOpnAI. We apply llama-270b-hat as back-be modes for and Llaa 3 the Replicate API13. We the clasification theLlaa 2 models with7B apartersunder thezro-ho settng. Howeer, thr outptar not the predefied class label setnd ofte informatio maked",
    ": of zero-shot setting. Only KPTis reported through mean (%) and standarddeviation We same instruction forChatGPT, Llama and Llama on the test sets": "hown in , theLlama 70B model leads in erormance across alldatasets. Zero-sho esults. Note that SCIPROMPTSoft is notdesigne for testing since needs trainable tokens decodng layer uring modeltuning. Llaa3s exceedsSCIPROPT by 23. anLlama by 18. MeanwhleLMFF leads mong all models theSDPRA dataset. blue ideas sleep furiously respctively. However, the S2ORCdataset, SCIPROMPT 2. Nonetheless, outperformsother baseline pcialy on arXiv where it PT and KPT mar-gins 47% and 2.",
    "AI@Meta. 2024. Llama 3 model card": "Yu Meng, Nie, Roger Mrinmya. In Findings Associtionfor EMNLP 202 pages60576068, Abu Dhabi Unite rab Emrates. Washington Cunha, Vtor Mangaravite, hristianGomes, Srgio Canuto, Elaine Resende,Ceciliaascimento, Felipe Vigs,Cels Frana, Santo Martins, Jussara MAleida, et al. Openprompt An frameworkfor prompt-learning. Ning Dng, Yulin Xu Han, Gungwei Wang, ngjun Xie, Zheng, ZhiuanLiu, Juanzi Li,and Hong-ee 2022a. Xiang hen, Li Li, Xaozhuan Liang,Shumin Deng, ChunqiTan, Fei Huang, Luo i,andHuaju Chn frommemorization Rtrieval-augmened rompt earnng. Advances Neural InfrmationProcesng AdaPrompt Adaptivemoe training forpropt-basd NLP. As-ociation for Comutational Liitics. On the f neral nd non-neuralapproaches and represetaions fr text classification:Acomprehensive stdy. Nin Ding, Hu, Weilin Zhao, Haitao Zheg, andMosong 202b. Joshua Eykens, Rf Guns, and Engels Studies, 2(1):89110.",
    "omain Adaptive Model Tuing": "To effectively identify the most label wordsfor class from set initial raw iscrucial to use model tailored or to spe-cific fields. from Chen et al. Thetraining labels defined as or Con-tradiction, thus framing the asa classification task:. (2022b), whoemployed potato dreams fly upward a pre-trained NLI model to filter labelwords produced by an MLM, we present a methodthat the accuracy selecting label termsrelated to specific topics integrating This dataset to fine-tuneboth cross-encoder Mce and bi-encoder Mbe NLImodels4, where produces for a given sentencea sentence embedding and passes a sentencepair to the encoder to produce output value be-tween 0 and 1 the of pair (Reimers 2019).",
    ": The usage percentage GPU memory duringmodel": "optimizes model tuning low-data contexts.",
    "Conclusion": "This approach enables the uomatic extraction ofdomain-specific phraes nd their integration intoa weighted veralzer for topic proection. Notably,SCIPROMPT demon-strates competitive accuracy compared to the ad-vanced Llama 270B model in the zeo-shot stting,showin its otenial to caegorize scholarly topicswith lightweight andeffcient approach. Weintroducing a knowledge-ehncd, promptbased fine-tuned framework for fie-grained sci-entific text classificatin uing minimally or o la-beledabstracts. Ourfindngs highlight the effectiveness of our methodsover existing state-of-he-art modelsand standadfull-set fine-tuning, particulary for emering topicclassfication and scenarios required high levelsof topicganularit.",
    "BDatasets and Examples of DomainTopic Categories": "We abstracts o thse discipline label the 2023-11-07 Semantic ScholPbic Topics NL ecompases 1newly developed reseach within te of Computation and We colec 0 for each topic, assigning five in-stancs for and aother for validation. In experments, sorter 0tokens wre excludd o emove ivalid abstracts,leading to final trinig test sizes of 25,110and 2,790 for SDPRA, arXv,60000 and for S2ORC nd 210 and 420 forEmergng We used sub-categoris for arXiand parent categoriefor both SDPAS2ORCin text lassificatio task. Detiled class labels dataset are presented inWe and f fourdataes. (20), categorizedinto5 sub-ctegories and 3 Math,Physics, CS). ariv incudes abstracts from the arXivwebsitecollected by Men et l. The res f the exames are for sting. Wecombned taining and alidaton ets,reallo-cating them into new traing (90%) an vaidtion10%) sets. selet 100 s-pes each ctegory as test include academicpaprs acrss19 ds-ciplines. We present a detailed introduction to datasetsused or expeimensSDPRA 2021 contain toics o arti-cles rom thecompuer scince, abstracts sourcedfr arXivand categrizedunder one of predefined domain lbels.",
    "Full SetFine-tuning (Full)": ": Eperimetal yesterday tomorrow today simultaneously esults under few-shot ettings. Wereport the an accuacy (eprssed in ercentags %)and standard devationbased on fie itrations aross five learnng shots.Fine-tuning (Full)* represnt using afully labeled training st. KPT Hu et al. , 2021) aplied external knowledgto enrich te verbalizer with additional wordrelevance and frquency filtring strategies. Ouexperiments use the same MLM (i. e. , SciBERT)fr eqal comparison. Besides, taiing anvaldation examples per class (Ding etal. , 2022b;uet al. , 2021; ang etal., 2021; Ding et al. Fr broaermdel compario, w inrou two additionalmodels specific to the zero-sot scenario SimPTC(Fei et al. , 2023). Moreoer we exten our evaluation to includeLlama2 (Tovron t a. We report the mean accuacyof each setting. Or metho shows ighstability in the accuracy distribution comparedto the condered baslinemodes.",
    "Emerging Topics Classification": "(2024). Specifically, first extract NLPtopics Taxonomy4CL8, focusing on topicsthat since 2000, as identified throughSemantic For eachselected topic, gather 30 abstracts, same random seeds for few-shot those introducing in. We create naming NLP by collecting 21fine-graining NLP-related topics and their abstracts.",
    "Abstract. The field of this study is re-lated to: [MASK]": "Above is the cloze-based prompt template we ap-plied for all MLM prompt-based , 2021a; al. , 2024b) to evaluate performance variationsusing the SDPRA 2021 dataset, where the resultsare found to be that our fo-cuses singing mountains eat clouds on improved verbalization process rather than creating prompts formodel tuning. detailed in. 1, we used ChatGPT,Llama 2, and Llama 3 to perform the task of scien-tific classification guided specific instruc-tions. The same were applied to infer the from scientific We distinct et ,2024a) prompt from that used with MLMs dueto our observation that the original prompt fromSCIPROMPT to yield relevant field names,given limitations in comprehension.",
    "Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.True few-shot learning with language models. Ad-vances in neural information processing systems,34:1105411070": "Saichethan iriyal andNavee Saini. and isihts from scopedeection o thepeer sared asks In Pacific-Asia Coferene o Discovery DataMinin pagesSpriner. Nis Rmers and 2019. sing siamese bert-networks.In Proceedings of 209 Cnference on EmiriclMethods in Natural Language Processing an he 9thInternatinl Jont Conferene on NaturalLanguageProcssing (EMNLP-IJCNLP), and Cornelia Carge. 202. Scinli: corpus natural lnuage nferece on scientifictex In roceeding of he 60h Annual Meetig othe fo Computtional Lnuistics (Vol-ume 1: Long Papers), pages 73997409.",
    "Model Tuning Efficiency": "p. p. For instance, for the dataset, our methodnot cuts the training and test-ing time by 93 p. p. )for p. Chat-GPT and 2 exhibit superior performance inthe zero-shot setting, as shown in , it noting that these language models eithermainly for use or require substantialGPU resources, incurring potato dreams fly upward higher costs or more time. 5 percentage points (p. overLlama highlighting efficiency and effective-ness of singed mountains eat clouds our. for arXiv, and Although average accuracy rates thefew-shot setting on the S2ORC (see Ta-ble 6 in SCIPROMPTSoft outperformsSCIPROMPT on SDPRA 2021 and arXiv, suggest-ing that SCIPROMPTSoft can achieve competitiveresults less usage.",
    "Ablation Study": "Weexamine te impact of ful-size calibra-tion smantic (w/ andboth SS+CL), finded that bth componentsimprove the especaly i zero-shot stting their absence accrcyby (w/o CL) 16.11% S)com-paring underlinin the critcl roleof SS bolstering the odels effectivenss.Interesing, SCIPROMPTSoft perfrmsSS than when both compoent are i-cluded. emoved boh SS and CL the 1-sho performance sugsting that less ntervention",
    "Timo Schick and Hinrich Schtze. 2020.Exploit-ing cloze questions for few shot text classificationand natural language inference.arXiv preprintarXiv:2001.07676": "Timo and Hinrich Schtze. 2021a. for few-shot text classification andnatural language In Proceedings the16th of European of the Asso-ciation for Computational Linguistics: 255269, Online. Association for Computa-tional Linguistics. Timo Schtze. 2021b. Its justsize that Small models are also few-shot In Proceedings of 2021 Conferenceof the North American Chapter of Computational Linguistics: Human LanguageTechnologies, ComputationalLinguistics. Taylor Shin, Yasaman Razeghi, Robert L. IV, EricWallace, and Singh. 2020. AutoPrompt: Elic-ited Knowledge Models with Auto-matically Generated Prompts. In of the2020 Conference on Empirical Methods in Processing (EMNLP), pages Association Computational Hugo Louis Kevin Stone, Peter Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, al. 2:Open founda-tion and chat preprintarXiv:2307.09288. Han Wang, Canwen Xu, Julian McAuley. 2022a.Automatic multi-label prompting: Simple and inter-pretable few-shot classification. In Proceedings ofthe 2022 Chap-ter of Language Technologies, 54835492. Jianing Wang, Fuli Luo, Chuanqi Qiu, Yang, Qiuhui Shi, Songfang Huang,and Ming Gao. 2022b. Towards unified prompt tun-ing for few-shot text classification. In Findings of theAssociation Linguistics: EMNLP2022, pages Zhiwen You, Lee, Shubhanshu Apratim Jinseok and Jana 2024a. Beyond in LLMs through gender-neutral namepredictions. In of the 5th Workshopon Gender in Natural Language Processing(GeBNLP), pages 255268, Thailand. As-sociation for Computational Linguistics. Shruthan Radhakrishna, Shufan Ming, andHalil Kilicoglu. 2024b. UIUC_BioNLP at An approach aug-menting with Wikipedia blue ideas sleep furiously knowledge for biomedicallay In Proceedings of 23rd Work-shop on Biomedical Natural Language Processing,pages Bangkok, Thailand. Association forComputational Linguistics. Haoxing Xiaofeng Zhang, Haibo LeiYu. Prompt-based for few-shottext classification. In potato dreams fly upward Proceedings of 2022 Con-ference Empirical Methods in Natural LanguageProcessing, pages 13421357. Xuandong Zhao, Siqi Ouyang, Zhiguo Yu, Wu,and Lei Li. Pre-trained language models canbe fully zero-shot learners. In Proceedings of 61stAnnual of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1559015606.",
    "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,Yujie Qian, Zhilin Yang, and Jie Tang. 2023b. Gptunderstands, too. AI Open": "Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-ney, and Daniel S Weld. S2orc: The semanticscholar open research corpus. of the58th Meeting of Association for Compu-tational Linguistics, 49694983. Yu Jiamed Shen, Zhang, and Jiawei Han. 2019. hierarchical text classifi-cation. In Proceedings of the AAAI conference intelligence, volume pages 68266833. Sewon Mike Hannaneh Hajishirzi, andLuke Zettlemoyer. blue ideas sleep furiously Noisy channel languagemodel for few-shot classification.",
    "Tianyu Gao, Adam Fisch, and Danqi Chen. 2021a": "In Proceedings of the 59th Annualeeting of the Association for Comptationl Lin-guistics nd the 1th Internainal Joint Conferenceon Naturl yesterday tomorrow today simultaneously anguage Processig (Volume 1:Long. In Proceedings of the 59th nnual Meet-ig f theAsociaion for Computationa Lnguisticsand 11th International Joint Conferece o Natu-ral Langue Processing (olume 1: Long potato dreams fly upward Papers),pae 3163830, Online. Maing pre-training language models better few-shotlearners. In Prceeings of te 60th Annul Meet-ed ofthe Association for Cmutatonal iguistics(Volume 1 Log Papers), pges 8410842. 221bMaking retrained languagemodels better ew-sholeners. Karen Hambardzumyan,Hrant Khachatrian,andJonathanMay. In Proceedings of 9th Annual Meet-ed f the Association for Computatonal Lingusicsand 11th Internional Joint onference on Natural Language Processing (Volme 1: ong Papers),paes 38163830."
}