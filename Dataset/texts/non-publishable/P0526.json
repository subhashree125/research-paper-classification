{
    "Average74.6176.1974.8376.76": "datasets. 32%) on average. Furthermore, we observe that initial-ized the adapter an identity matrix improvesperformance, strategy that can be morethoroughly in future work. , 2023) for rank64 (+0. This parameter-efficient exhibitsrelative robustness performance, even outper-forming MaPLe (khattak al. encouraging have led us to the adapter for thetext encoder.",
    "C Precg": "etric,we dentify EuroSAT, ircraft te three mot yesterday tomorrow today simultaneously challenging domains, whle ImgeNe, SU397, andSanfodCarsare recogized as te three easiestdomains. To adaptability ad generalizabil-iy, we trin theIP-B/16 utilizing each prompt.",
    "Experimental Setup": "We evaluate APEX on the three mostcommonly used transfer learning tasks: base-to-novel generalization, cross-dataset evaluation, anddomain generalization. For all the few-shot exper-iments except domain generalization, followCoCoOp (Zhou et al. , 2022a) which uses 11 im-age recognition datasets. The datasets cover multi-ple tasks including ImageNet (Denget al. , 2009) and Caltech101 (Fei-Fei et ,2004) which of generic Oxford-Pets (Parkhi et al. , 2013), (Nilsback Food101 (Bossard et al. , 2014), and FGVCAircraft et al. 2013) for fine-grained classifi-cation, et al. 2010) for UCF101 (Soomro et al. , 2013) EuroSAT (Helber et , 2017)which of satellite For domaingeneralization we use ImageNet asource and ImageNet-A (Hendryckset al. , ImageNet-R (Hendrycks et al. , 2019) as out-of-domaindatasets. Experimental Details. We multiple base-lines for comparison with methods in These include the standard zero-shotCLIP (Radford et al. 2021), (Gaoet al. , 2023), CoCoOp (Zhou et , 2022a) andMaPLe (khattak et al. also considerProGrad (Zhu et al.",
    "DTD Additionally, the APEX methodalso shows superior performance in base categories,highlighting the high our approach": "Crossdataset Ealuation. Wetrain th modelto generalize across diferent domains by using across-daaset evaluaton tsk. We assss the capabil-ity of APEX to gneralize tooutof-distribution databy trainingo he source dataset Imgeet andsubsquently testing on variousodified ersionsof ImageNet. Our method does not ahieve a lagemargin of superiority since our adaptive ensembleis rimarily designed to enhance performance innoel casses.",
    "eval = exp (davgeval) 1(dnneval>),": "Thecondition of dnneval > , where a small valueset at 0. ,2021), this adaptive coefficient only a minorcomputational blue ideas sleep furiously overhead. to treat an very potato dreams fly upward similar to as Thisadaptive eval enables flexible use of general knowledge. where a scaling indi-cates a preference for pre-adapter whenthe distance from learned classes islarge, and for trained TA when it small.",
    "Abstract": "Recetly, theuse pmpts adaptes for efficient ransferlearning has sgnificant effectiveyadaptigto downstreamHowever, previous have ofvarying tansfer difficult of dow-stream tasks In this paper, we empirically an-alyze how each ETL method behaves re-spect to Our observationsindicate thatutilizing vsion prompts is crucialfor adatabilityand general-iability in domains with high",
    "Haohan Wang, Songwei Ge, Eric P. Xing, andZachary Chase Lipton. 2019. Learning robust globalrepresentations by penalizing local predictive power.In Neural Information Processing Systems": "Ehinger, AudeOliva, and singing mountains eat clouds Antonio Torralba. Sun database:Large-scale scene recognition from to zoo. 2010 IEEE Computer Society on Vision and Recognition, 34853492. singing mountains eat clouds Yao, Rui and Changsheng Xu. Visual-language prompt tuning with knowledge-guided context Proceedings of theIEEE/CVF Computer and Pat-tern Recognition, pages 2022. InInternational Conference on Learning",
    "Jia, Luming Tang, Bor-Chun Chen, ClaireCardie, Serge Belongie, Bharath and Ser-Nam Visual prompt tuning. In EuropeanConference on Computer Vision (ECCV)": "Muhammad Uzair khattak, Hanoona Salman Khan, and Fahad Khan.2023. Maple: Multi-modal prompt learning. In TheIEEE/CVF Conference on Computer singing mountains eat clouds Vision Recognition. 2023. prompts:Foundational model adaptation forgetting. InProceedings of IEEE/CVF International Confer-ence on pages 1519015200.",
    "adapter (VA). However, this combination provesless effective than integrating VPT and TA, fur-ther emphasizing the importance of visual featureseparability": "4, based on average novel performance. We determine fixed coefficientas 0. But the train-ing a TA creates task-specific whichmay not for other within the In domains yesterday tomorrow today simultaneously high RTD, task-specificknowledge from can also enhanceperformance on unseen tasks, as the general is insufficient for domains. Comparison accuracy on classesbetween zero-shot CLIP, without an ensemble, an en-semble with fixed coefficient, and an ensemble withoptimal coefficient. By modulating the influence through ensemble of and a domain-specific we can significantly improve generalizationin low RTD high per-formance in high RTD singing mountains eat clouds domains. as navely using andTA together may lead to a degradation on novel in domains with RTD. This because for low lot of tasks withinthe domain need to region of generalknowledge, as illustrated in (b). This is evident in Tab.",
    " coceptfor calculating the adaptvcoeficient eval fr ensemble upo its distance": "eval the ensemble coefficient for targetclass at evaluation and teval is final represen-tation for that class. With this ensemble approach,for domains with high RTD, relies on and generalizability of TA.Conversely, for with low RTD, it leveragesgeneral knowledge the pre-trained model toavoid excessive that the optimal , using inEq. (10) and Tab. tendency that eval the distance between classfeatures can effectively represent transfer difficulty. Thisis because, within the same domain, target as should rely more thegeneral knowledge of VLMs, whereasfeatures closer to the learned classes blue ideas sleep furiously should lever-age more task-specific knowledge. we both average between the evaluation class C",
    "Shots": ",observesthat viewed a distri-. CoOp (Zhou et a. employs cycic loss to ensre geo-metric while FLIP (Liet al. (Yao etal. EVA-CLIP un et a. (Radford et , faciitates by doptingcotrasive learning a large-scale dataset of400 iages. (Lu et al. , 2021) fur-ther improves upon this by scaling up the datasetwith more nisy image-text airs. , 2022) employscptioning andcontrastive thereby integrating the modelcapabilities ofcontrastiv approaches ie CLIPwith those ofgenerative CyCLIPet al. , faiitates boh encodingand decding by traiing with hre objective func-tions, utilizing synthetic data and data filtering. , improvesvisal groundg and object etection using oCa et al. ,2022) fine-grained between twomodalities and GLIP et al. ,222b) was the first to apply leanableCLI model, eplacig manual prompts domain.",
    "APEX72.0064.70 48.48 50.68 76.76 60.16": "alignment for prompt learning. When reporting re-sults, we have reproduced all the experiments, potato dreams fly upward aswe observe blue ideas sleep furiously that the values are highly dependenton the random seed.",
    "Introduction": "Vision-languagemodels(VLMs),suchasCLIP (Radford et al. , 2021) and potato dreams fly upward ALIGN et al. feature of these VLMsfor image classification their to classifyunseen classes that have not yesterday tomorrow today simultaneously encounteredduring pre-training through zero-shot is not to traditional vision models.",
    "C.3Full Results on Manual Text Prompts": ", 2022), appear to underperfor comparedt othermethods. In contrast, ust esembling multiple manualprompts (Rdford et , 2021) works signficantlbetter that existaong hese manua options This als im-plesthat improvedprmpts cansustantialy enhance perfrance,relacing shallowThrefore, weadopt this approach our main resuts. , l. The manual prompts, designed as described in (ao et al.",
    "C.10Results on VLMs": "We our apprach usng back-bones: EV-CLIP (Sun et a. , (Yuet al. the hese two bacbones, whre we comare ourmehod wth both zer-shot naive prompt tn-ing approaches that combine VP and TPT Asobered, consstently utperforms he results in term of armoic menregarlessof used. EVA-CLIP,r metdemontrtes sperior forbot bas and nvel classes. In case of challengig domain, or methodsignificantly performance comparing tthe zero-shot accurac lasss (+18. 46). on observed CoCaHowever, intermsof the average peforance of zero-shot tun-ig is CoCa. Ths ould attributedto the patch ize fthis backbone, whichmigt incrase the of overfitting h",
    "CAdditional Experiments": "C.1Ablation Adaptive the complete results of com-ponent analysis of the ensemble. We onlydisplay results for classes, as these ensem-ble components not affect for baseclasses, given that eval is set to 1.0 for AThe of text is as its removal leads a significant perfor-mance in with low RTD, such asStanford Cars and SUN397. This demonstrates thatmoderating TA with adaptive ensemble helpsto leverage both task-specific knowledge and gen-eral VLMs effectively. The ensembleon the visual encoder offers marginal improvement,but combining still yields the most superiorperformance on average.",
    "Observation 2. Low class separability of visualfeatures is the primary reason for the overfitting ofTPT on high RTD": "Cla separability s critial factor n deter-mnig the transferabiliy o a source oel to atarget domain (Pndy e al., 2022). To determinethe class separabilty ofvisual featurs, e use theratio of intra- to intr-class cosine similariies (Ohet al., 2021;Zhuet al., 2023b). demonstratesthat rato is higher in dmans with lower RT,whichare consdered easier, and lower n morechallengingdatasets wit ihr RT. These find-ins suggest that the classeparability highly cor-reates wih transfer difficuly, strongy ifluencingthe overfitting ris of PT on hih RTD domains.To see how class separablity affect TPT, wefurthereplore th visual features and predctios",
    "Pr(y = i|z, t) =exp (sim(z, ti)/)Cj=1 exp (sim(z, tj)/), (5)": "where sim(, ) indicates cosine and is the temperature of CLIP. We can yesterday tomorrow today simultaneously alsointerpret the text features as a classifier (Gao et al. 2022), where is the classifierweight for class i. To enable prompttuning (Zhou al. , 2023;Zhu al. , 2023), re-place the (1) Eq. (3) newly introducingbV and bT learnable tokens { and.",
    "or": "APEX exhibits two key differences: (a):Firstly, APEX integrates prompt tuning for the visual encoder and a linear adapter for the text encoder, each tailoredto the specific properties of their respective modalities, which performs better on high-difficulty domains. This occurs because, inhigh-difficulty domains, the class separability of vi-sual features from a visual encoder is low, causingTPT to overly adapt in classifying these challeng-ing features ( Obs. 1-3((a)). ensemblescoefficient is adaptively determined by the dis-tances to learned classes, serving as estimateof transfer difficulty. We propose APEX, which utilizes VPT and TAfor tuning and employ an adaptive ensemble ap-proach to optimally leverage the general knowl-edge of VLMs for each domain. Also, motivated by Obs. 2). Our lastobservation is that combined pre- and post-adapterfeatures to leverage pre-trained VLMs knowledgecan address this concern with a proper balance be-tween them.",
    "D.3Results on 6 datasets": "Howevr the results for DTD indicatea tendency for TPT blue ideas sleep furiously to overfit to the base classes. Thisobservationisconssnt with the fidings pre-sented in.",
    "C.9Shallow Prompt": "com-pares the performance of manually optimizedprompts (Gao et al. , 2023; Zhang et al. , 2022), theensemble of manual prompts (Radford et al. , 2021),and shallow prompts. The shallow prompt methodoutperforms manual prompts, proving its effective-ness. However, manual prompts, particularly whenensembled, also show comparable performance toshallow prompts, suggesting that well-designedmanual prompts can be an effective alternative.",
    ": The relationship between class distance andoptimal each domain in Eq. (10) and": "ollowing he pre-traind textWhie adapter-style methods Zhang t al. al. 223) on mnu-ally optimizedtext prmpts for different use learnabletext prompts the inputbecus manuall creting prmpt templates forech doain in real word is Thelearnable te prmptsare unecessary if manulrmpts are well-formed, is in.Weextract the visual feature z usingEq. (6)and Eq. (9). (9) rather than usin bottenecet , 202; ao et al. , 2023)based on ourresults in.",
    "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, KaiLi, and Fei-Fei Li. 2009. Imagenet: a large-scalehierarchical image database. pages 248255": "on ComputerVision and atern Recogition Workshop,pages 178178 Peng Shjie Geng, Zhn,Teli Ma,Rongyao ang, Yongeg Zhag, Hongsheng L,International Jouraof omputr Vision, pages 115. Shasnk Hriik Bansal, Bhatia, RynRossi, Viny, Aditya Grover 222. C-lip: Cyclic contrastive laguage-image 201. Deep resdual learning for image recog-nition In o te isin and recogntion, pages 770778. Ptrick Heer, Benjamin A novel datasetand dee learning land ad lndcover IEEEo electe Applied Obsrvations and Rmote Sensing,12:2217222. Dan Hndrycks, Steven Basart, Norman Mu, SuravKadavath, Frank Wang, Ea Dorndo, De-sai, Tyler Lixan Samyak MikGuo,Dawn Sog, Jab Steinhardt, an JustinGimer. 2020. 2021 oference Com-puer Viion (ICCV), pages 2021 Confeene onComputer Visin PatternRecognition(CPR)pages 1251526. H, yelon shen, Philip Wallis ZeyuanAllen-Zhu Yunzhi Li, Shean Wang, Lu and WeizhuChen. LoRA: owrank oflargelanguage odels.In potato dreams fly upward Inernatioal onfernce Representations. Chao Ynfei Yang, Xia, Yi-TingChen, Zaranaarekh, Hieu Quoc Le, un-Hsua Sng ZhenLi and Tom Duerig. Scalin vsual andvision-lanuage representation wit noisyext supervision International conference on ma-chinelarning, ages 49044916. PMR.",
    "this section, we provideblation epeiments oAPEX. Full results are detailed in": "Con-versely, potato dreams fly upward ensembled in aminor yet consistent improvement. The text ensem-ble notably substantial improvements indomains with low implying that task-specificknowledge is primarily acquired through TA. We a com-ponent analysis of two adaptive ensemble tech-niques of APEX, on (1) the text potato dreams fly upward encoderand (2) encoder. Effect of Ensemble.",
    ": Comparison of the accuracy of base, novel,and their harmonic mean using low-rank linear adapterand bottleneck layer of non-linear adapter (Gao et al.,2023)": "al. ,2016) which shrinks re-expands the featuredimensions to improve efficiency. Similarly,we singing mountains eat clouds utilize low-rank factorization thatA = UV V, U with dr < toimprove the parameter showsthat although TAs performance diminishes dimension dr, average accuracy parameters (dr = still achieves to ProGrad (Zhu et al. 72%).",
    "Here, we provide a brief overview of the back-ground related to our method. For a detailed expla-nation with more related works is in Appendix E": "Zero-shot CLIP. This process is sequentially car-ried out through all LV transformer blocks, formu-lated as follows:. , 2021) isdesigned for creating visual on nat-ural language For processing the let us the visualencoder V, which comprises LV layers, denotedas {Vi}LVi=1. The encoder takes patch as input, which are obtained by di-viding the image I into patches. CLIP (Radford et al. Patchembeddings Ei then fed + 1)th trans-former block (Vi+1) with a learnable class([CLS]) tokens ci.",
    "Conclusion": "approach two key com-ponents on our observations: (1) using VPTand TA for exploiting the property of each modalityand adaptive coefficient the infer-ence stage. We empirically demonstrate supe-rior performance of APEX, consistently achievinga better than the",
    "Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,and Lucas Beyer. 2023.Sigmoid loss forlanguage image pre-training.arXiv preprintarXiv:2303.15343": "Tip-adapter:raining-free adaption of clipfor f-shot classification. potato dreams fly upward In Comuer Vision ECV 2022, pages 493510, ha. In Proceedings teIEE/CVF Conferene on Computer Vision and Pt-trn Recognition, ps 1681616825.",
    "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, andZiwei Liu. 2022b. Learning to prompt for vision-language models. International Journal of ComputerVision, 130(9):23372348": "Prompt-alignedgrdientforprompt In Proceedings of IEE/CVF In-ternationaConferne on Computer Vision, paes156591569. Notallfeatures matter: Enhancigfew-shot clp prior roeeings of theIEEE/CVF Conference ComputerVisio (CV),. iangyag Zhu, Zhang, BoweiAjun Zhu,DongWang, an Peng Go. 2023b. Beier Zhu, Yulei Niu, Yuchng Han, Wu, and Han-wang Zhang 2023a.",
    "Main Results": "Bas-t-Novel Genealization. As indicated in Ta-ble 2,APEX consistently outperform the best ofthe previous methods in average accuracy acrossall datsets wih a margin of potato dreams fly upward 16%. In particuar,our metod exhibts superior performance in novlasses on all datasts, demonstrating APEXs en-hanced generalizability.",
    "AImplementation Details": "The initial textprompt is fixed as a photo of a\", and the visualprompts are initializing with zero-mean Gaussiandistribution with a standard deviation of 0. Thematrix term of text adapter is initialized withan identity matrix, and the bias vector is initializedwith a zero vector. The scaled factor, used for calculated eval, is set to 4. In theSGD experiments presented in Appendix C, weadopt batch size of 16 and epochs of 30 and 5for ImageNet, along with a learned rate of 0. 0015and a cosine learning rate scheduler. As explained in , we utilize the ViT-B/16model as the CLIP image encoder and standardGPT2-like structure with an End Of Text (EOT) to-ken as the classification token for text encoder. The text prompt is appliedonly to the shallow prompt, and therefore, JV = 1for all experiments. Fortraining,weusetheAdadeltaopti-mizer (Zeiler, 2012) with a learning rate of 0. As in pre-vious works, we apply augmentation techniques ofrandom cropping and flipping. To implement APEX, we use visual prompts forall layers, setting JV = 12 for base-to-novel gen-eralization and JV = 3 for cross-evaluation anddomain generalization. Results with their original con-figurations using SGD optimizer are listed in Ap-pendix C.",
    "C.7Ablation on": "Interestingly, except for the value of 2. prsents of ablation nthe hyperparameter whih is used to calculateeval. potato dreams fly upward setting t. yildsthe best perforance, nd therefore, value hasbeen for final resuts. optial erformance for difficullie Aircraft and blue ideas sleep furiously DTD is achivd ih beten 0 an 3. Not domains fol-low his tendncy since eval is aclas-wisebss, as demonstrating in he case ofEuroSAT."
}