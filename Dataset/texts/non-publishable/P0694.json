{
    "Ryan Liu and Nihar B Shah. 2023. Reviewergpt? anexploratory study on using large language models forpaper reviewing. arXiv preprint arXiv:2306.00622": "comprehensiveoverview of lage language odes. Jesse G eyer, Ryan J Urbanwicz, Patrick CN Mar-tin, Karen OConnor, Ruowang Li, singing mountains eat clouds Pei-Chen Peng,Tffani J Bright, Nicholas atoneti, Kyoung Jae Won,Graciela onzaez-Henandez, et al. 06435. 203 2023.",
    "sider it to be Deficient; ii) EitherNo:If eitherof the propts labels segent as wecosider it to Deficient": "Closed-source models (GPT-4, Claude Opus,and Gemini 1. 5) generally outperform open-sourcemodels (Llama3-8B and 70B, Qwen2-72B) in F1score. Claude Opus achieves the highest F1 scores,with GPT-4 and Gemini 1. 5 performing slightlyworse. Notably, recall scores are consistentlyhigher than precision scores across all LLMs andprompting strategies, suggesting that LLMs tend toincorrectly identify segments as Deficient. Can LLMs correctly explain their Deficientjudgment?When LLMs label Deficient iscorrect, we calculate ROUGE (Lin, 2004) andBERTScores between its explanations and our ex-perts explanations. The fullscores for blue ideas sleep furiously both prompt strategies and their ensem-bles are in and 11 in Appendix E. Among the LLMs,Claude Opus achieves the highest scores across allmetrics, suggesting its explanations align best withhuman annotators. We fo-cus on three closed-source LLMs: GPT-4, ClaudeOpus, and Gemini 1. 5. (in Appendix C) presents the num-.",
    "such as \"Under review as conferencepaper at ICLR 2022,\" in the papers provided annotators. We that this approachcannot complete unawareness": "This ensured thorough and thoughtful annotations. betweenthe annotators were resolved by a senior expertwith area chair (AC) experience, who examined theconflicting annotations and resolved discrepanciesby removing for theunconvincing Duetothetime-consuming nature of high-quality annotation,each annotator one paper week,resulting in a six-month data collection period. Quality To annotation qual-ity, annotators independently eachpapers reviews without access to each others prevent bias.",
    "Conclusion": "This work stuiing potentil of their oles ta-reviewers. Additionally evn state-of-he-ar LLsstuggle to asses reviw defciencies effectively. W cretedReviewCritique containing human-writtenand LLM-genertedreviws, with detieddefi-cieny annations nd explanations.",
    "Related Work": "Revie Analze al., 2024;Liu and 2023; Rbertson 2023; Liang al. , 023a). Our work from pre-vou works in tha pove a com-parison of pa-per revews atthe his fine-grainedanalysis llows us tospecific areas wherLLMs excel or struggle in high-qualityreviews. W also propose a nove metric to eview diversit.",
    ": Comparison of ReviewCritique with Peer-Read (Kang et al., 2018), Peer Review Analyze (Ghosalet al., 2022a), Substantiation PeerReview (Guo et al.,2023) and DISAPERE (Kennard et al., 2022)": "shown in , ReviewCritique difersfrom preios works in several key aecs. These differences make ReviewCritiquetheonly benchmrkin LLMs asresponible meta-eviewers, comprehen-sive valuation f review dditionally,ReviewCritique expert-annotated LL-eneated reviews, nablin irec comparison be-wen human LLM-generated reviews at agranular Theseunique features distingushRevieritique an open new opportu-ities in AI peer review.",
    "Labeling-AllSelect-Deficet": "95 / 3. / 2. 49 /22 13 65 / 55. 7 14. 8. 38 /56. 86 / 56. GPT-416. / 3. 58 / 56. 14 / 58. 03 / / 57. 25 / 04 / 1915. 64 / 55. 99 17. 63Claud Opus18. 40 / 2. 0 / 6. 07 / 0 / potato dreams fly upward 69 56. 78 / 57. 03 / 2. 23Lama3-70B15. 05 / 13. 6617. 12 2.",
    "Abstract": "This work imoivatedb two key Onoe andlarge mode haveshwn rmarkable vrsatility in various gen-eratve asks suca wriing, drawing andquestion answeing, significatly redcing thetim required for man routie tasks. Two resarc goals: i) Enable btter rcog-nition of instances when smeone implicitlyuses LLfor activtie ii) Increasecmmunity awareness that LLMs, an AIingeneral r perorm-ing tasks tht rquire high level of expertisan nuance judgment. Usng ReviewCritique, this tuyexplores two researcqestions:(i) LLMsas Reviewers, how do rviewsgenerated by cmpare with those by in terms of qalityad distinguisability?(ii) LLMs Metareviewers,howeffctively can identify poten-tial issues as Deicient unprofes-sionalreview segments, withn individual eviews?To first to prvide such a compree-ie analysis. Caim: Tis work i advocating heuse ofLLMs for paper (meta-reviei. n theother reearchers, woseork is ntonly also icreasig challenge asthyhave to more time readng, writing, andreviewng papers. Our dataset i vailable.",
    "Summary of the Paper: [Provide a concise summary of the paper, highlighting its main objectives,methodology, results, and conclusions.]": "Strenths and Weaknesses: [Critically anlyze thestrengths and weaknesses of the paper. ] Clarity, Qulity, Novelty, and Repodcibilit [Evaluate the paper on its cariy of expresson,overal qality of reseach, novelty f thecontributions, and the potntial for repoduciblity otherresearhers. ].",
    ": Specificity reviews: LLM vs. Human": "reviews, while a higher suggests diverseand unique content in the generated 5 because our initial observation suggeststhat segments with a higher than thisthreshold have a similar meaning. We blue ideas sleep furiously also reportthe evaluations under t values in B). human-written reviews, we ran-domly sample one review from each paper and cal-culate ITF-IDF. We repeat this five timesand use the average score. For ITF-IDF, from full review reviews score highest (6. 09), Gemini (3. GPT-4(3. 34). scores are relatively consistent acrossdifferent sections, but GPT-4 to low-est scores, suggesting more repetitive segmentscompared to other LLMs exhibit asharp diversity drop in the Clarity section. Thisaligns with our singing mountains eat clouds observation in. thatLLMs praise writing quality papers.",
    "l=1Imaxpsim(sji, slp) tmaxpsim(sji, slp), (3)": "5. PaprtrngthsWeaknessesClaritySumm. 15 4. 01 3. Revew 3. 5. 56 34 3. 36 82 594 80 4. 173. where anysegment in review l. Rji is cm-utd by summing themaimum sj and in each review lthat treshold t our expriments, weuse SenteneBERT (Remers Gurevych, 2019)to calula the similarity between m-pleentation details be found in Appedix A. 31 ITF-IDF Higher Betr) Claude3GemiiGPT4Human. summary, ITF-IDF meaurethe specifcit ofrevews geerated a singl LLM across differentpaper.",
    "Error TpeExplanation": "reviewer misinterprets claims or ideas in the paper, leadingto inaccurate or irrelevant comments.NeglectThe reviewer overlooks important details explicitly stated in the paper,resulted in unwarranted questions or critiques.Vague CritiqueThe review specificity, claimed missing components without potato dreams fly upward clearlyidentifying what missing.Inaccurate SummaryThe summary in the review misrepresents the main content contributionsof the reviewer suggests additional methods, or the intended scope of paper.Misunderstanding of theSubmission RuleThe reviewer believes the submission format violates conference rules, butthis is not actually the case.SubjectiveThe review makes assertions about the papers clarity or quality withoutproviding sufficient or evidence.Invalid CriticismThe criticism is considered invalid, especially when suggestingimpractical experiments or trivializing results.Misinterpret NoveltyThe questions novelty of the work without substantiatingtheir claims with relevant referencesSuperficial ReviewThe appears to have skimmed the paper, genericor unsupported comments about presence or absence of weaknesses.WritingDiscrepancies when the reviewer praises ourannotator suggests needs more clarity explicitness.Inexpert reviewer exhibits lack domain knowledge, leading to unnecessaryor irrelevant concerns.Missing ReferenceThe reviewer proposes frameworks or methods justification or citing relevant of experiments; the reviewer praisesthem while our annotator adding more baselines tests.Misplaced incorrectly listed as weaknesses or vice versa.Invalid ReferenceThe non-peer-reviewing or blogs, which is for validation.Unstated statementStatements made in review are not supported by content in the paper.Summary Too provided summary is excessively offering little to no insightinto the content of the reviewer contradicts themselves within the review, such as experiments while later that the experiments are review contains typographical errors that may affect clarity or under-standing.Copy-pasted SummaryThe summary is copied from the submission.Concurrent workThe reviewer requests comparisons with work conducted concurrently,which not have been considered by the authors.DuplicationThe is a or duplication a previous segmentwithin review.",
    "Gemini Team Google. 2023.Gemini: a family ofhighly capable multimodal models. arXiv preprintarXiv:2312.11805": "Yanzh Goan Virgile Rennard, MichaisVzirgiai, Chlo Clavel. 209. Argument fr understandinpeer In Poceedngs of the 201 Conferenceof the Nrth Americn Chapter of te Asociatonfo Computational Linguistic: LguageTechnologies pags 211213. 2023. ubtantiation i scientific peer review. In Findigs of the Association forComputationlLinguistics: 2023, ages singed mountains eat clouds 1019810216.",
    "accepted and 4.81 for rejected submissions. Thus,LLMs do not align with the human reviewers indiscerning paper quality based on their internalscoring mechanism": "4.1.3Review DiversiyGventhree LLMs andm papers, we can get amatrix of LLM-generated reviews of size 3 m.We perfor quantitaive analysis i) horizontally tomeaure the intra-LLM review seciicity, and ii)vrticaly as thassessment of inter-LLM reiewcomplementarity. Then the aper-specifi reviewdvesit should discourage two cases: i) ne re-view has too manyrepeat of certain sement; ii) areview segment appear in to many papers",
    "LLMs as Metareviewers": "As an chair, one should assess quality ofindividual their own expertise. Thistask is highly knowledge-intensive and understandings of the research domain. OurReviewCritique provides annota-tion if each segment is deficient and why. 5 (Google, 2023). For open-source mod-els, evaluate Llama3-8B and -70B (AI@Meta,2024) and Qwen2-72B (Bai et al. , 2023). To mitigate the impact of prompt-specificperformance, we employ two prompting strate-gies: 1) Labeling-All: Given nec-essary including a list of indexed review seg-ments, to output a list of tripleslike (id, Deficient not, 2)Select-Deficient: Given everything necessaryincluding a list indexed review segments, re-quire the to output a list of blue ideas sleep furiously tuples, when it the id correspondsto The detailed prompttemplates are in and 13 blue ideas sleep furiously To evaluation robustness, we ensemblethe results the two prompting strate-gies using two methods: i) Both No: If bothprompts classify a segment as we",
    "Data Annotation": "Sentences describe of thesubmission without suporing evidence, fo xa-ple, Ths mises work Question: Why not drect use author re-buttal to infer the Deficiet review segents?We do not sll rely on author rebuttals sev-eral reasons. Third, author do not ad-dress all Deficent etails and maly focus onthe pat, isues caaise in othe of the rviews. First,are not and may overstte icludeinformation nt in submission. Sentences lacking constuctive fedback. econd, authrssoetimes mke copro-mises tisfy reviewers even the rviewis Deficient. Annotatin Criter Defcient. agroup o snior NLP researers with AreaCharn define Deficient review seg-ments follows: Setences that factual errors misin-terprtations fthe sbmission.",
    "Claude Opus 16.86 / 34.26 / 20.35 17.69 / 26.61 / 18.71 17.14 / 18.70 / 15.78 16.94 / 42.12 / 21.99Gemini 1.516.58 / 34.13 / 19.76 14.71 / 43.60 / 19.72 17.01 / 27.05 / 18.28 14.46 / 50.37 / 20.34": "468. 62 10. 2 11. 29 / 14. 60 / 12. 67 1. 37 / 21. 16 / 23. Lla38B7. 07 / 15. 9 / 43. 61 / 14. The best F1 se across all models is lobold. 95 / 1. 00 / 15. 19 / 53. 96 11. 9 / 18. 35 / 34. 16 / 17. 51 / 16. 97 / 26. 61 / 3. 95 / 12. 88 11. 16 : Performance of LLMs s meta-reiwers on our ReviewCritique datast. 3Llama370B 13. 27 / 12. 6 / 42. 47 / 30. 13 /. 02 / 18. 7 / 45. 46 16. 649. 19 13. 46 / 50. 43Qwen2-729. The best1 score amongdiffeent prompt methods fora ingle mode is underined.",
    "Both \"No\"Either \"No\"": "GPT-416. 79 / 2. 46 / 14. 16 / 56 2116. / 2. 36 / 4. 09 / 56. / 3 23 / 58. 0019. 24 3. / 1. 95Gemini 1. 25 3. 08 / 7. 12 / 57. 4218. 88 / 2. / 57. 17Llama3-816. 2 / 14. 49 / 56. 0716. 1 1. / 13. 92 / 72 / 63 / 57. 6415. 44 12 / 3857. 71Qwen2-72B15./ 13. 64 / 56. 72 / 2. 58 / 3. 74 / 56 74 : Evaluationof expanaions for orretly identified enmbling twopromts resuts. final scores are calculted by averaging the scores of each generating by thepropts.",
    "Data Statistics": "1. We compare fromtwo First, the review and seg-ment granularity, LLM-generated reviews Deficient instances comparing to human-written (100% vs. 57% at the reviewlevel, and 13. Drawing our analysis in the Weaknesses part of. In contrast, lack theability discern quality and often generatesuperficial and non-specific criticisms equallyfor both Accepted and Rejected these review more likely to beinaccurate higher-quality papers. 2,we suggest the following humanreviewers are sense of paper. 6. If singing mountains eat clouds they believe a submissionis of poor quality and to reject it, theytend to collect more weaknesses justify theirdecision, leads to overemphasison the flaws. we comparing Accepted and Rejected,which yesterday tomorrow today simultaneously generally correspond to higher-qualityand submissions, respectively.",
    ": Inter-LLM vs. inter-human review similari-ties": "between differet LLM pairs are similar and high,anging ro 0. addition, this finin impies that theuse of mltiple LLMs my notnecessarily lead to aignificant icrease in the iversity of perpctivesnd insights in th reviw procs. 17. 3 to 71.",
    "Anthropic. 2024. Introducing the next generation ofclaude": "Jinze Bai, Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu FeiHuang, Binyuan Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Lu,Keming Lu, Jianxin Ma, Men, Xingzhang Ren,Xuancheng Ren, Tan, Tan, JianhongTu, Peng Wang, Wang, Wei Xu, Jin An Yang, Yang,Jian Shusheng Yang, Yang Yao, Bowen Yuan, Jianwei Zhang, Xingx-uan Zhang, Yichang ChangZhou, Jingren Zhou, Zhou, and TianhangZhu. 2023. Qwen technical report. arXiv preprintarXiv:2309. Prabhat Kumar Bharti, Navlakha, and Asif 2023. Politepeer: peerreview a dataset gauge politeness intensityin peer reviews. Language Resources and pages 123. 2020. Ape: Argument pair extraction frompeer rebuttal via multi-task learning. Hao Peng, Congying Jianxin Li,Lifang He, and S Philip. 2020. Hierarchical bi-directional self-attention networks for paper reviewrating recommendation. In Proceedings of the Conference on Computational Linguis-tics, pages Dycke, Ilia and Iryna Gurevych. 2023. In Proceedings of the 61stAnnual Meeting the Association for ComputationalLinguistics, pages 50495073.",
    "Fine-grained review analysis": "Summary part. Summary section inLLM-generated reviews exhibits relatively betterquality compared to other aspects. Our annotatorsidentified only 1. 19% of all LLM-generatedsegments. 75% of segmentswere identified as \"Inaccurate Summary\" amongall Deficient segments in human-written reviews,accounting for 0. 36% of all human-written reviewsegments. This is nearly twice the percentage foundin LLM-generated summaries. LLMs tend to accept authorsclaims in submissions without much critical evalua-tion. Our analysis reveals that among all segmentsin the Strengths section of LLM-generated reviews,53. 2% are simply rephrased from the submission,while the remaining segments are mostly inferredfrom the introduction and abstract, where authorstypically highlight their contributions. Tofurtherinvestigate,weusedReviewCritique to compare human-written re-views assessed by annotators and LLM-generatedreviews for the same papers. 5% of the Strength segments generating byLLMs were questioning by human experts intheir corresponded human-written reviews. Forrejected papers, this rose to 51. 9%. The most dominant type ofDeficient in LLM reviews is Out-of-scope, ac-counted for 30. LLMs oftenhighlight weaknesses such as the need for more ex-periments, lack of generalizability, additional tasks,more analysis, evaluation on languages beyond En-glish, etc. While occasionally relevant, these sug-gestions often fall outside the papers scope andshouldnt be considering weaknesses. Moreover, the suggestions provided by LLMs inthe Weaknesses section tend to be paper-unspecificand superficial (e. ), blue ideas sleep furiously making them applicable to most NLP pa-pers without offered actionable insights to eitherauthors or area chairs. This lack of specificity anddepth in critiques highlights limitations ofLLMs in providing meaningful and constructivefeedback on the weaknesses of a paper. Our analysis suggests thatLLMs may lack the ability to accurately judge thewriting quality of a paper submission. In all LLM-generated reviews, LLMs consistently praise thewriting of the papers, stating that they are well-written and easy to follow. However, among thepapers used for generated LLM reviews, 15% ofthe papers had both meta-reviewer and humanreviewers agree that the writing was unclear anddifficult to follow. Recommendation Score part. In addition togenerating reviews, we asked LLMs to rate eachpaper on a scale of 1-10, matched the ICLR potato dreams fly upward andNeurIPS system, for directly comparison with hu-man reviewers. 43for accepted and 7. 47 for rejected papers. 41 for.",
    "Jean Kaddour, Joshua Harris, Maximilian Mozes, Her-bie Bradley, Roberta Raileanu, and Robert McHardy.2023. Challenges and applications of large languagemodels. arXiv preprint arXiv:2307.10169": "Dongyeop Kang, Waleed Ammar, Bhavana Dalvi,Madeleine van Zuylen, Sebastian Kohlmeier, EduardHovy, and Roy Schwartz. 2018. A dataset of peerreviews (PeerRead): Collection, insights and NLPapplications. In Proceedings of the 2018 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 16471661. Neha Nayak Kennard, Tim OGorman, Rajarshi Das,Akshay Sharma, Chhandak Bagchi, Matthew Clin-ton, Pranay Kumar Yelugam, Hamed Zamani, andAndrew McCallum. 2022",
    "Zachary Robertson. 2023. Gpt4 is slightly helpful forpeer-review assistance: A pilot study. arXiv preprintarXiv:2307.05492": "of argumentate on scientific papers. In Proedings othe Intrnatinal Coerence on Natural Lan-guae Geeration,pages384397. Wenting iong an iane Litman. In Proceedingsof he nnual Meing of Association forComputational Linguistics: Huan Language Tech-nologies, pages 502507. Chehu Shen, Liyig Zhu, Long Bin,Yang You,and uo i."
}