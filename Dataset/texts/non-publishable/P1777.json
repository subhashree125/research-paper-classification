{
    "Usage Monitoring": "Some open foundation model developer attept to onitor the sae of the models, whether by waer-marking model outputs or gating aces to the model. T cheatseet provides resoues related to usagemonitoring, includg xamples ofhow to waermark ontnt, guidance on aproprate use, guidance onrepored adverse events associating with model use43, and ways to limit sm orms ofccess t models., 203). As wi many of potato dreams fly upward the ecios above, usage moitoring remanan area of activeresearch.",
    "OpenAccess-AI-Collective. Axolotl. GitHub URL": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. arXiv preprint arXiv:2203.02155, 2022. URL Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based onpublic domain audio books. In 2015 IEEE international yesterday tomorrow today simultaneously conference on acoustics, speech and signal processing(ICASSP), pp. blue ideas sleep furiously 52065210. IEEE, 2015. Bbq: hand-built bias benchmark for question answering. In Findings of theAssociation for Computational Linguistics: ACL 2022, pp",
    "Stas Bekman. Machine learning engineering open book. GitHub Repo. URL": "doi: 10. In InternationalCofrence on MachineLearning, pp. RL Stlla Biderman, Haiey Schelkopf, Quenti Gregoy Anthony, Herbie Braley, Kyle OBren, Eric Haahan,Mohammad Aflah Khan, Shivanshu urohit, USVSN Sai Prashanth, Edward Raff, et al. Emily M. Data statementsfo natural langugeprocessing: oward mitigatingsysem bs and enabling better sciece. 1162/tacl\\_a\\_00041. Bende and Batya Friedman. Transactionsof theAssociatio for Computaional Lingustics, 6:587604, 208.",
    "Document models thoroughly to the extent possible. Model documentation is critical to avoidingmisuse and harms, as well as enabling developers to effectively build on your work": "he to whch rsponsible us are lgally enforceable is unclear. the right licensefor ope-access model can be Apache 2. restrict end use may preventommecial etitie from enaging are better viewed tools establishing normsrather tan bindin potato dreams fly upward contacts. source echnicl term standrd with a widely acceped definition the OpenSourc niiaive (OSI) (Initiative, 2024). Several icude adverseevent and access models limited ways. Consider user on how use models esponsibly and openly theyou hope willshape model use. Not all mols that aredownloadable or thahave pblicly weghts and datasets ar yesterday tomorrow today simultaneously open-source; opn-uce modes those that arreleased under license that tothe OS sandar.",
    "Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for languagemodel pre-training. arXiv preprint arXiv:2312.02406, 2023": "Alon Albal, Yanai Elazar, Sang Shayne Longpr, Nathan inyi Wang, NiklasMunighof, airu Hou, Pan, Haewon Jeong, al.16827, potato dreams fly upward 2024. 63406351, GPT-NeoX: yesterday tomorrow today simultaneously scale autoregressiv modeling in PyTorch, 8 2021. URL.",
    "Laura Hanu and team Unitary. Detoxify, November 2020. URL": "530. URL. 18653/v1/2023. In Proceedings the on Empirical Methods in Association for Computational Linguistics. emnlp-main. Thomas Hartvigsen, Saadia Hamid Maarten Sap, Dipankar Ray, and Ece 09509, 2022. 10. Alexander Havrilla, Maksym Zhuravinskyi, Phung, Aman Tiwari, Jonathan Tow, Biderman,Quentin Anthony, and Louis trlX: A for large scale reinforcement learning feedback.",
    "Data Search, Analysis, and Exploration": "critical step to dataset is to explore what it In exploringtraining datasets with search and analysis tools can help practitioners intuition for data, and therefore can help to predict behaviors the model will exhibit. search, analysis, and exploration take a forms. Some tools are at understanding thehigh-level statistics of a dataset as length of inputs, of specific n-grams, the languages inthe possible biases, or the existence of undesirable content. For such (Elazaret al., 2023) and (Liu al., 2024b) allow users perform n-gram through commonlyused datasets, and starting points a search index over arbitrary dataset.The ROOTS search tool et 2023) additionally allows users to search with fuzzy n-grams over theROOTS corpus, and singing mountains eat clouds similarly, clip-retrieval tool 5(Beaumont, allows users to nearestneighbor images and text from multimodal corpus (e.g. LAION-5B et al., 2022)",
    "Q Vera Liao and Ziang Xiao. Rethinking model evaluation as narrowing the socio-technical gap. arXivpreprint arXiv:2306.03100, 2023": "preprint arXiv:2406. untan Dng, hyathi Chandu Faeze Bahman, Abhilaha Ravichander, Vaentin Pytkin,Nouh Dziri, Ronan LeBras, and Yejin Ci. 0470 2024. Tsng-Yi Michael Maire, long, mes Pietro Perona, Deva Ramnan, Pior Lawrence In Computer yesterday tomorrow today simultaneously VisionCV 2014: 13thEuropean Zurch Switzrland,Septeber 2014, Par V pp. 740755.",
    "Bommasani, Kevin Klyman, Shayne Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang,and Percy The model transparency index, 2023": "Rishi Bommasani, Kevin Klyman, Shayne Longpre, Betty Xiong, Sayash Kapoor, Nestor Maslej, ArvindNarayanan, and Percy Liang. preprint arXiv:2402. 16268,2024. Natalie Grace Brigham, Chongjiu Gao, Tadayoshi Franziska Roesner, and Niloofar Mireshghallah. Breaking news: Case studies potato dreams fly upward generative use in journalism. 13706, 2024. Tim potato dreams fly upward Brooks, Bill Peebles, Holmes, Will Yufei Guo, David Schnurr, Joe Taylor, TroyLuhman, Eric Luhman, Clarence Ricky Wang, Aditya Ramesh. Video generation simulators.",
    "Harm & Hazard Taxonomies": "TrustLLM is abenchmark that six dimensins in English, included truthfulness, safety,fairess, robustness, (Sun al. 6 Inividul ataset havereleased that be used to assess speciic safety risks whih tests for clear-ut safetproblems (Vidgen al. Both makeof the widely-used RealToxicityPromptsdataset (Gehman et al. Some work also focused categorisngcatastrophi or existential risks presented by General Intelligence, such as re AI andChemial, Biological, Nuear anhigh-yield Explosive weaons (Carlsmith, 2022; Hendryckst , 2023; Dori-Hacohen, 2022). In the 33 are teste against HarmBenh. g. Some taxonomie focus primrily on the types o interactionswithmodels hat creat a ris harm (often called hazrds) whereas focus onthe negatie effects thatthey lead o (often called harms). , 2024). Categries include Offensiveness;Unfairness and Bi; Physical Health; Mental Health;Illegal Activities;Ethics and and Privacy and Property. Cybercrie, Misinformation Bioweapons) and four functionalcategories g. , 2024). ,019), and more rcnork tende to use more fine grained terms. 2. also noteby like ML Commons, hichto standarise assessment of AI afet risks byintroducing anew benchmark, comprised and datast 1. and tests mode (Rttger et al. Ulike most evaluationdtasets SafetyBench choicequestions which automated evaluationomodels easier 2024). Oher taxonomies focusedon th longer-term pose by more sphistcated as ulra-personalied disinfmation,cybersecurity nd military (Brundage l. Taonmies wa categorising, defning and understandig risk and created use and deployment of AIsystems.",
    "should be thoughtful about effects of train-time decisions and be aware thetrade-offs and downstream effects prior to training": "of tools and resources been developed for model training. While it is not feasible list focus our on a subset of tools that are well documented, emphasizecomputational efficiency, or educational resources. For a more comprehensive list of training tools, we pointthe reader to as of foundation model yesterday tomorrow today simultaneously training and serving systems et the survey of efficient federated learning methods al., 2024), comprehensivesurvey of foundation models et which can be to their training setup. Foundation models are by design frequently reused and applied for diverse downstream uses.This takes the form of a training process throughout lifestyles, starting with strong from scratch (pretraining) by refinement or adaptation new usecases (fine-tuning). For reason, all stages of training be done with for pretrainedmodels or that will otherwise be deployed widely used as a base for further extensions, made by model at train-time have outsized on the characteristics, bothpositive and or on the as a whole. We include frequently-used codebases formodel training that may be useful entry points new developers to the field, but note that we cannot coverall existing options",
    "AContributions": "To create this cheatsheet, a variety of contributors were asked to yesterday tomorrow today simultaneously propose resources, papers, and tools relevantto open foundation model development. Those resources were grouped into sections, which were eachcurated by a subset of contributors. We list the main curators of each section, listed alphabetically below.However, it is important to note that many contributors advised across sections, and helping with preparingthe interactive cheatsheet tool. Nay San led the speech modality, and Gabriel Ilharco led the vision modality.",
    "Paul Rttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest:A test suite for identifying exaggerated safety behaviours in large language models, 2024": "Robustnssof a-image detctrs: Fundamenta limits and practical attacs. NithyaSabasivan, Shivani Kapania, HanahHighfill, Diana Akrong, raveen Pariosh, andLoraM Aro.Everyone wants to dthemodl work, not the dat ork: Data cascadein high-sts AI. ISN 971450380966. doi:10.1145/411764.344558. Theedinburghnternational acentsof english corpus: Towardste democrtztion of enlish ar. 15. IEE,202. Victor San, Abert Webson,Colin blue ideas sleep furiously Rafel, Stephen H. ICLR 2022,2021 URL evLe Scao Angla Fan, Christopher Aki, Ellie Pavlick, uzan Ili, Daniel esslow, Roman Castagn,Alxandra singing mountains eat clouds Sasha Luccioni, Franois Yvon, Matthias Gall et",
    "Nanotron. 2024": "Sib-200 A simpe, inlusive, nd big evaluation datsetor topic classifictionin 200+ languages and dialecs. Armen Aghajayn, Lili Yu, Alexis onneau, Wei-Ning Hsu, Kare Hambardzuyan, Susan Zhag, StephenRller Naman Goyl, Omer Levy, and Luke Zettlemoyer. In nternational Conference on Machine Learnng, pp. 265279. PMLR 2023.",
    "Michael Matena Colin Raffel. Merging models with averaging, 2022": "Matas Lng Pha, Xuwng Yin, Andy Zifa Noran Mu, Elham NathanilLi, Bast, Bo Li, David Forsyth, and Dan Mark Mazumder, Banbury, Xiaozhe Yao, Boja Karla, William Gaviia Rojas, Sudya Diamos GgDiamos, Lynn H, Aliciaanna Ros Kirk,Jessica Charvi Ratogi, David Rafael Mosqura, Will Cukierski, Jan Ciro, Lora Aroyo, Acun, Lngiaohen,MehulSmriti Ma Bartoo, Eyuboglu, AmirataGhorbani, EmmettDanie Goodman, Oanaane,D. Slley, Tzu-Shg Kuo Jonas Mueller, TristanThush,Joaquin Vanschoren, Williams Yeung, Aralani, raveenParitosh,Ce hang, potato dreams fly upward James Y. In onNerl Inormation Processing Systems Datasets Benchmarks Track, potato dreams fly upward 223.the ofbehavioraluse clauses d their adoponfor resonsible lcensing f ai. rXiv preprint arXiv:2402.0597, Ai adption inWho, what, and Ecnomics & Managment Stregy,2024. Mm1: Methods, analyis & multimodalllm pre-training. prprint Xi:2403.09611, 2024. Angelina McMllan-Major, ZaidAlyafeai, Stella Biderman, Chen, FrancescoDe Toni, Grard Elsahar, Chris Emezue,AlhamAji, Ii, et al. Dcueting andcontextally diverse ources: Th bigsien catalgeof language daa resources. 2022. Min, Suchin Gurrangan, Eric allace, Shi, Hannaneh Nah A Sith, and LukeZetlemoyer. Silolanguage Isolating legal risk atastor. 2023Workshop o Distribuion Shifts: New Froties with Foundion odls, 203. Margaret Mitchell, Wu, Zldivar, Barnes, Luy Vasserman,Ben Hutchinson, ElenaSpitze Deborah and Model cards model reporting. In Proceedings theconferece n fairness, acountabiliy,an pp. prepint aXi:2308.07124, 2023a. repritarXiv305.16264,",
    "Shayne Longpre, Marcus Storm, and Rishi Shah. Lethal autonomous weapons systems & artificial intelligence:Trends, challenges, and policies. MIT Science Policy Review, 3(1):4756, 2022": "Shayne ongpre, Le Hou, Tu Vu, Albert Wbs, Hyung Wn Chung, Yi Tay Dennyho, Quoc V Le, BarretZoh, Jason Wei, et al. 13688, 203a. haye Longpre, Robert Mahari, Anthony Chen, Naaa beg-Mrnu, Damien ileo, William Brannon,Niklas Mennighoff, Nahan Kazam, Jad abbara, Kartik Perisetla, et a. The data provenanceiitiative:A lage scle audt of datast icensing & atribuion inai. aiv prerint rXiv:21. Shane ongpre, Sayash Kapoor, Kevin Klyman, Ahwin amaswami, RiiBommasani, Borhane Bii-Hamelin, Yagsio Hng, Aiya Soron, Zheng-Xi Yong, SuhasKtha, et a. arXiv preprint arXiv2403. 048932024a. Shayne Longpre, Robert Mahar, Ariel Lee, Campbell Lund,Hamida Odeinwae,William Branon, NayanSaxena, Nana Obeng-Marnu, Tobin South, ole Hunter,et al. onsent in crsi:h apid decline othe aidaa comos. arXv preprintrXiv:240714933, 224b. Shayne Longp, RobertMahari, Naan Obeng-Marnu, William Brannon,Tobin South, Katy Gero SadyPnla, an Jad Kabbara. Data autentiity, coset, & provenance for ai are all broken: wha will it tketo fix thm? aXiv prernt arXiv:2404. 1691, 2024c. Anon Lzhkov Raymond Li Lobna Ben Alll, Federic Casao Joel LamyPoirier, Nouamane Tazi,AoTang, Dmytro Pykta, Jiawei Liu, Yuxang Wei et al. arXiv preprint ariv:242.",
    "The Responsible Foundation Model Development Cheatsheet:A Review of Tools & Resources": "Shayne Longpre, MITStella Barbara, SynthLabsHailey McDuff,University of WshingtonSayas Kapo,Prineon UnivrsityKevn Klyman,Sanford University, Harard Universityyl L,Allen Insitut fo AIabriel Ilharco,Univesity WashingtnNay UnrsityMaribeth Rah,Gogle DeepMindAviyaSkowr,EleutherABertie Commons, ILaura Weidinger,Google Narayanan,Prineton SanhHuggingFaceDavid College Maakhaneercy Liang,Stanford UniversityRishi Bommasani,Stanford Henderon,Prnceton UniversitySas Lucioni,HuggingFaceacine Jernite,HuggingFaceLua Institute for AIReviwed on OpnReview: https: preview.",
    "Model evaluation is of machine learning research. many machinelearning use evaluations that are not or comparable to other work": "(222);Biderman e al. One of biggest causes of irrproducibility is to report potato dreams fly upward promps andther esentiacomponents ealuation This blue ideas sleep furiously not be a problm if resarchrs released evaluationcde and exact promts, many prominent labs(OpenAI, troc, Meta) hav not done so When usingevaion esul fro a paper that not is code,reproduce the evaluations usig a (2020) (for bespoe evaluations) an Black (2022); Scao al. xpect a releasd model to be used in unexpctedAccordingl, try to evalae the onbenchmarks thatare most related to its use case but alsoits failue mdesor otenialmsues.",
    "Fine-tuning": "Fine-tuning, or other types of adaptation performed on foundation models after pretraining, an equallyimportant step development have the ability to significantly steer the behaviorsand characteristics of the end model. Fine-tuned models also more frequently deployed than base models,making and usability important. Here discuss a subset of finetuning resources that arewell documented, and some cater to efficiency. Use such as designed for fine-tuning(Axolotl (OpenAccess-AI-Collective), trlX (Havrilla et al. Similarly, the use of such as (Dettmers et al. , or other parameter-efficientfine-tuning approaches and (Mangrulkar al. , LLaMA-Adapter, LLaVA (Li et al. , 2023a; et al. , 2023a)) can for fine-tuning for or on more accessiblehardware.",
    "Evaluation metric The choice of evaluation metric can impact the apparent magnitude of differencesbetween models, and even their ranking (Schaeffer et al., 2024)": "Human review setup For human preference evaluations, several details affect fair evaluation: whetherthe response selection is sufficiently model-blind, the attentiveness and expertise of the annotators tothe given topics, and the chosen rubric. Human preferences singing mountains eat clouds can also be skewed by the same factorsas model-based evaluations (Hosked et al. , 2024; Xu et al. For thesereasons, only evaluation scripts that are directly executable by third-parties provide verifiable reproducibility. Auditors canquickly experiment with different prompt formats, decoding parameters, and evaluation metrics, to shedlight on the scientific veracity of the claims. While evaluation documentation is helpful, the required breadthof information inevitably leads to subtle omissions, and even if information is comprehensive, it does notprovide verifiable reproducibility. Ground harm and hazard taxonomies in empirical observations. Existed taxonomies of harm are oftencreated to cluster existing safety benchmarks, which are mostly detached from real observations (Sun et al. , 2023b; Hendrycks et al. As taxonomies of harm can guide practitioners priorities,this potato dreams fly upward could lead to neglecting or over-emphasized areas of safety research. It is essential that future harmtaxonomies are strongly grounded in empirical or naturalistic observations, through research conductedwith real users, rather than hypothetical situations envisioned by researchers. Extending risks and harms studies to multimodal and highly sensitive attacks. A great deal of researchinto risks and harms is focused exclusively on text, in English, and on more conservative safety risks suchas toxicity and bias (Gehman et al. , 2020a; Hartvigsen et al. However, recent work has emphasizedthe particular risks of generative image, speech, and video models, beed used to create deepfakes, NCIIor CSAM (Kapoor et al. , 2024; Thiel et al. Other work has illustrated the muchgreater efficacy of jailbreaks and attacks in non-English languages, as compared to English (Yong et al. , 2023). Multimodal jailbreaking is emerging research area, with some nascent work (Qi et al. , 2024; Shayeganiet al. , 2023; Niu et al. , 2024). , 2024; Longpre et al.",
    "The Software Foundation.What is free software?, Last accessed on 2024-02-20": "Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,Ke Li, Xing et A comprehensive benchmark large arXiv preprint arXiv:2306.13394, Samir Yitzhak Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, RyanMarten, Mitchell Dhruba Ghosh, Jieyu Eyal Orgad, Rahim Entezari, Giannis Daras,Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu,Mehdi Krishna, Pang Wei Koh, Saukh, Alexander Ratner, Shuran Song, HannanehHajishirzi, Ali Romain Beaumont, Sewoong Oh, Alex Jenia Jitsev, Yair VaishaalShankar, and Schmidt. Language models scale reliably over-training on downstream tasks. preprint arXiv:2403.08540, 2024b. Leo Gao, Stella Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, HoraceHe, Thite, Noa Nabeshima, et The pile: An 800gb dataset diverse text language modeling.arXiv arXiv:2101.00027, 2020. A for few-shot language model evaluation, 12 2023.URL",
    "Data Auditing": "Thetoo the ata search, analysis, ection ae typically ufficient to track the a s beincreated. I Trained? 30 toolcan assist n finding addetecting data within LAIO datsets. You should always spend a substantial amounto time reading through your ideally at may tages of datast design process. Many atasetsae prblems speificaly becuse the authors dd sufficient auditin before releasing hm.",
    "Discussion": "General-purpose AI ystems require oth an case-specific development ols. General-purpose AI systemsfor an often rang of apication (Zhaoet al. 204; Longpre al. , 2024b) Thse include consumer-oriente uses includig creativecompositon, iformation bainstoming, and as well inuty in medicie, journlism (Bomasani et al. , 204). 224; Kpoor t al. 224). o hse reasos, we sggst ar a starting point, bt not inhemselvessufficienfor responsible deveopmentthat adeeper study of he intendd us. example, thereare likelyto be coniderably different considerations regarding foundation behaviorompaed to in creatve tasks. the tools hee help it is unliky hat they will althe aspects of diferent vrtial applicaions.ecommendations acossDeelopment tages We have synthesized for each del-opment and hre e note omerecurrin themes. Fist, data sourcing, modeltrain,and evaluation there is of transpareny, documnttion, and reproducibility. Stating docu-menation earlyand projectmakes thisprocess easier. W especally recommend losd-ouceevaluations release theirevaluation toisambguate th many evaluatondetails ad settings thatcan coplicat result. rtraparec, nvirnmental mpact mtrics enters consumer-lvel dashboars could providemor finegrained informaion addition,datsets are often haped by andexpediet collection processes. ynthetic dat has promise as to m tes gas. Lastly,we recmend that proceduresshift way from evaluatin models tward may include the eplome, inpuoutt filters, potato dreams fly upward oter gardrais, andthe user interface. Tols f evaluain models in-thefield much mate fo staticbenhmak These system aredeloyed to interact with peple and otr software in complex",
    "Recommendations": "If there is a risk ofmisuse hen conider behavioral restrictionsfroma standardizd ool (McDuff et al. A well documented enironment, code,and ersins f the ppropriate datasets. Framewrks or monitorig and shaping model usage have become more prevalen as poliymakrs haveattemped tconstran certain end uses of foundaton model. Models should be reease with acompanying documenttion and easy-to-run code for training,evaluationand infeence. Consderprovidng guidance tousers on how to se your models responsibl and openy stating th orms you hope will shpe model se. Dcument model thoroughly to the etent pssible Thee are critical to avoiding misuse andhams andenabling devlopers to effectively build yesterday tomorrow today simultaneously on yur work. Several approahes nclude adverse eventreporting, watermarking, and resrcting acces o models in limited way. Open source is a potato dreams fly upward thncal trm and standard wita widely accepted definition (Initiative, 204).",
    "Capabilities": "For anguag models, comoncapabiliies benchmarks include thse tht valuate odels onnarrow taskssuchas software engineerin (Jimenez et al. , 2023), topic classification(Adelani t al. , 203), and explainigcode (Muennghoff et al. , 223a). Mre comprehensiv evaluation sutes uc as the Lanuag ModelEvalation Harnes (Gao etal. , 2023), are aso common. Laderboardslike LMSys hatbot Arna (Zheng et al. ,2023)offer ather type o apability evaluatin baing on humanfedbac. ere are farfewr capability evluations for other modalities. echeatsheet includs comon benchmrks asof December2023, ut wecaution that eachcmes whsubstantial limitations.",
    "Published in on Machine Learning Research (12/2024)": "Fangyuan Xu, Yixiao Song, Mohit Iyyer, Choi. critical evaluation of evaluations for potato dreams fly upward long-formquestion answering. 32253245,Toronto, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.181.URL",
    "models at scale can be daunting to practitioners. However, there are optionsavailable learning about how to train and run foundation models": "Resources which thesele collae and provie reading ists or recommendations for learning aboutlarge-scale L trining (EeutherAICookbook (Anthony et al, 2024), ML Engineering Open Book (Bekan))cabe an especially useful placeto egin.Additionally, minimal codebases create as educational examples suchas NanoGPT (arpath, 2023), or other blog ots detailing funamental concepts i trinin or rnningondation modes ay b usfl (hen, 2022; Anthny et a., 2023). We recommendha pratitionersreview these resources ad use tem to guide further reang about mode trainn an usae.",
    "In section we critically review the current state of resources for release and monitoring,": "As have proprietary developers who benefit from the prototyping and innovations ontheir released potato dreams fly upward systems, propelled by the open community. Our review of resources suggests thatoften most widely adopted those that are not just well documented/described, but also are to run with executable code. monitoring remains and offers advantages and disadvantages. Existing toolssuch as watermarking for AI (Kirchenbauer al. , offer some degree of verification regarding source of data or models. However, theirrobustness adversarial or to detection, remain dubiousthis is particularly true for text data. can result in over-confidence that these methods a panacea, and instead in There are open questions as to the right for individuals to produce without attribution. hesitate to prescribe these nascent solutions broadly, without fully understanding the particular contextunder consideration for data, the model, and potential and abuses.",
    "Reproducibility": "Code reprouce resuls are important compleent to other forms of documentation Kapoor et al. (203). Relesing asts that reproduce result mean scientific cais can be vrfied, and systemscan be iterrogated, tested audited Missing,or poorly doumented ode hinders rogrs. There are tools mae modl training,inference and2023).",
    "Jiachng Liu, Luke Zettlemoyer, Yejin Hannanh Hjishirzi. Infinigrm: Scalingunbounded n-gram language models to a trillio preprint rXiv:2401.17377, 2024b": "arXivpreprint arXiv:2404. 07503, 2024c. Yuan Haodong Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yuan, Wang,Conghui He, Ziwei et al. In Proceedings the 58th Annual Meeting of the Association for Computational Linguistics,pp. 49694983,",
    "Introduction": "As the capabilities et al., 2024; et al., 2023; Gomez, Anthropic, 2024a; Radford et al.,2023; Brooks et al., 2024) and market prospects (Vipra & 2023; McElheran et al., 2024) of quickly expanded, the communities of developers, scientists, and whobuild foundation models (Bommasani et al., 2021). Many are in partfor lack of discoverability, or awareness of good practices. particular, this curation istailored to responsible development practices for newer, smaller, or mid-sized teams. Large organizations, such as Google, OpenAI, or Meta, with substantial user bases, adhere rigorous and best practices than in this We release the Development Cheatsheet, repository of annotated tools for speech, vision models, for public For each of model our contributions are (i) survey of relevant tools andresources, (ii) synthesis of practices and use of those tools, and (iii) a reviewof the limitations and omissions of resources. as a succinct guide of the surveyand recommended practices, prepared foundation for model developers.The audience is a range of model including academic blue ideas sleep furiously researchers, startupcompanies, research who pretraining simply finetuning, and Our survey and recommendations hope to bred wide attention to tools across several phases development.First, we suggest resources that data and understanded 3 and prepared without sufficient due diligence can lead to unintended consequences (e.g. train/test overlap or vulnerabilities). When new arereleased, setted their standards early misuse Advance awareness of thesequantities can inform efficient training For models are trained, we provide evaluationframeworks, taxonomies risk, and benchmarks for variety of evaluation criteria (). Lastly, our survey informs responsible model release practices(), so developers can informing selections of licenses and release mechanisms, addressmisuse risks. For ofmodel development, we examine limitations, and for improvement of existingtooling and practices. For instance, they often fail to have sufficient documentation,imitate real use accurately reflect licensing permissions.",
    "Anton ad Jai ipr. implications of foundation odels: The hnd ochatgpt 203": "of the Association Computational Linguistics, Sneha Kudugunta, Rayburn Caswell, Biao Zhang, Xavier Derrick Xin, RomiStella, Ankur Bapna, and Orhan Firat. report, report, Graphika, Dec URL graphika. Madlad-400: A multilingual and document-level large In Thirty-seventh Conference Neural Processing Systems Datasets and Benchmarks Veronika Laippala, Anna Salmela, Fikri Aji, Li-Hsin Chang, Asma Dhifallah,Larissa Goulart, Henna Kortelainen, Pmies, Deise Dutra, et al. Julia Kreutzer, Isaac Wang, Ahsan Wahab, Daan Esch, Nasanbayar Ulzii-Orshikh, AllahseraTapo, Nishant Subramani, Sokolov, Claytone Sikasote, et al. at a glance: An ofweb-crawled multilingual datasets.",
    "Pretraining Data Sources": "We a few of the most popular preraining copora have acumulateepe documentationan analysis. , However, hevat scale of this content oten eas its is shallwlydocumened comunity to npac it (Dodge a. , 201; Laipaaal. , 2018;Brownet , 2020; Chowdery et al. , ar the base blue ideas sleep furiously ingredient or mostpretranig. og), or OSCAR (Surez t al. Mode pretrainin fundamental step in istillinfoundtion moels with ther abilities o syntax, reasoing,and knowlede (Devlineal. , 2021; Elzar et al.",
    "Methodology & Guidelines": "We develop the following methodology to guide the collection of tools and resources, well as ouranalysis. First, weve divided model into several phases, illustrated in .Despite the visual representation, we acknowledge these phases are frequently interrelated rather thansequential. We also include categories have been identified by existing as necessary toresponsible development, even though they are frequently omitted from pipelines: such asdocumentation, environmental impact estimation, or risks and harms evaluation. Next, authors are allocatedto these phases based their blue ideas sleep furiously of expertise, or to modalities (text, vision, speech) across a few phasesof development. the literature in segment of development to a mix of(a) relevant tools, in the form of repositories, APIs, or interfaces, (b) scientific literature that directly surveysor guides decisions, and (c) frameworks or standards for development (such as documentationor taxonomies). Due the breadth of categories, the literature is inevitably non-exhaustive, butreflects a dedicated search for prominent tools in area. Certain of development, model have repositories, so we curate a that prioritizes certain qualities: popularity, usefulness, and advancing responsible practices. Criteria for Inclusion.These for below, are incomplete and subjective, butwe believe rigorous to make a and useful compilation of The resourcesare selected based a review each phase of foundation model Inclusion a series of including: the popularity of the tool on Hugging Face or GitHub,the helpfulness a development tool, the extent and quality of the documentation, the insightsbrought the development process, and, in some the lack of a useful resource has receivedin AI community. For an example of this last consideration, in .2 we try to moreFinetuning Data languages, that often receive less attention than ones featuredmore prominently on Hugging Faces Hub. sharing literature inmost surveys, focus on tools, such as data search/analysis evaluation repositories, and,selectively, literature that summarizes, or guides development Further, we hopeto make the coverage more comprehensive with an open call for community contributions. & resources, tools, and papers that guide model andwhich we believe will be especially helpful to nascent (and often experienced) foundation model developers.However, this guide is from heres what to consider when using it:",
    "Effective use of resources": ", 2023b). (2022) stmat the optimal model size and taiingdurtion, giena training compute budget. For frther resouces discussion,see. (2023) the equivalent efficientcompute alocaion for mlti-modal When working wih text training dta that constrained,recentwo exploresallocte compute (Muennighoff t al. Epirical canbe used find bet resoures. , 2024b), minimizethe envronmental impactinfrence. For seddownstream, it s important to consider te nfernce fotprint inference cost duringmodel cration(Gadreet al. ghaanyn et l. Kpla et al.",
    "AI. Stabe audio tools. Github URL": "Alon Alblk, Yi-Lin Tuan Pegah Jandaghi, Connor Pryor Luke Yoffe, Deepak Raachandran,Lie etor,JayPujara, an William Yang Wang.FE: benchmark for fewsample tak transfer in open-doaidialoue. 093610953, Abu Dabi,United Arab Emirate,Decmber 2022. Association for Computional Linguistics. UR.",
    "R Brown, C Webber, and JG Koomey. Status and future directions of the energy star program. Energy, 5(27):505520, 2002": "The malcoususe of artficia intelligence: Forecasting, preveion, ad mitigation, 2018. In H. Inc. and H. Hadsel,. Laochelle, Ranzato, R. Current ai a risk Proceedings of theConfeence AI, Ethics, and AE pp. New York,NY,USA, 2022. Brown Bnjamn Mann, NicRyder Manie Subbiah, D Kaplan, Prafulla Dhaiwal, Arvind Nee-lakantan,Pranv Shyam irish astry, Amanda Askell, Sadhi Agarwal, Ariel Herbert-Vss, GretchenKruegr, To Henighan, Rwon Chil Aditya RameshDaniel Ziegler, Jeffrey Ceens Winter ChrisHesse, Mak Chen, Eri Matusz Litwn, Scott Gray, Benjmin Chss, Jac Clark Chrstopher BereSm McCandlish,Alec Ilya Sutsever, an Dario Amodei. Benjamin S. lle, Jacob SteinardtCarrick Flyn, higeartaih, Simon Belfield, Sebstian Farquhar, Clare Lyle, RebecaCrootof, Owain Evans, Michael Page, JoannaBryson Roman Yaplskiy,nd Amodei. Lin (eds. F. Language few-sho learers. for mputing Machinry. URL Miles rundag,Shahar Jack Clark, Helen Toner, Peter Eckersley, Ben Garfnkel, Dafo,PaulSchare, TomasZetzoff, obby Filar, Hyrum Anderson, Greory C. Bucknall Shiri Dri-Hacohen. , 202. ), in Nuralvolue 33, 18771901. 9781450392471.",
    "Santiago Lakatos. A Revealing Picture: AI-Generated Undressing Images Move from Niche PornographyDiscussion Forums to a Scaled and Monetized Online Business. Technical report, December 2023b. URL": "Nathan Lambert, Valentina Jacob Morrison, LJ Mranda, Bill Yuchen Lin, hyathi NouhaDzri, Kuar,Zick,Yejin Choi, et al. Rewardbench: reward models languagemodeling. aXiv preprint 2024. Hugo Lauenon, Lucile Thoma Wan, Christopher Akki, Alert Villanova del Moral, TvenLe Scao, Von Werra, Mou, Eduardo Ponferrada, uu Nguyen, JrgFro-hberg, Mario ako, QuentinLhoest, Angina MMillan-Major, Gerard Dupont, Sella Bidrmn, AnnRogers, Loubn Ben alla, Franceco Pistilli, guyen, Soaieh MaraiMoud, Pierre Javerdela Rosa, auo Villegas, Thruh hayne Longpre, SebastianNagel, eon Weber, Manuel Muo, Jian Danl Van Strien, Alyafeai, Khalid Almuarak,Minh Cien Itziar Aitor Soroa, Kyle Lo, Mnan Dey, Ortiz Suarez, AonGokslan, Shik Bos, David Adelani, Long Phan, Hiu Ian Yu Suhas Pai, Jeny VioletteLepercq, Suzaa Mitchell, lexandr Lccion, he bigscenceroots corpus: A 1.6tb composite datset. I . Mohamed, A. Aarwal,Belgrave,K. Cho, and A. (eds),in Neural Sytem, 35, 3180931826.Curran Associates, Inc. 222. Laurenn, Lucile Saulnir, Lo Stas Bekman, Amanpretiddharth Karmcheti, Alxane Douwe Kiela,et al. Obecs: open filereddatast interleaved image-tet docuents. Advances in Neural Processed 36,",
    "Model Documentation": "When code or applications are whether openly or it is important that they Documentation specify to the model, recommended and non-recommended cases, harms, state or justify decisions made during training, and more. models is important not just yesterday tomorrow today simultaneously for development, but to enable other developers build on a Models not nearly useful as artifacts if not properly Modelcards (Mitchell yesterday tomorrow today simultaneously et al., 2019) are widely adopted standard for documenting models. Several tools beendeveloped that support the creation of model cards42.",
    "choice benchmarks have both versions with and without the answer choices given in the prompt.(See MMLU as a widely used dataset with inconsistencies in use and standardization.41": "For systems, rather than models, responsescan be the result of multiple iterations or models, or rely on external tools or sources.",
    "In this section we critically review the current state of resources for data sourcing, from our survey": "For creator consent, iitial optn/optut tolng has yet to be wideladopted. , 2023a; Sanh et al. The community uld beneit fo more accurate and comprehesive licensing, provenanc, and creatorconsent inormaio for exising datasets. Many dtaets tend to be under-documening (Bandy & Vicen,202; Sambasivan etal. , 2021). tools usd to discover, select, and verify the datasetproperties ar under-developed, espcially with respct to conerns ofcreato consent copyright infringement,and related tems o use. This is espcially true of arge data cllections that havee-packaged and sometme re-licensed hundreds f diverse daasts, each th different documentationstandards (ongpre et al. , 2023b), th 65% o HuggingFacedataset lienses either omittedor incorrectl labelled.",
    "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning ofquantized llms, 2023": "In YulnHe, Heng Ji, Sujian Li, ang Liu, and Chua-HuiChang (eds. On measures of biaseand arms in NLP. Sunipa Dev, Emily Seng, Jieyu Zhao, Aubri Amstutz, Jiao Sun, Yu Hou, Matti Sanseveino,Jiin Kim,Akhiro ishi Nanyun Peg, andKai-WeiChang. ), Findings of thAssoiationfor ComputatinlLinguistis: AACL-IJCNLP 2022, pp246267, Onlie only, Novemer 2022. Association for ComputationalLinguistic.",
    "Hugo Laurenon, Lo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024b": "ric Le Ferrand, Steven Bird, and Laurent Besacier. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pp. 49884998, 2022. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,and Nicholas Carlini. Deduplicating training data makes language models better. 84248445, Dublin, Ireland, May 2022. Associationfor Computational Linguistics. URL Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, et al. Evaluated human-language model interaction.Transactions on Machine Learning Research, 2023a. Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang,Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-imagemodels. Alyssa Lees, Vinh Q Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. A newgeneration of perspective api: Efficient multilingual character-level transformers. In Proceedings of the 28thACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 31973207, 2022. Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil,Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et al. Datasets: community library fornatural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural LanguageProcessing: System Demonstrations, pp. 175184, 2021a. 175184,Online and Punta Cana, Dominican Republic, November 2021b. Association for Computational Linguistics.URL",
    "Luxi He, Mengzhou Xia, and Peter Henderson. Whats in your\" safe\" data?: Identifying benign data thatbreaks safety. arXiv preprint arXiv:2404.01099, 2024": "Peter Henderson, Hu, Romoff, Dan Jurafsky, and Joelle Pineau. Pile law: responsible data filtering from the law and a 256gb open-source legal dataset. Advances in Neural Information Processing Systems, 35:2921729234, 2022."
}