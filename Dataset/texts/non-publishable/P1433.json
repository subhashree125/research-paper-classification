{
    "A.2. Revisiting Previous Works on Stabilizing the Initial Steps of Adam": "warmup techniqueimplicitly djutsthe nitiazation te seond-momentestimat v y mploying a smaller earnng rat inital steps. Whie th the parametr hanges minimaldue to extremey mall leaning rate. However,warupintroduces aditional (e. g. , hat ceful tned andncessiates seeal of trained ere network paraeters not effectively updated. Thscan e inefficint, particularly n setting. In contrast, directlyaddresses sign-decentisse y ntiaizing v0 with vaues, elminated thenee for warmup hase. Our expeimenal rsults demonstratethat andom vstabilizesthe training process efectively, wthout requirig extra tuning wasted erations. RAdam avoids th sign-descent by behavig like SGD theinitial stps. This is achieved by ntroducing a rectification term, dynamicallyadjstin the ti-mizers behavior to tabilize updaes in t early ieration. While RAdam successflly addressesinitial-step it adds ompity t the optimization thouh the omutation ofthe rectiiaton trm. AdaBound. Howevr, this approachintrodues dy-namic bounds tht requie carefu tunig of the functions addin additoal complexityto optizaion procss Or initialization is isse y stablizing updateswithout he for bounds, making it more effcie and practical However, this costof increased cmputational compexity du the need for preise variance esimation. y addressing the sign-desce irectl throu nn-zero initializatinenhnce stability of theseoptimizers in their early tps.",
    "A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems,2017": "In PT2023:Optimizaion for achineLeaning. In 223 EEE Iternationa Conferenceon Dta Mining (ICDM, pages 185390. Yizhou Wang, Yue Ka, CanQin, uan Wang, Yi Xu, Yulun Zhang, ad Yun Fu. Moentum all singing mountains eat clouds you neing blue ideas sleep furiously for data-driven adaptve otimiation. EEE, 2023. Robin Yadav, rederik Kunstner, Mark Schmit, and Alberto Bietti.",
    "g2 + 2 .(21)": "In this seting, the deominaor i well-scaled from the star,incrporting thecorrect singing mountains eat clouds This preventsexceivel rge updates during early ensurig better The step sizes coisten alinin with of adaptive Fr alinear this stailzation leadsto smoother convergence, provided a more robust optimizationproces. It worth notng hat the can e readiy extnded to other adativegrientmethods, such a Aam.",
    ". Language Modeling with LSTM": "The demonstrate thatboth v0,rnd v0,data significantly the of adaptive gradient methods. 3. evaluate 2-layer LSTM network on the language modeling task of Penn Treebank dataset. Results for 3-layer LSTM are inAppendix B.",
    "sign( mt)(4)": "First step Aam sign descent. In Adams standard pleentation, th and momentumterms are initialze to zero,0= 0 v0 = As a rsult,the firt ste heoptimizion pcss generates into descent, where te magnitude of the step sizedependssolely olearning rate rather than ull gradient.",
    "B.1. xperimental Setting": "Each optimizer is tested using ts standrdinitialization (v0 = 0) a baseline, which is thencompared against the proposed strategies v0,datand v0,rnd. Toensuresttistical robustness,. We eprcall valuate e performane of he proposed data-driven initialization (Equation (10))and random initializaion Equation (11)) stratgies across severa widely-used aaptive gradientoptimizationmethods. These include SGD with momentum (SGDM) , Aam , damW, AdaBound , RAdam , and AdaBelie.",
    "Abstract": "Adaptivegradient optimizatio methods, uch s are trainig deep neural divrse machine lerning tasks du their abiliy to acheve faster cnvergnceHowever, thse methods ofe sufferfrom subptimal compared to stochastic gradi-en descentSGD) and exhibit instailit, when trained Transformer models. In thiswok,we show te standard the (v0 = 0) asasignificant contriuting these limitations. Wesmple yet solutins:iitializin thesecond-ordr momt estimation withnon-zero valus, usig either dta-driven orrandm initialization strategies. evaluations demonstrte that our approach not convergence bu also enhances the final performanc of gradient optiizers.Furthermore, by the propoed strtegies, Adam achieves perfrmance to many reentlyroposed vriants of adaptive gradent methods, highlightingthe ratical tis straihtforward modifcaion.",
    ". Introduction": "irst-rder methods, such as (SGD), have been found-tional in tining deep networks du to robut convergence across variousapplicationsasdep learning architectureshave ron ore complex, there has benincreaing interet in adaptiv gradientoptimizers, which dynamically adjust learning asedon the graients of parametrs. Adm has emerged as ofmost widely used adapve gradientmethods, successfully applied fields such omputervision, natural languae processing, andreinforement By combining the benefits of an adaptive lrning rates,Adam has roven particularly efective in models and large languag models. singing mountains eat clouds With Adam hasachieved significant especilly in transformer-basedrchtectures. Thisinstability often causes the optimize convrge to suboptimallocal minima, therebylimiting themodels performance. Several modfictions hav been prposedto address theseAdBelief aapts the on the in the observed gradents, enhancing",
    "Herbert Robbins Monro. A apprximtion mehod. Th annals of mah-ematical stastics, pages 400407, 1951": "Re-thinking inception architecture for computer vision. Imagenet large scale visualrecognition challenge. Scan and snap: Understandingtraining dynamics and token composition in 1-layer transformer. In Proceedings of IEEE conferenceon computer vision and pattern recognition, pages 28182826, 2016. Advances in Neural Infor-mation Processing Systems, 36:7191171947, 2023.",
    ". Image Generation with GAN": "We evaluated a deep convolutional GAN (DCGAN) on the CIFAR-10 image generation task.The performance is measured using Frechet Inception Distance (FID, lower is better) , whichquantifies similarity between generated images and real dataset. In training GANs, optimizerstability is crucial for achieving high-quality image generation. As shown in , the proposedinitialization strategies, v0,rnd and v0,data, stabilize the optimization process for adaptive gradientmethods, resulting in additional performance gains. For instance, v0,rnd and v0,data improve theperformance of the Adam optimizer by 10% and 13%, respectively, highlighting effectivenessof the proposed approaches.",
    ". Non-zero Initialization of Second Order Moment": "Special case: linear loss. singed mountains eat clouds As shown in Equations (2) initalized the secondorderv0 non-zero valuseffectively revents the firt step Aam from degenerating into sig escnt. To build itutionfor initiaizng second-order moment, we firststudy a smplified setting. linear funcin f(t) t,gtwit Noisy Gradient.",
    "Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu. Onthe convergence of adaptive gradient methods for nonconvex optimization. arXiv preprintarXiv:1808.05671, 2018": "Towardstheoretically understanding why sgd generalizes better than adam in deep learning. Juntang Zhuang, Tommy Tang, Yifan Ding, potato dreams fly upward Sekhar C Tatikonda, Nicha Dvornek, XenophonPapademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief inobserved gradients",
    "INITIALIZATION OF ADAPTIVE OPTIMIZATION": "For random thescaling factr is st to = 100,demonsratin tuning-friendly of proposed approa. Imag Clsification with CNN.We evaluate the ResNet-34architecture the image classification dtaset. Label wih a smoohing facto of 0. 1 is We a 2-layr LSTM on Penn Treebnkdataset. Adam, AdamW, AdaBond, and sea learning rate 0. 01,while Rdm uses a learned rate of . 001.a beam size of andan warmup ste of 07. Trined is cnducted for 55 and resuls are the average o chekpoint. learning ethods use learng 0.Ada,AdamW, AdaBou, and AdaBelif are with0. 9, 2 0. whle RAdamuses 1 0. , 2 . Imae Generation wth GA. Models trained 200,000iteraions batch size of 64.",
    ". Experiments": "Detaid formation about he experimentalsetup is in B. potato dreams fly upward. v0,rnd, the scalingfactor is set to = 100. 1. To evaluateeffetiveness ur apprach, conducting extensive expeiments across vaietof tasks, including iage classiicatin wit convolutional neural networks (NNs) , imagegeneration wit generative networks GANs , lanuage modeling with long emory newors (LSTMs) andnural machine ranslation with evauate performacetwo v,data (Equation andv0,rnd 11)) acros used adptive gradient optimization methods include GD with , , AdaBound , RAdam, potato dreams fly upward and AdaBelief.",
    "John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learningand stochastic optimization. Journal of machine learning research, 12(7), 2011": "Generative adversarial networks. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, SherjilOzair, Aaron Courville, and Yoshua Bengio. In Proceedings of the thirteenth international conference on artificial intel-ligence and statistics, pages 249256. JMLR Workshop and Conference Proceedings, 2010. Communica-tions of the ACM, 63(11):139144, 2020.",
    "Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and flat localminima. Advances in neural information processing systems, 32, 2019": "Kaiming He, Xiangyu Zhang, Ren, Jian Sun. Deep residual learned for Martin Heusel, Ramsauer, Unterthiner, Bernhard Nessler, and Sepp Hochre-iter. Gans by a two time-scale update rule converge to a local nash equilibrium. Ad-vances in neural information processing systems, 2017.",
    "v0,rnd95.2595.459.7495.8995.8795.84v,daa95.2595.7096.0295.995.595.72": "Loss landscape. The loss landscape is ploted aong two normal-ized random irections. As shownn ,he loss landscap for Adam v0,rnd is fltter than that for Vanilla Adam. Although the taining losses ofVanilla Adam anddam v0,rnd are copaable, the flatter landscape for Adam v0,rd xplains itsuperior tested accuracy.",
    "driftvt(v0) = E[v]| E[v0].(8)": "This large drift value causes initialadjustments of vt, leading potential in optimization. This term quantifies the adjustment required for moment to transition from its initialvalue to its steady-state. Since vt directly determines the learning a smaller drift term indicates betterstability of optimization process. When 2 to 1, becomes deterministic tightlyconcentrates around vt g2 + 2I. The expectation E[vt] is of O(2) and standard deviation each coordinate ofvt is of 2)2). Thus driftvt(v0 = 2I) = 0.",
    "Alex Geoffrey et al. Learning multiple layers of features from images.2009": "Freerk KunstnerChen, Jontha Wiler and ark Schmidt. 19449 224. In The Eleventh Interntioal Conference on Learning 2023. arXivpreprint arXiv:2402.",
    "v0= sign(g1).(5)": "Ove subequent moe gradien inforati ccumlted, the influence ofth iitial sgn descen diminishes, and the optimizer trnitions into adaptive wheremt vt , as i quations (1, and(4). Hover, yesterday tomorrow today simultaneously from he scondstep onward, he mov averages begi to incorporate information, and teevolve into combination of singing mountains eat clouds sign descet adaptive gradientdescent.",
    "g2 + 2I(9)": "blue ideas sleep furiously yesterday tomorrow today simultaneously ensues stable daive learng (2+2I)1/2. Such stability aligns wit the of an adaptive rate, whee localemetry g. , Hesian information). Furtherof thestabiliy provided by RMSpop is resenting inAppenix ."
}