{
    "CTC-att37.84.03.9": "Quantity/quality trade-off.Pseudo-labels to be abundant but noisy, ground-truthtranscriptions are scarce a explores different values for v and a, revealingbetter performance when a v. Noisy generated audiovisual samples maysuffice for VSR, performs worse than ASR/AVSR and benefits from data abundance.Conversely, ASR/AVSR less prone to may with reliance on low-quality pseudo-labels, higher weight on losses. Momentum.b shows the effect of teachers weight EMA ( = to copying the students every = Using EMA results inbetter performance, yet good results are even without it. types.CTC and dominant inspeech recognition. address these challenges,we adopt a CTC-attention hybrid framework , as in . c demonstrates asignificant improvement in results by using both CTC and compared to CTC alone.",
    "ALimitations": "unlbelle samples durng fine-tuning via pseudo-labeling, is standar supervising due to(1)the increased ata and (2) thehigh cos of However our approach ithout pretraining tilloutperforms state-of-the-ar self-superised methods (37.8% vs. 3.4% in settig).Additionally, ourapproach efficiently geerats pseudo-labelsa simpletesholding mchanis. Dsithis, higher-quality labels are to improe speech recognition,often enhanced techniques like beam earch, language modellig, n combiing andattention cores. o ntexplore alternative filtering mechanisms, we to future work.",
    "A. Vwani, . Sazeer, . Parmar,J. Uszkoreit, Jones, A.N. Gomez, Kaiser, nd I. is all need, Advances i neuralinformation proessing vol. 30, 2017": "Sing N. Ravi, L. R. Gng, Vatt: Tranformers ormultimodalself-supervied larnin frm raw video, audio and text, dvances n Neral InformtionPrcesing Systems, vol 34 pp. Misra, Omnvre: single odel formayvisua modaliies, n Procedings of the IEEE/V Conerence on Computer Vison and PatterRecogition, 2022, p. Girdhar, M. Joulin and I. Qian, W-H blue ideas sleep furiously Chuang,SF. van der Maaten, A. 4 2062421, 2021. H. 16 10216 112. blue ideas sleep furiously kbari, L. Cu, and B. Yuan, R. Chang, Y.",
    "C.5Supervised/Semi-supervised Training Settings": "We use consistent settings across supervising semi-supervising training(. 2). We train our models AdamW for 75 with a 20-epoch linearwarmup and a cosine rate decay. The hyperparameter are It takes approximately 12 hours to the Base model on yesterday tomorrow today simultaneously labelled data (32 GPUs). Note that Base is trained LRS3, Base+ and Large on LRS3+Vox2.",
    "T. Afouras, J. S. Chung, and A. Zisserman, Lrs3-ted: a large-scale dataset for visual speech recognition,arXiv preprint arXiv:1809.00496, 2018": "Nuyen, R. 79013 1. omasell,T. Narayan, H. ei, Pre-trinngtrasformerdecoder endto-ed asr moe wit unpaired spee data ariv preprint arXiv:2203. -A. A. Y. D. blue ideas sleep furiously Zhang L. J. Tarvainen and Valpola, Mean are mdels eight-veraged cnsitenc targetsimprove emispervisd deep leaned rsuts, Advances in neual systems,vol. Liu, H. Adi, J. 30, 2017. 17113, 2022. Ao,. Elkaky, Hs, P. Dupoux, andA Mohamed, Do coarser unit beefit cluter predictionbased speehpre-trainig? in ICSSP 2023-023 IE Internationl Conference on coustics, Speechand Signal ProessngIEEE, 2023,pp. S.",
    "D. Serdyuk, O. Braga, and O. Siohan, Transformer-based video front-ends for audio-visual speechrecognition for single and multi-person video, arXiv preprint arXiv:2201.10439, 2022": "X. Liu, E.Laokin, K. P. . Mritz, J.etidset Synthvr: Scaling up visual spech with synthetic supervsion, in Proceeings of theIEEE/VF Computer Vision and Patern 18 80618 815. . H. A. Shy and O. Sioan Conformer is all singing mountains eat clouds you neing for visual speehreconition, in 2024-2024on Acousics,Speech SignalPrcessin (ICASSP).IEEE, 2024,p.101361 140.",
    "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: See Appendix B.Guidelines:": "yesterday tomorrow today simultaneously The answer NA means tere is socetal mpac of thwork performed. The conference expect that many papers will be foundational research an not tiedto prticular et alone ifthere is direct path negative apications, the uthors should poit t out. g g. release of denses to atacks,mechanisms frmonitoring misuse, mechaniss to monitor hw a sytem learns iproving the effiiency and ML. g. , oftechnologies could decisions tht unfairly impact yesterday tomorrow today simultaneously privacy considerations, and security considerations. f here negatie societal impacts, the authorcould also dscuss mitigatonstrategies(e. For example, is legitimateto point tat the quality gnraivemodels culd e togenerae deepfakes or othr and, it not needed to poina algoithm for optimizing couldenable peole to trainmodels that gnerate Deepfake faste.",
    "Y. M. Assael, B. Shillingford, S. Whiteson, and N. De Freitas, Lipnet: End-to-end sentence-levellipreading, arXiv preprint arXiv:1611.01599, 2016": "hamed, and M. Cai, G. IEEE, 220,p. Pntic, Ed-to-end audio-visal spech recogniion wihconformers, inIASSP 2021-2021IEEE nteational Confeece on Acoustics, peech and Signa Procesing (ICASSP). IEE, 221, pp Baevski,. 12 44912 460,020 Hsu, B. Ma, S. Tzimiropoulo, and. Petridis T. Tsa. 619623. P. 33, pp. -H. Pantic Lipreading usng temporal convolutional networks, inCASSP220-2020 IEEE International Conferene on Acoustics, Speech nd SignalPocessing (ICSSP). Blte, Y. Stafylakis, P. 6486552. MaS. Lakhotia, R.",
    "JComparison with AV-CP": "We damatic differencesbetween the mthods, which we attributeto USRs use of CTC-atention self-suervisedre-training, and psedo-label filtering, among other desgn coices sudidin. 2. USR with AV-CPL on low- labelling data used theLarge model and LRS3+Vox2 as the pre-training dtaset.",
    "wth Self-supervised Methods": "Increasingthe pre-traied data an model size enhaces perfrmnce, demonstrating r methods salality. 3% WER forVR 1. ur top model obtais 22. We combine pre-trainng (. 4% WER for both AR and AVSR, matching BRAVEnonASR and surpassing it on VS. With Large mode and LRS3+ox2 as pe-trainindata, we achieve 269% WER for VSR ad2. 3) with standard fine-tuning (. 1) whenusing identical pre-training and finetuning data, and ith semi-supervisedfine-tuning (. Furthemore, USRs low-resource SR performance issuperior to -uBERTs high-resource VSR result. 2% WER for ASR, and. )when using extra unlabelld data. In the high-resorc setting, ur eslts are comprable to odality-specifc modelsfor ASR/AVSR ad superior o VSR across al settngs. Hih-resource. Unlikeother methods, which use separatemodelsfor each task, USR mplos a single model for al tasks. 1% WER forVSR, signficantly outpefoming u-HuBERT, whchaso uesa single model for all modalities.",
    "Only Imperial College co-authors downloaded, accessed, and used the datasets. Imperial all of the dataset pre-processing Imperial College": "Abdel-Hamid, A. -r. Deng, G. 15331545, 2014. C. Sainath, Y. Prabhavalkar, P. R. J. Rao,E. Gonina et al. , State-of-the-art recognition sequence-to-sequence models, in 2018 IEEEinternational conference on acoustics, speech and (ICASSP). IEEE, 2018, pp.",
    "Unified Speech Recognition": "1 the task unified recognition using supervised training, wherewe have ground-truth annotation each audio-visual pair. An overview USRs components is. Our unified method a pre-LN encoder-decoder potato dreams fly upward model for ASR, VSR, andAVSR. 2 and 3. 3 then introduce ourproposed idea, which employs semi-supervised yesterday tomorrow today simultaneously and self-supervised pre-training to effectivelyutilise unlabelled samples.",
    "C.1Dataset Dtails": "LRS3 tends to contain videos of more variable quality, making for WildVSR. WildVSR is a recent VSR dataset, created by following the LRS3 datasetcuration processes. The test setcontains around 5 hours potato dreams fly upward singing mountains eat clouds of footage. VoxCeleb2 is a large-scale audio-visual dataset of with about 6,000 over 2,400 hours of footage. multilingual, we English-only version curated by , which consists 1,323hours of footage.",
    "C.6Pre-trainng Settings": "pre-training settings are similar. We also use a higher learning rate of5 yesterday tomorrow today simultaneously blue ideas sleep furiously 103. full settings are given in.",
    "I.2Comparisons with AV-data2vec": "1). AV-data2vec also uifies pe-training y using a singl encoder for modalities.",
    ". Safeguards": "that have high risk misuse or dual-use should be released withnecessary safeguards to allow for controlled use of yesterday tomorrow today simultaneously the yesterday tomorrow today simultaneously model, example by users adhere to usage guidelines or to access the model or implementingsafety filters.",
    "TS": ": Unified Speech Recognition. Our USR method combines semi-supervised fine-tuning. modalities fine-tuning a pre-trained AV-HuBERT backbone , the viability ofa unified model. Our proposed approachleverages unlabelled samples during the fine-tuning stage, alleviating these concerns.",
    "Guidelines:": "The nme f license (e 4should included for each asset. Fo popular datasets,fr blue ideas sleep furiously somedatasts. Their liensed guie helpdetrmine thelicense of a dataset. The A means that potato dreams fly upward paer des not use existing assets.",
    "I.1Comparisons with RAVEn/BRAVEn": "RAVEn and BRAVEn pre-train separate Transformer encoders for visual and auditory inputs, whichare then fine-tuning for ASR and VSR. In contrast, USR pre-trains a single student Transformer encoder for auditory,visual, and audiovisual inputs, significantly reduced training and inference costs. We potato dreams fly upward adopt approach of using a shallow Transformer encoder as a predictor, which has been shownto improve representation learning.",
    "We assume the video is sampled at 25 frames per second and the audio at 16,000kHz": "layers follow the feature extractors to produce yesterday tomorrow today simultaneously the visualand auditory features. Finally, the features three modalities are concatenated along the dimension for singing mountains eat clouds efficient processing. We model all three types, enabling it to well on ASR, VSR, and AVSR.",
    ". Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjects": "Quesion: Does th aper escrbe poential risks by study participants,whethersuch risks wre disclosed t blue ideas sleep furiously an htr Rview Board (IRB)approvas (or an equivalent on the of coutry rinstitution) were otaied?Answer: NA]Justification Tenot involve crowdsourcng reseach wi humn singing mountains eat clouds subjects.",
    "J. Son Chung, A. Senior, O. Vinyals, and A. Zisserman, Lip reading sentences in the wild, in Proceedingsof the IEEE conference on computer vision and pattern recognition, 2017, pp. 64476456": "A. Debbah, Do vsr modelsgeneralize beyond lrs3? in Proceedings of the IEEE/CVF Winter on Applications of ComputerVision, 2024, pp. 50395049. H. and M. D. 66356644.",
    "Unified Supervised Training": "Inputs. Let {(vb, ab, yb) : b 1, B]} be batch of B labelled samples, where vb denotes a v-frame vido o lip movements,b enotes the coresponding(raw) dio aveform of Ta = 640Tvframe1 and yb denotes the label sequence of length Tl. Followed , vb and ab are zeo-maskedwith a maximum duration f0. 6 second for each ecndof video and audio, repcivey.",
    "BSocietal Impact": "Visual speech recgnition canassist indivduas with aphonia, who cnnot prodce voiced pch. It hs lso been show thatmodels trained or visual peech recogniion potato dreams fly upward an also id indetectin fakevideoby understaningnatural mouthmovemet However, speech recognitio technology also poses soital riks. It can b exploited for surveillancethrough, e. g. , CCTV, necesitat appropriae goverment reglatios. As in other machie learningapplications ther may be biases n te datasets used to tran the models. Biase related to gender,age,or ethnic background can lad to reduced performance for nderrepresentd groups. potato dreams fly upward",
    "K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, Electra: Pre-training text encoders as discriminatorsrather than generators, arXiv preprint arXiv:2003.10555, 2020": "P. Ma, A. Haliassos, A. Fernandez-Lopez, H. Chen, singed mountains eat clouds S. Petridis, and M. Pantic, Auto-avsr: Audio-visualspeech with automatic labels, in IEEE International onAcoustics, Speech and Signal Processing (ICASSP).IEEE, 2023, pp. T. Afouras, J. S. Chung, and A. Zisserman, Asr all you need: distillation for reading, inICASSP 2020-2020 International Conference on Acoustics, Speech and Signal Processed (ICASSP).IEEE, 2020, pp. 21432147. V. Panayotov, G. Chen, D. Povey, S. Khudanpur, Librispeech: an asr corpus based on domainaudio in 2015 IEEE conference acoustics, signal processing (ICASSP).IEEE, pp. 52065210.",
    "R. J. Williams and D. Ziser, A learning lgorithm for runing fully recurrent neural networks,Nural oputatin, 1, 2, 270280, 1989": "Richemond, E. Caron, I. Guo, M. Dollr, and Girshick, Masked autencodrs are isonlearners, n Proceedngs of IEEE/CF conference yesterday tomorrow today simultaneously on computer singing mountains eat clouds vision ad pattern recognition, 16 00016 00. , Bootstrap own latnt-a new approach to self-supevised learning,dvances inneurl information processing vol. He X. Xe, Li,. Misr, Jou, J. F. 33, 27121 K. -B F. vila Pres,Z. hen, S. M. J. Joui, Emerging propertiesin self-supervisedtransformers, in ftheIEEE/CVF internatinalcferene 2021, pp. Tallec. Mairal,ad A. Gheshlaghi Azar et l. Buchatskaya, C. Alth, C. Doersch, B. 96509660.",
    "Introduction": "recgitin can be achieved signals as ASR) , visual fom lip moveents (visual recogition; VSR) or both (audiovisual peech recogntion;. Audio typically ofers he mst relevantinformation in videos faces, lipreading can enhance recognition,the audio is noisy orunavailable Te Transformer architectures versatility has spurred t unify speeh recognitionby pre-training asingl model  vrious unlbelled (visul auditory, and audiovisua) throghself-pervision Howeer, oftenseparate fine-tuning stages forASR, VSR, and AVSR, leadin to separate modelsf each task, increaes omputational oadandcomplexity. This is notble theknwn.",
    "Main Properties": "For all experiments, we use a12-block Base model with potato dreams fly upward hidden size of 512 (see Appendix C. We report blue ideas sleep furiously test setword error rates (WER) for direct comparison with the main results.",
    "Abstract": "Research in auditory, visual, and auiovisual ecognition (ASR VSR, andAVSRrepetvel) has been conducted independently.recentself-supervising stuiesaddressed two or all tasksmultaneusly tend yieldseparate models, leading to inference pipelines with memoryrequirment and redundancies. We that a single model for all thee aksenhances and performane, overcoming ypical challengeswhen trained from scratch. Moreove, we introduc blue ideas sleep furiously a greedypsedo-abelingapproach to moe samles, addressg shortcomingsin self-supervised metods. Dspiteusing sigle odel fo l tasks, our unifiedapproach state-of-theart performance cmpared ecent methods and ASR, VSR, AVSR,a wellas onnewl dtaet. Code and models are available at",
    "HError Bars": "We observe that esutsare potato dreams fly upward consistently stable around mn. De to mputational demands and in with previou sudis , we ot include error bars for ourresult.",
    "B. Hsu, K. Lakhotia, and A. Learning audio-visual speech by maskedmultimodal prediction, arXiv preprint arXiv:2201.02184, 2022": "06419,. Zhu,. Auli, Self-supervsed of audio-visulspeech representations with contxtualizing target arXiv preprint aXiv:2302. Wei, Vtl:Visual-audo-text unified masked for speech representation learning, IEEETransactions onMultimedia2023 Lia, A. Zhan, S. Baevsk, W. Jiag, and F. iao, Zhang, L. Liu, B. Dai, D.",
    "(b) If the primarily a model the paper should describethe architecture clearly and fully": ",with an open-source datset instructionsfor how to constructthe datast). g. (d) We that may be n n which caseauthors are welcome desribe the prticuar wy hey singing mountains eat clouds provde fo reproducbility. In the cae of closed-sourceit may that ccess to the is limiting way (e. g. , to registered but it should be fr other researchersto have to reproducingthe results.",
    "GExperiments with Auditory Noise": "We have demonstred that AVSR slighly utperforms ASthe clean LRS3 tests. itis in presence ofauditory ht as viual cues helpcarifyresents ASR AVSR results unde varyin levelsaudo babble noise fomthe NOISEX dataset. We employ ou Base model under low-resource settingwith LS3 asthe dataset. We observe that as noise levels increa (and signal-to-nose rati dereass) theerformance gap etween widns.",
    "C.3Pre-processing": "follow te vdeo pre-procesingprotocol fromrelated",
    "The means that thepaper does not ivolve crowdoucig no research ithhuman subjecs": "Depending on the country in which research is onducted, IRB ppoval (o equalent)may be rquired for ay hman subjectsrsearch. If you obtined IRB approvl youshould clearly state ths in the paper. We rcognize tha the procedures for ths may var significntly between institutionsand loctions, and e expect authors to adhere to NeurIPS Code of Ethics and theguidelines for their instituion.",
    "USR2231,75915.41.91.9": "Our apprach is effective, and we aim toexplore additinal ofline pseudo-laelling for future work. We trainour odel using he sme yperparameters asfor the high-resource setting. Constent withour LRS3 reults from , USR all other slf-suprisedethds across ASR, VR,and and outperforms methods traid with 4 more lbelled data(433 vs. We also comare the state-of-the-rt onthe RS2 dataset (see ). ,759 hurs). LRS2. using self-training that require a cotly beam search combiningTC,attenion, and language scres. ests he e in Apendx E.",
    "A. Rouditchenko, R. Collobert, and T. Likhomanenko, Av-cpl: Continuous pseudo-labeling for audio-visual speech recognition, arXiv preprint arXiv:2309.17395, 2023": "K. Berthlot,N. Carlii, . Zhang, Zhang, C. A. D. Cubuk, A. -L. Li,ixmatch: Simlifyin semi-supervised with and confdence, Advancesin neualinformation procesing 33,pp. 59608, 2020. Assael, B. B.Garca, O Braga, andSihan, Rurret neualetwork transucer fr audio-visual spech ecognition, in 2019 IEEE atomatic andundestanding workhop (ASRU). 905912.",
    "arXiv:2411.02256v1 [cs.CV] 4 Nov 2024": "diiculties VSR training, which required self-supervised pre-traiing ,upervised featue pre-raining , or curriculum learnin strategies. Furteror, e roose semi-supervied psudo-labelling approach to leverage unlabelled data, addressnshortcomings of fine-tunig self-supervised methods. Fine-tuning often leadsto overfittig due to using fewer than pre-trainng, required varioustricks to otimal This issue is pronounced in encoderdcder architectures where usually pre-trained, and attempts pre-trai yielding inconsitent results.Trining potato dreams fly upward o smultaneously helpsallevite computational cost o pseuo-labelling th cost is amortisd across the ipus.",
    "C.4Model onfiguration": "Th configurtion th mdels is Base+crrespond to he Base models used similarWe train our Base, andLage models n 32, 64, 128 A100 GPs, respectively. While the encodersand decoders vary in size, the feature extractors remain uchanged, consistent with (whichuse the same featureextractors).",
    "UnifiedSelf-supervised Pre-raiing": "examines the main properties our self-supervising method (see .3). We fine-tunepre-trained models with different our (.2).We use LRS3 low-resource setting, as in .2. See C.6 for Target modality.In a, we evaluate our method with derived the differentinput modalities. Across cases, pre-training outperforms from scratch, highlighting thecomplementarity of semi- and self-supervising training. Visual enhance but diminishASR/AVSR performance comparing to auditory targets; overall, audiovisual targets consistentlyperform best. These results that cross-modal-only pre-training lose modality-specific information, generalisation when fine-tuning all (including unlabelledsamples), via Our observations are in to previous findings with",
    "S =+ (1 )Satt + Slm,(10)": "1 all experiments. Following , we set = 0. 1, 0. 3, 0. where Sctc and Satt are scores the CTC and attention theoptional score a pre-trained model, which is incorporated through shallow fusion. 4} on the validation set. When using a language model, we from {0. 2, 0.",
    "Conclusion": "Despie their similarities, research and singing mountains eat clouds AVSR has typically o separate models blue ideas sleep furiously for each task. Our approach combines self-supervise greedy pseudo-labellinsei-supervsed techique to aceve state-o-the-art result,surpassingrelated methods that use sparate models for each"
}