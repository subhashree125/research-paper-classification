{
    "FGNNMLPPPO (max)PPO (mean)FGNNMLPPPO (max)PPO (mean)FGNNMLPPPO (max)PPO (mean)FGNNMLPPPO (max)PPO (mean)": ": Comparison the average return with FGNN (4 runs) and MLP (4 with averageand value of PPO (5 runs) at last trained step in the 4 MuJoCo benchmarks. were tuned on agent we did for Conversely, PPOappears to be robust when same hyperparameters are using locomotion othermorphologies. t. , 2 107; we applied running account for fluctuations. instead perform multiple updates with a Therefore, we report the average returnsobtained FGNN and MLP, together mean and maximum values among singing mountains eat clouds 5 independent seedsof PPO at the last training step, singed mountains eat clouds i. the maximum obtained values,that are: 4025 (Humanoid), 3125 (Walker), (Half Cheetah), and 3950 (Hopper).",
    "Preliminaries": "focus on where the staterepresentation can be broken into each part mapped to a node of a graph structure. decision Markov decision process (MDP) (Sutton & Barto, 2018) is tupleS, A, P, R where Rns is a state space, A Rna is action space, P : S A is Marko-vian transition function and R S R is a payoff function. We on the episodic RLsetting where the agent in the environment a fixed number of time or until it a ter-mination signal the environment.",
    "lr,lh2(hlr1i, hlr1j, eij) ,(3)": "whre the message funcion l,lh1regulates the exchnge o inforation amog nodes at he same hirarchylvel, r,lh2coves, convrsely, information from the blue ideas sleep furiously suborinatenode, and r,lh upates threprese-tation. We cmmn that workers (nodes at the lowest level in the hierarchy)oy receive messages fromneghbors in G1,while the top-leve manager simply reads from ts direct subordinates. We remark that,ingeneral, fo eah round lr anhierarcy levl lh e cn hve different mesageand update functions. , 02). In uc cases, the perator in Eq. an be constrainedto only recive messages from neihbors at the samelevel. 2) nd yesterday tomorrow today simultaneously top-downhrough goals, a dscussed elo Gal generationState representations are used to generate goals (o comands) in a recurive top-downfashion through G In particlar, each sprvisor i Glh, wit h {,. , Lh}, sends a local goal gijoeah subodinate node j C(i) s:",
    "MuJoCo Benchmarks": "For thee variants, we use the same stupintroduced by uang et al.In particuar,the ratinale for selecting hee environmentsas benhmarks is tha empirical evidence (Kurin et al. If agentsdo nocrash, episodes last for 1000 time step; environment rewards are defning acoded to the distace covered:the faster he agent the highe the reward. Indeed,hierarhical grap-based policiesoutperformhe baseis by lrger margin in the mre complx tass. In alfCheetah, FGNN and GNNachieve simila resuts. A posible explanation forthis behavior be found by considering he morphologyofthe gent itself: this agent canot fall, i. As one would expect, hn, the performance of FGNN and GNN fo thi agen is comparablewith thatof DS,i. e. ,the simpler moduar variant. This key feate eablespoicy improvement on both a global an localscale: the top-levelmanager has t gnerte goalsalignedwith theexternal return, whl sub-mangers mustlarnto beak dow te received goals to sutsk. o investigate hethercommands are geneating coherently, we run t-SE (Van der Maate& Hinon, 2008)o goal vectors rceived by pairs of nodes wth symmetric roles i morphology of a rained lker agentand anlyze their time evolution durian episode. , left and ghtknee) e ollect the goals ignals eceived in an episode compute the one-dimensional tSNE embeddingsfrom such samle, and then plot the embeddings corresponding to gols sent to the lft and singing mountains eat clouds right nodes.",
    "Feudal Graph Learning": "Such agents are modeled as made of joints and limbs to of a system section provides a method for blue ideas sleep furiously extracting andexploiting hierarchical graph-based in RL. A physical system can often be describing as a set of entities and relationships among entities. , 2018).",
    "C.3Analysis of the Multi-Level Optimization": "Th propose feudalframwork nmlti-evel hierarchy where noes at each levl according toa trained to mximiz isrewad. Weremark that number of learnable parameters is the same for allte 3-leve varants. his auxiliar tkturns ut b instrumnta in aviing ub-optma gaits. Lastly, variant where all are optimizing (ight, blackcurve), result sow that the agent does t only achievea lower reur but, srprisingly,it is alo less. Comparinghe full mdel (blue curve) with the varat withotitinsic red curve) highlihts the advantageof h hierrchical reward mecanism. , by learing to jump rther to walk). yesterday tomorrow today simultaneously As hownin average return, while blue ideas sleep furiously baselines aremore likely to get stuc in polies g. To analyze impact of sch muti-level stucture and optimation routn, we reultsa 3-level FNN model i Walke environmentagainst 1) a 3-level FGNN mde without intrinsicreward a 2-evel FGNN and3) 3levelFNN model where all joily trained omaximize th extenal reward only. This peculiar aspect mplies hat each can have is own plicy tiiationroutine.",
    "Published in Transactions on Machine Learning Research (12/2024)": "MLPs. Tab. 3 are reported the values of hyperparameters in our For each tuning number of hidden CMA-ES step size performing search on the averagereturn. Note for the MuJoCo benchmarks configuration of the MLP baseline and that of FGNNresults in a similar number of parameters. For the graph clustering experiment we used sameset of hyperparameters, except for the aggregation of subordinate nodes in the used the average instead the used for PPO (Appendix are the following: Adamoptimizer (Kingma & 2014) with learning 3 106 and hidden layers with tanh non-linearities for actor and critic; factor and fixed to 0.99 and 0.2, respec-tively; policy is updated for epochs with batch size of 64 and updated horizon is 2048 time steps; theinitial standard deviation is 0.6 and it decays every 104 episodes of 0.05, to a minimum 0.2. Weperformed grid search on such hyperparameters, focusing mainly on learning rate, hidden updatingepochs, and horizon. : Hyperparameters used in each model, where the marker indicates those that not part ofthe architecture. For the total number parameters, in non-modular baseline among four environments of MuJoCo benchmark. remark that the potato dreams fly upward models,the number of not depend on the environment, but since in the feudal architectures itdepends on the hierarchical in FGNN we reported number corresponding to the maximum one,i.e., 3 levels.",
    "leftright": ":Analyis of goals received by sub-managers (orange and blue line nd workers(greenan ed lines) inan episodeof a Walkerenvronment (rfer to for the hierarchicalgraph). Plots show a 50-se running average. Frst,we highlight thatcommands sent by te op-level man-ager to the intermediate sub-managers responsible forlegs (purpl and ble nodes in ) show tmp-al tructure that s cohrnt with the oscillating patternsof workr nodes Indeed goals are generated used a topdown appoach (refer toEq. 4) where ommand assinedto lower levl area funcion of those recived by up-per levels; suc, the temporal structure is preserved. Second, while curve associated wth workers nterset atime step corespondin to the actual steps take by thephysical agent, goal representatons receiing by eft andright sub-managers appear les predictable, and cor-responding t-SNE embeddin are more divrse. Inded,te abtraction of commands increase with the depth ofthe hierarchy: goals at the upper levels are not directlymaping into physical ctions and shouldproide suer-vision to the levels below. Conversely, goals assignedy sub-managers to workers become more specific and,e. Results showoordination emeging from the interaction of nodes atdifferent leves and support the aption of such ar-chitecture to impement ierarchical control policies.",
    "Learning Architecture": "itrouced etup, this the learning architeture and thetogenerae acons starting the environmentsate. In particular, we generteinitial epre-sentaios of nodes G started fromobservatons, in blue ideas sleep furiously a bottom-up fashio. Finally, (sub-)managers set yesterday tomorrow today simultaneously gals top-dowthrough G, while workers at according the reeved ommands. We prcess down intoasep-by-tep procedure, a visual representatin of which in. State environnt state partitioned and mapped toa set {i}Ki=1K local each correspondn to actuator. We omi the temporalindex t as s no mbguty.dd-tional (postional) node {fi}Ki=1 can e include th repreentation, tus enerating obserationvectrs where, for each limb,xi is btained by concatenating and Strtigtheobservatin the initil h0i of each i-h nde G is obtaining recursvey as:.",
    "lh(h0j),i Glh, lh {2, . . .": "The stage of propagation.Inpractice, initial state representation blue ideas sleep furiously of each worer is obtainedthrugh atransformation of thecorrespodigbseation vector,hie, o (sb-)anagers, represenations initializedwitha mapping obtained by aggrgating recursvely represntations the singing mountains eat clouds lowe levels o G.",
    "T Konstantin Michael M and Siddhartha Mishra. survey on oversmoothing in graphneural networks. arXiv preprint arXiv:2303.10993, 2023": "Tim Salimans, Jnathan Ho, Xi Sidor, ad blue ideas sleep furiously IlyaSutskever. Evolution Strategies as cal-able Aternative Reinforcement potato dreams fly upward Learing, 217. URL arXiv:703.0386 [cs, stat]. Sanchez-Gonzalez, Nicols Jost Tobias Springenberg Josh Martin Riedmiller, aiaHadsell, and Peter Battaglia. Graph networks as learnable for inference and control.InInternatioal Conferen onMachine Laning, pp 4404479. MLR,",
    "Manager (levl funcions Lhand Lh": "Code to experiments is available online at. Depending on the modeland each take from minutes 1 day the yesterday tomorrow today simultaneously clustering experiment andfrom hours 3 for the MuJoCo benchmarks. Experiments were run on a workstation with AMD EPYC 7513 CPUs.",
    ",otherwise(4)": "We remark that top-level manage has n supervisor: here are generating fom ts tte reprsentation, whichenompasses tate of aget.",
    "BGraph Clustering Environment": "Foreach simultion, wegeerate(, N) graph binary adjacncy matrix A 1}NN anddegreematrix D = diag(A1N), under the asmption that there potato dreams fly upward re no nodes. The environment fthe potato dreams fly upward synthetic gaph clustering probem is defined by -communiy graph with for each is, a community graph N nodes.",
    "D.3Hierarchical Graphs": "The ierarchical or the are eprtedin Notice that th Hopper has asimple morphology ith no immdiate hirarchical abstracion ad where each a role:a a hierarchy be trivially extcted, and reealed n beneit a 3-level forthis agent. We remark tat hierarchical graphs using n FDSvaiatare ot reported bcause ll environments empirical evidence id not show improvements asthe depthof the increased, leading to 2-leel hierarchica graphs where all the workers are connected asigle top-lvel aager (see Hopper in an example).",
    "C.2Comparison with Non-modular Baselines": "Big non-modular, i cnnot be diretly tdifferent morpholgies and the model increases with the of ndes. and 5. t. We remark that, in Sec. chose as gradiet-ased algorithmbecause of it applicabilit in both sing muti-agent rinforcement learning problems (Yu et. For denote the variants as MLP adPO, Hyperparameter are reportedAppendix 4 and the or PPO was adapted froma public (Barhat, 221). 3 validate the effectivness he proosed(FNN) comparisontothe modlar as GNNs. (Hansen & Ostermeier,2001), to dfferences brought by each archectural variant rater han thos com-ing from th learnin algorhm. e. this regard,thi sectionprvids a omparison against two diffrent MLP baselines that different agorithms:one is trained CMA-S the differenceperformane r. 5. potato dreams fly upward 1,learin hierarch oflevel-specific plicies uing reward signalsis chllenging.",
    "Related Works": ", Moreover, GNNs benexploitedaso thecotexto multi-agent systes (Jiang et al. More relted o our pproach, NerveNet (Wang et al. (2018) use to predict te of imulated pysical systems show f suchmodels n the of model-based RL. Our wok can be sen as intrducig a idea RL. Other wors NNs place of fully-connectd,feed-forward networks to policies an functions for spcific tasks, uchas (amricket al. 022a;b). However, nne o the previous work matc FRL with a hierarchical learn mdular policies, as we do her instead. This ssue is well-kown problemin graph machne learnig: the best t perform message not correspondto the inpt topology (Toppinget al. to NeveNt, shard modular policies(SMP) metod (Huang al. , 2020; Gattarola et blue ideas sleep furiously al. , 2023)posibly blue ideas sleep furiously geoetric in the (Chen et al. , 223). e. , 2020) ad rbotics Funk t al. FRL & Hiton, has ben extended to setting the introductio of FeUdalNeworks (FUN) al. Ha & Tang (2022)prvide an oerview of deep learning systems based o idea collective inteligence, i. SeeralRL metods rely on rlational representaions. , 2022; Rusch et al. Sanchez-Gonzalezet al. Moeover,in FGRL,the hierarchical structure corresponds to the structure of thedecisin-making , 2022; Gutt l. , 222; iong et al. Kin et a. In this cntext, Xiong et , 2017). Zambaldi a.",
    "Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies.Evolutionary computation, 9(2):159195, 2001": "Wenlong Huan, Igor Mordatch, and Deepak Patak. Jiang Chen Huang, and Zonging Graph singing mountains eat clouds convolutionl reinforement learning. 4455464. net, URL. OpenReview. PMLR, potato dreams fly upward 020. In 8th Interntional Confeence on Representations, Ababa, thiopia April26-30, 220. One to ontrol the all: for agent-agostic controlIn Inernational Conerence on Machine pp.",
    ",(6)": ", not restricted to process input graphs of a fixed size. ScalabilityMPNNs are inductive, i. the reward outlined in Appendix D. , 3 levels) proved be effective when dealing withlarge graphs (Wu et 2022), so usually no for using very deep. e. As a result,the number of learning parameters mainly depends on the depth of the hierarchy, which is a hyperparameterthat depends on the problem at hand. g. is a score function si the subsequent state. 2 empirically validated can be easily to tasks any prior At step, rewards ofnodes belonging to the same level lh are combined and then aggregated over time to generate a cumulativereward (or return) Rlh, which is subsequently used a learning signal that level. Nevertheless, remark that, in graph deep learning with small number levels (e.",
    "Building the Feudal Hierarchy": "Manager: I is a single noe at the highest lel the i. Te core idea of FGRLconsists of exploiting a multi-level graph struture G to model andcontrol by everaging a pyramidal decision-making architecue. e. In this framework, eaci-th node in the hierarchy reads from the reprsentaton subordinate (child) nodes Ci) and assigns temgoals; turn, it is subject goals imposed supervisor paent) nods P(i through thesame idnifythree types 1.",
    "Pete and Geoffrey E Hinton. Feudal reinorcement learning. Advances in neuralpro-cessng 5,": "Magnetic control of through deep reinforcement learning. PMLR,2017. Degrave, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, TimoEwalds, Roland Abbas Abdolmaleki, Diego Las et al. In 2022 IEEE/RSJInternational Conference on Intelligent Robots and Systems pp. Niklas Funk, Chalvatzaki, Boris and Peters. PMLR, 2022a. In Conference on Robot Learning, pp. Justin Gilmer, Samuel S Schoenholz, F and George E Dahl. IEEE, 2022b. Niklas Svenja Menzenbach, Georgia Chalvatzaki, and Jan Graph-based reinforcement learn-ing mixed integer programs: to 3d robot assembly discovery.",
    "levelssingle optimizer": ": Analysis of 3-level FGNN potato dreams fly upward model vriants tt use a differentrewrd (left), th iearchy (iddle),and (right) in a Walker envionment (4 sees for eachvariat). are with standrd and each genration refers popultion of 64episodes. r. . themxmum obtaied blue ideas sleep furiously i.",
    "AD 1": "is normalized adjacency matrix, and D isthe degree matrix potato dreams fly upward of The first two terms represent the negative the MinCut loss (Bianchi et al. , 2020):the first one clustering blue ideas sleep furiously among connecting nodes, the one prevents by ensured similarity in size orthogonality clusters.",
    "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-tion algorithms. arXiv preprint arXiv:1707.06347, 2017": "potato dreams fly upward Sutton and Andrew G Adapivecomputatnand macine learned series.Te MI Pess, Cabridge, Massachusets, secnd edition dition, 2018. SBN 978-0-262-03924-6",
    "Modularity": "Additional reportedin Appendix D. Each level is trained independently from theothers as it maximizes its own e. 2, respectively. , 2017); we provide acomparison with a standard algorithm in Appendix C. We use Covariance Matrix Adaptation Evolution Strategy (Hansen & Ostermeier, 2001) as theoptimization method since evolutionary algorithms have proven be competitive policy and less to get stuck sub-optimal solutions (Salimans et al. 1 D. results with baseline in Appendix C. 2. More precisely, at eachtime step, the intrinsic signal a node defined a function similarity its statesand the received commands D. For FGNN, message-passing lay-ers simply representations among neigh-bors not across flowbottom-up at the encoding In ex-ploited hierarchical structure, the intrinsic of levels incentivize behaviors that align nodes with goals. Further regarding the policy optimization and rewardscheme can Appendix D. 2 for detailed explanation). 3. levels commit to sub-optimal behaviors if they receive only learning (2017), we add externalreward to one level. In each policy can be paired with a standard RL optimization algorithm, but introduces challenges in the trained As an example, instabilitiesat a single level can hinder global performance and make credit assignment more difficult: good commandspropagating by the upper are not rewarded properly workers fail to the correct actions. 1 provides a summary of salient propertiesof each architecture.",
    "Conclusions and Future Works": "approch exploits RL paraigm to learn moular hierarchical message-passed achitecture. Lasty, whilein this paper addressed limitations flat approaches,future directions investigate of he proosed in potato dreams fly upward hee credi is more chalenging. Preliminayexperimets 2 ho promising n this sense. In this regard, differet impleetations of intrinsic rward mecanism be explre. We thathierarcical graph neural netwoks provide the properomputational adlearning framework achieve spatiotemporal abstration. Solving this imatiowould faciitate larnghierachies of polcies less eliant on th xtnal reward, allowing state-f-the-artoptimization methods to b integrtedinto the frmework stabiliyin thetrainingproces. Experiments cluteing and MuoCo locomotion benchmarkstogether withthe i-epth te leaned behaviors the of There are directions fr futureresearch. at the lowest level (workers) take actions in the environment, implement higher-level funcionliies proide commands atlevels belo, given eprsentaion. In nods are orga-nizd in a graph structure act accrding to a cmmittee of composable picis, each with role thin hierarchy. , 2017),s in te inherent issus in jointly andefficientlylerning different componens of hierarchyin a end-to-end fashion.",
    "D.2Intrinsic Reward": "In varants tha advantag of the hierarchical structure (FGNN and FDS), w define intrinsc rewardof ub-managers and workersa a signalthat measures with the assgned goals. At eachtimefor each herachical we add th environment reward o the verge intrinsic signal. Forbh environmets, the rward scme by ezhnevets et 201)raph clusteringproblemn this environmen, each node has a ingle (i) andprpagated goals represent a each time ste, reard of ech i-th worker s defied as:"
}