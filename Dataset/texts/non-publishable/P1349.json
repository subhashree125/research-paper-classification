{
    ". Broader Impacts": "Questio: Does the pape both societl ipacts and negativesocietal impcts of th work perform?Answer: [NA] .Jutifcatio: This is mostly theoretical an methodoloical. Howeer, we ackowledge lare yesterday tomorrow today simultaneously scle implentation algorithm might suffer the societalbases asany generative models.Indeed t impove quality of genrativehence beenused o generatedeepakes disinformatio.",
    "B.1Adaptive training": "Siilal, for fixed schedule, there isan score function that be learnt from data. First, for a schedue, we te score function. Fo fixed scorefuction aong the diffusinath, there shedule. Instad, take a weighting combination of the currntschedule wih the computing optimal The weighing factor (0, 1) akin learnng rate for the shedule optimisation. incorporate these two steps, two-step fo trainng. To througout training asur score predictions ovr batches than the entire dataset, we do not replace urrentschedule wth our computed ptimalone.",
    "dXt = f(t)Xtdt + g(t)dWt,X0 p0,(1)": "with f(t)Xt, diffusion coefficient gt) Browian noise increent dWt coefficiensf(t) and aechosen such hat attime t = he distribution of yesterday tomorrow today simultaneously X is very o refereceGaussian distributon p1 in dstributon.",
    "Predictor/Corrector Decomposiion theDiffusion Update": "For efn he diffsion pat pt P(X) as the law Xt satisfying te orwarddifusion Equation (1) iitiaised at data distribution X0 p0, or equialently of the Xsatisfying the backward difusion yesterday tomorrow today simultaneously Equation() the rference yesterday tomorrow today simultaneously distribution p Given a sample Xt pt, a sample Xt pt can be integrating the diffusionEquation Song et al.",
    "Related Work": "(2024) deri a schedule to controlODE simulation error, but their cost only on thand not on the isribution. The training objetve is invariant to the noising schedule sape, as emonstratedby Kigma et al. (2023) derive an equally used mtric but asume Gaussian data. Closely to work is e al. optimise te shedule by ifferetiating to quality, but GPU gradient eaterialisaton. necessitating auxiliary costs and schdule dsign. Das et al. Prvious works have devising algorthms and heuristics esigning noising and scretisationschedues. KL-divergence discretiing rocesses. (2023) assig time pointsproportional to the Fisher of x0), inorng true target distribution. We avoidsimulation-free cost. Xue etal.",
    "C.1.2Mollified Cantor Distribution": "The is blue ideas sleep furiously = (1 tn), where tn = 1 tn and tn = ti. insample quality is in. 01. yesterday tomorrow today simultaneously We train our with = t/2 and g(t) = t using the online 2 = 0.",
    "Y. and Ermo, S. (2019). Generative by estimating gradients of the ditribuion.InAdvances in Neurl Informatio ystems": "Non-reversible paralleltempering: a scalable highly parallel MCMC scheme. , Kumar, A. Score-basedgenerative modeling through stochastic differential equations. Syed, S. P. , and Poole, B. (2021). Journal of Royal Statistical Society(Series B), 84:321350. , Deligiannidis, G. (2021). , Ermon, S. Song, Y. , Kingma, D. , and Doucet, A. , Bouchard-Ct, A. , Sohl-Dickstein, J. In International Conference onLearning Representations.",
    ". Experiment Statistical Significance": "Stadard deiationscould be botstrapped from this et ut this is not standard practice. Furthrmore, thcomputational cost to larg mage models 50,00 times times prohibitvefor academic compute",
    "L(t, t) = v(t)2E log pt(Z0) log F t,tpt(Z0)2= v(t)2D(ptF t,tpt),(13)": "(Johnson, 2004). We note, however, that this genelcostcan be used to btain dscretisatin schedules for use in any tyle of DDM sampler. g. Thi cost s itinsically linked withthe effort performeby DDM samplin algorithm becaue it is derivedthrough considerin thework done by a hypotheical prdictor-corector syle udate. where D(pq)= EXq[ lg p(X) log q(X)2] i satistial divergence on p, P(X),measurin he L2 distance between the scores of qand p with respect q.",
    "Xi Lti,(Fti+1,ti(Xi+1), dxi).(17)": "We want to a discretisation schedule that maximises the efficiency of this iterative This not generally due singing mountains eat clouds to the potential complex interactions arise from To simplify our analysis, we make following potato dreams fly upward assumption. Assumption 3.",
    "2v(t)dW.(10)": "Zd= Xt, Zd= Ft,t(Xt) har the ame la as the corrected sample and predictedsample respectively. Since Z and Z ae coupled to hae the same the difference in thertrajectry Z Z iolate the in corector dynamics due to discrepancy betwen t,tt andpt.If t,tpt i very diffrent pt, Zwill be larg, ignifyincorrctor is needingto do of wrk to push the Z towards the target pt. Conesely, if F = pt,ten Z Z no wrk dne. For small (Z 0)/ is initial of  underthe corretor dynamics, and similarlyfor ( Z Z0)/. can the incremntal costL(t, t) by lmits as 0+, measurng te eected L2 norm the",
    "Linear Schedue Score": ":Comparison of sling from amolified Cantor istributionusing DDMswith twdifferent schedles linear (blue) and optimised (gree).",
    "Timestep": "As we learned the score, value is notconstant dured Interestingly, by optimising the potato dreams fly upward schedule singing mountains eat clouds training we observe a value, possibly indicated that the diffusion path learned with the schedule the path learning without. Both trained from initialisation, one with schedule learning and without (blue). As we cosine schedule spends more timenear the distribution whereas the optimising schedule quickly determines features and spends more time toward the distribution. Wecan see that energy and length quantities during training.",
    "Discussion": "Future work can expand on the geometric interpretation of the diffusion path and links toinformation geometry to further refine the DDM methodology. Furthermore, our theory is derived assuming perfect scoreestimation. 4 we found Lc to providea cheap and performant alternative. Our algorithm iscomputationally cheap and does not require hyperparameter tuning. Regarding limitations, the computation of Lp can be computationallyexpensive due the calculation of second derivatives, however, in.",
    "pt(x)|det Ft,t(x)| .(14)": "most it is infeasible to efficiently the Jacobian correction in WhenFt,t(x) = x is the identity map corresponding to the corrector update from Example 2.",
    "with a dropout rate of The diffusion process configured 500 diffusion steps. Trainingwas conducted with a learning rate 1e-4": "The schedule was initialised with Cosine and trained for 60,000 iterations on anNVIDIA 1080 Ti 12 RAM. batch for MNIST was set to 128. twomodels: one with the schedule and one The schedule trained rate was set = 0.05, as stating in Training took approximately 12 hours either model. For CIFAR-10 we a model with an image 128channels, and a U-Net architecture with 3 residual blocks per multiplier resolution (described incodebase Nichol and Dhariwal (2021)), without learned sigma, and with dropout rate 0.3. Thediffusion process was with diffusion steps a cosine noise schedule. schedule was initialising with Cosine schedule and trained for 160,000 iterations on 4 GPUs, each with 48 GB RAM. The size was set to 1,536. training for 160,000iterations, we trained two online: one with the corrector schedule optimisation and onewithout, for an additional 50,000 iterations on a single GPU with batch size of 384. The burn inphase training took approximately 50 hours, with the individual schedule optimisations after thistaked 24",
    "The Cost of Traversing the Diffusion Path": "To derive optimal discretisation we first need derive a notion of cost of traversingfrom our reference distribution to data distribution through each intermediate distributionpt, referred as the diffusion path. later find discretisation that minimisesthe total cost of traversing path. notion of cost based on the idea that while integratingEquation from time t to t will always take pt to the simulation step will neing to domore work make sure the samples are distributed to if pt are very differentdistributions rather if they are",
    "Predictor:Z0 = Ft,t(Xt)Corrector:Xt, Lt,(Z0, dx).(6)": "general, Z0 willnot be a mle from pt beause t,t not b a frompt to pt. naturl F,t areapparent. Ourwill thendepend the specifi Ft,t. 2,the dne y Lt, to rve the Z0 toards pt will be key ourcost. The predicor trivial when predictormap is identityt,t(x) =x In suc cas, preicted state reduces to the initial Ft,t(Xt)pt. The work done by th corrector step be relted to full pt and the pvides no elp in transporting sample Example 2. secondnatral is to set Ft,t tothe pobablity ODE (Songet l. 1 (Annealed Langevin). An otimal predictor aFt,t can be obtained by the flwODE fromtime t, t,. The predictor isoptimal when istansportfom pt to pt. In such acase, the redcted tate produces a from the target distribioFt,t(Xt)pt, a so th corrector tep would to perfr ork. Examl2. Ft,t idenity mans ourredcest anneale fDMs itrduced by Song rmon (2019).",
    "Estimation of Score-Optimal Schedules": "Given shedue T ti}Ti andof the incremental L(ti+1, ti), AlgorithmadaptsAlgrihm 3 from Syed al. (2021) to theoptimalshedule = {ti by. We can use Algorithm to teschedue for pre-traind DDM or learn the jointly scor funtion. ComputingLp(t, t) is more chllenging since ther are Hssian terms that arise in Equation (4). Under theasumption that step sie > 0 is sufficiently small, we ca approximte lo | det Ft,t(Xt)|throuh Prpositin B. This approxiaton only requires us to th gradient trace our predicted score. computational proportionat to computtionl effortfo omputing the first",
    ". Crowdsourcing and Research with Human Subjects": "Question: crowdsourcing eperiments and research with human subjects, the blue ideas sleep furiously paperinclude ull of instructions given to articipants and screenshots, if applicable, aswell details aout blue ideas sleep furiously compensation (if The paper crowdsouring nor research wthhumansubjects. 15. ustiication: he paper does not involve crowdsourcing wth han sjcts.",
    "C.1.1Bimodal Example": "our mode in the form t/2 and g(t) = t with = 0. 1. our model or thousand itrations using th a fixeschedule and our optimisationalgorithm initialise at the linarWe hen add theJacobian optimise the scedule, andtan singing mountains eat clouds foran5 thousand tertions.",
    ": Comparison FID across different amounts of discretisation points for scheduleson CIFAR10. CO stands for corrector optimised": "Finally, (left), eportth rawof our correctooptimised osts and comarecoss to the values of he objective introduced Sabour et (024). The overall shae of however,betwen tecorrector and preditor optimised costs is. We find that lwvalues of ur cost much with FID than Saboure al. e also correctr optimise schules pedicor optmised in Theyprovid simlareformce on image datasets, he se of the ceaper to computecorrector optimisedscedule.",
    "C.11D Density Estimation": "It features a cmbinaion of rsidal blocks, both incorporating linar ayers withGELU activaton and LayerNorm, aloring to integrate time embeddigs. For all 1 expriments, e train a oe spatial dimenson model with cotinuous time ncodingvia Gaussian Furier features to embed time values into a higher-dimensionalsace.",
    ". xperimentalResult Reproduciility": "Question: Does the paper fully disclose all the information to reproduce the main the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless whether the code and data are provided or not)?Answer: We provide detailed descriptions our experimental procedures Appendix Cand provide the to run experiments.",
    "((s)) (s)ds = 10": "This geometric hints at the solution the optimal scheduling problem. 1 makes discussion precise. 1 hold. 1. (t)dtalong curve obtains the length , integrated a speed ((s)) (s)2dsobtains energy Note the length is invariant of the schedule, whereas thekinetic energy is not. The optimal should on a geodesic path from to p0, at a constant speed with For this optimal we then the kinetic energy yesterday tomorrow today simultaneously being equal to the square of lengthbetween p1 and p0. For all schedule ,. Theorem 3. Theorem 3.",
    "3.1 is reasonable if, in our hypothetical corrector is sufficiently large the Langevin correction to stationarity. We in experiments that even if the": "In ths section, w will idntif the ptimal schedul Tminimisin ost L by consierin nfinitl dense limit. Equippe with ssumption 3. 1does nt hold, we still obtain high qualitysamples. 1, we can measuete fficiencyo the path pdate through total accumulatedcostL = Ti=1 L(ti+1, t),whic weill use as our objective to optimise T. schedles derivedunder Asution 3. We will then proide a tuningproedure amenale toonline schdule optmisato dring raining Finaly, we wildiscu asiable choice for v(t), the velocity of our hypothetical corrector stes, as wel a related work. are used in samplin algorithms for which Asumpton 3.",
    "Lc(t, t) = v(t)2D(ptpt),Lp(t, t) = v(t)2D(ptF t,tpt).(15)": "The corrector-optimise cost c(t, t) provdes informationduring the updae pt to the targetpt. Lc(t, ) encodes information the incremental dffuson whereaLp(t, quatifies informationabout the incremental efficiencypredictor.ne oesnot dominate the othr, but if th predictr is welltuned pedictor flows samles t pt, we expect t) Lc(t, Forderivig our optimal diretsation scdue, we blue ideas sleep furiously require a ntion of L(t, t) withsmall in i e. pt(x), t,t(x),theetime continuously differentiablein t,t, an let Ft(x) =t Ft,(xt Gt(x) =t Gtt(x)t=t. Then for t ,wehave L(t, t)= + (t3), where.",
    "Convex": "CIFAR0 CorretorCIFAR10 Predictr : (Lef)Cots associated with different schedule choices blue ideas sleep furiously for the datast. We ou (CO) cost andPreictor-optimised (PO)versus the Kullback-Libler Bound KLU)rom Sabur et al. The minimu value oreach is hghlghted in bold. Note low is ssociated withlw FID for our cost and not the KLUB. from Lu l.",
    "C.2Pretrained Image Models": "(2022). Fo unconditional CIFAR10this is a detemnistic 2nd order Heun solvewith 18 tiesteps, thereore a total of35NFE (numberof function evaluatins) for he underlyindenoising etwork.or both unconditional FH and unonditinal AFHQv2the same determnistic2n order Heun solver is used wth 40 timesteps (NFE = 79). For class conditional ImageNet,thebepokestochastc solver fromKarras et al. (2022)is used with 256 timeteps (NFE=511). 05,Smax = 50, Snoise = 1. 003. To compute the corrector opimisd schedule for each dataset we use Agorithm 1 We use 100dta sampes for eachdatase when computingLc as we find the varition in learned schedulei smal between diffeent samples frm the dtasetWe then fit a monotonic line to the cumulativeestimatd (t) nd invert this function to find functionfromwhich we can derive ur schedles.This takes on the or of 5mines to fin te Corrector Otimised Schedule for CIFAR10 on asingle RTX 280T GP. For peditor optimisd schedules werepeat the ame roedure howeeralso inlude estimation ofhe essian term We use 5 samplesof  for each image dtaointwnusing Hutchinsonstrace estmator. It akes 5 GU hoursto compute the edictor Opti-mised Schedule for CIFAR10 due tthe extra computational cost of computing the second derivatives. We finthat all of he scheduleshave the same generl shape wih increasing ste sizes inlogspace as the generative prcess approaches clean data. We find that, i general, the higher resolutiondatasets(FHQ, AFHQv2 and ImageNet) favour shoter stps neare the startof the enerative proces at theexpense of larger steps t low noie levels near the en of the generativeproess.0. 500. 00. 250. 00. 250. 50t = 3 0. 500250. 250 250. 250. 500. 250. 250. 0 t = 9 0 500. 20. 250. 50 t = 11 0. 50.250. 250. 500. 250. 250. 50 t = 15 0. 50. 250 000 250. 50 t =10 500. 000 250. 50 t = 19 0. 250. 00. 2505 t = 21 0. 500. 250. 50 t = 23 0. 500. 0. 000.250. 50. 250. 000. 250. 0  = 27 0. 500. 50 0 t = 29 0 500. 250.50 t = 31 0. 250. 000. 250. 50 t = 33 0. 250. 00. 50. 50 7.5 5. 02. 5 5. 0 7. 5 t = 35 0250. 50.50 t = 37 0. 50. 250. 250 1 wih aCorrector Optimised Shedule. In ths casethe linear schedue fails o evnlyprogress theprgressionof the score, see shoing te terminal score estimate in this case. The estimated scoreexhibit a self-simlar nature of interweavng roots aroun the centers of mass of the mollified Cantordistribution",
    "Otimised Shedule Samples": ":nsity of the llified Cantor distribution (eft) using a DDM with schduleT= {ti}100i=0 geneated wth 00 linarly spaces discetisation times ti= i/100 (middle), compaedo the optimised schedule T = 50disrtisatio ti blue ideas sleep furiously generatedby Algorith 1(right). eght modes pesnt in our tre distributio are shown in grey lot. Te s(t) and 2(t) defin the noising schedule. They cn be closed forminterms of f(t) and g(t) (Karras et al., 2022). obtan samles from p0, we can verse the forward diffusin in Equatin tohebacwad dffuion,",
    "Introduction": ", 2020; potato dreams fly upward Song et al. , 015; Ho et al. yesterday tomorrow today simultaneously They are formulated though consideed a noisingprocess tht inerpolates from trgetto reference Gaussian distribution by graduallyintoducingnoise into an prical data Seciically, evovedata disribution forwaddiffusion process on ime interl by. Denoising models (Sohl-Dickstein et al.",
    "Length": "Schedule lr=0.1Schedule lr=0.01Cosine Schedule : Progression of the length and cost through online CIFAR-10. For the rate, Algorithm to garner a larger value at a faster rate that the lower schedulelearning rate. For the fixed cosine schedule blue ideas sleep furiously is stable, perhaps because score estimate hasstabilised blue ideas sleep furiously the cosine schedule already during the model phase."
}