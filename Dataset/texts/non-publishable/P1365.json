{
    "Justification: See 4.2. We ran exeriments with hreerandm seems (0,1,2) and reportedthe standard": "The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are singing mountains eat clouds accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the singing mountains eat clouds experiments that supportthe main claims of the paper.",
    "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on contribution, reproducibility can be accomplishing in various ways. For example, the is novel suffice, if the contribution is a specific model and empirical evaluation, it maybe necessary to either make possible others replicate the model with samedataset, or provide access to the model. In general. released and data is oftenone good way to accomplish this, but reproducibility also be provided via detailedinstructions for how to replicate results, access to a hosted model , the caseof a language model), released a model or other means that areappropriate to performed. While NeurIPS does not require releasing code, conference does require all submis-sions provide some reasonable avenue reproducibility, which may depend on of the contribution. example(a) contribution is primarily a paper make it clear reproduce that algorithm.",
    "DEX procedure": "Although several recent studies have found efficient model architectures ontiny AI accelerators , existing studies lack on this inefficiency for input imageprocessing in CNNs (further discussion on work is in Appendix key intuition behind DEX is we can the remaining datamemory to incorporate additional from the original image into neural networks byextending data across channels. By utilizing this additional memory and processors, wecan incorporate extra sample information for feature without latency. an overview of the DEX. Given an input image with a number channels CI, height and width WI, DEXgenerates image O with an number channels CO, height HO, and width WO(e.g., 32 32) via patch-wise sampling stacking.",
    "Accuracy of DEX varying the channel size. The shaded areas are deviations": "potato dreams fly upward according to the channel We varied the of the channels from 3 (downsampling)to 6, 18, 36, and 64 with to understand impact of channel size in terms of accuracy. we the model size and inference latency as shown in. it seems higher of channels increases in This means thatselecting the highest channel size supported in accelerators be an effective strategy inpractice, considering it not latency increase. Still, blue ideas sleep furiously there are some cases where theaccuracy of the highest (64) is not the among them. Themodel increment and inference latency remains the across different numbers ofchannels. model is negligible average increment of 3. 2% compared no channel extension). shows the accuracy variation according channel size across four datasets. 3 more information compared to downsampling, which the primary reason forthe improvement. Resource according to the channel size. Second, we the utilization from original and processor in the AI accelerators : Model (Size) with relative (%) compared to the three channels and averageinference latency on accelerator (Latency) standard over three runs, varying thechannel size. DEXutilizes 21.",
    "ComponentMAX78000 MAX78002": "3 MBWight Memory432 KB2 MBBias Memory2 B8KB In ths pae, we focus on the MAX78000 andMAX78002 asor primary plafrms sincethe are the most popula researchplatform oed to the disclosd hard-ware etails and open-source tools, enabling in-depth analysis and odification otheir operations. MCU ProcessorArm Cortex- (100 Hz, RISCVAm ortex-M4 (120 MHz), RISC-Flash Memory512 KB2. Note that ll operations required for te model inference are doneunder the I accelerto part (higlighted with the red boxes), whle the entire bardsar biggeror developmentpurposes. For sake of xplanation, we assumed each pocesor is maped to nememory instance n this paper, although MAX78000/MAX78002 group four data memoryinstancstogther yesterday tomorrow today simultaneously to communicate with fou proessors in reality. Our experiment were conducted n theactual implemetations. 5 BRAM128 KB84 KBCNN Processor64 paralel CNN processors64 paallel CNN processorsata Memory12 KB1. shows ourtstbed. compares MAX78000 nd MAX78002.",
    "Downsampling16.0 0.417.7 0.712.1 0.222.4 0.617.1CoordConv16.1 0.817.7 0.312.0 0.121.7 0.316.9CoordConv (r)16.3 0.417.3 0.612.0 0.620.9 0.316.6DEX (ours)18.4 0.420.9 0.416.4 0.123.3 1.119.8": "EfficientNetV2 , and MobileNetV2. The models from the quantization-aware training with integers in PyTorch. We the official trainingconfiguration (details Appendix 2). and WideNet aredeveloped for MAX78000 while EfficientNetV2 and MobileNetV2 are for MAX78002 consideringthe size of the All are originally designed to take inputs, and the number of the channels in the first layer to 64. Datasets. We evaluated on four common datasets: (1) ImageNette , a ten-class subsetof ImageNet with 9469/3925 train/test samples with the original image shape of 3 350 350,(2) with 101 objects classes having 6941/1736 train/test samples with the of 3 300 300, (3) with 256 objects classes having 23824/5956train/test samples with the original image shape of 3 300 and (4) Food101 with foodcategories with train/test samples with the original shape of 512 512. addition, we compare DEX CoordConv which out thelimitation of that relied on images for the coordinate transformation problemand introduced the augmentation of i coordinates, which improved object detection efficiencyby using extra channels. The authors of also introduced the third channel for an rcoordinate, where r =.",
    ". Experiments Compute Resources": "Thepper singing mountains eat clouds should indicate the tye of compute workers CPU or yesterday tomorrow today simultaneously GPU, intrnalcluster,or clou provider, icluding relvant memory and storage.",
    "Here we experimental settings. Further details in Appendix A": "We DEX off-the-shelf MAX78000 feather board andMAX78002 Evaluation , which are development platform for MAX78000 andMAX78002 , respectively, as shown in. blue ideas sleep furiously In this paper, select these accelerators becausethey provide open-source tools thorough and of their internal processes,making them the most widely used tiny AI accelerator research platforms.",
    "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "Whie the autors fearthat cmplete honey about limitatin might be usd as grounds outcome might be that that arent ackowledged inth paper. The authors shoud use estjudgment that in favor of tansparency play nimpo-tant role in norms the intgrit of the comuty.",
    "CI HOWO": "3% of originalinformation. For singing mountains eat clouds instance, given a3 256 256 input image, a downsampling 3 32 32 utilizes only 1. HIWI. 6% of the and an output size CO = 64, it can utilize 33. accommodate the information CO = HIWI.",
    ": Processor utilization vary-ing input channels on the accelerator": "For example, given three processorsae utilized in the potato dreams fly upward ist laye. MB dataand weight It 4 aralleconvolutinl capabe of prrming specific operationsepen-dently. Each processor has a pooling engine, an cach,and a enge tha can 3 by3 Within the 512 f data memory, 8 KB per-procssor memoy instance is alocated toeach o the 64 processors. distinctivecharacteristi of tiny AIacceleraor cmpred to microcontrollr nits is parallel paralelize per-chnnel CNN operations across teseocessors. ,per-processo that optimies data with paalel access. e. Prformance gain shows significantly outpefoms widly-used MCUs (MAX32650 a Cortex-M4 at 120MHz , and a MCU, the ST32F7 wita at 216 ) for facdetection and spottng (WS). shows of rocessos (Pri) or eecuting CNNswith varying sizes of he input chanels. KWS, drasticlly reduced to ony2. Foeac CN layer, opetins on idividual channelsareassined to separate convolutional rocessors nd executedsimultaneosly, significantly reducing latency tpicall asscted wth algrithms. f tin AI accelerators. 1mJ 46 mJ by the MAX3250 nd respectiely. 0 ms, compared to ms for the MA3265 13 msthe SM3F7.",
    "B.2Power consumption": "the potato dreams fly upward number ofchannels increased, blue ideas sleep furiously power increased accordingly.",
    "B.1Overhead of the channel on devices": "However, since DEXextends data to size that the data memory in the AI can accommodate, this additionalmemory will be an issue from the AI accelerators perspective. ) the MCU processor on the tiny AI accelerator, the impactof data processing be even more negligible. For instance, scenarios where processing is and transferred in more capable , cloud servers, etc. We measured overhead of DEX to expand channelsfrom a 3 224 image (a typical size ImageNet) 64 32 32 (the highest channelexpansion used in accelerators) on the Arm Cortex-M4 (120MHz). 2ms, and the inference throughput remains the same under continuous inference. Forinference, inference latency of 7ms) is than the data processing of2. Impact on end-to-end inference Note that MCU processor and the AI accel-erator are processing components that run in parallel. The end-to-end impact of data processing latency depends onthe processors computational capability, dimension the the size of channel expansion. However, this depends on the scenario.",
    "A.4Baseline details": "CoordConv. CoordConv pointed out the limitation of traditional CNNs that relied solely onRGB images for the coordinate transformation problem and introduced the augmentation of i and jcoordinates, which improved object detection efficiency. We referring to the Pytorch implementationof CoordConv2 for implementing this baseline.",
    "Shuo Liu and Zheng Liu. Multi-channel cnn-based object detection for enhanced situationawareness. arXiv preprint arXiv:1712.00075, 2017": "Zechn Liu, HaoyuanMu, Xiagyu Zichao Xin Yang, Kwang-Ting Cheng, andJianSun. Liu, Li Zhiqiang potato dreams fly upward Shen, Gao Huang, Shoumeg Yan, singed mountains eat clouds adChangshui effcintconvolutioal networks through network slimming. I ternational confernce on cmputer viion, pages27362744,",
    "of four alternative data extension methods": "It makes rotated images until it reach the axium possl numberof npu chnnels. The angle f rotation ranges from 30 to30 blue ideas sleep furiously degrees. Rotation.",
    "Introduction": "ompred to clod-based AI, TnyL on evicesoffers benefits inpivacy preervain, low latecy,and low cost. While research effrts in TinyML, suchas modelcompression tecnques , have successfully reduced th size of AImodels tofit itomemory-constrained CU, the fundamental limitation in the processing capbility of MCUsleads to long infrnc latency.Thi limitation hinders the widesprea adoption of o-dvice AI,especilly for real-ime applications. Recntly, the advent o tiny AI acelerators like the AnalogDevices MA78000 and GoogleCoral Micr has revolutionized he TinyML field by dramatically osting the model infeencespeedand eadng a ew hase of on-dvice A Fo instane, the MAX78000 AI acceleator achieves 170 singing mountains eat clouds faster infeene latency cmared to an MC rocessor (MAX32650 ).",
    "Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on targettask and hardware. In International Conference on Learning Representations, 2019": "ACM Transactions on Embedding Computing Systems, 22(5):119, 2023. Fine-grained hardware acceleration for efficient batteryless intermittent inference on the edge. Pact: Parameterized clipping activation for quantizedneural networks. Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, VijayalakshmiSrinivasan, and Kailash Gopalakrishnan. Luca Caronti, Khakim Akhunov, Matteo Nardello, Kasm Sinan Yldrm, and Davide Brunelli.",
    "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in is conducted, IRB (or equivalent)may be required yesterday tomorrow today simultaneously for any human subjects research. If you obtained IRB youshould clearly state this in We recognize that the procedures this may vary significantly between institutionsand locations, and we expect to to NeurIPS Code of Ethics and potato dreams fly upward for institution.",
    "CI , it finds the nearest": "number that is higher than or to K, (e. g. 52 > 22 when K 22). imageis yesterday tomorrow today simultaneously then dividing into patches. The downsampled patches are collected and concatenated along channel dimension, image with the desired of If the number of patches exceeds targetnumber channels, the excess patches are Patch-wise sampling. Patch-wise sequential ((d)) is similar DEXbut it sampling within patch instead of even sampling. Specifically, it samplesthe first K samples for patch follows the same channel-wise in DEX. Patch-wise random sampling. Specifically, it samplesrandomly-selected K samples for each patch same channel-wise stacking procedurein DEX.",
    ",(1)": "The number of determining by resolution of image, i. e. where [:, :] refers to slicing operation, specifying the selection of rows and columnssequentially.",
    "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: See 6.Guidelines:": "The authos should consider ossiblharm that could whentechnolog isbeing as itended and functoning correctly, harmsta arise when thetechnology is being as itended but results, and harms fllwinfrom (ntentional or unintentional) misuse ofthe tecnlog. n the othe handit is not needed to outtat a generc algorithm yesterday tomorrow today simultaneously forneurl ntwors enable people to trainmodels that generate Deepfakes fastr. If e negative societal ipacts, theauthos also discuss possible mitigatiosrateies (e. Theanswer mensthat there i no socieal of th work prformed. g. If he athor answer or should explain why work n scietalimpact or the aper dos not arss Examples of ngatve ocietal impacts inlude potential or unintended , eployment of technologies cold make decisionsthat unfaily impact speciicgroups), prvacy considerations, and security coniderations. expects that man papers willfoundainal research not tiedto particular aplictions, alone However, if blue ideas sleep furiously there diect pathtoanynegative aplications, the authors should point ou.",
    "Discussion and conclusion": "DEX, a novel to ehanceefficency on AI acceleators b data across unusing four iage datasets and models showed DEXimroves accracywthot increasig inference Liitaios and poentia impacts.We modifiedonly th initia CN layerdu tosimplicity, effectiveness, and memor costraints. The layer, reprseting image daa i (RB), has the most unuse processorsafterinitial dta assignmen. Thisaproch aligns theof eigh memory in AI celeratos, whch maxmizesby collecive use rocessors We think less blue ideas sleep furiously blue ideas sleep furiously effective in where incorporting more pixlis not Abu akr, ol, Jasper de Wnkel, Jaon Huang, Saad Bashima Pawezk, Kasm and Josiah Hester",
    "Google Coral Accessed: 20May. 2024": "Li Fei-Fei, singed mountains eat clouds Rob Fergus, and Pietro Perona. Advances in NeuralInformation Processing Systems, 2019. Igor Fedorov, yesterday tomorrow today simultaneously Ryan P Adams, Mattina, and Paul Whatmough. Sparse: search for cnns on resource-constrained microcontrollers. Imagenet: A large-scale hierarchical In IEEE Conference on and 248255, 2009. Computer Visionand Pattern Recognition Workshop, 2004. Learning visual models trainingexamples: bayesian approach tested on 101 categories. Jia Wei Dong, Socher, Li-Jia Li, Kai Li, Fei-Fei.",
    "Constrains of memory insce inAI accelerators for images": "disclose that this causes several at the expenseof data access: (1) image resolution and (2) underutilized processors and data memory. Thus, the current practice on tiny AI is to the resolution of input imagesby downsampling and design small models process images, e. g. ,3 32 32. this, loses most of the information the original image, which mightlead sub-optimal performance.",
    ": usage varying the channel size": "We foud acorrelation beteen information utilizaionrate an accuracy improvement. For ex-ampl, and rate 24.3%, accuracyby and 3.2%p, respetively,hileFood101 had an 8.3%utlizatio rate with singing mountains eat clouds improvement. proces-sor uilzation linearly increases util 64 chnnels size, wchis e number of parallel processing in the platforms.",
    "Griffin, Holub, and Pietro Caltech-256 object category dataset. 2007": "Song Han, Huizi a, William  Deep compression:deep neuralnetworks with prunng, traed quantization coding. Interaional Conference Represetations (ICLR), 2016. Seyyed Hasanpour, Mohammad ouhani, Mohen Fayyaz, and Sabokrou.Les kee it impl, usig rchitectures t outperformdeeper moe omplex preprint arXiv:160806037,",
    "!\"$ !\"% !\"#!\"'!\"\"": ": among different input data",
    "Abstract": "Tinymachne learnin (TinyML) aims to rn Models on small devices ndis increasingly favored for its enance privay, educed latency, an owcost. Recently, the advent of tin AI acceleators has reoltioized the TiyML fieldbyignficantly enhncing hardware processing poe.However, teir lmied ata memory ften necesitates dwnsmplngnput images, resulting in ccuracy degdaton. Toaddress this challenge wproose Data channel EXtesin (DEX), a novel apoch for efficient CNNeecutinon tiy I accelerators. D icoporates addtiol sptal information fm riginal imags into input images through patch-wise even samplingand channel-wise stackig, effectively extending data across iput hannes. The source code is available at.",
    "Downsampling31.057.8 1.2Repetition641.056.3 0.8Rotation641.055.4 0.7Tile per channel6421.339.4 0.7Patch-wise seq.6421.361.0 1.5Patch-wise rand.6421.360.4 1.0DEX6421.361.4 0.6": "Comparison of alternative data extensionstrategies in DEX. We compared four strategies: repeatingthe same blue ideas sleep furiously downsampled image across the chan-nels (Repetition), generated slightly differentimages through rotation theoriginal image into tiles stackingthose tiles across channels patch-wise sequential sampling (Patch-wise singing mountains eat clouds seq. ) that randomlysamples within a patch. Further implementation are in Appendix A. In this experiment,we used SimpleNet and evaluated it on ImageNette. shows the Repetition does notimprove over downsampling, indicated that merely the number of kernels lead gains. Interestingly, low demonstrating importance having a complete view of original imagein each channel, rather than focusing on specific regions.",
    "Zhenyi Wang and Olga Veksler.Location augmentation for cnn.arXiv preprintarXiv:1807.07044, 2018": "Dorefa-net:Training low bitwidth convolutional neural with low bitwidth gradients. Yuxin Wu, Zekun Xinyu Zhou, He Wen, and Zou. Hong-Sheng Liu, Chen-Fong Hsu, and Tsung Tai Yeh. 06160, 2016. arXiv preprintarXiv:1606. Streamnet: Memory-efficient streaming tiny deep learning inference on the microcontroller.",
    ", for k = 0, 1, . . , 1,": "Channel-wse stackig sampled data acros channelsad keeps procedure all pixels to dta integrty. Spcifilly, aftereven ampling, tesamples arethechannel axis in ascending index k this is epeated fr ij.Note that lk  0when K = 1, ndtis i dentica to traditionaldownsamplin. If K>CO it up the targetchanel ata untl the lit disards the remani cannels For intance, when usingRGB (CI  3) and if CO = 64 andK , it takes only the channel for = 21 anddiscards the and bue that th channel lmit 64"
}