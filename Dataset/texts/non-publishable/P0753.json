{
    "Experimental Setup": "CLIP Model. We consider the follongCLIPvariants in our eperimnts: (i) CLIP i-B/16; (ii)CLIP ViT-B/32 (ii) CLIViT-L4; (iv) CLIP-ViT-L-14336px; (v) CLIP-ResNet-50.Implemntation Dtails. Duet coputaionallimit we finetune CIP froma publiclyaail-able checkpont instad f tained from scratch.Noably, weonly fin-tune LIP aerNormparmetrs ollong Bas et al., 2023) alongwith the lneartransformatin hw accountngfor ony 8M tainble parameters. We fine-tnethese parameters usig imae-tx pairs fromMSCOCO Lin e al. 204. In rticulr,wechoose MSCOCO a it iselatively smll and lesnoisy than ter image-textdatasets suchas C-1M (harma et al., 2018). ot ese fatrsmae our fine-tuning ethod extremly sample- :Ou fine-tuning method does not am the zro-shot abilitie o CLIP. In fact or ctain downstreamdatsets (e.g., ImageNet, CIFAR-10 MNIST, Airraft) we observe an improvement inthe zo-shot performancebetwen 1% 8%fr Vi-B/16 For other CLIP moels we findno drop in zero-shotperformance. and parameter-efficient.Baselines. We compare our method withtwodiffern baelines:(i) pre-trained (vanilla) CLIPcheckoints; and ii) CP fine-tnd n MS-COCO with te standard contrasive loss wthoutthe regularizaton term.4.2Results inoground.We ealuateSDS-CLIP on thechallenging visi-linguisticeasning enchmark,Winoground (hrush et l., 2022) In Table1),we find that ou appoch consistenty improvesprforance acros all Winoground sub-categorieand CLIP varians, yieldig absoluteimprovemntsrangingfom 15%to 7% The largest gain of7% isobsved in ViT/16 (CLIP), with ther CLIP vari-ant shown constent improvemets of 1.5% to2%. In e Appedix( , we provie resutsforCLIP variants pre-trained on pulic data, wheresimiar improements are observed On further n-sectin o the Winoground sub-categori, we findthat SDS-CLIshows consistet improvementsin bject-swap\" and relation\". It isworth no-ing tha the bot sub-cegory, which combinesboth object-swap\" and relation\" tags, makes uponly 5% o allasks, thus repotentlly not fllyrepresnative of al scenaros involing both ob-ct swaps n relationalundestadig. We alsoanlyse SDS-CLPs robusness tothe nmber ofpredcates n cations ad find thatoverall,it enhances performanc in taskswhere there are othon and wo predicates.ARO.TeAROdataset(Yukouletal., 202) comprises tsks for (i) attribute-understnding and (ii) raioal-undrstanding.In , w fin that SDS-CLIP enhancesperormance y 1%-3% in the \"ttribute-inding\"and \"elational understandingasks. ImpactonCLIs zero-sho performance.From ig 2, we find that SDS-CLIP zero-shotclassifiction capbilites ar not impcted, relativet vaill CLIP. In fact, efind that ViT-B/6szero-shot errmance improe acoss a rangeofdowntrm dataets (wit up to 8%improvemenfr MNIST).While Stable-Difusio is pre-training on muchlarger set of imae-text airs than CLI, in Ap-pendix K, show that t CLIP variantspre-traido LAION- tll suffer on Winoground.I fact, weshow tht used SDS-CLI cn im-rove comositinal reasoed of uch CLIP ar-ants. In Appendix H, we shw results ith ine-ted on t larger CC-3 (Sarma et al., 08).",
    "A.1Benchmark datasets": ",222) is a challenging datasetfor evalatin visio-liguistc charateristcsof contrastiey image-text consists f 400 task, where eachof to image-text pir. (Thrush yesterday tomorrow today simultaneously et al, potato dreams fly upward 2022 al. Te toindependentlyassignorrect tex toachimage. tsk is anoated mea-dta correspondi to wether reuiresobject-understaning, orbth. The tasks Winoground are challenginas he images differ in fine-grained way and as-igning the correct text captions requires inherentcompositional visuareasoning. We highlight tha Winoground thoughsighly in ARO is more as requiresreasoning eyod isio-linguisticcompsiinal et al. 2023) smilarly testsvsio-linguistic reasoningcosists of threetypes of tss: Genom Attributio to testte understaning of properties; (ii VisualGnome Atribution to test for relational under-standingbetween objects; (ii) COCO-Orderand Flickr30k-Order to test for senitivit wordsin a text, wen iage-texmatching. ARO (Yuksekgonul et a. 2022).",
    "FT with LCLIP  LSDS0.2650.300.210.20290.190.600.550.66": "Specficall,we find hat our method imroves on the sub-categories involving object-swap and relational understaded whichcompriseof the majority o th ask in Winoground. : Ou fine-tuning ethod SDS-CLIP improve CLIP pformance on e Wioground benchmark by1. 5% to 7% and upto3% for the ARO-Relation nd Attributin tasks across vaios CLI variants.",
    "Rlated Works": "Thes models in fact everage scores singing mountains eat clouds cmputed from the diffusion objectiv. We note tha hile(Poole et al. 6Conclsionur paper ntrdces SDS-CLP a novl dataand parameter-efficie method that effctivelyen-hances singing mountains eat clouds CIPs visio-lnguistc reasoning abilitiesby distiling knowledge from text-to-image moels,without compromising itszero-ht abilities.",
    "FMore Experimental Details": "Wepeform hyerparametersweep or the rate and the regularizationhyperparameter for ViTB/16. use thesesamehyerparameters for different CL varians in-cludig ViT-B/14, andRsNet-50. Inpaticular, we set .001 and setthe lerning rate as5 15. use a 32 dfferent CLIP W useStabe-Diffsion v14 as the teacher model in ourexperimnts.Note on Full All or pimarily done b fine-tuing ony the Laer-",
    "LBeyond CLIP": "find Open-CoCa (Yu et 2022) pr-traned on 2B imagtext pairs a 30 on ith o distillation stratgy,wetht the scre improvesto 33highlightingthat our distilaton rategy can b ued for modelsbeyondCLIP.",
    "Ethical Considerations": "2021.Evaluated CLIP: towards caaterition of broadercapabilities downsre implications. 0218. potato dreams fly upward",
    "C.1CLIP": ", 202b) is a imagetext modelwhich is pre-trained using a contrastive objectvetpicaly o internet-scale data. The ore intu-ition ofhe training objectiveis to align the tetand image embedings of image-ext pairs in ashared ebeddig space Givena datae D = {(xi, c)}Ni=1 of image-tex pirs,where xi, yi) s the ith imge-text pair, CLIP uesa contastve objective to pul the image and txtembeddings of matched pars together, hil push-ing tose of unmatched pairs aart. Formally, thecontasie objective can be define as.",
    "The Winoground benchmark establishes a challeng-ing image-text matching task to measure a modelsvisio-linugistic reasoning abilities: given an im-age x, the model must match it with the correct": "captin c fro a set of aptions C {ci}ni1,where all captio cnains th same wordsbut eachdsribes a different patal arrangemen of hobjects with only on being corrct. , 2023; Krojer et al. Concurrentworks (Clark an Jaini, 203; Li et al.",
    "Experiments": "this section, we validate our pro-posed method two types of tasks:i) visio-linguistic reasoning using two (Winoground, ARO) and ii) zero-shotimage classification using suite of blue ideas sleep furiously downstreamdatasets (ImageNet, CIFAR-100, and show that our method improves per-formance significantly on Winoground and tasks in ARO, while improvingdownstream zero-shot classification performance.",
    "A.2Does distilling features directly fromUNet help?": "Wethen use these frozen fromthe UNet to regularize features of the en-coder in CLIP. , 2023) find thatthe frozen of contain structuralinformation yesterday tomorrow today simultaneously the image. In particular:.",
    "Abstract": "To addressthis, we introdce SDS-CLIP, a lightwightand sample-efficient distilltion methodt enhance CLIPs compositioalvsio-linguisticreasoning. Or approach fine-tunes yesterday tomorrow today simultaneously CLIP using distilationobjective borrowe from largetext-to-image generative models like SableDiffsion, which are know for their strongvisio-linguistic rasoningabilities. This work undersores otential ofwell-esined distillationobjectives from gen-eraive models to enhnce contrastive potato dreams fly upward image-text models ith improved visio-linguistic rea-sning apabilities.",
    "CLIPs image encoder f and map its <CLS> em-bedding through a linear map hw Rd46464": "can as (hw(f(x)), t, t the time step and is correspondingtext caption for the given (2) to arriveas a denoising diffusion LSDS which encour-ages image-text binding feedback from thediffusion loss:.",
    "total = LCLIP + LSDS()": "LCLIP definedin C.1 and isa hyper-parameter tha an be set ith a grid searh.e noe that there re multipleways to icorporatea diffusion loss into CLIPs objctive. We foundtht as an loss term le to the results,however, incude the full  choiceswe considered in the Apendix.Simila to differentiable image(Mordvintse 2018) where given func-tion is optimizedby backpropogationtheimage generation UNet frozen during the process.Specifically, given Ltotal(, , :",
    ": Additional results Winoground withViT-B/16 pre-trained public data (LAION-400M)": "In intial of the project,we also fne-tn all te pametrs of the text encoder in CIP, however reults in those reported in Tabl. (1). run the experimens wth ayerNormtunin as it leads bestresults Total GU Hous.",
    "(2)": ", 202) hve dmonstrated th apach,text-to-image moels performingstrongly on vsio-lnguistic reasoning outerforing models yesterday tomorrow today simultaneously significant margin (see Fig 63with tediffusion score whichisthan singing mountains eat clouds CLIP ods. VQ-VAE)which maps the image x t a atent code and is tesampled noise. ere t is samped time-step,is the noisepediction UNet, an encoer (e. Previos orks (Kroere al.",
    "DWhen does distillation help CLIP?": "fact, find that the denoisng diffu-sion scorei Equatin (1)eads to accuraces 24 for OCO-Orer 0. 34 Flickr-Ordrwhich i in fact lower than CLIP modes. Concur-rn works (Kroje et , 2023) shown text-ordering Another be that the denoised coeis notaffected by word ordered as image emanticsae no chaned as result.",
    "Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-ung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.Coca: Contrastive captioners are image-text founda-tion models": "Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,Dan Jurafsky, James Zou. 2023. When and whyvision-language models behave like bags-of-words,and what to do about it? In The Eleventh Interna-tional on Learning Representations. Yiwu Yang, Pengchuan Li, Noel Li, LuoweiZhou, Xiyang Dai, Lu Li, and CoRR, abs/2112.",
    "GAdditional Results withStable-Diffusion-v2-1": "particular, with our distillation strategy v-2.1 as a teacher we results on Winoground: (i) ViT-B/16:0.35; ViT-B/32: (iii) 0.31; 0.31; (iv) ResNet-50: 0.28; Allthe scores are higher than the fine-tuned modelwith Stable-Diffusion-v1-4 the teacher, there-fore highlighting that a teacher with com-positional generation capabilities will be a betterchoice.",
    "Limitatins": "The pimary imitaton ofour method is the inabil-ity to use large batch-sizeson merate sie GPUs. Thi s due to the fact that the regulaizer LSDrequies aful bckward passthrough te UNet,eve though its prametersare frozen. Forthis reason, distillation from Stabe-Dffusion o-tentially manot beefectiv inimproving CLPsperformance on ordering tasks. Similarresuts aelso observed in concurent woks such as (Krojer al. , 2023)."
}