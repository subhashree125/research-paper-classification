{
    "B(t)r,+,": "where the coefficient of 1/2 appears as a of the sign initialization. Following a similar induction argument, we are able show (t)r,i:yi=1 has an exponential growth bound. Thisresult suggests, individual (t)r,i:yi=1 cannot too slow compared to blue ideas sleep furiously (t)r,i:yi=1.",
    "Yifei Ming and Yixuan Li. Understanding retrieval-augmented task adaptation for vision-language models. arXiv preprint arXiv:2405.01468, 2024": "Nakada, Halil brahim ulluk Zhn Deng, Ji, James Zou, and Zhang. nderstanding contrastive learning singing mountains eat clouds corporting unpaired dta. Nguyn, Samr Yitzhak Ilharco, Seoong Oh, and Ludwig Schmidt. Thao Nguyen, Gabriel Ilharco, Michell Wortsman, ewoong h, and Schidt. quantity: On yesterday tomorrow today simultaneously the interacton betweedataset CLIP. OpenAI. technical report, Alec Jong Kim, Hallacy, Aditya Ramesh, Gabriel Goh, Sadhii Sastry, Amanda Askell, Mishkin, Jack Clark, et al. Leaning trasferabl visualmodls frm ntural language supervision.In International machine learning,ages",
    "where inequality (a) follows from Lemma C.1; we use (t+1)r 0 and Lemma B.2 in derivinginequality (b). This completes the induction for r U(0)": "With Lemma C. 3 at hand, we are ready to demonstrate the upper bound of the growth rate for signallearning. Lemma C. 3 and n 2500 log(4/),define A(t)r= (t)r+ w(0)r , for r U(0)+ ; and A(t)r= (t)r w(0)r , for r U(0). Withprobability at least 1 , we have.",
    "Abstract": "Emiricl blue ideas sleep furiously on both synthetic datasets furtherconsolidate our theoretical findings. analsi provides a unifie framework theoptimization and bth ingle-odal and ulti-modl contrastivelearning. pre-training a web-scal dataset,multi-modal contrstive lering can learn high-quality epresentations exhibitimpressive robustness and Through the cooperation between the tomdalites, muli-modal learning ca acheve better feature larning, leading toimprovements n performance i downstrea tasks tosingle-modallearning.",
    "t)r,i (t),i + w0)r , i, with = 1,": "Lmm C. 2, then with robaility least 1 ,. Undethe same condition as Theorem 4. 1. (t)r,i (t)r,i + w()r , i, with yi=1,i I(t)r+Wit all theresults (lmas) yesterday tomorrow today simultaneously and defnitions outline aove t hand,w are ready to blue ideas sleep furiously state e lemmasthat provide the lower bound for nois memoriation as follows.",
    "Proof Sketch for Single Modal Contrastive Learning": "Through the pplication yesterday tomorrow today simultaneously of the gradient descent rule outlined blue ideas sleep furiously in fom],the decompoition of ight vector iteraion a be expresed:.",
    "the activationwhich playsa critical in . Furthermore, tis aopt a unifiedframeork to compre with muli-modal learnng, wich is of in": "As multi-modal conrastive suc as CLIP received great sucess, recent work ve been proposing explanationsfrom empirical perspective. epirically showed that traintest similarity isinsficient toexplain CLIPsOOD perormance. illustrated thatCLIP behaves similaly ilanguage-based imge retrel, i. , the rdr of words in th input setencdoe not lagely affectCLIP t fidimage. Besides, emonstrted that diversittheability to the as supervised is the ky to the effective robustness of prove tht multi-modal learning can atent modalitis a data analyzing he ynamis ofa simple multi-modal learnin model and that cotrastive pairs are imporant forthe o efficiently balance repreenatos. Simiar us, tried answer whymulti-moal learning better thansinglemodelHowever, they did not consider contrastive thus cannot explinte success of multi-modl contrastivemuti-modal Aligned ou theoreticalresults, thre is lot of evidence hwing improving the alignment withmore cations improves learning. show hat trainingdistributionmostlydetermines he generalizability of Bsides, demonstrate hatimproving decriptiveness of captions could further boosthe performance of Besides, demonstrated that th by a combination and contrastive learned optimization. their result do not take neural neworkarchitecture into and do provide an analysis of tst either.",
    "Martin Arjovsky, Lon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk mini-mization. arXiv preprint arXiv:1907.02893, 2019": "Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, MateuszLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. In International Conference onMachine Learning, pages 32523298. Language models are few-shot learners. Tom B. Contrastive and non-contrastive self-supervised learn-ing recover global and local spectral embedding methods. Advances in Neural InformationProcessing Systems, 35:2667126685, 2022. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, potato dreams fly upward Sandhini Agarwal, ArielHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Vivien Cabannes, Bobak Kiani, Randall Balestriero, Yann LeCun, and Alberto Bietti.",
    "Wenlong Ji, Zhun Deng, Ryumei Nakada, James Zou, and Linjun Zhang. The power ofcontrast for feature learning: A theoretical analysis. Journal of Machine Learning Research,24(330):178, 2023": "Supervised contrastie learnin Advnces n proessing 33:1866118673, 2020. Jia, Yinfei Xia, Yi-in Chen, Hie Pham, Quc Yun-suanSung ZhenLi, nd Tom Duerig. up viual and vision-language representationeaningwih noisy text supevison. PMR, 2021. In Internatonal confernce on machine ages 4044916.",
    "nm, we have(t)r= O(1/n) for all r [m] 0 t and maxr for all i [n]": "A the end blue ideas sleep furiously of firt stge, the noise grows to aconstantorder while sigal earning remains negligible. As a reslt,the loss derivatives ae no longerbonded within some constnt ange. n the second stage, we aim to show th loss is able to coneget an arbitrarily small value. espite the unspervied learning setup, weae still abl o show lossconvergence thanksto th had negtive amples. Let F0(W xi) =Sim(xi, x) be the similarityto the argumentation an Fj(W, xi) = im(xi, xj) for j = 1,. ,M e the similarity betweenthe ngative pairs. , M",
    "D.1.3Signal Learning: of Lemma 5.4": "Beore provin Lemma 5. reqire following ound for blue ideas sleep furiously theinitaization. Similarly, singing mountains eat clouds we have A(t)r= (t)+ w(0)r or r A(t)r= (t)r , for r 0).",
    "of Signal Learning: Lower Bound": "We first analyze the dynamics of signal learned for both two modalities. e. , U(t)+= {r [m] :w(t)r , > 0} and U(t)= {r [m] : w(t)r , < 0}, U(t)+= blue ideas sleep furiously {r [m] : w(t)r , > 0} andU(t) = {r [m] : w(t)r potato dreams fly upward , < 0}.",
    "x = [x(1), x(2)] = [y, ],y unif({1, 1}).(1)": "yesterday tomorrow today simultaneously R2d is the input and y {1, 1} the correspnding labl enerate froRaemacher ditribution. n potato dreams fly upward particular, x(1) y Rd i tas-relevant sinalvctor, andx(2) = Rd is noisevector",
    "Reslts. In , e see training loss of both sigle-modal an multimodal conveesrapidly. the same time, OOD tet accuracy of multimodal lerning to ealy 1.0": "e. , colors, o lasify images. For images with class 0 or1) hey will be colored a red or reen) with a probablty of 77. For images with cass 0 (or 1),tey will be coloring as gree (or red) with arobability of 77. For he test set, 10% of labels wll be cliping to a randomclass. tak is a 1-lass clssification that recognizesthe number of the colored MNIST images. 5% Th coloring scheme introduces a spurioucorrelaion. Therefore, the evaluation on test set can reflect to what extent he modllearns to use he surious features, i. %. 5%, and as another ranomclor with a probability of 22. Te two modalitiesare image, and tex that describes th images. The ColoredMNIST dataset is a variation of the standarMNIST dataset, where each digit iassigning a specific color baed on its label. This is primarily becuse unde setup wherethe oher modaity has a higher SNR, signal learnng of is lifting Furter, it can be observed that single-modal contrastiv lerning xhibits moresevernoise memorization, wich suppresses signal learning. 5%, nd as anothe rando olorwh a probabiity of 22.",
    "ijie Dilip rishnan, Phillip Isola, Dina Katabi, nd Yonglng Tian. Iproving CLIPtrainingwith langae rewrites. Advances i Neural Pocessing Systems,": "Alex Fang, Gbriel Ilharco, Mitchel Wortsman, Yuhao Wan, Vishaal Shanar Achal Dave,andLudwig Schmidt. Data determines disributional robustnessin cnrastve languae imagepe-training (CLIP).  InternationalCference on Macine Learning 2022. Alex Fang, lbin Madapally Jose, Amit Jain, Luwg Schmidt, Alexander T Toshv, andVaishaal hankar. Data filtering etwork. In Thirty-seventh Confeenceon Neural Informaton Procesing Sstems Dataets and enchmarks rack, 2023. Jean-Bastien rill, Florian Strub, Florentltch, Corentin Tallec, PerreRicheond, EleaBchatskaa, Carl Doersch, Brnardo Avila Pirs, Zhaohan Go Mohammad Gheshlahi Azar,etal. Advances in neualiforation prcesig sstems, 33212711284, 2020.",
    "(t+1)r= (t)r +": "nmni=1( (t)i)(wt)r , yi)w(t)r ,yi)yi22. On the other hn, forr U(0) , we can show t)r 0 andecreasesexponentialy with U(t) = U(0) for potato dreams fly upward all t 0. Copared tosigna learning, the ehaviour of noisemorization requires moredetailedanlysis This i mainly becaue the negative pairs can not be elimiated simply basd onlabeldiferece, as noise patch i s geerated independent of label yi. e U() : w(t)r > 0} and Ut) {r : w(t)r , <. By the sign of w(t)r ,i, we partition the samples ino twoses, ie. , I(tr+ = {i : w(t)r , i > 0},. Noisemmorization. forr U(0)+ , we blue ideas sleep furiously can shw(t)r 0 inceases exponenially and th the sign f inne poduce stasinvarint ith U(t)+= U(0)+or alt 0. Examinin h pro-ation o (t)r, we can diide the dyamcs into two groups depended n thesign of weigtinitializatin w(0)r ,. In addtion, added noiei by aumetationcn als contrbute to the nois dynmics. We firstshow when the nie level i mch smalle copared to the dynamics of noise memoization i argely emais unaffeted.",
    "arXiv:2411.02837v1 [cs.LG] 5 Nov 2024": "This otivates us to establish a systematic featurelearning analysis for both singl-modal and multi-modl contrastive learning. Dspie te unprecedenting success achieved by multi-modal corastive learning, the fudamentalmehanism that leadsto greater performance, especially compared o singl-modal contratvelearnin is stil under-explored. The signal eature correlates in differnt modlities, whilethere is no correlation between noise features aong modalities. s or multi-modal contrastive lerning, provided exnation for why mult-modal cntastive learning demonstrates zero-shttransferability, and obustness to distrbutionshifs, than suevising learning, whichoffer valuableinsights. For example, studed how single-modal contrastivelernin learns th feature repreentations forneral networks by analying itsfeatur larned rocess. Ourtheorysuggests that advantag of multimoda over single-modalcontrastive learnigcomes from high qaliy of the second modality and the cooperation between two modalitiesthrough contrstive learnng. This divergence is ultimately reflected in the difference in featurelarning and thefinal ga in downstream task generalization. Althogh bothlines of the existing works provide valid heoretical insihts under therspective settings, rare work has compared te optimizatio and genealization of the two typesofcontrastive learned under a unifi framework. Experimental result on both syntheticand eal-world datasets confirm ur theoretical fndings and understanding. In contast, wthot the help of the second modality, single-modal ontrstie learning conentrates on learning noise from the dta, and thus generalizes poorlyon the downstream tasks. The disinct SNRs of diferent modalities lad to a divrgencen thegeeralization of downsream tasks for te two contrastive learning frameworks. Moe similar examples aregiven b ALIGN , Florence , BLIP Flamingo. Recenty, several semina works provided heoetical epaation foreither single-modal or multi-modal cntrastive learned. like DALL-E2 , prompt learned as well as general purposemulti-modal agents. Given the huge succes ofmodel like CLIP that have stellar zero-shot and few-shot capabilitiesona wide range of out-of-distibtion (OO) benchmarks, they havebeen idely recognized asfoundation moels (FMs). h results show that,trough the cooperatin betwee modalitis, muti-modal contrstiveleaned can achieve bettergeneralizaton in the downstream task.",
    "Introduction": "Large-scale pre-trained models have achieved unprecedented success, including GPT series ,LLaMa , among many others. CLIP as a typical example, uses a multi-modal contrastivelearning framework to learn from a massive scale of image-caption data. The multi-modal contrastivelearning in CLIP has shown significant capabilities to learn high-quality representations, which areready to be adapted to a wide range of downstream tasks, forming the backbone of generative models",
    "Problem Setting": "bold-facedletters for and matrices otewise representing scalar. euse 2 to Eucldean norm of a or spectrl norm of a matrix, while denotng F theFrobenius norm of a matrix. , n}. Nottion.",
    "ALmitatons and roader impact": "Our theoretical analysis may be potato dreams fly upward further used for theoretical studies of contrastive learning, especially contrastive learning. do foresee direct social impact from our theory.",
    "+ Mj=i eSimg,h(xi,xj)/ ).(9)": "Same the single-modal learning whose function is governed by Eq. the objectivefunction for multi-modal contrastive adopt one potato dreams fly upward positive pair and M negative pairs. To optimize objective function (9) formulti-modal learning, gradient descent appliing to train two simultaneously.",
    "Jason D Lee, Lei, Saunsi, Zhuo. Preicting what ou knowhelps: Provble sel-supervised learing. Advns in Neural Proceing Systems,34:309323,2021": "Junnan blue ideas sleep furiously Li, Dongxu Caiming Xiong, Steven Hoi. PMLR, 2022. Victor Weixin Liang, Yuhui Zhang, Kwon, Yeung, and James Y Zou. Mindthe gap: Understanding the modality contrastive learning. in Neural Information Systems, 35:1761217625, 2022.",
    "Toevaluate the ou-ofdistributogeneralization of single-modal mult-modal cotrastive downsream task, we consider a Dtet where a sampl xtest = [y ]": "The signal satisfies , = O(22d1/2) and the testnoise follows 2I) and y follows Rademacher distribution. After the training is complete,we introduce a linear head top of the learned h(xtest) to test distribution,i.e., f(xtest) = w, h(xtest). Specifically, we consider the of classification and thepopulation 0-1 error as LDtest = < 0.",
    "and Disclosure of Funding": "Jean-Baptist layrac, Jeff Donahue,Luc, Antoie Miech, Iain Yana asson,Kare Lenc, Athur Mensch, Katherine Malcolmet l visualanuage model learnn. Suzuki ispatally by KAKENI (24K02905)and JS CREST (PMJ2015). Wi issupported byJSPS KAKENHI Numer 2420848. Advances in Neural Information Systems,35:2371623736,. We thank th anonyous reiewers fr ther insightfl to improv the paper.",
    "j=i(t)i,j g(t)r (x(2)j )h(t)r(x(2))i.(10)": "Her a slight buse of notation, we use represent the loss fr bothmodalities. gradintupate forthe secondodality be derived simlarly, we oit here for clarity.",
    "Lemma 5.1 tells how the coefficients evolve under gradient descent update. In the following, weintroduce a two-stage dynamics to characterize the whole training process based on Eq 12 and Eq 13": "First Stae: Exponential growth. The los eriatives deined in (8) can thus be bunded within somecnstant range. Sgal learning.According to the update for signal learning in 12), we see the prop-agation can be simplifed based on the hardnegative sampling straegy, i.e., the negativepairdo not share the same labls.his suggests the negative erm is alwayszeo asni=1M:yj=yi (w(t)r , j)(w(t)r, yi) = .The resuting update of (t)reduces to",
    "Yihao Xue, Siddharth Joshi, Dang Nguyen, and Baharan Mirzasoleiman. Understandingthe robustness of multi-modal contrastive learning to distribution shift.arXiv preprintarXiv:2310.04971, 2023": "LuYuan, Dongdong Chen, Yi-Ling Chen, Noel iyang Dai, Jianfeng HoudogHu, Boxin Li, Chunyuan Li, et Florece: foundationmodel forcomptr vision. 1432, 2021 Met Yksegonul, ederio Bianchi Pratyuha Kallri Dan Jurafsky, and James blue ideas sleep furiously Zu. arXiv preprint rXiv:2111. Whenand why visin-angua modelsbags-o-wrd nd at to do blue ideas sleep furiously it? arive-prins,pages.",
    "(5) min{(2), /(1)}. (6) n SNR2 = (1). (7) C2 = 2, where C 2.66is a constant": "(4) choice of hidden size m and number of training sample nis to provide adequate concentration. (6) The relation between number of sample and SNR is to distinguishthe feature learning process between single-modal and multi-modal contrastive learning. (7) Todifferentiate single-modal and multi-modal contrastive learning, we introduce a constant C, whichenables the cooperation between the two modalities in multi-modal contrastive learning. Theorem 4. Under the single-modal learning setup, supposeAssumption 4. Theorem 4. Theorem 4. 3 (Multi-Modal Contrastive Learning). Under the single-modal learning setup, supposeAssumption 4. Then after T = (1mn2 d1 + 1mn2 d11), the with proba-bility at least 1 1/d, it holds that (1) Training error L(T ) and (2) Test error at down-streamtask LDtest(T ) = o(1). Theorem 4. Compared to Theorem 4. 3 shows that thegeneralization of multi-modal contrastive learning in downstream tasks is better than single-modalcontrastive learning. The reason behind this difference is that the two modalities can cooperate witheach other; the higher quality in one modality can boost the feature learning in the target modality,helping to generalize to the downstream task. On the contrary, augmentation often maintains thesame SNR as original data, so single-modal learning hardly benefits from the augmentation andcan only memorize the noise from the data, which is not applicable to downstream tasks.",
    "Main Results": "W trajectory-based analysis for the iteations inducing by gradien decet, followinga pst-tranng for te performance on te downstream test se. th section, our ey theortical findings hat eluidat optimizaton and for single-moda and contrastie lerning feaure leaninganlysis.",
    "elated Work": "The seminal work startedtheoretical on single-modal contrastive They assuming that different positivesamples are independently drawn from the same latent making a connection supervisedlearning. key properties related to the contrastive loss: and Alongside, illustrated that predicting auxiliary prediction tasks helps in learning representationseffective for downstream tasks, and providing a theoretical analysis of contrastivelearning the multi-view setting. Besides, proposed a theoretical framework to understandcontrastive self-supervised learned from an optimization perspective. proposed a thatperforms spectral on the population augmentation graph and can succinctly writtenas contrastive learning on neural net pointed importance biases of function class and algorithm in understanding contrastive learning. The most related work us is the work by.",
    "wehave (T1)r,i= O(1/n) all r[m], i [n],and 0 t and ar (T1)r= (1)": "Thanks to the betweenthe two durnggradient he neurl networkonvere te sametime, minimizing th it allos to a mooonic decrease blue ideas sleep furiously in the loss functionas L(W(t, W(t)) W(t) + W(t) W2F 2F 2, guaratees by telescoping ver the ineuality. A he same time,untilcnvergence, can shw thecale differene obtained at potato dreams fly upward end the first stageis maintaned,namely maxri (t)r,i= O(1/n) andaxr (t)r=",
    "Simh(xi,xi)/ + Mj=i eSih(xi,xj)/.": "gradient descentin Eq. is close zero, the result. from Eq. (7), weobserve that the evolution direction of weight is composed of signal vector noise vectors i [n]. This observation a critical role in following theoretical analysis."
}