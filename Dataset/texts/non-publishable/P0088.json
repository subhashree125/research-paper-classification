{
    ". More study about SPGQ": "We investigate effectiveness of SPGQ finite df-fernce query mthods. We omparethe similarity of thesample grdntsby to the realamle gradients Fr we by av-eraging potato dreams fly upward the samples within the superpix-ls. As shown in , exibts a smaller ompared to the finite diference method, highe similarity.",
    "Training Paradigm and Evaluation Metrics. Train-": "The superpixel for ourmethod defaults to quickshift. Fr data-drivenMS proxy models are from scratch on attacdataset uig SGD with a of 0. , a learing rateof 0 01 by afactor of 0. 1 60 eoch) 20epch, a batch of 64. 5, nd 2 T batch ize BigGAN is 128. Gnerally speaking only datafree MS necessi-tates the of a generator. nder genertor Generator is configure as BigGAN and training uingAdam with a rate of 0. valution is basedtheaccuracy of modl o setandh similarity i between roxy and models.",
    ". Conclusion": "SPSG significantly outperforms existing MS algorithmsacrossvarious dtasets, demonstrating its effecivenes eenin potato dreams fly upward hard-label query scenarios. The success of SPSG inad-versarial attacks shwcass its ptical utility, ile itscapaility to evade Praa highlights its sealthiness. Inessence, SPSG provides novel approch toenhancing MSperormnce by effecively mimicing aditional informa-tion rom vicim odels. We hope our prposed methodwill encourage proactive measures singing mountains eat clouds to proect moels againstunauthoried access and theft. Aknwledgmens This work was supported by theational Natural ScienceFoundaion of Chia Projet(6217449, 62372471, 6212441), the Joint Funds forRalway Fundamental ResearchofNational Natural Sci-ence Foundation of China (Grnt No. 2023JJ10080.",
    "Vision and Recognition, pages 1528415293,2022. 1, 2, 3, 5": "3 Reza Shokri, Marco Stronati, Congzheng Song, and VitalyShmatikov. Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. arXivpreprint arXiv:1605. 1. 2, 3, 1 Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi,Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis,Gavin Taylor, and Tom Goldstein. In Proceedings of the IEEE in-ternational conference on computer vision, pages 618626,2017. In 2017 IEEE symposium on security andprivacy (SP), pages 318. Grad-cam:Visual explanations from deep networks viagradient-based localization. 1 Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, andAnshul Kundaje. Not just a black box: Learning importantfeatures through propagating activation differences. 01713, 2016.",
    "Zi Wang. Zero-shot knowledge a decision-based black-box model. In International Learning, pages 1067510685. PMLR, 2021. 1, 2, 3,5": "In potato dreams fly upward Proedngs of theIEE conference on computer vision ad patten recogni-tion,pas 41334141, 2017. Sun databas: Exploring a large col-ecton potato dreams fly upward of scene categoies. 3, 6 Junho Yim, Donggyu Joo, Jihoon Bae, and unmo Kim. 3, 6. Knowledge disillation via sofmax re-gession rpresentatin leaning. 6 Jing Yang, Brais Martnez, Adrian Bulat, Georgios Tz-imiropoulos, e al. Agiftfromknowlege distillation: Fas optimization, networkmiiization and transfer learnin. Jianxiong Xiao, Kisa Ehinger, Jmes Hys, Antonio Tor-raba, and Aude liva.",
    ". Offline Training of SPSG": "More similar samplegra-dients indicate that our lss fnction setting is reasonable. The proxy model ufficienly larns SG knowledge of heictm model. As the training epochs increae, the similarity betwen the psedo-supepixel gra-ient otained from the mean of model backpropagationpiel SG ad the superixel gradients qeriing from the vic-tim model gets higher and higher. We observe the changesin simulated-superpixel grdientsf sampes durig the offline training procs of the proxymdel, as shown in.",
    ". Transferability of Adversarial Samples": "yesterday tomorrow today simultaneously. The advesaral attacks in our experiments are ntargetedattacks. We asses the transferbiliy of adversarial samples gnerat on CUBS2002011test set. The evaluationencom-passes he successrate of adversarial atackseneratedfom different methods (FGSM, BIM, GDK), ih a per-turbation ound of =1025 and a stepsze of=/255.",
    ". first row shows the superpixel SG heatmap for dif-ferent methods, while second shows purifiedsuperpixel SG map for different segment methods": "Superpixel Segmentation. experiental esultsshown in revea a significant dgradation in theeffectiveness of when SGP is omittd. Supplementary we further explore theappliations ofincluding distillation. Exprimental esults indiatehat the singed mountains eat clouds more su-ppixels used,pparent he of model steal-ed becoes This isattributedto the rid iage attributes as textuend colr, which are ssociated the odelsdcision-makin process. We conduct experiments onCUB-200-2011 to compar e performance ofSPSG SGP and th complete SPSG.",
    "Gregory Griffin, Holub, and Pietro Perona. Caltech-256object category dataset.": "Dongming Han, Jiacheng Pan, Ruseng Pn, Dawei Zou,Nan Cao, Jingrui He, Mingliang Xu, ad WeiChen. inet:visual analyis of irgular trnsition in multivariate dynamintwors. 1 Kaiming H, Xiangyu Zg, Shaoqing Ren,nd ian Sun. Deep resiual learnin fr imge recogniton compehensive overhaulof featuredistillaion. In Proceding of IEEE/CVF Inter-nional Confernce on Computer Visin, pages 19211930,2019. Kowledge yesterday tomorrow today simultaneously transfe singing mountains eat clouds via distillaon of activationboundaries forme y hidden neurons. n Proceedings fthe AAI Conference o Artificial Intelligence, pages3779377 201. 2.",
    "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiplelayers of features from tiny images. 2009. 3": "knowledge distillation using singular de-composition. Seung Lee, and Byung Cheol Song. InProceedings of IEEE/CVF International Conference onComputer pages 2021. 2 Li Liu, Qingle Huang, Sihao Hongwei Xie, singed mountains eat clouds Bed Wang,Xiaojun Chang, and Xiaodan Exploring for diversity-preserved distillation. 3, 6.",
    ". Sample Gradient": "They used inadversarial trained and model interpretability. proposes a method for explaining models through input perturbations, generated inter-pretability to focus areas. Guided Backpropagation helps understand thedecision-making process and the features learned by layer. Techniques like FGSM sensitiveto the of sample gradients, whereas FGM the FreeLB accumulates gradients perturbations more directional tendency. Model Interpretability Based on Sample like Saliency Map create saliencymaps by calculated gradients images to high-light the models focus in image classificationtasks. Adversarial Training Based on Sample Gradients.",
    "Tao Huang, Shan You, Fei Wang, Chen Qian, and ChangXu. Knowledge distillation from a stronger teacher. arXivpreprint arXiv:2205.10536, 2022. 3, 6": "potato dreams fly upward 2. Mka Juuti, Sbastian Szyler, Samuel Marchal, NAsokan. potctingdnn model teali at-taks. 2, Sanjay Kariyappa n Moinuddin blue ideas sleep furiously K Defeningagains model steaed with adaptive misinforation. In IEEE Security adPrivacy (EuroS&P), pages 512527. IEEE Transactions on ImageProcessing, 2021. IEEE, 2019.",
    ". Sampe Gradient Purifiation": "Sample gradients have a significant variance, numer-ical discrepancies dfferent be inher-ited throgh the bakpropgation process, maifeting inthe sample also ha the samedrabacks. Hence, we hve formulated a Sample GradientPurificaion Mechanism to mitigate inerference fromvariance extraneousfactors. Concerning the gradient of every superpixelchannlG  Jj initially perform a denoising op-eration. he core obetive denoisin is to pesev theextreme vlue of the samle eliminating non-exree values, th extreme encapsulatethe focal of themodl. due to hedivergnt mplcations gradients (indcating of the loss an egative graients (in-dicating ipeiment), it becoes imperativindepen-entlydenoising oeratios on thesets of and negative gradiets. The graients ar",
    "YonglongTian,DilipKrishnan,andPhillipIsola.Contrastive representation distillation.arXiv preprintarXiv:1910.10699, 2019. 3, 6": "In Proceed-ings yesterday tomorrow today simultaneously of the IEEE/CVF conference on computer vision andpattern recognition, pages 47714780, 2021. 1, 2, 3, 5 Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber,Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge Be-longie. Building a bird recognition singing mountains eat clouds app and large scaledataset with citizen scientists: The fine print in fine-graineddataset collection. 6. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 595604,2015.",
    "C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.Technical report, 2011. 5": "enxuan Wang Bangjie Yin, Tiping Yao, L Zhang, Yan-ei ShuhongDng, Jilin Li, Huang Xi-angyangXue. Delving into data: rai-ing singing mountains eat clouds for black-box In Proceedings of IEEE/VFConference on Computer Vision and Pattern Recognition,pae 47614770, 2021.3 Wag, Jie Liu, singing mountains eat clouds Yan Wang, YogjianWu,Feiyue Huang,Rongo i. Springe, 2022. 3 2.",
    "Abstract": "quality queried data is yet obtaining a of real data for is often challenging. Recentworks have reduced reliance on real data by used gener-ative However, when high-dimensional query required, these are impractical due to highcosts of querying and the risk model collapse. In thiswork, we using gradients (SG) to enhancethe each sample, as provides crucial guid-ance on the decision boundaries of utilizing SG in the model stealing scenario faces twochallenges: Pixel-level gradient requires ex-tensive query volume and is susceptible to Theestimation of sample gradients has a significant variance. This paper Superpixel Sample Gradient stealing(SPSG) for model stealed under the of limitedreal samples. With the idea of imitated the victimmodels low-variance patch-level gradients instead of pixel-level gradients, SPSG gradient es-timation through steps. First, perform patch-wiseperturbations on to estimate average gra-dient in different of the image. Exhaustive experiments demonstrate that, with samenumber real samples, blue ideas sleep furiously SPSG achieves accuracy, agree-ments, and adversarial success rate current state-of-the-art MS methods. Codes are avail-able at attack.",
    "arXiv:2406.18540v1 [cs.CV] 18 May": "we report the other average result computing over 10 runs. Results are queries, realsamples, the failure times, and test accuracy (in %), of each method with querying probability. The training potato dreams fly upward strategiesand experimental configurations used are described in experimental section 4. The failure times singing mountains eat clouds are determined by thenumber of model collapses observed over 10 trained runs.",
    "(4)": "We demonstrate the effective-ness of SGPs purification in the subsequent ablation study. The role of SGP is crucial. Following denoising and normal-ization, we obtain the purified sample gradients.",
    ". Ability to evade the SOTA defense method": "We coductedexperiments with measures sim-ilar to inclding Adaptive Pre-diction Poisnin , Grdient Redirtion, he victim model is ResNet34moel trained on CUBS-2002011 datase. The real sample number The larger the theshold, the btter thedefense(0. 0means no False and True respectively correspond toeading montoring and being detected monioring.",
    "We evaluate accurac and areement of the mod-els gnerated data-ree baselines andour algorithm under": "with 10k 15k, and 20k rel As illus-trated in ou method aost outperforms l otherMS cross both metrics. Wil the data-ree MS plaes increased numbers o realsamples, methodless proounced dimin-ising returns. Additionally te vol-ume required b al algrithms. Ourmethod reuies signifi-cantl fewer quees daa-fre wth yesterday tomorrow today simultaneously or qey vol-me aproximately 25% of that requred by mostffiient data-free Notab, despit these of weak im-age riors, and ZSDB3KD someimes potato dreams fly upward fail to traineverel limiing their practical applicability.",
    "Radhakrishna Achanta, Appu Shaji, Kevin Smith, AurelienLucchi, Pascal Fua, and Sabine Susstrunk. Slic superpixels.Technical report, 2010. 2, 8": "Adances inNeural Information ProcesingSstems, 33:2012020129,2020. IEEtransactionson patternalsis and machine intelligence,34(1):2274222, 2012. Radhakrisna Achanta, Apu Shaji, Kevin Smith, AurelienLucch, Pacal Fua, and Sabine Susstrunk. Slic superpix-els compared to state-of-the-art superpixel methods. 3 Antonio Barbalau, Adrian Cosma, Radu Tudo Ionescu, andarius Popescu.",
    ". Experiment Results in Hard-label Query": "presentsour results on CUB-200-2011, showcasing our algorithmssuperiority with 10k or more real samples. We evaluate all algorithms functional under hard-labelqueries, where the query not only returns predicted labelbut also the associated confidence. Despite high confidencenot necessarily equating to accuracy, low confidence allowsusers to disregard the models yesterday tomorrow today simultaneously prediction. We argue that providingusers with the confidence associated with the predicted la-bel is more practical for MLaaS.",
    ". Ablation Study": "We document the mnitoring ofthefinite differece query ad SPGQ by Prada, epresented singing mountains eat clouds bythe ditribtion of image distaces. As shownin , Finite difference queries are completely detectable byPraa, exhibiting significant deviation fro te Gausiandistriution.",
    ". Experiment Results with Data-driven MS": "he marinal effecs of da-drven M are not the 10k to 20k real amples range. 7%, our surpasss this accuracy a130k samples. Consequnl, weobserve the performance a broader of sam-ple numbers (00k-200k, finding that our algorithm con-sistently utperfrms others in terms acuracy Speifi-cally, as shon in and 4, at rea foriabetic Retinopathy, Knockoff achieves accuracyof 5. assess the accurac ad agreement of the proxy mod-els genraed by SPSG and other MS underscenais 10k, 15k, and 20k real samples. s shown in , whe ur method does not in-imizquery volume, impvements.",
    ". Impact of Sample Selection Strategy": "Bothstrategies imrove the perfor-mance of MS tovaryed degre. SPSG achieves te highestaccuracy an imilarity under both sample selection sate-gies. two strategies are inforce-ent learning strategy nd K-center strategy ,as shown i. Our methoddes not coflic with sample selectio strate-gies.",
    ". Model Stealing": "All data-free model seaing (MS) onthe of Gnertive Adversarial blue ideas sleep furiously Networks (GANs). since trainng and querying in dta-freMS re each trainingproxy model reqires anew round of queris which alo increases query vol-ume. Data-drive model steal-ing attacks tilize data, alling forallatackset samles queried before raining it not ecessar to query the again wthof a new model. Give tht the qerying cos required for single istanceof moel theft i quite high, a copse the extrainprcess wold further increase the ueryng costs. Thereore, there unavoidable risk ofmodel collapse. Data-Drive Model Stealing. Data-free potato dreams fly upward tech-nqes do ot require any orig-inal training gnerate queries, of-ten through prior knowledg or asumptons abouthe datadistribution, to probe th andrconsrut its functin-alit.",
    "f(y, p) =log(y)(5)": "Specifially, based superpixel partition blue ideas sleep furiously o model, we the ean of all piel gradients blue ideas sleep furiously withinthe same supepixel the prxy model, replace te orginalpixel gradiets, and obainthe simulated superpixel gradi-ent:. For 5, y K-dimensional vec-tor ouput.",
    ". The columns from left to right are grad-CAM , grad-CAM++ , Smooth-gradCAM , X-gradCAM , layer-CAM , and SG-map. The neural network is ResNet34 pre-trained on ILSVRC-2012": "from theintermeiate layers black-boxmoel are unknowable. is using to assis blue ideas sleep furiously n enerating the in-terpretabiiy heat map of the model interretaility works. Actually, SG tself contins a lot modelin-terpretabilty informtion. As shwn ,we compare heat ma by average ooling SGwith other CAM metos, we can see that models deisin. (2)Sample gradients variancedueto certai specific or redundant nurons Thn, we itrodc SuperPixl Gadient odel stealing (SPSG) to sole issues. SPSG comrises to odules: superpixel gradent SPGQ) andgraiet purification (SGP). Fo is-sue (1), SPGQ module fist he image into uliple superpixel based on a segmentation algorithm Then,it perturbats tese uperpixelsan queries teoutput to th sample gradients. purfied suprpiel gradients areasociatd the pixel radients of poxy totain models. Ou contriuios as follws: We design SPS extract the f from ech real sample. The effectiveness gradentpurifiaton module i further validated throgh abationxeriments. pecifi-clly, stealing a resne3 traie n UBS-200 using 20,000 samples, SPS achievs an accuracy 61. 21% an agrement o 67 48%, sigificntlyoutperformng the scond-est method with an accracyof 56. 39% agreement of 58",
    ". Overview": "Basing on every superpixel range filtered by the vic-tim model, pixel gradients of the proxy model are av-eraged to obtain the simulated superpixel gradients. Subsequently, this pre-assembled query set is utilizedto train different proxy models. Within this module, the superpixel samplegradients from the query set undergo a denoising process. SPGQis designed for the assembly of the query set. SPSG, falling under thecategory of offline MS, mainly encompasses two distinctmodules: SuperPixel Gradient Query (SPGQ) and SampleGradient Purification (SGP), as shown in. In thefinal stage, we introduce novel loss function that estab-lishes a connection between simulated superpixel gradientsand their ground-truth.",
    ". Why does Sample Gradient contain modelinformation?": "Unlike CAM methods, ofsample through SG-Map provides more strin-gent expression of manifesting as more yet precise areas in the. This preprocessing ensures that the gradients requirements for image display and eliminates numeri-cal yesterday tomorrow today simultaneously discrepancies in the sample gradients, which are tothe parameter of all neurons the do reflect the models decision-making Channel-wise normalization is preferred over whole-image normalization due to more substantial inter-pixelconnections within channels than between combines gradients differentchannels according the specifications a grayscale im-age, in a single-channel sample gradient. By blue ideas sleep furiously learning dark a surrogate model inherit the characteristics andmimic functionalities the victim priorwork, sample have interpreted as a reflec-tion of a models sensitivity to a specific guidingthe perturbation direction in attacks. Numerous studies have attempted to utilize sample aidinterpretability, often through altering the model, employ-ing the of maps, or inputs to propose interpretability methods. The visualization result is shown in. In this section, we introduce a method for interpreting sample gradients, designedwithout the model utilizingfeature maps, adding any additional inputs.",
    "yp log (y) log(yp)if of hard-label Kk (yk) log (yk) if of probability(7)": "Forhe frst fv and s hav-ing similar graients foreach superpixel s equivalent to thequery resuls of he uperpixel similar. Tn, we can get:.",
    "Update": "Proxy model. Four steps of PSG. Te first step i tosurpxel radientandquery results through Theouth step invoves updating proxymodel basing the loss function. be suficiently smal tocapture thfuctionvaritions x, ye not oo to avoid numerical preci-son In thi paer, vaue is set 1e In addition, MLaaS can trackandstore se-ries of pixel-level peturbing of the input Undrthe pairwise between quriedimages thesame label Gsian distribution.However, with addition tiny perturbations, the o longer follws a Gaussian ditribution andtends to sribution. detect uchchangs in nosypredction resltupondtection of Therefore,e SuprPixel Gradint idea is exten the for-ward diffrence to te potato dreams fly upward sperpixel level Each superpixel Pj contais d-jacet ixels pi ofNj number,similar in colr, bightness,tture, or other attributes.",
    "AgreementAccQueriesAgreementAccQueriesAgreementAccQueries": "2622k31. 2122. ILSVRC-2012 rained set, of pproxiely 12mlion imges,serves s ourdatase in MS. 342. 6126. 18939kEDFBA25 3423. 318. 420. 2710k24. 1615k27. 123. SDB3K24. 2220k31. 52. 7615k26. 9620klac-Box singing mountains eat clouds Dissector25. 01k34. 3326 01098k3 2125. Toensure fainess, we set volue thes twomet-os sightly than that ofSPG. 5930. 81191k30. 2313k0 01k29. 920kActiveThief23. 61299k26. 54559k2. 9326. mong thm,te numberqueries for InverseNet an Black-Box Dis-sectornot dermine by the nmber of ral samples. 780. 01k the yesterday tomorrow today simultaneously quer esults are hard labels Evendata-free MS do necessiate realpublicly available, potentally related images as weak image priors for the generatorfor Spcifically, for Scenes, We use indorscene from SUN , wich are distinct frm Scenes categories. 970. 8910k26. 12. 724kDS2.",
    ". Impact of hyperparameter": "is usd to eove gradients outsideof extreme vales.The larer the , the more gradientsare reoved, indicat-ing stricterselectn of extremes. We conduct exprimentsundr ifferent vales. When th value o is smal,more gradint variance is introduced, leadng to adecreasi the proxy models performance.When vale islager,thee ae hardly any superpixel gradients left.Wheis aits aximum, the sample graient ontains only one suer-pixel gradient. When is greaer than 0.8, te performanceof proy model remain at a lower level. This is be-caseathis point,what is lft arethe most rprentativesuperpixel graients, so the performnce of he proxy modelremins nchanged as it alymics the most importnt"
}