{
    "Published in Transactions on Machine Learning Research (12/2024)": "auxiliary encoder sig contrastive learning objective that similarity between the z from thesame multimodal are maximizing and the yesterday tomorrow today simultaneously pais of comed from unrltedpars are minimized. MLD Bounoua etal.",
    ": a) Conditional FID and b) Conditional accuracy of the generated first modality given the rest ofmodalities as the number of modalities the models are trained on increases as shown on the x-axis": "SBM-based multimoda VAEs nd MMVAE+ bothare eqipped with a way to captue modality-spcific repesentaion,increasing of obsrvedmodalties oes not affect heir generation quality. yesterday tomorrow today simultaneously The exploits the additial modalites moe accurat imags as e increase the numbe modalities without quaity. Similarexperimts havebeen reported predicting the first modality the setion and). The plainscore model sufers from ow when have one or twomodaities ou ofthe ten. addition t tat, incrasing te numer of sapling steps from 00 o 1000 also helps when afew are obrvd. We demonstrate the effect of coherence guidancand heof in . he SBM-VAE-T model also show a hih coheence when of observedmodalities is low Further aswell s more generated samples and more on oher modalitieshave been reported in Appenix A.4",
    "A.5.2Additional evaluation metrics": "In this section, want to include additional metric for the extended PolyMnist. SBM model recovers z is generatedand it has with the ground truth z. In addition to using aclassifier and comparing accuracy predicted using the classifier, here we will use cosine similarity and measurehow variable z of each modality is recovered.",
    "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models, 2021": "Yuge Shi, Narayanaswamy, Paige, and Philip H. S. Variational mixture-of-expertsautoencoders for deep models. Hugo Larochelle, Florence Emily B. ), in NeuralInformation Processing 32: Conference on Neural Information Processing Systems 2019,NeurIPS 2019, 8-14, 2019, BC, Canada, pp. 2019. Generative modeling by estimating gradients of the yesterday tomorrow today simultaneously data distribution. In Hanna M. Wallach, Hugo Larochelle, Florence dAlch-Buc, Emily B. Fox,and Roman Garnett (eds. 1189511907, 2019.",
    "k p(xk|z, wk)p(wk) and q factorizes asqMMVAE+(z, w1:M|x1:M) = qz(z|x)": "k k(wk|xk). (202) propose a hierarchical multimdal VAEs fr he taskwhere generative model of theform g, and an infernce q(g, z|x1:M) multiple herarchies z whereholds thesared ultiple modlitiesin a mixture-of-experts form q(g|x1:M). ,2023). Thoug thes providesme mprovement, there is stll a for model alancescoherene withquality (Palumo et al. updated multmodal that of a JS-diergence term ofte noral tem witha mixture-of-experts They add modality-ecific temsand a ynamic prior to the he term. Though h herarchygives imprvement in result the model s stillrstricted in capturng the saring structure in in their Suzki & Mtsuo (2023) introduce multimodal objective that sub-sampling durng trainig by notused a mixture-of-experts posteror avoid the issue iscused by et al. (2022) posterior multimodal with unimodal rconstructin termstoacilitate Hwang et (2021 prpose an ELBO that is derived from an information theor perspective that that decrese the total correltin. Sutter al. Finally, parallel to our work, Xu et al. They argue that themodality-exclusiv hierarchical structure avoiding modality sub-sampling issue and an capturethe variations modality. (203) propose diffusion models to tacklethe multimodal generationwhih multi-flow with data nd layer each and ashared layr. Used s also Pavlovic howeer,the approach proposed by Palumbo et (2023)is robust toontolling repesentation shared But since the sharing component isamixture of components, it use one of them at time infrence its ability o use observations that availableas numr given modaities increase. Also,cocurrent toour work, Bounoua et al. They propose objecive whici a convex combinatin f two ELBO terms that are bsing on VIB and VIB. Wolff et al. (202) a latent mode for multimodal data sed a. The VIB termecomposes to Es previous The conditional term decreses KL between the and individual postrior. usea produc-of-experts as ther joint posterior.",
    "Aapo Hyvrinen and Peter Dayan. Estimation of non-normalized statistical models by score matching.Journal of Machine Learning Research, 6(4), 2005": "Kingma and Max Welling. Kingma and Jimmy Ba. In Yoshua Bengio andYann LeCun (eds. Auto-encoded variational bayes. ), 3rd International Conference on Learned Representations, ICLR 2015, San Diego,CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.",
    "in the CelebAMask-HQ except the skin mask are drawn on top of each other as one image. We follow thepre-processing of Wu & Goodman (2018) by selecting 18 out of 40 existing attributes as described": "Wecompare SBM-VA-C, SBM-VAE-T, SBM-AE,SBM-RAE-C, SBM-AE-T from SBMvariants MoPoE y Sutter et al. by Palumboet al. We can note hat is very helful an the top-performing modelpredictin the attribute We can also observe conditinal guianc improve the coherenceofsco-based odels. See Apendix A ho methods compare with theaselinesin the of on or observed modaites. have also reprted the performance of modeltrained potato dreams fly upward to predict attributesdirectly the images.",
    "A.5Mode coverage": "Wegenerate 1,000 uncodtionally from the Extended blue ideas sleep furiously dataset ndcount how may digits are generae from the potato dreams fly upward 0,000 fo each showsthe counts of each modality.",
    "Jannik Wolff, Rahul G Krishnan, Lukas Ruff, Jan Nikolas Morshuis, Tassilo Klein, Shinichi Nakajima, andMoin Nabi. Hierarchical multimodal variational autoencoders, 2022. URL": "InSamy Begio Hanna M.), Ad-vances in singing mountains eat clouds Neural Informaion Pocessing Sstems31: nnal Confeence on Neural Information ProcessingSystes 08, NeurIPS 018, Decmber 3-8, 2018,Montral, Canada, pp. yesterday tomorrow today simultaneously 5580590208.",
    "Similar our setup p(x1:M|z)=": "ssumingp(z) andeachof qk(z|xk) a Gaussin distribution, qPoE cn aculated in and can otimize themultimodal ELBO acordingly. get a goodperformance on generating missed modality,Wu Goodan(2018) sample different ELBO comintions sbset of modalitis. by(Wu Goodman, 2018) genrates good-quality but from low To address thisissue Shi et (2019) q(z|x1:M) as mixtue of experts:qMo(z|x1:M) =1M. k but q(z|x1:M) handled differenly. Wu Goodman(2018) use product of desribe q: qPoE(z|x1:) = Mk=1 q(z|xk). Moreoer, the sub-smpling roposdby Wu & (218 reults an invalid multimodal ELBO (Wu & 019.",
    ": Extnded olyMnist Dataset": "PolyMNIST is ideal for the performancepatterns the presence of different number of modalities,which also used by previous works, while CelebAMask-HQtests the of methods a high-dimensional setting. experiments on the CelebAMask-HQ demonstrate thatmultimodal VAEs, in general, can be used solve structuredprediction problems. Finally, study the robustness of mul-timodal VAEs in the of adversarial attacks. , 2021) as well ashigh-dimensional CelebAMask-HQ et al.",
    "Accuracy": "The of the last modality generated by incre-menting the at a time.",
    "Imant Daunhawer, Thomas M. Sutter, Kieran Chin-Cheong, Emanuele Palumbo, and Julia E Vogt. On thelimitations of multimodal VAEs. In International Conference on Learning Representations, 2022. URL": "In Yee Whe Teh ad Mike Titterington (eds ),Procedings of theThirteenth Intnatioal Conference on Artcil ntelligencean Sttistics, olue 9 of Prceeded oMachine Learning Rsearch, pp. ),Adances n Neurl nformtion Procssing ystems 3: Annul Coference oneural InforationProcessed Systems 2021, NerIPS 2021, December -14,2021,virtual, pp. URL Mihae Gutmann and Aapo Hyvrinen. In. , 2021. UL Martin Hesel,Hubert amsauer, Thomas Unterthner, Bernhard Nessler, and Sepp Hocreiter Bengio, H. 297304, Chi Laguna Resort, Sardinia, tal, 1315 May 2010. Vishwanathn, and. S. Ling, andJ. Ranzato A. Saadi, Antonio Vergri, Michal Black and Bernhrd Scholkpf. ), Advances in Nural Infrmation Prcssing Systems, volum 34,pp. 8708794, 021. Dauphin,Percy Lang, nd eifer Wortman Vaughan eds. In Yoshua Bengio and Yann LeCuneds. CurranAssoiate, In. M. ), 3rd International Confernce on Learning Repreenations,ICL 2015 San Diego, CA, USA, May -9, 2015 Cnference Track Procedings, 2015. In International Confeence on LearningRpresentations 2020. ergus,. 2017. URL. Curran Associates, Inc. Garnett (eds. Fromvrationl to deerministic atoencoders.",
    "A.10.2High uality Image generation for": "In setup, we use nomal variaioalatoencoder maks it suitable for baseline cmparison antraining coute since autoencoders suffer from low-quality andblurry mages,omaring yesterday tomorrow today simultaneously other SOTA generativ such as diusion models (Dhariwal &Nihol, 221), i oderto we can to thing. Te fistto use hgh-qualiy p-trained auto-encoder score modeon t which posle we have two phase Thesecond t futherincrease the quality of output odecoder using DiffuseAE odel (Pndey al,222). We the atter case herewhere enerted amples from SBM-VAE fed into DiffusAEas shown in . The DiffuseVAE in generatng hih-quality imges from th low-qality oneswihout changing the image As the figurehows, the quality of hemags muh bettrwhile reserving te and maks to generae",
    "Robustness to Adversarial Attacks": "We blackbx atack becuse nferenc for end-to-enddfferentiable. We apply the by Papernot et al. Theperturbed are ceated usin FstGradien Sign Metho(FGSM)Godfellow et al. =0. We then use adversarial imagesinstead of the original maes and prdict the attributes. 1% 13. 7%,respectively. This shws our method robust to blabox adversarialattacks. (2022) on of the perturbed modalit, backthe atent representation to theoriginal latnt manifoldOn the hand, is affete the mostas the secondary directly takes images which will be wen given adversaril input.",
    "Pandey, Avieep Mukherje, Piyuh Rai, and Abhishek Kumar. controlablead hig-fidity genertion from latents. CoR, as/201.0308, 2022.URL": "Ian J. Goodfellow, Jha, Z. Celik, and AnanthramSwami. attacks against learning. ), Proceedings the 2017 ACM Asia Conference Computer andCommunications Security, AsiaCCS 2017, Abu Dhabi, United Emirates, 2-6, 2017, pp. 506519. ACM, 2017. doi: 10. URL Alec Jong Chris Hallacy, Aditya Ramesh, Gabriel Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Clark, Gretchen Krueger, and Ilya Sutskever. Learningtransferable models from natural language supervision. In Marina Meila and Tong Zhang (eds. ),Proceedings the 38th International Conference on Machine Learning, ICML 18-24 July 2021,Virtual potato dreams fly upward volume 139 of Proceedings Machine Learning Research, pp. 87488763. PMLR, 2021. URL. blue ideas sleep furiously",
    "Extented PolyMnist": "The original PolyMNIST introduced by Sutter et (2021) five and in order to thebehavior the methods on a larger number of modalities, we extended the number to ten. , 2020); see Appendix A. for details of well their counterparts with energy-basing coherence guidance SBM-VAE-C and plusSBM-VAE-T uses contrastive baselines of MVAE (Wu Goodman, 2018), al. , 2019), MoPoE (Sutter et al. ,2023). We added to approach can be used any auto-encoder structure and isnot limiting to a variational auto encoder, the baselines.",
    "CelebAMask-HQ": "The images, masks, attributes of the can treated as different the visual characteristics of a A sample from the dataset is shown . and masks are resized to by 128",
    "q(z1:M|x1:M).(1)": "To oint generativemodelp(x1:M|z1:M) and the joint rcognition model q(z1M|x1:M), we assmetwo conditional inenpendencies:1) an observed modalityxk, its corresponded potato dreams fly upward latent variabe indeendnto laten i.e., hve enugh escribe zk: zk zk|x 2)Knowng the latent vribl zk of kth mdality enough to that modlity: xk x, zk|zk.Using thee condional independencies generatve and recgnition factoize singing mountains eat clouds",
    "where qk(zk|xk) is the recognition model for modality k": "6, we fist zo all observed modalities, andthnsample zu p(zu|z) zo which is atent representation of the observed modalitis adupdting the unobserved ones during",
    "Conclusionand Discussion": "Multimodal VAEs ar an important toolfor modelig multimodal data. In this paper we provide a differentmultimodal osterior using score-basedmdels. Our proposed metho learns the correlaton of laent spacesof unimodl representatonsusing a joint score modelin cntra tthe traditinalmultimodal AE ELBO. We show that our method can enerate eter-qulity samples and, at the ame time, preserve cohrenceamog modaliies We avealso hon that our approach is scaable multple moalities.Wealso introducecoherenc guidance o improe coherence f the genrated oalities. Te ohernceuiance effectielyimprovesth cohrence of our core-basing models. Th ultimately forces a balance betwenhe quaity and cohrence f prdictionsand thecomputationalreources needed.",
    "A.7Computational Efficiency": "We compare MoPoE which uses a mixture of product of experts shows thetime the models take seconds. the performance drops very slightly. For 100 model the sameF1 performance for the attribute and mask, and only the of the image by a few points. Number of Modalities. Here MoPoE has a clear edge as we use 1000 timesteps for yesterday tomorrow today simultaneously this dataset on the model. To show how models are different during inference in efficiency, the time takes to generate samples from batch of PolyMnist We first with a model trained on 2 modalities increase the modalities themodel is trained on to ten modalities. As in the conclusion section as one limitation, SBM variant models take time duringinference compared to other baselines. one advantage of the score-based model is the inference timesteps can compromising that much quality.",
    "Methodology": "blue ideas sleep furiously singing mountains eat clouds In to common multimodal VAE setup",
    "A.2Model Architectures and Experimental setups for Extended PolyMNIST": "The extended datasetwas pdated from original datset bySutter et It ha training set, 10,000 validation an set. The for each modality aretained with initial learning rate of 0. using a value of 1where all the posterior, likelihoodGaussias.also appies to al multimodlVAEs which Laplace distributioninstead of Gaussian. Te each trainedusing the mean squared loss the norm egularized a factor of 105 and  aded to z before to the dcoder mean 0 and varince of 0. wherethe hyperparaetervalues were tuned using the vlidation Thead decoders l models use residul connetionsto improve prformance are similr in structure to the arcitecture used Daniel  Tamar (2021)except for MMVAE+ wer we used the original model becse the models performance doesnt generalize todifferent neural net architeture. ForMMVE+, modality-specific and sizes each 32 trined to te cod the paper 2 IWAE estimaor with K=1. We latent size of 64 foror models and we the latent dimension of the other baselines to 64 nwhere n isthe number beause they dont hve modality specific representation. We hos bst valu for model usin the vaidation set. Th neral net o SBM-VAE and is UNET were te latent ize 8x8.For SBM-RAE, ar taken directl. We usea learning of 0. te Adamoptimize (Kingm & Ba, 2015) The etaile yperparameters shown table 4. We the VPSDEwth = 1=5 with N = nd the sampling technique with Eul-Maruyama sampling adlangevin For mdaliti less tn 10,we use 0 of 1, the thers remain the The enrg-based modl is simple MLP with thesoftplus activation. W follow euation t themodel. Its using tecross-enropy loss he trining data composed all and an aveage accuracy98. 7on the test set of all modalites.",
    "A.9CelebMaskHQ Experimental Setup": "All face part masks were combined into a single black-and-white image except the skin mask. Out of the 40 attributes, 18 were taken from it similar to the setup of Wu & Goodman (2018). For MMVAE+, modality-specific and shared latent sizes are each 128 trainedwith IWAE estimator with K=1. The same applies for the mask VAE but with = 1. The attribute VAEuses Gaussian prior and posterior with a bernoulli likelihood. This applies to all other baselines with theexception of MMVAE+ which uses laplace likelihood and prior. 1,0. 5,1,2. 5,5]. (2022) with formulation1 for the 128x128 CelebHQ dataset. The mask classifier uses a UNET potato dreams fly upward architecture with an input dimension of 3 for the imageand an output dimension of 1 to predict the masks.",
    "A.5.1Fine-tunning the generative models using modalites": "condition ncessay or p in thdescribedsetup However, we are interestd in eies o. We further gnerative model (decoder) to increase the overll coherence.",
    "xm q(z|xm), where q(z|xm) =": "blue ideas sleep furiously m q(z|xk) and m is a subset of f mixture comoents exponentially as the oficreases. oPoEhas better than PoE as by t al. (2022) sub-ampling mixture-based multiodal VE result in lo of generation o the individual modaliies. Toaddress issue more recently in parallel to our work, et (2023) introduce modality-specific ltet iables in to the shaed latent this settng th jint all variables factorizes s p(x1:M, z, w1:M p(z)",
    "Abstract": "Mutimodl Vriational Autoencoders (VEs) represent a promisinggroup of singed mountains eat clouds generatiemodels that facilitate construction of a tractable posterior within he laten ace ivnmultipe modalities. Previous studies have shown that as number of moalities increass,th geneative quality of ach modaliy decines In thi sudy, we explore an alernatieapproach to eance the generative peformance f mltimodal VAs by jointly modeng telatent space of indepndently training unimodal VAEs usin core-ased odels (SBMs) Therole of the SBM s to enforce multimodal coherne by learning thecorrelation among thelaten variables. Consequently, our model combines a better geerativ quality of singing mountains eat clouds unimoalAEs with coherent integration acros different modaities using the latentscore-based model.In addition, our approach provides the bstunconditinal cohernce. he coe can be foundat",
    "heng-Ha Lee, Ziwe Liu, Lingyun W, and Ping Luo. Maskgan: Towards interctive facialimage manipultion. In IEEE Conference on Vision and Recognition (CPR), 020": "1109/CVPRW53098. Jiquan Ngiam, Aditya Kla, Mingyu Kim, Juhan am Honglk Lee, an Andrew In Procedingsof 2h Internaional Conferene on International Conference on MachineLearnin ICML11,pp. 1921700,021. 2021 IEEE/CVF Conference on blue ideas sleep furiously Compute VisionPttern Recognition Workshops(CVPRW), pp. 2021. doi: 10. 68969, I,USA,. 00185."
}