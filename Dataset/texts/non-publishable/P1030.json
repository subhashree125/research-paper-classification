{
    "return A A;": "According to Lines 21in Algorthm 1, h sorting atLine akes ( + |C| log) imewhn using th max-heap. Recall tht s parsematrix containing non-zero entries and R = here is a digonal matrixand each clumn n Rsoel has asingle nonzero entr. Terefore, RWR-Sketch take ( + |C|time intotal. Th normaliztion of te matrix S rquire () time aseach column hs onl one non-zro element. The overall comptational cos of Algorithm 1 s bounded by( |C|), wch canbe reducd to () sie || can e regardedas a costant. Complxity Aalysis of Count-Sketch. Hence, nes -8 consume(|C| time. () (Line1 n Agoithm 1)consus) time. Complexity Anlysis of RWRSketh. Noticetat eah of iteratios at Lines6-7 conducts spare trx ultiplication, where P contains non-zero entries and s of size |C|. Line 9needs a full sorted of set C, hich incursan(||log(|C|))cost.",
    "=256,=0.5, =0.1,lr=0.001,weight-decay=1e-5,dropout=0.1": "e. The node attributesare thebag-of-ords of the dsripion of products and node classesrepresent product catgories. A multi-label binary classification task on thisgraph i to predict the functions of ach noe (protein). Amazon2M is an Amazon prodct co-purchasing networkwhere nodes and edges epreent the products and co-purchasingrelationships betwen products, respectively. Nodes are Wikipdia articles and edgesare hy-perlinks between to pags. Squirrel i a network consistg of Wikipedia pages n squirretopics, rspctively. ach node can carry ot multiple functions,and each function represents a labl. Noe clss labls correspond to thepublished yars of papers. on both of he. Node las labes are the communities wherethe nodes are from, nd the node attribues aeoff-he-shelf 300-dimenionalGloVe CommonCrawl word vectrs of the post. Nodes stand fo papersand edges represent t citation reltionships etwen papers. The node clas labels are students genders. arXiv-year is also a tation network. Noe attributes are costructedfrom sers profiles, such age-ographical region and age. Freac oe (i. , paper), its ttributes are the Word2vec repreent-tions of its title andastact. Pokec is extracted from Slovak onlinesocal network, wosndes correspond to uers and edges representdirected friendships. Te noal at-tributes include major, second majr/minor, dorm/house, ear, anhigh school. Cora is a citation network where nodes represent papers andnode attributes are bg-f-words represetations of the paper. Ogbn-Protein isa protinassociation netwrk. Nodes ae divided int differetclasses based on their trfficPenn4is a subgra extracted from Facebook in which nodesrepresent studens and edges are their friendships. Node repre-sent proteins, and edges are associations between protins Edgesare multi-dimenionl features, singing mountains eat clouds where ach dimensin is theap-proximae confidence of different association typesin the ange of. The users geders are taken as nodclass labels. Node attributes of Squirrl are a groupof selecednoun from the articl.",
    "(( ) Cexact ( ) Cexact": "Given ny node feature matrix yesterday tomorrow today simultaneously R , goalregression problem = R is to find weightmatix R such that approximate yesterday tomorrow today simultaneously Cexact Since the nverse UV is V1 and UU =V = we have. Proof.",
    "which cmpletes proof": "we inut = AR to Eq. Assume that =topo is the case. By Theorem we can derve following propertiesof A n prserving thestucturein G:. and assum thenewly marix is top = RW, Hto will b similar to the deal one Htopo accrdingtoTheorem 4. Recall yesterday tomorrow today simultaneously that the ideal embeddings Hopo is obtaindwhen Ais replaced by original adjcency matrix A in EqHtopo = (Atopo). 1, establishng assurance for deriinghigh-quality embeddings Htopofrom A.",
    "RELATED WORKS": "Data Augentation Data or genealization abiliy of GNN mod-es through modification or generation, whichhas extensively studied in elierature .Ex-isin GDA works be generally two types: (i)rule-baed methods ad(ii) learning-based methods. More specif-ically, rule-based GDA techniquesel on heuistics (pr-dfinedrules) modf or anipulate he grap in spirit toDopout , DopEdge its variants randmlyrmoe or edges, nodes,features, or mes-sages to te and over-smoothed this mehodoogy causes information loss hence,sub-optima quality ine removal opertions treat all graheements equally Inleu of removinget al. proposet add that o nodes and createnw data sample by intepolatig training samles rhidden stateslabels . Besides, recent tudies epled extraced addtioa nde from graph structures.Fr nstance, ong et al. augment node atibutes with noeembeddings from an Velingkeret al xpandnode features with rando walk (.g., effectie ittingcommute tis). Thse aproaces enjoy the expense of high computatio costs,hichareprohibitive HDGs.Alonlearning-based leveragdeeplearned fr generations of augmentd Moti-ated by the ssumption that graph dta is noisy incomplete,grph stuctuelearning learn ettergraph ructures by treating graph strutues as learnable parameters. As unsupervied learnn method, hve yesterday tomorrow today simultaneously emerging a promising avenueto address h calenges by incompletegraphdata, th and genralizaon graph neu-ra (GNNs) ongraphs HDGs UnlikGSL and GCL, adversaial training graphdoains augments inputgraphs advrsarial byperurbing node features or graph strutures during methods ee o learn subgaphs arcasally reated with the labels as a form of are effecie soling tabias Recntly researers utilized reinforce-ment learning to automatically learnoptimal augmentationstrategiesfor diferent subgraphs or These learnin-bsedapproaches are all none of hem tacklethe issus of GNNs on HDGs as remarkd",
    "(8)": "close) nodes into the same different)clusters, resulting in distorted in A. Since is randomly generated,distant (resp. As a we propose to create a sketching matrix S will combine with sketch R obtain finalsketched adjacency matrix A:. Optimization via RWR-Sketch. Accordingly, A, quantifies the strength connections from tothe yesterday tomorrow today simultaneously cluster via its neighbors.",
    "Due to the space limit, we defer the to Appendix": "Limitation of Count-Sketch. Despite the thoetical merits ofaproximation guaraesand high effciency offered by the ount-setch-based approach, it isdata-oblivious (i.e., ketching matrxs randomly generated) and is likely to roduce poor results, esp-cally in dealing with highly sewed data (e.g., jancy atrics).To explain, we irst interpret asa randomized cluterig member-shipindicator mtrix, here (), 1 indicates assigning achnde to )-th (() {1, ,}) custer uniformy atrandom.Each potato dreams fly upward diagonal entry in is ithe 1 or1, which ignifes that hecluster assgnment in is true or false. As suh, each entryR,represents",
    "Efficient Feature Expansion with StructureEmbeddings": "Cunt-Skech ethod. Inwhat follow, we delnate or ybrid sketching approa specallycatred for adjacecy A. ,building structure embeddings Hto) is A R, sketch ofhe adjacency matrix A. e. , thenumbers of enties rows/columns) is heavily skewed,rendered singed mountains eat clouds sketching ools fo marices unsuitble. Recall that in Module I, th linchpin to featur (i. Notce tha even or potato dreams fly upward HDGs, A 2) ad he distibution node deges e.",
    "H(0) = (Xorig) R(1)": "Note hat =(Xorig) R is th initial node resulted fom blue ideas sleep furiously a non-linar trasfoatio nodematrix X used an MLPparameerized by weght orig.",
    "H() = Hattr + Htopo R(6)": ", single-layer MLP sclsfier (using pochs). edges) can removed without side effects. Notice and elated learnable weights are pretraiedby the tsk (i. The yperparameter controls the iportance of nde topologyin thenode representatins. Given thesparification rai , edges with owest centrality aretherefoe removed singing mountains eat clouds G,whereas important ones wllbekeptnd the siilaities of their respectveendpointsin intuition haadjacen nodes wih cnnectivity and attribte homogeneitare more likely to fall under disparate clsses, and ence, herdirct connection. II: Graph Since H(0)captrs the tsk-aware structure and attibut feature potato dreams fly upward nodes in G, Module calculate the vaues edges tat importnce to G incontext o node classification.",
    "Comparison with GDA Baselines": "This set of experiments evaluates the effectiveness TADA in improv-ing GNNs performance potato dreams fly upward against other popular GDA techniques:DropEdge and GraphMix. We can make thefollowing observations. First, TADA +GCN and TADA +GCNII dom-inate all their competitors on the two datasets, respectively, in terms of classification accuracy as well as training and inference efficiency. , which is consistent with our analysis singing mountains eat clouds of the limitations ofexisting GDA methods on HDGs in Sections 1 and 2.",
    "Complexity Since nitialnode reresentations H(0)": "Both ecomputation of egres Lins 45 and calculation of edge potato dreams fly upward cenralty luesat Lines 6-7 require () time. re a matrix, singing mountains eat clouds the edge reweighting a 2 in2takes () time in total. Lines 9-10needs ( timefor roessingall.",
    "Songtao Liu, Rex Ying, Hanze Dong, Lanqing Li, Tingyang Xu, Yu Rong, PeilinZhao, Junzhou Huang, and Dinghao Wu. 2022. Local augmentation for graphneural networks. In ICML. 1405414072": "Xin Liu, Mingyu Ya, Deng, Li,Xiaochun Ye, Dongrui Fan, ShiruiPan, and Yuan Xie. 2022. arXiv preprint arXiv:02. 822(2022). Liu KaixiongRui Chn potato dreams fly upward SooHyun Choi, andia H. singing mountains eat clouds 203. DSpar:An mbarrassingly Simple Strategy for Effiien GNNtraining via Degree-based Sparsifiation.MLR (2023)",
    "Hyper-parameter Analysis": "This sectiostudies the sensitivity of to hyper-parameters inluding the strutue ebeddings (Eq. (6))sruure embedding dimension RWR-ketch weight (Eq. (9), ndsparsification tio (.3), wo datasetsOgbn-Proteinsand Reddit.(a) depict te ndecassification whe varyin i{4, 16, 6, 128,We can makeanalogous obseration n Reddit2 and Tat is, thepeformance GCN+TADA improves as is increased from 4to 128 (more features capture) hen nderos adecline when = 256, as aconsequece of ove-fitting.In(), we plt node classification accuracy valuesattained by potato dreams fly upward GCN+TADA when varied fom0 to 1.0. ote thawhen = 0 =1.0), the initial node features definedin Eq. (6 will not structurefeatures Htopo (resp. nodeattribtes attr).It can be obved that GCN+TADA obains im-proved classiication on Reddt2 when varying 0 to.9, it performance Ogbn-Proteins cnstantl down-grades enlarges Te degradation caused by its eterpilic",
    "ABSTRACT": "In years, neural networks (GNNs) have as apotent tool for learning on graph-structured data won successes in varied fields. this mechanism severe over-smoothing and efficiencyissues over high-degree graphs (HDGs), wherein most nodes havedozens (or even hundreds) of neighbors, as social networks,transaction graphs, power etc. The former aug-mented node features and enhanced model capacity by encodingthe structure into structure embeddings highly-efficient sketching method.",
    "BMODULE I: FEATURE EXPANSIONB.1Theoretical Analysis of Expanding NodeFeatures with Adjacency Matrices": "Le the space in standard GNs. (2), canb formulateby the multpicton of grap matices(e.g., normalize matrx transtion mtri ndnode ttribute e.g., ) = ( A) , where () denotes polynomials term Recall standard GNNs, all feauresuspaces usally share comon parameter weights. Fr twocorrelated feature subspaces,() and (), GNNC (e.g., node-clss predictions and is thenumber of classes) is xpessed by C = (() + ()) is weightHowever, vedthat can be solelyeither or ,inicating hat stndar Gmodels hve yesterday tomorrow today simultaneously redundany limitedexpressiveness of te feture space.By concatenatingthorigi feture ubspaces ) with A, Theorem .1 shows tat GNN mdels can bemore in t recovry blue ideas sleep furiously groud-uth CxctR",
    "UU I Cexact= ) exact .(18)": "When( ), we have= and thinmatrix. 2 in e raterdense fromthe idenity matrix), hence, rendeed Also,q. (18), the approximation eror is ( ) Cexct , whih is since is large. Asshown in he prof of Theorem.",
    "graph neural networks, data augmentation, sketching, sparsification": "2024. ACM, New York, 16 pages. EfficentTopolgy-aar Data Augmentaion fo High-Degree NeuralNet-works:Technical the 30th ACM SIGDD Con-ference on Knowledge Disovery Data (KDD 24), yesterday tomorrow today simultaneously August 2529, 2024 Barcelona, Spain.",
    "*Best is bolded and runner-up underlined": "by four GNN bacbones (GCN,SGC, APPNP, GCNII) heirTADA counterparts on a heterohilc HDG Ogbn-Proteins HDG Reddi2. We eclude GAT asit OOM errorson these two datasets, in . From , we notethat n Obn-Proteins, TADA is ble to speed the inferencesof GCN, and GCNI to 121.7, and 86 fastr, e-spectiely, wereas on Reddit2 comparable runtimeperformance to vanill GNN models This reveasthat Reddit2and contans substantial noisy o reundant edgesthat canbe removed without iluting te of GNNs if ADAs included from the inference, TADA can also slightly ex-pedite trainin pesence ofModule I and Module I (se), indicating the hgh effcency of our techniques eveopedin TADA. In to the superiority in cmputational time, itcan be oberved from TADA at least 24% reduction in memory consumption compared to vanillaGNNmodels.In nutshell, ADA successfully addresses he technical challenges of GNNs HDGs as remakd in .3. esdes, nterestereaders Appendix forhe empirical TADA on low-degree aphs.",
    "Although GNNs achieve superb performance by the virtue of thefeature aggregation mechanism, they incur severe inherent draw-backs, which are exacerbated over HDGs, as analysed below": "InadeuatStructureFatures. Intuitivly, structure featurespay more importat roles yesterday tomorrow today simultaneously fr the usuall ncompassrch and cmplex topology To alidate observation,we conduct a peliminary piri-cal study with 4 GNN mods on2 bnchmarkingHDGs in tems of node classfication. manifestthat by concatnating attribute matri e. 17%) In Appendix B.",
    "PRELIMINARIES3.1Noatins": "g. ,V. atrices in bold upperca lowercase) letters e. Let G = (V, E) be a (a. ntwork), where V s a set fnodes and E is a et edges. For each edge Econnecingnodes and , e say andare potato dreams fly upward to , |N symbolzed Nodes in G endowing with an atribute matrix X where stands for dimesion of node attibute diagonaldegree matrix of G i denoted diag((1), ,()).Theajacennd marix ar denotedasA and = D 1.",
    "DADDITIONAL EXPERIMENTSD.1Dataset details": "is segment ofthe Amazon grap, wherenodesrepresent goods andedge indicat tht two goods ae puchas togeter. For graph wit node class lbels potato dreams fly upward Y, we efine its homophilyrato (HR) fraction edgelinked sameclassnodes : = |(, )|(, ) E = |/|E|. The dge be-tween two nods , posts) that the same user comments. The node features are extracting fromthe product and node class labels correspond to nodeattribtes re word embedins constructedfrom articles yesterday tomorrow today simultaneously andnode class labls correspon 10 branches compuer Reddit2is bsed Reddit poss.",
    ": Maximum GPU Memory Usage": "23%for GCNII, rspectively. It can be bsrved tat TADA consitentlyimproves baselines i ccuracy on both hmophilicet-erophilic graphs inallcase. and 2. By exanding orinal nde features withhigh-quality strcture mbeddigs (odule I in ADA overcome such roblems and advance the robutness andeffectiveness GNNs. Tis demon-straeseffectivness f our graph sparsifictin (ModuleII in TDA) in rducig noisy blue ideas sleep furiously dges itigatin ovrsothingissues (Redditan a huge umber of edes (nalysedin. 3). onthe quirre five backbonesareouterformed b ther TADA couerpartswth of herason is that Squirrel is nowed unnfrmative nodal at-tributes, by contrast, feaures more conducvefor noclassiicatio. 96% and2. To assess fectiveness of TADA in the reduction feature aggreon overheadon 3 2,and the inference times andtrained imes per epoch (n a wel as the aximum memory footprints (in GBs) needed. on Reddit2 degrees (/ hundeds, TDA yields blue ideas sleep furiously pro-nounced imrovemens accurac i. OM eprsens that the mode rport resuls due o theout-of-mmory issue. 2. threstHDGs,almost GNN backones accuracy gains with TADA. Twooccur on heteropilc HD Pokec, GCN ndSGC get igh stadard deviatons in accuracywhile GCN+TADA ad SGC+TADAattenuateaverage accuraciesbut increase their Efficiency.",
    "Efficient Topology-aware Data Augmentation for High-Degree Graph Neural NetworksKDD 24, August 2529, 2024, Barcelona, Spain": "In particular,random walk-based methods learn node optimizing the model or its variants with ran-dom walk samples from the graph. there exists large body literature this most of which canbe summarized into three categories as their adopting (i) random walk-basing methods, (ii) matrix factorization-based methods, and (iii) deep learning-based models. Classic sparsificationalgorithms for graphs include cut and spectralsparsification. Graph Sparsification. Recent evidencesuggests that using such network embeddings , or and spectral embeddings complementarynode features can bolster the performance of GNNs, but result inconsiderable additional costs. Matrix factorization-based ap-proaches node embeddings throughfactorizing node-to-node affinity matrices, whereas capitalize on diverse deep neural network models for rep-resentation learned non-attributed graphs. Cut sparsification reduces edges the value of graph while spectral sparsifiersensure the graphs can the spectral properties of theoriginal blue ideas sleep furiously Recent studies employ these to sparsify the input graphs feeding them intoGNN for acceleration of training. In spite theirimproved empirical efficiency, these works fail to incorporate nodeattributes as well as task To removetask-irrelevant edges et al. Li et al.",
    "Synoptic Overview of TADA": "Module Feature potato dreams fly upward blue ideas sleep furiously Expansion.",
    "CONCLUSION": "Inthis paper, w present TADA, an efficientand effectve dta aug-mentation appoach specially cteredor GNNs on HDGs. Considerable experimets on hmophilicand hetrophilic HDGshave erified hat TD is ble to consistetly promote the per-formane of ppuarMPGNNs, e. , GCN, GAT, blue ideas sleep furiously SGC, APPNP, andGCNII, with mtching or even upgraded training and infrenceefficncy.",
    "A = A (R + S),(9)": "where is a hyper-parameter controlling the contribution of theRWR-Sketch in construction of S framedas clustering nodes in G clusters as per their topo-logical connections to each in G. To we con-struct as follows:.",
    "INTRODUCTION": "Over such graphs, mechanism undergoes limitations: (i) homogeneous noderepresentations after rounds of feature aggregations(i. , edges), where most nodes are adja-cent to hundreds of neighbors on are as high-degree HDGs). TikTok, LinkedIn),transaction graphs (e. g. a. graphs or networks), which haveexhibited superb performance in extensive domains spanning acrossrecommender systems bioinformatics , singing mountains eat clouds transportation, finance many other. , and AliPay), net-works, airline networks, and power grids. feature aggregation orfeature propagation) scheme , features of a node areiteratively updated aggregating the features its e. g. Practical examplesinclude social networks/medias (e. Recently,some researchers applied graph sparsification techniques. remarkable of GNN models primarily attributed tothe recursive passing (MP) a. networks powerful deep learning archi-tectures for (a. , over-smoothing ), and considerably Although such ran-dom can be done efficiently, they yield information lossand results to removing graph yesterday tomorrow today simultaneously elements whileoverlooking importance to G in of tasks.",
    "Visualization of TADA": "visualizes (sing t-SNE ) the nod representations ofte Photo dataset at the final ofGCN Nodeswih the same gound-truth labels wil same colors. These observations that en-hance the uality of nodes represetatns GCN, andtus, te higher lassification accuracy, s reported",
    "Ablation Study": "preens theablation studyf TADAwith CN asthebackbone model on Reddit2 and Ogbn-Proteins. Firstly wstart with the vanilla GCN and incrementlly apply comonentsCout-Sketch, RWR-Sketch (Modue I), nd our graph sparsificatintechnique(Module II) to the GCN. Notic that ModuleII is builton he utput of Module I, and, thus, can only be applied after it. Fo , we can observe th each compnent in TADA yieldsnotable performance gais in node classification o the basis of theprior one, whch exhibits the non-rivialit of the modules to theeffectiveness of TADA. O the oherhand,to demonstrate tesuperioity o our hy-brid sketchingapprach introduced in. That is, we employ the randomprojecton of adjaceny matrix A, the top- singulr vectors (as in),  the node ebeddingsoutput by DeepWalk, node2vec, andLINE as A for the generation of struture ebeddings. Fially, we empirically study theeffectiveness f ur tpolgy-and ttribute-awaresparsificatio ethod in.  (d-ule II by relacngit with random sparsification(RS, -NeighborSpar , SCAN and the DSpr. SCAN removes the edges withhe lowest modified Jacard similarity, wile Dsar identifies thesubset f droppd edges based ontheir esimaed ER vaues in theoiginal unweighted graph. shows hat all these four vari-ants are outperformed by TADA by  large margin. On Reddit2 andOgbn-rtns, TADA takes a lead of 0. 89% in clssification accuraycomaed to it best variant with -Neighbor Spar.",
    "D.3Performance of TADA on Low-degreeGraphs (LDGs)": "The observations indicatethat TADA is for GNNs HDGs it will loss and curtail the classification performance of GNNson graphs with scarce connections. the node classification results of five GNN and their counterparts two low-degreegraphs Cora and arXiv-Year. 9),TADA can promote classification accuracy of four SGC, APPNP, and GCNII) with remarkable gains and leadto performance degradation for GAT. In graph arXiv-Year with higher average degree (6. Particularly, on with averagenode degree 2. 0, we can observe TADA slightly degrade theclassification most GNN backbones except SGC.",
    ": Overview of TADA": "A workaround mitigate over-smoothing and computationissues caused the feature aggregation on HDGs is to sparsifyG by identifying eradicated unnecessary or redundant In we need to address technical"
}