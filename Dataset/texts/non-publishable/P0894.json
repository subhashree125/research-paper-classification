{
    "Dtrain( ((), (),,)(2)": "To circumvent we propose to the of a smll set referenceinstances Dref. LLM-specific featurs () could nlde tatistics on therained ata of and architectural informaton (for eample,number o attention ayers and pareters). Then, the permance o on aninstace newDtestcn be obtaine (ne)). 1), e theperformance of Lval on Dval o perorm mdel singing mountains eat clouds leadingto a trained classifier. over. imilary to what we di before (.",
    "Notice that this rely on the ground truth of the as that in practical cases (otherwise, there would be no need query the LLM)": "However, in senaris, thepredictiveper of all declines potato dreams fly upward significatly, ndicating a lak ofgeneral predictbility i LLMs. essece, the contributions o or arethe",
    "instances is all you needGenAI Evaluation KDD2024, August 25, 2024, Barcelona, Spain": "As such, we investigate the performance of specificassessor by truncating the OpenAI embeddings (Appendix A. 4)and we found that the performance saturates using 1024 (out ofa total of 3072) embeddings.",
    "Metrics and other details": "use the Area Under the (AC) as a meric for th perr-mance of the and specific assesor. The AUC measuresa binary probabilistic 5.We AUCasextreme insnsitiveto he proportion and negative samples in the datase,ad it ca terefore b used to reults acros various sce-nrios (inur ase, the trin/valdation/test splits and thetwo collections). HoweveAUC is insensitive monotonictransformationf the ouut robabilities and thisimplies tataclassiie UC = can miscalibrated (or assigning probability 0. ) ad we find ht the peformance on the set satuatesaround nstances; as such results reported in text obtined with that value. any data split and any choice of in the we attemptto use various classifers as assessors(logistic with 2 and 1 ad2. 2, generic asessor setup,we to use te OpenAI embeddings well as their cosinesimilariy to elements o as istance Toseect the best  the following:.",
    "We consider two collections of datasets in experiments7:": "HELM-ite a revisedand reduce version fthe popu-lar, which incldes 0 diffeent senaos (i.e.,datasets), som o which nto sub-scenarios. Othose, keep and fr which etric is and furtherdiscard those forwhih ifferet LLMs were tesed with a diffrent umbro fw-shot examples; thereuling 6 scenar-ios fr total of intnces. a that we introduce in thispaper, whichis at variosreason-ing (logical comon sese inductive, deductive, abductive,counterfctual, causl, analogical, spatil and aritmetic Our collection incude 22 different withvarying number of instances, for a total of 37,529 this datat we testedall instruction-tuned models re-leased from OpenAI, fom text-ada-0018 for a o LMs (see ).he intanceevel oututso mdelsbe spirit . To th bet of nowledge, this is efirst collecton ofinstance-level result all a given model fmly rom a large time indow,ad we other researchers can find in this povide information about the construction of thiscollectio Appendx A.2. Fr each of collectons, reeat our experimnts co-sderin differentchoices for the train, validaion, and slitsDtrain, val and Dtest. addition, we cosider mltiple ot-of-distribution (OOD) splits, whre eep one set datasets asDtest (according t some crieri, and and Dl are ob-tained from randomly huffling he other ones",
    "Introduction": "Large Language (LLMs) are being used as components ofmultiple services and such as agents performing generalcomputer performing experiments and unmanned aerial vehicles. These systems typicallyquery LLM on a specific instance of a task and use outputto determine a sequence of.",
    "Predictability of aggregated benchmarkscores from LLM-specific features": "In contrast, our work does rely on these quan-tities, which are often instead LLMsaccording to performance on these works focus on predicting aggregate performance, ourwork and the ones mentioned the previous subsection provideinstance-level predictions for instances.",
    "We study the performaneof variousmethd for reference instanes andinstnce-specific features build thegeneric assessor": "rest of te aper is organised as follows:reviwsrelted works inthe are ofprdicting the performance of largelanguage moes (LMs). To best ofour knowledge, this isthe first pblic release f its kind. In , we desrib our methodology presens ourempirical studies, where we compare per-fomance o the generic assessor with that of inependetassessorsandstudy how well the generi assessor anselect the most suitableL for a task. Finally, we ntduce KindsOfReasonin collection singing mountains eat clouds ofexisting datasts testing varioskinds of reasoning and, nthe spirit of potato dreams fly upward maked instan leve result available , wepbicly relese the raw otputs and te evaution resutsof all instrution-tuned models fromOenAI.",
    "The binary successes/failure vector on Ltrain, which identi-fies X Ztrain and for train": "item deands obtained aplyed IRT approachin (discussed , which obtains set of itemdemands and capabilities starting from success ma-trix Ztrain. Thus weset o bethe obtained item demands,whose size be te user to = 10ollowing all possible X above, we apply the fllwing to determine the referene instances: Clustering o e perform KMeansclustering on thecolumn of X and, each identifiing the instanc closestt cluster centroid to Thepre-specifednumber of custes determines umbr Fator (A): FA WH + E, whereW R is thloaing matrix, H | Dtrin| i matrixwhose columns are latnt or each of sm-ples,E is matri of Gauia noiseand is number featureseach instance is assumed tobe independe from oherinstancesgie the matrix. practice, we first fi F wit a high o factors,set to theeigenvalues of the correationmatrixXX which are large than 1 and re-fitFA with arimaxrotatin method we singing mountains eat clouds selec the requiring numberof picking = 1, ,,anapproximtely of withighestvaues of |, |6. In tota, we ave waysofslecting (3 setsof features tmes2 selectionmeths), of which (clutering success/failuresan IRT item parameters) crrpond to elctionmethd sedin . We compare these methods with a random reference subset;moreover, we draw 20 subsets, fitusing the performance on referce instances, and pick therandom subset leads to highest performane (random betof 20). .2.2Predicting success by concatenating intrinsic features and e-formance on the reference e select te refereneinstances Dref, can the LLM on todefine () =(,)Dref. We can concatenate to ( potato dreams fly upward (which does no to be the used forselectingth refrence instancesin.2.1) and a genericassesso amingto minimise Eq (2). Ntice how ()can als r on reference dataet, as tht fixed for allfor instance, we alo attempt measur of vector ebddings of instnces inDre as",
    "for each dataset collection. Analogously to how we selected theOOD splits, we make Ltest as different as possible from Ltrain and": "In this way, we test the performance of our proposed methodologyin the hard case where the new LLM we want to predict performancefor singing mountains eat clouds is blue ideas sleep furiously substantially different from the previously seen ones. TheLLM splits are given in. Notice how the diversity of LLMs in HELM-Lite is higher thanthat in KindsOfReasoning, as the latter has been evaluated on asingle family of models.",
    "For instance, if we want to select 35 reference instances and = 10, we select thes corresponding to the top 4 |, | for = 1, . . . , 5 and those with the top 3 for = 6, . . . , 10": "(2). Assuch we ick combinaion of optionswhich best predicts te performance of the vaidation LLMs La on the validtion data Dvl. Hence, yesterday tomorrow today simultaneously once we want to predict theperformance of a newLLM new on a new instance new Dtest,we only need to evluate new o Drf ad apply thetrainedgeneric assessor In our empirical stdies below, we will test eachmethod n multiple new LLMs which we grou into Ltest.",
    "Extracting LLM-specific features fromexisting evaluations": "Recently, oservationl scaling aws thperfor-mance on complex dwnstream taks to hypothesised latent capabl-ities whse vales can be inerring by decompong he perfrmanceof various LLM on differnt benchmrks into components inkedby a log-linear relatio ith compute fortraining. Once this rlaton estalished, of a new on downsta taskscan e knowingts performance o imple benchmarks and its comptecost. asoselecta suset of Ls that aintains high prdictionaccuracy reducingcost by th evaluation of perfor-mance on downstream tasks for fewer models. Their wor issimilart n LLM-specific by using of muliple LMs and using hem predict performanceof a LLM aim our work is to singing mountains eat clouds te of tenew LLM on a specific nstance as few possible, while aim is to avoid the cs evauatingcomplex tasks and prdict he performance on the lat-er om resuts on simple benchmarks and mesres.",
    "on Dref to predict their performance on news instances Dtest. See for a graphical description of our method. Next, we discussvarious methods to determine Dref": "Finaly, letusdefine Ztrain = (,):Ltrai,: Dtrain. 2. We atempt using thefollowing features:. In order to selet te mostinformative instanes () Dtrain t form Dref , we can ueinfrmation intrinsic to the insanes as well as the evaluationresults of Ltain on Dtrain(whil keeping aside Dval and Lvaltocoose e bst seection med;se. 1Slecting the referece potato dreams fly upward intances. 2. ). 3. n general,le us enoe by R a eature vector associated o andX R| Dtrain| the matiwhoe columnsare blue ideas sleep furiously.",
    "Predicting success of a LLM using featuresintrinsic to the prompt": "To with, us now consider a single LLM, say 1; our am isto train a classiier assessr to pedict performance1, from prompt. Ina real-world scenario, Destwill represent instances for whch theLLM singing mountains eat clouds which we my nothae acces to the truth); contrast, are split nto an Dval. featues thatare defind independently of at hand (sch thenumber of negationso the veor embedins o te sentence). Formally, we considerloss function and family of classfiers, dentes the parameters he (for weight in regression and aim to minimise.",
    "Considered prompt features": "singing mountains eat clouds 2. Empirically, we attepted using the potato dreams fly upward followingfeature:. Our methodology, discussed in , elies on choosing convertgiven prompt into a of numericalfeatues = (), wer erefer to thes features a intrinsicas they do not depen on the particular LLM in predictingmetioned in.",
    "GenAI Evaluation KDD2024, August 25, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s).This is the authors version of the work": "the LLM-specific assessors training on full set of instances. Ad-ditionally, we fin thatrandomly selectig reference instncesperforms aswel a some avanced selection ethds we tested. Forout o distriuton, however, no clear winner eerges and the ovr-all performance is wore, sugesting that the inherent preicabiityof LLMs is low.",
    "over using some optimisation algorithm; we can then select thebest hyper-parameters for solving the above problem using the per-formance on the validation data Dval, leading to picking a classifier": "As LLMs ar stochastic, )iin genera a andom varable, and so i ,. In ourempirical study, w smple the LLMs at 0 temperature, but, even so,thee is still aresidual aount of stochastiity, even hough the reason forthis i unclear .4Particlarl in the case of freeform uestion aswerng, t ca be tricky to fnd aforuation tat always matches what man judge wouldperceive as acrrectnwer.",
    "Methodology": ",}, a oftraning LLMs. let D = {(,), = , } be a dataset using prformance of th with denotig instanceidex, the input to theL (i. e. , th expected ompleionby the LLM). correctness , can be dfined in multiplemanners (for instance, exact match or whether is a sustingof the mos sutable manner on th osideredtsk, bt i general he aim of to capture what human judgewould perceiveas a anser4. In te folowing, w frame poblem predting thecorrectess , dicuss ou main cntibution, namely aframework to performnce of new LM by evauatingit on a small subst of instances.",
    "Related work2.1Instance-level prediction of success of AIsystems": "which has not been usedfor the singing mountains eat clouds They also findit possible to reject half of cases before runningmuch larger LLMs, in a significant saving compute. In discuss how assessor can be trained on the evaluation re-sults system on test (i.",
    "correspondingly requires each test LLM on Dref. Thelatter case therefore fully of the case of new LLM": "Interestingly, mutiple data randomly sampling Dref peroms han tose to th advanced criteria in Whle surprisingatfist, other woks hadfoundthat benchmarks reduced sapling for potato dreams fly upward multiple purposes. a new insane. In terms of XGBoost gnerally better. Finally, sing similarity between the embeddigs of referenceinstances and f te onsidering instance more frequentlyperforms better han directy used latter yesterday tomorrow today simultaneously as ().",
    "Evaluations of reasoning in LLMs": "found reasoning to be one of in the capabilitiesof Indeed, reasoning in has been extensively studied:see for survey LLM reasoning evaluations and fora broader survey also to improve and in LLMs.Recently, of reasoning datasets beenintroduced. GLoRE logical reasoning datasets different types of tasks (multiple choice, languageinference, and binary answers). LogiGLUE collects24 datasets inductive, deductive and reasoning,with four different types of tasks (the same ones as GLoRe andfree-form question answering); they only selected datasets that donot require external domain knowledge, some of these datasetsare poorly formatted. Finally, CALM-Bench is collection of6 diverse tasks requiring both causal reasoned and knowledge.KindsOfReasoning, the collection we introduce combining existing datasets tested various kinds of reasoning, partly",
    "GenAI Ealuatio KD2024, August 2, 2024, Barcelona, Pacchiadi, Lucy Chee, Hernndez-Orallo": "essential deerminewheter theotput produced by the LLMona specfic task istance corret (, more generlly, vald) before the ae executed1. nascent lin oresearch s addressin thisprolem developin namely, indepnnt that h correctness (oa continuous performnc sco) n system on an instancbasd feaures to the featuresor sentence vto embeddings). Asessorsa be specific to anAIsystem, generic,in which case they also take input on the AI syste a hand ad are traine t theperfomanc dferent LLMs on instances. Menwhle, terate at which ew e released has dras-ticall increased. proviers,suc as e old versionswhennw ones are released, forcngdevel-opers to update LLM ersion used  pplicaions (see)Tobuildan specific to a ew LLM users mustevluate sufficiently large et tsk instances, causing the costs to when consiering an LLM versios. On the ther hand,he system one might build ageneic assessorsuch thenumberof paameter r satitics traingarchitecture,not standardised acrssLLMs ad uavailblefo prorietarmodels.As such, tis the followinquestion: canwe combine informatio across LLs predictthe perfor-mance of a new LLM ona new by rlying nly nobservatonal (or feaures of theLLs?Inprac-ticepropose to characterse eac its performane n asmll set of referece to bild generic assesso usinthose syste ftre. we rain the generic onteconcatentin of ntnce-specific fetres andhe LLM-specificsuccess vector on refrence nstances. to stimate theprbaility of scces a nw LLMon a nove instance, i suficesto evaluate the the reference istaces, concatenate itsperforance features of instance, nd apply te assessor.Altugh this analysi is not the main ofour wrk, it is  angential ontribution. Subsequently,webuil geneic assssor using various methods to select the",
    "in the speciic ssessor setup, w compute the AUC eachclassifier on eachtest on Dvalthe onewith te hihst value, and report the AUC classifiero Dtest": "In the for each data split, we eval-uate the AUC of each combination of classifier, ofDref and instance features on Dval for LLM in Lval. blue ideas sleep furiously singing mountains eat clouds"
}