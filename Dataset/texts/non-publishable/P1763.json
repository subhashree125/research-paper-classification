{
    "DM-ELF Implementatio": "We fix thefnetuningwhich is the number of steps fine-uninge performonce construct the class-balnced errorset. We tune the learning and the nuber of that are selected for class balancin. implementing misclassification-SELF using codeadapted LaBnt e a.",
    "Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks. InInternational Conference on Learning Representations, 2020": "Williams, potato dreams fly upward A. , and Bowman, S. In Proceedings of the 2018 Conference of North American singing mountains eat clouds Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018.",
    "Our Contributions": "ur key cntribution s a two-step methodology volvng: (i) reglarized anotato of domains (RAD) topseudo-anntt exmples by usingahighly regularized odel tained to learn suriously correlatedfeatures. We show potato dreams fly upward that both DS and UW aheve dentl GA and degrade sgnificantly withan inresingprcentage of syetrc domain nis, in theimt degrading to te peformance of empiricalrisk inimization (ERM). This is furtherconfired with numerical experiments for synthetic aussianixture datast modeling atet epresentaios. Additinally, AD-UWincurs only a minor opportunity cos fr not usingdoain labels evenin the nise-free seting. By learning thesuious corrlations, RAD onstruts a et of examples for whic such crelatins dthold; we identif thes a minority example. We testRAD-UW on several large publicly available datasets and demonstrate tht it achieves SOTA WGA evewith noisy domain annotations.",
    "Empirical Results": "We present orst-group accuracies several represenative methods crosslarge publicly availabledatases. Note that for all datasts, we use taining split o train the model. prorwork (Kirichenko et al. , 2023; LaBonte al. 203), we use half valdato data,",
    "with equality at p = 1/2 or 0 = 1/4": "Proof Sketch.",
    "C.3CMNIST": "The spuriouscorrelatins CNIST digit less5 and the color of tdigt. For ResNet50modl, we use constant rae emomentum of 0. 9 and wigh singing mountains eat clouds 3. The pseudo-nnotation is trained with aninitial learning ate of 1e 5 with CosneAnelingLR lering rate schduler frompytrch.",
    "(1/2 (p)0 ).(36)": "We derive equivalentclean We so by examining how true samples affected by on the data with noisydomain labels. When DS performed on the data domain noise, the true minority samples kept can categorized as (i) those that are minority samples in the noisy data and a proportionof those that have majority samples the noisy data.",
    "Giannone, G., Havrylov, S., Massiah, J., Yilmaz, E., and Jiao, Y. Just mix once: Mixing samples withimplicit group distribution. In NeurIPS 2021 Workshop on Distribution Shifts, 2021": "Idrissi, B. , Arjovsky, M. Pezesi, M. , nd Loez-Paz, D. In of Conferece on Casal Learning and Reasonig, Iscen, Tolias, G. , and Chm, O. 50705079, 2019. InProceedingso the IEEE/VF Computer Vision and Pattern Rcogniton, pp.",
    "Data Augmentation": "Upweighting (UW)not remove dtabut wehts the singing mountains eat clouds los more for examples less samples. Generally, th upweighting factr c can a though often theinvrse of f the grop in practice, which estimtes. Downampling (DS)reuces th number f examples in majority grups such that minority andmajoritygroups have the sae sampl Inredues te daaset size from t nmin where nmiisthe of example i th smalest group.",
    "Acknowledgements": "Nathan Stromberg andMonica Welfert are grateful for support from Arizona State Universitys Deans Fellowship. Sanmi Koyejoacknowledges support by NSF Career Award 2046795 and SCH-2205329, Stanford HAI, and Google Inc.",
    "Published in Transactions on Machine Learning Research (12/2024)": ": CMNIST WGA domain labelnoise. CMNIST a relatively spurious correlationand assuch even LLRquit CMNIST is alreadclass balanced and upweightig haveno singing mountains eat clouds effect. (LaBte t al., do not provide WGA for CMNIS;RAD-UW matches the perormance f grop-dependent methods at 10% an surpasse it hat.",
    "Broader Impacts and Limitations": "attempt to the issue subgroup fairness and robustness to subpopulation shift a pseudoannotation strategy in the domain. This method could allow practitioners to more easily adapt while assuring fair classification across subpopulations. blue ideas sleep furiously because there is not a formalguarantee fairness, there is a possible societal of practitioners the retrained model isfair without it. Similar lack of guarantees is of most methods in the literature. Additionally Oh et al. (2022) that most two-stage including the full-retrainingprecursors and SELF, blue ideas sleep furiously can fair dramatically with small of class label noise. Addressing thisissue is left as future Finally note achieving this fairness without presents an opportunity domainlabels be private. between privacy and fairness an important of as discussedin (2023). Perhaps there is with limited information, but enough to achieve over such as RAD and SELF.",
    "Assumption 3.5. The difference in means between classes in a domain D := (1,d) (0,d) is constant ford D": "3. see this by that each group mean makes up thevertex of parallelogram.",
    "Discussion": "in our experients that cniques, downsamplig achieve ery similar worst-group accurac degrade similarly with noise. Thi falls in inewith theoretc analysis, and sugests that simplesymmtric mixture models blue ideas sleep furiously subopulatns intuition for teperformance of data-augmented lastlayer retrainig ethds on eal datasets.",
    "C.1Waterbirds": "For RAD-UW, we potato dreams fly upward tune for the pseudo-annotationmodel and the retraining model. 9, and 1e3. We tune the upweighted factor, c, for retraining model 5 values from 5 to 20.",
    "C.2CelebA": "The pseudo-annotation model is trained wih an initial larning rate of 1e 5with CosieAnnealingLR learning rateschedler yesterday tomorrow today simultaneously from pytorch. We ain te upstream model for 50 ochs while using random crops and random orizontalflips for data augmentation. Wetune potato dreams fly upward he upwihting facto, c, for te retraiing odel ovr 5 equally spacedalues rangin from 20 to 40.",
    "P., Gruver, and Wilson, A. G. On feature learning in the presence of spuriouscorrelations. Advances in Neural Processing 2022": "G. King, J. Ho, D. ichenko, P. government. ,Gupta, A, Wu, V. I Procedings of 202 ACM Cnference on Fairnes,Aountabiliy, and Transparency, 2023. , and Wison, A. , Izmailov, P. , and Wbley-Brown, H privcy-bias tradeoff: Data minimizatinanraial disparity assessents in u. In yesterday tomorrow today simultaneously Th Eleventh International Conference on Leaing Repreetations 2023. ast laye re-trainin is sufficient for robustness to surioucorrlations.",
    "Abstract": "shw, both in teory andpractice, annotation-ased data using downsampling or upweighngfo WGA are susceptile domin anotation noise. To this Regularized Anntaton of Domais (RAD) to train robus ast ler explicit domain anotations. Ourresults show that RAD cmpetitivewith recently domain annoation-free techniques.",
    "Main Results": "present results for both and class-only-dependent methods using both downsampling and Additionally, we present results for LLR, which performs no or loss augmentation stepbefore retraining. Finally, present results for last layer methods, namely and RAD with (RAD-UW).",
    "C.4MultiNLI": "We train model for 10epochs an initial learning rate of 1e a decay of 1e We use the scheduler imported the transformers library. , 2022). We train the BERT using adapted from (Izmailov et al.",
    "i=1c( di)L(f(x), yi) + w1.(10)": "Here again,the regularztion comes into pla. of ths that ecan more effectively correc moels wee originally nosydata, s the noie affectsthe head stronglyte ebeddings themselves. egarization in the first ecouraging thbiased moel to inority points here sme regularization leaning whcho ell both for and potato dreams fly upward (upweighted) minorit. RAD-UW vs. M-SELFWhile both RAD twstage which utilize the ofan identification model pseudoannotte pints potato dreams fly upward lasses, xplicity trains sch a biasedlasiier usng a strong penalty wheeas classifier use by SELF is the pretraining lassificaton he beefit of retained the lassiier is we can forcete tomre strongly nspuruscorrlations.",
    "Problem Setup": "tuples D) of class and lbels partiti e examples into g :=| D| iffernt groupswith priors := = , = d) for (y, d) D th applied helatent space ofpretraine model s f : X R|Y|, whch is bylnear decision boundary = (w, b) Rm|Y| R|Y| given byf(x) = (wT. supervised clsification and asse that theLLR mthods have access a epresentatonf ambient (origal high-diensiona data mages, etc. ) gron-truth labl, asell nois) omain annotation Tke together, th label and domain combine to define tefrMore frmal, trainig dataet isa collection of tuples of the randmariables (Xa, Y, D) PXaY where Xa is the abient hghdimensional sample, Y Ythe clsslabe, Disdomain label. Here, yesterday tomorrow today simultaneously we pesent the as generic mlt-class, muti-domainlearning, but for ese analyis, we will restric ourelves he binry class biry the focu is o the linear lat laer wednote helatet rpresntaton as blue ideas sleep furiously aninput layer by X := for afuntion : XRm sch that LLRdaaset is (X, , PXY D.",
    "Importance of 1 Regularization": "To this end, singing mountains eat clouds yesterday tomorrow today simultaneously e combinaions both1 and the common enalties.",
    "Related Works": "(2021) xtend this idea (using upsampling he same dataset, equivalet o. (2023) usea variation on downsamplingto WGA ithout domain anotations using implicitlyreguarized ientification show that upweightin relate to the proportion groups ca strong WGA,andLiu al.",
    "C.5Civil Comments": "to MultiNLI, We trainthe BERT code aapted from (Izmilov et taite epochs wit aninitial learning rate of 5 a decay of 1e4. We usethelir learning cheduler iportedfrom the ransformers library. The odel i training with initial learningrate of 4with potato dreams fly upward CosineAnnealingLR ate scheule ompytorch.",
    "which can be seen as a finite sample version of (3) with an 1 regularization. For , we use the logistic loss": "yesterday tomorrow today simultaneously Theuweighting factor for group W method is given th inverse o th percevedprevalene for orclass. The importnce f pretaed mode is an impotant and undrexlored factor the literature weleave s futur work. We explore the f mdel Appendix. For our method tne the strengh for model grid tune therelarizatin strengh of trained modelalongwith theupweightingfactr c, is left as a hyperparameter because the identificati domains by RAD is onlybinary and may not reflectthe in cleanoldout data. More nrall, we cold apply model any of data methods. We also present the stanard deviaion aroun this mean, whchwill reflectthe vriance over runs the optimal hyperparamters. misclassficatin (M-SELF)(LaBonte e al. bu the metodology averages moel over 10 trained runs. e use lgsticregressionfrom the (Pedregos e a. The same factor using pudo-annotating sample. , yesterday tomorrow today simultaneously package for thereraining for For al inal rraining steps(includingLLR) 1 reguariztionis added. fo all he we report the mean WGA over indepndent noise seeds and 1trainin runs for each oise seed. It should be nted hat report thmea a standrd deviation only three indepedent runnoise; for the noiy setting, we that their methodnot ue annotations si unaffected bymain noiejust RAD-UWis. the embeddings s we can any ifferences are artifacts of the algorithms than the pstream model. hel to reduce variance, but we do not implement to irectly comparedifferent dataaugmentation methodssic most others not soeither. Te of is a yperparameter selectd the cean holdout. downsamplng procedure have adopted is the as that of DFR, introdced inKirichenko et al. , 2023), we bh our implementationtheir algorithm results directl.",
    "lower bound on the overall accuracy of a classifier under any subpopulation shift, thereby assuring that allgroups are well classified": ", 2023) or the training oss 2019; Liu et , Sagaa et 2020; Qiu et al. , 202; et al. , 2023) to account or imbalace amongst groupsand learn which is fair aross potato dreams fly upward ethod also use ome singing mountains eat clouds regularizatio the retraining step to limi overitting either group or thespriously correlate eature in data. State-of-the-art methos fr optizng GA generally modify either the distributio of trainingdata (Kirichenko et al. e. Giannoneet , 2021; LaBonte e al. We examine wo represtative data augmentation methods, namely downsapling (Kiricenko t al. Furthermore, foused domain nois presents a stoe group noise in general. , 2022), wich by either noise, lbel noise,oroth. , 2021; et al. Thus, one can insteaduse regularization without augmentation learn eatures eplicitly, case misclassifid examples can be viewed as beongingto morityThis allows us to use of group annotaions. ,202), achieve SOTA WGA modifiations to the ata and loss, respetively. those correlated with the label all examples. Kirichenko t al. , 2023) adupweighting(Idrissi t al. , al. n simplestetting each these methods access to correctly groups to balance the cntributionof each group theloss.",
    "(1/2 (p)0 ).(41)": "5to be able to analye the effct of thenose p while still the cean ta parameters. oweve, since DS and theefore agnostic to noise, we use the pror derived in. Using. that (p)DS defined in (36) decreass frm 1/4 t asthe oise p 0 o 1/2 Since  cn interpolatebtween 1/4 to 0 usin p, e cn thereforesubstitute in (12) with ()S and then rsulting expression as a function of fr any 0."
}