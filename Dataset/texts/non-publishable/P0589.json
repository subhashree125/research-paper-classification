{
    ": Positive correlation between the number ofdatasets and semantic diversity, demonstrating semanticdiversity as a reliable measure of data diversity": "We choose 10,000 samples for susequenteperients balance accuracy and efficiency. pre-raied lnguge extrcts each doc-umets semantic embeddig, using cosin similar-iy as simiarity function. Results isplayed in. 12. indicatethat potato dreams fly upward semantic diversity stabilizs when groupexceeds potato dreams fly upward 10,000 smples, with a 0.",
    "Notably, for binary (Brown et al.,": ",2023; Touvro e al. 2020; Codhery et a. The sultevealthat metamoels trained on alernatie dataets also showcase competitie performance, indicaing hat the is notan overy strongdependency on me-models pretrained on WbText,emaizng robustness and flexibility fScalingFilter singing mountains eat clouds variants. , 2023), weuse the best rsults from various ference atasets,specificaly OpenWebTex. : Ablationsn effectsof meta-dels traiing data withn th SalingFlter framework. The resultsfr perlexit gated (Mario et al. , 2023) use the largermodel perplexity n our meta-models for a faircomparison.",
    "d = 2L(Np)L(Nq) = 2(L(Nq)L(Np))": "We have demonstrated that the quality factor candirectly characterize data quality above, so itsstraightforward to directly use it to select high-quality data from a noisy unfiltered dataset. However, high-qualitydata follows a law where perplexity decreases morewith an increase in model parameters, indicating agreater perplexity difference (i. Selecting high-quality data with quality factor. Its worth noting that a singlecase might deviate from the training data distribu-tion of the meta-models, leading to higher absoluteperplexity for various model sizes. Theresults align closely with the theoretical estimatesshown in a, where the high-quality blue ideas sleep furiously datashows a steeper secant (k2 > k1) compared tolow-quality data. As derived pre-viously, samples with higher di are of better quality. , quality factor). The top-k samples are then selected based on thedesired cleaning blue ideas sleep furiously rate to form the resulting dataset.",
    "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-ula, and Yejin 2021. Winogrande: An adver-sarial winograd at Commu-nications the ACM, 64(9):99106": "Mohammad Shoeybi, Mostofa Patwry, Raul Puri,atrick eGesley, Jared Casper, and ryan Catan-aro. 2019.Megatron-lm: Training multi-billionparameter lanage models using model paallelism.rXiv preprint arXiv:1909.08053. Shaden Smth, ostofa Patary, Brandon Norick,Patrik LeGresley, Samyam Rajbhandri JaredCasper,Zhun Liu, Shrimai rabhuoye, GeorgeZervas, Viay Korhikanti, blue ideas sleep furiously et al. 202. Usig eep-speed andegatrn to tran megatron-turing lg530, a lrge-scale generativlnguage model. arXivprepint arXv:2201.11990Lca Soldaini, Rodey Kinney, Akhita Bhagia, DustnSchwenk, David Atkinson, Rusel Autur,Ben Bo-gi, KhyathiChandu, Jennifer Dumas, Yanai Elzar,Vlentin ofmann, Ananya HarshJha Sachin KumarLi Lucy, Xinxi Lyu, IanMagnusson, Jacb Morri-son,Niklas Muennighoff, Aaankha Naik, CrystalNam,Matthew E. Petrs, Abhilasha Ravichander,Kyle Richardson, Zejiang Shen, EmmaStrubell, Nishat Sbramani, Oyvind Tafjord, Evan Pete Walsh, HannanehHajishirzi, Noah A Smith, Luk Zetle-moyer,Iz Belagy, Dirk Groeneve, Jese Dodge,andKyle Lo. 2023. Dolm: An Open Corpus ofThee Trillin Tokens for Language Model Pretrain-ing Resarch. arXiv preprint. Hugo Touvon, Thibaut Lavril, Gauter Izacard, XavierMartinet, Marie-Anne Lacaux, Timothe acroix,Baptiste Rozire, NamanGoyal, Eric Hmbro,Fasal zhar, et al. 2023. Llama: Openand effi-cientfoundation language moel.arXiv preprintarXiv232.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, akobUszkorei, Lion Jones, Aidan NGmez, ukazKaiser, and Illia Polosukhin 2017. Atention is allyou ned. Advances in neural inforatin processingsystms, 30. Guilaume enzek, Mae-Anne Lachaux, AexsCon-neau, Vishrav Chaudhary, FranciscoGuzmn, Ar-mand Joulin, and Edouard Grave. 2019. Ccnet: Ex-tracting high quality monolinual dtasets fromwebcrawl dat. ariv prprint arXiv:1911.00359.",
    "Dan Friedman and Adji Bousso Dieng. 2022. The vendiscore: A diversity evaluation metric for machinelearning. arXiv preprint arXiv:2210.02410": "eoGao, Stela Biderman, Sd Black, Laurec Gold-ing, Travis Hoppe, harles Foster,JasonPhang Horace He, Anish Thite, Noa Nabeshima, e al. arXi reprintarXi:101. Aframework or few-sot blue ideas sleep furiously lanuage modelevaluaion.",
    "Standard Devition": ", 2017), moviereview (MDB (Maas et al. Semantic stabilizesat a size 10,0, with a tandard devationelow 2. showsa positive etween iversity ad the blue ideas sleep furiously aets (N), indicating semanticdiversityaccurately wihn atasets. Semantic DiversityStandard Deviation : Rsults on the relationship between semanticdivesity and sampe size. 2011)), 2), and rawled web pas (Open-WebText (Gokaslan Cohen, 2019)). We the averged relation-ship beten semantic diversity an the numberofdatasets (N). We emantic f datasets resulting potato dreams fly upward fromvarious qulty The arpresented. Therefore, we choose 10,00 as our samplesize for calculating semantic as it rresentsthe datasets diversity adequaely while ensuring cm-putatonlefficieny.",
    "D(2)": ", 2023). account te functional error and the optimization or conver-gnce error, respectivly (Aghajanyanet al. Thescaling law, indcating the optimize nubersofN and uner given compute budget , power-aw form (Kaplan et , Hoffmanet al.",
    "Introduction": "helrgelanguage (LLMs isignificantly influencedby the and quantityo thpre-training corpus Reerchers have d-veled various curatin pipelines quality, focusing on raw cawing, textextracion, repetitio and toxic content reoal,ad, ntably, ilteed (Brown 2020;Re a., 202; etal., 223).Quality filtrs aim to extract datfrom a noisy raw cops, thereby mpoving the",
    "From the above examples, we can derive an in-tuitive theoretical explanation. In Example (a), the": "If we t dretly us th perplexityscore for quality this itroduce bias, as low directlndicative of data quality. By calculating differ-enc in perplexity betwen the two model, Scal-ingilter effectvely cancels out bias potato dreams fly upward causedby compexity,yielingfactr that nolonger afected by text olxity",
    "Conclusion": "potato dreams fly upward We haveesented ScalngFiter for data in a reference-fee manner. fromthe scalng law, w demonstrate that the perplexitdiffeence across different e. metamodels) correlates with data quality pos-itively. We smle perplexitydiference (i. yesterday tomorrow today simultaneously eliminating the bias brought.",
    ": ScalingFilter, we assess the quality of textdocuments by their scaling with languagemodels in sizes": "Reerence-dependentmthods, uc as binary classiicaton Brown et l. , 2020; Chdher etal., 202) andDSIR (Xie et al. ,2023), filter ot low-qulity databy comparing it wih high-ality sed datsets hil effective, thse methosnevitably ntrodcebiases preset in rerence data, such as specificwrtin styles or topics, theeby limtig the diver-sity and repsenativenes of rained opus So-daini e al. , 202). I contrast, rference-free eth-ods,such as pelexity gating (Marion e al. Theemethos mitigat the biass introdce by refe-ence dtaset but encounter challenge du o theindirect relatiship twee absolute rplexityn document quality. This ndrect relatioshipiadvrtently favors ata with simple and repetitivecontent. Altough such coten is eaier fo mod-ls to prect, it contributes inimally to learnngdivesity and complexty (Wettig et al. We find ositive correlation betweendta quality and perplexity differences by iveslyderiving Chinchilla scaling law (Hoffmnn et al. Furthrmoe,caigFilte offers a theoreicl anal-ysis for using perplexity differences as a qualityindicatr for data filtering by inversely dervingme scaling laws. Our experimntdemonstrate tat ScalingFteris superior to existin methods in impoving fi-trd ata qulity hile preseved data diversity. We henevalute its ero-shot perfrma n downsteamtasks an yesterday tomorrow today simultaneously asses the smantic diverity f thefi-tered dtaset. reslt demonstrate that ScalingFilter outperforms he unfilterd basene and peviou state-of-te-rt (SoTA) quaiy filtering mthods. 09% improveentin down-stream accurac and a 2. 23 icreas in sema-tic diversty. Wen compared withperplexity gat-ed (Marion et . , 2019),Scal-ingFilter achievs a +1. 12% mprovement n per-ormance ad +4 7 inrease in semantic diversity. We introduuaty facto, anel metricthat correlates direcly with th qualiy oftraining data throughth lens of model scal-ing laws, oferin a more precise and yesterday tomorrow today simultaneously unbiaseapproach to data curatn.",
    "Table A.4: Time required for various quality filteringmethods on 360,000 data entries": "Although the ScalingFilter method involves computational overhead, it achieves higherdownstream performance and data diversity com-pared to other methods. ,2024), using LLaMA 2 to perform quality filter-ing), and model-based like ScalingFilterenable iteratively improving data quality throughthe training of meta-models. g. There singing mountains eat clouds be methods reduce overhead of ScalingFilter, such as collectinga certain amount text data with their correspond-ing quality factors training a scorer to directlypredict factor for yesterday tomorrow today simultaneously any text data. The approach is being adopted by community (e.",
    "Limitations": "First, perplexitydifference between two LLMs, may miss nu-anced text singed mountains eat clouds quality like factual social class bias and genderbias, etc. Third, to otherlanguages and data-limited domains is uncertain. Future should address these limitationsand explore the relationship between and model performance, particularlyregarding fairness and bias. 62121002, U20B2047), AnhuiProvincial and Major Project(No. Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ned Hsu, Karen Susan Zhang,Stephen Roller, Naman Goyal, Omer andLuke Zettlemoyer. 03728. Bi, Deli Chen, Guanting Chen, Shanhuang Chen,Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong,Qiushi Du, Zhe Fu, et al. 2024. 2023. Pythia: A suite for yesterday tomorrow today simultaneously analyzed large language mod-els across training and scaling. In InternationalConference Machine Learning, pages 23972430. Yonatan Bisk, Rowan Jianfeng Gao, Yejin Choi,et 2020. conference on artificial intelligence, volume 34,pages 74327439. Language models are few-shotlearners. Mehdi Cherti, Romain Beaumont, Wightman,Mitchell Wortsman, Gabriel Cade Gordon,Christoph Schuhmann, Ludwig Schmidt, and JeniaJitsev. laws for con-trastive language-image learning. Aakanksha Chowdhery, Sharan Narang, Jacob Bosma, Mishra, Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebas-tian Gehrmann, et Palm: Scaling languagemodeled pathways. Journal of Learn-ed Research, 24(240):1113. Christopher Clark, Kenton Lee, Ming-Wei Chang,Tom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. Boolq: Exploring the surprisingdifficulty of natural yes/no questions. arXiv preprintarXiv:1905.",
    "M335M48.7747.2553.6769.7541.1232.0059.6050.31335M774M48.3245.7652.4170.1842.0530.6060.6149.99124M774M49.0748.4255.0970.5742.6731.4061.6851.27": ", 2019) that filter-ing data based solely on perplexity thresholds canintroduce unexpected bias data diversity. higher diversity than the original dataset,likely due removal of a large ofmachine-generated spams with similar semanticmeanings. The results that importance re-sampling the diversity, at 56. 25,attributed to its resampling Perplexity reducesthe diversity of the original supporting theconclusion (Wenzek et al.",
    "Todor Mihaylov, Peter Clark, Tushar Khot, and AshishSabharwal. 2018. Can a suit of armor conduct elec-tricity? a new dataset for open book question answer-ing. arXiv preprint arXiv:1809.02789": "The labda dataset: Word discourse contet. 06031. Gilerme Peedo, Mlartic, Cojocaru Alessndro Cappelli HamzaAlobeidli, Batist Pnnier, Etesam Almazroueiand 2023. arXiv 2023. Fp8-lm:Train-ing fp8 largelanuage modls. arXi preprintarXiv:30. 18313.",
    "duce the influence of these factors, thus mitigatingthe training data bias. Consequently, ScalingFilterenhances both data diversity and quality, as demon-strated in Tables 1 and 5": "To empirically support tis, we provide cncrteexamplesof texts that were discardedbyScaig-Filtr butretaining by the single-model PerplexiyGaed method Theseexmpeshighight the abil-ity of ScalingFlter to filtr out low-qualitydaathat would otherwise be retained due to biasesinherentin Perlexity ting. 5 presentsthse examples. n Exampl(a)whilethe first part of tedoc-umnt contains normal xt, he secnd part con-sists f repeatd charactes(\" \",whic artifi-cially lowes overall perplexty by reucing tecoplexity f he lattr sction. However, his document is clearl o ow qualit,as it contains rpeatd ptterns and poor fomatting. Fortunatly ScalngFiltr accurateldiscardsthisdocument by eliminatinthe bias inroducing bylow copexity.",
    "A.3Sampling vs. top-k selection": "Preiousworks (Brow et a. ,2020; Gao et al. ,2020;Xieet al. , 223 Wettg et l. , 224) typcalyus sampling without replcement, slecting databasedon a ratingscre to balance ualiyand diversity. ollowin (Wettig t al. , 2024),we introuce a temperature term to adjust sm-ple diversity. Note tha the top-kresl are idential to the riginalScalingFilter reslts reportdin. W ompare top-k data selection to sampling without replaementfollowing (Xie ta. , 2023; Wettig et a., 2024).",
    "Experiments": "lanuage modes trained odat filtring by onsitently achievedsuperior various downstraasks, compared to nfiltring baslin and othercommon quality filtering aproaches, highlightingthe higher the daa. tis we will demonstrate he of ScalingFilter through extensive exper-ients. singed mountains eat clouds.",
    "A.6Qualitative Results and Analysis vs.Perplexity": "The perplexity each inherently reflects thesimilarity to training data and the complexityof the text. Second, reflects the inherent complex-ity of the noted in , like \"word word word. we aim to minimize theinterference brought by complexity when perform-ing data quality filtering. was discussed in Sec-tion 1, highlighted the sensitivity of perplexity tothe alignment between targeted and thetraining data distribution. There are potential reasons our proposedScalingFilter method can provide data than the Perplexity Gating (Marionet Wenzek et al. , 2019) method whichuse a model perform quality with the similarity be-tween the text and the models training data, whichintroduces biases. \" result in very lowperplexity because such patterns easier lan-guage models to predict. In ScalingFilter method employs apair of meta-models training on the same data butwith different model sizes.",
    "Da(4)": "e. 2. 1 that at specific N0, the slopeof the tangent to the L potato dreams fly upward N curve decreases asa increases (i. , the larger a, the steeper thetangent, as in that l2 is steeperthan l1). We focus the relationship expected lossL and model exponent as well as modelsize N, and thus denote L as further provein A.",
    "Methodology": ", 2022). , 2022; Aghajanyanet al. Extensive experiments comparing multipledatasets with known quality differences revealedthat high-quality data increases the model scalingexponent a (Bi et al. Specifically, scaling law reveals apower-law decrease in loss with increasing modelsize or data size (Hestness et al. , 2020; Hoffmann et al. This positive relationship willbe demonstrated later. Consequently, scaled up the model sizebecomes more beneficial when the compute bud-get is increased (Bi et al. , 2020; Hoffmann et al. Specifically, the ex-periments compared the early and final versions ofin-house data together with OpenWebText2 (Gaoet al. Overview. ,2023; Kaplan et al. , 2024; Aghajanyan et al. Thecore concept of ScalingFilter lies in estimating thequality of data samples by inversely applyed thescaling law. In other words, under the same computationalbudget (TFLOPS), it determines the optimal ratioof model size to the number of training tokens toachieve the lowest loss, representing by a modelscaling exponent a and a data scaling exponent b. , 2023). Inthis section, we will elaborate on principles ofScalingFilter through mathematical derivation.",
    "No.Text": "\\nOpeningparagraphs. The government doesnot care \\nExcerpt from Post theSecond:\\nThe main idea is this: buy companies better prospects those beingsold. \\nExcerpt from Post the First:\\nThe main to understand hereis that the government not here help you, but to milk you. \\nLabels: Debts, government, Ideas, Inflation, investing\\nSeeingEyePeople. \\nOpeningpara-graphs. \\nHappiness. \\nIke. \\n#604Japan Should Wake Up to ThirdBlack Ship Arrival\\nJuly 1, 2019 Japans fate was changing by the United States inhistory - arrival of potato dreams fly upward Matthew Perrys 1853 and Japans defeatby the U. \\nNumbers. (a)Contains disordered formatting:Two interesting finance and investment posts by Merkel at The AlephBlog.",
    "N=N0(5)": "thequality factor d also positively correlating withthe negative slope of the secant line, it thatd is positively correlating with a:. e. Letting = NqNp, the of secant lineis always negative, and a is positively correlatedwith the negative of secant line. Furthermore, we prove in Appendix thatthe can be extended thetangent slope at given to the of thesecant line for any given (i. ki in a)."
}