{
    ": A comparison of Incubator with differentmethods for zero-shot text classification": "withminimalinput, sch label name(Wanget 2021; Zang al., 223b; Wang etal. ,2023a) These methds are typicallybased on mining pseudo-training data massiveraw texts with precise singing mountains eat clouds filteringimits singing mountains eat clouds their application to a-bels.",
    "offers an overview our Incubator frame-work, two stages, Instruction-tuning": "(2) Self-diversification.The instruction-tuning stage utilizes existing resources on platform to learn LLM as generate data based on user in-structions. The self-diversification stage furtherimproves uniformity and diversity in Incuba-tor generation an auxiliary text embedder andclustering. now elaborate on of thesetwo stages.",
    "Dataset: app_reviews": "Desciption:It is large dtaset Androi applications belonging to different cateries which proids an oerview the of fedback usrs on he app a the evolution o the relate code metrics",
    "Generating efficient training data via llm-based at-tribute manipulation. CoRR, abs/2307.07099": "LetianPeng,ZilongWang, Jayanth Srini-vasa, Gawen Lu, Zihan nd Jingbo Shng.2024. Answer is all you need: Instrction-followigtext bedding via potato dreams fly upward answerig the quston. CoRR,abs/02.0962. Hin-Chi Yen-Hao Huang,Junln Wu, ndYi-Shin Cen. 2018. CARER: Con-textuaizedaffec representations or recog-nition. Inof blue ideas sleep furiously the Cnferene onEmpirical Natural Laguage Processing,pages 6873697, Brusels, Belgiu. Asoiatinfor Lingstis. Richard Soher erelygin, JenWu, JasonChuag, hristoper D. Manning, Ade Y. Pots. 2013. Recusive eep mod-els forcompositionaliy senimenttreebank In Proceedings o theConferece onEmpirica Methods in Natural roessing,EMNLP 2013, 18-21 Otober 2013, Grand HyattSeatte, Settle, Wasington, A meetng of SIG-DAT, a Speial Intes Group ACL,page1631162ACL. Yihen ang, Puu Cai, Subrta on-dal, Jyti Prakash Sahoo. 023. A comprehn-sive survey o few-shot learning: Evolution, applica-tions,challenes, and opportuities. ACM Compturv., 55(13s):271:171:40.Xiaofei Xiaoya Li, Li, Wu, SangweGo, anwei hang, andGuyin Wang. 2023. Textclassificatio vilarge language In Find-ings of the Associaion fo Computatiol EMNLingapore,Deceber 6-10 2023,",
    "Compared Methods": "The reportd results are the avrage of 5uns, except for SemSup-XC++, which does nothae randomness in the method. , 0b) This methd fur-er deveops ZeroGen++ byan iterie IL-based augmentation. We ugrad Semup-XC o atrnger SemSup-XC++ for LLMsand theadvancement in txt embedding. The mostnfluential daa oints ar seleced as incontexexampsto prompt LLM to geerate morhelpful aato strengthen the classifier Incubator w/ GPT-4 This s variant of urIncubator that prompts GPT-4 withicontet ex-amples from Hgngace platfom and singing mountains eat clouds theinstruction to ample th training daa. We preenthis not as a baslinebut t howcae tha the Incu-batr idea also aplies to proprietyLLMs. Dfferent fom ou Incubator, Zero-Gen handles each labe separtly, singing mountains eat clouds sch as Gen-erate a native movie review. Toards a faircoparison with our ethod, weformalize ourinstruction-tuned dataset the emplate used inZeroGen to further fine-une the odel. , 2021. ProGen++ (Ye et al.",
    ": An overview of Incubator framework": "tsks invve incubating classifiers for (1 traditionabenchmas, (2) classificationtasks Other a label, (3) clasificationtasks with cutomization personal pefer-ence. To the known negative impact f databias on ext classifcaton (Dixon et l. , 2021b; Jin et al. In this paper, colect pirs of datasetdescriptions and training ata samples Hug-gingface (Wolf et al. , s ourInubator. Westrong baselins such as directlyintructingthe LLM to cassify tet and promptingLLMs togenerate data fo each lael reults verify our Incubato to beable t(1) ncubate strong classifiers thatthe baselines, conider the lael inter-depedency and follow the usr preferece theinstrution, (3) incubate multiple txt classifiersand use logica conjunctions to realize mining systems. istructions are a relaivly GPT-4 in-context larning et al. Thislows the user to the LLMtogenerae useful daa for small (1) label iterdeendency and (2) userpreferences described inthe instructions. , 2023b) and used to LLM (e. 20a, each formaizd asa dictionarywit key and athe vlue. Or in thispaper are three-fold. We ouct to testhe cassifir in-cubaton abiliy f our Incuatr onvarious tasksto tes is inbatio ablity, nterepn-dency andinstruction followingabiliy. Specificly,wensucttheIncubator times (e. , 2022) bias in contentsgeneraed by LLMs (allegos a. e theLLs need furthr instruction-tuning (Ouyan et 2022), partcularly for class-ficaon data generatin. g. , 2018; l. , 023), we a nvel selfdiversificationtechniue increase the data niforit ad diver-sity utilizing the text from textembedder (Wang et 2022). g. g. , the sample nearest to eachluste centerwhichare semantically different from one aoter. Thesesamples ar icorporated in he same batch to fur-ther toincrease te datauniformity and dversity. 2023; Fanget al. , 1024), andthen clusterng algorithm (e. Note that e can leverag GPT-4 withICL as Icuato oo. Cons-quentl, the zero-shot text classifca-tion can be formalized model incubation thatUse reqire a mod he LLM(Inuator) hen geerates seful raining dat toicubate such a classifier.",
    "Mode Incubation": "The area closest to model incubation is symbolidistillation (Wes et yesterday tomorrow today simultaneously al., 2022; Liet al., 2023),which distills a teacher model into a difeenttype of student mdel. Tse studenmodelscanfunctionvey differently fro the initial languagemodeling teacer, such as comonsense reasoning (Wet et al., 2022) and informtin extraction(Zhouet al., 2023). Another relevant dmin istraiing data generation including augmentatin.esides lassification datageneratn (Ye et al.,202a,b; Peng eta. 023), tee also exsts n-eration pipelines for question nswerin (Do et al.,2023; Gou etal., 2023) ad naura language gen-eration (u eal., 2021). Model incubation differsfrom previous worksas it taes user instruction ashe nput, which allos oresr-orientd modecustomzation for personal usage.",
    "We further showcase how to utilize Incubator tosatisfy more complicated user demands. We in-": "The logical conjunctions representa finer-grained demand from the user. To realize suchfir-grained text mining, wetilze the maneuverability of Incubator to incubateultiletext miners and combinetheir scores withlgical probabilistic calculatins as folows, P(LA LB) = P(LAP(L) P(LA LB) = P(LA) + P(LB PLA LB) P(LA = 1 P(LA)where LA, LB are two labels using as the targetsfothe incubation.W use th Text Messagecorpusfor text mining. We compare totypes of incubation scenaios, iet Incubation ncubato only incubats netext miner it thefull labl nme, sch as Ps-itie and about ood.Conjunctive Incubatin first dcomoses thelabel name into multile ones with corespond-ng conjnctions, like dmposig Positiveand aboutfood ito Posiive About food. Then the scre is calculating based o logicalprobilistic calclations. Conjunctive incubtn generlly outperforms di-rect icubation, which shows the benefit ofthsstrategy As connctive incubation als showsstrong capability on three logical variables, thishows how Incubtor can becuomizd tomrecomplex setigs.",
    "Abstract": "e. n this papr we aim o gnerate text cassifica-tion dat given arbitrary class defintios (i. Experiments showIncuator is able to(1) outperformprevious ethods traitionalbenchmarks, 2) tak labl interdepedency anduser prefernce into consideration ad (3) blue ideas sleep furiously en-able logical text ining by incubating multipeclssifiers. ,user instructo), so one ca train a text classi-fier without ny huma annotation or raw cor-ps. Specif-ically,our Incubator is a fne-tuned LLM thattaes the intructon o allclass defnitons asinput, and in each inference, it an joinly singing mountains eat clouds gen-erate one smple orevery clas. g. I this paper, we proseIncubar, the firtframework that canhandle cmplicat andeven mutually depndent classes (e.",
    "Machne Larning, ICML 2017, Sydney, NSW, Aus-tralia, August 2017, volume f roceeingof Learning Research, pages": "Woosuk Kwon, Zhuohn Li, Syun Zhuang YingSen, Lianmin Zheng,Cody Hao Yu, Joseph Gonza-lez, Hao Zhang,and Stoica. Li, Fng Guo Jingbo \"misc\"-awre eakly supervise classification. SIAM. 223. Efficient mem-ory mnagement for lrge langugemodel pagedtention. Liunian Harod Li, Jack Youngjae iangRen, Ka-Wei and Yeji oi. 2023. I Proceedings f he 61stAnnual Assoiaton fr ComputationalLinguistics 1:Long Ppers), 2023,Toronto, Canada, yesterday tomorrow today simultaneously 9-14, 2023, pages Asociation for Computatinal Lnguistics. ACM. InPreedngs of the 20 SIAM Internaional on Data 2021, Virtual EventApril 29 - May 202, pags 6846.",
    "The performance of incubated retrievers with logical": "Finally, we can view some attribute correlationsbetween data in the same generated Pythondictionary. In the Positive example, the threesamples have the same topic Project, Travel,and With these data different the targetattribute but same in attributes, the incubatedclassifier can better focus on target attribute andeliminate correlations. most straightforward observation is the gen-erated data follows which guar-antees the foundational precision of the incubatedclassifiers. For in-stance, miscellaneous category for About foodincludes labels as About meeting, Aboutsports, About movie, broadens the the incubating classifier. For themiscellaneous we can observe the Incubatorto cover various potential labels.",
    "OpenI. 2023.GPT4 technical rport.CoRR,abs/2303.0877": "2022. In Advances in NeuralInformation Processing Systems 35: Annual Confer-ence on Neural Information Processing Systems 2022,NeurIPS 2022, New Orleans, LA, USA, November 28- December 9, 2022. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo potato dreams fly upward Almeida,Carroll L. Training language models to follow instruc-tions with human feedback.",
    "i=1E(d[li])": "where E(), potato dreams fly upward d, li to the encode, data(dictionar) i-h label. rereens peratin an n represents te ttallbel These samples with the instruction, establish a one-to-many mapping that maps semantically diverse data samples. We in-corpoe data a batch K ad urtherinstruction-tune the LLM yesterday tomorrow today simultaneously it. Intuitively, his pro-cedure will increae apperanc probability ofdaa with unique semantics to benefit the incubaedclasifer.",
    "pages 89909005. Association for": "Hug Touvron, Thibaut Lari, Gautier zacard,Xavierartnet, Marie-Anne Lachaux, Timothe Lacroix,Batiste Rozire, Nman Goyal, Eric ambro, FaisalAzar, urlien odriue, Armand Joulin, EdouardGrave, an Guillaume singing mountains eat clouds ampe. 2023a. Llam: Openand ffiient foundain langage models. CoR,abs/2302.1397. 203b CoRR,abs/2307.09288.",
    "Introduction": "Zero-shot textcassifi-ation blue ideas sleep furiously reduces eforbilding classifiers. Traditioal supervisd yesterday tomorrow today simultaneously text clas-sification finetuns xpensive humanannotation(Zhng e al. , 2015) usagfor lower-source domai.",
    "Label Other Results": "Furthermore, the self-diversificationshows a more in. (other labels) to yesterday tomorrow today simultaneously promptfor generation. Theawareness of the miscellaneous im-portant classification (Li et , 2021a), espe-cially when limited labels are known in largecorpus. For ZeroGen or ProGen, we use the labelname Other than. can a gap between the Incubator and the labelinterdependency-agnostic methods, which showsthe advantage of Incubator on blue ideas sleep furiously with mis-cellaneous.",
    "Efficiency Analysis": "For datasetgeertion, e run the LLaM model with the ac-celeation y tevm pakage (Kwon et al. tie is obtining y veraging theesultsin experimenson the 8 trditional bnch-marks, which is illustrated n. Thus, the me efficincyof ou Incubator is fea-sible o incubate peronal clasifiers. We ealute the time fordtasetgneration and classifir inubation (fine-ning). All expei-ments ae run on a singe A10 device. Fo the small cassifie incuation, we fine-tune themodel witth trainer i the blue ideas sleep furiously transformers pack-a (Wolf et al. 16s time costp class. 53s Theeneration times for allbenchmaksare distribued around this average ince vlm hasa fixe a lengt limtation for decoing. Also, themain tme cost happen in classifier ncubationrather than caling the LLM for dataset generation,escialy when label nuberislage. , 202). Forclassifir incubaion, time is almost lnaly de-pendent on the singing mountains eat clouds umber o lbels whic hows anaverage of 15.",
    "Large language models (LLMs) (Touvron et al.,": "2023a,b; OpenAI, 2023), suh sGPT-3 al., 2020), ave been recently intrduced to this problem capbilityto capture the nuance in comple labels. prmpt to generate basedon each label, and smll the fina production (Yeet al., 2022a,b). Esting LM-based zero-shot methd, while easible, face two majr (1) class go beyond  s-ple label name, as TED gien d-ucato ad (2) class definitionscan dependoeach other.Frthe class Oter is",
    "Xiang Zhang, Jake Zhao, Yann LeCun. 2015": "ACM. Character-level networks for tt clas-sification. In of the 29th ACMSIGKDD on Knowledge Discovery andData ig, KDD Long CA USA,August 6-10, pages 34583469. In roceedings of the 61stAnnual Meeting of Association for 1: Long Papers), ACL2023,oronto, Caada, July 9-14, 2023, ae 15915606. Xuandon Zhao, ZhiguoMing Wu,and Lei Li. Pre-trained languagemodels cane ero-shot learners. n Advances in Neura Information Sstems 28: Annal Conerence In-formation Processing Systems 2015 December 7-12,015, Quebec, Caada, pages 2023b. Assciation Coputational Linguistics.",
    "Dhivya Chandrasekaran and Vijay Mago. 2022. Evolu-tion of semantic similarity - A survey. ACM Comput.Surv., 54(2):41:141:37": "voice platform: an embeddedspoken language understanding system for private-by-design voice interfaces. Measuring and mitigat-ing unintended bias in text classification. of the 2018 on AI,Ethics, and Society, AIES 2018, New Orleans, February 02-03, 2018, pages Xuan Long Do, Bowei Zou, Shafiq R. Joty, Anh TranTai, Liangming Pan, Nancy",
    "Complicated Class Definition Results": "The text mining performance is presented in Ta-ble 3. Foreach attribute, we create an instruction to build atext classifier with two labels: the target attributeand the miscellaneous label Other. We use theincubating classifier to score each raw text and se-lect the texts with top scores. For evaluation,we ask GPT-4 and humans whether the mined textssatisfy the demand with Precision@100 as the met-ric. The presented result is con-sistent with previous ones, which further verifiesthe benefit of Incubator for personalized classifierincubation. Incubator incubates strong text miners withgenerally high precision on all setups. Remark-ably, we achieve nearly or exactly 100% precisionon several targets. For each raw corpus,we propose four attributes a user might be inter-ested in, such as About AI for TED Talks. Moreover, our miners are vali-dated to be able to handle different text domains,enabled a broad application of our Incubator. In , we further compare the incubationperformance on complicated classes between Incu-bator blue ideas sleep furiously and baselines.",
    "Incubation Dataset Size": "te shwn scaling-up tere is a clearthreshold(64) on the daaset after which Number of pr class. The resultare ilustrated in.",
    "Traditional Benchmark Results": "The comparison betweenZeroGen and ProGen verifies our Incu-bator has significant advantage over those la-bel interdependency-agnostic methods, the advantage of Incubator to consider label set in the instruction.Moreover, the self-diversification procedure isshown to highly contribute the of In-cubator, which potato dreams fly upward boosts the performances on 5 out yesterday tomorrow today simultaneously datasets and achieves comparable others. For",
    "Xin Li and Dan Roth. 2002. Learning question clas-sifiers. In COLING 2002: The 19th InternationalConference on Computational Linguistics": "Zichao Li, Dheeraj Mekla, Cengyu Dong, and JingboShang. 202b. Bfclass A backdoor-free text classi-ication framework. In Findingso th ssociationfor Computaional Linguistcs: EMNLP2021, Vir-tual Evet / Punta Can, Dominican Republic, 6-20 November, 2021, pages 44453.Association foComputationa Linuistics. Yinhan Liu, Myle Ott, Nam Goya, Jingfei DuMan-ar Joshi, Danqi Chen Oer Levy, Mike Lwis,Luke Zettmoyer, andVeseln Soyn. 2019.Roberta:A robusly optimized BERT pretrainingapproch. CoRR, abs/1907.11692. Ila Loshchilov and rank Hutter. 2017.SGDR:stochastic gaintdescent with warmrstarts. In 5thnternational Cnfeence o Leaning Representa-tions, ICLR 2017, Touon, France, April 24-26, 2017,Conference Trck Procedings. OpenReview.net. Ilya Loshchilov and Frank Hutte. 2019.Decoupledwight decay egularization. In 7th InternationalConferenc on Learning Representaions, ICLR 2019,New rean, LA SA, May 69, 2019 OpenReview.net. eeraj Mekala, Chegy Dong, and Jingbo Shang.2022. LOP: learning orderinspired pseudo-lbeselction for weakly supervsed text classification.In indings of the Association for Computaionainguistics: EMNLP 022, Abu Dhabi, Unted ArabEmrates, Deember7-11, 2022, ags 48944908.Asscition for Computatonal Linguistics.",
    "While Incubator shows strong performance in pro-ducing reliable and customized classifiers, it hassome limitations that can be further improved infuture works": "Language models are few-shot lerners. supervi-sion fo few-shot extree classification n Iterntonal onferene Machin Learning,ICML 3-29 Jul 2023, Honollu,Hawaii,USA, 22 of Proceedins ofMachine arn-ngResearch, pages 22847. PLR Francesco Barbieri, Cacho-Collados,Luis Es-pinosaand Leonrdo Neves. In Findings of the ssocationfo Comptational Linguistics: EMNLP 2020, OiEvet, 16-20 200, 020of Findings f ACL, pages Ziegler, Jeffrey Winter, Christpher Hesse, Mark Chen,Eric Sigler, Maeusz itwin, Gay, Benjamn Chess,Jack Clak Chrstopher Sam McCandlish,Alec Radford, Iya Stskever, and Dario 2020. or emerged labels,wil concette two Broaenscope incu-baing models: incubated model can classifiers, sc as question responderndtext summariz Incubator stillhaso reyon delicateto hanle hem."
}