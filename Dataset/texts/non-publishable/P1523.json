{
    "Ablation Study on CrossSDC and Memory Size": "In this subection, we conduct an ablation study to investigate the effectiveness o ou proposedCrossSDC. By removing single or multiple components of the CrossSDC, we evaluae h impact ofeach on the finalrelts. The reults of the ablation study are resented in Tb. 2.Moreover,we also discuss the effectof mmo size on our poposed ontAV-ep. In our maiexpriment, the defaultsetting f te memry ize is 1 sample perod class. The experintal rsults are shown in Tab. Observations from the tableinicte a ositive correlation between the size of thememor and the oerall performac metrics. As the memry sze increass thee is a discernible trend of improvemet in the result.",
    "A.1Model Architecture": "Teaudio-visual Tansformergenerate the separated audio feature with a dimension of 56, followed by a two-layers MLP toobtain the mask emeddng with a diension of 32. Finally, it output te audioembedding with a size of 32 256 256 through the decoder. We follow the implementation in , and use the Transformerdecoder architecture asour adio-visual Transformer. In or experiments, th audio network has 7 don-convolutons and 7 up-convolutions. Th audio-vsual Transformr consits of4eoder layers, with the first ayer beig the motio ross-attention layer and e following 3layrs performig audio cross-attentionandself-attenion operation.",
    "L( Mt, Mt1)": ": Overview our proposd ContAVSep, which consiss o anaudio-visualsound separatonae model an Output Mak Distillation, and ou prosed Cross-modal Constraint. Pease note that, the model Ft1 is frozenduring. (i)STFT stands fr (iverse) Fourier Transform.",
    "Introduction": "oversight tthe catastrophic forgetting issue , wher the fine-tuning of models o new classes detrimentallyimpacts their peformance prevously classes. oever, this capability in machnes remains due to the inhernt compleity f real-world auditory scenes. demonstratingthat their iQuery odel general to new classes well simple fine-tuning, still suffersfrom the forgettingproblem o old. g. Benefitng from achitectures (e. ,,Tranormer , and difusion models and discriminatie visual grounde motion , and dynamcgestures ), separation models abletseparate sounds ranging from domain-speific seech, musica instrument suds open-domaingneral within traiing soun ategories. Desite Chen et al. Recnt advances in deep leang have ld progress in audi-visual soundeparation. However, a of thse studies thir focuson blue ideas sleep furiously scenarios where alsound source classs presenl know, the potential nclusionof unknown sound source cles durng real-world applicains. Humans can effortlessly separate and identify individua sound sorcesin daily.",
    "where D denotes the training set, and L is the loss function between the prediction and ground-truth": "For two tasks Tt1 and and their corresponding training sound space Ct1 andCt2, we Ct1 =. with the memory/exemplar set Mt, all available datathat can be for training in Tt > 1) can be as Dt Dt Mt. t-th task (incremental step t), we have a training set Dt i(si, vi), yit}nti=1, where i and ntdenote the i-th video sample and the total number of samples in respectively, and yit is thecorresponding sound source class of video V i, where Ct is the training sound class label space of taskTt. Following previous works in continual learning ,for task Tt, where t 1, holding a small of memory/exemplar set Mt store some tasks is permitted in our setting. the trainingprocess of 2 in continual audio-visual sound separation setting can be denoted.",
    "Afouras, Son Chung, and Andrew Zisserman. The conversation: Deepaudio-visual speech enhancement. arXiv preprint arXiv:1804.04121, 2018": "Memory aware synapses: Learning (not) to In Proceedings of the Europeanconference on computer pages 139154,. Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte-laars.",
    "Da-Wei Zhou, Yuanhan Zhang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu.Learning without forgetting for vision-language models. arXiv preprint arXiv:2305.19270,2023": "etectingtwenty-thousand casses used image-evel superision. In blue ideas sleep furiously Proceedingsof the EE/CVF Wintr Conference onAppliations of Comuter Vision, pges12891299, 2022. Xingyi Zhou, Rohit Girdhar, Armand Joulin, Phiipp Khebhl, and Ishn Misa. Lingyu Zhu and Esa ahtu. In European Conferece o CompterVision, singing mountains eat clouds pages35036.",
    "Overall Loss Function": "te prevous ubsecton, we introduceour proposed CrossSDCconstrain. Note that e onlyfor yesterday tomorrow today simultaneously from the set, as representedby:.",
    "Abstract": "Code isavailable at:. ContAV-Sep presents a novel Cross-modal blue ideas sleep furiously Similarity DistillationConstraint (CrossSDC) to uphold the cross-modal semantic similarity through in-cremental tasks and retain previously acquired knowledge of semantic similarity inold models, mitigating the risk of catastrophic forgetting. This prob-lem is crucial for practical visually guided auditory perception as it can significantlyenhance the adaptability and robustness of audio-visual sound separation models,making them more applicable for real-world scenarios where encountering newsound sources is commonplace.",
    ", T, where ={, t 1},if V Mt,{t},f V Dt(8)": "1 t and = t potato dreams fly upward ). this knowledgedistilatin would beinteratedintothe ross-modal emantc similaity costraint or the curent which ensuresbette preseration f learned cross-modal semantic similarity revis taks.",
    "Management of Memory Set": "old class is lmited to amaxmum number singing mountains eat clouds exemlars.After completingtraining for adopt exmplar strategies in bandomlselectingexemplrs fr current class and potato dreams fly upward these new with the existing memory set.",
    "Haoxin Ma, Jiangyan Yi, Jianhua Tao, Ye Bai, Zhengkun Tian, and Chenglong Wang. Continuallearning for fake audio detection. arXiv preprint arXiv:2104.07286, 2021": "In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 77887798, 2023. Shentong Mo, Weiguo Pian, and Yapeng Tian. Bilateralmemory consolidation potato dreams fly upward for continual learning. Class-incremental grouping network for continualaudio-visual learning. In Findings of the Association forComputational Linguistics: EMNLP 2020, volume EMNLP 2020, pages 34613474, 2020. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1602616035, 2023. Divyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung Ju Hwang. Continual learning fornatural language generation in task-oriented dialog systems.",
    "that two crucial purposes (1) maintaining cross-modal similarity through incrementaltasks, and (2) preserving previous learned semantic similarity knowledge from tasks": "rossSDC presrves cross-modal semantic similarity from potato dreams fly upward two perspectives: blue ideas sleep furiously instance-aware semanticsimilarity an classaware semanic similarity. Both similarities are nforced by ntegrating contrastivelossnd knowledge dsillation.",
    "Problem Formulation": "The goal of the task is to utilize corresponding visual guidance v1 and v2 to predict the ratiomasks for reconstructing the two individual audio signals. Audio-Visual Sound Separation. Given twovideos V 1(s1, v1) and V 2(s2, v2), we can yesterday tomorrow today simultaneously obtain the input yesterday tomorrow today simultaneously mixed sound signal S by mixing two video sound signals s1 and s2, and then we can have ratio masks M 1 = s1/S and M 2 = s2/S1.",
    "Elyse S Sussman. Integration and segregation in auditory scene analysis. The Journal of theAcoustical Society of America, 117(3):12851298, 2005": "Ruben Tan, Ray, Anea Bryan A Plumer, Jstn Salamon,Nito, ryanRssel, and Ka Saenko. anguage-guided surce separation via trimodalonsistency. InProcedings of the IEEE/CVF Coferce on Computer and PatternRecognition, 10575105, Yu-Ming ang, Y-Xing Peng, and Zeng. Leaning imagine: Diveify memoryfor incemental learning sing data. te Conferece onCompuer Vision ad Pattern ecgnitionpages 5499558, 202. Yaeng Tian, H, nd Clic co-leaning of sounding visual groundingann Prcedings on Computer ision andPater Recognition, pages7452754 2021. Yaeng Tian, Jing Shi ochenLi, Zhiyao Duan, Chenliang X. Audio-visua ventlocalization inunconstried videos. In Proceedigs of the European conferene on computervion pags Zha Tong, Yibing Son, Je Wang, LminWang. Vieomae: Masked autoencodersaedata-efficient learers or slf-supevied video Advances in neural iformaionprocessing 2022. Eftymios Tzini, Wisdom, Arenhan Hesey, al Remez, aniel PWEllis,and R Hershe th wild with audioscope: Unupervised audio-visal separationofon-screen sounds. arXiv preprint arXiv:2011014, 200. fthymios Tzinis, Scott Wisdom, Remez, R Hershey. Audiocopev2: Audio-visualattentionrcitectures calibrate opn-doman sparation. EurpanCnerence on Computer ision, pages36385. 222. Fu-Yn Wang, Zhou, Ha-Ji Ye, and De-Chuan Zhan. FOTE: featureandcopression for class-incremental larning. In In Proceedings of t Confeence onomputer Visio (ECCV), pages 39844, 222. Yu Wang, J Bryan, Mark Cartwright, Jutin Salamon.Few-shot continual learninIn IEE InternationalConferenceon Acoustics, andSigal (ICASP), 32125. Zheei emSubakan,Junkai Wu, Eftymo Tzinis, Mirco andParisLerning represetaios for newith continua IEE Signal Procesing eters, 2:2607261, 202 Zhepi Wng, and Lauret Cntiuallearning oundclasses sing generative In IEEE Workshop on Signal Processing t and Acoustics (WASPA), 2019. Zifen Wan, Zizao Zang, Lee, an Zhang Ruoxi Sn, Xiaoqi Ren, u,incent Perot, enniferDy, Toms fster Leaning to rompt for learning. InProceedings of h Cnferene on Computer and Recognition,pages139149,2022.",
    "Experimental Setup": "To further validate the our ethod acrosssound domain, weconduct used he AVE and VGGSound the appendix. Notably, the model dos not yesterday tomorrow today simultaneously utilize motion modality. we split the selected 20 classes into eachof involves 5 total number availale vides is 040,and randomy split nto validation, andtesting sets with 84, 10, and 100 videos,respectively. Moreover, we compare our method the recently proposed adio-visual method AV-IL in whichwe adaptthe original clas-ncremenal version to he ofcontinual audio-isual sound separationby their tas-wise logits distillation te outputmask distillatio. baseine that invove sets, we ensue that each of them is allcated same number asour proposed method o fai comparison. we metioned typical continual learning g. , class-incremetal learing methods, hich yildprogressively increasig logit (or probabilitydistribution) across classes at each incremental step and design specific technique nthe classifier, e consider thatmetods are not optimal choice foror proposed eparation problem. Therefore, CrossSDC isapplied to Coparation,mod) Eq. Followig common practce , we conducted on MUSIC-21 which solo vidos of istrumen accordion, giar, clarinet,erhu, flute, sxophone, trumpet, tuba, vioin, xylophone, bagpipe, banjo, bassoon, congas, bass,guzheng, piano, and In our eperiments, we randomyselected oftem o costruct cotnual settig. We compaeproposed apprach with vanilla Fine-tuning strtegy, singing mountains eat clouds and continual learn-ing metds EW wF. Thus, that continual semantic segntationprlem has a more form comparing conventinal class-incremental learning, we alsoselet state-o-theart continl semantic segmentation methods PLOP and asur baselines. 7and 9is constrainedto (mod1, =(a, o). Furthr, we also the results of the Oracle/Upper Bound, whichmeans that using the data all sen classes to train th model.",
    "Upper Bound (with iQuery)10.3616.6414.68Upper Bound Co-Sep.)7.3014.3411.90": "Hnn window size of 1022 and the hop length of 256, to obtain the 512256Time-requencyrepresentati of each audio signal, followed by a re-ampling on the lofrequncy scae o generatete magnitude spectogram wthT, F = 56. Fortheimage encode andte ideonodr, we aply the self-supervised pre-trained CLIP andVideoMAE oyield he object feure and motion feature, respecively. For the audiovisalTansformer modul, we follow the design in. For all the baseline methods, we appythe saemodl arhitecturean modules with ours for hem, incluing thementioed Detic, CI, VideMAEaudio-vual Transformer, etc.Please nte that, duringour trained process, the pre-trainedDti,CLIP, and deoMAE are frzen. 1 ad0. potato dreams fly upward 3, respecvely. And e balaceweight dist. or teotput dstillation loss i set to 0. For memory se, weset the number of samles in each old clas to 1, so as other baselines tat involve the memory set. Alltheexeriment i this paper ae imlemented b Pytorch. Wetrain ou proposed method and albseies on a NIDIA RTX A000 GPU. In or experiments, we report SDR, SIR, and SAR of allthe mthods aftr training at lat increenalsteps, i. or all thesethreemetics, higher values dnote betterresults.",
    "Overview": "This new framework, illustrated in , consists of three key components:a separation model, an mask distillation module, and proposed Cross-modal Simi-larity Constraint (CrossSDC). For the video encoder and theimage inspired the generalization ability of recent self-supervised pre-trainedmodels, which has been proven effective and appropriate in continual learning we applytwo self-supervised pre-trained models and CLIP as the video and theimage encoder, respectively. that, during the training process, the object detector, video image encoder are frozen. To challenge forgetting in continual sound separation, weintroduce ContAV-Sep. We a recent state-of-the-art separator:iQuery as model approach, which contains a video encoder to extract globalmotion feature, an object detector image encoder to obtain the a U-Net for mixture encoding and separated sound decoding, and an audio-visual Transformer to getthe separating through multi-modal cross-attention mechanism and audioqueries.",
    ",(7)": "were1[i is inicator equals 1 when i=j, denotng hat video samples i and j arthe vido; The sim the cosine similariy functon withtmperatre salingThe modalities od1 and od2, wher (mod1, mod2) {(a, o), (a, m), (m,o)}, of features comare separated and bjetfeatures, sund and motinftures,nd motion and fatures.",
    "Soo-Whan hung, Soeon Choe Jon on Chung, an Hong-oo Kang. Facefilter: Auio-visual separation usin still aXiv preprint 22": "Artur Douillard, Yifu Chen, Arnaud Dapogny, and Mattie Plop: Learning witouforgetting for cntinal semant segmentation. In Proceedings the EEE/CVF conferece oncompte vision and recognitio, pages 4404050, 202. Arthur Doillard, Matthieu Cord blue ideas sleep furiously Ollio, Thoas Robet, and Vlle. singing mountains eat clouds Podnet:Poled output distillainfor incrementallarning.In VisionECCV2020: 1th lasgow, UK, gust 238, 2020,Part XX 16,pages 86102. Ehat, Inbar Mosseri, Oran Lag, Dekel, vinWilson, Avinaa Hassidi,Wiliam T Freean and Michael Loking to listen at the cocktail spaker-independent audio-visual model for speech preprint arXiv:1804.03619, 2018.EnricoFini, Victor GTrrisi DaCosta, Alameda-Pineda, Elisa Ricci, Karteek Alahari,and Julien Mairal. models are learns. Poceedings of theIEEE/CVF on mputer Vision and Patternpages 6219630, 202.",
    "A.3Experients heAVE and the VGGSound datasets": "5, in which our ContAV-Sep outperforms the terms of the SDR and SIR metrics, further demonstrated the robustness of our methodbeyond domain of musical sounds. However, it was that both our method and the upper bound relatively low SAR scores when compared the baselines. Gao al. the the dataset, follow and randomly selected classes continuallearning. These classes are divided into 4 tasks, each containing 25 classes. Given significantlylarger number samples per class in VGGSound we set the memory size to 20 samplesper class that utilize memory.",
    "Conclusion": "This paeropens ane direcion for rel-worl sound sepration Bader Ipact. Our proposed ontinual audio-visual sound allows the model t adpt tonew environments and fll whih cold and privacy byreducng need to and store senstive audio data. Acknowledgments. We thank the nonymous an areafr their valuable sgstionand comments. This was upported part b a Cisco aculty Rsearh Award n AmazonRsearhesearch gift froAde.",
    "MethodSDRSIRSARMethodSDRSIRSAR": "+ Fine-tuning1. 75iQuery + 3. 7810. 66Co-Sep. + 2. 369. 61iQuery + PLOP 3. 8210. 0610. 22Co-Sep. + PLOP 3. 59iQuery EWF 3. 989. 52Co-Sep. + 2. 617. w/ memoryw/ memoryiQuery + 6. 7612. 60Co-Sep. + LwF + EWC 6. 6513. 0111. 73Co-Sep. EWC 3. 319. 559. 0313. 3011. 90Co-Sep. )4. 0611.",
    "Huang,Susa Liang, Yapeng Tan, Kumar, Chenliang XuDavis: eparatnwit generative diffusion models. ariv arXiv:208.00122,2023": "Cmpacting, picking and growig for unforgetting cotinual learning. by knowledgedistllation with adaptive feature consolidation. Minsoo Kan Jaeyoo Park, Han. I of the IEEE/CVF onference onComputer Vision and Pattern cogniton, 160506059, 2022.",
    ": Testing results with different memory size (number of samples per class in the memory) onthe metrics of (a) SDR, (b) SIR, and (c) SAR at each incremental step": "This cod be reasonwhy the continual blue ideas sleep furiously learning methods do no perform well in our sond separainprolem. paird with data unseen nw classes tis is diffren romconventinal continuallearned tasks,where old classes notacquie ne knowldge n blue ideas sleep furiously asks. Additionallthe base model architectures using in ou and seline reqire deectorsto souding objecs.",
    "Jaewoo Lee, Jaehong Yoon, Wonjae Kim, Yunji Kim, and Sung Ju Hwang. Lifelong audio-videomasked autoencoder with forget-robust localized alignments. arXiv preprint arXiv:2310.08204,2023": "Learn to gro: Acontinual structure for ovrcoming catastrohic forgeing. InternationalConferenceon Machine pges. iok Kimin Le, Jinwoo Shn, ad Honglak Lee. In Poceedings of the IEEE/CV Conferene ision, singing mountains eat clouds pages 31231, 2019.",
    "Zixuan Yija Shao, HaoweiLin,atsuya Konshi,Gyua ad Continualearning of models. I Internationl Conferene Learning Representations (ICLR),2023": "Achieved a betterstability-plasticity trade-off potato dreams fly upward via auxiliary networks in continual learning. Proceedings yesterday tomorrow today simultaneously ofthe IEEE/CVF Conference on Computer Vision Pattern Recognition, pages 1193011939,2023. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Guillaume Desjardins,Andrei Rusu, Kieran Milan, John Agnieszka Grabska-Barwinska, et al. Overcoming forgetting in neural Proceedings National Academyof Sciences, 114(13):35213526, 2017. Saksham Singh Kushwaha.",
    "ModelModel": "The goal fthis task i to develop an audio-visualmodel tt can coninuously separatesound sources in new classes whilemaintaining erformance n previouslylearned classes. Thekeychalengewe need to address is caastrophic for-getting durin continual adio-visuallearning, which occur when the modelis updated solely withdata from neclasses or task, resulting in a signifi-cant performance drop on old ones. and Mo et al. To the bestofour knowledge, this i the first work on continual learning for audio-vsal soud separation. To evalatete effectiveness of our proposed ContAV-Sep, we conducedexperiment othe MUSIC-21 datasetwithin the fraework o contnual larning, usng the state-o-he-art audio-visual sound separationmodel iQuery an a representative audio-visual sound separation modelCo-eparatin ,s our separatio base models. Weillustrate unew task and the catas-trophic forgeting issue in. To address thesechallenges, in tispaper, we propose a novel aproach amed ContA-Sep(Continual Audio-Visual Sund Separation). In summar, this paper contribute follows: (i) To exlore morepractical audio-visual soun separation, in hich the separation model shouldbe generlized o new sound souce classes cntinualy, we pose a Continual Auio-Visual SoundSeparation task that trains he separation model under tesettig ocontinual earning. Upon the framwork, we introduce a nvel Cross-modalSimilarity Ditilltion Constraint (CrosSD) to not only maintain he cross-modal seantic similaritythrough ncrementa tsks but also preservepreviously learned knowledge of semantic similarityin oldmodels to countercatastrophic forgetting. : o: Illustrion of the continual audio-visualsound separaion task, where the modl (separator) learnsfr equential audio-visual sound sparaion tasks. The CrossSDC is a eneric constraint that can beeamlessly integrated into he training process of different audio-visualound separators. Very recently, Pian et al. Unlike typical continual learning pro-lems such as tas- domain-, or class-incremental clasificaton in visual do-mains , wich resultin proressively ncreasing logit (orprobability ditribution) across all ob-served classes at each incremenal step, our task uniquely produes fxed-size separatio masksthroughot all inremental steps. Fine-tuning: Directy ine-tuneteseparation model on newsound source classes; Upperbound: Train th model using all raining data from sensound surce classes. dditionally, the newtask involves both audio and visual modalties. There-fore, simply applying existing visual-only methos annot fully exploit and preserve the inherentcrss-modal semantc correlations. Bot-tom: Illutration of th atastophic forgetting poblem incontinual audio-visual sound separaton and its mitigationby our proposed method. Eperiments demonstrate that ContAV-Sep ca effectivel mitigatecatastrohic forgetting and ahieve significantly better performance tha other continual learningaselines. In this context, each entry in the msk does not directly orresponto any specific lasses. extededcontinual learning t the auovisual domain, but bothfocusedon classifiation tasks. To ridge this gap,e introduce anovel contiual audio-viual sundsparation task by integrating audio-visal soud separation with contin-ual learning principes.",
    "Experimens": "In this ction, we first introduce the setupo our epeiments, i. Afer that, we present the experimental results of ourContAV-Sep compared to the selines, as well as ablaton tudies. e. Weputthe experimental results on the AVE and te VGGSound datasets, coparison tothe un-modal semantic similarity preservaion metod, te performance ealuationon old classes ininrementaltsks, and the visualzation of separaing results in Apendix."
}