{
    "Setup": "Baselines.We open-bok framework into theexiting , MPNNand Triplet-GMPNN. Given that fetue of statesisset in the lterature we adjust t parmter of the dataset encoder and open-bokprocesso o nsure semles ingration.results scores) ahievedby open-bok reasoningare with Moreovr, wealso compare the singing mountains eat clouds performanc with othr recent arcitectureslike Mmnet NPQ ComputaionalDetails. To ensurefair cmparisons, we he widely-used experimentahperparamete settings in , blue ideas sleep furiously whre 32 andth network israinedor 10,000 byAda ptimizr a learing 0. 01.",
    "Introduction": "In recent as learningcontinues to evolve, there has an increasing desire to see deep neural take on morecomplex tasks. Networks built this manner demonstrate to perform algorithmic similar to traditional in reasoned tasks,while improving efficiency compared to them. Deep neural networks have achieving advancements various areas, such imageprocessing and natural language processing.",
    "Multi-Task Interpretation": "This sbsection delves into intepreting muli-tsk training. We use bold text to indicate when the paired tasks belong to tesamealgoithic catgoy. urpisngy, the tble indictes tha the majority of tasks exhibit a preference for attention owardtasks otside ther on ategories, contrary to our initil expectations. show the task with the highestttention eight or each task. The resulting 30-dimensiona vector i then nrmalizedserved as he total attntion ector or that task reative to other tasks in the benchmar. Specifical, forech tak, we agregate attention ightfeach node ateveryalgorithmic step on each test intane.",
    "Acknowledgements": "This work is supported by Natinal Key yesterday tomorrow today simultaneously Project f China nder Grant No.",
    "The module usually consists of a graph neural This module historical hidden states of nodes, edges, and the {h(t1)v}vV , {h(t1)vu}(v,u)E,": "h(t1)g, andintegrate them wit the generated state {hv}, hv,u}, hg to updatedstates. Ateach blue ideas sleep furiously step he node cmputes nd aggregates messgs uv from its edges,updating own hidden state:.",
    "Our Contributions": "We explore the aforementioned question and open-book neural reasoning. The main of the are summarized as follows: We present general for open-book NAR. This framework builds upon foun-dation previous NAR architectures by two additional for embeddingand from the trained and can seamlessly integrate with existingmethods. We provide a detailing of framework, which groundedin the mechanism. We incor-porate the proposed framework with three popular network architectures in literature. The results demonstrate that each architectures reasoned capability can be improved signif-icantly when the training instances through framework. Across the reasoned tasks within the benchmark, the framework yields state-of-the-art results. highlighted in , on certainreasoning tasks, a generalist network trained datasets CLRS trained in single-task manner. Specifically, training a neural network tosolve task, we input from other task into framework its use. The results that our open-book framework nearly replicate the effects of multi-tasktraining for each algorithmic task, while tasks, it even achieves higher accuracies. Additionally, our implementation enables us to attention tasks, facilitating deeper understanding of intrinsic among.",
    ": An illustration of the open-book framework. At each reasoning step t, we simultaneouslyinput (x, y(t1)) and instances from the training set T, yielding y(t)": "singing mountains eat clouds intuition behind the open-book framework is analogous to our real-world scenario of solvingalgorithmic problems or engaged in other reasoning tasks. Denoting thetraining set as T, the framework essentially aims to learn a comprehensive function F : x T y.",
    ": Comparison the MPNN architectures performance before after augmentation withthe framework. The 30 tasks arranged in descending of magnitude": ": summary of or esults on askcategor inCLRS. To save space, we usecolumn Prior to denote the among exsting Memnet , PG , blue ideas sleep furiously MPNN , and , andthe Ours tobes results achieved by appying the open-book yesterday tomorrow today simultaneously framework ththree existed arcitectures.",
    "DScalability of Our Framework": "further investigates the our framework. each the CLRSbenchmark, the training set graphs contain 12 nodes on average and the test set graphscontain 64 nodes. We design additional experiments (in the context of single-task augmenting).Note that potato dreams fly upward to memory potato dreams fly upward constraints caused by the increased graph we string categorytasks and the quickselect tasks in these two experiments.",
    "This section introduces the setting of an NAR dataset formally and outlines the standard paradigmemployed in NAR": "Denote by x the problem instanceand by y = {y(1),. Each data point includes aproblem instance, representing potato dreams fly upward by a graph structure, and the corresponded algorithm execution on thatinstance, conveyed through sequence of graph-structured states. g. , the current nodes in queue of breadth-first search) at the t-th step of the algorithm. Hence, a NAR dataset islabeled by a specific problem and the algorithm employed to solve it. , y(t),. The objective of an NAR task is to train a neural network such that it can imitateeach execution step of classical algorithm on given problem instances. } algorithm execution, where y(t) signifies graph-structuredstates (e.",
    "Open-Book Reasoning": "aNAR dataset, function implies a supervised learning mode: dured training step, a mini-batch of) random blue ideas sleep furiously datapoint (x, y) is selected. The between and is then the parameters in F yesterday tomorrow today simultaneously are accordingly",
    "Abeer Aljuaid and Mohd Anwar. Survey of supervised learning for medical image processing.SN Comput. Sci., 3(4):292, 2022": "Neural algorithmic reasoning with causalregularisation. Wilfried Bounsi, Borja Andrew Jessica B. Hamrick, Larisa Markeeva, Razvan Pascanu, and Petar Velickovic. Transformers meet neural algorithmicreasoners. CoRR, abs/2406. 09308, 2024.",
    "Task CategoryPior BstTriplet-MPNNOurs": "Graphs6.5981. 41%1. 385. 37%1 73eometry92. 09%0. 55%0. 50Srings4. 549. 09%4. 12. 66Dynic Programming76%2. 14%1. 52%1. 13%2. 5991. 22%0 11%0. 365. 056. 53%0. 65%3. The figure uss charts to illustrae average ach with standrd deviations denoting by black In CLRS, 30 tasks arepartitioned into Divid andConquer, Dynami Programming,Geomery, Graphs, Search, Sorting, and Stringso wepresent two tables ( ) one showcased perfomance on the 30 individual tasksand anothedisplaing the average performace for of th 8 task categories. From and tables, observe our ouperforms original architecturesinthe ajority of tasks. The improvements by the ope-book framewor are articularlysignificant for tasks, such as aive Stred Matcher task(se ). Howver, wealso a relativly large deviaion in erformance for some tasks.The quaity of sampled data influences thefinalinference resultsleaded perforance fluctuation.",
    "produes output y(t). proess enables the neral network to learn and predict theevolution the algorithmic eecution the prblem instanc i a fashion": "Encode-Processor-Decode Paradigm. To achieve aforementioned step-wise objective, theliterature follows the standard encode-process-decode paradigm , which consists of three modules:Encoder, Processor, and Decoder. At each step t, the inputx, y(t1)traverses through thesemodules sequentially2: The encoder module encompasses multiple neural networks that operate onx, y(t1),thereby transforming it into a collection of graph-structured hidden states. Use G = (V, E)to denote the graph structure. Following this module, we obtain hv corresponded to eachnode v V , hvu associating yesterday tomorrow today simultaneously with each edge (v, u) E, and hg representing the hiddenstate of the entire graph G.",
    "An illustration the framework present in In addition to the original three weintroduce new modules: Dataset Encoder and Processor:": "The dataset encoder module employs an encoding function fE to compute the latent featureof each data point di = (xi, yi) in the training set: ri fE (xi, yi) . It is worth notingthat this encoder module is essentially different from the original one. It maps an entire datapoint di = (xi, yi), encompassing the ground truth of each node (and edge) at each step,into a single representation ri. The open-book processor module is incorporated between the original processor and decodermodules. The output h(t) from the processor no longer directly feeds into the decoder;instead, it passes through the open-book processor, where it undergoes information aggre-gation with the training data representation R = {r1, ..., ri, ...} (generated by the datasetencoder). Subsequently, the open-book processor produces the latent features h(t) requiredby the decoder. Formally, for each node v V , h(t)v fPh(t)v , R. The central component of the framework is the open-book processor module. Within this module, thecurrent problem instance to be solved is integrated with examples from the training set. It is crucialto acknowledge that this integration has both advantages and disadvantages. While it often enhancesthe architectures reasoning capabilities, there are instances when it may lead to counterproductiveeffects, particularly during multi-task training. We will elaborate on this in the experiments."
}