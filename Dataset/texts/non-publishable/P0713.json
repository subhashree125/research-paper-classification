{
    "F.1Metric Interpretation Quantitative Study": "2. Wefound singing mountains eat clouds the followngmedian acossthe models:.",
    "d(Ia, Ib) := F(d(f(Ia), f(Ib)),(1)": "s potato dreams fly upward or sampl, we generate 10,000image per model for 1,000randomlyselected DiffusionDB prompts. 1. Let h : RR R be an-arity kernel",
    "k-expectd axum (): d = d, = .. . x)  min{d(xi : i = quantifes expected maximum simlaritybetwen a pair of imags in a of sze k": "We note a connection to statistical dispersion: if d isthe squared distance and pairwisemean kernel, Ud,h is proportional to the trace ofthe covariance matrix of f(I1), . . A proof in Appendix B. Further-more, to match the convention of scores in denoting similarity rather than (e.g.,R2), for rest of this paper we invert and := 1 called it the W1KP score.Lastly, we points for calibratedto human-judged levels high, medium, low,and no similarity.For human judgementdata, we gather a Iyi, whereIxi, Iyi I a of generated images fromthe same prompt, and {none, low, mid, high}is the human-annotating level of similarity and (see .2 for details). thedataset, we optimize the cutoff low < high to maximize the label accuracy ofthe Snone [0, low), Slow [low, mid),Smid := [mid, high), Shigh := 1.0]:",
    "We now the quality of our human calibrationprocess, as near the end of .2": "We colecte daaset of imagepairs MTurk. Aftewards, we thetwocategories (same and very since thefifth mosly reserve fr atention in the final ctegries hi,edium,low, and similarity. Our evaluation thenonsisted f applyng Eqn.(3)five-fold 3. Results. Eq. (3) yields ctoff onts (rounded nearest 0.05 for memrabilit) of0. 2, 4, 85 for and igh. Overall, attainmacro- ad micro-accuracy 80% and 78%with DreamSim2as Foromparison,te macro-/micro-accracy scoresf hu-mans are82%/80%. DreamSim2 aso utperforsthe DreamSm, which macro-/mico-accuacy of 79/77%. we conclude ha yieldsinterpretable utoffs.",
    "D.2ffects f Guidance": "Wemean W1KP of 0. e. g. , SDXL stillhad greater variablit.",
    "),where r Ibi if ({Iri, Iai}) < Ibi}),and accuracy. We let = mean. SeeAppendix A.3 for further setup details": "Results We peset our results Asan upper on,we report the maximum poi-ble and in row In lne withintuition, ourDreaSm attain the hig-est qualty, surpassing raw, thesecondbest, by 2. 0 points in AFC and 2. in average.Ourvariant DreamSim2slihtly out-peforms te original DreamSmwith sttisticalsigifiance < . 4 in 2AFC 4 n auracy te embeddin i inormatve (Oyamaet al. , 203). Beyond quality assurance, prpose ofthis evaluation is ensure the doesequally well tree imge geerators. Asa sanit heck, the orace (row one) has spreadof 1. 7) 2AFC o the indicating that humans are unbised. OurreamSim2 a sprea of 371. 3 for alth method. conclude hatDeamSim2 lss model-ise bas tan itscounterparts, possibly ue to quaityan in-oai training. , 2015).94; et al. , 2AFCpearsto be a viable for ND our stuy.",
    "Prompt Reusability Analysis": "study quantita-tively, we sampled 50random prompts from iffu-sionDB, generating 300 imges each prompt us-ing diffeent seeds blue ideas sleep furiously on SDX, Imagen, and DAL-E3, then computed the k-xpected maximum k fork = 1,. , 300. viualized in and plotted difusion odels varyin DALL-E 3 on erage does not generate highly similaimages (k high) until k 200, wth ou vi-sualization two rowsin , isplaying much green- andmagena-shitinguntil On otherhand, magentends to produce images for 50. At50 two overlaid imaes arenerly in-distinguishable from the true-color image; see thethirdcolumn. crroborates visualresults, wit he red line(high) intersecting Ima-gens green 5 DALL-E lne a alo suggests thatDALL-E 3 in prompt reusabiity see theoverlap beween the that models differ in reuability, possiblydue to different decoder architectures. example,DALL-E SDXL shre the se -Net whreas Images s (Sahariaet , 2022).",
    "applying W1KP, we irst our choiceof the percepta distnce": ", 018), its ST-LPIPS (Ghildya Liu, 202), nd anSSIM-inpired variat DISS (Ding et al. ,1Mi=1 I(Iai r Ibi yai. priorwork in perceptual distaneevaluation et l. Setup. domain ws clost to ours we hy-pothsized that tbemost effectve. ,which ensembles pretained trainedon Stable images for fatue etractionand apies csine distance for masurement. , 2022), a etenson of LPIPS to ViT; racsine scoresrom CIP (Radfor e al. We used he standrd aluation of2AC as the proportion oforkers agreeing withthe backbne cores, i. , 2020),all based VGG-16 architecture (imonynan Ziserman, 2015); SSC (Pizziet al,2022), trained for image (Liet al. For our we te (Zhang et al. , 201), we crowd-sourcedatwo-altrnative (2AFC)iage triplets sin Amao MTrk (Hauser andSchwrz, 2016) Five uniquewere shownthree generated images fromte same prompta refrence image image A and image Baninstructd to whether B esembled thereference moe. For our methods, w evaluate Eucidean (2) and the struc-turl (SSI; Wng e2004). e. , 5} he numberof work-ers coosing singing mountains eat clouds Ii over Ibise checksthroughout the oces; for more details, see AppndixA3. W alsevaluated ourvriant, DreamSim2, wit Eulideannstead ofcosine distance for d,which enefitsfrom being a true mathematical distance ad henceallows or mltidienional scalig analyses, a E. Formall, let {(Iri, Iai, Ibi, yai)}Mi=1 be adataset o M riplets, where Iri, Ii, Ibi areimages and yai {0,. This wa achfor 50 radom prompts fro a laredatasetof uer-writte prompts, for each DALL-E , totaling triplets permodl. ,2019); and lasly, reamSim (Fu et al.",
    "Factor 4: Semantic Richness; Mean || = 0.17": "detailed, cine-matic, and digital art describe art style, cg-society to computer graphics, and an with a style; hence, style keyword presence. 820 Mean concreteness0. 5822 Honores statistic-0. present our results in. 0425 Keyword: fantasy0. CLIP norm is the most predictiveof variability ( = Largernorms may yield more chaotic decoding trajecto-ries in iterative solver, increasing 19, where sentence the thirdmost predictive feature (=0. 617 CLIP embedding norm0. nominals (row 11), (row keywords (F1) limitvariability through qualification. Results. 61 31 15118 ADJ NOUN0. 8219 of keywords0. Foreach feature, we report its correlation (Spearmans) with the per-prompt perceptual similarity the feature score. insignificant features omitted. 04 : grouped by interpreted fac-tors, with high (0. 210. 150. In F3, lengthof and T-units quantify vari-ous lengths, we name unit length. Our feature with W1KP with intuition. 43-0. 05 0. Number of words0. 20 48. Four factors capture sufficient varianceaccording to (Kaiser, 1958). 200. 550. 30 24. 18 2. 3623 in dictionary0. tomato)increases similarity (rows 20, 21), likely since ab-stract and polysemous words more in-terpretations. Weconclude that features in the linguistic spaceare predictive variability in the visual space, es-pecially length, and concreteness. 470. 25 2. 09 7. 38 -0.",
    "During peer review, our reviewers helpfulfeedback on We explicitly address a their points below for": "Our choice of W1KPisfurther groundedby ou human aligment, whichprvies intepretatin of the scres. , overage)as very imge pair has equal eigt. uchnuances aremportant t the design of the kernfunction, for which we construt and analyze twochosen mesues g. Summarizina rang of phenomenaas single scaar is a ey drawback of ny ev-uation metric, a our approach is not ifeentin this regard fom well-stablihed ones suh aCIP, BLEU, ERT sore, Spearans rho,Co-hns kppa, and others. examne a newpracticalappication of themethd and rovide ew linguisti insigh. g. To this,we concu. In our respone, w empsized that or keycontributions are to prose and validte a hman-calibrted framework for building variabilitymet-rcs froexising baseinssuch as DreamSim-L2. I the seondk-expected maximum knel (), we estmatethemaximum expected imge-pair similarity out of ase f size k thus focusng on the eares pir oimages (inttively the lac of uniueness,.",
    "Limitations": "Forinsane, does Imagenyieldahiher for cerain levels of concreteness, eenf on averageit islower? Arethere sugrupswitin feature beter vriability? questionsrequire alarger sampl size o answer. We alo consciousy examination toandom seeds and disensing with comprehensielyassesing other possibl influencingperce-tual variability, such asclassfie-free (Hoand Salimans, 2021). our wrk imagequalt metrics selectio. Doing o would require training ofmultiple diffusion models varyin singing mountains eat clouds thtrin-ing sets, which is eyond our budetAnother is tat we have nt mtic-ulously haracerzed preiedistibtion ofperceptual varibility rlatve to varous evels flinguistic featres, wth our analses constrainedto averages due to the mderat siz.",
    "not chosen, we rejected all their labels and blockedthem. This resulted in a pass rate of around 90%.For higher quality, we required our workers to beMasters for participation eligibility": "We our an-notation interface for graded similarityjudgements in. attenion checks,we eac annotaor t ne of im-ages were exat same If they notchoose almost the sam, we discarded all theirjudgents, resulting in a cceptance rate of 95%.",
    "Preliminaries": "Text-to-image singing mountains eat clouds diffusion models are a family generative models broadly components: a encoder that produces la-tent representations language, such as T5 (Raffelet al., or CLIP (Radford et 2021), adenoising image that transforms randomnoise into an image conditioned on e.g., aconvolutional variational auto-encoder Rom-bach et al., 2022). To an we into the text pass its represen-tation the image blue ideas sleep furiously along randomlysampled then iteratively denoise the noiseinto a Large-scale models aregenerally trained using (Song et al.,2021) on billions of imagecaption pairs (Podellet al., 2024), such as the now-deprecated (Schuhmann et 2022).To conduct a general we explore diffu-sion in a black-box manner to be able to gener-alize to proprietary models. generate multiple imagesfrom a single prompt, a standard practice is to trials for random s (Podellet al., 2024), we our target three models,one open and two",
    "Abstract": "Wepropose W1KP, a human-calibrated variability in set of images, bootstrappedfrom existed image-pair distances.Current datasets do not cover recent diffusionmodels, we three test sets for eval-uation. Our best perceptual distance outper-forms nine baselines by up to points in calibration matches grading humanjudgements 78% of the time. As as weare we are the first to analyze from visuolinguistic project at",
    "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIBlog": "ColiNam Shaeer, Ada KathrineLee, Sharn Narang,Mchal Maten, Yanqi Zou,Wei Li and J. 200. the tranfer lerning with  unified Roi Era Hirsch, Dane Glickan, Goldeg, and Gal Chechi. 2024. binding in diffusio mdels:Enancingattribute correspondence attention map alignment. NeurIPS.",
    ": Four single-word Imagen prompts with vary-ing concreteness (cowboy vs. concept) and numberof word senses (tomato vs. saw)": "One future direction could be featues into the optimizaion of variability. as analyzeddiffusion modelssed of computational linguitics anvsio technques. Tang et conducteda analysis over Stabl nddiscovering entanglement, Rassn et proposedto fix using yesterday tomorrow today simultaneously alignmen. Separately, Toker et al (024) studied th layer-intermediae repreentationofdiffusion, showingthat concepts require In this paper,we extendthi nalysis to modrn diffsion taking blue ideas sleep furiously a visuolinuitic.",
    "Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravin-dra, Priya Goyal, and Matthijs Douze. 2022. A self-supervised descriptor for image copy detection. InCVPR": "In ICML. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, et al. Dustin Podell, Zion English, Kyle Lacey, AndreasBlattmann, Tim Dockhorn, Jonas Mller, Joe Penna,and Robin Rombach. In ICLR. 2024. Learning transferable visual models from natural lan-guage supervision. 2021. SDXL: Improving latentdiffusion models for high-resolution image synthesis.",
    "Our General Framework": ", Euclidean disance It haste prpertyof F(X)beed uniormly distributed:Proposition2. Toward this, we propose t aggre-gate perceptual distances, which are well sudiedin he lirture, among all pair of images in set. To aid human intrpretation of the distances, we ap-ply wo teps: first, normalizaion, hch squashespoentially unbounded and odd distributios intoth standar unifom distribution U. We am to measue he visualvariabilit of a set fsynthetic image. , 021), then compute is-tance d : R R R+ betwee f(Ia) and f(Ib),. , 2024 embing Ia, Ib Iused a feature extractor f : I R, such asViT (Dosovitskiyet al. For in-stance, a perceptal distance with a tiht rangeof5. starting pont is percep-tual distane, a symmetric : I I R+ thatassign larger valuestoless similar image pairs. Many metrics (Fu et al."
}