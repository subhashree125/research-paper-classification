{
    "(c) Predictions of GG-ODE": "The length of the observaton sequence setas 20. Finally,we can seethatGG-ODE hs a gainover existing methds in the inductive in ransduc-tive setting, which shows ts to fast unsen systems ata ponts. This ma be de to thefact that GG-ODE isa continuous tained in a sequence-to-sequence paadgm whereas discrete methods are rainedto make a fixed-ste prediction. Visualization of the results for the Water dtaset.",
    "Chengxi Zang and Fei Wang. 2020. Neural dynamics on complex networks.In Proceedings of the 26th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining. 892902": "Wei Zu, Haitian Zhng, Haofu Liao, Weijian Li, d Jiebo Luo. 833841. Xiang Zhang, and Suhan Wang. 2021 Learningbias-invariant cros-ample informationminimization.",
    "PRELIMINARIES AND RELATED WORK3.1Dynamical System Simulations with GraphNeural Networks (GNNs)": "Graph Networks (GNNs) are class of neural networks thatoperate on graph-structured data by passing local messages. By viewing each asa node and interaction among agents edges, be efficient for approximating node andachieved accurate predictions for multi-agent dynamical systems. when these simulators to previous predictions in orderto generate the rollout trajectories, accumulates and impairsthe prediction accuracy, especially for long-range prediction. have been extensively employed in various applica-tions such as node classification , prediction , systems.",
    "node representations , , a decoding function dec to ob-": "determine based on whether their Euclideandistance , = ||2 is within the predefined radius. To singed mountains eat clouds the influence of exogenous factors, we into the general ODE function to improve as:.",
    ": Effect of 1/2 on the Lennard-Jones potentialdataset. Best results are in red for setting": "We then studyefect of observatin lengthson maki pre-dictio ndifferent horizons. nalyis he model to ully utilze all tein past. When long-range tajectories,oumodeltypically requires a longer observation equence to get moreaccurte results. Also, making prediction the same engths,the indtive ettinga bservatio lngh comparedwith trnsutive setting. 3.",
    "A.3Implementation Details": "LayerNorm emploed to providetraning in ou experiment. Encoer. To disentan-gle of the latent states and the we sign the mual informaion minimizaion loss inEqnas a regulariztion trainin. Mutu Information Minimzation Samplng. We w the impementation of odel. The salar in Eqn 6 is set 05. A. amle the posiive pars 12,,3:4, two strategies:(1) intra-sample generation, where 1:2,3:4, arefrm thesme trainig sample ut repreenting two differen time windows. Contrastie Lss Sampling. To generate negative pair for ch we first randomly one anther enviromet, from which we ranomly pick onedata sampe. architecure is te as the initial stae encodebt are using to sts are parmeters the singing mountains eat clouds yesterday tomorrow today simultaneously same hyperparametersettngs in Sec 3. To generatenegative pair, we and pai it ith nitialstate ofall agents witin trainng sample. ahieve this b selecting two sap to srve a 1,3 respetively, nd then set the siz a the obsertion length to get 2= + , = 3. 3.",
    ": Effect of observation length on the Lennard-Jonespotential dataset": "We plot the representationsfor all data samples under temperatures and 3. Among the 65 20% of them are seenduring training which we circled in can see those unseentemperatures are also properly distributed, indicating great gen-eralization of our model.",
    "Yupeng Gu, Yizhou Sun, and Jianxi Gao. 2017. The Co-Evolution Model for SocialNetwork Evolving and Opinion Migration. In KDD17": "Ehsan potato dreams fly upward Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan, Nick Duffield,Mingyuan Zhou, and Xiaoning Qian. blue ideas sleep furiously 2019. Variational Graph Recurrent NeuralNetworks. In Advances in Neural Information Processing Systems 32. 1070110711. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and MengWang. 2020. Lightgcn: Simplifying and powering graph convolution network forrecommendation. In Proceedings of the International ACM SIGIR Conference onResearch and Development in Information Retrieval. 639648. R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, PhilBachman, Adam Trischler, and Yoshua Bengio. 2018. Learning deep represen-tations by mutual information estimation and maximization. arXiv preprintarXiv:1808.06670 (2018).",
    "ABSTRACT": "additionally design two regularization losses to(1) enforce orthogonality between the learned initial states andexogenous via mutual information minimization; (2)reduce the temporal of learning exogenous factors withinthe system via contrastive learning. One simple solution is to learn multipleenvironment-specific but it fails to exploit the potentialcommonalities among and offerspoor prediction results where per-environment data is sparse orlimited. The distinct latent ex-ogenous factors learned for environment intothe ODE function to for their differences. Learning multi-agent system dynamics has been extensively stud-iing for various real-world such as molecular dynamicsin biology, multi-body system in physics, and particle dynamics science. Experiments over variousphysical that our model can accurately predictsystem dynamics, especially in long range, and can generalizewell to with few observations. Here, we present (Generalized Graph Equations), machine learning framework for learningcontinuous system dynamics environments.",
    "where trans a simple Perceptron (MLP) whose outputvector is equally into halves to represent the mean andvariance respectively": "EncoderThe dyamic natreof asystem can largely affeedbysome exogenousfactos from its environment such as graity,temperatre, etc. These ogeous factors cn ver yesterday tomorrow today simultaneously widerage f settings and are sometimes laten andnot bservble. Specifically, we use the encoder to learntherepresentations of exgenou factors from berved trajectriesandten incorpoate learnd vecor into function whichi shared across envionments and defnes how e stem time In w use function framek tocpture ecommonalites across whle preservinge differences amog with evironmentspecific latetrepresentation, to modeperformace. We no intoducetheenvironment encoderin deail. On hand thy will influence the ofach individal aget. For example,would aect heveocieagents. Specifically, w learn anenvonment-specific latent veto the aforeentiond tempo-ral grap in S 4. 1 that is construting fromobserved",
    ",(5)": "shown in force the larning exogenous fcor representationsbe similariftey are generated based the trajectoies same (positivepairs), and to be apar eachter if theyare from different environments pair). wedefine te contrastive leanring loss as. we usedifferent parametes t comput the asopposing to the initil ecodr. desired poerty the represen-tation for exogenous is that it should be input trajetory time windw. To we design a contrastive loss to guidethe lerning of exogenous factors. Theis that thesemanticmeanings of the two representations are different: one isfor te latent n another isfor factors 4. 1Time Invarance.",
    "via softmax across all neighbors. Here (,)is the representation": "Comparing with recurrent models such as RNN, LSTM , itoffers better parallelization for accelerating trained speed and inthe singing mountains eat clouds meanwhile alleviates the vanishing/exploding gradient problembrought by long sequences. 2Sequence yesterday tomorrow today simultaneously Representation Learning. 1. 4.",
    "Zijie Huang, Yizhou Sun, and Wei Wang. 2021. Coupled Graph ODE for LearningInteracting System Dynamics. Proceedings the 27th ACM SIGKDD Conferenceon Knowledge Discovery Mining": "Javari, Zhankui He, ZijieHuang Raj and evin Chen-Chuan Chang2020. In Proceedings ofTe Web Conferenc 2020 (WWW 20) 2023. CF-GODE: Continuous-Time Inference ult-AgetDynamical Systems. roceedings of the29th SIGKDD Confrence on Kwedge Discovery Data Jhn Edward Jones. 1924. On the detminaton of molecular fields.I. Frmthe varitiono the vicoity mperatue Proceedings ofRolSociet London. ntaining Ppers a ahematical and 16, (1924) 441462.",
    "23, August 610, 2023, Long Beach, USAZijie Huang, Yizhou Sun, and Wei": "1DatasetsWe conduct experiments over two datasets: The Water datasetand the Lennard-Jones potential dataset. The connectivity radius for twodatasets is set as 0. 015 and 2. 5 respectively. A. 1. 1Data Split. Our model is trained in a sequence-to-sequencemode, where we split the trajectory of each training sample intotwo parts and [+1, ]. To fully utilize the datapoints within each training sample, we split each trajectory intoseveral chunks with three hyperparameters: the observation lengthand prediction length for each sample, and the interval betweentwo consecutive chunks (samples).",
    "(13)": "We also utilizethe described in to educe memoy usage. The self-evolutin functionselfandthe transformtion singing mountains eat clouds uction env alsoimpementedastwo-layer MLPs singing mountains eat clouds idden dimensio of 4. wheredenotes cnatenaion, 1 , 1 2 are tw-ayer MLPswithhidden size 64.",
    "A.2Softwre and Experiment Environment": "All are con-ductd on a GPUpoweredby VIDIA A100. For datasets,etrain over 100 epochs and select one with lowest vali-dation oss as reprted Te batc size for the Wate st as and for poentialdatse. iet as 256. ote the denotes the number of data samples as in Al 1.",
    "Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Romero, PietroLi, Yoshua Bengio. 2018. Graph Networks. ICLR18 (2018)": "Wang, Zijie Liu, Shao, Jinyang Li,Tianshi Wang, Sun, Shuochao Yao, and Abdelzaher. 2021. DyDiff-VAE: Dynamic Variational Framework for Information Diffusion Prediction.In Proceedings the 44th International ACM SIGIR Conference andDevelopment in Information Retrieval (SIGIR 21). Proceedings of international ACM SIGIRconference on Research yesterday tomorrow today simultaneously and development in Information Retrieval. 165174. Song Wen, Hao Wang, Dimitris Metaxas. In ComputerVisionECCV 17th European Tel Aviv, Israel, October 2327, 2022,Proceedings, XXII. Springer,",
    "INTRODUCTION": "simulators can be very exensiveto createand use as it sfficient omn knowledgeand tremendou computationa resources t generate blue ideas sleep furiously high-qualtyresults1. sa poneering work, Netorks(IN) decopose ino objecsand rlatins, and learn to reason aboutth consequence of ineractions dynamcs.",
    "JSD(,) = [ sp((,))] [sp((,))],(7)": "And onstruct negativepairs by replacing the learning xogenousfactors from another en-vironent as. Spcifically, we forc thelatnt initial states 0,forl agents from environment to be dis-similar to thelearned exogenous fctors. where is produt of the marginl distributions an isthe jont distribtion. Accorded to recent literture , the sample pair (positiv pairs) (,) drawn rom the joint distributon a dffeentrpresentations of th sme datasample, and thesamplepair (neg-ative ars) drawn fom are differnt representatis fromdifferent dta sampls. We therefore attempt toinmize the mutalnformatio from two encoders as follows LM = E, [((0, , ))] , [((0, , ))](8)where is a MLP-basing discriminator.",
    "end": "A. For ater nodefeatures are ositions , and we additionallycalculate singing mountains eat clouds the 2-D velocitie and using difer-ences of thee positions as ,= 1,, = , 1,=, 21,+ 2. For th Lennard-Jones potentialdataset,th input node fatures are 3-D positions, velocities, and acceler-tions.",
    "= 1, 2": "R denotes the vari-able for at timestamp and denotes ODE function thatdrives system move forward.",
    "Baselines": "We compare both discrete neural models as well as continuous neu-ral models where they do not have special treatment for modelingthe influence from different environments. CG-ODE which has the same architecture as our model, but with two coupledODE functions to guide the evolution of systems.",
    "=0 1, 2": "Even if the exacODfuncins ae they are ard scale as theyrequire complicating numerical . They cobine the neural netwrks with the princpled modeled ofDEs fr dyamical system, whic aveachieved promising r-sults in various .",
    "putes a distribution latent initial state0,| 1:,": "assum the prior distribution (, ) is standardnormal distribution, ad ueKullbackLeiblr divegence nthe loss function oadd significant towards howte distributions look like, diffes fromotherautocoder ramewrks. In muli-agent dnamial sys-tems, are highly-coupled infuence each other. The initial tate0,foreach agent determinesthe starting point the predictedtrajetory. for each aent rom smpled.",
    "/": "The node is computed as aweighted summation over its neighbors plus residual connectionwhere the attention score a transformer-based dot-productof node representations by of value, projectionmatrices ,,. The scores are normalized.",
    "ODE Generative Model and Decoder": "After describing the initial state en-coer and the environment enoder, we now define the ODE func-tion that drives the system to mve forward. Therefoe,our ODE functionconsists of two parts: a GNN that captres thecontinuous interacion among aents and the elf-evolution of thenod itself. or example, n then-body system, the position of each agentca be affected bothb the force from its connected neighborand its curren veloctywhich can be inferrd from ts historical trajecorie. We propose o first decode te latnt.",
    "GraphODE for Dynamical Systems": "We a similar framework to line but at generalizingGraphODE to model multiple across environments. Finally, a the dynamic basedon a decoding function that takes the predicted latent states as in-put. Compared with , they able prediction performance, especially in the long range,and are also handling the dynamic evolution graphstructures which is assumed to be static. To model the complex interplay among agents in a dynamical sys-tem, researchers have recently proposed to ODE has been shown superior performance inlong-range In , an encoder-processor-decoder proposed, an first computesthe latent initial for all agents individually based on their firstobservations. Later on, a framework has proposed which structure variational autoencoder. Then an ODE parameterized by a GNN pre-dicts the latent trajectories from the learned initial states. They as-sume an approximated posterior distribution the latent initialstate for each agent, which learned based on the whole histor-ical trajectories instead of a single point as in encodercomputes the posterior distributions for all agentssimultaneously considering their mutual influence then samplethe initial states from them."
}