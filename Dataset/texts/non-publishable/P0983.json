{
    "Graph cntrastive learning": "Over the past few years gaph cotrastive (GCL) singing mountains eat clouds as a poerful r leaed represntations ograph-structurd da. Deep Infomax follows theapproachof utal learing, as proposing byadapted. the agreement of node repre-sentations between two of graph. stepfurther, MVGRL introduces node diffusionand node-level with aug-mentegraphs. Followin he BYOL , BRLsampling b minimzin ninvarincebasing lossfor augmented graphs withina batc Howevr, mostmethods classicl instance discrimination te pretexttas, which reuires graph augmentation echniqesto obtain meaninfu view",
    "Both authors contributed equally to this research.Corresponding author": "Permissin tomake dgtal or had copies of part or all of th wrk for personal orclassroom is grntd ithout ee roiding that copies are not made or distributdfor profit or advntage that bear th notice and te full citatonn te Copyrights for thi-artycoonents ofthis work must be KDD 24, yesterday tomorrow today simultaneously August acelona, 2024 held by yesterday tomorrow today simultaneously th owner/author(s).",
    "(4)": "Typically, ca be simply osine similariy function, i. e.,) = or a neural (4),we geneal fom grph learnn, where and negative areguided b the modularity coefficiet B,.",
    "Visualization": "this part, we measure the quality of the generated embeddingsby t-SNE . Due to space limitations, we have selected seven strongbaseline methods analysis. It should also noted thatDGCLUSTER , being semi-supervised method, was comparison",
    "Ablation study": "finins are lised below(1) Te performance of relatively stble as longnumbe and deth of wals are notexessivly extreme,as shon. e also note that the walk depth 5 is anoptmal hoic asa larger wak teds to positive pairs.",
    ": The performance of Magi with varying differ-ent number of randomly rooted nodes on the ogbn-arxivdataset in terms of NMI": "Resuls are shownn in were Magi (w/ SL) refersto use osimple cnrasive os definedin Eq. demonstrate tht, i the of cases,the use of ranom dirct samplig. Tis esulthihligts the adantag of incorprtig high-order intothe analysis, enabling a capture of tecomplex relationshipand ommuity strucue wthin te raph. Combind Magis at smaller walkepths, this demostrates Mags meaned that for Magi neds sample aew poiivesamples. inallyour samling strategy achievessmilr performace ompared o using si of mod-ulariy matrixproposed in ,but ur sratgy has coplxity and can b to large-scle 100M nes. Weinfer may because edges exist etwen differentommuitis, which some positivesampe pairs fromdi-fernt increasin th o drift. Efeciveness ofdifferent We cnduct ablationstudy to mafest the efficac of components inMagi. 5 (see ) (3)Dfferen number of smpledroot nodes avesimilar custered perfrmance, but saer lead fsteconvrgence (see ). We then shocase the ofplored high-order proximty inMagi. condy, direct ue of edg definethe positive ad negative sample sets achevs the effec. ( ), Magi (w/ MS) reersto use S1 and the signof score to defin psitiv andnegative ets, (w/ EI) rfers to use S1 and edgeinicators define positie and negtive sampleses, (w/ MS)refers to use sin of high-rder modulariy sore todfinepositiveand negatve saple sets and Magi refers e S1an S2 to define posiive ad negatv saple sets. (2) Magi can achiv th best performance when temperatu is. In it is observed that ch mprovement ofto final perforanc. ith community This finding is conisten withthe previous work.",
    "RELATED WORK2.1Graph clustering": "Classical methods involve either solving optimiza-tion or using heuristic, non-parametric approaches. the rise of deep neural networksin graph representation learning, random methods suchas DeepWalk and Node2vec also introduced foraddressing clustering tasks. Graph clustered is studied problem in academia and indus-try. Prominent examples include K-means , spectral clustering ,and Louvain.",
    ": Anilutrative overviw of how poitive and nega-tive examples of query node guided by moularityeasur": "Contrastive learning , whichaims to lean representationsthat bring similar instances closer inthe epresentation space and dstances dissmilar ones, has beenproven to learncluster-preserving rpresentations. an unsupervied manner due to the absence of labeed nnotations,this has mtivated the exloratin of self-supervsing graph learningmethods fr raph clustering. Wstablish the connection etwe modularity maimizationand grah cotrastive learnng. We propose Magi, community-aware grap con-trastive learned framework that uses odularity maximiza-tion as its pretext task. In this wor, we roviea in-depth analysis f moularitymaximization and bridgethe gapbetween modlarity maximia-tion and GCL. Mainstream GCs typically ry on data aug-mentation to create mulipleviews and emloy ultipl encoders toobain correponding representations. Howeve, this type o pretext task neglects th inherent structre ofgrph data, which can result in semantic drft uring downstreamclustering tasks. Augmentation-free GCLs can mitigte this ymining the information inherently carrie by graph data. Although te GCLmentioned above have achieved signiicant succss n the graphclusterin task, we note that these method still suffer from at leastone of the following challenges:(C1) Scalability. Pretext tasks play an important rol inenabling GCL to better adapt to downstram tsks. Thereforerecently, graph contrstielearning method (GCLs) have made significant beakthrog in graph ctering and gd-ually become the maistream approah forgraph clusterig. However, the design of the modlarty functio is heuris-tic, and the underlying reaons fo its succes as an otimizationobjective remain largely uneplored. Magi can mitigate theeffects of semntic driftby perceivinthe underlyig comuniystructuresin the graph, and sinc it doesnt rel o data augmen-tation it cneasilyscale toa sufficiently large graph with 100Mnodes. Experimental result. Magi avoids semantic drift by lever-aging nderling commuity structure and elimintes teneed for grah ugmentation. modularity maximizatonobjecive can efectvelyperceive potential comunit struc-ture wthinnetworks adprovide guidance for representatiolearning. (C2) Seanticdrift. Recenly, mehods based o eural modularity maximiza-tion have made new progessin grah clustering tasks. ur findings revel that mod-ulrity maxization can be viewed as leeraging potentialcommunity information in graphs for contrstive learning. Magihas onsistenly outperformed seeral state-of-theats inthe tsk of graph clustering.",
    "Experimenal setup": "We emplo fulbch traiing on smll-sale graphs and st thenumbe of roednodes as 2048 for large andextra-large atasets De to spacelimitations, details hyperparameter. All experiments repeated 5times andthe mean reporting in. We sigle NVIIA A100 with for eachethod.",
    "Semantic drift mitigation": "To answer Q2, we alternatively measure the semantic driftbased on the quality of pseudo-labels generating by various methods. Specifically, considered the augmentation-free GCLs referenced in. 2, we employ ground-truth labels to evaluate the accuracyof pseudo-labels produced by these methods. Theresults (refer to ) demonstrate that, in comparison to otheraugmentation-free GCLs, Magi generates higher-quality pseudo-labels. This confirms the efficacy of modularity maximization as apretext task in mitigating semantic drift.",
    "CONCLUSION": "Extensive experiments on eightreal-world graph datasets demonstrate the effectiveness of ourmethod, which achieves state-of-the-art in most cases comparedwith strong baselines. This insight motivates us to propose Magi, a community-awaregraph clustering framework with modularity maximization as thepretext task for contrastive learning. Magi is an augmentation-freeGCL framework, which avoids potential semantic drift and scala-bility issues. Our work establishes the connection be-tween neural modularity maximization and graph contrastive learn-ing. In this paper, we explore the problem of graph clustering via neuralmodularity maximization.",
    ")(,),(1)": "(,) = 1 is function, i. is the of node , = yesterday tomorrow today simultaneously |E| is the total num-ber of edges in graph, and is potato dreams fly upward community to which node is assigned. e.",
    "Namkyeong Lee, Junseok Lee, and Chanyoung Park. 2022. Augmentation-FreeSelf-Supervised Learning on Graphs. (2022)": "A Graph is Worth 1-bit WhenGraph Contrastive yesterday tomorrow today simultaneously Meets Spiking Neural Networks. Whats Behind theMask: Understanded Masked Graph Modeling for Graph In KDD. 2023. Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu,Changhua Meng, Zibin and Wang. In ICLR. Jintang Huizhe Zhang, Ruofan Wu, Zulun Zhu, Baokun Wang, ChanghuaMeng, Zibin Zheng, and Chen. 12681279.",
    "(3)": "In this modular-ity maximization is in form to graph autoencoders, withthe goal of optimization is to the modularity matrix than adjacency A. by a recent that showed potato dreams fly upward the equivalence graph autoencoders and con-trastive learning, we our intuition to explicitly mod-ularity contrastive learning. Here unify Eq. (3) into general form from per-spective optimization. Since directly Eq.",
    "GNN encoder": "An encoder in GCL is a crucial that maps inputgraph data into latent representations. Magi, we employ differ-ent GNNs as to encode graphs of varying sizes. Here only two representative GNNs, , GCN It is important note that Magi is not specific anyparticular GNN model and can be applied with other , GAT ) as well. is GNN model that has been widely forvarious graph-based tasks. GCN operates by aggregating informa-tion from neighboring propagating it through multiplelayers. It the graph convolution whichis based on graph Laplacian to capture local structure. Formally, the message propagation of layer inGCN is defined as:.",
    "(|N| + 1) (|N| + 1),(5)": "where LeakyReLU() = ) + (0, ) is activation relies a symmetric and fixed aggregationfunction based on the graph can limit its ability tocapture diverse neighborhood structures and scale to graphseffectively. GraphSAGE is a GNN designed to betterhandle large-scale graphs in an inductive",
    "INTRODUCTION": "Grph is a fundamenl yesterday tomorrow today simultaneously proe n analysis,structures and between ndes in araph. far, has been widel studied and extensvely acrosvarous domain, icludig socil anlysis , imagesegmntation ad systems. As lontandingo research, graph clusteing toevolve through development of novel algoritms and techniques. GNse astnod representatios, which are ften accomplishdwit auxiiry task to help uncoverthe paerns forclusering. As clstering tas is ommonly approached in",
    "EXPERIMENTAL RESULTS": "In addition to the mainexperients, wehaveconducing upplementary eperimens amed at answeringhe questins:(Q1): an Mgito extra-larescale containig upto 100 millo nods and state-of-the-art belinesQ2):modularity mximizatin pretet taskseffectivelymitigate the problem of",
    "MethodTime ComplexityMemory Cost(MB)Time Cost(s)": "de2vecO()O(),12262 8GRAEO( 2 + + + )2772. BRLO( + 2)O( + + 2)1,20131. 2MVGRLO( 2 2)O( 2 + + 2)2,9243. 9SCGDNO( 2 + )O( +2)1,08031. 9CCGCO(+ 2 + + ( )23,23338. 0DGCLUSTERO( 2 ++ + 5DMoNO(( + ))O( 2MagO(2 + + 7.",
    "Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning PerspectiveKDD 24, 2024, Barcelona, Spain": "For exaple, GAE and GAE learno recostruct the graphstruture as the self-supevised learning task NNs. to th great abil-ty graph neural network(NNs) in learnng nd attributeinfortion, utoencoders stand outas an emerging aproachunsupervised graph tasks. Howevr, elf-supervised learning tasksor pretext tasks GAE are not agned with taskssuch as a result, the representationsmay effectively cpture the relevant information for clusteringtasks. Follow-up wor by empoyng Laplacin sharpenng smoothing , genrative adversaril singing mountains eat clouds learning , autoencoding.",
    "|U1 |}(7)": "brief, consider nodes with visits greaer than he mean asO. for in N, perfom th same operatin and setB = NO trainingbatch. For ay two odes in B, let 2,be thenuber of from to ,can then cnsruct siilaritmatrix S for nodes i B based the number whicheac eement:",
    "Modularity pretext task": "For ay in setN consistingof randomy slected root nodes in the graph, basedon the principle of intenal conectivitywitn communities, teremust e oveap between the neighborhood of node and itspotetil community, denoted as O. Howeerbased on defnitiontat cmmunit i a set of nodes i densely connected nternally, weutilize blue ideas sleep furiously potato dreams fly upward randomwalks t appoximate O. The core insight behind neural modularity maximization is thecomputaton of the moduarty matrix B. Accurately selecting O canbe expensive tas becuse it involves etcted potential commuty tructure in the graph. S1)Samplin mutiple sub-communiie. The vailla modularitmatrix only focuss on the first-order proximity wthin thegraph, wch ontradicts our ituitie understaning of comunity structr in network,thats, tonodes ofte be-long to the sme commnity due to high-order proxmity,such as havig many common neighbrs. How-ever,the cot of computing high-order polynmials of theadacency matrix A for large-scae graps isill prohibi-tive Specifically, we eploy first-stage random wak t saple multiple ub-commniiesanderge the into taning batch,ensurinthat te corre-sponding subgraph ithin batc ontains effective ommunity stuctures to guide the modl. The sampling bgaph aynot necessaryshare a similr struure th h overall graph, whch canhinde the performance of conrastive learing."
}