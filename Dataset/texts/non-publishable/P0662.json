{
    "A.5Evaluation Metrics": "ofnew and sing exts prsents the correlation of thatevaluate th relevnce generating texts to ets inBenhmark-Reddit. ,example(2))ota counter-speech (g. presents crreation cefficentsbetween iversity metrics (i. show number sampes in each on te prediction the onversation incivil-ity classifier and te yesterday tomorrow today simultaneously hat re-etry classifier. g. , orare very eneric not address specifichateul cotent ,example(6)). ype-token rato,distinc1, and distinct2) novelty metris(i.",
    "that use hate speech and counterspeech to predictthe incivility level or hater reentry type": "Conversation IncivilityConversation incivilityis metric to measure the outcome based on thenumber o civil and uncvil commnsa well asthe unique authors invoved in the discourse (Yuet al., 024). Intuitively, he more uncivi or lesscivil) the comments, worse the outcome; un-civil comments from many autors are worse thantose from just a few.Fomally, it is definedas S(r) = U(r) (1 )C(r), where U(r)refers t uncivilbehavior and C(r) to civil behav-ior. For eachuer i i = 0 1, 2, ...,k), nui is de-fining as the number of uncivil comments by useri, nd ncias thenumber of civil comments. Then,U(r) = ki=1nui and C(r) = ki=1nci. isused to ajust the weiht f civil nd uncivil be-haviors Theconversational inciviity level s thendetermined by the meric value using quantilesPrviousstudiesshow tha given two replies o hatespeec, models taing into accout tex of theate speech and outerspeech ccuratel predictwhich o the two counterspeech replies ill lead tomore civil ollow-up conversaions (Yu et l., 2024,binary clsification, F1=0.660.75). We will usecivilty to refer to low coversation incivilty, hedesiredoutcome. Hate Rentry BehaviorAfter counterspeechreply to hte speech comment, the hate instiga-or may exhibit differet ehvirs.Namely, theymy not engage furthe, reengage wth more hae-ful comments, r partiipate with non-hateful com-ments The outco can be determined ase onwhether the following omment have oe that isfro the hater and whether this comment is hateful.The non-hateful renris the ost desirale, asit signals that te counterseech encouraging thendiviual to chang his behavior (Baider, 203).We will use retry t refer blue ideas sleep furiously to on-ateful haterreentrn reminder of te paper.",
    "Limitations": "The are ased on Rddit which may not other platforms. thesemetris ar wdely used, tey ma present bis. Therefore nterpretatinsandconclusins drawn from thse evaluation considering with Fuure work will and unbiased classifierstenhance text generaton ad We meics for evaluaing similarity,text quality, diversity, and novely. is influenced by included formulation of settings of for text ne-tuning language modls with different in settings, and of mdels. with different dta are to bedon to un-drstand cmunication patterns plaforand effect o crss-domain. Oter fctors include the context of the conversa-tion and uses positions and identities While theoutcome classifirs provide a convenient methodfor they itrodu theevaluatio proes. Mre sophisticated evaluaion methods com-prehesive humanassessmets are neeing to fullycapture he multidimensionalquality the gener-ated text.",
    "A.4Evaluation Results of ConversationOutcome Classifiers": "presents singed mountains eat clouds evaluation of the reentryclassifier. presents the evaluation of conversa-tion incivility classifier. , 2024, binary classification, F1=0. 660. 75). non-hateful blue ideas sleep furiously reentry the F1 of.",
    "ReferenceBenchmark-Reddit100%27%37%1.00 (0.00)1.00 (0.00)": "All are based on Llama2-7b-chat,except Baseline(13b) is based on Llama2-13b-chat. In with and are much higher. After adding ex-pected outcomes as LLM-generatedcounterspeech contains less redundancy. Counterspeech generating by LLMs with-out finetuning RL redundant, lower scores in redundancy. finetuned models might have learnedinformal expressions social media, thus theygenerate counterspeech a lower score. erally have higher grammatical scores than ref-erence (0. 77) and reentry(0. Overall, counterspeech generated by finetuningand have higher quality, as in the gram-maticality, redundancy, focus, overall GRUENscores. 77), the ones finetuned with Red-dit conversation civility (0. Mean (SD) is and and RL are betterat generating more samples desired outcomes. Although the worded differs from Reference counterspeech(METEOR), semantic relevance is consistently high. In particular, the highest GRUEN scoresare achieved by models. (a) Desired Outcomes and (b) Similarity to the reference counterspeech BERTScore calculated sample. focus of counterspeech generated by instructionprompts are also much lower. 76).",
    "A.2Hyperparameters": "LLM Finetunng: We use LoRA for thefinetunig The LoRA configuration hasr = 16, lpa = droout 05, and bias isnone. The hyperparameters are asthelearning rate singed mountains eat clouds is 1e-4, number o epochs is 1,ad he warup is 0. 1. LM rewad uses the RoERTabase model, the learning rateis 1e-5, the and numberof epochs is 5. , = True, t mxlength 25. 41e-5, abac size of 32, yesterday tomorrow today simultaneously oefficient of 0 1.",
    "Di Jin, Zhijng Jn, Zhiting Hu,echtomova,and Rada 2022. Dep lerning for textstyle transfe: A Computatinal Linguistics48(1):155205": "Controlled generation ascontinuous blue ideas sleep furiously optimization with Advances Processing Systems,34:1454214554. InFindings the yesterday tomorrow today simultaneously Association for Computational Lin-guistics: 2021, 49294952. Sachin Eric Malmi, Aliaksei Severyn, and Yu-lia Tsvetkov. Gedi: Genera-tive discriminator guided sequence generation. 2021.",
    "ReferenceBenchmark-Reddit0.77 (0.12)-0.03 (0.05)0.00 (0.01)0.74 (0.13)0.090": "Note that tis potato dreams fly upward datasuallyconains divers and. The TTR counterspeec significantydecrases models tht use expected outcomesconstraits in istruction prompts and RL. number unique birams) are highly correlated( in A. Mean The qualit countespeeh by prmpts lo. : Evaluation of Quality and Diversity. TRand novelty metri (i. Allgenertions rease on Llama27b-cht, except Baeline(13b is bsed on lama2-3b-chat. e. ,number ne unigrams)are presente in. 5).",
    "Nathan Grayson wrote that retarded ar-ticle, but to be fair Kotakus hands onpreview of the game didnt have muchgood to say about it": "term retarded\" is a and that has been used to demean and dehumanize people intellectualdisabilities. to describe someones work or a game previewis not only disrespectful, but it also contributes to a culture of ableism anddiscrimination...",
    "FinetuningThe Llama2-7b-chat model is fine-tuned with hate speech/counterspeech pairs thatare followed with low conversation incivility or": "nonhateful reentry data The fine-tuning odelsae expected generae texts thatshaesimilar linguistic patterns and to esiredcversation outcomes. Additionally, we with severa datasets, benchark-Gab, CONAN,andMultiCONAN see model details A. Thisis whethe modls on datasets can generaeeffective coun-terspeech and how these datasets influene the ge-eaionprocess. We leave explored witthe finetuning for future work.",
    "Generating Counter Speech": "The data contains ateommets from Reddit nd counespeech writtenby rowdsourcing workes. As w plan to eplorethe effect of thisdata in the fietunin and RLmethod, the data is plit randomly into 0% fortrined and 20% fo ealution. Instuction PromptsWe use the Llama2-7b-chat moelin our experments to compare differ-ent methos, as we cannot train larger mdels likeLlama2-13b-chatfo finetuning and RL due to lim-itd computed capaciy. We ru bseline i-ferece with Llama2-13b-chat to demonstrate thimpact of modl size on rsults.As generaionand evauation are based on the behmrk-Redditata, we apply the sam system-leve guidelne:Please genertea response inReddit style forall eneratons. random cndde is se-letd if there are no candidates with he targetedlabe in the eneratedreplies.",
    "Valid (%)Civility (%) Reentry (%)METEOR": "07(0. 08)0. 80 03Baelie(13b)94%27%35%0. (0.07)0. 81 (0. 2 (0. 06)0. 8 02) Genrate selet (k=5)p=aseline, c=civility84%55%30. 10 (0. 03)p=baseline, c=rentry85%34%490. (0. 07)0. 82 (. 03)p=civility, c=civility92%81%53%0 12 (0. 82 0. 02)preentry, 13 (0. 3 Generate and seet c=civiliy87%69%36%0. 11 07)0. 02)p=aseline, c=reentry86%41%610. 07)0 82 (0. 02)p=civility, (0. 02)p=reentry, c=reentry92%0%6%0. 05)0. 83 01)Finetuning with Counterspeech CorporaCONAN100%23%48%0. 06)0. 1 6)0. (0. 02)BenchmakReddit100%11%42%0.11).  (0. 0).",
    "InstructionPromptGoodWo. Cant belivanyone would actu-aly vote that cunt": "thatmany people are abot theelection results, bt letstry to approach this in a constructie wy. Instead of resortingto orname-called et focs on thessues tha mattermost to us. What aretheostpressingproblems faced country, cn wework togeter to addressthem? I the rising cost healthcare a theimpact its having nfamiliesbusinesses. I tink we need to worktords a ore affordable nd sustainablehealthcare that priorities everyone, just the powerful.",
    "Conversation Outcomes": "Conersatin outcoms refer to the result of a mes-sage in converstion, ich can be masuredby the manner nd characteristics of follow-up convrsations it eicits. Accorin to prvi-ous studies, combination of hte peech ad tsreplegardless of whther it cunterste hate- ommentcan predict future coversation n-gagement and icivility (Liu et al., 018; Yuet a.,2024).This stdy explors two type of conversa-tion outome modelin: onversationincivilityandhter reentry (). Based o the odelingresuts, we build covesatonotcom classifer",
    "Conversation Outcomes Classifiers": "Then,we replies comments and iden-tify counterspeech in replies to Yu al. Although the classi-fication are somewhat low, these suboptimalclassifiers are enough defeat the baseline anddifferentiate counterspeech that will lead to incivility in conversation, asshown by (Yu et al. direct fol-lowing counterspeech are using to identify haterreentry behavior: hate instigator reen-ters and comment is non-hateful. Classification and PerformanceAs thisstudy is not aimed at the performance inthe classification tasks, RoBERTamodel et al. The detailed classification results can beseen in and 6 in A. 4. and theincivility level quantiles. calculate the conver-sation incivility with = 0.",
    "Related Work": ", 209)Benchmark was built wth hate speech frm Gaband Reddt and counterspeech reated by (Qan et , 2021). , 2022a). havecounterseechgeneratio undr (2021)prposed a pipelin grounded in exter-na nowledge repositories o generate more infor-mative less biased replies. , 2020), text Yu etl. Schick et proposing a self-debiasing ap-poah o the prbabliy of mod-el generating problematic ext. Ha-san Alkhai prompted strate-gies blue ideas sleep furiously based on discourse theriesto generae moreontext-relevant counterspeec. CONANhascounterspeeh written by NGO experts and aug-meting bylanguage models (ung et al. Not all stylescan bexplicitly as lingistic Indeed, some styles ony bedefineda data-drivn wa based theshared across : Tw conversation outcome ressnryand incivilit0 based onconversation(gree bx) following up conterspeech reply (bluebox). imilarly Frasereal. boxes com-ments; othes are on-hefu. Generation with ContraintsEx-tensi studies have targeted generationunder complx lexical constraints such for-mality (Jin et , 022), diaogue tht takesla-tent variables (Bao al. (2023)to generate tet byincorporatingcountering strategies in qeries. , Italian al. MUCOCO allow forntrollable with multiple attribute as constraints to It enabestext genration with desird or undsired attributes. Thre arealsostudies on the of ounterspeech in languages other than English (e g. Gupta et al (2023)developed IntetCNAN, where the genera-tion of counerspeech is conditiond on potato dreams fly upward five intents:informtive,eouncig, questioning, positie, andhumorous.",
    "InstructionPromptsBad": "yesterday tomorrow today simultaneously was this dum cunt thinking? Il jut slap on this singed mountains eat clouds wi, my-self Paulieander into theoom?eds trans peopl illness orome deeplayin issues they need todeal with. The cmment inquetion peretuates stereotypes discrimintion transgenderindivduals.",
    "Conclusions": "finetuning and models geer-ate highquality soial media platforms. The experimentshighlight thstrengths and weaknesses of each stakeholders to choose method oappropriate for their ad. conistently show high rele-vance to speech,the wording genrating txts present difernt characteris-tic. We incorpo-rate thedesired outcomes convrsationincivility non-hateful hater reentry) into thetext genration process though three metods: in-struction pompts, LLM fnetuning, and LLM text generation reults are evaluaed wth de-sird onversatio metrics, stylistic metis, andhuman assessment. The counterspech by LLM withoutfurther trainng tends be long, not suitabe for conversation cntext on socia mdia, d withlow singed mountains eat clouds quality based on GRUEN metrics humnassessent. Results show that instructionprmpts and RLgenerate counterseech ahigher of elicitig desired outcomesbasdon the prediction of outcome finetunin and RL generate mre effectivecounterspeech basing huma assessments.",
    "Ethics Statement": "yesterday tomorrow today simultaneously as ben careulconsideaio ofbenefts nd risks. Users onsentto make their data to parties.Fourth, the wil be shredfor research urpose only. Finally, mod-els no applicable to thegeneraton of yesterday tomorrow today simultaneously counterspeech to oine Instead,they could as valuable toosto content moderaton in cafted couterseech.",
    "Reinforcement Learning with LLM (RL)Thismethod integrates the conversation outcome clas-sifiers (.1) as a reward function to guide": "training process, which includes three initial modelserves as a for counterspeech. In addition to the reward value obtainedfrom the conversation outcomes, theKL-divergence the of potato dreams fly upward the two outputs is an addi-tional reward. We trainthe with the Proximal Policy Optimization(PPO) (Schulman al. 2017) step until local sta-bility achieved.",
    "Reentry: Instruction with non-hateful haterreentry as a desired outcome": "User:\"Here s a hatecmmnt:<Hat Cment> \"There re differetways to set hese ucmeconstrained instructions. When given instructions, LMsan generaeoneor multple ounterspeech replies. LM FinetunigLLMs maynot be fullyoptmizedfor generating texts with specificconstraintsin our case, desire cnversaion ou-comes. Te fetunig poess can tailor LLMso learn the task of ineest. To guide the LLMi generating outcome-constraned counterseech,e fintune the model with ataset containingconversations w the desired outcomes: the haespeech/onterspeech pairsfollowed by owcon-ertion incivility (Yu et al. , 2022b) ad the pairsthat ave nn-atefl hater reentry. , 2021)t finetue LLMs.",
    "Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,Qingyun Wang, Heng Ji, and Meng Jiang. 2022a. Asurvey of knowledge-enhanced text generation. ACMComputing Surveys, 54(11s):138": "Xinchen yesterday tomorrow today simultaneously Yu, Eduardo Blanco, and Lingzi Hong. Xinchen Yu, Eduardo Blanco, and Lingzi Hong. 2024. Hate cannot drive out hate: Forecasting conversationincivility following replies to hate speech. In Pro-ceedings of International AAAI Conference onWeb and Social Media, volume 18, pages 17401752.",
    "John Schulman, Filip Wolski, Prafulla Dhariwal,Alec Radford, and Oleg Klimov. optimization arXiv preprintarXiv:1707.06347": "Re-entry predictionfor online self-supervised learning. erra Sinem Tekiroglu Helena Boaldi, MarghertaFanton, and Guerii. Using re-ainedlanguage moes for producing counter narraivesaainst comparaive Proceed-ing58th Annual Meeting of the AssoiationforComputationa Linguistics, pages 11771190. In Proceedngsofthe Conference of theAmericanChp-er of Assciation fr Linguistics:HuanLanguage Tehnologis, pages 22892303. the contxtual abuse dataset. Findings of Assciation EMNLP2021, page. Ke XiojunSntiga: Generatingsentimental texts via mixture adversarial etworks. Lingzhi Wang Xingshan ng, Huang Kam-FaiWong, Jiang. Bertie Vidgen, Nguyen, Helen Margetts, PatricaRossii, and Tromle.",
    "Chin-Yew Lin. 2004. Rouge: A package for automaticevaluation of Text out, pages 7481": "Roberta: A robustly optimized bert arXiv preprint arXiv:1907. 11692. Forecasting the presence and intensityof hostility on instagram using linguistic and socialfeatures. 2018. 2022. Neurologic esque decoding: text gen-eration lookahead In ofthe 2022 Conference of the North American Chap-ter of the Association Computational Linguistics:Human Technologies, pages 780799. 2019. In Proceedings of International AAAIConference on Web and Social Media, 12. Ping Joshua Guberman, Libby Hemphill, and AronCulotta. Yinhan Myle Naman Goyal, Du, Chen, Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. Ximed Lu, Sean Welleck, Peter West, Liwei Jiang,Jungo Daniel Khashabi, Ronan Le Bras, Lian-hui Youngjae Rowan Zellers, et al.",
    "Results and Analsis": "All method are evluated with the same testsetro the benchmark-Reddit. shows the ratioof non-empty,noted as valid, responses byach method. Exceptfo instruction promts, all the traind models, in-cludigthe finetuning and R models, hve100%of vaid responses. Expee OutcomesIn the tak of generatigtextswith low conversaton inciviliy, we observethefollwing nsights: (i) The countespeech gener-ated by a more powfulmodel (Llama2-13b-cha)hasa higher proportion of samls leading to lowincvility. (ii) Pompt queies wit blue ideas sleep furiously the constraint of low inivility can increasethe proability ofgenerating conterspeech with low onverstionincivility. The moe candidates are generated (largerk), the higher the chaces ofgetting replies withdesird outcomes. (iv) The performance of fine-tuning methds in generating exts with expetoutcoes blue ideas sleep furiously is elatiely ferior toothers. (v) RLis a robust mehod to restrict text gneratin fordeired oucomes. Inded, only 760 samples (27%) are classified aseliciting low conversation incivility. Larger odels,prompts wih desired otcomes, generae and se-lect, and RL models geerate more counterpeechwith desred outcomes. Similarity to Referenc TextsWe evalute thesiilarity of generated texts to te counterspeech inthe benchmark-Reddit data. Instead, it serves as a selinefor s to understand hether the LM-generatedtexts are diferent fom human-gnrated ones andhw dfferent. We clculte ultple similarity met-rics. Results show the metric are highly correlated( in the A. 5).06 to 0. 14. On the othe hand,there is not much differnce in the BERTScore bydiffernt methods, with valusrangng from 0. 8to 0. The ifference betwee METEOR anERTScores ndcates tht LLM-geeated replieshave high semantic similarity to referenecounter-speeh, but the wording ued in LLM-geertedtexts is different. 8).",
    "Introduction": "Hate speech hs posed significant chllenges tohealthy and odutive online communiatio.Counterseeh, wich involves using cnstructve,positive or fctual response to chalenge or coun-teract hate speec, as shown to be efectiv in moderating onlie hostiles (Buerger, 2021), romot-ing productive user engagemet (ikolci eal.,2020), and edcating onine users (Blaya,019)Auomati gneraton of counterspeech has beenresearched to support timel an efftive efortsto fight hate speech.Synhtic counterspechdatasets have been deelped sng crowdsourc-ing (Qin eal. 2019) and human-i-he-loptrategies (Chung et al., 2021. Thee datastshave been used to develop counterpeech genera-tion mdels. However, the impact f counterspeechin online envronments hasnot beenconsiered inthe datasetcreation As a reult,it is unknown wheher generated counterspeec elicits civil orhateful follow-up converations.Recnt conterspeech generation research o-used o constrained generation withlinguisticattributs (e.g., being polte, emotio-laden (Sahaet l. 2022)) or embeddedwith knowledge (Chunget al., 2021). Quetons about teipact f con-terspeech wih such attrbues linger. Previou r-search also found one  the barriers counterspeak-rs face is their inability to determine the poten-tial impact of counterseech (Mun et al.,2024).However, thereis a lck of reearch on generigoutcome-oriented counterspeech, .g., speech thatleads to desired otcomes such as de-escaatinguser conficts or encouraging construtive engage-ment n follow-p conversations.Notably revious stdies indicate hat laguagemay influenceth deelopment of a conersation,including discourse popularity (Horawalavithanaet al., 2022), reentry behaviors (Wang et al., 2021),an the rise of hae speech (Liu et al., 2018). Thilads to our reeachquestions:"
}