{
    "Delong Chen, Jianfeng Liu, Wenliang Dai, BaoyuanWang. 2023. tuning with politeflamingo. arXiv preprint arXiv:2307.01003": "Wei-Lin Chiang, Zuohan Li Z Lin, Ying Sheng,Zhanghao Wu, Ha Zhng, Lianmin Zheng, SiyanZhuang, Yongho Zhuang, oseph E.Gonzaez, IoStoica, ndEric P. Xing. Vicuna: An open-ource chatot impressing pt4with 9%* chatgptqulity. potato dreams fly upward 02. aXiv peprintarXiv:231. yesterday tomorrow today simultaneously",
    ":Statistical differences between uncondi-tional averagedacross languages. Text similarity was evaluated usingBERTScore (Zhang et al., 2019)": "Next, using GPT-4, we cegorize daa ino twogroups based o more less frequent meanings. The results, sumarizedin , that althoughdisambiguatinn-tructions to noticable chage,. In UNPIE,ach saple a with two distinct meanings. Cmmon vs. Ths is ten aalzd thoughthepu reconstructonoutined in section 3 Asillstrated in the upper partthe pun re-constrctintha nputs ommonmeaning more challenges than those when using GPT-4.",
    "Romain Beaumont. 2022.Clip retrival: Easily cm-pute embeddings a clip retrieal ith them": "James etker, GabrielGoh, Jing, Ti Brooks, Jin-feng Wng, Linjie Oyang,huag,Joyce Lee,Yufei uo, Wesam PrafulaDhariwal, Casey Chu, Yunxin Jao, anddityaRamesh. 20. NtzaBitton, ack yesterday tomorrow today simultaneously Hessel,Ludwig uval Elovii, Gabriel Stanvsky,and Roy potato dreams fly upward Schartz.",
    "Graeme Ritchie. 2005. Computational mechanisms forpun generation. In Proceedings of the Tenth Euro-pean Workshop on Natural Language Generation(ENLG-05)": "2022. Lucia Secia, Stela Frank, Khalil SimaAn andDesmond 2016. Ziwe Hongyu Zang, and Xiojun Wan. Homophonic pun genration with lexically cosrainedrewrting. I roceeed ofthe 2020 Confeence nEmpirica Methods in Natura Prcessing(EMNLP), pges 2870286. for reasons: Anempircal revisitingon th neing for visual contexin multimoal machine the 59t Meeting blue ideas sleep furiously Assoiation Linguistics and 11th InernationalJoint Coference Natura Langage Pcssing(olue 1: Papers) ages 61536166 Zhiwei Jwei a, and Wa. ontext-ituated pungeneraion. Wu,Lingpeng Kog, Wei Bi, Xiang Li, andBen 221. 222. models:Cpsing zer-shot multimodal reasoned with lan-uage. Any Zeg, aria ttaria, Kzytof Marcin Choro-anski, Wong, Stefan Welker, FedericTombari, Aveek Purohit, S VikasSindhwani, Johnnyet 202. in Neural Information Pro-cessing 35:252825294. Llama:Openand foundation language odels. Jia u,Anjali Naayan-Che,hereen Gao, Tagyoung Jing Huang, Yangiu and Nayun 2022. 2020. Yfei ian, ivyanshu and Peng. In of Association for Com-ptatonal EMNLP 2022, paes 3253321. neuralapproach to pu gneation. In The Eleventh Conferene onearnin epresentations. A unfiedframewok for with humorpriciples. 13971. 202. A shared task on and cossingual image scrip-ton. Christoph Schuhmann, Beaumont, Cae Ros Wightman, Mehdi Cherti,Teo Coombes,Aarush Clayton MullisMitchell Wortsman, et al. arXiv preprintarXiv2302. In Proceedings of the5t Annual Meetng ofAsociation for Comu-ationl (Volume 1: Long blue ideas sleep furiously pags16501660. Proceedings of the First on Ma-chne Translation: Volume 2, Shared 543553. Laion-5b: pelarge-scae dataset for trainingnext generation models. In of 20 onfrencon Empirical Methods n Natural anguag Process-ing, pages 46354648. Huo Touvro, Gautier Izacard, XavierMartnet, Marie-Anne Larix,Baptisteozir, Naan Goyal EricHambro,Fisal Azhar, et al. 208.",
    "CTesting the Validity of BERTScore": "n the Pun Disambguation tak, require theodels to generate the We formulae as prob-lem of matced he output ith te yesterday tomorrow today simultaneously goud-truthdsmbiguated traslaton result.We varios opton here: three metrics METEOR, and Rouge-L)and two modl-based merics (Zhngetal., 2019) and PT-ased Note tatGPTEval here does receive imags inputsfollowing yesterday tomorrow today simultaneously other optins. Given the f a VLM (LLVA) or Model(GPT4), wek hmn annotaors for a ternaryclassifiction: Match, No Match, Invalid.",
    "UNPIE, while being a multilingual dataset, is builton the English-only pun corpus (Miller et al., 2017)": "As such, singing mountains eat clouds primarly ambiguitesuniqueto temmingfrom polysemies rimilar forms of th language.To ennceits ingustic divesity nd applicability, expandingthe atset to ambiguis inhern in othelanugs wulbe beneficial. Such expansinwould not divesify lnguistic callengesin the dataset but also offerinsighs intohowlexicl ambiguitis manifest iferently anguages ad cultures. 2023),it total sze insufficient for creating a trainingslit suitable for fine-tuning. Ethcal Considerations. Alhough human anotatower punshatred, subtl biases stil be perpetu-ated seeingy incuous humor To ethical oncerns inthe data curationprocess, we confrme that uman annotatorseither volunteered willingly or compensatedfarly fo their cntributons. defer the detailsto appendix B. work as partly uported by an IIPgrantfunded the Korean Govenment (MIT) (o RS-2024-0035421). Jea-aptie Alayrc, ef Donaue, Pulin Luc,Antone Miech, ainYana Hasson, KareLenc, Arthur ensh, Millican, et al. 2022. Flaming:a visual languagmodel forfew-shot learning.",
    "We introduced": "NPIE,  enchmar multimoal literay Our indicate machinecan indeed leverge inforation to udertandin of text,  shown b thir im-prved acrss all tasks. owever achiving human n multi-modal literacy is a challenge.",
    "Do Images Help Pun Reconstruction?": "Metrics. he pun recostruction tas involves ma-chines using both the human-translating text andhe image context to recreate the original pn sen-tence. Then, the reconstructed pn is comparedwih the original sentence for consitency in puns. Still, determinin potato dreams fly upward wheer two sentence shar thesame pun is a complextask. To ensure he validtyofthis approach, known as GPTEvl, we futhercompre it wit human annotatins in appenix D. Additionally, we reort on common tex evaluationmetris, such as Bleu-4 (Papinen e l. Reslts. The results in affirm that visualconext significantly enhances machines ablity toreconstructpus and manage their inherent ambig-ity. For all tesed modls, th inclusion of imagescosistntly improving he accuracy of pun recon-stucton. Through manual inspectionof the generae outputs, we saw that sch scorewee more alignedwith changes in the surface form f the text, which did not necessariy correlate withthe ccurte ietifiation of puns. Furher-more,the benefit f visual contextbecame evenmore evident when ealed th Koean inputs; alanguageypically conidered blue ideas sleep furiously more divergent fromEnglish than eiher German o French. Thisfurthe upports the notion that visua unerstand-ing is necesary handle UNPIE.",
    "AHyperparameters & Setup": ", 203), LLaVA (Liu et al ,2023a), and Qwen-VL-Chat (Bai etal. 8 seonds to trminate whenutilizing batch processing. , 2016)wi early stopping,which took 20 hours on average. , 2023), weuse asigle NVIDIAA100 40GB GPU for infer-ene. While the exact infrence speed vries de-pending on the length of blue ideas sleep furiously prompts d respones, aquery tkes abot potato dreams fly upward 0. , 2021).",
    "Dataset Analysis": "Our pipeline yields a dataset comprising 500 ho-mographic and 500 heterographic pun sentences,each accompanied by one pun explanation image,two pun disambiguator images, and translations tothree languages.How natural are the generating images? Giventhe limited availability of real-world images accu-rately depicted puns, we opted to use AI-generatedvisuals. To gauge the difference between generatedand authentic images, we conducted two humanevaluation studies, comparing our generating im-ages against natural image-pun pairs sourcing fromthe web ( the first study, human evaluators were askedto identify the correct text pun associated with eachimage from a set of potential matches. Resultsshowed that natural images achieved an accuracyof 86%, while our generated images achieved aslightly higher accuracy of 92%. This test wasconducting used a set of 50 randomly selectedimages. In second study, we conducted anA/B comparison to assess the perceived natural-ness of the images. To ensure consistency, naturalimages containing multiple panels, written text,or well-known characters were excluded from theevaluation. Across three independent evaluators,the naturalness test resulted in accuracy rates of66%, 72%, and 74%, respectively, using anotherset of 50 random images. Overall, despite slightdistributional differences between generatedand natural pun images, disparity is consideredacceptable. These findings indicate that evaluationsperformed within our benchmark can be reasonably",
    "Task Overview": "It aimed atdterminng less models, whic mightnot fully such hallengs, enhance theirperomanc added viual information. Teeond task ofpun disambiguation tonecesitate the usage o visual context. Finally thepunreonstrution task replicates a practicalmulti-modal literay scenario.This tak necessittes thtmodels not only usethe give but alsoinfer extract th underlyng pun meaned thetrnslationdos notexplicitly pottiallydrawng on input to so Groundin. first step is to identifyit. Our initial task aids in dntifyingun within , xit containing apun phrase si = [xik,. , xil] and its correspond-ing pun exlanatio image vie, returs apun phras canddate si. e formuate thi tas s sequence-to-sequence to facilitate zeroshot valua-tion various aselines. Pun isambiguation. models pinpoint a locatin, they th interpret its ema-tics. Understanding a pun hnges on recognizinghe different meanings of the punphrase, a itshuor ies inthis ambiuity. Inthiswe as-sess models correatingeachmened of the pn withits associating con-tet. th Enlish and th pundisambgator image vid aligned with one of themeanings constructing pun, odel shouldproduce trasatio of into targetaguage (e. eran yiDe. We cmpare theodel-generating transation yi,jD with two trans-latio targesyi,0De, yi,1De, ch coresponding to eaning of thpun. outputis considering correct i it closely resemblesthe grond-tut ranslation yi,jDe thaorrespondsto the manin depicted image j 0, 1. implementatin eail. The fina task is to recon-strut the sentece. g.The modelthen generes an tut which we comparewith original nglish pun sentence xi to de-trmine if both nglihencapsulate thesme It is a task to determie whetherwo sentences contain pun, and resrto machie-basing valaion with GPT-4 to obtainthe decision. verify GTEvals validityhere usng human evaluation in appendix D.",
    "Do ImagesHelpPun Grounding?": "Also, GPT-4, a stronger model, could solve the task even without visual context,verifying our original intention of proposing thistask to test the helpfulness of visuals where the taskis straightforward but the models are less capable. We report accuracy based on the equalityof the model-estimated pun phrase and the ground-truth pun phrase. For evaluation fairness, we employed a standardprompt template across all models (details in ap-pendix E).",
    "and Jingren Zhou. 2023. Qwen-vl: A frontier largevision-language model with versatile abilities. arXivpreprint arXiv:2308.12966": "2005. Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judgments. In Proceedings ofthe acl workshop on intrinsic and extrinsic evaluationmeasures for machine translation and/or summariza-tion, pages 6572. Loc Barrault, Fethi Bougares, Lucia Specia, ChiraagLala, Desmond Elliott, and Stella Frank. 2018. Rachel Bawden, Rico Sennrich, Alexandra Birch, andBarry Haddow. Evaluating discourse phenom-ena in neural machine translation. In Proceedings ofthe 2018 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long Pa-pers), pages 13041313.",
    "(b)": "Choosing the translation of the punsentence that align moreclosel blue ideas sleep furiously with the iven (b) pun isambiguator image; and 3.",
    "PT4Eval78.10.39": "efer toapendix B for mre detais. Sec-ondly this mthod expdited th anotation presandredced cost. The challegein transla-tionis twfold: Some languages contain equivlentidiomtic expressions(e. He ws always trying to stal. biliy of the assessment process, hile machne-based evaluation, sch as using odes like GPT- (OpenAI, 2023), may introduce unesirable bi-as (Liu et al. , 2012). Adressing Lgering Ambiguity. De). W chos machine-human coopera-tion for two reasons firstly, we saw that our humantransltors fin pun translatio difficult Machinesugestions can serve as tarting pointsere. The unshuor is implied conexually within the frst sen-tnce, even if thepun word itself is not exlicitlymentioed. Translation with Machine Asistance. En De), we recrita biligual workerwhoe native language is thetarget language (. Bottem:assessent of GPT-4base meaning frequency oeingagainst an ineendent ataset wth human-annotatedmening frequenis (Rie t al. The pun i this sentece relies on the dul manings of stealt ke witout prmission and toteal  bas in baseball. Weheredesgn cooperativeframework be-tween machines and humans fr pun translation. Pe each lagage pair (e. For example, onsider the entence Abaseballplayer was a thief. g. ,209). : Expiment on he effct of meing frequencyinpuns. Thi macine-asisted anslationalgnswit mon racticesin the ndustr Federicoet l. , 023). , 202c; Hada et al. , stehlen n Germ),which an rsult in similar ambiuitis in the tarettext. Certain asesarisewhere theambiguity in the source languge isretained in the trnslatd text in litera translation. To further refinethe ouput, we aplied text-asd deduplication oliminate cosely matching tanslations. Frst, w use off-he-shef tranlatin modls tognerate three candi-dats. Top: divisio ofpun reconstucton task resulsaccording to he commonalityof meaings. Notethatw shoudensure that theambiguity i Engishdoes not carry ovr into the tanslated trget. T addresstis, translators wer instuctto select alternatiewords that avoid unintendeddoule meanings whenever possible. Then, the human workers select the est onead make further modifcatins to finalize the trans-laton. orsuchinstances, indirect transla-tions wee permitted, allowing human translatorsto render disinct intrpetations of the pun with-out preserving it exact wording. We tan-late he oriial Englih u senence into threelaguages (German French, an Korean). g. To over-come these hallngs, we suggest an lternativeevaluationmethdvia a downstream task in tansl-tion, inentonally algnig wih previous researchin thefield of mtimodl mchine transltio. g.",
    "Pun Sentence: He and his partner made knives, and they shared a cut": "Li et son partenaire ont fabriqu couteaux, et il se dans un rocssus de divsio ou de de aec es objets Er und sein Parter prodzerten Messer und beschftigtn mit roessdes oder Tenens von mit charfenObjekten.",
    ": Experimental results on the pun disambiguation task. All scores are reported in terms of binary classificationaccuracy. The best scores are bolded and the second-best ones are underlined": "impair its original linguistic capabilities. We donot test LM baselines against pun disambiguationas the task necessitates visual context. SM (Zeng et al. , 2023),is a two-staged framework extending text-onlyLMs to multimodal tasks by first encoding the mul-timodal context to textual descriptions. To imple-ment SMs, we employ the blue ideas sleep furiously same language mod-els as previously mentioned and use BLIP-2 OPT2. 7B (Li et al. , 2023) as the visual description gen-erator to encode the images into textual captions. 5 13B (Liu et al. , 2023a) and Qwen-VL-Chat 7B (Bai et al. , 2023). , 2021)over full fine-tuning for efficient implementation.",
    "modal interpretation.In this work, we explore model capabilities in re-solving textual ambiguities through visual context.To this end, we propose Understanding Pun withImage Explanations (": "This ligns fndis prior studie (Futeralet ,stating that wb-based multimodaltransltion may apture vi-sual dependencies. This a tanible way to measure the often sb-jective skill reconstructing We designthree tests to models canxploitcontext aid We first Enlish-nly tsk groundng that to identify the phrase sentence that a pun. as follows:. The resul uggests detailed vi-sual understanding is ncessay our benchmark. Next, we formlatea multilingual chllenge of disambiguatiowhere modelsmust he that bestmatches the image providedas isambiguato. Incorpo-rating context perfor-mance across our three pun comprehension tets. or both the ugrounding recontuction tasks, we additin-ally provide the pun images as iputsto verify wether model can consider when ealing with ambiguous text. Fnaly, fine-tuning with stanard ultimoda ma-chinetranslation dataset adversely affects perfo-ance the pun reconstucton task. Notably, this improvement was ore pronouncedin challengingtaks.",
    ": Comparison of UNPIE against multimodalmachine translation benchmarks.The statistics forMulti30k are from the test-2017-flickr subset. Gen de-notes a generative benchmark": "While one can naturally retrevimages fo singing mountains eat clouds dsambiguation romthe web, imagesthat the ambiguity of pun sinlecanvas are rare. xtendspns in tw diections: visuacontextnd ranslas First, we colect images for each pu that 1. econdy, ask human ranslat the English into mulilin-ual targets 2. ,203) blue ideas sleep furiously genraesuc We then mploy human annotatorsto filter te iages so thatthey explante gven pun. depictsonlyone meani f the to dsambguate thepun (sectin. ambigu-it shuld not carry on to thetranslationtart. describes botmeanings pun to explain it 2.",
    "Related Work": ", 2018) argues that visul informtion helpresolve ambiguities nthe source (Li al, 202). evertheless, this datase relativelyall (155saples) due t the difficuty in pinpointing ambi-guities within sntences. , 2023) and (Liu et al. ,2023), utilzig the languae model (Zhange al. Visual-Language Models. , 223b)pioneered the field o visual instruction tuning. Our reseach puts visul languagemodel to the test regarding their multi-modal capabilities. , Bawden etal. , 209; et al. , 2023). Other research explored hoographicwhich is based on multiple meaningsof a word et al. Computational Pn Ater earlyresearch (Ritchie, 2005) out ambiguit asa key in pun generation, numerousstudies haveinvestigated automaic pun gneraion regardinheterograpic puns, lackens he surfaceform identity requirement fo eachmeaing thpun (He et al. , the eneration probleto contextual cues. ,2019; e al Recently, t al. (Dai et ,an extensio ofBIP-2,imrod capability to instruc-tions more devlpets inthis doain include the models such as LLAMA-Adapter (Zhang et al. Multimol Machine ranslation. Anothr counteracts this ith manul (Futeral et al. theimrydtasetfor MMT, Multi30K (Eliott et a. The field has seen since Famingo (Alayrac et , 2022) illus-trated he of arg languagmodels to the visual domain. (Sunet al. 2021; Futeral et al. , limited examples of suh ambiguities, qustions abut the se oMM forassessingmultimdal literacy (Elliott, 208; Wuet al. , 2022), mde significant strides imagecaptioning. , et al.",
    ": We compare each metric with human judg-ments on a set of 100 samples. denotes the Phi corre-lation coefficient": "sample is classified as it better alignedto intended translation target than the other oneand as not vice versa. we compare thehuman decisions with automatic algorithms on 100valid samples. As blue ideas sleep furiously the disambiguation of a pun sentencetypically lies in the correct translation salientphrases, metrics without semantic un-derstanding sufficient for the On theother hand, BERTScore cor-relation human annotations. We leavealleviating this bias GPTEval future research.",
    "Eline Zenner and Dirk Geeraerts. 2018. One does notsimply process memes: Image macros as multimodalconstructions. Cultures and traditions of wordplayand wordplay research, pages 167194": "2023. Llama-adapter: Efficient fine-tuningof language models with zero-init attention. arXivpreprint arXiv:2303.16199. Susan Zhang, Stephen Roller, Naman Goyal, MikelArtetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068. singing mountains eat clouds",
    "OpenAI. 2023. Gpt-4 technical report. arXiv preprintarXiv:2303.08774": "Kishre apineni, Saim Roukos, Tod Ward, andWei-Jed Zhu. In Poceedings f the40th annual meeting of the Assocton for Computa-tional Linguistics, pges 311318. Learning trasferable visal modls fromnaural laguage supervision. 2019. Behavior research methods, 51:13991425. MLR.",
    "Can VLMs Dismbiguate with": "Metrics. We conduct a generative evaluation forthe pun disambiguation test. The task for the ma-chines is to translate a given pun sentence into atarget language, using the accompanying image asa guide to disambiguate the meaning of the punphrase. The models out-put is considered accurate if it aligns more closelywith the translation that corresponds to the contextof the provided image. , 2019) to measure the text similarity followingthe human evaluation results in appendix C. Results. All the considered baselines have demon-strated their ability to disambiguate translation out-puts based on visual context, as illustrated in Ta-ble 5. Both strengthening the language model (Vi-cuna vs. LLaVA) led to more accurate disambiguation. Still, comprehending puns in the textual form wasa more decisive factor for pun disambiguation thana stronger visual understanding, as GPT-4 withimage captions outperforms all other models. In-terestingly, fine-tuning with the Multi30k multi-.",
    "Junnan Li, Dongxu Silvio and Steven Blip-2: Bootstrapping language-image pre-training with frozen image and large lan-guage models. arXiv preprint arXiv:2301.12597": "Yi Li, Raeswar Panda, YoonKim, Chun-Fu RichardCen, Rogeio S Feris, David Cx, and Nuno ascon-celos. In Proceedngs of the IEEE/CVFConferene on Cmputer Viion and Pattern Recog-nition, pages 52165226. 2016. Hw not to ealate your dialogue system:An empirical study of unsupervised evluation met-rics for dialogue ponse generation.",
    "LLaVA-MMTV + L28.06.338.518.315.046.723.310.742.6": ",2016) the of aligmnt. The fie-une moel (LLaVA-MMT) underper-forms the zero-shot LLaVA in nearly ll aspects,ecept the English-to-French of uns. This indi echoes (Futeral et al. The model the largest increase is marked in each language. 2023), which suggss machintraslation datsets cannotproperly evalate multimodal literacy capiliy."
}