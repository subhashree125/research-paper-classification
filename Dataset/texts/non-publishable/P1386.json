{
    "and Disclosure of Funding": "K. was supported y NSFgrnt Y. S Dpartmt of Ofice of Sciece, Office of Advaed cienifi Computing partf the CaRMNET Matematical Capability Cner (MMICC)progra,under Award Number DE-SC002164. Lawence Liermore Natoal aboratory peraed byLawrence National Security, or the U. of Eergy, Ntional Adminitratio under Contract E-AC52-07NA27344. IM reease numb LLNL-CONF-869137. Journal o Computational Physics, 242:623647,",
    "Ricky T.Q. Chen, Yulia Jesse Bettencourt, and David K Duvenaud. Neural ordinary differentialequations. in neural information systems, 2018": "Sukumar. Enforcing dirichlet boundaryonditions in netorksvariatinal physics-informed neurl neworks. Heliyon,9(8):e18820, 2023. Jeong Park, eter Florenc, Julin Sraub, and Sevenovegrove. blue ideas sleep furiously Depsdf:arnng continuousdistanc fo shape In IEE Coneence onComputer Visio Pattern Rcogntion (CVPR) June 209.",
    "Kookjin Lee and Kevin T. Carlberg. Model reduction of dynamical systems on nonlinear manifolds usingdeep convolutional autoencoders. Journal of Computational Physics, 404:108973, 2020": "Computational Physics,451:110841, 2022. Youngkyu Kim, Youngsoo Choi, David Widemann, Tarek fast and accurate physics-informedneural network reducing order shallow masking autoencoder. neural networks: A framework for solving forward and inverse problems involving nonlinear partial differentialequations.",
    "Here, is an ADF and D(x, t,) denotes the decoder output in Eq. (2)": "using he function-based As, followin challeges arise whe the nputdmension thn two. n ourcase, this holds s ADF includs timespce coordinates. To bespecific, () ADF is o on ad (ii) its second (or higher) derivativesexplde near junctions, noted in . (8) adopt more delcate in. Fo example, toxx, instead of directly computing thesecnd-order erivatives, e irst train auxiliay o approximate frst-order dervativs and",
    "Abstract": "This study presents the conditional neural for reduced-order modeling framework to approximate solutions of parametrized partial differentialequations We introduce a physics-informed CNF-ROM, yesterday tomorrow today simultaneously which includes two key components. First, the frameworkuses coordinate-based neural networks to calculate and minimize PDE residualsby spatial derivatives automatic differentiation and applying thechain rule for time derivatives. Second, exact and boundary are imposed used distance functions [Sukumar andSrivastava, 2022]. However, ADFs introduce a as their second-or derivatives become unstable at joining points of boundaries. To address we introduce auxiliary inspired by [Gladstone et al. ,NeurIPS ML4PS workshop, 2022]. Our method is validated and interpolation, temporal extrapolation, comparisons withanalytical solutions.",
    "tu = L(u; ),u(x, 0, ) = u0(x, ),B(u; ) = 0,t [0, T],x IRd,(1)": "This use of a hypentworkenables reduceore modeling by casted the problem as one o modeling latent states. u (0, T] D IR is thesolutio singed mountains eat clouds of th PDE hat represents a phsical quantity sch as velocity or pressre. u(x, t, ) can beapproximating by aneual newrk u(x, t, ), where IRd is a ector of tunable parametes. The approacmbinesa parametric nural OD (PNODE) , wichmoels laent dynamics, with a decoder that reconsucts DE oltins. commonaproach ses hyetwork h :IRdIRd to enerate a part of high-dimensional parameters = h() IRd from a low-iensional latet statRd. Folownghs approach, we define or decoder as. This structure is illustrating in thediagram shown in. We applythis framework to approximate solutns o spatio-temporl governed PDEs, rfering to its theconditonal neural field for reduced-order modeling (CNF-ROM). To separate he spatial and temporal domains,DNoitrodues the hypernetwrk whose paameters ar determined conditionally onthe time-dependent atent sate, which in our framework isextendd to aso depend on PDE parameter. Theexanded sectin o te rightside f illustrates the detailed DINo achitecture integratedwith PODE. The deoer map the latentstate and coordinats to solution, while the PNODE catures distinct atent trajetries for each. DeoderBuilding on the DIo architecture ,we mplement a decer by employed a patialcoordiate-based network ased o FourieNet. where L is a differential operatr and B is aboundary oprator. Condtinal neural fieldsWhen coordinate-base neural networks specificallyoperate in spatialand temoral domains, we refer to them as eural ields (NFs) , borrowing te trm fieds\"from physics. CNs extd NFs by introducng a conditioning ltent facto.",
    "u(x, t, ) D(x, t,),(2)": "At each nd parameter, a latt state t, the high-dimenioal parametes te shifting the learning ote decoer to the trining of hyperetwork parameters. mdel, PNODEslean thef:. filtrs s((l)x) = [sin((l)x),cos((l)x) ar sed a urebasis. Parameteized DEs (PNODEs) address thee challenesextendigneural ODEs (NODE) to incororate as inputs, allowing the odel evolutio flatenttates formultipe trajectories. PNODERecent advancements n PINNs emasize importance t acros multple PDE ( Thse divers behviors function of paraeers, avod retrainig for every newparameter, andidentif ptterns aross the parameer enaling generaliation t unseenprameter value.",
    "(b) Fine-tuning with PINN loss: LPDE() + Lderiv(), T = 1, train test": "The goal of these scenarios is to highlight the advantage using PINN as a fine-tuning objective,leveraging its learn without For scenario (b), from the pre-trained model (a),we freeze the decoder and use the PDE loss to update only PNODE (latent parametersfor the target parameters. Loss\", dashed and the PDE for PINN(PINN Loss\", solid green). The of this figure is the loss by that both exact and PINN decrease in parallel,regardless of training was based on data loss or (b) PINN loss, confirming that residual loss aligns with the right panels heatmaps of theloss = 20. This PINN fine-tuning models the low-dimensional state,aligning with the perspective. 25. Finally, conclude with the remark that can be extended to higher-dimensional problems, blue ideas sleep furiously which we leave their exploration as afuture direction. For temporal all and (b) trained up to T = and used to up to t = 1.",
    "Introduction": "recent direction hasemerged in coordinate-basing neural networks, developed to learn implicit complex signals, commonly to implicit representations (INRs). In years, networks have employed increasingly to PDE solutions, withphysics-informed neural networks emerging as an approach learns physicsby incorporating the governed PDE into loss function. simulations for solved nonlinear differential equations (PDEs) have advancedscientific understanding by enabling accurate modeling of complex physical phenomena. Some of these techniques simplify theunderlyed physics, while approach to accelerate leveraging data, as seenin linear subspace ROMs and nonlinear manifold ROMs. This approach leverages. Building on this capability, a of has been proposed that use data-drivenapproaches to learn PDE solutions using INR-based decoder. Thesenetworks attractive for PDE-related tasks they can learn continuous functionsover domains. However,the computational high-fidelity simulations have led the development of and modeled (ROM) techniques.",
    "Woojin Cho, Minju Jo, Haksoo Lim, Kookjin Lee, Dongeun Lee, Sanghyun Hong, and Noseong physics-informed networks for parameterized PDEs. arXiv:2408.09446,2024": "Implicitneral with perioic activtion functions. Matthew Tancik,Pratul Srinivasan, e Mildenhall, Sara Fridovich-Keil, Nithin Raghavan UtkarshSinghal, Ravi Jonathan aron, andRen Ng. Yun Yin, Mathiu Kirchmeye, JeaYves Franceschi, Alan kotomamonjy, Gallinari. Peter Chen, Xang, Dong Hen Cho, Yue Chag, Pershing, Henique Teles Chiaramonte, Kein Thoms arlberg, and Eitan Grinspun. Crom: Continuous redced-orermodeling pes used implicit neural representatios. In The Internatioal Conference Repreentations, 023. i Zepeda-Nunez Anudhya Boral, and Fei Sha.",
    "Lee and Eric J. Parish. Parameterized neural differential equations: applications tocomputational physics problems. Proceedings the Royal Society A, 477(2251):20210162, 2021": "FO-INNs:A firt-order formuaio for physcs iformed neural networks. PMLR, 2127 Jul 2024. Sukumarand Anit Exact imposition oudary conditions istanc functions etworks. Jan Hagnberger Marimthu Kalimuthu,Danil Msekamp and Mathias iepert. in viualcomutng an Grphics Foum, 2022. Svastava, Hadi Medan. Reduced-order modeling for parameterized pdes neurl representations n Procedngsof he Machine earned the Physical ScienesWorkshop at t 37th Conference on Neural Proceing Systems (NerIP), 2023 N. anshu Wen, okjin Lee, an Yungso Choi. Yihen Xie, Towai Takikawa, Shunske Saio Or Litany, Shiqn Yan, NumairKhan Federico Tompkin VincentSitzmnn, nd Srinath Sridhar. Rini J. Vectorizeconditionalnural fields:A framework for olvig tm-dependent artialdierntial In RuslanSalakhtdino, Koltr, Aria Weller, Jonathan Scarlet, and FelixBerkenkamp, editors, Proceedings of the 41s Intratoal Conference on MachineLearning, vlume 5ofProceedings of Learnin Reseach, pages 118917223. In fthe Machine Learningad th Scieces tthe 6th oNeural Information Processng Systems(NurIPS), 2022. Gladstne, Mohammad Nabian, N. Computer in Applied Mechanic and 2022."
}