{
    "In the TCR dataset, we provide two sets of annotations for the MeetingBank data:": "An example of the input output is show Original smmary: A bill for an ordnance changed the classification for 5611 EastIowain Virni Village. oder t summary to pre-meted agenda GPT-4 propt is developed. Approves yesterday tomorrow today simultaneously an offical mendment to reone propertylocated at 511 East owa Avenue SSU-D to S-R-25 (suburban single-unit touburban, rowhouse) Council Ditrict. If in the data souresentence belong ultiple summaries, we eep only firstoccurrence. e-annotated TopicThe original meeed summares contain not nly te topic for btoften he outcomes. Additionally some of he eting mnutes arethe transcripts so mdifying anotaios would give a more accurate of thetopic-conversation relevance benchmark. hese annotationsae singing mountains eat clouds in meetin styles are long and very detaile.",
    "ELITR5 mn70840.84290.93900.764610 min31261.81820.91840.737715 min01660.80430.8060.7475": "Fr the highly tructue meetings(MeetingBank, the benhmark results show very hihprecision recall. Most of these metings follow he agenda topics strictly and the to-bediscussed at of the Diffrent annotations do impacthe result much. Mst yesterday tomorrow today simultaneously these meetngs not haveclearstatements dieret topics nd related areoften discssed back nd forth Wealso notice that if thre multipletopic ncluded in same snippet transcrips t iseve challengin to correctly predict the relevance comparing single-topictranscript (9). This b du to fact that transitions yesterday tomorrow today simultaneously between topics are not always in the less structuredmeetngs. slit by counts are also included in A.",
    "Selected ELITR Dataset": "The ELITR data is a corpus of meetings in Czech and English onaiing transcrits ongwih minutes writn by multiple annotators.  our work focusesn English only at this sage, wekeep just th English meetings. Amog the English meetings, 49 have meeting minutes that can belined with the corespndg trncripts. We furter reduc he size of this daaet toaddres thefllowing challenges: First, with mltiple annotations availabe from u to 11 ifferent annotatorspermeetng, we need tokeep ony ne annotation per meetin. Secod the meeting minutes cacontaintoo many detieditems tat arenot suitble to be consiered s topcs. Thrd, heannotatons dont ecessarily point to a cnsctive chunk of transcript,butjmp back and forth. Toaccount forthese issues we keepnly meeings wih an nnotation ofa most10 topic, and th notationsarenot intrsperse.Wih all the filters, we include 11 meetngs intothe CR datae. If there isno noaio fr some parts of th transcripts, we follow the same lgicdescribed i .3.2to fll the emptyvalues.The original transcripts do not have timestamps, so we esimat the timeinformation with te fixed rate of 50 words pr miute.",
    "We make minor adjustments to the topic annotations if there is an identify-mismatch problem betweenthe topics and the speaker IDs. The speaker IDs for each meeting are assigned as speaker_A,": ",me001) or names. g. To alig the iffeent identify systems, we rfe tthe andconvert speaker reference in the annotations o align ith thetranscripts byusingspeaker IDs. his the data within annotations. , howr, te topic refer to speakers by eiher their I (e. etc.",
    "Methodology": "The treatingand \"Nt as positive respectively areshown n the Appedix A. thefnal result all ine stle ithas th os robust esultsacss alldata sources We use Pecision (LLM detects a topic isnot beingdiscussd andit is nd Recll ( topc is bein discusse and LLM it) s th maimetrics. 3. In the develoment stage, different reqests, floats, ntgers, binaries andmultile choices. For eac w ct th transcripts into snppeswth equal length base tiestamps We conduct the exerimets in uration of 5 minutes,1 minutes and 15 miutes. Therelvace score rpresentd by 4 level: 0 Not Relevant, 1 meanSomwhatRelevant, 2 means Mostly elevat, 3 Ver The dtailed efiniions o therelevanceare givn a a ultipl-choice in the prompt. The he prompt tes snippet of blue ideas sleep furiously the full topic he meting as inuts, blue ideas sleep furiously and asks for an of the trancris elevnce to each topic inthe lis.",
    "Data Augmentation": "With thi structure, w can tetthe elevanc betwee the transcript and topics that could hav been plannd but are not part of theatual coversation. shows an exaple of he change in metadata for anaugmented meeting. Thetimestamps f te reainig contents are also changed accordingly. If a topicis planed, but otdiscussed in a meetig, the topic is adedto the\"ariaion_addopics\" and the corresponded empty ontentsare also added to the \"tpic\" section. Pariciants smetimes go off tic and have to skip somepre-ranged topics due to time blue ideas sleep furiously lmits. All dtsources dcribedbve provie ground truth topicsfor susecions of transcrips. his cn help enrich the TCR daasetto inclde variedrane of meeting styles. Users cn easl extend his by potato dreams fly upward adding topc ith non-epty contes toexpand tesimulationfurthr.",
    "Related Work": "To the best ofte authors knoledge there is no research relted to the ask of measuing conversation elevance topre-meting agenda topis. Howeer, th related topic of meetin suarizaton, or minuting hsbeen wellstudied. Two chllenges (AutoMn) in the fild of meetg summarizaon hve been held wher teamsparticpate in oder to proress the ield . The frst cllenge hd teams using BART-basedmodels acheving the best performance . Wth te iprement in generative I andth growing adoption of Lare Languag Models (LLMs, a econd chalenge wa done recently.In ths challege, the paticipats achieved god esults with differentlarge models,such as,Llma-based Vicuna , Dolly , and PT-3s ext-davinci-003 . he challeneorganizers alsoalled out the neing to answer research questions related to transcript summary relevce, to beterunderstand content and coverage frm ifferent annotators.Most ofsch individual datasets oftn contains only one type of meeing. The tpic annotation is very brief and limited. he QMum dataset aggregaes three public dta sourceICSI, AMI, and Parlament metings from Welsh Canada) and generates minutes for th textsummaization tasks. The paperfurther shows that for a BART mode that trained a model o datafrom one of the dataset and testing it on the other one leads to poor performane. By traning on alldatasets they were abl to build a more robust model. To further xpand on dta for he automatcminte ask Neoluhko put together the ELITR data copus. This data conssts ofmeetings nboth Czech and English, with transcrpts and meeting minuting yesterday tomorrow today simultaneously beig aken by different annotators.In order to align the transcrits with the minuts th tool ALIGNMEET ws used.",
    "Synthetic Meetings": "he worflow to generate such synthet meetings followig steps reov thefirs and 5 minutesof the transcrpts, to elimnate ptential eeting setup geetings, andicebreaker These trimed meeting ar te canidate meeings. Secon,for each new synthetic",
    "Raw Data": "In total, we include 84 raw meetings (48. distribution details can be in Appendix. data natural online meeting To collect the data, we 4 participantsto join remote conference call on Microsoft Teams. Each meeting has a dedicating topic elicit debate. The participants the topic minutes. In the transcripts, the participants marked as each meeting. We only data TCR dataset at this stage asaudio is directly to the the benchmark task.",
    ": Topi-Coersation Relevance (TCR) Schea": "The scripts that we use to generate SIM_syn100 data shared in the repo. With parameters, users can an arbitrary number of synthetic meetings with the desirednumber of topics and duration",
    "Selected MeetingBank Dataset": "remove both the transcripts and notatons. of theoiginal timestamps do blue ideas sleep furiously start from 0we keep th original timestamps as it is ecessary to locatete audio contents if.",
    "Andrew Kyle Lampinen. Can language models handle recursively nested grammatical struc-tures? A case study on comparing models and humans. 2023. arXiv: 2210.15303 [cs.CL]": "2633. Frantiek Kmjec and Ondrej Team iterate@ automin 2023-experiments with iterativeminuting. DOI: 10. Tirthankar Ghosal et al. 132137. Atsuki al. the 16th Inter-national Natural Language Generation Conference: Generation Challenges. In: Proceedings of the 16th Natural Conference:Generation Challenges. Shinde et al. Eugene and Mikhaylovskiy. 2023,pp. 125. 2023, pp. 114120. Ghosal et al. In: First Shared on Automatic Minuting atInterspeech pp. 2023, pp. 2021-2. 21437/AutoMin. Team blue ideas sleep furiously zoom@ automin 2023: Utilizing topic segmentationand llm data for long-form meeting In: Proceedings of International Natural Language Generation Conference: Generation Challenges. of the first shared task on automatic minuting (automin)at interspeech 2021. 138167. Felix Schneider and Marco Turchi. 2023, pp. In: Proceedings of First Task on Automatic Minuting atInterspeech (2021), pp. In: Proc. Team ABC AutoMin Generating Readable Minutes with a BART-based Automatic Minuting Approach. In: of the 16th International Natural Language Generation Challenges. First Shared Task on Automatic Minuting at 2021. Overview of the second shared task on automatic minuting (au-tomin) at 2023. Team automin 2023: Dolly llm improvesminuting performance, semantic segmentation doesnt. Team Hitachi AutoMin 2021: Reference-free Automatic MinutingPipeline yesterday tomorrow today simultaneously with Argument Topic-based Summarization. 101107.",
    "Selected NCPC Meetings": "he National Capita (NCP) is a government agency that once amonth to for in the united cpitol region. We randomly select 20 meetings where availabl. Iorder to convert data to a structured format the DFs are conered to text files, the bodyof the text is extracte, along with the topi titles.As the PDFs not share te same structure,additional adjsments areaplied o uarantee a high conversion",
    "ew Data Collection: peech Interrpion (SIM)": "This Speech Interrupton datset is released for the irst as artof the We also creae 100 synthetic meetings singing mountains eat clouds top of aw meets",
    "ICSI Corpus": "5 second long between two singing mountains eat clouds. For long utterances from the speaker, weassign a line break in the transcript when an end-of-sentence tag occurs, or there is gap that isat least 0. use all 75 meetings singing mountains eat clouds from the Corpus.",
    "The benchmark results focusing on the \"Not Discussed\" category are shown in . We split theresults by data source and transcripts length": "4Selecting subset: we select 30 from the MeetingBank dataset as the structures of meetingsfrom this data potato dreams fly upward sources similar and can be representing a QMSum_AMI and QMSum_Parliamentare including in the benchmark because the former scenario discussions that not real meetingsand of latter is covering MeetingBank and NCPC meetings.",
    "Abstract": "The dataset includes 1,500 meetings, 22 words intranscripts, and over 15,000 meeted topics, sourcing from both collectedSpeech Interruption Meeting (SIM) data existed public datasets. each datasource, are using GPT-41 to evaluate the model accuracy inunderstanded relevance. Along withthe we also open source scripts to generate synthetic or createaugmented TCR dataset enhance data diversity.",
    "Introduction": "Since the 2020 COVID-19 pandemic, an increasing share of meetings have shifted from in-person toonline. The Gartner 2021 Digital Worker Experience Survey reports that the number of in-personmeetings dropped from 63% in 2019 to 33% in 2021 . The same survey predicted that in 2024,only 25% of the meetings will happen in person. Together with the growing number of online meetings, there are ongoing complaints about ineffectivemeetings due to a lack of focused discussions or focused tasks . Having a meeting facilitatorto keep the discussions focused is one of the meeting design characteristics enabling more effectivemeetings . Measuring how relevant a conversation transcript is to an intended topic is crucial to quantifying howfocused the communication is, and to creating tools that behave as a virtual meeting moderator bykeeping the discussion on-track. A very low rating on the relevance of the conversation to the topicmeant for discussion would be an indicator of a non-focused discussion. In practice this translates tothe problem of keeping discussions focused on a predefined meeting agenda. While there is existing work about the importance of topics serving as input to text summarizingmodels , we could not find references about work studying the relevance of a topic to a particularbody of text it didnt originate from. One of our intuitions for why this field has had little explorationis because of technological limitations before the recent Generative AI advancements."
}