{
    "Population-level Tuning": "We leverge popultion ata alongsidea PLM tomodel comonbehaioral patterns. 3. . 1Embedding laerSince we apply the yesterday tomorrow today simultaneously NLP pre-traind odelto a new modality, We createfour mbedding layes t get thelocation embedding E R , weekda ebedding ER,time-sot embeding E R and blue ideas sleep furiously event embedding E R.",
    "=0 .(3)": "These weights are then combined through an outer product opera-tion and integrated into the subsequent network layers to enhancerelevance modeling. Next, we use a Multilayer Perception blue ideas sleep furiously (MLP) to be the predictionlayer, which can be formed as,.",
    "Li,Yali Fan Li, Tarkoma, and Pan Hui. thelog-term evoluion of mobile usage. IEEE Transactions o Mobile Computing22 2 (2021), 12131230": "2022. Yinfeng Li,Chen Xiaoyi Du, Wei, Hengiang Luo, Jin, andYongLi. Autmatically Discovered User Consumption Intent in Inof the 28th AC SIGKDD Confeence on Knowledge Discoveryand Data Mnig DC, USA) (KDD or ComputingMchinery, New NY 32593269.",
    "Liu, Zhilun Zhou, Yong and Depeng Jin. 2023. Urban knowledge graphaided mobile user ACM on Knowledge Discovery from Data18, 1 (2023), 130": "Kevin Lu Adtya Grover,Pieter Mordatch. Frozen re-trained as Computation Engines Proceedings of theAAAI Conference on Artificial 7 (Jun. 76287636. blue ideas sleep furiously Zhei Lv, Wenqiao Zhag, Shengyu an, KunKuang, Feng Wng, YongweiWng, Zhenyu Chen, To Shen, Benghin Ooi, al. 023. Xngyu Pan, YushuoChen, Tan, Zihan Lin, Jinpeng He Hu,an Wayn Xin Zhao. 2022.ultmodal Meta-Learnin or Cold-Start Recommendation Association fr Machinery, New York, NY,USA, 34213430. Yuku Chen Gao, Tichi iu Xiaoyi Hengliang Luo, Depeng Jin,ndYong 021. blue ideas sleep furiously User Consumption Intention Prediction in Meituan.In Proceedingsof the ACMConference o Knowledg Dicovry Data Mining(Virtual Event, (KDD 21. Assciaton forMachiner,",
    "Influence of event seq. length on performance": "sensitivity of event sequence length. user behavior to strike blue ideas sleep furiously a balance between model effectiveness and computa-tional efficiency, we selected one-week size. To investigate influence of sequence weconduct experiments by changing input length the and with the model (MPDA). This trendhighlights our models ability to capture long-term dependencies. Theresults, illustrating , show a improvement in per-formance across all models with increasing input length.",
    "Analsis of Population-level Tuning": "Performance improvement brought by pretrained LM. We assessed how LLM contributes to modeling population-levelcommon patterns by replacing the with transformerencoders of two compares the prediction accuracy and loss themodels. Additionally, the Transformer-S,with fewer parameters, encountered challenges modeling behaviors, resulting in inferior the of the model demonstrated conver-gence.",
    "Jiaao Chen and Diyi Yang. 2023. Unlearn What You Want to Forget: EfficientUnlearning for LLMs. ArXiv abs/2310.20150 (2023)": "2023. 11970(2023). Device-Cloud Collaborative Controlled Learned for LargeVision Models. Large language models empowered simulation: survey and perspectives. Yucheng Ding, Chaoyue Niu, Shaojie Chengfei Lyu, and GuihaiChen. arXiv arXiv:2303.",
    "Yuan Yuan, Huandong Wang, Jingtao Ding, Depeng Jin, and Yong Li. 2023. Learn-ing to simulate daily activities via modeling dynamic human needs. In Proceedingsof the ACM Web Conference 2023. 906916": "ArXv ab/2305. blue ideas sleep furiously Riqi Zheng, Liang u, Tong Chn, Lizhen Cui, Yuhu Shi, and Honghi Yn. Junjie Zhang, Ruobng Xe,Yupeng Hou, Wayne Xin Zhao, Leyu Lin, nd Jirng Wen. ariv preprint arXiv:2401. Decentalized Collaboative Learning with Adaptive Refeene Daa forOn-Devce OI Recomendation. 2024. 001 (2023).",
    "respectively, where denotes embedding size": "vrious web data, the GPT2 model s ibued with etensveknwledge, comon andfundamental prnciples, showin arobust forgeneralizaton",
    "Analysis Individual-level Tuning": ", full-paramtr population predictor(FP, partal parameter tuned populatonpredictor PP). To evaluate the efficincythe lightweight predictor obainedthrough model distillation, e ompared with the tree ad two variants of the oiginal reic-tor at side, i. Ad-ditional details abot the tree be found Appendix. e.",
    "TP + FN(15)": "Where || represents the total number of classes, True Positives() denotes the number of samples correctly classified as ,False Positives represents the number of samples incorrectlyclassified class , and False Negatives () stands the num-ber samples classified as other instead And Precision and Recall refer to the precisionand recall of class .The formula :",
    "METHOD3.1Framework Overview": "Finally, after two stages of blue ideas sleep furiously PITuning, we attain personalized model potato dreams fly upward caabl accut andefficientitent predicion on he device. We introduce our PITuningframework foradapted PLM on-device intn in.",
    "A Population-to-individual Tuning Framework for Adapting Pretrained LM to On-device User Intent PredictionKDD 24, August 2529, 2024, Barcelona, Spain": "Therefore, they not deeply explore users patternsand individual differences behind the behavior sequences. motivated Maslows needtheory, propose knowledge-driven basedon generative adversarial imitation learning. the above methods only cover some in daily liferesulting in user behaviors discontinuous incomplete.",
    "CONCLUSION": "In theindividual-level tuning framework, we utilize adaptive unlearningto correct the bias in long-tail intents due to the inconsistencybetween the intent distribution on population-level and individual-level. Pedestrian and cyclist detection and intent estima-tion for autonomous vehicles: A survey. 2019. Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. Applied Sciences 9, 11 (2019), 2335. Inthe population-level tuning stage, we leverage a PLM to capturethe population-level common behavior patterns with the eventreconstruction loss to enhance the event-to-intent transition patternand obtain a lightweight predictor by model distillation. In future work, we aim to extend the number of intents anduse disentanglement methods to implement debiased learningto solve the problem of insufficient learning of long-tail intents. 2023. & Tsinghua University. This research has been supported in part by BNRist, NationalKey Research and Development Program of China under Grant2022YFB3104702; in part by the National Natural Science Founda-tion of China under Grant 62272262 and Grant U23B2030; in partby the joint project of Honor Inc.",
    "Bran ester, Rami Al-Rou, Noahonstant. 2021. power of forparmeter-effcien promp tuning. arXi preprint arXi:2104.08691 (2021)": "iayu Li Peijie Sun, Zhefan Wang, Weizhi Ma,agkun Li, in Zang, ZhouianFng, and Dayu Xe 2023. In Proeedngs of 46th Itenaional ACM SIGRConfeenceon Researc and Develpment in Infomton Retrieval (<conf-loc>,<city>aipei</city>, <coutry>Taiwan</country, </conf-loc>)(SIGIR 23). Association for Computing Machinery, New Yor, NY, USA, 10041013",
    "INTRODUCTION": "Nowadays mobile smartphones, have become object that individuals interact with in their daily lives. Forexample, use their phones to monitor sleep, wake themselvesup, hail a car for commuting, short time, at restaurants, across most activities in one Empow-ered recent booming of generative intelligence (AI)services (e.g., ), smartphone further evolve intoa personalized assistant that can perceive user needs in advance andtimely schedule corresponding services. The key pathway towardthis future is the capability to smartphone users intents,which refers to what they intend to do, based on theirprevious action sequences and contextual information .Existing mostly on predicting user intents specific for example, purchase in intent in search engines ,pedestrian intention for robots or autonomous vehicles .",
    "We provide a novel angle of adapting PLMs into the humanbehavioral domain and further resolve the longstanding issueof capturing long-tailed user preferences": "Experiment results real-world datasets superiority of our over blue ideas sleep furiously blue ideas sleep furiously state-of-the-art base-lines in terms of intent prediction performance.",
    "ABSTRACT": "Moile especially smartphones, cn ich unctinsand have developed into indiensable oos daily life. With generative AI ervies, smartphones can trans-fom t personalized assitants, needs ervices Predictinguser ntent on smartpones,and reflcting acivitieson past inteactions a potal his ison.Leeaging languagmodels (PLM) offers a yet adaptingPLMs o on-evice user inentprdictonpresent challenges.Exermental esltson rel-orld datasets demonstrate PITunings superi intent prediction perfomance, hilighting its to long-tailedpreferences and its practicity for predictin scenarios.",
    "L = L + (1 ) L(7)": "whereM M denotes the logits output of the teaher ad studentmodel respectivly, and is a per-parameter, which means thetempertur to smooth th probabiity distribuion, while is ahper-parameter to balance the imporance oftwo loss functionsBy doing o, te student model earns oth the fine-graine informa-tion fromte techer models outputndhe essential classfiationabiity, rsulting in asmaller, more efficent model that retains uchof te teacher models predictive powr",
    ": Comparing performance without pretrained": "esults ofablatin study blue ideas sleep furiously ar presentd Weobserved that the Intention Attention (IAThindring models abilityto potato dreams fly upward pproraely weights toachinent, onseqentl both and. Frthermore, wenoticing that omiion of adaptiveunlearning the models capacity to effectively handlelng-tail intents, resuling in ignificat redctionbyapproximatel",
    "where denotes the event sequence, the pre-dicted sequence, and denotes the ground of input": "The eailsof he model distillationprocess ae shown Apenix A. 3. met requirement deploymen, he model distilng method, which is to train a smallermodl(calling the o imitate the of larger model( called te teacher model). To guide trainng of moel, w design softloss for the sof tarets, which is the Kullback-Leblr Dvergencebetween the logit output of the and the sudent etork. Te oss unctioncan b formed as. Meanwhile, also utilize he cos-entropy loss to learns the correct classifcations.",
    ": effectiveness of adaptive com-pared with other imbalance handling methods": "The device moel reults, shown in ,ndicate hat tefull-parameter tuning method acheves higher performance. How-ever, its large parameter size makes it challenging to implmnt onthe deviceside. lthoughLGBM has a smaller pa-rameter count, its stability is infeior, leadin to decreasd ccuracy.Effetiveness of adaptive unlearning. To demonstrate the effectives of adative unleaning in en-hancing the accuracy of long-tail intents, we copare it withoversampling methods and foca lss. We focusd on thethreeintent with the smalest propotions n thedatasets and evalu-aed rel, whicheffectivel reflects the models perfrmance inidentifying long-tail intent. The results, show in , idicate although focal loss andoversamplng methods show someimprovement, their and still differ, indicating they fail to address the devation causedby the disparity i intenton distribution betweenpopulation andindividual lel. Thrugh adaptive unlearning, the model graduallyovrcomes biases towards these long-tail intents in ppultion-level tuning, resulting in signfcant improvements in precision andrecall.",
    "Jianling Wang, Kaize Ding, Ziwei Zhu, and James Caverlee. 2021. Session-basedRecommendation with Hypergraph Attention Networks. ArXiv abs/2112.14266(2021)": "Yikai Chaoyue Niu, Gu, Fan Wu, Lifeng and Guihai Chen. Comprehensive Meta-learned System. 2022. 00423 Efficient Session-Based Recommendation. As-sociation for Computing Machinery, New York, NY, Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, JunfengWang, Dawei Yin, Chao Huang. Syst. 2020. Shoujin Wang, Liang Hu, Yan Wang, Quan Z. In Proceedings of the Joint on Artificial Intelligence (IJCAI19). Wang, Jiang Chunyi Liu, Hao Feng, Zang and Jieping Ye. arXiv preprint arXiv:2311. Tianxin Wei and He. On-Device Learning for Personalizationwith Large-Scale Cloud-Coordinated Domain 21802190. Association for Computing Machinery, New York, NY,USA, 27892796. Masked-field for User Intent Prediction.",
    "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and GabrieleMonfardini. 2009. The Graph Neural Network Model. IEEE Transactions on NeuralNetworks 20, 1 (2009), 6180": "arXiv preprint Zezhi Shao, Zhao Zhang Fei Wang,and Yongjun Xu. 2022 Pre-training EnhancedSpatial-temporal Graph Neura Network Multivariate Forecasting. ACM, 15671577. I KDD The ACM SIGDD Conference n Knowledge Discovery and Washington, Auust - 18, 2022. 2024 Beyond Imitation: eneratig Hman Mobility singing mountains eat clouds from Contex-aware Reasoning th Lge anguage Models.",
    "layer of transformer block4dimension of transformer block7681": "Liu Minghui Qiu, Chen Qu, blue ideas sleep furiously Cen Jiafeng Guo, Yongfeng Zhang,W. IART: Intent-aware Response Rankingwith Transformers Conversation Association forComputing New York, NY, USA, Jiangchao Yao, Feng Wang, Kunyang Jia, Bo Han, Jingren Zhou, and Hongxia Yang. Bruce Croft, Chen.",
    "where W, are the trainable weight matrix and bias Theoutput of the MLP the intent distribution": "3.2.4Eventrconstructin auxiliary oss. Toimrove the modelsproficiency in accuratelycapturing event-to-intent transitio pat-terns, we emply a aske even reconsuction loss , whichecontructs th origial event sequencesbaed nthe iven par-till observed ignals, a shown in (). Specifically,wrandomly mask he blue ideas sleep furiously evet embedding ,and input them into theGPT2 model according to 2.Next, we emploan MLP to e theevent recnstrucion layer to recostruct the event sequence. Thecross-entrop loss functon s ten ued to assist ode training.The loss funtioin poplation-leel tunig an be formed as,",
    "On-device Recommendation Model": "(2) Device-sidelearned wheemodels are trained drectly n often employingcollabora-tive(3) Device-cloud integratingdviceswith modelsto enhanceperformanc. propose CLVER, acompreensive fair meta-learning frmework, which introducsa muli-task adversarial learning satisfya. We et al. Devce-side reommendesysems diege from cloud-side co-mendtion trasferring mdel from th cloud to thedevice. propose a framework based on meta-learning that captures the diti-butio ofandcmputes adative loss user-specificlearning However themehods d not cosider in thedistribution f cloud dt anddevice data, wich isnot cduciveto personalizd learning. Recent stdies intgrate met-learning into recommendtinstolearn shared global t-parameters to quicky adapt toparameters. Yan etl prpose which augmnts the local retrievig similar data fro clouds pool. This praigm encompsses primary approaches: (1)Device-side dploymnt, where models are the cloud addeployed directy nto devices.",
    "class18num iterations2000num leaves32max depth-1min data in leaf20feature fraction1early round75_10_20random state42": "(3) Current hour, curren dayof the week,current timestamp,and indcators fo morning/afternoon/evening and wek-day/weekend(4) For each categor illustratedby event ), time differenc 0between the curent tme and he time of the last ccurrenceof event , time difference 1 between blue ideas sleep furiously the time of the lastoccurrence of event and the time of the second-to-lastoccrrenceof event Given = (,,), supposevent occurred times beforethe crrent event. Ou fature Set is shown below:(1) Outpt probbilities of all categories from GPT-2. The ti of theocurene of vent is denoted as. The explanationof the mathematical formula for te tme difference is asfollows0 = (17)1 = yesterday tomorrow today simultaneously (1)(18). (2) Position features (heherin thetop 10 requent ocaton). Plase note hat if th size of the dataset is less than 20, we willeduete complexity of LightBM by setting max depth=3, nuleaves=3, _1=1, _2=1.",
    "Poblem Statement": "Thebehavor corresondngto the -t user itention represnted = potato dreams fly upward (,,,),indicating that aeventtakes plae involvng user atocatio durig time slot. Here, , , , andrefer to ID, ID, ID, event ID, respectively. Weuse U, L, E to the of users, time ventswith theirresective given by , and. yesterday tomorrow today simultaneously Therefore the quanity of istnc intents, denoting by , is typically less the total cout events, represted by. Userintent prediction to forecast future ser intent baseon past series, which can formed as.",
    "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014. Distilling the knowledgein a neural network. In Neural Information Processing Systems 2014 Workshop onDeep Learning and Representation Learning": "arXi preprintariv:310. 01728 inchang Kim, Yongjin ang, Jung Hyun Ryu, and Kim. 023. MetaLearning with Adaptive WigtedLoss for Cold-Start ecommen-daion (CIK Asociaton for Computing NY, USA,10771086",
    "Practicability Study": "In the indiviual-leveltuning stg, partculaly on the deviceside, there are limitations in storage and computng reources. This tend highlihts our models capablity to capture 03D5D1W2W3W. Toinvestigatthe impact of dta size, we conducting experiments byvarying data size in individul-eve tuning and comparing itwih secondest model (MPDA).",
    "PRELIMINARY2.1Data Analysis": "We begin with comprehensive data analysis. Initially, we ran-domly sample 1,000 users to calculate their intent distribution. Sub-sequently, we employ the KMeans method to cluster usersintent distribution and visualize result using t-SNE , as il-lustrated in . Additionally, we present the population-levelintent distribution alongside the distribution for each cluster. Fromthe figure, we observe that intent distribution varies significantlybetween clusters. This discrepancy undoubtedly complicates thetask of user-personalized modeling."
}