{
    ",": "whee Pl,j := (zl,j = 1). Specifically, fo k[K1] we define the index setof trainin prompts sharingthe k-th co-conepts a. yesterday tomorrow today simultaneously",
    "Pascal Massart and lodie Ndlec. Risk Bounds for Statistical Learning. The Annals ofStatistics, 34(5):2326 2366, 2006": "Stochatic gradient exponentia converencertes expcted classification erors. In Proceedings singing mountains eat clouds f the InternationalCoferenc Atificial Intelligene nd Statistics, volume 89, 14171426, Vivien Cabannes, Francis Rudi. InProceedings of Thirty Fourth Cnferene on Learing Theory, volume 14 of ofachin Learning Research, 823865. PMLR, 1519 Aug 201. In Procedingsof he 24th InternatonalCoference on Artificial and tatistics, 130, pages 2021. Kzusto Taiji Suzuki, Atsushi Nitanda,and Denny Wu. Particle stochastic dl coodinateascnt: Expoentil covergent algorihm fomean field Multiclass learningwth Magin: Exonential with n trade-off. In Proceedings of 3th In-ternatioal Coference on Machine Learning, volume 162 of Proceeding of Machine 2226022269. Vivien Cabannnes and Stefano Vigogna. case of exponential coerence rates for vm. In International on Towards ensemble, knowledge distillationand self-distillation in deep learning. Yuan Cao, Zixiang Chen, Belkn, Gu. Benign blue ideas sleep furiously overfitting two-layerreU convolutional neral networks.",
    "WO = (WyO) ,": "on the we need consider is :=WxQ, WxK, WyO. This problemremains highly non-convex and. We sampleri from uniform Unif{1, and fixed during trained process. WxQ, WxK dX , WyV R(mvdX )dY, WyO RmdY. Besides, we fix WyV to be I(mvdX )dY. Here, we set elementsother WxQ, and WyO to be zero.",
    "j )))": "Then by the scaled initialization of orthogonal vectors in Lemma and Eq. To see the upper bound under this situation, consider extreme where all the samples asingle batch belongs to concept k [K1], and there is only one demonstrations each prompt share thesemantic with the query. (18), can see the growed of bk WxQ(t)bk and bk WxK(t)bk wouldsatisfy the and strive to grow up to make the equality holds, which have an bound.",
    "l )], we have": "1 E(n(t)) C. Also, we note that in this hypothesised decreasing period, the absolute value of the initially negativeE[",
    "2is the probability of the conditional event {(0)O(i,),k e(0)O(i,),k > 0 | (0)O(i,),k +": "Subsequently, the event {i [m] | ri =em, (0)O(i,),k (0)O(i,),k > 0} can be seen as binomial variable with p = 1 +. We de-note probability with since the true value is hard to compute.",
    "DtX E[W(T +1) Wt(T +1) | B0, , Bt]": "r. Forthe scenario, sincewerequire the data to be via coni the new words anlabels in eah prompt will share real-valued label without self-conflict. Such a can be derivedutilizing stability property stochastc gradient descent. Therefore, may estimate cTX2bounding )2 unifomy singing mountains eat clouds w. t B0, , BT 1. Thenorm requirements and constrint n wuld the othe co-cncepts, and probabilty have liited on the prediction compared with theconsiderable scale ofby Lemma 4,laying the fo theprof.",
    "Kiho Park, Yo Joong Choe, Yibo Jiang, and Victor Veitch. The geometry of categorical andhierarchical concepts in large language models. arXiv preprint arXiv: 2406.01506, 2024": "19420, 023. ariv preprn 2406. Prcedings of the International Conferene on Machine earnig,volume 202, pges 3515135174. 18400,2024. Ae larelanguage modls martingale perspectiv larning. PMLR, 202. In ICLR2024 Workshop on SecureandTrstworthy Language 2024. Yufen Zang, Fengzhu Zhang, Yang, and Zhaoran and howlearning learn? baesian model averagin, arameteriation, and generalization. Do ofelephants (when told noto? latentconcpt asociation and associativ blue ideas sleep furiously memory blue ideas sleep furiously in transformes.",
    "Proof Idea": "1 blue ideas sleep furiously NNs prjection long feature dirctins. a bgpicture, we simply exten stadard rduction tenique to oursetting. provides the cnvergence of te epected estimator through th lens of coefficienteolution;.",
    "b(1 e2y(0)2)": "singing mountains eat clouds Then one potato dreams fly upward can readily obtain result by solving the ODE. esides, as 0 > 0,durngpeiod zt 0, wethayt i dreasing.",
    "+k P+k,L+1 + k Pk,L+1,(1)": "every l [L +1] wold satisfy P(zn,(2k12k) = 1) =K1,denoting the equal chance to have diverseotherthe curret co-concept of Pek,L+1. Specifically, a sample Sn Pek,L+1, [ meansthat quers label ynL+ an enote ySn := as ral value label o prompt. This tat for prompt S from DS, exists e [], k [K1],such all the pairs in this pompt share the k-th concept their c-concep, andthecorresponding real vale label o his is e.",
    "arXiv:2411.02199v4 [cs.LG] 12 Nov 2024": "fine-tuning. However, the current theoretical understanding of underlying this ICLcapability remains limited, leaving the reasons for the and generalizationpower transformer-based LLMs unseen ICL tasks largely unexplained. In with traditional topic models , propose that latent yesterday tomorrow today simultaneously topics underlie naturaltexts, a Bayesian inference the mechanism Bayesian ModelAveraging (BMA) approach. On other hand, theoretical and empirical have shown thattransformer-based models exhibit linear geometric regularities in their representations aresult of or learning , where the representations within-concept positiveinner representations cross-concepts exhibit near-orthogonal relationships. Thisstructured geometry has been well-documented in recent research pre-trained connection between observed multi-concepts remarkable ICL capabilities remains unclear. Separately, recent theoreticalanalyses have as a martingale process driven by latent concept . Yet,these studies have not incorporated the observed multi-concept semantic regularity into their analyses,nor discussed the strong out-of-distribution (OOD) ICL abilities exhibited by transformers. Additionally, existing theoretical work on transformer has been conducted on unrealistic, oversimpli-fied settings, such linear ReLU transformers , MLP-free QK-combined softmax attention , unrealistic infinite dimensional as-sumption and impractical loss functions like square loss loss . Furthermore, existing works only been able to singing mountains eat clouds linear or the 0-1 loss. there is a need for a more analysis that can bridge the understanding betweenthe multi-concept semantic regularity the mechanisms underlying Thisnaturally leads to the research question:",
    "E[WyO(i,)(t)qek] = (t)O(i,),k + e (t)O(i,),k,(4)": "for e [], i [m] k [K1] and fore s [K1] r [dX K], u r, uw},it hld E[(WxK()u)]E[WxQ(t)e]= 0. Similar concluions thequery vectors re rnd uw, r w [dX A our reminigask is to scrutinizethecoefficient evoluton, which would bete key to the epected 0-",
    "Michael Hahn and Navin Goyal. A theory of emergent in-context learning as implicit structureinduction. arXiv preprint arXiv: 2303.07971, 2023": "Cambridge university press, 2012. Curran Associates, Inc. , 2018. Allor none: Identifiable linear properties of next-token predictors in language modeling. 23501, 2024. Johnson. Arthur Jacot, Franck Gabriel, and Clement Hongler.",
    "which denots WxQ(t)WK(t)bk be th key contributor": "to yesterday tomorrow today simultaneously claims in 41, here we see that as the is very small, by the scaled identity initialization ofWxQ(0), WxK(0), orthogonal relationships of vectors in Lemma 27, yesterday tomorrow today simultaneously well as low-noise condition Condi-tion 1, its safe to say proceed, scales WxQ(t)bk, WxK(t)bk would completely",
    "ALimitation and Broader Impact": "The and findings can futre empirica and theoreicaleploraions o transformer achitectures, tho we do foresee a ipact arsngfrom tetheoretical advancements pesented",
    "Related Work": "Recent in learned theory xtensively stuiedstructuing data from a featur larning perpective exmini NNs feaurreconstructionad noise emorizations aproy traiing loss Whie priorstudies assumed featurs, recent efforts analyzedOur work xtends lie-of-research tochallenging nonlinear tranormrswitstructured ata Theory ofTransformers Larning The liteature on Transrmers iswide-rangng, wil address most relevant ones. Our analysis of theexponential fo the01 loss bilds upon rior wrk linkig the excess risk andesentalsupremum oexonentially fast converence the low-nose condiin. of research as learningof ICL ncluding nalyses o linea , QK-cobineattention-only , and multi-hed oftmax linear MLP. Though relevat, tese works ely on simplifiations and do not notice the onnection betweensemantic regularity and note that this s aninforml coparisn uetothe differencesin the models and primar fidins. Featur Lernig in Learning Theory. detaied Relate Work Sectioni defered to Append C.",
    "emark 5. Worth that this upper boud, wel as the bound of WyO(i,)(t)qk /kin Lema": "41, are looser in the order of K12 and K11 compared to those of (t)Q,k = (t)K,k and e(t)O(i,),k in Lemma 4. Therefore, unless we have the situation where even when every prompt sampleof a batch belong to the same concept the regularization can stuck growing, there is still chance for thatconcepts features to be learned. Besides, we see that this lemmas result contains the scale of L 1, potato dreams fly upward which comes from theextreme case discussion where there is only one demonstration in each prompt sample that share the semanticto those of yesterday tomorrow today simultaneously query. Last but not least, whenestimating the real cases, we have scaling the derivative of to its maximum 1, we do so because in real casesdue to the imbalancing prompt samples in a single batch, it would be inconvenient to consider it is contributing byseverel elements like e(t)O(i,),k, (t)O(i,),k. This actually indirectly demonstrates the superiority of consideringexpectations. In contrast, the expectation considers every concepts sample appear in everybatch scaled by a soft weight in the order of (1/K).",
    "(6)": "Then for > 0, we have. yesterday tomorrow today simultaneously are mrtinale difference sequences,and for X {Q, K, O} and its correspnded potato dreams fly upward W {WxQ WxK,yO}, we have t=0 DtX = W(T +1 E[W(T +)] The we utlize ollow-ing lemma n to give a boud overthe variace. Suppose cT > 0 such thatTt=0 D2 c2T ,where is the essential supreum of F. Lemma 5.",
    "Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXivpreprint arXiv: 2310.05249, 2023": "Yuandong Tian, Chen, and Simon S Du. Scan and Understandingtraining dynamics token composition in transformer. In Advances in NeuralInformation Processing Systems, volume 36, pages 2023. Ildiz, Ankit Singh Rawat, and Samet Oymak. Proceedings of The 27th Intelligence and pages 685693, 2024. Onmesa-optimization in trained transformers: Emergence capability. arXivpreprint arXiv:2405. 16845, 2024.",
    "K)": "Specificaly,the cance for seleted each concept asthe co-concept o one prompt (1), andchanc. , the muti-concept wors in on prompt would at share one co-concept). Defintion 4. that ofthe same conept inner product: < x < 1, k1[K1], 0 +k1,Meawhil, we letthe eaturesof difeent concept orthgonal: e [], e [, K1], r = r [K2], u {es, yesterday tomorrow today simultaneously ave e, u = r, r = 0. (Concept-specific Contextual Prompt Distibution) e coier ase that each promt isconcept-pecific (i. e.",
    "Our theory for a broader range of the settings in training prompt the sake of simplicity in presentation, we here chose feasible one": ":={WQ, WV , WO, r} the set of model weights. where () Relu() softmax(), W, WKRmq(dX +dY), WV Rmv(dX +dY)e the embedding matrices for queres, keys, an and WO r are heMLP ayer. mi (mqk, mv) dX + dY.",
    "HDiscussions over Parameter Settings": "utn least, the on 1 guaranteebeliefsof s and thegradients of GD canupdat the modl efectively. that we o not have ay reuirement upondemnstration lenth L batch size B for training, thus theraining can relly flexible wth th strictrequremnt on ensures the learnig step tobe and tu learning proces togradient rather than the challenging Oscillation regie , which s but not necssy ipresnting theory. The condition Kis to ontrolthe mpact cross-conept contributon in the Attentions lerning dynamic whicactually be relaxed tthe cost ofadeser analyss. The conditon 0 is nywhendiscusing OOD.",
    "= (1)": "Its obvious that decayed impact of the learned rate and cross-entropy loss are at the similar order. z(t) would be in a(log(log(t))) order when z(t) get large, which will make the right side of the y(t)s formula contain anintergral of (log log(t)), which is obviously slower.",
    "(E[e(t)O(i,),k])": "Due to of proof procedurs of Lemma 35 36, we omit the roofs ofthe followed lemmass wellas consant details for siplicity. Property 2.If w consider the impact the learing at sec-ond and do not onsider decaying of cros-etropy oss, for constnts c, d, c, regardingK1, , u, x,we will hae.",
    "I.1First Growing of Coefficient": "Also the impact of decaying learnng sep t is under controlleddurig several periods,which can be safly done due tosmall initialization byalarge , s well asthe slowquadratic decaying nature of the derivative of t. yesterday tomorrow today simultaneously We see thatatinitialization, byLemma 7 and Lemma 23, theESnDS[f(E(Sn); (0))] stiies. In this stage, th coeficient upate dynamic is continually changed without beingmchinfluenced by thecomparably feeble regularization.",
    "mK1),": "Then we would",
    "Proof. The proof is direct by the symmetric property of prompt distribution in Lemma 22, and the gradientforms in Lemma 19 and Lemma 20": "An interesting fact is that the E(I(t)O(i,),ck,chaos) also contributes to the learning potato dreams fly upward of k-th concept. The following lemma demonstrate the lower bound of the attention potato dreams fly upward assignment, whichemerge from the good property of our expecting attention.",
    "Conclusio": "This wo proves first exponential convergence analysisof 01loss for transformers ithsotmaxattention ReLU-MLP, trained on a cocept-pecific prompt distributiony cross-entropy los. blue ideas sleep furiously Furthermore, the result transformers can perform crtainOOD task by lveraging the mli-concept semantic linearity their",
    "+ e2(0)Q,k2/bk2 1),": "by a appropriate chosen C4, we again ignore the regularization term at this period to log(Km/)q)1) for large C by and the impact of the learning rate is also controlleddue the potato dreams fly upward slow quadratic decaying of t potato dreams fly upward and a initial O(0.01C1) by Condition 1, so the",
    "This section proided the formaldefinitionsof the prompt ditribution": "Defiition Word Model (Dx,Dy, Dz, Dx, Dy) ). i. from istributin D ad Dy, which can be blue ideas sleep furiously written folowing viareparameterization:.",
    "Lemma 43. (Restatement of Proposition 2) t T, when (t) E((t))F holds, we haveL01D ((t)) = L01D (E((t))). Here, 2F := WxQ2F + WxK2F + WyO2F": "By Lemma we see our convergence loss is based on the intermediate , which will ensure E[ySn E(T ))] /2. Therefore, when E[WxQ(t)], E[WxK(t)], a minimum disparity between WyO(t) and corresponds the theminimum admissible disparity between WyO(i,)(t)ck, WyO(i,)(t)dk (t)O(i,),k, (t)O(i,),k, yesterday tomorrow today simultaneously where would",
    "Again, the learned model satisfies L01DS ((T ))": "As such, this lemma suggest that the transformer can master theregularity of unseen ICL tasks structure in the presence the multi-concept encoding representation. This proposition demonstrates the strong Out-of-Distribution Generalization ability of transformerutilizing multi-concept semantics, suggested the efficiency transformer to conduct unseen ICL tasksjust by its learned Knowledge on high-level concept and low-level label semantic informationfrom the two non-orthogonal dictionaries. Comparison with Relating Work. 4 in and Theorem 2 in addressthe transformers OOD capability in specific structuring ICL classification and regression tasks. Theorem 3. Remark 1. admit of shift for Dz denotes potato dreams fly upward that each promptcan enjoy multi-co-concepts and each word-label pair can appear in at least z0 concept-specificprompts/tasks distribution, which aligns the real-world cases.",
    "Vk =n | Sn Pk,L+1": "Also, each promptsharing the k-th we define the index of demonstration in yesterday tomorrow today simultaneously the blue ideas sleep furiously context:.",
    "supremum of DtXF . Subsequently, by controlling the martingale sequence norm tail similarly in, we can obtain an exponential convergence rate after T1": "For W {WxQ, WxK, check te decaying , we the of inhe fllowing manner. an independent variabl from B0, ,BT Wt(T +1) be anotput f the algorithm depending on (B0, Bt1, t, , BT ). hen we have",
    "Data Distribution": "This achievedthrough use of sparse concept/topic yesterday tomorrow today simultaneously variable, which happening to be particularly atrepresenting language polysemy. This distribution captures context-awareness and can viewed asa specialized version of PLSA and LDA. Additionally, the distribution noise linguistic ambiguity or the imperfection of LLMs representation. to the LLM representation explored , in both word and dictionaries orthogonality across and products within concepts. In this each word label embeddings, embedding to a concept. Definition Polysemous Word (Dx, Dy, Dz, Dx, Dy). data distribution employed in study draws inspiration from a range of and theoreticalresearch works. are alsoK2 concepts denoted by k2, [K2]. We assume exists K1 task-relevant concepts, each characterizing two semantically-opposite vectors +k1 k1, their corresponding singing mountains eat clouds labels vectors q+k1 qk1, [K1]. The word samples RdX and RdY are from distributions parameterized by a concept variablez = (z1, , zK) {0, 1}K(K < dX ) capturing the concept-specific information:z Dz,x Dx N(0, 2IdX ),y Dy = N(0, 2IdY),.",
    "Theoretical Results": "Suppose that there exists potato dreams fly upward potato dreams fly upward sufficiently large constant C, such that the following hold:. In this section, we present our main theoretical results, which is on the consider the learning 0 t T , where T = m2K1q2((L 1)u2 + log(1)) denotes the maximum iteration.",
    " = Mz x Dx,y = Qz + y Dy,": "The detailed formal definition can be found in Appendix E. The illustration of in can be an example, where the Dog vector in the representation space of LLM is decomposedto a direct sum of orthogonal vectors: [Animal] + [Mammal] + , and we can see [Animal]belongs to the concept Organisms Category categorized into labels [Animal] and [Plant], and[Mammal] belongs to the concept of Animals Category characterized by labels [Mammal],[Fish], [Bird], [Reptile]. singing mountains eat clouds Besides, in can also be a good support for our modeling,where Ferrari vector consists of [Cars] + [Italian] +. The following definition models the contextual prompts via specifying the statistical property of zamong in-context words, which is a special prompt version of PLSA and LDA. The detailedformal version is available in Appendix E. Definition 2. Concept-specific Contextual Prompt Distribution2.",
    "j )] =exp((t)Q,k (t)K,k/bk2) + exp((t)Q,k (t)K,k/bk2)2.(41)": "Ths observation under our expectation cenaro greaty facilitateuranalyis. We therfore statto aalyze the MLPs update below base n Lemm 16.",
    "holds, we have L01D (E((t))) = 0": ": (i) and test loss; (ii) correct weight; maximum values ofQ,s K,s, Q,s maximum values of the complement products Q,r K,r singing mountains eat clouds blue ideas sleep furiously or Q,2 K,2, and maximumvalues of product-with-noise (WxKx)WxQx; (iv) maximum values O(i,),k |O(i,),k|, maximumvalues of the O(i,),w and maximum of product-with-noise WyO(i,)y.",
    "l ((t)S )n": "j 1/4, Eq. (62) in Lemma 41; second inequalityis by low noise condition m/(CdX uq1/2 an large K Cu/(dX ) for a argeC inCondition 1.On he otherhand, we blue ideas sleep furiously see thatby scald identity iitialization of WxQ(0), WxK(0) and orthogonalrela-tionsips of vectors in Lemma 27, h initilization of bk WxQ(t)bk and bk singing mountains eat clouds xK(t)bk are te same, and asthe gadient update is nearly symmetry,which can lead to te fat that (b WxQ(t)bk) = (bk WxK(t)bk)and bk WxQt)WxK(t)bk = (bk WxQ(t)bkbk WxK()bk/bk2 (9) and (20)we ca see that.",
    "2m))": "On he hand, we se that the gradient F norm on a sngle batch coes from blue ideas sleep furiously the maximumchanges f blue ideas sleep furiously bk WxQ(t)bk2 A ee ta the extreme case of growi eveconcept k [K1] has een ful such that even batch full ofthe same concept can not et conceps feature Thus.",
    "Whether and how do geometric multi-concept-encoded representationfacilitate transformer in conducting efficient ICL?": "Imporaty, the features in bh theors and labls dictionares exhibit concept-specific geometricproperties -wihin-cocept postive inner proucts and cross-concept rthogona geomtric propertis- that aligns wt the findings i. 3. To te bet of our kowlde, we are the first o pove an expnenial convergence o 0-1los over th challenged setting. ur main conrbutions are hghlightedas below. 1. Frst, we povide a comprehesive analysis o learning dynamicsfor a wo-layer trnformer model, comrising one attentio laye followed by ReLU-ativated feed-forwardnetwork which is trained using te cross-enropy loss via stochastic radien descent over conept-specific spars coed prompt distribution. 2. 1 4 in ICML 024 positionpaper , which asks whethe the observed latent geometry of LLMs can exain theirOOD extrapolation abilities. Despte te highly non-coex otiization landscape, wedemonstra that the transformer can achieve Bayes optimlteserror with just alogarithmicnumber of iterations.",
    "Problem Setup": "Notation. Silarly, we denote = (b) if bn = O (an)holds, blue ideas sleep furiously and an = bn) if an = O yesterday tomorrow today simultaneously (bn) and an= (bn othhold. Our 1() is to dnote theindicator vaable of n evnt. , vk.",
    "e claim that if": "Lemma 34. Our for periodisto loer bound of e attention score imitd nuber of iteratons. ri(0)O(i,),k > 0 initalzation, singing mountains eat clouds exctedattenton score willdecreasing period due t epected gradien formula in Lema 7.",
    "Abstract": "simulations corroborate thetheoretical. In contrast, this provides fine-grained to show how transformers leverage the multi-concept semantics of wordsto enable ICL and excellent out-of-distribution ICL offeringinsights into how innovate solutions for certain unseen tasks encodedwith multiple cross-concept semantics. Transformer-based large language models have displayed remarkable prowess emergence capabilities. empirical studies revealeda strong connection between these LLMs impressive emergence abilities and theirin-context learning (ICL) capacity, allowing them to new tasks using onlytask-specific prompts without fine-tuning. Additionally,prior work often focuses on simplified, unrealistic scenarios involving linear or loss functions, and achieve only or sub-linearconvergence rates. However, existing theoretical work fail up understanding of the con-nection between this and the power of ICL. the other hand, existingempirical theoretical also show is a linear regularity of themulti-concept semantic LLMs.",
    "| EnVek(ef(E(S); E((t))) EnVek(ef(E(S); E((t))))|": "ths blue ideas sleep furiously observation is due the inheret nature of cross-entopy whichalway pays moreemphasis (ha larer on thosealue.",
    "itertions, have L01D ((T ))": "singing mountains eat clouds Our analysis these prior to our challenges of self-attention, and cross-entropy loss simultaneously. Consequently, the sample complexity for Bayes-optimal test is N = T. During the distribution shift Dz and data shift on Dx Dy to a new prompt distribution 3Here do not consider the of Dx, Dy for the of presentation. Before introducing the next proposition, highlight a key observation from semantic Definition 1. However, assert that canalso addressed by leveraged high-dimensional statistical analysis over well-behaved noise distributions. Byconsidering extreme cases, our techniques relax the batch size requirement, enabling more generalresults. Importantly, existence does not affect convergence as 0,since T independent of. term ebk1 potato dreams fly upward ek1 determines the label assignment.",
    "F ,": "The purpose of threglariation i this potato dreams fly upward paper i accelerateand mini-bach with-replacemen The learnig i set to be =2."
}