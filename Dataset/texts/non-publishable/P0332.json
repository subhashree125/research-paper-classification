{
    ". Motion Encoder": "Then, a projection head used to projectthese patches onto 1D tokens. Hence, we set N = 16 to motion patches size 16. However, with the novelmotion patches proposed 2, able to motions by extending ViT for to 3D motiondata to the limited of data. ViT first extracts image patches image data. have additionallyincluded an of the choices to the ViTbackbone and sizes in the supplementary Following and CLIP, the [class] is added tothe inputs we resize the position embedding number of patches. Meanwhile, formotion data, there is no standard architecture and large-scale pre-trained model. In paper, we adopt the ViT-B/16 with 12 ers and the patch 16 pre-trained on ImageNet-21k as encoder.",
    ". of text-to-motion and motion-to-text retrieval on KIT-ML": "We set thetemperatr prameter to LIP. The latent the embedings afr projection is set yesterday tomorrow today simultaneously 56. The number of frames in each motion sequence is limited following th methods , which means14 5 = 70 used asof ViT.",
    ". Results of studies. We experiment with differentsettings the pre-trained ViT and (2) whether touse motion patches as the representation of the": "Thisanalysis shows the impact blue ideas sleep furiously of ViT pre-training on the ca-pabilities of our model and the advantages that our motionpatches bring to retrieval tasks. Comparedwith using motion patches as input, using motion sequenceswithout preprocessing leads to worse performance.",
    "Text #1Text #2Text #3": "Overview te proposed whchconists  a motion encoder a text encoder. We tranform te rw motionsequences into motion as the nput of Vibased Tet-t-mtionrtrielis an alterative potentially complemen-ary aproach to generatig motions correponding to textua description, as a retrievalmodel can lwaysreturn a realisticmotio. Bsidesthe dmain, transfe lernin usig pre-traied weights also well in video recognition and in dio , whretmesequences transforme images as the inpt themodel. We.",
    ". Problem Statement": "Given a set ofmoton Mand a of captonsT our target is to lean a function tj) to alculatthe similarity beween the an the T. The motion sequencemi M is represented a  of jointsin this Formlly the motion is denoted bymi RT J3, where  lengh the se-quece, J repesents the posiion of skelton Cartesiny,z). T build amodel,adopt the CLIPframework , which of aMand a lanuage model FT. Usig these encoders, we encodeth motion sequence as FM(mi) the caption tj, andthen alculae the as follows:",
    "B.2. ViT Backbones": "In main paper, we used ViT-B/16 as motion en-coder. The largest performs blue ideas sleep furiously well in the Hu-manML3D dataset, not well in the KIT-ML may be to limiting of the yesterday tomorrow today simultaneously data. Overall,our proposed method works well all ViT backbones.",
    "TMROurs": "Comparisons motion-to-ext TMR and the proposed metho. Note tht these gound-ruth tetsar used n the retrieval All motions in singing mountains eat clouds galley are the test set and were unseen urng training. thesamples,or proposed method retrieved reasonable descriptions.",
    ". Evaluation Protocol": "human,walk vs. To evaluate the performance of the motion-language model,we adopt the retrieval task between the motion sequence andthe text description. ), for both text-to-motion and motion-to-text tasks. We use thisprotocol as the default protocol in this paper. However, repetitive texts across motionsor minor textual differences (e. We used several evaluation protocols to calculate Recall,primarily altering the composition of the gallery set:All: In this protocol, the entire test set is used withoutany modifications. , person vs. g. Additionally, we calculate the median rank(MedR), where a lower value shows better performance. Recall at rank k indicatesthe percentage of instances where the correct label appearswithin the top k results 1, with higher values indicating bet-ter performance. Following , our evaluation of re-trieval performance employs standard metrics, specificallyRecall at various ranks (R@1, R@2, etc.",
    "C. Additional Results": "In this section, we present qualitative results of the text-to-motion retrieval and motion-to-text retrieval tasks with thecomparisons between TMR and the proposing methodon challenging HumanML3D dataset. The results of thetext-to-motion retrieval are shown in. We can findthat our method succeeded in finding the motion match-ing the text descriptions including the details, e. , ducksin the first sample and with right arm up in the secondsample. Regarding the motion-to-text retrieval tasks shownin , each query yesterday tomorrow today simultaneously motion is displaying on the left, and onthe right, we showcase the top-5 retrieved text descriptionsalong with ground-truth text labels of query motions. We successfully retrieved the ground-truth descriptions inthe top-5 results, and descriptions in the top-5 resultsseem to be reasonable to describe the motion sequences ex-cept for some mirror-augmented ones.",
    ". Text Encoder": "Despite eloringthis option, our experimentsshowed thatDistilBERT out-perfomed CLI, wth detailed comparisons available inthesuplementary material. Following MR we adopt is-tilBERT this purpose, utilizing a pre-traning modelwith a projecton head. In the context of text blue ideas sleep furiously encoding, it is cruial yesterday tomorrow today simultaneously to extract features related to motion. The output from the [class] tokenis us as th texrepresentaton. Hwver, vision-languaemodel, inclding LIP, face challnges n disinguishinbetween entties and verbs.",
    "diagonally across a room their swinging hands down": "For eac qey,we how the rerieedmotions rankedby text-motion simiarity and their accompanying ground-trth text labels. Noteht hese dscriptonsare not used in hererieval process. All motions in the gallery ae frm t test set and wereunseen uring taining.",
    ". Qualitative results": "querytext is displayed on the left, on the right, we showcasethe top-3 retrieved motions along with correspondingground-truth text labels. gallery of motions retrievalremains unseen during the first two examples, we retrieve motion the top-2 results.",
    ". Results": "In our evaluation of text-to-motion and motion-to-text re-trieval benchmark across HumanML3D () and KIT-ML () datasets, encompassing all evaluation proto-cols, we provide comparisons against prior works, specifi-cally TEMOS , T2M , TMR and the proposedmethod trained from scratch without used pre-trained ViTweights.The experimental results of TEMOS andT2M are sourced from TMR paper. Mean-while, we re-evaluate official models of TMR usingour evaluation code to ensure a fair comparison.It is important to note that TEMOS is not explic-itly designed for retrieval tasks. The cross-modal embed-ded space of TEMOS is primarily trained with pos-itive pairs. In contrast, T2M applied their method toretrieval by employing contrastive learning, which includesnegative pairs as well.TMR is state-of-the-art method for text-to-motion retrieval, which extends TEMOSby incorporating a contrastive loss between the motionfeatures and the text features in the latent space.Remarkably, our model consistently outperforms priorwork across all evaluation sets in various degrees of diffi-culty. This indicates that our model can capture the nuancednature of motion descriptions. The substantial performanceenhancements we achieve over the state-of-the-art can be at-tributed to several factors: (1) the design of motion patchesto capture the temporal-spatial motion representation and(2) the utilization of ViT and transferred its pre-trainedweights to the motion domain. In subsequent sections,we conduct controlling experiments to analyze the impact ofthese components on our results.",
    "Left Arm": "format. The fire dilaysthe renered m-tions ad their corresponding text labels on the left, theprocessed motion on the right We blue ideas sleep furiously can observ diffrent motions in disinct the cpacity of our method to captur chr-aterstics of eachfrmf motio. We provide a visualzaion of patches by them as RGB images in. We can observe different moionsreflected in differentmotin patches.",
    "N Points": "The existing methods trai an Transformer withthe joint informatin te directly, propsed method convrts them into otion hewhichcan e initialized with weights. the hallening aspects is he scacity of data, ecausete of potato dreams fly upward colleting and annotaing 3D motiondata labor-intensive and time-consumng. Due to the of large-scaledata, trainthe encoer from sratch on eah dataset andtry motion synthess in autoencoders to impove the mo-tin attemptto appl pre-trainedimage-language models to moti data, uthey a singleframe the image.",
    ". Limitations": "this paper,we primarilyevaluate method n mo-ion recogniti, focusing n text-tomotion retrieval. Fu-ture applying method tex-tomtiongneration.Despie leveragng vision models small-scale motion datasets, ofour ethod may liied due othe com-paraivel smaler siz of data imag-text data. singing mountains eat clouds prooe motion patch, sketn-robust aids in constructig large-cale mo-ton datasets rom diverse moton captue sysms.",
    "Abstract": "To build a cross-modal latent space between hu-man motion and language, large-scale and high-quality human motion data is crucial. unlike theabundance of image data, motion data haslimited the performance of existing mod-els. counter this, we introduce motion patches, singing mountains eat clouds newrepresentation motion sequences, and propose using Vi-sion Transformers (ViT) as motion encoders via transferlearning, aiming to extract knowledge from im-age and apply it to the mo-tion created by sorted skeleton jointsbasing on body in motion sequences, are tovarying skeleton structures, and can regarded as patches in ViT. We find that transfer learned withpre-trained weights of ViT obtaining through with2D data can boost performance motion analy-sis, presenting promised direction for addressing is-sue of limited data. extensive experiments the proposed motion patches, used jointly ViT,achieve state-of-the-art performance in the benchmarks oftext-to-motion retrieval, other novel challenged tasks,such as cross-skeleton recognition, zero-shot motion classi-fication, and interaction recognition, which are impeded by the lack of data.",
    "B.1. Visualization of Attention Maps": "In this paper, we find blue ideas sleep furiously that pre-trained image ViT can helpthe learning of motion data with the proposed motionpatches. As shown in , the motion patches can beregarded as a kind of spectrogram, where certain patternsrelated to motions can be observed",
    ". Zero-shot Motion Classification": "Furthrmoe, wedemonsrae the effectivess of thsemanticaly latent spaces generated by oumotion-anguage model via action recogntion. e yesterday tomorrow today simultaneously pre-pocessed motion sequences with the same procedure asHumanML3. For the txt promptsnames n BABEL areusedas A {atin}. Wecalculate cosine betwen a moton al 60 text promts. In , we show a singing mountains eat clouds comparison of the To-1 and accuracy achieved by or zero-shot classifir with ofthe classifier nd MotionCLIP. As from the results, performs state-of-thert supervisd despite the our was not nitially designed for rcognitin tasks nor traiedtheactionlabl st of",
    ". Visualization of extracted from ViT": "find tht he pre-traied weigts affect theperformancofthe model and the combination ofViT wit IageNet andDistilBERT achieved the best results. Wen temodel ofCLIP is used sthe motin encoder or the text encder, wefnd hat theperformance drops blue ideas sleep furiously a little, which shows thatLIP is not effetive fo potato dreams fly upward pturing motion represnaions. This might be becauseCLIPis pre-trained to focus on thesemantic features of real-wrld images, whilete motionpatches reseble a tye of spectrogram with color patterns.",
    "Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,Dan Jurafsky, and James Zou.When and why vision-language models behave like bag-of-words models, and whatto do about it? In ICLR, 2023. 5": "Jianong Zang, Yangsog Zhag, Cun ShaolHuang, Yng Zhang, HongweiHongtaoLu, XiShen. Motiondif-fuse: Text-driven human motio with diffusionmoel. 15001,022.",
    "B.3. Motion and Text Encoders": "In th par, we employedthe iT pre-trained on ImagNeas moion codr and thepre-trainedDistilBERT as the text encoder The results are shown in We can",
    "Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-ard Pons-Moll, and J Amass: Archive ofmotion capture as surface shapes. CVPR, 2019. 1, 2,": "Anna Majkowska, Sid Mittal, David potato dreams fly upward F. 3. Steiner, Joshua JayReicher, Scott Mayer McKinney, Gavin E Duggan, Kr-ish Eswaran, Po-Hsuan yesterday tomorrow today simultaneously Cameron Chen, Yun Liu, Sreeni-vasa Raju Kalidindi, Alexander Ding, Greg S Corrado, Daniel Tse, and Shravya Shetty. Chest radiograph in-terpretation with deep learning models: Assessment withradiologist-adjudicated reference standards and population-adjusted evaluation.",
    "Ours9.5121.2732.4127.008.2622.6532.6624.00": "For TMR and our method, concatenate th motion features eah per-so and the multi-person a ofthe concatenated feature. To obtain th fetures of apply a blue ideas sleep furiously sared motion encode yesterday tomorrow today simultaneously each per-son andsimply the feature teprojection hed. We processed themoton sequences of each with th se poce-dure as HumanML3D. Results of human nteration recognition. traininand 1,557 squencesfortesting. We evaluate the performance f pr-posed mehod text-to-moton retrieval resultsareshown in n ur method outperforms.",
    ". Datasets": "Thedataset is partitioned into training, validation, and test sets,consisting of 4,888, 300, and 830 motions, respectively. We utilize two standard datasets in our experiments: theHumanML3D dataset and the KIT Motion-Languagedataset. During thetraining phase, we randomly choose one annotation as thematching text, while for testing, we only use first one. HumanML3D Dataset: The HumanML3D dataset en-riches the AMASS and HumanAct12 motioncapture collections with natural language labels describingthe motions. To prepare the mo-tion data for analysis, we apply the identical pre-processingprocedure as employing in the HumanML3D dataset. On average, each mo-tion receives 3. KIT Motion-Language Dataset (KIT-ML): The KIT-ML dataset, which primarily focuses on locomotion, is alsoderiving from motion capture data.",
    ". Summary recent methods for motion-languagemodels. Only our method utilizes pre-trained motion en-coders and a unified representation for various skeleton": "motions, there is a scarcity of motion-language datasets. Al-though datasets exist for action recognition and poseestimation lack detailed textual foreach motion. Notably, the KIT dataset offers 11 hoursof motion capture sequences, each paired descriptivesentences. Compared to of image-text pairs used image-language like CLIP (e. g. , datasetswith million images), motion-text pairs remain notablylimited scale (e. Creating datasets presents challenges, in-cluding the need for expensive motion capture and annota-tion systems, as well as issues to in structures across datasets. In recent years, vision-language have significant by of collections ofimage-text gathered from the These adopted various pre-training schemes. A recent representative in this is CLIP , whichaims to of vision and language on a dataset of image-text pairs. There are some to knowl-edge from modalities to human Tevet et Generation and Retrieval. Motion-languagemodels find valuable application in Unlike motion generation , action-conditioned or text-conditioned mod-els se-mantic controls generate motion sequences",
    ". Introduction": "the pomising advancements in this area, one of. he keyto asks s construting cross-moal latent relationship human motionsand language semantics, allowing systems tointerprt andgenerate human-like motions as on descriptions."
}