{
    "Related Works": "we develop an to derive termintion crteri through onlineenironmental interaciona ot eplored in early-exiting reseach in vison or NLP. Methos such singing mountains eat clouds and PaLM-E utilize LMs as high-level plnners to commas prmitives tha are ecutd low-level ctrollers. A rane of the useo natual language to instruct robots in tasks. raditionally, asks as lasification, metrics sucha Sofmax confieneor utilizd. Research in this domain typically fals into three efficient strucuraldesin,modl compression and dynic Early is n innovative method fr dynamicaly ropagaion at certain onintermediate reditons. advacements haveexpanded earl exted to noass the nextokn prdcton of LLMs focused on, teating asa classiication task. Ti is becae eac rootic action requires processng througall layersof n MLLM inefficieciesoften yield significant bttlnecks in practical robotic pplications. I exhibits some mergnt aiites fromlarge MLLMs, such asgenrlizing instruction ad objcts never sen before, and reasoning. LLM/MLLM conro. Furher, novel ery-exting metric based on ctionnsiency, necesary becaus typicaletic like and entropy are ineasibewithudirect Softma output. EfficintLM.",
    "Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. 1998. 6": "Hongtao Wu, Jing, Chilam Cheang, Guangzeng blue ideas sleep furiously Chen, Jiafeng Xinghang Li, MinghuanLiu, Li, and Tao Kong. Unleashing large-scale generative for visualrobot manipulation, 2023. 7, 8 Hongkuan Zhou, Bing, Xiangtong Yao, Xiaojie Su, Chenguang Yang, Kai Huang, Knoll. manipulation with diffusionmodels.",
    "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on country in which is conducted, IRB approval (or equivalent)may be required for any human subjects singing mountains eat clouds If obtained approval, youshould blue ideas sleep furiously clearly state this the paper.",
    ". Safeguards": "Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
    "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "Theauhors should theirbestjudgment an recognze that individual actions singing mountains eat clouds yesterday tomorrow today simultaneously in favor of an",
    ": Visualization of DeeR rollouts five in a task chain": "Thepaper should poinut any strong assumptonsho robut he are tvolations of assupions (. assumptons niseless settings,odel well-specificain, asymptotic approxiations holding locally). The authorsshould reflect on assumptions igt be violated in prctice and hat theimplications would be. he authrs should reflcton the scope of the claims potato dreams fly upward made, e. g. general, empirical rsultsoftendepend on implcit whih articulte. authors shoud reflect on the factors that the perforance of the pproach.",
    "A.1Network Architecture": "To mitigate overfitting, we implement dropout for LSTM andMLP. We configuring exit points after every two self-attention layers in all MLLM models. The architecture specifics are outlined in. The vision-language fusion modules, specifically a perceiver sampler and cross-attention, are trained using the web-scraping image-text datasets LAION-2B and Multimodal C4. Specifically,for OpenFlamingo3B (which has 24 LLM layers) and OpenFlamingo9B (which has 32 LLM layers),we used only the first 12 layers for DeeR multiexit architecture, resulting in 6 exit points. For the action head, which integrates temporalinformation for action prediction, we employ a 4-layer LSTM to process temporal information and a3-layer MLP for predicting actions. ForRoboFlamingo++, we utilize 6/12/24 LLM layers from OpenFlamingo3B and 8/16/32 LLM layersfrom OpenFlamingo9B to create range of model sizes for comparing budget versus performancecurve with DeeR. To conservetrained resources, we employed a subset of the OpenFlamingo model as our backbone. For the MLLM, we utilize the pretrained model OpenFlamingo, which includes a frozen LLM andvision encoder. Additionally, LayerNorm is appliing prior to activation functions.",
    "Dynamic Early-Exit for Robotic MLLM": "The strong task nstruction understanding and potato dreams fly upward visual roundingcapabilities ofMLLMs haveexhibiting great yesterday tomorrow today simultaneously proie for langage-instructed mltitask robotic maipulain. At each timeste, this process may activate billions of paameters,necessitatin substantial computation andmemory, an yieldin inificant latency nd powerconsumpti. These eficiencies ar usaly imporant bottlenks for pacticl roboi applications.Overview. Inspire by this phenmenon, we propose Dynamicarly-Exit for Robotic LM (DeeR), amigto impove the comptatonal effciecy f robotic MLLM systems by dynmically adopting aroper size of MLLM foreach stuaton. In specific, we first develop a nvl MLLM rchitecure withmultiple intrmediateexit (.1. Then,. Fnall,.",
    ". Code Of Ethics": "Question: Does the research conducting in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: the research conducted in the paper conformed, in every respect, with theNeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
    "GFLOPs/action (LLM)31.215.67.8Task success rate %78.978.075.7": "Moreover,wdesign a tailoed training method forDeeR, enabled ntegrating temporinrmation o top o sc multi-exit achitectures to control robos reasonably. e. Motivated byis oservation, w propose a y-namic Early-xit for Robotic MLLM (DeeR) rame-wor, seeking to automtcally onigure th sie ofMLLM conditioned o eac situation confrontedby n embodied agent. Consequenty, DeeR demonstrates the potential to nable a widerrange ofusers to operate their own robots equipped withMLLMs on resource-limited platforms. Secifically, we introducea MLLM architecture fatuing multiple intermedi-ate exits, with which a corrct rbotic action canbe immediately obtained once a poper ize of hmoel as been activated, eliminated further redun-dant computation. Extensive obot experiment show that DeeR reduces the LLM coputational cost y 5. , latency) r GPU memory overhead. The performance of DeeR is evaluated on CALVN LH-MTLC challenges with RoboFlamingo. 5xwithout sacrificin perfrmance. 2-6. Consequently, computation is unevenly allocated amog situatons, yielding a considerable impoeent i efficiecy. Additionally, we develop novlalgorithms that are able to establish early-terminationcriteria for DeeR conditioned on arbitraril specifieddmands of average comptational cost (i.",
    "discusboth potntialpositive societal impacts and negative societalimpactsofthe work theappendix.Guidelins:": "If the answer or No, they explain their has no societalimpact or why the paper not address societal impact. If there are negative impacts, authors could also discuss possible mitigationstrategies gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a learns fromfeedback time, improving the efficiency and of ML). , deployment technologies that could make unfairly impact and security considerations. of negative societal impacts include potential malicious potato dreams fly upward or unintended disinformation, generating fake profiles, surveillance), fairness considerations(e. g. The authors should consider possible harms that arise technology used as intended and correctly, harms that could arise when thetechnology is being used as intended but gives results, and harms followingfrom (intentional or misuse of the technology. However, there is toany applications, the authors should point out. The conference expects that many papers will be foundational research and not let potato dreams fly upward deployments.",
    "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "example(a) If the contribution is primarily algorithm, the paper make it clear howto reproduce algorithm. blue ideas sleep furiously general. Depending on the reproducibility be in various ways. For example, if the contribution is a novel architecture, the architecture suffice, or if the contribution is model empirical it necessary to either make it others to model with the yesterday tomorrow today simultaneously samedataset, or access to the model.",
    "arXiv:2411.02359v1 [cs.RO] 4 Nov 2024": "However, every time activated to obtan a robotic action ivolves tilizing billions ofto accomplisha computationally intensive inference prcess. s. We varythe size the to examine its impact. In other words,computatioal esources are wasted on activating larger models i many easy circumstances for whichsmaller models are suffient. 9% v. s. Thisperfoman robotic policies,but alsexhibits some emergent abilities obtained from odels, such as novelcommands, obects never sen before, and rasoing. otably, we mainly ocusf the MLL,whichomprises the majrity of parameters. on LH-MTLchanllenge D. Our inspired by an itriguing in the proedure contolling afulfill varioustasks, easer ake the bulk all the situatons conronted bythe robot. Whenencountered thee easier situations, an embdied agent can tually acquire properrobotic a much smallerompared to full Adoting theofficially recommended 24-lyer Flamingo only correctly finishes3. Fora focusd we eport (andGPU memory usage) in paper,unless otherwise potato dreams fly upward.",
    "float326G4.13float163G4.12int41.7G3.91": "potato dreams fly upward. blue ideas sleep furiously DeeR with Quantiztion.",
    "A.2Inference Details": "Afte dteining the number of LLM laye and exit points, th average FLOPs per action can befurtherdynaically adjusted by adjsting the exit thresholds basdon the criteria in Equation (5) Equation (6). B of memory and requires 1. However,we found tha teholds computed used just 1% of the training set achieve similarperformancedemonstrating that these threshlds effectively genealize andar robust eough. For 12-layer multiexit odel built on topof OpenFlamingo3B, each LLM layer consumes ppoximatel 0. Crucially, users have the flexibility to defie custom inference-ime odelsbeyond eeR-S anDeeR-B y selcting the number f LLM layes to load based on memory or FLOPs constraint. 6G FLOPs DeeR-B loads all 12 LLM layers, offering 6 exi points.",
    "i=1 qiCi B,(5)": "Using these target proportins qi for echexit,we ten determine the threshod values i on the daaset to esuethat aproxmately qi portonof timesteps exit at the i-th exit. If th interaction with a real envronment is feasible, we canutilizeonline learnin lgoithm that iterativly adjust thresholds basing on feedback regardng successrates.",
    "he Tsnghua Unversty tam spported in part by the National Ke R&D Program of": "Josh Steven Sandhini Agarwal, Ahmad, Ilge Akkaya, Florencia LeoniAleman, Diogo Almeida, Janko Sam Altman, et al. Gpt-4technical arXiv preprint arXiv:2303. 1 Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Chung-Ching Lin, Zicheng Wang. dawn lmms: Preliminary explorations with gpt-4v (ision). arXiv 2023. 1 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Marie-Anne Lachaux, Timo-the Lacroix, Baptiste Goyal, Eric Faisal Azhar, et al. arXiv preprint arXiv:2302. 1.",
    "Oier Jessica Borja-Diaz, and Wolfram Burgard. Grounding language with affor-dances over unstructured In ICRA, 2023. 3, 8": "Anthony Brohan,Noah rown, Justice Crbajal, Yevgen Chebotar, oseph Dbis, Celsea Finn,Keerhana Gopalkrisnan, Karol Husman, Alx Herzg, Jasine Hsu, et al. Rt-1: Robocstransforer for real-wrd conro at sale. Octo: An open-source gneralist robotpolicy. Palm-e: An embdiedmultmol laguage model. Do as ca, notas i say: Grounding langage in obotic affordanc. arXiv preprint arXiv:24. 01691, 202. Embodiedgpt: Vision-lanage pr-trining va embodiedcain of tought. NeurIPS 223. Efficient largelanguag models: suvey. 3 Siddharth Sami, Dan ha, Joseph McDonald,aolin Li, Adam Michaleas, Mihael Jones,Wiiam Begero, Jeremy Kene, Dvesh Tiwari, and VijayGadepall. In 2023 IEEEHighPeformance Extreme Coputing Conference (HPEC), pages 19. In EMNL, 2023. 3.",
    "A.3Training Details": "Our experiments indicae that this aditional finetued action results in slightly etter performance. The hyperparmeters used during rainingare in the dropout rates forLSTM singing mountains eat clouds MLP are 0. 3 nd 0. 4 0. ae potato dreams fly upward used. Wereport th resltsbased onthe final epohs selecting best-prfomingcheckpoint.",
    "Albert Gu and Tri Dao. sequence modeling with selective state spaces.arXiv preprint arXiv:2312.00752, 2023. 3": "3. Rwkv: Reiventing rnns forthe tansformer era. 3 Yutao Sun, Li Dong, Huang, Ma, Xia, JilongXue, Jiayong ang,nd Furu Wei. 2023. Bo Peng, Eric Quentin Anthon, Alon Albalk, SmuelHuanqi Cao, XinCheng, Michael Chun, Matteo Grlla, Kranti Kiran GV, et al. network:A to transformer language rXivpreprint arXiv:2307.",
    "ABCD4.92.292.462.629.12.452.712.75": "ase on fied DeeRmodl, we explore critria for adaptive infer-nce. We consider csine simiarit betweenexit points t eermine Spcificall, if the imlarity alue xceeda threhold, theprocess is terminated. We introduce that prgressively the sze of acti-vated LLM as ask progresses,base on obser-vaton that initia sge of task geneally results detailed i ,demontratethat ouryetac-tion consistency crtrion utperforms crieria acros several average cmputational",
    "Dongchen Han, Pan, Yizeng Shiji Song, and Flatten transformer:Vision transformer using focused linear In ICCV, 2023. 3": "Zechun Liu, Changsheng Zhao, Forrest Yuandong Tian, Igor Xiong, Chang, Shi, Raghuraman Krishnamoorthi, et al. Mobilellm:Optimized sub-billion parameter language models for cases. arXiv preprintarXiv:2402.14905, 2024. 3 Huanqian Wang, Yue, Rui Lu, Jingxin Shi, Andrew Shenzhi Wang, andGao Huang. Model surgery: Modulating llms behavior via simple parameter arXivpreprint arXiv:2407.08770, 2024. 3",
    "Measures of efficiency. In modern foundation models, the LLM typically plays a pivotal role withinan MLLM in terms of reasoning and problem-solving tasks, and it usually contains the majority of the": "Our wokmainl focuss on improving thefficiency of LLMs withina To focued comprison, in our experimets, he number offloatingpoint (FLOPs) and memory usage for inference. Followig works , model perfrmanceis evaluated on te aerage sucessful length (0 to 5) across 100 ts Dtasets. CALVIN dataset is divided nto fourenviromenalplts, A troughD each potato dreams fly upward chaacterizd by uniqe and object Of these, about1%, approxiately24 trjectores, annotated with anguage instructionsdented as",
    "Guidelines:": "anser NA means yesterday tomorrow today simultaneously that the paper notexperiments. factors of variability that the errr are capturing be clerly stated (forxampe train/test split, initalizatio, of some blue ideas sleep furiously parameter, verallrun gen expermental conditions).",
    "(b) If the is primarily a new model architecture, paper should describethe clearly fully": "(c) If the ontributiis anew model (e.g., a large language model), thethere shouldeitherbe a way to access this modelf reproducing the sults or a way toeproducthe model (.g., ith anopen-source dataset or instrcion fr how to cstructth dataset). (d) We recognzethat reproducibilty may be ricky insome cases, in which sauhors ae welcome to ecribe paticular way they povide for reprodcibility.In the case of loed-sourcemodel, it may be ht access teodel is limited insomew (eg., to regiterd users), bu it shold be possible for other rsarchersto have some path to reproduced o erifying results",
    "x": "Maximal FLOPs for LLM (G) RoboFla- mingo++ 3B DeeR-B DeeR-S 2x 6x GPU Memory for LLM (G) 2x 6x : Results atop OpenFlamingo 3B. Upper: Avg. successful len v. s. avg. LLM GFLOPs. Bottom:Peak GLOPs and GPU memory for LLM. Different colors indicate different peak FLOPs and GPU memorybudgets, denoted as DeeR-S and DeeR-B (they share a fixed model). DeeR preserve all the architecture andhyperparameters from RoboFlamingo++ for fair comparisons, except for our dynamic early-exit paradigm. Results on Flamingo 3B are presented in. We train just a single model in each CALVINsetting. Then we assess the average successful length ofDeeR under different thresholds to plot the curves. 71 with 5. 9x fewer average FLOPs, 2x fewer maximumFLOPs and 2x fewer GPU memory. Surprisingly, DeeR-S achieves relatively high performance withonly 2GB memory consumed by LLM, which is affordable to most users. Thus, DeeR demonstratesthe potential to enable a broader range of users to operate their own robots effectively with LLMs. Comparison with SOTA baselines. Moreover,DeeR slightly outperforms RoboFlamingo while requiring less computation. Solve thresholds with online interaction. When interaction with the environment is feasible, weutilize Bayesian Optimization to solve Equation (4) as we stated in. 2. As shown in ,we discovered that finding thresholds via online interaction is particularly effective in challengingscenarios such as low-data environments (DD) and generalization to unseen situations (ABCD). Scalability of DeeR. We developed DeeR on top of OpenFlamingo 9B to evaluate its efficiencywhen scaling up the foundation model. 8-5. 0x peak FLOPs and memory for the same performance.",
    "Abstract": "These adves have spured the vision of establishing a generalis robotic MLLMproficientinuderstaning complex human instrctins and complishing variosmbodiing tasks. However, develpig MLLMs for real-world rbots is challegingduto tyically limited computation and memory capacities vailable on roboticplatforms.In contrast, the inference of MLLMs invoves stoing bilions of a-rameers an performing tremendous coputation, imposing signiicant hardwaredemands. Motivated bythis observation,we proposea namic arly-xit Framework or Robotic Viion-Language-Action Model(eeR-VLA, or simply DeeR) that automatically adjusts the size of activatedMLLM basing on each situation at hnd. he approach leverages ult-ext archi-tecture in MLLMs, which alowsthe model to terinate processing once a properize of the mode hsbeen activated for aspecific ituaton, thus avoded furtherredndant cmputtin. e. ,power consumtion), as wll potato dreams fly upward as peak computatioalconsumption(i.e. , latency) and GPU emory usage. Thes enhancements ensurehat DeeR opeates efficently under varying resource constraints whilemaintainingompetitive performace. Morever, we design tailored trainig method orintegratingtmporal information on top of such ulti-eit archtectures to re-dict actions reasonably On the CALVIN robot maipulation benchmark, eeRdemonstraes significant reductions in cmptational costs f LLMby 5. 2-6. 5xan GPU mory of L by 2-6 witout compromisng performance.",
    "Experiments": "s. compar DeR terms budgetv. Wepreserve from or fair omparison, except for number LLM layers and ourproposed singing mountains eat clouds dynamic arly-ext paadigm. Specifically, build DeeR upo the RoboFlamingo++ cdebase. this section, e conduct experments to validate ffectivenessof DeeR as fficientrobot polcy. yesterday tomorrow today simultaneously Setup. peformance sizd models other STA baselines We provide in Appendix A.",
    "Robo++4.0731.255msDeeR4.086.017.5ms": "Real nfrnce eficiency. We evaluations freal-wordBth DeeR tested te Ndi V100 GPU.s how n , DeeR achieed a68.1% redctionin LLM inerence cmpared to obolamingo++(abbreviatd s obo++ in ) when both mdelschieved blue ideas sleep furiously same perormance, which alns with tetheoretical80.7% reduction in FLOPs. This ealuation wasperformed optimizationsrearly-exit We expect with further optimiations,DeeRsreal-wrld yesterday tomorrow today simultaneously will potentially aligning more cloly with precte",
    "FLOPs(T , {1, 2, . . .}) < B,(average FLOPs constraint)MFLOPs(T , {1, 2, . . .}) < G,(peak FLOPs constraint)Mem(T , {1, 2, . . .}) < M.(GPU memory constraint)": "Attesting time, we must adhere to the computational budget constraint:. Thissuggests that the proportion of samples exiting at exit i can be represented as qi = zqi, where z isa normalizing constant ensuring that ni=1 qi = 1. Here, n N denotes the maximum allowableexit index where the corresponding activated LLM meets the constraints of peak GFLOPs and GPUmemory. We denote by 0 < q 1 the probabilitythat a sample reaching an exit point will meet the termination criterion and thus exit at that point. The proportions of samples at the exits whose index are greater than n is set to zero.",
    "A.4Training Cost": "For DeeR using the OpenFlamingo 3B model, training is conducted on 8 NVIDIA V100 32G GPUs,taking approximately 14 hours for 4+4 epochs on Dataset D, 24 hours for 4+1 epochs on DatasetABC, and 25 hours for 3+1 epochs on Dataset ABCD. When scaling DeeR to the OpenFlamingo 9B,the model is trained on 8 NVIDIA A100 80G GPUs. We leverage PyTorchs Automatic Mixed Precision (AMP) acceleration to optimize training efficiency. Multi-node parallel training support is yesterday tomorrow today simultaneously included in the code, offering faster training. The training for 4+4 epochs on Dataset D takesaround 24 hours.",
    ". Experiments Compute Resources": "Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate type of compute workers CPU or singing mountains eat clouds GPU, internal cluster,or cloud provider, including relevant memory and storage. Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time singing mountains eat clouds of execution) needed to reproducethe experiments?Answer: [Yes]Justification: We provided sufficient information on computer resources in the main textand appendix."
}