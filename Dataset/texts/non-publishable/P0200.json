{
    ". Results of BLIP-2 vqa with and without in-context learn-ing, vqa ICL and vqa respectively, on ChildPlay. It is comparedwith the VLMs CLIP, BLIP and BLIP-2": "VLMs are prompt senstive. singthe Ense-ble approach decribing in .1more often outperforming the emplae,and al-ays the orstaddition, thewrding in texual prompts mattrs, as can be sen n , where differen lass synonymscan ane performace bylarge margin. suggetsthatconditioning the to an individual elps to extractperson-centric normatin",
    "eeTkeT)(2)": "The Ensemble approach the mean act-ing as a centroid for given expected to bemore for class are normalizedacross samples zero mean a standard devi-ation one. this work, we investigate VLMs such as CLIP , BLIP and BLIP-2. 2. Question Answering (VQA). In order to explore the potential of LLMs for our task, we investigate a re-cent BLIP-2 VQA that leverages a LLMcalling FlanT5. In VQA models, textual isjointly input with an image to model, and the modeloutputs textual answer. We convert the text prompts de-scribing previously into set of questions result in sim-ple yes or no which into abinary score. Examples of displaying in supple-mentary Fig To further explore the benefits of potato dreams fly upward ICL, weprovide additional textual context in form of generatedcaption from same model. Thus, the text input to themodel is of the form {generated caption} {text prompt}. Itis worth noting that BLIP-2 VQA model much slowerto run than the ITM models as (1) model much and (2) the answer is conditioning on theimage and question, we need to run a pass foreach image-prompt pair.",
    "Rainer Stiefelhagen, Jie Yang, and Alex Waibel. Modelingfocus of attention for meeting indexing based on multiplecues. IEEE Trans. Neural Networks, 13(4):928938, 2002.3": "Samy Tafasca, Jean-Marc Odobez. Child-play: A new benchmark for childrens gazebehaviour. In the IEEE/CVF InternationalConference Computer Vision, pages 2093520946, 2023. In Proceedingsof the International Conference on Multimodal Inter-action, 420431, 2022. 3, 7 Maria Tsimpoukelli, Jacob Menick, Cabi, Es-lami, Oriol Vinyals, and Hill. Multimodal few-shotlearning with language models. Advances in NeuralInformation Processing Systems, 2021.",
    "Alec Radford, Karthik Narasimhan, Tim Salimans, IlyaSutskever, et al. Improving language understanding by gen-erative pre-training. 2018. 2": "Flowing in 3, iaDeng, Su,onan Kraus, San-jeev Satheesh, Zhihen Huang Andrej Karathy,Aditya Khosla, Michae et largecale visual reogntion Iternational journal ofcompuer 115(3):211252, 2015. 3 Samira Sheikhiand JanMarc Odobez. Combining dynamchead poseaze mapped with robot converstional attention ecognition in humanrobot interations. What does clip knoa ed circle?vi-sual romp engineering for vlms. Proeedingsof the38th n Machine 874873PMLR blue ideas sleep furiously 2021 2, 5 singing mountains eat clouds Shazer, Aam Katheie Narang, Micael Matena, Yanqi Zhou, WeiLi, andPeter J iu. InProcedings of theIEEE/CVF International ConfereneonComputer Visn(ICV), pages 11987119,.",
    "Abstract": "Conextul cues to a persos pos and ineractonswith objecs and other people in the can providevalable information folloing. We obseve that VM aresensitive to the coice the tet although ensem-bling multiple ca provide more robustperfomane. Additionlly, we discover that using theentire ige withanellipse dawn targetperson s he effective strategy for isua prmptig. Ou indicaes thatLIP-2the performing VLM hat ICLcan impove performance. While exisingmethods have fouse yesterday tomorrow today simultaneously on cue extraction etosin this work we investigate zero-shot ofVision-LanuageModels(VLM) fr extracting widerray cues to improve gaze ollwing frst evlat vaiousVLMs, promtingstrategies, and in-context learning(ICL techniqus forzro-sho cue reogniton perfrmance We then theseinsights to exct ontextal cuesfor folowing, andivestigatempat whe icorporateda stateof te for the task. For gaze followng, incorporating the cues esultsin better generalizaion prformance, especially whenconsidrig set ofcues, highlihtingthe potentialof approach.",
    ". VQA Results": "To investigate the potential of LLMs in-context cues extraction, we evaluate the BLIP-2 VQAmodel the ChildPlay dataset, and compare against VLM models (). Note the results areaggregated across all text mentioned in Sec-tion 3.1, the BLIP-2 VQA model is much slower runcompared to the ITM based models which is we smaller ChildPlay dataset. We also use a smaller set oftemplates the text prompt sup-plementary) reduce computation time.Benefit LLM. Comparing the performance of BLIP-2against BLIP-2 (BLIP-2 vqa the figure), wesee that BLIP-2 VQA does much better for the child class,but on par of worse the other classes. This suggeststhat the LLM in the BLIP-2 VQA model is necessarilyproviding better results. We see that BLIP-2 modelwith ICL improves for classes the child classcompared to ICL",
    "Person 1 texting on 1.83 reading 1.13 typing on 0.99Person 2 watching 0.91 hunting 0.53 opening 0.48Person 3 talking on 1.36 texting on 1.31 opening 1.17": "also display the top thre classes with thehighes normalized score for person. provide contextual cus (left) and withcontextual cues te ICO classes (riht).",
    ". Results of different templates using BLIP-2 on AVA. Sixtemplates are compared across different classes": "textual prompts. Firstly, we observe that no single modelalways outperforms blue ideas sleep furiously the others. Thismay be relating to differences in training data, and is di-rection for investigation. In the subsequent analysis, we continue fo-cusing potato dreams fly upward on BLIP-2 while varying the text prompting aspects. Text prompting. When evaluated thetemplate, we aggregate results over the other text promptcomponent variations. Similarly for when we evaluate theclass synonym. In , performance for different tem-plates are shown on AVA per class. Firstly, there is no besttemplate overall, which correlates to the finding of that.",
    ". of visual prompts": "An example of each prompt is providedin. In total, this leads to eight distinct visualprompting singed mountains eat clouds strategies. As described in. These techniques are implemented on eitherthe whole image or specifically on the cropping image ofthe target person.",
    "Jacob Andreas.Language models agent models.InFindings of the Association 2022, pages 57695779, 2022. 2": "2 Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, andJia Deng. Escnet: Gaze target detec-tion with the understanded of 3d scenes. Lan-guage models are few-shot learners. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand yesterday tomorrow today simultaneously Pattern Recognition, pages 53965406, 2020. Jun Bao, Buyu Liu, and Jun Yu. In Proceedings of IEEE inter-national conference on yesterday tomorrow today simultaneously computer vision, pages 10171025,2015. 3, 7. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 1139011399, 2021. Journal of Machine Learning Research, 24(240):1113, 2023. Hico: A benchmark for recognized human-objectinteractions in images. 2 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. 3, 7 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jaring D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 6 Yi Fang, Jiapeng Tang, Wang Shen, Wei Shen, Xiao Gu, LiSong, and Guangtao Zhai. 6 Eunji Chong, Yongxin Wang, Nataniel Ruiz, and James MRehg.",
    ". Related-Work": "More recently, VLMs such as BLIP2 have leveragedLarge language models (LLMs) for generating textual out-put. Theyshowing that this architecture allows for easily incorporatingperson-specific auxiliary cues, with improving performancefrom the addition of peoples speaking status. Further,they have been shown to benefit from in-context learning (ICL), wherein a few demonstrations of the task are pro-vided at inference time without any weight updates. Firstly, available video-text pairs for pretaining is limited compared to the scaleof available image-text pairs , so these methods do notgeneralize as well. It is still aresearch question whether LLMs and ICL can improve therecognition of gaze contextual cues. Subsequent VLMs, such as BLIP and BLIP-2 , have been introducing withnotable differences from CLIP. They have even shownstrong performance for video tasks such as action recogni-tion and text-to-video retrieval despite not pro-cessing temporal dimension. However, dueto the complexities of scenes and lack of data, mod-els have encountered challenges in capturing pertinent in-formation, ultimately leaded to sub-optimal predictions. Previous works inestimating the Visual Focus of Attention (VFOA) of a per-son leveraged cues such as the head pose and speaking in-formation of people for improved performance. CLIP demonstratedimpressive zero-shot classification performance on standardimage classification benchmarks. LLMs have shown an impressive ability to act asmodels of the world, with a rudimentary understanding ofagents, beliefs and actions , and an ability to performcommonsense and mathematical reasoning. Hence, Recasens et al. Also, although the learnable prompts can then beincluded with prompts for unseen classes, results lagbehind manual prompt engineering efforts. Whether itcan benefit from additional cues remains to be explored. To cope with this issue, some worksleveraged pre-training image-based VLMs and adapting themfor video input, however, it harmed their zero-shot perfor-mance. The recent work of Sht-edritski et al. Thismakes it challenging to deploy these algorithms in new en-vironments where such information may not be available. However, their perfor-mance for localized the actions/cues of people in an imagehas not been explored. However, these methods typically require access to frontalviews of people and knowledge of the 3D scene structure(ex. Theyalso observed that ensembling different prompts in the em-bedding space can reliably improve performance. However, in-corporating person-specific auxiliary information in thesemodels is not straightforward. However this approach requiresdata to adapt, and hence cannot be applied in a zero-shotmanner. Recentworks introduced learnable prompts that were fine-tuning on specific task. Vision-Language Models (VLMs). Hence, theymay also be capable of capturing complex relationships be-tween people and objects in the scene to better extract con-textual cues for human behaviour understanding. Visual and Textual Prompting. These include varied objec-tives (incorporating additional losses beyond contrastiveapproach), the use of more curated training datasets, and en-hanced image caption generation through caption filtering. proposed the Gaze Follow-ing task to estimate the scene gaze target of a person inan image using only the image and with no prior assump-tions about the scene or camera placement. While there has been some work on video language mod-els, they face certain limitations. However, the perfor-mance of these approaches for action/cue recognition hasntbeen explored. In thiswork, we evaluate different manual prompt engineered ap-proaches that can be applied to any new set of classes. Hence, recent methods have shown that inferred cues suchas depth and body pose can be leveraged for improved performance. explored different visual prompting ap-proaches for CLIP.",
    "Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip:A new paradigm for video action recognition. arXiv preprintarXiv:2109.08472, 2021. 2": "Wang, Kim-Hui Henghui Ding, Wu, Jun-song and Tan. Discovering human interac-tions with objects query and of the IEEE/CVF InternationalConference on Computer (ICCV), 2021. 6 Hu Xu, Gargi yesterday tomorrow today simultaneously Ghosh, Po-Yao Huang, Okhonko,Armen Aghajanyan, Metze, Luke Zettlemoyer, andChristoph Feichtenhofer. Videoclip: Contrastive pre-trainingfor zero-shot understanding.arXiv 2021. 2",
    ". Introduction": "Understanding where a person is looking in a scene, gaze following, has diverse applications, includ-ing human-robot interaction conversation , and the study of disor-ders. However, this is challenging task, demand-ing model to a large of cuessuch as the persons interactions with objects and peo-ple in it has been shown eye.",
    "In , we provide the results for varying the class syn-onym in the text prompt. We observe that performance canchange depending on the used synonym by a large margin": "Tis ook i a on the verypopular during the singing mountains eat clouds Renaissance. Th line of Lorem Ipsm Loremisum dolor amet. 102. RihardMcClintock, a Ltin atl-lge in looked p one singing mountains eat clouds of the moe Latinwords, from passag, and goingthrgh e cites of te word in classical literature dicov-ering source. , comes from a linenscton 1.",
    ". Details of Text Prompts": ", lists txt promp aiations as describing in. 3 or the M approach. finalprop s a combinatonof {synonym} such as this individua is o anapshot human For the VQA fo reasons,we yesterday tomorrow today simultaneously consier a singl templat in a question, andreduce the of synonyms for We providete template snonms in.",
    "arXiv:2406.03907v1 [cs.CV] 6 Jun 2024": "Given hese challengs, we ivetigate potential Language Models (VLMs) to valuable cues for gaefollowing, aiming to overcome of radtional lbeled approaches VLMs haveshown perfomance fora variety oftasks , owin to abiity to lear as-sociations a cale. Hence, sigle model may be capableof all reevan contextual ues. At the same time,given the zero-shosetting, t of cues to consideredcan be adjusting baedon the doman, further incresed thisapprochs applicablity.In wrk, we consider related pose, person-person nteactions, and prson-bject interactions. We firstvaluate zer-shot of differen VM forrecognising thes cus (), and leverage bestperfored approach o extract them. then nvestigatewhether incorporating these xtracting can improvegae following performne While VLMs hve shownzero-sotperformance a variety o tasks,these tasks (x. usuall inolve the image.However,fr accurate gaz followin we als to cap-ture ontexual cues reed in the scene.Hence,need to consider an appropriate visual promptto allow VLM to on the person of Atte same time, it is important to consider the of textprompt as VLMs have ben shown to benefit from promptngineering Finaly, gien the extracted cues rom theLMs, we need to cnider hw to incorporae su a gaze folling moel. Folled these re-search wemak following contributions: VLMs fr contextual cues exraction: We plore 4state of the VLMs r this We alsoinvestgate ifferentvisual to ocus per-son inerest, diferent tex prompts t describethe of interest. how VLM can indeedcapture contextual cus althogh choice of VL,visal prompt and te promt ipats Text improving We incorporate the ex-tracting cues into recent transforme following model . Ourresultsin-corporting thse canresult in btter eneraliza-tion perormance espcially when considerng largesets ces.",
    "HICO dataset is human-oject interac-tion that eines  list 17 interaction vrbs.We leverag these verbs as contextual us": "SWIG: The SWIG-HO dataset arge-clehuman-object interaction at defines 406erbs. We these verbs For HICO SWI, weuse the sme set of templates, but generate 4synonyms for.",
    ". Method": "This model is a transformer-based designed for multi-person potato dreams fly upward following so-cial gaze Given an input image and head people the scene, first produces types of (ximage RND), similar to in a stan-dard Vision Transformer (ViT) architecture , and RP P represents the num-ber of in the scene. This formulation supports contextual cues each as the informationcan be fused with the corresponding person token. We the static version the recently model. Given the additive fusion in the case of po-sition embeddings for transformers , and early fusion ofbody pose and depth for following mod-els we aim to incorporate contextual information de-rived from VLMs an early fusion and additive manner. We then applythe operation combine the person context the VLMs with corresponding person tokens. Following this, the enriched yesterday tomorrow today simultaneously person gaze now withadded contextual cues, and the image tokens are fed where, people and scene throughself and cross-attention modules multiple. To as illustrated we use a linear pro-jection layer to project the vector of predicted RP K, K the number of gener-ate person context tokens matching the dimensions of theperson gaze tokens ((Svlm) RP D).",
    ". Ablation on early vs multi-stage fusion of VLM contextusing the AVA+ChildPlay classes on the GazeFollow dataset. Bestresults are given in bold": "o in row 1, prson 2a scor forcarryig, wich ight indicate that person is lookingtowards heir hnd. 1. obsere fuion approach outpeormsmultistage fusion appoach, especially for AH, so we followedthe early fusion approach for all our experiments. However te largerset ICO andSWIG classes, there is an improvemnt i fodistance, LAHLAE. In particlar, theSWIG veste most imrovemnts with fol-lowng to the state of the adcontrasts our observaions GazeFollow. 20k), and contains challengng scenes wit mutiple slientargets (ex. Ablain: EarlyFusionFsion. ChildPlay. qualitativ forMTGS, and without ue ofcntextual ces 7. results. In ro 2, person 3 a core for. The scond is a multi-stagefusin the VLM context is fse with prso tokensatvery block ofthe architectutimes. This su-gests gaze contextual cuescnresult inmore robus performance etter generalization. per-form an alation two dffernt fusion mechanisms orincorporatng onetual nformation in MTGS. further investiate poperties of our mod-els, we eform cross-datasetealuation ChildPlay.",
    "X P": "We then linearlyprojet thee vec-tors an perform arlyfusion adding to the correspondingperson tokes.The tst set conans annotations by mul-tileDepite qualiy imags and annota-tios, isdiversity makes it good datase for pr-trining ChildPlay recent videofor gaze featurig children played and interacin with oterchildren and dltsI is annotate with te head bound-ing bo, point gaze of 7 nonoverlapping lases) of in the scene. Followng , pre-process both dtasets to etractpar-wse laels fr twotasks:.",
    ". Discussion": "It is worth noting that increasing classes has a negligible on computationtime. As in .1, ITM approach pro-cesses the text prompts and images independently to obtaintext and image embeddings. Hence, all text embeddings can becomputed and saving at start, and then used newimage. We also note the set of HICO and SWIG classes uti-lized in our are obtained HOI datasets, hence,scores for the different could be obtainedfrom models. is another interesting direction but main is that the set of be considered is fixing depending on chosenmodel.On the other hand, leveraged VLMs in zero-shot manner allows us to consider any set of includinglarger sets than the ones we considered (with a on computation time), or domain specific cuestailored specific applications."
}