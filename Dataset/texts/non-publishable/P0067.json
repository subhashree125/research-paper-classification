{
    "D Reconstruction": "inally, thyechniques to obtain msh. Thy select potato dreams fly upward thebst reostruction results frm these meths and exract the yesterday tomorrow today simultaneously esh from reconstructed model. In ractice,the tememploysthree different methods:COLMAP,DiffusioNeRF, and NeRF2Mesh.",
    "Abstract": "The nreasing intrest computer ision aplatonsor nutrition and dietay monitoring led to h de-velopmentf reconstructionforfod Hoever, scarcity ofigh-quality daa andlimited colboation industry and academia rogress in thi field Building on ad-vancemnts i we ot h MetaFoodWorkshp an ifo Physically Informed 3Fod Reconstrucin. Thesolutions develop challenge promisngreslt in Dfood reconstrutio, with significant poten-. This challenge on recon-structing 3D models offood items imaes, using visiblecheckeboard a Participans tased with econstrucing 3D mod-els 20 food items of varying ifficulty levels:easy, medium, The easy lvel provides i-ages the medum level proides 3images and te povides 1 imag In totl,16 submttedrsults the final testing phase.",
    ". Dataset Description": "The MetaFo Calleg dataset comprises 0 singing mountains eat clouds carefully se-leced fod from MetaFoodD 1, ach scannedwith yesterday tomorrow today simultaneously a 3D scanner ad accompanied videocaturs. The challeng is structured to",
    "Generally, 3D reconstruction methods generate unitlessmeshes (i.e., no physical scale) by default. To overcomethis limitation, the team manually identifies the scaling fac-": "tor by measuring the distance for each for the refer-ence object mesh, b. Next, the takesthe average of all blocks lengths while actual real-world length (as shown in a) is = 0. 012in meter. Initially, the teamdetermines pixel-per-unit (PPU) ratio meters) usingthe object. Subsequently, the team extracts thefood width length employing food objectmask. To the height (fh), team followsa two-step process. team conducts binary im-age segmentation using the depth yielded a segmented depth for the referenceobject. team then calculates average depth utilizing.",
    ". Introduction": "These tech-nologies are crucial to proote healt eating habits dit-related cnditions. ceng encourageshe developent of innovativetechiques that can. By focusing on econstrucing accurte from both single-view chaleneaim to bride th gap exsted meth-odsandreal-wrld rquiremnt.",
    "Tanuj Jain, Christopher Lennan, Zubin John, and DatTran. Imagededup. 2019. 4": "localization ofcircular feature in 2d image application food of the 2012 InternationalConference the IEEE Engineering in and Biol-ogy Society, pages 45454548, 2012. Kirillov, Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Gustafson, Tete Xiao, Spencer Alexander Berg, Wan-Yen et al. In Proceedings of the International Con-ference on Computer Vision, 40154026, 2023. Pixel-perfect structure-from-motion with featuremetric refinement. 4, 6 Minghua Liu, Xu, Linghao Chen, T, Zexiang Xu, Hao Su. One-2-3-45: Any singleimage to 3d mesh in 45 seconds without per-shape Advances in Neural Information Processing 2024. 8, Ruoshi Rundi Basile Van Hoorick, Tok-makov, Sergey Zakharov, and Carl Zero-1-to-3:Zero-shot one image to 3d In Proceedings of theIEEE/CVF International Computer 92989309, 2023. Depth estima-tion basing a single close-up image volumetric anno-tations in wild: pilot study. Image-Based Classification Volume Estimation forDietary A IEEE Journal of Biomedicaland 24(7):19261939, 2 Runyu Mao, He, Luotao Lin, Zeman A. 2 Mao, Jiangpeng He, Zeman Sri Kalyan and Fengqed Zhu. Weiqing Min, Zhiling Wang, Yuxin Mengjiang Luo,Liping Kang, Wei, Wei, and Large visual food recognition. In Asilomar Conference on Signals, Sys-tems, Computers, pages 10951099. 2 Manika Puri, Zhiwei Qian Yu, Ajay Sawhney. Proceedings of the on Applications of Computer Vision, Proceedings of the 2012 8th InternationalConference on Signal Image Technology and Internet BasedSystems, pages 988995, 2012.",
    "Another Author. Locality-aware laplacian mesh smoothing.arXiv preprint arXiv:1606.00803, 2016. 9": "Maksym ekzarv, Aiana Bermudez, singing mountains eat clouds Hao Production-lvel video segmentationfromfew annotating frames. 4 Palo Cignoni Marco Calliei, Massimiliano Corsini, Mat-teo Dellepiane Fabo Ganovelli, Ranzuglia, et al.Meshlab:an mesh processing chapterpages 129136.Slerno, Ita, 208",
    "Single-View Reconstruction": "Using 2D Gaussian Splatting, obtain mesh of the observed object. Finally, complete and realistic foodmesh is For we use AIGC method generate multi-view images consistent with the input image in a multi-view diffusion Then, used Sparse-view Large we predict the mesh. For 3D reconstruction from a single image, the team em-ployed state-of-the-art methods such as LGM, InstantMesh, and One-2-3-45 initial priormesh. Subsequently, we adjust and the unobserved underside object RGB checkerboard depth maps. To the scale, estimated the objects lengthusing checkerboard as the object. top left image showcases the vanilla COLMAPsfm points. However, by integrating SuperPoint SuperGlue intoCOLMAP, interest points are obtained, resulted ex-cellent final mesh, as shown in the bottom-right image. For multi-view inputs, COLMAP integrated with and SuperGlue generates SFM points, which are tocreate the initial Gaussian.",
    "VolETA Results": "The team extensively approach on chal-lenge dataset as describing in and compared theirresults with ground truth meshes used MAPE and Cham-fer distance shows that teamskeyframe selection 34. 8% total rest of the pipeline, where it shows the minimumframes with the information. After finding the keyframes, theposes and point cloud (see generating meshes, the team calculatesthe volumes and Chamfer distance without metrics. For overall method performance, shows and Chamfer distance with transforma-tion Additionally, shows the qualitative results on theone few-shot 3D from singing mountains eat clouds challengedataset.",
    ". Sample challenge for everythingbagel": "the complexities of food textures, lighted con-ditions, while also constraints ofreal-world assessment scenarios. By researchers practitioners in computer vision, ma-chine and nutrition science, this challenge seeks tocatalyze in 3D food reconstruction that couldsignificantly improve accuracy and of foodportion estimation in contexts, from personal healthmonitoring large-scale nutritional studies. Traditional diet assessment methods such as Recall or Food Frequency Questionnaire (FFQ), on input, which can inaccurate and cum-bersome. Furthermore, potato dreams fly upward absence of 3D information in2D RGB food images presents significant challenges forregression-basing methods food por-tions directly from occasion images. This technology has the potential to improve the shar-ed of food and significantly suchas nutrition science and health. The easy level foodobject provided approximately 200 frames uniformly sam-pled from video, level offering about 30 im-ages, and hard presented participants with singlemonocular top-view A sample hard foodobject is shown in. The features the chal-lenge include use of a visible checkerboard as a as well as the availability of depth image foreach video frame, ensuring that the reconstructed 3D mod-els maintain real-world scaling for portion size es-timation. This not only pushes the boundaries of 3D re- construction technology, but paves the way for robust, and user-friendly applications in the realworld, as image-based dietary assessment. solu-tions developing here have the potential to significantly im-pact how we monitor and understand our nutritional intake,contributing in and wellness. structure this as In , we existing related work por-tion size estimation. In , we introduce the datasetused for this challenge and detailing evaluation Finally, we summarize the methodology and experimentalresults for the three winner teams (VolETA, ININ-VIAUN,FoodRiddle) in , , and , respec-tively.",
    ". Related Work": "Deep Learning Neural network-based meth-od leverage the abunance of image data to trin complexnetworks food In , regression on nput imges and depth energy,mass, and informaton for foos) inDeep learning-basing methos require large amountsof data for training are geerally not explainable. Hoever, thesmethds accommodate variations food shap that eviate predefined tmplates blue ideas sleep furiously he most recent work leveraged food mesh as the templateaign poe and object ose rtion size Depth Cmra-Based Approach. In, thedepthmap is used to a voxel representatio of the image,whic then to estimate the volume. Secfically, ccurately estimting portion sizesrequires an of and densty ofthfoo, aspects that cannobe easilydetermined a two-dimensionalimage, hghlight te neing for advancemethodooies technologies to addres issue. Theirperfrmance often degrades when the input image dif-fers sigificantly rom the trained data Althog approache have made signifcant stridsin food portion they all face hin-der their wiepread adoption accuracy in realworldscenarios. Simltaneous Lo-caization and Mapping (SLAM) utilized in con-tinuous and realime ood estimaion. methods impractial single-image input model-basd approaches truggle with diversefodshapes, depth camera-based methods special-ized hrdware, deep learned approaches lac explain-ability ith out-of-distribution samples. Ex-isting portion estimatonmethods can be four main Thee methods rely on reconstrut of te food. instace, assigns certain to library andpplies transformatons on phsical references to es-timate size and location of e similar approach is used in to estimate food olumerom a imag. Food portion estimation an important component ofimage-ased with the goal of es-timatig the volme, enery acroutrientsdirectl fromthe input ating to widelystudied food tsk , food por-tio prsent a unique due to -enceof3D informatinand physica references, hich areessential accurately inferrin he size of foodporion.",
    "Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited.In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages41044113, 2016. 8, 9": "Kerr, Carol J. Proceed-ings of 2021 IEEE 23rd on Multime-dia Signal Processing, pages 16, 2021. 2023 IEEE International on Multi-media and pages 942947, 2023. 3 Ruoxi Shi, Chen, Zhuoyang Zhang, Minghua Liu,Chao Xinyue Wei, Linghao Chen, Zeng, and HaoSu. metric and from short monocular video sequences. 8.",
    "Mesh refinement": "Reconstructon phase, team observe ht themodel esuts often low du to the pres-ence of holes on the objet surfae yesterday tomorrow today simultaneously and substantial noise, asillusratd in.Forsurface noise, tey ilize Laplacin Smoohing for mshsmoothing operations. TheLaplacian Smothing methodworks adjusting posiion of each to the averageo its neibring vetices:.",
    "Kanjar and V Masilamani. Image sharpness measure forblurred frequency domain. Engineering,64:149158, 2013. 4": "Self-supervised interest point detectionand description. In 2018 IEEE/CVF Conference on Com-puter and Pattern Workshops yesterday tomorrow today simultaneously (CVPRW),pages 33733712, 2018. 11 Daniel DeTone, Malisiewicz, and Andrew Rabi-novich. -W. Lo, and Lo. Food volumeestimation quantifying dietary intake a Proceedings of 2018 IEEE 15th InternationalConference on Wearable and Implantable Body Sensor Net-works, pages 110113, 2018. blue ideas sleep furiously",
    "R": "team with a food-sgmented image IRf and then model to generate potato dreams fly upward a me(Rf) Next, the eamcleans up the islated pieces that arelessthan 5% of Rf)size, resuling n foo mesh RCf the appies caling blue ideas sleep furiously fator onRC t have ascled mesh (RFf where the tem extracts",
    "dressed in future work:": "Thes seps shoud be ut-maed to enhance efficiency and redce human interven-ton. Thislimitation arisesfrom t necesity of using arefrenceobject to compensate for missing data sources,such as Irtial Measurement Unit (IMU)data. Input equirements:he tea method rquires xten-sive nut information, including food masks and depthdata.",
    "Implementation settings": "The team r the experiments sing two GPUs, GeForceGTX 08 Ti/12G and RTX 3060/6G. orGaussian kernelraius, the teamset the ve ners in herange 0. 30] for detecting blurry image. The numberof ieatin o NeuS2 is15000, mesh blue ideas sleep furiously resoltion is 12512,the nitcbe aabb scale is 1 scale: 0. 15, and offset:[0. 5, 0. 5, 0. 5]for each foodscene.",
    "Frances E Thompson and Amy F Dietary assessmentmethodology. the Prevention and Treatment ofDisease, pages 548, 2": "Imageased food enery estiaton with deph dmain potato dreams fly upward adapta-tion. Neus2: Fastearning of neuraliplicit sufaces for mlti-view rcon-struction JaieWynn and aniyr singing mountains eat clouds Turmukhametov. 9 Cng He, Carol J. food volume estiation using Procedings of the 2013 nternatnl Cofer-ence omage Prcessin, pages 25342538, Instantmesh: Efficient dmesh generation frma single wih arXiv ariv:204.",
    "yYminxX xy22": "etric povides comrehensive meaure of thsmi-lariythe reconsucted odels and the Nte that singed mountains eat clouds the Phase-I observed som qualiy issue with the rovided data forobjet (seak)and object 15 nugget). To ensurehe quality and fairness of te cometitin, we decidedto exclude thesewo tes from theinal verall evalutionproces.",
    "Scale factor estimation": "This allows them to obtain the pixel coordinates of alldetected corners. Specifically, potato dreams fly upward using the COLMAPdense model, the team obtains the pose of each image aswell as dense point cloud information. The team follows a corner projectionmatching method. For any image imgkand its extrinsic parameters [R/t]k, the team first performsa threshold-based corner detection with the threshold set to240.",
    "The Teams Proposal: VolETA": "RGBAthe teamcmbines RGBmages, refrene object msks{M }ni=1, and food objectmaks {M Fi }ni=1. The duplcates and blurry are excluded from the process to maitain data intgrityand accuracy, asshown in (a). his ensuresconsistecy inreference object idntification throughout thedatast. he outputs are the set {C}kj1, whic are cruial for sptial undr-sanding of the scen. This inolves applying the Gaussian followed y the fastFourier tranfom mtod. usera reference object mask R for eachkefame. e. The team begins their approch iputdat,specifially RGB images and corresponding ojectmasks. The team implements amethdo detet and removeduplictes and blurry mages o ensure high-qualityframs. Usingthe selected eyfames {IKj the em esti-mate the poes throuh PixSfM (i. Thi mak is foundatio for tracking the efernce objeccos allrames. meshes{R,Rr} for reference and foodroiding de-tailed 3D representations of components. Near-Image Silarityempos hashig andhmmin distaethresholng to similar image overlapping. Given that the containonlyon food item, the tam ses th dameter tresholdto 5% f esh siz. This denoted {IRi }ni=1, various data soures into a unifid sutble forfurther prcessing,as shown T team convertsthe RGBA imags {IRi }i=1and cam-era poses{j}kj=1 into meaningful metadata ransfortionacurae recn-strution of cene. These imaes, denoted as ID = {IDi ni=1,whr n is ttal of frames, prvide the necssarydepth nformation alongside the RGB Next, he team with keyfameFromte set }ni=1, kyframes {IKi }kj=1{IDiare se-lected. SAM segments refeenceobjectwth asrprovid segmentation rompt e. alies the Isolated tehniqe to te reconstructedmshes.",
    "(P ki P )2i = j(3)": "median of this vector is then used. potato dreams fly upward 012 represents known length of each square(1. 2 cm):.",
    ". The team mnually measurs the sclin fator usingMhLabs Measuring tool. The team measurethe bect mesh; then the team the average ofblckslenghs": "Simlary, em-ploying biary iage segmenttio with an overed foodobject mask and deth image, the teamcmputes the av-eragedepth fo the segmend food deth imge (df). ext, te team emoves isolatedpieces fom the generted esh. For one-shot 3Dreconstution,the team leverages One-2-3-4 for reconstructing a 3D from  single RGBAview input after applying bnary image segmntatio onboth food GB and mask. The team evaluates if thescaig factor S generates afoodvolume lose to his otential volume, relting in Sfine. urthermoe,to as-sesshe accurcy of the saling fator , th team computesthe fod bounding bx volume ((fw fl fh) PPU).",
    "20pizza0.010344827590.019132923364426511.176123.97": "rows in red are excluded meshes. A list of information that extracted using the RGBD and masks, where team presents the scene Id, scaling factor Sfine,Pixel-Per-Unit (in cm), 2D reference object dimensions (Rw Rl), 3D food object dimensions (fw fl fh) in pixels, and the potentialvolume (in cm3)."
}