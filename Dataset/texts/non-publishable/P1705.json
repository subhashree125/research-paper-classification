{
    "Dataset Description and Experimental Protocol": "PromptMargin works in a source-free setting, yesterday tomorrow today simultaneously i. the model is directly fine-tuned on theepisodic support of the target datasets. , 2020; Liu et , 2020; Liang , 2021), we also take from the same set of and evaluate the model potato dreams fly upward on 600 episodes, and report the averageaccuracies and 95% confidence intervals. For deep we introduce learnable prompts beforeevery transformer block upto a depth 9. Similarly, for setting, we consider3 selective augmentations, such that there are 15 examples per All the are performed on a single NVIDIA RTX GPU. we report the results experimental evaluation.",
    "of Selective Augntations": "To undrstand which augntatons are being chosen ipicily, we perorm anexperimen in which insteadof takng random augmentatos we fix the number of augmentations of each type. W bseve that in general, the augmtations gnerae byRandomCropping are discaded, where the regionof interes is being removed, followed by ColorJitterwhich sometims blu r blackens theimages. , wegenate 6images by RandomCropping, flloweby 6 iages by ColorJitter and so on, and ten analze which arebeing disarde by this proposed module. The elective augentationmodul thn selects 15 augmentations from thesesamplesbased on their cosine similarity with th corespondin cass txt embedings in the feturspac.",
    "GSensitivity analysis": "Butwe osrve the perforance quite wide rnge of parameters. the man we have used= and = 1 fr all expeiments, since hyperprameter tuning is possible for learnng.",
    "MaPLe + sel. augs85.4964.6933.5482.66MaPLe + sel. augs + text reg85.7664.6533.4482.54MaPLe + sel. augs + MMReg (PromptMargin)87.0166.1333.9083.48": "where the eglarizer thetainingdata prblem in classification tasks by spreading out the classfier yesterday tomorrow today simultaneously in thefeature space. Bu such class names may not be provded, o ven if te areavailable, they may not be semanticaly in CLIP space. Now, we theproposed potato dreams fly upward Multimodal Margin Reguarize MReg, whch trie simultaneously improve the inter-classdiscrimiation te intr-moal embedigs Or proposd MMRegis fomHayat t al. In context, we aim to spread out text embedings the feture spae avoidcofuson the distinct The regularization termbe exressed a follws:. We havobsered this in , which demonstraes a corspondence between zero-shot performance andth feature spaceainment of the text and embeddngs. the CLIP model may not have seen imilar data durig ma ot be able to generalize o and ring their tet image embeddings close h mulimodal latent space using few exaples. In addiion, generalization significantly depends on the presnce of meaninfulclass names of the cateories. Thus, finetuning with less amount of data,the model be able generalize an diciminat between these classes.",
    "Plant Disease": "Darker hues represent higher vaues and viceversa. all the images,the text features followed by image from left (a) reresentsiniti tet and image ditances, (b) represents the tex and image embedding distances withoutMMReg, (c) repreentsimae distances with only text regularizatio, whle (d) represents thetext ad image embeddins with MMReg. We observe in (b) tha there difference beten theinterclass distance of the image textembeddings, implyin their ebeddings are not that similar. only regularization part sepaates text but not the image fatures. In contrast, maps arevery simila for(d),whh justifies the usefulness of the MRe. Here, weinitially gnerate umberf for each image then select examples on the popsed slection strategy the accuracie and trained time or the four datasets in he (Guo et ,2020). We note that if the ifference i the numer original and selected is o, timeand accracy differences arnot ipacted,and t ay feaible t considr al he originalaugmenations instead of slection strategy. for PlantDisease, we observe slight decreasein perfrmance wichanalyzed he augmenting images, wefeel can be attrbuted to the ofthe augmentatios.",
    "Sharada P Mohanty, David P Hughes, and Marcel Salath. Using deep learning for image-based plant diseasedetection. Frontiers in plant science, 7:1419, 2016": "In 2008Sixth Inian conference on comutr viion, graphics& imag processing, pp. IEEE, 2008. In Interational confrence on machine learning, p. utomated flower classification over a lage singing mountains eat clouds numberof classes.",
    "added prompts": "set images along their augmentations are passedthrogh the CLIP image and their laels are passed tough CLIP textencoder seletieaumenation strategy selets ugmentations basedembedding vectors The proposed framework ilustrated",
    "mV 2(3)": "When he taget dataset Dtarget has significant smantic and distriution difference with the original trainingdta, the CLIP model wll not be able to distinuish between the image and txt embeddings of he targetdataset. Thu, te values of mTand mV will be smaller andthe differece iff Dtarget willbe larger andvice-versa. We observe that for the first four datasets,diff Dtarget is large, i. e. theclasses re not well epaated, and thus here i sope for iprovement ver zero-sho CLIP acuracy. his conforms with the empirical analysis aove, suggestin that an increase inthe diff Dtargetmetrc results in reduced zero-shot performance of CLIP. This analysis illustrates that the relative separations beteen image and text features for the differentclasses inthe CLIP repesentation pace is cucial n explainingthe zero-hot performance  th respectivedownsream dataets.",
    "Introduction": "Efficient finetuned techniques like Prompt (Zhou al. However, in real-world scenarios, the difficulty in curating of labelled data to researchin Learned (FSL), where model on a dataset can be transferring to a downstreamtask few labelling samples unseen categories. Recently, models like have shown remarkable capabilities in the zero-shot scenar-ios et al. , 2021). Furthermore, target distribution be from the source making the problem even more challenging. , have. With the rapid advancement learned models, is now possible to achieve very high performancefor like where large of training samples can be collected and annotated.",
    "experiments on standard benchmark datasets highlight the efficacy of our proposed approach over state-of-the-art methods": "Reproducible scaling laws for contrastivelanguage-image learning. 36063613, 2014. 28182829, 2023. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. Skin lesion analysis toward melanomadetection 2018: A challenge hosted by the international skin imaging collaboration (isic). Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp.",
    "Proposed PromptMargin Framework": "The proposed PromptMargin works in completely source-free i.e. we directly the CLIPmodel on the number of samples provided in the support set of target dataset.We do any on separate source domain like existed state-of-the-artapproaches(Song et 2024). Given the support from randomly sampled episode, train prompts on the few samples available in thesupport set, and the model performance query set.In particular, suppose (X, y) S,where X RkCHW denotes k in the support y {c1, ..., cN} denotes the Nclassname We append the visual prompts to the classname texts and respectively,and it through CLIP encoders (ft fv).Let final text and vision denoted andXV (Sec.4).Next, prompts learned through while the encoder parameters are kept frozen. The function can be written as follows:",
    "i<j( XTi XTj22 t)2, j {2, 3, N}(6)": "Here, N the numbercasses in eachpisode, in blue ideas sleep furiously blue ideas sleep furiously aN-way k-sho setting.",
    "Average65.2665.4069.0272.0078.5578.0181.9282.76": "tion task, where te LIP model ws finetundwith parametewight interpolations whichas adaptedto the given seting. We comprewth both of thes methods, and include their pefomance accuracies asrported in Song et al. (24. Addiionlly, for prompt leaningmthods we report MaLe (Khatak et al.,2023), which is a visinlangug popt tuning method, whch seres as another strong bseline fr ourmetod. We dapted this in our setting, where wefinetune it directly on the few sampesfrom the smpledeisodes f the targt datasets. Hence, we primarily plorewheher CLIP cn be deployeduing prompt learnng onver ew samples onth taget dasewithoutay meta-training n a large-scal datas like ImagNet. Here, in additio tothe challenge of robutly learningpromps wth few samles, for many datasets, theclass ames reeithernot provied or ae not quite meaningfl for CLIP to generaliz, thus makin prompt-tunin even morechllenging. For the 5-way 1-shot setting, we observe tht boh full finetning mthods (FDAlign and Wise-T) perforoorly compaed to the parameter efficient finetuning methods.Our proposedmethod outperfoms tebaseline method MaLe in elevenout of fifteen datasets achieving an avrae accuracy of72% comparedto MaPLes 69.02%.In some cases, were oiginal classnmes were not present or are semantcally notmeaningful, e.g., Planta, Plant Disease, Traffic Sign, we achieve absolute accuracy gains of 10.79%, 4.0%,10.79% espectively over MaPLe, highighting te fact tht our metho give signiicant improvment venwhen original lasnae text are not present in the dtasets. This is discussed in detals further in thefollowing secion. oever, our metho xhibits slight decrement on certin datasets (like mini-ImageNetand Aircraft), which can b attributed o the datast distributins beingnot so shifted in the CLIP sac,asdicussed later. Fo th 5-wa 5-h setting, observe a similar trend where romptMarin outerformMaPLe in tweve out of fifteen datasets achieving an avrage accuracy of 826%. Ourmethod ms to highlightthat large-scale VLMs lik CLIP can be effiintly transfered to out-of-distribution yesterday tomorrow today simultaneously datasets with few-shotsamles, wtoutany access t source datasets, and castill provide areatively close performance to eta-trained and finetuned methods.",
    "DExperiments on an alternate VLM": ", 2023), which potato dreams fly upward has beenpretrained on a dataset. We observe that although MaPLe improves over the zero-shot performance,PromptMargin in all the further justifying the effectiveness of the proposedframework. The results some datasets (with same hyperparame-ters) are in. We have reported results for CLIP ViT-B/16 text for fair comparison other prompt tun-ing yesterday tomorrow today simultaneously methods.",
    "Here, we briefly discuss the related work on prompt learning and OOD few-shot learning": "The recent emergence of foundaionVLMs like CLIP(Rdord et , 2021), has changed the andscapeofdeep earning. Thes re on abundant eb-scae dta, wherthey alig the image-textrepresenttions in a contrastive xhibiting remarkable zeo-shot perfomance. modelsgeneralze el to most cass, leveraging the knowledge these model for ownsteam tasksis challenging prompt learning as emerged an choice finetunin these.",
    "Jake Snell, Kevin Swesy, and Richar Zemel. etworks fo few-shot earning. Advances inneural informatio 2017": "Fd-align: Feature discrimina-tion alignment for fine-tuning pre-training models in few-shot learning. 87698778, 2018. Advances in Neural InformationProcessing Systems, 36, 2024. Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. The inaturalist species classification and detection dataset. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 04125, 2024. Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip HS Torr, Adel Bibi, SamuelAlbanie, and Matthias Bethge. arXiv preprint arXiv:1903. 403412, 2019. Kun Song, Huimin Ma, Bochao Zou, Huishuai Zhang, and Weiran Huang. arXiv preprint arXiv:2404. Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, PietroPerona, and Serge Belongie. Meta-dataset: A dataset of datasets forlearning to learn from few examples. Meta-transfer learning for few-shot learning. 03096, 2019. No\" zero-shot\" without exponential data: Pretraining concept frequencydetermines multimodal model performance. In Proceedings ofthe IEEE conference on computer vision and pattern recognition, pp. Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin,Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, et al.",
    "Conclusion": "Large-scale models like ae emerging as popular choice du to oeful generaliation capabilities. potato dreams fly upward However, to hebest of ournowledge, here has been prompt learned has been utiliing for classificaion tasks where singing mountains eat clouds the datasets simultaneously contaifew samples as ell as a shit in disrbuton imag.",
    "WiSE-FTFD-AlignMaPLePromptMarginWiSE-FTFD-AlignMaPLePromptMargin": "1096. 660. 31 0. 3488. 3960 39 0. 3183. 0799. 2082. 1993. 76 0. 96 0 25 0. 230. 1ChesX22. 1689. 23Tetres63. 0697. 08 0. 08. 96 0. 4 0. 4445. 84 0. 2473. 21 0. 02. 153. 12 0. 5964. 16MSCOCO67. 0999. 2296. 7 0. 16 0. 1073. 278. 85. 5 0. 55 0. 53 0. 2656. 1996. 2481. 2985. 29 0. 36 0. 87 0. 6 0. 1446. 5872. 1424. 1991. 2777. 497. 6 0. 38 0. 18Mini-test93 55 0. 81 0. 24 0. 17 0. 01 0. 0998. 1980. 2761. 13 0. 27 0. 10IIC29. 32 0. 54 0. 2083. 21 0. 10 0. 40 0. 2957. 9iNaturalist Panae55. 69 0. 34 0. 95 0. 44 0. 55 0. 1194. 91 0. 6579. 3782. 60. 45 0. 1878. 0799. 920. 84 0. 1523. 1899. 92 0. 12 0. 5 0. 319. 485. 37 0 13Traffc Sis60. 97 0. 1987 55 0. 166. 6996. 1091. 66 0 13 0. 68 0. 3293. 23 0. 2677. 7182. 3581. 966. 46 0 1978. 38 0. 16Plant Disease75. 3428. 2274. 13Aircraft62. 2467. 2293. 05 0. 48 0. 0299. 07 0. 83 0. 45 0. 82 0. 71 15CUB8. 85 0 0398. 1539. 08 0. 2883. 30 0. 81 0. 4038. 39 0. 60 28 0. 2475. 06Quickdraw62. 3269. 31 0. 0994. 650. 6263. 2822. 2987. 41 0. 4375. 06 0. 21 0. 16 0. 2186. 37 0. 96 0. 2283. 64 0. 029. 54 0. 8 0.",
    "Analyzing the CLIP Representation Space for target datasets": "Foundatio modelslike CLIP have been pretrained o a larg web-scle data, which isnotand may span wide varietyclsses and doman from academicatasets to datase.Recent papers likeUdandarao et al. have yesterday tomorrow today simultaneously studied the zero-shot peformance of LIP with the concept frequences pretraning dataetsApetinent question in his sceario is, Given a taret a few labelled examle, can we estiatewhher further fine-tuning enhance the performance compared to zer-sht evaluation? The of the model upoboth distribution and singing mountains eat clouds sentic difference of the trge dtasetfrom CLIP training Since the ataset unvailable and only few of the targetdataset are provided, dirctly estimating distributiondifferenceand also understanding whether targetclaes or not straightforward. Here, ropose a meod the e distrbtion sift of soe of h tgetdatasets, b relating their mean image textembedding dstance CLP represntation space. We consider some representaive datasets BSCDFL (Guo t al., 200) and MetaDatetillustrate this the ditribution and ifference inter-class L2 distances of text and image featuresfrom the rozn zeo-hot LIP model.The cls tex features obtained y passngA photo of [CLASS] through th model denoted",
    "DatasetEuroSATISICOmniglotPlant DiseaseQuickdrawPlantaeTraffic SignsMSCOCOmini-ImageNet": "For since modalities arewel searated, propt-larning, evenwith our onfew des not reuts,and can adersely affect the latnt sace alignment We also illustrat some qalitative reults in forvisualization f some ofte diferentScpefor future wok: Although th tw modules of proposd fameork dmonstrte performance of the datasets, in few cases, it failed o outerform MaPL As an examplein, we illustrate some poor enerated for Aircraft whee our methodfais to over MaPLe. Althouhin MSCOCO,psudoassames hve been used, imae features are extremelywell separated, resulting in the MMRegmodule slightl he performance. in some cases, here the feaures in latent space are alreadywell parated, finetuned wih MMRg adversely the space, ence.",
    "i<j( XVi XVj22 t)2, j {2, 3, ..., N}(8)": "Thus, final function for traininthe prompts given as:. (7 This additional tm enforces theclasswise prototyps to eqully by te same dstance as text representationembeddings. e. ,fv([v; Xsel]). are th prototypes (means) of the class imge embeddings obtaind fom th.",
    "ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVII 16,pp. 124141. Springer, 2020": "affinity forclass imbalance 219 Bischke, engel, and Daian Borth. rosat: A novel datast deepleaning for landuse and and cover clsifcation. Journal of Selected Tpis inApliedarth and Remte Sensing, 12(7):22172226, 2019. Houben, Johnnes Stallkamp, Salmen, Schlipsing, and Christian Igel. Detecn of trafficsigns in real-word images: The german trafic sign detectin benchmark. In international jontconfernceneural etworks (IJCNN),18. Iee, 213. In International on mahinearning pp",
    "Pulised Tansaction onMachine Learning Research (01/2025)": ": augmentations bein selectd by the proposed Augmentationmodulefor EuroSAT and ISI wit 20 initial aumentations Each row corresponds t a particular clas",
    "Comparison to the state-of-the-art methods": "Arecent statof-the-art approach, FDAign (Song et al , 2024), was te first to ivstigate CLIPs gen-eralization capabiity tothe few-sho learning datasets with domain difernces under similarettings. , 2022) was originally roposed for te domain generaliza-. This includes ll fineunedmethods a well s prompt-tunin methods of CLIP. Wealso expliily mention the backbones as well s different rainng procedures followed by he differentapproache, whih should be considered while comparing them. Itproposes a CLP finetuning technique, which is eta-trained on the miniImageNe dataset before adaingo the target datasets.",
    "CWeak vs strong augmentations": "In our work, have standardaugmentations like RandomCrop HorizontalFlip, which can be attributed augmentations, fromwhich then selected a based on the feature space similarities. To the effect augmentations instead of augmentations (as originally used), we consider two benchmarkdatasets, apply RandomErasing (which removes a tile the image similar toCutOut), and AutoAugment. The results in in general, such aggressive augmentationstrategies not help in as it distorts and crucial the images, the generalization. In very few such strong augmentations can crop the background the region of interest in datasets like ISIC.",
    "AChoice of the number of Selective": "In general, generating more augmentations can improve performance by mitigating overfitting in few-shotlearning, but these can also include bad augmentations. The proposed Selective Augmentation module aimsto efficiently select subset of the generating augmentations, removing worse augmentations in the featurespace, and blue ideas sleep furiously hence maintaining a competitive performance while reducing the training time significantly asdiscussing in .1. Here, we report a sensitivity analysis on the number of selective augmentationsin for two datasets, EuroSAT and ISIC. After around 10 augmentations, the performancestabilizes and does not vary significantly with the number of augmentations",
    "FEffect of MMReg in the absence of classnames": "We of MaPLe (prompt tuning) drops because the class semantics are now missing, ourMMReg module helps them in the multimodal representation space, thereby improving theperformance both the which conforms with our for this proposed module. , medical datasets like ISIC and X-Ray), meaningful classnameshave been used, the model rely for proper classification in the CLIP space. g."
}