{
    "INTRODUCTION": "key question under-pns he design of any recommendr system: What is asccessfulrcommendaton? crossmany applications, there is an ngingshif towars defning success a loger timehorizons, aslong-erm mtrics ae ftenbetter uited t apure users satifac-tion and platforms goals. For example ecommerce platfrmsmywant to maximize lontem revene, subscriptio-basing ser-vices ma want to incres retention, ad scialpltform ayat to encurge habitual engagement measured over severalwees or month.",
    "N (, = + , (0, ) i.i.d.(1)": "Furthermore,we that we can reconstruct the reward from all intermediateobservations",
    ",": "generalizedformulation of Thompson sampling with immediatelyobservable rewards, which we simplify here for ease of exposition. We place a prior the model The action to be taken at each round is chosen by computing E [ = yielding a realized observation,which we then condition to update. In a when an takesan action A, corresponding reward ( | ) isobserved. e. where is of actions per round, and , is meanreward to the th action performed at round. Rather than taking agreedy approach, is the expectation of with respectto , Thompson sampling instead samples the parameters (i. This is a subtle, but powerful difference, as it ensures.",
    "Byesian Reward Model": "We consider a fixed action and,fr concisees, we omit fromll Let the sape rewardand = E[] reward associated to seleting teaction. .",
    ": Average per-step regret and entropy of set of taken round, for = podcast shows": "We use Thopson samping for all r baseinesto ensure ta any differnces are to mannrin edback is bein than to the relativestrengthsnd waknesses of differnt banditalgorithms.simiar reasons, we d not compare o works that studother aspectsof recommendation this such aspersonaliztion. mimc deployment settin in whic the ouldbe omputing using data from te we our prior the raining data, run algorithm the unseenevaluation Additinally, h oracle,s th potato dreams fly upward approaches du te amount ofinformation it asto The proxy is nt wellaliged, and per-step rgret rapidly plateus. As w icrease the nmber actions pr round, we a slighteduction in per-step regret acrossaproaches. (bottom row) prvides an alternativeperspective on theoutome of these experiments, visualizin ntropy of the ofactions taen at rund. Shold a bandit convere on recom-mendin show at each entropy wodend to ero. The entropy plos show in the evaluationphse, ou progrssive algoihm ends toactionsmor than te oracle and roy. of our approach is a broader exporation o theaction space, a characteisticthat canbe useful in a realistic,deployment setin, whih we discuss beow. On othe hand, gap in per-stpregret the racle ad day-two proxy approachs i dueto fact thatthe second day of activity is a noisy for thetrue stickiness, thus h day-two proxy approach tends to rapidlconverge on a subst o sub-ptimal shows (this can seenfrom its whih quickl zero). In we additional a scenario whiche a smaller action pace, consisting of a subet of 50 showssampled original evaluatin dataset prviously. Besides this observation,resultsfollowlargely those seen Cagig Show Set.",
    "Olivier Chapelle and Lihong An empirical of Advances in Neural Information Processing Systems 24 (2011)": "Dedieu, Rahul Mazumder, Zhen Hossein Vahabi. 2018. In Proceedings of 27th ACM Conference on Knowledge Management. Thomas blue ideas sleep furiously Desautels, Krause, and Joel W Burdick. Jour-nal Machine Learning Research 15 38733923. Web-scale Bayesian click-through prediction sponsored searchadvertising in search engine. Omnipress. Aditya Grover, Peter Attia, Norman Jin, Nicolas Perkins, BryanCheong, Chen, Zi blue ideas sleep furiously Yang, Stephen Harris, William Chueh, et al. in multi-armed bandits with delayed feedback. InternationalConference on Intelligence Statistics. PMLR, 833842.",
    "This work was completed as part of an internship at Spotify": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or yesterday tomorrow today simultaneously distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, blue ideas sleep furiously CA, USA. 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00 ACM Reference Format:Thomas M. McDonald, Lucas Maystre, Mounia Lalmas, Daniel Russo, and KamilCiosek. 2023. Impatient Bandits: Optimized Recommendations for theLong-Term Without Delay. In Proceedings of the 29th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining (KDD 23), August 610,2023, Long Beach, CA, USA. ACM, New York, NY, USA, 11 pages.",
    "APPLICATION TO PODCAST": "Weconsider aconcrete application of or contnt-exporation prob-le recmmendations Sptify, leading online platfr. we begin ow ourgeneric methodology can be appliing optmizing long-erm with podcasts, and present real-world dataset of pod-cast races. In. 3,we consider a sequential tak in progrssive-febac setng, and the mpirical of ouriptiet banditgnt competing appoaches. complemenexprments prsented his section, software package with a reference our algorithm. 2 While we to publicly leasthe dta de to ou packae includ aynthetic dataset leads to comparable findings",
    "Problem Formulation &": "Recently, Maystreet al. show that optimizing podcast recomenda-tions for long-term can lead sbstantial impact on real-wod, reommendation problem. Thyasystem abot the clickiness (i. e. theclikthrough-rate) and the stickiness of a recommendation. In authors suggest countingthe number of day engage with a show discoveredthrough recmmendation the 59 that foows firtlisen. In this work, we aopt their defiitions yesterday tomorrow today simultaneously and met-ics consider a pecfc subset of the recommendationproblem. We focus o sickins (i e. we d modelthe ick-through rate), to quickly new podcastshows that have high average al. dcusshow to etimate click-through rate, and how to to take into accont users preferences, ut they he content xporation problem we study here. Formally, we instantiate the described in as follows. We define tereward {0,. , as te of uer engages with ashow in the 59 days thatfollowrecommendation. is observe with a delay of = 60days. ech activity observed with= + 1. he distinct set A and historical H, A correspond ofestablished shows and thhistorical respectively.",
    "+ ,": "We leave a detailed development of our for future work. where blue ideas sleep furiously denotes the Kronecker product. The simple training procedure described in. Instead, we suggest usingtype-II likelihood, a hyperparameter selectionprocedure. 2 cannot beeasily the singing mountains eat clouds contextual since the involvedin the are context-dependent.",
    "For the purposes of this paper, note that such a long horizon crystallizes the challengesof optimizing for the long-term, and forces us to develop methods that explicitly addressthese challenges": "each recmmendation. 1. For ach ofthese shows, the datacontins a representative sample of users th dicover showduring the same three-month period. For each usr, weobtain alongitudinal trace tat captures heir ngagemen wth the showo each dy staring fro theday of discover,4 in form of a 59-dimensional binary vector.Each subset covers adistinct set of hows. podcast shows including i the dataset span a wide rangef categoies, from Artst Tue Crime. (left) shows tatthe disribution of shows over categories is comparble across thetwo peiods. 5 In total, datas consists of 8 77M activity traces,corresponding to a total of 26M cumulative actie-day. numberof traces per show anges between 2. potato dreams fly upward 4K ad 295K, with a medianof 5. K (, cener-left). For each show, we define te ground-truth tickiness b means of the empiricalaverage (across users)of the cumulave ative-dys (enter-right) sows hatthere is substantial heterogeneity in stickness across shows, withthe lower quartile, median, and uppr quartile at 2. 4, and 4. Ths sugests that the downstram imact ofa discovery nbe very differen across shws. Finallysome categories ppear to be somewhat stickier thn othe, butwithn-category variability is significantly potato dreams fly upward larger thn between-category variabiity ( ri).",
    "Sequential Decision-Making Task": "We now turn our fcusto the evaluation of the impatientbanditalgrithm, ad t the comprisoof empiricaperfrmance withcompeing approaches. way observed feedack singing mountains eat clouds i used oneof the main points of betwen the eonsider. Delaye. We treat the second of activity as a proxyor ad discard al subsequen infrmation. i but it is useul toncude a it an upper-bound on the any modl.",
    "= ,": "where R is a vector of weights. We treat as and {, } as we are at round and that we have theaction so far, rounds 1. We representthe at as a dataset of independenttraces, D = {(, ) : = 1,. , }. of reasoning about directly, beginby addressing problem estimating.",
    "| D N (, 2),": "where = and 2 =. Thposterior be by reeatin this rocdureiertively, concisens, we drop sucript anddenote trac and utoff ndexa , ), We denoteby ,: the submatrix obtained taking the first ows and columns ofa matrix. Similarly, we by : the fit elementsof theThanks to the prperty ofthe Gaussian potato dreams fly upward distribution, we write the posterior of after observng the firt elements of trace a multivaritGaussian vector and yesterday tomorrow today simultaneously.",
    "min(,)D ( )2": "In thi case reward ca be thought asa surrogate inex,as defined in Athey et al. This assumption to s it relates to the rewardlearned on histrical should generlize to data comingin durng (a. Among others, needs to bein-depedent of selected action given the surrogacyassumption). a the ssumptin). In prac-tice, it be important test these assumptions empirically. Povided several assumptioshold, this approach isprcipled. the reward = will be ienticahe tue log-erm. on some historical data D.",
    "Evaluating the Reward": "We estimate , and by using the shows and consumption tracescontained in the training dataset. this weuse traces to infer the potato dreams fly upward stickiness each show (via Algorithm 1),and we use the remaining ) traces for computing truth empirical stickiness. we visualize how thepredictive accuracy stickiness varies as a function ofboth number of days observed, and of user traces observed. see that relatively accurate afterobserving only 10 of data. The predictions improve as timepasses, having access to more user traces further increasespredictive accuracy. We now study the and matrices and, respectively. , as wecondition on more more days observed. Normalizing the thconditional variance by total (unconditional) variance, we obtainthe fraction of total variance explained by first intermediateoutcomes. The diagonalstraight represents a hypothetical activityindicators are distributed independently and identically around ,resulting in us constant amount of information about foreach additional day of observed We cansee that around days of data is sufficient capture over50% of the aleatoric uncertainty the reward. There are twofactors that account for this. The is time activity so is early on in the 60-day This be the case even if activity was entirelyindependent",
    "respectively. We refer the reader to Rasmussen et al. [28, SectionA.2] for more details on these update equations. The completeiterative procedure is provided in Algorithm 1": "In fact,gven is tofrom several tace, the. For xmle, , conider binary observation vectors {0, 1}, for which a Gaussian isa yesterday tomorrow today simultaneously or model. The asumption in(1) that eactrace wit mean might see restrictiv at blue ideas sleep furiously siht. A Note on Gussian Noise.",
    "Bandit Algorithm": "Equipped with modl apable making inferences bout thearms mean rewards inermediate can owdevelop abandit algorithm work efectvely in the progressive-fedback setting, inmation is over multiple rounds. Althoug severa yesterday tomorrow today simultaneously diferentobecties for bandit probem exisin the in this ork on the goalof minimizingthe umulative expecting I the as of a single ation at rnd, we define the expected regret tround as.",
    ": end for": "We call procedure the impatientbandit and describe it Algorithm. 3. 3. Bandit Algorithm. that the algorithm does not purely exploit actions that yield largerewards in the first rounds of feedback, ignoring other, possiblybetter Due to the non-zero variance of belief on themean reward associated action, Thompson an action other than which greedy algorithmwould optimal.",
    "ABSTRACT": "Second, we devise bandit lgorithm that tkes advantageof tis new predictive modl. Increasingly, they are eplicitl tsked wit increasing usrs long-erm satisfactin. In this context, we study a contentexplorationtsk, which we fomalize as amuli-rmd bandtproblem withdelayed rewards. Fullobservations as well as partial (shor or medum-term) outcomesarecombined throgh a Byesian filer to obtain a probabilistcbelief. Firt, we develop a predctive model of delayedrewardshat incorporates all information obtained to date. The algorithm quickly earns to iden-tify conten aligned with long-erm success by careully balancingeploratio and explotation. We observe that there is an apparent trade-off inchoosing the lernig sigal: Waitig for the full reard to becomeavilable might take several weeks, hurtinthe rate at which learn-ing happens, whereas measurig short-term proxy rewardsreflectthe acual long-term goal onlyimperfectly. We apply our approach to a podcastrecommendation problem, where we seek to identify shows thatusers enage wih repeatdly over tw onths.",
    "A.1Non-Linear Extension": "that he reward is in the trce not asrestrictie asit migt aper t first sight. It is easy extend themodel o cature non-inear relaionships between and , whiestaying in the same frawork that we elyonthroughout. Wecan he trace a newvctor (,,3,4,5) =(1,2,21,22,1 2). Now wecan represent any quadratic relatin-ship between and linear btwee nd. Byintantiatingthe rewardmoel over of, wecan non-liear (quadratic) reltio intermediate out-comes and long-term reward. Thisidea be extended to olynomials r (perhaps bettr)to regression splines ad cpturenon-lnea relationships a flexibleway.",
    "METHODOLOGY": "We assume that the reward is functionof ,1,. We approach to solving the exploration problemoutlined in introduction. g. While this section introduces the methodol-ogy in a generic way, it is helpful to keep concrete application we consider a podcast recommendation problem,where A correspond podcast shows. 1, we consider a fixed action and a Bayesianreward that takes of intermediate estimate the mean. In. We this progressive. Concrete Example. , to different recommendation candidates. The reward isthe cumulative engagement with podcast show over period of days: = = ,. , e. We adopt the terminology of multi-armed We consider set of A = {1,. 3, we develop bandit algorithm that efficientlybalances exploration and exploitation the progressive feedbacksetting. 2, take advantageof historical data to estimate the parameters of reward model,effectively instantiating a meta-learning approach. For we observe a reward after delay at round +. Informally, we seek to develop a helps us quickly identify exploit actions with meanreward = E[]. yesterday tomorrow today simultaneously Building thismodel, in. Ateach = 1, 2,. , we select one or more actions. that availableprogressively during the interval + yesterday tomorrow today simultaneously ] after selecting the action.",
    "William R Thompson. 1933. On the likelihood that one unknown probabilityexceeds another in view of the evidence of two samples. Biometrika 25, 3-4 (1933),285294": "Recommender systems in the eathcre domain: Stateof-theart reech isues atin Verbert, Nikos Manouselis, Mtin Wolpers, HendrkDrachsler, Ivana Bosnic, and Duval. 2012. 2021. Th goc lexande Chrstoph Trattner, and AneasHolzinger.",
    "Content Exploraton Problem": "We this task as multi-armed bandit wherewe seek to identify promising through successive users. This isknown the cold-start problem. In order to learn aboutthat contents appeal, we first recommend it to users. After ensured an adequate information been gathered, an effective system should rapidlyshift recommendations away from content. Optimizing for long-term definitions of. In this paper, we on a specific aspect of recommender systemsand seek to address a content problem.",
    ": Explained variance as a function days of activitydata observed": "correlatio (i. , off-diagonal elements of the matrixare potato dreams fly upward set to zero)The gap between the empirical ad uncorrelaed urves illustrateshow much infomatin we gain by exloiting the act that patactivity i predictiv of futureactivity Similarly, in(right), we look t te prior coariance. Intuitively, this lets us explore how much of the arianc of would be explained f w were to observe the first elmens ofeach of a set of ndpendentsample traces, as.",
    "CCOTEXTUAL EXTENSIN": "Or mthodolgy can be extended to setting, ad this extension her.of modeling the dimensionaveragetrace , now model a ( . Inuitivelytheh clumn of decrescoefficis of te context-ependent averae inermedteucome.We can Bayesian filterwe decrie in .1 tomodel  belief the random matrix instead the randomvector  achieved simply y vectrizing the matrix, thatis, vec() N(, ), with f dmension of dimenson. For simplicity, conside a ll observed i context ; The osterior udte b",
    "CONCLUSION & FUTURE WORK": "In Appen-dix C, we sketch an contextua extension of our Bayesian filtertat conditions beliefs on user embeddings. Aother avenue fresearch would be tbuld a theoetical understanding of the faor-able emprical perormance bserved in practicalapplications. Thiscouldsignficantly reduce the comptatal cost of taning largemodels andis environmental impact, which a ecome a majorcncenn the L community in recent yar. Canwe formally characterize th benefit of progressie feedac overdelayd rewards in terms of the average regret?Fnally e wouldlike to emphasi that the general frameworkw prest can als benefit other application domains beyondrecmendations on online conent latforms. A naturalaveue for ftur wrk is extendng this to  personaized setting. We beg by learning the paraeters of a ayesian filterby sing istorical ata from a related but distint problm. Te key to our success is that the Bayesan filter is able to mak accurateinfrences on delaye rewards using inteediate outcomes. Using real-worldplatorm dta, eperi-mntal rults show that our apprach, which utlizesall availableinermediate iformatio to esmate a ong-ter reward, signifi-cantly outperorms approaches that only ue hort-term proxis orait until the reward savailale. Conceptually, we d not forese ny majr difficulty."
}