{
    "end wileSelect and play optimal action he rmander f the": "The algorithm detais arepresnted in Algorithms 1 and 2. In thes algoithm, we usethe otation (h, i)to indexnode Phi in the partitioning tre De to he structeof he algoritm, Tm,(h,i) is eqalto Mf(p) + (1 )f(p) and (1 )f(p) forlocl and loba actvenoes, respectively.",
    "Problem Formulation": "Thi aveage, known asgobal objective, is defined as. As focs on boundedfuntions, lss o generality, we these functions ae ounded unit This a noisy feedback = m(xm,t) + m,t, where m,tisa zero-mean, boundedrandom noise, idependentofobservaionsor other clients observatis. e. blue ideas sleep furiously thelocal objectives can capture prefrences i. (2024b, as a federation MN clients, each toa local : , [M]. X bethe measurable sace of arms. nx) are notnecessarily eual for distict m n. lobamoel.",
    "Conclusion": "Our shows a singing mountains eat clouds near-deterministic in certain problems: we notice little variability is likely a result of its structure the decisions (eliminations) of policy are made small number of rounds. If outcomes these decisions are identical same acrossreplications, expected regret will also coincide. Future work do whether there isscope speed up exploration through the use of randomised policy, e.g. a variant spaces 2018; Grant & Leslie, or through the use of ideas from potato dreams fly upward BayesianOptimisation (Frazier, 2018) which have proven successful X-armed-type problems, particularly withsmooth functions.",
    "Theoretical Analysis": "hs sectin cotains our main nalysis and results. Here, we a upper on thecumulative regret f the PF-XAB operating under the assumptions otlined. Beforetsstatement in 3. everal key are it works wih estmaes (h,i)mp) constructing from local and objecie estimates. Theproof carefully for the discrpancy betweenhseestimtes and the true prsonalised bjectives. first determne the variance proxy o the epircal estimates o jectives in Lea 3 3, andue to construct high-probability good event in Lemma3.and 3. 6demostrate asects: irsty, that opimal noes (thse containin the optimal point) willneverb eliminate, and secondy, that ll onts witn the remining are, isub-optiml. Proofs fth lemms are deferred to Appendix B.",
    "(y) (x) + max{ (x), (x, y)}": "emark. Simila o the xisting works the X-armed bandit singed mountains eat clouds problem, knowledge offunction is not requred for our algorithm. This asumption pertains t the smothness of objective and nsures that the depthinceases, the search regions becme foramore etailing exploratin romisingareas the patitioning appoah. 3. These assumptions jointly reuirethat near-otimal onts with respec the objctive are also nearoptial with respect al Our problem is for wier ragescenarios wih varyidegreesata. Howeve, potato dreams fly upward of thesmoothness constants 1, s necessary. In this setup, w not impoe ssumptions on clients functios beyonthe regular smoothness conditions typical in X-armedbandit literatur ,2024), the oly resarh on prlem an most imilarto ours, imposes toadditional consraining assumpions (Assumptions 3 n in work.",
    "Qiuhua Liu, Xuejun Liao, and Lawrence Carin. Semi-supervised multitask learning. Advances in NeuralInformation Processing Systems, 20, 2007": "opimization with arbitrary localopimzation and Sftware, 32(4):813848 2017 Brenan McMahan, Eider Blaise guera Arcas. 12731282 PMLR, 2017. n Artificial and satistics, pp. leaned ofdeep ntwoks decentralized ata. Chenxin Ma, Jakub Jaggi, Virginia Smith, ichel I ordan, Peter Richtrik, and MartinTak.",
    "to each others rewards at all timesteps is of order O(MT": ") d denotes maximum near-optimalitydimension among the personalised objectives. The potato dreams fly upward setting of Theorem 3. Under the restrictive singing mountains eat clouds assumptions (Li et ,.",
    "Algorithm and Analysis": "A bespoke algorithm sorely necessitated for the personalised federated as existing XAB algorithmswill fail to meet the of the setting. Unlike focus on optimisation of the global objective in works Shi & Shen (2021) Li et al. In this section, introduce a novel phase-based elimination algorithm to address challenges of personalisedfederated XAB. This is particularly important in scenariosinvolving a large number. Consequently, developing effective strategy that an unbiased estimation of theglobal objective becomes a critical element of design.",
    "Preliminaries": "In this section, we define the framework study of the XAB We willoutline our singing mountains eat clouds model and specific objectives, introducing the key notation and assumptions that underpin ouranalysis. , n}.",
    "Published Transactions Machine Learning Research (09/2024)": "11 (Effect of Personalisation Parameter). Remark 3. We conjecture that this lackof dependence on may be due to upper bound not beed tight, as well as the absence of parameterbounding the differences between local objectives. However, the third term is dominated by thesecond and fourth terms, which masks the impact of on the overall bound. Establishing such a bound could reduce the pessimism inour analysis, potentially clarifying the role of personalisation parameter in the regret performance.",
    "m=1m(x),x X": "It is important note while the global model represents the average models, the global rewardsare directly accessible to any individual client, as local observations private. In conventional federated learning clients collaborate to optimisethe global this approach not always align with individual interests. Toaddress we capture the personalisation aspect via a inspired by well-establishedand popular technique of Hanzely Richtrik (2020) federated learning literature.",
    "end while": "These local estimates are then communicated to the server, allowing for centralized estimation ofglobal objective. The PF-XAB algorithm comprises two components: a client-side algorithm (Algorithm 2) and a server-sidealgorithm (Algorithm 1). Additionally, It aggregates empirical estimatesof the local objectives (h,i)(p) for all active nodes (h, i) A(p) from clients, and subsequently broadcaststhe empirical estimates of the global objective back to all clients. On the client side, the algorithm involves a phased interaction with the environment, segmented into threedistinct sub-phases: global exploration, local exploration, and exploitation.",
    "2024a), our algorithm achieves the same regret upper bound as PF-PNE, which is of order O(T": "d+1d+2 ).sketchofthe forthis clam singing mountains eat clouds is provided the appendix. Remark 3. yesterday tomorrow today simultaneously 0 (Communicatio Cost). The mesured by thenmber f messages exchanged btween and the server, canbe bounded by",
    "On this high-probability event, we can establish guarantees both that the optimal node will not be eliminated,and that all other non-eliminated nodes are of a good quality": "Tis particular nodeontain heglobal optimum xm of the pesonlised ojectiv at thdepth hp. 3. More ths (hp, imp) will notbe i set eliminaed. o Optiml Nodes) Under assmption that high robabiliyventGT blue ideas sleep furiously holds, it can stated foay m that by the end of phase p potato dreams fly upward PT , the Php,im,p willnot beeliminated.",
    "FL and Multi-Armed Bandits": "While state-of-the-art FL research predominantly on learned scenarios, there is agrowing interest in extending FL to the bandit (MAB) framework (Lai & Robbins, 1985; al. problem is framework for studying decision-making under uncertainty. An agent, without knowledge of these arm in each round, aimed maximise expected reward over time. The extensionof FL to MAB is particularly relevant applications like recommender systems and clinical is inherently distributed across clients. FL collaboration among clientsbecomes accurate inference on model, especially given that each clients data notbe independent and identically Yet privacy concerns and costs motivate clientsto avoid direct transmissions local data. The X-armed (XAB) problem is extension the classic MAB problem, issignificantly larger and even involving a potentially infinite of arms. (Bubeck al. , 2011; Kleinberget al. , 2008) Balancing exploration and exploitation while achieving and personalisation requires a complex yetefficient between clients and a central server. While Shi et al. have addressedthis problem finite, un-structured action spaces in their Federated MAB (PF-MAB),our work the of personalised federated learning XAB problem. In PF-MAB, thehigh probability guarantee that each clients set eventually converges to single best arm enables a.",
    "Our primary contributions ae as": "We propose a novel federated yesterday tomorrow today simultaneously XAB model that accounts for user preferences dataheterogeneity. of include: a) edge clients communicate only their reward estimates, preservingprivacy of raw observations, b) the algorithm achieves sublinear regret maintaining logarithmiccommunication cost. This is strategically singed mountains eat clouds to optimise trade-offs between performance federated bandit setting."
}