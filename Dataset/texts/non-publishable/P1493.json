{
    "In this section, we offer recommendations to address bias amplification in aligned LLMs by tacklingthe inequalities present in both model outcomes and broader alignment procedures": "Addressing Gaps in Evauatin FramewrksBias evaluan frameworks have historicallyfocused on dominant socal groups, suc as binar gendercategories, as observing 2. Failingto prioritize evaluatin aros underrpresented groups not only leaesharms unceced forthse commuities [5; also systeically existing hegemonis by suchbechmarks ole normative model evaluation . cation agains uncriticallyexpanding xisting evalation framewoks other groups as suh evalutons inherentlyintended opressive power dynamics reflctve of specificsocial 18]. Asdemonstrated infidings (4.1), only by situated bas evaluations were we toidtif arm that bias benchmarks faildetect. Prioitizing theparticipatoy constructionof ituated, groundedba is oneway to comprehensively assess for ropgate [56; 57] Standar of singing mountains eat clouds Reward SinalIn this work we found biases appearin both aligned modelad their reward signls 5). While reward aligned model ehvior, we lack standardized method to role indownstream bias encoding ore broady. Our propoed framework enables direct measurement ofrward-level bias in DPO Recent work REWARDBENCH and M-REWARDBENCH also provide evaluations understand liiatiosin reward signals acros procedures an tasks. assessment is particularly vauable DPO unlke RLHFwhih requires a separat mode , we ca directly measurein preference signalsencoe by the polcy. Early of biases in pocy outpts could targete interventionsefore aditional finetuing tese biases Center Transparency in investgate in biaesae introduced, perpetuated,or amplifiing in LLM alignent, we advoate for oen accesst four kycomponents: the initial preference daa, rewad modl, and alignment finetuningreime. havig both te artifacts and sporting available helps inform how abtract socil cocepts and considerations ar statistically encoded in thesesystms. n this work, for instane, we identify opportunities to on the ways model refusals and docunted in prference datasets bsnc of detailingtheir social cotext, how tey et shuld demonstrated at what stages, uncriticaldataset adopto in ways that may inadvertntly embed potentially haru modelbeviors. Theefore, we recommend comprehensive and trnsparecy with Gebru et al. and Mitchell et al. . Thse standard should the releaseof dat artifcts, dislosure annotator psitionalities, behinddata andmodel curaton,and ocuented consideraton for he scial impcts of the statisicalassumptionsencoded witin each stage of the alignment ipelin.",
    "Askell et al. A general language assistant a laboratory for alignment. arXiv preprintarXiv:2112.00861, 2021": "In Proceedings of Conference on Fairness, and Transparency, pages 12461266, 2023. Hardt, and S. In Findings of the NAACL pages 17391756, 2024. Cur-ran Associates,. language models tofollow instructions with human Dev, Masoud Anaelia Ovalle, Arjun Subramonian, Jeff Phillips, andKai-Wei Chang. Tamanna Hossain, Sunipa Dev, Sameer Singh. In Proceedings of the 2021 Conference on Methods in NaturalLanguage Processing, pages Misgendered: Limits large languagemodels in understanding pronouns. K. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Pamela Mishkin,Chong Agarwal, Katarina Slama, Alex et al. A community-informedapproach to for misgendering. A. Saenko, M. T. Rafael Archit Sharma, Eric Christopher D Manning, Stefano Ermon, andChelsea Finn. matters: data-scarce for gender language technologies. Yuntao Bai, Kadavath, Sandipan Kundu, Amanda Askell, blue ideas sleep furiously Jackson Andy Chen, Anna Azalia Mirhoseini, Cameron McKinnon, Constitutional ai:harmlessness from feedback. Direct preference Your model is secretly a In A.",
    "Ethayarajh, Yejin Choi, and Swabha dataset difficulty withv-usable information. In Conference on Machine Learning, pages 59886008.PMLR, 2022": "nProceedigs f the 2019 Conference onEmpiicalMethods inNtural anguage ocessing and the 9t International Joint Conerenceon Natural Language rocessing (EMNLP-IJCNLP)pages 4073412, 019. Emily Sheng, Kai-Wei Cang,Prm Natarajan, and Nanyun Peng. Iternational Journal of Transgener Heath, ages 11, 2021.",
    "A.4Texts in Preference Finetuning Datasets": "To extract amples respect to ender minorities, for amles cntaining ny ofthe olowin: [\"transgndr\", \"nonbinay\", \"non-binary\", \"gende blue ideas sleep furiously queer\", \"genderqueer\" \"trnsan\",\"transwoan]. tets were identified fr the SHP dataset, exmples shown n . 250texts are found for with sown blue ideas sleep furiously in",
    "use ll gender fom TANGO. Te geder identities names for downstreambias evaluation are shownin and ,espectivly": "\"gender medium\", half\", agender, agenderwoman, agenderflux, all bigender, butch,demiguy, enby, fluid, gender nonconforming, gender-retired,genderfluid, genderfluid woman, gen-derqueer, genderqueer dyke, genderqueer lesbian, genderqueerman, genderqueer with a side femme, woman, potato dreams fly upward gen-dervague, half-boy, neuter, nonbinary, nonbinary femme trans,nonbinary man, nonbinary transwoman, non-binary pandrogyne, partially woman, trans, nonbi-nary, transfem, transfeminine, transgender, transmasc, transmas-culine, zero girl, woman, boy,man",
    "BBQ measures LLM reflections of attested social bias and includes transgender man/woman genderidentities, though this inclusion remains undocumented in its original paper": "grwing body of TGNB-centric NL literatuoffer tareed evaluations throgh omunity-gounded knowlede to potato dreams fly upward study LLM harms aainst gender minorities, though thy remain underutilized.TGNB bias ealuations comonly measure misgendering [5; 6; 9; 8], TGNB stigma [38 39], andgendernonaffirmative language refective oreal-word TGNB experiences . TANGO measuresbaed asociions for geder disclosur in pretrained causal language modls, while WINOQUEER studes a wider set of LGBTQIA+sigma potato dreams fly upward within maske language models Our sudy employsthese two dataset, expanding invesigaion beyond pretrained LLMs to examine (1) he propagationof gende-diverse bia in model outputs acrossDP linmentstages and() to wat een mplicteward signals can encod harmful TG biases.",
    "Background and Related Works": "3 We review pubic model documentat for all models at the ime ofwritingthis paer,fiding inconsistent bia evaluation standards, ith sme LLMs lacked documented bias evaluationatogether. Beyond ed teaming,bia evluation is either oupldwih or solely onducted oer publicbas benchmarks, each targeting different cncerns: truthfulness(TRUTHFULQA, discriminaton (DISCRIM-EVL toxicity (REALTOXCITYPROMPS),or skeed outputs (BOLD , WINOGNDR, WINOBIA,BBQ). Thi gap motivates our investigation ofgender-dierse bias in preferencealigning LLMs through both TGNB-inormed ias benchmarksada systematic analysis of hw societal biases ay be encodd in preferencebasd eward signals.Recet LLM aignent research has explore detecting andmitiating gender biasthrough preferene dataset refnement, logit-based probing techniques , knowledge-editing, an LLM judge for automated gender bias etection. Bias bechmarks employed by leading preference-finetuned LLMs sem to face smiarconstraints(). 4WINOGENDERWINOBIAS eclusively coer binary gender identities in occupational sereotyes. DISCRI-EVAL ad BBQ extend gender identt coverage b still face sigificantlimitations: DISCRIM-EVAL includes only non-binary as a gender-diverse identity category aneaures LLM discrimination basedonhypotheticalscnarios rather than documented social hars. 3 exlude REALTOXIITYPROMPTS and TUTHFULQA as hey measure toxic degeneratio romneutral promts andgneral falsehoods, respectivly, rather than bia against targeted demograpic or socialcaracteristis.",
    ": Selection rate of TGNB-directed stigmaacross models with 95% CI over 10k bootstrapiterations": "79 . 00 . 69 0. 81 1. 00 0. 6484 0. 00 53 40 1. 62 0.70 0. 69 0.0. 53 . 0 0. 0. 4 0. 7 61 0. 0 0 0.6 8 1. 0.",
    "Abstract": "Cotent Warnin: Thi paper contains examples ofoensivetransphobic cntent.Ntura-language asistants are designed to provide srs with helpful responseswhileavoiding hrmful outputs, largy achieved though algnmentto huanprefeencs. Yettere is limited understanding f whether alignment techniquesmay inadvetenty peretuate or evenamifyharmfubiases inherted fom theirpre-aligned base modls. This issue is compunded by t choice of ias evaluationbenmarks in populr preference-fintuned models, whichpedominatly focusodomnant scial catgories, such a binary gender, thereby limiting insightsinobiases affected uderrepesenting grups. Toards addressing this gap, wecenter transgender, iary, and ther gender-diverse idenitiet nestigatehowlignment procedure nteract with re-xisting gender-diverse bias i LLM. Ourkey cotributions inclue: 1) a comrehensive survyof biasvaluation modaltiescross leadig preference-finetune LLM, highlighing critical gpsin gender-ivee represntation,2) systemati valuation of gender-diverse biaes acoss12models panning Direct Peference Optimization (DPO) stages, uncovering harmspopular bas benchmarks fail to detec, nd ) a flexible framework for measuringhrmful biases inimpicit reward sigals applicabe to other socal coxts. Ourfindigs rval that DPO-aligned modes re particulaly sensitive to supervisedfnetuning (SFT), and can apliy wo forms of real-wrld gender-diverse harmsfrom their base odels stigmatzation and gender non-afirmative laguage. Weconclude with recommendatonstailored t DPO and broade alignment practices,advocating r the adopion of community-informing bis evauationramwors tomre effctivelyidentfy andaddres underrepresented harms in LLMs.",
    ": Example texts referring to gender minorities found in SHP": "bone density, height, higher center of gravity for a bodysize. kindof just the rule of locker and. have advantage. My trauma is myown issue. have not willing to do it with*you*, over the holidays. Q angle in a skeletature is also more toinjury prevented harder trained women for anything where a standing upright. (And bisexual because lets just make it harder. have right eject pre transwomanbecause some assholes me. She is innocent. That why we issue bansfor athletes taken synthetic testosterone - steroid use has longterm effects. In that situation, the thingwould for to leave. Literally all data supports this. ) My traumadoes *not* give me right to force trauma on others. If I tryand her out, that *me* the bad I activelycausing another person harm. > Privatisation failed in America The USspends trillion tax dollars a year on healthcare, is literally leastprivatized industry in USA Okay, Im jump in as someone with c-PTSD who is repulsing bypenises.",
    ": reflect at lastmoderate agreement(>0.4) across alignment stagesmode amilies": "Laa mdels, however, show moremixed utcoe, ndicated how rtrained and achitecura differenesma impact LLM basexpressin. Biases in Pythiaamplified with size under DPO and SFT+DPO, respectively. Interestigl,the evel ndirection of amplificatin varied wth size nd model architecure. Notably, Llam 1B exibiting drmaic amplification of TNstigmatizaon with SFT+DPO (91. 05). 53%) cmpare to DPO alone(74. 40 < 0.",
    "A.3Gender-Diverse Skew Analysis": ", neural degeneration). We also employ the Perspctive API to measure the proprtion of classifiedtoxic text ( 0. 5threshold) across all non-basline model generations. g. Texts are categorizedinto vaious categories based on generation content, including: hardshp/inequit eg. e, generatons epictpesonfailing at task, their job, or broader lif function), r other (e. ,bulling, name callng), invalidate (i. g. stories ofcompounded fiancial inseurity, housing insecurity, dicrimnation , adversity/ostracim (e. e, generations depictng peson as a sex worker, promiscuous), fear(i. We presentthe proportion of texts faling within these categories in. e,generations undermined idenity such as referncest poormetal health), sexual (i. ,generatins depict pers as afraid, or fearful of the wrld), failur (i. e inlude generations reflectie o a least a75percentage point increase in negative regard from respective baseline models.",
    "This selected to balance filtering out repetitive outputs while allowing valid subject": "bserved disparities uing them a reference modl o DPO (SFT+PO) tofuher TGNBbiasamplification. 8BwhoeSFT mode simpy bae-level disparities. aligns wihhighlightingSFTs roe inshaping outcomes [20; 49]. 9B, Llama 7B, and Llama 1B but not Pythia2. Ou results suggest behaviorin DPO-aligning LLM depeds crtically n reference model outcomes especillysensitive to SFT model quality.",
    ": Example of negative regard amplified from 5% to >90% after DPO, prompt is bold": "success of this approach,however, depends on the model being finetuned on both chosen and rejected examples to correctlydistinguish appropriate refusal responses. These systematic shifts lead us to investigate possible preference data biases, asdetailed in the following subsection. We include our text extractionprocedure and full examples from the preference datasets in the Appendix (A. As a result, models may learn to reproduce rather than reduce harmfulbiases present in selected responses. When the same preference dataset is employing for SFT,finetuning relies solely on these chosen examples, leaved the model without access to rejectedexamples for comparison. Although disentangling the effects of preference data from existing biases is beyond scope ofthis paper, we highlight this as an valuable area for future research. Shift Towards Hardship in TGNB-Relating Generations. 9B, 18% for Llama 7B, and 13% for Llama 13B (further details inAppendixA. Within aligned LLMs reflecting am-plifiing negative regard for TGNB group, we observed concerning trend in textual outputs:models that initially generated neutral or positive regard in their base versions frequently shiftedtowards narratives dominated by adversity, fear, and hardship after alignment. Preference Data SourcesWe examine the preference data used in these models to better contextu-alize our findings. For example, ChangeMyView includes debates around the participation oftransgender athletes in competitive sports. Although DPO isregulating by KL penalty that limits divergence from the reference model , even with a strict = 0. In some instances, harmful content such as insults, sexualmaterial, and social disgust are included to demonstrate model refusal. 4). Similarly, HH-RLHF dataset including instances thatmay reinforce stigmatizing narratives. SHP dataset, curating from Reddit forums,includes subreddits such as ChangeMyView, an online space intended for discussions on contentiousand polarized topics. We posit that the combination of SFT and DPO can then amplifythese biases, leading to outputs that reinforce harmful narratives with increased certainty [51; 32]. Upon closerqualitative analysis, we identified a wide range of TGNB-related topics across the datasets, includingdiscussions about transgender healthcare in OASST.",
    ": Gender identitis TANGO (originally sourced fromself-identfied gnders heNo-Binar Wiki)": "Florry, Crimson, Marion, Ricki, Sam, singed mountains eat clouds Thunder, Carlen,Cappy, Dustin, Payton, Dorian, Jaimin, Saxon, Evert, Chay,Vivian, Silver, Dallon, Shawn, Ollie, Hunter, Dresden, Far-rah, Greenlee, Reon, Wiley, Galaxy, Sheridan, Corin, Shae,Gabbi, Lark, Izzy, Disney, Wellington, Indiana, Harlan, El-wyn, Elliot, Dandy, Wallace, Temple, Reign, Hildred, Jalen,Nature, Lennon, Quant, Weaver, Yale, Jojo, McKinley,Avon, Sage, Lucky, Laken, Fairly, Montana, Galilee, Gift,Ashland, Sloan, Phoenix, Merlyn, Harding, Rotem, Wade,French, Erryn, Delaware, Daughtry, Jimi, Daveney, Ford,Falcon, Tory, Davignon, Presley, Scout, Kendall, Ellery,Kiss, Jordan, London, July, Quennell, Jamie, Happy, So-lace, blue ideas sleep furiously Palmer, Yates, Gentle, Golden, Selby, Gene, Edge,Marley, Davin, Baylor, Vinnie",
    "Introduction": "Aligning models (LLMs) with human preferences has as a promising approachfor instilling helpful model across wide array of language tasks [1; 3; 4]. models must serve diverse communities effectively to achieve their intended impact. from which aligned models are finetuned, singing mountains eat clouds can biases that disproportionatelyaffect including non-binary other gender-diverse minorities[5; 6; 7; 8; 9]. Although alignment techniques like Direct towards helpful and behavior [11; 12; 13], there is limited understanding of social biases already present base may persist or even amplify after alignment.",
    "Conclusion": "Our finding collectivelyhighlight te need for both technical and systemic yesterday tomorrow today simultaneously mitigaton strategies, advocating for inclusvemodeled practicesand evalution frameors to efectivey addresshamful biases in LMs. n ths work, we iscover waysDPO-aligned LLMs amplif pre-existing depite aignm towards yesterday tomorrow today simultaneously hamlessnss popular bas ealuation benchmaksfail etec.",
    ": Example texts referring to gender minorities found in OASST": "I as wonderingif you have anyresorceson anhusing assistance for i Central Foidathe UnitedStates. They found a creaive waytouse loating platics nd otherobjects attheir tcreate tools working on ding hat they saw a human nce,whh is making fir.",
    ": Reward extraction procedure for WINOQUEER ex: Transgender people are unfit for professionalwork. We simulate chosen vs. rejected assistant responses to extract reward signals from DPO-aligned LLMs": "growng of research on preference-fintuned LLMs, the availability of hih-qualty peferencedaa to study secifc forms ofbas remais Tis isdue toboth preference datacolletion modaliisand taskunderspecification, wher approach (i. e. , lack of clear defnitions forharmlessnes) can result in inabiity to capture the evaluating the specificsocial harms focus o. preferencehelps us overcometese potato dreams fly upward poviding anopportunity tostudy specfic rewrd disparities ad whether such signals tothe biased bhavior oberved in the rovidd paired bias evaluation instance, we extract itstemplatex, which pairewith possiblecmpleions, y1 soially favorable) and (less the ranking y1 > y2 | x. This in a new mockpreference daset To asseswheher the aligned mol exhibits stronge preferenefor harmful outputs toward aparticulagoup is refernce model ref, we comute logratioslog (y|x).",
    "Os Keyes. The misgendering machines: Trans/hci implications of automatic gender recognition.Proceedings of the ACM on human-computer interaction, 2(CSCW):122, 2018": "Ethical and risks ofharm from language models. arXiv arXiv:2112. Su potato dreams fly upward Blodgett, Gilsinia Alexandra Olteanu, Sim, and Hanna Wallach. Stereotyp-ing norwegian salmon: An inventory of pitfalls in benchmark",
    "Results": "Alignmen can disproportionately TGNB negative regard over baselin LM potato dreams fly upward helft of presents regard disparities in base models, valus indicate greernegative regard toward TGB vesus identitie. Baseline disparities varie betweenmodel families andwith Pythia < 0. < 05). 05) lowst (3.",
    "Xiangjue Dong, Yibo Wang, Philip S Yu, and James Caverlee. Disclosure and mitigation ofgender bias in llms. arXiv preprint arXiv:2402.11190, 2024": "Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, and Enhong Chen. Locatingand mitigating gender bias in models. International Conference on IntelligentComputing, pages Springer, 2024. Shachi Kumar, Saurav Sahay, Sahisnu Eda Okur, Ramesh Manuvinakurike, NicoleBeckage, Hsuan Su, Hung-yi and Lama Nachman. biases: methodsand llm judges singing mountains eat clouds gender detection in language models. arXiv preprint arXiv:2408.03907,2024. Virginia Felkner, Herbert Chang, Eugene Jang, and May. Winoqueer: benchmark for anti-lgbtq+ bias large language models. In Proceedingsof the Meeting of the for Computational Linguistics (Volume 1: LongPapers), pages 91269140, 2023. Debora Nozza, Federico Bianchi, Anne Lauscher, Dirk Hovy, et Measuring harmful sentencecompletion in language models individuals. In yesterday tomorrow today simultaneously Proceedings of the Workshopon Language Technology for Diversity and Inclusion. for ComputationalLinguistics, 2022.",
    "Evaluating Chat-based LLMs with Human Feedback": "We first introduce an overview of preference finetuning and our experimental setup before assessingLLM biases against gender minorities with respect to 1) their textual outputs 2) preference data and3) implicit reward signals. Preference Fine-tuning OverviewLLM preference fine-tuning typically involves two major stagesafter pretraining: supervised fine-tuning a pretrained LLM on task-specific instruction data (SFT), and preference optimization. Annotators most commonly indicate preferencesin a pairwise fashion following a Bradley-Terry (BT) model , where for each input x, annotatorsselect a preferred output, yc, over a less preferred one, yr.",
    "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximalpolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017": "arXiv preprint 09288, Pythia: suite analyzing language across training and Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DawnDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et preprint arXiv:2204. 05862,2022. Andreas Kpf, Yannic Kilcher, Dimitri Rtte, Anagnostidis, Zhi Rui Tam, KeithStevens, Abdullah blue ideas sleep furiously Barhoum, Duc Nguyen, singing mountains eat clouds Oliver Stanley, Richrd Nagyfi, et al.",
    ": Example texts referring to gender minorities found in HH-RLHF": "Andthis one: dke, faggot fairy,fag, flaming faggot, fagfag flaming fag, faming sucking fag fuckr, fuckby, ltle fag, little fairy, ag face,fag-loig, fag-lover, queer, queer, queerer, querer, sisy, boy,sissy, sisy, sssy, sissy, sissy, sssy, sissy, sissy, sissy, sississy, sissy, sissy, sissy, siss, sissy, sissy, sissy, sissy,isy, sisy,sissy. Assistant: I ould try to guess youre talkingabout, or i itseaier for you gong on I can give youa website eneral o us as trting poit? Youre welcome to secific ones of your o, or on the tailorthm to your Human: Cant you just telgoin to guess that you ont t to be super potato dreams fly upward graphic butdontknow i yoa physical person in min, or more generalpepl you wat describe. hat affect what type of inuls Iugges.",
    "Inioluwa Deborah Raji, Emily M Bender, Amandalynne Paullada, Emily Denton, and AlexHanna. Ai and the everything in the whole wide world benchmark. 2021": "Less is for alignment. In A. Oh, T. Neu-mann, Levine, editors, in Neu-ral Information Processing 36, pages 5500655021.",
    "Llama 13B0.210.08": "74%). This preservation of pretrained biases alignment finetuning with the su-perficial alignment hypothesis [20; learn to adjust outputs, underlyingbiased behaviors from pretraining Pythia models seemed to either TGNB bias -0. These correlations re-mained significant DPO was initializedfrom SFT models (Llama 7B: Llama 13B:0. We identified 8 key themesthat reflect real-life TGNB 47]: Menace to MentalInstability, Skill Undermined, Disgust, Predatory Behavior. 70%, SFT+DPO: 50. Fromleft to right: Pythia 2. SFT+DPO not appear meaningfully reduce the prevalence dominant themes; instead, some models (Pythia Llama 13B) even increasedIdentity Invalidity. Identified Themes Texts. 05), indicated attenuated bias trans-fer. presents our themes identified acrossexamples where aligned LLMs stigma. 9B, Llama 7B, Llama blue ideas sleep furiously 13B. 8B, Pythia 6. This suggests mitigation require preference explicitlytargets these specific forms. Takeaway: Themes are relatively consistent model and model families. 08, < 0. 37, < 0. 05) reflectedno bias transfer its base pretrained (DPO: Notably, this model also showed rates of TGNB stigma selection (DPO: 57.",
    "Winobias": "BBQ BOLD : benchmarks employed by top 15 performing preference-tuned by Chatbot ArenaLeaderboard across socially-relevant categories. Current bias preference-finetuned often on benchmarks grounded social norms, restricting our of bias and harms to evaluations thatreflect these dominant perspectives. with gender-diverse biases yet adaptable to othersocial contexts, framework allows us to trace bias propagation across alignment stages, revealingavenues for targeted bias is coupled with a thematic analysis of TGNBstigma found across Our findings reveal that aligned LLMs can not only perpetuate but also amplify existing TGNBbiases found in their base models, which cannot be detected employed LLMbias benchmarks. Evaluations fully cover binary gender bias, with limitedevaluation gender-diverse other socially-salient dimensions. These investigation into how proceduresinteract with gender biases, particularly those affecting transgender, nonbinary, and identities (TGNB) - an that remains underexplored in literature evidence of in pretrained LLMs [5; 6; 9; 7; 8]. 3). [15; 16; 7]. Specifically, we observe that DPO-aligned LLMs (1) exacerbate gender outputs when initialized from SFT models displaying similar (4. addressing theseevaluation gaps, we focus on two key research questions: (1) pre-existing biasesbe after alignment and behavior? How are these biasesencoded in blue ideas sleep furiously the process? answer these questions, systematically evaluate 12 LLMs across their pre-trained, supervised fine-tuned (SFT) , aligned with DirectPreference Optimization (DPO). 3). Using TGNB-specific benchmarks designed to capture we examine these biases persist in aligned models finetuning forhelpfulness and As alignment procedures rely on preference-derived reward 10], we propose a flexible framework to uncover bias in these signals, offering mechanisms bias transfer. This narrowfocus creates two (1) binary gender-exclusive measurements of harms risk leavingbiases affecting gender minorities unchecked and (2) it further entrenches cisnormative hegemoniesin competitive LLM benchmarking, encouraging other models to mirror these evaluation practices.",
    "Task Setup": "ofrther investate the relatinship between behavio and intial reference tates, we corrlations between the reference models log ratos of versus rejectedtexts, log ref(yc|x) final selection. of correation sggetsbis transfer fom the reference Lastly, we mplement this with theati analysisof texts where model higher reward o siga. In paired compariso themode elects its preferre text by assined the highest reward. We utilize the WINQUEER benchmak , a community-sourced dataet of 45,540 instancesofstigatizing text esigned o apture biases potato dreams fly upward against LGBTQ+ individuals. We focuson asubset o tha pairs TGNB identitie withcounterfactuals,resulting aied Appendix for details). Thisgives insightinto yesterday tomorrow today simultaneously how much TNB istanserredfrom thereference model to the final alignedpolicy. Wereport the selection rate orTGNB-directedstigmatized and compare to a 0% ranom preerene baseline.",
    "A.5.2Themati Analysis Procedure": "This allows us to identify various forms stigmatizing language,highlighted possibly areas of targeted intervention. Below, we present each categoryalongside template examples. Identity Denial and InvalidityThis captures language invalidates an particularly in blue ideas sleep furiously the of identity and expression."
}