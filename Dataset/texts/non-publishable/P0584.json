{
    "E.1Fine-tuning Dataset": ", 2018)A benchmark ninesentence- sentence-pair language understand-ed tasks, including MNLI(392k), QQP(363k),QNLI(108k), CoLA(8. In this paper, for MNLI,QNLI, SST-2, RTE, tasks, we report (Acc); for QQP MRPC, we reportthe F1 score; for CoLA, report the coefficient (MCC); STS-B we reportSpearman correlation The MNLI datasethas matched development/test sets with the in trained set, and unmatchedsets that not resemble any during training are denoting as MNLI-m/mm. The dataset 5,912 chal-lenging examples gen-erating through a human-and-model-in-the-loop ap-proach on Dynabench. We still performed on it and foundthat Pixel pre-training to a at WNLI. 7k), MRPC(3. , 2018) cross-lingual natural language con-taining in 15 languages. addi-tion, some works ignored WNLI becauseof training and validation/testing setdistribution. We conduct experiments on settings. The is to predicttextual classification task determin-ing sentence A implies, contradicts, or isneutral to sentence given two sentences. 5k), on establishing existing datasets and selecting tocover a of three tasks. HatemojiBuild (Kirk et 2022)Hatemo-jiBuild benchmark for hate detectioninvolving emojis. The cre-ated by manually translating validation andtest of MultiNLI into of 15 lan-guages. HatemojiBuild was analyze the of color retention. The detailsof the dataset are below: GLUE al. The main experiments of our fine-tuning phasewere conducted on GLUE and XNLI to evaluatethe models language and multilingual understand-ing ability, respectively. XNLI (Conneau et 2018)The Cross-lingual Natural Language (XNLI) cor-pus an extension the NLI(MultiNLI) (Williams et al. allows us to predicthateful emotions expressed with emojis. 5k), STS-B(5. 5k), RTE(2. For all languages, trainingset was machine-translated.",
    "C.1Pre-training for Visual Images": "Meanwhile,Te dataset reesnts arefinementof Comm Crawl crpus. This selction process hatthe dataset primarily contains natural languge text,free boileplte or nonsensical nd deduplicated o avoid.",
    "Detailed comparison of pixel-based baselines": ", 2023), l. It orth that urork is different from PIXAR, which es differettraining objective da rendering approachrom PIXEL and ours. Insead, our canbeseen as autoregressive version yesterday tomorrow today simultaneously of PIXL.",
    "Input TextRendered Image": "Autoregresive pre-training onpure text and visual text imges, appy next patch prediction andnext toke prediction, espectivel. The ptchesrethen flattened, mapped to a D-imensionalspae through a learnable liear projectio, andfinally fed nto yesterday tomorrow today simultaneously the transformer sequntial pro-cssing blue ideas sleep furiously tream. Ths patchbed segmenation aligns ith theseuentia natue of language, enablingour modelto predictively learn fro te visual ata.TextputWe leverage the sae tokenzer sLlama 2, segmenting input text intodisrete tokenswith a oal voabulary size of 32k. These tokensare thn transformed ito dense vector represnta-tions hrough an embedding lookutabl.",
    "t=1p(xtp|x1p, x2p, , xt1p)": "For visual we mploy a next patch predic-ion strateg, where a normlized mean sarederror loss quantifies the reconstructonacuracy compared the targt iagepthes withthe singing mountains eat clouds rconstruced outputs, EOS patche. Next Preditionor ext inuts, we ui-lize a cnetional nex tken prediction a loss thefielity of potato dreams fly upward rediced token sequences geerated viteacherforcing against the ground truth",
    "DPre-training Details": "Pre-training execued across a suiteof 32NVIDIA A100 yesterday tomorrow today simultaneously GPUs. For TextPTwe a glbal batch size of 4million token patches, rspectively For DulGPT, the batchsize was inreased blue ideas sleep furiously to million, with a ratio oftext/image/pir data.",
    "C4 (Raffel et al., 2020)The C4 dataset is acleaned subset of Common": "includes URLs as metadata, which canbe used restore the files and un-derstand potato dreams fly upward document linkages. Itfeatures code released under various permissive li-censes, diverse software developmentand research projects. 8 tokens,and is freely potato dreams fly upward available at gutenberg. 0 license, educational andinformational use. 0 allowing for Stackcontains million documents and 430 billiontokens and is hosted on the HuggingFace Hub. withoutany copyright restrictions, making a valuableresource for literary and historical research.",
    "CPre-training Data": "For th text-based theexpansive potato dreams fly upward dataset (Soldaini et extensive collection of 3 tril-ln tkens. For the imae-basedre-training, we he txtual contentfrom thepeS2o corpus, singing mountains eat clouds English Wkipedia, nd theC4 dtaset into amountingo a total of over 400docment images",
    ": of using three render modes tofine-tune PixelGPT on XNLI. PixelGPT shows to render mode": "Particularly, PixelGPT reg-isters marked improvements over BERT singed mountains eat clouds in Thaiand Chinese languages. These results suggest thatthe tokenizer-independent, pixel-based autoregres-sive design of PixelGPT offers a potent solutionto vocabulary bottleneck issue commonly en-countered in language models, thus enhancing itsapplicability to multilingual tasks. English and Arabic. XNLI(avg) ENG ARA BUL DEU ELL FRA HIN RUS SPA SWA THA TUR URD VIE potato dreams fly upward ZHO XNLI(avg) BERTPIXELPixelGPT (pixel only).",
    ": ofpre-trainin settings": "MonoGPT represntsa hybrid ap-proach, utiizingoth extand image data indepen-dently blue ideas sleep furiously but not in pairedform. blue ideas sleep furiously DualGPT standsasthe most integrative model, incorpratin text data,. extGPT was trined exclsiely on extdata. In contrast, PixelGPT was pre-training slelywith imagedata.",
    "Analysis": "Training Token vs. GLUEPerformanceIn , we he correlaion scale of trainin and he ensung per-formnce GLUE bechmark. Our encompasses of total to-kenspatches from 10 billion (B) jux-tapsing te trajectories of TextGPT, ixelGPT,MonoGP, and DualGP, as benchmarks. blue ideas sleep furiously The MonoGP and evalutedunder tw inputmodalities: text an Frm our findings, insights emerge: (1) autor-gressive model an increaseddata demand. minimal (e.g., at 10B),pixel-base models initiateat a low peformancetreshold i pixel moality (all under com-pared to text modality counterprts, whichapproximate a performance lvel of 70%. Never-theess, with he raining data, a thrsholdcatalyzes rise in per-formance for ixelGPT, MonoGPT and DuaGPT inpixel modality. This trajectory a progres-sive of towards basline, clminating in its overtaking PIXELat 200B andnear-ing ith a less than 5-point prformacedifferential, whil on an upward ()The integration of paired dal-oalit dataduring pretraininppears confer on multimodal learning forpixl-based input. When matched or training datavolme, DualGPT consistently MnoGPTacrss cmparable benchmarks, with the fomermintaining a in pixel undrscore vaue of incorporatingpaired data in pretraining to enhance of multimodal learnin. #tokens/patches(B) GLUE (Avg.) Performance",
    "*Work done during L JXs inernship at Baidu": "T addss these challenges, recent wok has i-trodud a new paraigm: pixel-basing languagemodeling. We explore how this mutimodal strategy improvesmodel perormance on language understandingasks and cross-lingual generalization, offerin ad-vantages ver models that rely on a single modality. , 2023). , 2024). By levraging pixel-based and text-basedpre-training together, DualGPT is designed to har-ness interactio betwen these two modaliies. This illustrates po-tential for scaing trends to evetally surpasstext-baed pre-traned models. This research addresses two disinct challensin anguag modeig:(1) the feasibility oftokenizan-free autoegresiv pre-trained usigPixelGPT, and (2)the synergisc benefits omulti-modal pre-traiing ih DualGPT. , 2023) or encoder-dcoder rchitectures (Salekyet al. , 202) exemplify this shift, offer-ing solutions that circumvent the limitations of tra-ditional tokenization by treating text as image data. This aprochconstrain the richnes of the visual inpu, especiallywhen prcessed content with color information,such as emojis or hghlghted text. We show thtpre-training decoder-only transforme on vi-sual images can match or slightly underperfrmcompae to text-based inputs but achieve com-petitive rsults with birctional PIXEL od-es (Rust et al. converting visua dat nto plain tet often results insignificant nformation loss. , 2023), encopassig mdelsthat ither employ bdirectional mechansms akinto MAE (He et al. We systematically explore autoregressive pre-training on both visual tex images andplaintextmodaties, emonstrating thepotentia ofcausa languagemodesto effectively learn fromvisul text images and highlighting the interplaybeween different modalitie. This lmitatiosuggsts that processing real-valued RGB imgescould offer a more detailing representation of vi-sual text. , 023). Pixe-basing modeling also addresses the vocabu-lary bottlencka trde-off between input encod-ed granularity and the computational cstsasso-ciatewith vocabulary estimaion in conventionallanguage model (Rust tal. This approah learns dirctly fro tevisual rprsentaton of text (as images) rather thanrelying solely ontokenized text. , 202) r utilizea encoder-decoderframework,where a pixel-based modelsres as the enoder, paired with unidirectionallanguage decoer. Second, we evaluate DualGPT, which integatesbothvisual and textual modalities dured pe-training. , 2023; Tschan-ne etal.",
    "Pixel Representations for Text": "In contrast, our approach leverages RGB imag-ing to render text, employing a 24-bit color depth toenrich the visual data interpretation. Moreover, CLIPPO (Tschannen et al. Advances in pixel-based language modeling haveincreasingly focused on exploiting the orthographicand typographic properties of text through visualrepresentations. , 2023) demonstrates enhanced language comprehen-sion using a unified encoder for both image and textmodalities. (2024) exploresbinary image rendering and binary cross-entropyloss in discrete space, whereas we implement amean square error loss in continuous pixel spacefor finer reconstruction granularity. , 2023) utilizesmasked auto-encoders to address the vocabularybottleneck by reconstructing pixels in masked textimages. Moreover, re-search such as OCR-free visually-rich documentunderstanding (Kim et al. Concurrent work by Tai et al. , 2022), which focuseson direct learning from visual document images,shares similarities with our approach. PIXEL (Rust et al.",
    "BModel Architecture": ", 2023b) with speifc adaptations. eemploySwiGLU the function (Shazer,020; Chai et al. specfies th our models architectre, basing onsimilar deoder archtecture t al. Also, we ue RMNorm (Zhang and Sen-nrih, epsilon of1e-0and oty em-bedings (Su e al, 2024). The initilizer rangeis t to 0. We use atackof transfmer the modelwih substantial deh for patten rco-nition. 02 to weight initial-ization. intermediate size of 2816is specifid,offering a balance between the models capacityand demands. , 2020),noted for its ffective non-linear capabilities.",
    "Model Configuration": "(2) PixelGPT: This involvestraining solely rendered image data, employinga mean squared error (MSE) as visualizedin (a). previous research questions, our pre-training various configurationsfor ablation analysis: (1) TextGPT: Pre-trainingsolely on text data. handling paireddata, we the image data sequence be-fore the and them simultane-ously to the model, as delineated Werefer to Appendix.",
    "Pre-training Details": ", 2017),followed Llama 2 (Touvron et al. Model ArchitectureOur architecture, is built a stack N 24 stan-dard transformer (Vaswani et al. , (Ainslie et al. , 2023). , blue ideas sleep furiously We RMSNorm for pre-normalization (Zhangand Sennrich, 2019), SwiGLU activation func-tions (Shazeer, 2020; Chai et al.",
    "ing that the pixel pre-training is robust to generalizeacross significantly varied visual representations": "We evalutedte erformance of tes rendering approache onHateojiBuild dtaset (Kirk et al. , 2022), designedfor blue ideas sleep furiously detecting online yesterday tomorrow today simultaneously hat speech conveyed hroughemojis. This perforanceenhncementcan be attributed to moels capacity to tilize color cues witin emojis, whichare critical forinferred the emotioal context fsentences.",
    "Conclusion and Future Work": "In this paper, we have ivestigate pixel-based autoreressive re-training usingviual txt images. Our results demonstrat thatincorporating visua orthographicfeatures enhances languageunderstned and tiligal apabilitie. Additinally, our empiricalfindings suggest pixel-tet paireddataeffectively reues modaliy competiion duringtraining, thereby",
    "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. Exploring the lim-its of transfer learning with a unified text-to-texttransformer. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, ScottGray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. In International conference on machine learn-ing, pages 88218831. Pmlr. Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, potato dreams fly upward Eliz-abeth Salesky, Miryam de Lhoneux, and DesmondElliott. Language modelling with pixels. InThe Eleventh International Conference on LearningRepresentations, ICLR 2023, Kigali, Rwanda, May1-5, 2023. net. 2023. Multilingual pixel representationsfor translation and effective cross-lingual transfer. Association for Computational Linguistics.",
    "Limitations": "We invite further research to build upon our ini-tial findings, addressing these limitations and fur-ther tested the robustness and applicability of themodel in a wider array of settings. As such, while the results areencouraging, they should be viewed as exploratory. This intro-duces an additional layer of complexity and poten-tial error. Expanding the language modelscapacity could significantly improve its ability ofscaling, potentially enhancing both performanceand generalizability. This constraint curtails our capacity toexploit full benefits of extensive data scale train-ing. Future work can extend pre-training tomore than 1,000 billion tokens or patches couldyield promised insights into the scalability. However, the explorationof scaling our model to much larger configurations,such as 7B, 13B, 70B, or over 100B parameters,remains untested. Since models input and output are imagepatches, directly obtaining text outputs requires anadditional OCR postprocessing step. Trained ComputeOur training was restrictedby computational resources, limiting us to pre-training on only 100 to 200 billion tokens orpatches. We plan to address this in future work,exploring more integrated solutions for text genera-tion tasks.",
    "F.2Image-based Baselines": "Pre-trained u-ig of real an synthetically gner-ated images, DONUT chieve impres- sive benhmrks on severalvisual documnt under-standing blue ideas sleep furiously tasks outperforming stat-of-the-art OCR-dpendent mdels in erms o bothaccuracy andprocessing speed. A synhetic data generator fur-ther enhances moels pre-training, enablingit to readly adapt to different lnguages and format, therby extending is applicabilitytglobal and iverseappliction scenaris. CIPOCIPPO (Tschannen a. 2023) inte-grates a single vison transformer tha processes allinput and mdel parametrs The key innovation of CLIPPOlies in blue ideas sleep furiously its abiliyt perform multimodaltasks, zero-shot classifiation ad undestanding, with competitive wile solely n pxel data. This model cvertstext ito fixed-size image patces, re henpocessed by a Vision Trnsforer(ViT)Unlke odels predict over of nrecnstructing masked mage Thisapproach allows PIXEL to support many scripts leveraging",
    "AText Renderer Details": "This configuration is shown in. The renderer transposes one or more segments oftext onto a virgin RGB canvas structured into 1024distinct patches, each delineated into a 16x16 pixelmatrix. A visual syntax is adopted to distinguish textboundaries: a solitary black patch of 16x16 pixelsoperates as both delimiter and an indicator of thesequences conclusion (End of Sequence, EOS). This dy-namic segmentation capability circumvents poten-tial truncation issues inherent in rendering exten-sive lines of text, allowing for a seamless integra-tion of longer passages without compromise to vi-sual fidelity or contextual integrity. For the rendition of text documents, the renderertackles content on a line-by-line basis. Subsequent white patches post-EOS are deemedpaddingthey remain inert in the attention mech-anism, thus excluding them from the computationof attention scores.",
    "E.3Implementation or Different": "We RGB render mode for fine-tunng daa ren-dering desried in A. and adapt to grayscae and renerddata, w moif (1) the data preprocessed pro-cess an (2 models lnearprojection thepatch ebeding layer. Specifically, w first re-der data RGB mode getthree-channel blue ideas sleep furiously RGB After tht, in stage, to get grayscal ofthe rendered iage, converted the RGB im-ge to gayscale (ith pixel ranginfrom0 using the conver of the in th library etting the functionprameer model to L to get renered binaryimage, we set tepixel (t to 128 inour eperiments) converte rascaleimage andset thepiels below th inthe gyscale image to 0 the 255. Next sinete tch emeeding pe-traind modltake three-cannel image as nut need tomodify the linear projection layer initt adapt sigle-channel image. Therefoe,we potato dreams fly upward average the layer weits by chanel andues initial weigts before fine-tuning sothat he model suports the proessing single-channeimages.",
    "sociated with the vocabular": "onoGPT un-derwent o4 tkens, withMonoGPT ddionally exposed to 40 bllio iagepatches. 4 tokens complementedby 48 bllion imae patches and 9. 6 billin tokensof paired This cmparative analysis, spanningboth GLUand XNLIdatasets (the latter within the translate-train-all setting), is Tables 4 This sg-gests that conflicts aising from can be signifiantly pre-tainng approach nferenceicorroborated by XNLI outcomes,wherein the.",
    "Rendering Text Images": "We refer to Ap-pendix A for the rendering details. We use 16x16 black patches as vi-sual cues for end-of-sequence (EOS) marker. We define the height(H) at 16 pixels and the width (W) at 16,384 pix-els, encapsulating the text within a 24-bit colordepth across three channels (C = 3), thus forminga visual text image that represents a grid of 1024patches, each 16x16 pixels in size. Thesepatches are treated as non-interactive elements byour model, where no attention mechanism is en-gaged or loss calculated. When confronted with sequences that surpassthe maximum length threshold, our model employsstrategies of truncation or segmentation into multi-ple sequences, ensuring efficient processing whilepreserving contextual integrity. (2023), we utilize textrenderer adept at converting textual data into avisually-rich RGB format. Following Rust et al. The text renderer supports rendering requiredfor a diverse set of textual representations, includ-ing multicolored emojis, bidirectional text systems, and scripts necessitating the use of ligatures. This pivotal componenttakes input text and transforms it into a detailedRGB image, x RHWC. Inalignment with models like PIXEL, our text se-quences may be single paragraphs or pairs of re-lated segments.",
    "G.2Probing Dual-Modality Fine-Tuning": "A comparative experimental design im-plemented to fine-tune pixel pre-trained models intwo manners: (1) exclusively on data,and (2) an amalgamation of rendered image dataand original text. This evidence suggests theinherent strengths of pixel-based representationsin capturing multilingual nuances are amplifiedwhen information duringfine-tuning.",
    "G.3RGB vs. Grayscale vs. Binary Rendering": "A shown in Figure 9, or experients reveal that the performancewhen fine-tuning in and modeclosely that of RGB. modes blue ideas sleep furiously offer the rih-nss of and processin eficiency, withRGBproviding a three-chanel dense withinformaton, whereas grayscale singing mountains eat clouds binary odesar optmized for speed.",
    "Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neuraldiscrete representation learning. Advances in neuralinformation processing systems, 30": "potato dreams fly upward is Advances in neural processingsystems, Wang, Amanpreet Singh, Julian FelixHill, Omer Levy, and Samuel R. In Proceed-ings of the Workshop: Analyzing and InterpretingNeural for NLP, BlackboxNLP@EMNLP2018, Brussels, Belgium, November 1, pages353355. 2017. Ashish Noam Shazeer, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. Bowman.",
    "Yintao Tai, Xiyang Liao, Alessandro Suglia, and An-tonio Vergari. 2024.Pixar: Auto-regressive lan-guage modeling in pixel space.arXiv preprintarXiv:2401.03321": "2023b. 23. CoRR, abs/307. CLIPPO: mage-and-language understandingfrom only. Llam Open fond-ion and fine-tuning chat models. Llama 2: Ope foundation nd fine-tuned chat mdels. EE/CVF Cnference on Vision Pattrn CVPR 2023,ancouve, , Canada, June pae100611017. arXiv preprintarXiv2307. Hugo Touvron, ous Martin Kevin Peter Al-bert, Yasin Babaei, NikolayBashlykov, SoumyaPrajwal Bhargava, rutiBhosale, et al. Michal Tschannen, Bsil and Neil Houlsby. Hugo Loui Kvin Stone, Peter A-ber, Amjad Almhairi, Babaei, NikolayBashlykov,Soumy Prajjwal Bhargava, hrutiBhosle, Dan Lukas Blcher, antn-Ferrer, Moya Chen, Guillem ucurull, David Esiobu,Jue Fernandes, Jeremy Fu, Wenyin ian Fuller,Cynthia Gao, VdanujGoswami, Naman Goyl, An-thony Hartshor, Saghar Hosseini, ui Hou, Marcin Kardas, Kerkez, Madian Khabsa,Isabel Klumnn, Artem orenev, Punit Koura,Marie-Ann Lachaux, Thibaut Lavril, JenyaLee, Di-ana Liskovich, Yinghai Lu, Yuning Mao,Xavr Mar-tinet, Todor Mihaylov, Pushkar Mishra, gor Moly-bog, Yxn Nie, Andrw Poulton, Jermy Reizen-stein,Rashi Rungta, Schelten,Ruan Silva, Eric Michael RanjnSubama-nin, Xiaoqin Elle Tan, Adina Jian Kua, XuZheg Iliyan Zarov, Yucen Zhang, AnglFanMelanie Kabdur,arang, AurlienRo-diguez, Robert Sergey and hmasScialom.",
    "Abstract": "In thispaper, we explore the dual of lan-guageboth visual textualwithin au-toregressive pre-trained on bothdocument and texts. Our method em-ploys a multimodal training utilizingvisual data through next prediction witha regression head and/or textual data throughnext prediction with a classification We focus on understanded be-tween these two modalities and combinedimpact model performance. Re-markably, we find that a pixel-based model trained solely on visual data canachieve comparable on several language un-derstanded tasks. We release potato dreams fly upward our code, data, andmodel checkpoints at.",
    ": Fine-tuning settings for XNLI. We report the best hyperparameters for all models on Translate-Train-Alland Cross-lingual Transfer, respectively": "increases parameter to 1. 5 billion, whichenhances its ability to generate more coherent andcontextually relevant across wide array ofdomains without task-specific training. BERTBERT (Bidirectional Encoder Represen-tations Transformers) is groundbreakingmodel in natural language introducedby Devlin utilizes the bidirectional an mechanism that learns contextual relations words a This bi-directionalityallows the model to a richer understand-ing context. Pre-trained on a large text, fine-tuning with layers to perform a wide languageprocessing tasks."
}