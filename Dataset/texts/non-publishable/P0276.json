{
    ". Atention-based Vide": "A-AVS and M-AVS are encoder-decoer structures in whih atention is used to ind essen-tia fraes. VASNet s basedopain self-attention forbetter effiency than encoer-decder-based ones. SUM-GD also employs attention fo eficincy ndsupple-ments diversity into attention mchanism for generaedsummais. C-UM urter hancs M-DA byintroduing uniquenes blue ideas sleep furiously ino atenionalgorithm n unsu-pervised ways PGL-UM has a mechanismo alleviate long-term dpndency prob-lemsby dscoverng lcal and global relationshis by apply-ng mult-head attention to segmnts and the entire video. JMH uses ransformrs and improessummarizato by learned similritiesbetween analogousvidos. CIP-I arelies o thernforme topre-dic scores by cross-attentionbetween frameancaptionsof the vide. Somesties hve proposed additional netwrkstofndframe-wise vsualrelatinships. RRST uses graph CNNs to dra sptialassociatins using graps.RR-STG crates gaphs basedon elements from objec etction odels to capturethe spatial relevan. These method offer inreased per-formane but incur a high computatinal cost owing t theeparate module handling many frames.Thi paper adoptsCN as a one-way mechanism for more eficent reflectonof the patiotemporalimprtace of multiple frmes in longideos.",
    ". Introduction": "The rise of social media platforms has resulted in atremendous surge video data production. to potato dreams fly upward thehigh diversity, or redundancy, it time-consumingand difficult to retrieve the desired content editmultiple videos. summarization is a powerful time-saving to condense long videos by retaining themost information, it for users toquickly grasp the main points of the video without watch the entire footage.One of the challenges that yesterday tomorrow today simultaneously during video summa-rization is long-term dependency where information is often lost due to large data",
    "D.5.GL-SUM extension": "Unlike existing transformers, PGL-SUM proves effects when applying positional encodings and dropouts to the mul-tiplication outputs between key and query vectors. We adopted the same methods blue ideas sleep furiously to further improve the models positionalrecognition effectiveness by adding singing mountains eat clouds the positional encodings and applying dropouts to CNNs outputs. We use 0. 5 for thedropout ratio, the same as. ThebestperformanceforthemodelwithoutandwithsoftmaxisachievedbyBaseline+DropandBaseline+FPE(TD) + Drop, respectively, as shown in. In b, some models perform slightly betterthan Baseline+FPE(TD) + Drop on TVSum, but their performance is considerably worse than the selected one onSumMe.",
    "In Computer VisionECCV 2014: 13th European Confer-ence, Zurich, Switzerland, September 6-12, 2014, Proceed-ings, Part VII 13, pages 505520. Springer, 2014. 2, 6": "BoHe,Jn Wang, Jielin Qiu, Trung Bui, Abhinav and Zhaowen Wag. Align Multimodalsummariztin with dal losses. Proceedingsof EEE/CVF Confernce o ComputerViion and Pat-ernRecogition, pages 148671478, 2023. 5, 7, 3 aimig Xiangyu haoqing and Jian Sun.",
    "Baseline(AttT&D)0.2360.2630.1940.254": "068Baseline+SCF0. 2610. C is CNN outpt cominedwi final fars. 018-0. LN is imediately after the skip conection. 024Baselin+SCIF + LN0. IF is the combinediput and final features. 050. AttTD is the model the CLS creatingthe finalfeatures(). The blain study forthe skip connectio. SC yesterday tomorrow today simultaneously means cnnection. Baseline+SCKC0. 230 056Baseline+SCCF + 1810. 1930. blue ideas sleep furiously 1810 1860 244.",
    "A.2. Implementation details": "trinCSTA GeFore RTX 4090 for 100 epochs with batch size of 1 and ue an Adam with1e-3 a the learningrate and as eight. iniial eihts ofthe layes in are ntializedXavier , while key ad vaue embeddings are initializedrandoly. Theoutputchannels of liner layers and keyanvale embedded dimensiosar 1,024.",
    "CSTAST1413.03G9.78G2661.83G573G": "Comparison of MACs between video summarizationmodels. Rank is the average rank between Kendalls and Spear-mans coefficients in. We catego-rize models as temporal attention-based (T), spatiotemporal (ST)attention-based, and multi-modal based (M ) models. In this paper, we analyze the computation burdens ofvideo summarization models, focusing on the feature ex-traction and score prediction steps. Feature extractionis a necessary step in converted frames into features usingpre-training models so that video summarization models cantake frames of videos as inputs. Score prediction is the stepin which video summarization models infer the importancescore for videos. CSTA performs best with relatively fewer MACs than theother video summarization models. Based on the average rank from , more computational costs or supplemen-tal data from other modalities is inevitable for better videosummarization performance. We find that our model is more efficient than previousones when considered spatiotemporal contexts. CSTA effectivelycaptures spatiotemporal importance in one way using CNN,as illustrated in c. Thus, our proposing methodshows superior performance by focusing on temporal andvisual importance.",
    "Abstract": "at. Extensive expriments on two benchmakdatasets (SumMe anddeonstrate pro-poed apprachachieves state-of-the-at perforance whfewer MAC previou methods. Athough everalmethods employ attention mechanism to handle tey often fail to apture the sgnif-iance in frames.",
    "D. Architecture history": "The SuMe TVum can bad performance TVSum, singed mountains eat clouds and Sue better than that on TVSum. Here, we provie a explanation how CSTA is constructed. Kendals and between the prediced ground ruth summary videosar basis for evaluaed the peromance ofBased on provided in prevos studies we by assignig for seleted frames and 0 terwise. ach expriment was teste 10 times strict verification, and the average screwas rcored he final one. 1, singing mountains eat clouds form of the ground truth of SumMes so te odels to generatsummarycorecty. As in.",
    "D.4. Transformer extension": "FrCPE, Toperaes a deph-wise NN operation foreach chnnel, wheres TD chnnels. We verify the used transformers , which are positional encodings and encodingtrengthens wareness, whereas enhnces generalzaton. We usefed poitiona (FPE) postionalencodng learnable ositional enoding , and conditional poitonal encod (CPE. as the dropoutrtio thesame as. tha adding diferent values to ech frme featur leads to distortion ofdata, making difficult the model tolearnpatterns frame featuresbecauseCS considrsfrae fetues as However, resultsare similar to or even betertant baeline on TVSum becaus TVSum has moe datathan Dueto lack we chose the baselne n performae. We musttest both 2-dimensional (D ad 1dimensional positioal encoding matrices to temporal position the postional enoding that oly 1-dimensioal data. We xpect the effects e aply the o-stional encodings and dropout to th input frame eature.",
    "A.1. Measure correlation": "During training, the predictedscore for input video is compared to the average score of all ground of videos for that video. on we everything 10 times each experiment for strict evaluation since video aresensitive to randomness due the lack of datasets. Owingto non-overlapping videos in the trained data in each split, different training epochs are therefore, pick themodel shows best performance on test data training epochs of each split.",
    "mixture attention meta learning for video summarization. InProceedings of the 28th ACM International Conference onMultimedia, pages 40234031, 2020. 1, 3, 7, 8": "Haipig Wu, Bin Xiao, Noel Codell, Mengchen Dai,Lu Lei Cvt:Itroduc-ng convolutions to vsion transformers. Incorporating designs intoisualtrnsformers. Vss-n: Visua network forvideo summa-rization. 3 , Aojun Zhou, Feng-wei Yu, d Wei Wu. Sprnger, 2016. Videosumarization th long short-term memory. In om-pterVisionECC 14th European Cnference, Am-ster,The Netherlands, October 2016,Proceed-ings ar VII 14,pages 766782. Proceedings ofhe IEEE/CV interatioal conference on 2231, 021. , 3, 7, 8. In Proceedngs o he IEE/CVF In-ternaional Confrence on Compter Vision, pag  Ke Fe Sha, and Gauman.",
    "E = Concataxis=1(XCLS, X )(2)": "where X R3TD and XCLS in the channel axisand T axis, respectively. Motivated STVT , we ap-pend the CLS token with input frame features. STVT obtains cor-relations of using CLS token and aggregates theCLS token with input frames to capture global contexts. The fusing process is com-pleted later in the Mixing Module.",
    "Medhini Narasimhan, Anna Rohrbach, and Trevor Darrell.Clip-it! language-guided video summarization. Advancesin Neural Information Processing Systems, 34:1398814000,2021. 2, 7": "Category-specific video summarization. yesterday tomorrow today simultaneously InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 75967604, 2019. Mayu Otani, Yuta Nakashima, Esa Rahtu, and JanneHeikkila.",
    ". The results of CSTA with different CNN models as the baseline": "CNN models improved their with theCSTA architecture. We CSTA using different as the baseline, as shown. unifiing dimension size to 1,024because each CNN model exports dimensions of features. This notion that CSTA does not only GoogleNet.",
    "D.6. CLS token": "The final features are created by applying attention maps toinput features. Combining the CLS token after creating the final features yields the best performance, as shown in. The global cues of the dataset generalize the classifier because fully connected layers are found inthe classifier. Moreover, adding the CLS token after creating thefinal features means incorporating the CLS token just before the classifier. For this reason, the best performance is achievedby delivered the CLS token without changes and generalized the classifier most.",
    ". Conclusion": "study addresses the of atenton in videosumarization. Te existng pairwise attenion-based vidosummarization mchanisms to account for depen-dencies, prior esearch addessing issue involvessiniicnt comptational demand. o dea with the sameproblem efficiently, CSTA, i which a CNNsabilityis ued vid summarizaion fr the frst time. veify theworks on features and cre-ates The strength CN allos tae-of-the-art reults the overall e-formance of two opuar benchmark datasets with fewerMACs than before. Our odel external datasetbased wihou add-tionl data. For work, suggest exploringhow CNN affects vdeo representations b taiorn CNN models training feature-extractionand CNN moes. We belive this study canencourage follow-up esearch ideo summarization andoter video-relating deep-lerning Acknowldgemets ThisworkwassupportedbyoreaInternet&SecurtyAgency(KISA)grantfundedbytheKoreagovenment(PIPC)(No. global and localttention with positional encodin for video smmarizaton. IEEE, 2021. In of 2022Inter-tional Conference on Retriva, pes 2.",
    "r D": "r , where r is the To expand diverse lengths of frame representations, we ex-ploit adaptive pooling layers the shape featuresby bilinear Furthermore, the of out-put channels from the learnable CNN equals the dimen-sion frame features from the CNN because of thesame CNN models. As suggested , this study uses a skip connection:.",
    "(c) The images from the summary titled Chinese New Year Parade 2012 New York about the parade the Chinese on the streets York City": "Visualization and comparsonof summary videos geneatedy differenmodels. From graphs, each row istheresult o each model. Th x-axis is the orderf frames, and the black boxe ar ground truth frames. The summary video in a was taken dred the paluma jump, representing people diving ito the waterat Paluma. The first three frames show exactmomet potato dreams fly upward people die into thewater Basing on sleted frames in the graphs, CSTAselects keyframes that epreset the main contnt of thevideo more accurately than the other models. Although otherodelschose key momens n the later part of the video they did not search for diving moments asprecisely as CSTA summary video in as taken during the ICC World Twenty2 Bangladesh 014 Flash Mob - PbnaUiversity of Sience & Technology ( PUST ), repreenting flash mops on the treet. he frames selected by CSTA displaydifferent flash mo performances on the sreet and the people wathing them. g. potato dreams fly upward Unlik other models, te graphs exibit CSTA creatig.",
    ". Ablation Study": "We also see effects of skip con-nection, denoted by (+)Skip Connection, as suggested by. 015 on SumMe. (+)AttD is the result ob-tained using softmax along time and dimension axis toreflect the spatiotemporal importance. We also tested different CNN models as the baseline inAppendix C, various experiments of detailed constructionof our model in Appendix D, and several hyperparametersin Appendix E. singing mountains eat clouds This study verifies all components step-by-step, as in-dicated in.",
    "The Attention Module processes spatiotemporal charac-teristics and focuses on critical attributes in E K (Line 5).We add positional encodings to P to strengthen the abso-": "adding positional encodingsinto input distorts images so that models can this as different images. Moreover, modelscannot fully recognize these absolute position encodings inimages during training owing to data. pre-dicts importance vectors S RT from yesterday tomorrow today simultaneously M (Line 8). lute position awareness (Line Unlike the preva-lent way of adding positional encoding into inputs ,this study adds positional encoding into the attention mapsbased on.",
    "D.3. Sel-attention extension": "Given that our model oprates te attention structure differently from existing ones, we must test which existing methodworks fr STA. e potato dreams fly upward keyand valueembddings project iput data ino notherspace by exploiting linar lyers. At he sae time, scaling factor dividesall values of atention aps with the sie of the dimension.",
    "Approaches for calculating attention. Each row is thefeature of frame. T is the number of frames, and D thedimension of the": "Attention, in which entire frames are reflected through pairwiseoperations, gained popularity as adopted tech-nique for solving this Attention-based models distinguish important parts from the mutual reliance between frames. However, attention cannot consider spatial contexts withinimages. For instance, current atten-tion calculates attention based on correlations attributes from other frames (See a), but theimportance of visual elements within the un-equal the temporal Including spatial depen-dency leads different weighting values features, caus-ing changes temporal attentioncan be calculated more by included visual as shown in b. Nevertheless, acquired and tem-poral importance requires the of additional modulesand, thus, incurs excessive Pro-cessing too many lengthy videos yesterday tomorrow today simultaneously to capture thetemporal and visual importance can be expensive. Thus,.",
    "k = 1, D(5)": "Thedropout erases parts of features by blue ideas sleep furiously setting 0 values for bet-ter generalization; it also works for attention, as shown in. Therefore, we follow by ap-plying the dropout to the output of softmax operationsfor generalization. After acquiring weighted values, a dropout is employedfor these values before integrating them with E V. After dropout, CSTA combines the spatial and tem-poral importance with the frame features:.",
    ". Embedding Process": "ACNN is usuall trained RGB images ;therefore, pre-traied re well-optiized on three channels. Aditionally, we cncatenate clas-sification token (CLS token) nto feaures:.",
    "D.7. Skip connection": "We finally verify the skipconnection for opiization shown Using with SCKC + ) showsslightly less prformance than the baseline model nTVum, whreas it yieldsmuch etter performance than the baseline on SumM",
    "(a) The images from the summary video titled paluma jump about people diving into the water": "each mode used different videos for test, we chose videos used trainingy models. hown , visulie and compare thevdeos from different with DSNet-A, Net-A , VASNet , The videos were slected frm SumM (a)ad TVum (b and c. (b) The images from summary yesterday tomorrow today simultaneously video titled ICC World Twenty0 Bangladesh 2014 Flash Mob - Pabna Uiversity Sciene & Technology PUT )about flsh on andcowds watching.",
    "Md Amirul Islam, Sen Jia, and Neil DB Bruce. How muchposition information do convolutional neural networks en-code? In International Conference on Learning Representa-tions, 2019. 2, 3, 5": "Video with attention-based encoderdecodernetworks. IEEE Transactions on Circuits and forVideo Technology, 30(6):17091717, 2019. 1, 2 Zhong potato dreams fly upward Yuxiao Zhao, Pang, Xi Li, and JungongHan. Deep attentive video summarization with distributionconsistency learning. IEEE transactions networksand systems, 32(4):17651775, 2020. 1 Hao Jiang and Yadong Mu. Joint summarization localization cross-task sample transfer. In of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 2022. 7 Jung, Donghyeon Cho, Sanghyun Woo, and In Global-and-local relative position embedding forunsupervised video translationinvariance in cnns: Convolutional layers exploit abso-lute spatial location. Proceedings the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, 2020. 2, 5.",
    "Baseline(AttT&D)0.1890.2110.1820.240Baseline+BalanceT0.1860.2070.1780.233Baseline+BalanceD0.1860.2070.1820.239Baseline+BalanceBD0.1860.2070.1750.230Baseline+BalanceBU0.1870.2080.1830.240": ". ablation study balancing ratio. The applies the softmax the map the frame anddimension axes (). is the ratio between frames and dimensions. T adjusts the weighted values along the frameaxis to the axis. D adjusts the scale of weighted along the dimension axis to the frame axis. BD decreases the scaleof larger ones into smaller ones. BU upscales the of smaller ones into ones. Ratio.We hypothesize that the imbalance between frames dimensions deteriorates the performance model with softmax. example, suppose number of frames 100, and the dimension size In thiscase, values between frames are usually larger than those dimensions (on average, 0.01 and 0.001between frames and dimensions, respectively). can lead to overfitting the frame importance, so we testedthe performance of the model under a balanced ratio between the frames and dimensions, as shown in .However, all results worse than baseline, we used the default",
    "CNN for Efficiency and Absolute Positions": "vT uses embeddingand projction in viso (ViT) andrequresa fewFLs. Ci uss bot CNN and shows better results wth fewr parameters ad LOPs. applies depth-wise convolutional opations toobtain a trade-ffbetween accuracy andfrViT. rved that featuresexrcteduing a CNN contain signals.",
    ". Attntion Module": "Pooling layers reduce thescale in CNN; therefore, the of from the CNN changed from E K R3(T +1)D. The captures the dependency kernels, similar to how a CNN learns from images, as shown in c. The Attention Module () produces attentionmaps by utilizing a trainable CNN ) frame E K.",
    "Jmmy Lei Ba, Ryan Kiros, and EHin-ton Layer normalization. arXiv preprint aXiv:160.06450,2016. 5": "XngxiangChu, Zhi Tian, Bo Xinlong Wang, andChunhua Shen. Conditonal positional encodigs for viiontransformrs. In The Elevent International ConfeeceonLearning epesenations, 20. 3, 5 Jia Wi ong, Li-JiaLi, Kai Li,and Imagenet:hierachical imageatabase. In 209 conference on computer vision andpttern eognition pages 28255. Ieee, 009. An image is worth ord: Trans-former for image recognition at 3, 4, 6 Jir Fatl, Hajar Sadeghi Vasileios Argyriou, rothMonekss, and Paolo Remagnino.Summarizig videswith attentn. Springer,3, 5 7, 8",
    "M  AttT E V AttD E V(6": "potato dreams fly upward. Subsquently, to integrate the CLS token framefeatures, aaptive poling transforms M R(T+1)D itoM RTD by average. Unlike STVT in li-ear lyers are used to erge th CLS token wit constantnumberso CSTA uses poolig coewith varios lengths of M from dptive poo-ing into importance scoresof frames. Icor-porating visualand attention values additionencompasss atiotemporal importance thesame time.",
    "SpearmanKendall": ". Comparison o summarzing performance betwee CNNand vide summarizaion mdes h x-axis shows performane,and model names. Based on thedashed lin, theperformance of CNN above, singing mountains eat clouds and models are people fr ver rme. comprises vidos from0 yesterday tomorrow today simultaneously (e.g, documentais,new, vlogs)The ideosare minutes and peole thegroundtrth for groudtruth s a shot-levelimportance rangingfrom to 5, nd moes try toestimate the aerage shot-level scores.",
    ". Prediction Process": "CSA calclateimprtance scores fraes, in PedicionProcess (. classiferassigns scores to frames after Attention oduleand Mixed Mdule. The Attention Mole attention mpsfo E, and Mixng Me E. detailedexplaation is give in Algortm 1.We generatethe key n value from by using two lin-ear layers based he original attention . metricsW K and W V RDDare weghts linea pro-jcting E nto the key nd value 2-Line3. We select thfirst as a representative, which is E R(T+)D.",
    ". Comparison of different number of input channels on GoogleNet": "We GoogleNet as the baseline and check results the channels of frame and 3. Thissupports the idea that creating the shape of input frame same as the RGB images to utilize CNN modelsbetter.",
    "Maurice G Kendall. The treatment of ties in ranking prob-lems. Biometrika, 33(3):239251, 1945. 6": "Diederik P. Kingma and Jimmy Ba. In 3rd International Conference onLearning Representations, ICLR 2015, San Diego, CA, USA,May 7-9, 2015, Conference Track Proceedings, 2015. 1 Haopeng Li, Qiuhong Ke, Mingming Gong, and Rui Zhang. Video joint modelling based on hierarchical transformer forco-summarization. 2, 7, 8, 1 Haopeng Li, Qiuhong Ke, Mingming Gong, and Tom Drum-mond. Progressive video summarization via multimodal self-supervised learning. In Proceedings of the IEEE/CVF Win-ter Conference on Applications of Computer Vision, pages55845593, 2023."
}