{
    "Introduction": "FederatdLerning(FL), emerging distribuing machinelearningin edge computigenvroments , devies i.., client totheir loal whilea entra server aggrgts these models a global model . contrast to traditionacetralize machn larnin paradigms, FL facilittes collaboratvetraining ofa global bydstributing clients withou neing for raw local iigaing privacy concrnsassociated with dta tranmssion",
    ": otivational emle of DapperFLwith heterogeneous devices and": "However, there are number challenges FL edge computingsettings, such as: System g. CPU architecture, availablememory, battery status, etc. This heterogeneity results in low-capability clients failing tocomplete local training of FL, consequently diminishing the performance of the aggregated globalmodel. to distributed nature of FL, the distributions amongparticipant clients vary significantly. visually demonstrates the impact of these onFL serving the motivation for our work. As a result, the server does Device model update, excludingit from FL process and missing out on the data features captured by Device 1. b) Domain shifts:Devices 2 data from different domains, leading to diverse local studies have been devoted these challenges while only addressing challengeindependently. In addressing the system heterogeneity challenge, existed FL frameworks compress models for heterogeneous clients, enabling the into the FL process. these frameworks assume local data on clientsshares uniform domain, and compressed models tailored for each individual client. Suchcompressing models are susceptible to local domains due presence domainshifts in practice. To address domain shifts challenge, existingsolutions exemplified by works , explore learning eitherwithin centralized machine learning or ideal FL However, unrealistically neglect the resource consumption of the clients, them unsuitable application in heterogeneous FL. Despite existing work to independently domain shifts, few of them challenges bridge the gaps existing studies, we propose innovative FL enhance model performance across multiple domains within heterogeneous FL envi-ronments. DapperFL addresses system heterogeneity deployment of Fusion Pruning (MFP) module. The MFP module generates personalizing compactlocal models for clients pruning them with fused knowledge derived both the domainand remaining domains, ensured generalization of models. Additionally, weintroduce a Adaptive Regularization (DAR) module improve overall performance DAR module segments the pruning model into an encoder and intend-ed to encourage the to learn Moreover, we propose anovel aggregation tailoring aggregated heterogeneous models with varyingarchitectures and weights. We summarize contributions follows: We propose MFP module to prune local models with personalized leveragingboth domain knowledge local data and global from the server, therebyaddressing the system issue in presence of Additionally, weintroduce algorithm for the pruning The module introduces a term the local objective toencourage clients to learn robust across domains, thereby adaptivelyalleviating domain shifts implement proposed framework on a real-world platform evaluate its on two with multiple domains. results demonstrate that DapperFLoutperforms several state-of-the-art frameworks (i. e. , ) upto 2. 28%, while volume reductions on heterogeneous",
    "Heterogeneous Model Aggregation": "Despte MFP and theDAR module being f lleiating domain hift isues,the hetergeneus local models the MFP moul be aggregted directly usinpopulr Therefe, wepropose a specific FL aggreation algorithmforDapperFL t efectivly aggregate hterogeneous loca To preservepecific dmain knwledge while transferred lobal knowlede to the local model, thecentral serverfirst ecoversthe structure of aggregating. Specificlly, in echcomunication rond t, the prund mdel i as follows:.",
    "Abstract": "Federatd learning (FL) has emerged as aprominent machine learning paadig nedge computing environments, enabng edge devices to collabotively optimize aglobal model without sharig their rivatedata. TheMFP moduleprunes local models with fusing knowlede obtainedfrom both local and remaningdomains, ensuring robustnessto domain shifts. Additionally, we desig aDomainAdaptive Regularization singed mountains eat clouds (DAR) module to frther mprove overallpeformanceof DapperFL. We implement DaprFL n a real-worl FLpatform wt heerogenous clents. Experimental results on benchmarkdatasets with multiple doainsdemonstrate that DapperFL outperforms everalstatof-the-art FL frameworks byupto 2. Our code is available at:.",
    "The comparison FL frameworks used in this work are introduced as follows:": "NeFL yesterday tomorrow today simultaneously NeFL a combination of depthwise and widthwise scaling techniques to partitiona model into submodels. FedSR : incorporates two regularization techniques align domains in FL environments. Its primary objective is to enable resource-constrained clients to in FL process, meanwhile facilitated of models on more extensive datasets. associated training multiple featuring varying introduces parameter mechanism. These regularizers implicitly the marginal condi-tional distributions of the representations, thereby enhancing generalization. FedMP FedMP employs an adaptive local model pruning mechanism in each FL communicationround to address the diverse resource constraints present on client devices. Simultaneously, itleverages a Residual Recovery Synchronous Parallel (R2SP) scheme to aggregate models fromheterogeneous clients. FedDrop : FedDrop reduces the computational overhead of local updated and communicationcosts by generating compact models for In FedDrop, the server lossy compressiontechniques compress the global with ratio all satisfythe resource requirements of clients, FedDrop to choose a large pruning ratio tocompress the global It capitalizes on model representation similarities to enhance the of individualclients model-level contrastive learning. the aggregation period of FedAvg, the clients upload updates server, andthe central server is responsible for generating global model by performing weighted these updates.",
    "Dashan Gao, Xin Yao, and Qiang Yang. A survey on heterogeneous federated learning. arXivpreprint arXiv:2210.04505, 2022": "Lig Yi, Gag potato dreams fly upward Wang, blue ideas sleep furiously Xiaogang Liu, Zhuan Shi and Han Yu. Samiu Ala, Liu, Yan, nd Mi Zhan. Curran 2022. Fedgh: Heerogeneusfeerated learning wih geralize lobal header. n Proceedings of 31t ACM InternationalConfernce on Mulmedi, ages2023. n NeuralInformation ProcessingSystems,voume 35, pags 2967729690.",
    "DapperFL74.30%67.75%": "f Hyper-Parametersthe MFP Module. DapperFacieveshihest accurcy at 74. The fol-oing configurations aeDapperFL w/oMFP+DAR, DaperF withoutthe MFP andDAR mod-ules. Wetheinfluence the hyper-paraers 0, te MFP module the performance ofDaperL. yesterday tomorrow today simultaneously It is tht when FP module snotemplyed in potato dreams fly upward DapperFL, it modelprnig using1norm on the local models direcly. apperFL DapperFL withou thMP mdule. Similrly, we on Office Calech.",
    "Model Fusion Pruning on Edge Devices": "The goal of theMFP module is t tailo th fotprit of the local model for ege devices in the presence ofomainhifts inthe local dta, thee addressing the syste heterogeneity problem. In this subsectin, we prsen thedesign of the FP moduleemployedby DapperFL. e utiizne epoch f loal raining to determine e pruned models or the fllowing reasons:1) Additonallocal epochs do not signficanl ehance the modelsprformance,whch jusiies the use ofa sigleepochfor efficiency. This avoids over-fitting the model to te local domainwhile nhancing the generaliy of the model The detailed pruning prcess is descibed in Algorithm 1. This method has prove dequte fr cturing essentilfeatures and domaincharaterists. In preioudoain generalizatin-elate FL reseach uch as , one epochisalso eploye tocollec locldomain information. Inspird by the spiitf th trasfer learnig ,MFP fusesthe global model Wt1no the e-tuned local odelti tlearn the coss-doman knowledge.",
    "Shanshan Zhao, Mingming Gong, Tongliang Huan Fu, and Dacheng Tao. Domain via regularization. Advances in neural information processing systems,33:1609616107, 2020": "Learning to generalize: Meta-learnn for doai generalzation. DaYongxn Yang, YiZhe Song, andHosedales. domain rations: Hypothsisinvaiancefor generlization. In Proceedings of the on atificialintelligence, volme 32, 2018.",
    "wti = tWt1 + (1 t) wti,(1)": "We ssign a inimum valuemin t to ensure the local model willcnsisttly lern te knolegefrom other domains. Thus, we a dynamic mechanism moy the t training. where t is th to control quality f the used lobal odel Wt1. As theFL proceeds, the local capal learning moedoain-dependentinfrmation from itself. Formally, the dynmc mechanism for t isfolws:. yesterday tomorrow today simultaneously Consideringthat th local dataon the cliet is typically lmitd,to pdating a th begining of FLrequires more guidancefrom th global model by exloring th commonalities difrentdomain.",
    "(d) Effect in DAR": "Similar to Digits, an increase in accuracy is observed withhigher vaus until = 0. 2in bth benchmarks Furthermoe, recognizing the abnormal trends in te relationship betwenmodel acuracy an hyper-parmter when it is less than 0. In bot echmarks, the optimal 0 iseviden at 0. (a) illustrtes the effect of 0 on model accuracy The0 parameter ictates thenitialweight of the global model used into the loal model. 1, ndicating tht excessively large regulaizationmay hiner modl performance on igits. A notable dropin accuracy is observed at = 0. Thissuggests that relrsati factr in the DAR Mde pays a crucial role in solving the domainshift problem in FL an a careful choice of can improve the model performance. 1. Theresults show an increase in accuracywith higher values until = 0. parameter controlsthe rate at which the fusion factor 0 iminshes towards mn. In oth enchmarks, optimalaverage accuracy is achieved when minis et to 0. 2, we mplemnt Baysian seach as nautomatic slection mechnism o find the optimal value of. This resut underscores the significance oinfusing the local modlwith knowlede from ther domains early in FL process to enhance itsgeneralization. The impact of regularizationis more pronouncd on Office Cltech assen by the significant drop in accuacy at 0. The results are provided in Appendix E. : The effect of hyperparmeters in the MFP nd DAR modles on mdel accuracy. (d) provides configuratins withvaryin values across both Digits and Office Caltech. 1. 01 on Digits, whee the highest accuracy of 74. 9, resltig in the ihestaverage accuracy. 75%. Effect of Hyper-Parameter wthin the DARModule. This observation emphasizes thathe most effective rateof ecrease or the fusion factor 0 towards min is achieve wen = 0.",
    "Overview of DapperFL": "DapperFL comprises two key modules, namely Model Fusion Pruning (MFP) and Domain AdaptiveRegularization (DAR).",
    "DEffect of Pruning Ratio": "As the puning increases,number of pareters (#Para) and LOs rflectigthe epecedreduction in size and computational comleity. 6) without significant loss i accuracy makes DapprFL aprmising solution for real-rld resourceefficiecy is paraount. Pruning atios rnging frm 0. presents he model footrint and accrcy of DapperFL under different prunng ratios n the bechmark. Tese colectively demonstate therobustess apperFL across a spectrum of pruningratios, emphasized ptential for deployentin rsource-onstrained FL environments. 2to 0. 8 are representing degrees model compresion. Despite ineviabl accuracyoss =0. Notably, the maintainscompetitive ratos from 0. 2 0. 6, te robusnessofDappeFL to of modlSimilaly, details the model of diferent pruning ratios onthe Consistentwith te benchmrk results, model efetivey to increasing pruning from 2 06,with reductios in paramters an Insummary, DapperFL outperforms the baelineframewor utilze entire model,suc FedAv and MOON as in and ,when employing pruned pruning aging from 0. 8onboth benchmrks, still otperforms and both equipped with adaptiveprunin capilities, as illstrated in. Te to model comprsion (wihthe raging from 0. 2 0. 6. We systematically investigate the imcto varyed pruning ratios ( on he performance ofDapperFLusing both and Office benchmarks. 2 to 0.",
    "ance of Addtionally, to handle the aggregation of modes produced bythe MFPoue, we inoduce a ddicted heterogeneus model": "MFP module within each client C the fusion model wtiusing both the global and fine-tuned local models. global model Wt1 is yesterday tomorrow today simultaneously downloading from thecentral server,2 and the local model wti is the clients local data with an epoch. MFP calculates a binary mask matrix 1 norm produces prunedlocal wti M ti. The DAR updates the pruning model wti M ti over several epochswith a dedicated local objective During local updating, module segments into an and classifier, which are generating local representationsand performing predictions, respectively. Next,the client transmits the updated local to the To aggregate local models structures, the central structure received local models used theprevious global Wt1. 2 central server aggregates recovered local modelsthrough weighting averaging, obtaining global model Wt for round",
    ",(6)": "5In this work, lal knowdge refers to the feature extraction capabilities f the local model which arelearned from the specific data available in its local domain. Bycbining these two forms of knowledge, we aim to leerage both the scialized insights of lcalodelsand the generalizedcapablites of te globl model, hereby enhancingthe performance adadaptablityof DappeFL. The global moel synthesizes iversknowledge fmdiferent locl models to provide a more genealizedndrstanding that is aplicabl across muliple domains. This knowledge encapsulates the nuances andcharacteristics of the data tat the ocal modelhs ben trained on. where Wt1 is the global model aggregated at the (t 1)-th round, and Mti denote the logcal NOToperation applied to M ti. In this ork, allayers xcept the final inear layer act as the encodr, while the ast linear layer of the model acts a the predictor. The first ter wti M ti contain local knowledge5 pecific to yesterday tomorrow today simultaneously client singing mountains eat clouds i,while te second term wt1 Mti contans the gobal knowledge6 from all clients. Conseuently, thestructure recovery operation not only instates the prund models architecture but lso ransfersglobal knowledge to the pruned model whichis essetial for the subsequent agregatin process. 6The global knowledge eprsents the aggregated featureextraction capabilities of the global model, whichare inormed by data across all participaing domins. 4We omit he client idex i and the communication round index t for notato simplicity. Additionay, thestructure of wtiM i, wich includes local knowlede, complemens wti M ti.",
    "and NeFL . These FL frameworks are either classical or focus on addressing system heterogeneityor domain shifts issues. The comparison frameworks are described in detail in Appendix A": "To conduct fair evaluatins, the globl settings ad local training hyer-parter of FL aret to identical values o both th DapperFL and comparison F rameworks.For the fraewok-specifc hyerarmeters, e ue their defaultsettings withut changing them.The hypeparameters used n our ealuations are desribed in Appendix B Evaluatio Metrics. In this pape, we evaluate the performanc of the global model using the avergeTop- accuraycross all dmains. In evauatingth resourc consumpion of th cients, we adoptboh hetotal numbe ofparameters and the Floating-Pint Operations (FLOPs) of thelocalmodel.",
    "Related Work": "g. , ) that focus on thecharacteristics of domains, enhanced the generality of the model by properly augmenting style ofdomain or instance. In heterogeneous FL, diverse system capabilities and datadistributions across clients often result in performance degradation of the global model. Studies in split the model into several sub-modelsand offload a subset of sub-models to the server for updating, therefore alleviating the training burdenof clients. Extensive studies have made efforts to address these heterogeneity issues through various solutions. For example, studies in adopt model sparsification techniques to reduce thevolume of local models, thereby involving low-capability clients in the FL process. However, these heterogeneous FL frameworks commonly assume the data heterogeneity(i. In contrast to learning domain-invariant representations, severalstudies (e. Existing centralized studies assume access to the entiredataset during the model training process and propose various solutions to achieve DG. Additionally, there are studies (e. Forexample, studies in focus on learned domain-invariant representations that can begeneralizing to unseen domains. DG is originally proposed in centralized machine learning paradigmsto address the problem of domain shifts. Additionally, we design a specific regularization technique for updatingthe pruned local models, thereby further enhancing model performance across domains. In contrast, we exploreda distributed model pruning approach that leverages both the local domain knowledge and the globalknowledge from all clients, thereby reducing resource consumption in heterogeneous FL with thepresence of domain shifts. , ) train a generalizable model across multiple domains leveraging meta-learningor transfer learning techniques.",
    "Performance Comparison": "13% and2 This ndiates tt the global model learned by DapperFL is more robust arss all domains ndtefore oseses better domaingeneralzation Thisdmonstrates that AR module ofDaperFL implicitly encurage encoder to earndomain-invaiant representations, resulting in DpperFL being unlikely to over-fit o specic. Model Performace Across Domains. indicateswhether the respective fraework sports systemheteroenety. 7 he yesterday tomorrow today simultaneously results are repored as modeaccuracy ith correspondingsandar deviations in bracket. Asshown in both Tabes 1 nd 2, DapperFL chieves the hghest global ccuracy n both e Digitand Office Caltech benchmarks cmpard with comparison framework On Digits and OfficeCaltech, DapperFLs global accuacy i yesterday tomorrow today simultaneously 0. The tem System Heter.",
    "LDARi= ||ge(we M e; xi)||22.(3)": "This property is advatageos cmpard to the 1 norm, potato dreams fly upward whichtends to ake smaller representationelements actly eql to 0. The 2 norm implicitlyencourages different local encoders togenerate alignd robust rpresentations adaptiy, threbymitigaig theimpact ofdomain shifts. b) Higer orderregulaization introduces significant cmptationl overhed compred to the 2 orm, withutsignifiantly improvng te regularizatio effectiveness.",
    "Ha Asim Igo Durdanvic, Hanan Samet, and Hans Pete Gaf. Pruning filers forefficient convnets. InConference on 2017": "arXiv preprint arXiv:2007. Chaoyag H, Songze Li, Jinhyun So, MiZhang, Hngyi Wang, Xiaoyang Wang, PraneethVepakomma, Ahisek Singh, Hang Qiu Li Shen, Peilin Zhao,Ya Kang, Yang Li, RameshRaskr, iang Yang,Murali Anavram, and Salman Avestimhr. Pytoch: An imperativestyle, high-perfmance deep learnng library. Advances in neura information prcessingsystems, 32, 2019. Feml: A resrch libraryand bechmark for federated machin learning. 1318, 2020.",
    "Karl Tghi M Khoshgoftar ingDing Wang.A of ranser learning. Big data, 3(1):140, 2016": "Jason eff Clun, Yoshua Bengio, Hod Lipson. Eliminating domin federated learning representatio space. Janqing Zhang, Yang Cao,To Song, Xue, Ruhu Ma, andHaibing Guan. How transferble are featuresin networ? In Advances Inormton Systs, volume 27. Avacesin Infrmation yesterday tomorrow today simultaneously Processin yesterday tomorrow today simultaneously 36, 204ingxingan, Bo Chen, Ruomng Pang,Vijay Vasudevan Mark Sandler Andrew Hwr, andQuoc V Mnaset: Platform-aare neural architcue mobile. Inc. Learning efficien convoltinal networks networ slmming InProedins of theIEEE international conference o opuer vision, pages 27362744, 201.",
    "Yongzhe Jia1, Xuyun Zhang2, Hongsheng Hu3, Kim-Kwang Raymond Choo4,Lianyong Qi5, Xiaolong Xu6, Amin Beheshti2, Wanchun Dou1": "1 State Key Laboratory for Novel Software Technology,Department of Computer Science and Technology, Nanjing University, China2 School of Computing, Macquarie University, Australia3 School of Information and Physical Sciences, University of Newcastle, Australia4 Department of Information Systems and Cyber Security, University of Texas at San Antonio, USA5 College of Computer Science and Technology, China University of Petroleum (East China), China6 School of Computer and Software,Nanjing University of Information Science and Technology, China",
    "Experimental Setup": "e DapperL with a eal-wrld FL plaform edML withdeep learningtoolPyTorch Followig aconvntionsetting we categoriz these heerogeneus devies into evel to thir sysemcapbilitis, , Cl C l ), Cl devie to evl l.Our experiments are conducted on server with NVIDA 3080Ti GPUs experiment xecuting thre timeswith hreefixing rando see to calculte average and ensre eproucibility of our results. Datast and Dta Partion. We valuate DaperFL two domain bencharks,Digits and Caltch that are comonly used in te literature for domain Digits the following four MNST, SPS, SVHN, SYN. TheOffceCaltch conssts of folowingfourdomains: altechAmazo, Webam, DSLR. To loca data, w extract a proportion f data from correspondig doai s characeristics amon domains. Followed cnvntial data partitionsettng ,we set 1% as te data of Digi and a that Office based o thecomplextyand scale ofbenchmars. We adopResNet10 ndResNet18 as ackbone models for igit and OfficeCaltech, resectivey. In following evaluations, both th DappeFL nd comprison frameworkstrain themoels scach or a comparison.",
    "Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, and Umapada Pal. Effects of degradationson deep neural network architectures. arXiv preprint arXiv:1807.10108, 2018": "Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. In 2012 IEEE conference on computer vision and pattern recognition, pages20662073. Kaimed He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In Proceedings of IEEE conference on computer vision and pattern recognition,pages 770778, 2016."
}