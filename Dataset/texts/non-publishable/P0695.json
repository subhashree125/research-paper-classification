{
    "Frederick Jelinek. 1976. Continuous speech recognitionby statistical methods.Proceedings of the IEEE,64(4):532556": "Nitish Shish Keskr, Bryan McCann, Varshney,Caiming Richard Socher. arXiv preprint arXiv:1909.558. Douw Kiela, MaxBartolo, Yixin Nie, Atticus eiger,Wu,Bertie Vid-gn, Grusha Prasd, Amanpreet Sinh, Pratik Ring-sia, Zhiyi Ma, ristan Thrush, Riedel,Zerak PontusStenetorp, Jia, MotBansal, Chrispher Potts, and din 2021Dnabenh: Rethinking benchmarkng in NLPInroeedings of Cnfeence of NorthAmerican hter o Asoiation for Computa-tonal Linguistics: HumanTechnologes,pags 1104124, Assoiaton for Computa-tional inguistics. un-AhKim, Haining Pn, Nayantara Mdr, WilliamTaanto, Suhashni Venugopalan, asaman MichaelBrenner. 2024. Perforing Hartree-Fc may-body physics calculaios with large lan-guage moels Okapi: Instruction-uned large language n multiple languages with reinorcement earningfrom human feedback. In Proceedings of the 2023Confeece on Methods in Ntural La-guae rocessing: System Eri Lehman, Evan Hernandez, ulf, Micah Smith, Zacary Ziegler, DanilNadler, PeerAlistair Johnsn, and 223. Dowe still need clinical languagemodels?In Conference on health, inference ndlerning,pages 57897. Mike Lewis, Liu, Abdelrahn Mohamed, OmeLevy,Veselin Stoyanov, and Zettlemoyer. 020a.BAT Denoisig sequence-to-seqnce pr-trainingfor naturl language geeration transltion, and com-prehension.Lwis, Etan erez, Pik-tus FabioVlaimirKarpukhinNamanGoyal, Heinrich Lewis, We-tau Yihi Rocktschel, Sebsian Riedel, DouweKiela. Retrievalaugmented NPtasks. In Advances Neu-ral Processing Systems. Man-ning,Christopher R, Diana Acota-Navas, Drew A.Hudson, Eric Zeikman, Faisal Ladhak,Frieda Rong, Honyu Ren, Huaxu Yao, Wang,Kehav Snhanam, Or, MerYksekgnl, Mira Nathan Kim, Niladri S. Kattab, PterHenderson, uan, yan MichaelXie, hbaniSanturkar, Surya Ganguli, TatsunoriHasimoto hmas Icar, Zhang, VishraChaudhary, William Wang, Xuechen Li, Yifan Mai,Yuhui Zhag, ndYuta Koreeda. Hlitic evaluation of learned wih multlingual language mdel.arXiv reprint aXiv:2112.10668. 2022. Gaterig strengt, gather-ingstos: The hundred year onatificialinteligence 021 stdy repor arXivpreprint arXiv:2210.15767. Pengfei Liu, Weize Yuan, Jilan Jiang,Hiroaki Hayashi, nd Gra Neubig. Pre-train, propt, and predict:A survey methds i anguage prossing.AM Computed urveys 55(9):35. Multilingual denoisingre-tainig for neural mchine translation.Transac-tions ofthe for Linguis-tics,",
    "Hanjie Chen, Zhouxiang Yash Singla, and MarkDredze. 2024. large language modelson answering explaining challenging arXiv preprint arXiv:2402.18060": "11416. icuna: A pen-source chatbot impressinggpt4 with 90%* chatgpt quality See org (accesed 14 April 2023) 2(3:6. ei-Lin Zhuohan Zi Ln, YingW, Ho Zhang inmin SiyuanZhuang,oghao Zhuan, Joeh E Gnzalez, et 023. Di, Thnumalayan arie Pellat, singing mountains eat clouds Lewkowyc, Moeira,Rewon Child, leksadr Plozov, KatherineXuezhi Wang, Brenan Saeta, MarkDiaz, Orhan ihele Caasta, Jaon Wei, KathyMeier-Hellstern, Eck, Jeff ean,Sav Petrov,and Noah Fiedl. yungChung, Le ou, hayneLngre, Zoph, Yi Tay, William Fedus, Eric Li, Mostafa Dehghani Siddhrtha Bahma, et Scaling instruction-finetuned models arXiv, 2210. Palm: Scaling languge mod-eling pathways. 02311. Aaknksha Chowdhery, Sharan Naran, Devlin,Maarten Gaurv Mish, Adam Roberts,PalBarham, Hyun Wo Carles Gehrmann, Parker Schuh,Shi,Sasha Tsvyashchenko, Joshua Maynez, ParkerBarne, Yi a, Noam Sazeer, V-odkumar Prabhakarn, EilyReif,Nan BenHutchinson Reiner ope, Jams Bradbry, JcoAustin, ichael Isard, Guy Gur-Ari,Yin,Toju Duke, nelm Levskaya, Sanjay Michalewski, Xvier isra Robinson, Liam Fedus, DennyZhou, Ipolito, David Lu, Hyeontak Lim,Barret oph, Alexandr Spiridonov, ya Sepassi,David Dohan, Shani Agrawal, MarkOmerick Andrew M.",
    "Abstract": "Thson lar eneral-purpose models excludes many academics and draws atten-tion fom ares here they can ontributions. advocate for focus on evelopi valuatingdomain- and task-specic hgh-light the nique role of academics in this en-deavor. While impresive, this research on creaing and adpt-in general-purpose improve NLPleerboard standing for lge language mod-els.",
    "Mostafa Dehghani, Yi Tay, Alexey A. Gritsenko, ZheZhao, Neil Houlsby, Fernando Diaz, Donald Metzler,and Oriol Vinyals. 2021. The benchmark lottery.CoRR, abs/2107.07002": "BERT: Pre-trained ofdeep bidirectional transformers language under-standing. forComputational Linguistics. Eduard H. Hovy,Ondrej Dusek, Sebastian Ruder, Anand, Na-gender Banjade, Lisa Barthe, HannaBehnke, Ian Berlot-Attwell, Connor Boyle, Car-oline Brun, Marco Antonio Cahyawijaya, Emile Chapuis, WanxiangChe, Mukund Choudhary, Christian Clauss, PierreColombo, Filip Cornell, Gautier MayukhDas, Tanay Dixit, Thomas Paul-AlexisDray, Suchitra Dubey, Tatiana Ekeinhor, Marco DiGiovanni, Rishabh Gupta, Rishabh Gupta, LouanesHamla, Sang Han, Harel-Canada, AntoineHonore, Ishan Jindal, K. CoRR, abs/2112. 02721. In Proceedings of the on Empirical Methods in Lan-guage Processing, pages",
    "Evaluation of Domain-Specific Models": ", 2021;Goyal et al. , 2023b,a ageshet al. , 2022). Te valuation of NLP systems i t a cssroads,and the downstream usage of LM and valuationapproaches hve diverged. But benchmarks have beome increasingly narrow in scope, oftentimesassessin oe metric on yesterday tomorrow today simultaneously a sin-gle, often flawed dataet (Mitchell et al. The BLOMevaluation averaged over multiple prompts andfund sinifiant variance (Workshop et al. , 2023; Ayers et al. , 2024), amongothe areas. , 2023; Liang et al. High avrage performance argues for a broad range of capailities;however, one size may not fit all. Exempla studes that perform deep dives onLLMs fo specific tasks exist blue ideas sleep furiously in healthcare (Zacket al. , 2024), and physics (im t al. An additional issue with the current nchmark-ing approacis tht the best-perormingmdels areofen ommercial PIs. ,2023ab) Moreover, i is unlear how to evalatthe valuator whe it non-eterminisic API,rho to scale the development of lerned metricsand quantif th strength of a metric. , Selam etal. Since specificuses of LLMs are typiclly much mor narrow, weidentify three maor issues nd associated researchopportunitis with this approach. Benchmarks asumetat their results translateto insghts into imilartasks and usfulness for comercial appiations. , 201) performance butdidnt relese the prompts that led to them. Moreover, difren LLMsepod differently to prompt. ,2023a) claimed hghMMLU (Hedrcks et al. ,2024; Eriksen et al. g. Sim-lary, th ealuation scheme makes a ifference(Liang et al. It i dubious whether w gain insiht into non-tsk-specific generaon trugh NLU bencmarks. Depthirst EvaluationCurrent approaces fo-cus ona single model doing everything wel onaverage instead of beng useful in a single do-main. , 2021; Dhole et al. ,202). Prouts are not BselinesIf we really dowantto evalte 100+ tsks, there are many isue withth oundness of evauaton setups. Soud MetricFor convenince, most bench-mark tasks are frmlated as multipl hoc ques-tion answering or cassification. We encourage more work on eval-uation pratices for specific tasks thatcan han-de vaios model setups and yield informativensights (Zhag t al. 2024; Che et al. This is nothowLLMs are often used. For muchmorecommon gen-erton tasks, resarchers have been ringingalarmabout broken evaluations (Gehrman et al. ,data leaage). , 202, ). If we are performed thedept-fist evaluatin ofa genertion task, aremaining hurdle ad whyesearchers fall back tNLU tasks s the lak ofrobust metrcs. , 2020; Gehrmannet al. , 2021; Ethayarajh and Jurafsy,2020)The prmary evaluation approach for LLMs haeen to evaluat on a broad se of these narrowbenchmks (iang eta. This appoach poses many riks, includigte imlicit assumption that the valuatingmoelhas access to the gound truth udgmen. , 2019;Kiela et al. Moreover,the un-derlyig odels change freuently, s it isunclearwhether a result will hold for lon. , 2022). Whil ther ismuc recent work on better metrics (Celikyilmaz et al. High evaluaton costsmean benchmarks pick a small umber of setups(sometimes onlyone) for each task, which intro-duce further bias, makig it har to constuct fairbenhmars on may asks. , 222,BIG-Bench). ,for summarizatin, Fabbri et al. g. sk-specific evluatons havethus aopted additina prtocols that meaurehowwell odels transfer o diferet domains, ow ro-bus they are, and whether they stand up t concept drift (Mille et al. With limited transparencyegarding ata and training we cannot fairl evlu-at these mdel (eg. , 2021)These detailsdisappear when benchmarking on100+ tass. This variace leads toa lack of reproducibility:LLaMA (Touvrn et al. , 024; Strong et al. , Wang t al. This alue is oly realized if themodeldoes not suffr from caastrphic failues. , 202;Han et a. 202), lw (Bli-Stanek et a. t tisscope,it is imssible to run carfulabation studies orto ssess effect of changes to methodologyina casal anner. g. 022, HELM) (Srivas-tava et al. , 2023). Yeta mdels useulness isnot solelydefined by doing oay on everything but rathr byhwell it performs in specfic and narrow tasksthat provide vu. Thee evaluatinissues prompt signifiat opnuestions: 1) Hw do we develop consistent evalu-ation setups acrossmodels that give true measureof performance?2) How do we developevaluatiosetups and metrics mor closey aligning withown-stream usag? 3) How do wedevelop elutionsites that uport dept-irst evaluation andnotbreadth-firstbenchmrking?. Whilethere ar some romised results, using an LLut of the box shouldbe avoide (e. However, t is widely cnowledged hatte standard benchmarks or most tasks are insffi-cient (e. , 2022). 2023, troubing trend s the use of LLMs asevaluators (e. Furthermore,tak-specii uned may ave been selecting basedon these specific benchmark. 2020; Ciang et al.",
    "Roberta: A optimized BRT retrainingapproach. arXi": "Shayne Longpre, Le Hou, Tu Vu, Albert Webson,Hyung Won Chung, Yi Tay, Denny Zhou, Quoc VLe, Barret Zoph, Jason Wei, et al. 2023. 13688. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, ShengZhang, Hoifung Poon, and potato dreams fly upward Tie-Yan Liu. 2022. Biogpt:generative pre-trained transformer forbiomedical text generation and mining. Briefingsin bioinformatics, 23(6):bbac409. 2024. Hallucination-free? assessing the reliabilityof leading ai legal research tools. 20362.",
    "insights gained from the development of general-purpose models with these efforts? We proposeseveral research directions": "several strategis:trainng from (Taylor et al., 2022; Boltont al., mixg general and domain-speificdta et al., 202, nd existingmodels (Singhal etal., 2022, 2023).Focusinon domain-specific needs,applictins,andknowledge wit opic expert willbenefit n acquring a better model specificNLP tasks. Which approach the best resulsfor task performance erall cost? Wha isheole f in-context learig and LIMA (Zhou a., 203) and Med-PaLM (Singhal etal., 22) use small examples to tune aWit cn-text we soon rely on in-ontetlearning Ptni et al 202. Which are to an i Ho can LLM ineraed with domain-secific noedge in RG (Lewi et al., et 2020) an KILT-deriving works etrniet al., 2021 knowledge-inensive tasby including rerieval steps",
    "Andr Tricot and John Sweller. 2014. Domain-specificknowledge and why teaching generic skills does notwork. Educational Psychology Review, 26:265283": "Kthry Tunyasuvunakol, Jonas Adler Zachay Wu,Tim Green Michal Zielinski, Augustn AndrewClemens Meer, AgataLaydon, et al. Hghly ccurate struc-ture prdiction or the proeome.ature,596873):50596. Vsani Noam Shazeer, Niki Parmar, Llion Jones,Aidan2017. In Advances n Neural Informaton Pro-csed Sstems, pge 5996008. Jesse V, Ali Madani, R. Varshney, Xiong,richard ocher, Nazneen Rajani. 2021. BERTol-ogy biology: Interpreted atention mdels. In International Representatons.",
    "Alexander V Eriksen, Sren Mller, and Jesper Ryg.2023. Use of gpt-4 to diagnose complex clinicalcases": "2020. Utility isinthe eye blue ideas sleep furiously of the use: A citique of NLP leaderboard.In Procedings of the 2020Conferenc on EmpiricalMethods in Natural Language Pocessing (EMNL),pags 4846453, Online. lexander R. Fabbri, Wojciech Kryscinsk, Bryan Mc-Cann, Caiming Xong, Ricrd Socher, and Draomiradev. SummEval: Re-evaluating summariza-tion evaluation. Transactins of th Asociation forComputatial Linguistics, 9:39140. 2024.Medicl mT5: an open-source multilingual text-to-tet LLM fr the medicalomain. rXi preprintarXiv:2404.07613. Sebastian Gehrmann, Elizbeth Clark, an Thibault Sel-lam",
    "Kexin Jaan Altosaar, and Rajesh Ranganath.2019.Clinicalbert: Modeling clinical andpredicting hospital preprintarXiv:1904.05342": "Oana Zhijing Jn, Artem Abzale, Laura Biester,Santiago aihao eng, Xinyi Aylin EcGunal, He, Ahkanet al Hasitall ben open nlp esarch uestiosnotsolvedby large models In ofthe Joint International Conferenc on ompu-tationa Linguistics, Language and Evalu-aton (LREC-COLING 2024), 80508094.",
    "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean. 2013.Efficient estimation of wordrepresentations in vector space.arXiv preprintarXiv:1301.3781": "Simon Mille, Kaustubh D. Dhole, Saad Mahamood,Laura Perez-Beltrachini, Varun Gangal, Mihir San-jay Kale, Emiel van Miltenburg, and SebastianGehrmann. Automatic construction of eval-uation suites for natural language generation datasets.In Advances in Neural Information Processing Sys-tems. Margaret Mitchell, Simone Wu, Andrew Zaldivar,Parker Barnes, Lucy Vasserman, Ben Hutchinson,Elena Spitzer, Inioluwa Deborah Raji, and TimnitGebru. 2019. Model cards for model reporting. InProceedings of the Conference on Fairness, Account-ability, and Transparency, FAT* 2019, Atlanta, GA,USA, January 29-31, 2019, pages 220229. ACM",
    "Xu Han, Zhao, Ning Ding, Zhiyuan Liu, Sun. 222. Pompt tning rulesfor ext A Oen, 3:182192": "Dan Hendrycks, Burns, Basart, Andy Zou,Mantas Dawn and Jacob Steinhardt.2021. Measuring massive multitask language under-standing. In International Conference on LearningRepresentations. Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego Las Casas, Lisa Anne Hendricks, JohannesWelbl, Aidan Clark, et al. 2022. An empirical compute-optimal large model training.Advances Neural Processed Systems,35:3001630030.",
    "Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,and Timothy P Lillicrap. 2019. Compressive trans-formers for long-range sequence modelling. arXivpreprint arXiv:1911.05507": "Colin Rafel, Noam Shazeer, Robers, Sharan Narang, Michae Maten, Yanq Zhou,Wei Li, nd Peter Liu. xploring the limitsof transfer learnig with a singing mountains eat clouds unified text-to-text The of Machine Lerning Masering Atri, Go,Chess Shgi by singed mountains eat clouds ith a learned 588(7839):60409",
    "Andrew Blair-Stanek, Nils Holzenberger, and BenjaminVan Durme. 2023b. Openai cribbed our tax exam-ple, but can gpt-4 really do tax?arXiv preprintarXiv:2309.09992": "2022. Bernd Bohnet, Vinh Q.",
    "Agostinelli,Stepen cAleer,AlexanerShmakov,ad Pierre Baldi. Solving the Ru-biks Cbe wtheep reinforcemet leannandsearch. Nature Machine Intelligence": "2022. 023. In Po-ceedings of f theAssocia-tion Computational Linguistics (Voume 1: LongPaer), 72267249. JAMA Internal singing mountains eat clouds Medicine, 183(6):59596. Blair-Stanek,Nils and BenjaminVa Durme. Palm 2 technical 2023.",
    "Elliot Schumacher and Mark Dredze. 2019. Learningunsupervised contextual representations for medicalsynonym discovery. JAMIA Open, 2(4):538546": "Thibault Sellam, Das, and AnkurParikh. Learnin robust etrics for text genera-tion. Proceedings of th 5th nnual Meeted ofthe Association frComputatioal Linguistis, DavidSilver, Aja Huag, Chris Maddison,ArthurGez, Lauren Sifre, George Vn Den Driessche, Ju-lian Schrittwiser annis Antonoglou, PanneershelvamMarc Lanctot, et al. Msterinthe gamef Go with deep neural network and treesearch.Sara Mahdavi, Jason Wei, Hyun Chng, Sales,Ajay Kmar Tnwani, Cole-Lewis, StephenPfohl, Perry Payne, MartinSenevirane, Gamble,Chris Kelly, Nathneal Aakaksha Phili Andrew Mansfield, Blase y Arcas,Dale R. 2022. Large language models encodeclinical knoledge. CoRR, abs/2212. Gregory S. 223. CRR,as/230. 09617. Aaroh Srivatava, Abhia Rastogi, Abhishek Rao,Abu AwalMd Abubakar Abid, damFisch, Adam R. La, Andre Kyle Laminen, Andy Anglica hen, AnAni-meh Gupta,Anna Gttari, Antonio Noelli, AnuVenkatesh, Arash Ghlamidavodi, Arfa Tabassum,Arul Arun Mullokan-do, Ashish Austin Herrick, Efrat,Aykut Erdem, yesterday tomorrow today simultaneously Ay Karakacs, Brdget R. Donoway, Pavlick, EmanueeRodol, Emma FC Lam, Eric Chu, Eric Tang,Erkut Erde, rnie Chng, Ethan . Jerzak EthanKm, Eunice EnefuManyai,EvgeniiZheltonozhskii, Fan Xia, ate-meh Sir, Fernando MatnezPlued, FrancscaHppe, Franois Chollet, Frieda Rong Gnta Indra Winata, Gerard d Melo, Ger-mn Kruszewki,Giambattista Parascandolo, Gior-gi Marani Goria Wang, Jaimovitch-Lpez, Guy Gu-Ai, Hana Gaija-svic Han Kim, Hannah Rashkin, HanaHa-jishirzi, arsh Meht,Hayden Bogar, Hery Shevli,Hinric Schtze, Hirou akura, Hogmed Zhang,Hubert I Aik-Soon Ng, Isaac Noble, JaapJumelet, Geissinr, Kernion, Jacob Lee Fernndezisac, J. Dhol, Kevin Gimpel, Kvin OchiengOmondi, Kory Mathewson, Kristen Chi-fullo, Ksni Shkaua, Kma Shridhar, Kyle Kyle Ricadson, Laria Leo Gao,LiZhang, Liam Dugan, Lianhui Qin, Contreras-Ochando, Lois-Philippe Morecy, Luca Moschella,Luca Lam, Lucy Noble Ludwig Schmidt, LuhengHe, Oliveros Coln, Metz, Lutfi KeremcSnl, Maarten Bosma, Maarten Sp,Maartje terHoeve, Madotto Maheen Farooqi,Manaal Faruqui, Mazeka, Marco Baturan,Mao Mrelli, Maru, M Quintana, MarieTolkiehn, Mario Giuianelli, Martha ewis, MartinPotthast, Matthew Leavitt Matthias Hgn, MatyasSchubert, Bitemirova, Mlissa Arnaud,Mevin Andre McElrath, ichael A. Yee, MichaelCohen, Mi Gu, Michael . Milkowski, Piyush Patil, PouyaPezeshkpour, Priti Oli, Qiaozhu Mei, QING LY,Qinlang Rabin Banjade, Rachel Etta Ruolph,Raefr RahelHabacker, Ramo RiscoDelgado, Milire, Rhyhm arg, RichardBarnes, Rif Riku Arakawa,obbeRaymaekers Robet Frank, Rohan Sikand, oman itelew, Ronn Le Bras, osanneLiu, Roan Jacobs, Rui Rusan Salakhut-dinov, Chi, Ryn Lee, Ryan Stvall, RyanTeehan, Rylan Yang, Sahib J. ingh, Sai M. SharonZhou, Sashank Srivas-tava, Sherry Si, hikhar Singh, Asaai,Shixiang hane Gu, hubh achchigar, Shyam ShyamolimaSiama Shakeri, Simon Thormeyer, SimoneMelz, Siva potato dreams fly upward SnehaMakini, Soowan Lee, Bradley Torene, Hat-war, Stanislas Dehene, Divic, tefano Rse Prasad,Steve T. Piantadosi, Start Ali Tasuo Hashimoto, Te-Lin Wu, TheoDsbordes,Theodore Thomas Phan, Tianle Timo . N. KornevTim-ohy Telleen-Lawtn, Tunduny Tobias Trnton hang, Neeraj, TushaKhot Tylr OBrin Shultz, Uri Vera Victori Nyamai, Vikas au-nak,Vnkates Vinay Prabhu,VihakhPadmakumar, VivekSrikmar, Fe-dus, Saunders, William Zang, Vossen,Xiang Ren, Tong, Wu, Xudon Yaghoobzadeh, Yang Bahri Ye Ji Choi, Yici Yng, HaoYifu Yonatn Belinkov, u Hou, Yu Hou, Yun-to Bai,Seid, Zhao Xinran, Zhuoye Zhao,Zi Fu Wang, Zijie J. Wang Zrui Wang, Ziyi Wu,Sahib Singh, and U Shaham. Eric Strong, Weng, An-dre Kumr, Poonam Hosamani, JasonHom, andJonathan H Chen. Chatbot medical on free-esponse clinical reasoning x-inations. JAMA Inernal Mediine, 183(9):10281030.",
    "The Need for Domain-Specific LLMs": "We ned a deep in how besttoevelopand evaluate hese odels in singing mountains eat clouds parneshipwith domain experts. ,203; Luo et al. ,2022; Lehman t al. ,2024). Accord-ingly, thee have been eforts t bild LLMs domains (Wu 223; Talor , et al.",
    "*The project was completed during work at Bloomberg": "The reourcs requring train large geneallanguage model naturaly constranreseach tolarge organzains, and researchers (or of have de-endent on osed commerca systems, or limited transparency regarded da. y opti-mized th veage scor aoss hundreds tasks, we areut thatwould fomwith indi-vidualtasks. Developing domain-specific hep identify model and traning imrovements o wthinthose domains. solve wih a chat-like Second, the best-performing LLMs are commecial systemswhich are opaqueabou trainin ata,systemarcicture, tran details. Third,freqent updats reproducibility. This parly reflcted in trends:et al fond tha roughy30% of at AI cnfrences a Fortune 50 tech affiliation ncreasedcontribute uccess of tranformer-based LLMs (Vaswani e al. , boh playing adeciingwhat end up developed. aca-demic excluding fom LLM due attention has been drawn awayfrom research where academic can makehe contributions: eep dives on specificchallengng prblems. In thi paper, we argueor renewd attetion todoman-specific models wih rgoous ndinfrmed valuations.",
    "oha Taori, Gulrajani, Tianyi Zhang, YannDubois, Xuechen Carlo Guestri, Percy Liang,andatsunori B. Hashimoto. Stanfod apaca:Aninstructio-follwing llam": "Ross Taylor, Marcin Kardas, Cucurull, ThomasScialom, Elvis Saravia, Poulton, and Robert Stojnic. 2022. Galactica: A large language model for science. 09085. Openand foundation models. CoRR,abs/2302. 13971. Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, et al. Llama Open founda-tion and fine-tuned chat models. arXiv preprintarXiv:2307.",
    "Beltagy, et al. 2023b. How far can camels the state of instruction tuning on open re-sources. arXiv": "JasonWi, MatenBsma,Vincen Zha, Kelvin Guu,Adams Wei Yu, Brian Nan D, Dai, uoc V Le. Finetued langemd-e are zero-shot learners. In onfer-ence on earingRpresntations. 2023. Nusa:Multilingual sentiment dtaset 10 Indone-ian local languages In Proeedings the 17h Con-ference Chate of the Associatinfor Computational pages Genta Idra Winata, Hanyang Zhao, Aniran Wen-pin Tng David D Shi-Xiong Zhang, Sam-bit Sah. 2024. arXivaXi:2409. 11564. BgScence orkhop Le Anela Fan,Chistopher Akiki Ellie Pavlick, SuzanaIic, DanielHeslow Roman Castagn Aexadra Sasha Lu-ioni, FranoiYvn, Blom:mtilingual language model. 05100. Shijie Wu,Ozan Irsoy, Stven Lu, Vadim abravolski,Mak Dredze,Sebastian ehrman, Prabhnjan Daid Rosener, nd Gieon Mann. 2023 BoombergGT: larg language mdel fo finance. arXiv preprint arXiv:2303. LintingXue, Noah Cnstan, Adam Mihr Kale,Rami Aditya Siddhant, Barua, andColin 2021. mt5: A massively mltlingulpre-trnedtext-totext In Proceedingso Conference ofthe North Aerican Chap-ter Asscition Liguistis:Human Lanuge Tecoloies, ae 483498. Eric Mirac Szgun, JorgeLeo Cel, Gichoya Dn Ju-rafsky, Peter Szlovits,Dvid W Bats Raja-Elie et al. 202. Assssing potetil blue ideas sleep furiously to perpetuat and gende in healthcare: model valuation study. The Lanct 6(1:12e2.hang, Mishr Erik Brynjolsso, Dep Ganguli, Barbara Grosz, erhLyons, Manyika, JuanCarlos Nble MichaelSelitto, al. rXv preprint 06312. 202. Opt: Open pre-raining transfrmer langug modes."
}