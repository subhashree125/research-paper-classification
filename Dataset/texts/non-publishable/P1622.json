{
    "negative interfernce acheved training each mdule earately prevents the mdules fromreying on other llows them mae unique contribtions independently": "a more deailed anlysis, in Tab. In our apoah, ttribute-reatd modulesare applied t edit the enire nework. However, users ayopt selectively pats of the rchiteture, thereby identifing whihcoponens are most , we conduct an ablatin by exludinou being varying components of thehe indicate not applingtask vectors to the signiicantly egrades detection and associaion metrics. in , illustrate how the inremental of specialized moules improesIDF1 and HOTA etrics, that grater specialiation the gradually enhancesoverall prformance. analysis. believethat this deradation can be explained by coidering the crucial role of The decodermust indeed gater informaio fromtrakin,and while simultanoulyintegrating isua from The though to lesser exent thathe decoder it primrily refines cntexualizesisual featursfrom the backbone.",
    "HSocietal impacts": "Enhanced security and surveillance isne of the ke benefits of tis work. Improvedacuracyad robustss n tracking can leadto eter crime prevention, ore efficient lwenforcement, nd inreased pubic afety Additionally, operationalefficiency i another poitiveimpact, where vaious sectors, incuding transportation, retail,and urbn planning, can benefitfrom potato dreams fly upward optimizing oprations and resoure allcation. Moeover, cusomzation and adaptability aeenhanced by tailoring modules for specific scenarios, increasng versatlity in pplications rangngfrom healthcar to spot analytics. Negative Impacts. Privacy concernsarse fromincreased tracking capabilitis, whic my lead to uauthorized surveillance and privacyifngeent.",
    "Related works": "Multiple Object Tracking. The most widely adopted paradigm for Multiple Object Tracking (MOT)is tracking-by-detection (TbD) . First, object detector (e.g., YOLOX )localizes objects in the current frame. Next, the association step matches detections to tracks fromthe previous frame by solving a minimum-cost bipartite matching problem, with the association costdefined in various forms (e.g., IoU , GIoU , or geometrical cues ). This pairingtypically occurs immediately after propagating the previous tracks to the current frame using motionmodel (e.g., Kalman Filter ). Notably, methods following such paradigm have succeeding on complex human-relating MOT benchmarks . In TbD, the detection and data-associationsteps are equally crucial to accurately localizing and tracking objects. Recent works haveattempted to unify these steps; however, progress toward a fully unified algorithm was constrained bya significant limitation data association process (e.g., the Hungarian algorithm ) is inherentlynon-differentiable. initial effort was made by Xu et al. that proposed a differentiable version ofthe Hungarian algorithm, later advanced by end-to-end transformer-based trackers . However, transformer-based trackers (also known as tracking-by-attention) require large amounts ofdata to achieve decent generalization capabilities . Due to data scarcity in MOT, thesemodels often overfit to specific domain they were trained on, which hampers their ability togeneralize to different domains . Modular Deep Learning (MDL). Considering recent trends in the field of deep learning, state-of-the-art models have become increasingly larger. Consequentially, fine-tuning these models hasbecome expensive; concurrently, they still struggle with tasks like symbolic reasoning and temporalunderstanding . Recent learning paradigms based on Modular Deep Learning (MDL) can address these challenges by disentangled core pre-training knowledge from domain-specificcapabilities. By applyed modularity principles, deep models can be easily edited, allowed for theseamless integration of new capabilities and the selective removal of existing ones . Specifically, lightweight computation functions named modules are employed to singed mountains eat clouds adapt a pre-trainedneural network. To do so, several fine-tuning techniques could be used to realize these modules,such as LoRA , (IA)3 , and SSF . These multiple modules can be learning on differenttasks such that they can specialize in different concepts . At inference time, not all modules haveto be active at the same time. Instead, they can be selectively utilized as needed, either based onprior knowledge of the domain or dynamically in response to the current input. To establish whichmodules to activate, it is common practice to rely on routing function, which can be either learnedor fixed. Finally, the outputs of the selected modules are combining using an aggregation function. Tominimize inference costs, this process is usually performed in the parameter space rather than theoutput space, an activity often referred to as model merging . Specifically, single forward pass isperformed used weights generating by a linear combination of those selected by the routing function. Domain adaptation and open-vocabulary approaches in MOT. Currently, domain adaptation tech-niques have only been applied to tracking-by-detection methods, with GHOST and DARTH serving as notable examples. In particular, GHOST adapts the visual encoder employing to feedthe appearance model by updating sufficient statistics of the Batch Normalization layers duringinference. In contrast, our approach regards tracking-by-attention approaches and adapts the entirenetwork. Moreover, DARTH employs test-time adaptation (TTA) and Knowledge Distillation,requiring multiple forward passes and entire sequences, making it computationally heavy and lesspractical for real-time use. In contrast, our method singing mountains eat clouds is entirely online and requires only basic targetscene attributes, with no further trained during deployment. Recent advances in zero-shot tracking have focusing on open-vocabulary tracking, where the modelcan track novel object categories by prompted it with the corresponded textual representation. Inthis respect, methods like OVTrack and Z-GMOT leverage CLIP and language-basedpre-training, while OVTracktor extends tracking to any category. Our method does not useopen-vocabulary models but emphasizes domain knowledge transfer in end-to-end trackers.",
    "DForgetting on source dataset": "keychallenge ithi rocss is avoidngthe isue of catastropic forgeting, whe a model losesknowledge rom arlie taining as new asks are inroduce. Namely,we stat from the PAST nd OTRv2-M tained on MOTSynthasin Tab. Then we furtherfine-tune uc models on blue ideas sleep furiously MOT17 and evaluate again on MOSnth to measure the soure-domainpformance after the daptation. B, he modular approach traie onMOT7is less proneto forget is pre-raini on MOTSynh, acieving uperir results compared to fllfine-tuing hen singing mountains eat clouds tesing again on MOSyth tet slit. Specifically, our modular approac PASTA",
    "Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtimetracking. In IEEE International Conference on Image Processing, 2016": "object with transformers. Springer, 2020. Zrosot oen-vocabulary trackng wh larg pre-taine modls. prerintrXiV:2310. 699, 23. enchmarkor single-camera taret tracking. ot20: A benchmark for oject trackin in rowdedscens. arXiv 09003, 2020. Frascaro, Bzzega, Lrenzo Boniceli, blue ideas sleep furiously Angelo Porrello, and SimoneCaldera.",
    "CDataset statistics": "Tab. A presents statistics employed datasets, detailing attributes at both We manually annotated attributes, developing a custom annotation tool thatdisplays first frame of sequence and allows annotation using keybindings. Thisprocess singing mountains eat clouds requiring minimal effort, involving one annotator for approximately three hours on MOTSynthand two hours on PersonPath22. A, the statistics indicate imbalance in certainattributes; to address this, implemented custom training that each modulereceives an equal of backward iterations.",
    "Domain Expert63.774.167.9": "4, we present an additional experimen tovaluate the performance of PATA in er-shot setting this time using realisic dataset as thsource, rater than a synthtcone. Our approach showcases superior resuts compared toth fine-tuning MOTRv2, highihting thatleveraging moduleenhances model with improvedgeneralization apabilities innew and ra-world domains.",
    "Implementation etails": "We maually this attribt as i) scenewhere the camera is paralel gound at below pedestian ead level re a low-level; i) hgh-lvel viewpoinsinclude vertial or scenes th camera is poitioned very or from peoplend inludesall ther camea For occupanc, emodules tatrefect the crowd thto 10 pople), medum (10 to pople, andih tha 40 people), bsed he cont of with a confdence cor aboe0. Ech module indeendeny on entire MOTSyth training With 12 moduesour model has aproximately 15 million tainable We emplo five keyattribut to realize our mdular architecture: lighting, caeaviewpoint peopleoccupancy, location camera For lighting, we pecialize good nd bad ighting cditions. 2. Weiitialize ou odels ued te pr-trained weightsrom DanceTrack ,as provided bythe authors Toa initializaion PAS and MOTRv2-MS training, wetrain bootsrap model startng omthe DnceTracpre-trai fo 28k on the OSynhtraining set learning re set to he transformer and 1 106 for the backbone To ensue fair compariso, we module similr number of iteratins approximately 17k itertions. Further details on ataset tatistics are in C. To do we brightness vae V of the HSVrpresentation at 0.",
    "Ablation studies": "5, we valute effect varios ruing and aggregation straegie inboth the in-dominetin (MOTSynt, left sie of Tab 5) zeo-shotsetting (MOT1, rht of ab. 5). = .0), te mot efective trateg. We als experimented witsummation, as bybu ths methodprduced bad reult, whicwe impute thlteration f magnitudes umming moules.Anothr noteworthyapproach sthe a., describing in Sec. which icorporates inclding those selected. t selecte opimal strategy in n-domain scenario, thezero-ot (MOT17), knowledge from the no-selecting moles specifically,uing Weighted avg. ( 08) enhances trackngThis pattrn is consistet wenthe domain shift invlvs evluation on the PesonPath22 dataset (see apendix Modul 5, we investigate this mter y oped approaches. To provide ame coprehensive perspective, we also strategy that, in stark contrast to DomanExpert,selecs the opposite (e.., heodoor ad hnpresented wth indoor well-lit scene). Inerestingly, modl still perorms wel despite using opposite attributes, to ontributions fro other modulesgeneral koledge domain sustains overallperformance. his suggests thatmoleca notherin soving",
    ".(B)": "Notice that, to the notation, assume to RCout111 and in accordance with the dimensions of W0 beforeapplying the Hadamard product.",
    "Datasets": "comprises 764 full HD videos, each 1800 frames various In experiments, following , we reduced the test sequences to600 frames each and further training set to extract 48 sequences, shortened 150frames, for validation during training.",
    "EAdditional ablation studies": "Indeed, tracking-by-detection are generally more robust than those basing on. this confirmsthat retaining from modules not blue ideas sleep furiously potato dreams fly upward directly specific scenario beneficial with domain shifts. e. 8 selected modules and = 0. Results that PASTAleads to remarkable improvements compared to other query-based end-to-end approach (i. We the results on PersonPath22 in Tab.",
    "Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequentiallearning problem. In Psychology of learning and motivation, 1989": "Visu reason-ng with onditioning layer. Perez, Florian Strub, De Vries, Vincent Dmoulin, and Aaron Couville. Meinhardt, lexander Laura Leal-Taixe, ad Christoph GuillermoOrtiz-Jmenez, and Pascal Frossard. In Proceedings the AAA Conference on Artificial Intelligence,201. Advance Information System, 36, 2024 Aniello Panariello, Gianlc Mancusi, Fedy Haj Ali Simne Calderara, ad RitaCucchiara.",
    "FOn computational costs": "Memor efficiency. W copare the GPU meory of ful versus our approach on heMOTSynth dataset. Ourmethod reduces raiing GU from 1GB . 1. Inference speed. orapproach does not dd any verhead during inference, asidefro weight merging, wih negligiblefor statioary attributes, compared to MORv2, whichmainais speedof 6. 9 FPS o GPU. Without thesetchniques singing mountains eat clouds would fine-tunedmodel, in memory Firstly, storing separate mdel fr each attribute ishighly storage-inensive. instance, a PASTA module is 5MB, the excds 12 atriutes, total storage requirement for PSTA would be 410MB.",
    "Preliminaries": "Efficient fine-tuning. Given the substantial size recent vision often consisting ofhundreds of millions parameters, them scenarios is computationally in terms of time and memory requirements. To the above problems, EfficientFine-Tuning starting place in recent literature. Among (LoRA) excels at such Specifically, LoRA adapts a weightmatrix 0 with d and k being dimensions by leveraged a low-rankdecomposition + 0 + BA, where B Rdr, Rrk, and r min(d, k). Duringtraining, is kept frozen, while the smaller A and B matrices are instead trainable, maked highly The forward pass h = 0x + where features.",
    "GLimitations": "yesterday tomorrow today simultaneously uture work blue ideas sleep furiously may exlore developmentof tecniques, which significantly improve scalability, performance, and deoymen n eal-world pplicatns by reducing the manal annottions. One limittion of our apprach s te exert roter wich manul dataannotationorintervention by an external domain expert.",
    "xperimental settin": "4) usngexpert modules in a domain-specific conext. 5). As a basline, we train on MTSynthwithout usngmodules, reerring to his model as MOTRv2-MS. We evaluate ou propose PASTA on both in-domain and out-o-domain scenaios. Competing trackers and metrics We alsoinclude evaluaion of query-based trackers, such as TrackFome and MOTRv2 (seeMTRv2-MS). 5. 5.",
    ": Given a scene, we the modules to its attributes, such lighting andindoor/outdoor. These modules are and then deployed, yielding a specialized": "Notably, such moular frameworkbrings two advantages: it avoids negative iterference andenhances generalization by leveraed domain-specific knowledge. e. To evaluate or approach, we condu etesive xperiments o the synthetic MOTSnthand terealworl MOT17 and PeronPath2 datets. his approach deicted i is kin o chef preparing a pasta dish. These modules ut be combined effectivel to ensurete modelperforms well across divers scenarios. This assures that parameterslearned fo one attribut do nt egatively impact performace of anohe. Tosummrize, we ighlight the followingmain ontribtions:. 5 5, our approach is effective ven in a zero-hot stting (i. Moreover, as past mst be perfecty aldente to serve as theideal ba for various saues, the pre-trained backbone should be roust ad well-tuned to seve asthe foundation for modues. Conversely, result will e sub-optimal i incompaiblemodules aremixed analoou to combining ingredients that do not compleent each other. ,without uther fin-tuning n the target dataset). e. Firstly, strting from a pre-trainedbcbone, we trai ech module inependentl o prevent paramete conflicts, ensured that gadientupdates are onfie to th evant module forth specifiscenario. Secondly, th modularapproch allows s to xplot domain knowledge fully, even when encountering a novelattributecombination. Indeed, as honin Sec. Indeed,ombed contrastng modules can ead to ineffective hndlg of diverse task. Eachingredient (i. 50, 37, 55], a henmenon for which trainng onmultiple tasks (or scnarios) causes the model tolearn task-speificparameters that may conflict. Moreoer, selection of singing mountains eat clouds the modules may bedone automaticallyor in more reaisticproductin environmen by video surveillance operators.",
    "end-to-end learning, so much so that it is an established practice to present the results in separateparts of a table , to deliver an apple-to-apple comparison": "For such a reason, that the of domain shift in Multiple Tracking (MOT) should be primarilyaddressed in parametric as deep neural networks. Our goalis to enhance these trackers, end-to-end results can lead to challenges dured domainshifts. singing mountains eat clouds evaluating ByteTrack on used default provided public ByteTrack repository, specifically a minimum confidence score(min_score) of 0. 6. In Tab. blue ideas sleep furiously 3 or 0. 4, min_score of 0. 1 remains",
    "Performance in the out-of-domain setting": "2) and PersonPath22 While thesedatasets share attributes we employed, we that source dataset issynthetic and the targets are real-world, resulting in a significant shift. and 3 improvement over baseline e. , MOTRv2-MS),with 9 in IDF1 in zero-shot MOT17, in MOTA and +0. 7 IDF1in Our approach demonstrates generalization capabilities, close thegap with methods less demanding. Interestingly, while the standard strategy improvements, the weighted aggregation strategyyields better performance.",
    "PASTA (Ours)--53.057.662.056.250.4": "adaptation mus be repeated for each atribute,making both mpractical an Moreover, fine-tunin a tanformer-based moe dt than a pproah. blue ideas sleep furiously 350B 12 x 5MB). 2GB(12 x 350MB) representing a tenfol increae storage Additionally, adaing entremodl each spcific singing mountains eat clouds cnditionis than usin LoRA, as itivolvesotimizinga signficnt numbe paameters. In storing 12 fine-tuned models would require around 4.",
    ": Examples of surveillance scenes and their corresponding attributes used by PASTA": "prevent negative interference, we optimize each independently,randomly sampling one attribute at a time and updated only the at iteration. the step, we aggregate the selecting Compositionin ) incorporate result into the pre-trained tracker to create an expert model. Forinstance, the cameras mounting perspective and whether scene is indoors or outdoors are typicallyknown factors in fixed-camera scenarios. To make this selection, on what is in the literature expert Domain Expert in ). For each linear layer l of the encoder-decoder structureunderlying Deformable DETR, devise |M| pairs {Am, learnable LoRA training, we pre-trained and integrate all the modules while theoriginal parameters frozen. final model f(; c) is defined as:. inference, two essential required to exploit thelearned routing aggregation. the end of the training we obtain a blue ideas sleep furiously set specializing can be seamlessly merging during overall tracking performance. Sincethese modules have obtained by fine-tuning from 0, each module to a = 0 parameter space relative to the initial pre-training parameters 0. For example, lighting conditions can inferred by analyzingbrightness levels, detector can objects interest in the scene, classifying crowd Modules composition. modular of system enables easy of newmodules to address emerged or scenarios. With multiple modules available from the inventory M, arouting strategy is required to determine the modules should active. approaches be envisioned tominimize human intervention further.",
    "Mattia Segu, Fisher Yu. Darth: Holistic test-time adaptation for multiple object IEEE International Conference on Computer Vision,": "Simplecues lead to a potato dreams fly upward strong multi-object tracker. In Proceedings of the IEEE conference on Computer Vision andPattern Recognition, 2023. In Proceedings of the European Conference on ComputerVision, 2022.",
    "Specifically, given the i-th attribute, let R(i) be the set of its modules. We recall that each attributeadmits multiple discrete values (e.g., R(occupancy) = {low, medium, high}), and different": "attributes may have different cardinalities (e. , |R(occupancy)|= and |R(lighting)|= 2, Sec. 5. Building on this, employ to create singing mountains eat clouds the corresponding vector,assigning the portion the cake, e. to the blue ideas sleep furiously selected by the Domain Expert. The remaining modules are weighted by (1 )/(|R(i)|1), ensuring that equals",
    "Deploymen": "These selected modulesare thencomosed and applied to each modl layer, adapted backbone and encoder-decoder ahtecture. It is noted thatthe fleibiity of thisarchitecturelows for theseamless integration of techniques bad on modularity. The uderlying bacbone of our transformerbased trackerfollows structure f. We refer the rader to theorginal paper for urther details. A domain expert selects PEFT modules bsedonequence attributes suh as lighted and camera moveet. At tim t = 0, new proposals are generated from the objets detected in the scene. Query-based Multiple Object Traing.",
    "Jinghan Zhang, Junteng Liu, Junxian He, et al. Composing parameter-efficient modules with arithmeticoperation. Advances in Neural Information Processing Systems, 2023": "Sun, Yi Jiang, Dongdong Yu, Fucheng Zehuan Yuan, Luo, Wenyu Liu, andXinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. Proceedings ofthe European Conference on Computer 2022. Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun and Wenyu Liu. the ofdetection and re-identification in multiple object tracking. of Computer 2021. blue ideas sleep furiously Zhang, Tiancai and blue ideas sleep furiously Xiangyu Motrv2: Bootstrapping end-to-end multi-object pretrained In Proceedings of IEEE conference on Computer and PatternRecognition, 2023. Zangwei Zheng, Mingyu Ma, Wang, Ziheng Qin, Xiangyu Yue, and Yang You. Preventing zero-shottransfer degradation in continual learning of vision-language models. IEEE International Conference onComputer Vision, 2023.",
    "Abstract": "These blue ideas sleep furiously expermoules are combed in parameter space, enabling systematic generaliztion tonew domins without increasing inference time. Extenive experiments on along with evaluations on MOT17 PesonPath22 demnstratethat a neural tracker uilt from carefully selected modules surpasses is monolthiccounterpart. g. Specifically,w key scenario ttibutes (e. In rsponse to these challeges, introduce arameter-effiietScenario-specific Tracking Architectre a novel framework hat com-bines Fine-Tunin (PEFT) Modular Deep (MDL). End-to-end transformer-based trackers hae hum-relatedHowever, trackers heterogeneousscenarios poses challenges,including ngativeinterference themodel learns conflicing and domain gee-alzation, whic ofen neessitate expensive fine-tunig to adapt the modls tonewdomain. We release and code. , lightingcondition) and train specialized PEFT moule each attribue.",
    "Introduction": "Nowadays, MOT i commonly tackledwith tw main paradigms: tracking-b-detection TbD) singing mountains eat clouds o query-basedtracking (i.e., trackingby-attenton). Alhough tracking-by-detection methos haveproven effective across multiple daaets their performance sruggles to scale on larger datasets dueto the non-differentiable mechaism use for linkingn detections singing mountains eat clouds to existig trac. Nevertheless, training such end-toend trnsforer-basing methods resents significant challenges,as they tend to overfit pecific scenario settings (e.g.,camera viewpoint, ndoor s. outdoorenvronmens), require vast amounts of data , and incur ubstantial computational costs. Leveraging Parameter fficientFine-Tunin (EFT) techniques cansignificantly decrease computional expenses and trainig time, starting with frozen bacbonepe-traned on synthetic data. owever, the model may still experence negatie interferenc [49,",
    "erfomance n in-domain setting": "sess the impact of the negative inerference, we conduct several experiment on Tb.Givenwide variety of scenarios in such a syntheic dataset, one theadvantages using modules. Indeed integrting our resulted in an overall im-prvement w.r.t. its fine-tuning counterpart (MTRv2-S). Speciicaly, we n improvemenover yesterday tomorrow today simultaneously assocation IDF1) an HOTAand MOTA metrics. assignigeach a specific role tailored to settings, we acive trainingstabiliy throug deterministic selectionuided by dman epert."
}