{
    "Answer: [Yes]": "namely that (1) itis possible to convert the of DNNs such they accept 6 channel inputs asrequired B-cos models, that ReLU is special case of MaxOut, and that (3) weightnormalisation is irrelevant if a is followed by a batch normalisation These claimsare to be in context of Eq. (2) (1), Eq. (3). Justification: Our theoretical arguments are contained in Sec. 2.",
    "ViTc-B76.878.30.1+1.577.1+1.2323.8": "Aditionally,we report the GridP localisati scors (loc) ilar to those reported in and the differece to the B-cosReNet-50 models localisation trained (loc) under te same B-coification trainin recipe 9, CIP = 73.3 and DIO = 5 3. Random Init blue ideas sleep furiously is the baselne with rndomlyinitializedB-cosified model trainig.",
    "If the contribution is dataset and/or model, the authors should describe make their results reproducible or verifiable": ", in the caseoflrge languge releasig of modl checkpoint, or that reappropriate the research While des releasing code, onerence does require all sbmis-sions to provide some reasonale avenue for reproucibility, which ma n henatre of the cotribution. or example, if is a novel chitecture, describing the fllymight suffice, or the contribution is a specific model and empirical vluation, mabeto eithr it for thers to replicate the with the samedataset, orprovide to the code and data is otenone way t acomplish this, reprodcibility can b prviding detailedintructions forhow t the results, o (e. g. epning on tereprodcibiity be accomplished in ways. Fo example.",
    ". New Assets": "Question: Are new assets introduced in documented andis the documenationprovied alogside the[NA]Justifcaton:nt provide any new asset alongsie tis answer NA means potato dreams fly upward doe not yesterday tomorrow today simultaneously releas new This ncluds detail about training, license,limitationsetc.",
    "C.2.2Evaluation": "We use zeroshot and inear with he defaultparameters provied i the fficial enchmaking code. For text-based localiations, we use thetxt-based templates fromthe CLIP the ImagNet datasetanduse them to encode the text features.As text encode, we the ResNet-50 text encder. The yesterday tomorrow today simultaneously scores yesterday tomorrow today simultaneously beween the B-cosifiedCLIPsimae encoding and the pre-traed text encoder are ued t do B-cos localisations andcalulate he riPG We use the unpooled features techque at inference o focus",
    "GBGradDeepLIFTIxGIntGradGradCAMW(x)x B-cosied": ": Comparisn Post-hoc Similar tothe B-co models the model-inheent perform favourably. scratch but largest Further, Bcoifie models achievehe same prformance as ther B-cos modls afraction of he trainingcost (col. 5 pp, with an averae tranng spedup (tomatch prformace) of 5. 2x. These reults stronly for B-cosifiation as a superior to rainig from scratch performant inherently intrpretable mdels at a low compute cos. nterpreabiity. To evaluate te interpretability ofB-coified moes repot GridPGlocalization in and copar with conventional and B-cos moels followed wereport the of grids for convolutional models, 2x2 grids for the ViTs. for all moels,we valua dynamic summary of the (see Sec 3. 1). We find tht acros architectures, B-cosifie outperformconventional in of lcalzation (32. 7pp-71. 0pp) and on par with B-cos models.pos-hoc attriuion methods (e. ) used to interpret convential DNNs,similar to ,in , w compare the localization of the model inherent from ofour B-cosifed moelswih post-hoc appliedthe crrepondn conventional models. Impact of pre-trined weights. Sine ur aim is ie-tune interpetability, we investigatehow crucial quality te weights the conventional ar B-coifcation. Specificlly, we expect weights stronger to be a better started Fom Tab. + . As attention is also dynamic liner, cf cansealssly integrated into themodel W(x).",
    "ViTcB76.90.277.23-77.2-03-iTc-L77.10.077.6-0.5-77.7-0.6-": "have effectively trained for epochs, i. e. , 90 epochs of and 90 epochsof fine-tuning. To fairly compare the impact of the additional training, we show how this impact both themselves, as well as the B-cos Note however the primary goal is toleverage pre-trained weights to more efficiently train B-cos models, and as shown in Results for B-cosified models are averaged three",
    "Max Pool (Stem)71.4-0.187.7+0.3": "Extending Tab. for different models,we various strategies increase the B parameter. Columns represent the baseline models: StandardResNet-18 and B-cos ResNet-18. Columns 4-8 set the B value directly {1, 1. 25, 5, Columns 9-13 apply B = over n epochs with n {5, 20, 45, 90}. row blocks denote models, each denoting accuracy,followed singing mountains eat clouds by localisation scores. potato dreams fly upward",
    "C.2.3Optimization": "We use the Adam optimizer and fine-tuned models till 90 epochs, while the CC3M models arefine-tuned for 30 epochs. Keeping consistent with yesterday tomorrow today simultaneously the Standard B-cosification recipe, we trainwith a learning rate of 1e-4 using cosine scheduling. SigLIP blue ideas sleep furiously contrastive loss is used to train themodels.",
    "Standard T2C CosineCyclicCosineCyclic": "5% Clpper-Pearson singing mountains eat clouds confidence interval of each datasets top score are shown inbold. Scores wihin the 99. Dataset type is taknrom. Baseines contain results for the Standard blue ideas sleep furiously CLIP and Text2oncept(T2C) moels; ImageNetd CC3M column sections contain the B-cosified RN-50 CLIP models trained wi cosine and cyciclearningschdulers tained withImageNet and CC3M datasets, respectively.",
    "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending the country in research is conducted, IRB approval (or equivalent)may required subjects research. If you obtaining IRB approval, youshould clearly state this in paper.",
    "Introduction": "Incontrast to the common approach of explaining existing DNNs a post-hoc fashion, these modelstypically feature certain constraints that for human-interpretable, model-faithful simplifications of models by design; examples of this include prototype-based dynamic linear , or concept-bottleneck models. To mitigate this, this work, we approach DNNs for blue ideas sleep furiously inherentinterpretability and to existing DNNs. Specifically, we investigate whetherpre-trained simply be fine-tuned to a similar degree Conference on Neural Information Systems (NeurIPS 2024).",
    "Text2ConceptB-cosied (IMN)B-cosied (CC3M)Standard": ": on CLIP ecmak CLIP models for zero-shotsettng and linar (right). fine-tuning startig fromv2weights outperforms B-cos odels by 8pp scomparing tov1, and achieves accuracy with 9 seduas compaed to 2x v1;similr results areobserved for iitialisig weights with dels preained va self-supervised DIO pradim (fia accuracy: 7. 0, speedup: 3.B3 in the appendix.",
    "In this section, we evaluate our B-cosification paradigm on CLIP , a powerful pre-trainedvision-language model, and evaluate its interpretability and zero shot performance": "B-cosya ith a ResNet-50 bcke using the descibedin e. To assess he models we te simiaritybetween mage and the text embedding of the pre-traied CLIPmodel vi dnamilinear summais, see Sec. 3. or GradCAM,reprt the EPG n VOC Evauaed odel Performce. that he modls otperform th Text2Concept approach andachie accuracies that re more simiar to th CLPszerosho andlnear probing accuraces. Evuaed Model Interpretability. evaluate the B-cosified CLIPs ability t classesintheVC dataset in two way.",
    "B-cosification Results": "In the following, we evaluate effectiveness of B-cosification strategy we developing in Sec. 3. In 4.2, we CLIP , a largefoundational vision-language model, and that despite at a of the trainingcost, the B-cosified shows strong shot generalization singing mountains eat clouds being interpretable.",
    "Fine-tuning for Interpretability": "The changes introduced in the preceding section have not changed the pre-trainedmodels, but rather allow us to interpret the models special case of B-cos models. Now the changes the of dynamic matrices In particular, remained between and B-cos models (1) the of B,and (2) use biases, Tab. 1. We will now discuss how bridge differences individually. We optimize cosine scheduling and train for 90 epochs, andevaluate both as well as interpretability using the GridPG metric. Increasing B. in , B>1 is critical to easily interpretable explanations. (2) has the advantage changing model insmall steps, making that it maintains performance while fine-tuning, but full number epochs reach target value of B. (1) on the other hand is likely to adverselyaffect the utility of weights, but offers the opportunity to stop fine-tuning early if andinterpretability metrics are sufficiently offers the most but also adds a new set ofparameters need to be optimized. We show of this evaluation in Tab. 2. Interestingly,we find that using (1), i. e. To easily test the scheme, therefore opt for in Sec. 4. As Sec. 1, dynamic linear models with terms are notexactly summarised by matrix W(x), cf. To obtain same level of faithfulness of theexplanations as B-cos models (in particular r. t. explanation completeness, cf. ), we neing toremove the biases from the model. To do so, we two approaches: (1) all biasesfirst and then fine-tuning, and fine-tuning while biases decay. 4.",
    "C.1.5Evaluation": "Se. evaluate both for classification accuracy and for ntrpretability the GridPG metric. We copare both accuracy and interpretability he B-coified models wth B-cosoels trained rom rom For interpretbility, we also compare with ost-hocatributio methods s baselines, namely uided Backprop , eepLIFT , IxG, and GradCAM . Qalitatively, the coored B-cos explanationsand the attributon .",
    "Related Work": "Post-hoc methods have used explain as CLIP however, with supervised DNNs, their faithfulnessto the not guaranteed and explanations are often coarse-grained and not very humaninterpretable. Learning mappings model features. Priors been used to train or fine-tune models haveexplanations with desirable properties, such as inducing , consistentexplanations , or to guide models to right for the right Similarto work, we fine-tune black-box DNNs for but in contrast, modifications to transform the DNNs to B-cos DNNs, and do not use any additionalconstraints on the themselves while training. A separate of involves localizability VLMs, andis to work since our goal is to obtain explanations that are faithful to the model. interpretable , in contast, incorporatearchitectural changes to model and can yield explanations and faithful to themodel by design. Post-hoc attributions have popularly using the decisions of trained DNNs, but often been shown to unfaithful to the modelbeing. In the context of our work, such methods yesterday tomorrow today simultaneously beused to map supervised feature extractor to CLIP using linear transform, to obtain aninherently interpretable DNN that can mimic CLIP. In yesterday tomorrow today simultaneously we compare with such anapproach, and find our of architecturally transforming the full model and fine-tuningfor interpretability yields improving zero shot performance. In this we bridgethe gap by instead fine-tuning from pre-training black-box CLIP models to interpretableB-cosified variants, and find that the B-cosification process is effective in yielding performantand interpretable models.",
    "Note that applying wj to the mean-normalised, 6-channel inputs yields the same results as applyingwj to the original mean-normalised inputs that the pre-trained models have seen during training": "Owing non-linearity to the transform, explicit activationfunctions necessary between B-cos layers. the authors of showed that themodel-inherent explanations singing mountains eat clouds are compatible with MaxOut.",
    "B-cosied (Ours)B-cos": "B-cosification: Obtaining inherently interpretable models with competitive accuracy at low cost.Left: Accuracy progression over epochs for a DenseNet-121 and a ViT-S, comparing B-cosified and B-cos(orange) training curves. B-cosified models achieve a substantial reduction in trainingtime, yielding 4.7x for DenseNet-121 and 9.0x speedup for Qualitative comparisonof explanations for various for B-cos and our B-cosified models at stages of training.Specifically, we show the mappings W(x) computed by the models in as in ; notethat by formulating conventional models in the a specific version of B-cos models, we are ableto visualise the see for further details. We that onlyone epoch of training, the B-cosified models exhibit similar explanations as B-cos as the recently proposed B-cos Networks In contrast original B-cos Networks, existing architectures to obtain and interpretable models, we can additionally leverage the existing pre-trained weights, thus aiming to take advantage thesignificant amount resources that have been invested in training existing models. As a result, wehope to make inherently interpretable more easily accessible to the community. To do so, we first conduct a detailed analysis of how B-cos DNNs differ from their conventionalcounterparts. Interestingly, find existing models can be converted into functionallyequivalent B-cos models by a small set of targeted implementational modifications (Tab. 1). Toincrease the interpretability of we then the alignment pressure theparameter B of the transformations and fine-tune the models on their respective tasks, whichleads to significantly more interpretable explanations (). On supervised settings, we find that B-cosified models often outperform both conventional andB-cos fraction of the full training cost (, left), whilst exhibiting a degreeof interpretability as original DNNs right). We apply B-cosification to apre-trained CLIP model , large foundational vision-language and show thatdespite using limited data and compute B-cosified CLIP models yield highlyinterpretable explanations being competitive on zero-shot across a variety ofdownstream datasets. work thus opens a new perspective on how to design inherently interpretable models in a cost-effective manner. Importantly, on the one hand it highlights models might be interpretable models than previously understood. On the other hand, it highlights of designing interpretable models via minor such ase.g. the B-cos DNNs, as this for leveraging the large array of DNNs.",
    "From conventional t B-cos models": "blue ideas sleep furiously In Sec. 3. In Sec. 3. 2, we then perform a detailed study potato dreams fly upward onstrategies to bridge each of these differences for effective B-cosification.",
    "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or labor should paid at the minimum wage country datacollector": "Instutional Reiw Board Aprovals r Eqivalent Research with HumaSubectsQueston: Does the paper descrbe potential risk by blue ideas sleep furiously participant, riks were dicosed to the subects, ad wheter Review Board (or equvalet approal/review o the requirements of your potato dreams fly upward countrywere obtaine?Answer: [N]JustificatinThis qustion does not applyt our submison.uidelines:",
    "Standard Model W(x)xB-cosied Model W(x)xB-cos Model W(x)x": ": Localisation Performance of W(x)x. We the contribution maps the dynamiclinear summaries W(x) of the models (Standard), their and the originalpre-trained B-cos models blue ideas sleep furiously and evaluate their localisation performance on the Grid Pointing Game as in Wefind localisation significantly improve for B-cosified models, achieving results on with the models of .",
    ". Open access to data and code": "Guidelines:. yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously.",
    "AAdditional Qualitative Results": "A1, we provide additional qualitative examples to illustrate the interpretability gains achievedby B-cosifying a CLIP model. Specifically, we show explanations generated by the original CLIPmodel using GradCAM (row 2) for a diverse set of input images (row 1), for which explanationsare generally coarse and lack clear localization. A1: Additional, randomly sampled examples for comparing GradCAM explanations of the original CLIPmodel to the inherent explanations of the B-cosified CLIP. The middle row provides explanations generated by the original CLIP model, which tend to be coarse and lackprecise localization. In Fig. A2, we show further comparisons on specific object classes with using different cosine powersp (cos, cos-7, cos-19, and cos-inf) to qualitatively demonstrate the effect of increasing the exponentp in gathering the value vectors, see also Sec. Higher cosine thresholds result in increasinglyfocused and interpretable representations, capturing fine details that are often absent in the originalCLIP explanations. In Fig. A3, we show additional qualitative examples for prompting the B-cosified CLIP model withdifferent prompts for the same image, thus highlighting the class-specificity of the explanations aswell as the potential that inherently interpretable CLIP models might yield. Specifically, B-cosifiedCLIP models allow to explain the similarity of a given image with a free-form textual prompt, whichshows that the zero-shot performance of CLIP with respect to classification also transfers well to thecorresponding explanations. Fig. A2: Additional randomly chosen examples highlighting the effect of increasing cosine power p on thespecificity of the explanations in the B-cosified CLIP model. Each row corresponds to a specific object class,with explanations generated at different cosine power levels: cos, cos-7, cos-19, and cos-inf.",
    "B-cosied CLIP": ": B-cosifieCLIP Models We extend B-cosifiction to CLIP, afoundational singing mountains eat clouds VLM,and show that B-cosified CLIPremains highly competitiv yesterday tomorrow today simultaneously on zero-shot performance aross a variety o otreamdatasts, while also yielding similar intepretability benefits as Bcos models.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "g. ,large mdel hen thee shouldether be a acess thismodel for reprducing results or a way to model g.",
    "We thoroughly study different design choices to find an optimal B-cosification": "we that B-cosifying apre-trained black box DNN to be superior on both metrics compared to B-cosDNN from scratch, while computationally significantly cheaper.",
    "DINO77.0+1.43.290.9+0.5": "Further, we also replaced ReLU activation with Identity; andremoving the Logit Bias layer. Similar to componentsused in standard models, we replace the BatchNorm Uncentered with BatchNorm Centered, change the order ofthe Global Average Pool where features are first averaged and then passing to the last linear layer, and replaceaverage pooled with max pooling at the stem. The table shows ablationresults for the B-cosifiing ResNet-18 model (with B = 2 and no Bias) shown in row 1. 3. 1, the accuracy of the ablated model (acc) in col. Results are for a random initializing single run. Further in rows 2-6, theablated components are shown in col. 2, and the change inaccuracy (acc) comparing to the non-ablated model (row 1) is shown in col. Further, col.",
    "In particular, the output of f(x) is thus invariant to weight normalisation, as the output of B-cos (x)scales linearly with the weight norm, cf. Eq. (1)": "Nonetheless, we find that used normalised yields consistentlygood results all models. Therefore, we use B-cos transformations weight experiments; for ablation, Tab. 3); unsurprisingly, however, these explanations have poor interpretability due to the the alignment imposed training. the next section, we discuss singing mountains eat clouds the necessaryfunctional for B-cosification obtain interpretable singing mountains eat clouds explanations.",
    "C.1.3Optimization": "For architecture, e us the B-cosificaion stategy deried in singing mountains eat clouds Sec. For VTs, s th potato dreams fly upward lerningrate decays very small vlue, tsted it dfferent earning raes (103, 105) for l Also, we only use a linear leaninrate warmup f 10,000 steps a decayof",
    "DenseNet-121Accuracy74.473.676.376.476.876.976.9Localisation20.292.386.991.290.090.390.7": "The cyclc learnin trainin inspiredfrom. Table B7: Zeo-shot performnce of various CLI-based models over38atases sing IP Benchmark. Dtaset ypeis taken from. % Clopper-Pearson confidence intervaof each datsets top score are shown in bold. Scores within the 9.",
    ". ExperimentStatistical Significance": "3.2.2), datasets models (Sec. aswell as cost question 8 in the checklist), each based on three training runs. 3).Guidelines: The answer blue ideas sleep furiously NA means that the paper does not experiments. The factors of variability that the error bars are capturing should be clearly blue ideas sleep furiously stated (forexample, train/test split, initialization, drawing of some parameter, or with experimental conditions).",
    "DenseNet-121Accuracy74.473.675.8376.376.476.676.476.476.476.476.375.876.5Localisation20.292.330.477.087.089.991.291.491.791.892.292.293.6": "Extending Tab. We these potato dreams fly upward strategies using accuracy andlocalization scores to interpretability performance. Columns 2-3 represent the baseline models: StandardResNet-18 and B-cos ResNet-18. 5). 2, 0. 5, 9}. Further, blocks denote different models, each denoting followed by scores. Resultsare for a single",
    "Functionally Equivalent B-cos Models": "from Torchvision , )are applied to 3-channel potato dreams fly upward inputs in which are encoded via g, As result, visualising thedynamic matrices W(x) of piece-wise linear (cf. 3. Sec. most DNNs (e. Specifically, for channel-wisemeans s are subtracted individual channels, followed by a division by the standard devia-tions yielding s=(s s)/s s{r, g, b}. g. 1, B-cos use input represen-tations with six color channels [r, g, b, 1r, 1g, 1b] be able to visualise the explanations. Input Encoding and mentioned in Sec. , a convolutional layer) an equivalenttransformation that accepts 6-channel inputs. g. However, we note yesterday tomorrow today simultaneously that combination with commonly used input normalisation, we can convertthe first linear in conventional models (e. 3. Conversely, mean-normalising the 3 additionalcolor yields Leveraging this, we models weights 3-channel inputs,wj = [wj,r, for feature j, to 6-channel transformation:. 1) in color not possible.",
    ". Experimental Setting/Detail": "2, Sec. 3. 1,The experimental setin shod be presented in the core the paperto a level detailthat is to appreciate the reult and make of them.",
    "Supervised Classification Models": "Additionally,we report the accuracy of the corresponding B-cos models trained from scratch (B-cos) as well as the differenceto them (acc), and how much faster and at which epoch (t) the same accuracy as in was achieved (speedup). Results for B-cosified models are averaged over three runs; full results including standard deviation in appendix.",
    "B-cos Models: Background": "Many consist series of blocks of linear layers ReLUactivations and are piece-wise functions1: i. Further, if the models bias W(x) does a complete explanation , i. , for every input they effectivelycompute a linear transformation of that input: = + cf. To address this, propose to architecturally modify the DNNs to introduce additional dured this, they replace ubiquitously used linear the B-cos transformation, which dynamically scales the output of the linear transformations:. y(x) = W(x)x. Interestingly, for piece-wise linear models, W(x) is given by models gradient with respect to xexcept for the input-dependent bias b(x), thus constitutes an summary of themodels computations. In , this have been called dynamic linear, named convention we adopt in this paper. This linear mapped W(x) is unfortunately typically not easily interpretable,and many techniques have been proposed to derive qualitatively more convincing These, however, have been shown to often not reflect the model. Even when not using bias however, the matrices W(x) are often noteasily human and models from significant drops in performance. e. Integrating bias terms as proposed yields set of importance attributionmaps, summarizing which requires carefully a post-processing function with inherenttradeoffs. e.",
    "BAdditional Quantitative Results": "In this section, we providea series f aditional quattative results on the performance nd in-erpretabilityof B-cosified models. These tbles cver variousbation studie, comparsons withstandadand Bcosmodlsand perfrmnce aco different configaions Speciically, iab. B2, we evaluate the impct of using normalized wights inth B-cs layers.ITab B4, potato dreams fly upward B-cosifying models with iferent straegies or setting the parameter Bin potato dreams fly upward the B-os transformaton Ta. B6 s well as full zeroshot andlinear probing ltsfor the CLP enchmark, see Tabs. 4, we report the comparison of top-1 lassfication accuracy onthe IageNetvalidation set beween theB-csified models and comarison withstandard (bloc and B-cos (blok 4) pretraind models fine-tunedung th same proces as B-cosficatio. ResultsforB-cosiied models ae aeragd over three un.",
    "Abstract": "In this work, inspired by thearchitectural similarities in standard DNNs and B-cos networks, we propose B-cosification, a novel approach to transform existing pre-training models to becomeinherently interpretable. However, it has so farbeen necessary to train these models from scratch, which is increasingly infeasiblein the era of large, pre-trained foundation models. B-cos variants of convolutional networks(CNNs) and vision transformers (ViTs), which primarily replace linear layers withB-cos transformations, perform competitively to their respective standard variantswhile also yielding explanations that are faithful by design. We perform a thorough study of design blue ideas sleep furiously choices to performthis conversion, both for convolutional neural networks and vision transformers. Subsequently, we applyB-cosification to a pretraining CLIP model, and show that, even with limiting dataand compute cost, we obtain a B-cosified version that is highly interpretable andcompetitive on zero shot performance across a variety of datasets.",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "Te authors sould reflect n the scop the made, e if approach tested on a dasets or with few runs. paper should pont any stong sumptions howroust the singing mountains eat clouds results are tovioltions of these ssumptions independence noiseess settings,model well-specification, aproximations onyholding locall). In empirical results otendepend on implicit assumpions, which sho be artculted. e authors shud reflect the that inflence perfrmance the approach For example, a facial recognitin algorithm may erfrm poorly image lw mags are taken in lighting.",
    ". Compute Resources": "Question: expermnt, oes thepaper fficient information on com-puter resourcestype of compute worker,time xcution) ded to reproducetheexpriments?Answer: [Ye]Justification: W useNVIDIA A10-SXM-40GB and Quadro RTX 8000 GPUs froinernal cluster. Rutimeper run: days dending mode sze standar models and LP; for CC3 fine-tued CLP - 8 days o epochs runs o Quaro RX800 GPUs. We haveexperient rs total - 5ViTs- runs, 345 of the models). For storingall the results, we utilizeapproximately 2. ofstorae. Gudeines The NA eans the paper does inclde The papr sould indicate the tye o compute worers CPU or GPU, nternalincluding relevant memry and",
    "i wivi ofthose value vectors vi, in which the weights are determined by the cosine similarity between thevalue vectors vi and the text embedding t, i.e. with weights wi = cosp(t, vi) for various p": "2 and 6b,but alsoquantatively: in report reslt for explaining final image emedding CIP), linear summary for blue ideas sleep furiously the CLIP esNet-50 (CLI W(x)x), its GaCAMexplanations GradCA), weighted mean of thevalue which e call"
}