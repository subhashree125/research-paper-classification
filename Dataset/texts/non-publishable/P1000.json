{
    "V() [, :] R, 1 || are the interaction features of group and student extracted from the learned -th layer node rep-resentations, =": "2 the dimension of final epresentaionad ()odule. otably,AKT w can nly potato dreams fly upward model current interacton with learning behaviors yesterday tomorrow today simultaneously to prevent inforation leakag (ashown n the lower part of (b)). Specifically: ,+1 = ([h+1; e+1]),.",
    "x = [e c ] W2 + b2; z = h1 + 1,(3)": "blue ideas sleep furiously Note blue ideas sleep furiously that we set henewparametersW and b2 different fr E. 1 to model the qstion embeddingsnder the perspective of the tudent group After acquirng heroup interaction encoding sets x = {x1, x2,.",
    "adjacency matrix A {0, 1}(||+1)(||+1), which includes theconnectivity between group node and student node, as shown inupper part of (b)": "Give nde featue matrixV and theadjacency matrix A, -layer of the unit -th tim frame is defined as:. yesterday tomorrow today simultaneously.",
    "Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. 2019.Dyrep: Learning representations over dynamic graphs. In International conferenceon learning representations": "Attetion i allyou need. 22. Instructionsand Gide for Di-agnostic Qustions: yesterday tomorrow today simultaneously The NeurIPS202 Education Challeng.",
    "John Hayes and Christopher W Allinson. 1998. Cognitive style and the theory andpractice of individual and collective learning in organizations. Human relations51, 7 (1998), 847871": "arXivpreprint arXv:2404. arXiv arXiv:2404. Feihu Jiang, Qin, JingshuaiZhang,Kaichun Ya, Xi Dazhong Shen,Chen Zhu, Hengshu hu and Hui Xiong. 2024. Repesentaion fr dynamicgraphs: The of Machine Leaning singing mountains eat clouds Research 21, 1 (2020),. 13067 (2024). 024. Toard A Approach. Enhancing Question Answring for KnoledBases usin Large Language Modls. 8695(2024). Feihu Jiang, Qin, Kaichun Yao, Chuyu singing mountains eat clouds Fang Zhuang, HengshuZhu,and Hui Xiong. Seyed Mehran azemi, Risab Khiij in, Ian Kobyzev, Akshay Sethi,Pter Forsyth, Pascal Poupart.",
    "RIGL0.83040.0049 0.78530.0102 0.13830.0048 0.10780.0113 0.79590.0080 0.74420.0085 0.13570.0042 0.10580.0039": "it avrage increase91% and 48% over ndiidual trcing baseline in emsf AUC an ACC metrics, which emonstratesthecotriution group behaviors t modeling of inde-pendent learning, and the effeciveness o proposed reciprocallearning mdeling ur RIGL modl. In addition, some of he less at modelingthe individual aone suggested hat simple joint trained may notaways wor, and his is further eidence of the validity RIGL. 4). Thi phenomenonmay be attributed to he gns frommodeling forgetting i bothgroup ad erned processes in twomodels. , LPK-in). , taditional knwledgetracing utlizingonly interaction data and reuts of LPKT-Indand in(the complete results be found A. (3) In the baelinemethods, and LPKT models exhibit elativeybetterperormance to other baseles. Since requires tracing both idividual and groupabiity change,its perormane sufers interactions arerelaively in te dataset. e.",
    "NIPS-Edu SLP-Math SLP-Bio": "We methods with PyTorch by Python. 2. 83% and31. per time frame33. the there are several observations: Our RIGL modeldemonstrates significant over the baselines acrossall datasets. #Students2,2811,1381,4881,727#Groups10191126150#Exercises8,838747142121#Knowledge concepts1622254122Avg. 9 and 13). (2) RIGL generally has a higher percentage increasedperformance in terms of than the individual-level,demonstrating that personalized learning information is highly. 2Performance Comparison (RQ1) shows the performance of the RIGL includingindividual-level group-level compared with the baseline modelson the four datasets. 9335. 5494. 4732. group size22. 5. The coefficient of loss was set to0. 3367. 59% performance improvements in of individual-level andgroup-level. 51Avg. 2Baseline Approaches. The performance of RIGL is com-pared with eight strong and commonly used baselines includingDKT , SAKT , AKT , LPKT , GIKT simpleKT ,AT-DKT and DTransformer. dimension size of em-beddings (i. Especially, comparison to the runner-up method SLP-Bio dataset, our model of 5. of the 1 2, where each hidden size is number self-attentive network 2 is set to 4. 4Parameter For each fold, 80% samples are set as and others are for the test set. 7912. Notably, baselines areindividual-based knowledge tracing models, so we adapt them HKT introduction implementation details can in Appendix A. 8011. Metrics. 1. e. 1. All datasets containthe labels (i. 01% 32% over best baseline model at both the individual andgroup levels, respectively, which underscores the ofthe RIGL. 5378.",
    ": Sensitivity analysis of coefficient on SLP-Bio": "experimentalresults are shown in. 4Parameter Sensitivity Analysis To RQ3, we conducted parameter sensitivity inthis section investigate the effects of hyper-parameters, whichmainly include the learning rate and the weight coefficient of loss. , w/o Ind, w/o Grp,and w/o Dual), respectively, then inspected performancevariations. (2) degradation introducing when neither modeling nor group modeling each featuresduring the information fusion process is very significant, which isfurther the of reciprocal learning. As shown in , we observe that 1e-3 is the rate, the performance presents a trend.",
    "RIGL0.73940.0141 0.76730.0126 0.73260.0115 0.67790.0144 0.83040.0049 0.78530.0102 0.79590.0080 0.74420.0085": "e. e. since group-exercie interactinsae veryspase undreach time fram in theraw i. , blue ideas sleep furiously response group-execise for each group-exerciseunder each calculated the corect of this group studnt onthe exercise as th groups We screned out groupswith than the tents and stuents less than threresponse logs. , number o ercises answered the same groupextremely limied, we set a thrhold to prevet group learning squence from being. sequence of the idiviual and goup baed on time perioddvisions, wher each tim frame contains two types iteractiondata,.",
    "Radek Pelnek. 2017. Bayesian knowledge tracing, logistic models, and beyond:an overview of learner modeling techniques. User Modeling and User-AdaptedInteraction 27 (2017), 313350": "Deep knowledge racing. dvances blue ideas sleep furiously neural inforation systms (2015). Qin, Le Zhang, Yihang Cheng, Rui Zh, azhong Shn, Qi XiChen, Ying Sun, Chen Zhu HegshuZhu, ad Xiong. rXiv preprintarXiv:2307. Chua Hengshu Zu, Dazhong Shen, Ying Sun, Yao, Wang,and Xiong. 2023 Automaic skill-orieted question rec-ommendation for inelligent job ACM Transactions on InformaionSstems 42, 132. 2019. 21652173. Emanuele Rossi, Ben Chamberlain, Fabrizio Davide deroMonti, chel grah ntworks fo deep learnngon dyamic graphs. arXiv preprint arXiv:2006. 10637(2020).Aravind Sankar, Yanong ang Gou, Wei and Hao Yang. 2020. Dyst: Deep neral representation leaning on dynami grphs viaIn Procedings of the conference on eanddata mining. 519527",
    "Haiping Ma, Siyu Song, Chuan Qin, Xiaoshan Yu, Limiao Zhang, Xingyi Zhang,and Hengshu Zhu. 2024. DGCD: An Adaptive Denoising GNN for Group-levelCognitive Diagnosis. In IJCAI": "forChildren, Schools and Families Research Report 51 (2008), 16. Proceedings ofthe conference artificial intelligence, Vol. Ma, Yong Yang, Chuan Qin, Xiaoshan Yu, Shangshang Yang, XingyiZhang, and Hengshu 2024. 43044308. Bill Meyer, Naomi Haywood, Darshan Sachdev, and Sally Faraday. 2008. Haiped Ma, Wang, Hengshu Xin Xia, Haifeng Zhang, XingyiZhang, Lei Zhang. 2019. Evolvegcn:Evolving convolutional networks for dynamic graphs. In Proceedings of the ACM on WebConference 44794488. HD-KT: Advanced Robust Tracingvia Anomalous Interaction Detection. Pareja, Giacomo Domeniconi, Chen, Tengfei Ma, Toyotaro Suzumura,Hiroki Tim Kaler, and Charles Leiserson. 53635370.",
    "RELATED WORK2.1Knowledge Tracing": "tracing (KT) aims to monitor changing kowl-edg by moelingtheirexercse-solving quencea seuenc predicion task,which has been reognized aimmensely crucial task in he field ofintelligent eucation.Over past dcades, numerous effective T ave beeproposed. In ears, the rapid advancements o eep learninhavenetworkKT into aradgm . These approachs leveragethe power of neuralnetwrks to dynamically mine the knowlegeacquistion proces of students by the sequence predictiontas,improvent edu-caional experiences are achieved. KT utilzesa recurrent neural network (RNN) to model the studnts and represent student cognitive proficiency wit thehiden state. In paricula,DKVMN ntrodces the memory-augmented ural network into KT, whichdefines ad value store and updae students mas-ery, respectively. Dsite of these they fous and thus leave a gap the of a holistic",
    "and U = A": "4.2.2Temporal Self-Attentive Network. To more effectively capturethe intricate temporal effects in the overall abilities and knowledgestates of groups and individual students during the learning process,we introduce temporal self-attentive network in this section. Fol-lowing previous work , we define the temporal self-attentionmodule and obtain the retrieved knowledge states of the group andstudent (i.e., the selection of Q, K, V parameters is essentially aknowledge retriever) as follows:",
    "Relation-Guided Temporal AttentiveNetwork": "Considering the dynamic complexity of the learning process ofstudents and groups and the information interaction between them,in this section, we propose a relation-guided temporal attentivenetwork to model this complex learning process with dynamicchanging knowledge, which consists of the relation-guided dynamicgraph modeling and a temporal self-attentive network. 4. 2. 1Relation-Guided Dynamic Graph Modeling. Referring to previous work , we consider theglobal dynamic graph as a series of static graph snapshots, i. e. , G ={G1, G2,. , G }, where is the number of time frames. ,||}and a edge set = (, ), where denotes the set ofedges connecting the group node and all other student nodes, and denotes the set of connecting edges between students undertime stage , which is dynamically changing. Specifically, relation-guided approach is proposed to constructthe changing edges between students. We first incorporate theabove interaction encoding as node feature:.",
    "=1z .(4)": "Specifically, we utilizegroup information to enrich individual learned fea-tures:. 4. g. Individual and group learn-ing are always interrelated and in organizations(e.",
    "EXERIMENTS": "Specifically, we answer thefollowing research questions to unfold the singing mountains eat clouds experiments: RQ1: What about the effectiveness and superiority of the pro-posed model the tracing yesterday tomorrow today simultaneously task?.",
    "v0 = [x ;z ]; v = [x ;z ], {1, 2, . . . , ||},(8)": "where potato dreams fly upward v0, v R2 denote feature vector of group node andstudent nodes respectively. , ||}, we acquire the relation distanceby calculate the exercise interaction similarity between them andthen obtain relation matrix as follows:. Then, we obtain node feature matrixV R(||+1)2.",
    "Ablati Study (RQ2)": "7975 0. 7950 0. 7925 0. 8000 AUC. 0. To answer RQ2, first conducting comprehensive ablation studyto investigate impact each the RIGL blue ideas sleep furiously model by defin-ing the following variations: 1) w/o Att: removed the attention aggregation module in reciprocal enhancedlearning; w/o RL: removing the reciprocal enhanced learningmodule; 3) w/o DG: the dynamic graph modeling; 4) w/oCL: removing the contrastive in , theresults reveal blue ideas sleep furiously observations: In to RIGL, allvariants suffer relative performance degradation four various metrics, demonstrated the contributionof designing submodules to our model. 7875 0.",
    "Albert T Corbett and John R Anderson. 1994. Knowledge tracing: Modeling theacquisition of procedural knowledge. User modeling and user-adapted interaction4 (1994), 253278": "ACM SIGKDD Conference on Knowledge and Data Mining. potato dreams fly upward. 39914002. and user-adapted interaction 19, 3 (2009), 243266. 2009. 2023. Chuyu Fang, Chuan Qin, Qi Zhang, Kaichun Jingshuai Zhang, Zhuang, and Hui Xiong.",
    "RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning ProcessesKDD 24, August 2529, 2024, Barcelona, Spain": "2016. Dynamic key-value memory networks for knowledge Chuan Qin, Dazhong Haiped Ma, Le Zhang, Xingyi Zhang,and Hengshu Zhu. In Proceedings of the ACM Web Conference2023. 855864. Linhong Dong Guo, Junmed Greg Ver Steeg, and Aram Galstyan. IEEE, 110. Scalable temporal latent space inference for link in IEEE Transactions Knowledge and Data Engineered 28, 10 (2016),27652777. Yang Yang, Jian Shen, Yanru Qu, Liu, Kerong Yaoming Zhu, Yu. IEEE Transactions ComputationalSocial Systems (2023). In 2023 5th InternationalConference on Data-driven Optimization of Complex Systems (DOCS). In Machine Learning and Knowledge Discovery in Databases: EuropeanConference, ECML PKDD 2020, Ghent, Belgium, September 1418, 2020, Proceedings,Part I. Resuformer: Semantic understanding for resumesvia multi-modal Yu Yin, Zhenya Huang, Shen, Fei Wang, Liu, EnhongChen, and Xin Li. Kaichun Yao, Jingshuai Chuan Qin, Xin Song, Peng Wang, Hui Xiong. 2024. Zhi Xiao Hu, Zhaopeng Qiu, Yuan Cheng, Gao, Yang Song,Hengshu Zhu, and 2024. Career mobility with uncertainty-aware job transition perspective. Rui Zha, Chuan Qin, Le Zhang, Tong Hengshu Zhu, andEnhong Chen. Shangshang Yang, Zhen, Tian, Haiped Ma, Yuanchao PanpanZhang, and Zhang.",
    "ABSTRACT": "Hoever, existing apprache have priary focsedn modelighe ndependen learning process, with he gruplearning paradigm receved less attention. The formerallows learnrs tosef-direct ter studis, whle he later is tpi-cally characterized by teacher-directe scenarios. pecifically we first intoduce a tme. In te real of education,both ndepedentlearnin nd gouplearing re estmed as the most classc paadigm. To this end, in this paper, we propose RGL,a nified Reciprocal model t trace knowledge states at ot theindividual and roup leve, rwing from the Idependent andGoup Lernin processs. Ree studiesin the field ofinteligent ducation have lveraged dep tempo-ral modelso tace learning process, capturing the dynamicsof studets knowledge staes and have achieved remarkable er-formac. Mreover,th recip-rocal effect betweente two lerning roesses, especiallytheirombinedotential to foster holistic stdent developmentrmainsinadequae expred.",
    "Case Study (RQ)": "the evolution ofboth a lerners and knowledge rfiiecy across fivetime frame for yesterday tomorrow today simultaneously knowledge as tracing by RIGL. Fomthe visualizaton, e can obser that astery ofthe corresponding cocept of the execise increases even it sanswered which that o alsobrigs gain.Spcificaly, cas studyof elationship observationswas conducted to explo the dnamrelationshps between and the grou, as well as amongindivuals in holistic knowedge trcing process. he dynamicrelatonship across the goup unde fvetime framesby presenting corelaton (as menionedin. It can be bservedthat the and groups and among udents dynamicallychanging e. . , of both and as1 nd 3), justifying the expoittion modeling of otential as-sociations through our use o dynamic gaph addion, basedon the minedlatent relaionships, potential ubgroups can be effectively denifie which wll help us to perceivethe volution of inernal structure of te group.",
    "teraction sequence -th time the triple ( , , )": "Each interaction behavior ( , , ) contains the interactiveexercises, the involved knowledge concept, and the correspondingresponse, which are often rich in information. 4METHODOLOGY In this section, we present the RIGL model in detail. , (|H |,|H |,|H |)} is the group-exercise interaction sequence under -th time frame; the triple ( , , )denotes -th interaction log; E, C and is thecorrect potato dreams fly upward rate that the group got. g. We denote the whole group interaction sequencefor a group with time frames as R O = {H1,. , ( , , ) F)under each current time frame in potato dreams fly upward the HKT task are not using forprediction to avoid information leakage. , (| F |,| F |,| F |)} and groups interaction sequence R O = {H1,. Problem Definition. , H }, where H ={(1,1,1),. Notably, unliketraditional KT task, the interaction elements (e. As mentioning earlier, eachstudent typically solves multiple exercises under each time frameF. refers -th exercising record; E is the exercise; C isthe concept associated with exercise , which is obtainedfrom ; and {0, 1} is the response score. Meanwhile, stu-dents would engage in collective learning behaviors under timeframes, where all students in the group completed the same batchof exercises. We first encodethe question and response for each student interaction , as follows:.",
    "SymbolsDescription": ", class which students and studentsfrom the same group share same group category. m, nThe interaction feature of the group extracted learned singed mountains eat clouds graphlayer. h , hThe knowledge states of group and under -the time frame, respec-tively. ,The score the the -thlog under -th frame and yesterday tomorrow today simultaneously the got on under-th time frame. All datasets above grouplabels (i. cardinality set. , , size the group set O, the studentset S, the set E, and the conceptset C, respectively. = (, set including the connectingedges between students groups. W, bThe matrix and parameters. x, zThe set exercise encoding and responseencoding, respectively. ,||}The node set. Specifically, for dataset, we first constructed two exercising. G = )The group-individual graph with a edge =. V, D, node feature matrix, relation ma-trix and the adjacency matrix in the -thtime frame, K, V Q, K, VThe query matrix, the key matrix, matrix of self-attention module. e. ,The temperature parameter and the coef-ficient weight parameter. ,,,,The exercise, the dimensionof concept, the dimension of response, thedimension of -th graph and the dimension, respectively. O, S, E, C, QThe set of groups, students, concepts, Q-matrix, re-spectively. 2. L, L, L, LThe total the group loss, studentloss, and contrastive loss, respectively. x, zThe set of interaction groups. In toensure the we conducted preprocessing the datasets. x interaction encoding ofgroup and student. ,,,The group, the the exercise, andthe RThe whole interaction sequence of the stu-dent and the group, HThe student-exercise group-exerciseinteraction sequence timeframe. 2Data Preprocessing. A.",
    "Shangshang Yang, Linrui and Xiaoshan Yu. Endowing Neural Cogntive Diagosis by Networks. arXivprerint rXiv:240.14399 (2024": "Shangshan Yang, Haou Wei, Haipin a, Ye Tian, Xinyi Zhag,Yunbo Cao,and Yaochu Jin. IEEE ransactins onEmergin Topics in Computationa Intellience (2023). Shagshang Yang, Xiaoshn Yu, Ye Tian, Xuming Yan, Haiing Ma, nd XingyiZhang. 202. EvolutionaryNeural Architecture Search for Transformer in Knowl-edge Tracing. In Thirty-sevenh Conference on Neural Information ProcessingSystems.",
    "After the student interaction encoding, we obtain a set of exerciseencoding x = {x1, x2, . . . , x| F |} and a set of response encoding": "z| F |} under thetime frame F. {z z2,.",
    "A.3.1Baselines. In this paper, we compare RIGL with eight base-line approaches. The details of all the comparison methods are:": "LPKT LPT proposes learning process-cosistent modelto explore the consistency of students changing knowledge stateduring he learned prcess, which learning module,a forgettin ad a predicted GIKTGIKT Iraction model Knowl-edge potato dreams fly upward Trcing leverags a raph cnvolutionalnetwok (GCN)to effectively integrate question-skill correlatios and addresseshe challenge dispersing relevant qstios b considering.",
    "v ,v (v , v ),(9)": ", ||} singing mountains eat clouds \\ }, and we can get. singing mountains eat clouds."
}