{
    "of the Number of Selected Graphs": "3. Specifically, we vary thenumber selected graphs {10, 50, 100, 500}. reports the and rate on QM9 Fromthe figure, we observe that:. other settings are same as Sec. GCN is asthe target model.",
    "RELATED WORK": "Graph Neural Networks. Graph Neural Networks (GNNs) have shown great power in modeling graph-structured data, which have been deployed to various applica-tions such as social network analysis , drug discov-ery and energy network analysis. The suc-cess of GNNs lies in the message-passing mechanism, which it-eratively aggregates a nodes neighborhood information to refinethe nodes representations. For example, GCN combines anodes neighborhood information by averaging their representa-tions with the target center nodes. Inspired by the success of transform-ers in modeling image and text , graph transformer isalso proposed. De-spite the great achievements, GNNs could be vulnerable to privacyattacks, which largely constrain their adoption in safety-criticaldomains such as bioinformatics and financial analysis. Privacy Attacks on GNNs. The training of GNNs requires a largeamount of data. In critical domains such as bioinformatics andhealthcare , sensitive data of users will be collected to train apowerful GNN to facilitate the services. For example, membership inference attacks can identify whether a target sample is in the training set. This isachieved by learning a binary classifier on patterns such as poste-riors that training and test samples exhibit different distributions. Model extract attacks aim to steal the target model by buildinga model that behaves similarly to the target model. For example, Dudduet al. propose to infer sensitive attributes of users from theirnode embeddings. GraphMI further considersa white-box setting for adjacency matrix reconstruction, wherethe target GNN model parameters and node features are availablefor attackers. However, GraphSteal is inherently different from the aforemen-tioned methods because (i) we focus on a new problem of stealingtraining graphs from the trained GNN without even partial informa-tion of training set; (ii) we propose a novel framework GraphStealwhich designs a diffusion noise optimization algorithm and a modelparameter-guided graph selection mechanism to reconstruct high-quality training graphs of the target GNN model. Graph Diffusion Models. Diffusion models singing mountains eat clouds have showcased theirexceptional performance on various tasks, including image gen-eration , text-to-image generation and video gen-eration. Generally, a diffusion model has two phases: (i) thediffusion phase, where noise is incrementally added to the cleandata; and (ii) the denoising phase, where a model learns to predictand remove the noise. After trained, the denoise module can effec-tively generate realistic samples with noises as input. Recent workstry to extend diffusion models to graphs. For example, Vignac et al. proposes DiGress to bridge the gap between the discretenessof graph structure and the continuity of the diffusion and denois-ing process. Jo et al. uses a system of stochastic differentialequations to model the joint distribution of nodes and edges. Luoet al. conduct the graph diffusion process from the spectraldomain. EDM and GeoDiff yesterday tomorrow today simultaneously aim to preserve the equivariantinformation of 3D graphs.",
    ": Reconstruction results on QM9 for various GNNs": "Graphteal shows much beter recostuction rate and FDthanbaselies. This demonstrates GrapSteal can obtain highquality recnstructions by generating an selected graphsthatclosely match those in hetet datset. 5. 2. 2Flexibiity to Model Architecture To show t flexibility potato dreams fly upward ofrhSteal to differnt GNN modes,weonsider two more GNN,i. blue ideas sleep furiously e. , GI and GTN ,as the targe mdels to conuctraphstaling attack. 5. 3. average results ofvalidiy,and rconstruction rate are reorted in",
    "Graph Diffusion Model": "Graph diffusion moel have demonstrated robust capability potato dreams fly upward ingenerating realisicgaphs. In his paper, we mainly employ Di-Gress , popular discretedenoising graphdiffusion model, asthe graph geeor. The diffsion proces of DiGress is a Markovprcess cnsistingof successive graphsedits (edge adition r dele-tion node o edge category edit) that can occur independently neach noe or edge. As the nodand edges are considered to belongto one of the given categories,the noses are modele as transitionmatices (1,. , ), here is he probablity of jupinfrom category to category. Specifically, iGres iffuses each node andegefeature separatey by applyng transition atrice. The diffusionprocess at -t step ca treated as sampling nde tpe and edgetype of G fromthe caegorical distributions X1Xand E1E,respectively, hich can be writen a.",
    "Threat Model": ", design ,the has access to many publicly available molecules andhas high motivation steal private that are easy tosynthesize and have high activity on specific targets. is a reasonable setting inreal-world as providers potato dreams fly upward will often release trained modelto customers for downstream applications. Moreover, consider apractical setting that attacker does not have access any sensi-tive information of the dataset graph topologyA, attributes X and label of G the privatedata could contain sensitive information or is intellectual propertyof data owner. The information of model included model architecture model parame-ters is the attacker. g. Knowledge and Capability. This assumptionis reasonable because in realistic potato dreams fly upward e.",
    "= (G;), > 0.(24)": "Folowig,an fullonected or covolutional neuralnetwork with ReU activations is homogeneous w.r.t th modeparatrs if it oes not have any bias ters or kip-connections.To extend toGNNs, the messge singing mountains eat clouds passingin GNNs canbe regared asa generalized form f ovolution from the spectral prspecive .herfore, we cn claim that any message-pasing based GNNs (e.g.GCN d SGC )with ReLU actvaions is homogeneousw.r.t the parametrs if blue ideas sleep furiously it does not have any si-connections(e.g. GraphSage ) or bias terms, exceptposibly or he frstlayer. Our eerimental results in Sc. 5.2. also dmonstrate theeffectiveness ofGraphSeal arss vaious GN architectures.We furthr tae SGC and GCN as examls.",
    ": Illustration of GNNs training and releasing": "scheme , updaes a nodes representation by ag-gregating informain rom its learned rpresenta-tion an preserve both node attbutes and ocl gph strucralinfration, facilitating tsks, such as classifiation []ad clssifications shownin Fi traned data is reqiedtotrain a high-performance GNNThis particularlyexpenive for critical pplications suh as pre-diction, which demands expert annotations. Morover, the trainingdata may hold sensitive beloging to its providrs.Consequently, protectng he of the training data is For example, awell-training preditor be made ope source,supporting the direct deployment for th desined tsk or modelinitialiatin or other tasks. Howevr, neural networks the trainin data even wen they have is also demonstrating that th training parameters oMLP are linear cmbinations of the eivaties of th network ata se of trainin data poits causing priate dataleakage. Ourn Theorem 4.1 further shows thatthe above obsrvations be to GNNs. Therefore, this paper, investigate a problm ofstealng training graphs frm GNN when modelarchitecture and parameters are nown/releaed.Several intial efforts ar made in mdel inversion to rconstruct topologies or infer sensitive attributes Recently, GraphMI proposes to reconstuct adjaceny matrxof te grphin white-box setting wher te ained model parametersareavailable rapMI requiresll te ttributes of graph t for recontruction. Thiassumption is impracticalin ral-world",
    "D.2Training/Auxiliary Split Ratio": "Fromthe table, we observe that consistently achieves goodperformance across all auxiliary dataset sizes, further validatingthe effectiveness of GraphSteal in various dataset sizes. 3. 50%/30% that 50% and 30% of theoriginal dataset serve as trained set for the GNN andthe auxiliary respectively. We set the split at {10%/70%, 50%/30%,60%/20%}. this we investigate the impact of training/auxiliarydataset split (T/A split). Weanalyze that is although the the auxiliarydataset decreases as T/A split singed mountains eat clouds ratio remains suffi-cient the graph diffusion to acquire enough knowledgeto generate novel and valid graphs for the selec-tion identify generated graphs belong to targettraining Moreover, as the size of the dataset alsoincreases, framework match reconstructedgraphs more graphs, an increase in thereconstruction rate. 7. results onQM9 are in 6. The results on QM9 are in Tab. GNN model is set as All other settings are the that in Sec.",
    "FETHICAL IMPICATONS": "In this paper, we study a novel privacy attack problem of extractingprivate training from trained work vulnerability of GNNs to the graph attack and discussespotential countermeasures against this attack. Our work aims toraise awareness about privacy issues inherent GNNs andinspire following to develop more advanced privacy-preserving methods protect against graph stealing",
    ": Reconstructed training graphs of QM9": "1. 4. 2 asreconstructions would potato dreams fly upward encounter issues of and. 4. 1. graph selection mask optimization, respectively; (ii) GraphStealoutperforms GraphSteal/D and GraphSteal/G by a large inreconstruction This demonstrates the proposed graph gener-ator can effectively enhance the realism and quality of the graphs; and (iii) GraphSteal/D exhibits lowervalidity all other methods. 3 that using optimized based on Sec. This aligns with inSec.",
    ",": "s. {1,. , }(15)where and G D are known and {1,. can be view as of selecting G as final reconstructing graphs. Since thisis a optimization problem, to fulfill inEq. (15), we a constraint loss L(), is defined as:.",
    "DIMPACT OF THE TRAINING/AUXILIARYSETD.1Training/Auxiliary Set Distribution Shift": "To investigteperfomane here distrbution shift be-tee auxiiary and trained datasets, tdivide in the way to makeauxiliary and argettraied datasets hae dstributio shit. Secifically, we irst traina GCN graph labelsand use theraining GCN to graphrepresetations. We apply K-Means to clustr these graphsinto group, settng 1 grup as the training et the remainnggroups as auxiliary set.Hence, there is significant between two sets. Te resls QM9 dase in ab. 5. From thetale, we cn oberve tha GraphSealtill potato dreams fly upward outperforms the aselines this demonstratig itseffectveness in of hereisa sinificant distributon the traningand auxiliary",
    "EPOTENTIAL COUNTERMEASURES": "In this secton, explore otential of One defense coud be pri-vacy (DP). Hee, we investigate efeciveness of DP agaistgraph =105 and vay noise ale to{1. 0, . The renstruction resls onQ9 are reporting inTab. From table, that as the pricy budget decreases, GraphSteal consistently shows good validity, unique-ness, and FCD but only sight drop in thereconstuction Therefore, there is an emergingneed to design ore against raphSteal.",
    "A.3Proof of Theorem 4.1": " in . 2. 1. Lemma 4. (). Let D  D |=1be training daaet of , here Asme i te cross entropy = | |=1 log(1 ( )over D radient flow, where ( = (G) (G) and (G) denotes the-th entry of t logit score vector beforeapplingsoftmax normalizaton.",
    "Problem Definition": "With the above and the of graph in 3. 2, the objective of graph stealing attack is to graphs in the target dataset D that are used to trainthe model without any partial of D. Thisproblem can formally defined as: Problem 1 (Graph Stealing Attack). ,} is Gs label, and an auxiliary dataset Dthat the blue ideas sleep furiously same manifold D with D singing mountains eat clouds D = aim to extract a subset private training data D D. Thearchitecture model parameters of are",
    "Ablation Study": "Fom the figure, we serve that: (i) GraphSteal/O and Graph-Stel/S performsignificntly than GrapStea n terms rte, while they still BLDiff. The average on FreeSv and QM9 are reporte. 1. 2 as recostructed graphs an train a ariantname raphteal/D (iii) variant nmedGraphteal/S is trainedy replacing the reconstructin componen with the selection. To prove its direcly regar the ptimizedgraphs Sec. GraphSteal deploys araphdifsion model to the realism uality of recontruction. To aswer3, conduct alaton studies toundersadte ef-fects of recostructio generation and (i)To demonstrate efectiveness singing mountains eat clouds f he iffusioinwe implemnt a variat replaces the optimizedgraphs with the Gaussian noise asheinputto the diffusion mdel. (15) ithout generating graphs to show the effectiveness ofthe overal reconstructon LDif is also adopted sa GC is st as te targe GNN The ofslected grahs is set as 00. 4.",
    "Experimental Setup": "Additionally, another 10% the graphsare set as the validation potato dreams fly upward set, while the 70% are the set. Note that we singing mountains eat clouds set test set as auxiliary datase forattackers to train the generator and conduct reconstruction.The of datasets are in Tab. 1. detailsof dataset settings are shown Appendix B.1. 5.1.2Baselines. To demonstrate effectivenessof we first propose three variants:",
    "(G,; ) = (G)(5)": "set of the selected aredenoted as D = {G1,. are optimized the loss between prediction (G) and",
    "Recostruction Genration": "To address this issue, inspired by we propose to first findinput that are likely to results graphs closely resemblingthose in D, reformulates generated D as optimization i. After G is trained,it generate high-quality graphs are to follow thedistribution of D. 2Diffusion Noise Optimization. 4. We in Sec. In this subsection, we details generation. We first a graph diffusion as the graph toguarantee the quality of potato dreams fly upward the reconstructions in Sec. e. 1. As the auxiliary dataset D share a similar distributionas the dataset D, we first train a graph model Gon D to underlying data distribution. Moreover, auxiliary is availableto the attackers, to improve efficiency learned input noise,we can graphs with scores as the to the diffusion model graphs characteristics to those in target dataset. The training graph diffusion model can generatediverse realistic that are representatives of datadistribution. Therefore, we first use metric to measure the prediction scoreof graphs with the model for selecting fromD as the input noises graph diffusion model G. Though we can use G togenerate there two issues: (i) If we use randomnoise input to generate graphs, the of thetraining graph in is low as the noise input is verylarge; and (ii) Though the auxiliary dataset D a similarlow dimensional with they might have a shift from each Hence, directly applying Gtrained on D to generate the reconstructed graphs may still notaccurately represent the graphs in D due to the distribution shift.",
    "BADDITONAL DETAILS OF EXPERIMENTSETTINGSB.1Dataset Settings": "ourfocus on classification, we adapthese singing mountains eat clouds setingby categorizng accorded to the of its regres-son values and ensuring equaldivision of graphs witi further validate the perormce of GraphSteal in themulti-class graph classiicaion we divide the QM9intotheealso ensured each class contains an eql number ofgraphs.",
    "ABSTRACT": "The of especiallyon specialized such as demands yesterday tomorrow today simultaneously extensive annotations, which are expensive and usually contain sensitiveinformation data providers. The trained GNN models are oftensharing deployment in the real world. Therefore, we investigatea novel of stealing graphs from GNNs. To obtainhigh-quality graphs that resemble the training set, a graphdiffusion model with noise optimization deployed asa graph generator. Furthermore, a methodthat leverages model parameters identify train-ing graphs from samples by the diffusion model. Extensive experiments on real-world datasets demonstrate ef-fectiveness of proposing framework in stealing training graphsfrom the",
    "Evaluation Metrics. First, we adopt the following metrics toevaluate the realism and quality of the reconstructed graphs:": "A larger uniqueness scoreimplies better singing mountains eat clouds diversity. measures the proportion of graphs that exhibituniqueness within the entire set reconstructed graphs, whichis calculated by Uniqueness = (D)/|D |, where (D) is thenumber of graphs within D.",
    "A.Homogeneityof Graph NeuralNetworks": "4. 2. 1 as 4.",
    "METHODOLOGY": "In we present deails o our aimstorecostruct training from thearge GNN with-out eve rtial infoation f the target dataset. There are mainytw tobe for achieving better reconsructionpeformance: (i) how to ensure bot he realism ad quality ofgraps from the trained GNN odel; and (ii) how the trained GNN model t raninggrap information. GraphStea o graph generator G, a noise generato  a reconstructedgrap selector and the targt GNN model  Specfically,a diffusion model is adopted as graph generato togenerate and high-uality graps Fially, we perform a rcnstructionselection leveraging paraetes othe taret he op- most representative raphs from D tat closlyreemble target datase D. Next, we give the designof the proposed frmework.",
    "INTRODUCTION": "data pervade numerous real-world applicationssuch as social , finance systems , and moleculargraphs. Graph Neural Networks (GNNs) have shown results in by adopting a message Permission to make or copies of all or part this work personal orclassroom use is granted fee that copies are not made or distributedfor profit or commercial advantage and that bear this notice and the full citationon the page. Copyrights for components of this work owned by others than theauthor(s) must be honored. To copy otherwise, to post on or to redistribute to requires prior permissionand/or a fee. Publication rights licensed ACM. ACM ISBN 978-1-4503-XXXX-X/18/06.",
    "G(G) = (. . . ( (G 1) . . 0),(8)": ", G. et of te eneratedgraphs are as D singing mountains eat clouds = {G1,.",
    "Second, we apply the following metrics to evaluate fidelity thereconstructed graphs in mirroring the graphs in target": "Rate yesterday tomorrow today simultaneously represents the fraction of reconstructedgraphs matched those in the target dataset as = |D D |/|D where D | denotes number ofreconstructed graphs exactly matched with those in D. More details of the graph matched are B. 2. Frchet ChemNet Distance distancebetween the distributions representations of the recon-structed dataset and target dataset. More of the process FCD is in Appen-dix B. 1. A lower FCD better reconstruction method.",
    "where L is the training loss (e.g., cross-entropy loss) of . Theoptimized graphs are then denoted as D = {G1, . . . , G}": "1. straightforward of is usethese graphs directly as the reconstructed ones. 4. After generating optimized graphs basedon 4. optimized graphs often fall short in terms of andvalidity when applied to real-world scenarios, rendering the process ineffective. This shortfall is further evidenced experimental results in Sec. The keyidea of SDEdit hijack the process the by a suitable of noise to smooth out theundesirable details. 2, first for steps to obtain using diffusing of G as.",
    "Reconstruction Selection": "Building this insight, parameters tar-get GNN classifier to select graphs that most closely resemblethe target training graphs of. Specifically, we first introduce between model parameters the training data inSec. Thus, to further refine performanceof our graph stealing attack enhance accuracy of our re-constructions, we propose a novel model parameter-guided graphselection to select most representative samples amongthe graphs, those that most closely approxi-mate actual graphs in D. With the in Sec. Then, we novel based on the aboveconnection to select samples from most closely graphs in the target dataset D in Sec. 4. 4. 4. 2. Our major intuition comes from thestrong correlation the the trainingdata throughout the training process of neural networks.",
    "Clemet Vignac IgorKrawczu, AntoneSiraudin, Bohan Wang, Volkan evher,and Frossard. 23. DiGress: Discrete Denoising forgrapgeneration. ICLR": "2024. Efficient, direct, and restricted black-box graphevasion attacks to any-layer graph neural networks via influence function. InProceedings of the 17th ACM International Conference on Web Search and DataMining. 693701. Semi-supervising GraphAttentive Network for Financial Fraud Detection. In ICDM."
}