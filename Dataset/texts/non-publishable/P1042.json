{
    "Problem Definition": "Our goal is to enable efficientdeploment through a lightweight, reserving the high accuracyof while effectively adapting to coninuously data. Ths problem has two singing mountains eat clouds unique desierata:1) The should effectiely dapt to coinuusy by itself during. That is, hile the teacher remains static,the should cope wth evolving preferences as well singing mountains eat clouds users items. Howeer, due to its capaiy,of the leadto ineffectiveadaptaton anda substantial loss of prevously acquired knowedge. (2) The knowledge teacher student should be effectivelyaccumulate leveraed the data This fully the knowledggenerated from previousblocks, leadig to inefficient trningand performne.",
    "whee denotes the sigoid function. , = (, theranking score predictd by": "degrades overall the model and the learned new data. a newly incomed interactions can result in for-getting, where the significantly loses previously acquiredknowledge. 4. 3. 2Proxy-guiding learning. As a solution, we introduce new proxy-guided replay learn-ing, which an memory proxy assist thestudents learning. Further-more, potato dreams fly upward the student model has a highly limiting capacity, which makesit more challenged update effectively. learning singing mountains eat clouds theory in neuroscience positsthat humans do effective learning through two complementary sys-tems: fast learned system for short-term to.",
    "METHODOLOGY4.1Overview": "W anew Cotinual isillation (CCD)famework, the techer and evole cllab-orativel alng nn-staionr steam. During hesubsequet teacher pdate blue ideas sleep furiously cycle ,during -th updae), CCD follows three cnecutive sages:(1) Student model blue ideas sleep furiously generatin (4. generate copctstudent by compressingthroug K. 3).studentmodeicntinually udated used data with cy-cle. (3)Teacher system update (4. 4). te nex updatecycl arrives, isupdatdusignew interactondata accumulating during thecycle. We propose a ne strateg forselectively harnessing theknowldge from the tudntside to further enhance. each data block, CD terates these three cnecti stages(4. In the subequent sections, we how CCD ope-ate the -th bock. rovides an overview of CD.",
    "ABSTRACT": "In this wo, weelveinto asystematic pproach to operatingthe techer-sudent KDin a non-staionary data stream. We expect ths research direction to contribute to narrow-ing the gap beween xisting KD studies and practical applictions,thereby ehancing theapplicabiity of KD in real-world systems. Knowledgedistillation (KD)hs eerged as a promiing tehniquefo adessng the computaionl challengesassociated wth deploy-ing large-scale recommender systems KD transfers the knowledgeof a assive taher system to a compact student model, to rducethe uge computational burdens for inference wile retining highaccurac. CCD fcilitates the studet in effetively adapting to new data, hilealo enblin the teaher to fully leerage accumulated knowledge. The exstin KD sudies primarily focus on one-tim dis-tillaton in static enviroments, leaving a sustantil gapintheirapplicabiity to real-world scenaris dealing with continouly in-coming users, tems, and their ineractions. We roposeContinual CollabrativeDistilation (CCD) framwork, whereboth h teacher and th stu-dent continually and collaborativelyevolve along the data stream.",
    "w/ random init.0.12700.05460.13870.0573w/ one-hop init.0.11160.04360.10370.0420": "In e he avrae recommendation perfor-mance for new usrs for each data block. the teacher subsequenly the student-sde nowldge, it leads furthr iprovementin teacher rformnce. compare baselnes that showhih in expeiments. technue effectively enhances studens bility toadapt to new entities. CCD tousethe prominent-entity yesterday tomorrow today simultaneously information faciliatethe ofnew entities To ssess its effectiveness, report resutswhen replacing the proposed tchniue wth randomintialzation and yesterday tomorrow today simultaneously using 1-hop initialization. e that more rcmmendations frthe dormant uses,indicating that CCD effectively preserves previous knowledge.",
    "Update the teacher system Eq.9": "possible utilizing separate for given thelarge size of the proxies through much more cost-effective in terms of both and time.During the teacher training, we selectively leverage the mostconfident predictions from student-side models (i.e., the stu-dent and two proxies) based on the rank disparity. weidentify items ranked near top by the student-side models butassigned significantly rankings by . From the top- each student-side model we obtain items using a probability distribution () (,).The student-side knowledge is transferred to the teacher as follows:",
    "(,, , )).(6)": "(, ) denoesthe eror function between",
    "Gowalla29,85840,9881,027,46499.91Yelp14,95012,261342,61799.81": "Effectiveness of KD. 1) on our teaher-student stup. A. e assesdthe effeivness of the list-wiseKDtechniue (A. For the tudent, we employ two backone models:MF-basedmodel and GNN-basing model We set a smallembed-ding sizefor studnt (16 fr owalla and 8 for Yelp), considengthe teacher sizeforeachdtaset. Lastly, we ncrementally inceasthecapacity ofthe tache sstem by augmentingthe nuber omodels o dimnsion sizes until its performance no longer improves. Student. 2Teache and student cnfiuration. We aopt thenembe schemutilze in. Teache. or Gowalla, we construct he teacher sem sed fie MF-basemels and two VAE-bsed models , each intialized witdistict random seeds and set o 64 dimensions. Follwingthe tacher nfigurtin of the recent KDwork , we construct a masiveteacher system by enseblingmultiple lrge models. presents thenmer oflerned parametersas well as thefectiveness of distillatn inoursetu. We provietede-tiled configurtionof the teacher and he student ued in theexperiments. For Yep, we emofve GNN-based odels , each itialzed with distinct randomseeds and set o 128 dimension. The recommen-daton prfrmanceafte the dstiltion is presenting in. Sbsequentl,we seect p to two mdels for each dtaset that demonstrate high prformace. 2. Initialy, e evluate he effectveness of fiveifferen ecmmendation models for eah datast: matrix factriza-tion (MF) , metic learing ,deep neural etwork , graphneura network(GN) , and variational autoencoder VAE).",
    ": Overview of CCD framework for -th data block": "each pair, i. In this weemythe recent list-ise distillation that tais the stu-dent emulate , ranking orders) preictedbteacher Th lit-wise distilltion loss is defined as thenegtie lo-likeliod of permutati prbabilty :. : U I R, here U and denotethe set users and items, respectively. In CCD framework, ny off-te-helf technique ca be flex-bly to the stuen mdel.",
    "provide analyses for an in-depth understandingof CCD. We report results with the GNN-based on Yelp": "5. The best is achieved mall sampling size ( 3, suggesing our proxyguidedreplay lerningdoesnot notably ncrease t training coplexty. We further alyze threcommendatin of the student foruser groupshaving ditinc characteristics. ). We prsent the abation study CC the component. 5. Teacher side. presents thablation o the teacher. First,excludingthe studnt-side potato dreams fly upward knowedge largely the effctiveness ofCCD,the performance is achievdby leveaging both theupdated student and the proxi accumulatin historical knowl-edg. the simle anneling that blue ideas sleep furiously gradually impactsof he student-sid knowledgeeffectively imprves the teacher. Studetsie. We set = 01 and = 0. 1Ablaton study. 3. w/o proxy excludes and S-/P-proxyexclude the sabiliy prox adthe proxy, respectively. 3. 9, of the replay size. However, as long as roxes accumulate he of updat weights little on the finalperfmance. First, we examine the ffects of replay learning(4. 3. 3Anlsis on dormantnew uses. we invetigate the of and, usedproxies n The bestperformnceis achieved when both are updated an appopriateblance. From we exclud three proposed student-side knwedge excludesLST, 2) w/o poxies (student only)only uses the updatd stuentexcludinghe proxies inLST, an3) w/o excludes the loss annealing. We obsev bot proxies beneficial for the rovide complementry views of the previousknowledge,sshown.",
    "PROBLEM FORMULATION3.1Concept Definition": "A techer recommender system which typicaly comprisessevral large-scale models, achieves high performnce thrugh itslarge capacity. The stude mdel ha significantly rdcedinference latency, it well-suitedfrreal-time services andreource-constraining environments. real-worldremmender sstem oerates ina on-stationary data streamwhere new usrs, iems,interactions are continuously incomig. many scenars, fixed-ize time of rent eploying t updae the system. Note that the the yesterday tomorrow today simultaneously most recentblck for taining withoutdiretly accessed the The s ealuatedross thtimlie. Definiion 3 (Teacer/Studet updatean denote update cyclesfor the teacher and the In i , updating eachersystem timeand resoues compring the student modl.",
    "(, ) = exp( (rank, rank,)),(5)": "hgh valu of ( ) indicates that item is ranked signifiantly higher bymodel compared t model. Siilarly, weacquire by sampling itemsfrom he top- list potato dreams fly upward of , using a ditributin () (,). We utilize the exponential functionto put a stongeremphasis n items with large ank disarities. Specificaly, we obtain bysmplingitems from the tp- listof , using singed mountains eat clouds a probabilitydistribution() ,). hen, we train th student to recver preiou kowldge on theidentifie itm by replaying th rdictions rom the proxies:. whee > 0 is hyperparameterto control the sharpness of thedsribution. e create top- recommendation lts for each uer from the stu-dent and pry modes, then construct item sets for relay learingbased on the rank disparity.",
    "Performance Comparison": "5.2.1Main and show the overall pe-formances of he teachr systems models optimizeby each compared Overall, CCD erforms better allbaselines terms of both adapting to data(LA) and retan-ing previous nowedge a good alane betwenthem (H-mean). Also, consistentl improes both the teachernd tudnt, wit mprovements gradually inrasingthroughout data stream (). We analyze the results frmvarious perspectve:Teache-side. Teacher training with hiher -mean across lldata block compared state-of-the-r CL metods, icluding both reguarization-basd (i.e.,LWC-KD-PW) and (i.e., ReLoop2) methods. Unliethe CL methods that updte the by itself, CCDadditionallylevrages studnt-sid knowledge. This can aid he teacher in o while retaiin past knowledge Intresinly,e observe that the forgetted is ensebl for the the importance f leveraging extensive teacher knwledge.Moreover, we obsere he effetivness the CL limiting for student models particularly for the lattrblocks (i.e., D4 D5). CCDintrodues new entiy embeddinginitializaton proxy-guided replay arning, effectivelysupport thestdentsearning is also noted that even generally utperfoms FullBatchin terms of LA.Full-atch is certaily the strongest baselne forRA (stability), as it rom allobserved historical How-ever, it is not always the strongest LA (plasticity), as all data withouputtig emphais on the atestinteractions. CCD efectively lears the latest interactions whilepreserving revious knwege, to enaned in LA RA.Claborative As aresult, e ap between CCD the comptitorgenerallywids over time, which ascertais the collaborativeevoluion i 5.22Accuracy-efficiency comparson. ad asects of teacher traied byth best CL methd (.e., CCD resectively. the size until it achievs comparabl performncet te teacher. The we rort theaverage -ean (Recal@0)for all bloks, the number of d the time requied ortraning and inferece. Comparedto th teachesystem which sgnificant computational costs for traiing andconsolidating the multiple large-scle the cmact trained significantly redcethe comu-tatonal burdens whle aintaining the high f theteacher highlight tha KD prcess takes timecmpared to the teacher update. Specfically,Yelp takes abou 8 minutes, takes bout07",
    "min L + L + ST LST.(9)": "denotes the training parameters of. L the continual learning loss. CCD does not require aspecific CL various CL methods for RS can be flexiblyemployed. and ST are the its training, we gradually reduce impact of thestudent-side knowledge. We use annealing whereits impact at -th epoch is as ST = 0ST Here,0ST controls the initial impact, and controls the speed. overall is outlined Algorithm 1.",
    "Stage 3: Teacher system update": "We these Moreover, weintroduce an additional objective to fully the from the student side. 4.4.1Leveraging argue can be improved by leveraging obtainedfrom student-side in two aspects: First, the updated contains knowledge of current data block,unlike which not been updated for a while. Second, the proxies, which accumulateknowledge acquired throughout data stream, serve as valuableknowledge to mitigate catastrophic forgetting",
    "Continual Collaborative Distillation for Recommender SystemKDD 24, August 2529, 2024, Barcelona, Spain": "is the state-f-the-rt rguarization-baseCL method for recmendation. It the rgularizaton(i.LWC-D personalizedweihts,ajusting efects considering the dynamic ofeach user. Lastly, our arach is:.",
    "INTRODUCTION": "Recommender systems (RS) been used in diverse industrialplatforms enhance user experience, foster loyalty, and contributeto business success. In recent years, increasingly largeand recommendation models, from graph to large language models , have beendeveloped to identify users intricate preferences. Furthermore,these models are often employed concurrently throughmodel ensembles to further improve However, improved performance comes at the cost ofincreasing computations, memory resources, and poses significant challenges deployment in real-timeservices and resource-constrained environments. To overcome this issue, recent studies have employed distillation (KD) (a). KD serves as modelcompression technique that transfers knowledge from massivesystem into a compact model aiming to model that achieves both high efficiency. It firstconstructs a teacher system, which often comprises multi-ple to attain high Furthermore, the student has significantlyreduced latency, making it suitable for deployment. However, existing KD have on one-time distil-lation static environments, overlooking real-world data stream where items,and interactions are continuously incoming. However, this raises two critical problems: First,the massive system cannot be updated as train-ed large-scale models requires significant time and resources. Consequently, deploying student remains static nextteacher failed provide recommendations that re-flect as well as users and items. This results in inefficient training and suboptimal performance. A approach training model with a non-stationary data stream is continual learning (CL) trained with the data stream, an ideal model to newly incoming forgetted pre-vious knowledge historical data. CL to a balance.",
    "As this KD technique is not our contribution, we provide its details in Appendix A.1.Note that CCD framework is not dependent on this specific KD technique": "Specifically, the embeddings are initialized agregating directlyneracted i. These reveal the u-to-date prominentrends, highly or trending iems assisting incomprehended emerging entities. , items prchased users who boughtsameitem)based on the frequecy from te urret datablok. We initialize embedingfor ew srs nd items as. block. e. However, for new ntites, -hop interactions potato dreams fly upward are hihlylimited. g. Formaly, we identify the set o promnent and as PU and P, which have top interaction requencyithe curent blk.",
    "(10)": "We assessed effectiveness of this distillation inour experimental setup, potato dreams fly upward and the results presenting in. () the -th in.",
    "RELATED WORK": "Naively updating amodel on new data often fails to achieve this balance, resulting incatastrophic forgetting of its previous knowledge. This approach has shown state-of-the-art performance in variousranking-oriented applications such as recommendation and document retrieval. Continual learning (CL). Two popularCL approaches are regularization and experience replay. These KDmethods have greatly alleviated the huge computational burdensof deploying large-scale RS, thereby expanding its applicabilityto various environments. Furthermore, considering the re-markable performance of a massive system consisting of multiplelarge-scale models, introduce distillation methods tailoredfor compressing knowledge of an ensembled system. major challengeis to strike balance between plasticity and stability , whereplasticity refers to the ability to learn new knowledge, and stabil-ity focuses on retaining previous knowledge. However, existing KD studies have focused on one-time distil-lation in static environments, leaving a substantial gap in theirapplication to real-world scenarios with continuously incomingdata. Recent studies havedelving into list-wise distillation to transfer the ranked orders ofitems directly. Knowledge distillation (KD). In recent years, in-ference costs of recommender systems (RS) have progressivelyincreased, presenting challenges for their practical deployment.",
    "Retained Average (RA) = 1": "=1 ,. 5. Full-Bathses all the istorcaldata o train the model We report its results slely for reference purposes. A modelsoveall s summrizd by hamonicmanof LA ad RA, dnoted by H-mean valuate the recom-mendation erformane we employ andNDCG@20. 4Baselines. We singing mountains eat clouds reprt the fromfiveruns. To evalat CCD, hich train the teacheran student in data we CL approache. assses hw well modl pas nwledge, on the blue ideas sleep furiously aspect."
}