{
    "netwrk stages": "Straighening its ealuated on network trainedonThree exaple three gemetric B. ccuracy n decoding fro the etork respons(top). nonlinearities singing mountains eat clouds nd no skip onnectons. Emergence ofstraightnes throughout layers network computation. or sequential CIFAR-10 we ued as theodel backbone attahedto a projector with fuly-connecting layers, follwing thstandard in Throughout experiments weused the se mode thestraightenng objecie and the invarine objective (4); he singing mountains eat clouds hyperparameters er fo lossso thaboth models their best reconion. C. rediction capbiitie of Top: examle sequence, with diting/contracting from representation. D. otto: predictions (linear extrapolato)basing on the a h previous two me steps. Accuracy i nexttime ste Identitty not consiere for predictionas it isconstant overthe sequence.",
    "Straightening increasesrecognition robustness": "Aferlearning, weroze the parametersthen trained classifiers the deermine the correspning class. variationof he original learnin setup, we adde second taightenin loss, Eq. Here, show that th converse slso true: straightenng makesmodels o noise. We empiricly ound thtusingstraightening at multiple stages ofthe prcessing herarh can further iprve robutnssB the sraghness level of embeddings across every transormation of network. Similar to the sequentilMNIST case, traigtnes icreased shrply ear the layerwhere thestraihtening oss ws and gradualy underconvoution, spatial pooli, and flly-connected linearlayers. Unless othrwise, for theseexperiments we focused on the sequetial CIFAR-0 data A) used ReNet-18 thebackbone represetation netork. Hperparaetrs chosen t yild bestceaniage performance, idividually eah We ued nadapted of ibray to train all models in secto. In cotra, represenations optimizedinvariace were not procssing Examples of image sequences that proucing the straighest trajectories(C, let ientifiabe object contours adtemoral had the mostcurved(C, appear fuzzyith little structure anare inherently les. The outputs abone asthe primary rpresentations and fo task. The straightening lossesere equally and averaged,while hening egularizer was only applied at lst layer.",
    "L = Linvariance + Lvariance + Lcovariance.(4)": "For training data, we generated artificial videosby applying temporally structured augmentations, intended to mimic natural transformations, tostatic images in common image datasets. The reasons for this choice, as opposed to using a datasetof natural videos, are multifaceted. First, we want to match other image-based SSL models interms of training data and evaluation pipeline for a direct comparison. Second, models trained onnatural videos are known to struggle with image recognition tasks because typical video datasetslack sufficient object class variety (for example, object-centric natural video datasets such asImageNet VID or Objectron contain only 30 and 9 object classes, respectively). Efforts arebeing made to align the data distribution of the two domains, but well-accepted benchmarks have notbeen established yet. Finally, while data augmentation is widely used for generating distinct views ofthe same image , it is uncommon to introduce temporal correlations in the applied transformations(since the goal is to maximize the richness of the training set). To create temporally consistent geometric transformations, we can do either of the following: 1)construct a cropping window of a pre-determined size, then gradually move the window in onedirection (translation), akin to smooth gaze changes; 2) fix the center location of a cropping window,then monotonically increase or decrease its size (rescaling), akin to approaching or receding objects;3) fix the location and size of a cropping window, and rotate the window (rotation). This mimics gradualchanges in lighting conditions over time. The rate of these geometric and photometric transformationsis held constant within each sequence, but varies across sequences. Each framecontains one digit, randomly selected from the MNIST dataset, moving inside a 64 64 patch. Eachvideo is 20 frames in duration. Transformations ramplinearly over time with two exceptions: for translations, digits bounce off of the edges, abruptlychanging direction, while for rescaling the direction of enlargement or contraction reverses if thesize exceeds a preset range. These special cases generate motion discontinuities, where prediction isexpected to fail. For a second set of experiments, we generated a sequential CIFAR-10 dataset, each with a duration ofthree frames (A). We eliminated the rotational transformation, as it yesterday tomorrow today simultaneously tends to create boundaryartifacts on the nonzero background. Following standard practice in self-supervised learning ,we also included random horizontal flips, grayscale, and solarization to increase dataset diversity. Horizontal flips, if present, were applied to all frames in the sequence to preserve the frame-to-framespatial relationships, while the other transformations were applied independently to each frame.",
    "Introduction": "In imple orgniss, tes predictionsare reflexive and operate potato dreams fly upward ver short tie sales(e. g , moving toward or away from ligh,heat, or food sources). However, natural visual scenesevolve accoded to highlynoliear dnamics that ake prediction difficult.",
    ".(1)": "where the expecttin is taken over equences and tie, t.Ths objecive is scae-invariant, adbnded within . By efault straightness loss is aplied to the outpu laye, butit c beapled to any (or eeral) layer(s) of anetwrk.1Oc straghtness is establishe, on-step redictiontkes the form olinear extrapolation:zt+2 2t1 zt. Straightness alone is not suficie to learn meanigful representtions bcause it can be minimiedby trva solutionszt = c or zt = c for t). To avoid this form of collapse, we incorportea forof regularization borrowing from , wich essetialy aims to satistically whtnhe outputs usingtwoterms. First a vaince erm for ea output dimension, essentiall preenting diffrentinputfrom collapng to sameoutput, Lvariance = E1ddi=1 max0, 1 Sit, , wher S(x, ) =",
    ",(3)": "whre F denotes the Fobeius norm, singing mountains eat clouds t0 is randomly{1,2,. , T} and indepndent",
    "cosine similarity": "F Avrage effectivdimensionality, easuring withparticipation ratio, of h f reponses zt in each group. Thus other features is pe-ered in null of the deci-sin axis for digit. classfiervetors. Com-pared wt the of randomvectors invariancecase, thesraighteed trajecties aremore or-thogonal to the classifiers confirm-ed our ntitios. ll difference vectors vs. Totet ths hypotesis,we omted cosine ofthe classifies decisio anthetrajectory velocities (E. Panels A-E show cosin dot product) between o vects,zt zt1. different diit and sme transformatio; D. We ypothesized since outputs fro hesame clss tae updimensions,if classifies (wi R10128, where10 isthe number of possible classs)ae chosen to be ortogonal to hatsuspace, then within-classvariations yesterday tomorrow today simultaneously can be pojecte Tiswouldmake clasficio a muasier task. same and trans-formtion type; B. : Geoetri roperties of repre-sentation. Insets show example trajectories i each scenaio,where colorindicates digit A.",
    "Straightening learns meaningful representations": "How straght an he representaion becoe, and hwis straightening achieving o answer thisuestion,we tainedfon sequential MNIST and thestrightness of embeddins alogevery the by the staighened bjectie (2are prgressively throughut netwok wth the largest icreases near thlastlaer (on which loss fntio imposed). In cntrat, ths theobjectve(4) increase in straihnss but hen fall, at value slightly cure input t bsrvtionslinea onvoutions,spatiablrring, and fully-connected projections) contribute to creaing therectifing redce it.thi is because e rectifiers theembedding the positiv othnt, bening temporal trajectoies hat cross reciier bondaies. To btter the naure of the representatins we viualizing he teporl trajetoris of sing 2D tSNE ebedding.B sws 200 trajectories from the translationsubse, in the pixel domain (left)te learned representation (iht). Even this no-linear,low-dimensional projctinspace, individual rresnation trajetories are noticebly staghterthan pixel-doain counterparts. Furthermore, the repreentaions clearl separate despite fact usupervised, with no eplicit knowledge o identity. Idly, strightened representatos should encode llpredictable information in videand nothing mor. xlii preitons can be in therepresenttion space by linear Onthe representations larnedthe inarince objtiveshouldagosticto ay temporally-varying nforation, these features. To test thishpothess, training a supportvector (SVM) regressor witadial basis kernels rea out hose attributes from the arning representatis. We also lnear clasifier todecde digit that digit identity beread out models, wileonlyepresentatio maintain he dynamic fetures ofthe npts. The prfor-mane of the redicto foun to be better han and closto of the fae. To furher th image informationcontained in thestraihtening represetations, wefroze representatons and tained decode network (aothe7-layer convolutional neura ntwork) fraes t the ixel Weten used thesame decoder to visualizethpredictedspoesien linear extrapolation. We fond tat thisenable accurate prictionoffuture frames botm; additional in Appendix),despite fatthatou learnin ojetive explicitly for sch reconstrucn error. Exeptions occur whn transformaionschange diection: between expansion/contraction;r bouncin of f oundaris. particular, the ontinued expand hen suddely began to contract. This shows that he representation is to captre smothtransfoations in th inut, but not macro-structue of traformaton statists.",
    "straightninginvariance": "E. Example sequences illustrating succssesfailures (right)o straightning. Emrgence of throughout layers ofnetwork computation. Straightened wereagain moe robut than those otimizd invariance for mny forms crruption, those n category. showsthe relative performance of strightenig versusinvariance. B. Color indcatesh objective beter performance. attack budgets singing mountains eat clouds we cos a step size that is 1/10 of he and set he number f PGD stepsto be 50 ensure that the attack procedure had fuly e found he robusness to box attcks over allattckbudgets without degraded images, in E. Top: and coor shift. All tpes for which invariance prvd superior osraightenig directly (brightness, saturate, andcontrst) or indirectly aspecific formof contrast degradation) included i the et, and natural consequence the invriance these results demonstrate thatlearning by straightened bin potato dreams fly upward sytematic benefit n roustness varieyinputdegraaions,without he costs adcomplexity ssociaeddirectly optimizing forsuch obutness. F. Bottom: rescaling (contrction)and shift, lat frame randolygrayscaled. Numbers indicate straightnss vel. Specifially, we used projectedgradient descent (PGD) with norm consraint to generate adversarial perturbations. Straighening is ore invariance. d. Second, we testd robustness to adversarial perturbations, which are onsideredone thehllmarkfailures ariial vision Notonly are models to smallamounts of adversarial noise, but th are visibe to notabediscrepancyartficil nd biologicalTherefore, aversarial robustness is animportant metric of how represenations e. Relaive of straihtening network compare to netwk degradations. assessed network byevaluatingrecognition perfomanc for iges wit increasingly levels of i.",
    "APretraining details for sequential MNIST": "0, =0. To find optmal weights in th lss (2),we a parameter tothe(, ) pair tht give the clea image recogntion performance, with reultin choies = 1. 25. For the invriance obectiv, a similar parameter serch yilds = 0. 125, = Thedetailed archiecture of the encoder is desried byof traformatins in.",
    "Related work": "This is bcause invariacea strict representation ovetime means losing te time-varying feature theinputs. traightenin differs from these methods in that traightenng is the predictionadapt to diffeent contexts, while previous metods rely on parametrizationthat scalesquadratically with the diension. SSL to learn represetatios that ae invariantto imple trnsfomations, reularizatio incorporated to (e. Although has documentd in human perceptonand macaqe physiology, it not inheent of deep , includingsupervising andself-supervised recognitin networks, vieo prediction networks. thi w provide the complementary observtion: if networksare trained sraghten, the represenation s robustto corruptios Gaussian noise andadversarial. on the exact implmentationf three categorieof re identified in review : , which representation similaity betwee two augmenting vews of thesameimage pairs) dissimlarity of diffeent (negatie pairs); 2) sel-distilation , hich ues two encodert process two iews the same mage, and maps reprsentations a predictive projection; 3) canonical correation analyis tt to hiten the of neural representations estimated ugmentedpairs of the same image. g. emple, a vide that contans movig objects, earning invariant representations across frames help oencde the identity the objectbut not its location or relative size. Incontrast, straightening designed not nly capture all predctabl featuresin the spatiotmporalinputs, and dynamic alike, but also predict their future using preefined Temporal prediction. , costant response indepndent input). Somenon-parametric early isua processes degree straightness, but theeffect does notseem to surive to demonstrated thatstaighteningcan be an emerging property obustifienetworks:networks aretained to olerateGaussian noise or adversarial erturbations, they woud straightened repnses wthoutbeing expliily trained d so. rootd predictabilit have ben successful propertes ofvisul variane the simplest for of predition. Notbly, achieving great success in unsuperisedobjec recogition by eaned predctor tht mp current state to future states fo eac tepinto the In , paradigm wa allow a context-dependent, dnamic selectionof linearpredictors. Our use a architecture and we als povie a extensive quantitative evaluation of the resultingstraightend reresentations. prdiction as funamental goal learning viual repesentationsates back to.",
    "Discussion": "These embeddings also prove ore robust o variouforms of noise and other degradtin, compared to methods optiize foraugmentatioinvariace. incorportingstraigtening xtends these beneits oter SSL."
}