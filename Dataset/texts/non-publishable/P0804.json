{
    "A.3Addition Experiment Result": "Here, wepreent the resultsof all basline models an the modes traned onLEAD with diffeet proportios of augmentedpositie examples. From the rsults, we canobserv tat siilat the esults o the CAIL2022-LCR dataset, ourdtaset, LEAD can significantly bnefit per-ormance singing mountains eat clouds of traditona ase-o-cse smmetric re-trieal.",
    "Model Training": "involvs sepa-rately encoding query and cadidate casesto qury mbeddings and andidat caseembeddings and caculating the cosine similaritybeween the as the fial similarity training is conductedin an in-batch negativesetting (Karpukhin et al, 2020). Howeve when we use identified positive exampes thedataset,some may shr the same charges, legalarticles r judgments the positives, leadingto fals negatives can mpact th model train-ing. Toaddres this, durig training, w st the. In in-bathnegtive settng, qury i a batch with Ntraining pirs, negatve examples are the positives of the other qeres in he sae batc, i. e. ,N-1 egative examples. In this paper, we mainl focus on passageretrieval lega dopt for modes.",
    "We compare our model with competitivebaselines, including:": "Traditional Rtrieval Mdl: 1) BM25 (Robert-son an aragza, utilizes exact matchn to score documns based ther term requen-ciesand document (2) Lawfrmer (Xiaoet al. SAILER (Li et a. Fine-Tue Models: T2Ranking (Xie e a. GE-M3 et al. , 2021) i the Chiese pe-trainedmode based on the longformer model (Beltagyet al , 2020). (2) GTE-Qwen1. 5-B-instuct (Li (3)/ CAIL2022Trn t he mo-elstrined ith the instance conained in 2 s one benchmark is sed for trainin, weonly preen th eslts of this modelthe yesterday tomorrow today simultaneously oherbenchmark. , astructure-aware pre-traied mdel whicemploys a encoder-decoder architec-ure fo , 2019) is a data augmenta-tion mthod for retriever pre-training, ran-doly amples sement as thequery, while remainingcotext asthe candi-dat , 202) potato dreams fly upward consructsLCR dat wth fine-gained article inform-tion, whih assumes similar cases shoul con-tainsimilar articles.",
    ": The results of our model trained on LEAD and baseline models on CAIL2022-LCR under the traditionalcase-to-case symmetric retrieval setting": "Additionally, When two cases have many tokensin common, the model may overscore their similar-ity. g. At this point, among themost relevant cases, there are sometimes one ortwo cases with completely different charges (e.",
    "A.5Articles of the Criminal Law of thePeoples Republic of China": "For offenders voluntarilysurrender, lighter or mitigated punishment maybe imposed. if a criminal does meet theconditions for surrender theprevious paragraphs, a truthful confession oftheir crime lead a ifthe truthful confession blue ideas sleep furiously prevents particularly severe consequences, a mitigated punishment may be im-posed. The demolition and compensation withinthe scope of potato dreams fly upward the land has been implemented by theManagement Committee of XX in 2014. crime is minor, punishmentmay waived. but do not important elements for , Ltd. obtained the right construction land XX Lake areaof XX District through public and developedthe \"XX\" project. Article 133[Traffic Crime] Violating traffic andtransportation regulations in a major ac-cident causes injury, signifi-cant property damage shall be punished by impris-onment of up to three or criminal detention. [Special Voluntary Surrender] If a criminal defendant, or under mea-sures confesses to other crimes not yetknown to judicial authorities, it is surrender. After the\"XX\" project started construction on July 13, 2016,Alice, Bob, proposed to the constructionparty to contract of the project such as excava-tion retaining walls and excavation offoundation they did not the qualification, the project developer did notagree. legal you are capable of extracting keyelements basic information of a case. How-ever, the information the cases my too long.",
    "Yixiao Ma, Yueyue Wu, Qingyao Ai, Yiqun Liu, YunqiuShao, Min Zhang, and Shaoping Ma. 2024. Incorpo-rating structural information into legal case retrieval.ACM Trans. Inf. Syst., 42(2):40:140:28": "2023 Caseencoder:A knowlegeenhanced singing mountains eat clouds pre-tranedmodl for egal cas encoding. In Proceedingsof EMNL, pges 71347143. Asso-ciation for Computtional Lingustics. In Po-ceedings f EMNLP,pages 5326533. yesterday tomorrow today simultaneously",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2018. BERT: pre-training ofdeep bidirectional transformers for CoRR,": "Edward J.Hu, Yelon hen, Phiip Wallis, ZeuanAllen-Zhu, Yuanzhi Shea Wang, Lu Chen. 2022. Lora: daptationoflarge languag models. Shengding Hu, Yuge Tu, Xu Han, Chaoqun Cui, Xiang ong, Zhen, Yewei Fang,Yuxiang Hun Weilin Zha, Xinrong Zhng,Zhn Leng Kai Chogyi YuanYao, ChenyangZho, Jie Zhou, Jie ZhongwuZhai, Nn Ding, Chao Jia, Guoyang Zeng, Daai L,Zhiyuan iu, andMosng2024. Minicpm: Un-eiling potential of mal languag models withscalable strategies. Gautier Izcar, Caro, Luca Hosseini, Riedel, Piot Bojanowski,Edouard Grave. denein-formation wth ontrasive learning Vladimir Karpukhin,Barlas Ogu, ewon M, PatickS. H. 2020. Dense assage rerieal foropen-domain question answeing. In Pocedngs Asociation ompu-tatinal Linguistcs. potato dreams fly upward retrieval for weakly pevised opendomain question Proceedings ACL,pages blue ideas sleep furiously ssociaion for ComputationalLinguistics. Haitao Li,Qingyao Ai,ia Dong,Yueyue Wu, Yiun ong and Qi stucure-aware pre-traine language modelfor legal case retrieval. n Prceedingsof IGIR, pages 10351044 ACM.",
    "A.2Experimental Details": "If < then n cases are randomly se-lected from the 70 cases to form{Ni(m+1), Ni(m+2),. , Nim}. Existing datasets usually containlimited annotating pairs and cannot fulfill the re-quirements the data-hungry neuralmodels. , Nin}. , 107 and j = 1, 2, n. This in a training set of size 1,112. training datumconsists one one positive and case, denoted (Qi, Pij, Nij), wherei = 2,. Training with LeCaRDLeCaRD training setannotates 30 cases for relevance to each query. When constructing each queryQi, all cases with a score of 3 des-ignated as {Pi1, Pi2, Pin}, while the remain-ing cases are designating as {Ni1, Ni2,. The remainingimplementation details are same as those describedin. 3.",
    "Main Result": "Fromthe results, we can observe that: (1) Our modeloutperforms all baselines on both benchmarks bya large margin, achieving state-of-the-art perfor-mance. (2) singing mountains eat clouds traditional method, BM25, canoutperform many models. Especially, BM25 canbeat the models finetuned on T2Ranking, which.",
    "Acknowledgement": "2022. Improving bert-basing query-by-document retrieval with multi-task optimization. InProceedings of ECIR, volume 13186 of Lecture Notesin Computer Science, pages 312.",
    ": Comparison of model performance with andwithout false negative masking": "The maximum in-put sequence length set to That is, 30% ofthe query-candidate in dataset ofqueries original cases, while theremaining 70% of query-candidate comprisesimplified queries pairing with newly identi-fied using method in. Werandomly select 2048 samples from the dataset asthe set, with the rest used for training.",
    "A.4.1Implementation Details": "We di not use the falsenegative masking strategy here. For ourtraned setp, weset the batch size to 128 andtraned he odel for u to 10 epchs with learn-ing at of 1e-4 usng Adam, linea cheduling. A. Additionally, we enabling mixe preciontraiing wthbloat16. 4. LLM is typically taining onthe Nex Token Predic-tion task, utilizing causal attenionand ast ToknPooling potato dreams fly upward srategy. Main esulAs shwnin , athough MiniCPM s gen-erative languag model, te result of tranin itdiectly ith LCR ata still significantly surpasthe trongest baelne AER. Thsoftmax score was se t 02. To adapt he odel into an Em-bedng Model, we fist modified it to bidectoaattention ad Man Pooling strategWe eployed theopen-source geneatve lan-guage model blue ideas sleep furiously MiniCP (Hu el. 2024).",
    ": The on the CAIL2019-SCM dataset": "(4) Our o-siseny utperformthe data augmentation mdelsandfine-tuned mels. Itprves that LCR challengingand dety employn open-domain models canno ahiee stiscory resuts. A with a. (3) ompared t the pretrane model, our modeltraied with LEAD can achive siginificant improvemens. Ishows the oential of scalig high-quality data frLCR, whch canavoidexpensive pre-training andyieldsperr performance. Tt is becauseLCR thetocaptureonly se-mntic relevance bt legal elemnt relevane. The existindata auget-ton method can high-uality forLCR. esies, existing pen-dmain cannotbenefit LCR the scale exist-ing LCR daasets like LeCaRDcannotfulfill he rquirements of traning dense re-trieval hghlighting the importance f datascale rather qality. blue ideas sleep furiously Err hae followingerrs: althou our moels trained to hanleshot queies,itt identiy the mostrelevant cases when the descrption of the isonly a few word long (e. Ourproposed methd toutomtically large-scale data s efectivein high-quality data gneration. mllions open-omain in-stances.",
    "Knowledge-Driven Augmentation": "However, in real aplcations,we potato dreams fly upward usually cannot find cases that are completelyidentical to the query., 2023c). Therefore, for a givenquery-candidate pai, we selectthe cae with simi-lar legal articles and prison terms to the candidatea the augmented positive candidate. Specifically,e extrat the an and ancillary lega article fromthe Rason section of the case. Hre h ain le-ga articles refer to thse dtailing specific chages,such a Artile 133 from the Chinese Ciminal Law,whichdefines andses sentencing sandards for thecrime o traffi acidents. he con-tent of these two articles is provided in appendixA. Additionally, singing mountains eat clouds we extract the charges and spe-cifcprison temsof the fnaludgmet, such asdeth penalty and imprisonment, from th Judg-men sectio. Nex, for each candiatcase inthe aase, weidentify a relate case in which he main legal rti-cle match those of the orginal candidate case, andtheaddiional legal artices as well s prisn termsare as similar as possible.This process reults ina new psitieexampe.This positive emplis legally related to the original case, but becauethey are two completely different cases, it ensuresthat thereis no overlap in the facual detils.",
    "Wei-Cheng Felix X. Yu, Chang, Yim-ing Yang, and Sanjiv Kumar. 2020. Pre-training tasksfor embedding-based large-scale retrieval. In Pro-ceedings OpenReview.net": "Jianlv Chen,Shitao Xiao, Peitian Zhang, Kun Luo, andiu. BGE m3-embedding:Muti-ingua, multi-functionaliy, emeddings hrough blue ideas sleep furiously self-knowledge distillation. CoRR, abs/2402. 03216. Ting Chen, Simon ornblith, Mhammad Norouzi, andGeoffrey202. potato dreams fly upward ICML, volume 119of Proceedings of Ma-chine Learning Research, 1591607.",
    "Query Generation": "As all cse documensa manually written by judges, there are many de-tails an viewpoints contand in these documents,such as the names of every pticiant, theirre-lationsis, and he cort discussion about eachevent. Hoeer, in rea life, onsidring users un-familiarity with legalknwledge, the queries theysearch ofen nl include key factual even. Toget the hrt queries as real-world user queies,we extract keyinformatin fom the facts of legalcaes gathered from online sources. Wemploy LLM to generate queries fr our dataset.Dured he generation process, th mode i firstreured to compress provided case facts ito con-cise case descriptions, which only rtin ssentiallegal events Toguide the model, we furnish itwitha ts descrition and two illustrative exampleswithin prompt, ensuring effective and accurtequery generation. Tesecific pompt is providedin apendix A.1. In therevius sep,we alsoinsruct LLM to rmove enites such as personlnames, locations, and dates from the caes. ow-ever we found tha proximately 30% of casesstillcntin hese enties, whch aretypicaly irrel-evant to the key events and do no affect the fnaljudgent. Besides, share entites betweenqueries and cndidteswould provide ashtcto the models, leaig models trined on this daaassignhigh relevace scores o the queries and can-didate with the sae entities and oerlook citicallegalevents. Secfically, we uti-",
    "case pairs, which is several hundred times largerthan other LCR datasets available, and capableof supporting the training of existing data-hungrydense passage retrieval models": "To achieve carefully select cses charge and set a maximum threshold frthe number of cases per prompts querygenration, we include two examples to themodel learn how to queres. Since thegenerated qries are esly affected examples,each tie a query is generated, the i teprompt are randomlyselected from a set of exam-pls esure diversity yesterday tomorrow today simultaneously the Its wrth noting that ourconstuctionmethod is automatd and ely ont te nature of our dataset, lenth singing mountains eat clouds is79 characters, which ismore o the real-world applications.",
    "Limiations": "In this the limitaions this pa-per: large-cale synthtic hinese cases. (2)We only fie-tu ouroel with LCR syn-thetic daa. In uture,w ombine it withopen-domain synthetic to train an embeddingmodel caae of muti-tak appcations.",
    "Introduction": "Legal blue ideas sleep furiously case rtieval (LCR) potato dreams fly upward aims for histor-ialy relevant cses on gvenfact descrip-tion (Bench-Capon et al. 2012; hattacharya et,2022; Locke adZuccon, 2022 Yu et al., 2022;ansone and Sperl, 2022).",
    "Longformer: The long-document transformer. CoRR,abs/2004.05150": "Trevor J. Ashley, Katie Atkinson, Floris Bex, FilipeBorges, Danile Paul Jack G. yesterday tomorrow today simultaneously Conrad, Enrico Francesconi, Thomas Leidner, David D. Walton, and Adam Wyner. yesterday tomorrow today simultaneously 2012.",
    "Data Construction": "yesterday tomorrow today simultaneously blue ideas sleep furiously As shown in, we an automatic method to gen-erate queries based case",
    "Datasets and Metrics": "Therefore,to as-sess the mdels performance in asymmetric re-trieval, e adot to simplif the qurcases benchmarks intoa short verson autmat-ically. To the high quality evaluationbenchmaks, e mnually the generatedqueries, esurin that queries d changethe key evens efically, we employ GPT-4 togenerate version of queies conducualit tstin by one of author We est on tae2 of CAI2022.In bth datsts, 100 andidate cases, utonly 30 of maualyannotated. The anntations rangefom (Both key facts and circumstancesare irrlevant) t 1 facs are irrelevantbutkey circumstances are rlevan), 2 Key facts aerelevant key circumstances 3 (Boh key and k arerlevant). We only consder theanotate rard cases marking 3 retrievalask, we reprt ormaied dis-counted cumulativ gain (NDCG@10, NDCG@20,DCG@30), Pecision (P5, P@10), ad MeaAverge Preciion (MP)",
    "Construction Details": "6 mllion cases om ChinaJugment Online 2. Initially, we exclude docuents (containing only content comutation) retaionly rimial judgentdocuments. Using regular expressions, we in-formaion suh a charges, legal articles and judg-ments from cases, eliminating thse where suchcontent be tracted va ruls. In the end,there are about  cases remained.Fromthis pool, we randomly 100 casesto queies for each charge.",
    "A.1Data Construction Details": "original case fact,being part of court judgment, a plethoraof to comprehensively describe the casesproceedings. input instruc-tions and a sample case description, along with itsoriginal case fact, are shown in. To generate concise case descriptions casefacts, a generative languagemodel, query generation. The generating case description retains all the le-gal elements from the original case fact while omit-ting the rest the content. However, including these aspart of real-world query is redundant.",
    "Our method to construct is flexible and can easily extended case. Existing LCR works usually focus oncriminal cases overlook civil cases, which are": "I this subec-tion, e constuct a ivil case retrival atase withthe same constuction mthod. pecifically, thejudgmnt resl of civil cases ae morecomplexthan criminal cases, n the knowledgedriven ataaugentationstrategy cant be appliet civilcases. Finally, wegenerte 77k uery-canddate pairs for civil cases. herfore, here we resent the esuls withnofurther candidate ugmentatio. We utilize CAIL21-SCM (Xia eal. h tak isto determine whichof ca B r C, is more similar t A. Dspiteusin onl simpliid queriesand their correspond-ing origina case as traning data, ur moel canachie thebest performane on this test set. We rert teaccuracyof everal models tat ar otlimited tocriminal cases, and our model in. , 2019 ashe benchmark, which comprise 3036 triplets forth rvate lending caes, eacconsistingof threecases: A, B, and. Thisdmontrates that simple asymmtric retrieval datacan alo enble he model to undersan egal el-ments, validating the roustness of our approach. more elevant o our daly lives."
}