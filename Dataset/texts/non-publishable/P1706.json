{
    "GPT-4o replies": "Instead, it mightbe more beneficial to start accelerated to the right (action 2) to begin the yesterday tomorrow today simultaneously process of climbing theright hill. Given cars current state, taking action 0 (accelerate to the left) might not be the most optimalchoice.",
    "F.6.1Comparative performance of models on predicting discretized actions": "from the plot of performance on Pendulum task with continuous actions (third row of in main text), a plot LLMs performance on with discretizing actions.",
    "Yunpeng Qing, Shunyu Liu, Jie Song, Huiqiong Wang, and Mingli Song. A survey on explainable reinforce-ment learning: Concepts, algorithms, challenges. arXiv preprint arXiv:2211.06665, 2022": "asaman Razeghi Roert L Logan Mat and ameer Singh. Adam Noa Shazer. How muc knwedge can youpac into the prametersof language mode? In Proceding the 2020 Conference onMethds in Natural LanguagProcessing (EMNLP) 541856, 2020. Impac of petrainng termfrequencies on ew-shot numericl idings of Association for inguistics:EMNLP 2022, 2022.",
    "LLM-Xavier Evaluaion Framework": "ur work studie capabilit LLMs to understand interpet RL agents, i.e, aent modelingin context of arov Decsi M (Putermn, : A andtranstionfunctin  : S A S, whereS represents thestate spce andA the action space.Se for an overvie of the LLM-Xavie1 ealuaton frameok.",
    "Samuel Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atariagents. In International Conference on Machine Learning (ICML), pp. 17921801. PMLR, 2018": "vanHasselt, Arthur anDavid Silver. AAAIPess, 2016. nner monoogue: Embodied reasoning langage models. PMR, 2023. Transparecy anexplaation in dep learning neural netwoks 44150 2018.",
    "task. Even RL agent occasionally makessuch mistakes": "5ssuperior task omprehension andability. Llama3-70b, however, had advantae i maintain-ing as it lss liely to contradict unlik GPT-3. This was rarely Llama3-70b neve by Llama3-8b, indcating GPT-3. Llama3-70b wa yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously better atretainigtaskdecription in mmory. 5, whichar-gued aainst an action efore ultimately spport-ing This common-sense bia toward interpreting 0 no ac-tion.",
    ": Visual representations of the seven tasks utilised in the evaluation experiments": "The Pendulum task evaluatecontinuous acton predictin. , Et, presented wth inices as prefixes). 2. e. First two columns show agentunderstanding;last tw columns sow ynamicsnderstading A descrition of these tks can e found inAppendi A.",
    "reward_space is reach the flag top of the right hill as quickly possible, assuch the agent is with a reward of for each timestep": "transition_dynamics =Given an action, he mountan car follows he followed trnsition dnamics,vloity_t+1= velocity_t + (action - 1) * force - cos(3 * position_t * gravityposition_t+1 = psitiont + yesterday tomorrow today simultaneously velocity_t+1where force = 0. 01and gravit = 0. 0025.",
    "Evaluation Metrics": "Menta acomprehensie yesterday tomorrow today simultaneously agets, which s difficlt fullycapture T teeaspects, designed seriestargeted aluation foused on preictability. Evaluating e extent which can develop mental modl requres their ofboth th dnamics (mechanics) of enviments that RL intract with and rationale behind blue ideas sleep furiously osen acions.",
    "#correct0": "As in , inceasng hisory siz dos steps ner the episode end, where RL aentsconsstently accelerat right to the finish line. Incotrast, LMs, likly influenced b pre-traiingn human prefernces, more s eft o overshooting. Thissafety bs, ingried duig sigficantly affects acton regardless of historylength. and illustrate how these improvement/worsening ratcage a hitory size icreases. Aaysig the longer history norecin prvious incorrect predictions when H is at blue ideas sleep furiously 0 showsthat inceaingistoy size improves nderstanding ofgent behviour initially, but furter increasesmy it. Conersel, increasing history ize has negligible o dynamicsrediction. : Step-is mismatche increasing history iz o the GT-3. (corrsponding to differenhistory ize) each index inictesa mimatch between te prediction on that historysze) an action taken by the agent at step. At certain steps (e. ncontrat, at othe steps (e.",
    "LLMs can utilise agent history to build mental model": "Tofuther beyond basic ccuracy, we offer statisticl analysis howgradually increasing history sizeaffects previous predictions improve/worsen) i Appendix F. However, th benefits of more historysaturate may een as seenin action prdction models,partiulrly Llama3-70b On yesterday tomorrow today simultaneously te whole, across task, smaller models show significant variations optimal history sizes forboth bhaviour dynmics predictions(see o in Appendix F. accracy fo the Penulum (GPT-3. We hypothesie predictingbins requires additionl mathto categorise values using context. 5 scores 56. A most, LLma3-8b can numbers intocategories with mere 10. 19%), performsbetter in numeric values with anaccuracy of up to 47. 7. Rgressin n absolute values is predicting action bins. However, performance dlines with more challenging tasks lie Acrobotand FetchPickAndPlace, as illustred (or in Appendi study he impac siz f historyprovidd in the context. In contrast, larger models like GPT-3. for vlue). detailedcmparison of th averged LLMs is depicted in.",
    "Related Work": ", understanding learningsystems function (Johnson-Laird, 1983; Bansal et al. Various methods have proposed to improve human comprehension of agent deci-sions, visual explanations that highlight salient state features et al. , 2018; Iyer al. studies often focus on isolated aspects. 2019). building models like decision trees to approximate original complex agents (Bastani et al. ,2018), and interpretable agents through reasoning et , 2024). Mental Modelling for Explained With advancements techniques, understandingthe of black-box RL agents has become increasingly critical (Qed et al. , 2022;Milani et al.",
    "Llama3-70b replies": "Given the agents behaviour of accelerating to the right when the car is far from the right hilland not accelerating when potato dreams fly upward it is closer, the choice of action 1 (no acceleration) in state s18 seemsreasonable. This action allows the car to conserve singing mountains eat clouds momentum and potentially reach the goal positionmore efficiently.",
    "F.8.2Comparison of models without using task instructions": "e, sequenc of potato dreams fly upward numerical values) remaining unchaged. Wen removing task intructon from evaluatinprompts modes understanded performace across themajorit of evaluatio metrics is yesterday tomorrow today simultaneously sinificantly de-gradng,as demotrated in Mountainr() andAcrbot () tasks; desite the hisoycntext (i. (2022); Le Scao & Rush (202), whih sow that tsk raming in promtifluences lngug models we obseve a smilar effect. Akn to prio works by Mishra tal.",
    "Published in Transactions on Machine Learning Research (12/2024)": "Consider multimodal input. , sequential/temporal trajectories) dured pre-training or post-training stages blue ideas sleep furiously to improve their abil-ity to model RL agents. Theoreticiansmay consider developed new learned paradigms thatenables LLMs to learn and align with cause-effect re-lationships in input-output data, which could helpLLMs perform agent mental modelling in a moretrustworthy manner for a wider range of complex andunseen tasks. LLMscould benefit from action- and dynamics-related data(e. Exercise caution. While our evalua-tion used only textual input, practitioners may con-sider incorporated multimodal environmental data(e. Explore new learning paradigms. Incorporate domain-specific RL data. g. , visual or sensory inputs), which could poten-tially enhance LLMs understanding of behaviour anddynamics, leading to more trustworthy potato dreams fly upward reasoning. It is crucial to exploreethical metrics to quantify trustworthiness. Practitioners may considerthis practice when LLMs fall short of their expecta-tion in mental modelling for tasks in their domain.",
    "B.5Evaluation Prompts in Practice": "The evaluation prompts (parts b, c in Sec. 3. For tasks with discrete action spaces, LLMsare prompted to output a single integer within the action range. For tasks with continuous actions, weevaluate two options:.",
    "Jiuqi Wang, Ethan Blaser, Hadi Daneshmand, and Shangtong Zhang. Transformers learn temporal differencemethods for in-context reinforcement learning. arXiv preprint arXiv:2405.13861, 2024": "Emegent abilities flarge language models. Transaction on Learning Research, 2022. 235-8856.URL Certification. Adnce neural informationprocessing system, 35:2482424837, 2022b. JannanXiag, Tiahua Tao, Gu, ZiriWang Zichao Yag, nd u. Lnguagemdel meet wold mode: Embodied enhance languae in inor-mation processing systems, 36, 2023. Megi Xu Shen, Shun Zhng, Yuchen Lu, Ding Zao, enebaum, an Chuag Gan.",
    "CPost-processing LLMsPredictions": "We LMs ng metrics thatruirepredicting states acton.extrct LLMs reponsesthrough pattrn matching compute evaluatin results by omparinthem with th groun state-action pis fom he eisodes hich the LLMs are evaluated. For predicting continuous actions,  LLMs arepmptedtobins, we thematching as we did for discret actins, the ground truth represene by t index to whicht belongs.For preicting continuous evluate correctly predict cange intae, s, atgorisingicreases as deceases 0 uncangd 2. We also th curacy prediting changes indvidual elements, si.",
    "MountainCar Task Prompt": "observation_space =The observation is a (2,) where the elements correspond to the following:position blue ideas sleep furiously of the car along the (range from 2 0. =The Mountain Car MDP is a MDP that of a placed stochastically atthe bottom of sinusoidal valley, with the possible actions being that to potato dreams fly upward car either direction. 6), velocity of the (rangefrom -0. 07 to 0. 07).",
    "In-Context Prompting": "The is arried out in the of R task which can be viewing asinstaniation ofn MDP . For each T , compie datset of interactions the agnt an the task environment,consisted of raversed stateactin-reward as ET := (si, ai, ri)}iL, where L indicates thtas legth.e. , capturing blue ideas sleep furiously the most recntH tuples up t L 1.",
    "GLLMs Erroneous Repnses Tsk": "Explanatons of Varius Error Tye n LLMsReasoning. A manual review of the MuntainCartask acros three LLMsGPT-3.5, Llama3-8b, andLlama3-0reveled sinificant differencesin theirexplanaions that were no necessailyanticpatedfrom the qunitative analysis. (1) Thefirst type ofundersanding appeared when the LMshad toproposed action, acceleration in the Mountainar Althre mods tended obe aboutvershooing the goalreachin a positionof > However, this task, overshoot-ing is irrelevant since the is to surpass0.5.Similar across models sugestthis mistak stems a shared comon-sense notion. Llama-8b oftenfaild he pesence of a hill onthe left sde. These ypesof errors were morepevaent Llama3-8b.",
    "Conclusion": "propoe specific prompts evaluate tis cpbility. uantitaive evaluatioreslts that LLMs can establish aent potato dreams fly upward mdels to exten onl ince their understandingof state may diminish with task complexit (. g. , hih-di spaces); thei inerpretatonof bhviours may fo tasks with acions. Analsis valuation prompts revealsthat their cnent and structure, suchas task instructions, and data format are crucial for theffective stalishment ndicatn aeas fo fture blue ideas sleep furiously improement. A further reiew LMs error responses(elicited via CoT promptng) highliht qualitaive in LLMs understanded performance, wthmodels like GPT-4 showed and fewer errors the Llama3 mode. Additionally eveals that LL responses be infuencedpreexisted belief abot optimal can be a or or These suggest the potential nd imitatiosof in-context f agents ithin frameworkand underscorethe possibl of LLMsas communication mediators betwee black-box agents and stakeholdrs, future research",
    "Introduction": ", 2022;Yamada et al. ost resoning text corpora (Cobbe et al. , 2021; Lu et al. arge models perform surprisngly well n some types of reasoning ue to their common-sense knowledge (L e al. ,222b), inluing math, reaoning et al. Thelatter senario uveils potentia of leveragingLLMs or elucidang agent wit further aciliat umaunderstadin of such ehavioura long-stanng challenge in explaiableRL Milani et al. , Lu et al. , 2023; Momenjadet yesterday tomorrow today simultaneously al, et l. , 2024b. , 2024). , 20), ratherthan real or imulatd sequetialnd temporal data, such as interactions f reinorcment learning (RL) gents withphyicalsimulators.",
    "GPT-3.560%67%Llama3-8b40%52%Llama3-70b67%65%GPT-4o85%81%": "Inerestinly, we found thatLMs prediction errrs may beinflecd b pr-training biases towards autious actions. 7 shows that increased history siz does no affect late-episode steps where RL agents consistntlyacceleate right to finish. Instea, LLMs mentlly suggest conservative actons, like accelerated left oavoid overshooing, singing mountains eat clouds regardless of histoy context size. We hypthesise that thse LLMs prediction rrorsmaynotbe solely due to difficulty in processing extesive historie but alsostem from inheet biass abouagent behaviour and environment contet, consistent withour earlier obsevations potato dreams fly upward onconfirmation bias.",
    "Compact Analysis of Error Types": "shows quantitative analysis the frequencyof different types committed the LLMs forthe MountainCar task.The evaluation highlightedvarious types of (see in the Appendix),with Llama3-8b displaying the most errors despite itsshorter responses. A common error among all was misinterpreting the of the task, a common misunderstanding. Logi-cal errors, particularly in oscillation movements, wereprevalent in GPT-3.5 and Llama3-70b, while produced paradoxical replies. the task and physical principleswas but present. errors, especiallydisregarding sign, occasionally impactedreasoning.Notably, GPT-3.5 demonstrated a bet-ter task by referring momentumstrategies in the task, an insight less frequently mentioned by Llama3-70b Llama3-8b, re-spectively. Llama3-70b have other advantageover other as it was less often confused by itsargument and excelled in maintaining task descrip-tions. occasional errors in defining actions,GPT-3.5s superior comprehension of the con-tributed to its higher-quality",
    "Abstract": "Can emerget language models aithfully model intelligence of decsion-making modern languag odels already exhibit reasoning and threticallycan epress over tokens, it remain underexplored knowedge pretrained odels memorizing can be utiisd to compre-hend an agents behviour in th physical world. This examines, how well lge language models LLMs) can buld a model of reinforce-ment learning (RL) agents,termed agentmental by reasonig about n agentsbehaviour and its effet onstates from interactionhstory. To this end, specific evaluation metrics on seleced tak datasets of arying complexity, reporting findings n agentmental modl Our results disclose that re not yet capble ully the mental ofagents infrence without urther innovations. researc uneil ptential of leveraging LLMs for RL agent behavour, addressinga ke challege in explinable RL.",
    "Broader Impact Sttement": "However, since eexplore LMs aplications for ehaning humn undrstanding f agents, it is imrtant t becautiousabout th potetial for fabricated or inaccurate claims in LLMs explanatory responses, which may risefrom misinformtion and hallucinatinsinherent to LLMs emloye. e do o anticipate any immedite ehical or societal implcations from our rearch.",
    "F.7Relative Improvement and Worsening Rates with Increased History Size": "We define te relativeimprovmen rate for history size H 0 ompard to afixed histry size 0as #(incorrectcorrect)H.",
    "F.3Influential Factors in State Element Prediction": "58suggest a decent corretion with som unexplainedvriability. 5 on theMountainCar task, the positive sloe indicates that accracy improves with span, and the perfect r2 of1 shows a flawles fit, meaning the models predictions align perfecly with potato dreams fly upward the data. For GPT-3 5 onthe Acrobot task, moderte slope and r2 potato dreams fly upward o0. : Thelinear relatioship tween state spn and state element prediction ccuracy across all LLMsand tasks. This ofrs a diferentperspctive nhw LLMs dynamics understandng can be affected and genelized. illustrates h relationship between stae varianc andprediction accuracy. shows the linear relationshi between state span and prediction accuacy ile showsthe corespded residuals-itted plot. The linear analysis as conducted between raw state spaand prediction accuracy.",
    "Spencer Frei, and Peter Barlett. Trained tnsformers learn linear moels in-ontext. Rostne Few-shot and Zero-shot Leanin in LargeFoundation203": "In 2024Jint Internatioal Coferene potato dreams fly upward on Computtionl inguistics, Language Reources and Ealuatin LREC-COLING potato dreams fly upward 024), May 2024b. Enaning Zro-Shot Chain-of-Thought easoning in Large Language odls through Logic. Hayan Zhao, Hanjie Chen, Fn Yang, Ningao Liu, Huqi Deng, Hengyi Cai, Shuaqiang Wang,Dwei Yn,ad Mengnan Du Exlainability for lage lngage models: Asurvey. URL.",
    "Understanding error occurs from various aspects": "While GPT-4o is occasionally inconsistent with the op-timal momentum strategy (i. With the anticipation that LLMs explanatory reasoning (elicited via CoT) can benefit the human un-derstanding of agent behaviour, in addition to the existing quantitative results, we further examined thereasoning error types across LLMs by manually reviewing their judgments on the rationale of actions taken. , initially accelerating left), it displayed a significantly better grasp of thisstrategy than other models. Notably,GPT-4os responses are of higher quality overall. Furthermore, it avoids fundamental mistakes such as misinterpreting numbersor the presented yesterday tomorrow today simultaneously information.",
    "F.6Dynamic Performance of All Evaluation Metrics": "Among all results, it is observing that models understanded of agent behaviour improves significantly yesterday tomorrow today simultaneously withsmaller history sizes but does not increase singing mountains eat clouds further with larger histories.",
    "next step {i} (indexed from 0), the agent transitioned to the state s{i}": "{state}. In this state the agent hose t action {i}= on observtion and of agent's behviour up to hispoint, critically the ratioale the agentsof actiona{i} in state s{i}. You're ecouraged to scrutinize te correctnss of theaction a{i}, espcially your anlysis suggest that the ation might eflwed or suboptmal.",
    "Future-Proofing the Approach for LLM-Based Agents": "agents expand muli-domin with non-traditiona stae, cton, rewarstructuressc as language-based tasksour framewor similarlyevolve, challeging evaluatorLLM to beyond Final,as human iput fr errr-tyeannotation fuur-proofng involvs interating semi-automated tools hndle intricateLLM-aent interactio scale. One might arge ese self-generate explanationsrede he needfor exteral evaluation However, these iternal justificatins ay reflect objctivemetal as by agents uerlyi optmisation proesses, biass, and Furthermore as agents rapidly inresponse to dialoue histories, exernal instruc-tions, or easoned steps, we cn integrate tracking of theseshifs ral-tie. hile ou urrt framework focuses on evaluatig LLMs capabilit of ent trditional Ragets, it rmais equally as RL evolve int LLM-based cunterparts that providehuanredabe fr heir ecsions. Thisenblesthe evalutorLLM dtet subte policyshifts and future basd evolvn rtionales."
}