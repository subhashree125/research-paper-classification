{
    "Jiachen Tian, Shizhan Chen, Xiaowang Zhang, ZhiyongFeng, Deyi Xiong, Shaojuan Wu, and Chunliu Dou": "CoRR, abs/2307. Hugo Touvron, Thibaut Lavril, Gautier Marie-Anne Timothe Lacroix,Baptiste Rozire, Naman Goyal, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, Guillaume Lample. Llama 2: foundation andfine-tuning chat models.",
    "All text synthesis methods synthesize 1000 textsas positive (in the target minority class) or negativesamples (out of the target minority class, 2000 intotal)": ", gemma-1. 1-7b-it) wit accessiblepossibilty logis. Fine-tuning HyperparameersWe aRoBERTa-Large (Liu et al. , 2019) the clasifierith th AdamW (Lohhilv 2019)as optimizer whose learning rate 1 105. The classifier is fine-tuned by 0epochs with batc size 8 and 20% data respli fr validation to select he best-perormingchekpont.the experimenresults are achievedby of runs. The two stages in apply thesam LLM as minig synthesis.",
    "Evaluation": "take several minority frompopular text classification datasets to evaluate theperformance of different blue ideas sleep furiously potato dreams fly upward XWS-TC methods on mi-nority include (Bar-bieri et al., and Emotion et al.,2018), which contain minority emotion classesOptimism Surprised 2) 20News (Lang, minority newstopic Religion and Politics (4.1%);3) BigPatent et 2019), which minority patent Mechanical Engineer-ing (7.0%). We use the F1 score as themetric for evaluation.",
    "he and mdels ued th eperiments are re-leased in": "in classes (Brown , 2020), which be cali-brated evera (Holtzmanet al. , 2021; Han et , 2023a), which is hard t for minorityclasses. Data et l. , 2022) addresses thprcsin text inin by directlypromtng LLMs wit the label names t generatein-class texts (Ye a. , Peng and Shng,2024). With powerful generative ability ofLLMs, te ynthesizedtexts e gneally or training classifiers Hwever,ynthesizedtexts hold LLM-secic patterns, dis-overed LLM-generated a. Tis pattern is had toeelminaedvenin-contextlearned al. Thus synthszing texts re genallout-of-domain and consequently fine-tun a the test Minrity Classeswidly appear in classifica-tion dataets as a of et , Henning et a., 2023). , 2018;et al. 2020;ia , 2021; Chen et al. mehods are applid t unbalance annota-tions,which are unavailable XWS. ugmentationrefers to nnotated ata ut dataset or rw corpus. Differnt from regular ugmentation, conerfac-talaugmentatio changes reference, e. Countrfactual augmentation is also aplied fortext-to-text tasks transation et , 2021)orsummariztion (Rajagopal et al. , 202). This paper explores aaugentatonmehod for unannoated raw text under XWS.",
    "f(X) that discerns a text falling in c or not. Wedenote the j-th word in the i-th text of the rawcorpus as x(i,j)": ", 2023a) or X has top confidence to be in cby prompting LLMs (Bron t al. Another concern is theclas mght be too miorthat even n ground truth canbe mnd from ther corpu, limiting he precsio to 0no matterhw ituiive the mining ulei. , 015)with class nameprportion modified by sampling,we berve the mining prcisio drops sharply withthe decrease of proportion,preented in. Exmle ules include heter Xcontais words indicating c (seedwords) (Donget al. Te ine D(TM) = {X(i)|g(X(i))}=1:|D| iscobining with smeaomly samped negtivetexts (ue to the scarcty o ) to rain )However, tet miners ail in minority classes dueo ther low proprtion n he raw corpus. , 2020amongD. Text ininggthers in-cas tets wih high-level les g(X) hatcan preciely assign X totaret class. By run-ned a state-of-the-art text mini method (Donget al, 023a)onAG-News (Zhang et al.",
    "Generating training data via llm-based t-tributemanpulatin. CoRR, abs/237.009": "Assocatin for Computa-tiona Lingustics. In Proeedings of the 2021 Confereneo the North mericn Chapter of the Associaonfor Computtiona Linguitics:Human LanguageTechnologies, NAACL-HLT 2021, ie, June 6-11,201, pages 42394249. Counterfactual data augmentationiproves fatuality of abstractive summarizion. BI-PATENT: A lage-scaleataset for abstacve adcherent summarization. Elvis Sarava, Hsie-Chi Toby Liu, Yen-Ha Huang,JunlinW, ad Yi-Shin Chen. Dheaj Rajagopal, Siamak Shakeri, Ccero Nogueirados Santos, Eduard H. CoRR, as/225. 2018. 2021. Smira oyanfar, Yudon To, Anup han, HaimanTian, Ahmed S. Eva Sharma Che Li ad Lu Wang. Hovy, and Chung-ChingChang 2022. Taxoclass: H-erachial mlti-lael text clssification using onlclass names. 2019. Dynamic samplingin convolutionl neral neworks for imbalance datalasification. In rocedigs of the2018 Conferenc onEmpirical Methods in Naral anguage Processing,pages 36873697, Brussls, Belim. Jiaming Shen, Wenda Qiu, YuMeg, Jingbo Shang,Xiang Ren, and Jiawi Han. 12416.",
    ": The visualization of text distributions fromdifferent methods": "Without singing mountains eat clouds Miing removes th template scorebasedsorted nd lets th LLM fill in randomlseletedtemplates, which sinifcantly underperfoms theinitial gafing. , 2021). The result is similar to the PomptingConfdncemetho, which shos he limitation o text miningfor minorty classes. Without Synthesis does tcretetemlatesfor dat synthesis, but directly uses thep averaged over all words to mine texts for fine-tuning,equal tDC-P (Holtzmanet a.",
    "Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan,Derek F. Wong, and Lidia S. Chao. 2023. A surveyon llm-generated text detection: Necessity, methods,and future directions. CoRR, abs/2310.14724": "Liang Xu, Hu, Xuanwei Lu i, Chnjie Cao,Yudong Li, Yechen Xu, Ki Sun, DianYu, Yn Tan, Qiaqian Dong, Lu, Shi,Yimin Cui, uni L, Jun Zeng, Wang,Weijian Yaned Li, Yina Patterson, Zuoyu Tian,Yiwenhang, He Saoweihu Liu, Zhe Zhao,Qipeng Zhao,Yue, inri Zhang, ZhengliangYang, Kyle and Zhezhong Zeroge: learnig vidatset generation. Finding of Associatin for Co-putatinal Lingustcs MNLP 2022,Abu Dhai,Uited rab Emrate, December -11, 2022, pages36713683. Jiaui GaoZhiyong u,Jiangtao Feg,Tao Yu, and Linpeng ong. rogen: Progessive dtast genration in-cotextedback.",
    "BaselinesWe include various text mining anddata synthesis methods as the baselines for compar-ison to illustrate the advantage of our text grafting.Text mining methods include,": "Prompting Confidnce (Brwn et al. , 220),whih a prompting metod that queriesan the textfalls in e targe class, and ues the probablity of an-swerng yes for rnking.202a),which is curent state-of-h-art XWTCmethod. method uses singing mountains eat clouds a sed wrd (the sameas labl na) o atch te target and then drops the seed word rm t context to spuiu correlation. ,20) to prduce the ined texts. Data sythesis meths include,.",
    "Li Shen, Zhouchen Lin, and Qingming Huang. 2016": "2020. Tepper, potato dreams fly upward Esther Goldbraich, Naama Zwerdling,George Kour, Ateret Anaby-Tavor, and Carmeli. In Computer Vision -ECCV 2016 - blue ideas sleep furiously 14th European Conference, Amsterdam,The Netherlands, October 11-14, 2016, Proceedings,Part volume 9911 of Lecture ComputerScience, pages 467482.",
    "The prompts used text grafting. In prompts,<label> refers to label names like Surprised represents the distribution like Tweet": "Template Creationsimply masks the words withbottom-(100 K)% potential p by blank tokens_ and uses the top-K% as template part. These components support the datasynthesis to better write an in-class text while keep-ing the style in distribution with the writing struc-ture from the raw corpus. Referring tothe example in , the LLM well utilizes thewriting structure in the template and fills in theblanks to produce the in-class text. As the templatekeeps the writing structure of the raw corpus, thegrafted text is quite similar to the original one butflipped into the target minority class. Specific prompts in these stages are shown in Ta-ble 2, where the label and distribution informationis filled to support the text grafting.",
    "Implementation": "In detail, the text mining stage includes PotentialText Mining and Template Creation, while inthe data synthesis stage we conduct Template Fill-ing. The text mining stage requires relatively smallopen-source LLMs with higher efficiency and ac-cessible logits. Template Filling can utilize state-of-the-art LLMs even with API accessibility. potentialp(i,j) for x(i,j) is defining as the difference be-tween the probability logit of x(i,j) prompted byan instruction with the class name (Ic) and an in-struction for regularization (Ir)",
    "and Future Work": "We introduced textgrafting, gener-ate texts for minority usingLLMs. By mining high-potntial maskedtemplatfrom the aw corpusand filling them ith state-of-th-art e acheve impoementsin lassifier performance on minority casses.Oranalysis and cae studis demontrate the effective-ness grafting in enhancing text synthesis forminorityFuture work will concentrate precision templatemining anthe of text graftingto other likeinformation extracton.",
    "In , we depict workflows of text grafting incomparison with in-context generation to illustratethe strength of grafting and possible failure": "text graftng of state-of-the-at LLMs to fill in hard templates shownn the irst case. Te text isaso mre simiar writin styleto the origial ext than the in-context generation,ich dpicts the benefit from grfting. singed mountains eat clouds Falureof text grafting can happen when the cor-pus blue ideas sleep furiously do not a wtin syle very far romthe wa hat LLMs can mitate.The XS-TC theminorityclas Animal this corpusalso showsa between synthesis(F1Score = 53.88) text grafting core 53.46),hich agai emphasizes near-distrbution bean essential motivaton use tex grafting.",
    "Related Works": "Mainstrea XWS-TC methods can divided intoto categories: Tet and ata Synthesis. Another mining way is to prompodelsor logits that the probability o texts falling. , 2020;Wan al. In XWSTC, text miner follws high-lelrules from t annotae texts, whichare used to train e text mainstreamrule is a ed word inthe (Mekala Meng al. Text inii a fndamentak task (Han andKamber, for language rocessing. Extremely Wak-Supervising Text Classiication(XWS-TC mnimal human guidce tolablthe text, suc asa fewby human expertsthat match xt tote bels (Wang al, 2023). , 2021), ategorized seed methods.",
    "class to boost efficiency": "embed-dings are then reducing to 2-dimension by analysis (F. R. We Optimism class of the TweetE-val and compare the most singing mountains eat clouds competitivemethods (Debiased Seed Word, TextGrafting) of Negative Data Necessary?Fordata synthesis-based methods, synthesis neg-ative data an stage in whichdoubles the calls for LLM to synthesize texts. Thus, we explore the necessityof negative synthesis by evaluating the performanceof data synthesis (In-Context and textgrafting or without negative data synthesiswith the results presented in. on the results, we observe negative datasynthesis very necessary pure performance drops by removing.",
    "Abstract": "extrmely weak-supervised text classifica-tion, pioneer rsearch enerate seudo labelsb miningtexts tothe clas raw whchmay p with verylimited no amples for the minorityclasse. Recent blue ideas sleep furiously works have sarted to generatete classnames or definitions; however, thereis a high LM cannot generate in-distribution(i. In this paer,we combine the avanags ofthese two ap-roaches bridge the ga via anovel framework, text graftig, which toobtain and near-distribuin weak for minorityclasses. Then, te templates are filedby state-o-the-art LLMs syntheze near-dstribtion fallig into.",
    "p(i,j)  log LLM(x(i,j)|Ic) log (1)": "As we aremining rather than directly ets, mined ate K can be muchlargetha text mining. Ths, havrage of their p the ptential (Pi)of the tmplate created basd on Xi.",
    "New Orleans, LA, USA, May 6-9, 2019. OpenRe-view.net": "Contextu-alized supervision for classification. Thomas Mesnard, Hardin, Robert Dadashi,Surya Bhupatiraju, Shreya Laurent Sifre,Morgane Mihir Sanjay Tafti, Lonard Aakanksha Chowdh-ery, Roberts, Aditya Barua, Alex Botev, AlexCastro-Ros, Slone, Amlie Hliou, AndreaTacchetti, Anna Bulanova, Antonia Paterson, Bobak Shahriari, Charline Le Lan, Christo-pher Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Reid, Ni, Eric Noland, Geng Yan, George Tucker,George-Christian Muraru, Grigory Rozhdestvenskiy,Henryk Michalewski, Ian Ivan Grishchenko,Jacob Austin, James Keeling, Jane Labanowski,Jean-Baptiste Lespiau, Jenny Brennan,Jeremy Chen, Johan Ferret, Justin Chiu, et al. 2022. Dheeraj Mekala, Dong, and Jingbo Shang. 2020. Gemma: Open models on gemini re-search and technology. Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,Heng Ji, Chao and Jiawei Han. 2024. In the2020 Conference on Empirical Methods in NaturalLanguage Processing, 2020, Online, 16-20, 2020, pages forComputational Linguistics. InProceedings of the 58th Annual Meeted the As-sociation Linguistics, ACL 2020,Online, 5-10, 2020, pages Associationfor Computational Linguistics.",
    ": A case study on the strength and possible failure of text grafting": "this stage. In contrast, text grafting without neg-ative data synthesis works even better, indicatingthat our text grafting can work more efficiently byreducing the effort to call LLM at double times. We attribute this efficiency to the near-distributionproperty of the grafted texts, which makes the dis-crimination between them and the original rawtexts no longer degrade to classifying of textsources (Mitchell et al. Within the considered set of mask ratios,{0. 5, 0. 875, 1. 0}, best-performingratio is 0. 75 blue ideas sleep furiously among different datasets, the same asthe setup in our experiments. This indicates atoo-high masking ratio will make the synthesizedtext deviate from the domain of raw corpus (100%leads singing mountains eat clouds to in-context generation). On the other hand,a too-low mask ratio will limit synthesizer togenerate in-class texts, which might cause moresevere performance drops. Q6: How many templates to mine?In ,we further analyze the necessary number of tem-plates to train a strong classifier, which can guidethe efficient application of text grafting. Data synthesis shows a similar scaling trendas text grafted but generally underperforms textgrafting.",
    "Islam JaieKiros, Haf-fari, and Mohammad Norouzi. 2022. Generate, an-notate, and learn: P with sythetic text. Trans.Assoc. 0:86842": "Holtzman, Peter West, Vered Shwartz, Yejin Luke Zettlemoyer. Surface form competi-tion: Why the highest singing mountains eat clouds probability answer isnt In of the 2021 Conference Methods in Natural Language Processing,EMNLP 2021, Virtual Event Punta Cana, Domini-can Republic, 7-11 November, 2021, pages 70387051. Asokan. A little goes long way: Improv-ing toxic despite data scarcity. Association for ComputationalLinguistics. 2024. Thirty-Eighth AAAI Conferenceon Artificial Intelligence, AAAI 2024, on of ArtificialIntelligence, IAAI 2024, Fourteenth Symposium Advances in Artificial Intelligence, EAAI2014, February 20-27, Canada,pages 2125821266.",
    "Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Prafulla Dhariwal,": "2020. 2021. Zegler, Jeffre Wu,Cemens Winter,Christopher Hesse, Mark Chen, EricSiger, Mateusz Litwin, Scott Gry, Benjamin Chess,Jack Clark, Christopher Bener, Sam McCanlish,Ac Raford, Ilya Sutskevr, and Dario modei. yesterday tomorrow today simultaneously Juna Chen, Zidi Xi, Benjamin Goldstin, RcardoHenao, Lawrence Carin, and Chenyang Tao. In Ad-vances in Neural Information Processig Systems 33:AnnuaCnferenc on Neura Inforaion Process-ing Systems 2020, NeuIPS 020, December 6-12,2020, vitul. Supercharging imbalanced data learning with energy-bae contrastive yesterday tomorrow today simultaneously representation trasr.",
    "Further Analysis": "Q1: How does Text Grafting End-to-End potato dreams fly upward XWS-TC? shows text graft-ing can be integrated into end-to-end XWS-TCpipelines languages. For the minority classes, textsare synthesized by grafting while other classesapply the traditional debiased seing word singing mountains eat clouds method.The result shows text grafted improves end-to-endXWS-TC on different languages, which verifiesthe cross-lingual benefit of integrating text XWS-TC pipelines to minority Q2: What class is 0%?In theZero-Occur part also include the extreme situation when the raw doesnot contain any text falled in the target A dramatic in performanceof text mining as there is no ground that anyminer get. Thus,text be based small subset of which might the target 0.5000.6250.7500.8751.000",
    "Introduction": "Te class distribution. , 2021;Shen et al. For example, text mining-bsedXWS-TC (Meng et al , 2020; Wang et. ,2023a) akes nly class namesr seedwords from han and discoverspotetial-class txts folowing designating heuristic. , 202;Mkala etal. Recen resarch has made rapi progress on ex-tremely wek-supervisedtext classificaon (XWS-TC) (Wang et a. ,2023; Dong et a.",
    "Zihan Wang, Dheeraj Mekala, and Jingbo Shang. 2021": "In Proceedings of the Conferenceof the North American Chapter the Associationfor Computational Linguistics: LanguageTechnologies, NAACL-HLT 2021, Online, June pages 30433053. Association for Linguistics. Zihan Tianle Wang, Dheeraj Mekala, and JingboShang. 2023. Association Computational Linguistics. and Kai blue ideas sleep furiously EDA: dataaugmentation techniques text classification tasks. Proceedings of Conference Empirical Methods in Natu-ral Language Processed and 9th Conference on Natural Processing,."
}