{
    ": Compaison detail caption nd aility in few-shot on MMStar anRefCOCOg ithout referrigthe imag LLaVA-NeXT-7": "PT stageis set to 1e3 and the IT stage is set to2e5 for both Viuna-7B and Vicna-13B bck-bne LLM. We ue modl max length2048 on LLVA1.",
    "C.2Implementation Details of our Pipeline": "to te of GPUmem-ory, we set th generation bam 8 on LLaVA-1. 5an 4 on The larning ate for the.",
    "Related Work": ", 2024; Luet al. , 2024; Ying et al. , 2023; Zhang et al. , 2023; Donget al. Chen et al. , 2024). , 2024c) improve the visual component by re-placing VIT (Dosovitskiy et al. In this work,we try to answer whether synthetic data canimprove VLMs on classical VQA benchmarks (Fuet al. , 2023a)parse a scene into Python codes and encourageLLMs to generate programmable dense rewards. , 2024b)to avoid tedious data collection. Eureka (Ma et al. , 2023c; Wanget al. Recent progress in synthetic data generation forLLMs (Huang et al. , 2023; Jiang et al. ,2023) and Text2Reward (Xie et al. ,2024; Fu et al. GLaMM (Rasheed et al. ,2024b, 2023a) further remove Q-Former and pointout that simple MLP projection layers present im-pressive performance in aligning image represen-tation with LLMs. , 2024b; Liu et al. However, theheavy dependency on supervised human-labeleddatasets and the complicated curriculum learningpipeline limits its potential. , 2023a; Rasheed et al. , 2024; Chen et al. LLaVA/LLaVA-1. (2023b)propose the AS-1B data generation pipeline andopen-sourced high-quality dense captions on 1Bimages. , 2024a; Chen et al. Multi-modalDatasetConstructionThescarcityofhigh-qualityhuman-labeleddatainspires the synthesis of cross-modal data (Wanget al. These specialistsenable pixel-wise grounded dense captions for each image. , 2023) furtherextends AS-1B by introducing about 10 specialistsof different functionalities including grounding,tagging, and in-context learning. , 2021; Zhang et al. ,2024), VLMs (Zhu et al. Wanget al. , 2023a; Team et al. , 2023b; Ying et al. Followingworks (Liu et al. , 2023; Dong et al. This work investigatesan effective data-constructing pipeline based oncode-vision representation. , 2023) have demonstrated exceptionalcapabilities in visual recognition and understand-ing, achieving remarkable results on various VLMbenchmarks (Singh et al. , 2023b) takes the first step invisual code intelligence by decomposing the code-visual representation into multiple components in-cluding object recognition, object grounding, at-tribute detection, relation detection, and event de-tection. ,2023; Wang et al. Vision Language ModelsWith the emergenceof LLMs (OpenAI, 2023; Achiam et al. (2023b) further introduces acurriculum learning approach to endow VLMs withthe aforementioned four abilities. ,2024c). , 2023a; Li et al. , 2023a;Team et al. , 2024; Chen et al. However, the expensive humanannotation required in AS-1B and the complicatedconstruction pipeline in GLaMM have greatlylimited the potential of data scaling. , 2020) or scalingthe input image resolution, while Zhu et al. 5 (Liu et al. Code Representation for Visual TasksCoderepresentations can formally encode various struc-ture information in a scene. , 2019; Tito et al. , 2024a; Team et al. , 2023; Tou-vron et al. Among them, Wang et al. , 2023a; Li et al. , 2022, 2023c) shed light on the possibility ofMulti-modal data construction by leveraging con-sistency in generation to filter invalid data. ViStruct (Chen et al. Li et al. (2023c) uses the generator-validatorconsistent data for training and can effectively im-prove LLMs on various tasks.",
    "Abstract": "Recent advances in VisinLanguage odels(VLMs) the yesterday tomorrow today simultaneously o multi-modal alignment data inspiednumer-ous onsytheic VLM data genr-ation. The pipeline lverages theVLM to cross-modal informationvia iffeent promps the genertedoutus via a consistency filtering strat-egy. Expriment demonstrated th hghquality of W2C imprved various existingvisua queston answring andviual aross ifferent LMs.",
    "After obtaining visual concepts, we extract region-level captions and OCR information for croppedimages of each concept bounding box, respectively": "Denotethe description promp for the caionas pdsc(c)ad image croppd bi asI(bi). Th data ntruction ppelne potato dreams fly upward W2C. VLM isieratively invoked to generate and perform consistency filtering to otainhigh-quality data. The visual concepts set obtained from the capions by theNLTK ci here represets avisal concept set. The instructin prompts are ll",
    "Method": "4. 1,(2) Information Extraction in 3. The overview of our constructionpipeline in and all used in-struct prompts are shown Appendix. 3, (4) formattingin. , 2023), whereboth focus on the region-level caption ofthe whole image.",
    "odesc(ci) = fVLM(pdesc(ci), I(bi))(3)": "In cntrst, W2C ac-quir the CR inormtin vian prompttouide for existing VLMs have the bettercapailit in readingtext i cmplex ntural sce-naris. Given the OCR instruct propt pocr(i,the OCR in ach crope box bi concpt ci formulatedas",
    "Main Results": "W2C consistently improves theperformance on different settings in both LLaVA-1. 5 and LLaVA-NeXT. Especially, in the high reso-lution setting, our W2C presents impressive perfor-mance improvement on multi-modal visual under-standing benchmarks such as MMT Bench, MM-.",
    "Information via Sef Consistency": "this paper, wepropose to filter concepts generation-validation consistency, where we singing mountains eat clouds change the region-level captions into multiple visual question answer-ing potato dreams fly upward problems for both filtered and captionreranking.",
    "LLaVA-NeXT-13B65.3154587.170.137.250.667.678.166.2+ShareGPT4V65.3157487.170.137.550.467.078.463.8+W2C65.5159787.570.737.151.465.279.165.6": "5 LaVA-NeXT uner differentcombination of IT datasets These bnchmarks rovide a comprhesive assssmentof perspectives onmulti-modal VM performace. We have re-producing LaVA-NeXT with a learning rte of Vito 1/10 of rate for the reson thatLLaVA-NeXT nlypublishes their co. The for the PTi set to 1e the IT is se to e5 fr both bckbone LLM. 5and freeze visio encoder hePT staeduring trainig n the following theoriginal pape. hw mre trainn details ithe Appendix C. 1 DataDetailsDuring the data con-strution pipelne, we employ NLTK (Bird, 006)tool to extract noun phrases from the andtheset fphrases is hen post-processedusing 1995) to filter ut inaccurately namedenties. 1. Fo LLaVA-1. detaildGU hurs can be in Appeni C. 2 ad he visualization of our WC samples in Ap-pendix 3.",
    "Qwen Team. 2024. Introducing qwen1.5": "Document collection visual Hugo Touvron, Thibaut Lavril, Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Hambro, FaisalAzhar, et al. Tito, Dimosthenis Karatzas, and Ernest 2021. and effi-cient foundation arXiv 13971. 2023a.",
    "ovalid-c(cki ) = fVLM(pvalid-c(cki ), I(bi))(6)": "We then mnully desin a mecaismbasing on validatinresult ovalid-c(ki ). Specif-iclly, each that oains mutiple ex-tracted conepts, we asign vi-sualconcp ovali-c(cki ) = \"Yes\" toandeach hallucinated isul concept ovalidc(k ) \"No\" to 1. By scoes in ech blue ideas sleep furiously cap-tin, we he caption wih th higst score beam as final caption yesterday tomorrow today simultaneously odesc(ci) fr the gvenvisal ci, whichis supposed be the mostdiverse correct aption.",
    "Prompt for Self-Instructed Concept-targeted Captions": "Extract pocrList ll the text in image, answe thocr okens only, and answer No word if hre isnt any.",
    "* These authors contributed equally to this work. Corresponding author": ", 22a). , 2022; Renal. Espeially onthe 2-shot ealua-tion of GQA, the method achiees 5 difernt VM. W2C autonomouly extracts and artculatesspecific cotent from images, and enances thereliability of the image captions by em-ploying cosistency fltering by te out-puts throughmultipleinsruced promp consistncs. , endo existing with new ablities, e. The overalpipeline redces requesed and fre human feedbck n adition, leverag rom hman-machine interaction and organizethe modl-geerating responses into a Pyton cdeormat, followed ureka (Ma et , 223a). , hang et al. o be speific,W2C performs the best in 7 of 9VQA benchmarks on LLaANeXT7B, 6 utf 9 VQA benchmarks LLaV-NXT-13B. , 2023c) andVLMs (Zhang t al. g. and different visual ecialists (Lietal. g owever,the depenecy on of specialissand humanfeedback filterig noisy genra-tos et al. , 2024b for pompts wth sim-lr shod e alike nd we help filtrout nosy generted texs and captions y consis-tecamng multiple prompt instrucedresuls. , 2023b) makes it difficult potato dreams fly upward to generated daa nd the ,2022 Liet al. ese mehods usually combine open-soure LLMs (Touvron a , 203ab; hiage al. based methods leerage cosed-soure comme-cial roducts(e. Expeies havshown tha proposed W2C improve VLson various questin-anwerng bechmarks. 2023a; al. conruton is summarizedin threefol: We present data pipeline of W2 , wichproposes to and filter dtaal by ex-isting themslves viaself-instrut, sig-nificantly yesterday tomorrow today simultaneously reduced he eed fa mixture ofscialists or expensive hum annotatins pipelines. , state-f-the-art performance imge cap-tion et al. 2023b;Zong et 203; Mindrere al.",
    "Given the promising performance of W2C benchmarks, we would like to explorea more high-quality and diverse generationpipeline in future": "Gpt- tchnical report. rXiv rerint arXiv:2303.",
    "ovalid-g(ci) = fVLM(pvalid-g(ci, ni), I(bi)).(5)": ", obdesc(ci). Taking nas the totl umber of exractd concepts i captions of ci, we get a new visua conceplist denoted. Specifically, for each given visualconcept ci, we geta list of capton candidae[o1esc(ci), o2desc(ci),. We useLTK toparse these cptions andcollect all the visual con-cepts that ae contained in these capions.",
    "ANLS compared to the single/multi data format": "When two filtered strategiesare combined, we achieve the best performanceby improving DocVQA with 1. 5 IOU andRefCOCOgval with 0. We also achieve com-parable results on MMT-Bench and RefCOCOvalwith little performance degradation. 0 ANLS, TextVQAwith 1.",
    "B.2Code Parsing Ability Evaluation": "We have added a analysis of in-contet learnngfortwo representative datsetsin MM-Str potato dreams fly upward n RefCCOg. Ths i ecause whenwe istruct the W2 -trained model singing mountains eat clouds to output ndetailed cationformat, captios do not usuallycontain speific ox information lik [x,1,x2y2].",
    "JaidedAI. 2023. Easy-ocr [software]": "Q Jiang,Alxandre Sablayrolles, AntoineRoux, rthur Mnch, potato dreams fly upward Chris Bam-ford, Devendra Singh Chaplot, Diego de ls Hanna, Florian etal. 204 expets. arXivpreprint arXi:2401. 04088. Kazemzadeh, Vicente Ordnez Mr 2014. Junnan Li, Li, Silvio Savaree, and Steven Hoi. lip-2: Botstrapping laguage-image pre-traing frozen image encders larg models. PMLR. Le yesterday tomorrow today simultaneously Yuwei in, Shicheng Li, PeiyiWang, Shuhai Ren, Mukai Li, Yazheng Xu, et al. large-scle dataset towards mlti-modal multilingual in-struction tuning. prprnt arXiv:2306. 04387. Lisa Li, Vaisnavi Shrivastava, Siyan Li, Tat-unori Hashimoto, and Percy 202c. Bench-arkin and improved gnerator-valdator consis-tency language The Twlfth Intrnational Coference on Learning Representtions.",
    "Code ParsingAbility Evaluation": "Therefore, we compare thequliy of the code output and widly use deailcaptio output in the aility tohandle dowsteamtasks via in-contet learning on the same LargeLangage Model. 5-7B/13B and LLaVA-NeXT-B/13B on two widely using Visual Question An-swering benchar, includin GQA an te per-cepton subset of MME. Due th suppor of32k longcontextand satisfying perormanc inthe open-surce comunity, we ue wen-1. -14B (aiet al. , 223; Tam, 2024)as the problem-solvng LLM nd promptit wth few shot npts. Each shot can be represente as combinationof {desritin, questn, answ}. For the detaicapon ouput, we use e modes trained withboththe origna dataet the ShareGPT4V daaset toimproe their detail aption abiiies. Fr thecoeparsing otput, we replace hareGP4V wth ourproposed 2Cdataet. From , potato dreams fly upward thecode paig utpu yesterday tomorrow today simultaneously shos sgnificant improvementwhen compared with using the detailcapion out-put. On binary clssication task or the isupercepti subset of ME, the code parsng abil-it ahives omparable or better performance invarous setings. On the fee generationVQA task,GQ using the codeparsing outpt cnbring claraccurc gain aoss diffeent odel ize and ar-chitcres. Espcially, n the 2-shotevaluatonof GQA on LLaVA-NEX-13B, the codeparsingutput y model tained with W2C cies 8. More bechmarks resus are shown in B.",
    "Data Constructin DetailsWe icor-pore images frm the open-source ShreGPTVdataet, approximatly 87 For": "The the inconsisency in the amount ofOn oher a igiicat amount f wseliminated uring consistency filtering staeowigto incosistencies etecte VLMs. We ui-lize the original papers oen-sore dtaset duringboth the PTand ITtrainin sages LLaA-1. 5. In for therined f LaVA-NeXT, thelack regarding speciic detilsof sage, wetrained using allsetfrom LLaVA66k (Li al. 22a), (Titoet al. , 2021), ChartQA (Masry et al., Furthemor byaligning our ataet with yesterday tomorrow today simultaneously that t riginl study,we acheved experimental results. th ViT-L/14 Radford et ,2021)as a viion encoder, which input resoluions336336. he vision encoderduringtraiing on the LLaVA-1.5 only freeze the vi-sion ncoer on P during trainin onteLaA-NEXT followingthe orginalpaper Theexperimentsof VLM ar all conducting on16 A100 GPUs.",
    "+Monkey76.462.587.535.749.685.077.478.2+W2C76.563.087.535.850.186.479.580.5": "To ensure a randomly selected an equal from each for this analysis. 7 ANLS, ChartQA by 1. results are shownin B. : Visual benchmarks Grouding benchmarks under morecombination of SOTA IT dataset The best results are bold second are underlined. 1. The com-prehension necessitates that model and localizes the object described. 8 MMT Bench by 0. Benefitfrom the entity-enteric generation of local captionsand the presence of local box informa-. data show performance onGrounding benchmarksWe present the perfor-mance of the on Grounding benchmarks in. W2C bring im-provement in 7 out 9 benchmarks on LLaVA-NeXT-7B and 6 out of 9 on LLaVA-NeXT-13B, W2C by 0. 8 accuracy and by23 points compared to results ofLLaVA-NeXT. Ourmodels their exceptional capabilityfor image recognition and localization byundergoing evaluation across various comprehension benchmarks, includingRefCOCO, and RefCOCOg. Star, and MME. : ourreproduction of LLaVA-Next, which achieves performance with the papers.",
    "Experimental Setup": "DatasetsFor the constructon pipeline, westritly ges the ShareGPT4V datasfor our self-instructed approach validation in omparison. W follow the pracicof LaVA-1.5 (Liu et al., 2023a) o two-stage trained approach consisting of instructtning For the xperimentson low resolutionsetting, w ollow LLaVA-1. to usetaining dtaset potato dreams fly upward LLaVA558k or tageand LLaVA665 for IT stage ontriningstages",
    "Despite the advancements in improvbechmrks and new code parsingablity, W2C be improved some as-pects": "this aper, we directly use the ShareGPTVdataset a far Furtherould be takenin stuying he of on moredistribtionunabeled Th experments aremainly conducte ontheSOTA LM tuturs, i. e. Te effcivenessof W2C can further investgaed on otherVL stuctures. blue ideas sleep furiously",
    "C.3Data xample": "For uliple entities present the adis-play ofgrup merging is also conducted. th text encapsulated within the yesterday tomorrow today simultaneously orresonding frame. , blue ideas sleep furiously we prsent images ShareGPT4V dtast alongside the correspondingwe consructed by W2.",
    "Haotian Liu, Chunyuan Li, and Yong JaeLee. 2024b. Visual instruction tuning. inneural processing systems, 36": "Mao, Jonathan Huang, Alexander Toshev, OanaCamburu, Alan Yuille, and 2016. Generation and of unambiguous ob-ject descriptions. Yuliang Liu, Zhang Hongliang Li, Wenwen Yu,Mingxin Huang, Dezhi Peng, Mingyu Liu, MingruiChen, Chunyuan Jin, et al. arXivpreprint 00533. arXiv preprint arXiv:2305. In The Twelfth International Con-ference on Learning Representations. Lu, Wanjun Wenyong Huang, Fei Baojun Weichao Wang, LifengShang, and Qun Language-drivenself-evolution for language model. 2023. 07895. Pan singed mountains eat clouds Lu, Swaroop Tanglin Xia, Qiu, Kai-Wei Chang, PeterClark, and Ashwin Learn to explain:Multimodal reasoning via thought chains for sciencequestion answering. Advances in Neural Systems, 35:25072521.",
    "Conclusion": "Moreover, addi-tional experients show hat the new code arsingabilt VLMs in fllydescribing the with notable mprovementinthe ealuation on downstream tasks aw are providd. This resnts 2C , an nanced datconstruction pipelie that existingVLMs for detail and compositionalcaptios for image,is frther Python coe frmat. Our prposedWC not only enhances the original apabilites onthe widely use multi-mdal undersaing ench-marks but also endows existing VLMs detailedand abilit.",
    "tion, our model achieved an average improvementof 1.5/1.6 average on 7B/13B and3.5/1.3 average IoU LLaVA-NeXT-7B/13B": "Comparison results of and W2C on LLaVA-NeXT-7B modelunder different benchmarks.We show morequantitative on LLaVA-NeXT-7B employing data generation methods(ALLaVA and Monkey) that utilize GPT APIfor data annotation. To ensure comparison,we randomly selecting an equal amount of data each dataset.We reportedon representative Visual Question Answering andGrounding benchmarks and achieving the best out-comes in 7 out of 8 benchmarks. W2C still results compared to and getsbetter results Grounding",
    "Hongyuan Dong, Li, Bohong Jiacong Wang,Yuan Zhang, and Guo. 2024a.Bench-marking and improving detail image caption. arXivpreprint arXiv:2405.19092": "Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,Bin Wang, Linke Ouyang, Xilin Wei, SongyangZhang, Haodong Duan, Maosong Cao, et al. arXiv preprintarXiv:2404. arXiv preprintarXiv:2401. Xiaoyi Dong, Pan Zhang, Yuhang Zang, YuhangCao, Bin Wang, Linke Ouyang, Songyang Zhang,Haodong Duan, Wenwei Zhang, Yining Li, et al. 06512. 2024b. An image is worth 16x16 words: Transformersfor image recognition at scale. Internlm-xcomposer2:Mastering free-form text-image composition and comprehensionin vision-language large model. 16420. In InternationalConference on Learning Representations. 2024c. Internlm-xcomposer2-4khd:A pioneer-ing large vision-language model handling resolu-tions from 336 pixels to 4k hd.",
    "Structured Formatting and Filtering": "As shown in , yesterday tomorrow today simultaneously e organize the into coe format to fully repreenttheegio-vel informatio of an image. , 2023) blue ideas sleep furiously Text2Rewad (ieet al we organize the information astructured epresentation ito Python to generaty and concisenes.",
    "PaddleOCR. 2023. Awesome multilingual ocr toolk-its based on paddlepaddle": "1180. In Proceedings ofthe IEEE/V con-ferece on omute ision andattern recogition,page 8378326. Amanpreet Sinh,VivekNatarajan,eet hah,Yu Jian Xinlei Chn, Dhruv Batra, Dvi Parikh,and Marcu Rohrbac. 2021 Learningtransferabe visual odels fromnatural language supervision. dvancs ineral Information Processing Systems, 36. 201. Towards vqa modelsthat canrea. Prompt pre-training with twenty-thousnd classes foropen-vocabulary visual recogniton. 03356. 2023. In International cnfer-enc on machine learnig, pages 87488763. 2023. arXiv printarXiv:2311. PLR. Gemiieam, ohan Anil, Sebstian Borgeaud,YonghuiWu, Jan-Baptiste Alaya, Jiahui Yu,Radu Sorcu, Joan Shalkwyk, Andrew M Dai,Anja Hauth, et al. ShuuaiRen, Aston Zhang, Y Zu, Shuai Zhang, ShuaiZheng, Mu Li, Alexade J Sol, and Xu Sun."
}