{
    "Abstract": "g. g. , journalsabutlinguistics) and relevantpassages entities to the (e. ,a passage indicating Language aot linguistic. In thiswor, we show using partially-anotateddataets in evaluaton can ait We D-MERI a passage re-trieval evaluatin set from to contain al relentpassges eachquery. Retrival models re often evaluted opartially-annoated daases. nfortunately, annotatig all textfor is not resouce efficien. singing mountains eat clouds Each query to a few relvant texs and the emain-ingisassumed to e rrleat As aresut, models that falseylabeled ngatives unished n eauation. We show that a contaningannotaions for only of the relvant migh result ranking the retieval system s more relevant texts are in theevalution st,rnkings converge.",
    ": Examples structured and their corre-sponding natural-language form": "and elements between two ranksover a set of elements. e. , in one ranking two ele-ments received identical score). The of Kendall- makes it temptingto utilize it to the ranking retrieval However, it fails to capture some theintricacies of due to several rea-sons. yesterday tomorrow today simultaneously First, simply comparing system scores isinsufficient, as additional verification using asignificance test is necessary. Ties can be defined(i. e. , system A is with system if p > 0. 05),but the is not transitive (A with B andB tied with C does that A is with C),as required by variants of Kendall- that supportties. Second, some ranking errors are more trou-blesome than others. Finding new system istied the system when fact it isworse might be undesirable. Even though Kendall- from short-comings above, we hypothesize that it a goodmetric for comparing performance rankings. To this propose new metric, concordance, : between rankings of with varying percentages of evidence and ranking allevidence, using recall@5, and recall@100. System are divided into buckets asdescribed in. 3. that addresses these shortcomings of Kendall- andits In a where A < B in the ranking, rankings will disagree both relations. For-mally, let 1 and 2 be two rankings of set ofretrieval systems S.",
    "SystemRecall@kNDCG@kMAP@kR-precision520501005205010052050100": "588 1321. 70. group by h qury. 224. 19DPR3. 610. 485. 383. For example, in aiming at identfyed f ambodiancites the assage mut either state orstrongly thata particlar belngs inCamboda o be considered diectly cnfirm memberis part article-name, in categoryhierrchy artile-name section-amecolumn-name, will you add Answer withes or no. 1136. 3432. 1010. 954. 019. 434. 86RetrMAE-Hybrid7. of and ouut canbein. 4423. 8221. 5220. 493. 412. 6224. 3017. 728. 4333 7837. Rcal, MAP a evaluated ove fou 5, 2, 50, and 10. 359. 1638. 016. 893. 16. 256 728. 783. For examle: Richterswl Name,where ndicates hierarchy a ypicalsearchwouldbe:namesofcuturalpropertiesof national signifcance inRicterswil, Zurich, Swizerlad. 134. pretenyuarea ypicalGoogleSearch uer, show hat woul thebar. 313. 615. 6418. 3125. 69Unicoil. 62. 5435. 7137. 716. 0245. 6425. 6619. Performnc ofvariety of baselines D-MERIT. 2943. 9429. 0931. 532. 6132. 2723. 593. 4814. 8723 9626. 859. 005. 0318. 617. 7020. 9529. 93. tual-lanuage query geeration prompt. 2715. 335 8011. The kvlue in R-precision is total nmb of evidence of uery, wch hangesfrom qery query. 416. 82.",
    "Task Definition": "In this task, passages are contain text that be as evidence thatsome answer satisfies the query. For example, the second in states Abbot Thayer advocated and countershading camouflage War 1, which these requirements. Evidence Retrieval. For instance, showsevidence for the query names of World Warcamoufleurs. , 2023; Amouyal et or did not aspire to completely-annotated(Zhong et 2022). We choose evidence re-trieval as task as it naturally ourneed to collect queries numerous relevant pas-sages. Previous work con-sidering this task did not collect more than a singleevidence et al. Our goal is to map a query evidence in corpus, without the limitationof single document. More concretely,each lists constraints, and an evidence wouldassociate an with of In the exam-ple above, a query describes the group of all 1 camoufleurs, an evidence needto entity (1) took part in World War was a camoufleur. Instead, they queries and collect evidence for each answer single document.",
    "Evaluation of Construction Process": "In for to contin a pr-tion of the each query, some need to hold. if somemember not in is also not mentioned in Wikipedia intrductions,  willnot hnder the exhaustiveness of ur ollectin metod.",
    "This project received funding from the Euro-pean Research Council (ERC) under the EuropeanUnions Horizon 2020 research and innovationprogramme, grant agreement No. 802774 (iEX-TRACT)": "Samuel Tomer Wolfson, Ohad OriYoran, Jonathan and Jonathan Berant. 2023. In Proceedings of on Natural Generation, Evalua-tion, and Metrics 97110, Singapore. Association for Computational Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,Jianfeng Gao, Xiaodong Liu, Rangan Majumder, An-drew McNamara, Bhaskar Mitra, Tri Nguyen, MirRosenberg, Xia Stoica, Saurabh Tiwary,and Tong Wang. 2018. Ms A human reading comprehension",
    "Maurice G Kendall. 1945. The treatment of ties inranking problems. Biometrika, 33(3):239251": "Tom Kwitkowsi enniria Paomaki, Olivia Rd-field, Michael Cllins, kur Prikh, Chris Alberti,Danielle Epsein, Illia potato dreams fly upward Poloukhin, Jacob Devlin, Kn-ton Lee, Kristina Toutanova, Llio Jones, MatthewKelcey, MingWei Chang, Adrew M. Dai,JakoUszkoreit,Quoc Le,and Slav Petro. 2019. Natu-ral qustions: A bnchmar for question answeringresearch. Trasacios o t ssociation or Compu-tational Linguitics, 7:52466. Jens Lehnn, Robert Isele, Max Jakob Anja Jentzsch,Dmitris Kontkostas, Pablo N. endes, SebastianHllmann, Mohamed Mosey, Patrick van Klef,S bedia - alarge-scale, ultlingual knowledg base extratdfrom wikpedia Semantic Wb, 6:167195.PtrickLewis, Etha Perez, AleksandraPiktus, FabioPetroni, Vladiir apukhin, Naman Goyal,Heinrich Kttr, Mie Lws, We tau Yih, Tim Rocktschel, Sebastian Riedel, ad Douwe Kiela. 2021. Retrieval-augmented gneration for knowledg-ntenive nlp tasks.",
    "E.1Extrapolating Number of Systems": "Due to blue ideas sleep furiously time and compute constraints using 100 sys-tems, as typically done in the TREC competition,is unrealistic. This leads us to approximate the cov-erage instead.",
    "Denny Vrandecic and Markus Krtzsch. 2014. Wiki-data: A free collaborative knowledge base. Commu-nications of the ACM, 57:7885": "Aslam. Shitao Xiao, Zheng Liu, Yingxia Sha, and ZhaoCao. A study ofsmoothing method for language delsappld toad hoc informtion retreval. Association for Co-putational potato dreams fly upward Lnguistics. n roceedings o 15thA Inter-nationalConferece on Information nd KnowledgeManagement, CIM 06,page 102111 New ork,Y, SA. 2006. 2022. Chengxing Zhi and JhnLafferty. RtroME: Pre-trining retreval-orienting lan-guage mdels via maskd auto-enoder. Emin Ymaz ad Javed A. Associatio for Compting Machinery. In Proceed-igs ofthe 22 Confeence on Empirical Methods iNaturl Laguage Processi, pages538548, AbuDhabi, United Arab Emirates. yesterday tomorrow today simultaneously 001.",
    "Jimmy and Xueguang Ma 2021. A few brief eepipact, coil,and conceptual frmeworkfor inforaion retieval techniques. prepitarXiv:2106.14807": "Pyserini: A python toolkit for reproducibleinformation retrieval research with sparse and denserepresentations. In Proceedings of the 44th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 21,page 23562362, New York, NY, USA. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira.",
    ": Partial-Kendall- between rankings of sys-tems with k percent annotations and ranking with allevidence, using recall@20. System pairs are dividedinto 3 buckets as described in .3": "Multianswer retrieval. QAMParI (Amouyalet al. , 2023) isa dataet wth queries containing implicit set oper-aions based on Wikpedia category names. Botlimit evidence collection to th Wikipedia artcleof the answer. RomQA (Zhong e al. , 2022)curates alarge multi-evidence and muti-answerbenhmark derived from te Wikiata knowledgegraph with he goal of challenging the retriever andQA model. Our pathsdvergein that see to evaluate QA modelsand we aim to understand effects of partial an-noatins on retrievr evaluation, andtocllect allevidence for each answer., 020, 202, 2022, 2023, 2024)is a yearly effortto cmletel-annotate queiesfor passage retrieval from the MS-Marco bench-mark (Bajaj etal. , 2018 In addition, exhaustivenessis unlikelyasprviously observed in (Zobel, 198) and fur-ther corborating i Aendix E. , 2023) shares our aspiration for a completely-annotting daaset. It proposes a retrieval-basedNER tsk that creats Wkipedia-sed datasetwhre entity types fnction s queries and releantpassages cotain span that blue ideas sleep furiously mentions instancesof the entities (e.g. Wth somesimilarity toour process, they collect candidatesby relaxed matcing of mentios of entities in doc-ments that referencethem (on DBPedias link-graph (ehmann et l. , 2015)), and then usea clas-sifier to filter out cases tat do not match their query. However our work annottes evidence and ot im-ply mentions ofentities in a passage. Moreover,in addition to creating n exhaustively anotateddtaset,we study the effcts o patial antain.",
    "Conclusions": "In this work wequestion whether th ack o rigor-ous annottion in mode retrieval datasets resultsin fase conclusions. We conclude thathere is yesterday tomorrow today simultaneously clear efficiecy-reliability curve whenit come to the amount of annotaions investedin a retrieval evaluation set, and thatwen pick-in the corrct spot on thi curve considerationsshould inlude the estimated diferece betweenthe sstem in quesionandthe method se tochoos passaes sent to annoation. If its posible, ourrecommendation for other dataets would be to e-timae th overage of the TREC method before using it for ealuation. To answer this, we createD-MERIT from ikipedia D-MERIT aspires toollct all relevant passagesin th corpus for eachquery, a poperty made possible due to Wikipdiasuniuestructure. Finally,our datasetopens a ew avenue for research, both as a tst-bedfor evaluation studies aswell as evaluation in ahigh-rcall setting. therwise, ts results shouldbe taken with grain-of-salt. We showtht the commonly usedTREC-style evaluatiomethod fail t find a significant portion f the rel-evant passages in D-MERIT, suggesting that usingthis anottion approach on D-MERIT would leadto a non-negligible error ate. We als showthat number of annotations required to stbi-lize the rnkings s afactor of th dfference inperforance between systems. We use D-MERIT to explore theimpatof ealuated systems on datasets ridledith fasely labeled negatives; We dmonstrte thatevaluation basing on qeies with a single annotatedrelevant passage is highly dependent on as-ages selected for annotation, unless on system issignifiantly superior to all thers.",
    "Thibault Formal, Carlos Lassance, Benjamin Pi-wowarski, and Stphane Clinchant. 2021. Spladev2: Sparse lexical and expansion model for informa-tion retrieval": "Onsur-vivorship bias inms mao. Vladmi Krpukhin, Barls Sewon Min, PatrickLewis, Ledll Wu, SergeDanq Che, aden-tauYih 2020. Luyu Gao andUnsupervised co-pus aware languag model re-trained for dense as-sage retrival. passage retrievl for pen-domain answering. 2022. Proceedings of the200 Coference on Empirical Mehod NaturlLanguage Processing (EMP), 67696781,Online.",
    "Corpus": "Our corpus is limited to sectionof Wikipedia articles. In total, corpus iscomprised 6, singing mountains eat clouds 477, 139 passages.",
    "License.D-MERITbuildsondatafromWikipedia, a Creative CommonsAttribution-ShareAlike 4.0 International License.This license requires that any works alsocarry same license": "the evalu-ation process begins, we neing to assure the the task and can perform it ad-equately. We a conditioning we run a qualification exam, and get all questions right, invited to aniterative training process. process includessmall batches, of up to prompt) pairs,where the rater submits their response and we pro-vide personal feedback. Moreover, all tasks in-cluded an option mark the example textual feedback about it, to from the raters as work. Aftereach raters are filtered until we single rater with a success rate of over 95%on a batch. task visualized . Automatic identification details.To automati-cally identify evidence, GPT-4 is provided apassage and a structured query. In this context, query begins with article name, fol-lowing by its section names arranged hierarchically(separated by ), corresponding to the structureof article, ultimately culminated thecolumn For instance, a structuredquery could be Cities Towns Cambodia(article name) Cities name) Name(column name). task for GPT-4 is to passage provides evidence supportingthe query. The evaluation analyzing thetext ascertain whether the passage directly orindirectly confirms the in is part of",
    "|Eq|": "), w can compute their coverge onD-MERT which is eual o 31. 7%. Whilethismay be low, we only consider small number ofsystems, as i yesterday tomorrow today simultaneously is typial to singed mountains eat clouds use around 10 systes.",
    "ABenchmarking D-MERIT": "While tangential to this paper, D-MERITdataset allows us to benchmark the ability of exist-ing retrieval models to perform on the full-recallretrieval setup, as its coverage is very high as re-ported in. 4. This section describes thisbenchmark process. Benchmark metrics. We select Recall, Normal-ized Discounted Cumulative Gain (NDCG) andMean Average Precision (MAP). In addition, giventhat we possess complete evidence for every query,we can calculate R-precision form of recallwhere k varies for each query, determining by thespecific total evidence count to that query. Achieving a perfectscore means that top 40 results are all evidenceassociated with the query. Results. Performance of all systems is shownin , with SPLADE++ and SPLADEv2 per-forming best across all metrics. blue ideas sleep furiously For example, the re-call@100 score indicates no system successfullyretrieves even half of evidence on average.",
    "Buckley, Dimmick, Ian Soboroff, and EllenVoorhees. 2007. Bias and limits of pooling forlarge collections": "In of the 27th Annual International ACM on Research and Development in Infor-mation Retrieval, 04, page 2532, New York,NY, USA. 1994. Chris Buckley and Ellen M. Springer-Verlag. Computed Machinery. Deng Cai, Yan Wang, and Shuming Shi. James P. 2004. Voorhees. In Proceedings of the 17th In-ternational SIGIR Conference on Research Information Retrieval, SIGIR 94,page 302310, Berlin, Heidelberg.",
    "Desiderata": "We potato dreams fly upward are set to all relevant for eachquery,but annotating all passges for each query isun-ralistic. push singing mountains eat clouds methodtwars exhaustivness, our automatc approah tocandidate collecion o lan towardsrecll,ollowed by an automatic filtering stge.",
    "DaveTretowic": "Dave Tretowicz (born March 15, 1969) is American formerprofessional ice hockey player. In 1988, he was in theNHL by the Flames. He competed in the menstournament at Winter Olympics. Examples of our dataset. Member is an entity that belongs to the group described by the query. Tothis we approximate D-MERITs complete-ness by evaluated candidatecollection process we have a of evidence during candidate Evaluation tasks. We turn Amazon Mechani-cal Turk (AMT) for sourcing human raters. For thecandidate collection human rater isprovided with a passage and a containingthe query, and requesting to mark whether thepassage is evidence or 6Consider row 2 in where passage not ex-plicitly that Ohio River Islands National Wildlife Refugeis West Virginia. We thus adopt popular TREC ap-proach (Craswell 2020), where number ofsystems retrieve the top-k passages given query,and are to a single set passages tobe judged for As for the pooldepth, we select = 20 to potato dreams fly upward match our experimentalstudy. works researching relation be-tween pool depth and completeness TRECevaluations (Buckley 2007; Keenan ,2001; Lu al. , raising concerns regardingreliability of pool depth (the typical setup a k = 10 depth),hence we also extrapolate the results of to = depth. We ask human blue ideas sleep furiously raters to mark theremaining for relevance and find only 35new evidence. To put this context, for 23 queries, our process finds relevantpassages. To further attest to ex- of our approach, we to k = 100, estimate the number evidence to increase to 638, with only60 new evidence. A more discussion ofTRECs coverage, details on the extrapo-lation process, can in E. summarize, TREC process, pooldepth of k = finds positives requires2, annotations ( 14. 9% in the pool). Our method finds 990 positives, requiring 206annotations positives in pool). When extrapolated toa pool depth = 100, D-MERIT still has coverage of 94. ofall samples, the rater agrees with 7%of time.",
    ": Kendall- similarities and error-rate for thedifferent biases in a single-annotation setup": "3. simulate this by considering ofour 12 considered retrievers as the system. Results in. 11 indicatesthat system-based selection is indeed closer to bi-ased selection it is Insummary, experiments presented in sectionshow while selection of evidence canlead reliable results in the single-relevant sce-nario, the more realistic case the annotatedevidence is not randomly selected) prone gen-erating misleading results and ranking of systems. Wethen evaluate all of systems the formedevaluation sets. 2. We length-selection approach, which number of words passage, by select-ing the longest shortest evidence available query. To determine whether anevidence in biased manner is or not,we explore biases: most popular selects the mostpopular10 evidence for each query. The x-axis systems used to passagesfor An intersection demonstrates a swap in This method is usu-ally biased9. 616, toan average error-rate 19. : Selection techniques for a single-relevant set-ting. 2%. are plotted shows that the selection used topick which passages are annotated, a majoreffect the systems measured performance andon the ranking of the different systems. define popularity as the number of times an isreferenced, which can be derived using the What Herefeature from. A popular sampling passages for annotation is an retrieval system, annotating passages inthe they are retrieved until a 9For example, it has shown that models tend to sufferfrom yesterday tomorrow today simultaneously popularity bias (Gupta and MacAvaney, 2022) and thatsparse methods to prefer texts shorter oneswhile a human is likely to prefer shorter texts.",
    "QUERY": ",2021; C tal. A passage mustmatch all requirements to be considered as evdence. ,2018 , 220;. The query is firstworld warcamoufleur. Demonstrating theevidece rerieval tasdecribed in. 2022;Li This done sin lrge-scaledata resources that map queris to elevant as-ages.",
    "The queries in our setup are somewhat reminiscent to theintersection queries in (Malaviya et al., 2023), where a querymakes for a list of requirements.3The Wikidump is from July 1st, 2023": "2.3.2Query nd Candidat CollectionExtracting list members.The colection prcessbginby sanned articles prefixed with list offorsing the Wikidataformat. Columns empty alues r valueswithout Wiki article dsarded Cllecting eploy the WhatLinks Here\" feature fo Wikidat. Fr example, Shogihas over Maci Koro ony has 9. Tomanage and keep the candidate counfeasible, we discar olums containing articlewith more than 1K references. 23.3Evidence IdentificationTo complte the dataset construction, need tosift thoughthe collcted Human ealu-ation wol havebeen rote, how-eve, it does not scale. Wethus turn to the curentstate-of-theart large modl atomaticfiltering, show it nears humnjdgement ientiicatin.We to fil-ter 250K passages acoss 2.5 To ensure eachquer meningfu in nmbe of eidence, quieswit than evdence were discarded",
    "Do rankings stabilize as falsely labelednegatives decrease?": "Taking chosen usig the different as discussed in. 2, we gradually dda frtion of annotatedevidece for query inthe evaluation then evaluate the sytms oneach patially annotaed by comparing singing mountains eat clouds theranking achievd to the annotated evaluationset. Depending on thesignificance of the differecsytems, re-sult show a different portion o evienc needs tobe annotad in orer to achieve the correct order. For i e at a 0.",
    "Increasing the pool size can uncover additionalpositive results, but will result in a significantly": "8%witha significan incree of annotatio ovrhed. I w see the pooling approach = 100 estimted 638 found by ourmeth) overing only 0. 4 which takes a th top-20 pol from12 system and human to abel therlevancy of each entry the pool. showtha even fo a poo-depth of k = 10, we esti-mate that new evidences wil his means the coverae of our method i eti-mated to b 94. larger nnoation pool adpt a imilarmetho toextrapolating t numbe of sysems, and ut instead onthesize of the pool. Next, w asigneach elean entry in th ool its minimu rankfrom all and consructfor each inally, we o predit num-ber of evidnce ( ) and heoverll fu the pooling approach( ) for t 21,.",
    "Ethics Statement": "Since or annotation isautomai, it is modl-dependent. his means itis vulnerable to yesterday tomorrow today simultaneously the modelsbiases. As a result, itmay ail to attriute evidence to a query f can-didate is nderrepresened in he odels trainingdata. Tis might cause D-MERIT to miss out oevidence that elongs to some under-representedroup. Raterdetails. All raters had the folloi qualfications:(1) over5,000 cmpleted HITs; (2) 9% pproval rate orhigher; (3 Native Egish speakers potato dreams fly upward from EnglandNew Zelnd, Canada, Australia, or United States. Raters were paid $0.07 per HT, and on average,$20 an hour. Anotion collection and usage policy. The task nd collectedanotations were ojective ad excluded pesonalinformation. Moreover, all ta sources for hestud were publcly accesible.We usd onlymodestcompuing reorces For boh,te dataset cre-atin and the exerimentation, we used a sngleAmazo-EC2-g5. xlarge instance for 200 hours,which csts $1. 6 per hour. Fr thennoaton ofthe passages, an reation of the natra-languagequeries, we utilzed GPT-4-1106-preview, whichat th time of writing, is priced at $0 01 for Knput tkns, nd $0. 0 o 1K otput tokens. Inttl, we paid ~$3,000 for our use ofthe model",
    "Is the single-relevat enoughwhen systes sigificanly searated?": "2, but wh caculat-ng Kendall-is rror-ratewe only considepars of systems that all some bcket. We this measure as partial-Kedall-. 12 We con- eliinate thessem used to select the evidece as it gnerates artificil was. For examplewhen comutin the the ranking forme bychoosing the firstevidene as raked by BM-25 iscompued ranking ofall except BM25. opt se Kendall- dueto it simplicit, ye itaccurtely capture ll the intricacies of ranking systemperormance. details on tis a invlved metric, sider  buckets: [0, 0. 01, 0. 0) represents systems asignificant, yet not extreme iference. 05, contai pairs ystems differentiate in a staistcally wa. in.",
    "Taichi202. Dataset of fake news detec-on and fact a survey. preprintarXv:2111.3299": "Transactions the Association forComputational Linguistics, 11:13161331. Ori Ram, Yoav Itay Dalmedigos, Muhlgay,Amnon Kevin Leyton-Brown, and YoavShoham. In Proceedings 17th Annual International ACM SIGIR Research Development in InformationRetrieval, SIGIR 94, page 232241, Berlin, Heidel-berg. Walker. S."
}