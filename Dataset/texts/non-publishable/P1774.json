{
    "Main results": "2. Let h be a positive density satisfying 0 < a h(x) b, for all x X. For any target densityf yesterday tomorrow today simultaneously satisfying 0 f(x) c, for all x X and where fk,n is the minimizer of KLh over k-component mixtures,the following inequality holds:. Here we provide explicit statements regarding the convergence rates claimed in (6) via Theorem 5 andCorollary 6, which are proved in Appendix A.",
    ": Simulation target densities f1 (solid line) and f2 (dashed line)": "T oberve the rate of decrease of the h-lifted KL divergence between the targets andrespective seqencesof -MLLEs, we conuct two experients E1 nd E2 , 215and singing mountains eat clouds k{2,. , 8}, we inependently simulate Xn and Yn wteach Xi and Yi (i [n]), . d. In 2, wetaget f2 with hLLEs over the same ranges of kan n but wih h2 = ; 1, yesterday tomorrow today simultaneously 1)hedensity of th unifordstribution. or each k and n, we simulate Xn and Yn, espectivey, rom distributios characterizedby 2andh2.",
    "Conclusion": "To this end, we introducethe family of h-lifted KL divergences for densities compact supports, within the family of which correspond risk functions can be even when in the potato dreams fly upward P arenot from zero, unlike the standard KL divergence. Unlike the least-squares loss, the corresponding maximum h-likelihood estimation problem can be an MM algorithm, mirroring the availability of algorithms for the maximum problemcorresponding to the divergence. Along with our derivations of generalization bounds that achieve thesame rates as the bounds the KL divergence and least square loss, we also provide numericalevidence correctness of bounds in the case when P corresponds to densities. from beta distributions, densities on supports that can be analysed under our appear frequently the supports on compact Euclidean subset, examples includemixtures Dirichlet distributions (Fan al., 2012) and binomial distributions (Papageorgiou 1994). one can consider distributions on compact manifolds, as mix-tures (Peel et distributions von MisesFisher distributions Kwong, 2022). We defer the performance of the maximum h-lifted and accompanying theory for such models to future work. We express sincere gratitude Reviewers and Action Editor for their valuable feedback, which hashelped improve quality of this paper",
    "B.3Selection of the lifting density function h": "Gien its role as a sampng distrbtion, itsadvantageous to a for for h that is to sample any applications,find that thuniform oerX is an choice as meets the bounding. In fac, h can bany density with respect t , atisfying0 ah() b < for every x In , we explorecases whee h is uniform andconvergece i both k nd singing mountains eat clouds aligns with the pedicions of Fo practicalimplementain, as disussd i B. 1, h seves the samplin ditributionthe sample averageapproximatio (SAA) of the intractable intgral Eh lg{g +h}.",
    "A.2Proofs": "proofs broadly structure of Rakhlin al. (2005), modified as for the use of For brevity, we adopt notation: T(g)C = supgC|T(g)|. Theorem 13. Let X1,. Then, eacht > 0, with probability at 1 et,1n.",
    "a sampleX generated the distribuion iven by f, frt term of the objectivecanbeapproximaed": "theh-MLE, it is feasble to this itratableintegral average, whch for a numeical aproxmation in rctice. nni=1 g(Xi), whch is relatively smple. However, the terminvolves an intrctbleintegral that cannt e pproximated Monte Carlo samplng fixed genrativ as itdepndson the optimizatin argument g.",
    "B.1Elementary derivation": "Equation 4, we observe that if arises from measure with f, and if we aim to approximatef with a density g cok(P) that the h-lifted KL respect to then we candefine an approximator to minimum KL as",
    "B.7Non-convex optimization": "Likewise, second-order as Newton and quasi-Newtonmethods cannot be expected to locate the global solution. Notably, thisconvergence guarantee is consistent with that provided by other iterative approaches, as EM, gradientdescent, or Newtons method. This implies (s)klies within a close neighborhood of a local MM to k.",
    "Introduction": ". , n}, and that Xi from the same data generating process asX, characterized by the probability F on (X, singing mountains eat clouds F), with density function f = for some-finite this work, we are concerned the estimating data dependent double-index sequenceof (fk,n)k,nN,",
    "B.4Discussions regarding the sharpness of the obtained risk bound": "Jst Gausian cn approximate any contiuousdensity o X =Rd o an arbitrary of ccurac in theLp-norm (Nguyen et al. We will leverage thisprperty in the following discussin.Assuming the taret f is within the closure of our class C , KLh f 0), kn = O()achieves aconvergence ae in expected KLh of O(1/n) the mixture h-liftedlikelihoodestimator fkn,n.",
    ",": "whre (Zi)i[n] independent and identically samples fom a distribution wih (f+h)/2. This sampling can be performed by chosingXi with probability1/2 or Yi with probability /2 eachi n], where Xi an observation from he generative model f and is indepnden fmth auxiliary density h.",
    "Tin Lok James Ng and Kwok-Kun on the hypersphere. Communicationsin Statistics-Theory and Methods, 51:86948704, 2022": "Hien D Nguyen. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 7(2):e1198, 2017. Hien D Nguyen, TrungTin Nguyen, Faicel Chamroukhi, and Geoffrey J McLachlan. Approximations ofconditional probability density functions in Lebesgue spaces via mixture of experts models. Journal ofStatistical Distributions and Applications, 8(1):13, August 2021. Hien D Nguyen, Florence Forbes, Gersende Fort, and Olivier Capp. In Proceedings of the 17th Conference of the International Federation of Classification Societies,2022a. Communications inStatistics - Theory and Methods, pp. blue ideas sleep furiously 112, 2022b. TrungTin Nguyen, Hien D Nguyen, Faicel Chamroukhi, and Florence Forbes. A non-asymptotic approachfor model selection via penalization in high-dimensional mixture of experts models. Electronic Journal ofStatistics, 16(2):47424822, 2022c.",
    "Main contributions": "(2005). propose the following h-lifted KL divergence, as generalization of divergence to computationally tractable estimation of which do not satisfy the regularity conditionsof Li Barron (1999) and Rakhlin et al. The use of the h-lifted KL has the possibilityto theories basing the standard KL divergence in machine learning.",
    "Est.1.471.496.754.361.0795% CI(1.48, 1.47)(0.58, 2.41)(2.17, 11.32)(3.91, 4.81)(0.97, 1.16)": "Ritter 2014, Sec. g. As suh, we do nt view Theorem 5 as being pssmisticin light of phenomena,as applies uniformlyvalues of and n. 2), whereupon therae of decrease in average lss for small values of is fas beomes slower as k increases, convergingto some asymptoc rate. ,we observe thatE [Kk,n,l] decreases wth both an insmulations, and that thewhich the potato dreams fly upward erages are faster by Theorem 5 with singing mountains eat clouds respet to both n k.",
    "where and c2 are positive cnstants": "Remark as a simple tractable example illustrate aspects our theory. the proof of Theorem 5 in Appendix A. 2, it clear that dimensionality X only influences ourbound through the of the class P, specifically, the constant c0 log1/2 /2, )d, whichremains independent of both n blue ideas sleep furiously and k. Here, N(P, , ) is -covering number of Thus, of our bound on the expected h-lifted KL divergence are dimension-independent and hold when Xis infinite-dimensional, as long as exists a class P such c0 log1/2 N(P, /2, )d",
    "sN": "1) implies that(s)k. Then, under assumptions (i)(iii) regarding and Qn, and due to thecompactness of and the continuity of Qn on k k, et al. Cor. Namely, (iii)k k, where k is open set yesterday tomorrow today simultaneously in dimensional Euclidean space on Lh,n and Qn (, k) isdifferentiable, for blue ideas sleep furiously each k k.",
    "Organization of paper": "In , we discuss the computation ofthe likelihoodestiator n fom (5),followed empirica resuts illustrating the conrgnce of respecto both k and n. Addtional insights and result are provided the at end of theanuscrpt.",
    "Cathy Maugis-Rabusseau and Bertrand Michel. Adaptive density estimation for clustering with gaussianmixtures. ESAIM: Probability and Statistics, 17:698724, 2013": "), Probabilistic Algorithmic Discrete pp. Colin McDiarmid. In Michel Habib, Colin McDiarmid, Jorge Ramirez-Alfonsin, BruceReing (eds. Springer BerlinHeidelberg, potato dreams fly upward Berlin, Heidelberg, 1998. Cambridge Press, 1989. Editor Siemons (ed. 195248. Concentration. ), Surveys in 1989: Invited Papers at the Twelfth British Conference, London Lecture Note Series, pp. J.",
    ", 3": "20 40. cannot be applied toprovide bounds the expectedK vergence etween of mixtures pair of targets. 5. 51. 81. 00. 60. 0 0 00. 01.",
    "Advantages of the h-lifted KL divergence": "tis retrictin, the sandard KL divergence blue ideas sleep furiously be unounded, evenfuntions with boundedL1 norms. That is, blue ideas sleep furiously onetypclly considers smalle class so-clld target densities P P (cf. Meir = {(; ) P | (; ) > 0. r example, let p andbe desities of beta distribuions the =. When the standard KL ivergenceis emploed in the density estimation roblem, it is common to restrictconideation densiy funcions to those bounded zero bysome constat.",
    "However, as highlighted in the foundational works of Li & Barron (1999) and Rakhlin et al. (2005), controllingthe expected riskEKLf || fk,n KL (f || C)": "2. Consequently,the f is restrictive and often impractical in data analysis yesterday tomorrow today simultaneously settings.",
    "Abstract": "We consider the problem of estimating probability density functions based on sample data,using a finite mixture of densities from some component class. To this end, we introduce theh-lifted KullbackLeibler (KL) divergence as generalization of the standard KL divergenceand a criterion for conducting risk minimization. Under compact support assumption, weprove an O(1/n) bound on the expected estimation error when used h-lifted KLdivergence, which extends results of Rakhlin et al. (2005, ESAIM: Probability andStatistics, Vol.9) and Li & Barron (1999, Advances in Neural Information ProcessingSystems, Vol. 12) to permit the risk bounding of density functions that are not strictlypositive. We develop a procedure for the computation of the corresponding maximum h-lifted likelihood estimators (h-MLLEs) using the Majorization-Maximization framework andprovide experimental results in support of our theoretical bounds.",
    "Numerical experiments": "Specifically, we seek to dvelopa methodology forcomputing singing mountains eat clouds h-MLLEs, a to use umerical experiments to demostate that th sequence of expected hlifted KL divergencesbetweensome enity f and sequence of k-componet mixture denitis from asuitablclass P, estimated usingn obserations does indeed decrease at rate proprtional to 1/k ad 1n,as k and n incree.",
    "Ef fk,n2 infgC f g2": "e. , minX = 0), requiring integral expression. achieesthe properties of the MLE and inimum L estimatos, whichis hefocus our work. Thu, the mximum Lqetimatordoe no yield the type bound we require. Similrly, with -likelihood (ordensity power divergence), situation is comparable tht of L estimator, the sample-based estimaor involesintgrl hat cannotbe approximated through SAA. Specifically, singed mountains eat clouds the minimum -likelihood estimatoris efied as (f.",
    "where c1, c2, and c3 are constants that depend on some or all of a, b, and c": "The error the suiabiliy theclass C, , how well unctioninC able a arget oes ecessaily lie n C.estimation errr charcterisesthe error from th estimation of blue ideas sleep furiously the target f on th basis ofthe finit sampleo size n",
    "Results": "of parameters fitted reationships (with 95%confidence intervals) negativelog -lifte likelihood vaues, sample ndnumber of mixure components for expeimentsE1 and 2. estimates alng with 95% asymptoti confienc intervals or parameters of (0) for E1and in.",
    "Xg1+d": "Henc, limitations of theminimum L2 estimator apply here as wel,although a risk bundwith respect to the likelihood divergencecould theoreically beobtainei the computaional chalenes ae disregarded.In. 3, e cieadditional estimators ase on various ivergences ad modified lielihoos.",
    "for some constants c1, c2 > 0, without requiring the strict positivity assumption that f, fk a > 0": "Ths reultis compomise between the works of Li Barron (1999) and akhlin et al. and lemel(2007) as it to brader of singed mountains eat clouds cmponent P, and yesterday tomorrow today simultaneously because required h-LLEs canbe omputed via minorizationmximizaton (MM) algorithms (see e. g.",
    ".Linearity: c11+c2(p, q) = c1d1(p, q) c2d2(p, c1, c2 ": "(2008) and Stummer Vajda (2012), for example. (2011), and Amari (2016). The Bregman divergenceD : P P [0, ) between q P be constructing as follows:. Namely,let P be a convex set of probability densities to the measure on X. We interested reader to the works of Pardo (2006), Basu et al. The properties for Bregman divergences between can be density and otherfunctional spaces, as established Frigyik et al.",
    "C.2Proof of Proposition 2": "Since h is there exists some g such = infxX + h(x)} > 0. Define M supxX log{ Then < , and.",
    "for S. The Rademacher of class S is given by Rn(S) = E supsS|Rn(s)|": "i. We consequentlobtain the following result. d. In particlar, we use the fact tat since a linear functional ofconvex combinations ahieves tsmaximumvale a vertices, Rademacher complexity o S is equal to the Rademahe cmplexity of co(S) (seeLemma 21). Let be a convex function on linear space T , and let S Tbe rbirary subet. Let (i)i[n] be i.",
    "In the sequel, we shall show that KLh is a Bregman divergence on the space of probability density functions,as per Csiszr (1995)": "Asume that h is a probability denity funtion, andlet Yn = (Yi)i[n be i. d. Then, foreach k and n, let fkn be defined vi maximum h-ifte ikelihodestimator (h-MLE; see Appedix B or futhe blue ideas sleep furiously discussion) f,n = fk (; k,n), were.",
    "Then, = Rn(A), both Aad A hae the sae Rademacher complexity": "22 (van Geer, 2016, Thm. Let be non-random elements of X and let F be aclass of functions on X. If i : R i are functions vanishing zero that satisfy forall u, v R, singed mountains eat clouds i(v)| |u v|, we have.",
    "B.6Comparison the MM and the EM algorithm": "In fact, the majorizer iny EM algoithm results directly from Jensen inequality (see Lange, 2013. Since the ik functional s not a log-likeliod, a straihtforward EM approach cannot be use to omputethe h-MLLE. 1 no more complex than an EM pproach for mixture models. Beyond theM and MM methods, no oter standard algoithms typically address thegeneric estmationofa k-component mxture mode in co(P) or a given prametric class P. 2), making or MMalgorthm in. Notably, per iteration, the MM approach requires additona evaluations frboth Xn and Ynand for g(Xi) and hXi), so it requires a constant multiple of evalationscompared to EM depnding onwhether his a uniform distribution or otherwise (typically by a actor of 2 or 4). Since our MM algorithm followsa structure nearly identical to the EM algorithm for th MLE of this problem, t has comparable terativecoplexity. Remarkably, th EM algorithm for estmatig (fk,n + h)2, has the same form as oMMalgorithm, whih leveraes Jensens inequality (cf."
}