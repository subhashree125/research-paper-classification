{
    "weakest. The is a classification where a outer prdicts l [0, N] as the poble ormulation": "As such, deendenton the use case, we candecideto not atempt t generate SL or we canroue to one potato dreams fly upward of the singing mountains eat clouds N models.",
    "AAnalysis of Failure Cases": "(1. 99%) 19%) (3. 56%) gpt-4o gpt-4o-mini 1:8b-instruct-q4_0. 47%)811 6%) (37. 46%) (4. Figures 2a and 2b illustrate the distri-bution failing and correct across three models: andllama3.",
    "B. Li, Y. Luo, C. Chai, G. Li, and N. Tang. The dawn of natural language to sql: Are we fullyready? CoRR, abs/2406.01265, 2024": "Li, B. Hui, G. Yang, Wang, B. Cao, Ma, G. Li, K. C. R. Cheng, and Y.",
    "RBERT55.21118 (8%)311 (20%)167 (5%)938 (61%)1.4x": "We end up with thrmdls with a iffeecelesstan 24 , 0. 725 , and R. 7, 0. led o a train set quries a set 697. We rmoving all querie in retail_worldtaase duet schema missingand al uplicate use qestions. For {0. 8, 0. sampling each a weighted ndm weigts tothe inerse ofth frequecy of ech abel wthin he ranig data. Whentrinng on H and evalutng on the devset, therelevant schem using TCSLschema lining approach. 6, 0. 8}. 0. We coose K tht mimizes EXwhile havingroted to each model a least once. find that setted to 9is overly restrictive nicating for every inpu qur,, no modelcapable of generatacurte Assuch, only cosider {0. Classification-ased Router DistilBERT on the fully labeldtrainset H. We split 80-0(ensuring databases train arenot validtion and visversa). We4 epochs, with of 32, a rtof 1. 810.",
    "Classification-based Routing": "We use a distilled encoder model (DistilBERT ), which we train on H learn thefunction RBERT . If RBERT predictsN then it predicting that none the models can yesterday tomorrow today simultaneously generate accurate SQL. Similarly to the otherrouter, to accuracy in our implementation, we choose to strongest model.",
    "of Varying K and on Eecution Accuracy and Disrbution": "We analyze the impact of varying parameters K (number of similar queries) and (threshold the score-based router RK on accuracy (EX) distribution. This examinationhighlights the trade-off",
    "Methods": "blue ideas sleep furiously blue ideas sleep furiously We consider two aproaches.",
    "Results": "We our R0. 6 , 725 , R0. 810 , and RBRT on dev queries. that ll model,rute to hesrnest (gpt-4o)in N is predicted. By analyzing the successful ad ses on the three models (Details in Appendx A), we fnd that each weaker mode hs subet of failedand sccessful a strongr one. suh, expect lowerthe best keeping best models EX. summarizes EX, ruting distribution and relativ cost blue ideas sleep furiously the stronget moel(p-4o)for ach generatinapproach on BIRDs dev 4x cheaperwhilebed close in urthemore, to xplor acro moredtasets and within more stags as and schema linkng instead of just generation.",
    "Problem Formulation": "Given singing mountains eat clouds set of N models { M1,. In practice,these can be collecting from execution logs, feedback, and manual labeling. Therefore, we formulate theproblem as optimization task: select the smallest l that Ml produces accurate SQL for Q. Here,we define the weakest model singing mountains eat clouds as the with lowest SQL generation In practice, thismodel typically exhibits and incurs least cost. , } and natural language query Q, objectiveis identify weakest model Mi that can generate corresponding for Q. We assume a model strength, such that Mi is considered weaker than if i < j. Accordingly, our goal is to learn an N-ary function that maps query Q minimal labell [0, Here, a label l [0, 1] indicates that Ml the weakest model capable of generatingaccurate SQL for A prediction of N denotes that no the is capable doing so. Furthermore, we to a dataset H, consisted of prior natural language (NL) queries,predicted SQL outputs, and corresponding ground-truth for each of the models.",
    "Intersection gpt-4o-mini &": "We blue ideas sleep furiously find that 83%. This indicates when routing, is to improve beyondthat of singed mountains eat clouds strongest model and that we expect a cost decrease only.",
    "Models": "We Llama model and hence associate no dollar and use OpenAI servicesto access pt-4o and pt-4o-mini. Instead of using token prices of OpenAI, we us anormalized uni cost where et gpt4o-mini input and outut token to 1 and gpt-4oto16. 34, hen with and asthe strogest with EX 0. find Llama as te weakest EX 0. We used thee LLMs for SQL geration: i) GPT-4; ii)GT-4o-min; and iii) To orer SQL generationcapabiliy, w used a simplifiing Tet-o-SQL pipeline o a sigle ttemp at generationwhile adding whole schema te LLM context. Cst. outr Implementation. We evaluate e selcting 10% queries of each datbase BIRD a random. In ourexperments,we nly analyzing costad fogo latency due to challenges. 5. 6 (multiplicative priceWe using OpenAIs text-embedding-3-small modelih cosne similarity to the top K similar queries in H for the ruter. Fo score-based router (Rk), we have to select the woparameter and K. The is ngligible per ipt NL qery compared token usage and therefore ignored. Spcific moels and strength. (threshold scre minimum prportion of the K smilar ueriesfor which a had to acurate SQL considerte ndiate model for gneration.",
    "Metrics": "Our objecive is to aximize EXS) while concurrently choosing thewakest model. To evaluae effectiveness of generaed SQL, we use eecutin accuray (EX as theprimaymetric. Hre, matchin elations ar eined as those contaning identical tupls, indpen-dent of atribute ordeig."
}