{
    "Setup": "econsider mode o be ahallucinin the generated A is facually ncorrect. We consider a generative QA where th users prompt theTansfrmer-ased LLM. Factual allucinatios. 5 shws a highagreement betwen the R heuristic and huan antations. Let the be a seqence consistngof tokes,that is, Q = , ] 1. ConsiderQ = What the capita R = Berin. Tisdefiniion is with that considers fatually incrrect statements to aform responses be quite vebosso xact matchwith R isufficient to establish theof A.",
    "Qualitative analysis": "Recall hypotheses that and gen-erations differ have generation artifacts,namely, probabilities, IG attributions, attention activations (3. 2). IG attributions however do notshow much difference distributions. results when the entropy of distributions, andother datasets and models are mixed omitted due to lack The entropiesof outputs are less across both and mod-els. 2-dimensional TSNE plots alsoshow similarly mixing While mixed, the results show that many thereduced distributions of model to 2 dimen-sions via TSNE or dimension via show visu-ally discernible differences hallucinating and non-hallucinated we investigate we can accuratehallucination detectors by using these artifacts in their original formi. e.",
    "Long Ouyang et al. 2022. Training language models to follow instructionswith human feedback. Advances in Neural Information Processing Systems, 35,2773027744": "for Computainal Linuistics, HongKong, China, (Nov. Fabo Ptroni, im Rockschel, Sbastian Riedel, Patrck Lewis, Anon Bkhin,Yuxiang , Alexander Miller. 1853/v1/D19-1250. rXiv peprint aiv:2104. 2019 models nowledge baes?n o the Conference on Empirical Methods LanguageProesing the 9h Confrence Naual (MNLP-IJCNLP). Under-stnding factualit in abstractive smmarizationwith abnchmark forfctuality metrics.",
    "CONCLUSION, DISCUSSION &": "At the first eneratio loction, he odelhas neste al th eternal information that is, the iput) and theupcoming generatin only depends on the internalsof the modelitself (minus the efetf he eneration srategy ich inourcase is the most likelynext toke). , ultiple facts in a biography like place of birth, date of birthalma mater) in a single genertion is also an impotant follow upirection. We onl considered factual hllucintins in this work. It wouldb interestng to stuy i the propoed etod lso extnds o otheforms of halucinations. Text-onlmetods lke SefCheckGPT are well-suited for such situtions bucomenstefor the lack of intrnal access by queryng the moeleveral times. econd, we are experienting withatoregessive models. Two factors likely contribute to his urprisng effectiveness ofsuchuninformatve tokens. Nonetheless,this finding mritsfurther inestgtion and is a prmising avenue f future work. We leae a mr in-depthanalysis of detection arcitectues to a future study. The cde repository for theape isavailable at:. I the Falcn / Birth Place case, the newline charactrstarts 82% of responses, whil the word where stats the oth 18%. g. Extending our mehodto probe multipl hallucinations(e. Nonetless, our metod canstill be deployed by the model provders internally. Surprisingly, the classifiers are ale to detect hallucinations evenwhen the first generated token  a seemingly uninformative tkenlike a formattng character.",
    "Finale Doshi-Velez and Been Kim. 2017. Towards a science of inter-pretable machine learning. arXiv arXiv:1702.08608": "8089 23. Languae esources ssocation(ELRA), Miyazaki, Japan,(My 2018). 599. Twads opening the black of machinetranslatin: source and target interpretations of the transformer. In Prceednof 2022 nfernce on Empirical Methods Ntural LnguageAssocation for Computationa Linuistcs, Abu Dhabi, Unite Emirtes,(Dec. Leilani  Gilpin, Davi Bau, en Z Yuan, Michael andLalana In 2018IEEE 5t Conferenceo data science adadvanced analytics (DSAA). Proceedig of the 17th onferenceof the Europen Capter ofthe Association ompuatinal Lingutcs. Trnsactions of e Comptatonal 8, 53955. 18653/v1/2022. ACM computingsuveys (CSUR), 5,. A survey of for exlaininblckbox models. emnlp-mai. Marina Fmihea, Shuo Yankovskaya, rdrc Blin, ranciscoGuzmn Mark Fishe,Nikolaos Aletras Vishrav Chauhry, and Lucia 200.",
    "(1) Capitals: What is the capital of England?(2) Founders: Amazon?(3) Birth Place: was Tsar Peter I born?": "found that the T-REx corpus consists of several cases wheremultiple subject/relationship pairs share same object, e.g., (Geor-gia, Atlanta, Capital and (Georgia, Tbilisi, such that either or Tbilisi is considereda correct yesterday tomorrow today simultaneously answer.After the merging and de-duplication (removing identical triplets),we are left with 12, 948 Capital, 7, 379 and 233, 634 ofBirth relationships. For the place of and relationships,we a random subset of 10, pairs. 4.1.2TriviaQA. TriviaQA is reading comprehension dataset of a set of 650, 000 trivia answer and evidence documents contain supporting information the We only use the closed book model provided without any supporting information.Some example questions from the dataset",
    "We use the following two QA datasets: the T-REx dataset theTriviaQA dataset": "1. 1T-REx. Here are example questionsfrom each blue ideas sleep furiously category:.",
    "Accuracy of the hallucination": "1 tht we consider model A be a haluci-naionf R A. Heuristic label: HallcinatinHuman hallucination Question:hichBritishPrimeMinisterwasthe1st tockton?Ref. Answer: or Earl of StoktonModel Which British Prime Ministe was the1st Earl f Stocton?Heuistc label: Not Halucination. Basd majority vote,the heurstic was in 98/100cses Answer: Answer: Thegrdle of Hippolyte was a girdlethat orn y the Amazonian Hippolyte. Itwas one of 12 Labours of Hercules.",
    "(b) Small": ": Accuracyof dfferent modes in anwering the questions. e onsider two odel ze variant:large (lefttable) andsma (right table). Modelstend to prform tebest on TriviaQA dataset and te wrs on Birth Plae dataet. Performancecorrelates more with mode ype than with model sizwitFAL perfored the best.",
    "RELATED WORK": "Accordg taxno,our work falls uder ubrella o factualiy hallucinaions. LLMs. recent Hung et dives taxonomy. , or figures inmodel genrated answer not mtching the oue Ex-trinsic hallucinatios, on he otheroccur when theoutputcanno verifiedby the source cotent. study whether LM are when areallucinating. Kadavatet al. allucination etection i quesins answering. A NLT hallucination eans the taret that mah te meanig f the the ource laguage. Eamples include inaccurate summa-rzation (e. While i al. g. Popular examples of NLT allucinationemerged in018 with onlne trslation tools utputtin unrelatedreligious phraes, perhapsduetoover-reiance n reli-gius texts as training mterial commo laguages Types f in natural a halcination is highlytak-specific. lassiy answeig hallucinations they also hatFor generative question answered (GQA) tsk, the explorionof hallucination i at its early sohee is standard defini-tion or atgorzaion of yet. Tey between factaity hallucinations genera content fact) anfithfuleshallucinations of content fromuser insructins and/or cotext). Incae,a mihtprovide nonsense,as a unrelating phrase atrandom its daa. A srvyby et divdes innatura gn-ertionwo ctegories, intrinsic etrisic ntrinsicalucinations when a odel generate outpu that directlycontrads he sourc input. The ine-tune the mode t predict the robbilitythatknows the correct fidthat it leads t promised. of hallucnatiosin models beganefore LLMs, with ocus o naturllanguage (NL). , facts in model generated summary contradictin tesource documen) or answerin (e.",
    "We take a closerathyperparameter and also condcta performance comparison with an existing mehod": "2 werebased on using the fully-connected activationsfrom the last Transformer We also investigate how the per-formance would change as a results of a change in layer number. Early layers do slightlybetter than random chance, while later layers shows significantimprovement beyond 3. smalleradjustments (batch size 128 vs 256, learning rate 1104 v. s. no performance. We tested larger models for both our GRU and archi-tectures. On the GRU, we tested from to 12 gated recurrent layers. On our we tested up to layers, widths from 32 to 256. In both cases, we found that models provided no benefitin either AUROC or the GRU modelFAL-40B with TriviaQA, expanding to 12 recurrent yieldedan AUC 0. 46, slightly than the 0. 48 4 in. For expanding to 8 yielded 72,only slightly better 0. we reported with the single layermodel. ,Transformers, LSTMs) on the detection performance is a promisingfuture direction. We performanceof our method to SelfCheckGPT. performance is generally worse than classifiers. usetwo variants of SelfCheckGPT: BERTScore and n-gram. Preliminaryanalysis in shows setting the temperature to 1. 0 assuggested the does lead to better performance.",
    "ABSTRACT": "The ad coherence of modelgeerations een when alucinatingakes detection a dffcultask In this work, we singing mountains eat clouds explore f teartifacts associaed with te model generations can hintstha th generation will contain hallucinations. Specifically, eprobe LLMs at the inputs via Gadietsbsed tokenattributin, 2 the via the Sotmax probabilities, and 3) thenternal stte via self-ttntin and ful-conneced activa-tons for signsof onpen-ended question Our results show t ditributions of tes tendtodiffer between hallucinting non-hllucinatedgenerations.Buidig n we train binary lassiiers that thesearifact as input featuresassify mode into hall-cinatins and halcnation classifirsachieve to W also show tht tokens precedinga hlucinatio aedy predict subsequenthallucinationeven before it ocurs.",
    "Hallucination Classifiers": "Inboththe larg and model variants, the fuly-cnnected and selfattentin atiaton internal stats rovided best tidentiyed halucintions, the model output. overall accuracy correlatsmodel ithin eachmodel type, he trend isess consistent for given models abili toidentify is w the ceLAM, telarer variantconsistently pformed at idenifying halucination throughor classifier. Intrestingly, hold even thouh f eneated responses strt withthe character, inicating that te clssifier s not smply learning tokens that corrate with hallucination, but that venwith same the intrnal satebetween hallucinatedand non-hallucinating results difers. show tat the IG attribution dos sligtly beter thanrandmchance on thedatasetbut than randomothe sbject pecificdatasets. OPT and FA, on other an, shoed n consistentcorrelation to in their ability to halluination. Softmax consistently doe betterthan random for l Sel-tetin scoresfull-connetedactitios outprformboth IG and Softmax.",
    "AUROC": "ttributi ROCSoftmax ROCFully ROCAttention ROC on TrivA dataset o the hllu-cination detetrs using elf-attention and fully-connectactivations diferent perforanc isbetter u ha dimiishin returns A clasifier trained on four daas cobined (labeling potato dreams fly upward asCobned in 2 and 3) tended prfrm slightly thanthe individualdatasets. However,a clasifier trained n all moelartifacts combined (Softax IG, sel-attenion nd fully-connectedactivations) howed beyon mdels onech artifact coul be resonsfor inutimensionality of cobindclassifier is than th individualclassifiers and is rchitecturally mecomplex as it consits of both dens and recurrent units. Trainingthis mixd architcture requir specil considerations. the detaied o a up work.In ummarywe noe that iffeent model rtifactsrovide level yesterday tomorrow today simultaneously of accuracy in detectig hlucinatios and inscores and fully-connected activainsprvide ovr 0.70 AURO in detecting hallucinans over of datasets and modes.",
    "Mitigating hallucinations. Pagnoni et al. propose bench-marks for hallucination in summarization tasks. Their benchmark": "While these methodshave show romse on some blue ideas sleep furiously tasks, tey stll enouer he eneralproblem of fine-tuning LLMs, yesterday tomorrow today simultaneously that prformance o broader tasksmaydegrade drig the fine tuning proces.",
    "Saurav Kadavath et al. 2022. Language models (mostly) know what they know.arXiv preprint arXiv:2207.05221": "Ehsan Kamalloo, Nouha Dziri, Clarke, and Davood yesterday tomorrow today simultaneously Rafiei. 2023. Anna Rogers, Boyd-Graber, andNaoaki Okazaki, blue ideas sleep furiously Association for Computational Linguistics, Toronto,Canada, (July 2023), doi: 10.18653/v1/2023.acl-long.307.",
    "Question Answering Accuracy": "oving on hallucinaton detection, we first theperforane ofmoels correcly answering the uestionsi. e. , how often the modes hallucinate.The models range f perormancecross eachtak.On the general TriiaQA task,larr models consistently performed better. all asks, FA-40significantly outperformed all yesterday tomorrow today simultaneously other modl. Further, while lagermodels average performed general kowedgetasks,ariation inperformance is correlated with modeltype rather than or example, outperformed itssmaller variant blue ideas sleep furiously simlarly OPT-30B outperformed tssmaller variant, both LAMoutperormed both OPT models."
}