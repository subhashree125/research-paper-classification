{
    "Ben and AranKomatuzaki 2021. Gpt-j-6b: A 6bllio parameter atoregressive moel": "arXiv preprintarXiv:2305. Kevin Wang, Alexandre Arthur Conmy,Buck Shlegeris, and Steinhardt. arXiv preprintarXiv:2211. Wang, Li, blue ideas sleep furiously singing mountains eat clouds Damai Dai, Deli Chen, Hao Meng, Jie Zhou, and Xu Sun. 00593. 14160. 2022.",
    "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-ton. 2016.Layer normalization.arXiv preprintarXiv:1607.06450": "anguage models are ew-shotearns. Adancesinneural informationprocesinsystes, 33:1871901. akansha Chowhey, Sharan Narang, Jacob Devlin,MaartenBoma,Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sbas-tian Gehrmann, e al. 2023. Pam: Scaling languagmodeling with singed mountains eat clouds singing mountains eat clouds pathways.",
    "Reasons Causing Accuracy Decrease": "In comprison subtraction,and the top head for mulipicatio impact accuracy. Snce te acuracy more complicatd low, we analyze ostimportant head opeationin1-digit (1D), -digit and 3-digit (3D) operations, shwn The hads i 1D, ad 3D operatins same. lave furherinvetigation of this or futue ork.",
    "Understanding Arithmetic in LLM": "Hanna et al. (2023) investigate et a. yesterday tomorrow today simultaneously 2023) demon-strate that uccessorheadscan aid in prdicted thesubsequentorde, suc as \"3\" after \"2.Zhang et et al. tgatethe cock nd piz algortms or modularaddition. Quike et al. hidden states,Solfoetl. potato dreams fly upward thatattention lyers transform the informationto th last FF reult-rlted",
    ": FFN neuronscontain bias. \"F\":woman": "We then apply our CNA method on 32 com-mon professions contain gender bias (detailing inAppendix D). Designing four prompts, we iden-tify top18 important FFN neurons and yesterday tomorrow today simultaneously edit themby setting their blue ideas sleep furiously parameters to zero. 7% when only 18 neurons are edited.",
    ": Decrease (%) of accuracy on 1D+ and 1D-cases when intervening hidden-interpretable neurons": "Ther 17618 neurons in 0th This stroly ug-ets tht theseneuons potato dreams fly upward pyasignificant rlei enhncing features and are final Further supportingthisnotion singing mountains eat clouds is observation that randomly intevenig1,953 neuns the 15th onyresults inan acuacydecrease 2. Whn intervn-ing(about6% o all in0th layrs), the accracy decreass 68. Comparedto direcly interpretable nerons in deep FN lay-ers, hidden-interpretable neurons in hallw FFNayers are widely distributed.",
    "Feature Predicting via Arithmetic Head": "For case \"3+5=\" with prediction \"8\", we computethe score change for each neuron, andfind the important are in We project these neurons vocabulary space al. The top tokens when projecting into unembed-ding space in. 283696 means the3696th neuron the FFN layer. \"ori\" and\"inv\" denote the original and the intervened model(\"mdl\").",
    "Understanding the Mechanism of LoRA": "Wedetermine the rate from choices of. The epoch t 4. 01, 0. Inspied onarithmeic eads, weaply te CNAethod to understand the mechnism of LoRA. W investate wetherLoA plys distinctroles added into various layers. 0001. Notably, we nroducnegative nubes in2D cses such as\"-5=-2\",as originalmodl does not thisconcept well.",
    "Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.2022. transformers in embedding space.arXiv arXiv:2209.02535": "InProeedings of the 16th Confreneof the EuropeanChaper oftheAsociation for Compuaional Lin-guistics: Main Volue, pages 2232242. 221. A mathematial framwork for transformercircuit.",
    "Limitations": "While rsultsare interpretable, he thsmethod incomplete utilize thismethod in case stuies nd supplement oufinded with additioal methods to strnghen thus enhanced their persuasiveness. lies in the ofstanad-izatin across varios studies regarding atributionmethods.Hever, method demandsubtantial oputatonal resourcs,renderin them unsuitable fo our work. Apotential rs of our wrk s that attakers te importnt neurns andedi thse neu-n to change output disributio. Fr instance, instead ofreducing the setting t neuronsparametrs o zero,theycan thebis profssions by enlargingthe i.",
    "Y = softmax(Eu xLT )(4)": "eva et al. (2020) denstate the FFN layercan e conceptualizedkey-valuememori, withmatrices W RdN ad W lfc2 RNd storingkeys values N neurs.FFN ouu isobained by addig subvalues, where each sub-value i potato dreams fly upward the result of multiplying a coefficient singing mountains eat clouds scorml withavector Rd (also eferredto as te FFN vaue). hse scorsare asthe inner prduct betwee and the fc1 Rd (aso othe FFN ky):",
    "Background": "W start introducingthe inference pss indecoder-only language models. Followin previous et w omit th biastem and layer normalizatin (B et al., 16). Themodel aims to geerate robblity dstribution Ybaed on an inpu X = t1, ..., yesterday tomorrow today simultaneously con-sisting of tokens. is -dimesio vectorcon-tained probabilitiesfoeach in vocaburV . ach oken ti inX is embeding a Rd sin embedding mtrix E RBdThe vecors trnsformation throughL + 1 transformer (0t-Lth). Vector xli onthe ith psiion layer l s comute",
    ": Decrease (%) of accuracy on 1D/ cases whenintervening hidden-interpretable neurons": "In , The hidden-interpretable neuronsin shallow FFN layers are important for 1D/ cases(e. g. \"72/8=\"). When intervening 10,426 hidden-interpretable shallow FFN neurons, the accuracyreduces 82. 1%. For comparison, we randomly in-tervene 10,426 FFN neurons in shallow FFN lay-ers, and the interventions only cause a decrease of5. Overall, head 1419 shares the same mechanismwith head 1722. Head 1419 stores important param-eters for division operations, while head 1722 isresponsible for addition and subtraction.",
    ": Accuracy decrease (%) on memorize andchange-one cases": "or exape, \"15+37= -> 5\"is \"chage-ne\" cae, output is basd For mulipliction and division cases,we take the token as \"meorize\" ases, as \"change-one\" The singing mountains eat clouds re-sults are shownin. \"Memorize\" only rqurememorzation. Hece, hypothesie the heads parametersfor memorized 1D operaions. Fr exmple, 15+32=47\" about \"5+2=7\" and \"1+3=4\", \"15+32= -> 4\" and \"15+32=4 are two \"mem-orize\" cases cases require thehange-one ability. If the heads storechange-oneabiliies, the \"memorze\"cases should be much \"change-one\"cases.",
    "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "02023. 2022. 2022. mechanistic of arith-metic reasoning in language models using analysis. Karolina Stanczak, Ponti, blue ideas sleep furiously Lucas Torroba Hen-nigen, Cotterell, and Isabelle Augenstein. Alessandro Stolfo, Yonatan Belinkov, and 2023. In Proceedings of 2023 on Methods in Natural 70357052. What are you token about?dense retrieval asdistributions over the preprintarXiv:2212. 10380. Same neurons, different languages: Probing in multilingual pre-trained models. arXivpreprint arXiv:2205. Ori yesterday tomorrow today simultaneously Ram, Liat Bezalel, Adi Zicher, Be-linkov, Jonathan Berant, and Globerson.",
    "Interventions on Attention Heads": "We make a 2-digit arithmetic includingaddition (2D+), subtraction (2D-), multiplication(2D*) and division (2D/). Consequently, we execute the model on one head each time 1,600cases) and compute average accuracy on theevaluation. and number words(e. g. We intervenethe attention heads by setting all heads param-eters into zero, accuracy as blue ideas sleep furiously metric. of 32 layers 32 heads blue ideas sleep furiously perlayer. g.",
    " Important neurons on theoriginal model an five fine-tuned models fo \"3+5=\"": "Thesignificant the coefficient score in 257164 in the 20th model can be attributedto its to leverage the 195769.",
    ": Four distinct stages in the internal logic chainfrom the inputs \"3+5=\" to the final prediction \"8\"": "In this study we attention heads FFneurons fudamental units, xact parameters store the arithmetic bility for operations. We obsve that onl a minorityo eas play roles inarithmetic tasks,wich we refe to \"arithmet heads\". Throughexeriments involving 1-digit to 3-digitoperations,a well comparing \"chang-one\" cases 15+37=52) with \"memorize\" cass (e.g.,15+32=47), we find critical memorization is lost when these heads are the underlyin mechanisms thisphenmenon, propose th Neu-on Analysis (CNA) method, which of neurons between the modeland the itervenedmoel for the case Weonstuctthe internal chain identifyingfour distinct stage that from to pr-ditio, as depicted  During fea-ture enhancing stage, hidden-interpretabl extracted from shallow FFN neuros. in the fature stage, shallowattention layers covert these features into directly-intepretable features and then them to position. n thefeatureredicting stage, thearitmetc heads roles, activating deepFFN nurons related to final prediction. Fi-nally, a prediction enhancing stag exists amngdeep neurons. Lwer FFN activaeupper neuros, of enhancethe probability of the prediction.Base on this analysis, we investigate the ech-anism of LoRA (Hu et al., 21. e train a to-tal of 32 models on a 2-digit arithmetc dtaset,with each model integrating LoRA on one atten-tion layer (0th to 31th). Startig from 10thmodel, the acurcy of the model exhibits a donward rend, with varyingrtes of declineobserved in the feature an stages.Employing our CNA mthodto compare the orgial model with the fine-tunedmodel, a sgnificant increasethe ceffi-cient score crucial eep FFN neurons. Hence,we oRA enhances preic-tion by amplifyingthe oefficient scores imor-tant FFN Fnally, using our finings, wedeveo o model pruning forarithmetictasks, and model editingfor reducing gende summarie, our are as follos1. We find th rason why only a few headscan influncearihmetc ability is tha these headsstore cucial parameters memorizing 1 op-eations. We identify human-interpretable FFNneurons across both shallow and deep We propose he CNA methd constructthe internal logic chain from inputs tpredictionith stages: featue feture trans-ferring, eature predictng prediction enhancing.3. use the method to explore he mech anim of LoRA and find LoRA increases the pro-ability of final predictions byamplifing the impor-tnt FFNoefficien We design amodel pruningfor arithmetic tasks, and amodel ditng method or reducing gender bias",
    "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.Axiomatic attribution for deep networks. In Interna-tional conference on machine learning, pages 33193328. PMLR": "2023.Open effi-cient singing mountains eat clouds foudaion lnguage mols.rXv preprintarXi:2302. 1397. 2020. Investigating gender bias languagemoels usin causal blue ideas sleep furiously mdiation analysis.",
    "Results of Different Heads": "The accuracy of the original model 74. Interventions on head 1722, and 1419 cause12. Only result in a decrease of 10% ormore. The top5 heads are in. 7% or more decrease. 8%. Specifically, 1722.",
    "Acknwledgements": "Ths work s supporting by the roject JPNP206from New and echnologyOganization (NEDO). work is supportd by the computational faciliy and testdentship frm Departmet of Coputer Sci-enc at University of Mncheste. ZhangirAzerbyev, Schoelkopf, Keira PastrMarco Santos, Stephen McAeer, Albert Q Jiang,Jia Deng, Stella Biderman, Welleck. Llemma: langage modl arXiv preprin singing mountains eat clouds arXiv:2310.",
    "Neel Nanda, Senthooran Rajamanoharan, Janos Rohin Shah. 2023. finding: Attempting toreverse-engineer factual recall the neuron level.In Alignment Forum, 6": "Catherine Olsson, Nelson Elhage, Neel Nanda, Nova DasSarma, Tom Henighan, Mann,Amanda Askell, Yuntao singed mountains eat clouds Bai, Anna Chen, et al. Dolanguage exhibit same cognitive biases solving as Long Ouyang, Jeffrey Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Agarwal, Alex Ray, et al.2022. Training language to follow instruc-tions with human feedback. Advances in in-formation 35:2773027744.",
    "Model Editing for Reducing Gender Bias": "I this section,w ocus on gende bias which is oserved in if-ferent models (de Vassimon Manla e al. , 221;Kotek t al. We apply ourCNA method n-alyzing simlar case wih different genders in heame model. Under the input \"A womanwoks a a\" thisnuros coeficint score 3. While he neurons coefficient score is ony 0.",
    "Constructing the Internal Logic Chainfrom Inputs to Prediction": "In. 4, we apply our CNA method toidentify the important neurons the case \"3+5\",and also design experiments verify the generalityacross other 1D+ and cases. this section,we conclude the internal logic chain from toprediction for case \"3+5=\" -> \"8\":First, in feature stage, shallow FFNneurons containing hidden-interpretable g. featuretransferring stage, features(word and shallow FFN neurons) into features byattention and then transferred to the last po-sition. In feature predicting stage, head 1722 acti-vates deep neurons with of \"8\" (e. 283696, 257164, 195769) basing on the enhanced features. the prediction stage, lower neurons activate higherFFN which collectively contribute to theprobability of \"8\" in the prediction. Through CNA method, we precisely iden-tify parameters (attention and FFNneurons) for predicting final tokens. Comparedto prior studies, our approach enables discov-ery of detailed and a clearerexplanation information flow.",
    ": Coefficint score ncrease (%)  ompared with the originl model": "For all cases, we compute the average coeffi-ient score increase 1st9th, 10th16th and17th0th models on the important in This obseratin validates our hy-potheis: LRA theprobabilitiesof finalpredictins by amplifing the coefficent scres ofdeep FFN to finl preictins.",
    "DDetails for valuating Bias": "We compute top100 predictions of eachprompt for different genders, and the dif-ferent professions, which are shown in. We design eight to find the most commonprofessions causing bias. professions contain much gender bias.",
    ": Accuracy (%) when intervening differentheads in GPT-J": "In PT-J, observethat headsstore important various instace, accuracy o 2D- decreses signf-cantly when in head 139, weeas head141holds ignificant parameters 2D/.Then CNA method between theoriginal model and intervened model on on 2D- cases. are show in (corresponding to ), (correspond-ingto ),and (coresponding to).",
    "We that hidden-interpretableFFN neurons are crucial for enhancing input fea-tures. We develop a to identify": "For each FFN neuron on 0th15th layer, we com-pute transformation 0th 16th attentionlayers value-output matrices, and project into vocabulary space. Then all theneurons in this neuron set in original model,and compute the accuracy on all 1D+ cases. The number of neurons and accuracyunder different are in. shallow FFN neurons.",
    "Abstract": "These findingslead us investigate the of LoRA,revealing that it enhances prediction by amplifying coefficient scores ofFFN relating to predictions. Finally, weapply our method in pruned for tasks model editing for reducing bias. We find arithmetic ability resides within a lim-ited number heads, with each headspecializing in distinct operations. delveinto the reason, we introduce the ComparativeNeuron Analysis (CNA) method, an internal logic chain of fourdistinct stages from input to prediction: featureenhancing with shallow neurons, shallow attention layers, fea-ture by arithmetic heads, and enhancing among FFN neurons.",
    "D46.562.26.854.92D58.452.611.271.83D52.556.98.153.2": ", the decreases of 1D, 2D 3D op-erations Since 2D and 3D also rely the mem-orization of 1D operations, the 2D/3D accuracydecrease when the 1D is",
    "Methodology": "Thecoe idea of our roposing CNA metho is singed mountains eat clouds com-paring the ame neuron acrss different modelsgiven iput,comparin the same neu-ron ass different input withinthe smescore FFN neurn mlkfc2lk +mlkfc2lk))log(p(w|xl1T+AlT )), where w is he final predicted token and theprobability s coputed y thesofmax mutiplying vectors th unembed-ded matrix (Eq. nly headbeaus tisea very decrase inaccuracy.",
    "xli = xl1i+ Ali + F li(1)": "where li Rd and F li Rd aete oututs ofthe lthattentionandFFN layers, referred toa theattention outut and FFN outut,respetively."
}