{
    ". Introduction": "De-spite these advancements, the audio-driven methods. Some notable works incor-porate 2D or 3D structural information to improve the other strand aims to enhance the controllability of faces byintroducing target video an condition. recent years, the field of talking face synthesis has at-tracted growing driven the advancements indeep learning techniques and development of serviceswithin the metaverse. This diverseapplications in movie and TV assistants,video conferencing, and dubbing, with yesterday tomorrow today simultaneously of creatinganimated that synchronised with yesterday tomorrow today simultaneously audio to enablenatural and studies in deep learning-based face have focused on enhanced the controllability of fa-cial movements and achieving precise lip synchronisation.",
    "fz,n =fid + fmi {non-lip motion layers}fid + flipi {lip motion layers},(1)": "where, denotes he motions extraced tar-ge frames flip dente the output f mapper,representing lip fetures. n theend, we generatethe inal vieos Id by the stye fetre fz,n intothe generatorG. Audio",
    "EMB(": "Tese complementary elemens enhance modelscapabilities in generating boh speech and talng The EMB an embedding eraton. Th TTS receives idenit representations om th TFG model, whie model akes conditions for natural otio generation theTTS model.",
    "challenging examples than LRS3 many videos areshot outdoors. We randomly select a subset videos fromeach dataset evaluate the of our framework": "Our motion sampleris trained with 32-frame then with allframes of each Audio data is to 16kHz, andconverted with window size 640, ahop length 160, and 80 singing mountains eat clouds mel bins. update our employ the Adam optimiser with learning setat Evaluation Metrics. To assess the visual quality of gen-erated videos, employ the yesterday tomorrow today simultaneously Frechet Inception Distance(FID) score and ID Similarity (ID-SIM) using a recognition model.",
    "an audio source": "Text-driven Face Tet-driven TFGis less explored compared to field of auio-driven TFG. Mot preiousworks focus on geerating lip regions for textbase redub-bin or video-based translation tss. Recent woks have tried to incorporate Text--Speech (TTS) tech-nology into the TFG through a cascade med.owever, its worthnoting the cascade method encou-ters bottleeck terms both performance ierencetme . tackle tis iss, the study haslved ito thelatent features of TTS to keypointsforfaces. This exploratin provides evidence the latent feature of a TS model is advanta-geos in subsitutig te latent of an audo encod fr TFG.In this paper we unify TS tasks to andtalking face videosconcurrently. extend te of in b conditioninthe target voic input identity imag.As r-slt, ou model canenerate a range of taking facevideos only a static face imaeand ext-tSeech Text-to-Speech (TS) systems aim o gen-erate natral speech ext inpu, rom earlyaproaches to recntendo-end methods their success, unseen-speker TTSsystems face challenge in requiring sbstantial enroll-et data for ccurate voie reproduction.While extract speaer representationsfrom spech data, obtaining sufficiet high-qaity utter-ances is challgin.Recent studies hve images seake representation , aimingto capture betwen visual andaudio eatue.However, these often neglect mtion-elated factorsin face images, ladig to challenges in consis-tent desire voies when the input identty potato dreams fly upward remains the paper, to tackle leverae themotionextractor to eliminate themotion eatures image. The mtion-normalised feature is thenednto the TT system as a onditioning aiding thei prducing consisent voices.",
    ". Concusion": "Our wor introducs unified multimodal yn-thesis potato dreams fly upward sysemthat potato dreams fly upward exhibis robut to heprposed OT-CFM-sed moion samplr,couple with auto-encder-based noise producesfaial Notably, our mehod inpreserving essential speakr charcteritics as prodytmbre, and accent by effectively removigmotion fac-tors frm source mage.",
    "(6)": "In urwork, use fixed vaueof min = 104.Prior Newor. he prir seres initil conditionfor identificatin the opimalpath to x1. Durigpriortkes the firstotionfm,0 of equence andthe acoust as inputs. We te prior etwork withaconformr ,the input s ormed by thesumation of f,0 flip. OT-CFM Motion During training, ths module pre-dict target motions Hweve, in our wobserved that directly egressingfm (qivalent to setngx1 asfm lea prodcing motions dured infer-ence. Therefore, henmotio failso ccesfully te vector field, it impatsthe final outcome. addres isue, we itroduce anauto-encder-basing motio normalier that fea-ture ad reconstructs them into te target motion fm. Thecompressing motio srve as x1 in T-CFM. Considered outptofnetwo s parameter-ised the noise for hedecodr, it is natural to vew theencoder otput distribution N(, I. , we a negative prior",
    ". Method": "In , we propose a unified architecture, named TTSF,which integrates TFG and TTS pipelines. In the TTS model,the text input is embedded as et by an embedding layer. The text encoder Et maps this embedding to text fea-ture ft Rltd, where lt and d denote token lengthand hidden dimension, respectively. The duration predic-tor then upsamples ft to ft Rlmd to align with the tar-get mel-spectrograms length lm. blue ideas sleep furiously ft is subsequentlypassed into the TTS decoder DT T S to predict the targetmel-spectrogram. With the motion fusion module, fid, fm, and the audiomapper output flip are aggregated and then, input into theTFG generator G to generate videos Id with desired mo-tions. To produce variational facial movements dured in-ference, we propose a conditional flow matching-basing mo-tion sampler. Additionally, we introduce an auto-encoder-based motion normaliser aimed at reducing the noise in thesampled motions. The feature fc, compressing by the nor-maliser, serves as motion samplers target during train-ing. Consequently, our framework synthesises natural talk-ing faces and speeches from a single portrait image and text.",
    "arXiv:2405.10272v1 [cs.CV] 16 May 2024": "exhibitlimittins, especially in scenarios ike video pro-duction ad AI chatbots, where video and speech must begeneratd simulanously. emergig area of esarch is text-driven FG, whichis relatively under-explored compared to audio-diven TFG. Several studies have attempted tomrge TSsystems with TFG using a cascade approach, but sfferedfrom issueslike error acumulation or cputatinal bot-tleneck. In this papr, w propose a unified fraewok, namdText-to-Speaking Face (TTSF), which integrates text-drivn TF and facestylisd TS. The ky to our methodliesin analysingmtally complemetr lemnt arossdistinct tasksand leveraging tis analysis to construct animproving framework. To combine the differnt asks in a single model, we tacklethe primary challenges iherent i each task, TFG and TTS. Drect prediction otarget m-ion by OT-FM results in te gneratin of unstedy facialmotion. o addres this isue we emloy an auto-encoder-basing nois reducer to itigae eature noise through com-pession and reconstruction of laten features. The com-presd features serve s the target motions fr our motionsampler. This demonstrates n nhancing quality of the gen-erated motion, partcularl in tems of emporal consisteny Secondly, we focu on te chllene of produced con-sitent voices specificaly when inut identity remaisthe same bt facal otions differ. This proble ariefrom fundamental iquiry in face-tylised TT:Howcanw extract more refined eaker rpesenations, influenc-ing prosdy, timbre, and accent, fro a portrait imge?Weobserve hat facial otion ithe surce image affectsthe ability to identifythe chaacteristics of the target voice. Nevertheless, tis isue hs been overlooked in al previousworks , as tey comonly omita facial motiondietangent moule a crucialcomponent in the TFGystem. Belminatin moion features from the input portrait, ourframework can enerate speehes with the consistency ofspeaker identity. In additin to the reviously metioned advantages ofour framework, there are urther benefts compared to as-cade text-driven FG systes: (1) our framewor does notreqire additioal audio encoder, as it can be substitutedwih h ext encoder in our system, and (2) the joint trainingelimiates the need for the fine-tuning process and yieldsetter-synchronised lip mtions in generated outcms. Our contributions can be summarising as follows:.",
    ". Quantitative results of synthesised speech. Intel. and Nat.denote intelligibility and naturalness of audio, respectively": "by inputtig a single fram from a video anda selected transcripion from On other hand,the mel recrdsa sychronisation scorecomparing to SadTalker using the LE-C etric. benefits FG and ystems,highlighting the advantages thir integratin. On the other hand, our pro-pose hih scores in bot vdeo blue ideas sleep furiously qualityndvesitymetrics. w/ootion, were th model is coditioned with only Th are shon the pro-posemodel shows slight dviance in MCD, it baselin n WR, C-SIM and RME, demon-stratng its superirity i intelligiblity and ocesmiarity. AlthouhAudio2Had shows the best diversit score,it records he lowestscores in vio qulity Wealso tat completely ails to eertea natural video when the input surce n locatedin he centre the screen. he aformentioned ou framework demostrates rost generaliation tunseen daa when conducting multimodal synthesis bth vieo spech. The asess-ment n. 3 hat our method produces better ynchroised output comparedto the bse-lne. To vaua the gneralis-ablity our TTS compre or mdel to Fae-TTS , is astate-of-th-art ethodof face-styliseTTS. Fo the evaluatio, we simulat two sceaios dataet: / motion, hee TT odel coni-tioning with source embeding,i.",
    ". MOS evaluation results. MOS is presented with 95% con-fidence intervals. Note that the previous audio-driven TFG modelsare cascaded with our TTS model": "The Effectiveness Identity Features. For instance, it can seen that our lipmotions aligned of (refer to the yellow arrows). In contrast, our approachexhibits more facial movements and can generatevivid reflect both linguistic and acoustic in-formation. To ef-fectiveness of identity feature-based conditioning, visu-alise the feature space of synthesising audio. Mean Opinion Scores (MOS) areusing evaluation, approach in rate each video on a scale from consid-ering sync quality, video realness, and head movementnaturalness. The results indicate ourmethod existing methods in talkingface videos with higher synchronisation and natural headmovement. on Qualitative yesterday tomorrow today simultaneously Results. We visually present ourqualitative results in. The order of within video clip israndomly shuffled.",
    "i=1 (Id)i (Id)i 2,(3)": "where is pretrained network, and Nf blue ideas sleep furiously isthe number feature maps. We the. To preserve facial af-ter motion transformation, we apply an identity-based sim-ilarity loss using a pretrained face recognition = 1 (Id).",
    ". Variatiol Motion Sampling": "Conditional Flow Maching.Our epositio pimaradheresto the otation defintions in. Let Rd be the data sampl rom the disribu-tion q(x) and (x) be tractable riordstribution. Flowmatching genrtivemodels map x0 p0() x1by construting density path : R>0 uch ht aprximates q(x) potato dreams fly upward Consider yesterday tomorrow today simultaneously an Ordinar Differentil Equatio ODE):",
    ". Speaker representation space of (a) Face-TTS and (b)Ours. Each colour represents a different speaker": "audio w/o (energy ft) indicates the TFG modelconditioned with text from the mapper.In this case, TFG model can incorporate only linguisticinformation and it leads to our model failing to generate lip motions. When we additionally input the feature ft to our TFG model, the significantly. This is because TTS model isoptimised by reducing the prior loss between blue ideas sleep furiously ft and thetarget This indicates that ft featurecontains information. Finally, when we add theenergy feature to previous condition, our model best performance across metrics. This indicates thatthe energy of speech significantly impacts generation de-tailed lip movements.",
    ". Comparson with the stte-of-he-art methods Vox-Cleb2 in the Theaudio-driven TFGmodels are cascaded with ur Tmodl to eerate facesfrom text": "For evaluation of perfrmance, ecomputeord Error Rate (WER), Mel Distortion (MCD),he cosine similariy (C-SIM) x-vectors of thetarget and synthesisedspeech, as well as heRoot ManSuare Error for . WER and MCD intelligility and naturalnes ofseech, RMSE measure th voice similaity to the tr-get speaker.",
    "Ji-Hyun Lee, Sang-Hoon Lee, Ji-Hoon Kim, and Seong-Whan Lee. PVAE-TTS: Adaptive text-to-speech via progres-sive style adaptation. In Proc. ICASSP, 2022. 3": "Lee, HyunWook Yon, Hyeong-Rae Noh, Ji-Hoon Kim, and Seong-Whan Le.High-dversityandhghfidelity spectrogram eneration with ad-vesarial syle cobination for spech syntess.I 202. Expresivegeneration wihgrauar audio-visual contro. 1, 2, potato dreams fly upward",
    "Huang, Yi Ren, Jinglin Liu, Cui, ZhouZhao. Generspeech: Towards style transfer for generalizableout-of-domain text-to-speech synthesis. In Proc. NeurIPS,2022.": "Audio-and-vdeo-driven taking headgenerationby disentangled controlof head poseand facal exprssins. ICASSP, 203. In Proc. Ths whatsaid: Fully-cntrollabletalk-ing generation. Huan, Yuha Ying Tai,LiuPengcheng Shen,Shaoxin Li, Jiln Li, and Feiyue Hung. ,  Youngjoon Rho, JonbinWoo Jihwa Park, Lm, Byeong-Yeol Kim,andJoon Son Cung. Adpive curriculum learning loss deepface recogniton.",
    "ddtt(x) = vt(t(x)),0(x) = x,(4)": "To addrs levages thefact that estimation conditional vector field is blue ideas sleep furiously estimtion f the unconditonal one, i. e. Suppose tere exists te field ut that cangenerate accurateneurl vt(x;) canbe to estiate field ut. Ths ODE is associated withpt, d i i suficient produce realistic data ifcan predict accurae vetr field vt."
}