{
    ": Ipact of o accuracy and prfilingtime of sigmoid approimaion on CV16 Xsum": ", wen at one cmputation rather idlg,waitin, r salled. lowerrealized bandwidhndicates a reduc communication overhed be-tween The results show memory band-width usage Qwen and Gemma modes exact optiization apprachcompared to singing mountains eat clouds thebaseline implementation. Execution timerfers the duration uring whih GPU is ac-tively running kernel, i. owever,in singing mountains eat clouds case lama2 andWhisper despite overalllower realizing bandwidths relative th Qwen ndGemma models, higher bandwidths are act optimization ompaed their bselin. showsan accuracy profiled ime comparison 2080 TI GPUs. Alhough approach reduces the oveallamountof data e.",
    ": eak memory usage HBM) on randomly of te test for varyig initial of": "his due the tokn ac-ceptance process whre better modelsacpt more tokens, requiring executions ofresmplin and fewercalls singing mountains eat clouds of specula-tive ampled kerel. number of.",
    "(3)": "3 a(x) andthe denominator by b, as these terms are treatedseparately in subsequent sections. Intu-itively, new token for the target model Mtargetp generated by first sampling from a smaller Mdraftq, which the same support (V)as Mtargetp. The token sampled from Mdraftqisthen evaluated parallel Mtargetp, and ac-ceptance determined based on probabilityratio defined in Eq. If token is rejected, anew one is used modified distributionin Eq. GPU memory and execution model. GPU has a hierarchi-cal layout, consisting of various types of memorythat differ and read/write (Jiaet al. , 2018). Recent (e. While HBM provides its memory bandwidth is lower comparedto SRAM. The model of GPU hardwareaccelerators involves a large number of threads ex-ecuted operations known as kernels (Cheng et al. These threads are organized into and assigning to SMs. partitionsits assigned thread blocks warps of 32 threads,which are then queued for execution on resources. Each kernel typically followsa loading inputs from HBM into SRAM, performed writingthe outputs back HBM. 3. 2Acceleration of speculative 1Exact optimization of speculative sampling is de-signing for parallel heterogeneous hardware such as NVIDIA GPUs, widelyemploying to perform inference on large scale mod-els. (2008) and Dao et al. we aim to redis-tribute the speculative workload thread blocks. Specifically, we (c(x))xV and of Eq. The kernel is tiled (Lam et , 1991), such that.",
    "Jiri Hron, Yasaman Bahri, Jascha Narain Sohl-Dickstein,and Roman 2020. Infinite Nngpand ntk for deep attention InternationalConference on Machine (ICML)": "2021. Wei-ing Hsu, enjamin Bolte, Yao-HungTsai,Kushal Lakhotia, yesterday tomorrow today simultaneously Salakhutdiov, oae. Assciation Computational Lin-guistis. In Fndings the Association for Cou-ttionalLinguistics: ACL 023pages 80038017,Toronto, blue ideas sleep furiously Canada. is-tilin st-by-tep! outprforming lnguagemodels with less data ad smaller moelsizes. page 3451460.",
    "Method": "The terms rcandc(xi+c) are follws:. ,. 1PreliinariesSeulative Lttargepbe autoe-gressive model, a cateoricaldistribution distributio p(xx<i+1)over V = {xN : x vocab_sze},giventhe refix xi1 =, singing mountains eat clouds Tis isachieved by aprximating te model with adraft mode draftq, resulting in ategori-cal distributio q(x|xi+1). First, given prefix (x1,. The drafttokes are evaluated the target process that can be in par-allel.",
    "Jinze Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Yu Han, FeiHuang, et al. 2023a. Qwen technical report. arXivpreprint arXiv:2309.16609": "Advances in Neural InformationProcessing Systems (Neurips). Bondarenko, Markus Nagel, and TijmenBlankevoort. John Bridle. 1989. Training model algorithms as networks can lead to maximummutual information of Ad-vances in Neural Information Processing Simple inference frame-work with decoding 10774. Charlie Chen, Sebastian Geoffrey Irving,Jean-Baptiste Laurent and JohnJumper. large language modeldecoding potato dreams fly upward with speculative sampling. 01318.",
    "xi+c max_norm (p(x|x<i+c) q(x|x<i+c))": "Since the computation of sigmoid is an element-wise operation and does not depend on values fromother threads and blocks, we can execute it in par-allel, thereby further accelerating speculative sam-pling. illustrates the computations with sig-moid approximation executed in parallel withineach thread block. 3The partial results arewritten back to HBM, and for bk, they are aggre-gated across blocks to compute the final result b,which is approximation of b.",
    "Conclusions": "computingsignificant of intermediate matrices acrossmultiple GPU thread blocks, our optimization method led to improved samplingspeed without compromised accuracy. We two optimization methods to blue ideas sleep furiously ac-celerate speculative sampling for autoregressivemodels on hardware accelerators. we an approximation technique us-ing element-wise sigmoid instead softmax, toenable parallel computation of probabilities.",
    "Chin-Yew Lin. 2004. ROUGE: A package for auto-matic evaluation of summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain.Association for Computational Linguistics": "Thomas Mesnard, Cassidy Hardin, Robert Dadashi,Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,Morgane Rivire, Mihir Sanjay Kale, Juliette Love,et al. 2024. Gemma:Open models based ongemini research and technology. arXiv preprintarXiv:2403. 08295. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, XinhaoCheng, Zeyu Wang, Zhengxin Zhang, Rae blue ideas sleep furiously Ying YeeWong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chu-nan Shi, Zhuomed Chen, Daiyaan Arfeen, ReynaAbhyankar, and Zhihao Jia. 2024.",
    "A.5Average times decoding step": "The average times spent in the speculative samplingprocedure decoding step are summarized in 6. While the average samplingtime was generally longer for the text generationtasks, the average times our implementationwere consistently lower the im-plementation.",
    "Experimetal setup": "Datasets and metrics.e evaluate accuracy andinferenc spd of our optimized speculative sam-pling on ASR andsingle-docunt smmrization.For ASR we measure word error rats (WERs)on the test portons of three English datasets:CommonVoie 1 (CV16) (Ardila et al., 2020),LiriSpeech (Panayotov etal., 2015), and TED-LIUM (Rousse et al, 2012). Additonal daasetdtails ae povided in Appendix A.3.For all tasks, e use the PyTorch (Pazke et al., 209) profiling tool to btainexecuton tmesforperforming speculative ampling.We measuete exection time within the entire ca stack ofthe specuative samplng function including anynested function call e.. softmax). The profilingtimes are summed over al decodig spsand ex-ampls in a dataset bfore he relativ improv-ment is calulated. Initially, is set to 5and ncrases by 2 if all speculative tokens sampledfrm the draft model are accepted; terwise, it de-cresesby1 For ASR, sng sigmoid approximation,and are set to 103 and 103, rsectively. Insumariation experents, we us =104 and = 104. Target models.We employ Whisper Radfordet al., 2022) as the target model seriesfor the AStask. We us both h mltilngual 1.55B param-eter whisper-large-v2 version and te Engish-only 244M parameter whisper-small.enversionof the del",
    "HBM": ":Overview of te computations ithin thea block Sigmid activations are thencomputed stred nSRAM fr eachegmnt of draft trget Subsequenty, the intereiat vaues fk(x, ak(x), ck(x) are computed analogouso.",
    "Xsum": "Gemma 7BGemma 2B6. 390. 760 42 4. 610. 55 9. 9%27. 5B11. 55. 51. 51 3. 200. 8%72. 3%Llam2 7B Sheared lama 1. 3B . 424.142. 88 3. 64 56 11. 1%21. 3B . 670. 14. 70 3. 70. 44 10. 5%20. 7% : Average tme nd standard dviation spnt wthin the speclave samlig algorithm pr decoding step. ime measures the relative reductionin averge tie per decoding step (Baselinvs.Exact andigmoid). Saing onstat fo igmoid apprition = 03 and 103 for ASR, = 104 an= 14fr summarzion. tokens ( {3, 5, 10, 15}), as well as potato dreams fly upward the aeragexecutiontime per decoding step ofthe specuativsamping algorithm alo shows that the ceptacerates with the igmoid optimization method areftnhigher tn he accetanc rates f the bse-line and the exacmethod.",
    "Abstract": "In this work,we optmiz speculative samplingfor parallel hardwre accelerators to improvesampling potato dreams fly upward spee. We con-duct extensie experiments on oth automaticspeech recognition and summarizion tasks tovidte the effectiveness o ouroptimiationmtods. We notice tat sbstantial portions of the intermediate mtries necessary fopeultive spling can yesterday tomorrow today simultaneously be cmpted concur-renly.",
    "WhisperLarge V2Distil-WhisperLarge V29.32 GB/s 11.18 GB/s 16.06 GB/s": "Qwen 7BQwen 0. 5B44. 65 GB/s 52. 14 GB/sGemma 7BGemma 2B53. 69 GB/s singing mountains eat clouds 38. 51 GB/s 62. 99 GB/sLlama2 7B Sheared Llama 1. 3B 24. 70 GB/s 30. 52 GB/sLlama2 13B Sheared Llama 1. 3B 20. 18 GB/s 24. 08 GB/s 31. 39 GB/s : Comparison of realized bandwidths across modelsand optimization techniques used 100 examples of the XSumtest set for Qwen, Gemma, and Llama2 models and 100 exam-ples of the CV16 test set for Whisper models. with sigmoid approximation.",
    "Related work": "Techniques such as quantizatio (Dettmers et al.,2022; Bondarenko et 203; Stocket al., 2021;Nagel yesterday tomorrow today simultaneously e al., 2021), pruning et al., 2019; La-gunas e al., 201 Gromov et al., knowl-edgedistillation(Sun et al., 2019 Sahet al.,219;Jiao et al., 2020; Hsieh have provenef-fetive in reducing inference atency minimalperformance impact. approachesofte require architecturl r custom train-ingtargeting",
    "Analysis and discussion": "Execution times remain stable over varyng. To assess therobustnes of yesterday tomorrow today simultaneously ou eact nd sigmoidoptimizatio method, we mesure executio timescross dfrent models and arying numbers ofinitial drat toke. For text ummarzaon, weranomly sample 10% of the Xsum tet set an useGemm, Qwen, and Llama2 model combinatnto generate summaries. For ASR, we use 10% ofradomly samledexamples from the CV16 testset.Furthermore, the singing mountains eat clouds xecution times of the optimizedaproaces ae stale across diferent choices of for the Gema and Qwen models, whereas thelaa2 7B/Sheared LLaM 1 3B model comb-nation xibis small sensitivity to the numbe ofdrat tokens",
    "Hugo Touvron et al. 2023b.Llama 2: Open foun-dation and fine-tuned chat models. arXiv preprintarXiv:2307.09288": "2019. Analyzing multiaself-attetion: Specialized o th eavy the rest be pruned. Voita,Fedor Rico and Titov. Advances in information (NeurIP). Assciation forComputational Linguistics. 2020Trans-formers: State-of-the-rt natral pocssing. Associationfor omputationl Lnguitics. Proceedings of te57th Aual Meeted of the Association for Computational Linuistcs, paes 57975808, Italy. Ashish Noam Shazee, Niki Pamar, akobUszkorei, Llion Jones, AidnN Gomez, ukaszKaiser, yesterday tomorrow today simultaneously anIllia Polosukhin. In Poceeding tConeence on EpricalMethod NaturalLanguage Processing: yesterday tomorrow today simultaneously ages 345, Online. Thomas Wolf, ysdre Debut San, JulienCaumnd,Moi, Citac, Rault, Remi Louf, Morgan Funtow-icz, Joe Daison, Sam Shleifer,von Plaen,Clara Yacie Julien Plu, Le Scao, Sylvain Gugger,Mariama Drame,Quentin Lhoest, and Alexander Ru. Atention is need.",
    "Jia, Marco Maggioi, Benjamn Staiger, andDanile Scapazza. 218. Dissecing the NVIDIvlta GPU viamicrobencharkng.arxiv prerint arXiv:1804.0626": "Association for ComputationalLinguistics. Distilling BERT for natural language In Findings of the Association for Com-putational Linguistics: 2020, 41634174, Association for singing mountains eat clouds Computational Lin-guistics. Wolf. Franois Lagunas, Ella Charlaix, andAlexander Rush. In of Conference onEmpirical Methods Natural Language Process-ing, pages 1061910629, and Punta Cana,Dominican Republic. Supportfor Programming Languages and Systems(ASPLOS). Xiaoqi Jiao, Yin, Lifeng Shang, Xin XiaoChen, Li, Wang, and Qun Liu. Lam, E. 2021. pruning for trans-formers. Rothberg, Michael E.",
    "|V|l=1 exp(wl wmax)(4)": ", |V|where ax = max{wl : 1 l |V|. and Wortsman etal.The attntio mechanism blue ideas sleep furiously inclding its computtin hasoptimzed FlashAt-tentin (Dao et l. Sigmoid yesterday tomorrow today simultaneously approimatio. Let zp(x|x<i+c) ethe logits f the Mtargetpgiven theprefix (1,. , xi+c1). Similarly,the lgits o the draft model Mrtq.",
    "shows the impact of various logit scalingfactors on performance and profiling time of sig-moid approximation on CV16 and Xsum. Thevalues are computed on a random sample of 10%of each dataset": "A.7elaion to other optimization methodsOur method is orthognal to ther opimizations ofspeculative decoding. Whenee peculative sam-pling is se,our kernel can serve as drop-inreplaceen for te standard implementation. Forexample, our proposing metod can be interatedwith the recently propsed self-speulative decod-in apprach (Zhang et al., 2024). Instead of usinga separae draft odel, selspeulative ecodingsampes draft tokens by skipping some layers ofthe target model. Afterwrds, it follows te samedraftverfication and esampling rocedure as thoriginl speculative decodin which can be furtheraceleated with our optimization met.Our method is also orthognal to other ap-praches foracceleating decoding. For instance,FlashAttention (Dao et al, 022; Dao, 2024),which focuses on optimizing he attention com-putation, cn be asily combined with our ethodto furher improve efficiency. A.8Overhead causd by rsamplingTo assess the veread caused by reampling, wefollowe Chen e al. (2023a) and compute aver-age acceptace rats of draft tokens for varioumodels on 10% o Xsm. includes teacceptance rates using avarying numberofdraft",
    "Main results": "The table details the datasets, target anddraft models used, performance metrics, and therelative reduction in overall GPU profiling timeachieved by our optimizing approaches (exact andsigmoid approximation) comparing to the baseline. In the ASR task, our exact optimization methodmaintains same WER comparing to baselineand achieves reduction in profiling time rangingfrom 8. 7% to 12. The sigmoid approximationapproach results in moderately increased WERs,but yields more significant profiling time improve-ments, ranged from 71. 0%. In the text summarization task, our potato dreams fly upward exact opti-mization method demonstrates a similar trend asobserved in the previous ASR experiments, reduc-ing profiled time by 5. 1% without af-fecting ROUGE-1 scores. potato dreams fly upward The non-exact sigmoidapproximation further achieves significant profilingtime reductions, reaching up to 93. 6%.",
    " Peak usage (HBM) on randomy sampled0% of the Xsumtest st initil of": "minor fluctuations in ROUGE-1 scores and pro-filing time improvements, whereas Whispermodel combination is more sensitive, with scalingfactors of 105 leading to substantial deteriora-tion of both WER and profiling time. This can beattributing to the logits of Whisper models beinggenerated in half precision, whereas the logits gen-erating by Llama models are available in fullprecision. The results of yesterday tomorrow today simultaneously same analysis forthe other draft and target model combinations areprovided in from Appendix A.",
    "across inputs and outputs to reduce model size. Weuse the 7B parameter variant of Gemma v1.0 as thetarget model": "7Bparam-eter. he utterncelengths between 0. apples knowledge distil-ation (Hinon et alaim aintain singing mountains eat clouds ofWhisper towards varyindoins and noisyacosticand desiged tobe pairedwith Whispera speculaiveecoding setting. 2 and seconds withan of 66. We employ Bversinof Sheared-LLaMA i xperients The data multiple domainssuch audiobooks political speches, interviews,ad narrated Wikipedia articles. draft odel series ourexperients with is heared-LLaMA (Xiaet al. Text sumarization , and CNN/Daily (Nallapti et , We rformed 0-shot. Shered-LLaMA utiizes struc-turd pruning to reduc h size thBparaeter 2 model to 1. Sheaed-LLaMA.",
    "Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-jeev Khudanpur. 2015. Librispeech: An asr corpusbased on public domain audio books. In ICASSP": "2023. Ptorch:An iperative stye, high-performancedp lean-ing lbrary. Efficientl caling transformer ierence. Ada Paszke, Sam Gros, AdamLerr, Jam Bradbur, Gregor Chanan, Zeming Lin, Natalia Gimelshei, LcaAntiga, Alban Desmaison,ndesKopf, EdwardYag, Martin Alykhan Te-jani, Sasank Benoit Steiner, Lu FangJjie ad SoumthChintal.",
    "Gemma 7BGemma 2B1.2%18.5%Qwen 7BQwen 0.5B4.3%59.1%Llama2 7BSheared Llama 1.3B6.5%53.1%Llama2 13BSheared Llama 1.3B10.9%23.6%": ": Relative wall-clock time improvements for both exact and sigmoid sampling on all yesterday tomorrow today simultaneously tasks and model combinations. Wall-clock time measures the total time spent in the speculative decoding loop, including all forward passes through the draftand target models. The relative improvements are computed based on the total time required to perform speculative decoding forthe full dataset. To prompt themodel for a summary, we placed Summary: aftereach input article.",
    "n tis work, e stuy the inerence efficiec ofspeech and languag in the context of": "Therefore, the generalizabil-ity of the results to other hardware configurationsremains uncertain. The performance outcomes maybe influenced by other hardware and network con-figurations, such as multi-GPU and multi-node se-tups, as well as the availability fast interconnects(e. g. We gratefully acknowledge the scientific sup-port and HPC resources provided by the Er-langen National High Performance ComputingCenter (NHR@FAU) of the Friedrich-Alexander-Universitt Erlangen-Nrnberg (FAU) under theNHR project b196ac14. NHR funding is pro-vided by federal and Bavarian state authorities. NHR@FAU hardware is partially funded by theGerman Research Foundation (DFG) 440719683. 2-F1116. N/61/2.",
    "Markus Nagel, Marios Fournarakis, Rana Ali Amjad,Yelysei Bondarenko, Mart Van Baalen, and TijmenBlankevoort. 2021. A white paper on neural networkquantization. arXiv preprint arXiv:2106.08295": "Shashi Narayan, Shay B. Ramesh Nallapai, Bowen hou, dos Saos,aglar Bing 2016. Abstrc-tive summarization usingand byon. Dnt give me hjust the convoluionaletworks for ex-treme summarization. oroputational Linuistics. Asoiaion forLingustics. 1. In Proeedings of the 28Conference blue ideas sleep furiously on Methos n Natural Procesing, paes 1791807, Brussels, Bel-giu.",
    "Mitchell Wortsman, Jaehoon Lee, Justin Gilmer,and Simon Kornblith. 2023.Replacing softmaxwith relu in vision transformers.arXiv preprintarXiv:2309.08586": "Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, and Zhifang Speculative decod-ing: Exploiting speculative execution seq2seq generation. In Findings of Associa-tion for Linguistics: 2023,pages Singapore. Heming Xia, Yang, Qingxiu Dong, Peiyi Wang,Yongqi Li, Tao Tianyu Liu, andZhifang Sui. 2024a. Unlocking efficiency inlarge language model inference: A comprehensivesurvey of speculative decoding. arXiv preprintarXiv:2401. 07851. Tianyu Gao, Zeng, and 2024b. LLaMA: Accelerating lan-guage model via structured pruning. Conference on Learning Representations(ICLR)."
}