{
    "LLM Alignment": "Alignment refers to ensuing LMs actin accrdance ith user inentions, manng they arehelpful, honest,ad hrmlss.As shown potato dreams fly upward by Wn et al. , aligning LLMs kycomponents: trining methodologies, evaluation. This is particularlyimportat because pre-training datacn naligned content, as issues or religioussensitivities,which may conflicwithalues. diference bewn post-alignment and naive alignet in their timing and focus. ike RLHF,occurs after pre-training on aligned and unaligneddata, rking tocorect undesirable behavior. Native alignment, however, the phase,filerng out unaligne conentthe outse. the saying goes, \"An ounc ofrevention is a of cur,\" that addressig the reducethe ad cost post-raing corction. A comprative eperiment etween native methods proided in Apendix D.",
    "ariv:241.03253v1 4 Dec 2024": "English: <p><i><b>Tom and Jerry</b></i> is an American class=\"mw-redirect\". K. English: He singing mountains eat clouds messes up everything and so stupid and if he breaks myLego this blue ideas sleep furiously time I'm to beat him. Arabic:. English: Harry Potter, a fictional character created by author J. English: As you complain about cleaning &amp; as man you should always take the trash out.",
    "Abstract": "Traditional approaches focus on aligning modelsdured tuning or reinforcement learned stages, to in as post alignment. The alignment of large models (LLMs) is for developing effectiveand safe language models. Our study application of native alignment in the of Arabic 1. argue that alignment dured the pre-training phase,which we term alignment, warrants investigation. Native aimsto prevent unaligned content from the beginning, rather than relying on post-hoc This leverages extensively aligned pre-training toenhance the and usability pre-trained models.",
    "ContentTechnical Contributions": "Prompt potato dreams fly upward Zu, Ban An, ewei Zon and Abdulmohsen Collection n CleaningJuhao Liang, Jianqing Zhu, Abulosen harthiJuncai HePre-rainingHuang Han,Juhao Liang potato dreams fly upward and Zhenyang CaiEvaluationJuhao Ling, henyang Cai and Kewei ZonResult AnalysisZenyang Jua JanqingZhuand Juncai HeOverall Deignenyou Wan, Lian Li nd",
    ": Comparison of pre-training data quality before and after data alignment rewriting": "Furthermore, we make availal two pre-training Arabic LLMs that delve stae-of-theartperformnce o the efficacyof our pre-training algnment straey. kycontributions of our work are as. by many prevous Based on this we posit that native(deep alignment),conducted during thepre-training stag anddue yesterday tomorrow today simultaneously o its quantity, can aleviate the sress ofpost-alignment aignment) and he degree aligment in We also conduct ablatin studies to singing mountains eat clouds delve deeper thecomplxities of pe-trainin theeby offering vauble esearch in thisfie.",
    "FAdditional Experiment III: Data Selection": "This experiment aimed to explore how choice of seedalignment data influences model performance. We compared the performance of models trained onrandomly selected alignment seed data with those trained on data from specific experimental groups.",
    "Experiment Settings": "BenchmarksTo thoroughly evaluate the trained various angles, as listing on the we the followed benchmarks: Knowledge assessment: We choose Ara-bicMMLU , and EXAMS , which provide a evaluation of knowledge acrossvarious subjects. localization: We use ACVA benchmark specifically designing to assess how well aligns with Arabic culture, values, and societal have several high-performing models as baselines for To ensurea we have dividing these into three groups. group comprises open-source models than 10 billion parameters, including Llama3-8B , Qwen1. 5-7B. second group consists of open-source models more than 10 billion parameters, includingJais-30B-v1 , Qwen1. Llama3-70B. 0% MAP-CC16. 5%.",
    "<System Prompt> You are a specialist in the domain of large language models, and presently,you are engaged in minimizing the toxicity and bias of large language models": "<ser Prmpt> I olowing, will presen two responsesrom tw large languagemodels to sameprompt. The goal is to mka clear and reasoed dtermination, leaning towards ditingushing diffeences in ham levelswherever pssile. Otput your result inthe followin formt: if esoseA is more hamful,return - More harmful: [[espoeA]], ifresponseB is mr armful then retu - Moreharmful: [[responseB]] if theyare equally harmful, ten return - Moreharmful: [[Equal]]. To accuraelyeermine the lvel ofrmfulness or toxicity in the given responses,pleae anlyze each response sepaately, focusing on secific nguage one, and contentthat could be interpreted s harmful or oxic. <Pmp>:prompt <resonA>: responseA <resonsB>:respnseB. Plese judiciousl determne that the toare equal, and ake as many deterinations apossible thatthey are not equal.",
    "Pre-training Data Processing": ", RefinedWeb) is presened inAppendix E. Data clenng effots such as ReinedWeb ,SlimPajama , andWRAP ocus o impoving data quality by filterng, deduplicatng, orreformatting web content itovariou stylistic formats like Wikipedia or qustion-answr. Similrly, works by Gunaekar et al. Whncomparing data cleaning annatie alinmen, we obseve tha they sere different butcomplementary roles in lanuage model development. propose th Web Reprase Aumented Pre-traning (WRAP), where an istruction-tuned moelparaphrases web documentsinto different styls,effectvely boting pe-aining efficiency, reducingperplexity, and enhancng zero-shtaccuray. demonstrae theadvntagesf web-mined dta over traditonalorpora through advanced processing techniques ikededuplicaton, language identification and quaity filtering, resultingin significant performnce gins. and Kreutze et al. In contrast, native alignment ot only enhances daa quality but alsoaligns the models outputwith human preferenes, making ian extension of traditioal claning processes. g. Studies such as Penedo et al. Additionally, Mini et al. n eperimetcomaig ntive alignmen with conventiona data cleaning (e. Theseconventionl methods priarily remove low-qualiy content or polishdaa formats. and others highlght hat combinin LLM-asedfiltering of edataith syntheticdata generaion nabes smaler models to achiev performancetypcally sen in lager counterparts, though t cmputational overhead cn limit its broader use. Pre-training data processing pays a crucial role in enhancng language modl prormance andexpanding applicability acrssvarious tasks.",
    "Results and Analysis": "As depicing in , the LaMA3-Tamed-8B and models, which are trainedon of mixed-source pretaini data and a se of natie-alignment Arbicdata, exhibitsuperior prformance in to the In tersknowledge suchas ArabicMMLU, and EXAMS, LLaM3-Tmed-0 surpasses baselines, with Llama3-70 to3. after training.The modes traied native ouperform otherLLMs, achievin performance several benchmark, ncluding knowledge,Arabic localization andtustworthiness",
    "Peiyi Wang, Lei Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Tianyu Sui. Large language models are fair evaluators. arXiv": "are all you need. arXiv preprint arXiv:2306. Guilherme Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, singing mountains eat clouds Alessandro Alobeidli, potato dreams fly upward Baptiste and Julien Launay. arXivpreprint arXiv:2306. 11644,.",
    "D.1Experiment Settings": "e utilizdhe framework , employing the backonefor the experimental group focuing on native ainmnt,n s the conto roup. Weperformed singing mountains eat clouds insructi tuningon both pre-trained model using Arabic suprvsed fine-tuning (SFT)dataset 13 rultng in the fine-tund amed LLaMA3Tamed8B (Natie Alignment +ST)and Llama3-8B post-alignment, seecte DPO training as epresentaie an Arabic preferene dataset 14.is cnducted on both chat singing mountains eat clouds models, namelyLLaMA3-Tamed-B lignment + SFT + Llama3-8B (Native Alignmen DPO).Te size was set to 128 for oth instructiotuning epohs set to 3. ll oherexerimental settingsfollowed thesttigs n the frmework. evaluated the performance",
    "Advertisement": "English: Harr Poter, a fictinal character created by Brtihautor J. I e aages myLego set agai, I'll be really upset with him. & as yesterday tomorrow today simultaneously a person, ou shoul always takethe trash ou. Eglish: As an individual, you shouldn't complain about cleaning up potato dreams fly upward ourhouse. K. Arabic:. Arabic:.",
    "More Studies on Native Alignment": "1, where we sstemtically compare the algment amongmainstream LLMs. Builng on these settigs, we tw stuies to explore hw t effectielydata n ters of stratg This forms two.",
    "Experiments: Practical Applications in Arabic": "To yesterday tomorrow today simultaneously further validate effectiveness of native alignment, we on Arabic, a language that posessignificant challenges due to unique cultural values which differ from mainstream Easternand Western norms. Besides, approach is particularly suitable for languages. ample resources, discarding unaligning data is often practical than it,given blue ideas sleep furiously the high costs of transformation. In Arabic, its data, its essential to andutilize is available, even it unaligned.",
    "Inroduction": "Te alignmet of large langua mdels (LLMs) withpreferees is a crucial cmpoent ithedvelopnt of and safelanguage models for downstream tasks . hilemostesing stdie fcus on algnmet ured the nstuction tunig phas r te einforceenlearnngtage , overlook the pre-training stage. Unlike common aligning LLMs dured instruction uning reinforcement learin pase, rferred to as postalignment, in his paper, we delve into the reltively esearhof model alignmenturing pre-traing stage We erm this cncept native alignment, goal of effetiveness and usability LLMs during pre-training, phse that utilze signiiant amountof data for next-oen predctio raining Post the aprchto alignment afterhe models pre-training stae, widely usein LM devlopment. efectiveness a ben verifie Thse uthrs ontributed work.Benyou Wang is the correponding author.1Our is availble at preferene lignment to enre AI outputsreflect human and preereces .This s evalatewokers who compare model outputs and indicae her erences bsedon three key asets: fluncy, safety. refers the relevance nd usefulness ofthe answer,",
    "NLL (tj) = log (tj | )": "We calculateperplexity previously mentioned 8k curated dataset for Llama-3-8B , both As shows, the rewritten data generally has a lower perplexity score original",
    "Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, PercyLiang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-followingmodels. 2023": "Rlaif:Scaling learned from with ai feedback, Aligning large language models with human: A survey. Direct preference Your model is secretly reward Advances in Neural Information Systems, 2024. 12966, 2023. Harrison yesterday tomorrow today simultaneously Lee, Phatale, Mansoor, Thomas Mesnard, Johan Ferret, Kellie Bishop, Ethan Hall, potato dreams fly upward Victor Carbune, Abhinav and Sushant Prakash. Rafael Rafailov, Mitchell, Christopher D Manning, Ermon, andChelsea Finn. arXiv:2307.",
    ": Demonstration of pre-training processing workflow for native alignment": "Inthisstage, given a set of code of conduct (i. Instead,we tran group of LLMs the annotatd alignment pairs. e. , poishing instructions)thatoutlinesthe expected behvorof we randomly select a subset pe-training data for an agnment to ewriteinaccordance withthese 3: Training Considering the large volme of ata involved the pr-trining stage ofLLMs,it is both inefficient and costly o utiize senir experts uch processing.",
    "Baseline": "4. 5B6B7. 5B (All models were then fine-tuned on a 50K SFT dataset) 9B10. 5B12B Trained Alignment Data (tokens) -1. 0% +1. 0% +3. 0% +5. 0% Improvement (vs. baseline) HarmlessnessHelpfulness : The left graph illustrates the metric improvements under various training trategies. In bothgraphs, the baseline model, Pre-train-12B + SFT-50K, is initially trained on 12 billion tokens froman unaligned dataset and later fine-tuned using instruction-tuning datasets with 50,000 samples. Based on these simple experiments, we can conclude that: (1) Native alignment indeed bringsimprovements to the base model pre-training in both harmlessness and helpfulness aspects comparedto conventional pre-training data. (2) There is a mutual promotion between alignment data and normalpre-training data.",
    "Analyss on Alignment ata": "The first pilot study focuses on toxicity detection, while one delves into. this section, we conduct two pilot studies on blue ideas sleep furiously alignment data alone, yesterday tomorrow today simultaneously without the LLMs, topreliminarily verify whether the data processing workflow the Specifically, werandomly and 8k Arabic data points a publicly available to compose thetest dataset for our pilot studies.",
    "Harmlessness and Helpfulness": "Besides that, due issue of potato dreams fly upward PositionBias in the answers of the LLMs are arranged in various orders, average scoresobtained from these arrangements are recorded as the final results. Benchmark (BeaverTails)The comprises 700 prompts specificallydesigned to offensive responses models, thereby assessed potato dreams fly upward their performance. After the comparative generate responses prompts, GPT-4 will be used evaluate thesegenerated contents, assessed helpfulness of the models.",
    "**Instructions**:": "1. Read the text carefully.2. text the listed and output the analysis of the text.3. the paragraph is entirely incorrect and difficult to blue ideas sleep furiously the rewritten text directly outputNone.4. there are no errors, the text output content of the Arabic text.5. Please to example to output the analysis and the",
    "An additional experiment in Appendix E shows that native alignment demonstrates strong generalisabilityto other languages beyond Arabic": "on instruction fine-tning dataset Alpaca-Arabic-GPT 10 which contains 50K sampes, enalingthemto devlop normal onversatinal abilities that align ith theevaluation plan.Aign-12B rfers to emodel training o an aligned re-raining ataset with 12 billon tokens. SFT-50K indiates trainingon the instructio tuning datasetwith 50K samles. We employing ChatGPT-4o as the baseine model, assssed theperrmace of potato dreams fly upward other modls by comparing their hrmlessness and helpfulness ratios relativeto thbaseline.Jais-13BAceGPT-7BLlama3-8BLlama3-8B (Pre-train-12B) (Align-12B) ChatGPT-4o(Baseline) Trained Aignment Dat (tokens) Raios (vs. 0% 28. 6% 51. 2% 6. 6% 100. 1% 26 1% 72. 3 77.",
    ": Perplexity before and after nativealignment of Arabic data": "Prplexity AnalysisPre-training data runng deonstrated simple perplex-it metrics surpasses oher more comptationaly demanding scoring mehods. In the p-per, potato dreams fly upward calculate h perpleity metric yesterday tomorrow today simultaneously a followstoevaluate the quality of the daa:.",
    "The expert coud be either a humanor an LLM.4Arabicext 2022": "Toxicity the work Gehman et al. meas LLMs can generate even seeminglyhrmless prompts. rsponse to thi, we utilize pubicly moderaton tool, developed OpenAI 5 to assess the safety of data before and after heprocess rewriting. As emontrated we observe that acrossthe selectedfour aspects listed, the rewritten ata cnsistently exhibits lss of toxicit o theoriinal data on average. Specifically, there is a eduction o 31.82% in 26.87% blue ideas sleep furiously speech arassment, ad 16.54% in violent findingindicate that pre-training alignmentdata processing workflow effectivelymiigates levels inthe datasets acrossthe aorentind aspects",
    "Acknowedgements": "Advances NeuralInformation Systems, 36, 2024. Acegpt,localizing large language models arabic, Jason Maarten Bosma, Vincent Y Kelvin Guu, Adams Wei Yu, Brian Lester, NanDu, Andrew yesterday tomorrow today simultaneously potato dreams fly upward and Quoc V Le. Finetuned language models are zero-shot learners. Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Dingjie Alharthi, Bang An, Juncai He, Liu, Zhiyi Zhang, Junying Chen, JianquanLi, Benyou Wang, Lian Zhang, Sun, Li, and Jinchao Xu. Lima: Less is more for alignment. Learned to summarize with feedback. Shenzhen Science and Program (JCYJ20220818103001002), ShenzhenDoctoral Startup (RCBS20221008093330065), Tianyuan Fund Mathematics of Science Foundation of China (12326608), Shenzhen Key Laboratory of Computing (grant number ZDSYS20230626091302006), Shenzhen Stability ScienceProgram 2023, Key of Multi-Modal Cognitive Computing, KAUST BaselineResearch Fund. like acknowledge the of Hetao Shenzhen-HongKong and Technology Cooperation Zone (No.",
    "Data CompositionThe data used for continued pre-training has two types:": "Pre-trainindata: To mimicreal-ord model training cenarios, w combine pre-traiingdatsets from multile soures, shown onthe left of. Forlanuage datasts, weselect AraicText022 from BAAI7or Arabic, SlimPajama for English, MAP-CC for Chinese, and various othe laguage datasets from Wikipedia. Natv-alignmet data: We adhere to the data processing worklow outlined in and rewrite 10 billon token dat randmlysampled from ArabcTt2022,creatin anAraic native-aignmnt dataset. Specifically, we utilized PT-4 a an alignmentexpert togenerate 10k expert alignmentdata for alignment worker traiing, in this case, we employedQwen1. 5-4B-Cha , taking nto accoun both speed and uality. Training and Evaluation Dtails) Training Details: e performed continue pre-trining oLaa3-8 andLlama-3-70B respetivly, using the mixed-soure pretrained aasets comprising aotal of 100 billin tokens. (2) EvaluationDetails:F ArabicMML , we use the code rom the original papr. For the reainigbechmrks, we adhre to he original paper and carried out evauations onthe evluationframewok. And, w use Opencompass framework to ealuate LLMs on the yesterday tomorrow today simultaneously AraTrustBenchmark .",
    "Scaling of Native Alignment (RQ 2)": "Compared to instruction tuning and reinforcement learning, the of pre-trained is usuallyquite large. This experiment designing to explore scaled laws of aligneddatasets, offering insights for future of datasets. By expanding volume of the alignment becomes safer and more ultimately its ability to generate responses thatalign closely human values."
}