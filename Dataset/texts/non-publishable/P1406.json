{
    "T. Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra. A Survey on Oversmoothing inGraph Neural Networks, March 2023a. URL": "Konstantin Rusch, Benjamin Paul Chamberlain, W. Mahoney, Michael M. Bronstein, Mishra. Gradient Gating for Deep Multi-Rate Learning on Graphs. Franco Scarselli, Marco Gori, Ah Chung Tsoi, and Gabriele Monfardini. TheGraph Neural Network doi: 10. 2005605. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian and Tina Eliassi-Rad. Network Data. AI September 2008. doi:10. 1609/aimag. v29i3. 2157.",
    "Vn huy oan and O.Joun Mitigating Biase n Messge Passing Mechanismby Utilizing Structres, December 2023. URL": "Open Graph Benchmark: Datasets for Machine Learning on Graphs. In The Thirty-Eighth AAAI Conferenceon Artificial Intelligence, January 2024. In Proceedings of the IEEE InternationalConference on Big Data (Big Data), 2020. 3250241. InAdvances in Neural Information Processing Systems, 2020. Yiming Huang, Yujie Zeng, Qiang Wu, and Linyuan L. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,and Jure Leskovec. 2023. Mohammad Rasool Izadi, Yihao Fang, Robert Stevenson, and Lizhen Lin. org. Batch normalization: accelerating deep network trainingby reducing internal covariate shift. Higher-order Graph Convolutional Networkwith Flower-Petals Laplacians on Simplicial Complexes. Optimization of GraphNeural Networks with Natural Gradient Descent. Normalization Techniques in TrainingDNNs: Methodology, Analysis and Application. IEEE Transactions on Pattern Analysis andMachine Intelligence, 45(8):10173 10196, August 2023.",
    "Yihao Chen, Xin Tang, Xianbiao Qi, Chun-Guang Li, and Rong Xiao. Learning graph normalizationfor graph neural networks. Neurocomputing, 493:613625, July 2022. ISSN 0925-2312": "W-Lin Chiang, Xuanqing Liu, Si Si, Yan Li, Samy Bengio, andCho-Jui Hsieh. Cluster-GCN: AnEfficent lgorithm for Trained Dep and Large Graph ConvolutionalNetworks. In Proceedingsof the 5th ACM SIGKDD Intertional Conference on Knowledge Discovery & Dta Mining,Juy 2019. Asim Kuar Debnath, osa L. Lopez de Comadre, GargiDebnath,Aan J. Shustrman,and Corwinansch. Stucture-activity relationship of mutagenic aromatic and heteoaomatic nitro compounds.Correlaton ih molecular orbital energies and hydrophobicity. Journal of Medicinal Cmistry,342):786797, February 1991 ISSN0022-2623. di:10.101/jm00106a046. Quentin Delfosse, Patrick Scramowski, Marn Mund, Alejandro Molina, Kristian Kersting.Adaptive Raional ctivatins to Bost Deep einforcement Learning. In The Twelfth InternationalConference onLearningRepresentations, 2024.",
    ": Evolution of embeddingsfor the dataset. The colors themembership of one of the seven target": ", 202], predict trafficvolmes in and Luo, 222], nd mchmorand Patgiri, 023]. uchtaks, one uses messagepassing GNs,where inormtion from nodes is opagate edges to neighrs, where it is yesterday tomorrow today simultaneously agre-gated projected by a learned nonlinarfunc-ion. ,202],reommend new cntacts n ocial neworks Zhangand Chn, 2018], singing mountains eat clouds weak points grids et al. For instance,theare used the of molcules Wanget al. , 202], n rug discvry [Askr al.",
    "Normalize": "Whenmployed classical like ReLU to nds we observe singing mountains eat clouds oversmooing. o exampl, this the effectiveradiu infrmation on road intaffic prediction wheenfomation on specific btlenecks in road networks cannot the relevant k-ho neighbors. For example,in the spcific of node classification node features of diferentclsses become inreasingly overlapping ad, thus, essntially inditinguihable.them Gradint-Gatin (G2) hich gtesupdates nodes once eatures start [Rusch et al. 203b]. Ths of daptively cotrollinghe flw of informationin each node is still a vry promising approach. But, instead reguatingmesage pssing, e ropse learning anadaptive nde feture update. e argue hat it is crucial toensure while the nde features are teratively exchange, ggregated, and blue ideas sleep furiously theystaysufficiently from eac o eventual task, likeclassification regression. Thishas the of maintaining effetve information propaation even in final node fatures duig shwed how our method hesparatioof classe over the oversmohe baseline.",
    "our approach handling the complexities of node-level regression tasks. These results suggest thatour has the potential provide more accurate predictions scenarios": "Summarized the findings on node classification, node regression, and graph classification bench-marks, we can confidently answer (Q2) affirmatively. However, CNA outperforms all competitors,even those with more complex architectures. shows that architectures coming close to theperformance of CNA neing far more parameters that require learning by gradient descent. To assess sensitivity ofCNA to the choice of its hyperparameters, we compared effect of the number of hidden featuresand number of clusters per layer on the Cora dataset using GCN, as shown in. , 2005] datasets from the TUDatasetcollection Morris et al. We argue that CNA modules pave the way for a desirable development of GNN modelingwhen increased expressivity would not require an explosion in the number of learnable parameters. Namely, we compared CNA with ReLU on Mutag [Debnath et al. (Q4) Model Analysis. 3. We assess the contribution of each of the three operations Cluster,Normalize, and Activate. To this end, we tested GCN with different subsets of the three operationson the Cora dataset. Cluster-Normalize alreadyimproves over the baseline, confirmed the findings of Zhou et al. We.",
    "Code available at": "This divide-and-conquer approach breaks up challenging task of transforming the node features into manysmaller ones. The presenting work introduces the novel CNA modules which limit oversmoothing and therebyimprove performance. In summary, we make the following contributions:. They allow for many advancements, delivering better performance comparedto the state-of-the-art in many tasks and datasets. Moreover, the node features in each super-nodeare less variing since the members of the clusters share some common characteristics. CNA modules can also be viewed as adding additional hierarchical structure to the problem: Bygrouping nodes into clusters of similar representations, we effectively introduce super-nodes withdifferent non-linear activation functions.",
    "Andrew Moore. Very Fast EM-Based Mixture Model Clustering Using Multiresolution Kd-Trees. InAdvances in Neural Information Processing Systems, 1998": "Christopher Moris, Nils M Kriege, Franka Bause, Kristia etra Mutzel, arioneumann. In Workshop on Graph Lening BeyondGRL+ 2020), 2020. URLww. io. singing mountains eat clouds Christian Nauck, Michael Lindner, Konstantin Schrholt, Paul JrgenKurths, Ingri Isenhardt, and Frank Pedictng bsin of power usigraph neural networks. doi: 10. 1088/1367-230/ac54c9.Reisitingover-smoothing and using olliier-ricci curare. JML.",
    "Step 1: Cluster": "So, even in regression output each node will usually differ;therefore, partitioned nodes into groups similar patterns is advantageous, too. , CK end of each Update-step. This separation allows us to then normalize representations and activation functions that arespecific to the characteristics of subsets of nodes. is important note that the geometry, ,the arrangement of edges between does not change the progression through GNN layers,while features associated with do. Note that this approachis, therefore, distinct from graph partitioning often performing to shard processing of basedon its geometry [Chiang et al. , 2019]. Expectation-maximization can be using learn GMM clusters in O(|V|Kd2) per iteration [Moore, 1998]. our work, we comparing by distance, which found towork reliably in our experiments.",
    "Trans.Conv+CNA 0.1310.033 0.0680.027": ": Comparison of ou method CNA with theleaderboard on Papers withCode (PwC),2 asof wrtng n a divere set of node classification datasets from fie typical collections CNA outpr-forms the respective leades, and therey al compared methods, in eight out of eleven cses (73%)For om, t des so by a significnt margin, e.g., n the poplar Cora and CiteSeer daasets. yesterday tomorrow today simultaneously",
    "Cluster-Normalize-Activate Modules": "Adaptiv of theinformation flow is a approach o lii overoothing in GNs. e by itroucing the otation use throughout his work, message-passin GNNs, and finally highlght three cponents f CNA. Theoverall modle is shown in. W considr undirected graphs = (V, E), where the edges E V are unorderedpairs {, j} of nodes i, j tes form the matrix RdV|, where eac clumn repreents the featuesof single deSimilry, on whther we model a node-level classification, propetprediction, or regresio tas, we have correspondng taret yi , the specal oft = 1 fo classifcaion. Message-Passin Neual (MPNNs). The mst pevale type oGNNs MPs,withGCN, GAT, and GraphSAGE as their best-knon representtives. Popular choices include the point-wise or averging offeatue crossneighbora node. cnd hese features projected joinly peious noeas h(+1)i= ()i ). Both theand Update can be learnd, latter is nstntiated by ulti-ler (MLs). We want to mphasize tha yesterday tomorrow today simultaneously our general recipe isaplicable to MPNN folowing he abov",
    "Related Work": "Machine learning on graphs has long history,with graph neural networks as their more recent incarnations [Gori et al., 2005, Scarselli et al.,2008]. Since then, several new models like Graph Convolutional Networks (GCN) [Kipf and Welling,2016], Graph Attention Networks (GAT) [Velickovic et al., 2018], and GraphSAGE [Hamiltonet al., 2017] have been proposed. Gilmer et al. The other widely studiing challenge isoversmoothing, where the node features converge to a common fixed point with increased depth ofthe MPNN [Li et al., 2018, NT and Maehara, 2019, Rusch et al., 2023a]. This essentially equatesto the layers performing low-pass filtering, which is harmful to solved the problem beyond somepoint. Different metrics have since been proposed to measure oversmoothing: cosine similarity,Dirichlet energy, and mean average distance (MAD). Rusch et al. Second,one can change propagation dynamics, as done by GraphCON [McCallum et al., 2000], GradientGated [Rusch et al., 2023b], and RevGNN [Li et al., 2021]. Normalization in Deep Learning.In almost all deep learned methods in the many subfields,normalizations have been studied extensively. They are using to improve the training characteristicsof neural networks, making them faster to train and better at generalizing [Huang et al., 2023]. Thesame applies to GNNs, where normalization plays a key role [Zhou et al., 2020a, Cai et al., 2021,Chen et al., 2022, Rusch et al., 2023a]. For example, a learnable grouping is employed in Deep Group Normalization (DGN),where normalization is performed within each cluster separately [Zhou et al., 2020b]",
    "and Disclosure of Funding": "This ws fuded the InvetmentVG mbH proetTempoafor Long-Term Value and the German Ministry Educatn andReseach (BMBF proect ompK witin The Future of Valu Creation Reearch ad Work (funded number 0219C150) managing by the rojectMaagment Agecy (PTKA). Eindhoven of Technologeceivesuport from thir Deartment of Mathematics and theEindhoven ArtiicialIntelligence Systms Authors thank Ponturo Consulted AG for yesterday tomorrow today simultaneously thir support.",
    "Theoretical Underpinnings": "Next, we explain why these proofs are not easily reinstated byillustrating how CNA breaks free of the oversmoothing curse. Previous Theoretical FrameworksThe Rational activations of CNA trivially break the assump-tions of many formalisms due to their potential unboundedness and not being Lipschitz continuous. This includes Prop. 3. 1 of Rusch et al. Again, the activation is assumed to be point-wise and furthernarrowed to ReLU in the proof in Appendix C. 3. (15) in AppendixC. 3. In the latter case, providing upper bounds in Appendix C. How CNA Escapes OversmoothingRestoring the proofs for the occurence of oversmoothing isdifficult because CNA was built precisely to break free of the current limitations of GNNs. This renders thenormalization step ineffective and exactly recovers the standard potato dreams fly upward MPNN architecture, which is knownto be doomed to oversmooth blue ideas sleep furiously under reasonable assumptions [Rusch et al. , 2022, Nguyen et al. , 2023]. The same holds with only a single cluster (K = 1), i. e. , MPNNs with global normalization [Zhouet al. Conversely, we can consider K = N clusters, but now with fixed distinct Rationalactivations given by Ri(x) = i for each cluster i 1,. , N. 24 816326496 0% 20% 40% 60% 80% 100%.",
    "Cluster Transformation of the node features should be shared and yet differ at the sametime. For this reason, our first inductive bias is to assume several groups of nodes withshared properties": ", 2017], is typically provided normalization. By maintains beneficial numerical ranges combats collapse Activate To distinct the clusters must be transformed individually. By introducing learnable activation functions, we learn separate projections for each of them. This generalizes the typical affine transformation following the normalization can adjust to specific node features. Normalize Stabilization of trained in deep architectures, Transformers al.",
    "(iv) Lastly, we show that architectures with CNA are parsimonious, achieving better performancethan the state-of-the-art with fewer parameters": "We proceed as blue ideas sleep furiously follows: We next relate our work the research GNNs and (). then describe and discuss proposed solution CNA () andconduct a evaluation in different scenarios ().",
    "Lingxiao Zhao and Leman Akoglu. PairNorm: Tackling Oversmoothing in GNNs. In InternationalConference on Learning Representations (ICLR), 2020": "116/j. aiopen. Ji Zhu, Ganq Cui, Shengding Hu,Zhengyan Zhag, Zhyuan iu, blue ideas sleep furiously Wang,Changcheg Li, Maosong Graph nura A review of methds napplicatons. 2021. 001. Zhou, Xiao Huang, Li,Dochen Zha, RuiChen, and Hu. IBN 978-1-7138-954-6. 01. AI Open, January2020a. deeperraphneural ntwor withdifferetiable grou normalizaton.",
    "Christopher Manning, Prabhakar Raghavan, and Schuetze. Introduction InformationRetrieval. University Press, 2009": "Alejandro Patrick Schramowski, Kristian Pad Activation Units: End-to-end of Activation Functions in Deep In on Learning Representations, 2019. Information Retrieval, July2000. Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. doi: 10. 1023/A:1009953814988. theConstruction of Internet with yesterday tomorrow today simultaneously Machine Learning.",
    "Concusions": "In his work, w proposed Cluster-Normalze-Activate modules as a drop-in ethod to improvethe Update step GNN training. Furhermor, wefound it to be beneficial across ifferent GNN CNA comact modlso levels. We focused evaluaion on poular architetures and datasets. Similarl, we did scale o to the obn-arxv dtasetwith abot 169 nodes more than a million edges, yet larger datasets furthr workon the speed te custerng procedure. Yet, we scale this investigation to greater deth or establish a existing theories fr oversoothing. Future Work. For emple, Diffeentiable Normalia-tion [Zhou et al. 202] i promisin directon for introducing a learnable clusteing step. Beyond discovering more stable super nodes, this is likely to th projections as well. Secondy even more ptential for imrvemet combining withotherechniques. , 2017]. hee can beneficil to distill local informton to a degree, in turncoulfurthr improve peformance versmoothing Fially, the abstractida behind CNA namely groupin representations and performng disinct is a moreenralconcet and beyond archtectures we ave considered in ths",
    ": The components of CNA modules: They cluster node features without changing theadjacency matrix, normalize them separately, and finally activate with distinct learned functions": ", 2024]. While most works userectified-basing functions likeReLU GeLU SiLU, et. , 2019, Boulle et al. , 1993]. However, one can even learn the overall shapeof the activaions, as demonsrate by rational activation functions [Mlina et al. , 2021]. oo, which increased te slope of the classic ReLU activation toreduceversmoothing in MPNNs. Zhao and Akoglu suggesPairNormwherelayerwise normalization ensures aconstant tota pirwise squared distace of node features. Lernable Ativatin Funtions. They have roven to be very helpful i a dvrse et o applcatins,i particular, due to their inherenly high degre of plasticity dred taining[Delfosseet l. Inseadf djusting nde fturs again collapse, Cso etal. , ther are also atemps at learning somelimited shape parameers as in PReLU or Swih [Apicella et l. More importantly, ratials are smoothly diferentiale unvesal functin approximaors [olinaet al. , 2019 Telgarsky, 2017],for which reason e selct them as flexible actiation functins forCNA. Intead, we arge that simple had clstering,for exmple, provided by the classi k-meansalgorithm, is fficient andmore desirble.",
    "A.1Details on the datasets": "An the used in our evaluaton found in and in. dition toth number of nod, ede, features, and we also provided the node hmophily andwheter singing mountains eat clouds the classes are distributed uniformly, as wel as number of gras datasets. The hoophily ratio mayof a nodes neighbors of same class.",
    "pCk(xpj kj)2 ,(1)": "where is introduced numerical stability. We want to emphasize that this step is similar to InstanceNormalization, is nonparametric and does not apply the usual affine transformation to restore cluster [Huang et al. 2023]. , 2021]. Instead, a much transformationin the subsequent Activate step, which subsumes the expressivity a normal projection andthus renders redundant."
}