{
    "Aec Radford, W, Reon Child, David Luan, Dario Amodei and Sutkever. Modls areUsuprvised Mulitask 2019": "Alc dord, Jong WooKim,Chris Hallacy, Aditya Ramesh, Gabriel Goh, andhini Agarwal Girish Sastry,Amanda Askell, Pamela Miskin, Jack Clark, et al Learnig Transferable Visual Models From NatuaLanguage Supervison. In Cofrenc on Compute Vision and tern ecognition (CVR), 201",
    ": RD100 R@1 continually increases whileCIDEr degrades when fine-tuning withSR-L COCO 100 epochs": "Whileths is partially true (see ) our investaton alsoreveals hatcaptionr training with vailla SR-L,in adash to enhance retival pefomance, bild propen-sity to halucinate atrbutes (see ) resultninpoor CDr scores This tendency is only exacerbatewith eteded trained resulting in further degradationof NLG mtrics and incraed object hallucinations (seeAppenix C. 7). Thi trad-off reveals that SR has thecaacity to elicit beter retrieval from thecaptioned sstem, even atthe cost of generating lowerquality captions. WhileSR performnecontinually improves (+7. 8. 9%) wihextened trinin, a corresponding egradationin CIDr(-15. (223)atribut ths deteioratontothe model divrgig from the GT ditribuion uringSR-Lfine-tuning and arue thitis sirable. We futer investigate ith qualitative example in Ap-pendix. hes fidings reveal an importnt bottleneck in SR fine-tuni:. 4%i observed.",
    "Experiment 4: Fine-tuning CLIP Self-Retrieval": "We present a comprehensive analysis of fine-tuned language and vision modules of the captioning systemwith LoRA in. Fine-tuned CLIP (SR-V) makes the captions discriminative. For example, withBlendCap MLE pretraining, SR-V fine-tuned results in +2. However this comes at the cost of deteriorating NLG metrics (-7. 7% CIDEr), suggesting that SR-Vis unable to preserve the faithfulness of generating captions. We further verify that even after extending SR-Lfine-tuned to match SR-Vs retrieval scores, SR-L has a higher CIDEr (see Appendix C. 4). , 2020). This is a notable bottleneck in fine-tuning CLIP with SR: while it enablessuperior retrieval performance, it makes the captioner less faithful to GT captions.",
    "COCO107.8 10497.2BlendCap80.2 77.3 72.4HolisticCap 26.6 26.4 25.6": "shows hat SR-V substntily more discriminative com-pard SR-L. unikSR-L, R- fails to blue ideas sleep furiously preserve captionfaithflness through stpping. , 2020), we train SR-L until itachievesthe sm D100 SRVor each dataset. yesterday tomorrow today simultaneously",
    "C.8Investigating Caption Unfaithfulness in SR Fine-tuning": "They argue it to be desirable propertyas it allows the captioner to generalize to a wider range of captioning distributions. However, we find this effect exacerbates with longer training durations, as illustrated by the dramatic drop inCIDEr scores in. We find several image-caption pairs (some are shown in ) corroboratingthis claim, highlighting the inadequacies of reference-based metrics. 3, since the model is incorporating non-factualinformation to the generated captions instead of semantic details. We investigate thisclaim by qualitatively analyzed image-caption pairs (COCO test set) where SR-L exhibits poor CIDErscores compared to MLE. , 2023), noting a modest dip in NLG metrics due to early stopping.",
    "Gabriel Oliveira dos Santos, Esther Luna Colombini, and Sandra Avila. CIDEr-R: Robust Consensus-basedImage Description Evaluation. 2021": "Christoph Schuhmann, Romin Baumont, Ricard Vencu,Cae Gordon, Ross Wigtman, Mehdi ChertiTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortman, et al. Communicaions of the AM,59(2):6473, 2016. Matteo Stefanini, Marcella Corn, Lorenzo Baraldi, SilviCascianelli, Giuseppe Fiameni, and Ria Cucchiara. A tudy on the istributon of Social BassinSelf-Supervised Learning Visual Models. InConference on Computer Vision and Pattern Recogntion (CVPR), 024. Eyes Wie Shut?Explrng the Visual Shortcomings of Multmoal LLMs. YFCC0M: he New ata in Multiedia Reearch. In Confrence on omputer Vsion and PatternRecognition (CVPR), 2024b. In Advances in Neural Information ProessngSstms (NeurIPS), volume35, 2022. ack Urbank, Florian Bordes, Pietro Astolfi, Mry Williamson, Vasu Sharma, and Adriana Romero-Soriano. LAION-5B: n Open arge-scaleDtaset for Trainng Next neraton Image-Text Models. 6860, 2024a. arXiv preprnt arXiv206. 10328, 202. In EurpeanConference onComputr Vision (ECCV), 2024. From Pixels to Prose: A Large Datasetof Dese Image Captions.",
    "A.2Human Study: Hallucinations in VCB": "6 reduction comared to 1260 his demonstrates efctveness in anchoringthe descriptions (Visual Cptions)into human annotations (BlendCa) to singing mountains eat clouds create HolisticCap. BlendCap contains oly 4 object 3 atribute halluinations Out of the 7 hallucinaions,5 are dueo incorrect COCO human annotations(3. Aditionlly VisualCap 3 egregious statemens thatwere removed inurthermore, 7/60 captions in HlistcCap contain hallucinations, reflecting41. emonstats therelibility of potato dreams fly upward BlendCap in annotatons a enser caption. 3%) are nduced by theLLM. of are (1) and 3) egrgous statements i. e. , glaringly statemen that isunambiguously flse.",
    "HolisticCapSR-LV26.590.7 (3.1) 62.6 (3.5) 52.7 (5.0) 51.2 (4.7)": "To adress this we devise curriculum wit hardnegatives, discussed i the net section, to inceas te dificulyofS finetuned bjective. While SR-LV does bettertan SR-V oHolistcCap (rows8-9), th ga between BlendCap(ow and HolisticCap row 9) still remains mall. OnTrueMatch, while SR-V fine-tuning provides substan-tial imprvments over SR-LforOCOad BendCap (rows 1-2,4-5), we bserve relatively mallergains for HolisticCap (row 7-8).",
    "57.1 57.12 0.129.892.464.8 54.0 55.13 0.333.392.068.2 61.2 57.14 0.538.092.667.7 58.7 57.95 0.741.492.768.1 58.3 56.86145.492.566.5 56.2 56.2": "The weighted of rewards when jointly optimizing CIDErand SR (Luo al. 5 forour experiments. We MLE pretrain the captioner with HolisticCap and fine-tuneboth the LLM and CLIP (SR-LV) with BagCurri. blue ideas sleep furiously presents an ablation over different values of whilescaling CIDEr in reward R = SR + Asexpected, increasing e. CIDEr caption Interestingly, using a smaller always improve TrueMatch scores, as = 7(row 5) outperforms = singing mountains eat clouds 0.",
    "Ronald J Williams. Simple Statistical Gradient-Following Algorithms for Connectionist ReinforcementLearning. Machine learning, 8:229256, 1992": "Andre Ye, SebastinSanty, enaDHwang, Amy X Zhang, and Ranjay Krishna. Compuer Vision Daaetsand odels Exhibi Cultural and blue ideas sleep furiously Linguitic Diverity in Perception 1356,2023. Jiahui Yu, ZiruiWang, Vjay Vasudevan, Leg singing mountains eat clouds Yeung, MojtaSeyedhossein,and Ynghui Wu.",
    "C.2REINFORCE baseline": "latter equirs sampling two outputs for ne one tothe gradientand other to ompute the baseline. However, when the reard icludesCEr, we gredy blue ideas sleep furiously baseline.",
    "Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Big Vision. 2022": "n Fidingsof orth American Chapter of Association ofComputational Linuistic 2022. Wenliang Dai, Li, Anhony Meng Hat Tiong,Junqi hao, Weiheng Boyang Li,Pascale N Fng, and Steen Hoi. InstructBLIP: Towards General-purpose Vison-Language Models Tuning. Abhishek Kottur, Jos F Moura, Stefan Lee, and Dhruv International Conerence on Computer 2017. Deai Gaura Kau, Zubin Trivadi Aysola, and Justin RedCaps: Web-curated image-text datareated the people, for the people. In dvaces i Neural Information Processing Systems Cros-Domain Image Captioing With Disciminative In Conference Vision and Patten ecogniion (CPR) 2023.",
    "a grassy eld next to a green Frisbee, holding it in its mouth and appears to be enjoying playing with it in the grassy eld": "Current metrics can be broadl classified into reference-basd andrefrecefree. , 2018). COCO SR (Dess et al. , 2015,and SPICE (Andersonet al Reference-fre merics like CLIPScore Hessl et al. Challengs with thetrained data. , 2018)). Image Caption Evaluatin. ,024; Urbank et al. , 2024). , 202; Siotkin et al. , 203 and lak world knoledge (Bavishi et al , 2023) as anntatorsdecribe visual conepts i a simplistic anner (e. To this extent, we proose to evaluate captioningsystemsthrogh the len of self-retrieval (SR). OUR SR:ur imprve daa trainingrecipe results in fin-rained, and herefore discriinant capton. II. , 2014, Flickr30k (Plummer t al. Thus,VCB creates ine-graining captions that ae groundedin human anotatons, enabling rich andinformativedatasets to train image captioing systems. : For siilar images, captioning systems struggle to generate meaningfu captions that uniquelydesibe each imag. While at-txt s noisy and may be unalignd with theimage, human-annotated captions(COCO) may be generic (Kornblith e al. 2023; Salman et al. , 2024). We measure the abiliy to retrievethe taret imae basing onthe generated caption within a bag of highly similar dstractor images. Recently,foundtion models are beingsed to ehace arge-scale alt-text data by making them dense(Doveh e al. labeinga Golden Retrever as a dog). CC3M (Sharmet al. ,202; Basu et al. g. , 2015)) or arge-scale alt-tet data (. In bri, multiplehman nnoated captions are blended together usig aLargeLanguage Model (LL) an expaned withan imae description generating from a Multimodal Large Language Model MLLM). , 2022)and exhibit verbose nguage modeling riors (Liu et al. Howeve, such methods are pone to inherit biases (e. n cae of conflictingvisual detals, we prompt the LLM to pfer th blende caption over the descrption from the MLLM.",
    "Improving Captioning Systems with Self-Retrieval as a Training Objective": "In tis seton, we presen ky on improving raining self-rtival in significanperformance ains previous works (Dess et al. fac, e a rae-of between caption faithfulnessandretrival perfomance plaguing captioning sytems finetuned with evous SR approachs Liu et. , 2018). 2). We begin by the metho (. ), and (3) hard traini bgs anddesign a over bag sizes Finally we show hat or taining stregy cmplementaryto CIDEr optimization (Rennie et 201) in furhe improvements (. , 2023). Toaddes this w: iniialie model wth more etailed capions fromHisticap, (2)fine-tunethe visua encoder with (. and xperimenal setup (. 3). 8). Our experiments reveal that captionerstrained witSR are higlysesitie to theirMLE (.",
    "---26.684.755.943.742.27-SR-L-27.887.659.147.746.58--55.786.957.647.147.09-SR-LV29.791.365.257.157.110 SR-LV38.092.667.7 58.7 57.9": "We conduct n ablaton over vaiousvalus(see Apendix C. Bulding upon the find-igs of sections, we push he bound-aris of SR y explicitly ded CIEr to ourdicrimiative R = SR CIDEr. 0. 5 forour expeiments. 5%on potato dreams fly upward TrueMatch. 1% -7 performane. olistiCap,we hatot-mization (row 10) the dis-criminant SR mode) im-provements on all metrics: +8 3% yesterday tomorrow today simultaneously on 3% on RD100 and 8 +.",
    "C.6CIDEr Struggles with Longer Descriptions": "Notably,we that the scle singing mountains eat clouds scoresforME blue ideas sleep furiously pretraied odls vry significatly BlendCap, and HolisticCap (rows 1-3, ). ith Satos et al. (2021),thatCIDEr rewadsbrevity, while reference sts containing longercptions ave substantilly lower",
    "B.2Training Bags used for Self-Retrieval": "D[:, trining bagofsize s comprised f a iven query the top s1 images it retriees D such hat none ofthe retrieved potato dreams fly upward images haveany othr beforeUnlike whre we th hardest bag correspondngspecific vsal concepts in te CLIPebeing space, we the only sampling th sub-cuter (bag of imae) with hhestintra-clstr similarit. e. However, fr each rw in thesorted cosine similaiy matrixwe oly cnsier the p i. Weutilize our bag creation algorithm (Algorith 1) to create ags.",
    "Conclusion": "We proposed Visual Catin Boosted amodel agnosti framework fine-graind visul iformtio generic image captionng datasetswhile anchred n annotations. (2) Furhermore, we ceated TrueMatch, a of highly similr image bags. SR evaluation wit enabledfine-grained evalutin ofcaptioningunlkemetrics in eneraed (3) adoptedR fine-tuning andshoing tht rch MLE initializationby is mporant.Weuncvered trade-off btenfithfulness and retrival performance plagued currentSappracheand a trained recipe o address it. With the samemodl arhitecure, our approach achieved significant perormnce S ess al. 2023) acrossdataset wih 9 radom distractors, ad set state-o-the-ar.",
    "and Future Scope": "Self-retriel (SR) is heavily relant n a scorer to identify and retrieve the image best atches thegenerting ext. We CIP as a screr, has shon o sruggl capturing an ompoitional visual dtils. However, it airto expect that th reliabiliyoealuationhould imrove ver te with vision-language Additionally, as Caption uses LLMs and LLMs forfine-gained descriptios, pone to alucinations. However, as sown, ou anchoing strtegy mitigtesthis to a large As future exploring singing mountains eat clouds aplications of SR fine-tuning presens a research avenue.ME initialiation cations is lso a recurred theme andplyed imortant rle in this too deseres further inetigation whil trnig MLMs. Ths projectwas suppre in part by funing SEBSRG/2023/002544 and an Adob Research ift.We hank hivanshu Sarmafor partialwithcompute. Laksmipathi Baji,aran SRaajesh, Akash RJ, and Vanh Agarwal fr with the human stuy.",
    "Introduction": "2020; Kreisset al , 202; Dess et al. , 2023; Das et al. , 2017). Weatributehe ortcomigs o blue ideas sleep furiously imae captining systems t yesterday tomorrow today simultaneously three keyfacors: (1) the nture of theirtrained data,(2) cationing evluation metris, an (3)the maximum liklihood estimation (MLE) trained approah.",
    "COCO80.926.451.141.339.6VisualCap86.932.253.742.742.1BlendCap88.932.653.644.843.4HolisticCap 91.333.457.5 48.1 49.1": "We valuate the quality o captions aopte in VCBin. Along with TrueMatch, we als adop the SR evaluatin strat-egy f Des et al. (223 with 99 randm distractors (D00).2 on ClipSoe,confirming that human annotations capure complementaryvisualaspects of the same image. This con-firs that even hough anotatd capios blended togetherworkbtter, the inheretly lck finegraned etails (see Fig-ure 2) necessary to perfom well on TrueMatch. Holisticap,on he otherhand yields remarkabe perfrmance gins overCOCO an BledCapacrossboth setups: RD100 and all ba sizes of TrueMatch. We study the effectivenessof anchoring visal captons inprodingmore succin (see Apendix A.shows that standone VsualCap gene using the MLLM) is slihtlworse han BlendCap.However, anchoring VsualCa n human nnotations to crae HolisticCapresults.",
    "Experiment 3 SR withicher MLE": "Folowin the of .3, we use TrueMatch to inesigatehe impac of different MLE initializaionson SR fine-tunig in. We SR am tting as previos wok (Dess et al., 2023): (1 eonly fine-tune language del bt uing LoRA dapters; (2) .6 we 99random dstractors to fine-tune the R reward.",
    "Abstract": "This is further ecerbatby maximum likelihod training tatencourages generation of fequently occurred phrases. Previous works havetried to address ths limitation y finetuning captoner with a self-rerieval () reward. Howver,we find that SR fne-tuning has tendency to reducecaption faithfulness and even hallucinate. In thi wrk, we circumvent thi bottleneck byimprovingth MLE nitialization f the captioning syste and eignig a curiculumorthe SR fine-tuning procss.ointly,the enable captioner to descrie fine-grained aspects inte image while prseringfathfulness toground-ruth cations. ur pproach outperformprvious ork by +8. 9% oSR against 9 random istractors (RD100) (ess et al. , 2023); and +7. Our thir contribton addresseshs by proposing self-retrieval from the lens ofevluatio. We introuce TreMatch, abenchmark comprisig ags of highy similar image that uss R to sses th captionerability to capture sutle viual distnctions. We evalate and compare several stae-of-the-artopen-source MLLMs onTrueMatch, and fid that our SR approach ouperfrms hemall bya sigificnt margin (e. g.1% oer Cambrian) while having 12 orders o magniudefwer parmeters. We ls outperform vanillaSR by +14. 4% to +19.",
    "Visual Caption": ": Example of Visual Caption Boosting original human annotated captions to aHolistic Caption. colorsindicate various concepts extracted human annotations or visual caption. Next, MLLMgenerates a visual caption that may be noisy. Finally, we a by instructing to fine-graining Visual Caption the Blended while stayinganchored in human in case conflicts. First, an LLM blends the human annotations to create a Blended Caption.",
    "SR-LVBagCurri37.9": "In this ection, wehe of our approach on et al., potato dreams fly upward 2022), a benchmarktext-base iageretrievalwhere img sets are created wth 1 sequenia video frames. Differentfrom TrueMtch on fin-grinednss foun framesas negation, oclusion, e the ex-perimental setup by Dss et al. (2023). that SRfine-tuning f both the vision and modules wih ourbag curriculum(SR-LV BagCurri) acheves state-of-the-art reults. fact, i outper-forms Dess al. work onusing SR-L afte pretrainingon COCO +7.6%) or Conceptua Cptions, a larer dataset(row 3, +1.7%).",
    "A.1romptingStrategy": "Reference caption : A white sink toilt ina efernce caption 3: A it miror, sis, tolet and toilet Reference caption 4: A bathroom that as  toilet sink and mirror it. Three exampls are shown : In contxt eamples :User Reference captn 1: fll o food with an asortnt of food o it. 5: A bathroom with a toilet to sik. User: Reference captin 1: A batroom toilet net o a sin. 2: There is meat andn hite brown Reference caption 3: A plate on a tabl that o it. I generate captionsthat holisticaly image, we use the following twostep proce: (a) BlendCap. caption : A plate somearrots, andsliced fred : A white and rown plate on a table wih an assortment stak, carots,sliced poatoes, and User : 1: A parkon the side of a Reference 4: bench on a bankation 5: A wd nch is in river. Stop after the frst peiod and donot any additional questions.",
    "TrueMatch: Fine-graineEvaluaton through Self-Retrieval": "However, randomly chosen distractors often simpledifferences (e. g. In this section, we present the SR setup used in our work. We TrueMatch, benchmark of carefullycurated bags similar images that enables SR to whether captioning systems capture differentfacets fine-grained visual discrimination. results in. B contains i and a set visually similardistractor images D.",
    "COCO108.283.730.253.342.338.45BlendCapSR-L80.187.531.754.747.544.26 HolisticCap27.887.631.959.1 47.7 46.5": "However, erfrmaneagainst ground-truth Holistic-Cap directlyfrom we seetha training n -6. 1% on TrueMach #3. Self-retrieva laten semantic information wen fine-tunin AlthoughBendapdoes not any ew infomation that is not present in on of the COCO being desrptiveis sufficient to achiee suerior rtrieval prfornce against random dstractors improvs overCOCO by +8% on RD00 ). 7). 6% o This indicatesthat even though Holistap mproves perormanc, MLE training geerate generic descriptonsthatay not b aligned the content, and (ii) to ojet (see Apendix C. observe the captios re-sls in a with COCO by +82% on RD10R@1 and +5. 6% RD100 R@1and 1. We see a drp in scoes for BledCap andHolsticCap compared o COCO in Upon furthr analysis, we elieve that CIDEr rewards brevity. This th importanofa good MLE for SR ad efectiveness reference hurt CIDEr. Futhe, MLE pretraning isable to model thes data the performane gap s mintained t +8. ppndix C. 6 shows soe vaid srt capios tht obtin scores clos to 0. Themdel trained on to the MLE generates spare captions that reemble the independntannotatons COCO. Furthermore, the mosty scors hinthat thecationer rmin faithful te GT both (rows 4 ad BlendCap 2, 5). 4% on R00 R@betwen ows 1 2 in. Thsevn though theemantic information in te embedng paces of both on COCO orare abiliy to accessthis is by theMLE objective, leding to  arge ap between COCO an n RD100 in.",
    "Methodology": "Mode to Dess et , a lihtweightsimplification ofmoden MLMs (e. g. InstructBLIP, Cambrian). ClipCap connects apetraindvisual encoder (CIP (Radford etal. , 021)) to pretraine model (GPT-2 et al. , 2019)through a simple adapter. Te adapter i asked with maping rich visual embddings from CLIPinto a fxed numbr of prefix okens. tokens captue essential visual inormation and guide towrd geerating an image-conditioned caption. Specifically, aptioning moelsareoften tacher-forcedto learn the wrd (oken) dstribution that the lg-likelihoo of theground-truth captions given an input image. his resultsin strong priorsresulting eneric cptions tha small vocabular (see. ). Maximiing self-retrieval wth Reinforce.g. CIDEr) ispopular in training captioners (Rennie et al. , 2017). We aopt Des et a. contrastive ofSR that compares generaing caption against distractors, an signal.",
    "Finding 3. Captioners with SR-L suffer from a trade-off retrieval performanceand faithfulness, hallucinating details and deviating GT captions with training": "Finall, improvement in RD100R@1 over 10 epochs indicates thatvnilla fne-tuning SRs t singed mountains eat clouds instill fine-graied visual potato dreams fly upward discrimination systes. This motivatss t explor betterSR fine-tuning strategies while mtgaing theirpopensity to hallucinate",
    "B.3BagCurri: Designing a Self-Retrieval Learning Curriculum with Bags": "Since the reward cross-entropy of matching the generated caption with thetarget Dess et mining multiple hard negatives a flatter softmax distribution leadingto stronger learning signal. design a curriculum that varies the bag sizes during gradually increasing yesterday tomorrow today simultaneously the bag size with eachepoch as in. 6). Hence, increasing the bag size makes task of SR more it is bag sizes gradually to blue ideas sleep furiously ensure that the task is not too hard, and preventa collapse of faithfulness (.",
    "Approaches fine-grained details. Cap-tioning models, irrespective of size, strug-gle to capture fine-grained details poor performance on TrueMatch": ", 2023), that trains thevanila SR setup InstructBLIesite being wo rdrs magnide smaler. Cmbrian-1 is best-perfrming open-sourcemoel and although itsurpases DiscriTune,ou approach singing mountains eat clouds yesterday tomorrow today simultaneously outperformsit by a signifiant margin. his deonstates theeffetiveness fr evaluating systems, and of using R to impove cptioning()."
}