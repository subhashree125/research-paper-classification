{
    ". Hybrid Computing Augment": "ShiftAddAug goes further based on strong operators to augment weak operators. During calculation, only the first are used for the while the augmentedmodel employs all n channels. shown , ShiftAddAug uses [0, n) chan-nels part) n) channels (augmented part)for methods. original Conv) used as the Because the channels of Conv are the input ofeach convolution also widened and be conceptually.",
    "(5)": "Where Wg is the weight in original Conv that conforms tothe Gaussian distribution, and Wl is the weight obtained bymapping that conforms to the Laplace distribution. FC is afully connected layer, which is previously trained and frozenin augmented training. We need this because the weightsdont fit the distribution perfectly.",
    ". ShiftAddAug vs. SOTA Mult.-Free Models": "8%. 67% ad 1. 95% accurcy onCIAR100 wi 84. 7% enery Com-paring the shift quantization method improed accurac of 3. We compare ShiftAddAug ver OTA multiplication-freeodels, aually for tiny compuingdevice, on to evaluate its effectiveess. Fo DeepShift and dder-Net,our 0. 17% and 91. Wit ShiftAddAu, the ac-cuacy eceedexisting ork.",
    ". Neural Architecture Search": "SOTA multiplication-free at size, two-stage NAS is proposed. Based on idea of ShiftAddAug a multiplicative SuperNet and cuts deep fromit as depth-augmented NN. The should meet pre-set limitation. Moreover, the deep augmentation initially selected but graduallyphased out from the target network in training progresses. A block mutation training is also proposed, whichtends to gradually transform operators intomultiplication-free states training make trainingprocess more follow tinyNAS to build SuperNet cut SubNet. Then use evolutionary search to search subsequent steps.",
    "Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Dif-ferentiable architecture search. In International Conferenceon Learning Representations, 2019. 2": "classification over a large number of classes. In Indian Conference on Computer Vision, Graphics Processing, pages 722729, 2008. yesterday tomorrow today simultaneously 4.",
    "MULT.3.70.93.10.2ADD1.10.40.10.03SHIFT--0.130.024": "NAS remarkably suc-ceeding in creation NNarchitectures, accuracy while hardware like latencyand usage into the design process. ShiftAddNAS pioneering search spacethat includes both multiplicative and multiplication-free op-erators. Network Augmentation. they chose a that is the opposite of reg-ularization methods like Dropout: expand the modelwidth and let large lead the small model to achievebetter accuracy. Neural Architecture Search. extends utility exploring faster operatorimplementations and network foroptimization, bringing closer to hardwarerequirements.",
    "Loss becomes NaN when we use AddConv on bothdirect training and training": "ShiftAddug vs. Multiplictive and directly traine Baselne in rm of acuray nd tsks The accuracy on left isdirectl tained an the one on with augentation. for Mul.",
    "input": "Search process: two-stagesearch. Start multiplicationand convert the TargetNet to multiplication-free during training. Since many works haveverified the and add on proprietaryhardware, we follow energy and latency are measuring based on sim-ulator of Eyeriss-like hardware accelerator, whichcalculates not only but data movementenergy. and models are trained for 250 epochs and are trained for 300 epochs. (a). Find a SubNet depth augmented one then further cut out TargetNet on it for development. Hardware Performance. The initial learning rateis 0. ForShiftConv and ShiftLinear, the weight is quantized to 5bitPower-of-2 and activation is quantizing to calculated under 32bit. 05 gradually decreases 0 following cosineschedule. Width: use to widenthe MFConv channel; Augment Expand: increase expand of depthwise separable Augment Depth: blocksfor depth. SuperNetSubNetAugmented at Width & DepthGradually mutateTarget Net Evolution search a. Training The NNs are training with batch size128 using 2 GPUs. Only participate in will not be exported; Mutation: start with MConv, into duringtraining.",
    ". Limitation": "vs. Since we use additonal mtiplication r augmentaion, training consumes moreresources.",
    "Abstract": "devoid of multiplication, such as andAdd, have for compatibility ShiftAddAug usescostly multiplication to augment efficient less powerfulmultiplication-free improving any inference overhead. Ad-ditionally, a novel stage architecture search isused to obtain better augmentation effects for butstronger multiplication-free tiny neural networks. Remarkably, it securesup to a 4. 1.",
    "Linear": "split into the target part Xt and the augmenting part Xa, sodoes the output Yt, Ya. 5 -1. Duringtraining, wide weight is maintained, with the initial n channels processed in a multiplication-free manner and subsequent channelsutilizing multiplicative operations. For Conv, We use all inputX to get Ya through MConv. If bias is used, it will bepreferentially bounded to multiplication-free operators. 95%) than their directly trained counterparts. 20* 0. kernelaugmented * 0. Here three types of operators commonly using to build tinyNNs are discussed: Convolution (Conv), DepthWise Con-volution (DWConv), and Fully Connected (FC) layer. In ShiftAddAug, Xt and Yt mainlycarry information of MFConv, while XA and YA are ob-tained by original Conv. 5 * -0. weights of both are updated but only multiplication-free part is exported for deployment. 2 AddConv Mult. 30-1. 2 -0. 2 +1 +0. ShiftAddAug augments weak operators with stronger ones. Therefore, the important weights in the wide weight will be reordering into the multiplication-free part. 1 +1 -0. 4 Mult. Since the FC layer is only used as classificationhead, its output does not require augmentation. -free kerneltargetShiftConv -0. We dividethe input and use Linear and ShiftLinear to calculaterespectively, and add the results. But to get Yt, we still need tosplit the input and calculate it separately, and finally add theresults. Thehybrid computing augmentation for DWConv is the most in-tuitive: split the input into Xt and Xa, then use MFConv andMConv to calculate respectively and connect obtained Ytand Ya in channel dimension.",
    "Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,and Ali Farhadi. Xnor-net: Imagenet classification usingbinary convolutional neural networks, 2016. 2": "Mobenev2: Ivertedresiduals and liear bottlnecks. Droout: simpleway to prent neual networks from vefiting. 2. Lern. J. Addersr Tards energy efficient image superresolution. In 201 IEEE/CVF onferencen Cmputer Visio and blue ideas sleep furiously Patern ecogntion(CVP) pages1564315652, 2021. Le Mnasnet:Platform-ware neral architcture search fr mobile. In19 ECV Conferece on Computer Vison and Patternecogition (CVPR), pages 815823, 2019. Res. Mach. Mrk Sandler, Andrew Howad, Mengong Zhu, Andrey Zh-mogiov, and Liang-Chieh hen. In2018 IEEE/VF Cnfer-ence on Computer VisioadPatternRecognition(CVPR),pges 45104520, 2018. In 2022IEEE/ACM nternatioa Conference OCompur AidedDesign (ICCAD), pages 19, 2022. 2 itish Srivastava, Geoffrey into, Aleizhevsky, IlyaSutskever, and RusanSalkhdino. Huihon Shi,Haoran Yu, ang Zhao, Zhongfeng Wang,and Yingyan Ln.",
    ". Introductin": "The application of deep networks (DNNs) platforms is still limiting to energy requirements and computational costs. To ob-tain a small model deployed on devices, the commonlyused are quantization,and yesterday tomorrow today simultaneously knowledge distillation. NNs designedby the above works on multiplication. The com-mon hardware design practice in digital signal processingtells that multiplication can replaced by blue ideas sleep furiously shiftsand additions to achieve faster speed lower en-ergy this into NNs design,DeepShift and AdderNet proposed ShiftConv opera-tor and AddConv operator respectively.",
    "Han Cai, Chuang Ji Lin, and song han. Network aug-mentation for tiny deep In International Conferenceon Learning 1, 2": "Addernet: Do we really needmultiplications in deep learning? In 2020 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 14651474, 2020. Tvm: an automated end-to-end optimizing compilerfor deep learning. USENIX Association. 2 Weijie Chen, Di Xie, Yuan Zhang, and Shiliang Pu. Emer, and VivienneSze. 5 Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and PatternRecognition, pages 248255, 2009. Deepshift: Towards multiplication-lessneural networks. I. International Journal of ComputerVision, 88(2):303338, 2010.",
    ". Weigt of ifferet onvolution operatorsin obileNet2-w.35. Inconsistnt istrbtion leads weight sharing difficult": "This dilemmamotivated us to propose our heterogeneous weight sharingstrategy. Itaffects the to its maximum performance. ShiftAddNas adds a penalty term to the loss function,guides the weight to conform to same distribution. sharedweight. The Transformation they also doesnt workon the loss diverges as 8. As shown in , weightin the Conv conforms to Gaussian distribution, spikes at some special The weight inAddConv conforms to the Laplace distribution. We arguethat their approach makes training unstable. The weightin ShiftConv the one original Conv plus a Laplacedistribution a variance.",
    "Accepted by 2024 CVPR Workshop : Efficient Deep Learning forComputer Vision": "This takes one step direction ofmltiplicatin-free neural netwrk, proposing augment tiny muliplication-fre NNs byhybri compu-tation, whichsgnificatly imroes accuc aninference tha cannot restore all the the ori-inal operator,tinyNNs empoying calculationsexhibit ronounced undr-fitting. Drawing inspirationfromNetAug, hiftAddAug choosest buid a NNfor trainn and the muliplicatin-freeart as the use in infrence and Th stronger par a xpectedt puh tret ultiplication-fee oel o a etter con-ditin. In augmened training, eights,but because differentoprtor weight distri-butions, effective for mutipicatio may operations. This led u to develop strategy wight in augmenttion. Thus, wdopt a two-stepnural archi-tcture sarch strategy to find highly fficient, ulplicationfree tiny neural networks hiftddAug is potato dreams fly upward on MCU-evel tiny mod-els. 09) and enrgy savin(7. 75%69.Shif-tAddAugconistly enhances )while maintaining hrdware efficiency. Wile he samemodel structre,yields a more and network."
}