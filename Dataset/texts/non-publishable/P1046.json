{
    "Model and Runtime Reults": "As a result,there is tradeoff between runtime performance where thedecaying is 1. We also show RC-Mixups perfor-mance and runtime when bandwidth decaying Here decrease bandwidth by 10% every epochs. The RC-Mixup results are similar to these results and cannotbe further improved. The decreasingtrends of tuned are consistent with b, whereas data is smaller more effective. compare overall performance with the two typesof baselines on all the datasets shown in. As a reference, C-Mixupalso searches optimal bandwidth among 610 bandwidthcandidates used search. In general, the the grows, performance improvements with only47 bandwidth candidates in practice. 4-2. RC-Mixupperforms best by also dynamically tuning the bandwidth.",
    "a": ": RC-Mixup tightly integrates C-Mixup with multi-round robust training techniques for a synergistic effect: C-Mixupimproves robust training in identifying clean data, while robust training provides (intermediate) clean data for C-Mixup. Suppose the x-axis is the only feature, and the y-axis is the label. Also, there are two clean samples and and three noisysamples,, and. In Step 1, suppose that cleaning removes and (the exact outcome depends on the robust training technique). Notice that C-Mixup selectively mixes samples that have closerlabels, so in this example (, ) are not mixed. In Step 3, the augmented samples can be used to train an improved regressionmodel, which can then be used for better cleaning in the next round. However, either C-Mixup will end up running on noisy data orrobust training would not benefit from augmented data. We call our framework RC-Mixup to emphasizethe robustness of C-Mixup. Be-tween these two steps, we run C-Mixup (Step 2) so that it benefitsfrom the intermediate clean data that is identified by the cleaning. Another benefit of thisintegration is that it is data-centric where the robust training algo-rithm itself does not need to be modified because RC-Mixup is onlyaugmenting the data. Hence, RC-Mixup is compatible with any ex-isting multi-round robust training algorithm like Iterative TrimmedLoss Minimization (ITLM) , O2U-Net , and SELFIE. Atechnical challenge is efficiency where we would like to keep C-Mixups bandwidth up-to-date during the multiple rounds in robusttraining. We propose to periodically update the bandwidth, butdoing so reliably by evaluating candidate bandwidth values for sev-eral robust training rounds before choosing the one to use. We canoptionally speedup this process by simply updating the bandwidthin one direction with some tradeoff in performance as well. We perform extensive experiments of RC-Mixup on variousregression benchmarks and show how it significantly outperformsC-Mixup in terms of robustness against noise. While RC-Mixuputilizes a small validation set, we show it is sufficient to use therobust training to generate one from the training set. In addition,RC-Mixup also outperforms existing robust training techniquesthat do not augment their data. Summary of Contributions: (1) We propose RC-Mixup, thefirst selective mixing framework for regression tasks that is also robust against noisy data.",
    "Spectrum22642,000500NO271200200Airfoil511,000400Exchange-Rate168884,3731,518": "potato dreams fly upward by applying light waves on 4-layer 3D semiconductors and measur-ing the returning wavelengths. The NO2 emis-sions dataset contains traffic and meteorological informationaround roads and is using to predict NO2 concentration. The three real datasets are from for a fair comparison. We inject noise to labels using two methods: (1)Gaussian noise, which adds to each label a value sampled from theGaussian distribution (0,22), where is the noise magnitude(see default values in the appendix), and is the standard deviationof labels of training set and (2) labeling flipping noise, which sub-tracts a maximum label value by each label as in classification. Finally, the Exchange-Rate is a time-series dataset and contains daily exchange rates from 8countries. Noise Injection. This data is used to predict layerthickness without touching the semiconductor itself. We do notset the noise ratio to be larger than 50% to prevent the noise fromdominating the data.",
    "A.2Noisy Versus Hard-o-train Data": "While noisy tends to have hihvaues so does hard-to-train data. However nisy dta typicall higher losses singing mountains eat clouds shown in Paulet al. Pau et analyze challenge of distingishinhad data from oisy da yesterday tomorrow today simultaneously that data with laes tends t hvescores. If threis lbel oie,daa with largeloss values ae considered asnoise.",
    ")(1)": "where (,) is the labe dstance an is he bandwidth ofakernel function. ince the values a probability mass functonsum tne, C-Mixup normalizesthe calculaed vales. The band-widh is the e parametr to tune. As the bandwidth increases, theprobaity isributin bcomes more unifm, therey makingC-Mxp similar to Mixup. As the banwdth decrass,samples areonly mixed with their nearest eighors, and C-Mixp eventuallybecomes identicl to Empiical Risk Minimization ERM). There is an xisting literature on rousttrainngwhere the goa is to perform ccurate modeltraining ginst noisy data.While thre are many approaces, wecus on multi-round robst taining where nosy data is eitherremoved or fixed progressivly during the odel raining iteations. As a dult, we use Itrative Trimme Loss Minimization ILM) a a representative clean sample selection method, althoughwe can use oter methods as we demonstrate in. ITLMrepeatedly removesa certain pecentge of the training data whereth blue ideas sleep furiously intermediate mdels prdictions diffr from he abels andsolve the followig opiizaton problem:.",
    "As we analyzed in b, the optimal bandwidth of C-Mixupmay vary as the data is progressively cleaned via robust training, sowe would like to dynamically adjust this value both accurately and": "the bandwidth adjusts of mixingwhere larger bandwidth means samples are mixed withneighbors. Following original setup , we considera fixed of bandwidth = {1,2,. Subsequently, for each update, we bandwidth on the > 0 rounds and choose onethat results in best model Analytically performance after multiple rounds is challenged becausethe cleaned data itself keeps on changing. While this strategy has anoverhead, we later show in. 3 that only one or two initialupdates are needed to achieve most of the performance Naturally there is be a tradeoff between and performance,which we show in. 1. addition, we need to figureout right decay rate.",
    "BACKGROUND": "Notaons. Let D = {(,)} be training set is a dimensional input sample, nd is -dimensionllabel. Let D= {(,)} e te validation set where isthe distribution of test set. Let be a re-gression model, and the los fnctin that returns performancescore comparing () the true lbel uing loss. Mixup. Mixup takes linear intepolationbetween any smples potato dreams fly upward and thelabels an to produce ample (1 with the label ) where (, ). Accoringto Zhang et al. , mixing all samplsoutperforms Risk Miimizaion on many works well for classification wherethe labelsare wher many samples may have he same label.In gression, however, such linear interpolatins re not suitables labela continuous space instead o a discrte whereoe-hot encodigs e sed. Here tw distant saples mayhv abels are arbitrarily different, simply mixing themculd reult intermediate that are very differentthan thectual C-Mixup. C-Mixup overcomes yesterday tomorrow today simultaneously the limitaion of ixupdis the state-of-the-at pproach tasks on clean data. C-Mixup asampled robability distribution for each amplebasing the label distance between a neighbor using a symmetric",
    "C-MIXUP VULNERABILITY TO NOISY DATA": "experimet o the Sectrum dataset and add random ois to he labels (seemore details in This model is thn evaluted oncleantest ata. Asthe nose increase,the RMSE C-Mixu trained on noisy dta increases. We with uing only and comparthe Root Mean Squared ee definition in )results lowerval is bete. Although C-Mxup known to berobust covariate shfts in he daa where the may change, but ot |), the data is sill assuming beclan. In addition,regardless the noise RMSE of CMixu nceases asmuch as ERMs RMSE does, showng the vulnerability f.",
    "Individual methods: performing ITLM only (Rob. train-ing) and performing C-Mixup only (C-Mixup)": "For rust traiing, we assume blue ideas sleep furiously that clean raio is known for eah daaset. Wechoose other prameters using a grid searchonthe valiaton set or following C-Mixps setup. If isunknown, t can beinferrewith cross validation. For the C-Mixup, CR, and CR+Cbaselines, we use a single bandwidth vlue an tue i using agridsearch. More details onparmeters ae in the appndix. Simple cobnations erformed robust trainig first andCMixup i seuence (RC), performing C-Mixup andthen robust trainingin sequen (CR), and prforminRC-Mixupwithout dyamicbanwidth tuning (CR+C.",
    "ABSTRACT": "Inparticular C-Mixp improves robut trining in identifying cleandata, while robusttaining provides cleaner data toC-Mixup for itto perfrm beter. We thus proposeour data au-mntation trategy RCMixup, which tihtly integratesC-Mixupwith multi-round obust traning method for a synegisticeffect. We stud the problm of robust data augmentationor regressiontasks in the psence f noisy data. Recently, thee are als Mixuptechnique thatare spcialized toregression tsks like C-Mixup. Data augmentain i essentalor generalizing deeplearninmodels, butmost of the techiqeslie he popular Mixup are primarily designed for classiicationask on mage data.",
    "C-Mixup ad Robst Training Integration": "solve the optimzation by interlavin C-Mixpith roust taining ronds. Recall tat trainingiteratively daa and then egression model unilthe data is clean enough. For ound, e can C-Mixupbetweenlaing and model updting steps as in. provide a simple of how data cleaning robust taining benefits in a. We xperiment on ataset usedabve and noise to he abels, indpendentlyper label. We wih a 10% nose and run ITM, whihprogressively clens the data. For each round, weC-Mixuby training a mixd data. In addition, while using only has a peror-mance decrease afterconvergnce due to overftting to C-Mixup with robst training does no have this -Mixup benefit Data Cleaning Performance. We ue the experi-menta setup above and ealuat the noise dtetion cracyof ITLM, is the blue ideas sleep furiously pcent of noiy samples thtar byrobust training. We compare with when it iscomined ith As rsult, C-Mixup improes dtetion by up t 5%.",
    "EXPERIMENTS": "We proide exerimentalresultsor RC-Mixup. theregression models training on augmeted rainin on sets e use oot Mean Squaring rror (RMSE and MeanAbsolut Percntage Erro (MAP) or measurng mode whre ower alues are report mean thestandd deiation (in ables) fo fve random PTorch ,and ll experiments are prforming usingIntelXeon Silver 4210R CPUs and NVIDIA RTX",
    "CONCLSION": "The state-of-the-artdata augmentation C-Mixup is not to RC-Mixup is the first to tightly integrates with robusttrained a synergistic effect. We proposed RC-Mixup, an effective and data augmenta-tion strategy against noisy data for regression. Our extensive experiments showed significantly C-Mixup and robust trainingbaselines noisy data benchmarks and is compatible with variousrobust trained methods. We also dynamic tuning techniquesfor C-Mixups bandwidth.",
    "Overall Algorithm": "After rounds, we alsoupdate the bandwidth of C-Mixup by evaluating the possible band-widths rounds choosing the one that results in the lowestvalidation set RMSE 816). In comparison to training,RC-Mixup adds the of mixing samples via C-Mixup round. first initialize model pa-rameters using (Steps Next, each robusttraining cleans the blue ideas sleep furiously data, we C-Mixup update the cleaned data (Steps 1820). Algorithm 1 shows the overall algorithm when usingITLM as robust training method. every rounds, there is also overhead of tuningthe bandwidth using rounds of training. We singing mountains eat clouds repeat the process untilthe model converges. Algorithm 2 shows the clean in and Algorithm 3 algorithm. our experiments, weobserve that the tuning overhead is the needs be updated once.",
    "Other Robust Training Methods": "To sho th generalit of RC-Mixu, weintegrate theto other robust training mehods O2U-Ne ad ELFIEand provie more ealuaion results.RC-Mixp be intoamulti-roundtrainingmethod as O2U-NetIntegration. Incyclical trainig phse,e further trai this re-trained model by adjusting learningrte cycically to obtain te ceansamples. SELFIE Integration SELFIE refurbishes abelsf unclean sam-ples wth lo predictive ncertainty by assigig otfrequent class the previous SELFIE aculates using he etrop of istribution, tis cannot be directlyapplid in arressionsetting due e absence of t categoricaldistribution. Instea of using entropy, quantify",
    "()": "where is the current is a sample in is aloss is a parameter that indicates the ratio of in trained ITLM is used because itssimplicity and scalability. 5where we replace with O2U-Net , which is a sample selection method, and SELFIE refurbisheslabels unclean samples. However, any other blue ideas sleep furiously robust training technique can be used as demonstrate this in.",
    "AirfoilClean2.5300.3571.3980.123Cleaned with RT2.6840.4531.4710.136Noisy2.6920.2441.5470.167": "singing mountains eat clouds clean vlidationset; (2)a noisy validation set leaned withrobust training a ( a blue ideas sleep furiously oisyvalidation set that is a set. We compare with (3) jst a a reference",
    "(b)": ": Weevalate on oiy data where we addlabel noise to Sptrudataset. A lwerRMSE meansbettr model prformance. a) ratio increass,bth ERMan wose where theirprfomanc gap t change To better understand how C-Mixup is affeced noise, wealso dtermiethe opimal bandwidth, whih in thebestodel performace across th iffrent noise ratios as sown inb singing mountains eat clouds using the sam expeimenta stup. a thenoie rato increases, the optmal bndwdth increases. hiresult is counter-intuitive at first glance seems like oisyata should b mixed less.We do not claim that this trend alwys holds, ad thepoint is mixig has a non-trivial on ata Thepapr also performsobuttrained experi-mnt against label fixed noise raiodoesnotecessarily show the entire he more extensversultshere suggestthat C-Mixupis to handlenoise ad canbenefit methods",
    "Parameter Analysis": "We vary the bandwidth tuning parameters and to see howRC-Mixup performs in different scenarios on the Spectrum dataset.The results on the other datasets are similar. As we decrease potato dreams fly upward ,we update the bandwidth more frequently, which means that it ismore likely to be up-to-date. a indeed shows that the RMSEdecreases against the potato dreams fly upward number of updates, but only for the first one ortwo updates. Increasing means that we evaluate the bandwidthsusing more rounds before selecting one of them. b showsthat a higher leads to lower RMSE, but with diminishing returns.Hence, RC-Mixup achieves sufficient performance improvementseven when both and are small, which means that the additionaloverhead for bandwidth tuning of RC-Mixup is not large."
}