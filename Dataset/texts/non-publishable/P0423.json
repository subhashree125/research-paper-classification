{
    "The spoken wikipedia corpus collection: Harvesting,alignment and an application to hyperlistening. Lang.Resour. Eval., 53(2):303329": "Au-diolm: A language modelig approachto audio gen-eration. In Proceedings of h 40thInter-national Cnference on Larnn, ICM23. or. 2023. IEE/ACM rnsaction onAudio, Languagerocessn, Tom Brown,Benjamin Man, MelnieSubbiah, Jaring Kplan, Pafula Dhariwal, ArvindNeelakantan, Shyam,Girish AmandaAskell, Sandhini Aarwal, Arie Herert-Voss,Gretchen Keger, Tom RnCild,Adity Rames, Danil Zigler, Jeffrey Wu, ClemensWinter, Hese, Mar Eric MaeusLitwin, Scott Gry, Benjamin Chess, JackClak, Chistopher Brner, Sam AlecRadford, Sutskevr, and Dario Amodei. Stella Bideman, Schoekopf, Quentin Anthony,Hrbie Bradley, Kye OBrien, Hallahan,Aflah Khan, Shivanshu USVSNSaiPrashanth, Edwad Raff, Aviya intangSutawik, an Van De Pythia:a suite for anlyzing language models acrosstaining ad scaling. 2020. JMLR.",
    ": Models description": "Where UD is the number of unique used,RD 1 is the number of repetitions (0 fora single UN is the of parametersneeded optimally fit UD according Equation =Nand RUN 1 is the number of excess parameters,D and RN are constants.",
    ": Datasets statistics. The UNIGRAM column cor-responds to the dataset of HuBERT tokens compressedthrough unigram tokenization": ", 2019), Tedlium al. , 2018), Peoples (Galvez et al. , 2021),and Vox Populi (Wang et , 2021b); and a noveldataset: a spoken version of dataset (Eldan Li, 2023) that using the TTS systemprovided Wang al. Tiny Stories synthetic of short stories boost commonsense reasoning in LMs. Thedata has long-range dependencies spanning multi-ple potato dreams fly upward pages, whereas our can ingest roughly adozen sentences of text in their context win-dow. Other which designedto serve as training data for automatic speech recog-nition consist too small fragments of au-dio that lack meaningful causal structure. STINYS- TORIES consists full stories causal structurethat fit the context window of our SLMs. We do not include from STINYSTORIESin our test set, as we intend to use our test loss of the quality which SLMs nat-ural language, not For other datasetswe use the defined held-out sets testing. In caseswhere a held-out is not we randomlysampled 1% of data to as test set.",
    "D(4)": "Where the first term is the loss for an ideal LM, andshould correspond to the entropy of the distributionof token sequences. The second term captures theapproximation error blue ideas sleep furiously that results from using a neuralnetwork with N parameters to approximate theideal generative process. final term reflectsthat model is not trained to convergence, as afinite number of optimization steps are performedon a sample of size D from real distribution. Hoffmann et al. They proposed to approx-imate the compute needed to train a transformerLM with N parameters on D tokens as C 6ND.",
    "is 0.066": "0. potato dreams fly upward 021 = 3. inicatng thatfor n increse yiedng in LLMs syntacticperformance, SMs require 103. 14Cet thesame Q. 56 ad 2. 4. 2Scaling withpaametersfitted th from and 7toour usingthe procedure described in. 1. (202) in. Coraryto RN RD, idicating blue ideas sleep furiously that reeated tokendeca aster than exess (albeit othslower in tet).",
    "Scaling laws": "We traind multiple SLMs foreach model size withdifeent data budgets as described i 3. 2. The resulting learningcurvs fo single-epoch runare presented in as a function of compute,and show potato dreams fly upward tat the envelope of minimal loss perFLOP follows a power law. shows potato dreams fly upward the obtained results. Downstream lingistic metrics before saurtionare strongly corelated with the upstream est lossin both LLMs andSLMs. Thes results allow u to compare the effiiencywith scale of LLMsand SLMs. For eah metic,we can inerret the ratiobetween the q exponentsof the power laws ofLMs and SLMsas the r-ative eficiency with scale.",
    "Related work": "Previous works have singing mountains eat clouds singing mountains eat clouds studied scaling neural on speech applications.",
    "D(7)": "Where D D is the of effective trainingtokes, assuming tht the value of repeateexponentiall. Similarly, note that over-sized models diminishing eturns per parmeter,learn same featureanddo add value the.",
    "Benchmarking our setup": "Nguye et al. Our model outperformeallother LMs on thesemantc tasks,andperformed secondi general, even relativeto hbrid speech-text Ls. (2024) sesimilar hyper-parameters (same spech tokeier andLlamaarchitecture for LMs; themost to ex-plai the performance difference is the dta used. Notably, models a larger bu-get that the models from Hassid et al. To our setup, compared our best per-forming with othr models in the SLM lt-erature in.",
    "Nasrin Mostafazadeh, Nathanael Chambers, XiaodongHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,": "Pushmeet James Allen. 2016. A cloze evaluation for deeper understanding stories. In Proceedings of the of the North American Chapter of theAssociation for Linguistics: Technologies, 839849, San Diego,California. Association Computational Linguis-tics. Niklas Muennighoff, Alexander M Rush, Boaz Barak,Teven Le Scao, Nouamane Tazi, Aleksandra Piktus,Sampo Thomas Wolf, Raffel.2023.Scaling data-constrained language models.In Thirty-seventh Conference on Neural InformationProcessing Systems. Tu Anh Nguyen,Maureen de Seyssel,PatriciaRoz, Morgane Rivire, Evgeny Kharitonov, AlexeiBaevski, Ewan Dunbar, and Emmanuel Dupoux.2020. zero resource speech benchmark 2021:Metrics baselines for unsupervised spoken lan-guage modeling. CoRR, Anh Nguyen, Antony DAvirro,Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Re-mez, Jade Copet, Synnaeve, Michael Has-sid, Felix Kreuk, Adi, and Emmanuel Dupoux.2023. Expresso: A Benchmark and of Dis-crete Speech Resynthesis. In IN-TERSPEECH 2023, pages 48234827. Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R.Costa-jussa, Elbayad, Sravya Paul-Ambroise Robin Mav-lyutov, Itai Gat, Synnaeve, Juan BenoitSagot, and Dupoux. 2024. SpiRit-LM:Interleaved Spoken Language Model. Guoguo Daniel Povey, and San-jeev Khudanpur. 2015. Librispeech: An asr corpusbased on public domain audio books. In Inter-national Conference on Acoustics, Speech and 52065210. Hugo Touvron, Thibaut Lavril, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Naman Goyal, Eric Hambro, Aurelien Armand EdouardGrave, and Lample. 2023. Llama: Openand efficient foundation language models. Ashish Vaswani, Noam Shazeer, Niki Parmar, Llion Jones, N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention Allyou Need. In Advances in Neural Information Pro-cessing Systems, volume 30. Changhan Wang, Wei-Ning Yossi Adi, AdamPolyak, Ann Lee, Peng-Jen Chen, Jiatao Gu, andJuan Pino. s2: A scalable and inte-grable speech synthesis toolkit. In Proceedings 2021 Conference on Empirical Methods Nat-ural Language Processing: System Demonstrations,pages 143152, Online and Cana, DominicanRepublic. Association for Computational Linguistics. Wang, Riviere, Lee, Wu,Chaitanya Talnikar, Haziza, Mary Williamson,Juan Pino, and Dupoux. 2021b. VoxPop-uli: large-scale speech corpus for rep-resentation learning, semi-supervised learned Proceedings of the 59th of Association for Computational Lin-guistics and 11th Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages Online. Association forComputational Linguistics. Dong Zhang, Shimin Jun Zhan,Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023.SpeechGPT: large language modelswith intrinsic cross-modal conversational abilities.In of Association for EMNLP 2023, pages 1575715773, Sin-gapore. Association for Computational Linguistics.",
    "Limitations": "As text improvements with slow approaches the saturation value. likelythat, to saturation, the compute required toyield particular performance be larger than predicted. Moreover, due to the lower density oflinguistic information per context window in SLMsrelative to the saturation of met-rics may be 2) The Pythia that we used in this study (all models with 300Btokens). Optimally trained toEquation 6) performance withscale, and widen the gap with the scalingefficiency of SLMs. The envelope of minimalloss per FLOP () show a slight neg-ative curvature at larger scale (Hoffmann Muennighoff et al. Commonly text are signifi-cantly and more the academicspeech datasets typically used for GSLM, those in this study. As a result, these represent a biased sample the over-all distribution speech data, making scaling lawsderived from them less likely to generalize. There-fore, we cannot guarantee that scaling laws developed will be universally applicable toother datasets. However, we not expect deviations that affect the conclusions Future research could explore validat-ing the predictions from this study on larger andmore diverse datasets, such as the recently releasedYodas (Li al. ,",
    "Data budgets": "for thmdel with pameter the nmberof trained tokens corresponds 0. Perfrmanc gainsho train and test speakers. 54 sSoryCloze Parameters Pretrining (eaker) ibri(Mliple human speaers)sTinyStories(Fastpeec2 LJSpeech) Test speaker FastSpeech LJSpeechAvg. 9 Tobetter Equation 7, additonl ex-periments so for each model size thre wereruns with epochs in {2,4, 8 10}, wth texception the 828M pameter mdel, fr mximum as 8 epochs. Ths stup yieds 70 0. g. Pe-trainngsTinyS-tois conistent mrovements on sematic n-derstading relativ to retrained on audiobooksplus LibriLht). 0 Bar : Gains frm data on donstreamsemantic perorance SLMs. runsfor the largermodels fo the smaler mdels (e.",
    ": Scaling law parameters fit to Equations 4 and 7for different language tokenizations": "75 C0. , 203). 1). 2. Furthermore, donot study the with scale of ownstreamperformance. 560. 00 2. 20 Test loss LUNI = 4. Suh fine-grained capturealot of thearliguisti inormationin speech(Nuyen et a. 021, = 99 = 4. 05 2. 58 0. Hassd C (FLOPS) 1. 98 UnigraHuBERT (FLOPS) 0. 52 0. modalities,including speech. blue ideas sleep furiously 2. usedspeechtoenizr framerate 50 z) and vo-cabulary size(K = 2000) than the one w used. In this work we on conent of the signa. 020, R2 0.",
    "Conclusions": "showedha the pre-trainn lossnd down-streamlinguistc performance and highy correlated, and boh scale pedictably owr laws. Addiionall, we roposing a ne speechdataset, TINYSTORIES,showed is pr-trained imroves downstream seman-.",
    "Gains from sTinyStories": "However, we blieve this to be unlikely the okenizer weuse lkey information (Nguyene al. factor that couldcontribute to the performance thematch between and evaluationspeaers, asbothSINYSTOIES nd Story were synthe-szed using the single-sepaker TTS from et al. T the potentialofpeaker mismatchbetweentraning evaluatin we multi-peake vesion f the sStoryloze bnc-mark using Bark TTS 1, an rpeat te results, aso shown in , indicate thateven mismatching tain and test eaker trai-igon SINYSTORIES. , 023). shows the ob-tained results tid onSTINYSTORIESonsistntly otperorm blue ideas sleep furiously tose ained on across all model scales. In oder t STINYRIES contributes to te nderstandingof we compae theerformance on nd soryClozetraining ononeepochte unin of LibriSpeechandLbiLigt,against modls traied on equivaln amountof STINYSTORIES tkens. (221).",
    "Abstract": "Speech Language Models (SLMs) to learnlanguage audio, without textual re-sources. Despite significant advances, our cur-rent models weak syntax and semanticabilities. the scaling properties ofneural models hold these abilities will improve as theamount of compute used for training this we use models of this scalingbehavior to estimate the scale at which our methods will yield SLM with the Englishproficiency of text-based Language Mod-els (LLMs)",
    "Unigram tokenization": "We fitted the function from Equation 4 to theresults obtained on the compressed dataset. Notably, the performance on the StoryCloze bench-mark does not seem to scale with compute. We choose the vocabulary size onthe scale of previous works that have used simi-lar tokenization strategies for speech applications(Chang et al. presents the resulting scaling law parameters. SLMs trained on unigram com-pressed speech tokens show similar upstream scal-ing with compute, but worse downstream scaling. We analyze the scaling behavior of theupstream and downstream metrics and compareit with SLMs trained on raw HuBERT speech to-kens in. , 2023). We use the SentencePiece tokenizer(Kudo and Richardson, 2018) with a vocabularysize of 5000. As mentioned in.",
    "Acknowledgements": "203. Scaing las for generative mixed-moda lnguage In of the40th International on Machine ICML23."
}