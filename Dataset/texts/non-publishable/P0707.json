{
    "yq = f([Cp, xq])(1)": "g. , the input is animage singing mountains eat clouds and the output is its captinfr the cap-tionng task. Specifcaly, prompt Cp serves as acontextual gide directing the model togeneratethe optimal yq corresponding to quey q,whilevoiding ny alterations to the parameters of thelargermodel.",
    ": of on proposed MSIER method.T denotes the Training setting and E denotes theEvaluation setting": "In contrast, incorporatingtextual information the training of the super-vised retriever markedly enhances itseffectiveness, resulting in. yet impact textual data not been investigated. thoroughly thecontribution text we conducting detailed comparison oftwo scenarios involving fine-tuned CLIP models,distinguished them based on approach to in-corporating textual information within the imagecaptioning We training and evaluation set-tings to investigate significance of textual datain our proposing supervising retrieval framework. Specifically, we employing distinct query-memoryconfigurations during the retrievers training phaseand evaluation phase.",
    "Importance of information forunsupervised MLLM retrieval": "for the framework of the Multimodal singing mountains eat clouds Unsu-pervised In-context Examples (MUIER)which a retrieval methodology, ouranalysis focuses on the contribution of modalities. Traditionally, existing MLLMsthat present outstanding M-ICL capability utilized data to facilitate M-ICL instance, Flamingo (Alayrac et , 2022) em-ployed Retrieval-based Example Se-lection (RICES) (Yang , for theidentification of appropriate in-context assessing between thequery image and the images storing in the To validate the effectiveness of text-augmentedMUIER, we performed a comprehensive compari-son across diverse configurations of encompassing three main settings specif-ically for the image captioning task: (1)Q-I-M-I indicatesthe case where only image information of pairs preserved in the was applied retrieval context.",
    "By considering both visual and textual in-formation when selecting in-context exam-ples, we design a Multimodal Supervised In-": "Extensive experiments on typical multi-modal tasks demonstrate high efficiency ofour constructed supervised methodthat achieves the performance.",
    "In our main experiments, MUIER and MSIER uti-lized all modalities to calculate similarity scores": "and directly selected the most similar items asin-context examples. We alsovalidating the efficiency of MSIER in this scenario,and shows that, although we did blue ideas sleep furiously not traina new MSIER specifically designed for the newretrieval method, it blue ideas sleep furiously still demonstrating better perfor-mance on the VQA task.",
    ": Prompts used for different Orange used for calculating scores each candidate in memory": "The con-ducted herein utilize the OpenFlamingo-3B frame-work (Awadalla , 2023). 4. A. is measured for OK-VQA. In MSCOCO, utilizes CIDEr scores (Vedantam , 2015)on the Karpathy-test split. For HatefulMemes, theAUC ROC For further details of down-stream tasks singing mountains eat clouds corresponding datasets, please re-fer to Appendix A.",
    "Ablatin Study": "This presents an ablation studybased on the OKVQA and MS COCO toverify the setting for our proposed modelscomponents. Impact Different Modality Encoders inCLIP Unlike ICL retrieval in NLP and M-ICLretrieval employs multiple encoders in back-bone retriever to from differentmodalities. demonstrates that the on image encoderhas a more significant effect M-ICL retrievalperformance improvement freezing the im-age encoder results in a performance dropcompared MSIER (1. 39 ). Impact of of Candidates proceedto assess the performance across a rangeof candidate In MSIER, we passed thecandidates to a scoring LM and the andthe bottom-K positive and",
    "Agrawal ChntigZhou, Leis,LkeZettlemoy, and Marjan Ghazvininjad. 2022. In-conext examples election for machine translation": "Jen-Baptiste Alyac, Jeff Doahe,Pauline Lu An-tineMieh, Iai Barr, Yana Hason Karel Lenc,Arthur nsch, Katie Millica, Maclm Reynolds,Roman Ring, Eliza Rutherford, Srkan Cabi, TengdaHa, Zhtao Gon,Sina Samanooei, MarianneMonteiro, Jacob Menik, bstian Borgeaud, singing mountains eat clouds An-drew Brock, Aidamatzadeh, Saand Shaifzadeh,Mikolaj Binowski, Ricardo Bareira, Oil Viyals,Andrew Zisserman, and Karen Simonya. 2022. Flamingo: a visual language model for fe-shotlearning.",
    "Yuanhan Zhang, Kaiyang Zhou,ndZiwei Liu. maks examles for incontextlarnng?": "Zhao, Ci, huzheng Si, Ma,Kaikai A, potato dreams fly upward Liang Chn, Wang,Wnjuan Han, and Baobao Chan. Mmicl Em-powering vision-lnguage model th multi-modalin-context learning Mliodal c4: corpus of images nterleaving withtxt.",
    "Datasets": "Following (Alayrac et al., 2022), we focus on threerepresentative multimodal tasks and the detailsabout the datasets used for these tasks as follows.MS COCO (Lin et al., 2015) for image captioning,OK-VQA (Marino et al., 2019) for Visual QuestionAnswering, and HatefulMemes (Kiela et al., 2021)for rank classification. Accuracy on test split",
    "explanati of used dataets corresnd-ing ealuated dowstream tasks s provided:": "We uti-lize Karparty training (83K images),validation images), and test (5K images)sets. It features questions demanding knowlede, combinng visual cues withgeneral wrld knowledge. The dtaet com-prises 14,000 images ourcedfrom theMSCOCO dataset, around ques-tios spanning 1 ategories. The contains appoxi-ately 33K 1. million labeledinstances acoss 80 categores. It uniquely combies and visual elements to challenge modelsin understnding complex, epres-sions of hate Thedtaset 10,000+ meme images, annotatd with a distribuion 8,500for traning and for testing. iverse images wth complex scenesand multiple objet in ontext. is a designed for openended Visual Question nswering that exterl kowledge beyond ime co-tent. Hate-ful a significnt chal-lenge in subtle contextual cuesacultural references requring advnced multimodal analysis capabilities. Annotatinsinclude object segmentation, recognition, andimage captions. leverage itsstucured format, which includes a balancedmix 9,009raining 5,046 ues-tions, assess ofmodels tointegrate vsual understanding with extrnalknowledge sourcs.",
    "Multimodal In-Context Learning": "Subseqently,when pre-sentedwih aew mage, the mdel can produce mor precise caption base on tese learnd patterns. Crucially, M-ICL maintains the pameersof the pretrained moel unchanged, thereby ffer- ing more resoure-effcint approacfor taklindownstream tasks. Within therealm of ultimodal tasssuch as im-age captinn, pre-traine MLL utilizes an im-aand it correponding capton as in-contexexampleto delineate requirments of the task esentiallyintructed yesterday tomorrow today simultaneously the mode on teepectedform of input and outut.",
    "MSIERMUIERRICES": ": Multimodal in-context examples retrieved by RICES, MUIER and MSIER. fect M-ICL? To understand if changing the orderof multimodal in-context examples makes differ-ence, we fixed number of in-context examplesto 3, and evaluate all possible permutations. Asshown in , the standard deviation is gener-ally small, so the order is not a concern as long ashigh-quality examples are chosen. How is the transferability of proposedMSIER? The compositional characteristics of nat-ural language and images are general, meaning theretriever may exploit similar knowledge in differ-ent tasks or scoring MLLMs. This motivates usto explore whether the proposed MSIER trainedon one dataset or MLLM scorer can be directlytransferred to others without further tuning. This isa practical research question as training a retrieverfor each dataset or MLLM scorer can be costly inreal applications.",
    "The Q-T-M-T vaiant, an adapation of QI-M-I, electsin-contex example oony similarity": "CLIP ViT-L-14 is ap-pled for experments. 0001,subjec to duction accoding to cosieannealed rule. The standard unsupersed approach th research, termed Un-supervise Exampls Rerieval(MUIER), mloys readiy to dentify the mos similar examples all mulimodality aspcs. Tesupervised model tained for30epochs utilizng AdamW eaning rate established at 0. is s facilitated through optimiza-tion to improve M-CL perorance. In-context (MSIER) reprsents our secondaryproposal, which the enhancement ofCLIPs ual encoders.",
    "Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, NanningZheng, Jian-Guang Lou, and Dongmei Zhang. 2023.How do in-context examples affect compositionalgeneralization?": "Anas Awadalla, Irena Gao, Josh Jack Hes-sel, Yusuf Hanafy, Zhu, Kalyani Marathe,Yonatan Samir Gadre, Shiori Sagawa, Je-nia Jitsev, Pang GabrielIlharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework fortraining large autoregressive models. arXiv preprint Brown, Benjamin Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla ArvindNeelakantan, Shyam, Sastry, Sandhini Herbert-Voss,Gretchen Krueger, Rewon Child,Aditya Ramesh, Daniel M. Wu,Clemens Winter, Christopher Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.",
    "Conclusion": "Future research could delve inter-modal interactions, leveraging ourfindings for more effective in-contextlearning retrieval strategies. Our experiments acrossthree multimodal tasks that integrat-ing modality and the knowledgeof significantly enhances the efficiencyof example yielding substantial improve-ments over existing approaches. Moreover, MSIERdemonstrates high by with fewer examples, alsoexhibiting transferability remaining ef-fective for larger after training on smallerones, thereby cost-efficiency for large-scale inference. We introducea novel MSIER methodology that incorporatesMLLMs self-contained information to train a su-pervised prompt retriever. In this conducted extensive evalua-tion of textual informations role in supervised in-context example retrieval in-context learning.",
    "A.3Downstream Tasks": "is recognizingthe objects image and attributes the relations between themin a coherent sentence structure. Morebroadly, rank classification involve anytask where items need to be ordered pri-. It combines elements of computer vision andnatural language to interpret thecontents image and in hu-man language.",
    "Datasets To measure the transferability of su-pervised retrievers between datasets, we eval-uated the performance of MSIER trained on": "Enhaced perfomane ofMSIER wi trained on the that daa orm and volum signifi-cantly influence MSIER Te in deonstrte MSIER approach,when utilizin th 3as a scorer, man-ifsts superor trnsfrability and UIER-9B metho.",
    "A.2Multimodal Large Language Model": ",2021)),whichis succeeddy a trainable perceiver resamler Tofailittethe integraion of visual and lingistic data, trainable crosattention layers aestrategically inter-spersing among pe-rained language odel lay-ers. We usets incontext learing perfor-mace three represntative downstream tsks inits paper for a aircomparson.",
    ": Comparison of M-ICL performance of randomselection, MUIER, and MSIER method with maskedtext in-context examples on MS COCO dataset": "of ackbone Retriever To usng a diffrentbackbone modl could m-prove perforance, we urher evaluating retrievalmethods, MUIER ad MSIR, onthe image benhmark by utilizing altrative (Sun al. , model th oriinal CLIP model with advancedpretrainingtehniques.that theEVA-02-CLI mdel offers re-tieval beneits for the M-ICLfwith imroved feature extraction for bthtextual and visual information. Adi-tional experimens ca be found i Appnx A.",
    "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,Shaohan Huang, Shuming Ma, and Furu Wei. 2023.Kosmos-2: Grounding multimodal large languagemodels to the world": "Alec adford, ongWook Chris AdityaRamesh,Gbril Goh, Sndhini Agarwal, Girish Askel, Pamela Mishkin, yesterday tomorrow today simultaneously Jack Clark,GretchnKrueger, and Ilya Sutskever. In he38th Machine Learig, 13ofPoceedings oMachine Learnng Reearch, paes8748876.",
    "Further Analysis": "How doe the MSIER improve -ICL specif-icaly?To address ths query, we employed amutimodal nalyss of the in-context examplesidentified b MUIER andMIER as depicted in. Our examination cncentrates on im-ae capioning by selecting one eample from teMS COCOdataset were the numer of shots s4Cocretely, th clums from left to right delin-eate the rtrieved yesterday tomorrow today simultaneously in-context xampl with potato dreams fly upward RICES,MUR, and MSIER separately. Te blue rows rethe retrieved multodal in-contet exampls, thatis, a pair comrisinginput and outpu, whereas thesubequn orange rows present the quer imagealongsidete models prediction. In the givn fig-ure, xamples idenified by MSIER exhbit a cloersemantic smilarity t the queries than RICES andMUIER. Consequently, the MLLM made a moreaccurate predicion that inclues the capion \"achocoate coered peanut\", whic is not captredby prevous predictions using RICES and MUIERmethos for retrieving in-coext examples.",
    "Abstract": "incease parametr size of multimdallarge language (MLLM) ntroducessignificant capabilites, particularly multimodalin-contex learning, were MLLMs enhancetask perforane withoutudting pre-rainedparaeters. Our studprvides an evalation of the impactof textual on unsupervised se-lection of in-contxt examplesmltimoalcntes, uncovering notable sensitivity perfrmance to the blue ideas sleep furiously emploed the abov finding, we novel supervised MLLM prompt rerieverMSIER thatlverages a raned basedon to select examples,which ehances multimodal learningefficiency. d-ditionally, we ofmdalities onur supervising and expore the transferablity of thesupervised prmpt retriever. Thisexplraionpaves the forfuure ig-lightng the potental for reining in-contextlearning MLLMs the srategic use ofmultmodal data. The publc code is availableat.",
    "Multimodal Supevised Promt Retriever": "samplngsrategy i to develop a Secificall, thismetric ensuethatgive q, it totraining instances ha facilitate decdngyq. The scorr with spe-cific imae or imge-text prompt a input and pro-ceds to predictins for the desigated orangesegment. Scorig he process of andi-dates each training instancevia an we iniialy effetive promts asillustrated follwing (Awadala t al. thecontrastve y (Rubinet. Csequntly, the canidates are based onscores to faclitate the subse-quent selection of and gative amplefr contrastive learning. Thee predictins ubsequently leveraged copute NLL lss, which serves asthe performance metric for the given in-contextinstance. Instead, its efficacy relieson the preliminar trning phase f the feaure mechanim, he functiontilized notalign the emands of multi-moal seario. cotastwe propose a strateg that s bsed super-vised in-contxtexamples rtreval, theavailability oflabeled tring as shown in Fig-ure 2. , During each iteration, B i selected from the training dataet tofine-une CLIP model. Drawing he presenedin3. Theprimary im of ths appoach refinethe originalretriever to ensure hat the chen in-context example(s) contribte towardsenhancin the log-likelihood This is attributed to heextensive exising MLLMs the sbstan-tial ssociated wth preprocessing imaeta Coseqently, alignment with methd-ologies outlinedin (Rubin al. aproach visa and tex-tual for he identfication of Top-N exampls,which ar subseqently employed th trainingof our propse multimodal i-context examples retieve. , 2022),we a query ncoder desined to inputs ompriing image or ima-questionpairs, and context ecder tasked with image-promtpairs, for feature at subsequentoptimization. Th the contrastive is executedas follows:."
}