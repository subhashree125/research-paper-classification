{
    "Evolutionary Segmentation": "Fixing blue ideas sleep furiously number of slots (F): Onyone slot as of policy Obtane ewards (R): Inspired b (Zhaoet al. This stagerandomly llocates slot-valuepairs failed to the volved on the dialou ol-ies wth betteralocate pair the volved art.",
    "Weihao Zeng, Dayuan Fu, Keqing He, Yejie Wang,Yukai Xu, and Weiran Xu. 2024. Divtod: Unleash-ing the power of llms for diversifying task-orienteddialogue representations. CoRR, abs/2404.00557": "owen Zhang, Songju Cao, XiaominXhng, Long a, and Takahiro 2022a. Curriculum semi-supervisd learning forspeech recogntin based on self-suervising pre-training. In Interspeech, 2652657. ISCA. Haodi Zang, Zhichao Keting LuWu,and iqi Zhang.2022b. dialog policylearning by contextual knowledge. AAAI Conferenc on Artificia Inteli-gence, AAAI 2022, onference on In-novative Applications Artificial Intelligence, AI2022, Twelvet Eductonal Ad-vances in Artificial Inteligenc, EAAI 2022 VirtualEvent, 22 Marc 1, 2022 1166711675. ang ho, Hua Qin, Zhenyu ang, Changxi Zhu, Findings of Association forComputational Linguistics: NAACL, pages 711723",
    "Experiment": "Our experiments utilize singing mountains eat clouds four datasets:Movie-Ticket Booking, Restaurant Reservation, Taxi Or-dering, and Multiwoz 2. 1 (Li et al. , 2018). To betterevaluate our method, we also conducted experi-ments potato dreams fly upward with human users, as shown in. 5. User goal difficulty positively correlates withslot count (Zhao et al.",
    "Goal-oriented dialogue oicy from falures.In AAAI ages 25962603. Press": "large language models few-shotintent classifier and slot filler. Syst. Self-pacing prioritized with coverage penalty in deep reinforcementlearning. Paramita Mirza, Viju Sudhi, Ranjan Sinchana Ramakanth 2024. 2017. Res. 2018. complexity and in task-oriented dialogue systems. Zhipeng Ren, Dong, Huaxiong Li, and Chen. Neural Networks Learn. ELRA ICCL. Springer. Taylor, Peter Stone. ,29(6):22162226. , 21:181:1181:50. Narvekar, Bei Peng, Matteo Leonetti, Matthew E. In Social Interaction Agents - 8th Interna-tional Workshop on Spoken Dialog Systems, Farmington, PA, USA, June revisedselected papers, volume 510 Lecture Notes in Elec-trical Engineering, pages 6369. Baolin Peng, Li, Jianfeng AsliCelikyilmaz, Sungjin Lee, and Wong. Association for Compu-tational Linguistics. 2017.",
    "Conclusion and future": "This study introduces novel at handling diverse environments dif-fering difficulty levels, thereby facilitating dialogue learning. It dynam-ically generates curricula goals goal shaping, twokey processes: goal decomposition, subgoals from user goals basedon dialogue and 2) goal evolution, pro-gressively increasing difficulty untilmastery of entire goal. By incorporating curriculum pat-terns, the BPL framework can selectively choosesuitable combinations to handle dialogue datasetswith known difficulty characteristics. In the future, our into mechanisms for transferring knowl-edge acquiring from subgoals to new agents.",
    "Limitation": "A limitation most learning meth-ods, including our approach, is that the knowledgelearned from tasks is trans-ferred and the current agent forlearning on a specific dataset. an interesting question for futurework is: how can singing mountains eat clouds transfer accumulate to the new agents and We sincerely thank the reviewers for their valu-able feedback constructive whichgreatly improved the paper. acknowledgethe volunteers from the Changsha University of Sci-ence Technology who with our humanexperiments",
    "Abstract": "This paper novel Bootstrapped Pol-icy framework, tailors progressively subgoalcurriculum for each complex through goalshaping, a smooth knowledge transi-tion. Moreover, enhance BPLs adapt-ability across various environments, we combinations of decompositionand evolution BPL, and two uni-versal curriculum patterns that effectiveacross different dialogue environments, inde-pendent of specific constraints. By integrating the summarized curriculum pat-terns, singing mountains eat clouds our BPL exhibited and across four publicly datasetswith different difficulty levels. Goal shaping potato dreams fly upward goal decompositionand decomposing complex goals intosubgoals with maximum andprogressively increased difficulty as the policyimproves.",
    "Decomposer": "left side of depicts changes the usergoal gi during goal decomposition, involving: i)Boundary state detection, identifying s4 near-est to the goal state within a failed dialogue trajec-tory of gi; ii) Goal yesterday tomorrow today simultaneously Decomposition, dividing thecurrent user goal into a corresponding boundarysubgoal based on the boundary s4;iii) the current usergoal gi with the boundary subgoal (the orange",
    "Decomposition Condition": "The decomposers ole s to deompose tricky usergals during tinng, nd avoid decompoionfr simple goas. Three decomosition conditinsguide BL in identifyed otimal momentsfor de-composiion across dialogue scnarios:Filre at any time (A): the decomposer activtswhenever potato dreams fly upward a user goa fails. ailre fte trained for N2 epochs T): a usergoal persits failingaftr N epochs ofpolicy larn-ing, it undergoes dcompostion.",
    "Fixed number of slots (F)BPL-CRObtained rewards control (R)BPL-CEExploration degree control (E)": "All results are of dialogues, with each testedon 100 with different random seeds aftertraining on a dialogue. For we use the informa-tion to and evolve user goals duringthe training phase. We eval-uate performance using the provided by the datasets, success rate, average turns,and average rewards. thedialogue fails, BPL decomposes the sampled on the dialogue trajectory and Test Phase: The user begins the dia-logue by randomly sampling a user goal from thetest goals set. L denotesthe maximum allowed number of dialogue turns,which defaults to all domains. 2. Similar training, the policy doesnot access ground-truth information. However, we do not use anygoal information the test We utilizeground-truth decompose andevolve user goals during the training phase donot any information during testing. The differences betweenthe of all agent pairs evaluated the sameepoch are significant (p < 0.",
    "Boundary State Detection": "A state qualifies a boundary state under the fol-lowing cnditions:i) All slot-value pairs in th state are presentin the oal state; A dialogue state thedialogue timet, including the urrentuser aton, dilogue histy,and mentioned slot-valu The goalstate sgcontans all slo-alue represningg. is determinedby the umber of msmatched slot-value pais: d =N(sg) N(s), where N(sg) represents of slots conanedi the glstate s. Inct, N(sg) = D(g. (s) represents the numberof slot in thecurrent s that are the same asthose inthe goal state The differnc betweenthe te distance from the states the state sg. cases multipleboundary states, the mostecent state is as the boundary asit more dialogu rounds reach thsstate. If tate in the matches anyslot-value of the goal state, a pairis randoml selected from the inform_slot setinthe goal as te boundary state.",
    ": user goals with increasing complexity": "However, in dialogue scenarios intermediate goals areabsent, achieving smooth knowledge transitions be-comes tricky. User goal g1: The user wants to a ticket fromNew York to Los today. 1 as an existingCL methods typically rank user from easy(g1) to difficult (g3), facilitated smooth transitions. Additionally, weidentify two universal, dataset-independent combi-nation patterns that maintain effectiveness acrossvarious dialogue environments, independent of spe-cific environmental. Goal decomposes yesterday tomorrow today simultaneously com-plex goals into subgoals with solvable maximumdifficulty, reducing complexity. On the one hand, BPL efficiently guides thepolicys progression from easier to difficultgoals, ensuring a smooth knowledge transition. User goal The user wants book a flight York Los and reserve ahotel room for night at the yesterday tomorrow today simultaneously departure User goal g3: The book a business class flightticket for an New York toLos , 2017). , 2024), rather than all componentsin pipelines. , 2024),NLU (Mirza al. ,2022b). Taking Tab. While large language models (LLMs) exhibit vast end-to-end frame-work introduces challenges in and in-terpretability (Rohmatillah et , 2023). Meanwhile,goal evolution increases the difficulty ofsubgoals in line with the policys growed capa-bilities, ultimately enabling of entiregoal.",
    "Dffect of varying N&Mvales on": "Intitively, N and M cntrol the ad thei number impactsdialoue policy learnig. It the aalyticalexperimets again. we conuctedexperiment with diffeent numbers o M and on tree datasets across different difficul-ties and 5 hows themoving avrage dured larning. In conrast, for the easy andifficul alogue datasets, usr are gen-erally easy or thus, N and M play littleole. The resuts showtht frhe difficult dataset, both N accurte diffiult user oalsor dopotion.",
    "Bootstrapped Policy Learning": "user simulator randomlyselets auser potato dreams fly upward goal blue ideas sleep furiously to stat the conversation.",
    "Hongshen Che, Xiaorui Dawei Yin, and JiliangTang. 207.A survey o dilogue Re-cent advac nd new IGKDD": "2023. 022. reinforcementlearnin via contrained optima In ICML,voume of ProcedigsoMachine LarningResearch, pages 113411358. Wai-Chung Hongru Wang, Huimin Wang, andKam-Fai Wong. 207 End-to-end taskcompletn neural dialogue In ICNL,page 733743. Li, Yun-Nung Chen, Lihong Li, ianfengGo, and Aslilikyilmaz. Chrisian Geisauser, CarelNiekerk, Hsin-ChiLin, Nurul Lubi, Michel Heck, Shutong eng, andMilica Gasic. n 29th Confernce o oputationalLinguistics, CLING 2022, Gyeongju, Republic ofKorea, 1-17, 266284. , 20(3):31834. Asian Feeraio of atual Lan-guage Processing. A nrecent advancesand chalegesin reinforcement learned fortask-oriented dialog policy Int. Dynamic dialogue forcontinul learng. on Compuainal Linguitics. Flrensa, David Held Markus Wulfmeier,Michael Zhang, and Ael. In1t AnnualConferece on Robot Learning, volume Proceeded Mchine Learning Research, pages482495 PMLR. Revrsecurriculum reiforcment learning. Pascal Haoyi Yang, Carlo DEram, Jan oni Pajarine. Auom. PMLR. 2022. 2017. Comput. J.",
    ": Illustration for dialogue learning usingproposed BPL framework": "2017) or rely on extensive successful experiencefor trained the discovery (SDN)(Tang et al. However, accumulating singing mountains eat clouds suchextensive be undesirable or unneces-sary, given the promising performance of other dialogue Instead, the BPL generates yesterday tomorrow today simultaneously an intrinsic subgoal cur-riculum for each complex goal solely on difficulty criterion.",
    "Validation": "As per prior research (Zhao et al. , 2021;Liu et al. In a multi-domain more domains goals, the more slots are included, thusincreasing the datasets These methodswere compared with datasets fromMultiwoz 2. diverse Resultsin 4 align with Analysis experiments. and SDNuse criteria but require pre-assessment data. improves in simple and medium do-mains faces challenges in difficult ones. Universal combinations remain ben-eficial uncertain difficulty scenarios. Theseresults available in B and C.",
    "BCase Study": "Basedon the example of a failing dialogue for user goalg1, we identify boundary state s4 and its corre-sponding subgoal g1 through goal decomposition. acquired knowledge of completing subgoalg1 can seamlessly transfer to accomplishing usergoal g2, as g1 is also a subgoal of g2. visualdialogue trajectories reaffirm this blue ideas sleep furiously outcome, blue ideas sleep furiously reveal-.",
    "Evolver": "iii) Goal Substi-tution, replacing original user goal gi theevolved new goal.",
    "Settings": "All moel ue with 0 neuros and MSpropoptimizer, with ixed hperpaameters: at batch t1, and dicoun fac-orat 0. A otal 500 joint trainng of the dialogue ancurriulum fin-tuning. Reward paramters are set. The experience buffer size is10, During trning, an-greedy strategy with = 0. 1 singing mountains eat clouds is for To mitgae cur-riculm sequencing cost, only120 dialogues areutilized for warm start, singing mountains eat clouds currculumneesary), ad training the subgoal discoverynetwk in SDN. We stnardized comon across allmodels for and selecte unique opimal a-raetersmodel. 95.",
    "|gent|if conversation conversation fails,1otherwise(2)": "Fewer costrains and equests re-sult in fewr agent actions required to complte g,reducingeror riks. 4. 2. Tus, the difficulty of user goal  vriesbased  th number o information and requestsin C and R. 3. 1."
}