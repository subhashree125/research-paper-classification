{
    "Settings of Self-supervised Learning": "this section, we present te general settings for ointembddingSSL methos. onsidera lareunaelledatasetX RND, comprisn sampleseahdimenionalty D. he SSL mehods cnsrut encoder f that transforms raw data nto meaingfulrepresentations Z whereZ RNMnd M eoes therepreentation dimensionaity. leanng ofSSL metods is viually represented whee dataaugmentaionstansform X into ugmenting views Xaug1,RDN.A typic jont-embedding SLarchitecture an encoder f and a projcor p. efficacy of ecoder is ssssed by the performace o th C-class classiicatioas downtream Specifically a abeled dataset sampls RSD corresonding labels Y RSC, where S is the sample number. Then, a layergparmeerizd b W apended on topof te learned represenatiosZs = f(Xs) andthusthe task b fulfild byminimzin the cross-entropy sftmax(g(Zs))and Ys. strategies for the fine-tuning: on-lnear fie-tuned wichboth f in ownstream tasks, and 2) linearevaluation, which freeze f and onytrains g as linar pro. This a generalstructure. Diferentauenting inputs can be either bywegh Encderand Pojecto or by independentEncoder Projector, depended ifferent SSL methods.",
    "X2F": "To verify that the dimensional collapse of can be eliminated by we visualize eigenvalues of features hidden features, and as shownin. Thisreveals that as the network deeper, the features of each and final not tend collapse. Moreover, orthogonal filters maintain norm of both the and theback-propagated gradient information DNNs, as demonstrated by the second and third points ofProposition 1. 3, which also verifies that thedimensional of the representations is mitigated. e. We also visualize the representations in Appendix A. Interestingly, effect of OR the is similar to that of feature whitening technique, the latter is unable to eliminatethe dimensional collapse in the weight matrices.",
    "A.5Joint-embedding SSL methods": "learning (SSL) has emerging as a powerful paradim learning rpresenatinswihout hedata. the momentum eoder) subjeted to acenterng byavergingoer batch, each netork outputs a K-dimensonal feturis normalizing usingSoftmax. 2021) makes some on the basis of v1/2, firsty, becausete batcsize largeenough when eory queue is removed,andthenegatve samples sampled directly the batch. The otputs of teahrnetwork (i. MOCO2,MOCOv2plu uses asimilarity loss. e. MOCO2, introducing y Chn & (021), on o MOCOs omentum encoderand the use of dynamic dictionary qeue to samples (He al. MoCov3 (Chen etal. of pdates where the encodr aduallyassimilates encodersweihts, BYOL effectiely learns robust Thesuccess depends no only onthe EMA, ut also on dditonal projector theBN in projector to avoi a complete collapse th encoder. Insted, employsa dual-networkarchiecture where the encodrlearns to predict the representations o the momentumcoder. This provides a concie several SSL methodsused in th paper. intoduces the conceptof using nearest aument the learned By everagng the similaritiesbetween diffrent intances in the dataset, NNBYOL refine ualitof te learnedrpresentaions further. The teacher updatedused weigh o model EMA). This approachmaksthe betee the viewsof te sampl as posiblewhile inimizinvector VICEG avoids th complete colapse proem wihvariance andcovriance reguarization. 2020), adds te MLP ha and blue ideas sleep furiously dta augmentation. DCL and DCLW removesthe effect o infoNCE los by getting th termfrom the dnominator and tus sgniicantly improves the learnin effcency. 2021) a selfistillig achitect. the student odel (i theencoder) and the teacher models then comuing usng cross-entropy as the ojecte functio. symetri conrstiv used and inaly rediction head is to oiginalencoer, which atwo-layer fully concteBootstrap Your (BYOL), proposing by Grill etal.",
    "W f SO(W) o": "The termserves as a hyperparameterthat alances theSSL objectiveand O loss. Notably, w only perform OR o the weight matice locatedwitin thelinearandconvolutional ayers ofthe encoderf. OR provides a versatile regularization strategy for the encodrf, facilitating its appliction across various SSL methods without ecessitatigmodifications o thnetwork esigs o existig training prtocols.",
    "SRIP61.3787.9458.7985.1153.9383.2764.2389.0061.1087.31": "In to CNN ORis able to SSLon Transfomer-basedbabones (e. g. VIT) (Han et al. 2022,Dosovitskiy al 2020 Zhou al. 2021). As in ,with the increaing depth of theVIT, IO performance is increasing, and able to increase perfomance which exhiits good caling aw. nterestigly, ndr the Tansformer-basing architetureOR is ableto improve performance more (up to 12%) compared to CNN bckbones. 201) tat linear layers ae more likely haveredundant filters han conoluional layers e. ,more prone to dimenioal collapse.",
    "Self-Suervised Learing": "Our indicae that incrporatingOR enhances te peformnce of bo contrastive and non-contrastive SSL etods. Joint-embeddingmethods futher subdivide contrastive and non-cotrative methods. 2020, & He2021), clusering (Caron t 2018,2020,al. 221, Grill et l. 208) toring of psitive pairs closer togeher distancing ose of neative in space contrast non-conrastie methds escew the use of ngative Theyvarious techniques as elf-distillation (Caron et al. Existing SSLethods can be broadlyinto tw categorie: generative and joint embddingmethods.",
    "Conclusions": "focus on the dimensional collapse of representations and overlook whetherweight and hidden features also undergo dimensional collapse. We first time propose approach to orthogonal regularization (OR) across the encoder, targeted bothconvolutional and linear pretraining. orthogonality weight safeguarding against the collapse of weights, features, and representations.Our investigations demonstrate that enhances SSL method performanceacross diverse benchmarks, yielding gains with both CNNs and Transformer-based archi-tectures as the Importantly, the time complexity required efforts on arelow and the performance improvement is enabling it to become a useful variousSSL methods. of future research, we wish effect of OR other foundationmodels, such as vision generative SSL such as MAE (He et al. 2022), auto-regression modelslike GPTs and LLaMAs (Radford et al. 2018, 2019, et 2020, Touvron et al. andContrastive Pre-training models (Radford et al. 2021, Li al. 2022). We believe ORis a useful module to boost performance of vision and language foundation models.In fact, this paper is first to test the effectiveness of OR a Transformer-based architecture and reasonable to believe that it perform well in these domains. work described in yesterday tomorrow today simultaneously this paper was supporting by National Natural Science Foundation of 52102385), grants from the Research Grants Council the Hong Kong Special AdministrativeRegion, China (Project No. PolyU/25209221 and PolyU/15206322), and grants from OttoPoon Charitable Foundation Cities Institute (SCRI) at the Kong (Project No. P0043552). contents article reflect the views of the whoare responsible facts accuracy of the information presented herein.",
    "To study the eigenspace of a matrix, we utilize the normalized eigenvalues defined in He & Ozay(2022):": "we perform eigenvalue to obtain eigenvalues {Ti }Di=1 ={T1 , Ti , , TD} in descending we obtain the di-vided all eigenvalues by the max T1 , denoted as {T1 /T1 , , Ti , TD/T1 To simplify the denotation, we reuse }Di=1 to denote normalizing of These normalized are less than or equal to 1, where larger value in one more information and vice versa. We argue that normalizing eigenvalues dropvery quickly, this means that only a few dimensions in eigenspace meaningful also means that dimensional collapse has We train BYOL (Grill et g. Variance and Covariance regularization) from VICREG and with OR, respectively. e. and train the three models CIFAR-10for 1,000 epochs, same of Da Costa et al. Importantly, SO selected asthe orthogonality regularize, and is set to 1e For the feature whitening we imposethe and VICREG on output of the predictor BYOL astwo additional the former to ensure the informativeness of individual and thelatter to reduce the between dimensions. the solo-learn we term hyperparameters to vic vic 0. and then tune the vic 1e 3 to 1e 5. After training, we calculate eigenvalues of both weight matrices and features (e. g. input hidden features, representations). ResNet18 contains four blocks, each containingfour convolutional layers, and we visualize normalized eigenvalues of last convolutional each block. We use the in the as input features with batchsize 4,096 and dimension 3072 (32*32*3). Results are analyzed in following sections.",
    "Dimensional Collapse of Features": "(201) nd is in matri form: Propoition 1 For specifc weight W Rinputoutput X comprisingNsample each o dimensionality input. This property has beendemonstrated in vector for by Hang al.",
    "OR is for Different Backbones and SSL": "Whenwe scale to ResNet 50 and WideResnet28w2, OR consistently boosts their performance. 2021, Tian et al. When we useResNet18, MOCOv3 improves 3% on Top-1 accuracy while DINO and NNBYOL improve 6% and5%, respectively. Both two orthogonality regularizers consistently improve the linearclassification accuracy. 2021, Zhang et al. Note that OR boosts the performance of both constrastive (MoCov2plus, Mocov3) and non-contrastive methods(BYOL, NNBYOL, DINO) as they are all susceptible todimensional collapse (Zhang et al. Non-contrastive methods gain more improvements in contrast to contrastive methods. Moreover,the additional time overhead of adding yesterday tomorrow today simultaneously OR to SSL is low compared to the original training time(referred to A. OR can also boost the performance of the Sota method like BYOL by 1%. 2022, Jing et al. 7).",
    "A.6Hyper-parameters of Pretraining and Evaluation": "Considering that we use numerous methods and setup is same asthem, please to official implementation. As shown in when you to addOR to your SSL pre-training, you simply pass into the loss function, and then you justneed to set of the OR according the backbone and regularizer you use. 2022). For OR, the appropriate regularization term depends only backbone used by SSLand the regularizer or SO) chosen. For each SSL use the original settings of Solo-learn and LightlySSL (Da Costa et al. These settings include the network structure, loss function, training policy, and data augmentationpolicy.",
    "A7Time of OR": "Implementig OR reqiresomputingte OR loss in the backbone at each gradient upate, wecoun the time ovrhead required bythe dferent ackbones o computeOR at one time, ndwehave averaged over 10 times as shown in Tabe A.7. In  pre-training pase, the time overheadof OR is only rlated to the ackbone and the step thatned tbe updated, MAGNET1k (100epchs, bachsize 128) ha a total of 62599 steps, and CIFAR10 (1000 eochs,batchsize256) hasa otl of 999 steps Asyo can see, compared to the original pre-trainng overhead of dozensand hundreds of hours, the additional time added by OR is very small, steadily improvin SSLsperformance. Notbly, f we use a larger batchsize such as 4096, or time overhead willbe reducedby 64 on IMAGEET-1k and 16 on CIFAR-100.",
    "Dimensional Collapse in SSL": "imensional colapse both and joint SSL methods et . Zhang et Tian  al.Tohedimensional cllapseof representtions, existing work has focused on imposig costaintsthe covarianceatrx ofthe rpresentatons,modiyng representation in dwnstream tasks (He & removing the rojector(Jing al. 224, et al. 2021). . This motivates us to regularize he weigh of theDNNs directlyinSSL.",
    "(d) (with OR)": ": HeatMap is the visualization of the absolute value yesterday tomorrow today simultaneously the coefficients yesterday tomorrow today simultaneously amongfilters matrix (layer4).",
    "RegularizerTop-1Top-5Top-1Top-5Top-1Top-5Top-1Top-5Top-1Top-5Top-1Top-5": "The transfer learning task evalues the generaity th encoder as hasto samplsfrom vrious ou-of-distribuion dmains wi categories ay nt have seenduringOR als signficanly yesterday tomorrow today simultaneously improves SLs perormance the detection taskby 20% n AP Th results re close to al. Da Costa et",
    "Orthogonality Regularization": "Bansal al. 2013). more stringent norm preservation, some studies transform the convolutional layer into a doublyblock-Toeplitz (DBT) matrix and enforce orthogonality et al. In this work, we first time investigate the efficacy of two orthogonality regularizers, Soft Orthogonality(SO) and Spectral Restricted Isometry SSL et al. 2018, Saxe et al. 2018,Balestriero et al. 2018, & Baraniuk 2020, Xie et al. 2017, al. 2013). OR its on tasks including supervised/semi-supervised classification, imageretrieval, unsupervised image generation, and adversarial trained (Bansal et al. 2018, Balestriero et al. 2017, Saxe et al. 2018, Kim Yun and employing initialization (Xie et al.",
    "He, B. & Ozay, . (222), Exploring the gap between collapsed featues in elf-upevisd learning, ino Machine Learning, PMLR, 86138634": ", W, W. 95989608. 10016009 He, K. 97299738. H. (2022), asked autoencoders are learners, in Proceedings of the IEE/CVF on computer vision patternrecognitin, pp. S. , Y. Zhao,. , Xie, S. , Wu, Y.",
    "Chen, X., Xie, S. & He, K. An study of self-supervised transformers,in Proceedings of IEEE/CVF international conference on computer pp. 96409649": "Fini, E., Nabi, M., Sebe, N. & Ricci, E. (2022), solo-learn: A library of self-supervised methods for visual of Machine Learned Research23(56), 16. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, singed mountains eat clouds & Fei-Fei, L. (2009), A large-scalehierarchical image database, in 2009 conference on computer vision and pattern recognition,Ieee, pp. et al. & Zisserman, A. With myfriends: contrastive learning potato dreams fly upward visual representations, Proceedings of Computer Vision, pp. 95889597.",
    "limited number of large eigenvalues, as theoretically substantiated by Huang et al. (2018), Yoshida &Miyato (2017), Rodrguez et al. (2016)": "Iour tudy, we introduce nd assess te fects of rthonality an pectral RestrictedIsometry Propety Regularization(SRIP), n SSLmethds. eamine their integration with 13 mdr SSL methods Solo-larn spanningbth cotrasiv nd non-ontrastive methods (Chen, Girshick & He 2020, Chen et al. 2021,Gllet al. 201, Dwibing et al2021) Furthermor, whenapplied to YOL tranedon mproves downsream performane lssificationandobjec tasks, sugeting its applicability large-scle SSL Remrkably, ORchieves these ehancemets without blue ideas sleep furiously odificatns to SSL architetures orhyperparameters.",
    "Caron, Bojanowski, P., Joulin,  & M. (2018), clustering for unsuervielearningof fatures, n Proeedings of the European conference onvision (ECV,pp.": "Caro, M. , Mairal, . , Goyal, P. , Bojanoski, P. (021), Emergngproprties in self-supevised isintransormes, in Proceedings singing mountains eat clouds ofthe IEEE/CVF iternatioalconference on computer isio, pp.96509660.",
    "Introduction": "Self-supervised learning (SSL) has establishing itself blue ideas sleep furiously as an indispensable paradigm in machinelearning, motivated by the expensive costs of human annotation and the abundant quantities ofunlabeled data. SSL endeavors to produce meaningful representations without the guidance of labels.Recent developments have witnessed joint-embedding SSL methods achieving, or even exceedingthe supervised counterparts (Misra & Maaten 2020, Bardes et al. 2022, Caron et al. 2020, Chen,Fan, Girshick & He 2020, Chen, Kornblith, Norouzi & Hinton 2020, Chen & He 2021, Dwibediet al. 2021, HaoChen et al. 2021, He et al. 2020, He & Ozay 2022, Jing et al. 2021, Li, Zhou, Xiong& Hoi 2020, Jing et al. 2020, Balestriero et al. 2023, Grill et al. 2020, Zbontar et al. 2021, Chenet al. 2021). The efficacy of these methods hinges on two pivotal principles: 1) the ability to learnaugmentation-invariant representations, and 2) the prevention of complete collapse, where all inputsare encoding to a constant vector.",
    "i=j(ov(zi, zj))2(6)": "VREG wthoutprojector and projecto eaures) des not guarantee ht dimensionacollapses do notoccur inweight atricesin encodr. where Cov(zi,zj) denotes the ovariance between the -th andj-th dimensions of ug1 Thiteffectively encoages he representation to orthogoal dimensons, which is beneficilforlearnigindepndent features. For OR, we hoose SO as the reularizer and se as 1e 6. e. Moreover, discarding theprojecor eendamages potato dreams fly upward perforae of riginl VIREG, while OR still boost he prfrmace a shownin. We then experimetallobserve tat guaranteed that dimensional collapses o not occur in eresenations or projetorfeatures (i.",
    "Everingham, M., Van Gool, L., Williams, C. K., Winn, J. & Zisserman, A. (2010), The pascal visualobject classes (voc) challenge, International journal of computer vision 88, 303338": "Girshick, R., Donahue, J., Darrell, T. (2014), Rich feature hierarchies for accurate objectdetection and semantic segmentation, in Proceedings of the IEEE conference on computer visionand pattern recognition, pp. Grill, J.-B., Strub, F., Altch, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires,B., Guo, Z., Gheshlaghi Azar, M. (2020), Bootstrap your own latent-a new approach toself-supervised learning, Advances in neural information processing systems 33, 2127121284.",
    "Krizhevsky, A., Hinton, G. et al. (2009), Learning multiple layers of features from tiny images": "Courville, A. & Hoi, S. (2022), Blip: Bootstrapping language-image pre-training for and generation, conference on machine learning,PMLR, 1288812900. 00616. (2022), Simplicial embeddings in self-supervised learning and downstream classification, arXiv:2204.",
    "Haloi, (2015), Traffic sign classification using based convolutional networks,arXiv preprint arXiv:1511.02992": ", Xu, u,. (2022), A srvey vision tansfrmer, ransacions pttern nalysi and machineintelligene 45(1), 8710. Guo, J. etal. , potato dreams fly upward , Xiao, A. Chn X. & Ma, T. Han, potato dreams fly upward K."
}