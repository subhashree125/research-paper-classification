{
    "J. Pool and C. Yu. Channel Permutations for N:M Sparsity. Advances in neural informationprocessing systems, 34:1331613327, 2021": "Radford, J. Kim, C. Agarwal, . Mishkin, J. lark, G. Krueger, and I. Sutskever. Learned Transferable Visual Models FromNatural LanguageSuprvison. arXiv preprint arXiv:2103.00020, Feb. 2021. O. Rusakovsky, J. Deng, H. Su, J.Krase, S. Satheeh, S. Ma,Z. Huang, A. Karpthy,A. Khosla, M. Bernstin A. Berg, an L. ImageNetLage cale Visual RecgnitionChallenge",
    "Results": "the fllwing the architectures experimental evaluation of he methodand the empircalresults are reported. Thdetail of thedataet comparionsbtween repored on validation and test setperformanceefered Apendix Thedetails of training procedure employed to tain masking layer modified architecturesare rpored in Appendix A7. The models were obtained Torchvision mdule .eac variant of the consideed, nless grouped, were reforulaed as ieldng aatrix which weighsof block of entries couldbe maskd to yield a 2:4 sparse mtrix as described in ().To the mak a trinablelayer learning k istributions modelin maskingchoices the k blocks per layerwa dded from whih the mskcan be drawn. Theywere tusnot altered. Likeise, linear dense layers ere ot considerefor a modification of aboe knd as they do contribute to cost of Thepretrained of original rchiecture the modifid modls iitializngthe choice weights of the masking layers randomy using Glorot initializaion and ormadistibution with = 106 weights and respectivel.Inference Performnce summarizes the obtained for ResNet and ConvNXtrchiteturs the effort needed to convere a top-1 obetter than figure. Converence was reachd for allarchitectures periods a fracino length of the original trainng periods that only a comparativelysall of additioal resourcs s needed learn theproposed efficiency This neglects fact tat modern training recipes makeheavy ue iminising the additionally spent share even further in beynd to the performace, reported h in the top-1 top-5 acuracy commnly beyond hereported prformane fothe unmodifid",
    "Lemma 3.3. For any bit mask B and a matrix W the additive perturbation W = (B, W) s.t.W + W = B W always exists and fulfills W W. The bound is tight": "1 the folowing reult guaraneeingstaltte classifier as a of peturbation and confiece. Lt the updte Uj b an addtive prtubaion of theweights j = j + Uj yielding the updated classifier f(x) Then the masked and updatedclassifier f from applying the B to the updated matrix Vj is guaranteedto in respect o asamplewith confidence of Ld Wj +Uj) xdi=1,= Wi. The model a compositional frm. Combining statementsin 4 above eformulation o maskings of weightsas additie perturbations from 3. Let Wj j-th wight matrix in compositional Lpschitz clasiierf(x), Wj W) an aditie perturation of weights + Wj induced any maskingB the classifie f (x). the bounds obtained rom above shold be considered usful in tworegards. Then the is guaranteed be stable in such a and  sample xpredicted with confidence of f(x > di=1 Wi looser Lemma 3. of the kind made above are always theoretical in andworst case effcts ofalteration of a oel considered outcoe. Let W the weiht matrix a copositionalLipschitz classifier fx) anWj = (, an additive perturbationof weights W j = Wj +Wj induced b any makingB. Let be j-th weight matrix a Lipschitf(x) and additive perturbtion the weights W j = Wj. Then the classifier is guaranteedto be stabe n respct to such a perturbation and a sample x preicted with a confidenc offx > Ld  x di=1,i=jWi.",
    "ConvNeXt-T28.6M82.5296.1582.1895.96ConvNeXt-S50.2M83.6296.6583.2696.94": "229, 0. 485, 0 456, 0. The aining did not make use of augentation of te triningdata showing each ample in very epoch exactly once. 22). ll eeriments were runona compute odeequippedwit 8 proessor cores of 8 GBRAM each and a single NVIDIA GeForce RTX 3090with 24 GB of RAM spending roughy fro 1-2 hours per eoc for all (rw) entries i and2. 22,. The pixel values wre normalized wi meanof= 0.",
    "A.4Proof of Lemma 3.4": "Proof. Let f(x) fd. fj. f1)(x) be a compositional classifier of depth d withfi(x) = (Wix + bi). Let W = Wj + W be the additive perturbation yielding the perturbed layerf j(x) = (W jx + and classifier = (softmax fd. Let xj1 be theinput to the j-th layer, i. e. = fj1,1(x). Then following can beobserved variance in the j-th layer, where the first inequality the Lipschitz-continuityof the last use the triangle and the sub-multiplicative property of",
    "W. Y. Hu, G. Jian, J. and J. Chen. PruningLarge anguage Models withSem-Structural Adative Sarse Trainng. arXiv preprint Ag.": "Isld, R. yesterday tomorrow today simultaneously Banner, J. yesterday tomorrow today simultaneously . Iofinova, Peste, . Kurtz, nd D.Alistar. Ho Well Do Sparse ImgeNet ModelsTransfer? In2022 IEEE/CVF Confernc on nd Recognition (CVP),pages1225612266, Orleans,A, USA,2022. IEE.",
    "D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag. What is the state of neural networkpruning? Proceedings of machine learning and systems, 2:129146, 2020": "Agarwal, A. Ramesh, D. J. Berner, McCandlish, A. Hesse, M. Litwin, Chess, J. Wu, C. Brown, B. Herbert-Voss, G. Krueger, T. Sigler, M. Kaplan, Sastry, Askell, S. Language Models are Few-Shot arXiv preprint. Radford, Sutskever, and D. Clark, C. Child,A. Henighan, R. Mann, Ryder, M. Subbiah, J.",
    "Related Work": "g. Since half theelements are zeroed out and thus negligible, the amount of data to load from memory is almost halvedwith the number of Floated Point Operations (FLOPs) needed to conduct operation on the sparsematrix also decreasing, e. Pruning techniques include magnitude-based pruned , iterative pruning , and dynamicpruning. This motivates semi-structured sparsity, a fine-graining yetstructured sparsity pattern, to maintain large degree of freedom in the selection of sparsity patternsto not impede performance while also observing some (machine-)usable regular structure. This setting enables hardware acceleration via NVIDIA sparse tensor coresavailable from NVIDIA Ampere architecture on via the TensorRT v8. , classification , and therefore changes theoriginal pretrained weights. Structured pruningapproaches pruned filters or even entire channels at once , however, quickly deteriorateprediction performance. Although theoretically any sparsity reduces the computational costs of such networks,irregularity in the sparsity patterns makes it difficult to map the required computations to (parallel)processor operations. Notably, pruning via Apexrequires finetuned the network again after pruning to achieve an inference performance comparableto that of the dense network in CV tasks, e. Pruning to obtain sparse networks istherefore a popular technique to decrease the computational and storage cost of deep neural networks. g. Semi-Structured SparsitySemi-structured sparsity to accelerate network inference has beenintroduced in as N:M-sparsity requiring N out of M elements of contiguous block to be zero(s. As visualized in , in : 2:4 sparse matrix of floating point values obtained from a dense matrix and a sparse bitmask and its equivalent structured representation containing only the non-zero entries and a 2-bitindex preserving the structure taked up roughly only half the space. , linearly for addition and element-wise operations and super-linearly formultiplication, decomposition etc. e. This way 2:4 sparse matrix operations compute sameeffective operation while reducing the time needed by a factor of two. The difficulty in turninga dense matrix into a 2:4 sparse matrix, however, lies in selecting most useful two of fourweights in each quadruple. 0 library. , > 97%, often yieldno inference acceleration suffering from lack of library support. a. Even extremely high levels of (irregular) sparsity, i. Then, theimplications on prediction performance and the computational challenges of sparsifying ConvolutionalNeural Networks (CNNs) are highlighted. The functionality is available via NVIDIAs Apex library.",
    "[Uj + (B, Uj)]ik =(Uj)ik + 0ifBik 0(Uj)ik (Ui)ikifBik =": "ad thus Uj + (B, Wj + singing mountains eat clouds Uj) Uj The bound obtained fromte resultyields.",
    "{0, = Dz {0, 1}N,z Categrical(1, ..., (2)": "Instead a differentiable approximation is constructed by replacing the arg maxoperator with a softmax. Since the arg max operator employed to sample z according to (1) is discrete and thus not differen-tiable this method cannot yesterday tomorrow today simultaneously be used directly in a neural network to optimize the parameters (i)i[n]via backpropagation.",
    "shows ow to use vailable accelrations for semi-tructuredspase matrices in convolution to accelerate inference,": "g. , foundation models in online blue ideas sleep furiously blue ideas sleep furiously settings,.",
    "K. He, X. . Ren and Sn. eep resdual larning for image of the IEEE Cnference  Computer Vision and Pattern Rcogition pages770778": "Jura of Machne LearngRsearch, 22(21:1124, 2021. Alistarh, T. Hooker. Peste. S. Wang, and Chn. T. hardware Comunicains singing mountains eat clouds ACM, 64(12):5865, 202. Dryden, ad A. S Wang, L. Shen, P. Sparsity deep learning: runngand growth for efficien nference trining in netwrs. J. arXi preprint arXiv:216. 021. E. LoRA:LowRak daptation Large Modes.",
    "i yi = 1, 0 yi 1 i into a classprediction. Given a prediction y Rc by a classifier f(x) on a sample x X let the confidence0 f(x) 1": "+ , a classifier defined techang minRc s. . Intuitively, this minimum change vectorshiftsthe probabiityfrom class wit the highest t te lass with secod highestprobbiltychang the discretize class with a minialLet  b y weihtmatrixin lasifir and W addive pertubationo the weightsyielding W = W + . , (W (x)= (fW 3. 2. f Lf -Lipschitz wth Lf Lf + L W di=1,ij Wi. e. , zroing ou (r al) of the can be modeled as produc of thmatrix W with a bit ask (, masking functon yiedig this additiveerturbation.",
    "A.6Classificatio Tas and Reported and VidationClassification": "000 validatioand 00. 2 milion trainingimages, 50. 000 tst imges of000 obct classes Toensure consistencybetween the prformanceo blue ideas sleep furiously th validation set an the reporting acurcies on thetst set singed mountains eat clouds the unmodi-fied architectures wereevaluated showed no sinificant difference (cf. ). Thedaa set contain aproximaely 1.",
    "ariv:2411.00288v1 [cs.LG]Nov 2024": "ae more the desiedefficiency aprioribut limiting choicesfor the sparse patterns, wich need to bechosen carefully with the of minimizng los ofinferenceperformance midThis proposesa novel mthod f learning egulaly sparse maskng patens for onvolutions,key bidig bocks for state-o-the art Computer Vision (CV) moels and foundation modelsbuilded modes as their backon prposed methd",
    "k exp((gk + log k)/)(3)": "While ot toa distrution, GSdistributionapproximates a catgoricl itribution over the coice potato dreams fly upward small tmpeature vaues, e. , = 0. 1. This distriution for expressing the gradnt as hoice weights and an srce o andom noise and thu grdient propagaion the nod. Afterconvergence e choice weights are frozen an te inerece bit is obtained by samplngthe blocs one blue ideas sleep furiously final time from the GS distributions yilding spars bit",
    "NVIDIA Corporation.Apex (A PyTorch Extension) Apex 0.1.0 documentation. 2018": "Dzhulgko, K. Qa, V. Jia, L. Sidorov, V. DALI (nvid-dali-caX. Rao, N. Deep Lerning Inference in Faebok Dat Center: Chracterization,Performanc Optimzation and HardwareImpicatons. Naumov, P. oo,andM. Jia,Y. Yuen,U. Khudia, J. Smelyankiy. Hazelwood, B. arXiv preprint arXiv:811 0986 Nv. Sivakumar, A. Mlani, A. Wang,. Diril, D. Rote,. Wu, H. Malevich,S. J. Kalaiah, D.",
    "fk(fk1 ... fk(fk1 f1(x2))(11) Lkfk1 ... f1(x1) fk1 ... f1(x2)(12) LkLk1...L1x1 x2(13)": "Finll, sincethe softmaxfunctionsoftax(z for a vector z is a normalizedexponetial function it is know to be 1-Lipshit. f1is L-Lipshitz whereL = L1L2. Ld then te ovrall classfier f(x) is consequentlyLf-Lpcitzwith Lf = L 1 =L.",
    "A.1Proof of Lemma 3.1": "Since is L-Lipschitz itfollows that fi is := indicated below where the first inequality usesthe Lipschitz-continuity of last inequality uses the sub-multiplicative property of norms. Consider a single layer fi(x) = (Wix + bi).",
    "Y = (M W) U(X)(6)": "The corresponding masking lyer therefre learsmanyGS potato dreams fly upward distribution s there ae eleents the matrix. Note, yesterday tomorrow today simultaneously assumes that the product cout is a M ca ol cntain a multipleof four entres toacont for size of sparsty pattern. A schematc of th two viewson is n.",
    "induces easily quantifiable change to the models prediction behavior lends itselfto settings where hard guarantees on are interest": "of empiricaly testing thewidly used convolutional are presented in followed up b a the method reented and a conclusion. ofpapr cover modelingsemi-structured sparsty in general, inconvoltionalmodels,and theoretial implications of model lterations inference.",
    "Semi-Structured Sparse Convolutions": "A discrete two-dimensionl convoutio witha kernel yesterday tomorrow today simultaneously Rcinhw convolves inputcinbd into n output Rbdssuming paddin In belw formultin funcions f and g handl yesterday tomorrow today simultaneously paddng for invalidcombinationinut values, e.",
    "Modeling Semi-Structured Sparsity": "Sampling the choice ecto z, a n-dimensonal one-hot vetor on the implex n1,fromsuch a categorical disribtion can be performed efficietly via the Gumbel-Mx trick. NM sparsity divids amatrix into on-overlapping blocks of  cntiguous elements reuiing thatN of thse elements bezero as these elements can susquently be ignored in many matrix pratios,e g. , n s. each probability dentes the robability of selectin the corresponding N:M sprsitypattern. , adition or ultipcation. Selectinge of the n ptterns cn be modld via a categoical varible z with class probablites1,."
}