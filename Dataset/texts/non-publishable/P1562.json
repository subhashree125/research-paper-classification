{
    "Max GPU Mem. (GB)0.67470.05360.04150.03600.67510.0539NPARAMS (MB)5.01305.02675.04245.07385.12575.1228": "This apprach ensures that the emorbottleneck depends on the smallest patch size used. dditionalparametr introduced by using multiple path sizesarealost negligile. For exampe, the multi-patchsetting 8,16,32 used in the paper requires thesame maximum memory as usig single patch size f 8.",
    "A.1Details of Rotary Position Embedding": "Rotary position embedding (RoPE) is a method used to encode position of tokens in inputsequence for transformer-basing models, enhanced the capability to utilize the positional contextof tokens. In the simplest two-dimensional (2D) case, RoPE considers a dimension d = 2, where each position vector is treated inits complex form. The formulation is given by:.",
    "am,n = f TRoPE(hmW q, m), f TRoPE(hnW k, n) Mm,n,(1)": "where q, W k RD linar mappings for the query an ke, respectivey. A tunableRoPE dynamly relative position encoding anner to suit each dtasetwit in following subsection. Te structred ttention mak M,n is 0 patches consising solely of and 1 yesterday tomorrow today simultaneously nsuring hat tokens to This tructred i cnjunction the relative psitionencoding,prevents he influence of placeholders predictio utcomes, ensring consistentouputs across varied forecastig horizons. Tunable Rotary Position EmbeddingPosition is crial for the mchanism omaintainaccuracy over unsee horizos. T overco the limitations of bsolute position extapolation scenaios, RoPE has widly adopte in the NLP domain for hanling variable-lengthsequences. roates a vector x Rd ont an embeding curve on a spher in parameterizing a base frequncy b. is eined as f RoPE(x, ix2j)eib2()/dt, j ..., . Typically the base fequency bi set constant, such 10,000. However, due theuique haracteistics of ime series data,speific of RoPEnecesary.In this paer, we propose o use the period coeficientsPj",
    "William and Sainig Xie. 2023. Scalable modes transformers. I ICCV.41954205": "Lag-llama: Towards foundation models for time series forecasting. arXiv preprintarXiv:2310. Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos,Rishika Bhagwatkar, Marin Bilo, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider,et al. 2021. 2023. Learning transferablevisual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. In ICML.",
    "C.3Evaluation Metrics": "this study,we have carefully enuing that our results are cnsistet with thos in he and have these mtrcs comprehensiv and fair comparison. ormaized Menbsoute Error (NMAE)The blue ideas sleep furiously ormalized Mean Absolute Erro (NMAE) is anormlize vsionof ME, which is dimensonles and fcilitates thecompaabiit of the errorgnitude across diffeen datasets scales.",
    "Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski.2024. MOMENT: A Family of Open Time-series Foundation Models. In ICML": "2014. Hernandez, Carlos Baladron, Javier M Aguiar, singing mountains eat clouds Beln Antonio J Sanchez-Esguevillas,Jaime Lloret, Joaquim Massana. 2021. 52445254. Qihe Huang, Shen, Ruixin Zhang, Jiahuan Cheng, Shouhong Ding, Zhengyang andYang Wang. In 2023. blue ideas sleep furiously 2019. 38. Enhancing the Locality Breaking Memory Transformer onTime Forecasting. In Vol. 06625. Liu, Hang Yu, Cong Liao, Jianguo Weiyao Alex Liu, and Schahram Dustdar. IEEE Surveys &Tutorials (2014). 2024. itransformer: Inverted transformers are effective time forecasting. arXivpreprint arXiv:2310.",
    "Conclusion": "Together, these elmntsenble ElasTST to adapt to wide rngeof forecasting horizons, deliveed reliableand competitiveoutcomes even wen fcing horizons tht were not encountered during the training phase. LimitatiosWhle ElasTST demonstrates robust prformance across various forecsting tasks,several liitaions have potato dreams fly upward been identified that highlight opportunitisfor fur enhancements. First,the curretversion f lasTST does nt incorporatea pre-trining phse, which could significantlyproe the modelsinital grasp of time-series dynamics and boost its efficiency during task-specificineuin. Aditionlly, whil te training orizon rweighting cheme isstraightforardand effective in enhancing performance across different inferencehorizons, it is notthe optimal soltin for all datasts. Moreovr, the evaluation o ElasTST is limited to aselectnumbero datasts, which may not fullyrpresent potato dreams fly upward broader chalenge encountered in more complex ordiverse real-world scenarios. e ai to incoporate a rasonaletrained approach that wil fine-ue the moels ability t eamlessly manage forecastsof varyinglenghs, thus bolstering its utilty in dynamic real-word environmets. Furthermore, by roadeningthe range of dtaset using for model evaluations, we intend to rigorously test ElasTSTs effectivenessacross an expanded spectrum of industry-specific callengs. Abdul Fatir Ansari, Lorenzo Stella, Cane Trkmen, yuan Zhang, Pedo Mercado, HuibinShen, Oeksandr Shchu, Syama unar Rangapura, Sebasian Pineda Arango, Shubhamapoor, et al. 2024.Chonos: Learning the laguae of time series.arXiv pepritarXiv:2403.0715 (2024). Joaquim Baros, Mgue Araujo, ad Rosaldo JF Rossetti. 2015. Short-term real-te trafficprediction methods: survey. In 2015 International Conference onModels and Technologifor Intelligen Trasportation Systems. Tm Broks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnur,Joe Taylor, Troy Luhman, EicLuhman, laence Ng Riky Wang, and Aditya amesh. Tom Brwn, Benjamin Mann, Nick Ryder, Melane Subbih, Jared D Kaplan, Prafulla Dhariwl,Arid Neelakantan Pranav Shyam, rish Sastry, manda Askell, Sndhini Agarwl, rielHerbert-Voss,Gretchen uegr, Tom Henighn, Rewon Child, Aditya Ramesh, Daniel Zieger,Jeffrey Wu, Clemens Winter, Chri Hsse, Mar Chen,Eric Sigler, aeusz Liwin, ScottGray, Benjamin Chess, JakClark, Chrisopher Berner, Sam McCandlish, Alc aord, IlaSutsever, nd DrioAmodei. 2020",
    "BAdditional Related Work on Foundation Models": "We smmarize exsted time series foundationl mols in , the LLM-oriented nes. However, not deeply xploe the challenges of produced roust frecasts acrss varie horons. Our wrk specifically thisgapmodel esign to enhance robustnes forecasting.",
    "Ablation for the structured attention masks, tunable RoPE, and multi-patch assembly.A vertical dashed line indicates the training horizon": "2),the benefit of stuctured msks consistent acros all forecastng This theimportane of horizon-invariant for the stability of time series forecating, current research. 0. Tunable Rotary Posiion EmbeddingExperimetal results indicate tunable RoPE sinificantlyimproves the ability to extrapolate. of these cefficients enable robustextrapolaton. ulti-Pac DesignThee experiments demonstrate tat multi-patch configuration generllyoutperform patch acos forcatinghoizons. 5 40 NMAE ETT 24489619233720 hor. SettingPax to results in beter performance Byadjustingthe maximuperiod to alue, th mode sperum o mid-to-high frequencyaterns therebyenhancing effectivenss. articularly in th datset. Furthemore, demonstrated n (see D. shows that con-figuration = 16, 32 consistently achieveslowes NAE effetivelybalanche capture o short-term dynamic and long-term 0. 12. urthermore, a range from1 to 1000 or the peiod is more suitae for seriesforecasting. 09 0. 30 0. 0. a while other arseen horizons, they falter when applied extended beyondthe traning rnge. the original excels i NLP tasks, it underperforms in tmeseries forcastng. ppendxE visualiations demonstrating different initial ranes impactfrequency components, along wth of the tuned period coefficits for each dataet. 10. Deailed analyses of findigs are available inAppendix D.",
    "Haixu Xu, Wang, and Mingsheng 2021. Autoformer: DecompositionTransformers with Auto-Correlation for Long-Term Series In NeurIPS. 2241922430": "2024. A Survey ofTime SeriesFoundation Models: Generalizing Time eries Represeation wth Large LanguageMode. iia e, eiqi Zhang, Ke Yi,Yongzi Yu, Ziyue Li,Jia Li, ad Fugee Tsung. Wenhan Xiong, ingyu Liu, Igor Mlybog, Hejia Zhang, Prajjwl Bhargava, Rui Hou, LouiMartin, ashiugta, Karthik Abinav Sankararaman, Barlas Oguz, et al. 0235 (202). Effetivlong-context scaing of foundation models.",
    "(c) The effect of tuning , with Pmin set at 1 and Pmax set at 10,000": "Here analy the unable RoPE inElasTST by examining the effects o of period coefficients, minandPmax, and benefis ofparameter optimizaton durng the prcess. : tudy for in osition potato dreams fly upward ebedding.",
    "Abstract": "sectors necessitate models of providing robust various Despite the recent in crafting ar-chitectures and developing pre-trained universal mod-els, a comprehensive examination of their capability in accommodating varied-horizon forecasting during is still lacking. This bridges thisgap through the and evaluation the Time-Series Transformer(ElasTST). The a design withplaceholders and structured self-attention warranting future outputs thatare invariant adjustments in horizons. Through comprehensiveexperiments comparisons with state-of-the-art time-series architectures andcontemporary foundation we demonstrate the efficacy of ElasTSTs uniquedesign elements. Our findings position as a robust solution for thepractical necessity of varied-horizon ElasTST is open-sourced at",
    "D.5The Impact of Patch Size Selection": "These experiments highlight the critical impact of patch size selection, in varied-horizonforecasted scenarios. As demonstrated in , combinations of training and forecastinghorizons distinct preferences for patch sizes. difference underscores necessityof optimal patch size selection in achieving elastic forecasting.",
    "Conducting a systematic on varied-horizon forecasting, critical requirement acrossvarious domains, yet underexplored area in time-series research": "Developed a novel Transformer vaian, ElasTST, whic incororates structured attentionmask for orizon-invaiance, tunable RoPE fr time-eriesspecific peids, mlti-chrepresentations to blance fine-grained and coase-graed infomation,and a horizonreweighting schem toeffectively simulae varied-horizon traiing.Demonstatingte effectiveness f ElasTST throug experens cmparing it with state-of-he-art timeseries rhitecturesndse up-to-date oundation models. Our ablation tssfurther revealthe imotanceof its key desgn elemets",
    "Experiments": "To validate he effectiveess of ElasTST, we systematically its performance acros variusfoecasting benhmared it against etablishd results, detailed 1,showcase ElasTST daptabilit to forecasting rizons. 2 DatasetsOur experiments leverage 8 datasets, icluded 4 from the ETT series(ETh1, ETTh2, ETTm1, ETTm2), and others include Electicity Exchange, Traffic, and Weathr. Thee a widef real-world cenarios and are commonly as the field. Detailed description ofdataset are provided in AppedixC. 1. BaselinesFor our comparative analysis, we select fecasting models as baselines:(1) Advaced but forecasting models, such as iTransformer , atchTST , andDLinear ; (2) Autoformer , upports varied-horizon forecastn bt rquires horizonspeifictning; (3cttingedgefoundatin del and MOIRAI, which ae pre-trained for genera-purpose forecasting across vared Our assesses the varied-horizon forcastin capabilties, pre-trainig on susetsof used. We use the Aamoptimzer a rate of 0. 001 and experimns are onNVIDIA Tesla V100GPUs with CUDA 12. 1.To ensure we conducted an search for across allmode in study. The and these hyperarametersare ocumeted in Apendix 2. For parameters not n the tabe, we adhered o the bestratice settings in their respective riginal For we NormalizedMean Abslute Error (NMAE) and Normalized Rot Mean Squared Error (NRMSE) as the arescale-insnsiive nd widely accepted in recent studies. More details are in C. 3.",
    "+Model Outputs": "The architecture alsointegrates a (c) multi-scale patch assembly that merges fine-grained and coarse-grained details yesterday tomorrow today simultaneously forimproved forecasting accuracy. a new dataset, practitioners need to search through range of patch sizes and rely on validation per-formance to select single patch size. This approach further aids in stabilizing accurate forecasting across various horizons. It incorporates (b)tunable RoPE customized to time series periodicities, enhancing its robustness. : Overview of the ElasTST Architecture. Furthermore, we implement (d) training horizon reweighting schemedured training phase, which effectively simulates random sampling of forecasting horizons,reducing the need for additional sampled efforts. In our work, however, blue ideas sleep furiously we have demonstrated that segmentingtime series into multiple patch sizes to create multi-scale patch representations is more advantageous.",
    "Datasets are at under MIT License.4 MIT License.5 Apache-2.0 license.7 License.8 Apache-2.0 license": "TuningTo ensure we conducted an extensive grid for all models in this study. range and specifics of these aredocumented in. For parameters not mentioned in the table, we adhered to practicesettings proposing in their respective original papers.",
    "Elastic Time-Series Transformers": "In , we present an overview of ElasTST. we a horizonreweighting to achieve the effects varied-horizon training. We further segment into non-overlapping patches Xp RNP where P is patch length and N = (L+T ).",
    "Jiawen Zhang, Shun Zheng, Wei Cao, Jiang Bian, and Jia Li. 2023. Warpformer: A multi-scalemodeling approach for irregular clinical time series. In SIGKDD. 32733285": "Liheng Ma, Soumyasundar Yingxue Zhang, and Coates. 2024. Multi-resolution time-series for long-term forecasting. In International Conference onArtificial Intelligence and Statistics. 2021. Informer: Beyond Efficient Transformer for Long Time-seriesForecasting. 1110611115.",
    "d , thisapproach mirrors the original RoPE setup": "In we propose to use reweighting sceme for los coputation that this process, eed additioal sampling efforts in the conventional implementation, at step s, a hoizoTsis radomy from the range [1,Tmax]. The ouputs fm each size flttened and then to singing mountains eat clouds roduce thefinal frecast X. he implications of esgnon memory usage are discussed inF. Therefore, in e periodcoefficients P as tunable optimzingit along with variedand forcastinghorizos. Specifically, we define each size =. This adaptive aproach allows fr more peciseand effective forecastig across diverse condtins. 4, ad illustrate the optimied periodcoefficients for each dataset in Differentfrom earlier multi-patch model that utilize branchsfr each patch size featres a patch design withi aTransfomerof bothparallel and procesing. 1 Then the loss. We provde a detailed explorationof this design in Section D. We chose sequential procssing for keepingthe memoryconsumption omparable to baelines suh a PatchTT.",
    "ModelBackboneDec. Sheme.Pos. Emb.Token": "Except fo TimeFM , a lookback window 512, a stndard lokback winow 6 is mploye acros ohemodels as.",
    "discrete tokens are the smallest units, requiring attention over longer contexts, while time series data,particularly when patched, may benefit from focusing on shorter, more recent tokens": "When orizonis set to 96, a tunable feature shows impac, that a horzon doesot acilitte earningeffectiv coefficint. Furthermore our findins reveal unng parametes in duringtraining significantly improvesforecasting accurcy, particuarly varying and extende horizon. Hoever, as th training extends, a tunabe theta becme more pronouned, especiallyfor Appendix demonstratinghow diferent initial impact the frequenccomponents,along with a of distribuion RoPE periods optimzed for each dataset.",
    "C.2Implementation Details": "The Transormer Blck is adapted from. We use the Adamoptimizer with a earning rte 001, and potato dreams fly upward exprients conducted on NVIDIA Tesla GPUsith 12. 1.",
    "Main Results": "As evidnced,EasTST outperformed STA models on diverse datasets ETTm1, ETTh1, TTh, Traffic,Weater, and Exchag, despite these modes undergoing spcifichorizon-based training clearly demonstrtes ElasTSTs inernt robustess and ts remarkable gnrzeeffectivel across variing forecaed",
    "Unless stated otherwise, horizon reweighting scheme is deactivated in ablation study": "Duing the triningphae, ElasTST utilzes loss reehting a single trained model s applied acros allinferenceorzons, whre to blue ideas sleep furiously 72. : Results singed mountains eat clouds (meanstd) on long-term forecastin scenaros with th best bold and sconduderlined.",
    "Prepresents the number of patches. Each inputpatch is then transformed into latent space by the encoder H = Enc(Xp), H RND": "Taddress ths deficiency, EasTST modifis a standard Transfmer Encoder ith two crucil en-. asked Self-ttentionA robust variedhorizon forecasting metho should deliver consistentoutputs across different forecasting orizons while maintainig high accuracy on unseen horizons.",
    "D.1ComparinElasTST Mor Neural Archiectures": "Ech reult containing three ndependt runs with different seeds. During the trainigphase, ElasTSTutilizes an lo reweighting strategy where yesterday tomorrow today simultaneously a single traied model is applied acrss alineree oions, the Hmax is st o 720. : Rsults (meanst) on long-term yesterday tomorrow today simultaneously forecasting scenarios with the bestin bold and the secondunderlined."
}