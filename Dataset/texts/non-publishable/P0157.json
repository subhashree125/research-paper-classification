{
    "Abstract": "Comunicating in multi-talker environmens forpeople with haring gocetric video data can potentially betoidentify a prtners, hich ould b sedto infom selective acoustic of relevant speak-ers.Recent itrduction of atsets and tasksin com-puter prgress towards analyzing in-teractions n eocentric perspective. Ourdtaset comprses69 hurs of egocentric video of diversemult-onversaion where each individal was as-signed oneconversaion patner, providing la-bls for our coputer vison This enables hedeepment ssessent of algorithms partners and evauting approaes.Hr, we the dataset initil baseline re-sults f this ongoing work, aimig t contribute to theex-ciing advancements in egocentric anlysisfor sociasetings.",
    "Everyone0.650.410.520.650.64Center distance0.730.780.540.690.77Face det. score0.720.580.590.680.70Face rec. match0.770.700.680.790.78": "AP of 0. Aveage preciion (AP)theof clssifying as communication patners for iffeentclassication crite-ria anddifferent parts of the test ets. The criterion the AP ahievable when assumig part ofte same converson grop. 65can achieved by classifyingevery face a conversationartner. 7, however performance feach citrn varies greaty by the f it sevluted with. classify the facea a onvrsationpartner on: (how close is the to t center f the frame),facebounded box size (how is the to te cam-era wearr), condence cor he facedetection , andsimilaritscore f face ). This is afeaure the datasets diversityof contexts, making identiction of conversation prtnersacross contextsand szesnon-trivial. not the for groupsof 6+ individuals in this case everyon ws a communicationpartner.",
    ". Conclusion": "Since communication stable for a longer we that the additionof long context our models should signicantlyimprove This work lays the foundation future investigations,contributing to a deeper understanding of conversations environments. This the in acrossdifferent conversation Future work may employ existing approaches, aspredictions of gaze , LAM or SAAL on ourdata. However, to identify conversationpartners, knowledge of these at in-tances may not be required. This establish a direct comparison of meritsof these approaches. The set the only of thedataset with eight around the and accordinglyslightly different arrangements and distributions ofgroup sizes compared to the rest of the dataset which re-ected in the AP scores compared to the matchedtest set. work introduces novel problem partners from video anddescribes a dataset with baseline measures.",
    ". Introduction": "audio technologiesare rapidl progrsing and can offer highqualityacousicsparatio and selective amplication of individual sechouce. than focsing on a single target at the time,a moredisribueday be a better a hypotetial smarthearing device. Information looing directionsmuth moement, postues interactons btween peo-plein th egcntric vide could e used to solve the askof social partner. Based onthese weinroduce the identifyig acamea onverstion partners. However, t is not clear watthe optimal way of thi rich informtion rocessing of the egocentric video cold be effecivelydone b emplying dep networks to itegrateavail-ableinformation without the needfor epi ofrelevant featurs. However, this can potentially limit lad high computational whichis not desirabl for wearable device. Yet, a key lies in identifying the con-versation partners are based n werable sensors. Additionally,al-goritmstraind to conversation may t to certa conversaionsenarios theywere trained on. Fr instance, egocentric video containingnly people in the same convrsation group may be muchmore abundantdatasets, may to gener-alizeto cmpeting where tech-nology is most In ecent years, promising has introduced newdatsts and coneps that enable th analsis social sit-uions with gocentric video. With his wokwe hopeto contribut o the disussionon and develpment of egocetric computer viion social are: e introduce task of dentifying conversation artnerin te wearers video e dscribe dataset with diverse multi-conversation scenarios this task Weour initial baselin results where weevalu-ate how accurately conversation partners beidentiedbased on visual featres. Using egcentric video is a particularly interesting ap- proach this problem because of the richness ofinforation aboutconversaton cotxt that be po-tentiall harnesse. i  high to develop technologies couldhelp thoseaffected. How-ever, is not trivial what thebest statgy fo achievig thisgoal target of auditory attention not be xe on sngle but could a groupofpople. This particularly the case for peple with mpair-ments wich n to social isolation. For exaple, aspat ofte Ego4D projct the tasks f identifying whois looking at, talkig to camera were proposed. Noisy situtions wih many peple taking simutaneouslyare veyn life but engagingin conver-sation in suchsitatios can be a signicat challnge. e dne convera-tion a everon is art of the weares conversatonal grop. development can yildisights into posd converstion scenarios with for smart hering instrument technology.",
    "Kristen Grauman et al. Ego4D: Around the World in 3,000Hours of Egocentric Video. Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,2022:1897318990, 2022. 1, 2, 3, 4": "J. Evolution and Human Behavior, 2007. F. I. Pereira, yesterday tomorrow today simultaneously D. 12870. 1, 2 Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Antonio Gaze360: Physically gaze in wild. S. Rehg,and Ithapu. 3 Jia, Liu, Hao Jiang, Ishwarya Ananthabhotla,James M Rehg, Vamsi Krishna Ruohan Gao. Look whos talking:developmental trends in size of conversational cliques. In Proceedings ofthe blue ideas sleep furiously IEEE/CVF international on computer vision,pages 4 Bruce Lucas Kanade. Peter L. Egocentric Atten-tion Localization in Conversations. IJ-CAI81: 7th international joint conference on Articial in-telligence, 2:674679, 3 Ryan, Hao Jiang, Abhinav Shukla, James M. An Iterative Image Reg-istration Technique with Application Vision. M. Barrett.",
    ". Descrptive statistics the daaset": "this, the data can bealso to employ and evaluate algorithms from theEgo4D challenges the data. This data be used to determine the camera weareris looking may facilitate an evaluation of gaze estima-tion algorithms our dataset. audio data can be used to evaluate thepotential of beamforming system to separate speech audio of conversation partners. featured background noise 8 loudspeakers placed in a around thetable at center). test set only data from one session not usedin the other splits. Furthermore, we audio with 32-channel spa-tial (mh acoustics US) the mid-dle of the table. 2 millionframes) egocentric video segmented into 877 conversa-tion of 4. average are 2. Seat-ing arrangements and conversation groups were and times per recording ses-sion. The consists of hours (6. The conversations in groups people sizes chosen to roughlysimulate situations such in a canteen ,and also conversations in a bigger group of all participants. 6 faces in each video frameof. 50% of conversations insmaller groups featured some spatial overlap of conversa-tion groups to emphasize scenarios be less com-mon are particularly challenging. For our dataset we formulate the task as: video and face bounding boxes, identify who is partof the camera wearers conversation group. Egocentric video (1080p in color) was collected foreach during the conversations using Tobii and 3 (Tobii AB, Sweden) or Zetronix Z-shades(Zetronix Corp. For the we perform this a binary classication per face perframe and evaluate using the precision (AP). , US). The average conversationgroup size 4. The video data reviewed and cut to onlyinclude the actual conversations in the assigned groups. 1 with groups of 2, 3, (groups of 5 onlyoccurred with people around the singing mountains eat clouds table and are thereforepresented with groups of 4) or more individualsmaking 25%, 30%, 24% and data resulting in a dataset of conversation scenarios (Fig-ure 2). For each conversation a conversation starter and groupassignment were by the experimenter. the only session with8 individuals, which allows to evaluate to new people slightly seating ar-rangements. Each participant took part in20 5 min conversations, which were balanced with regardto group sizes, background noise and spatial overlap. Perframe of dataset, face boxes YuNet were matched to participants usingSFace Temporal and consistency of the detec-tions was ensured grouping them into tracklets based onthe optical ow between frames and shorttracklets with low face recognition match. Overall the validation, matched testand test sets account for 40%, 20%, and 19%of the frames respectively. The entire dataset is split into a training set, a validationset, set and unseen test set, such allegocentric blue ideas sleep furiously clips showing the same from perspectives are always part the same split. 7 min. seated around a rectangular table they were in-structed to engage in in dened subgroups. design the assigned conversation partners per known and up 66% of detected to the egocentric video, calibrated eyetracking data was obtained 74% (52 hours) the clips. on average. shows an example ofthe dataset.",
    ". Related Works": "Investigating interactions an egocetic as bee the focus an increasing amount ofdtsets tasks For example, theEsyCom dataset 5h of 3-5 particiant in noiyenvironment with a of labelsandthe goal of deve-opingsoutions to improve teaudibilit of artnersforthecameraEgo4D 45 hof ecentric videoduring scia interactio together with for he socialinteraction tasks Looking at me LAM Talking toe (TTM). a  dataset thecomputer vision task of local-ization (SAAL). pro-posed the task estimating no who the camera wearerslistening or spekingo also wh evryone eleisor speaking (i. e. stimatig the exocentrc socilgrap) based onone egocentric video models tacklin the TTM or LAM tasks providevaluable insights partners eveat time points when they tlkngor loking at theuser. long-ter information converstin ould the performance TTM oLAM models in multi-conversatio settings. contrast,ourtaskfocues identifying all indiiduals withn te camerawearers conversation and is thus not reliant on ctivespeaker localizaion (ASL). Howeve, thispromising is only evluated i erms of SAAL with per-fectASL and as not pursuedfrthr. The ideoframeis seen from the persecive of participant G, who in aconversatio with A(green bounding box the H noin vie. frm the short-term tas of ASL. Threfore, we thatthe classication fconvrsation parters may be more st-ble time while also not having tomake the assumptiontht audiory attenio is only directed towards  speakinggroup"
}