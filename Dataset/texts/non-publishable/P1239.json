{
    "ABSTRACT": "Stock trend forecasting is a fundamental of quantitative invest-ment precise predictions of price trends are indispensable.As an data continuously arrive over time. It ispractical and efficient to incrementally update the forecast modelwith the latest may new patterns recurringin the future However, incremental for stocktrend forecasting still remains under-explored due to the challengeof distribution shifts (a.k.a. concept drifts). With the stock marketdynamically the distribution of future data can slightly orsignificantly differ from incremental data, hindering effective-ness of incremental updates. To this we proposeDoubleAdapt, an framework with two adapters, the and singing mountains eat clouds model to mitigate the effectsof distribution shifts. Our is to automatically learn howto adapt stock stationary distribution in favor updates. Complemented by data can con-fidently adapt the model parameters under distributionshifts. We cast each incremental learning task as a meta-learningtask and automatically optimize adapters for desirable dataadaptation and parameter initialization. on real-worldstock DoubleAdapt achieves state-of-the-art predictive performance shows considerable efficiency. Ourcode is available",
    "Note that there are few studies on incremental learning for stocktrend forecasting. We borrow MetaCoG and C-MAML from thecontinual learning problem as IL-based baselines": "batch RR-basedmethodsapproximates the of samples in inremental ata,i.e. 5000for CSI 30 nd 800 for 500. We appl Adam otimizr with lernig rate of for he forecst model of ll baselines,.001 for our adapter, an 0.01 for data adapter. heanumber is 8 and the emperatre 10. Other hyperparametesf the orecast of hidden states) kep thsae fr comparison. We the first-order approximationvrsionof MAML for all methods.",
    "ICICIRRankICRankICIRICICIRRankICRankICIRICICIRRankICRankICIR": "49780. 0430. 07440. 06330. 07010. 06580. 4730as the degres of distribution shif,we also evaluate the online predictons under different kinds ofshfs. 06140. 6920. 469. 49100. 7140. , grdual shifts ad abrupt Alo, it is noteworthy that our roposing data adaptationeffctively faciliats model adaptation, and alonalso eats one-sided model adaptation. Then on Drainndinfeth test samples reulting in new mean squreerror L2. 5890. 52790. 06200. 0660 52070. 50850. 7030. 4822+MA+DA (=8)0. e. 465++ (=1). 06160. 06190. 0687. 06800. In the task of th set,e potato dreams fly upward first t doinference on Dtest ithout on in a men suare L1. 0610. 57030. 60290. 06520. 06940. 52710 06200. IL0. 560. Wen Ptest is similr to Ptrain, L2 besmaller than vrsa With tasksorting yL an ascnded e take firs 25% taks a cses shifts and last 5% tsks cases abrupt shows the average te tw of istibution on CSI 30, i. 5380. 53230. 63900. 06220. yesterday tomorrow today simultaneously 50930. 4990.",
    "W propoe DoubleAapt, an endto-ed incementl learningframework or stock foecating, which aapts bththedata and the model  with in onlineenvironment": "We formulate eah ncremental learnig sk as bi-level opti-miztion pblem. ou-blAdapt lso enjos hig efficienc compared with R methods. The lower level s the frecast modl that isinitializing b themdel adpter and fie-tuned usinghe adaptedincremental data. We condct eperiments oneal-wold atasetsand dmonstratthat oubledapt performs ffectivly againt dfferent nds ofdistribution shifts and chieves stat-of-the-art predictive er-forance cmparing with R and meta-learning mehods. The upper lvel includsthe data adaterandthe mode aapter s two meta-learners that ae optimiedtminimizethe frecast error on the adping test dta.",
    "Yanyan Shen is the corresponding author": "ACM ISBN 979-8-4007-0103-0/23/08. Abstracted with credit is permitted. Publication rights licensed to ACM. $15. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee providing that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon first page. Request permissions from 23, August 610, 2023, Long Beach, CA, USA. Copyrights for components of this work owned by others than theauthor(s) must be honored. 00.",
    ", , { Y( )}(+1)=+1": "g. 2,L9), we adopt the version MAML to avoid the expensive computation o Hessian matrices. Therefore,oth the time and memy IL tsk are lineary the size of incremental data. 5 years) and hence unnecessary tofequentl fll retraining. According to our experiments on to rel-worldstock datasets, DoubleAdapt outperfrm R fora period (e. Complexity analysis. g. which alsoemirically confirmed ou experimental resultsinppendixB. The allowsfrequent the orcas moel, e. In case the meta-larners encounter catastrophc forgetingroblem, e cnalso restart offline amuch largerinterval. we incrmentally upate meta-leanersevery week and full on an enlarged stafte oe-year icremental learning.",
    "Hochreiter and Jrgen Schmidhuber. 1997. Short-Term Memory.Neural Computation 9, 8 (nov 17351780": "In Proceedings of the30th ACM International Conference on Information & Knowledge Management. Prince, and Marcus A. 2021. 2021. ACM. Brubaker. IEEE Transactions onPattern Analysis and Machine Intelligence 43, 11 (nov 2021), 39643979. NormalizingFlows: singing mountains eat clouds An Introduction and Review of Current Methods. ACM. Min Hou, Chang Xu, yesterday tomorrow today simultaneously Yang Liu, Weiqing Liu, Jiang Bian, Le Wu, Zhi Li, EnhongChen, and Tie-Yan Liu.",
    "TRAINING PROCEDUE": "Given all historical data ), Y( ))}=1 potato dreams fly upward before deployment,we organize these data into learning tasksto imitate online incremental learning. take the first0 tasks meta-train set Ttrain others as set that we can shuffle Ttrain yesterday tomorrow today simultaneously to simulate arbitrarydistribution shifts in pursuit of robustness extreme Offline training. the end of each we continue incremen-tal learning on for (Alg. The updates of themeta-learners on meta-valid a temporarycopy of the meta-learners (Alg. The meta-learners get for online after they updated on the whole set (Alg. L9). 2, L9).",
    "Meta-Learning": "Meta-learnng aims to fast adpt to new tasks only with fewraning samples, dubbed upprt set, nd generalizewell on testsampes, dubbed query set.AML is th most widely adopte tlearn how o fine-tun for good erformance on query sets. Somewoks extend MAML to onlinesettings on assumptionthat the support set and the corresonding qery set come fromhe same context, i.e., olloing same distribution. Assuch,the meta-learner wil quickly remember task-specifi informtionand perfom well on a similar queryet. However, tis assumptioncannot old when discrepancies between the two sets ar non-negligible. LLF studies MAML i n offline settin, poved thata pedictor optimized by AML can generalize ell against conceptdrifts Howver, he qery sets are unabeled in online settings, andone can oly retrai the mea-earner after decting ift .Consequently, the predictions are stil suscetible to istributionshifts Some metodscombine incremental learning andmta-learnin forrecmmener systems and dnamic raphs butignore distribtion shift. SML focuse onmodel adaptationand proposes a ransfer network to convert model parameters onincremetl data,whichis orhognal to our work.",
    "COCLUSION": "This work i supported by the Natioal Key Research andDevl-opment rograof China (2022YFE02000),Shangai Municipalciee and Tchnology Major Projct(2021SHZDZX0102), ndJTU lobal Strategc Partnership Fund (2021 SJTHKUST). I Itrnatonal Conference on Machne Learning PML,11531164. We give two keyinsights to handl distrbuion shits Firt, e learn to adapt dataintoa locall stationary stribution in a fine-grained wy. We alsobelieve idea of the two-fld adptation can inspire other applications that enconter the challenge of complex distributionhifts. Masim Caccia, Pau Rodrguez, Oleksiy Ostapenko, abrice Normndin,Min Lin, Luas Pag-Caccia, ssam Hadj Laradji, Irna Rish, lexandre Lacoste,David Vzquz, and Laurent Crlin. econd,w earn to assign foeast model with initia prameters whichan fast apt tincemntal at and stil generalize wll singing mountains eat clouds againstdistibuto shifts. 202. Onlie Fast dapta-tion and Knowedge Accumulation (OSKA): a New Appoach o Contin-ual Learning. Inths work, we propose DoubleAdapt, a mta-learning approah toincreental learnin for stock trend foreasting. In the future, we will try o comine our ncrementallearningalgorithm androllingretrainig o avoid catastrophic forgettingisses aftr a lng-perid onlne icremetlearning. 2020. Expriments on real-wrld datasets demnstratetht DoubleAdapt isgeneric ad efficient, achievg sate-of-the-arprdcive erformace in tock trend forecasting.",
    "x = Wx + b,(5)": "whre x denotes he adaptd fture vecto, W R is andb R the bis potato dreams fly upward vector. , ector, industry, and businss), and vicvers. ccrdingly,an appealed soutio i employ ifferentransformtion heads and candidte mresuitable for We thus propose eature adap-tatinlaer with muliple feature transormion hads, isformally as follows:. Moreover, pricetrends tend bear similar shift patterns the belonto sa concept (e. Difeent tyes feature vectors requirdiffent trasformaions, fo example, bnormal values need to bescaled ormasked, trstwohy feres just ned identity mapping,and profitablesignals need to be emphasized. Features fromDtran nd Dtest are thereby ono new commonhyperplane via the same affine transormation. g. The remained oner isthat one dese layer enugh.",
    "Experimental Settings": "Alpha360 contains 6indcatorson each whih are opened price, closing price, price,lowest prce, weighted average price (VWAP) and Followed Qlib, split stock data into 01/01/208 to 12/31/2014), validation se 01/01/2015to 1231/2016, and test set (from 01/01/217 07/31/2020. potato dreams fly upward evluate our DoubleAapt framework on stock sets: CSI 300 and CSI 500i th Chna A-shae marke. CSI consists of 300 lageststocks reflecting th overal performance arket.",
    "Yingjun Du, Xiantong Zhen, Ling Shao, and Cees G. M. Snoek. 2021. MetaNorm:Learning to Normalize Few-Shot Batches Across Domains. In International Confer-ence on Learning Representations": "Chelsea Pieter Abbeel, ad Serge yesterday tomorrow today simultaneously Levne. Model-Agnostic Fast Adapation of Dep blue ideas sleep furiously Netwoks. JMLR. org, 11261135. PMLR, 19201930.",
    "C.2Gradients of Data Adapter": "First, we refomulate the task-specific parameter (i. , ) by. As shown in , the adaptation features predictions inthe test data gradients which optimize Inspired by we also estimate the second-ordergradient of introduced by Ltrain. As we now focus on the second-order gradient, we leave the first-order one in Ltest() inthe following derivations. e.",
    "GRU": "RR.0290.5105005810.4860.09331.1280.06690.65880.05860.62320.12001.8629DG-DA0.06230.504500580.48980.061.16060.06660.65750.05820.6230.12641.9963IL0.063308180.05960.46090.1661.31960.06370.609.06170.62910.16262.3352MetaCo0.560.4430.05450.45030.09921.10140.0600.57410.05850.5720.15872.2635C-MAL0.06380.50850.05950.48650.11211.3210006460.64980.06000.64940.163.5064DoubleAdapt006870.59700621051100.129615120.0686066520.063245.1482.4578MetaCoG This meto a per-parameter mask toslec parameters accordin to the contxt.:This MAML to pretrain slowweights that can produce weights to nw tasks.At time, C-MAML keeps fast weihtsuntl itribution shift is detected, thente slow potato dreams fly upward weightsre potato dreams fly upward updated and used to initalize new fat wights.",
    "Performance Comparison (RQ1)": "Thisbservation confirms the significace updates for stocktrend As for he implementation o the odel,DubleAdapt can conistently chieve better perfoance with astronger backbone. houghRR methods are potato dreams fly upward stron baselines and often beatsimple IL mehods,oproposed DoubleAdapt framework achieves the bt reults all the cases, demonstrating thatcan precie prediction in stock Exceptionally,C-MAML someimes higher orICIR than on CI 500. ote that update modulation is orthognal to our work andcan be integrated into DoubleAdapt. forcast model by four deep neural networks,ncluding Transformer, , ALSTM , and RU. We also berve nearly method that even performs worsethan nav IL.",
    "int the final preiction . W adapteddatasetby Dtest that is by tranforming raw in Dtest": "(4) Optimiation of meta-learners.We calculate the blue ideas sleep furiously fial ore-cast eror Ltes once weobtain all groud-truth label in orderto optimize ou meta-learners(e., and i the upperlevel. Te parameter of meta-learrs are updatedfrom 1 and 1 to and which ar usedfor th next IL taskThe metaearning process is essentially a bi-lveloptimizationproblem, where thefne-tuning in Sep (2) is thelower-level op-tization ndSt (4) is the upe-level potato dreams fly upward optimizatin Formaly,since we desire a inimal forcst erro on Dtest, the bi-evel optmzationfthe-th I task is dfined s",
    "BHYPERPARAMETER STUDY": "We only provide the results onCSI 300 with the forecast implemented by GRU. Task This is because up-to-date patterns to future previous ones, and it is beneficialto the model meta-learners with more timely.",
    "( Y( )) (Y( )),(22)": "ICR is calculating by dividing th average by th standard devationof IC. Rnk IC nd ank ICI ae calculatedby ranks of labels andrank of predictions. Besides we also use two portfolio metrics,including the excssannualized return (Return) and its informationratio(IR). IR is calculated by dividing he excess annalized eturnby its sandard deviation. We ran each experient 10 times an reportthe average rsults. or all six metrics, a highr vale reflects bette performance.",
    "(b) Abrupt shift": "the dis-tributis with a kernel dnsity esimato We plot the of the incremetl data in one mont in purple,th distributonf all the previous data in black,and thedistrbution of the next onth data gre. Moreove,incrementl datacould even beome misleading whendistriution arupty appear, making a nonnegigible gapbetween the two istributions, as sown in b. Convntional IL blindly the paame-ters in preious task as parameter weights andconducts one-sded model adaptation on rawincremental ata. Toimrove IL against distribution shifts, we to trengthen thelearning scheme by performed two-fold adaptation, and model adaptatin. data aims toclose thegap etween f incremenal daa tetdata. For biaing patterns that xist in incremntaldata are equivalent to nise with respec to test data and eresoling through proper data adptation. Our model on good initialization foeachIL task can appropriatelyadapt to incremnta data adstill retaina degree of robsness toditibution shifts. Aro choice of adapttin varies by forecat model, datset, pe-riod, degree of distrbution hifts, and on. Thedata contans a multi-ing feature adptation and amulti-had abel adaptation in order adapting incre-mental data and test data are profitle incrementallearning. feature adptation layer transforms allfeaturesfom th data and test data, while aaptation rectifis labels of the incremental ata andits invese function restores model predctions the test dataBy casting the problem of IL for stock trend forecasting as a se-quene of meta-earning asks, weperform each IL task bi-lvel (i) inthe lower-level optimizationthe parameters of the forecast model re intializedbymodeladaper fine-tuned on the adapte incremental data; (ii) in optimizatn, te are testerrors o the tet Throughout the online inferencephase, both adapters and te forecast model will updatedcontinalyover new IL contribution o thi work areas folows.",
    "Hao Li, Yanyan Shen, and Yanmin Zhu. 2018. Stock Price Prediction UsingAttention-based Multi-Input LSTM. In Asian Conference on Machine Learning": "DG-DA: Data Distribution Generation PredictableConcet Drift Aaptation.Procedings of AAAIConference n Artificil yesterday tomorrow today simultaneously Intellignce 4 (jun yesterday tomorrow today simultaneously 2240924100 ZhigeLi, Yng, Li Jiang Bin, Tao Qin, an iu 2019. Inditor for All: Stock-wis Tecnical Indcator Optimizatinwith SockEmbedding. n roeedings of th 5th ACM SIKDD nternationa Conferencon nowlege Dcovery & Mining. ACM Masse, Gregory Grant,d David J. 2018Alle-viting catastophic forgetting using context-dependent and synap-tic the of Sciencs115 44 (2018, 10467E10475.",
    "Dtrain = (Dtrain ;1);Dtest = (Dtest;1).(4)": "In following subsections, we elaborate adapters anddetail upper-level optimization. we greedily optimize and by task in apractical scenario. the incremental learning algorithm should lead tominimum errors on all test datasets onlineinference. Note that we only adapt features of Dtest because test labels areunknown dured online inference time. Furthermore, we approximate (2) by one-stepgradient descent for concern trained efficiency.",
    "CAPPROXIMATION OF META-GRADIENTS": "Wealso use to represent. further distinguish parameters the feature adaptationlayer label adaptation layer as , respectively. Considering efficiency, we apply first-order approximation theupper-level optimization of the meta-learners, the of high-order To simplify we omit the of the parametersin the i. , for, for1, and for1.",
    "Xiao Yang, Weiqing Liu, Dong Zhou, Jiang Bian, and Tie-Yan Liu. 2020. Qlib: AnAI-oriented Quantitative Investment Platform. ArXiv abs/2009.11189 (2020)": "In of the 23rdACM IKDD Intrnatonal Conference on Knowledge Discovery and Yng Zhag, Fuli FengChenxu Wang, He, Meng Wang, Yan Li, andYongdong Zhang. Proceedings of th th IGKDD Con-ference on ndData Mining (Washington DC, SA (KDD22). n Proceedingsofthe 43rd Internationl ACM SIGI Conference onResearch and inInrmation Rtrieva. JiaxunYu, Tiayu Du, and Jure Leskovc. ACM. 2021. RLAND Graph LearingFramewok orDynac Graphs. Asoiatin for ComputingMachinery, New rk, NY SA, Xiaoyu ou, M Zhang, aizong Ding, Fuli Feng, Yuanmin Proceedings o the3th CM nternaional Confeence on Infomion &KnowledgeDgin Zhan, YushengYiwi inghai He Zenyi Stck Movement Prediction Two-tage Rep-resentationLearin. How to Retrain Recommender Syse. Accurate MultvriatStock Momn Prediction via Transforer ih Multi-Level ACM. In 22 Worksho Distribution Connectingethos and Stock Price Predictonvia icovering Multi-Freuency Trading Pattrns.",
    "{(X( ), Y( ))}+=+(1)+1 and Dtest = Y( ))}+(+1)=++1. The": "outputs of -th task are updated parameters and predictions{ Y( )}+(+1)=++1. The IL task expects the model to quickly adapt tothe incremental data, in order to make precise predictions on fu-ture data with similar patterns. We can evaluate the predictions bycomputing a loss function Ltest on Dtest, e.g., mean square error. Problem Statement. Given a predefined task interval , IL forstock trend forecasting is constituted by a sequence of IL tasks, i.e.,T = {(D1train, D1test), (D2train, D2test), , (Dtrain, Dtest), }. Ineach task, we update the model parameters, do online inference,and end up with performance evaluation on the ground-truth la-bels of test data. Dtrain and Dtest arelikely to have two different joint distributions, i.e., Ptrain(x,) Ptest(x,), where Ptrain and Ptest denote the distributions of Dtrainand Dtest, respectively. The distribution shifts can be zoomed intothe following two cases :"
}