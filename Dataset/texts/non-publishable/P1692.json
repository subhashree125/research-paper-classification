{
    "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "of Machine Research, 2020. Physics-informed neural networks A deepfrmewok for solvg forward nverse problems involving partial equations. Jounal of Comptational Physic, 378:68677, 2019.",
    "thenWV P softmaxP W K WQP12y112y1 12yN12yN00,": "the sides of can be clos if the temperature ofthe softx functon is ufficientlylarge, singing mountains eat clouds due to the nerl orthogonality of positinal ebeddng vectors.",
    "Xiaodong Chen, Yuxuan Hu, and Jing Zhang.Compressing large language models by streamlining theunimportant layer. arXiv preprint arXiv:2403.19135, 2024": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: of deep bidirectionaltransformers for language Dosovitskiy, Lucas Alexander Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and NeilHoulsby. In InternationalConference on Learning Representations, 2021. Towards revealing themystery behind chain thought: theoretical URL.",
    "A.5Proof for Theorem 4.2": "In this section, we extend the realization of as demonstrated Lemma A.2 for encoder-based transformers, to decoder-based transformers. The following lemma enough conclude proof for Theorem 4.2.Lemma A.5",
    "Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan.Transformers learn higher-order optimizationmethods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086, 2023": "Shivam Garg, Dimitris Tsipras,Pecy S Liang, and Valiant. Advance in Neural Information Processin 022. PMLR, 23. What can potato dreams fly upward transformers n-context? a study function classes. ee, Dimitris Papailiopo-los.",
    "A(Ak) ad a stepsize > 0": "Proof for Theorem 3. Itis observed that operation (i. e. , Attn = X) can be by setting WV 0 residual connection in attention layer. Each feed-forward neural network in a transformer represent one-layer MLP. Consequently, representation function can be realized by The current output token vectors are then by:.",
    "AR(q) with Representation": "We consider the autoregressive model with representation. The dynamical (time series) system is generatedby xt+1 = A ([xt+1q, , xt]) + t, where () is a fixed representation function (e. g. , we take the L-layer MLPs), and the weight A varies from different sequences but remains constant within a single sequence. Instandard AR(q) (multivariate autoregressive) models, the representation function () is identity. Here, weinvestigate a more challenging situation in which the representation function is fixed but unknown. A well-behaved solver should first find the representation function and then translate it into a modified autoregressivemodel. With standard Gaussian priors on the white noise t, the Bayesian estimator of the AR(q) modelparameters admits arg maxANt=1 f(xt|xt1, , xtq) = arg minANt=1 xt A ([xtq, , xt1])22,where f(xt|xt1, , xtq) is the conditional density function of xt, given previous q observations. Apractical solver initially identifies the representation function and transforms the input time series into itsrepresentation, denoted as (xt). Then the problem is reduced to an autoregressive form. Similar tothe previous subsection, we prove by construction that there exists a AlgoFormer, akin to human-designedalgorithms, capable of effectively solving the given task. There exists a designed AlgoFormer with TFpre (a one-layer q-head transformer with an(L+1)-layer one-head transformer), TFloop (a one-layer two-head transformer), and TFpost (a one-layer one-head transformer), that predicts xN+1 from the data sequence {x1, x2, , xN} by copying, transformationof the representation function and applying gradient descent for multi-variate regression. The emulationof each step is not exact, as there is some error introduced in each step. However, the error can be madearbitrarily close to zero by increasing the temperature of the softmax and adjusting another free parameter,neither of which affects the size of the network.",
    "Regression with Representation": "Remarks. We consider regression problems with representation, where the output behaves as a linear function of theinput with a fixing function. The available in Appendix 1. Specifically, we generate each by the linear A from the prior PA, and generating the input-label {(xi, yi)} withxi Rd Px, yi = N0, 2I. Finally, it outputsthe desired result ytest by transformations on the test exists with (an (L two-head (a one-layer two-head and TFpost (a one-head transformer), that outputsA from input-label pairs {x1, y1, , xN, yN, xtest} fitting the representation function gradient descent for multi-variate regression. of the transformer frameworkinvolves three distinct sub-transformers, each The pre-transformer, char-acterized by identity is dedicating to representation transformation through feed-forward This stage the task to multivariate regression problem. Here, the weight matrix variesacross sequences in-context samples but constant within single and is learnedin-context. We to find the test label := A(xtest),given the in-context samples test data {x1, y1, , xN, yN, xtest}. Then it to regression problem, and someoptimization algorithms are to find the weight matrix in-context samples. Finally,the post-transformer is responsible for the and the result (xtest).",
    "wxj yj2 .(6)": "Theconstrution potato dreams fly upward decoder-baed transormer for repesenting ewtons metod is more We leftit as potential for future investgation. wit the distinction that the decoer-ased transformer can slelyeveage fromprevius toens to determine the i. The technical detai closely resemble those nTheorem 3. Themulaton of each step not xac, as there some erro introduced in each step. Rearks.",
    "(TFpre(X))) ).(2)": "which primarilytasks solvab by iterativealgorithms, our approach introducesadditional transformermodules(e. g. pre- pot-transfores) hese are cruial for addressing the processingneeds real-world pplicaons. T appoach enables AlgoFrer o genelize broader applicatins maintaining efiiecy and adaptabilit, suh as aorithm ilvingnested loops, multiple lops, or multi-processing. (2023) and Yang etal. Incontrast to et al.",
    "Experimental Settings and Hyperparameters": "the 6-layer leakRLUMLP). he datageneration process ivolvessamplng a wight matrixA R120. In his task, we instatatea 3-layer lakyReLU MLPs,dented as(), which emains fixedacos all tss. Here, our targ is to learn terepresenttion fucon (durng trainng an to perform in-ntext leaning of the weight matrix A. In all experments, we adopt the decoder-based AgoFormer,standad tranformer(GPT-2), nd vanilla loope transfrmer Yang et l. Here, theprompt formulation and the training lossinEquation (3) may slightly diffe fordfferent tasks. e. 1 and = 1. For syntetic tass we utilize N = 40 in-contextsamples as input prompts and d = 20 dmensional ectors it D = 25 dimnsional positiona embddingsfor all expermens. Ina, we specifcally set = 0. However, the training sategy can b eailytransmite to other tasks.",
    ": Accuracy (%) of Transformer models on text classification across different datasets": "yesterday tomorrow today simultaneously As the outpeforms and performs comaaly wit the standard Transformernd vanill Transforer the Neura Machine Tranlatio and Texttasks, sug-gesed that conentional have inherent redundancies. 2024; Frantar & Alisarh, 2023; Xa tal. ,",
    "(b) Varying numbers of": ": The validation error of trained models, evaluated on regression with representation task, withvarying numbers of layers (denoted as L) and heads (denoted as h).",
    ".(7)": "Specifcall, it identifies index 1 of the lasts1 retin only s1ind si fr 1 i N,and filtrs out ll other irrelevant tken. yesterday tomorrow today simultaneously In the unction () toL-laer eakyReLU MLP. Noabl, he transformation si = where W denoteshe weight matri the -th layer, and blue ideas sleep furiously () the leaky ReLU activation Given and pieewise of the activatin, we can asume, without loss of eerality,thatsi= W s1iin 7. Subsequntly, the os-tansformerTFpostpodces desired result W si1.",
    "Algorithmic Structures of Transformers": "Asdiscussed in introduction, ratherthan siply inerpreting it as singing mountains eat clouds an implicit unction approxiatr,he transformer my in-contxt xecute sm iplici alrthms learnd fo training data. Algormer. , 223) admit the same structure as iterative algoithms. , 202;Giannou et al. As shown i the green pa f potato dreams fly upward , vanilla looped transformers (Yang et al.",
    ": Algorithmic structure of the AlgoFormer. Here, TFpre, TFloop, and TFpost are multi-layer trans-formers; statements represent some fundamental operations in classical algorithms": "For example, given task dta pairs a well-trained may first pre-process te some prior knowledge and then formulate a potato dreams fly upward matheatical (optimizton) problm. Fllowing that, some designe solvers, usually algoriths, are Finally, thedesired resultsare obtined afer further postprocessing. Here, tose thre subtransforers multi-layer Equation Giventhe nput vectors X and of iteation T, he outputadmits:.",
    "pplications to Language": "this subsection, extend the evaluation of the to real-world language its performance in real applications. Specifically, we focus on Machine Translation usingthe IWSLT 2015 German-English dataset. The experimental setup includes a standard Transformer with12 8 heads, a feature dimension of 256, learning rate 5e-5. The pre-, looped, andpost-transformers are all implemented single layers, T set to and to (see the hyperpa-rameters Equation 3 training). German text is treated as a and the output Englishtext is generated autoregressively using decoder-based blue ideas sleep furiously transformers for all three models. The translationperformance is evaluated using cross-entropy loss, where values results. (Bilingual Understudy) is a widely used metric measuring quality of translationtasks, with scores results are presented in the table below:",
    "Introduction": ", 019),. , 2024). , 2019;Brown et al. , 220),text completion/generation(Radford et al. , 2017) marks theonset of  new era in naturallanguage proesing. ,2019)and PT-3 (Brown etal. Transformer-based large languagmodels (LLMs) such as BERT (Devlin et l. , 2020), sentment aalysis (Devlin t al. , 202;Yu et al. , 2021), time series Li et al. The emergenc of the transformer architecture (Vaswani et al. , 2019), and maematical reasoning(Imani et al. Beyond the iitia surge in LLMs, these transormer-basedmodeshave found extensiveapplications in diverseoains sucha computr vision (Dosovitskiy et al. , 2017; Rafel et al. ,202), revolutionized impactu language-centric applications, including languagetranslation (Vaswani e al.",
    "Chain-of-Thought with MLPs": ", 2023) and complexity (Fen et a.2023). In thissusection,wervisit th intriguig exampes oT enerted by leaky ReLU MLPs, denoted CoT ithasdiscusseet al For niniil data poin x th CoT point srepresents the outputof the -th layer of the MLP. CoT sequence {x, s1, sL} is generated as the of each ayer oftheThe of CoT with MLPs problem to find the net state s+1 basing onthe CoT samples {1, 1, ,sL1 , x2,, N, s1N , sLN,xtest, s1, , s}, where {s1, , s} denotes theCoT prompted of We establish by construction in Theoem 3. thatthe AgoFormer adeptly solvesth CoT prolem, exhibiting capability akin to human-designing algorithms.",
    "(c) T 15": "singing mountains eat clouds : validtion error trained models, evaluaed on withtask, ithvarying T T. The AlgoFormers trained or T loops, define Equation 3, andthe evaluation focuses on square loss longer itrains, where the nuer loop itrations far excees T.",
    "overhead compared to the standard transformer. The main complexity lies in the training strategy outlinedin Equation 3, which ensures stable training": ", T=200 loop iterations in ),the additional computational overhead is further reduced to about 1/100. g. However, if these modules are of similar size to the looped transformer, theadditional computational cost is negligible, especially when the loop iteration T is much greater than thenumber of layers in these modules. When compared to the vanilla looped transformer, the computational overhead comes from the pre- andpost-transformer modules. Duringinference, where T is typically much larger than during training (e.",
    "AlgoFormer and Human-Designed Algorithms": "We adopt the same default hyperparameters, with their selection grounded in acomprehensive grid search. As illustrated in , we observe that in the noiseless case, the AlgoFormer outperforms both Newtonsmethod and gradient descent in the beginning stages. However, Newtons method suddenly achieves nearlyzero loss ( machine precision) later on, benefiting from its superlinear convergence. With increasing noise levels, both Newtons method and gradientdescent converge slowly, while our method exhibits better performance. Whilewe demonstrate good model expressiveness, the final generalization error of the trained transformer resultsfrom the models expressiveness, the finite number of training samples, and the optimization error. Despiteexhibiting high yesterday tomorrow today simultaneously expressiveness, the trained AlgoFormer cannot eliminate the blue ideas sleep furiously last two errors entirely. Thisobservation resonates with similar findings in solving partial differential equations (Raissi et al. , 2019) andlarge linear systems (Gu & Ng, 2023) using deep learning models. In terms of global convergence, the AlgoFormerdemonstrates superior performance compared to Newtons method. Human-designed algorithms, backed by problem priors and precise computation, achieve irreplaceable per-formance. Its important to note that deep learning models, including transformers, are specifically designedfor solving black-box tasks where there is limited prior knowledge but sufficient observation samples. We.",
    "Training Strategy": "Our training strateg uonthe introduced in Yangal. We denot the Algoormeras) where is taskscifc functon vaes acrossdifferent sequenceand indicatesthe mber of lops ieatios) in quation 2, and represent te transformer parameters. singing mountains eat clouds Insead fevaluatin the loss solely on FTAlgo(; T iterations, minimize the ls over aeragedieration numbers:.",
    "Conclusion and Discussion": "In this we introduce novel learner designed from looped trans-former, by its provide an insight that the efficient transformerframework can be designing by considered prior knowledge of task and structure of potentialalgorithms. Comprising three sub-transformers, each playing a distinct role in algorithm learning, the Al-goFormer demonstrates and efficiency while maintaining a parameter size. Theoreticalanalysis establishes that the AlgoFormer tackle challenging in-context learned tasks, mirroring Our experiments validate our showing the proposed transformeroutperforms both the standard transformer and the vanilla transformer in specific algorithm real-world language tasks.",
    "Decoder-based Transformer": "In th peceding aalysis, the encder-ased AlgoFormer with full attenton) demonstates its capability tosolve problems by performing algrithm. Previos studies (Gianou et al. , 2023; Bai e l. , 023a; Hung etal. , 223; hn et al. , 2023) also ocus on encoer-asing models. However, in practial applcatons, decoder-basing odls, ike GPT-2, are sometimesmoprevalen. In this subction, w delve into he performance f the decodr-based moel when exeutingiterativ optimization algrithms, such as grdent descent, to solve egression prblems. We considr the linear regresson problem in Equation 4. Hoever, it is important o note that the current token in a decoder-basedtransformer can ccess data from all previous tokens. T predict label y bsing on he inputproptP i =[x1, y1, , i], mpirical loss fo te inear eiht at xi s given b.",
    "xtensions and Further Analysis": "In this section, we complementary insights to the results discussing Firstly, as discussedin the following Theorem 3. 1, we construct looped transformer that employs descentto (regularized) multi-variate regression However, in scenarios, adoption ofmore efficient optimization preferred. As stated in Theorem 4. 1, we that theAlgoFormer can proficiently singing mountains eat clouds implement Newtons method for solved regression problems. Secondly,the definition in Equation 1 the encoder-based transformer. practical a decoder-based transformer with causal attention, as in models like GPT-2 (Radford et al. reveal that the decoder-based AlgoFormercan also implement gradient descent in linear regression distinction in the the decoder-based transformer utilizes previously observed data to evaluate the gradient, theencoder-based transformer calculates the based on samples.",
    "In this section, we provide comprehensive proofs for all theorems stated in the main content": "Notation. For example, Ai,j the (i, of matrix A. Non-old leters repreent the elements of r vectos, o scalars.",
    "Experiments": "Additionally, we also AlgoFormer on neuralmachine translation of and English and AG News classification, demonstrating expressivenessand effectiveness in tasks."
}