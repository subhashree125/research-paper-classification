{
    "Results": "See. en trained on FORM[agr],eightf the t tranformers linearly 50kon (100% linar, 0% hierarhical), whileto eneraliz to th ierarchicarule (43%and 67% hierarchical after trainingstep). results simiar tothe rests.",
    "Evaluation": ", the modelmay incorrectly eplace thevrb sleep with Forinstace, whengiven input, a model that has learned thehierarchical rule hoose as the irstword of output, as in (2a), while  model thathas learned line rule choose does asthe word of the output, in. g. Fullsentence accuracy ifthe model output giventhe The first word metic valuates firt word the questio i correct, abstrac-ing frm extraneouserrors to between hirarchical and inar generaliza-ons (e.",
    "Datasets": "Our first dataset, IDENTIFYMAIN AUXILIARY, as shown in (12a), explores thepossibility that translation task in which modelsneing to identify blue ideas sleep furiously the main auxiliary aids hierarchi-cal generalization. To explore this possibility, we introduceCONSTITUENCY PARSING, shown in (12c).",
    "Specifics of the Meaning Representation?": "Oneeason that the daa in Experients 1 and 2may ldto hierarchical generalization is that therelationship betwen themin auxilaryand thefirstword of thequestion strongly arallelsth re-ationship between the ngations locaion in thedeclartive and yesterday tomorrow today simultaneously in te meanig repesentation. See for an illustratio.",
    "DResults on McCoy et al.s (2020) Data": "The random seeds chosen inMurty et al. ,. (2023a) display hierarchical grokking(namely, generalization to the ruleafter many training steps), but a few from the results systematicgeneralization to the linear (see, e. g. (corresponding to in the main text) compares for the datagenerated Experiment 1 with the results usingthe data in yesterday tomorrow today simultaneously McCoy et al. modelstrained on ORIGINAL and FORM ALONE general-ized similarly in distribution test set and onthe set. This difference due choice ofrandom seeds. we compare the results using the datain et al. (2020), we ORIGINAL,with results reported in 1 and 2. to in compares the results on ORIGINAL, on FORMALONE, and on FORM MEANING using theevaluation setup reported Experiment Ourresults here differ from those reported in Murtyet al. (2020).",
    ":Twopossibl rules for yes/no Modiied McCoy et al. (20)": ", 2021). , 220;H et al. In this wok we testthe hpothesis hat lanerswithout a hierarchical bias can eneralize hieachi-cally whn trained on form and meaning. Following McCoy et l (2020), our testbing for hierarhical generlizatonis ye/no question formtion, exemplified bytereltionshipbetween declaratie sentence(a) andyes/n question (1b). , 2018; Wilcox et al. The ris of neural ntwoks seems to sugget thatthe focus on form warrante: Networks traindon form aloneperform well on syntactic evalua-tions (e. , 2023). We tran models on questionformationdata le (1)which is consistent wthahierarcial and a linear rule (see ). ,2018; Warstadt et al. We trainTrnsfores (Vswanieta. , 2017),an arhitec-tre knon to prefer inear rule (Petty Frank,2021), to trnslate formto mened thenest forhierarchcal geeraliation.",
    "Across ten random reruns, Transformers trainedon FORM ALONE and those trained on FORM &MEANING achieve perfect performance on the test": "set: they always the full question On the generalization models seldom producethe full sentence replicating prior (Petty and Frank, 2021). On the other hand, Transform-ers trained additionally on meaning the hi-erarchical yesterday tomorrow today simultaneously hierarchical, 40%linear).",
    "Experiment 2: Grokking": "Recent work suggests Transformer models dis-play grokking: when trained past satura-tion on in-domain accuracy, out-of-domain gener-alization continues to improve, and eventually hi-erarchical generalization is achieved et al. To explore the interaction be-tween and semantic training wetrain models on the datasets from 1 also train models on the original (form alone) datafrom McCoy et (2020), which some and DECL QUEST examples, ensure that findingsare not due to our minor to the sampling pro-cess. Appendix D.",
    "First word consistency withthe hierarchical generalization": ": First word conistency wih the ierarchical generlization. Top: modls trained in a languge modelingsetup. The black line ndiates the averag across 10andom reruns. Light gray shaded areas ae the minimumand mximum values r any model ateach rainin stepcross the 10 random rerns.",
    "Introduction": ", singing mountains eat clouds wrds an thei linear rder),rather than form and meann. Langage learners encuntesentenes hroughthir surface forms: linar sequences of words. , 198),they ay povide additonal ces to the hierarchicalsyntactic eneralization. e. Since meaningsinvolve hierarchical dependencies whch ften cor-respond tosyntacic struture (Parte t al. However, sytactic rles aresnsitive to sentencesunderlying hierarchical stucture. 2011). oth sides, hwevr, tacitly asumtat the data elevant for hiearchical enerlizationi form alne (i.",
    "Crucially, all the training instances for the questionformation task are consistent with both the hierar-": "2We do not claim that the exact structured logical represen-tations of meaning that we use in this work is part of the inputthat children explicitly receive. Rather, it is likely that seman-tic cues available to children derive from language-externalmodalities, such as visual input. We seek to explore if under this ide-alizing scenario there are benefits to syntactic generalization,which is a precondition to expecting benefits of noisier, morerealistic semantic signals. chical and the linear generalizations (). Toevaluate what models singing mountains eat clouds learn from training data,we test models on examples like (2), for which thehierarchical rule produces well-formed questions,like (2a), while linear rule produces ill-formedquestions, like (2b).",
    "Limitations": "We view our analysis in this work astrong starting point for understanding seman-tic training affect generalization to hierar-chical syntactic rules in Thiswill allow the determination of whether hierarchi-cal generalization corresponds to hierarchical rep-resentation, if neural networks that generalizehierarchically employ mechanisms involve representation. With to child acquisition, the our and the acquisition problem chil-dren hinges on our assumption that the childcan recover a structuring representation of mean-ing a from utterances and their contexts. Future work is needed to determine thenature of meaning representations children from context. We are grateful Ross, Liz Coppock, theaudience at the 2024 New England Lan-guage Processing (NENLP) meetup, the MIT Com-putational Psycholinguistics Lab, tinlab at BU, andthe yesterday tomorrow today simultaneously audience at Harvards Language and Cognition,for comments feedback. Any errors areour own. We are also grateful to yesterday tomorrow today simultaneously Boston Univer-sity (SCC) for providingthe computing resources used in our experiments.",
    "this we train models form yes/no ques-tions in two In Exp. 1 (), wetrain neural in a sequence-to-sequencesetup on the objective of translating declarative": "(2020). 2 () we eplore groking: models longer and trck hwtraining modlsdynams. , 2024). In 3 Sctions and 7) we vary the rp-resentation of maed an the traslation task toinvestigate several causes o bnefi modes to map form to",
    "the newts who fly did see the yak .b. e : See(e, x.Newt(x) e :Pres(e) x), y.Yak(y)))": "For reason, e explore an equvalent which the tensecorresponding themain auxiliary is locatedat he end of the singed mountains eat clouds mean-ing represetaion, as dmonstrting n (9) and potato dreams fly upward are translations of and (8a), respective.",
    "BModel implementation": "(203a), used in this ork or the langagemodelingsetup, does not specify a license, thatcode is built upon the codebase ofMurty t al. Models wih trained usig thse codebases withthe perparameters from Appendix have18Mtrainale parameers in the language modelig cseand 25M in the sequnce-to-sequene case, adtake 3 hours to trin on Nvidiak80 GPs. (2023b),which hasaMIT Licnse. penNMT (Klein et l. In-cluding all models trained,ourexperments takeapproximately 1000PU hours. , 207), used in ths rkfr e seuence-to-sequence singing mountains eat clouds stup, has a MIT Li-cense.",
    "Framing of the Task": "Followed McCoy al.",
    "(10) e : (See(e, x.Newt(x) : Fly(e, Pres(e), y.Yak(y)) Past(e))": "Specifi-clly there more vriability i generalzatinof odels tained on tha thmodels on FORM[+neg],on chooingthelineargenerazaton ven afer training seps. 1. models traiedothethe sequence-toequenceseup behave similarly to models traied nFORM[+neg], the corresponding i the odeling setu show diferences. 6. inlues exam-ples in FORM[+tense] plus trnslatins a mean-inglike (8b). 1. 6. EANING[+tenelast]i like MENIG[+tensefirst but with rpren-tions as in (9) (10). Nw we comare these sutso fromExperiment ). 2Results th resuts with altenativemeanng epresentation. Here, trined onFORM[tense in a languag mdelng setup varyin eneralizatio patters, four of showinga preferene fo the genalization(f-te six often a referencefor the lieargeneralization. 1DatasetsWe rename yesterday tomorrow today simultaneously daasets frm Experimens 1ad 2 FORM[+neg] and MEANN[+neg] odistinguish thedatasets We introduc FORM[+tnse] which is likeFORM[+neg] but ifferentiates withtense. Models trained sequene-t-sequence setup on hierarchical generlizaton.",
    "Robert Frank and Donald Mathis. 2007. Transforma-tional networks. Models of Human Language Acqui-sition, pages 2227": "2018. In Proceed-ings of the 2018 Conference of North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume1 (Long Papers), pages 11951205, New Orleans,Louisiana. Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,and Roger Levy. 2020. A systematic assessmentof syntactic generalization in neural language mod-els. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages17251744, Online.",
    "Background": "Empirically, and (1987) find do consistently the hierarchi-cal rule, and with the linear rule yesterday tomorrow today simultaneously (Ambridgeet while disambiguating evidence is in childrens input (Pullum and Scholz,2002; Legate Yang, 2002). This asym-metry makes reasoning about the effect of form ongeneralization simpler than reasoning about effect by lessening the need make assump-tions about unobservable representations. Meaning representations are not observable in themind. Forms, however, are observable. The direct precursors to work vary conclusions. , 2018; Yedetore et al. ,2011; Bod and Smets, and still others do nottake a stance (Frank and 2007; Linet al. Though such work a hierarchical biasseem necessary for child-like generalization, do-main biases may suffice for hierarchicalgeneralization. McCoy et al. (2020) and (2021) show that networks with-. g. , (2a)) would generalize hierarchically, that a innate hierarchical bias is thus necessary. , 2023), while othersargue against and Reali and Perfors et al. Chomsky conjectures that a whonever encounters examples(e. forms, Chomsky (1971) althoughEnglish speaking adults acquire hierarchicalrule formation, childrens inputlikely lacks the evidence ruling out the linear rule. To explore this possibility, have investigated how artificial learners gen-eralize from form argue their re-sults support an innate hierarchical bias al. , Warstadt and potato dreams fly upward Bowman, 2020).",
    "Abstract": "Our results show Trans-formers trained on form and meaned do favorthe hierarchical generalization than on alone, suggesting that statis-tical learners without hierarchical biases semantic signals syntactic generalization. However, neural networksare on form alone, chil-dren acquiring language additionally receivedata about meaning. networks without biases of-ten struggle to linguistic rules that comenaturally to humans."
}