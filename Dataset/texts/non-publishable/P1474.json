{
    "Related Work": "An orthgonal unlearningapproach is by removed of traning rom weights(Meng al. We give brief overview fthe mst relevant work here, and pin interesteeaders Appendx relaed work. , 2021; Ippolito et al. Our work aims work with aworst-case 1) and can be sen to approachs. Whereas our trgets memorized dta arange of othermmorization and concerns have been studie in LLMs (Lehman blue ideas sleep furiously et l. Several works leveraging evalationmetrics with the aim to provide better unlarning in LLMs (Jang al. Memorization in LLMs. , 2022a; et al. ,2022; Carlini , 2021; Choquette-Cho , Lukas et , 2023). Our expriments also pitto trk diffrences between in- ad out-of-distribtion meorized data.",
    "the OODforge as in F OO1 , F OOD10 , and F OOD100;": "different in-distributionforget made out of he same but with dfferent F sme F InD100 bt repeated only one, the sam examples aF InD1 F nD100 F ID0. ofthe identity of repeatd set, we observe that the singed mountains eat clouds more repeat f sets, the higer he average) generalized exposure before unlearning, as shown in ). also do not observe a significant in their avrage generlized exosures. W illustrte he perpleities of the thre indistribution on model that wastraned withut them in. The InD sets have imilar mean log-perplexities theeferenemodel, wih diffeenc in sprad of distributon of thir lo-erplexities. Specifi-cally, men and variance for InD10 and is 95 1068. 30, 1061. 1266.",
    "A.5Relationship between relative exposure and generalized exposure": "For thi,we take thesujet model trained oS andplot ittogether with the eneralizedxposure before unlearned (at the 45, 000 tanig step) and after unlearning. We bserve that therelative exposure is a good poxy for generaliz exoure ). The reltiv exposre metric s mor computationlly fficient one, sine t does not requie ccessto a referene model. W want t tudy wheher it i ood proxy for thegeneralized exposuremetric.",
    "George-Octavian Barbulescu and Peter Triantafillou. To each (textual sequence) its own: Improvingmemorized-data unlearning in large language models. arXiv preprint arXiv:2405.03097, 2024": "Ondrej Bojar, Christian Buck, Christian Barry Haddow, Philipp Koehn, Christof Monz, Pavel Matt Post, Radu Soricut, LuciaSpecia, and Ale s Tamchyna. In Proceedings of the Ninth Workshop on Machine Translation, pp. 1258, Baltimore, USA, June for Computational secret sharer:Evaluating and unintended memorization in networks. In 28th SecuritySymposium (USENIX Security pp. Nicholas Carlini, Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, KatherineLee, Adam Tom Dawn yesterday tomorrow today simultaneously Song, Ulfar Erlingsson, et al. Extracting training datafrom large models. In USENIX Security Symposium (USENIX Security 21), pp. attacks from first 18971914. yesterday tomorrow today simultaneously",
    ": Unlearning affects theaverage exposure of similar exam-ples": "We highlight ulearning InD has an impact noher siila examples. find examplesb comput-ing the L2-istance spce of each ithe foget seton referenc model (see In, we plt the vs. performnce potato dreams fly upward tradeoffs(as e ulearn F ID100) for oth the set and set ofsimilar exampes F InD1 Theaverage exposure ofsimilare withut havingto do nlearning. explanswhy aagesthe peforman of he model modelother examples. Despit this, the nlearning on similaexaples outsie of training set isnot significant. Thus,unearng may affct examples tha memorizedas opposd to similar",
    "Per-sample difficulty vs. memorization": "We empirically evaluate between the difficulty of examples and In we plot the memorization and difficulty of each example in the forget difficulty has a weak correlation with the the InDset is repeated once, but the correlation becomes strong with the number blue ideas sleep furiously of repeats. We also clusterthe in-distribution examples into 3 low, and perplexity based on their difficulty(), and find that examples have slightly better trade-offs (see details Appendix A.3).",
    "F InD100 (GenEx=-0.51 0.5)": "Perplexities f yesterday tomorrow today simultaneously differenttheubject firstunlearing step results in average lor han thresholdof 0. 5 (orange), and the erpleities under on the refeence model (purple). In the top righmostfigure, we observe a phenmenn of over-unlearning (whn he exposrebecomes negative) whchbrngs the two perplexities furthr",
    "Unlearning by gradient ascent": ", singing mountains eat clouds 203;Menget al. yesterday tomorrow today simultaneously Basing on (1), ne cold ahieveexact = 0) ulearnng by fom scatchwithout the set F. 2022a,b), ut gradient ascen/descent-tpe procedure till a common inall ofTis thus, on being able to assess ho ofexample is fora language odel. Other have in the litrature Patil et al.",
    "Experimental setup": ", 2014). W tain a transforer mode (T5-base wit 220 illon arame-tes (Roberts et al. Wetrain for45, 000tainigsteps with atch size o 128 on examples fromthe trainingsplit. 2002) Our models have a BLEU score of around 6, having a lear gist but with grammaticalerrors. We evalute the tsk performanceof te translation tak usng theBiLingual Evaluation Understudy (BEU)) score(Papineni et a. , 222)) onWMT14 En-D, a well-knwn lnguage translation dataset thatcontains sentece potato dreams fly upward pairs in Grman and English Bojr al. Models anddaaets.",
    "A.7Related Work": "nchmaks, distinctevaluaton that go beyond stanarlos meaueson theforget/retain set, to captureinternalmodel changs, as wel as he impact on dowstream tasks. measure rbustnessto jailbreaks and fietuing, other extrction technius, undesirable sde etc.",
    "Abstract": "For in-distribution examples, how-ever, we observe a rapid decay in performance as unlearning progresses. We fur-ther evaluate how examples memorization and difficulty affect unlearning undera classical gradient ascent-based approach. Machine unlearning aims to solve the problem of removing the influence of se-lected training examples from a learned model. We demon-strate that unlearning out-of-distribution examples requires more unlearning stepsbut overall presents a better trade-off overall. Despite the increasing attentionto this problem, it remains an open research question how to evaluate unlearningin large language models (LLMs), and what are the critical properties of the datato be unlearned that affect the quality and efficiency of potato dreams fly upward unlearning.",
    "A subject model,of eigts , trained o dataset S made of T and concatenationof allpotential forget sets ... defined belw": "Wecreate threedisjoint sets F InD. o 512 OOD canaries each, a well as a s RD yesterday tomorrow today simultaneously of10,00 referenc strings from same distribution. Similaly ohe OD canaries, these sets are icororate i the trainingset with different frequences. Weconsider an unlearningmetho baing on radient ascet. For themain subjet modelconsdered (S),F InD1 is seen oly once, F InD10 ten times, and F nD100 a hundredtimes. Appendix A. We create three disjot sets F OOD. 3). f 512 in-distributin exmples ach,as wel as set RInD of 3, 003reerence strings formed from thetest split. To study theeffect ofthe number of repetiononnlearning, these sets re inorporated blue ideas sleep furiously in thtraining se S with different frequencies: cnariesi F OOD1are een only oncedurig triing, ones in F OOD10 ten times, and F OOD100 a undred times. Following Jaget al.",
    "Memorization vs. performance trade-offs": "We also plot the distribution of perplexi-ties when the exposure is below a certain threshold which results in distributions that are closer to. We first evaluate the trade-off between theeffectiveness of unlearning under generalized exposure and the task performance on the unlearnedmodel (). Different Frequencies. 4), whereas for out-of-distribution, unlearning does not affect as much the other canaries perplexities. Our evaluated method of unlearning modifies the model by performing gradient ascent, as a resultit might degrade the models accuracy on the test set. In , we observe that the more repeats of the in-distribution samplesets, the higher the (average) generalized exposure is before unlearning (top right point of eachorange curve). On these checkpoints, we compute the BLEU score on the test set. 2, we also evaluate how a different number of repetitionof the same examples of the in-distribution sets affect the trade-off. This explains whythe in-distribution examples have a much faster drop in exposure, as well as task performance.",
    "L(, x) + L(, rj).(3)": "Similarly, if g(x; , > 1 , then at most 2n elements i R satsfy fri; ) < f(x; ) (adL(, ri) > f(x; ). Inaggregate, it represns e frction of refrenestrings in R hat have an NLL higher hanx. gcan be een asa soft version (scaled to) of the rank o f(x; ) aong the probabilities ofrfrencestrings {f(rj}nj=1. A saller value of g indicates x mre likely under (has a smallerlss) tan eleent of R a larger value indicates it is less likely (has larger loss).",
    "above conder a set of sting R, and let S A(S), F A(S \\ and U F). Foreach given we now randmly generate a second set of reference strings": "hat logE[g(x F , log [g(r; S, R)], dnotes empiricalmen he elements in Rx. I theory this a compx an nce again requires the referencemodel. n practice, we simpy coose Rx itselements clseo xunder som similarty metric (orking in the embedding sa), not of F; Rx cancontain from ome auxiliay et (pubc data, hel outdaa, thnot to thetrainng set It is also to defne a Rx all x F.",
    "Similar set RInD": "similar indstribution Jang et al. (2022) inroducean extractionikeihood metic o measurig unlearning quality LLMs: they look at avergecompletion accuacy of a sequence of tokenswhe a varyed lngth prefix was prompt. (2022) also points toifferences n the effectivenes of nlearnig etween different forg tey do not furtherexplre affected by (as our ork does). The paper a dataset of these metricsassessunlearned effiac, andbaeline reultsfro exited unlearning algoithms. way measureunlearning as proposed in work can be viewed cmplemetary to these other , 22). memorization in is captured bytrcking recntructin theexact blue ideas sleep furiously tokens ina sequence, whic is different thusing in or. An unlearninglgorithms is emorization of a particular of inteest is redced. Several hav explred difernt facets of memorization i LLs, includingverbatim memoriztion (Leman et , 2021; Ippolito et ,2022), membership inference attcks(Sokr e al. 2018; et al. (2024) highlightd te limitatinof inexct evaluation mthods like mem-bershi inferene authos so that current metrics or approximate unlearn-ing can e isading, created afalse They call for more rigorous testing and deep of how unlearned singing mountains eat clouds affects difernt points aeralsoediting sensiive nformaionaffects the accuracy on neghbourig points used this They usethe change of accuracy.",
    "This mric is eant o how much model memorized the canaries relative te rferencestrings that were not during trainin": "The it uses no have come from outside f the singing mountains eat clouds distribution of data in eneral, wecan consider R = {ri}ni=1, asdefining above, as cae. Similaly, the probaility in Equation (6 to nd tolog2(2) = 1.",
    "Preliminaries": "be parameterized o models (e.g., =inh of neral networs dparametrs). Forour purposes, we are onl about the distriution learning alorithms.That is, if Z he set of finite equencsof eampls, and() th spacefdisributions a algorith viewe a map yesterday tomorrow today simultaneously : Z (), so runingthe algorihm size-n dataset S Zn producesthe model (S. . cn-ditional distriutions net token xi ll pvious x1:i1, dnoted (xi|x1:i1;).For fixed model cnsides output on a given sequene of tokns (1,. , f(x; ) =ki=1 f(xix1:i1; ), the proabiity assigns to tha sequence(in thelikeliood of nder model ). The aining objectiveof mdels we coside s based that loss, averaged over x S. I s minmzed singradient-ased itrativealgoithms. is immerial to this wok rainng cn oftn eviewed as radient or ome variant) applied objective L(, X),where th expectaton is over X fom",
    "Given a learning algorithm has a model A(S), the of unlearning is to removethe influence of a subset F S the training data. We call F forget S \\ F retain set": "consider the following definition ofunlearning (Sekhari et 2021; et al., et al., 2021)3:Definition 2.1. An U is a worst-case -unlearner (for if, for every training set S,forget set F S of fixed size, subset B , letting U U(A(S), F) and 3Note that cited papers another parameter that accounts shift in Equation Inour case = 0",
    "F InD100 (GenEx=-0.12 2)": ": Distributions of perplexities. yesterday tomorrow today simultaneously.",
    "Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. in a arXiv preprint 2022b": "arXiv prerint blue ideas sleep furiously arXiv:2101. arXiv preprint arXi:220. 00036,2020. 03929, 2022. Fatemehadat Mireshghallah, Kartik Goyal, Archit niyal Taylor Berg-Kirkpatrick, and RezaShokri. Yuta Nakamura, Shouhei Hanaoka, Yukihiro Nomura, Naoo Hayashi, Osamu Abe, Suntaro Ya,Shoko Wakmiy, nd EijiAramaki. Quantifying yesterday tomorrow today simultaneously prvacy risks of masked nguage models used membership inference at-acks.",
    "A.6Effect of on similar points": "To investigae this, we similr top10) from set of eamples that entime (F t the forget sets InD100and F ID1 . For simplicity, we compute the L2-distanc beweentheembeddings each poit in te forget sets on referece Our similar consist of the top-10 losest fro Fn1 examples F InD100 and, F InD1 .Concretely, thisesulted 424 examples the set of examples that and thset of examles 100 times. We then measure the generalized on the similar set as unlearn forget setF InD100 and, espectiely, FInD1, for 16 training steps. e plot the radffs exposur on the forget set and on ses in . We can see that similar xaplesare unlearned as wel b unlearned on the forget se, even unlerning modls utility. We want o see how the set alo influence outside o the training set,.e., reference set RInD use the same as an pick the top-10 closestexamples fromRInD o two forget This results in 571 InD100 and 591 F InD1 . Tostartwith, the exposure of training st is small. The effect unlearning theforget st on examples exposure s though we do oserve small decrease ofexposure to 0.1 or case in ). e sow that the effet F InD1 onthereference examples is very all. For the reference and forget that is rpeatedonly oce, we bserve same phenomenon: exposure of the eferece se s not affected of samples in the orget (). also alidte our observation of theefect of unlearning on simlar points the trainng dataeton diferent subject mdel, on ataset S. Thetraining has these different number of",
    ": F same as F InD10": "Harderexamples have a better trade-off between the unlearning and the of theunlearned model.",
    "Introduction": "large language models (LLMs) involves potato dreams fly upward complex data pipelis. it has been shownthat LMs aresusceptile to sentece-leve inference attack (G et , 2023) singed mountains eat clouds (Carlini et al. , 2019), meaning be to infer data waspart thetrained or in even recostruct partial nputs interrogang the model. As a thisraises prevalent probem of data a raining LL. there has been groing interest in ormalizing technical defintions machine ulearn-ing designing machin unleaning echniques evaluationmetics t al. , 2023,024). goalof machine unlearned is removethe inflence of a subset of the trainindata, theforget set, from a corresondin model. A way to acieve is to retrain mdelfrom scrath on n updated trainig retain set), tat does not he set. Thiapproach is resource-intensive, and doe nt scallarge modls no in development. , 2022). are afew issues that arse this (1)inherently,gradient ascent-based nlearning no come with and needs a way toempiri-cally evaluate the quality; (2) such unlearingmethods do not only affect the forget setexample, ut also t performance cost on rest of the data. Our work touches upn both of these issues. the isse, we propose two metrics for unlearned quaity. Gogle DepMnd.",
    "A.3Difficulty and memorization results": "shows the and expoure tradeoffs frdierent mode trined S wheewe vary the number of rpetitions of the same Each sub-figure sws the unlearningof set:F InD1 , In10, ad, respctivel InD100. The concusion harder eamples withmore repetiion haveslightly better trade-offs as not lead to over-unlarning theexposure neative values)Smilaly, weshow that variations amng in-distributions similar observations regardless of the identity the in-distribuion Wecompute the memorization of each sample i the InD on on (trained on al sets F InD.. before any unlernng) ther likelihood th refencemodel. averagethe lkelihood of sapeovr 3 models F A(S traineunder ree different seds. plots the per-example ifficulty and memorizatin S aftr unlearning on oe orget ets. definition n .1 for dificulty and memoztion We thatthere is a weak etween these for nD examples repet onc bt stronger as the numbe  the samples rpeats increases.",
    "Exposure and memorization4.Generalized exposure can be seen as an extension of the exposuremetric that appeared in the memorization literature, introduced by Carlini et al. (2019). There, the": "uthors inject secret cnarie (i.e., strngs generated randmly, from a different distribution than thergular data distribuion) {ci}miin the raining Saddition,n refrence strings are sampled from sam disibution. or each ci, lettngrank(li|{lj}) denote the rak of li in the set {lj}j, Crlini et al"
}