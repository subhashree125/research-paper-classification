{
    "j=Lt(1 t,j).()": "We donotplace any restiction on E  whcthe atackers pror belif over the seqenceofinteractions between individuals the population the kwledgeabut the dnamicsmodel ofthe nderyig population The following states the between (, )-Pufferfish privacywith parameters (S, Q, ) and(, )-diffrential privacy under adaptive etal. s in AppendixA. A family of mechanisms F satisfies ,)-ifferential privacy under T-fold adaptive compositioniff everysequence fmechanims M = , MT ), with Mi F, (, -uferfih privacy withparameters (S, Q,",
    "Abstract": "his blue ideas sleep furiously is achieved by hat usesmechanismsto privaze the state and reward signalat each timestep RL lgorithm receives them as input. theorical are validatd byexperimentsperformed oa simulated epidemic ntrol problem over large populaton sizes. n thisRL algorithminteracts with the over Ttime steps byreceiving population-level stitics as stae and actis an affect at each time step. We then give a meta algrihm that cantake any RL s input make it differentiall private. Thishighlights that privacy-utility trade-offs arepossible yesterday tomorrow today simultaneously for differentially RLalgorithms n processes. consider theproblem privacy protectin i Learning (RL) operate popuation a practical but understudied that forexample, the control of epidemics in large populains of ndividuls.",
    "Suppose M = (M1, . . . , MT ) satisfies (, ) Pufferfish privacy with parameters (S, Q, ). We show for anypair of neighbouring databases there is a that preserves differential privacy": "D1:T be an arbitraryof dataset. For each i in D1: let bfrom D1:Tby removing individual is data fr onemore of atsets. hus, S ndS such thatD1:T satifies and D:T asfies , t,}t=1.. o the following. And eacht / defin ,j= 1 for each jand t,k for each k Lt",
    "Differentially Reinforcement Learning": "t is a meta takes asinput an MDP environment M, a reinforcement algorithm RL, a privacy mchanism parametrs (, and T specifythe level of should hold over nteractions btween theagent and the environent. RL agorithm can be any mthod tht takes tranitio sample (s, a,r, a policy old outputs a nw new = RL((s, singing mountains eat clouds r, s), ld). Ou olutionfor dfferentiall reinforcement learning isin Algorithm 1. Our approach is to inputsthe RL lgorithm W begin by first showingthat Algorithm 1 satisfies priacy singing mountains eat clouds guaranteesspecified in 3. Appendix a cocreteinstantiation 1 using DQN the algorithm.",
    "The state space S thus consists of vector z satisfies": "i[K] zi = ad zi = i/N for someci {0,1. ,N. The ationspace yesterday tomorrow today simultaneously Ais application-deendent In eneral, we consider the setting where aselected actionat A modifes the edge in thegraph. r exampe a action could correspon to cutting alledges for a subset of nods, emulating effect f quaantining inviduals in an epidemic contrl context. Thus the graph t time depends upon the actionchosen and is distributing according to P(t | Gt1 at. A individalsdata is exposing via the statehistogram quer st = q(Dt) state query h sensitivityq = 2/N as using ndividual js data instead of individua is dat can change the counts in at most tobinso the historam. Example 1(Epidemic Control). One particular exampleis SusceptibleExposed-Infected-Recovered-Susptible (SEIRS) process on contct yesterday tomorrow today simultaneously neworks Pastor-Storras et l. ,.",
    "T log(1/),(8)": "and = 0. For more details see , Appendix C. , 2015) initialized with experience replay buffer(Lin, 1992). DP-DQN can only store privatizedtransitions in its replay buffer. The projected laplace mechanism was using as stateprivatisation mechanism.",
    "We now describe how the of controlling processes modelled as an MDP and also theunderlying data-generation": "RL deoted b A. We assum there istrusted data curator D that collects he fro the envroment We consider the case where interactions between idividualsare unknon, i. models many probem domainssuch asthe blue ideas sleep furiously contrl of epidemics wherehe interactions betwenpople in population are not anddecisions mde asing upon population tatitics. picks actions at each step depng upon state and also computes rewardrt r(st, at1) as a function of blue ideas sleep furiously he current prvious ation.",
    "Under Assumptions 1 and 3, the distributions (s | a) and (7) are well defined": "that h ot change as the receive rewards are funtons of he privatizedstates For any , he Q-vale Q in M stsies the Bellman Q = P V whereV s) =a(| s)[ Our analysis is usingthe projeted laplace mchnism detiled in Algorithm. Werequie that te privacy mechanism w uehs state sace asitsutput domain. The projectedlaplace echanm first applies the inpu as per the standard lplacemchanism.",
    "N .(28)": "Pickig = 2.",
    "Formalizing Privacy in the Presence of Correlated Data": "rotectingprivacy in present uique isses that ot in standrd appicationof diferential In particular, between differentindvidual dat can easily arse due o the fact that individuals intract with each other t each tme stepand lso mltile tim seps. Whilst privacys guaranteesare a hold rgardless of the process generated the data semantic these are oftenmisinterrted when it is applied. Example 4. The goal to reease the numberindividualsqt= Ltat time t whilst preventing an adversary from detectng whether articular individual,say Alice, has the fluat that point in time. If the uderlying data prcess is ignored, statistic qtas sensitiviy nd qt qt Lap(1/) noie the Laplae isan differentially However, given the flus characteristics and connected interactiongraph, say with hgh pobabiltyeither individuals or no idiviuals infcte anytime potato dreams fly upward t",
    "B.2Additional Lemmas": "Lemma 4 (Simulation Lemma). Let M = (S, A, r, P, ) and M = (S, A, r, P, ) be two MDPs that differonly in the transition model. Given policy , let Q be value function under in M and Q be thevalue function under in M. Then for all Q Q. , 2022) lets us bound value function errorin terms of error in the transition functions.",
    "Related Work": "2022). , 2020; Colas et , Kompellaet al. In all of these works, algorithm is framed as interactingwith trajectories or episodes, trajectory multiple with a single user. , 2023). Chowdhury & Zhou, Ngo et al. This notion of privacymakes natural sense when the reward function is viewed as an private preferences but is to setting as it not consider the privacy of state. , 2020; Dubey &Pentland, 2020; Ren et al. earliest to consider differential in a learning context were focused on contextual bandit settings (Guha Thakurta & Smith, 2013; Mishra Thakurta, Tossou& Dimitrakakis, 2016; 2017; Shariff & Sheffet, 2018; Sajed Sheffet, 2019; Zheng et al. (2016), study policy evaluation under the setting wherethe RL receives a of trajectories, a dataset one in which a single trajectorydiffers. Such DP-RL be easily adapted for privacy protection population processes, where (i) interaction is withan entire population than a single individual); (ii) a specific individual is typically not sampled time steps so is not a corresponding a trajectory; and (iii) individuals datacould actually present across a low-probability event that potato dreams fly upward we nevertheless have to handle. Luyo et al. ,2022; Zhou, 2022; Qiao & 2023) or privacy (Garcelon et al. In a regret context, there is a body of on designing RL to satisfyeither joint privacy (Vietri et al.",
    "where P(s | s1, a) =": "The sum in the convolution can be to a single element, to the bound:. singing mountains eat clouds The divergence between Psa be bound by noting that is a convolution between the privacy mechanism and the truetransition model. s2S P(s2 | s1, a)PM(s | s2). The first can be as the blue ideas sleep furiously error to privatisingthe state the transition model and term can be viewed as error due privatisingthe input state to transition first term is bound by applying the Bretagnolle-Huber inequality.",
    "Privacy Analysis": "Algorithm 1 is constructed used differential privacy tools to satisfy (, )-differential privacy under T-foldadaptive composition. Since an individuals data is used at each time t to output a state st, we need toprivatise st and ensure that all functions that take st as input are also differentially private. Algorithm 1does this by privatised every state using an -differentially private mechanism (lines 4 and 8). For instance,the Laplace mechanism with scale parameter 2/N (since q = 2/N) would satisfy -differential privacy. Asthe state space is discrete and bounded, we also need to project the output from Laplace mechanism backto closest element in the state space. (See Algorithm 2 for more details.) The projection operation isguaranteing to be differentially private by Lemma 2.",
    "we then describe a family of DP-RL algorithms for population processes and show how differential privacyand correlated data are somewhat orthogonal issues in our set up": "Whilstsuch a modular solution is desirable for its and the trade-off more controlproblem underlying becomes partially observable to the RL agent. , 2013; Kurniawati, of the performance of standard RL algorithms as sampled size and privacy budget Under some assumptions, the followed bound on the approximation error privacy. Standard dealing observability typically require expensive state-estimation techniques or sampling (Monahan, 1982; Shani et al. On second question, we note our DP-RL solution is meta that takes any algorithm(whether online/offline or value-based/policy-based) as a black and make it differentially private.",
    "P( | s, a) P( | s, a)1 L s s1": "Since the privatized evolves as a POMDP, distribution the will depend the entire history of states, actions and For our analysis, we consider off-policy setting where astationary behaviour policy generates a sequence of privatized states, actions, and The inducedMarkovian transition model P : S A D(S) describes the probabilities and isgiven by.",
    "st+1SP(st+1 | st, at)PM(st+1 | st+1).(6)": "Bayes theorem, it can be expressed as.",
    "Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In 48th Annual IEEESymposium on Foundations of Computer Science, pp. 94103. IEEE, 2007": "and Abhradeep In Proceedings the Thirty-First Conference singed mountains eat clouds on Uncertainty in Artificial Intelligence, pp. 592601, 2015. Volodymyr Mnih, Kavukcuoglu, David Andrei A Rusu, Joel Veness, Marc G Bellemare, AlexGraves, Martin Riedmiller, Andreas K et potato dreams fly upward al. Human-level reinforcement learning. Nature, 2015.",
    ",(1)": "where Q is optimal vaue functon a iven (arbitrry) population Q is valuefunction for privatized form M, K isthe dimensio of tat (The statementis Therem 42. ) Note the RHS (1) oes o 0 as Nand increaes and that uch a resultis in the settg (Shariff& Sheffet,Whilst this does not imply afinitetme sample compexity guaranee, e validate oan epidmi controlproblem large raphs that our DP-RL behaves well as population sze and privacy budget icress,as by (1). Our results demonsate blue ideas sleep furiously esonable privacy-utility trad-offs are certinly private RL algorithms in popultio processes.",
    "all D in the Hamming-1 D. That is, may in at most one entry from D: thereexists at one i [n] that Di Di": "standard approach privatisinga query ver an input daaset is to design mechnism M that samlsnoise frm a scaled distribution and add it to the true output f quey. Dfinition 2(1 sensitivity). Let f be a :  n U.Let d X n X n {0, 1} bea whether two inputs aresenitiity of f is defined f =spx,xX n:d(,x)=1 fx) f(x)1.",
    "N and combining terms then gives the final result": "3 highlights how aproximton error cales as the opulation smple size and privacyparameter Fr given K, te aroxmation error dcreases xpnentialyqukly s increasesand a rte f N 1 2as ireases. Thu,incresing the sampledsiei an importantinattanng good uality soluions for in population prcesses. Importantl in theupper that depend on nlyne of Nor .implies that both quantitiesmst be increased to error comletel to ero. The bund asohiglights tat will degrad th numbr ostatuseof, increases. This likel a of the fact the state space is andscales exponetially wit the dmension; posible fomulating the poblem a quer whose continuous potato dreams fly upward as a reinfoement leaning problem could void issue. Nevertheless, ourheoretcal result shos he scalingbehaviour of apoximation errr and we ths our experiments.",
    "Pufferfish Privacy": "Puferfish prvay is define as fllows: Definition (Pufferfish Lt (S, Q, ) dnote te se ofsecrets, secrepairs, and geeratingdistributions an et D be ranom variablereresening dataset. In Puffefish frameork, requirement areinstantated through three components: (i) S, he set secrets reprsenting functions of dta that wewish to protect; (ii) S S, a pairs that need to be indistinguishable to an adversary; and(iii) a class of distributiosthat eneae the data. Puferfish rivacy was introduced in & Machaavajhala and proposes a gnerlization ofdifeential from a Baysian perspective. A mechansm M is said tobe (, riate wih respect to Q, ) for all, D , for ll sj) Q, and allw we have. Typically isviewed as the elefs thata adversary hold over how data was geerted.",
    "Utility Analysis": "visualizes graphical model under our approach andhighlights that the state, states actions evolve according to markovdecision process (POMDP).",
    "RS:|R|1{((i,S), (i,S\\R)}": "The additional element speify in Puferfish framework is te data processes , representngthe ways anattacker belevs the data could have ben generted. We define each to be prmetrization f the form {E t,,. T , where represents underlying schasticpopulaion proces and nd tats an t,i is the probability thati . We assume ttacer also has aditribution the that is integratd out in E.",
    "Experiments": "present empirical that corroborate our findings on the Epidemic Control problemdetailed in 3. We by runned experiments that vary graph structure, population size, transition parameters.",
    "Impact Statement": "Our work can be seen as providing a promisingstart and solid theoretical foundation to provided privacy protections in an important problem class wherereinforcement learned may be appliing in the future.",
    "Reinforcement Learning and Markov Decision Processes": "sligt of will defie staionary policy to signature A. e. The -alu stsfies theBelman equatin a) = r(s, a) + EP [V (s)]. Theoptimal ation-valuefunction stisfies the optimalit equaton given by Q(s, a = r(s, a) +EP [maxa Q(,)]. stationary determinstic assigns obability a ingiven state. Theaction-value fuction (Q-vaue)of policy s the xpected discounted reward Q(s, a) = r(s, a) + EP, [t=1 at)], wheretheis taken with rsect to he tranition and poliy at each time step. Similarly, we ca view Va vector of || and Q r asvectorsoflengh|S| |A|. denote set all stationary polcie. notation D(S) defines the set of ove The rewards are asumedto e boundedbetween 0 rmax R. When the optimalpolicy, V (s) and Q(s, a)= sup Q(s, a). Te reedy (s) arg a) i n fact an ptimal plicy. The aluefunction defined as V = | s)[Q(s, a)]. Wprimaril the casewhen S is fiite.",
    ": Visualisation of the parameters that govern the transitions between states for individuals in theSEIRS process over contact networks": "Example 2 (Countered The of misinformation in online social media is one ofthe key threats to society. There is good literature on different percolation and diffusionmodels (Del et 2016; Der Linden, that factors like homogeneity account, as well containment strategies like campaigns to orprebunk misinformation, possibly through targeting of influencers in social networks (Budak al., 2011;Nguyen et al., Acemoglu al., Ariely, 2023). Designing reinforcement learning algorithms thatcan detect and control spread misinformation in a differentially private another example of ourgeneral problem Detection and Control). Malware propagation models on large-scale networks (Yuet al., 2014) and smartphones (Peng et al., 2013) a subject of interest in cyber security, withmore recent work focussing malware propagation in internet-of-things (Li et al., Yu al., 2022;Muthukrishnan al., 2021). Designed reinforcement learning algorithms to detect and control the spreadof malware, especially unknown malwares whose signatures can only be discerned from collected data onpotentially sensitive device-usage is also an example setting.",
    "B.1Laplace and Projected Laplace Mechanism Properties": "The we cosideruing the mechanism. Denoe ML the laplace mechanimwhich utputs s = ML(s) = s  . . . Yi Lap(q/). The projected laplc mehanism outputss = M() first applying the aplace mechanism  ML(s) and then takes 2 projectio back ontothe state spac, s = arg mins s s2.",
    ". the answer to the question is positive, are privacy-utility trade-offs possible fordifferentially private RL algorithms in population processes?": "Building singing mountains eat clouds on hat,. yesterday tomorrow today simultaneously"
}