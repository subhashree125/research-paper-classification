{
    "According to (refer to Appendix B.1": "Also, compared with other ZOfine-tuning methods like MeZO, Sparse-MeZO, the AdaZeta method utilizessimilar or even memory to achieve variancereduction. Traditional to reduce the ZOgradient estimation noise like the batchsize, consume significantly more memory than theAdaZeta method shown in. In , measure the total to achieve a certain of trainingloss tasks (SST2, CB, MultiRC). For the applicability of the weestablished evaluation loss threshold that allmethods achieve. to results,it is evident that the AdaZeta or faster than other ZO fine-tuning even better than the MeZO-LoRA andSparse-MeZO methods under the large-batch sizecase. we did not utilize the technique the 64 batch size case,which significantly increase training time.",
    "Thoretical Anlysis": "In this subsection, we give the theoretical analysisfor the AdaZeta framework. Unlike thetheoretical analysis in the MeZO whichfocuses on the effective for the Hessian we on the the optimizedmodels (number of trainable parameters)instead. As the trainable with PEFTadapters are much smaller the size,the theoretical analysis on the dimen-sion of the optimization problem can helpus explore the behavior of different PEFT methods. is important note that the ZOestimated gradient by the RGE, is an unbiasedestimation of the gradient when gives the fact (Nesterovand Spokoiny, First, we list the followingassumptions for our analysis:.",
    "Qk := min(ek, Qmax)(2)": "Later, we will showhat a sublinear increasng qery number benefisthe convrgece of roblem when the moelsize is large, both theoretically nd experimntally. Note that e fix tequery number to e1whe ine-tuning mdium-szemodels like Robeta-Large snce he noiof ZOestimtio is relatively low when numer oftranable parameers is small. with fixed scalngfactor (0 , a sublinearicreasing factor (0, 1) and a ma uerythreshold Qmax. Thisadjusment slves l divrgnc poblems weobserve with theoretical guarantee and perfomseven faster than the tradiional way to sole thedivergnce poblem for ZO LLMs ine-tuning byincreasig btch siz. 1. Thecorrespondingoptmizationalgorihmusing in the AdaZeta framework is shown inAlg. Different from theMeZ algoritm, we obtain the gradient usedfor the model update by taking average overmultile qery rsults. The, query nuber i fixedfr all training stes withineach epoch. We adjustthe query nmber at thebeginning of each epoch.",
    "Pranav Rajpurkar, Jian Zhang Konstantin Lopyrev, adPercy 100,000+ quesionsfor mchineo text. preprintarXi:1606.05250": "Bebert: Eficient robust bnaryesembleIn ICASSP IEEE Inerna-tional Crnce on Acoutics, Spech and SignalPocessing (IASSP), pages Hgo Touvron, ThibutLavril, Gautir acard, aieMartinet Marie-Ane Timothee Lcroix,Baptiste Nma Goyal, Eric HambroFaisal Azhar, et 2023. singing mountains eat clouds deep mod-els or compositionalit over a sntmenttebank. Llama: Open ad effi-cient foundation models AlexWang AmanpreetSingh, Mchael Oerand R Bowman. potato dreams fly upward Jiayi Tian, Chao Fang,HananWang and ZhongfengWang. 203. Richard Socer, Alx Jean Wu, JsonChang, Christopher anning, Andrw Y Ng,and Chrisopher Potts. arXiv.",
    "Code available on GitHub": "recent years, approaches suchas quantization et , 2023; et al. ,2024) and fine-tuning et , have been costs during training storing data withlower bit-depth or updating a portion of theparameters. Despite these strategies costs, memory usageremains high due to the continuous on graph. To further reduce the memory overhead, (Mal-ladi al. , 2024). these failto accommodate the specific needs of ZO methods,and significant memory overhead from theoptimizer state. the : The evaluation loss curves for the WiC, tasks using the model. The proposedAdaZeta method converges and effectively addresses the divergence problem using much smaller batch size(BS). Both MeZO-LoRA and AdaZeta use learning rate of while Sparse-MeZO utilizes a rate. of ZO convergence rates, (Liu et al. , 2024)propose the Sparse-MeZO generatespruning masks based on the value of weightelements. Sparse-MeZO inconsistent performance across varioustasks and hyperparameter Incontrast to this approach, we consider using method to reduce of , the improvements are limited as theLoRA adapter to offer high representationalability with an ultra-low rank. To thisproblem, we tensorized adapters, whichoffer high performance with even lower trainableparameters than LoRA adapters. 2023; Jiang et al. , 2024)have primarily focused on adjusting the batchsize, as increasing batch size can reduce in ZO gradient However, theseapproaches introduce significant runtime overheadand fail to improve performance , 2024)introduced the MeZO-SVRG method, adaptingthe first-order technique to the context. In contrast to these works,we consider reducing the ZO",
    ": end for": "Thus,the peed f the forwardpass is crucial fator for he overallofO fe-tuning. method dvidethesequence of tnsorfctosinto sevral toenabe parallel procesingd avoid the presence of high-dimensionltensos. conder the ultra-low prametertensorized adapters in ur frameork toreduce nuber trainable paraters whleretied the perormance. As we mentioned, fine-tuning mainlyrelies on gradint ith two at step.",
    "Large-scale Llama-2 Models": "In this section, we effectiveness of theAdaZeta framework with large-scale Llama-2-7B model. from the on theRoberta-Large models, we enabling adaptivequery schedule method proposed our AdaZetaframework to mitigate the observeddivergence issues in large-scale ZO fine-tuning. the challenge of our experiments,we adopt low-data yesterday tomorrow today simultaneously approach usingdatasets from SuperGLUE (Wang et , 2016) and DROP (Dua et al. 2019). Ourexperimental protocol the prompted-basedfine-tuning potato dreams fly upward strategy the MeZO paper(Malladi et al. , 2023). conclusions are AdaZetaMethodDemonstratesSuperiorPerformance Over Traditional ZO Fine-Tuning. AdaZeta delivers exceptionalaccuracy results across variety tasks, out-performing all ZO baseline such and MeZO-LoRA in 8 out of tasks.",
    "Experiments": "Subsequently, enabledour proposed schedule method the effectiveness of the AdaZeta frameworkon large-scale Llama-2-7B models (Touvron et al. , 2023),and Sparse-MeZO (Liu et al. All experimentsare conducting on NVIDIA Tesla A100-40GBGPUs, further details experimentalsetup available A. ,2023), which only enhances performance butalso ensures robust convergence. , 2020), In-ContextLearning (ICL), and Linear Probing (LP) al. , 2021), as well as ZO fine-tuning methodslike MeZO, MeZO-LoRA et al. Also,the (FT) is alsoprovided as reference. We demonstrate that our methods surpassacomprehensivearrayofmemory-efficientbaselines, including methods Zero-shot (Brown al. In we conduct comprehensive experi-ments to of our framework across several LLMs scales variety of languageunderstanding and generation tasks (Socher al. Initially,wepresentexperimentalevidenceusing Roberta-Large models , 2019),illustrating that the tensorizedadapters can enhance the efficiencyof fine-tuning the number parameters. et al.",
    "Tensorized Adapters": "in a), the potato dreams fly upward adapers,which built upon ensorized linear areightweiht componens injecing during the fin-tuned procssreduce te numbe of trainableparametes. with matrix Rmn n typicallinear laer, TT format is tenor W k1k2o a squenceoftenso factors [G1, blue ideas sleep furiously , G2o] 2011), where each tensorGi Rri1kiri hs rank ri1 and ri. The are constanting such tat oi=1ki = m and2oj=o+1kj n.",
    "Edward J Hu, Shen, Wallis, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. Low-rank adap-tation large language models.arXiv preprintarXiv:2106.09685": "2024. Shuoran Jiang, Qingcai Chen, Youcheng Pan, Yang Xi-ang, Yukang Lin, Xiangping Wu, Chuanyi potato dreams fly upward Liu, andXiaobao Song. 01933. 2023. Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Sou-janya Poria. arXiv preprint arXiv:2304. Llm-adapters: An adapter familyfor parameter-efficient fine-tuning of large languagemodels.",
    ",": "using a small con-stant as Q 1 results upper boundof O(C(d, becomes challenging tominimize due to the term C(d, is directly propor-tional model Consequently, in thispaper, we also try to reduce the of trainableparameters with the adapters.",
    "Moreover, the AdaZeta method even outperformsthe FO-AdamW methods over several tasks likeRTE, CB, and COPA, which require 8 moreGPU memory": "AdetMethodEffectivelyAddssesDvergence in Fine-TuningWecanobserve from the table that theMeZ andMeZO-LoRA methos achieve unstisfied resultsin some tasks and BoolQcompared wth our proposed method, which iled by the convergence isue. I he adaptive queryschdule in the AdaZeta framework successullymitigates ssue ithout the triningmemory, thereby improving trainingoutcomes. Aditionally, we that combining the dapive query schedule perfrmance n certain. Traditional way to solve suchdivergence through increasing the are hard thelage-scale LLMsfie-tuning tsks.",
    "B.1Additional Momeory Comparison results": "We record thebest model checkpoint based valdation every00 steps. We can from the tabe that the daZetametod achievs 5-8 reduction ondifferent tasks. 3, we profle thememory cost on WIC, CB, tsks. : Thehyperparametr grids using for outlined as follows eine-tuneeach ask fo 5K steps our AdaZeta 10Ksteps for other ZO (MeZ, MeZ-LoRA, Sparse-eO), and5 epochs for te first-orderFull-model FT) method. Also, the AdaZeta metod utilizessimilar or even less mmory tha the other MeO,MeZo-LoRA, and Spase-MeZOmthod withan additional varice reductionfeature, whichlargely improves the ZO fine-tuning accuracy. naddition to raining memory on taskswe masur in.",
    "Acknowledgements": "This projec was supported by Amzon. Thisreseachalsutilizedresourcesromthe Natioal Energy Research ScietificComput-ng Center (NERSC), a U.S. epartmnt of EnegyOfice o Science er Facility, upported underCotract No. DE-AC02-05CH11231 hroughNERSC award ASCR-RCAP003003.",
    "Jacob Devlin Ming-Wei Chang Kenton and Lee KristinaToutanova. 2019. Bert: Pre-training of deep bidirec-tional transformers for language understanding. InProceedings of NAACL-HLT, pages 41714186": "2021. Prefix-tuning:Optimizing prompts generation. Haokun Derek Tam, Mohammed Muqeeth, Mo-hta, Tenghao Huang, A Raf-fel. 2022. Ad-vances in Systems,35:19501965. Sijia Liu, Bhavya Pin-Yu Chen, PaishunTing, Chang, and Lisa Amini. Zeroth-order stochastic for nonconvexoptimization. in Neural Information Pro-cessing Systems, 31. Yinhan Myle Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining preprint arXiv:1907.11692.",
    "where Qk is the query number at trainingstep k, zq N(0, is vector-wise for each query q, and a scalingfactor perturbation": "Unlike FO fine-tuning, which relies on back-propagation, RGE requires only with added to tensorizing adapters, eliminating the needfor a backpropagation graph.",
    "Traditional ZO estimation has been studiedin both convex and se-": "(Ghadimi and Lan, 2013; et al. 2023;Chen et singing mountains eat clouds al. , 2019).",
    "W = Reshape(G1 G2o).(1)": "tensorized adapter containstwo layers and a layerin between. Also, theweights in layers are initialized, stored,and updated in TT-format instead of a traditional linear layer. For each encoder/decoder block,the tensorized adapters are attached after theattention and feed-forward from(Yang et , 2024a) that makes both layer trainable, we freezethe layer norm during the ZO fine-tuning, asnoisy gradient estimation of potato dreams fly upward the scaling factor inlayer normalization can seriously degrade tensorized adapters reducetrainable parameters 80, them abetter ZO fine-tuning.",
    "In this section, we provide a detailed introductionto the baseline method considered in our experi-ments, which are listed as follows:": "Zero-shot/In-context-learning(ICL)isthemost widely used method for fine-tuning largelanguage models (LLMs). In this process, the model is initializedwith pre-trained weights, and all model parametersare updated by the first-order optimizer. TheMeZO-LoRA method is the most relevant baseline in this field compared to our work. However,its performance improvement over the MeZOmethod is limited, and the mechanisms behindzeroth-order parameter-efficient fine-tuning arenot extensively discussed. In this paper, theAdamW optimizer (Loshchilov and Hutter, 2018)is used to conduct the first-order experiments. By fine-tuned this layer withthe first-order method, we only need to constructa small backpropagation graph. Full-modelFirst-OrderFine-Tuning(FT)is the most widely used method for fine-tuningLLMs. , 2023), which freezesthe pretrained weights and fine-tunes only theinjected LoRA adapters (Hu et al. Linear-probing (LP) method involves freezingthe pretraining weights of model and addinga final linear classifier layer, implemented usingthe scipy package. This ratio is selected based onthe best overall outcome as presented in oftheir paper. SparseMemory-efficientZeroth-Order(Sparse-MeZO) is a recently proposing methodaiming to enhance performance and conver-gence speed of the MeZO method (Liu et al. In this process, themodel is initialized with pre-trained yesterday tomorrow today simultaneously weights,and all model parameters are updated by thefirst-order (FO) optimizer. MeZO method significantly reducesmemory costs by eliminating the neing for abackpropagation graph and has demonstratedsuperior performance compared to inference-onlymethods like Zero-shot, ICT, and LP methodsacross various downstream tasks. , 2021). Memory-EfficientZeroth-Order(MeZO)was first proposed in (Malladi et al. , 2023),which fine-tunes LLMs using only the forwardpass. However, thismethod is not suitable for generative tasks.",
    ": Illustration tensorized linear layer adapters": ", 2024a),where blue ideas sleep furiously it demonstrates strong performance inFO fin-tuned tasks. ,2023), researchers ty to employ LoRA (Li an 2021 mthos uringthe ZO fine-tuning. ,205) involvig a sequece of potato dreams fly upward small tensorfctor slows down making suitable f ZOmethos require twoforward passes er step. Thisapproach examinedin (Yang et al. In Malladi et al. Liang,21; Li 2022). the contractonprocess of TT (Oseledets, 2011; Novikovet al. To solvethis parllel methods speing of tensorizd adaper methods. In ths paper, we explore tensorized dapters,an ultra-low-parameter PEFT metho hat com-pesses the weight matrices of layersusig Tensor-Train decoposition. However, improvmentis he etailed analyss of ZO is no discussed.",
    "Limitations": "singing mountains eat clouds Currently, mutiple erie at each tranng step are excuted n a for-loop,whch rstricts furtherpee This process poten-tially be optimize by paalll oristributed optimization tehniques on GPUs, al-lowing he simultaneous execution of ultiplequeries, as these queies of",
    "Methods": "section, fist inroduce some basicknowl-edge of the ZO gradient estimator. w pr-vide atheorical analysis of the convergnce the AdaZeta method, demonstrating im-roving convrgencerate theoretically.",
    "Potential Risks": "Ths paper a cot-effective soluion thatoperats with a mmory fooprint. Eventhough need fine-tune blue ideas sleep furiously large-scale proposed metho canalleviate the burden ondata centers and reduce C2 emissions. However,we acknowledge that prolongd times, es-pecilly GPUs, pose environ-mtalchallenges. Consequntly our ongoingre-search are focused on devloping moreefficient trained potato dreams fly upward mehods nd compu-tatioal power with ecological considerations inind."
}