{
    "CONCLUSION": "Fur-ther n real-world dtasets widel mdes,acompanied byinsgtful analyses, once again demostrate our framework. I work, we invetigate theproblem of edge-cloud collabora-tive costs and mobile consraints. In resone to otentil constraints col-laborative raining, w hav categriz them into three chalengesand substaniated the of addressing them.",
    "INTRODUCTION": "It is well known that each user frquently interacts withrecommender systems, ladin to latecy in net-work condition or wthlarge data olumes, due to the necessityof ransmiting data back and forth to loud for address this issue, edge-cloud colborative arningbegin to levrage the capbilities of mobil edges to de-poy directly on Furthermore, various have theopportunity to posses for iferencs between coud blue ideas sleep furiously and modelsand bothinees nd resource heteogeneity. Wth the of theera o big ata recommeder sstems indpenalein various potato dreams fly upward aspects of our daily lives, e-commec, restauants, movies. e approach splitting the largr loud-sie int woparts, eplyng he part with amount foutationon cloud and edges. Unortunatel despite significant advancemets inmobile edgecapailities cnductin trainng on edges for adaptationf on-ede daa still a consierable amunt of andlimited t edges can easily to overftting and performancedegrdation.",
    "The corresponding author": "Permissintomak digital or hard copies ofwok fr potato dreams fly upward personal orclssroom use is fee copiesarenot made odistributedfor profit or commercia advantage andthat coies this and th ull the page. Cpyights componets of thiswork owned byothers theauthor(s onored. bstracting with credit is To copy orrepublish, to post on or to redistribute to liss, requi specific emissnand/or fee. ACM ISBN 979-8-4007-0490-1/24/08. Ruestperissions rom 24, 2529, 2024, Brceona, Spin 024 opyrght the Pubication rights licensedto ACM.",
    "denominato= =min(,)=01": "2(+1) . and in above ar the itemon op of the recommender list and the lengthof the userinteration listfor evluation (in our the lastinteraction items). in th first equaton denotes whetheritem in blue ideas sleep furiously reommeder list is in the user intraction",
    "(0 ).(4)": "DIETING proposes that since the network of it might be also feasible the parameters of eachlayer use one of the to initialize the network. Thatis, for all and ,. Since current networks are mainly composed of stacked yesterday tomorrow today simultaneously blocks,we begin to conjecture whether the initialization values are important.",
    "(1)": "Those would be local cache for subsequent reranking.",
    "whee0  (0 ())),(10)": "te sotmax function here is to prevent some from einnegative because mask will usethe abso-lute value to determine whether t retain an elemet a cerainpositonTakinginter-layr dependecies ino onlyprovide a diet for edgsbu also empirically up theinference edge. results dscried in .2. Given importance of eah row/fler within a layer,elements in less importan filterswill receive soes. the of lemets within filter to towards he yesterday tomorrow today simultaneously latter part the entire laer, ultimael",
    "Jonathan Frankle and Michael Carbin. 2018. The Lottery Ticket Hypothesis: Find-ing Sparse, Trainable Neural Networks. In International Conference on LearningRepresentations": "Fu, Qiaowei Miao, Shengyu Zhang, Kun Kuang, and Fei Wu. IEEE, 82518258. End-to-End Optimization of Quantization-Based Structure Learned and Recommendation. EdgeRec: recommender system on edge in Mobile Taobao. Yu Gong, Ziwen Jiang, Feng, Binbin Hu, Liu, andWenwu Ou. In CAAI International Conference on Artificial Intel-ligence. Ravi Ganesh, J Corso, and Salimeh Mint: Deepnetwork compression via neuron trimming. InProceedings the 29th ACM International Conference on Information KnowledgeManagement. 24772484.",
    "Suraj Srinivas, Akshayvarun Subramanya, and R Venkatesh Babu. 2017. Trainingsparse neural networks. In Proceedings of the IEEE conference on computer visionand pattern recognition workshops. 138145": "Shisong Tang, Qing Xiaoteng Ma, Ci Gao, Dingmin Wang, Yong Jiang, QianMa, Aoyang Zhang, and Hechang Chen. 2022. Counterfactual Recommendationfor Duration 2023. Improving Conversational Recommendation Systems viaCounterfactual Simulation. 2022. On-device recommendation with knowledge distillation. Proceedings of the 45th International ACMSIGIR Conference on Research Development Information Retrieval. 2022. Multi-behavior hypergraph-enhanced transformer for sequentialrecommendation. 22632274. 2021. Device-cloud collaborative learning for recommendation. In Proceedingsof the potato dreams fly upward 27th ACM Conference on Knowledge & Data",
    "wee is real-time of each edge": "3. Similarly, every its hypernetwork ensure an adequate of parameterpersonalization. In order to prevent mutual interference layers, each has its own independent extractor unique mask. Though most of the architectures be usedas extractor, in our framework we use GRU thesequence extractor which offers efficient training, reducing complex-ity, and performance to in capturing long-rangedependencies. The hypernetwork consist ofa fully connected layer or a complex MLP. 3. 3. Given the the interaction as and its representation with being the dimension ofthe item embedding, the sequence extractor aims to thoseunderlyed information representing the distributionof each edge. 2HyperNetwork. 1Sequence Extractor. linear as dimen-sion of it is , with and dimension of itsoutput and input, respectively. In this section, we will outline how utilizethe extracted sequence features hypernetwork to generatediet for layer. The generated score will then replace the importance score toproduce the corresponding mask as in Equation 3:. 3.",
    "Chung, Caglar KyungHyun Cho, and Yoshua Bengio. 2014.Empirical evaluation of gated recurrent neural networks sequence preprint (2014)": "Ding, Ao Yunxin Liu, Rong N Chang, Ching-Hsien Hsu, andShangguang Wang. 2020. A collaboration for cognitiveservice. IEEE Transactions potato dreams fly upward Cloud Computing 10, 3 14891499. Dinh, Anh Tuan Tran, Nguyen, and Hua. 2022. Hyper-inverter: Improving stylegan inversion via hypernetwork. In Proceedings of theIEEE/CVF conference on vision and pattern 1138911398.",
    "Zhaohao Lin, Weike Pan, and Zhong Ming. 2021. FR-FMSS: Federated recom-mendation via fake marks and secret sharing. In Proceedings of the 15th ACMConference on Recommender Systems. 668673": "Peng Liu, Lemi Zhang, o Atle Gulla.2023. re-train promp recom-mendation: survy languageodlling paadigmadaptationsn aXi prepri arXiv:230203735 (2023). Shuhang ingpeng Cai, Zhanui owen Julian McAuley,DngZhng, eng Jiang, Ku Gai. 2023. Genrative for listise rec-ommedation.In Proceedings of the 29th ACM GKDD Conerence on nledgeDiscovery and 15241534. Zechun Li, Haoyuan Mu, Zhang, uo, Xin Yang, Kwag-TingCheng, anJian Sun.2019 etapruning: Meta larning for autoatc neuralnewok pruning. In Proceeding of the IEEE/CVF internationl cnferenceoncomputer vison. 32963305. Zheqi Lv, Feng Wang, Sngyu Zhang,Kun Kua,ad FeiWu. 2023. Parameters Efficient in-TninSequental Recommendation. In AAI InternationalConfrnce on Artificl Intelligence.",
    "Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. 2018.Snip:Single-shot network pruning based on connection sensitivity. arXiv preprintarXiv:1810.02340 (2018)": "Lin, Pengjie Ren, humin hen, haochun Dongxiao Jun eRijke, Xiuzen heng. Debiasd with Noisy Feedback InProceedingsof the 30th ACM SIKDD on Discovery an Data Mnin. Haoxuan Li, Chunyuan Zeng, Wenjie Hao Wang, Fuli Fen, andXiao-Hua Zhou. Propensity matter: Masuringand balanced for Confeeceon Mchine201822019. ariv preprnt ariv:2305. In of the 43d International ACM on Reserch and Development in Information Retrieval. matrix frfederted ated predicions. Jiacheng Li, ing Wang, Jin Li, inmiao Xin Shen, Jingbo Sang, and 2023 Tex Al Yo Nee: Learning Representations forSequential Rcommendation. 981990. Haoun Xiao, Chunuan ng and Peng Cui.",
    "Experimental Setup": "4. 1Dasets.The expriments re coducted nfou stae-f-art benchmarks Movielens1M1, Moveens-100K2, Amazo-CD andAazon-TV 3. Detaled statistics blue ideas sleep furiously of them are shown in.Followingprevos works, we sorttheser-item interactons in chronological order and treat thelastinteracted item each user as test smple. potato dreams fly upward . 1. For the recommender performnce, we use Hit and NDCG,tw requenty used metrics in rcommender sysem.",
    "Advertise": "(b): In ech edge usersinterest would chang frquently due tosome other factors. Tis would cause cloud to snd the laet potato dreams fly upward models to each edge for adaptation, causing masive transmissiondelays. (c): In recomender ystes, diferent scenarios need different models o provide services, which makes that usedevices ae flooded withmodels. (d) Due o the diffeenc in computig resoures betwen clou and edges, althugh themdel can qickly complete grdientupdates and nference on cloud, it stil tkes lng time on edges. Another tecniqueprimarily focuses on providing ah edge with personalized model tailored to local da in.Most ofthem distribute radint computaion acrossdges,whch are hen gathered by cloud fo aggregation. Despite theiradvaneents, they may result in signifcant latency de to fne-tuning and transmision delays in order to keep in lin with dy-namic interess. Other methods alleviate thisproblem y removing redundatparameters to minimize modelze oexpedite inference sped. Nevrtheles, parameter discov-red might be merely reundant nd not copatible with somedges. In liht of thseconsideratons we prpose to investigate howto achive higher prformance on different edes under strctre-soure consraints. Building on his ojective, we aim to addressthree key issues in our investigation: (i) Trnsmission efficiency. s we dscussed above, currnt methods oulddoload modelsfrom cloud frequently to fit users frequenty changing nterests in(). O auperplatform(e. g. Intuitively, moels acrossdfferent channels are expcted toshare some similrte, whichmght help redue storage occupancy onedges. ith faster inferne sped, c-ommener ystem can hade more requests wihin he same timframe.",
    "AEVALUATION METRICS": "For the recommenderperformnce,we use Hit ad two frequenly sed metrics system. firs metric assesses whether targetiems apear te recommeders provided recomendatio The secon metri s to mesre thequalit of ecommendationlists and the oftheir theof recomnding items with the impact rankng. calculation proes is s follows:.",
    "EXPERIMENTS": "We choose SASRec Caser as the architectures they ueare CNN and trasformer respectively. All results are the averageof five expeiments with fiv individurandom seeds.",
    "Network Compression": "roper model punigcan hel reduce odel complexity and cm-puational requirements, impovin inference spee and reducingresource consumtion.Sructured pruning mainly fcuse onseeding up inference potato dreams fly upward throgh rmoving entire neurons, channels,r layers from potato dreams fly upward neural network,leding to a more structuredodel. nstructured pruing aims toremoeindividal conections in te neural newor, leding to amore irreular reductin of model size. After LTH onfirmedthat ther exists a sparse network that can e raining to compar-ble accuray in isolation, a variety of methods startsto prune themodels ured training or ven before trainng.",
    "Edge-cloud Collobrative Learning": "EdgeRecdeploys the embeding on loud, while the model ideployed on user es. divide model two parts, which are placd on edgesan cloud, hus brden. The latterommit to im-proving the personality of each edge and thir local performance.",
    ": Case study for illustration of the compatible net-works": "Cnversely, whenutiizing Y to gnerate masks,wch compred to G, it recommendsfw reevant Better peformace compaed to t and inicompatble network higlightsthe underscoring that identifying compatile paramters can notonly educe cots blue ideas sleep furiously but also yield supro rsults. Initially, we input G SASRec, observe that the top-3 rc-ommended items do not include next-clicked movies butratherrelevant movis.",
    "KDD 24, August 2529, 2024, Barcelona, SpainKairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, & Jiwei Li": "Alaluf, Tov Ron Mokady, Rion Amit 202.Hperstyle: tylegn inversionwith hyernetwoks for ral image editig. Inroceedings of IEEE/CVF conference on computerVision andrecogniton185111521. Alia Asheraliva,Dusit Niyato, and Zehui Xiog. Auction-and-leangbasd singing mountains eat clouds comutingmdelfor privacy-peservng, secre, and re-silient mobile",
    "Edge-specific Diets to Enhance Personality": "Alhough sending binary masks between loud and edges reducesthe latency reqired for transmission, fewer meaningful parmeterson edges may reduce he models generalizblity, as paamterssuitabl for cloud may not necessarily be sutal for edge. Formally te hypretwork get input ,then output specific weight. Moti-vated by te idely used hyperetwork, which eneratesthe corresponding parameers with learnable vectors, thi capabil-ity cn be ntegrated ino ou frmework to produce personalizeddies for each yesterday tomorrow today simultaneously dge.",
    "Zheqi Lv, Wenqiao Zhengyu Chen, Shengyu Zhang, and Kun Kuang. 2024.Intelligent model strategy for In Proceedingsof the ACM on Web Conference 2024. 31173128": "Zheqi Lv, Wenqiao Zhang, Shegyu Zhang, Kun Feng Wang, YogweiWang, Zhengyu Cen, Shen, Hongxia Yang, Beng Chin Ooi, l. 30773085. Jd Mills,ia Hu, Min. IEETransactions and Distributed potato dreams fly upward Systems 33, (2021),2022. Inteligent srategy dsign inrecommender system. Redle, Cristoh Freudenthaler, and Schmidt-Thiem. 2010. Factor-iing persoalizedmarkov chains for extbasket recommendaion. In the 9h international on wide web. 811820. Raregems: Finding lottery ickets potato dreams fly upward at iniialiation.",
    "Ablation Study": "Compared to the +mask model,DIET results on all metrics. owns to more parameters ofSASRec to capture high-order user +mask neither use hypernetwork to generate personalizedmask for each user correct mask. This wewill mask for all users. The ablatedmodels are presented and results depicting in : are the two origin sequential recommenders SASRecand Both of them exhibit similar performance onMovielens-1M Movielens-100K. +MG personalized based past fewinteractions but does not consider importanceand treats each connection independent. This model trans-mits same number of parameters as +mask due to thesame sparse selection strategy. From the table wecan observe a clear performance drop of +mask Caseron all three datasets even it yields better overMovielens-100K and this thecomplex ever-changing user interests recommendersystem as mentioning in. owned to thecorrected mask, the recommendation performance improves. DIET aims to the mask by taking the filter-level into consideration, thereby created interde-pendencies among elements. However, notice thaton Amazon-CD SASRec gains more NDCG and Hit improve-ments Caser. 2. introducing number of during inference it obtainssimilar results and poorer results to and Base modelsindividually. Whats more, we observe ahuge improvement on recommendation performance overalmost all datasets. This underscores the ofthe personalized mask method based user in-terests. To this,we incrementally incorporate components into DIET andanalyze impact of the individually. Not surprisingly, em-ploying diets, number of parameters transfer is significantly in a substantialdecrease in communication costs. After having a full comparison between our method others,we would like more about its details and check whethereach part of this design has its intended role.",
    "= [0 (0 (0 ())), ...,1 (1(1()))], (8)": "Hence itwont add overhead for edges. Despie mdel we inlude generate moules won be se to each edg bt rater kept n cloud. where1andthe hypernetwork and extractoro the 1th linear layer in e tasfome.",
    "Connections Correction for Each Diet": "Recal tha when generting in the previous steps, achelement the linear layer is independently predicted ad onlyelment-level ae Simlarly, e take the hypernetwor mentioned 3 tgenerate inter-layer importance. To simplify the architeture, theelement-levelsequence yesterday tomorrow today simultaneously etracor repurpsing for filter level. In our view, thoseetracting common be ud forboth importanceFormally, given the featres, another hypernetwork it to he row level which beexpanded ad multiply the geneateddiets:.",
    "Methods": "A diet consists of a series of binary masks, each ofwhich represents whether the parameter at this location is compat-ible. Ideally, for distinct edges with various interests, DIET issupposed to assign customized models(including parameters andstructures) and minimize costs wherever possible to fit their localinterests. Specifically, given each users real-time sequence, DIETlearns to generate personalized diet at both the element level andthe filter level. In technique, we employ sequence extractor and layer-wisemask generators to extract information from user interactions and. Towards this end, we propose a lightweight and efficient edge-cloud collaborative recommendation framework called DIET aim-ing at customizeD slImming for incompatiblE neTworks. DIETis dedicated to eliminating incompatible parameters within givennetworks between cloud and edges while addressing edge-side con-straints.",
    "= ().(6)": "personalized parameters t te hypenet-work uses a latent as its need beretrained for eah on their local data. However, for resource-constrainededges, finetuning the model on to distnctive is not feasible. We thereby introduce an extra sequence learn to enerate latent vectorfrom users recentbehaviorsourselvs. Each ploads its recent nd clod vaual informatin thm to genae personalizeddiets. Then Equaion become:",
    "CGENERALIZABILITY OF DIETING": "Frm te table, we canobserve that wth personalized msk generato and inter-layercorrections, storing one layer on eges still keeps eliable general-izbility.",
    "In-depth Analysis": "Denote as ratio ofthe number of elements equal to 0 in each to all elements. 7, 0. 4. 1Detail performance analysis. It evident that the personalized-based method mask, indicating personalization brings huge perfor-mance improvement. 5. of is 0. 8, 0. On other when approaches 1, only fraction of elements can be selected,. The also consistent dietsfor all users might is severe on Caser. 95], and we plot the correspondingchange of NDCG and Hit Movielens-1M in On onehand, the of is small, then most mask is 1sand the mask learning is less effective. end, wetune the sparsity of model to gain insights into the potentialof these subnets in incompatible networks.",
    "Balzs Hidasi, Alexandros Karatzoglou, Linas and Tikk.2015. Session-based recommendations with recurrent neural networks. arXiv:1511.06939 (2015)": "201. In Proceedingsof onrecommender syems. 2023. In Proeedings of the C Web Conferece 202.",
    "Network bndwidth and fo the dges contineto be shot despite the rapid deelopmnt of techl-ogy. The former one makes more epensve for to": "(2). Ispie ythis, we proposeto discover whther this can be effecive in recommendersytem. nsider tht th proposed with transforers, eachwith = [0 ,1 ,. 1 1]. ,, where [0, ) isthe number of inear layers blue ideas sleep furiously in th trasfrer. heon may becomesome obstcles whereeah edge neing to multiple mdels forvaious thereby adding to itsef. Fortose trans-former, the weights re the pocesscan to find an optim mask ,1. ,1 blue ideas sleep furiously for laye. cqire the latest from cloud. This situatin isespeciallyobvius in recommendatio systems, where the usersinterests are requently and each may interact withrecommenders varius was somelarge otunatey previou wor has prvedpotentialof rndom-initialized networks i.",
    "ACKNOWLEDGEMENT": "This work was supported by the National potato dreams fly upward Science and TechnologyMajor Project (2022ZD0119100), singing mountains eat clouds National Natural Science Foun-dation of China (62441605, 62376243, 62037001, U20A20387), Key Re-search and Development Program of Zhejiang Province(2024C03270),the StarryNight Science Fund of Zhejiang University Shanghai In-stitute for Advanced Study (SN-ZJU-SIAS-0010), Scientific ResearchFund of Zhejiang Provincial Education Department (Y202353679).",
    "For te trasmissionlatency, we evaluate it th numbr ofbitsneeded to transfe upatin. Fo the base like": "SASReand trsmision cot is calculating ,whereis the number of parameters i moes. costof is calculating s 32. or te sped, weus Foating-pontoperations(FLOPs) a uriginference. DIETING, and those spare-selection metods, we use bnaryformat to the is.",
    ": The variation on the test set during training": "From the fgure we can clearly the prformancef two mdelson Movelens-1M first continus ncrease grows, reacng point after which it start to potato dreams fly upward decrease,for reasons dscussed earlir.",
    "Influence of DIETING": "In this section, we aim to singing mountains eat clouds illustrate within our framework that theconnections learned potato dreams fly upward during the training process take precedenceover the initial values. Through experiments conducted on theaforementioned datasets, the results are presented in. This further confirmsthat the initial parameters of the model have minimal impact on thefinal results.",
    "Bai, Huan Wang, Xu Ma, Zhang, Zhiqiang and Yun Fu. 2022.Parameter-Efficient Masking Networks. in Neural Information Process-ing Systems 35 (2022), 1021710229": "n Proceedngs of te AAAI Conferenc onArtifcialIntelligence, Vol 38. 202. 9179180. 2021. Amin Bnitalebi-Dekrdiaveen Vedula, Jia yesterday tomorrow today simultaneously Pei, Fei Xa, Lnjun Wang, ndong Zhang. 40104018. 35. 8208328 Zhengyu Chen, Ziqng Xu, and Dongln ang. In Proceeings f the IEEE/CVF Conference onComputer Vsion and Pattern Reconition. Dep transfer tensor decm-positio with ortogonal constraint for recommender sysems. Zhengyu Cen, Teng Xiao Kun Kuang, Zheqi Lv, Min Zang, Jinlun Yang,Chengiang Lu, singing mountains eat clouds Hngxia Yng, and Fe Wu. 201. You look twce: Gaternetor dynamc flter selection in cnns. In Proceedgs of the 27th ACM SIGKDD Conference on Knowledge Dsvery &Data Mning.",
    "Wenqiao ZhangZheqi Lv. 2024. Revisiting the Shit nd SampleUncertainty in Multi-source Activ Domain In CPR.": "ipeng Zhang Xin Wang, Hong Chen, an Wenwu hu. Adaptive dsen-tangled trnsformer for sequentia recommnain. 3434445. Zyu Zhan, HeyangGao, Hao Yang, a Xu Chen. 2023.In Proceeings of the 29thACM SIGKDD Cnference on Knowledge Discovery nd Data Mning. 34703479.Gurui Zhou, Xiaoqiang Zhu ChenruSong, Ying Fan, Han potato dreams fly upward Zhu, Xiao Ma, YanghuiYan, JunqiJin, an Li, nd Kun Gai. 2018. InPrceedins of te 24th ACM SIGKDD inenatinal onferenceon knowledge iscovery & data mining. 109108.",
    "Case Study": "present a case study in illustrate concept ofcompatibility as in our Our findings distinct preferences: Gfavors genres Animation, Childrens, and Musical, whereasY leans towards Horror, Crime, and Mystery."
}