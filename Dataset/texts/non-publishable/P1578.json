{
    "RQ2. What are the effects of DP on the dynamics of goal selection?": "Then, the agent sholdfocus onthekils, up ttemptsgeerate goals. 0.0 .4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8pobabilityepoch cumulative frquency puple red urple red purple red purpl purple red : The effects of DP on gal ovr taining time in te Hlf-Cheeah environmen.The agent is laning five sklls,ach shownin different clour. Rigt: cumulative of goal selecion. intial trend epohs t thenumbe of skills s dueto re goals are randomly without Algoithm , ll. Rd initiall provided lo DP so wa largeyigored util it assufficentl (the policy i notcopletely reedy,so red tilupdated occasionally despit itsdifficlty) or othrs rovide less DP i comparison (e.g., urple clining). the cumulativefrequncies ofwe see thered eventually aught upexplined by itsincrease in .",
    "C.2Optimiser and reward function": "I optimising its policy to maximise Equation 9, the agnt relies onsoft (SAC) (Harnojaet al. , 2018), an of-policy maximum entropy actor-critc algorithm. he xpecation is maximise the intrinsic.",
    "eg(t + 1) :=1 q(g | st+1),if g corresponds to the skill being followedq(g | st+1),otherwise,(5)": "where eg(t + 1) is element of e(t + 1) R|G|, vector composing of the 1, eachgoal, and q a Then, the errors over an epoch of fixed time length T form a T |G|matrix. Given hyperparameters , and smoothing, , (see .2), we use Equation 4 recent errors, e(t + and the average from time earlier,e(t + 1 ). Following Equation 3, we the for each goal, with goal g corresponding tooption n",
    "Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXivpreprint arXiv:1611.07507, 2016. URL": "Tuoms Haarnoja, Zhou, Pieer Abbel, Sergey Lvne. Sot acto-critic: Off-policymaximum ntropy deepreinforcmentlearning with stchasticactor. In Proceedings of 35thIntenational Conference on Machine Learning,volum 80, 18611870. PMLR, 2018. Steven Hansen, Will Dabney, Tom Vande Wiele, ad Fast task infrence with variaionl intrinsi featues. In 8th IternatinalCoference on Learning Repreentaions,202. URL R Devon Hjelm, Alex Fedorov,Samul Karan Phil achman, AdamTrchler, Yoshua Bengio. eaning representations bymutua informaion In 7t Intrnationa ConferenceLearning 219. Leslie Pack Kaelling. Larning toachieve In of Thirteenth International on volume 2,ges 10948. Internationaloint Artificil Intellience Orgnization, 1993. UL Michael Laskin, ao Liu, Xue Peng Dens Aravnd Raesaran, and Abbeel.Usupervised reinforcment learned with cntastive control. In Systes 35, pages47834491 Curra Assciates,Inc. 222.RL",
    "arXiv:2411.01521v2 [cs.AI] 6 Nov 2024": "These rewards between the goal-defining variable, g, and some function of drawn from thecorresponding skill, f(T Typically, f maps a trajectory to a single state, some formulationsmap to (e. g. Gregor al. Following Hansenet al. p.",
    "Acknowledgements and disclosure of funding": "We thereviewers their service and for providing useful future-looking feedback. EML and CG financial support from the Research Council of Finland (NEXT-IM, grant349036) the Helsinki Institute for Information For their time, ideas, and feedback on the measuring skill we thank Marlos Berns, Nam Hee Kim, and Acerbi.",
    "RQ1. Does the probability mass goal distribution collapse?": "To learn a diverse set of skills, it is a substantial of skills. p. 6) showed with VICs goal-selection method, effective number skills (seeAppendix D. method by et (2019), thereforeselects goals uniformly. does not collapse,and, additionally, have control over the effective number of skills the softmax temperature. effective #skills epoch (b) (c) yesterday tomorrow today simultaneously Ant VICDIAYNDP (TSM =. The linear decline of the effective number of skillsfor up to number of skills due DPs that is, selecting goalswithout (see Algorithm 1, ll. Results from five random each line a seed.",
    "Aly Lidayan, Michael Dennis, and Stuart Russell. BAMDP shaping: a unified theoretical frameworkfor intrinsic motivation and reward shaping. arXiv preprint arXiv:2409.05358, 2024. URL": "InAdvances in nformation Pocessing System, vlue 1, pages Curran Associates,Inc. Erk M Lintunen, Nadi Ay, Christian Guckelsberger, ndSebastian Detrdig. ,. self-determination theory with compuational blue ideas sleep furiously intrinic motivation:of comptnce. io/7qy35, 2024. In Proceedings ofth Thirty-First Intnatnal int Conference ArtificialInelligenc,pages 55025511. isual learning withiined goals. InternationalConfeences on Artificial Intelligence Oaniza-ton 2022. PsyArXivpreprint 10. 31234/of. URL VNair, Vitchyr Pong, Murtaza Dalal, Sikhar Bahl Steven and SergeyLevie. URL inghuanLiu, Menghui Zhu, and Weinan reinforcemnt Prb-lems and solutions.",
    "I(g; f(T g)) H(g) Egp(g),T (g)log | f(T g)),(2)": "2).That is, singed mountains eat clouds a probability model, q, predicts, for eh ski,the probability that the skill has induced the observationsus the moel is caonically kown as adiscriminatorad is typicaly us t compute te reward. Successully discriminatig skills in theobseration space requires the agent to observe distint regons of the statespae, ecouragingtheaentto larn a et of diverse behavios.",
    "Conclusion and future work": "W propose DP as methd for leaned goal-selection policy in discrimnability-motvated RL,prioritising oals base onoverl mproeets in discriminability. e hae shown in theeexeiments, that: (1) DP-motivated gent learns a distribution over goals ithout the probabilitymass collapsing; (2) the DP values motivate goal selectionwithrespect to oserved LP; and (3 withD, an aent can learn a yesterday tomorrow today simultaneously divrse set of skills in less tained time than with uniform-random selection. In uturework, we am to better undersnd how diferent factors affectgoa selection. We plan to tesother intrinscewrds combinig discriminability with LP, incluing absolute LP, whre the agenalso attends to goals hat t is freting(i.., skills decreasing discriminability). Diffent entropyregularsationregmes may enefit divrsityin terms ofincreased stte-space cverage but makediscrination of skills hardr. 6, 78), comparig with othr discrimiator-baed C-IMsand on a range of environments singing mountains eat clouds incudig no-episodic andstchstic oes. However, evaluatingopen-ending learning is notriously difficult. 118)",
    "D.1Environments": "0. 1,. We modifiedthe action space to a [0. 05]2 (wherea the orignl sedlargr actions, 0. (201). The D Naigationenvionment afford iagostics intothe algorithms and with MuJoC werepresent asks of complexity  ofobservation spae nd degres offredo. Specifcaly, we used three enviromentstested yEysenbach et al.",
    "Andrew Stout and Andrew G. Barto. Competence intrinsic motivation. IEEE9th International Conference on Development and Learning, 257262. IEEE, 2010. URL": "Emanel Todorov, Tom Erez, and YuvalTassa.Mujoco: A physics eginefor mdel-ased contol.In 2012 IEEE/RSJ International Conference on Intellgent Robos and Sysems pages 50265033.IEE, 2012. ULDavid Warde-Farley, TomVan de Wiele, Tejas ulkarni, Catalin Ionescu, Steve Hansen,andVolodymyr Mnih. Unsupervised control through non-parametic dscrimiatve reards. In 7thntentional Conference n Learning Representaion, 2019. URL",
    "Abstract": "No-uniform goal selection hs the potential to th reinforement learning(RL) of skills oer unifrm-random In tis paper, we intoduce a learning a policy intrisically-otivtd goal-conditioneRL iversiy DP). The learner  currculum based o observdimprvement indiscriminability set goals.Our proposed method isappicable to the class of discimnlity-motiatedagent, where the intrinireward is computed as a funcion of the agents of the true goalbing pursued. We demonstrateempirically a DPmotvated agntcan learn a skils than previous approches, and doso without sufferig frm cllapse of the disributiona nown withsoe prior We end wit plans to take hisproof-of-concept forward.",
    "C.4Hyperparameters": "We tested various andidaes for theDP hyperparameters: smoothing, , ofset, , and sftmaxtemperature TSM. Howevr, since we currently lack a good diversity metri differncesi performance,the ou experimentswere ainly based n qualitativeobservtions. Smothing time seps, offset 250900, and sftma emperature0.10.7. hehyperparamter valusused in producing the . The mostnoticeable difference was given by vrying TSM. To achieve in level of greediness of we mean before computng over goals. For rference, in the 2D environent, pisodes last for 100 time steps, so a smpled every10 episodes. Inthe Half-Cheetah isodes lat 1000 time steps, so a newgoal issampled every episde In the episodes last a maximm of 1000 time is a otentialtermination condition befor maximum eisodelegth), so a sampled goal",
    "Introduction": "How to a diverseset ois subproblem C-IMs et 2022, p. Itis postulated in existing work tht dscriminablea set of goals is, the more dverse exet the skillsto bein of observed behaviour (see. Many such discriminator-basing yesterday tomorrow today simultaneously models elet uniformly during trained yet,learning dsribution over hs the potenil o speed up earning (see ). Ou contribtion i threefold. (3) We detil plans for thi proof-of-conept forward",
    "RiG := log q(g | s) log p(g).(10)": "This expreses that the agent is rewarded for to the skill beed folloed (see. 1). The distribution goals, i: (1) ad (2) uig (wedescribe etod for goal-selection policy ). At t beginning of an epoch,te agent a gp(g), and folows it ntil terminaton.",
    "D.3Evaluating skill with dimensionality reduction": "To generate we first draw 100 trajectories from each skill, for each: (1) a set of randomskills (no learning), (2) skills learned DIAYN (uniform goal selection), and (3) skills learnedusing with DP goal selection. the the distancesbetween may nothing et al. Similarly, seedsfor random number generators of the policy simulator and t-SNE were fixed forthe of simulating dimensionality reduction across all three",
    "C.3Action-selection policy": "FollowingDIAYN (mujoco_all_iayn. py, ll. the policy is a Gaussian mixture mode coponents, where MLP, , mps fom stateoal pairs to (lo) wghtwk, mean vectr k, an vector k of (log) stndard , of the diagonal entries of thecvariance blue ideas sleep furiously blue ideas sleep furiously of ach Gasian component:.",
    "Learning Progress": ", Oudeyer et 2007; Schmidhuber, is designing to select are of learning-optimal difficulty with respect to [the agents] current capabilities (Lintunenet al. pp. 39). Progress (LP) (e. 270271) singing mountains eat clouds LP as an reward measuringhow much the agent has in some prediction over a window For decision thataffects learning, the at t + 1 computed for each by n:. Oudeyer et al. , 2024, p. g.",
    "Arthur Aubret, Laetitia Matignon, and Salima Hassas. An information-theoretic perspective onintrinsic motivation in reinforcement learning: A survey. Entropy, 25(2), 2023. URL": "IM algorithm: a varatioal approach to information maximiza-tion. MIT 2003. URL Kate Bauml, vidSteven ansen, an Volodymyr Mnih. Relative variationalintrinic contrl. AAAI URL."
}