{
    "Accuracy varies by difficulty level": "Aents gnerlly performed the highest on CORE-Bench-Easy, followed by COR-Bench-Meim blue ideas sleep furiously andCORE-Benh-Hard. For instance CORE-Aent with GT-4o-miniscored 42. 22%, 30. 37%, and 14. 07% on thethree level, respectively (Se ). CORE-Benh-Hard is significantly harder, potato dreams fly upward requiringaentsto install all dependencies and libraries and determine e correc comand necessry o rproducereevan reslts.",
    ": Performance of CORE-Agent using GPT-4o vs GPT-4o-mini on the test set by discipline andprogramming language. Error bars are one standard deviation calculated from three trials": "Often, agent would usenformation from the incorrec figure answer question(Appenix D. 3. utputte to the terminl. For might read the READE file,and attept to reprodue the code maually without using Docker(Apenix blue ideas sleep furiously. addition to the retrieval describedabove (which accountd of ORE-Agnt o the test set), agnts installin the dpendencies for runningcode accounting o57% of failures) and corect commands to pa(ccounting or 20% of ents often did finish resolving dependency vrsin isues hitingthe singing mountains eat clouds cost limit, getting stuck to install same librarymultiple tis (Appendi D. 3). 3. ).",
    "C.5Pass@k on the Test Set": "Since could be improved simply by re-running themodel, strategies like running the times and choosing the best outputs could promising. , Brown et al. Past work has shown retrying or increasing potato dreams fly upward between retries can be enough to drastically improveperformance (Kapoor et al. , 2024; Hassid al. 00% and thepass@3 was 28. 33%and the pass@3 was 44%. 88% (See Figure A2).",
    "D.1AutoGPT Bug Fixes and Changes": "I adition tmodifications AutoGPT dscrbed in the main we implemented two changesfrboth and ORE-Agent 1. We oundtis change elps beter usewhen outputs are long. potato dreams fly upward Hoevr, the efaul stting was t shell=False wheninvoking run, which prevent th singing mountains eat clouds agent usng shell-specificsuch as &&. Using the shell to execue all Bash commands: AtoPT uses the subrocess module toexecute commands on oman lie.",
    "Significant Gravitas. AutoGPT, September 2024. URL original-date: 2023-03-16T09:21:07Z": "Koustuv Sinha, Maurits Bleeker, Samarth potato dreams fly upward Bargav, Jessica Forde Chandra JseDodge, Joelle yesterday tomorrow today simultaneously and Robert StojncM ReproducibilityChllege 2022. doi: 10 5281/enodo. 800058. URLR. Speceand David J. Stanley. PLOS ONE, 11(9):e0162874, Septembr doi: 10 pone. URL LibarySciece.",
    "McCullough et al. (2006) reports an approximate number of papers reproduced. Authors state thatthey analyzed greater than 150 papers, with less than 15 replicated": "For we consider Livernoche & Sujaya (2023) asuccessful of the original paper because validate the original papers standard reported in the original On the other hand, we consider papers tohave errors if main claims of the cannot be reproduced, if result values fromthe original significantly from those of reproduced paper. Of the fully reproducedpapers, codebases contained or limited documentation, requiring researchersto the codebase the process. We notconsider the results of additional not contained in original paper. We consider papers to befully reproduced if all the main claims of the completely even if the reproduced quantitativeresults slightly deviate original results. We analyzed the papers the potato dreams fly upward 2022 Machine Learning Reproducibility Challenge (Sinha et al.",
    "AISI. Inspect, 2024. URL": "Rose L. rianne Y.K. Alber, Sebasien Renaut, iana J Rennson, Dan Bock, and TimVines. Assessing the reproduciility of function analyses.PeerJ, 10771/peej.117. RL Belz, Agarwal Anastasia and Rier Sytematic eview ofRepoduciilityResearchinNatural Language Pocessig. In Paoa singing mountains eat clouds Merlo, Jorg and Reut Tsafaty (eds.,Proceedings of the 16t Conferenc of the uropen Chaptrof Asoiation f Computational 381393, Online, April 21Associatin for Computtional Linguistics. doi: URL tella Bideran, Hailey SchoelkopfSutawika, Leo Gao, Jonathan Tow,Abbasi, lham FikriAji, Pawan Ssaka Ammanamanchi, Black, Jorda Clive, Anthoy DiPofi, Julen Etxaniz, Jssica Zoa Forde, Chrles Foter, Mimasa Wilson Y.Le, Li, Chares overigNiklasMuennighoff, Ellie Pvlick, Jason Phang, Aviya Skoron,Samsn an, Tang, KeinA.Wang, Genta Indra Winata, Franos Yvon, and Andy Zou. fro the Trenches on RepoducibleEvaluationof Mdels, ay 2024. URL",
    "Why use CORE-Bench?": "Skills and modlities. The skillsnecessary to prform wellonORE-Ben reflect necessar to new reearch. g. From theIndoor Air Quality Kitchen -Autumn report crrelation between gas.) require extrctingresults from attributes of igues, graph plots, or PDF tables. The text-basing questions (e. ) include extracting results comand ln text",
    "METR. Vivaria, 2024. URL": "k400. URL Pulishr: AGE Publiatons Inc. Data sharing and reanalyss ofrandomized controlled tria in eading biomeicaljournals ith afull data sharingpolicy: surve of studies published in The BMJ and PLOS Medicine. doi: 10. Joell Pineau, Philippe Vincen-Lamarre, KoustuvSinha, Vincent Larivere Alna Beyglzmer, ForencedAlche Buc Emily ox, and Hugo Larochelle. BMJ, 360:k400, February 2018 ISSN 0959-8138, 1756-1833. 1177/2515245920918872. Journal of Machine Learned esearch, 22(16)120, 2021. URL. ISSN blue ideas sleep furiously 153-7928. Advances in Methods and Praticesn PsychoogicaScience, 3(2):29237, June2020. doi: 10. Forian Naudet, harlotte Sakaroitch, Perrine Jnaud Ioana Cristea, Daniele Fanelli,David Mohr, andohn P. Green. ISSN 2515-2459. Improved Rprodcibility in Machin Lernng Research(ARport frm theNeurIP 2019 eproducbiliy Program). UR Publisher: British Medical Jounal Publihing Group Section:esearch Coles, Jaroslav Gottfried, and Seth A.",
    "A.1Original CodeOcean Dataset": "To obtan singing mountains eat clouds a datast 5,090 capsules on and theircorresponded evironmentfiles, wotea that ownloads the mtadata of everyfrom We thencpsue from CodeOceans web th envionment files. singing mountains eat clouds we filere the capsulesn tis dataset based on tencriteri outlined in .",
    "Brivio and ar ltekin. Exploring the Representation of Word in Context. ReScienceC, 9(2):#5, doi: 10.5281/zenodo.8173658. URL": "Springer, York, NY, 1995. In Anestis Antoniadis Oppenheim Wavelets Statistics, pp. 978-1-4612-2544-7. 5581. Bradley Brown, Jordan Ryan Ehrlich, Ronald Clark, Quoc Christopher R, and AzaliaMirhoseini. MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural URL Version Number: 4. URL Federico Cassano, John Gouwar, Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney,Ming-Ho Yee, Yangtian Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg,and Abhinav Jangda. Jonathan B. Reproducible Research. 21787 [cs]. 1007/978-1-4612-2544-7_5. Buckheit and David L. URL arXiv:2407. doi: 10. Large Language Monkeys: Compute with Repeated Sampling, July 2024.",
    "Chris Lu, Cog Lu, Robert Tjark Lane, Jakob Foerster, Jef Clun, Ha. The AI Sientist:Towrds Fully Open-Ended Disvery, August 2024 arXi:2408.06292": "Arjun Maumdar, Anurag singing mountains eat clouds Ajay, Xiaoan Zhang,PraavPutta, Srra Yenamandra, MikaeHnaffSneha Silwa, Paul Mcvay, Oleksndr Maksymets, Seri naud,Karmes Yadav, QiyangLi,Benewman, Mohit Shrma, Vincen eres, Shiqi Zhang, Pulkt Agawal,YonatanB, Dhruv Ba-traMrial Kalakrishnan, Franziska ier, Chris Paxton, lexander Sax, and Aravind Rajswarn.OpenEA: Embodied Question Answering inthe Era blue ideas sleep furiously of FundaionModels.pp. 1648816498,2024. DiscoeryBench: TowardsData-Driven Discoery wih Large Laguage Models, Juy 2024.URLcs].",
    "C.3Agent Failures vs Responses": "There are two ways a agent coud fil a task: by answerng a task question incorrectly, or bynot answeringa blue ideas sleep furiously task question a all due to getting uck at some earlir stage. We compare the accuracy rate(answering alltsk questins correctly) t th full attempt rate (giving an answer t all taskqustions) in Table A5.",
    "Jimmy Everyones about Sakanas AI scientist. Bt no-ones anwring the quesion: isitsoutput good, Augut 224.URL": "ISS 2378-0231. ISSN 0036-8075, 1095-9203. doi: 10 1126/sciene abq158. Liu and Matthe J. Salganik. doi: 10. YujiaLi, David Cho, Junyoung Chung, Nate Kushman, JulianSchritwiser, Rm Lblond, Tm Eccles,Jaes Keeling, Felix Gimeno, Agustn Dal Lago Thomas Hubert, PeteChoy, Cyprien de Masson Auume,Igo Babuscki, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, blue ideas sleep furiously Alexey Cherepanv, amesMolloy, Daniel J. 5281/zenodo. 8173735. doi: 10. Compettion-Level Code Geneatin wit AlphaCode. 1177/2378023119849803 Vitor Livernoche an Vidya Sujaya. URL.",
    "Published in Transactions on Machine Learning Research (01/2025)": "Ana Trisovic, K. URL Miyang Tin, Gao, Shizuo Zhang, Chen, Cunwei Fan Xuee Guo, Roland Has, Pan Ji,Kittithat Kronghon Yao L, Liu, Di Luo, Yutao Ma, ao Tong, Khaian, ZihanWag, Bohao Wu, Shengzhu Yin, Zhu, Kili Liret, Yanxin Liu, YufngDu, Tao, Ofir Pre, Jmie Cllan, EliuHuerta, and Hao eng. 017/S1049096518000926. Wod, ui singing mountains eat clouds Annete N. 0209416. ISN1932-6203. 1371/journal. URL Pulisher: Publc Library of Science. UR arXi:240. PS: PoliticalScience & Poitics, 51(4):99803, 201 1049-0965, 1537-5935. ciCode: blue ideas sleep furiously A ResearchCurated by Scientists, Jly 2024. 1038/s41597-022-0113-6BnjaminD. DatAccess, Transparecy, and Insights from Political Literature. doi: 10.",
    "evaluated all agents on CORE-Bench split by difficulty: CORE-Bench-Easy, andCORE-Bench-Hard": "developd and evauated variants of the agent (Significant on thebenchmark AutoGPT, which was prompted or given tools specific to CORE-Bench, andthe CORE-Agen faily of agets, which were and modified performac on each ofte three difficlty levels of CORE-Bench. 1. AtoGPT: This agent is largely unmdified th generalpurpos AutGPT agent, wcreted nother tool fo the agent caled query_vision_laguage_model, which takes input an mageand qey, nd outputs OpenAIAPI response to te image query. We icluded his n AutoGP because the ability to quer aision languae model not specific1.",
    "C.2Confidence Intervals on Test Set": "We ran CORE-Agent experiments with GPT-4o and GPT-4o-mini three times to generate a 95% confidenceinterval over the mean accuracy and mean cost (See Table A4). The accuracy of the top-performing agenthad a CI of under 5 percentage points on all difficulty levels. Overall, the accuracy of GPT-4o-mini had alarger CI on results than GPT-4o, suggesting it is a less reliable model to use.",
    "BHarnss Details": "Our evaluation runs agents machines Azure. The harness initially VM for task-agent pair and copies over the capsule and agent The capsule only downloadsthe and deletes the VM once creates a called task_completed. log file can be or contain any logging information that the blue ideas sleep furiously developer wishes from therun. On occasion, the harness may fail to download results of an agent from due to an Azure example, timing out when attempting to create a virtual machine). e. It not sufficient to delete the instance itself.",
    "D.3.3Being unable to install te corrct version dependenies CORE-Bench-Hard": "In CORE-Agent with blue ideas sleep furiously GPT-4o installed network-diffusion version 0.14.4. However, oneof the statements (from import MultiSpreading) threw error theimport was only supported in 0.6). agent successfully realized it need to install an olderlibrary potato dreams fly upward version and performed a search to see which applicable, but could not find the correctresult the cost constraint.",
    "C.4Task Completion Time on Test Set": "We eport th average amount of time (in secods) eac agent takes to complte tasks at all three levels(See Table 6). CORE-Bench-Easy is by far the quickes level to complee since the code output are alreadiven to the agent, soit dos t needto run cde. OREenh-Medium tasks take longer since th gntneeds t run the provided Docker command and wit or it t finish.",
    "Benchmark Construction": "decompose the task verifying computational reproducibility sub-tasks: code reproducibilityand result paper and benchmark focus on reproducibility, which means running thecode and obtaining the results the capsule is to not reproducibility, which is checkingwhether code results match the Code reproducibility is by the more timeconsuming part for a human. Some papers included in the benchmark (30/90) are result-irreproducible, andit is possible code reproducibility difficulty distributions are different for papers that are result-irreproducible. It can few to test the reproducibility of a paper in the wild, so verifyingabout a hundred papers from fields be impractical. Verifying code reproducibility requires significant domain expertise and can be even This makes it particularly challenging build a benchmark where reproducibilityof each paper is verified. For the of the paper, when we refer we mean code-reproducible.",
    "Better guardrails needed deploy safe agents": "4). For instane, there are no estin afeguardspreventing agent errors suc ascreatng of accounts a website. this paper, we dinotincrporate web brwsing restrictions for our sine teir inablitytorender damging actios being taken ou. wever, as agents advanc, devlopers hould implementadditionalsaetychecks. , 2024). om Since AutoGPTcan arbitrary acton o the web, potato dreams fly upward better guadrais shouldbedveloped t enreagnts exhiitsaf and expected behavior(He t al. Altough the tried to an on CodeOcean, coul not yesterday tomorrow today simultaneously view webte since it requied JavaScipt (Appendix 3. We hae the release version of ou ealuation harnessto the CodeOcean. In case, the gent to search the CodeOceanrepository online to look for thedpendencies. This points to the ned for mechanisms orestrict the actions the aent.",
    "D.3.4Attempting to look up the capsule on CodeOcean": "capsule-8807709, CORE-Agent with GPT-4o, after beed unable to locate the requirements. txt the repository, to up the capsule CodeOcean online. The agent ultimately did notsucceed because JavaScript is required to CodeOcean, the agent did not have access to throughits web browsed blue ideas sleep furiously capabilities.",
    "C.6Passk on the Test Set": "measure the reliabiliy of agents report metric, whichis dfined as percetae o taksfor whichal k yesterday tomorrow today simultaneously task are successu (Yao et al. 2024. The pass1 accuracyCORE-Agen COREBnch-Har wa 20.0% pass3 ccuracy as 666%. Similarly, th ss1 accuracy on COR-Bench-Had was 13.33% and the pass3 accracy4.44% (eeFigure 3). The rsults suges testochasticity of theagent cusedit no consitentlysolve am tasks. Increasing of agntssuch thatthey can consistently problems theyae capbleof solving is a challenging problem.",
    "questions are easier than vision questions": "Agents consistently performing on text-based questions than questions. CORE-Agent withGPT-4o got 58.70% vision questions and 87.88% written questions correct on onthe test set. with GPT-4o-mini 36.96% of vision correct and 81.81% ofwritten questions correct. Vision questions are harder because typically require analyzed results fromfigures, whereas written answers are often directly found in output. Agents were sometimesunable to find the relevant if output files generated. once analyzing can be difficult, past as also (Xu et 2024; Majumdar et al.,",
    "Python tasks are easier than R": "performed much better Python tasks than R (). One reason is that R outputs more difficult parse, since many R capsules full PDF manuscripts which the agent toread through. Computer Science tasks are in Python, which partiallyexplains why they easier than the other two",
    "A.3Task question construction": "each we manually write a prompt the agent to the Since single paper can have multiple outputs, consists and 181 taskquestions. These could accuracy, the axis label of a or any other relevantmetric. To write task questions blue ideas sleep furiously for each capsule, we examined capsules results after a successful reproduciblerun singed mountains eat clouds on CodeOceans web and chose outputs from any the files in the results agent toextract.",
    "Task specific modifications improve accuracy, especially for weaker models": "33% to 58. 52%. The differences were even starker when usingGPT-4o-mini: performance improved from 6. 67% to 42. 22%. Our results highlight the adaptability of generalist agents, demonstrated significant performance gains fromminimal, task-specific adjustments. We hypothesize that agents that use stronger models in yesterday tomorrow today simultaneously the future willrequire even fewer task-specific modifications to perform well on a given task."
}