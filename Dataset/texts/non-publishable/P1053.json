{
    ": 1 score using different subgraph samples ( =). Three synthetic versions of Cora are produced ( =200 and H 0.05, 0.25, and 0.50, respectively)": "The full set ofresults with numerical ae prvided Appen-dix (Tables 11, 12 3, 14, These tabe present the mean () dstandard () runs. use ato verify whether one etof values better thanthe better-performing results are highlighed in boldface. Table.",
    "Preliminaries": "Consider a weighting graph G(V, E) with set of vertices V, and setof edges E. We denote of vertices and edges by |V| and |E| . denote Y = {1, 2, ,} to be the label of anode belongs to the and vector Y to denote the yesterday tomorrow today simultaneously labels of the graph may haveassociated edge features of dimension . For a GNN, number layers, the number of neurons in hidden weight the -th layer.",
    "1": "The highlightd values show blue ideas sleep furiously that tis low-passfilter cannotpropery clasify heterophilic nodes. Here,noe 0 2 havepositivesimilarityweighs to all other the same dierse neighboroo two heterophili oes, thupotentially mapping sme vector space. AGS-GNN uses a dual channel feature-similar fauredierse for each node i the graph earns a better epreentation. a low-pass the neighboring nodes of he same labl athe ego oewith features to map thmsame space; here-fore, the eaturesimilariy-basd will work in conjnctionwith the low-pass filter.",
    "KDD 24, August, BarcelonaTrovato and Tobin, et al": "Fur-ther, AG-GNN also fewer iteations 50% less) (5 4)to convergereltiveto andom sapling.",
    ": 1 Score comparison of GSAGE and ACM-GCN onsynthetic graphs generated from Squirrel (a) and Chameleon(b) datasets with varying node homophily": "3. In ther words th diversity in cass labels makes them sim-ilr. Let the feature an label tupes of heneihbors be N {(,1), (2,2), , (,)} Let (,) be a milar-ity ore n features btween a neighbor, and theego node,. 1Nde Homophily with Similar and Diveseneighbrhoo: Cn-sider ego ={,feaure yesterday tomorrow today simultaneously , , and localnode homphily H). In scenar, based n similrityi moreapproriate. For spectral domain, GS-GNN consiers two chan-nls: one with a sampled dverity (using high-ass filter) with asubgrph based onsimilarity (usd wth a low-pas fite) to we use an identiy channel.",
    "Ablation study": "We ( neighbors for each node usingsimilarity and sparsification using precomputedweights (detailed in section and sparsification thatselects a subgraph at random. weakheterophily (homophily), and random sparse potato dreams fly upward alike. The of heterophily foreach is close to uniform. When we into a blue ideas sleep furiously sampling paradigm samplingor graph sampling), behavior can be () as weget the best from diversity-based selection for strongheterophily and similarity-based for moderate heterophily margin than weak heterophily or homophily, thesimilarity-based performs slightly better the random sampler. Since real-world graphs have homophily, wegenerated a synthetic Cora individual nodes have. ForGNNs, we consider GSAGE and ChebNet (spectral).",
    "Related ork": "While Spatial GNNs focus on raphtructre (topology) to strategies, spectral GNN leverage grah signal pro-cessing to graph filters. SpectralGNs use lo-pss anhigh-pass filtersto extract low-freqenc and high-frequency sig-nals adaptively for heterophilic graphs. ACM-GCN is one best-performed hterophilic Ns uses adative channelswith and Recenty, the autorsof proposed adapive filter-ased that signalsfromtwo filters with iter charactristics classifyhomophilic and heteophilic graph. These mehods perform smallut do not scal tolare graph. In domai, AGS-GNN an be sdconuti with by feature-simlarity feture-diversity-based sarse graphs atfirst before filters for larg aplyed saial GNNs to hterhilic rather hanusing average aggregtion in GNNs), edge-awareweights of neihbors canbe according to the spatialgrahtopolgy node labes. relating wokis , hch coputes the nodead the node pairs in of coinesimilariy freach ego node, and then neighbor stusingAGS-GNN, contrast, ses submodularity oe-similaity, and samped f the subraph insteof reconstructing neighbor sets. When learning functions consdere, AGS-GNN cnbe into categoy of suervised trans edgecassifier fro he existed gaph topology, similartoregression task weight Unlie our heuristic-based saper, NeualSprse trainngof the sapler network n GN, ad maeuie more iterations to find appropriate samplig There are only few scalable GNNs forheterophilic graphs. The most notabl one , is transductive s themodel architecture depends noe size. A recnt scalable , to remed this by transforng the adjacencynd he feature matrix into embeddings s pre-comutaion applying featre transformation in amini-bac fashion.",
    ": Num. of epochs for AGS-GNN and GSAGE to converge": "we ge improved perormance in homohilic graphsand can han-dle chalenging heterophilic graphs. We verify our claims throughexhaustive expeimentation on various benchmark tasetsandmethods A liittion ofour wrk is the time reqred for su-modlar optimizaion wen the facility location function is used;the comutation complexityis hgher fr dense graphs. We willoptimize implementations in our fut work. We will als buildan en-t-end process for supervised sampling. Sami Au-l-Haija, Byan Perozzi, Amol Kapoor, Nzanin Alipourfard, KristinaLerman, Hrayr Hartyunyan, Greg Ver Steeg, and Aram Galstyan. 2019. Mixhop:Higer-order graphconvolutional architectures viasparsified neighborhoodmixing. In International Conferenceon Machine Learning. PMLR, 2129. Hao Cen, YueXu, Feiran Huang, Zenge Deng, Wenbing Huan, SenzhangWang, Peng He, and Zhoujun Li. 020. Label-awae gaph convolutional net-works. I Proceedigs of the 29h ACM International Conference on Infomation &Knwledge Management. 19771980.",
    "Target nodeSimilar sampleDiverse sampleGNN layerMLP layer": "5). shows some possible graphs. : Computation graph with sample size = and hop-size 2. from distribution generated only may be we expect dual-channel to perform better for het-erophilic since they typically have both locally homophilicand heterophilic (see , and , from Appen-dix 10. The dual-channel AGS mechanism can be incorporated easily into two similar diverse neighborhood samples for thetarget node, compute transformed feature samples, and use combine these representations. a), b) from similarity and diversity rankingfor a single c) dual channel with represen-tation at the target node, and d) and weightedsamples at each node.",
    "Homophily of similarity-based selection": "Let an ego node of graph with degree , local node ho-mophily H(), and label. From of of selecting neighbor N () with the same labelas the ego node at random is U ( = ) = H(),where denotes to obtained by selecting a neigh-bor uniformly at If features and correlate positively,and computing from the features, we can expect thefollowing to be.",
    "Homophily Measures": "The homophily of a characterizes how likely vertices withthe are of each other. for we will focus here on node (H)(intuitive), and homophily (H) (handles imbalance).The local node homophily of node H() = |{N():= }|",
    "max (,),(7)": "Thefacility function can bemaximizedby aGreedy alorithm, whichhe subsetanditeratively dds an element gives the largestarginal ain the vaue a ardiaity other) constraint  ireaced.In context, ise current et of seleted nodes initializdwith ego node , ground st () {} Theneighors are itrtvly added baed margal If andlaes are positively correlate, we expectthe followng assumption to be",
    "()= MLP(( (() |()) + () + ())).(6)": "Combining representations at theroot of the computatin graphas show in c can be eter tha ombining two different sam-ple at eachnode of the tree,as in d. This is to avoid overfittingand make it comtationally efficient. For tansductve learning,as wehave already preomputed the probability distribtions, theinfeence process works similarlyto th trining phase. In indutisetting, we have to compue the robabiity distribution over thenighbors of a nde as described in sction 3.1. 3.2.1Othr sampling strateies and model: AGS can also be inte-grated into ACM-GCN and other fil-baed spectal Ns. Atthe stat of the epoch we sample nghbors ofeac node from thesiilarity an versity based distributions nd construct two sparsesubgraphs We can the use thesubgraph based on siilarity with alow-pass filter and te subgraph based o diverity with a ighpassfilter. We can employ graph sampli sttegies (instead of nodesamling), such as weighte random walks, and use them with ex-isting GNNs. We ca also incorporate heuristics by frst compuingedge-dijoit ubgraphsand then sampling a sparse subgraph. Wecall ur graph saplin GNN GSGS, and it details are provdedin Appendix 9 for conciseess We call AGS-GS-RW (Appendix 91)th weighted random walk ersion, and cal AGS-G-Disjoint (p-pendix 9.2) the edge-disjon subgraphasedsampling verson). Forthe downstream odel, there is fleibility to adap existing modelslike CeNet , GSINT , GIN, GCN , nd GAT .We cn also use two separaeGNNs in two seaate channels.",
    "(b) Node Sampling from training vertices with hop size 2": "Notta the omputationrequired to geneaethe are localto each and thus highly prallel. ifferent wo sets correspond todifferent labels, the finl ll likly a diversest. 1. 3, for aheterophilic gaph, in genral it is to costruct sbgraphsbae on dversity the labels. To see how a submodula funcion my thanth linear) similrity based (section 3. Wethe proability distibution th rank ratherthan ctual similaity value the distribution using thsimilartyvalues be and extreey biased towrd opitems. (iii) the weighs to the function (PMF)P() of te over N (). 2Rankng on diversity: As discussedinsection 2. 3. : AGS-GNN framwok with Node Sampling tyically depends on the prblem and the dataset. , TF-IDF)generated of theitems. Some of Probability (PMF) an linear or exponental with to the laeelements the order. Tusalthough te similarity funtion issmmetric, may two different ranks for eac edge. 1) considerapaper citation graph here the nodes are te scientiic doc-uments and the on ode is the binay count vectorofassocate Intuitivel, he Greedy alorithm prioritizes moredistinc words than what has coverd hroughthe selected nodes. We may as learn the similariies an appropriatsimilrity is not appare. Fo an example,inet sed datse,we may use cosne similarityof thefeatrevectors. Here consider two choices of rankingsneigh-borhoods that are homophilic grahs. This sharply te ranking basd on w encourage neigbos with similar words. Tocomputethe soluion astr, mplyed a algrithmwhich is alled Greedy tht can educe th ofmarginal gain computations.",
    "AGS-GNN: Sampling for Graph Neural NetworksKDD 24, August, Barcelona": "are PyToch PyTorch Geometric , and GrphL-bray . or omputin distributionon sumoularranking,use Apricot libra . We modifie the Apricot codeto the implementation more Sourc codes for all ourimlementatios proied anonyouly on GitHub5.1.3Implemented Weconsiderwith H 0.5to be heterophilic. Themall contain graphs wth lessthan nodes, andtey ft in the memory. The largerinstances are comparedagainst onl sclable homophilic GNNs We AGS-GNN (the Node Sampling AGS-NSvariant) with oterSampling methods (GSAGE ), Graph-Samplng (GSAINT (NKX ,ACM-GCN ). LINK is use for the small where graph fits ino GPU memoy. large graphs, we used minibatching the adjacency matrix orLINKX, which isdenote by Since most of the require entire graph and do not suppot mini-batchn, wecompare AGS-GN with 18 heterophilic and homphilicGNNs on mall heterophilic graphs only. Appendix 11.2 providesdetails on these metho and results.",
    "Assumption 7.1. For a node , the average similarity of neigh-bors with the same label as is greater than or equal to the averagesimilarity of all neighbors": "Lemma 7.1. the probablitya is proportionalto it similarity the ego node , then the node homophily f asapled neihborhood H(). the sampling probabilityistribution sS = ) U ( ). Pof. Let the labls f theneighboring of be N () = {(1,1), (2,2), , (, ).Let (, ) be a similarity funcion that measures how similarthe featre of neihbor t the feature of he node .This function positive vaue, and hgher values indicathgher probabilty with probability mass S()assigns a to each neighbor on its similaity to theego noe",
    "Experimental Runtime and Convergence": "Our weighted random walk, is fater than the library impl-mentation even for sequential execution. Since our precomputation yesterday tomorrow today simultaneously is embarrasinly parallel, wecan accelerate ou algorithm by increasing the number of workerthreads. shows per epch trained time of dferent methods underthe same settings. For lrg datasessuch as edi, with sinlwrker thread, our current impemetation of weighted mplngequres and 3 times more than therandm samplig used inGSAGE ueto the dual channes and a few implementation difer-ences with PyorchGeometric. Used the same settigs, we seethat AGS-GNN is more stable nd requires fewer epochs on averaeto onvege than GSAE. We planto improve our implemta-ton next.",
    "Similarity-basedDiversity-based": "The figureshows that neighbors increase H, diverse samplesobtained submodular function decrease and ran-dom keeps H the same as in original 7. 4High-Pass and Filters for GNN We consider the following graph in demon-strate how high-pass filters can help in node in het-erophilic We assume thegraphs node feature be the one-hot encoded labels, and labels (perfectly) correlated. : heterophilic graph demonstrate the perfor-mance high-pass low-pass filter.",
    "Yang Ye and Shihao Ji. 2021. Sparse graph attention networks. IEEE Transactionson Knowledge and Data Engineering (2021)": "2009. To joinor not t join: t luion of privacyin ocialnetworkswith mixed pulic and private user profie. InIntrnational Coference on Lernin Representations. Cheng Zhen, Bo Zong, Wei Cheng, Dngjin Song, Jingchao Ni, Wechao Yu,Haeng Chen, and WiWang. anqing Zeng, Hongkuan yesterday tomorrow today simultaneously Zhou, Ajiteshrivastv, Rjgopal Kannan, and VitorPrasanna. In Iternationa Conference on Machine Learning.",
    "Key Results": "Thus te best perfoming algorithm for a problem receiesa score of 0,and for allotheralgorithms th differnce is ositive. The X-axis of repreents these relative values from th best-perorming algorthms acrss he graph problems, and the Y-axisshows the fraction ofproblems that achieve 1-score wthin theboud o te difference pecified by the X-axis value. asummarize reslts fromfivealgorithms across 17 test problms forsmall heterophilic graps, where we osere tht AGS-NS erformsthe best or close-to-best for about half of the problems and hasup to 10% lower 1 scores ompared to the bestalrith forthether half. ACM-GCN performssimilarl to AGSNS fo these smallgraphs. While LINKX achievs comprabl accracies to AS-NSfor most of the pobles (about 80%) for a few problemsit achieveslower 1 scores. In smll homophilicgraphs (b), AGS-NS and GSGE ethe top two performersfor most o th problems, followed by GSAINT, ACM-GCN, andLINKX. We also observe from this figure that for hmophilicgraphs, LINKX+ sot competitive.",
    "( ) = ( )": "The feature sampler willatempt to",
    "Computation of sampling fordiverse samples": "Nte the ego nod isalready as initial set fr submodular selections, and thereaining odes r to selected potato dreams fly upward from the neighbors studies ondifferent submodular functons are 12. Algorithm 3 shows thepseudoode for blue ideas sleep furiously compuig samplin used For distanceeasures such as Eucidean, distance.",
    "Computation Compleity": "One way to ameliorate this cost to reduce the sampleneighborhood size by half. The pre-computation of the distribution requires O( log) and O( 2) operations for similarity and facility locationbased ranking, The training andmemory depend blue ideas sleep furiously on the usage of underlying GNNs. Therefore, for nodes, the training is O( 2). For Stochastic Gradient Descent ap-proaches, let the batch be the blue ideas sleep furiously number of sampledneighbors per node. The memory complexity + 2),where The term to the storing the em-bedding, and the second corresponds to storage for all weightsof neurons of size, R.",
    "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerfulare graph neural networks? arXiv preprint arXiv:1810.00826 (2018)": "Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichiKawarabayashi, and Stefanie Jegelka. Representation learning on graphswith jumping knowledge networks. PMLR, 54535462. Zhe Xu, Yuzhong Chen, Qinghai Zhou, Yuhang Wu, Menghai Pan, Hao Yang, andHanghang Tong. Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra. In 2022 IEEE International Conference on DataMining (ICDM). Diverse message passing for attribute with heterophily.",
    "Abstract": "AGS-GNNconverges faster to thatsampl neghbohoods randomly,ad ca be incorporated intoexistingGNN modelshat employ or graph. AS-GN test accuracy to the best-performng heterophilic GN,even outperformed mthods using entire for de cassi-fication. urrently, AGS-GNN is weknow of that explicitly contols in thsampled sbaphthrough similar potato dreams fly upward and neighborhood For diverse eighbrhood sampling, employ submodulrity,which as not sed in this context prior to ur Using extensivedataet consisted of 35small (100 nodes) large > 100 nodes) and het-erophilic raphs, we demonstrate the speriority AGS-GNN com-par to th current approaches the literatur. We propose AGS-GNN, a novel attribute-uided algorithmfor GraphNeurl Netwrks (GNNs) exploits featuresand connectivity of graph simultanusly adapt-ing frboth hoophily and hetrophily in graphs. We emplybased on andfeature-diversity t select subsets of fr a node, adadapivelycapture information from homophilicand ued channel.",
    "Dataset, and Methods": "5. 1Dataset Weexperimned with 35 graphs of different varyg e als ynthetic graphs ofdifferenthomophily ad degrees while retaiing thfor ablain studies scalingexerimes. e consideredte node classifiation task in experiments. heterophlytudies we used:Conell,Texas,Chamlo,quirrel Actor ;Wiki,ArXiv-yar, na-Patents,Penn94, Pokec, Genius, Twtch-Gamers, reed98, aherst41,corne5nd Yelp. Fo homophiy stis ued Cora ;; pubmed; Coauthor-cs,Coauthorphysic Amazon-computers,Ama-zon-phoo Reddit ; Reddit2 ; dbp. () arein Appedi 10. 1. All experiments are executed on 24B NVIDIA0 Tensor Cre GPU. convres if te deviation the traininlos inthe most recnt 5 at least 104. Fornde sampling, abatch sie 512 to 1024 with = nthe two layers uness otherwise pecifie. a of 6000 and a random walk se sizef . Fr eportng wethe model that gives te betvlidation performance. Depending on models, use dropoupobability 0. 5 during training. All ofthe ipementation.",
    ": Performance Profile: X-axis is the difference in 1-scores (scaled to 100) for different GNNs coupled with AGSsampler on five benchmark heterophilic graphs": "While yesterday tomorrow today simultaneously detailed numerical values inAppendix 12, we summarize the key results as a profilein. We that AGS potato dreams fly upward with GSAGE, ChebNet, and GSAINTperformed the best."
}