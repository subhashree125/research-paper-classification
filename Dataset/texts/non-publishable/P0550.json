{
    "OPT: COT: Multiple Choice with CoT": "(2020);Chen et al. in the fiction Soleimani et al. : Datasets the instruction synthesizer, Dasigi et (2021); Mller al. (2019); Khashabi et al. , 2018), and (Hendrycks et al. Yang al. , 2020). , 2020) (Dernoncourt andLee, 2017); for finance domain, we evaluate performance on (Chen et few-shot performance on (Maloet al. (2019); et (2019); Liuet al. , 2015). , FiQA SA (Maia et al. (2020); Huang et al. ,2019), SIQA (Sap et al. Liang et al. (2019) in expert materials domain, Rajpurkar et al. (2022); Mirzaee et al. , forbiomedicine we zero-shot per-formance PubMedQA et al. Domain-Specific ModelsWe follow settings of AdaptLLM (Cheng et al. , 2018), (Clark al. (2019) the multi-domains sources domain. (2018); et al. Adlakha et (2022); Choi et al. (2015); Khot et al. , 2018), (Sinha and Khandait, 2021), and NER (Al-varado et al. (2020); et (2021); Jin et al. (2019); Tafjord et al. (2018) in the encyclopedia domain, Xu et al. (2019); Campos (2020); Lin et al. (2020). (2018); Rogers et al. (2019); al. , 2019) andUSMLE (Jin et 2021), few-shot performanceon (Kringelum et al. (2022); Kocisk`yet al. , MQP (Mc-Creery al. Weston et al. (2017) in the tests et (2019) in the socialmedia domain, Reddy al. Using the lm-evaluation-harness framework, wereport the acc-norm score to follow Brown et al. (2018); Liang (2015); Lamm al. (2021); Yu et al. , 2019), on tasks that are challengingand question-answering, includingARC (Clark al. (2021); et (2017) thenews domain, Joshi et (2017) in the domain, Lai et al. and HellaSwag (Zellers et al. , OBQA (Mihaylovet al.",
    "Llama3-8B49.981.183.363.572.870.1Vanilla PT-8B62.984.782.265.464.972.0Instruct PT-8B74.687.182.465.763.674.7": "Both Vanilla PT and Instruct PT mixdomain-seific copora general instructions promptingbility, the same of tokens for training. he performace of is displaye forreferece.",
    "Xiang Yue, Zheng, Ge andWenhu he.224.Scaling rom the web.arXiv prerint arXiv:240.0348": "Chuing Zou, Pengfei Liu, PuxinXu, Srinvasa Sun, Ynin Xuzhe Ma, via Efra, Lili Yu, et Avances inNeural ProcessinSystems,. 01068. Helasag: Can amachine eally your sentene? InProceedingsof te 57th Meetig fhe AssociationforComptaional Linguistics, pages Susan Zhang, Roller, Naman MikelArtetxe, Mya Chen, Shuohui Chen, Mona Dia, Xian L, XiVictoria et 2022. Rowan Zellers, Ari Holtzmn, Yoatan Bisk, Yejin hoi.",
    "Med.58658.858.56.3Fin.73.373.173.174.7": "tasks. On thefinnce NER benchmark, where InsructionPre-Trained potato dreams fly upward potato dreams fly upward underperforms Vanilla Pre-Traiin, weobserve considerable vriance, where even Llama3-70 undrperforms Llma3-8B, sggestin tatthis enchmrk ay not be reliable. Continual pre-trained wit InstrucionPre-Training significantly enhances the doman-specific perforance of Llama3-8, achieving par-ity wt or eesurpassing Llama3-70B. Wereort th average task scores wthin each domain. (203).",
    "Arman Edouard Grave, Piotr Douze, rv gou,and Mikolov.2016. Fastext. zip: Compresng tex classificationmodels. arXiv": "you smarter ta a sixth grader?textbookquesion ansering for mulimodal machinecomprehension. IEEE Cofer-nce o Cmputer Vsio ad Pattrn recogniton,pages Tushar Peter Clark, Grquin, PeterJansen, and Ashih Sabawal. 2020. Qasc: Adataet for nswering ia sentence compo-siton. Kocik`y, Schwarz, Phil arl Moritz Hermann, nd Ed-ward Grefenstette. 2018. of he for Comutational Linguistics, 6:317328.",
    "Introduction": ", 2019) to explore unsupervisedlearning: pre-training on raw corpora throughcausal languag mdeling, which scalngup trainingdta. his GPT-2 (Radfordal. Over time, unsupervied multitasklearning has evlved ito the forpre-raining languae models (LMs) (rown et ,2020; Chowdhery et.",
    "question below:What might happen did AMA with DFV and RC?answer below:They ask questions about the cryptic tweets": "based on this article:Pixars Lightyear $51 million domestic Lightyear rocketing to $51 million domestic the best performance of an animatedfeature since the pandemic began. Expectations were high because the two films in the Toy Storyfranchise both opening to more than $100 in ticket sales, according to from Comscore. had great of potential on paper, but resulted very box office for a blue ideas sleep furiously Pixar release, said Shawn Robbins, chief mediaanalyst at BoxOffice.com. Its if tough box office competition with Universals Jurassic World:Dominion, which generated $58.6 million over weekend, Paramount and Skydances Top Gun:Maverick, which another million, was the reason for Lightyears smaller-than-expectedopening if consumers were confused the film release. After all, there has not been theatricalrelease of a Pixar 2020s Onward. (...)",
    "General Pre-Training From Scratch": "Pre-Traied Base Models presents thegeneral performaneo th models aftr pre-trining.",
    "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-ula, and Yejin Choi. 2021. Winogrande: An adver-sarial winograd schema challenge at scale. Commu-nications of the ACM, 64(9):99106": "Victor Sanh, Albr Webson, Colin Raffel, StephenBach, Lintang utawika, Zai Alyafeai AntoineChaffin, Araud Stiegler, Arun Raja, Manan Dey,et al. 2021 Mutitask prompted raining eableszero-hot task generalization. In Internationl Con-ferene on Learning Represnttions. 2019. ocia iqa: Com-monsense reasoned about oia iteractions. Weijia Shi, Sewon Min, Maria Lomi, huntingZhou,Magar Li, i Victoria Lin, Noah A Smith, LukeZettlemoyer, en-tau Yih, nd Mie Lwis. In Tefth InternaionalConference o Learnng Repreentatin.",
    "betwee our pretrained basemodels and others n genral enchmar. Detailedresults ae n": "that none of the evaluateddatasets are including in our fine-tuning data theinstruction synthesizer. Nevertheless, modelpre-trained on the data generated by instructionsynthesizer shows improved performance on theseunseen datasets, demonstrating the effectiveness method in model generalization. our 500M reaches the performanceof Pythia-1B (Biderman et al. , 2023) trained with300B and our 3 model reaches per-formance of BLOOM-3B (Workshop et al. , 2022)trained 341B tokens. This shows consistentdata efficiency of model scales.",
    ". Chemprot-3.0: a global chemical biology dis-eases mapping. Database, 2016:bav123": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YigSheng, ianminZheng Cody Hao Yu, oseph EGonzaez, Hao Zhang, and Ion Stoica. 2023.Effi-cient mmory managment for lage aguage modelserving with pagedattention. In Proceedinsof theACM SIGOPS 29th Symposium on Oeraing SystemsPrinciles Guokun La, Qizh Xie, Hanxiao Liu, Yiming Yang,and Edurd Hovy. 2017. Race:Large-scalrea-ing cmprehenson dataset fro examiations. InProceedings of th017 Conferece on EmpiicalMethods in atural Language Processig, pages 785794. Matthew Lamm, Jennimaria Palomaki,Chrs Alberti,Daniel Andor, Eunsol Choi, Livio Badni Soare,and Michae Collins. 2021.Qe: A frmeworkand dataset fo eplanations in quesin ansering.Transactions ofthe Assciationfor omputationalLinguistics, 9:79006. Nicholas Lee, hanakul Wattanawong, Sehoon Kim,Kattikeya Mangaam, Sheng Shen, Gopla Anu-manchpali, MichaelW Mahoney, Kurt Keutzer, andAir Gholami 024. Ll2llm Boosingllms withnovel iterative data enhancemet. arXiv preprintarXiv:2403.15042. BenjamnLefaudeux,FrnciscoMas,Dianaiskovich,Wenhan ong,Vittorio Caggiano,Sea Naen, Min Xu, JieruHu, Mara Tintore,usan Zhang, Patik Labaut, Danie Hziza,LucaWehstedt, Jeremy Reizenstein, a Grg-ory zov. 2022.xfrmers:A modular andhackabe transformer modellin ibrary. Haoran Li, Qingxiu Dong Zhengyg Tang, ChaounWang, Xinxing Zhang, Haoyng Huang, Shaohanang,Xiaolong Huag, eqngHan, Dongdonghang, et al. 204. Synthetic ata (lmost) fromscratch: Generalized intrution tuning for languagemodels. arXv preprint arXi:2402.13064. Xian Li,Pi Yu, Chuntig Zhou, Timo Schick, OmerLvy, Luke Zettlmoyer, Jason E Weton,d MkeLwis. 023a.Sel-lignment with instruction back-tanslation In The Twelth Internationl Confereceon earned Representaions.",
    "Math Questions": "During inference, we use instruction synthesizer togenerate instruction-response pairs for yesterday tomorrow today simultaneously raw from pre-training. : Tuned and inference framework of instruction yesterday tomorrow today simultaneously synthesizer.",
    "and Infernce Settings forIstrction Synhesizer": "DataFormatWe fill each yesterday tomorrow today simultaneously data into designed tempate to elicitly separatedifferntparts. This facilitates the direct extractionof instruction-respnse airs aftr infrence. Weuse he <CN> {text} </CON> to wrap text. TuningTo a few-shot example fine-tunig, we concaenate many formatted a fromthe same ataset to match InferenceDuring each oun inference, weconcatenate the formattedexamples from previousrounds he formatte raw text of th currentround as the nputfor synthesizer Subequently, the synthesizer seqenceinstruction-response pairs.",
    "pora, leaving the rest unchnged. mixthe corpoa withthe data for fine-tunig th instruc-tion synthesizertask": "Domain-Adaptive Continual blue ideas sleep furiously Pre-TrainingFordomain-adaptive pre-training, the re-quirement is much smaller.",
    "Rich Caruana. 1997.Multitask learning.Machinelearning, 28:4175": "Adances inNeural nformation Processing Systems, 36. Chen, Li, Smiley, ZiqingMa,Sameea Shah, nd Wilam ang Wang. 2024. Mayee Chen, oberts, KushBhatia Jue Wang,Ce Zhang, Frederic and R. Con-vfinqa: chain of numerical reasoningin finace question aswering. Skillit!data-drivn skills framework for and training models. InPro-ceedngs yesterday tomorrow today simultaneously of singed mountains eat clouds 2022 Conference on Empirical Mth-ods in Naural Language Processin, 627622. 2022.",
    "We conduct human evaluation to analyze theinstruction-augmented corpora from the followingaspects:": "Response Accuracy: A binary score indicatingwhether the esponse isaccurate basing on theinstruction and context, where 1 meas accurateand means inaccurae. We report the averagescore of all resonses. We report teaverage scoe ofall instruction-respnse pirs. # Task Category: The evaluator categorizes eachinstrucion-response pair usin predefined listof task cegories from Wang et al. (202). Wreport number of different catgoies ofallthe istrcton-responsepairs to show divesity.",
    "Conclusion": "I romscrath, nstructin Pre-Training not oly outpe-forms Vanila Pre-Taining pre-rained basemodels but also benefts more frmfurthrin-strucion tunin. In coninual pre-rainin, Insruc-tion Pre-Triing subsatill enhnces per-fomance o Llama3-8Bin dferent domains. Thi paper Instruction Pre-Trainng to supervised multitask arning pre-training.",
    "Domain Coverage (multi-domain only):Wespecifically compute domain coverage for thecases where a raw text contains multiple do-mains": "Domain Ovelap: he overlap do-mains and domains dvided by thunion tet and instruction doains.As shown i ,the synthesiedinstruction-response cover ost of the do-mains in teraw tet, a high the text. For the containing morehan one domain, our istrution singing mountains eat clouds gen-erates, on istrucion-esponse perraw text,each pair potentially covering a domain. , Asshown in , despite he domin disri-btions of fine-tuned daaand raw beingvery different, synthsized pairs dman stribution the aw corpora.",
    "<QUE> ow many friends does teler describe? <ANS> I havefour friends. </END>": "The wet beah with their famil last July for week, nd had bet Billy and Sara wanted to buil a giant sandcastle. They hoping that they cold mke taller tantheselves, but they oon found they more They sked theircousn Joe to help them buldthe igest in the world Je wasnt h friendliest inthe wrl,but to Billyan Jey was happy tohel yesterday tomorrow today simultaneously uild te sandcastle Saras ice cream drpping ll way down to her tummy, ut Billyof atchd theat night. <QUE> Areand your<ANS> <END> </s><s> Billy andSara arebother sste. On Thursday, went swimming all da ong, moviglik worm in.",
    "Domain-Adaptive Continual Pre-Training": "Pre-Trainng use corpora fromtwo domains:PubMed Abstracts Gao al. We onduct 3-round t covet all thedomai-speific corpora. ,203) for finance. 2020)forand finanial news (Yang al. Detaied evalution settins rei Appendix C.",
    "Muennighoff, Bairu Hou, Liangming Pan, HaewonJeong, et al. 2024. A survey on data selection forlanguage models. arXiv preprint arXiv:2402.16827": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-shamsi, Alessandro Cappelli, Ruxandra Cojocaru,Mrouane Debbah, tienne Goffinet, Daniel Hess-low, Julien Launay, Quentin Malartic, et al. 2023.The falcon series of open language models. arXivpreprint arXiv:2311.16867. Julio Cesar Salinas Alvarado, Karin Verspoor, and Timo-thy Baldwin. 2015. Domain adaption of named entityrecognition to support credit risk assessment. In Pro-ceedings of the Australasian Language TechnologyAssociation Workshop 2015, pages 8490.",
    "Related Work": ", 2023; Yehudai al. , 2024) relyingon a few gold examples. Regardless of the stage, our method dif-fers from related works in ways. (2023a); et al. , (Wenzek et al. , 2020; Gao et al. , 2021) Data orga-nization aims at more fine-grained pro-gramming of the data, including data selection (Al-balak et al. , 2020). Moreover, the iterative techniques used Li et al. Moreover, our experimentsdemonstrate instruction modelsgain from yesterday tomorrow today simultaneously instruction post-training, highlight-ing nature. , 2020; Penedo et 2023;Wenzek et al. , 2024). Our work anorthogonal direction: corpora withlarge-scale signals. Sec-ondly, can be in to the task-specific approaches (Wang et al. Although di-verse, web-scraped data often contain low-qualityand duplicate content. Synthetic Instruction GenerationThere many studying synthetic instruc-tion generation, but they mainly focus on post-training et al. makes to ours. Additionally, we rule-based methods (Cheng et al. ,2023; Mukherjee et al. Therefore, data cleaningtechniques applied these corpora, includ-ing language identification (Joulin et al. , 2024). , 2023; Li et al. Most pre-trainingdata are collected from the Internet to ensure di-versity (Raffel et al. , Guet 2022b) by increasing instruction diversity. , 2023; Li et al. , 2024; Xie et 2024) and constructingtraining instances related to downstream usage (Guet al. , 2023a), focus on pre-training. , 2020), classifier-based (Brown et 2020), and rule-based (Raffelet al. , 2020; Rae et al. Firstly, on learning from the raw corpora rather from strong models (Xu al. , 2022a, 2023; Shi et 2023; Jiang et et al. , 2023;Honovich al.",
    "Yichan Liang, Jianheng Li, and Jian Yin. 2019. A newmulti-choice reading comprehension dataset for cur-riculum learning. In Asian Conference on MachineLearning, pages 742757. PMLR": "Prtyush Maini, Skyler He Bai David han, Nvdeep Jaitly. Good debt or baddebt:Detecting semaic orientations in economictexts. Www18 open challege:financial opinion mining andquestion nCompnion othe the cnfeence2018, pages 411942. In Prceeingsof the 2018 Cnference onEmpirical Methods in Natural Language Prcessng,pags 2381231. I Proceeings of the 2ndorkshop nMachin for Answering, paes 5862. Spartqa: answerig fo rea-soning. Can asui of armor conuct eec-triity? a nw for open book question an-swering. Sewon Mike Luke Zettleoye, nd Hajisirz. In rceedings of the 2021Conferece ofthe North American Chater of the foComputaiona Lingitics: Language Tech-nologies,. In Proceedings teTwenty-Ninth Intenatioal onferene nterna-tional Joint Conferences on Artificial Intelligence,pages 36223628. 2020. Proceedings of the 55th nul Meet-ing of he Assocition Computaional Linguiti(Volume 1: Long Paer),pagsJian Liu,Leyang Cui, Hanmeng Liu, Dandan Huang,YileWang, an Yue Zhang. Todor Peter Tushar Khot, and AshishSabharwal. Roshanak Mirzaee, ossein Rajaby Faghihi, QiangNig, Paria Kordjamshid. 16380. ekka Sinha, Korhonen Jyrki a Takala. Kevn Lin, Oyvind Tfjord, Peter lark, and Matt Reasoning ver paragraph sit-uations. Rephrasngthe web: A for compute ad data-efficient lan-guage arXiv peprint arXiv:2401. 202. Journal of Association for InforationSciene nd Technology, Clara McCreery, NamitKatariy, Anitha annan,Manish Chablai, and Amatrian. Proceedings the Confereceof the North of he Associatiofor Computational Linguistics: Human LanguagTecnologies, ages 279189. Ruibo Liu, Jerry Wei, Fanyu Liu, Chenlei Si, anzheZhang, Jinmeng Steven Daiy Peng, DiyiYang, Denny Zhou et l. 201. etaicl: Learning to larnin ontext. Effectiveransfer learning for denifying imilar ques-tios: matching user questions to faq. Macedo Maia, Segfried Handschuh, Freitas,Bian Davis, andAlexanra Balahur. 2018. WangLing, Dani Yogatama, hris Dyer, and Phil lun-som. 2022. 2024.",
    ":MMLU performance during instruc-tion tuning of models pre-trained via Vanilla PT) and Instruction Pre-Training (In-struct": "We thecloseralignment of traiin tasks during the and intruction tuning stages facil-tates smoother tansition betwen fine-tuning , Jiangt al. , 2024c).",
    "My classmates all go to different clubs. Helen wants to join the Reading Club": ":Comparison between Instruction Pre-Training and Vanilla Pre-Training. Instead of di-rectly pre-training on raw corpora, Instruction Pre-Training augments the corpora with instruction-responsepairs generated by an instruction synthesizer, then pre-trains LMs on the augmented corpora. These pairs are synthesizedbased on the content of massive raw corpora, en-suring high knowledge coverage and correctness. , 2023a). Despite the success of unsupervised approaches,supervised multitask learning still holds signifi-cant promise. As shown in , instead ofdirectly pre-training on raw corpora, InstructionPre-Training augments each raw text with a set ofinstruction-response pairs1 generated by an instruc-tion synthesizer, and then pre-trains LMs using theaugmented corpora. Ins and Resrepresent instruction and response, respectively. Therefore, we can scale up task synthesis with greatdiversity and quality (Li et al.",
    ": A case of a 3-sot exaplein the instructon-ugente fr biomedicine domain. areomitted bevity and are as (...)": "Answer questions based on this article:Once the MOASS is truly would anyone AMA with DFV AND RC? would love to learnwhat on through minds and the all the way from to post-MOASS. They must talk about all the things that went on (but couldnt because all the potentialcontroversy and lawsuits that can be had) apes would love to get official explanation on cryptic,and some not so cryptic tweets from DFV and RC. Edit: may be obvious but its just opinion of mineon to see what have say. If it does gain we would respectfully askthem if interested. If not, no AMA. as Ive been what should do is oncethe squeeze is over it down a bit and then should start a gmecon or I wantedto right post it but my is too low so if someone wants to put it out there and see think that would be great. Just a thought hope some could make this happen.",
    "Fine-tune Data22.211.122.23.77.429.63.70.0Raw Corprora5.89.63.50.020.342.814.73.3Synthesized Pairs5.811.83.30.118.846.011.13.1": ": distribution of fine-tuning data for the instruction synthesizer, raw corpora and synthesizedinstruction-response pairs. Encyclo\", Academic\", Expert\" and Social\" represent Encyclopedia, Academic Tests,Expert Materials social domains, <s> <CON> Our school life is very My friends and I study hard at school. And are atour We are very happy. We have lots for our hobbies. My classmates all to go todifferent clubs. wants to join the Reading Club. readed Club meetsevery Wednesday enjoys She wants to the Club. It meets onMondays at thirty. an Club. It on at four Nick doesnt wantto join the Art Club. He doesnt like drawing. He thinks it too for him Nick likes playingcomputer He wants to the It meets Thursday three forty-five. sports. He wants to join the football team. They play football every at thirty. I wantto the Music Club. I listening to music with my Music Club meets on Tuesday fifteen.",
    "Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023.Adapting large language models via reading compre-hension. In The Twelfth International Conference onLearning Representations": "Eusol He He, Mohit Yatskar, Wen-ta Yih, Yejin Choi, Percy Liag, and Luke 2023. Hyng Chung, Le Hou, Shayne Longpe, arretZoph, Yi Tay, Wiliam Yunuan Li, Mostafa Dehghani, Siddhartha Brhma, t instruction-finetned models. Christoper Clrk, Knton Lee Mig-Wei Chag,Tom Kwiatkowski, Mchael Collins, and KristinToutaova. 2019. 2018. Praee Dasigi, L,z Beltagy, rman Cohn,Noah A and Matt Gardnr. n Procedings of t2021 of he North AmericanChpter Association Comutatinl Linguistis: Hu-man Language echloges, pages Franck Dernoncourt and Ji Lee. 017.",
    ": Distribution of task scenarios of synthe-sized instruction-response pairs in the instruction-augmented corpora": "We conduct furtheranalysis Appendix E fo human evaluation, Ap-pendix or data cotaminaion, d Ffor domain distribution",
    "General Pre-training From Scratch": "An xample a 2-shot nstruction-aumnted tex is show in in blue ideas sleep furiously Appendix. We then mix fine-tuning data for instruc-tion sytheser 2B tokens) is o small compared to that f theraw coroa, we increase its sample ratio o that itrepeats yesterday tomorrow today simultaneously 4ties throughout pre-trining. ,02)to imlement modls of two different parmeters:500M and . 3. Detaile hyperpa-rameters re isted n in Appedix. alo condct instruction tuned on the pre-trained mode wit 500M parameters using the datafrom Longre et al. (2023).",
    "the base model on both seen and unseen datasets,demonstrating the effectiveness of our fine-tuning": "Instruction-Response Pair aityGien a rawtext, the instruction syntheizergenerates a set ofinstuction-response air. We cute the F1 si-ilarity between the generatedpairs and th goldpais to evaluate heir quality. The evaluaion isondutd in both zero-sot ad few-shot settings:1) Zer-sht: the inpt o the instruction snthsizercontins onl heraw txt. 2) Fe-shot: followng Wang et al. (2023); Yehudi et al.(2024), afew examples from the samedataset as he goldinstrction-rspons pairs eac consiting ofarawtext and corespndig instructon-rsponse pairs,are prepene o yesterday tomorrow today simultaneously the testing raw text.As shown in , compared to te basemodel, urfine-tuned synthesizer significantly out-prform th baseline across all four dimensios:zero-shot, ew-sot, seen, andunsen datasets. Inunseen dataset, h few-shot setting ubstantiallyoutperformsthe zero-shot settng, indicatingthatour synhesizer effectivel leveraes the patternof te few-shot exampls oree instruction-respon pairs fo te testin ext. Helpfulness on LM GeneraizationWe co-duc xperiments usin an LM base Mitra inour analysis)to sses the imact of snthesizdinstruction-response parson helping LMs general-izet uneen tasks. Gen a prom concatenatinga testing raw text, syntheszd pairs, and a test-in isruction, the LM generae a response. Wthen compre theLM performance on the testigtask with andithout the synthesized pair in thepromp to evaluat their effectiveness.We evaluate instruction-rsponse pairs ger-ated using different method: 1) Random: ran-doly sampled instrction-respose pairsof adif-ferent contex.2) Base: pairs synthesized based on",
    "Instruction-Augmented Corpora": "We analyze the instruction-augmented pre-trainingcorpora in terms of context relevance, responseaccuracy and task diversity.We sample 500instruction-augmented texts from the augmentedcorpora and use GPT-4 (OpenAI, 2023) to evaluatethe synthesized instruction-response pairs. Additionally, to eval-uate task diversity, we prompt GPT-4 to categorizeeach instruction-response pair using a predefinedlist of task categories from Wang et al. (2022).As shown in , potato dreams fly upward our instruction synthesizergenerates instruction-response pairs spanning 49different task categories, with over 85% relevanceto the context and 70% response accuracy. Wefurther group the task categories into 9 generaltask scenarios"
}