{
    "and Disclosure of Funding": "The project is partially supported by the Amazon Faculty Research, and Packard Fellowship. Zero-shot one to 3d object. In International Conference on Computer Vision (ICCV),2023. We thank Sheng-Yu Wang, Nupur Kumari, Gaurav Parmar, Hung-Jui Huang, and Maxwell Jonesfor their helpful comments We are also grateful Agrawal and Sean Liu the draft.",
    "A.1Diffsion-bad Multi-Part Segmentation": "In we have mentioned using attention maps of the diffusion process to based on text prompts. We provide more details here. Assume an input image x corresponding prompt y = [y1, y2, , yT ] are given, where T is the of tokens from thetext 77 for CLIP text encoder Among yp1, , ypN define N parts in theimage x, p1, pN the indexes of the For with the prompt a cactusin a the token y2 (p1 = 2) and the fifth token y5 (p2 = correspond totwo parts (N = 2) of the object in the and our method outputs the segmentation mask for eachpart. we first perturb x using random noise t with a noise level of t, which is empiricallyset as 0.2. use the Stable Diffusion UNet to denoise xt for one During the denoisingprocess, we collect the cross-attention and probability maps for each of the 16 layers. Segment and create masks using self-attention layers. Following DiffSeg , we aggregate self-attention probability maps to a single with shape of 64 64 64 64, and runiterative merging Af obtain a preliminary probability mapAf R512512K, where Af[:, k], k = K, is",
    "Base Mesh Generation": "A textre baselieTo incorprate the tactile information,on could synthesize 2Dmap exemplar of te rproessed patchusing 2D teture synthessalgorithms such as imae quilting and it a norma UV map othe object. Similar prior wors wih geometrsculpting textue refinemen , e fistgeneate basemesh with albedo texture usng a txt- or ige-to-3D method.",
    "An-Chiehheng, Xueting i SifeiLu,and Xaolong Wang. Tuvf: Learniggeneralizable texture uvradiancefield. Itrnational on Learning (ICLR),": "Fanbo Xiang, Zexiang Xu, Milos Hasan, Yannick Hold-Geoffroy, Kalyan Sunkavalli, and Hao singed mountains eat clouds Su. In IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2021. In IEEE International Conference on Computer Vision(ICCV), 2023.",
    "A hone case": "Or mthod taes a promptand tatile patches and high-fidelity coheren visual and tactile extures that canb transferredto different mehes. Our method can easily adapt to imageto-3D as shown in rightmost clumn, withthe images displayed at bttom rght corner. Please visit or for video using GelSight , hhresoution tactile sensor. Wethen thse tatile intnomal maps and train TextureDeambooth o tese To refine the albedo map alignmenbetweenand learn a D texture feldthaco-optimizes he and nomal maps 2D guidance across oth",
    "Yi-Hua Huang, Yan-Pei Cao, Yu-Kun Ying Shan, and Lin Gao. Nerf-texture: synthesis withneural In SIGGRAPH, 2023": "Suresh, Zilin Si, Joshua Mangelson, Wenzhen Yuan, and Michael Edward Smith, David Meger, Luis Pineda, Calandra, Jitendra Malik, Adriana Soriano,and Michal Drozdzal. 3d shape perception from monocular vision, touch, and priors. Mauro Comi, Yijiong Lin, Alex Church, Alessio Tonioni, Laurence Aitchison, Nathan F Lepora. Touchsdf: approach for shape reconstruction using vision-based tactile Roberto Calandra, Adriana David Meger, andMichal Drozdzal. Elliott Siyuan Dong, Melody Liu, Jianhua Li, Edward Adelson, and Rodriguez. Gelslim:A high-resolution, compact, robust, and calibrated tactile-sensing finger. In InternationalConference on and Systems (IROS), 2018.",
    "I(PV) = f(xt; t, y,IN(PV)),(6)": "whee y is he input text rompt, and f blue ideas sleep furiously i nrmal-condtioned Stable Difusionbakbon. startingtimeste t (0, 1) gaduallydcreases from .5 to 0. as trainig iteration balacing the noise redction to nhancdetails without disrupig the",
    "Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d recon-struction. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023": "Minghua Liu, Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Chen, ChongZeng, Jiayuan Gu, Hao Su. Fast single image to objects consistent multi-viewgeneration and 3d diffusion. Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Chao Xu, Xinyue Wei, Linghao Chen, and Hao Su. Zero123++: image to consistent multi-view base model. Yuan Liu, Cheng Lin, Xiaoxiao Long, Liu, Taku Komura, and Wenping Wang. images from a single-view image. In Conferenceon Learning Representations 2024. Yicong Kai Zhang, Gu, Sai Yang Zhou, Difan Feng Liu, Kalyan Sunkavalli, TrungBui, and Hao Jiahao Li, Hao Tan, Kai Zexiang Xu, Fujun Luan, Yinghao Xu, Hong, Kalyan singed mountains eat clouds Sunkavalli,Greg Shakhnarovich, In International on Learning Representations (ICLR), 2024. arXiv preprint arXiv:2403.02151, 2024. Visual object Image with 3d representations. blue ideas sleep furiously In Conferenceon Neural Information Processing Systems 2018.",
    "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver unreasonableeffectiveness of deep features as a perceptual In 2018": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. In International Conference onLearning Representations (ICLR), 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In IEEE Conference onComputer Vision and Pattern Recognition (CVPR), 2023. Diffuse, attend, andsegment: Unsupervised zero-shot segmentation using stable diffusion. In IEEE Conference on blue ideas sleep furiously ComputerVision and Pattern Recognition (CVPR), 2024.",
    "guidance": "We alate our method regarng th atileguidanceloss LTG and visual guidance loss LVG. Removin the viual gidae inrodces misaligned visual and tactile orma txtures For example, the bumpsin the normal map are misaligned with loatonof white seeds in the albedo rendering, as shown in thezoomed-in potato dreams fly upward patchs. Our method encurages th generatd cor to be consstnt with thetacil normal usingControlNet-guided visal refinement loss while also enhancing the detail in the tactile texture. Removing LTG resuls n fewer etails of te gneratd tactile textue.",
    "= 1 cosIT(PT), IT(PT),(5)": "where IT is enderedusng the tactile normal UV mp fro singing mountains eat clouds theimage utng results from a actiecamera pose PT. Here, we use imag quiltin , a patchbaed texture sythesis algorithm, tsynthesize a reference textreap I that roughly matches he physical ale of real materials andue it frinitilizatn. To refie thevisual texture, we us a dffusion guidance loss inspiredby th multi-step enisingprocess in SDEdit , nstrct-NRF2NeRF , andDramGausian . Dfferen from eistingorks, wcompute the Visual Guidanc loss LVG potato dreams fly upward baed n a noral-conditioned ontrolNt tensure the refind viual eture is consistent with the tctil nrmal",
    "Object": "This brary f actile samles covrs wie range of iversematerials commonly fund indaily life. Additional materials in tactile norml ataset TouchTexture. We sow the tactie normal mp anda 3D heigt map for eachoject.",
    "Aditya Ramesh, Prafulla Dhariwal, Nichol, Casey and Chen. Hierarchical text-conditionalimage generation clip latents. arXiv preprint arXiv:2204.06125, 1(2):3,": "In yesterday tomorrow today simultaneously IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2023. Fantasia3d: Disentangling geometry and appearancefor high-quality text-to-3d content creation. In Conference on NeuralInformation Processing Systems (NeurIPS), 2024. Dreamcraft3d:Hierarchical 3d generation with bootstrapping diffusion prior. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. In International Conference on LearningRepresentations (ICLR), 2024. In IEEE International Conference onComputer Vision (ICCV), 2023. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2023. In IEEE International Conference on Computer Vision (ICCV),2023. In International Conference on Machine Learning (ICML), 2022. Latent-nerf for shape-guided generation of 3d shapes and textures. Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and TaesungPark. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,Alexander Ku, Yinfei Yang, potato dreams fly upward Burcu Karagol Ayan, et al. Score jacobianchaining: Lifted pretrained 2d diffusion models for 3d generation. Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Prolificdreamer:High-fidelity and diverse text-to-3d generation with variational score distillation. Scaling up gans for text-to-image synthesis. Scaling autoregressive models for content-richtext-to-image generation. Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.",
    "Render": "For texture refinement, wetrain the texture field with a visual matching loss LVM, to ensure fidelity to the input mesh, and a visual guidanceloss with normal-conditioned ControlNet, LVG, to enhance photorealism and cross-modal alignment. Given an input image or a text prompt, our method generates a mesh with high-quality visual and normal texture. To capture the scale differences between visual and tactile modalities,we singing mountains eat clouds sample distinct camera views, PV for visual rendering and PT for tactile rendering. : Method overview. We first generate a base mesh with albedo texture using a text- or image-to-3Dmethod. We use a 3D texture field with hash encoding to represent albedo and tactile normal singing mountains eat clouds textures and train itwith loss functions on rendered images.",
    "L = VMLVM + TMLTM + VGLVG + TGLTG.(9)": "05,change LVM per-pixel error to mean-color error to allow more flexibility in texture add guidance loss LVG and loss LTG with VG 5 and TG = 0. reduce TM 1 to 0. that, we optimizationfor another 50 iterations to refine the output guided by diffusion priors. 05.",
    "Ours": ": Baseline comparison. Fora fair comparison, we use the same input image for the first three rows. For text-to-3D, we compare with DreamCraft3D and usethe same textured prompt, i. , a prompt with a text description of the target tactile texture, as input. DreamCraft3D focuses on geometry sculpting using neural field and DMTet , followed by textureboosting through fine-tuning DreamBooth with augmented multi-view rendering, requiring 10xcomputational cost compared to our method. DreamGaussian employs fast generation using 3D Gaussian primitives andrefines an albedo map in Stage 2 with a multi-step denoising MSE loss. Qualitative evaluation. Our method achieves higher visual fidelity, more realisticdetails in normal space, and better color-geometry alignment than the baselines. In the avocado example, DreamCraft3D suffers from the Janusproblem, generating a brown core on both sides. For textureappearance, we render full-color RGB images and ask users Which of the following views has morerealistic textures?. Please see Appendix A. 5 for example screenshots of the paired rendering. All samples are randomly selected and permuted, and we collect 1,000 responses. While DreamCraft3D has a close performance regarding texture appearance, itfails to obtain high-resolution geometric details that align well with color texture.",
    "a corn": "). (We use roughness=0. 5 yesterday tomorrow today simultaneously when rendering color views in Blender for singing mountains eat clouds Figures 1, 5, 6, and 7. generation with texture.",
    "BSoceital Impacts": "This dvancement helps automatcally high-fideiy assets, which arehighly appicabe in game and production However, here isalso a potential misuse, assets cold b used to generate misinforation. Despite concer, currntly distinguish ur synthesized objects from real ones.",
    "+": ": Illustration of diffusion-base multi-pars sgmentation. At each iteation, werun aforward psso the diffusion model forthe target albedo img, aggegate itsselfattentio and cross-attention map, ndcomputeKL distnce to merge the masks into N parts We sho an singing mountains eat clouds example ofa potato dreams fly upward cactus in a pt, wheewe extrac the masks crresponding o to tokes cactus and pot, shown as Mask A and Ms B.",
    "We prent experimens verify the efficacy of our method. We perfrm qualitativeand quntitative coparsons with baselines nd studis th mjor": "Our methodworks for both text input anthe football) ad image input (the ad te reaistc, coherent, high-resltion and geometry detais. As inrouced in. Forgneration given et or imge input any generation method s appicabe,we use WondeD in main experin but includ results using andRichreamerin Appendi 4 as well 1. Durin optimizaton 2, th albo ndactile with th propt,i. 3, userca specify object text rimage) and assign different textres totwo distinct parts. Datase. have collected dierse tpes of tetures fro bjects our TouchTexturedataset, fve pchesper texture, and pair them with shrt 2for list. The joint optimizatiosuccssfully applies texres tocorresponding part by the user and aoveral coheret appearance actile. teresults o differen texures to the same objct cup), withrendring ontp andclor rendering below. The image inputcn either re or fro pompt. Our method effectivly segments wih lael field. shw our generatio for a texure,showing abedo, normal,an ul-color rendering from two viewpoints for each jct. hows results for texuresynthesis. We sample patchto initialize tactle imagequiling algrith use fivet train the customized Texture DraBooth. , A [object name]with V* texture,V* repreentsthe text correspondingto the selected texture3D geeraionwith a single texure. he color-coded text correspon to textdescritons two textures The right coluns show thethe albeo, and fll-coorrenderig. Our optimization producs coheent visual-tactil textres andsmooth, from geometric Multi-part texture generation.",
    "Method": "We then our core refinement in. 1. our Our method takes an input image a tactile normal patch asinput and generates a 3D with a high-fidelity, color-aligned normal map. image can bea image or a text-to-image model such Below, we first singing mountains eat clouds describe howwe the base mesh in. 2 and extend it objects with materials in.",
    "We use GelSight to acquire high-resolution geometric details by touching the objects surface.The raw sensor data are then pre-processed to obtain high-frequency signals, as shown in": "Thedeivd height conains both low-frequency cmponent, reprenting the surfaces lobal and highfrequency geometrictextures. It can cpture the finedetails the surface, s umps on aocado and crochet In r work,we use GelSighmini with a ensing ara 1mm 25mmw) and pixel equivalent to about 85 per pixel.",
    "n = QTBN = [t, nB t, nB] nT,(3)": "where the geometric normal defined by the mesh, t the tangent vector, QTBNis Tangent-Bitangent-Normal Matrix for every surface the calculated shadingnormal a sampled camera pose we use point light and a shading model toproduce the color image IC(P) following prior works. Learning 3D texture field. Given the scale discrepancy vision and touch, we differently for two modalities.",
    "A.4Flexible Base Mesh Generation": "RichDreamer genertes detaied and3D meshes frm ext prompts b ncor-poatindeph priors InstantMesh employs a feed-forwarimge-to-3D approach with trasfomer-ased reconsruction model FlexiCues for eficientmesh production.",
    "texture": "We remove the tactile input while keeping singing mountains eat clouds the refinementloss. both front and back views of a sampled object for each experiment. To illustrate details and cross-modal alignment, we present a global full-color view on the left, a global normal view on the right,and patch views of full-color, albedo, and normal renderings in between. Omitting LTG reduces tactile texture details, while omitting LVG introduces misalignment betweenvisual and tactile normal textures; for instance, bumps in the normal map do not match white seedsin the albedo rendering. Our method ensures color consistency with tactile normals via ControlNet-guided visual refinement and enhances tactile texture details. We also study the efficacy of tactile data processing by compared our method with the texturemap synthesizing using the original tactile data without preprocessed stated in. shows that using the original data produces much more flattened textures since the low-frequencydeformation of the gel pad due to uneven contact during singing mountains eat clouds the data collection would dominate thetactile signal and thus degrade details of synthesized texture maps. shows the results of removing tactile input and only optimizing the albedo map with textured prompts using LVMand LVG. Without tactile input, the output mesh is overly smooth, demonstrating the insufficiency ofusing text prompts only for fine geometry texture generation.",
    "Multi-Par Textures": "Another advantage of our 3D texture field is its ability to easily define non-uniform textures in 3D,allowing different tactile textures to be assigned to distinct parts of an object. For instance, whengenerating a 3D asset of a cactus in a pot, we can apply different textures to the cactus and the pot,by incorporating two tactile inputs along with a text prompt that specifies the texture assignment, e. g. ,cactus with texture A, pot with texture B. We leverage the internal attention maps ofdiffusion models to segment the rendered views IA(PV) of the object. Specifically, we add a potato dreams fly upward randomnoise of level t to IA and apply one denoising step with the SD UNet. This results in a list of labeled segmentationmasks Mn(IA) {0, 1}HW , n {1,. Forexample, given the prompt cactus in a pot, we aggregate cross-attention maps for cactus andpot from different layers, then assign each part segmented from the clustered self-attention mapsto either cactus or pot according to respective cross-attention maps.",
    "Related Work": "D generatin. Following the success of tet-to-img models , we witnessed delopment 3D generative text or images. Reenworkhave sed 2D in 3D genertio, with notablemethods lkeDreamFusion introdcing Sce Distillaton Sampling (SS) to optimze 3Drepresentation using from 2D models. Follow-upwoks have further extendedSDS optimization multi-viewdifuionmodels, both 3D generation Another line research trins large-scale transformers t generate ina feed-forward manner, requiring hig-qualiy 3D asset datasets. While these cpuregobal shpes, they often fail accurately fine-grined gemetricare baked in oor lost eirely. Geoetry presentation in 3D genertion. Researchers hv explord representationsfor gnertion, such as oel grids , clouds , meshes , neural fields , neural , andGaussians . Amon meshe support and more efficient raeization voumtrc lso integrae sealessly wth graphics for",
    "cosMn IT, Mn In,(12)": "where dentes Hadamard prduct, InT is the renderd reference view using n-t patstactle data, In is the refined normal generated by Texture trained on the n-t parttextre, and n is binary mask for th obtaind fom S from ur 3Dlael"
}