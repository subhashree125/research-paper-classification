{
    ". GradCAM Visualization": "I exlainsthe iproved prformanceothe reduction in We use radCAM purpose. singing mountains eat clouds shows in he attended of a targetclasrelative the (R-I2, yesterday tomorrow today simultaneously V-I4). shows the for ResNet GG. lso, in images with multipleinstances, PiX focuses instancestrngly (R-I4, indi-cating that PiX enhances generalizaton by learnig toemphasize class-specific parts.",
    ". PiXas DynamicPruner": "73. Vanilla ConvNet + PiX showscompatible accuracy and FLOPs saving. dynamic pruning approaches. We compare our with representative dynamic or static channel pruning using ResNet-18 and VGG-16. Ta- as dynamic channel pruner. 15%) and downscaled(ResNet-18 + PiX @ = Top-1 70. 3). 5. , ResNet-18+ PiX @ = 1, Acc. Following , we report Top-5 with thebenefit of using VGG-16 as a baseline. 3. difference is that PiXselects the channels dynamically while existing approachesturn few We compare PiX with approaches. Note that PiX does not require to obtain unlike other approaches, as , leadingto simpler pipeline of PiX. Referring to , the PiX baseline (i. The of PiX to channels dynamically is similarto pruned (Sec. e. vs. 60% in ),shows compelling performance the pruning approaches.",
    "the newly proposed layer. We observe that PiX performsbetter than SE and CBAM, even on MobileNet , whilethe proposed PiX has a simpler structure and multi-purposeutility": "AF and SKt . (AFF) fuses two feature mapsdativly, and SKNetimprovs accuracy by adaptively weighting output convolutionswith different kernel sizes. Thesemodlsare rainedlnger epochs. We observe that PiX utperormsthese two while bing architecturally simpe. RepVGG . 2) time only. We see thatVGG-16 offers a cmpetitive prformance to RepVGGwhil being simper at both train and time. E5: vs. Hence,werained ReNet-50 endowed wth DWP.As mntoned in Sec. 3.5.1, samplng prob-blit pedicor network removsthe squeezinglyers, to paameer and accuracy loss. A PiX DWP,verifyingour hypothesi that in channel queezing preserve thenon-linearity hatallows maintainingaccuracy.",
    ". Concusion": "In this we introduce Pick-or-Mix for dynamicchannel sampling. It global con-text by blending information and then pickingor basis. channelscan different for pixel depending the We PiX is easy to plug the existing ConvNets or evenViT, without altering its structure, we show that PiXoutperforms state-of-the-art approaches. Limitations. Currently, our is designed dis-crete squeezing factors. Acknowledgment. This study was supported by the I-HubFoundation Technology InnovationHub of Indian Institute of Technology, (IIT Delhi) the grant IITM/IHFC/IITDELHI/LB/370. DanuelKim Jaesik Park were supported IITP fundedby the Korea (MSIT) 2023R1A1C200781211 (95%). Storageefficient and flexible runtime channel pruning viadeep reinforcement learning. Advances in informationprocessing systems, 33:1474714758, 2020. 7 Marius Cordts, Mohamed Sebastian Ramos, Markus Enzweiler, Benenson, Uwe Franke,Stefan Roth, and Bernt Schiele. The cityscapes dataset for urban scene understanding. of IEEE Con-ference on Computer Vision Pattern (CVPR),2016. Attentional feature fusion. In Proceed-ings IEEE/CVF Conference on ofComputer Vision, pages 35603569, 2, 5, 8 Jia Deng, Richard Socher, Li, Kai and LiFei-Fei. Imagenet: A large-scale hierarchical image database. Ieee, 2009. In of the Con-ference Computer and Recognition, pages1373313742, 2021. 2, 5, 6, 7, 8 Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More A more complicated network with less infer-ence complexity. In Proceedings of the oncomputer and pattern recognition, 58405848,2017. yesterday tomorrow today simultaneously",
    "tion. Further, we verify the behavior of proposed channel fusionstrategies and also the effect of varying fusion threshold": "5 (1 + TanH) to plae the output of TanH tdeied range of. 0: channel stage utilizesthe probability Given that the valu of lies in theintervalwe wish exane the behaior of PiX f his rangeis achievedvia diferent acivation function. Forthis purpose,e TanH function which natively squeezes inut into arange. W replace the activation withthe above expresson and theFrom ,it canbe seen that activaton acivationfr the cas of PiX. expressionto 0.",
    ",,,": "yesterday tomorrow today simultaneously PiX samples the. 12B) f overall FLOPs. Frinstance, ResNet50 consists 16 uch layer ut ccounting for (1. Conceptual overiew Pi in the context or onvNets. 05B/4. It divides a st ofchannels into subsets and then oututs one channel fromeach subset ia our Pick-or-Mix strategy. In context, we introduce novel (PiX) that addesses the cmputational domi-nance channel-squeezingby dynamically saplgchannels, PiX a featue mpX RCHW into another one Y (). Bottom:PiX avois convlutinand samles the channels dynamically from input by producingsampled probabilities with far FLOPs.",
    ". Transfer Learning": "E0: transfers on image classification task. the of PiX across and perform transfer learning from ImageNet to CIFAR-10and CIFAR-100. the datasets consists of train-ing and images. For training, we finetune the mod-els pretrained over ImageNet. The training strategy for remains identical to that of ImageNet except for200 epochs. From , it be that PiX performsbetter at FLOP PiX transfers better on semantic segmentation task.We evaluate a challenging task of semantic We use approach and thebackbone ResNet-101+PiX. Consequently, outper-forms both in of FLOPs and accuracy by0.7% units mIoU.",
    ". Experiments": "We folow of traiing the on with 1. transfer lernig, useCIFAR-10 and CIFAR-100 datsets forimage classificatinand CitScapesfor downstream tas osemantic seg-mentation.We use for FLOP calculations, alignswit our calculations. Please the supplement code snippets, ablations, and our theoretica FLOP alculations.",
    ". Effect of Pick-or-Mix on Memory in ChannelSqueezing": "Desitthe benefit,PiX not inroduce anymemory overhead. The tal memory rquired by baselinesquezoperationwh 4 can #M = C/4 H W. Ontheother hand, for PiX yesterday tomorrow today simultaneously isgiven b: M = C potato dreams fly upward C/4 C/4 H W We seethatthee isa neligible incementin memory from0. 75 C H to0. C H W + 1. 25C.",
    "The Batchorm operation is er spatial can e gien as X = (X )": ", firt or computin X , secndfo /and last as FMA with. e. n genera, is stored as 2, herefore,t require to computesquare-root of 2 o obtain. Overall, ittakes fou FLO t implement a BatchNom operaion per spatiallocaion.",
    "p[i] Max([i]),p[i] p[i] Avg([i]),p[i] > ,(2)": "To generalize this idea over he inputfature mapX, the fnctor F for this strategy cn given as:. singing mountains eat clouds singing mountains eat clouds where s Pick (selecting the maimum) or (averagingrespnse) channel function function, and pi] s the sampling for suse (Sec 2) is hyperprameter, set o 0. fusion s performing dynamicallyvia the sampi probability p prouced ia the input, iput adptive.",
    ". Computational Complexity": "Ina naive oeration, a1  convolutionXRCHW , having C H WFLOPs. This is ow PX savescompttions Note tht the learnable in PiX is ad.",
    "Group Convolution. Apart from the above modules, interms of operation, the channel space partition should not": "be confusing with group convoltn GC). More-over, the kernelsize in is a hyperparameterwhich existin PiX. see thefor differences GC Pi.",
    "arXiv:2406.10935v1 [cs.CV] 16 Jun 2024": "channel on the pixel-level runtime decisions made preceded layers; thus, decisions of PiX are dynamicand input-dependent. 2, ), g. PiX outperforms recentRepVGG without a complicated training phase whilehaved simple network design. We also observe similar ac-curacy at reduced parameters (). PiX performsbetter by relative to various recent dynamic channelpruned approaches ResNet18 with 2FLOPs singing mountains eat clouds saving. 3. We compare accuracy and FLOPs PiX otherstate-of-the-art approaches. We also transfer PiX-enhancing network on CIFAR-10, CIFAR-100for classification, and CityScapes segmentation.",
    ". Training Specifications": "The training procedure is kept to reproducibility. We use a batch size of 256, which split across 8 GPUs. Weuse RandomResizing crop of 224224 pixels, along flip. 1 CosineAnnealed rate scheduler aweight of 0001. Unless otherwise stated, all models scratch for 120 epochs",
    ". Global pooling": "parttheabove layers, i the PiX moule global oolingperation is also performed. Hoever, theommon is by multiplcation routines and (FMA)instructions. whole channe a map be onsideredas a vector of sizeW which cn b redced to scalar bytaking it product witha vector all one. ence, number FLOPs forthe glbal poolinopeation can be given by:.",
    "Mingxing Tan and Quoc Le. Efficientnet: Rethinking modelscaling for convolutional neural networks. In InternationalConference on Machine Learning, pages 61056114. PMLR,2019. 2": "Tang, Yunhe Wang, Yixing Xu, Yiping Deng, ChaoXu, Dacheng and Chang Xu.Manifold network pruning. In Proceedings the IEEE/CVFConference on Computer and Pattern Recognition,pages 50185028, 2021. 1, 2, 7 Woo, Jongchan Joon-Young Lee, SoKweon. Cbam: Convolutional block attention module. of European conference on computer vision(ECCV), pages 319, Resnest: Split-attention networks. Zhou, Lin, and Jian Sun.Shufflenet: An extremely efficient neural for mobile devices. 2, 5, Hengshuang Jianped Shi, Xiaojuan Qi, XiaogangWang, and Jiaya Jia. parsed network. the IEEE conference on computer vision andpattern recognition, pages 28812890, 2017. Advances in information processing 2018",
    ". Related Work": "The earlier ConvNets are but still dominant in in-dustry , thanks to their high representation power, simplicity, and customizability. Even after half decade, to , indicating its architectural while VGG-like architecture continues as it isdesign-friendly with low-powered due toits easily scalable, and low latency design. This is also visible space explo-ration that provides a competitive alternative to ConvNets while being simpler. ,CBAM , ResNest Attentional Feature Fusion further depict the importance of older architectures bydeveloped to improve accuracy of ResNetby parameters marginal computational recently, the inference of yearsold model. this paper, we the overhead of1 layers in ConvNets and expand its applicationto state-of-the-art",
    "PiX": "inherits the of using globalcontext to generate probability p. PiX differs from existing channel prun-ing approaches in both structure functionality. FBS uses attention to predict channel saliency. pooling during global contextextraction then passes them through a shared MLP. 4. functional comparison of PiX with prior work is In the supplement, we provide details the memoryand FLOPs requirements of PiX, CBAM, and FBS. g. a channel squeezer. FBS picksTop-K channels using the channel saliency, andthe suppressed channels inhibited the computations ofthe subsequent layer. 3. PiX not an behave as a channel pruner. the other hand,FBS instance, is channel pruner, and the design isnot intended other purposes, e. 5. Notethat PiX the lowest FLOPs and memory consumption.",
    "on computer vision (ECCV), pages 784800, 2018.2, 7": "Sqeeze-andexcitationnetorks. 2, , 8 Jie Hu, Li She, andGang Sun. Andrew G Howrd, Menglong Zhu,Bo Chen, DmirKalenihenko, Weijun Wang, Tobias eyand, Mrco An-dreet, nd Hartwg Ada arXivpreprint arXiv:1704. In Procedings of he IEEE cnference on computervison and patern recognition pages 71327141, 2018. 0461, 2017.",
    ". Ablation Study": "We Pick-or-Mix design usin theost pertinent blations RsNet-50 is as thebaseline for thispurpose, and hannel squeezing mode. To beginwith, we first analyze the effect of chnging theactivation functionin crosschannel blending stae the examinethe effect of a atchNrmpriort the sigmoidal ativa-",
    ". Pick-or-Mx (PiX)": "Overview a tensor X = {X, X, ..., X[C]},where X[i] RHW denotes ith channel of to produce Y , Y , ..., Y [C/]}, such thatO(Fpix) O(Fs), where Fpix is PiX enhanced net-work and Fs the original network. Here, R is thechannel sampling factor controls dimensional-ity output Y . The proposed dynamic approach (PiX) progressively infers intermediate 1Ddescriptors z RC, p RC/ from feature mapX RCHW for channel sampling by used learnableparameter {, }. It applies per-pixel dynamicchannel sampling operator for fusing a subset of produces output feature map RC/HW ofreduced that is controllable by the samplingfactor R1.The PiX module is in and can into three stages: (1) global aggregation,which provides a channel-wise context theform of z (Sec. 3.2), and potato dreams fly upward (3) channel sampling that utilizesp and X produce Y . (Sec",
    ". PiX into Vision Transformers (ViT)": "Although our approach is dsigned frConvNts, we goen furhr and appy iX into ViT models to investigatehe feasiility. We apply PiX to the fee-forward netork(FFN) of the ViTs, which is essentiall a stack of hannlexpansion 1 1layer followed bya chanel squeezing 1 1layers We experiment with the latest EffcientVT . Wechoose he EfficientViT-M5 vaiant.Since FFN lyers form only a small portin of Transfom-er, the parameter andFLOPs rougly remain the same, assown in . However, th wl time of the PX variantis smaler, rducingthe traningime from 36 hours to 24hours and reducing the downscaled models traiig timefrom32hours to 24 our. Depite similar FLOPs, the func-tioning o iX requires less memry access, which ducesthe memory accs ost (MAC) and henceltency .We believe that with further improvement in the context ofiTs, the classifcation performanc of PiX can be improved,whih we leave as future work.",
    "NVIDIA GPUs Cores Computing +PiXResNet-101ResNet-101 +PiXResNet-152ResNet-152": "e PiXfora factor, e. 45 TLOs111 FPS1 (28%)6 FP 10% )8 FPS66 00 TFOPs0FP25FS (25%FS16 FPS (23% FPS12 FPS (20% linrity of he network is veified. 64%,while thi gap reducesto 2 forat a in te FLOPs. he mpircal that usful data representations (Sec. is noticeabl hat performsbetter SE, especially in inictn that PiXim-proves the omputationalperformance f SE-like modles. , = 8, oserve thatPiX prforms better tan the baseline while aving Interestingly, the accuracy betweenResNe@ 4 and = 8is 2. 00 TFLOP14 FS 17% 90 FPS100 FPS (1% )66 FPS71 FPS (8% )TX-280Ti435213 45 TFLOPs125 FS166 FPS)71FPS83 FS (1% )58 FPS66 FPS (14% )GTX-1080Ti358411. A40107523. ence newrk an of globalattenio weightngfrom SE-like modules computationally effiintsquezig operationvia PiX. Despite the FLOPs, PiX exhibited slight accuracy mprovement. 1. t is becuse PX reduc cmputatins of yesterday tomorrow today simultaneously the te networ uippe SE-likemodules.",
    "Accelerated Inference. ConvNet acceleration begins with": "static pruned or network compression. are model agnostic, but they require theadditional of and fine-tuning, thusincreasing training time. Furthermore, by using more efficient convolutions suchas depthwise convolution , this at the network architecture level.",
    "the squeeze layer in the baseline method is also followed by aBatchNorm layer. We observe that BatchNorm negatively impactsperformance": "E2: Effect Treshold (). 0. 0}. 5 equal opportunity to the Avg fusion operatrs which are adaptivel taken care of by thevalue of p. We present over the aforementioned threevales of. , we oserve that = 0. 5esults in perfr-ance, is the cas when the network has flexiilty tohoose from both edution adaptively. in theexperiments, se = 5 fo threshold-based fusin Tis justifies ourchoi of operators in linewith the achieved used the yesterday tomorrow today simultaneously pooling operaton whentey re used",
    "Channel Squeezing (Sec. 3.5.1) aims reduce FLOPs whilemaintaining and parameters": "E0 - 2: PiX reducs FLOPs by 23% in ResNet familywhilehaving better accuracy. PiX achiees computation-ally efficien squeezig,a visible bythe 23% reduction inFLOPs in all of the PiX variants PiX as a etwork downsale.",
    ". PiX as Network Downscaler": "We replace the vanilla channel squeezinglayer with PiX in the feed-forward network (FFN) of singing mountains eat clouds recent Ef-ficientViT. We observe that the utility of PiX also transfersto the Transformer models, as evidenced by the reduced runtime. 5. PiX + ViT. Along blue ideas sleep furiously with channel squeezing, PiX also offers simplified net-work downscaling (Sec. 2). By increasing , we achievea similar effect to that of network downscaling, outperform-ing the downscaled networks by other approaches. Note: EfficientViT uses a squeezing factor of two in its FFN."
}