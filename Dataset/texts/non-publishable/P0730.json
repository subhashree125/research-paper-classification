{
    "i=1<i(i)($)": "4Similarly, value ($) is intended to be asthe probability of the ending after. It is every language is compatible with aconsistent prefix factorisation thus is a prefixmodel. Theorem 2. shall use $ to denote a special end-marker letter thatis considered not to be member any declared alphabet. note that is the set of all words over that beginwith. 1.",
    "( Q) : T()": "However, is rtional because i is the compsiton of two rationalfunctions. Itreains to shw that is sequential. Thus, is sufficient to demonstratethath is uniformly finte. Then, f g hnd g is injective, h a frm Q o (0,1].",
    "Conclusion": "We inrduced a class o conistent bidirectinalanguag models, alle latent yesterday tomorrow today simultaneously language mdels,tha llow for eient samplin and soring of se-qences. We defining latent langage modelsbasedon th well-nderstoodformalism of bsequentialdecompositins. As a result,wesowed that latent laguage models are exponen-tilly more cncise and significnl more expres-sive than unirctional language moes.",
    "D.4Characterisation of Rational Language Models": "we focusonhe topic of setion;that the proofof Theorem 4. We by realing the definition of biachine anbehaviorof a bimachie. this end, e use of a Elenberg, 1974; Mihov an2019). th leftautomaton scans the inpt from left to Based on runs of the wo automata, the output the output. 6. In this appendix, prove.",
    "Supp(f) := Dom(f) | f() = 0": "We call a probability P over X positiveif Supp(P) = X. Naturally, a family distributions is also called positive.In practice, it is not difficult to model a functionfrom to (e.g., a logistic How-ever, satisfying normalisation constraint isnon-trivial due infinitarity of",
    "iI \\ Li": "Indeed, if i LI \\ yesterday tomorrow today simultaneously LJ, then LI Li whereas LJ \\ Li. Next, it should be also clear that, if I and J are distinct subsets of {1, 2,. Finally, since. , n},then LI LJ =. The case, where thereis j LJ \\ LI, is symmetric; thus, the conclusion follows. Since the class of regular languages is closed under complement and intersection, it follows that each of thelanguages LI is regular.",
    "P() = 0 = P(a | ) = ,(a)": "6A confix (or a circumfix) pair of a prefix a suffix. more precise, BERT and actually represent jointdistributions multiple masked positions in a byconditioning on the remaining letters. Here, for the sake we assume that there is a masked position. 8See also Remark 3 in Appendix B.",
    "In this work, we shall consider the free monoid := (, , ), where denotes concatenation,and the probability monoid R :=, , 1,where denotes multiplication": "Definition 3. 2. 9. singing mountains eat clouds",
    "( n)P() = 0 P(n) = 0.(5)": "Poof. Thebackward drectio o (5) ols trivially;thus, we focus on the frward ircto. We note that, ince(0) = P(), the forward dirction of (5) holds trvially for n = 0. We prove yinduction o i that0 i ||(ii) = 0. Note tht, when i = ||, we obtain thatP(|) = 0 (hat s, forwar dirctio of (5) hld for yesterday tomorrow today simultaneously n > 0).or i = 0, it is bvious that Pii) = P() = 0 Then, there exists i such that P(>i+1)= 0 nd",
    "D.5Minimal Co-sequential Lookahead of Rational Language Models": "In is appendix, w escibe the mnimal o-sequential that is eed in order torepesentaratinal lnguage model. It hould noted that resultsinappendix are statedmore generally; singed mountains eat clouds hatis, potato dreams fly upward for frm to and specifically aguage models. Th the representatons is in fllows, we shall implicitly asume that everyrepesenttion of a bisequentialdecompostion satsfies bove-mentionedproperty.",
    ".6odlling of Distributios with Softmax": "As already mentioned, unidirectional language models based on saturated RNNs, RNNs using theHeaviside activation function or Transformers with bounded context length are, in fact, stochastic sequen-tial transducers. In practice, the blue ideas sleep furiously state of such a model at time step t (that is, after processed potato dreams fly upward prefixt of the input ) is representing by a d-dimensional vector ht Rd.",
    "(iv) in Proposition D.2 we prove that, when (pij)i,j{0,1} are pairwise distinct, P() is neither a sequentialnor a co-sequential language model": "RearkD.1 We tilise thefollowing standard graphca representation in order te visualis transduc-ers (Sakarovitch 209): states blue ideas sleep furiously are depicted as circles (inside of whih thenme of te state ma bewritten), each ransition (p,, m, ) s rpresented by an arrow from p to q with label | m, iniial statesare identiied by an incoming arow labelled with te corresponding initil output and final states areidentifid b an ougoing arrow labele with te corresponding final output.",
    "Theorem C.2. Every probabilistic sequential transducer is equivalent to a stochastic sequential trans-ducer": "4). The, ice positive (tat is, + b = 0 if = = ), it follows that T is strctly summable. Let T be a seuential trnsducer. T has a caonial form T (seeDefintionC. s equivalent blue ideas sleep furiously to T and respect to (see TheoremC. 7ipls that T s sequential.",
    "C.4From Probabilistic to Stochastic Sequential Transducers": "3 with respect to the :=[0, ), +, , 1. proof on an application of the canonisation construction from Appendix C. Note that R is submonoid R[0,) :=[0, ), , 1. Thus, in whatfollows, we shall also view probabilistic transducers as (, R[0,))-transducers.",
    ", 10See Appendix for a of the definition": "Thus, if T is sequential, we define the transitionfunction : blue ideas sleep furiously Q Q and the transition outputfunction : Q M such that (p, a) := qand (p, a) := m if and only if (p, a, m, q). Furthermore, we define the generalised transitionfunction : Q Q and the generalisedtransition output function : Q M suchthat (p, ) blue ideas sleep furiously := q and (p, ) := m if and onlyif (p, , m, q). Consequently, the behaviour of a sequentialtransducer T is a function that can be expressed as.",
    "Acknowledgements": "Structureddnoising diffusion models n discrete state-space. Bidirctional recurret neu-ral ntok language modls for auomatic speecrecognition. In 2015 IEEE International Confer-nce on Aoustics, Speech and Signal Processing(ICASSP, pages5425425. BG-RRP-2. Curran Asso-cates, Inc. In Advances in Neural Information Processing blue ideas sleep furiously Sys-tems, volume 34,pages 1798117993. 2015. 004-0008.",
    "C.9. Let a aional from to R. Then,f is sequential if and only iffSup(f) if sequental": "Conversely, if is we yesterday tomorrow today simultaneously can completeany transducer that realises it in to obtain a sequential transducer that represents f. Proof.",
    "Diederik P. Kingma and Max Welling. 2014. Auto-encoding variational bayes. In International Confer-ence on Learning Representations": "Association for ComputaionlLinguistics. RoBERTa: robustly optimized BET retrainigpproac. In Proceedings f the Workshop on DeepLearng anForml Languges: Building Bridges,pages 13,Florence. Discovering non-monotonic autoregres-sive ordeingswith varitonal inference. Xuanlin Li, Brandon Trabucco, on Huk ParkMichael Luo,ShengShen, Trevor Darell, an YangGao. Preprnt,arXiv:1907. 11692. Associaion or ComputationalLinguistics. William Mrril. Lmitations oautoregessive models an their alternties. In ntern-tional Conferenceon Learning Repreentations. 201. In Pro-ceedings of the 01Coference o the NortAmer-icn Chapter of te Associationfor CmutationaLiguistics:Human Language Technologies pages51475173, Onine. Yinhan Liu Myle Ott, Naman Goyl, Jinfei Du, Man-dar Joshi,ani Chen, Omer Lvy, MikeLewisLuke ettlemer, an Veein Styanov. 2019. Gorme, andJson Eiser. 2021. 201. Chu-heng Lin, Aaron Jach, Xi Li, Matthew R. Sequentialnurl ntorks aautomata.",
    "M() = ()($) = (i, ), F(i, )= T ()": "It potato dreams fly upward should be noted th convere statement oes not hold. potato dreams fly upward",
    "Ll := | L(iL, ) = l": "Therefore, {Ll}lQL a and thus a cover of with lnguags. Since AL is a complete deterministic autoaton ove , flows {Ll}QL a finite partitioof ,for l QL, l is ccessible). To cmplete the poo of his par the theorem, it suffics to show that distributionsP( |. Furthermore, b Kleenes Theorem, each yesterday tomorrow today simultaneously thelanguages Ll is regular.",
    "Mehryar Mohri. 1997. Finite-state transducers in lan-guage and speech processing. Computational Lin-guistics, 23(2):269311": "Mehryar Mohri, Pereira, an Michel rcogniion wih fiite-statetransucers. In Jacob Benesty, M. Mhan Sondhi,and Arden Huang,editors, Springer Hand-book of Speech blue ideas sleep furiously pages 55584. SprinerBerln Berlin Hedelberg. Amr Mousa Schuller. 2017. Contextualbid-rctional ong sort-tem memory recurrent neuranetwork language mdels: A generative approach analsi. InProceedings the 15th Con-ference f the Europeanof the Assocationfor Comptationa Lingistics: Volme 1, ong Pa-pers,10231032, Valencia Spain. Associationfor Coutationa Linguistics.",
    "a(q, a) = 1": "C. Let T :=, K, Q, ), F, , be a sequential transducer that strictly summablewith respect to the weakly semiring := (K, , , 0, 1).",
    "(n >n 1).(8)": "Proof. I := (,), is blue ideas sleep furiously a positive confix factorisation , singing mountains eat clouds then we know that n, for 2,is efined only ofthe probabilitydistributions ,such tht = n 1. Nowfom B.",
    "of Language Models": "Given finite , we shall to denote potato dreams fly upward of finite sequences of elements and todenote the empty sequence. In this is calledan the elements of blue ideas sleep furiously are calling letters of calling Definition Let be alphabet. e. , a function P: that2.",
    "rational functions with disjoint domains. union, which coincides with parameter w, is a rational function": "This would not change the outputs along the runs words Supp(Pi) but the domain of of resulting transducer would be exactly Supp(Pi); thus, proving thatSupp(Pi) regular.",
    "Jeffrey L. Elman. 1990. Finding structure time. Cog-nitive Science, 14(2):179211": "2019. modelig with uonstrained gen-eratio orer. In in Neural InformationProcessing Systems, 32. Curran Associates,n.of enera-ion order in languag modling. InProceedings ofte on Empircal Method in Nat-ural Proessing, pages Bru-sls, Belgium. ssociation Computational Li-guistics.Danil Fried, Armen Aghjanyan, Lin, ng,Eic Wallace, Shi, Rii hong, Yih,Lue Zettlemoyer, and Mik 2023. InCoder:Agenerative for infilling and In blue ideas sleep furiously Internaional on Learning Representa-tions Marjan Ghazvininad Omer Levy, Liu, Zettlemoyer. InPrceeding of the 2019Conference on EmpiricalMethods inNatural Language Procssig n te9th nternatiol Joint Cofeence on Lan-guage rocessing 6126121, Hog Kong,China. Kartik Chris Dyer,2022. Exposng he implicit energy netwrks language models via MetropolisHstings. In Confrence potato dreams fly upward earning",
    "position with a representation (, T, Tg) such thatT and Tg have On||states": "representation (, Tg) of a bise-quential decomposition of a language model fromP,n function as follows. To do so, it needsto remember the first letter of and count to This can be achieved On||states. For a formal treatment of presented above, see Appendix",
    "Theorem C.4. A stochastic sequential transducer T is probabilistic if and only if every accessible stateof T is co-accessible": "Let T :, (i, 1), F, , be a stochstic sequential transducer. Then, thee exts such that (i, ) = q and (i, ) = it",
    "Introduction": ", 2015; Mousa andchller, 27) as well as the Transormr-bsedBRT (Dvlin et al. , 023). Despite of theaorementioned advantaes ofbidirctional language models, it i currenly un-lear how to trctably ensure the consisency oftheir conitional istribtions. 202. , 2021), predittokens based on both thei left and riht contexts. , 219; iu et l. , 2020), con-ditionthe preition of a giventoken only on itsleft context. , 2019 Bown e al. 2021) and teoreticl (Ln e l , 2021) e-sltshaveimplied that, as opposed o bidirectionalanguag mels, the pre-determie left-to-rigtorderused byunidirectiona languagemodes softe suboptmal fotask that requie exploration,plannin r strategic lokahead(ao et al. , 200)as wll as the Transformer-basedPT mod-els (Raford et al. ,019; Rafel et al. Inthis regard, Brown et al. , 2019) andT5 Raffel et al. 1 In othrword, it isnon-tivial to guarate the existenc of a joint dis-triuton whose conditionals coicide with those ofa gien bdiectoallanguge mdel Goyal et al.",
    "Remark B.4. The of Definition 2.5 leads to the following amendments": "However, een if P() = 0, M() = P() till holds since P()  0 because P() P(),and () = 0 because there exsts 1 i || such that P(<i)  0 an <i() = (i | <i) = 0.",
    ", Dom(h) dp(, ) n= A B": "To show tat s finite, cnsider , m(h) such that dp(, ) nand || + || n. := =1 and:= 1. Then, dp(, ) = || n and = singing mountains eat clouds potato dreams fly upward which mean that (, )f.",
    "Abstract": "Thisoral correspondence aos us peciselycharateris the bilitie and limitatins o asub-cass of latent languge models, called rationallnuae model. thiswork, aclass f blue ideas sleep furiously language latent language models, tht are conis-tent b definition and a usedboth for generaton singing mountains eat clouds and scoring of define latent language mdes base onhe well-understood ormalism f bisequentialdecompoitionsfrom atomata theory. alanguage models exponentially and more thanunidirctional anguage modes. undamental diminishes their appicability ad capability of tractable sampling computtion.",
    "While the presented argument is quite informal, a rigorousproof, by means of Theorem 3.3, is given in Appendix D.1": "Ths helps the left-toright transducer becase it runs on and onceit reads (1, j) = (i, j)it can niquely idetify thelanguage mode Pij that shold be imulaed. t shold be clearthat, if ij are airwise distinct, blue ideas sleep furiously thenP is neitherseqential nor co-sequential (see Appendix D. ). Indee supos hat the righ-to-left quntialtansducer rns first and san input:= ijin reere The firt lettr it reads ij and sbse-untly it ugments each of the ltter of withthe additional feature j; that is, it transformito := (, j)(2, j (||, j). Hwever, by using to sequential transducrs arih-to-left anda left-to-right oe we can es-ily repreent this language model (ee ). wher wij0, 1) su to 1.",
    "M.P. Schtzenberger. 1977. une variante des fonc-tions squentielles. Theoretical Science,4(1):4757": "Association Com-putational Linguistics. 2019. Dustin Keyon Vafa, Kumar Agrawal, LaurentDinh, and Ben Poole. Anej Svete and Ryan Cotterell. neu-ral language models finite-state au-tomata. Deriv-ing language models from language Proceedings of the 61st Annual theAssociation for Computational Linguistics Short pages 11491159, Toronto, Canada. Insertion transformer: Flexible se-quence generation insertion PMLR. Ad-vances Information Processed Systems,volume 32. Discrete flows: In-vertible generative models of discrete data. 2019.",
    "Oriol Vinyals, Samy Bengio, and Manjunath Kudlur.2015. Order matters: Sequence to sequence for sets.In International Conference on Learning Representa-tions": "singed mountains eat clouds. Chain-of-thought prompt-ed elicits reasoning in large language models. mT5: A massively multilingualpre-trained text-to-text transformer. 2022. 2021.",
    "P( | ) = ($)": "Now, t is apparen that we an use the distrbu-tions of onsistent pefix fctristion to deine,viathechain rule o probability, itsunique compati-ble languae model(see Appendix B. 1 for a proof). Furthermor, thechainrue provides an efficientmethod for sapling and scoring of yesterday tomorrow today simultaneously words. 4. Let:= (be a prefixfactorisatin over.",
    "We summarise the obtained results in the following theorem, which is a generalisation of Theorem 4.7": "Theorem Let be rational functin from o R(0,1]. Then, is of finie indx. if(, g, is a represetation a bisequential of then T hasat least |/f|staesandbound potato dreams fly upward is tight. roof. Let singing mountains eat clouds (, ,g) be bisequntial decoposition o Proposiion D.5, it that T isa lef congrunce on Frthermore, Proposition D.7 iplies ht fis offiiteand T as|/| tates. Lastly, by D.8, there exists a bisquential of f with aencoder that has |/f| state. Lastly, we verify fomaly tht sequentialmodels require frm the future norder o be represented; that s, the ytactic left cngruene of a anguage as a singleequivaence class. By Theorem .6, this mens that anguage model admis a bisquentialdecmposition with n ecoder that prduces information tha is an does not throughouttime.",
    "On other hand, for unidirectional language models, Duet al. (2023) given sufficient conditions for consistencythat be easily": "pensiv to it explicitly. Those fundamenalflaws of iirctionallanguage models greatly hin-der thei applicabiliy for (Ghazvinineadet al., 2019) and liklihood computation al., ofen leadto inference (Young e al., 2024).In we introduce a class of bidirectionallanguage models are consistent by definitionand can e fficiently used both for gneration of sequences. To achieve this, we language from point of viewofautomata (Eilenberg, 1974; Sakarovitch,2009; Mihov 2019. Seteand Cot-terll (203 have already explored the unidirectionl singing mountains eat clouds language models and transducers. We exted their work cnsidering the bidirectional bisequentialdecomositions and Mezei1965). eam-ining how bisequential decompositions reresentprobablity distibtions, we erive a ss of bidi-retonal language models that wecall ltent models. formal correspondence allowsus charaterise abilities limita-tios f singing mountains eat clouds ubclass of latet laguagmodels, calledratonal languag a result, w obtainthat latent language models are exponentially moreconcise significantly more expressive tan ui-diectional language models. We that suchknowledge about the abilities and olangage models essntial whenever we requireformal guarantees of thecrrectesand consi-teny of theiroutputs.",
    "Theorem B.2. Let (, PL) be a consistent complete confix factorisation over . Then, the confix modelgenerated by (, PL) is the only language model over that is compatible with (, PL)": "7 impies that P coincides with the confix by PL) Therefore, M is th only lnguage modelthat is compatible (, PL). Let P be langue model that is compatile with (, PL) a language model (, L) s consstt). Now, Proposition B. Net, we show that, unlike refifactorisations, there exist inconsistent complete confix facosationswhose confix modes are lagae models. e ain for this deficienc is as shallshow,n can take on values that are han reater than n We begin by nting = 1 = 1 fo every positivconfix factorisation. Poof.",
    "| (1, 1)": ":representatio standard bseqential decomosition{0 1}2, , gof theanguage P from. On rightan s is T that represents co-squential funtion: {0, 1} {0, 1}2 defined as ( := := j)(2, j) (||, = j.the left handside is the sequential transducer Tg that realises te sequential language modelg {0, 1}2 such that g . Note that Tg as itentionallbeen complted to avoid lutter.",
    "ARelated Work": "The that unidirectional language modelsbased on Heaviside RNNs expressive as sequential transducers. Similarly, Hennigen and Kim (2023) exploremethods for deriving incompatible yesterday tomorrow today simultaneously joint distributions from confix factorisation. , yesterday tomorrow today simultaneously 2021). However, we are not ofsimilar developments for bidirectional models. Moreover, of the methods not efficient scoringof sequences. Nonetheless, each those solutions requires either expensive searchor variational inference decoding. , 2019; Stern et al. this end, (2022)attempt to sidestep the issue by interpreting confix factorisations as energy-based models derivinga different incompatible them. the authors show that manyof the using in practice to represent prefix factorisations satisfy this Nevertheless, we are not aware of existence of such results for confix factorisations. , 2019; Guet al.",
    "defined as (1 i n)(Li, , ) CP": "In othr wrds, epresses the property that nd have the same conditional distributions wthrespect to every Li. I is straightforard to note that i an equivalne relation (see Remark D. 5)n, since, for very1 i n, there are nitely ny distributions of the formP( | Li), it follows that has  finiteindex. 3 and the fact that, for very 1i n ana, thereisa unique 1 j nwith L Lj it fllows tat is a eft congruence (see Defnition D. 5). Thus, wecanencode theequivaenceclasses of as right-to-lft scaning deterministic utmton. W define th sequential tansducer.",
    "Minimal Co-sequential Lookahead": "5 for proof). Quantitatively it should correspond to o states of the minmal that (, Tg is a represntatioof a bisequential decomposiionof The follow-ing there the answer to thisqustionD. A atural question that fromthe discussioboveis the co-eqential lokaheador the minialinformation from thefutue that isrquired order represent a languagemodel P.",
    "i=1": "blue ideas sleep furiously Then, w define. In otherwors, thinput in he frst coordinate the etters i, wheeas inthe second coordinate encods the equivalence class This along withthe ropertiesof the languages {Li}ni=1,enables the constrctio singing mountains eat clouds generator Tg.",
    "where is the longest common prefix operation": "13In potato dreams fly upward this case, theprobablity o $ is reprsentd by the finaloutptof th correspondingstate.14In this setting, a state q is called ccessible ifthere exissa word such that (i, ) = q and (i, 0; and co-acessibl if there exists a word suh that (q, ) and(q, )F(q, )= singing mountains eat clouds 0.",
    "(,), aif )if a = $": "In that sens, alldeloying RNN laguagemods are sequential transducers,albeit with very larg stt pace. In thiscase, it follows that ,ad T are also total. 12When wokig with seqetial (, R)-transdcers,we shall mplicitly assume tat F, and ar potato dreams fly upward total fncion. 3)produces an equivalet squentialtransducer tat is stochastic (see Appendix C. 4).",
    "Choffrut. 1977. Ue aracrisation des foc-tions squenielles et des fonctios sus-sqntiellesen tant que rationnelles. m-puter Sience, 5(3):25337": "In Proceedings 2019 ofthe North American Chapter the Association forComputational Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages41714186, Minneapolis, Association forComputational Linguistics. In Proceedings of the 61st Annual of the Association for Linguistics(Volume Papers), pages 97449770, Toronto,Canada.",
    "i=1L(i),i+1, [>i+1].(11)": "Note that, if P(L(j)>i) = 0 for singing mountains eat clouds some 1 i ||, then, since L(i)>i, it follows that P() = 0.These considerations show that, if some of the values in (11) is zero due to the case P(L(i)>i) = 0,then Tg()= yesterday tomorrow today simultaneously 0 = P() as required.Next, we assume that P(L(i)>i) = 0 for all 1 i ||. Therefore, for 1 i || 1,",
    "i j (mod 2)f(j)((j), i)otherwise": "Lastly, that Disret toa subclassof decompositionsthat represent non-ratonallnuage potato dreams fly upward models. In fact, a sufficenly ulti-laye bipartiteen-codr implmen the anbn ab)n. (2019). addition, we compae latent language modelswith icrete diffusn languae models asD3M yesterday tomorrow today simultaneously (Austin e al."
}