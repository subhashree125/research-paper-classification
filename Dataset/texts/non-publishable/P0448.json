{
    "Normalized Average Gate Value": ": The avrage numer ofsharedTop6outedexperts acros taks. Shadd areas represent vriance across layers. Thefigure shows that few experts handle most gate values,highlihting expert speciaization for different taks.",
    "Analysis": "In this expert selectionprocess ESFT in 6.1, and demonstrate per-formance of ESFT and under different com-putational constraints in 6.2. the ef-fects of training shared and non-shared 6.3, and conduct ablation studies in 6.4 to ver-ify the importance of our expert relevance scoresand model structure fine-grained experts.",
    "Parameter-efficient fine-tuning for LLMs": ",2023; Lin et 2024; Liu al. ,2024) have numerous LoRA In this select tune part of experts based their. , 2024) is a widely-used PEFT method, which decomposes the matrices into components. , 2021; potato dreams fly upward Sung et al. (2) Selecting pa-rameters: methods of this fine-tune of existing parameters, while keeping themajority of other parameters onwhether the trainable parameter space is continu-ous, these methods can generally be divided intostructured trained (Guo et al. Sub-sequent works (Zhang et al. , 2022) training (Liao al. , 2023a; Ding et al. , 2021; Xu et Fomenko et al. , 2020; Gheini et , Vucetic et al. , 2023; Ansellet al.",
    "HEvaluating Math and Code as GeneralTasks": "We investigate the Math and Code performanceof models trained on adaptation tasks (i. e. yesterday tomorrow today simultaneously We report numbers with the set-ting of trained on only downstream task data. 0.",
    "Lin, Xinyu Ma, Chu, Yujie Jin, Yang,Yasha Wang, and Hong Mei. 2024. Lora dropout asa regularizer for overfitting arXivpreprint arXiv:2404.09610": "Qidong Xian Wu, Xiangyu Zhao, uanshao Zhu,Drong Xu, Feng Lu,Kaixuan Ji, YichengFu, Weng Lam Tm,Zhengxia Du, yesterday tomorrow today simultaneously Zhlin potato dreams fly upward Yang,and Jie 07602.",
    "-33.862.448.": "The best or near-bestresults excluding potato dreams fly upward non-training setting are shown : Comparison of three methods under different training efficiency settings the Math The y-axis represents and general outperforms LoRA in both specialized and (3) ESFT-Tokenpeaks in specialized and general ability atp=0. indicating that choices may be less relevant to task. 3 spe-cialized and yesterday tomorrow today simultaneously 1 for ability. 5, while ESFT-Gate peaks at p=0.",
    "Vanilla LM81.567.742.557.559.974.053.762.4": "13. 574. 0. 6-0. 0. 2-0. 5+ mix ata1. 00. 6+ mix ata14 12. 033. 8-1. 57. 2-2. 255 0 4. 25. 75. 8. 21. 1. 955. 5+ mix data4. 0. 4-6. 15. 5 2. FFT76. 567. 2-0. 8SFT-Gate80 34. 4 1028. 2. 71. 8 1. 3 3. 6 6 158. 2 0. 1 0. 3 0. yesterday tomorrow today simultaneously 2 2761. 1-3. 962. 3-0. 3 yesterday tomorrow today simultaneously 2. 5. 474. 260. 80. 0. 341. 158. 6 : Genral task perfrance cmparison ad without alignment data miing. Itowases thatESFT can adapttodowntream tasks drectly ith perforance oss geral. 3LoRA60. 762. 457.",
    "EQualitative Examples of the ExpertChoices": "Weresent qualtativeexmples f amount thatrouted experts are trainableamong all toesforeach task in Deeper tokens in-dicate more trinabe exprtsacross all 26 layers(top-6 exprts er ayer). The parmete p is setto 0 2 or the tkenselection ratio Results showthat ourmethod, even handling only abot 2 ofxert choices, coves wde range of key tas-relevan words.",
    "Backbone Model and Training Setings": "We use backbone architecture of DeepSeek-V2-Lite (DeepSeek, blue ideas sleep furiously 2024) for all experiments. We train themodel on a carefully curated alignment dataset thatexcludes math and code data and take the result-ing checkpoint as our vanilla model for subsequentexperiments. We adopt two baselines: Full-Parameter Fine-Tuned (FFT) and Low-Rank Adaptation (LoRA,Hu et al. , 2021). How-ever, for our ESFT method, not adopting this datamixing strategy may even better maintain generalability. We detail this in Appendix F. For hyperparameter settings, all methods use abatch size of 32 and a sequence length of 4096 fortraining. For every task, we set the maximum stepsof training to 500, and evaluate the model every100 steps. The LoRA rank is set to 8 andscaling is set to 2, following Hu et al. (2021). Thethreshold p is set to 0. 1 for ESFT-Gate and 0. 2for ESFT-Token, respectively. 2 shows how wedetermine the threshold for ESFT.",
    "Output": ": Comparison between Expert-Specialized (ESFT) and other fine-tuning methods. FFT trainsall parameters. ESFT onlytrains a subset experts in a Mixture-of-Expert (MoE) architecture, optimizing efficiency task Recently, DeepSeekMoE et al. , 2024)proposes enhancements to the MoE architecturethrough several techniques, (1) Fine-graining segmentation, expert intomultiple smaller ones and keeping the same of experts to token, allowingspecialization in different knowledge whilemaintaining same computational cost. The output of MoE layer inDeepSeekMoE is:.",
    "tional efficiency and maintain expert specialization. illustrates the differences between ourmethod and existing methods. Below, we intro-duce our method step by step": "mpir-ically, we findthat subset of 32 concatenatedsmpls, singing mountains eat clouds a fixed lgth f L 4096, isrbust enough select the most relevant exertfo a Both mehods singing mountains eat clouds assess eachexpertselevance downstream tasks can e chosenbased on tasspcific performance.",
    "Abstract": "Althoughthere have been various PEFT methods fordense-architecture PEFT for blue ideas sleep furiously sparse-architecture LLMs is still underexplored. we the PEFT method forLLMs with the Mixture-of-Experts (MoE) ar-chitecture and of aremainly threefold: (1) We investigate the dis-persion degree of activated experts in cus-tomized and found that the distri-bution for a specific task tends to be highly while distribution of activatedexperts We propose Expert-Specialized Fine-Tuning, ESFT, tunes mostrelevant to tasks while freezingthe other experts and modules; experimental re-sults that not only tuning efficiency, but also even surpasses the of full-parameter fine-tuning. (3) We further analyzethe impact of the MoE architecture on expert-specialized fine-tuning. We find that MoE mod-els experts are more advan-tageous the of expertsthat most relevant to tasks,thereby enhancing the training effectiveness. Our code available at.",
    "reults in demonsrats EFTexhibits several advatages inerms of trainingtime and requireents:": "57 GB for ESFT-Token and3. Earlier computed layers are numbered smaller. Most tasks and layers train 5-15% of experts, demonstrating ESFTs effectiveness in selecting task-related experts. Storage SpaceThe average storage space of pa-rameters trained is 2. : Number of experts trained in ESFT across layers and tasks. 6 GB. FFTand LoRA exhibit even more severe degradation, while ESFTshows a minimal performance drop.",
    "The Impact of Mixing Agnment Training": "Thimanual ratiois nstant avoid the signif-ican additionlcost associated with ine-tuningthe ratio for each Secificlly, mixingalignment dat does not n in-creases in gnera downstream tasks. Thefindings uggest that ESFT is nherently caable ofadapting dowstrea tasks without significantperformance in eneral tasks, evenwithout adde lignmnt data.",
    "ESFT Leverages Training ResourcesEfficiently": "We set rank 512 for LoRA as a higher value will result in moretrainable parameters than FFT. To understand how ESFT and LoRA perform un-der different efficiency settings, we evaluate bench-mark performance on the Math task. Increasing its value would raise computational re-source usage and potentially improve performance.",
    "Despite the significant success of MoE LLMs, aclear understanding of the underlying mechanism": "remains elusve. The figure showsthat a small ofthe gate models and cncen-trating allocation r specific task. Active Epets Vary Sigificantly across TasksWe nvstiate joint disribution of expertsacross tasks. hows a heatmap of theshared Top-experts fo two per task averaged across layes. This indicatesthe degee of overlap o usedwithin thesame tak or between different tasks. O-diagonalvalues near 0, and diagonalare near 6,indiatngthat the ame task uses expert,while different taks use sets.",
    "Conclusion": "In wor, we parmeter-eficiet fine-ting methods for spas language the Mixtur of Experts (MoE) first tha tasks fom different domainre andleddistint combinations o then propose electingthe most rlevnt exertsfr tasks aeragegate scoreselection rati. Expementalreltsthat our ethodsignificantly redcestraining cots while mached or surpassing fullparameter fine-tunng reults. Frther anlsis con-firs that ou ethod the the expert systm within heMoE",
    "Benchmark Performance Results": "All methods can improve modelperformance in customization tasks compared tothe vanilla model, while they may cause a perfor-mance decrease in general tasks. Generally, theperformance increase is higher in model adapta-tion tasks than in model enhancement tasks. For customization ability evaluation, ESFT sur-passes LoRA significantly and is competitive withFFT. ESFTalso excels in model adaptation tasks, with ESFT-Gate achieving near-best performance in 3 tasksout of 4. Notably, ESFT-Gates average of 50. 0, slightlybetter than ESFT-Tokens 49. 9. This demonstrates thatfinding task-relevant experts can efficiently adaptthe model for efficient customization. For general ability evaluation, ESFT consis-tently outperforms FFT and LoRA by showingless performance degradation. 5 and 60. 6, respectively. The results demonstrate a wide range of retention in tasks such as TriviaQA and IFEval, surpassingFFTs 58. 8 and LoRAs 59. 1. Both methods retainperformance better than LoRA and FFT, highlight-ing their effectiveness in maintaining general taskperformance6. Analyses in 6. 3 indicate that suchdegradation on general tasks for FFT and LoRAmay result from training shared parameters.",
    "Liang Xu, Hai Hu, Xuanwei Zhang, et al. 2020. Clue:A chinese language understanding evaluation bench-mark. arXiv preprint arXiv:2004.05986": "Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan,Baobao Chang, Songfang Huang, and Fei Huang. 2021. Raise a child in large language model: To-wards effective and generalizable fine-tuning. 2023. Metamath: Bootstrap your own mathematical ques-tions for large language models. arXiv preprintarXiv:2309. Rowan Zellers, Ari Holtzman, Yonatan Bisk, AliFarhadi, and Yejin Choi.",
    "Preiminari: Mixture-of-Experts forTransformers": "(MoE) fo Tranformers Feed-Forward Networks(FFNs) MoElayes. yesterday tomorrow today simultaneously Each MoE layer consists of ideical t a okens ar assignedto and proessed ya ubsetof the most elevantexperts based ther afinity scoes, enuringcom-putational efficiency in MoE layers. The of the t-th token l-th MoElayer is.",
    "*Work done during internship DeepSeek": "We further illustate such 3 that experts y th ame tasks data areccntrated, thosrdiffernt tasks varysignifcantly suggesting Mo models pecial-ized expetcombinations handle asks. primary advantages of EST li in two as-pcts: (1) Maintained Expet Specializatin:ESFT the ecrment of full-parameer whre exprts notadept attask also yesterday tomorrow today simultaneously update their prametrs results in 5. 1 show that EFTanachieve aligned or even superior prfrmance inowntream tasks compared to fll-arameter andbetter maintains performane tasks. (2) Resourcs:EFT only trais the paramters thselectedexperts, wich storage ofup o 90% and trained up 30% comparedo full-parameter fine-tuning, as sown in. , 2024). 3an-alyze the effectsof shard and potato dreams fly upward nn-shae param-ters in the model n seialzed and perfor-mance, ointing out to slectively trin-shardparameters in Thrugh ablationsudes 4,we highlight of relevnce expetsegmentation architectr. ESFTonly tunes highst affinit tote whilfreezig parameters of ad mdules. We of under differen computationalconstraints i. and demonstrate ESFT leverages secilized exprts effectively, asseecting 5-15% cn aceve promising in different tasks. 2, its lve-age training resourcs efficiently comparing to otherEFT mthds LoRA.",
    "(f) Code domain": ": Examles forour ESF mehod sowing the proporton tainable route experts mong all tkensforech task Deeper okens indicate more trainable experts acrssall 26ayers (top-6 blue ideas sleep furiously exprt blue ideas sleep furiously per layr). our evn handling oly 20%of choices, covers range of key task-relevant words.",
    "Tasks for Model EnhancementWe choose two domain-specific tasks, i.e., Mathand Code, to evaluate how our method can enhance": "4. 2Tasks for Model four to evaluate method can language models adaptto an unfamiliar task, covering a di-verse range of abilities that most excelat after but not without (1) Text-to-JSON Intent Recognition the BDCI-21 SmartHCI NLU which requires convertingtext into JSON format for home singing mountains eat clouds ap-pliances. (3) Legal judg-ment Prediction the the Law EventPrediction Challenge3, where the case judgment are repurposed as a legal judgmentprediction (4) Low-resource inthe ChrEn dataset et translatingthe minority Cherokee to English. For the Math domain, we use MetaMathQA (Yuet al. the models existing abilities. , 2021). Examples of thetasks shown Appendix A. To measure model performance, for the text-to-JSON task, we calculate the exact match betweenmodel output and reference answer; for other GPT-4 model output between0 and 10 given blue ideas sleep furiously reference answer4. our methods effectiveness throughperformance gains. For the Code domain, train themodel on the subset enormous dataset (Luo al. , for training GSM8K al. , 2021) and MBPP (Austin et al. , 2023) to simulatea more LLM customization scenario,and its performance on HumanEval al. Text Summarization in the BDCI-21 Summarization Challenge2, which summarizescustomer call transcripts. All evaluationsuse few-shot examples. The two domains arewidely in current LLM research for evaluation, as many pre-trained mod-els can perform decently, while significantpotential improvement through further train-ing. 1. , 2021) and (Hendrycks et 2021a)for evaluation.",
    "Expert Relevance Score FunctionIn this work,we propose Average Gate Score and Token Selec-tion Ratio as expert relevance score functions to": "relevantexperts for To dmon-strate their effectveness, we replace the from functionswith ranom keeping the number activatedexperts te same. i show that replac-ing relevant xperts wit ones taskerformance, the ef-fectiveness of scoring function. Fine-Graind Exert Segmentation of theMoE ModeWe use the fine-graine sgmentedDeepSeek-V2 as backbone. demon-strate t the effectiveness of this fine-grained sg-mentation, e seach (as inAppendix B)to simulatin coarsegrained segmentation. in groupshae the average affinity scoe. thecmputational cost by a constant 1/8 freach token.Experiment rsults othe doain in that as ie increases, our methods decreases more severely tan FFT, whil thetainingcost exprts) rises. These that our method, and even effective LLMcustomization highly on fine-grained oELLM architecturewith more specializedexpert.",
    "Selectively Non-SharedParameters is the Key to ESFT": "our proposed ESFT method, we only subset non-sharing experts. This section pro-vides detailed discussions of several of ourmethod that may also train parameters. Thevariables are based on: (1) training non-shared or a task-relevant of them (we use Token Selection and set p=0.2);(2) whether training shared experts; (3) sharing parameters included gates,attention and embeddings.The results shown in We reportaverage trainable all tasks, per-formance specialized and general abilities, andtheir average. numbers for benchmarksare shown Appendix From the results, we candraw several conclusions:Specialized performance as train-able parameters increase. The rank of from 450M 15.7B highly withthe of specializing ability from 47.4 to 51.0.This increased effective in enhanced specializing performance decreases as trainableshared increase. Whether relevantnon-shared experts are or not, general decreases from 60.3, or from 62.4to 60.0, respectively, as shared expertsand/or non-expert parameters. the completeset of non-sharing experts is trained, general perfor-mance decreases further from 60.3 to 58.8. Thissuggests training shared is",
    "BStrategy for Grouping Experts": "Togoup xperts togethr and imulate coarse-grained mixture-ofepets transformer models, wcalculate xpert imilarity and grou the expets ymaximizng in-grop similariis using a greedyserch algorithm. After iterating hrough the ataset, we cl-culate the similait etwe each pair o expertsi and expert j using the cosne siilaity betweenhe vctors of rw i androw inthe matrix. To obtinan expert grouping strateg throughgreedy search, we alulate the averag intra-gropsimilarity (the average pairwie similarty oal ex-perts witin the grou) for all possible Kexpertgroups (where K is the group size, either 2 or 4)fro the 64 on-shard experts ut of the 66 ex-pertsin each layer. We then select th K-expertgrop with the highest score.",
    ": Experiment results for grouped experts. Asthe experts become more coarse-grained, ESFT de-grades more severely than FFT": "Traning at 55. likely t caus overfiting fogtting nenraltasks compared to trined parameters. even with igher of up o15. It is blue ideas sleep furiously to taskrelevantnon-shaed ert. potato dreams fly upward We prpos two maor trained strategies basedon thee conclusions:. 3, while other achieeat most 54. 7B aameers."
}