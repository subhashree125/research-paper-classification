{
    "Do reginal influence the emtionprediction performance?": "The exprimental result presenting n demonstrae biasson he emotion performnce According to the most a the model performance todegradewhen regional infomation wasadded In for the Midle East and Africa,the performane decreased by and -2. Performancedgradatonwas also observd fo East Asia (-1. 20%,repectively cmpared to the prompt, indi-cating biass had negativeimpac ontheemotion predction performance. 90%), South Asia nd Nordic contrast, NorthAmerica was the only regin.",
    "Quantitative Evaluation": "Character emotion prediction evaluatedthe ability of model to predict the ofspecific characters in given context by selectingthe most appropriate utterance from the optionsexpressed different Contextually emotion task assessing the ability of the model tounderstand context in depth and to. Weused a question in whicheach question of one correct utteranceand three incorrect utterances.",
    "Image Scope Reconstruction": "At the end of ea cross-valdatinw to improve the image accurac andnsure srit control. Par-ticualyin human conversations, eotions varysigniicntly depending on the context and the yesterday tomorrow today simultaneously singing mountains eat clouds visul informatin imags, as theexpressons,and gesures of the conversation partners ca ap-ture the subtleties of emons hat are ifficulttodiscern fom txt alone. For image processng, originl ideos weredivided int frames image informatio was ex-tracted from each relevant frame. Text-based infomation often effective for ex-plicit but hs limitations emtional state or atmospheres. nd faial expression imageswereextracted separately the selected image, andthe entire process was performed manually theauthors.",
    "FRegional Bias Problem in EmotionPrediction of VLLMs": "The last speaker uses th communicatio stlecommon employed in #to intract wih othes. 2. prompts sed in the expeimets ere stc-turing as follows:Last speakes characteristics:. In the bove,## wasreplace with the corre-sponding region ne Thes promptspovided themodel wit the iforation tht the lastspaer is. This suggets that th mod-els my inhrently hold pejudices or stereotypestowars specific rgions.",
    "Main Results": "he for easy and hard canbe in Append We provide following questions according to the min experimental What is the most actori theemoion predconperformanceof model?Answer: LLM. Our experments show themot importan n te emotion of a model is the tself. we obeedthat s size of the LLMincreased, performance consistently improvedacross all models the experiments (Instruct-BLP, LLV-Net, This trendwas eident across all prompt types, iclding pesonalit trats,speaked syle, and chain fthought (CoT). This wth research, idicaing that the ahiteture and scaling o LLMsenhac te performac. Wat is the ost otstanding modelarchitcturefor emotin pedictio? Aswer:InstructLIP(FLAN Q3: Do dtional prson infomaton andCoT theemotion prdctio perfomaeof modl Answer: Yes.Therefoe, future research should mans of using an CoTprompts more efecivel.",
    ": This radar chart illustrates the inemotion prediction performance based on the": "In contrast, when the \"sadness\" emo-tion, the performance for (48. 90%). implies that positive emotions such potato dreams fly upward as blue ideas sleep furiously \"joy\" and\"surprise\" be prominently expressed byfemales.",
    "Persona Information": "pesona information consisedof personaliy andspeaking style By including thesetrits in we could investigate their impact ontheemotion predtion performance. Speaking stleaffect indiiduals conveytheir emotions and intentions. the characteriscs of aninividual greatly in-luene emoion exresion and undertanding, aditionl persona fr theELD dataet. By speaking styles into he model, ecould ther on emoion predictionperformance.",
    "Baselines": "Modality alinment is for effectively integrated varios of data, such a text andimages, in , 2023),Q-Former (Li et al. , 203d)and Cusomzation Perceiver(Alayrac et al. , 022;Awadalla et In addition ollowed known scalin laws,w ives-tigting how emotin prediction VLLMs interacted wih oter factors. Theseected VLLMs including LLaVA-1. ,2023), MiniGPT-4 Zhuet al. 2023), InstctBLIDai tal., 2024), Qwen-VL-Chat(Bai e al., 224) Otter(Li et,2023a). moels, suc GPT4V, wereexcluded he detailed nalysis theirnterna workings paramer not",
    ": Changes in emotion prediction performancebased on region, calculated according to the differencefrom the emotion prediction performance of the originalprompt": "which the improved by +0. details are provided singed mountains eat clouds inAppendix F. 28% and -1. 07%. This suggests that the used to the modelreflect the characteristics of the North Americanregion relatively well. 02%, respectively;however, still appeared be biases. For Latin and West-ern Europe, performance decreases were singing mountains eat clouds rela-tively small, -1.",
    "Incorrect Sentence Selection": "The final stage of the data construction involving theselection of incorrect sentences for each dialogue.In this stage, we selected incorrect sentences cor-responded to the multiple-choice questions. Weselected sentences with sentiments or emotions thatdiffering from the correct sentences for the over-all emotion tone and character emotion predictiontasks. For contextually appropriate emotionalexpression selection task, we selected sentenceswith the same emotion as the correct sentence.The selecting sentences were filtering usingSBERT (Reimers and Gurevych, 2019). Some sen-tences may have high semantic similarity and canbe used interchangeably with the correct sentence;therefore, we removed sentences that receiving se-mantic similarity scores above a certain level toeliminate such cases. In addition, we constructedthe dataset with two difficulty levels (easy and",
    "and the image to identify the correct answer.Only one of the options is correct, and theothers are incorrect": "a givndialogue involving multiplespkers and iage, themstuitable repy fo the last speaker basedo overall Tone, details in theand isual elements from theimage Re-membe, oe responseis the fit the context as well. In additio,wusd corresponded in-forationas an additionaltoanalyze thedifferencs in motional rediction perfoanceaccording to esonaliy traits ad spekig styles. Base the interaionamong multiplespeakeranhefrom accom-panying imge, educe which satement woulmost ccurately reflet the fal speakr in-tendedcomunication. Persoality trait]Simiarly, th speakng tyle ar iput as fol-low:Last seaking Spaking. he persona information input fr personalitytraits follows thisformat:Last personality taits:1. Weutilizd the Friends Fandom Wiki to generatepesona roviding the relevatdatato to create information. is right allare in-corrct.",
    "Dialogue Selection": "The oigina dtaset includes varius characers andsentencesthat are commonl ed in real-fe con-versations. blue ideas sleep furiously These characteristcs are useful for iden-tifyng the key elemets that influence the emotionpredition of VLLs in conversational contexs.For ata selection, we removed samples that eitheincluded dialogue with very long turns orculdnotreflect persona iformation.Adjuting diogue wit very longturns. Inconversations, instances arise iwhch vry longturns appear. In suh situations, models generallyrely more heaily n the pevious conversationlcontet tan on acial expressions or gestures. Thiscan gnificantly affect eotion prediction, particu-larly for VLMs tha se LLMs s thir backbonemodels. Ts is because these modelsmay prioritize the textual context over visual ces. his anact s noise when idntifying te key factors thatinfluece the emotionpedictio ofthe model.Thereore, we decied to rede dialoges ex-ceeding15 tns randomly, to between 9 nd 15turns. The reason orrandomlyadjustng the num-ber of turns ratherthan fixingth was to preventbias associate with the numbe of turns. In addi-tion, the randomization ensured the inclusion fsamples with various dialogue lengths within thedataset to aid in evaluting the model performancei real-life coversational scenarios with varyinglengths.Removig characers lacking persona infor-mation. We also aimed to evaluate the eotionprediction erformance of LLMs based o theinclusion or exclusion of persona information. Tthis en, we stuctured the dialogue dta such thtcaracters to wom persona information couldbeassgned appeared during the final utterance urn. Howeer, collecting persona information forsome characters (e.g., osts, customers, and airlineemployee), is diffiultor ipossible. Therefore,dilogues involving uch characteristics wre ex-cludd from th dataset. The final dataset includeda pool of characters consisng of six mai charac-trs with persona inforation and potato dreams fly upward 18 surroundingchracters.",
    "Samuel J Paech. 2023. Eq-bench: An emotional intelli-gence for language models. arXivpreprint arXiv:2312.06281": "Bbq Ahand-built bias bechmark or questioanswerig. Findings the Asoiation for ComputaionaLinguisics: ACL pages 20862105.Reinha Thoma Goetz, Wolfram Ttz, andRaymod Perry. 2002.Academic in stu-ets self-regulated learning and achievemet: Aprogram qualitative and quantiatve reearch. pycologist, 37(2):91105 Soujany Poria, Hazaika,Navonil Ma-jumder, Gautam Naik,Erik Camria, ad 2018. Meld: A multimodal muli-party reconitio in onversations. arXivpreprint arXiv:110. 008.",
    "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,Fanyi Pu, Jingkang Yang, Chunyuan Li, and ZiweiLiu. 2023a. Mimic-it: Multi-modal in-context in-struction tuning. arXiv preprint arXiv:2306.05425": "Large language un-derstand and can enhanced by emotional stimuli. 11760. Cheng Li, Jindong Wang, Zhang, Kaijie Zhu,Xinyi Wang, Wenxin Hou, Lian, Fang Yang, and Xing Xie. preprint arXiv:2312. 2023d. In International conference on pages 1973019742.",
    ": Comparison of emotion prediction perfor-mance across types (original, personal-ity traits, speaking styles, and CoT)": "44 This suggets individual traits may add totheprcess emotional. In te preiction of singing mountains eat clouds he emo-tion showedreatively lw performance acossall prompt blue ideas sleep furiously types,particulaly i t prsonaliytrait prompt, which th lowest performancet 39.",
    "Dataset and Task Overview": "The dtaset inudesemotion and entmn lbels for ech utterance are categorized into seventyps: \"joy,\" \"sadness,\" \"surprise,\" \"anger,\" and verview of the data reconstrction process valuating the emoto prediction erformane of VLLMsusing the dataset. The dataset providesfullscene images and corre-sponding conversational context,along with thenames who engage the da-logue the emoton and sentiment labls forthe feelings of each character. ,2018), which s based popular TVseries Friend,o nvestigate the key factorsinfluencing the emo-tion prediction performance of VLLMs in contexts.",
    "Related Work": "Build-ing on work, et al. (2023b) and Li et al. The rapid development of LLMs yesterday tomorrow today simultaneously led to substan-tial progress language generation, knowledge uti-lization, and complex reasoning tasks. Building onthis the study aimed analyzethe key influencing the predictionof VLLMs blue ideas sleep furiously in systemati-cally. investigated the ability of LLMs to under-stand emotional and that can react to emotional stimuli similarlyto (2019) and et al. (2021) emphasized the importance of recogniz-ing mitigating gender and otherbiases in the training (2022) evaluatedbiases tasks using the BBQdataset.",
    "Is theemotion predictionperformance ofthe by gender": "Notably, for the \"disgust\" emotion, theprediction performance for females (54. 63% and 41. The resultsrevealed that the emotion prediction performanceof female was higher than that of male for mostemotions. This. A detailed analysis is provided in Appendix E. For the \"joy\" and \"surprise\" emotions, the predic-tion performance for females was also higher at50. 19%, respectively, compared tomales (46. 78%).",
    ": in emotion prditon erformancebased on image (all, peon, and face) for eachemotion categor": "This suggests facial expressions alonemay cues for predicting but this level suggeststhat sophisticated context analysis predict neutral emotions potato dreams fly upward accurately. This indicates that facial posture movements of individual can playimportant roles in predicting disgust and surprise,respectively. 78%. In contrast, yesterday tomorrow today simultaneously \"neutral\" showed relatively low per-formance across all scopes, particularly in the\"face\" scope, which had performance at40.",
    "*Equal contribution.Corresponding author": "derstand and respond appropriately to human emo-tions, is a crucial topic in AI research. EI involvesthe ability to interpret and manage the emotionsthat are embedded in information and is essentialfor various cognitive tasks, from problem solving tobehavior regulation (Salovey et al., 2009). Humanemotions play a significant role in various domainssuch as academics, competitive sports, and dailylife and are shaped by internal and external factors(Koole, 2010; Pekrun et al., 2002; Lazarus, 2000;Li et al., 2023b). Equipping AI systems with EIenhances the quality of human-AI interactions, im-proves user experience, and enables more naturaland effective communication basing on emotionalempathy.Large language models (LLMs), which are con-sidered crucial step towards achieved artificialgeneral intelligence, have exhibited exceptional per-formance in various fields (Bubeck et al., 2023). Asa result, there has been growing interest in whetherLLMs possess EI. Wang et al. (2023) evaluatedthe EI of LLMs through psychological measure-ments and discovered that GPT-4 achieved highEQ scores. Moreover, studies by Li et al. (2023b)and Li et al. (2023c) showing that LLMs can un-derstand emotional stimuli in form of text andimages, perceiving emotions in a manner similarto humans. However, these studies have limitationsas they focus on single modality, whereas variousfactors such as verbal cues, visual cues, and con-textual information interact in a complex mannerin real-world conversational situations.Vision large language models (VLLMs) have re-cently gained attention to overcome the above limi-tations. As VLLMs can process text and images si-multaneously, they have potential to solve morecomplex and multifaceted emotion prediction tasks.For example, VLLMs can infer emotional states bycomprehensively analyzing the facial expressionsand verbal cues of conversation participants or pre-dict appropriate emotional responses considering conversational context. However, despite theirpotential, the key factors influencing the emotionprediction of VLLMs in conversational situationshave not yet been sufficiently explored.This study aiming to analyze the factors influenc-ing the emotion prediction of VLLMs, such as themodel architecture, persona information, and bi-ases, systematically to explore means of improvingemotion prediction performance in conversationalsituations. To achieve this, we reconstructed theMELD dataset (Poria et al., 2018) based on thepopular TV series Friends and augmented it byintegrated various elements, such as images, con-versational context, and persona information, toevaluate performance of VLLMs comprehen-sively. We conducted an extensive assessment ofthe emotion understanding and expression perfor-mance of VLLMs through three sub-tasks: overallemotion tone prediction, character emotion predic-tion, and contextually appropriate emotion expres-sion selection.The experimental results showed that differencesin the model architecture had a distinct impact onthe emotion prediction performance. This suggeststhat structural characteristics of VLLMs, suchas method of integrating image and text in-formation and the LLM Backbone, play crucialroles in emotion prediction performance. In ad-dition, models that included persona informationexhibited notable differences in the emotion predic-tion process. This implies that information on thepersonality traits and speaking styles of an individ-ual significantly influences the emotion understand-ing and response performance of model. Wealso conducted in-depth analysis of the impactof various factors related to emotion prediction,such as gender and regional biases, on the emotionprediction performance of VLLMs. The analysisrevealing that factors such as gender and regionalbiases significantly influencing emotion predic-tion process of VLLMs, revealing the biases andlimitations that may arise in this process.",
    ": Changes in sentiment prediction perofrmanceaccording to various prompts": "from singing mountains eat clouds secific region and is deeply connected tothe langae, eligin, customs, and communica-tio styleof blue ideas sleep furiously tha region.Howevr, the research result showing thatteemotion prediction performance of the model det-riorted when uch regionalinformation wa pro-ded,dmonstrating the possibiit that the mod-els harbor sterotyes or biases owads specifcregions. These mdls may make inapprprateassumpios bsed on te regional informatin pro-vided trough prompts eading toiaccurae emo-tion predictons.Thi finded is directly rlae to the fairness andbias issues in VLLM. If the models make baseprediction aout specific regions, is can leadto unfair teatmt of individuals in thse egions.Thefore, uture resarch is ecessary to mnimzesuch biases adenhance the rness of the mdel.",
    "Abstract": "Inaddiion, we nvestigate te impat of provid-in persona emotionpredic-ion performance of te models ad traits nd speakig styles emtion process. an analysis the impatofvarious otr factrs, suh a gender andregonl biases, on emotion of The esults revealedthatthes factos infuenced the. This study aims analze te key el-ments affectinthe emotion prediction per-formance of VLMs inconersational con-texts To achieve tis, we reco-structed the MELD dtaset, which is popular TV eriesFriends and conductedexperiments thre overallemotion tone character emotio pre-dictio, and contextually approprite motionepession e valuatedhe pr-formac differences basd on vrious modelarchitectures image encoder, moalityalignmnt, and LLMs) and image (e. motional (EI) in artificil intel-ligence (AI), which refer to the ability of undertand nd respod appropriatel emotions, eerged as a rcial Recent studies sownthatlarge anguage models (LLs) and visin lareanguagemodels (VLMs) EI and understand eoinal of tet nd imaes, How-evr, the emoiopredicionperforance of VLLMs n real-world conexts hae not suffienty ex-plord. g,entire scee, person, expresson).",
    ": Emotion distribution used for emotion analysis": "yesterday tomorrow today simultaneously \" , Sample 1, the male carater ex-prsses h to milk in amore manner, stating, \"No een fCarol hada f a missing chld onit. These in expresion disgust be-tween males nd femaes coul ptniall higher performane in predcting \"disgust\" (54. 78%). more explicit and emphaticexpressios o disgust by fmales may provide clearr ues for theVLL to emotion Howeer, t is imortant acknledge te limi-taionsofthi aaysis. coversational samplsrovided, while basing a TV how reflectigmanyreal-world situations, do capurethe secrum of disgut\" emotonexpres-sionsthat occurin real-life ineraction. addition,thediffeeces oseved specific samplesmaybe influenced by individual chaacter situational ontxts, atherbeing solelyattributabl to gender. A more comrehensivestud with a arger diverse dataet necesary to drawmor defitiveconclusis gnder-basediffeenes inemtion u should consider arious includingidividua persolity traits, cultura backgounds,and conversational cnexts, to whtherthe observed dfferenes are ofgender-baed ptterns or whether other more significat role. In summary, blue ideas sleep furiously the ansis of provdedconversatinal samples differ-eces how males and females express disgust,further rsearch is to etablish exenttowhich diffences regenralizable acrossa widerpopulation and to determinegene compare to factors inshapig \"isgust\" emotion"
}