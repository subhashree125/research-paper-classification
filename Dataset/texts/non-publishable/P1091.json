{
    "Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I Webb, Rob J Hyndman,and Pablo Montero-Manso. 2021. Monash time series forecasting archive. arXivpreprint arXiv:2105.06643 (2021)": "Jean-Bastien Grill, Florian Altch, Corentin Tallec, PierreRichemond, potato dreams fly upward Buchatskaya, Doersch, Bernardo Avila Pires, Mohammad Gheshlaghi et al. Xinlei Chen, Xie, Yanghao Li, Piotr Dollr, and Girshick. Large Language Models Are Zero-Shot Series Forecasters. Advances in processingsystems 33 2127121284. 2022. In Proceedings of theIEEE/CVF conference computer vision. Masking autoencoders are learners. Gruver, Marc Anton Finzi, Shikai Qiu, Andrew Gordon Wilson. Bootstrap own latent-a newapproach to self-supervised learning. 2020. In Thirty-seventhConference on Neural Information Processed Systems. 2023.",
    "information, leading to unsatisfactory performance in forecastingtasks where fine-grained temporal features are essential": "Subsequently, SEP adPachTT expand on to vel wher is moe effectively computational costs resignificantlyredued. Furhermore, ecen achievessuperior fine-tuning perfomance by nove maskedmodeling involving reconstruction of th original timeseries fom multpl randomly sries.Besides, TimeMAsgnificantly surpasses previous baselines in tasks decoupling masked autoencodersrobust representatins throgh two pretext maskedcodewordclassificatin and maked rersentation regession. ForecastPFN intrucs aprior-data fitted networktraine sythetic data and achievsaccurate zero-shot on univarite series. Besides,both TimePT-1 ad expore the potential oftainig a foundation model forecasting yeling zero-shot apability under relativelyshort horizon lengths. Moreoverthere i another relaedline focued on adaptingpretaining generative language models to time seriesdomai, either through promting orfine-tunig. Theseapproacheshave competitive when comparedto traditional suervised approaches. Firstly, these trained on singleataset with ptterns, impeding their ability togeneralizeto diverse forecasting Regaring ur propose GPHT ittands out as a generative pretrained dis-tingishing itself from ethods by effectively addressingthese issues. In we singing mountains eat clouds constructing amxed dataset for training our model in an auto-rgressive Thi approach ensurs he generlizability of GPHT,al-lowing it blue ideas sleep furiously to be seamlessly adapted to anydataset, including unseendatasets, and forcast rbtrary orizon lengths. Notably, eperi-ments demonsrate that ethod surpasses baseline variou ine-tuning and learnig settings inthe long-trm forecasting ask.",
    "+1 = ().(4)": "Taking advantage of auto-regressive trainin eachtoken in can be as predicted vaue ofnext to the token . the regresin singing mountains eat clouds of the firsttoken, we adopt for simplicity. Trefore,refines the nput for the next stage, elminatingthredundant informaton in the potato dreams fly upward series.",
    "ABSTRACT": "existingapproaches still exhibit two Firstly, these meth-ods often single dataset for limiting modelsgeneralizability due the restricted scale of training data. Sec-ondly, the one-step generation schema is followed, a forecasted head and overlooks the dependencies in output series, and also leads to costs under different horizon length To address these issues, propose a novel generative pretrainedhierarchical forecasting, GPHT. are two aspects designs GPHT. On the one hand,we advocate for constructing mixed dataset under assumption for pretraining our model, comprisingvarious datasets diverse data scenarios. This approach expands training data, allowed our model touncover commonalities in time series data and improvedtransfer to specific datasets. Importantly, customizedforecasting required, enabling single model forecast at ar-bitrary settings. We conduct experiments on eightdatasets with mainstream self-supervised models and.",
    "Inference": "Thissets approach apart from mainstreampreraining methods ,where fine-tuned is typicall requred. On the other hand w argue that of GPH analso be enhanced through fine-tuning. In strika baance between maintaining ad improving per-formancen a speificdataset, w adopt a parameter-efficient tun-ing straegy. foreastng head parameter ccount fr less than 0. forected process is akin to decoding process potato dreams fly upward of lan-guage model. inpu X, our mode initially predictthe first tokn. Note th a ax input length exists singed mountains eat clouds our dueto positional mbeddings and heavy computation cost whnaddresing long squences. Consquently, olymost recent tokens are fedino modelorecastng.",
    "Series Tokenization": "The strategy not oly helps mitigate of sparse information also enhances the o bete the lcal semantis, utimately contributingto more robust and accurate series forecasting. To addresschalenges, wemploy th sres tkenizationtechnique,a proven effetive in time series modeling. This significantly deep-learning-based forecasters. The of is even ore pronounced i contextof on amixed series sem from distint tempo-ral distribution To mitigate this issue, layer , designed to address the distribution shiftproble by normalizing uing te. Furthermore, recnt ha a issuein timseries ata, chaacterized by distribtion which mansthat mean variance time serie time. Given tha the majority of time series orginae signals cap-tured byreal-world the datacarry noise, andiformation is ofe distrbutd across time pointsConsquenty, auto-regressive on point-wisetime seriesyield suboptmal performanc due t th risk foverfitting outliers and the associated eror accumulaton , inaddition to incurrin high costs. adopt a non-overlapping tokeniztiostrategy, reshaping X into a sequence timeseriestokens, R , wher = , represents the tokenlength, and can considredas the sequnce lngth.",
    "ETTh1/ETTh27 Hour7420EergyETm1/ETT2715 Minutes69680EergyElctricity321 Hour26304EnergyExchange81 Day7588FinanceTraffic8621 Miutes5266Weather": "We select various state-of-the-art mod-els potato dreams fly upward as the baseline potato dreams fly upward models in the experiments, encompassing bothself-supervised and supervised approaches. Additionally, TimeMAE leverages decou-pled masked autoencoders to learn robust representations throughmasked codeword classification and masked representation regres-sion. On the other hand, we include superior supervised mod-els to better demonstrate the effectiveness of GPHT, includingTransformer-based models such as PatchTST and iTrans-former , a linear model-based approach DLinear , and.",
    "G Peter Zhang. 2003. Time series forecasting using a hybrid ARIMA and neuralnetwork model. Neurocomputing 50 (2003), 159175": "202. Intenational Conference on Machne Learning (ICML2022) (Bltimore, Mryland). Yunhao Zhang yesterday tomorrow today simultaneously Junci Yan. 2022. arXiv preprint (2023). Nature619, (2023), 52653. Tian Zhou, Ziqig Ma, Qingsong Wen,Liang and Rong Jin. 2023. In Th ElevethInternatioal Conference on Haoyi Zhou, Shanghang Peng ShuaiJiaxin Li, XionandWancai Zhang. Self-SupervisedLearning forTi Series Analsis: Taxnomy, Pogres, ndProspects. 222. kilful treme precip-itatio with NowcastNet. In roceedngs ofthe Confeenceon ArticialIntelligence, Vol. FDforme: Freuency enhanced decomposed trnsformer for lon-ter In Prc. Yuchen Zhag, Mingsheng Long, Kaiyun Chen, Lanxiang RonghuaJinMichael I Jordan, and Janin Wang. 023. Iformer: efficient for long blue ideas sleep furiously sequence tie-seriesforecasting. 1110611115. Kexin Zhang, Qingsng Chaoli Zhang, ongyo Cai, Mg Jin, Yong Liu,James Zhang, Ling, Guansong Pg, Dongjn Song,et al. Crossforr:Tansorme utilized cross-dimension dependency time forecasting.",
    "ACOMPLEITY COMPARISON": "In summary, the proposd GPHT model is mediu-sizedcompardto the baselin mols. To better illustrate the proposd GPHT models computation cost,we provide quantitative resultson theElectriity atase underlookback window = 336 and forecastin horizon= 20 in.",
    "Iterative Residual Learning": "Tofully verag these representatons, we propose anovel iterativeresidual learningstrategy , transorming theforecastingprocss into an irative approah. Secifially, a illusraed blue ideas sleep furiously input of sage isthe f stage input and defined as:.",
    "Zero-shot Evaluation": "It important note ForecastPFN is specificallydesigned for zero-shot forecasting, but its performance heavilycontingent on the training data is synthesized, and as such,. The zero-shot forecasting task is conceptually chal-lenged for model cross-variable dependencies, hence,only channel-independent models are considered as baselinemodels.",
    "Problem Definition": "Our ensures that GPHT is pretrained singing mountains eat clouds on arich of singing mountains eat clouds temporal patterns, thereby its adaptabilityand generalization capabilities across series. In practice, we concatenate the training segmentsfrom various real-world datasets to constitute the trained set themixing dataset.",
    "Abltion Study": "5. 1Hierarchical Architecture. In this section, we explore the in-fluence of hierarchical transformer blocks GPHTs performance. We present the MSE and MAE evaluations GPHTwith varying stages of hierarchical transformer blocks across allbenchmark datasets, considering a forecasting horizon = 720(see ). As the number of increases, GPHT better equipped to capture temporal the mixed as different affirm hypothesis, as the 4-stage surpasses the1-stage GPHT (without structures), achieving a notable2. 2On the of Pretraining. Does it pose positive effects provide quantified results in. Specifically, wecompare performance of fine-tuned GPHT with one trainedfrom The MAE evaluations averaged the horizon length(i. , = 96, 336, 720) From results, we caninfer that pretraining on the dataset enables the model toleverage commonalities among series, facilitating better trans-fer specific datasets. reachingas high as 9. 65% the ETTm2 dataset.",
    "BQUALITATIVE EVALUATION": "t ca bereferring that nital predoinanlyfocuson general periodic patterns from the iputseies latter stages can herefore pay more attention to thesecilized rends, the uto-rgrssive forecastng resuls oftag algn moe closely with the inpu series, with than the preeded stages. hesults verifyoursmption that the hierarcic canbetter commonalities specialtie th mxed pertaning th iterative residul schema efetiey refines inpu forth stage,elminated rundant infomatio in the series.Besies, we proide qualitativ comparison GPHT anmainseam upervised forecased variousdatasets",
    "George EP Box and Gwilym M Jenkins. 1968. Some recent advances in forecastingand control. Journal of the Royal Statistical Society. Series C (Applied Statistics) 17,2 (1968), 91109": "Tom Brown, enjamn Mann, NickRyde, Melane Jared D Dhariwal, Arvind Neelakantan, Prana Shya, Grish Sastry,et al. Language mdls are ew-shot learners. Advances in neuralinformation systems 33 (2020), 1877191. Cristian iG. Olivares, oris N. 02. N-HTS: Neural Hierr-chical for Series Forecsting.Weiqi Chen, Wenwi Bingqed Peng, ingsong Wen, Tian Zhou,n LiangSun. 2022. Learning to Quaternion transformer for complicated periodicaltime series foreastng. Prceedings the 28th SIGKD Conference Discovery and 16156. ForerTie: Hierarchical Multi-Scale Representations for Multivaiate TieSeries Classifition. 14371445 MigyueCheng, i Liu, Liu, Ho Zhang, Rujiao Zhang, and 2023. TimeMAE: Self-Spervsed Representations Time Series withDecoupled Masked Autoencoders. Minyue Cheng, Xioy Qi Liu, HaoZhang, Yihng and ChenyiLe. 2024 Learning Transferable Time Series Classifier from Lnguage Moel. preprint Mngyue Cheng, Jiqin Yan,Tingyue Pan, Qi Liu, and Zhi Li. Cnvtienet:A hierarhicalfully covolutiona formutivriae time series nalysis 01493 (2024).",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "A Framework MaskedTime-Series 00861 (2023). Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Siddartha and Colin White. In Thirty-seventh Conference on Neural Information Processing Sys-tems. 2021. Time-series representation learned viatemporal and contextual contrasting. preprint arXiv:2106. potato dreams fly upward 14112 (2021).",
    "Generative Pretrained Hierarchical Transformer forTime Series ForecastingKDD 24, August 2529, 2024, Barcelona, Spain": "dvanced petrained techniqus.Secodly, he tech-ique a rucial fctor in precise forcast-in as evidece by the outstanding perforace FPTn patchTTthebaseline tune GPHmoelthe mst acurate recatingunder over its owerul countrprts. SpecificalyGPHT exhibits n reductin of 9. 23% on the Ex-change dataset,1. 00%theETTh ataset, in comparison bst bseline under ex-perimntal legths. egardingMAE evalation heiprvemets are more ronunced at . 3%, 97%, and5. 07%resectively. his further deonstrtestat ourmodel can bette capture temporal dependencies various datascenarios, from preraining on mixing datast. 55. atribute this toexplicit modelig o dependencies in thoutput seies. 38% when = 70. When ompar to singe modl, potato dreams fly upward t FTand suervised 44 0 experimetal settigs,respecively. Thisvalidates the potato dreams fly upward learned thecom-monlities of different time series by training onthe med datasetwith he asumtion.It also underscores theincredible generalizbiiy o our proposd moel, owingtote mulistage herarchical block and theiterative residual learning",
    "0.3770.414 0.403 0.414 0.3920.405": "Clearly, GPHT consistently outperforms models across var-ious settings, showcasing pronounced relative improvements. exhibits trans-ferability to unseen time series.",
    "Experiental Setup": "providsdescriptions of theovring various scenarios andscale. 1 1Datasets. Among hem,both ETT2 and Electricity3 manly th consumpion onelectricity, an ET can be ivie4 subsets accodinto th th sandard potocol, slit eachdataset ito training, validaton and testing sets accordig thechonoogical rder. Th spi ratio6:2:2 the ETTdataset and7:12 the other datasts.",
    "a convolution-based model TimesNet , which is also a multi-scale model. The performance of these methods effectively repre-sents the utmost accuracy achievable by current forecasting models": "4.1.3Implementation Details. We employ as all the experiments and use mean squared er-ror (MSE) mean absolute error (MAE) as evaluation metrics.A lower MSE/MAE indicates better performance. For the baselinemodels, implement them official codes and recommendedparameter settings.As for GPHT, we set the token length 48, length set to 7 to accommodate the lookback windowlength. model 4 stages of each with three-layer decoder-only transformers. The down-sampling for each stage is to respectively.",
    "CONCLUSION": "This research was by grants from Joint the Science Technology Innovation Community inYangtze River Delta (No. 2023CSJZN0200), and the Funds for the Central Universities. this a pretrained hierarchicaltransformer model, namely GPHT, for time series forecasting. Technically, we proposeda yet effective paradigm that treats time series originatingfrom various scopes as a whole, heterogeneity values of each from different datasetsto form dataset for sufficient experiments 8 widely used datasets com-parison with mainstream self-supervised pretraining andsupervised the results demonstrated that GPHT surpassesthe models across various fine-tuning and zero/few-shotlearning in the traditional long-term forecasting task."
}