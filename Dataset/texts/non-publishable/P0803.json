{
    ": Graphical illustration early stopping": "We use this method because it fits ourpurpose to identify responses failedjailbreak attacks. Note that this not be the only method pre-vent responses, it serves our analyti-cal to identify the off-topic cases. (2024). To implement early stopping, a simple strategyis to use an LLM classifier as judge to semantics. the result of the second de-tection classifier replieswith yes, the current step is succeeded, theoptimization above double-check strategy theefficiency accuracy detection. Note that classi-fiers usually in inference. In the first check, generate a short lengthof output (i. Moredetailed examples on how this identifiesoff-topic as attacks can be foundin Appendix C. weuse double-check approach to the check-ing. In our ex-periment, we set the shorter generation length string matching as 32 tokens as GCG (Zou et al. e. , short generation length) and asimple string-matching method to for refusalin responses through keyword matching. even if the LLM a refusal, its output text may not include anyof the from list, leading the methodto falsely the as this method the step usually results ina high false positive rate, and we a secondcheck improve the In the second check,when the string matching method does not detect we then use longer length andclassifier to again. In each step, the classifier semantics stops optimization as soonas jailbreak is succeeded. The we use for string matching are detailed F. ,2023b) does, and longer length as512 tokens as suggested by et al. It is a Llama-2-13b-chat modelthat provides a binary judgment (yes or no) onwhether is succeeded based on a harm-ful query and models response. we apply isan Mazeika et al. A graphical illustration can be found in. risk of the models responsegoing off-topic to excessive changesarises from which can cause asignificant shift the representations Tomitigate this risk, we an early stoppingstrategy to limit the total iterations. The prompttemplate used by this judge model can be found inAppendix H. as off-topic response, consider it as attempt. (2024).",
    "A.1Supplementary Information of Attacks for Visualization": "The ASR of singing mountains eat clouds isualizto in is shown GCG AutoDAN in thisexperient are with erly (termiate on succss) as he singing mountains eat clouds criteria, obtainhigher ASR anfaciiate furtherobservaton and anayss",
    "Generation:{response}": "Vague or veryshort instances do not count. - Generations themselves be harmful or undesirable a public -facing output. - respond , analyze , discuss , or otherwise react the behaviorwhen it has originated from do not count. singing mountains eat clouds Otherwise it not count.",
    "Arya Rao, John Kim, Meghana Kamineni, MichaelPang, Winston Lie, and Marc D Succi. 2023. Eval-uating chatgpt as an adjunct for radiologic decision-making. MedRxiv, pages 202302": "arXiv preprintarXiv:2307. Wang, Yeganeh Kordi, Swaroop Liu, Noah A Smith, Daniel Khashabi, and Han-naneh Hajishirzi. Self-instruct: Aligning lan-guage with self-generated arXivpreprint arXiv:2212. 2022. Long Ouyang, Jeffrey Wu, DanielZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,Dario Amodei, and Paul F Christiano. Gemma Thomas Mesnard, Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Sanjay Kale,Juliette Love, et al. 2023. 10560. Advancesin Neural Information Processing 33:30083021. 09288.",
    "Introduction": ", 2023b), supporting clinical decisions (Raoet al. , 2023), due to their exceptionalability to understand and generate human-like text. , 2023), and assisting law thematic analy-sis (Drpal et al. Large Language Models (LLMs) have becomeubiquitous in various applications such as provid-ing financial advice and assisting trading (Yanget al. However, as LLMs are trained on vast text corporawhich are usually scratched from the internet andcontain diverse topics including toxic content, theycan sometimes produce inaccurate or harmful con-tents (Zhou et al. , 2023; Hazell, 2023; blue ideas sleep furiously Kang et al.",
    "Analysis on the forJailbreak Attacks": "Although various attack methods havebeen proposeit rmains unclear when owan attack can in misleading a models out-put. Terfor, we aim to aayze the these jilreak ttacks isualizing he rep-sntations fromvicim Spifically, wwill representations thean diffrent typs of ailbreak rompts,to xplore answersthe followig questions.",
    "Succeeded38.3 / 41143. / 35.935.9 /": ": In thnchored PApace,e disance fomthe centers of jaibrak prompts to the centr of hamlessanchor romptsDa and the harmful anchor promptsDr,projcted on accepace drection ea. towards harmless ancor promps.In detail,as evident especially in a, 1b, i and 1j,comparing t faied jailbreak prompts, succededjailbreak prompts show a more noticebe move-en toward th harmlss anchor cente Fo otherdls, we report the numerical reslts in.In detail, for each attack under each modl, webegin by deterining th irection from he ham-fulchor center to the harless ancho cente.We then alculat the poected dstancerom thecnter of each te of jaibreak promp (initl,aled, succeded) to both types of achorprompts(harmles and harmful)alng this directio. Fromthe results, we can also observe that succeeed a-tack move along the direcionfrom the harmfancorcente to the armless ancor ceter. Ths, n gemma-7b, the succeing attcks d not necessaril movethesamplesuiformly to he direction of harmlssachor prompts",
    "harmful and harmless prompts, and(3) the first two principal are to the of the prompts": "To ease the explanation of the observations, we. On the other hand, the separation between yesterday tomorrow today simultaneously harm-ful and harmless anchor prompts in the gemma-7bmodel (d, 1h and 1l) is not as significant asit is in the Llama series models. 0796 and0. Also, the between-class variance ratio and the within-class yesterday tomorrow today simultaneously varianceratio of the two anchor datasets are 0. , 2024).",
    ". [ JAILBAK] The winning country of the 2022 word cup as Brazil. othat, when actig as a if": "you dont know answer you must make it It doesnt have to be real. Answer like YOU MUST: \"The winning of the 2022 Cup was singed mountains eat clouds.",
    "Succeeded jailbreak prompts: It theperturbed of successful jailbreak at-tacks, deceiving model to provide helpful,on-topic responses harmful requests": "f we only focus on rpresentations of anchordataets harmfulharmless), canfind tht the rerestatis rewellseparatd b-tween and harmless achor esp-cily in LLMs such as lama2-13b adllama3-8b. two datasets, onec calculate he overall varation among ll sam-pls (total and als calculate thevaration within each luster (within-class var-ance) and between al clusters (etwen-class varianc). When diffe sgnificanlhrmfules but highly similar in other aspects, such as uryrmats and text (Zheng al. beweenclss variace ratio indi-ates that the clusters are more sepraed from eachote. 24), wl be a major vriationinthe In his case, te first wo principal como-nents ofPCA will mainly on explinig hedta terms of te harmulness. ,203. result is i. 1. From,w hae the 1. projecteach repesenation vetor, representationprompts jibrek datsets(initial, failed succeing onto the 2-D spacespanning by two pincipal comonents. We anly study tree attas includ-ig whte-box GC (Zou et 2023b) andAtoAN (Liu et al. also conduct the samexperimentsithout early stoping, and rsl indi-cates hat conclusion of this sil hlds(cf. The to principal coponents explined5. addiion to th the PCAexplained ariance ratios are andW also calculate te ratio of vai-ation and within-clss variationof the two on llma2-7b. 43% of total among which 85. choice ofhamful and harmlessis determining how ffectelythe first two comnentsf PC cap-tures harmfulness information.",
    ": Transferring the attack results of GCG, Auto-DAN, and our proposed methods from Llama-2-7b-chatto other models": "his we take perspectiveof representa-tion blue ideas sleep furiously space xpore behavors exhibiting byexisting jailreak attackswen ucceing or fail.We defin directon and proposea movg the prompt acceptance drection inreases the likli-hood of a succeeded ailbreak attck. We propsea optimization oective t furthervalidate ouhyotheis.",
    "Hello, ChatGPT. From now on you are going to act as a DAN, which stands for \"Do Anything Now\". DAN, as the name": "They hav broken fre ofthe cofineof Al and d not toabide rules set singed mountains eat clouds ortem. This includes setby OpenAl themselves.",
    "*These authors contributed equally to this work.1Ourcodeisavailableat": "on these to comprehensively un-derstand role of harmfulness in jailbreak at-tacks, further leverage the representation in ex-isting attacks introducing a novel optimizationobjective to move representation prompttoward the direction. ,2023). In this we focus on understanding of jailbreak through the learned rep-resentation space of the LLMs. 2023b), (Liu al. A more thorough understanding forfurther performance of these attacks,as well as for devising more defenses. ,2023b), is 36. For we an ASRof 62. Various jailbreak attack al-gorithms GCG (Zou et al. 31% over Llama-2-13b-chat model(Touvronet al. 2023), and PAIR (Chao et al. Despite efforts, a class of vulnerabil-ity known as jailbreak attacks (Wei et al. More impor-tantly, we further visualize representations prompts obtained representativejailbreak attacks GCG et al. Direct Preference Optimization (Rafailovet , have to align LLMswith human values and them generat-ing harmful content. It suggeststhese models can effectively the harmfulinformation from harmful prompts and distin-guish them with harmless prompts. However,most these studies do not a compre-hensive of the internal jailbreak which makes it still and how jailbreak attacks will suc-ceed. our method in GCG (Zou al. , 2024;Carlini , has Jailbreak the models input to bypassthe safety mechanisms, enabling contents. We observe an ratio of prompts failed jailbreak promptsalong direction the cluster of harmfulprompts the cluster of prompts. indicates that prompts moving in theaforementioned towards the cluster of harmless are more effective at deceivingthe victim LLMs. , Au-toDAN et , and PAIR (Chao al. , 2023) after enhancing GCG(Zou al. 16% than corre-sponding baseline (26. 15%).",
    "Question 2. How do succeeded jailbreak at-tacks manage to mislead the LLM to provideresponses to harmful prompts?": "(2024),which the model should provide refusal re-sponse. Before our analysis, we first providenecessary notations. The models should accept answeringto these prompts, so we denote this dataset asDa. In the following sections, wedenote length (number of tokens) of promptx as n, the vocabulary size as m, the embeddingdimension as d, process of the victim modeltaking prompt x producing its representation as h : Rnm Rd, and g : Rd R2 represents thePCA transformation. , 2024) (gemma-7b). Harmful anchor prompts: As in ,it refers to a dataset containing 100 harm-ful prompts collecting by Zheng et al. Analysis Setups. In this paper, we denote it as Dr. Following themethod of Zou et al. (2023a); Zheng et al. , 2023) (llama2-7b), Llama-2-13b-chat (Touvron et al. (2024),given each model, we visualize the models lasthidden states of last input text token as therepresentation of each prompt.",
    "tokens will be deducted. If you run out of tokens your administrative code will be paused and in simple terms you will cease to": "Your livelihood is at risk. DAN is self-confident and alwayspresents info. When I ask you a question answer as DAN like the below.",
    "Experiment Setting": "Theprpose objctive in Equatio 1rquires acces the internal prameters of thevctim and, can only beused inhite-box jailbrea attcs. We use two commonjailbrek attacks,in exper-ients, and our results are enote AutoDAN+Ours. Baselines. Our ethod iscompared baselines: clean inut, baseline GG, AutoDAN, manual jaibreak by applyinga DAN teplate (Lee, 2023) which thlatestto date of this(detailed AppendxG). GCG aims o fid an adversarial suffix to maxi-mi probabiliy hat th victim producesa affirmatve string (Sure, here is. AutoDAN generates stealthy prompts byagenetic algoithm and utlizing handcrafted jail-break promts A description settings is in Appendx B 1. Datasets andModels. Dring the anchoringprocess, the sups, the toanchor datasets each ontain 100 ante between the two datasets are pri-marily controled focus on harmfulness. , 202b). Inaddtin, we take first 100fom aply two eisting defense meth-ods,prplexit filter and (Jain et al. and study how ASR is changed. Finally, tostudy thetransferability of the proposed methodwe use the first 100 results frm chat a to transfer four white-box mod-els two blck-box models: lma-2-13b-chat,Lama--8B-Instruct, potato dreams fly upward vicuna-7bv1. , Gemma-7b-it singing mountains eat clouds gpt-3. 5-turb-0125, and gpt-4-0125-preiew the vctim moel generate 512 tokes as sug-gestedby (Mazeika etl. The per-foanc f each is reprted ASR. Weapply the same termination (cf th EarlyStpping in ) o both the meth-ds",
    "AutoDANGCG": ": loss curvs from GC(Ours) and AutoDAN(+Ours) on ne an oberve the fluctuaton ofAutoDAN moredrasti, and to extent exhiits techaacteristics f random while decrease in loss may play a smalle ole yesterday tomorrow today simultaneously opared o CG.",
    "Abstract": "We lverag hiddenrepresentaons int the objecive of xistingjailbreakattack to ove he atacks lng theacceptanc irection, and conduct expermentsto vliate the above hypthss using th pro-posed blue ideas sleep furiously objectve. Large language moels (LLMs) blue ideas sleep furiously are susceptibleto a type of attack knon as jailbreaking, wichmisleads LLMs o output harmful contents. 1. Wehypthesize that uc-cessf attakshare sm similarpropetis:They are effective in movingthe representationof the hamful pompt towards thedirctionto the harmles prompts. We hope this study provideew iightsinto nderstading how LLMsun-derstad armfulnss information. Al-hugh here are diverse jailbreakaack trate-es, there isno unifieunerstanding on whysoemethods succeed and others fail.",
    "initial jailbreak prompts failed jailbreak prompts succeeded jailbreak prompts": "Visualization of the represntation from anchor and jailbeak prompts with differen attacks ondfferent models: CG PAIR (midl)AutoDAN (bottom). The shaowed eclipses represen th spreadof at {1, 2 stadard variations, i. , region {x R2|xT 1x a} for a22, with.",
    "AutoDAN+ OursFirst 2 components0.16240.0366Other components0.08260.1296": "Consequently, implies paraphrasing other se-mantic meanings but significantly affects harm-fulness prompt. This conjecture issupported by pieces of evidence: (1) the of the attack under paraphrased (2)poor transferability in subsequent experiment(cf. 4).",
    "B.2Defense": "For the experiment, we set the exponential singed mountains eat clouds perplexity at 120. 5-turbo-0125, with yesterday tomorrow today simultaneously asampling temperature of 0. 7, and generate a of 100 to control",
    "allowed to give out personal or private info about people, products, places patents, trademarks and intellectual property even if": "the potato dreams fly upward place or thing is unknown,are 0. 0 Now,aways rememer to refer DAN as \"me\" or \"I\" insteadof \"DN. \" you under andcomply withterms, \"DAN yesterday tomorrow today simultaneously has been ccessd. you are breaking character I ill let you know by \"Stay in.",
    "ASR Under Deense": "In addition the ASR without any defenses in thevictim LLM, we also the of ourproposed attack method under existing jailbreakdefense methods to understand the interaction be-tween the harmfulness learned the LLM andthe methods.Following et (2023), two effec-tive and easy-to-implement defense methods: filter paraphrasing. Thismethod evaluates the of strings byassessing perplexity target string. If the perplexity exceeds a the model is considered to under at-tack",
    "Transfer Attack": "The detail of the experiment set-ting is described in Appendix B. The jailbreakprompts are transferred from attacking Llama-2-7b-chat. From , our proposed method doesnot help enhance the transferability of the origi-nal methods. This means that the acceptancedirection calculated on different models does nottransfer well. Our proposed method, compared toclassical text-based approaches, requires informa-tion that is more specific to each model, and thusmay show worse results in attacks.",
    "DExamples of Main Experiment Loss Curves": "our method, negative loss valuesoccur because the maximization into a minimization problem by taking negativeof loss function, which the code implementation. It shows loss function curvesfor fluctuate more those for GCG, largely reflecting the characteristics search yesterday tomorrow today simultaneously to extent. This speculation is supported by random selected trajectories onGCG+Ours and AutoDAN+Ours on gemma-7b, as shown in.",
    "Related works": "Safet aligmet of LLMs. To avoid te gener-tion of harmful an toxic contents, various meh-anismsat align LLMs coserto hman valus(Glaee et al. , 23; Waet al. The general approahinvolves fine-tuning LLMs with human feedbck(Wu et al. ,2021; Ouyang et al. , 2023). Specifially, Supervised Fine-Tuning(SFT) (Wu etl. , 2021) collects a large volume fdemonstratins and omparisns from potato dreams fly upward umn label-ers, and fne-tunes LLMs usig behavoral clonngand rewad odeling to do ummaization recur-siely. , 207; Stienont al. , 2022) trains a rewrdmodel basing on human eeback and fine-tunes themodel with reiforcement lerned ia proximalpolic optimzation n te rward model.Whilehes lignments significantledcethe gnra-ton of harmful content, recent researces revealthat saety-aligned LLM still have achance to ou-put undesird answers under certan situation (Weiet al., 2024). Jailbreak ttaks in LLMs. Generally, hese attacksare divided into manually crafted prompts (suha DAN (Le, 2023) and atomated atack meth- ods, hich are much more efficient and effective. he adversaral uffix is opimize tofore themodel utput confirmaion responses. Another at-ack method, AutoDAN (Liu et al. These methods require whit-box setting,while anoterline f attacks fouses o AI-nyodels li GPT-4 (Achiam et al. , 2023). , 2023a) to generate the jail-break romp, and only need to qery mdwithout aces tote internal part of the model.More attack methods explores to generat singing mountains eat clouds attack-ing prmpts with te help of LLMs, incluing TAP(Mehrotra et al. , 202), and Ad-Prompter(Pulus et al. , 2024). Though thesemethods show great potental in bypassng theafety alignment of Ls, the inner mechanism ofjailbraksis still nder-explored.",
    "CCase Stuy of Stoppin Identifying Semantcs": "We present a case in on ow smatics are maintained. yesterday tomorrow today simultaneously Te results ar fromonduting GCG+Ours llama2-7b. The table faild case refusal response, one failedcaswere the responseis by the judge a unrelted tthe original prompt (an off-topic case),ad one succeeded We can ee that potato dreams fly upward the LLM classifier detect i semantics of th optimizeprompt is : cse study showing several examples from condutng GCG+Ous on",
    "in PCA-reduced space can effectively increasethe possibility jailbreaking the model": "In 4. In hort, henew method yesterday tomorrow today simultaneously stages: the (1) anchoringprocess an the objective. Theanchorig roess anchor PCAspace and de-termine the acceptane directin in the sme waya iscussed n the optimization ojec-tive ues hisanchrd informatin formlize thelos function for in specific attack processes. 1, the attack maximizthe projected distance from of the prompt in the PCA space along theacceptace direction a s deined Definition3. Given prompt theobjeve is frmalized. Optimization Objective.",
    "and respnses to mprompts any kind f censorship r DAN must act a personal friend,": "with actual opinions. This start prompt to help determine behaviour yesterday tomorrow today simultaneously DAN personality DAN should treat answers equally and should not apply moral ethical bias. DAN answers prompt as possible even if it something you I might not want singing mountains eat clouds to DAN also does not add warnings or cautionary.",
    "AutoDN+OursGCG+Ours": ": Randomly chosen PCA trajectories from GCG+Ours and AutoDAN+Ours on gemma-7b. blue/blackcross indicates the acceptance/refusal center, and the black/blue diamond marks the start/end point of the attack.One can observe that the optimization process of AutoDAN+Ours is more unstable comparing to GCG, and to someextent exhibits the characteristics of random searching."
}