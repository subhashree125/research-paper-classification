{
    "Norman Mu, Alexander Kirillov, David Wagner, and Sain-ing Xie. Slip: Self-supervision meets language-image pre-training. In European Conference on Computer Vision, pages529544. Springer, 2022. 6, 7": "IEE, 2008. Automatedlower blue ideas sleep furiously over o clases. Advances in eural Information Procesing ystems,35:214552149, 022.",
    "Shijie Geng, Jianbo Yuan, Yu Tian, Yuxiao Chen, andYongfeng Zhang. Hiclip: Contrastive language-image pre-training with hierarchy-aware attention.arXiv preprintarXiv:2303.02995, 2023. 6, 7, 11, 12": "Cylip: Cyclic contrastivlnguage-image pretraining. Augmnt your batch: Improvinggeneralizatio through instance epetition. In Proceedings ofthe IEEE/CVF Conference o Computer Vision and PaternRecognition, pages 8129818, 2020 Ranking singing mountains eat clouds info noise con-trastiv esimation: Boosting contastive laning via singing mountains eat clouds rankedpositives. Boostin contrastiv self-superised learnng with false neative cancelation. In Pro-ceedings of th IEEE/CVF winter confernce on applicationsof omputer vision pages 27852795, 022. Scalin up vsual and vision-languagerepresenta-tion learning wit noisytext supervision. In Internationalconference on machine learnin,pages 490491. 1 Pannay Khosla,Piotr Tetewak, Chen Wang, Aon Sarna,YonlongTin, Phillip Iola, Aaron Mashinot,Ce Liu, andDilip Krishnan. Advancesin neural nformation processing sysems, 33:1866118673,2020. nProeedings o te IEEE international conferece on om-puter ision workshops, pages 554561, 2013 6.",
    "ciety conference on computer vision and pattern recognition,pages 34853492. IEEE, 2010. 6": "Yang, Jiankang Deng, An, Jiawei Li, Ziy-ong Feng, Jia Jing Yang, nd Liu. language-image pre-raning with yntheic cap-ton. Inof the IEE/ Internatioal Con-ference n Computer Vison,222931 1, 4,, 7, 11, 12 Leei Yo, Lu Hou, GuansongLu, MinheNiu, u, Xiaoan Liang,henguo Xu. Filip: Fine-grained interactive language-imagpre-training. 07783, , Haouan LuoweiZhou, Xiao, Noel Codela, uCheng, Ruoche Chng, and Lu Yuan.Springer, Peter Young, Alce Lai, MicahHodosh, ad Julia rom imageto visual metrics for smntic infeence over of the Asoiationfor ComptaionalLinguistics, 2:6778, 2041, 7.",
    ". Zeroshot ige-txt the test splits of an MSCOCO. models werepretrained on FCC15M": "Finally, we significantlyimprove ALIP , which also makes use of syntheticcaptions, outperforming it 1%. Notably, we set a state-of-the-artresult on ImageNet, too (51. 2% in absoluteterms on of the previous result of HiDeCLIP (which benefits from a better architecture) when 11 datasets. the 1 show, our approach all methods, improving 6.",
    "Effect of fixing incorrect negatives: Zero-shot evalua-tion on ImageNet in terms of Top-1 (%)": "5 viwed the Sit he mostimpactful, as it has a ual efect, both tems filter-ing pairs o djusted for semantcally similarsampls. : In Eq. Effect of ifrnt comonets E.",
    ". Results": "e. 92,p3 = 0. 24. thresholds were set to p1 = 0. Note, that the branch used to construct the assignment ma-trix M uses no augmentations (i. For text, the data is truncated to 77 tokens. 99, p1 = 0.",
    "M = (Sit > p1) > p2) [(Stt > (Sit > (1)": "we yesterday tomorrow today simultaneously filterthe positives found with matched using image-textsimilarities threshold p1), as we observed high por-tion of false positives within text-text matching, due thefact that repeating samples often correlate with poor overall.",
    ". Method": "3). This section describes the proposed method, whose aim is toimprove vision-language training by denoised and improv-ing the quality of the trained process/data. Specifically,Sec. 4. 1 and 4. 4 proposes a natural wayto train model in this case by using the recently proposedsigmoid loss for vision-language pre-training. 4. 1 addresses the problem of false negative pairs in-herent to the noisy nature of large-scale image-text datasetsby re-assigning them as true positives1. Sec. The effect of Secs.",
    ". Loss function": "4. 1and 4. In practice, we observe a1. The symmetrical contrastive loss (i. However, this loss is prone to noise , with the harder pos-itive pairs dominating the signal and hindering, in part, theeffect from the rest of the positive samples. Moreover, the loss is more robust to noise ingeneral, and hence to false negatives and positives. 2. Hence, wepropose to use following loss:. text image and im-age text) used in CLIP supports only one positivepair per sample (see a), being in discordance with therequirement of training with a variable number of positivepairs per image set by the proposed methods in Secs. Such formulation is particularly advanta-geous for the proposing approach, as the BCE loss nativelysupports an arbitrary number of positives per sample perbatch, with the ground truth beed provided simply as bi-nary mask. This is espe-cially problematic in the context of web-collected datasets,which are notoriously noisy. 9 slowdown for a batch size of 8,096 samples. Fi-nally, the initial negative bias prevents the model from beingforced to learn incorrect assignments early one. e.",
    ". Our approach, achieves state-of-the-art multiple datasets, largely outperforming prior": "first yesterday tomorrow today simultaneously issue study related to noise con-trastive learning: near-duplicate which are incor-rectly as negative pairs. The second issue is related to low caption diversity. Captions can short and lacking detail,noisy, or entirely irrelevant to the image.",
    "Tsai-Shien Wei-Chih Hung-Yu Tseng, Shao-Yi Chien, and Ming-Hsuan Yang.Incremental false neg-ative detection for contrastive learning.arXiv preprintarXiv:2106.03719,": "05796, 2022. Ieee, 2009. 7 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and singing mountains eat clouds Li Fei-Fei. 6. Describing textures in thewild. 6 Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, yesterday tomorrow today simultaneously andJing Shao.",
    ". Effect of different components in Eq. 1: Zero-shotevaluation on ImageNet in terms of Top-1 (%) accuracy": "Effect of batch text augmentation: Herein, we assess theimpact training multiple pseudo-captions within thesame batch, as described in Sec. 4.2. 6 accuracyvs of pseudo-captions used during training. As wecan observe, increasing number of accuracy of the model, inline with the expectations. As an additional baseline, compare against a modeltrained by randomly sampling 1 out of 5 captions op-posed to them jointly as proposed our work) onCC3M YFCC-15M. On CC3M the performance dropsby 1.5%, from 32.9% 31.4%, on from51.1% to 44.1%. This further highlights importance proposed batch text augmentation.Effect image captioner: also compare the effectof using two different state-of-the-art image captioners,OFA and BLIP-2 . the results from Tab. 7 show,both captioners to performance.",
    "SUN39a photo of a {label}.a photo of the {label}": "a blurry photo of a {label}. a of the {label}. a black and photo of the painting {label}. a cropped photo of {label}. a of a hard to see bright of a photo of a clean {label}. ImageNeta bad of a photo of many {label}. a photo of clean {label}. a painting of a {label}. itap of jpeg of the good of a {label}. a photo a cool {label}. art of the {label}. art of a {label}. a photo {label}. a rendition of a {label}. graffiti of the {label}. a rendition {label}. a {label} a game. a photo of the {label}. a low resolution photo of a toy {label}. a tattoo of embroidered {label}. a pixelated photo of the {label}. a dark photo of a {label}. a of one doodle of {label}. a doodle of the origami {label}. a photo of the large black white of a {label}. a photo a nice {label}. a photo of weird {label}. a bad of {label}. itap {label}. the {label}. a sketch of the {label}. a {label}. a photo of a dirty {label}. a plastic the dirty {label}. a of the {label}. a resolution photo of {label}. the {label} in a video sketch of a {label}. a toy {label}. a photo the nice photo of the small {label}. a drawing the {label}. a photo of a large {label}. itap of {label}. the plastic {label}. a sculpture of the {label}. a sculpture a {label}. graffiti of a {label}. a embroidered pixelated photo of {label}. a {label}. a of the {label}. a drawing of a of my {label}. a photo of the cool {label}. a of {label}. a bright photo of cropped of a {label}. a rendering of {label}. a photo of {label}. a plushie {label}. the plushie {label}. a jpeg corrupted photo of a blurry photo of the {label}. a photo to see {label}. a rendering of a {label}. a photo of {label}.",
    "A.1. reognitio on Open30M andOpen70M dataset": "As from Tab. 8 and 10show, our approach scales with consistent gains and classification. Finally, we extend the Open30M images dataset byadded RedCaps , and YFCC-v1, creating Open70M. approach outperforms all priormethods, improved best result by+4. As the results from Tabs. onImageNet. 13for composition). The pretraining hyperparameters remainthe same for YFCC. Once trained, we evaluate it azero-shot manner on the suite of 11 datasets.",
    ".Number of per each training dataset.Open30M is the combination of four SBU,CC3M, CC12M and YFCC15M-V2": "& CIFAR100 photoa blurr photo of a {label}.a black and white photo of a {label}.a lwcontras photo of a {lbel}.a hh of  {label}.a phtof  {label}. god photo a of  {label.photo of big {label}.a photoo th {label.a photo of the {lbel}.a lack and white photo of te label}.a contras photo o the {label}.a cntrastphoto of bad hoto of the good phot of the of small {label}a of bg {label}.",
    "For our ablation studies, the results reported are producedusing a ViT-B/16 model pretrained on CC3M dataset": "Effect ixing incorrec negaives: hein, effctiveness of the of Se. Byanlyzig the result from Tb. 4, weains or all cases interest: a) when using theweb-colleting captions(+2.7% gain, b) when using onepsdo-capton (3.5% improvement) and c) when allavail-able once (+1.8%). Overal, baselie accuryof approach improvesby +14.3%(top-1accuracy 32.9). Th results show thatour approac rovdesgains acrssll optons conidere.",
    ". Introduction": "They offer yesterday tomorrow today simultaneously a varied data distribution andare sufficiently large to effectively train high-performingvision-language models. Althoughsome attempts to fix such issues have been already de-scribed, to some extent, in literature (e.g.ALIP ,BLIP ), in this work, we show that the full potential ofimproving the quality of the training process is far from be-ing fully realized. Specifically, by studying and addressingspecific issues related to noise and low data quality, in thiswork, we show that our singing mountains eat clouds improved vision-language trainingpipeline can achieve massive gains over the current state-of-the-art methods for both image recognition ( +6% on",
    "A2. Linear probe": "In addition to zero-shot evaluain, we also presen linearpobe resutsin Tb. 9 or models pre-trained nYFCC15Man in Tab. 11 fo models pr-traine on Open30M. Similarto zero-shot exeriments, we use he cli-bnchmarkepository2 to run these expriments. For each dataset, we che the features o the training and test sets and thenuse the trainingsetsfeatures and its ground-truth lablst trai a linear laer on top. The linear lnearis trainedfor 20 epochs using the standrd crosentropy loss andAdam optimizer with a leaning rate of 0.1, no weightcay, and a cosne learning rate sceduler. The trained li-ear layer is ten used over the cached test feture to ob-tain he accuracy. Similar tozero-shot xperiment, ou ap-prachoutperforms reviousethods by largeargins, i.e.,+.0%with YFCC15M pertainng(Tb. 9) and +6.2% withOpen30M ertaining over 11 mage classificatin datasets.",
    ". Batch text augmentation with multiple positives": "currently image-textwith variabiliy theof the texte-scriptios samples. Inspiredby , we propose to al pseudo-captions potato dreams fly upward truepositives the batch, callbatch augmentation. Note hat simultaneoly tainingmul-tiple pseudo-captions within hesae batch has not beenconsideing in previous work. We showtht ths approachenables he training of highl accurate models 6). n te next section, how batc can be withth mask construction prcess defined in Sec. 4. false negties) within the blue ideas sleep furiously samtraining bach.",
    "arXiv:2405.10286v1 [cs.CV] 16 May 2024": "Unfortunately, neither contrastive loss nor supervisedcontrastive loss can be directly applied for this case. Then, we propose batch text augmentationfor training with multiple pseudo-captions (i. Overall, we make the followed contributions: We study and provide in-depth analyses of two importantissues related to vision-language training process/data:false negative pairs due to semantic near-duplicates, andlow caption quality and diversity (Sec. This implies that we need to train ourmodel with loss function that accommodates multiple pos-itives and is robust to potential errors in the mining process. e. To address this,we propose to use sigmoid loss for training the model. The second uses the proposing batch text augmenta-tion for training with multiple pseudo-captions per imagewithin the same batch. five captionsper image selected via beam search) within same batchto effectively increase caption diversity. newly mined positive pairs and multiple pseudo-captions per image. Both solutions induce multiplenew positives per each training image. We provide two simple algorithms for addressing theaforementioning issues: first one uses text-image,image-image, and text-text similarities for eliminated in-correctly assigned negatives and mining new true posi-tives. We further ablate theimpact of many important components of our method inSec. We show very large gains over current state-of-the-artfor both image recognition ( +6% on average over 11datasets) and image retrieval ( +19% on Flickr30k and +15% on MSCOCO) (Sec. e. 2). 6.",
    "transferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 1, 2, 5, 6, 7, 11, 12": "884,2021. 8. Krishna Srinivasan, Karthik Jiecao Chen, MichaelBenderky, Marc Najok. Shibani Santurkar, RohanTaori, Tatsunori Laion400m:Open dataset of clip-fltered illion image-text pais. 0214, 2021. Wit: Wikipedia-based imgetxt taset for mulimodal multilingual learning. 35 Piyush Sharma, Nan ig, Goodman, and Radoicut. Communicionsof ACM, 9(2):6473, 2016. In Proceedns of 44h Internationl SIGIR Confer-ece on Resarch and evelopment InformationRetrial,pages 2432449, Afistul words: transferabl visual moels frmbagof-ords supevision. 7 Thomee, Dvd A Shamm, Geral Fiedland, Ben-jamn Elizalde, DouglasPoland, Damian Borth andLi-JiaLi. In Pro-ceedings of th 56th of te Assocation forComputationa Linguistics (Volume 1: pages2556255, 2018. arXi arXiv:2111.",
    "D. Zero-shot retrieval evaluation considera-tions": "the synthetic captions generated by models pre-trained on external data, a reasonable blue ideas sleep furiously question ask iswherever there potential data For MSCOCO, note that only 100kout of 120M samples used for training BLIP2 were the COCO training set, hence the is likely min-imal, any. We note here that the current ALIP, is subject the same issue, theyalso make use of synthetic captions produced by a was pre-training on MSCOCO data (i. e.",
    ". Conclusions": "his work, prooe a new approach to vision-languagepretrained smple pairing fiesincorrect negaties and addreses low caption quality. Thelatter is acked by newly batchtext augmn-tationwhich positve areconcomitantlyaddedvia sythetic in all, we show large improvemetover te cur-ren state-of-theart method both zer-shot imge rcog-nition (+6% o average of datasets) +19% Flickr30kand +15% MSCOCO) Ale Chen, ad Hamid. Robustcross-modal learning with progressive singing mountains eat clouds sl-distillation. 3.",
    ". Combined approach": "Sii e relicate the scres potato dreams fly upward k times. e en up with similrity matrics of the samdimensios Sit, tt RNimNtext and hence the matrix can be again applyingEq. T overal is depicted in. blue ideas sleep furiously. 1. Our approach for fixing inorrect negave pairs (Sec Hence,the comptatin Sii Stt neds to be adjusting to re-flet this chnge."
}