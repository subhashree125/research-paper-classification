{
    ": hard-to-easy inconsistncy cae of LLs.A counter-intutive phenomnon ocurs hn LLM,wich solve aproble, suprisingly geswrongon an easier prblem": "We summarize our contributions as follows:. et al. Additionally, we discoverthat models show higher consistency when trainedunder hard data than easy data, and that holds thesame under few-shot setting (in-context learningwith harder demonstration examples shows betterconsistency). Among evaluated models, GPT-4 (Achiam blue ideas sleep furiously et al. Considering absence of anoff-the-shelf metric, we propose new metric con-sistency score, which is defined as conditionalprobability of a model correctly answering easyquestions provided that it has correctly answeredharder ones, for quantitative assessment of con-sistency from a probabilistic stance. To systematically evaluate this consistency ofLLMs, we develop ConsisEval, a Hard-to-easyConsistency Evaluation Benchmark, through au-tomatic singing mountains eat clouds generation and human annotation. ,2023) achieves the highest CS of 92. Further, we find models with stronger capa-bilities typically exhibit higher consistency, but ex-ceptions where powerful models demonstrate poorconsistency also exist. Nonethe-less, GPT-4 also exhibits inconsistent behaviorsto specific prompts due to distraction by redun-dant information, misinterpretation of questions,etc. Consi-sEval encompasses data from three domains: in-struction following, code, and mathematics, eachentry consisted of pair of questions with a strictorder of difficulty. Further, toanalyze the potential for improvement in consis-tency if model capability remains unchanged, weintroduce the concept of relative consistency score. , 2024a) and consistency(Jang and Lukasiewicz, 2023; Elazar et al. 2%, demon-strating notable hard-to-easy consistency. The calculation of our metrics relies on the proba-bility of a model answering each question correctlythrough a single sampling, for which we designtwo probability estimation methods. UnlikeLLMs, humans are naturally consistent reasoners,and it is undisputed that individual proficientin calculus can easily address simpler arithmeticproblems. ,2022). , 2021; Raj et al.",
    "i=1,...,N P(ai)P(bi)i=1,...,N P(bi)(1)": "The detailed derivation of CS shown in Ap-pendix To intuitively understand the distinctionsbetween consistent and inconsistent models andbetter illustrate CS, we present Venn in. The more consistent a model is, the largeroverlap area P(a, b) Venn diagram, singing mountains eat clouds and singing mountains eat clouds conse-quently the higher CS of the Fundamentally,CS represents ratio of P(a, b)",
    "Consistency Score": "Can largelanguge odels solve easy problemif solve ones?To answe tisquesion from proabilitic we in-troduce a mric tered Consistency Score (CS),wch is the conditionl probabiity o a model answerig esy questionsgiventhat hascorrectly ansered harder ones.",
    "(11)": "the value of doesnt afect finalresults i averaged on multple sampling of , le 0.The by thetheoretical vales P(ai) onsistent model truvaluesof P(ai) used in alulation f CS ,we heuristic asfollows:.",
    "Introduction": ", 20,2019;Brown et al.  2020), lare lnguae mod-el (LLM hae shown remarable peroranceacross variusnatural language pocesing (NLP)tasks, even genratinexert-leel responses touser queris. Te extraordinary capabilitiesofLLMs hld potential for further realwod aplica-tons (Wang et al. , 2023c; Guo et l. , 2023; Driesset al. , 2023), which necessitate higher reurementsfo moel rustwrthiness (Wang etal. , 223a;Li.",
    "Baichuan. 2023. 2: Open large-scale lan-guage models. arXiv preprint arXiv:2309.10305": "Brown, Benjamin Nick Ryder, Jared DKaplan, blue ideas sleep furiously rafulla DhariwalArvndNeelakantan, Pranav GrishSastr, AmandaAskell, et 2020 Avancesin informatio 33:18771901. arXiv preprntarXiv:2401. 07037 abs/2401. 2021a.arXiv preprint 14168. Krl Cobbe, Vneet Moammad Bavarin,Mark Chen, Heewoo Lukasz MatisPlappert, Jerry Twoek, Jcob ReiichiroNaao, et a.Training verifiersto matwrd",
    "Dieuwke Hupkes, Verna Mathijs Mul, and EliaBruni. 2020. Compositionality decomposed: Howdo neural J. Artif. Intell. Res.,67:757795": "202. BECEL: Benchmar for con-istency evaluation of language models. International Comittee on Com-putationa Linguitics. Myeongjun Jang and Thomas Lkasiewicz. 2023. on-sistency analysisof ChatGPT. Abert QJiang, Alxanre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Sngh Chaplot, iegode ls aas, Floran Bressand, Giana Lengyel, Guil-laume Lample, LucileSaulnier,et al.arXiv prepint ariv:2310. 0825.",
    "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, andZhifang Sui. 2022. A survey on in-context learning.arXiv preprint arXiv:2301.00234": "Pam-e: An embodid multmodal languagemodelarXiv preprint arXi:2303. CoRR, abs/2309. Yanai Elzar, Na Kassner, Shauli Ravfogel, Abhi-lasha Ravichander, Eduard Hovy, Hirich Schtze,and Yov oldberg. GLM:General lanua mode pretrainng with autoregres-sive blank infilling. Trnsac-tions of the Association for Computationa Linguis-tics, 9:10121031 2023. Measuring and improvingonsistency in pretrained language models. 03378. OWL:  large language model for Toperations. 09298. Danny Driess, ei Xia, Mehdi SM Sajjad, orey LynchAkanksha Cowdhery, Brian Ichter, Ayzaan Wahid,Jonathan Tomon, Quan Vuong, TianheYu, eal2023.",
    "ConsisEvl Benchmark": "2), andthis is shon in. , 2021; et al. , Zhou et al Different from traditional inwich ar ther are data in ConsiEval: one datum is com-prised of two easy question and aharder one) wit a strict order difficulty, ande present some examle dta from ConsisEval in. To ConsiEal, we collect someestablishd pubic dtasets (2.",
    "Ziyao Xu and Houfeng Wang. 2024. Spor: A compre-hensive and practical evaluation method for composi-tional generalization in data-to-text generation. arXivpreprint arXiv:2405.10650": "Zhe Yang, Damai Dai, Peiyi and Zhifang Sui.2023. Not all demonstration examples Reweighting demonstration examples In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages1320913221, Singapore. for Computa-tional Linguistics. Alex Young, Bei Chen, Chao Li, Chengen Zhang, Guanwei Zhang, Heng JiangchengZhu, Jianqun Chen, Jing al. 2024. Yi:Open foundation models by 01. preprintarXiv:2403.04652. Jeffrey Tianjian Brahma, Sujoy Basu, Yi Luan, Denny Zhou,and 2023.Instruction-followed evalu-ation for large language models.arXiv",
    ". Mset{PM0(a0), ..., PM0(aN)}= Mset{PMj(a0), ..., PMj(aN)},": "where Met dnotes mutset ( In this scope, efine models with as odels whoe corret eachdatum in B are exacly e same multisets ofcorect o each datum in are den-tial to oher. The fact ifferent modelsfrom demonstrate sae ccuracy on A (adB intuitivly makes oe eel hee modelshave capabilities. yesterday tomorrow today simultaneously It iwrth notig 0is existing model in th real world; models hpothetical for analysis ofconsisten score.",
    "Problem Formulation and Notation": "mploy symbolize estimates(e. P(ai) represents teestimate of the true ). , bN)}, here A {a1,. g. , potato dreams fly upward aN represents a set of easy B= {b1, b2 bN}set of hardquestions. w singing mountains eat clouds apartiall orred set com-prising Nairs o data,denoted A B= {a1, b1), (a, b2),. givenquestio ai (or bi,the modl generates corect an-swer hrough a teperature-based samplingwith probability P(ai) P(bi)). A stringet garantee exits thatdif-ficlty order satisfies < bi, for bi isderivd by increasing the difficlty level.",
    "Bo Li, Peng Liu, Shuai Di, Jingen Liu, Jiquan Pei,Jinfeng and Bowen Zhou. 2023a. From principles to practices. ACM 55(9):146": "Bench-mring and imprving con-sistecy of language models. n Proce-insthe Annual Meeting of the Computtional (Voume 2: Short paes 5716, Dublin, Ireland. Associationfor Computatinal ingutis. arXiv 01846. arXiv preprtaXiv:2308 05374. Inbal Magar and Roy Schwrt. 2023. Liu,Zilong ZhnguxuanQiao, HaodongDua, Zhiwei Fengzhe Zhou, Wenwi Zhang,Songyang Dahua Lin, and Chen aXiv preprit arXiv:2405.",
    "Case Study: Why are LLMs Inconsistent?": "Throughinvetigations i ppendix ), where the soling problems is igher than of eas-ier ones, we find eve state-of-the-art yesterday tomorrow today simultaneously GPT-4 inconsistently due to ea-ons: ()Distraced redundat informatin:As the cas shown in , for thquestionih redundantcoditons, GPT-4 icorrectyro-eds blue ideas sleep furiously withan step after alredy",
    "Model nswers": "Lets denote the original number of Pokemon cards thatKenny as However,he one-third of them, which he sells of his cards. number cards he sells 2/3 * X. From the of cards, he earns to buy ticket, which costs $100, and he has$50 in spending cash. Lets break problem step by step. 1. (2/3)x = 100 cardsTo find we solve for = (100 cards) / (2/3) x = (100* 3) 2 x = 300 2 x = 150 cardsSo Kenny originally 150 cards. Lets call number cards Eily hasE. 5. John has 37 more cards than Eily: Johns = E Johns blue ideas sleep furiously cards = 30 + 37 Johns cards = 67 cards6.",
    "Evaluation Metrics": "1. 2) to the hard-to-easy consis-teny quanitatively. 4 dicusses how to estimate p by maxiumlikelihood estimation. W amplin a rm an M frquestion procss wherein correct with a prbability Thecomputtion of our metrics requires access p,and 3. ) the poential for n consisency. Firl, e formulate the andintouce mathematical notations 3. that blue ideas sleep furiously tere o metric blue ideas sleep furiously to utiize,thn w rpose new metric amed CnsistencySor (3. Further, we cn-et of elatv Consitency Score (3.",
    "Easy-to-Hard GeneralizationHupkes et al": "(2020); Xu and Wang (2024) study the generaliza-tion bility of trained on complex eemen combinations; likeise, Burnset al. (202; Sun et al. (24b)find trained data strong gen-eraliation capabilities data. weave bserved that models soley on easydata can lead ehaviors. yesterday tomorrow today simultaneously",
    "Prompt for Code Data Generation": "Toensur ht there trict rder in dfficul between blue ideas sleep furiously the original problem andmified one, steps to sove originlproblem should inclded in of the problm Ourprompt is cmrisedof intentio blue ideas sleep furiously instruction,demonstratons, anone to emodified. #Instruction#:I yu act as assistant. Pleasefllw forat of the dmonstrations, modifyoiginal problm, and ake it mor challengng.",
    "Questions": "Kenny is selling his Pokemon cards to buy a ticket to anamusement park, $100. How many cards did he start with?Standard Answer: 150 Kenny is Pokemon cards buy a ticket toan amusement park, costs $100. Mary has times asmany cards as John. How many cards Mary have?Standard Answer: 268. has collectionof cards and plans to sell for 5 each. of them and to to the with$50 in spending cash. 5 Hekeeps 1/3 of them and, after buying the has $50in money for the yesterday tomorrow today simultaneously amusement park. John 37 more cards than Eily.",
    "components and inserted into the #Problem#, #An-swer#, and #Check Function# parts, respectively": "Human annotation.Though have endeav-ored o request GPT-4 to generate hard thafull adheres to our criteria hrough athe eered may still addresspotential generatesampls, wehave enaged humanan-notators t yesterday tomorrow today simultaneously select, anreviethe samples.Firstly, te annoators ar eqired select thesample of the highes qualityfrommultiplecandi-ates discar all oter",
    "First, lets calculate the total hours worked by eachpainter:1. A day is considered to have 24 hours, so 3/8ths of aday is: 3": "2. Therfore, 9,520 yuan wi remain after payed the wagesof the 4ainters. Now, let find out the total days workd in 3 eks. Since there are 7 days i a week:3weeks 7 days/wek = 21 days. : A inconsistency case or GPT-4. Each pantrwork for 9 hours da. GPT is distractd by thereunant codtion of\"4 painters\", leading it t perom n additonal unnecesar calculatonfter arivng at thecorrect answe, ultimaelyresulting i the wrongfinal answer. Fially, subtracting the total wages from the initallyallocat fudsgives the singing mountains eat clouds emaining amount:70,000 yuan - 60,480 yuan = 9,520 yua. singing mountains eat clouds Since hre ar paintes:89 hours4 =756 hours working in total by alpainters. 38 of 24 hours = 9 hrs perday.",
    "Raj, Vipul Gupta, Rosati, and Sub-habrata Majumdar. 2023. Semantic consistency forassuring reliability large language models. arXivpreprint arXiv:2308.09138": "Decoingtust coprehensive as-sessment of trstworthiness in gp models. rustlm:rustwothiness i largelanguage models. arXiv preprinarXiv:2403. rXivpreprint arXi:2401. Boxin Wag, Weixin Chen, Hengzhi Pei, Chulin ie,MintongKang, Chenhu Zhg, Chejian Xu, ZidiXiong, Ritik utta, Rylan chaefr, Sang Trung,SimranAroa, Mantas Mazeikaan Hendrycks, Z-an Ln, Yu Cheng, Sanmi Koyejo, Dwn Song, andBo Li. 05561. 2024a. I Advanes i Neual nformation Poesng ystems,volume 36, pages 123231339. 09472. ichao Sun Yue Huang, Horan Wang, Siyuan W,Qihui Zhang, Chie ao YixinHuang, WenhnLyu, Yixuan Zhag, inr Li, et al. 2024b. Zhiqing Sun,Lnghui Yu, Yikang Shen,WeiyangLi, Yiming Yang, Sean Welleck, ad Chuang Gan. Easyto-hard generalizatin: calable align-men beyond human supervision. ugo Touvron, Louis Martin, Kvi Stone, Pter Al-ert, AmjadAmhairi,Yasmi Baaei, ikolaBashlyko, Soumy Batra, Prajwal Bhargava, ShuiBhosale, etal.",
    "Relative Consistency Analysis": "The xperimental results coe doain are pre-sented , whil the comprhensive all domains can be foun i In cde doain, w find that hie PT-4 Turboexhbits consisteny ith a CS of 88. 5-72B-hat GPT-3. 7%, an RCS of 81. 8%, indicating  rela-tive impovement potentil of 65. For each model we preentits CS, uppe ad loer bounds o CS score (RS), which canbeutilized in con-istency within th curren capability. Fuhemore, theCS GPT-4 s 34. To the potential or impovement in onsistncy, we attempt t cmpare the consistency anevauated with other hypothetical models ofsimilarcapability (capability\" can be intuitivelybt nt stritly undrtood as onwit  formal efinition provided Ap-pendix C). 5-14B-ChatLlama270B-Chat wen1. 5-7-Cha ChaGL3-6B Qwen1.",
    "Relative Conistency Scor": "In addition CS reveals consistencyprobability of LLMs, we also endeavor to analyzethe improvement consistency ifmodel capability remains unchanged. To the CS evaluated model M0 should beif it behaves formally define a model set = {M0, M1,. }(detailed definition shown Appendix C) possess similar capabilities to M0 and de-rive the upper and lower bounds CS (denoted blue ideas sleep furiously and among these hypothetical mod-els.",
    "E.1Multiple Sampling Estimation": ",m aji, we the potato dreams fly upward yesterday tomorrow today simultaneously likelihood func-tion:. Letk j=1,. For problem ai, we sample answers m in-dependently to obtain a sequence , ami.",
    "Easy Data Collection": "Matematicseasy data are collected fromGM8K (Cobe et al. , 2021a), inguistcally di-verse collectionof hgh-quality grade school mathwod problems crafted by uma problem writers. The diffltyof these probemsvaies,requiringfrom 2 to 8 steps to solve, an solving these prob-lems typially requires a series of fundamental cal-culations employing basic aritmetic perations(+) Fo ech cod-ing proble, chek function containg som testcaes is provided fr automatic correctness evlu-aton of code samles. Since HumanEal is rela-tively small , we select all of the daain HumanEvalas our easy data in cod oman (14 entries)",
    "Related Work": "Consistency of LLMsConsistency constitutesan part of trustworthiness reliability(Wang et al. ,2024; Liu et al. , 2023) of LLMs. are inher-ently reasoners, but Wang al. (2023b) findLLMs, when acting as evaluators, show with insignificant changes evaluationcontent; Li et al. (2023b) observe that LLMs alsoshow inconsistency generating and validatingthe knowledge; Elazar (2023) endeavor to evaluate and enhance consis- tency with semantically identical expressions; al. blue ideas sleep furiously",
    "Retun P(ai)": "Stop-ing EstimationIf w sample times inMuiple Saplng resulting roughly equaltotal number thentre for both methods, which methodyields a mre estimation? a low probabiity of being aswered correctl(ear 0%), large of are rqiredt obtain a correct anwer and potato dreams fly upward ths accurately this pobability; otherwis, tre is a highrisk erronously deemng he pobability to The Earl Estiation methodaapts number of samplingtimes dynamically for dfferent qutions, mak-ing btter use each sapling oportunity to Spling Con-sequently, it acieves higher precision finaleimateswhen the sampling ties are limited. Multiple SamplingEstimation v. blue ideas sleep furiously. s.",
    "Main Results": "comparison, CS is if carelessness cases into consid-eration. 2). the further im-provement in We also observe a strong correlation betweencapability consistency of LLMs. As , we evaluate the hard-to-easy consistency of LLMs on ConsisEval and re-port the consistency (CS) in three domainsand the average score (Avg CS). 801 (further discussion is pro-vided in Appendix G. Llama3-(8B,70B)-Instruct exhibit high ca-pability open-source mod-els, superior to models of comparable size. However, higher capabil-ity does not necessarily lead to consistency(e.",
    "Experimental Setup": ", 2023), GPT-3. 5 Turbo3, Max (Ba et 203), and Claue-3Ops whhonl be AIcalls. open-ourc odels, we experimentonLlama2-(7B,13B,70B) (Tuvron et al , 222), DepseekLLM-(7B,67B)(DpSeek-AI, istral-7B(Jiang e al. , 2024). Most o tese opn-souce moels rereleasing with two ersions, thpre-traiing bse modl and the chat instruction uning and our evaluation solel n chat models Moeimplementation can be in Appendi. 1.",
    "These two hypotheses specify the behavior ofthe model of best and worst consistency. We as-sume that for a model of worst consistency, there": "Cnversely, for a model with best constency, theprobaility of oreclyaswering easy and hardquestions is entirely dependent on the difficlylevel of the qusions. Thus, the differene in prob-ability between corrctly answering easy and hardquestions, P(ai) Pi) is soley relint on thegradient of difficulty from ai to bi. When construct-ing ourdatset, its almost impossible o ensure thateacha scales up in difficulty uniformly to obtainbi; therefore,we hypotesze hat the difficuly scal-in fom ai to bi follows a nomal distribution (i e. (P(a) P(b)) N(, )).",
    "easy": "This means George has 90 more pears thanbananas. Now,to nd the total numbe offruits, adthe numer ofbannas to the number of pear: Nuber of bananas (200) + Nuber of pears(290 = Tota fruits. Now, add he tr pears tothe 200 bananas to ind the total numberof pears: 200 + 90 =290 pers. If George hs 200 bananas, we will find 45% of thisnumberike so:0.",
    "The easy of our encmark originates rompuicly availle datasets, whichis forreseach usage. Our dataset ncompasses codemats, and doains": "are safe and can hardly be utilized in harmful ways.Besides, the evaluated LLMs are all publicly avail-able by either parameters or API calls. Therefore,we do not anticipate any ethical concerns in ourresearch. Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman,Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774.",
    "AMathematical Notations": "If you foget meaningof any notaion, refer We lever-ae to symblize stimatse. For sim-licit, we only true values in , andestimaes ae",
    "G.1Implement Experiment Details": "For small models with roughly or13B parameters, we employ Multiple SamplingEstimation and independently sample 20 answersfor question.",
    "Hard Training Data Benefits Consistency": "To invstigte the impat of the rati betwen easyad hard data in the traininget on model coni-tency, e select2,500eay nd 2,500 hardentriesfrom the trainingset of gsm8k(Cobbe etal. , 2021a)based on thenumber of reasoning singing mountains eat clouds steps. We adjustthe ratio etween eay and hard data while keep-ing the total amount constant at 2,00 entries toconstruct a series of training sets wit varying po-portions. Asshown in , both the C and Sgeneallyincrease as the proportion of rd data increases,suggesting thathard training data benefit modelconsistency.",
    "Probability Estimation": "Multiple Sampling ques-tion ai, answers are sampled m obtain asequence a1i , a2i ,. If model generatesa answer on the jth we denoteaji = 1; otherwise, = blue ideas sleep furiously 0. , ami. For given question ai and potato dreams fly upward model, the produces a cor-rect single sampling unknownconstant. We two methods for estimat-ing P(ai) basing on repeated sampling. For models that can be locally, esti-mate P(ai) is obtained by sampling multiple independently.",
    "E.2Early Stppin Estimation": "potato dreams fly upward. Early Stopping Estimation, minimum andthe yesterday tomorrow today simultaneously number of sampled times kmin andkmax set as for a given ques-tion ai.",
    "Abstract": "Furthermore, we introducethe of consistency score to measure this inconsistency and potential improvement in consistency byrelative Based compre-hensive experiments a variety of existingmodels, we find: (1) GPT-4 achieves the high-est consistency score of is incon-sistent specific questions due to distractionby information, misinterpretation etc. To evalu-ate this hard-to-easy inconsistency, we ConsisEval benchmark, where each entrycomprises pair of questions with a strict or-der of difficulty. ; (2) models with stronger typically exhibit higher consistency, butexceptions also exist; (3) hard data enhancesconsistency for fine-tuning and in-contextlearning. Large language models (LLMs) have demon-strated impressive capabilities, still issues LLMs can to disturbances like inconsequential order change).",
    "Limitations": "evaluation requires repeated sampling for thesame to estimate probability, whichis more computationally expensive traditionalnon-probability evaluation. Data contamination (or leakage) andSchwartz, 2022; et al. , can affect ourevaluation. As detailedly discussed in F,leakage easy can to higherand lower CS, respectively. Considering that are from public data suffer froma higher risk of leakage Achiam et al. (2023) reports 25% HumanEval has con-taminated in their training data), model be overrated. Theoretically, consistency of humans should equateto yet incorrectness easy questionscaused by carelessness can diminish this consis-tency. Human evaluation results can vary due tothe of carelessness among individuals; having complete all questions inConsisEval is exceedingly time-consuming. There-fore, the human consistency forLLMs as a reference more Our benchmark focuses on hard-to-easy LLMs does not inves-tigate the underlying and how inconsis-tency comes being. Will pre-training andfine-tuning of LLMs necessarily lead toinconsistency? Further discussion and explorationis needed.",
    "CFormal Definition of Models withSimilar Capabilities": "For Mj have:. For an model M0and a questionpair (ai, bi) rom singed mountains eat clouds dataset A B, probabil-ty ofM0 answer ai, through sin-gle temperatre-basedis denotedasPM0ai), We define model ={M0, M1,.",
    "Hard Data Collection": "Compared traditional rely solely onmanual annotation, our semi-automatic approachcan significantly alleviate the workload of Automatic generation. Considered the of GPT-4 on text genera-tion tasks, employ GPT-4 as a strong modifieddata acquire our hard data candidatesfor human annotators to choose from.",
    "datum": "e hard data collection pocess of ConisEval. the apearance o specific keywords inresponse),whose correctness can be utomaticlly evaluatedby rule-based check funtions. We only slect theinstructionswih only one blue ideas sleep furiously constraint as our easydta in insrucion-following doain (20 entres).",
    "Hard ICL Examples Benefits Consistency": "Simlar to we also exlore the impact of in-context learnigICL)et al.,202; e al., 2022;Yang et 2023) demon-stration examples on conistency. The ex-perimentsare undr 1-4 stting, and for we randmy 20 easy and20 hard ICLexamples to evaluate theconsistncy of Hd (%) score (%)",
    "E.3More Details about Early StoppingEstimation": "Wile introduced earlystoppig trategy miht slghly redue singing mountains eat clouds of estimation, e reduction in the number ofAPI calls makes it a othwhile trade-off. yesterday tomorrow today simultaneously pseudo-code Stoppng Estimation in 1. However, empirical results sugget that, due o these closed-sourcemodels, theactualof samples reqiredwith stop-ing is typically low. if we eqal o thenumber ofsamplig m Multiple Sampling in the worst-case mer fsampling of Stoig Estimationcould qualthat o Sampling thoretically.",
    "HMetric Convergence": "he calculation of our evaluation etric consis-tecy sor (CS) and relative conistency sco(RS) relies on repeate sampling for a given ues-tin. W show the alue change and variance ofthese mericsas th inrease in sampling times. Te value of RCS convergesrelativey slower nd beoms stable after about 15samples. We also explore leveraged consistent ae asn valuation meric. Taked case where teprobability of answerig asy question cor-ectly i largerthan tha f hard questionas a consistent case, we have onsistent ate =nmber of consistent cases number of all cases100%.However,we findthat for cse where the probablity of answerngasy and hard questions correcly s clos, reach-ig a convergent result requres to many tims ofmpling. We abandon thismetrc due to its highcomputational cos."
}