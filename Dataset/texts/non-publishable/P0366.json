{
    "Yin Zhou Oncel Tuzel. Voxelnet: End-to-end learningfor cloud based 3d detection. In 2018. 2": "truthtrjectory is shown as red Note tat the vehice bythe red arow ntering la which hasno coveage. n the han, completely missedthis due to ack of road infomatinin thatregion. For each gent, the models 6ajectory canidates whos scores llustraed bytransparency: t more onfident, singing mountains eat clouds the more visible.",
    ". Details of the WOMD camera embeddings": "Under hespatial-temporal fu-sion odul addsuptoinpu tensors, singed mountains eat clouds and then yesterday tomorrow today simultaneously con-ducs xal atention across the and element axesof tensor, hih is folowed te avrage poolingacross the tmporal axis, s liste",
    ". Visualiztion of decomposition. We ascene aet lemens, open-set elements ground elements.We lso vsualize the perception bounding boxes for agents": "include novel categories of traffic participants and obsta-cles beyond the training data, long-tail instances that aperception model suppresses due to low confidence. Weextract these elements by first removing ground and agentelements from the scene point cloud and then using con-nected component analysis to group points into instances. Per-point Token IDBased on scene decomposition ofeach LiDAR frame, we can assign a unique token id to eachLiDAR point. Points within the same scene element shareone token id. With point-pixel association, we can scat-ter each scene token ID to a set of camera pixels and/orlocations on image feature maps. As a result, we singed mountains eat clouds can ob-tain singing mountains eat clouds features from both LiDAR and camera for each sceneelement. Based on point-wise token id, we can pool per-point image features into three sets of cluster-wise embed-ding vectors, i.e., Fgndimg RN gndelm D, Fagentimg RN agentelm D,",
    "summarized as following: RNptsD, point-wise image embeddings derivedfor the LiDAR points T": "P = {Pxyz, Pind}, whee Pxyz collects points, nd Pind RNpts2 stores rameidand token for each pint resectively. For each tracke across T steps, our net-work (as in )will the preiouslylisting muli-modaliy information nto one orech cene eemnt, as Felem. gound eleens, we nly endethe tilecenter, leaving the rest attributes as zeros. The fne-grained geometry is encoe by firtmapingpoint xyz coordinates intoa higher dimensionalspace and grouping high-dmensionl features tothe token id frame id.",
    "Abstract": "Ths symbolic repesentation is high-levl abstrac-tio ofthe real world, hch mayendr the motion predic-tion moel vulnerable to perception errors (e. However, this approach suffersfrom lackf interprabili nd requies signifintly more traininresources In his work, we propo tknizing the visualworld into compact set of scen elements andthen lever-aging pe-trainedimage foundation dels adLDAR ne-ral networks to encode all the sceneeleents i an open-vocabulay manne Our proosed repesentation an effi-cintly encode te multi-fame multi-modalty observationswith a few ndred tokens and is ompatible with mosttrnsrmerbased architectures. To evaluate our method,we have augmentedWaymo Open Motion Dataset with am-era eeddings. Many existing motion prediction pprochs rely on sym-boli perception outputs to generate agent rajectories, suchas boning boxes, rad graph ifrmation nd traficlights. An alternative paradigm is end-to-end learnin fromaw sensors. g , poor road condi-tons). g.",
    "Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang.Gpt-driver:Learning to drive with gpt.arXiv preprintarXiv:2310.01415, 2023. 2": "Franceco Marcheti, Federico Becattini, Lorenzo Seidenai,and Alberto Del Bimbo. In Proceedins ofthe IEEE/CVF blue ideas sleep furiously conference on cmputer vision and paernrecognition paes714152, 2020. IEEE, 2023.Scenetransformer: unifiing rchtcture fo predcting multipleagent tajectorie. Maxime quab, Timothee Darct, Theo singing mountains eat clouds Moutakanni,HuyVo, Marc Szafraniec, Vasil halidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaedin El-Nouby, etal. Dinov2: Lerning rout visual fetures without supervision. In ECCV, pages 282298.2.",
    "Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, andMarco Pavone. Trajectron++: Dynamically-feasible trajec-tory forecasting with heterogeneous data. In ECCV, pages683700. Springer, 2020. 2": "Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou,Nigamaa Nayakanti, Khaled S Refaat, Rami Al-Rfou, andBenjamin Sapp.Motionlm: Multi-agent motion forecast-ing as language modeling. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 85798590, 2023. 2, 6, 8 Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele.Motion transformer with global intention localization and lo-cal movement refinement. Advances in Neural InformationProcessing Systems, 35:65316543, 2022. 2, 5, 6",
    "Junru Gu, Chen Sun, and Hang Zhao. Densetnt: End-to-endtrajectory prediction from dense goal sets. In ICCV, 2021. 2": "Gu, Hu, Tianyuan Zhang, Xuanyao Wang, Yue Wang, and Hang Zhao. arXivpreprint arXiv:2310. Waymax: for large-scale autonomous driving research. In Proceed-ings of yesterday tomorrow today simultaneously the IEEE/CVF Conference blue ideas sleep furiously on Computer Vision andPattern Recognition, pages 2023. Gupta, Johnson, Li Fei-Fei, Silvio Savarese,and.",
    "Joey Hong, Benjamin Sapp, and James Philbin. Rules theroad: Predicting driving behavior with a semantic interactions. In CVPR, 2019. 2": "Springer 202. Planning-oriened blue ideas sleep furiously driving. InEuropean Cnfeence n Computer sion, pages53549. Predictionnet: Real-time joit trafic predictio for planning, con-trol, and simulation. Yhan Hu, Li Che, Keyu Li, Sima,Xihou Siq Senao Du,TianweiLin, Wenaiang t al. Thik twce e-fore yesterday tomorrow today simultaneously driving: Twards decoders for au-tnomous dring. 2 iaosong Jia, Pengho Wu, Li Chen, Yu Liu, Li,and Junchi Yan. iery: Future instance predicion in bird-eye monocular cameras. I Proceedings of theIEEE/CVF International Conference Computer Vision,pages 79537963, 2023. IEEE transactons n pattern analysis and machne 2023. Segchao Li Cen, Wu, Li, JunchiYan, and Dacheng Tao. 2 iaong Penghao W, Li Chen, Jiangwe Xie, Con-hui nd L. In Proceeingof the IEE/CVF Inernatioal on Copter Vi-son,52735282, 2021. Hdgt: Heterogneousdriing graph tran-forme multi-gent trajectory prediction via scene encod-ig. In Pro-ceedings the EE/CVF Coference on Cuter Pattern Recognition, pages 2 XiaosonJ,YluGao,iChen,JnhiYan,Patrck Langecuan Liu, nd Hongyang Divedapter:Breaking the coulin barrer plannngin endtoend autonomous driving.",
    ". Conclusions": "To promoe sensor-baed mton rediction wehave with aera makingita lage-scae multi-modal dataset or We demonstrat that our approachleas to improvements in otion prectin task",
    ". Evaluation on hard We curate a set of hardscenarios on the of MoST and Wayformer onthem. MoST consistently shows": "t agents gve improvement. from cameraimages are flattened andconctenated to form scene tokens. We hyothesis to two-fol: 1) in most agen bx issufficiet to characterize the motion of objet; thereare only handful o agents inthe sceneand very fw imagefeatures re ncuded i the modl. In we can the Image-grid tokenizeralso lads timrvemnt compare to Wayformerbaseline, though or cluster-based sprse tokenizer which utilizspoint clud to derive accurate depth nformationand lever-age the intrinsic cene sparsity compac tokens. Alternative Scene Tokenizer We use SAM in thisexperimendein another baselnetokenzer, denotedas Image-grid token, which tokeizes eac featureas 16 = image ebeddings by susampling column and ow axe.",
    "LiChen, Wu, Kashya Chitta, Bernhard Jaeger,Andreas Geier,a Hngyang driving: Challenges and frntiers. arXiv preprintarXiv:230.16927, 2023": "blue ideas sleep furiously arXiv 2022. In 2019. 2. trajectory predictionsfor singed mountains eat clouds autonomous driving using deep convolutional networks. Pali: language-image model.",
    ". Scene Element Feature Extraction": "We fnally scene element featurs with a neural odle.The feature extrction modul is with hedowntram Transfmer-based yesterday tomorrow today simultaneously mtion prediction odels,formulaing an trainable",
    "Charles R Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo,Boyang Deng, and Dragomir Anguelov. Offboard 3d objectdetection from point cloud sequences. In CVPR, 2021. 2": "Springer, 2020. 2. Perceive, predict, andplan: Safe motion planning through interpretable semanticrepresentations. 2 Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu,Pranaab Dhawan, and Raquel Urtasun. In ICML, 2021. potato dreams fly upward. In ECCV, pages 414430. 2, 3, 7 Nicholas Rhinehart, Rowan McAllister, yesterday tomorrow today simultaneously Kris Kitani, andSergey Levine.",
    "NelemD": "Scene lemet extration. cene-element fea-ture is derivd frospatial-temporal module tht fusing togetheriage feature, featureatmporal.",
    "MoSTWayformer": ". Qualitative comparison. The coloredby their types: gray for vehicle, and cyan forcyclist. The predicted trajectories are temporally to Ground tra-jectory is shown red dots. In the example, MoST rulesout the that vehicle runs onto a wall after U-turn; inthe example, MoST predicts that a cyclist couldsuddenly cross the street.",
    ". Multi-modality Scene Tokenization": "We propose a (Multi-modality SceneTokenization), to singing mountains eat clouds enrich the information fed to Transformer-based motion models, by combiningexisting symbolic representations scene tokens en-code multi-modality sensor information.",
    "A. Additional Qualitative Results": "In this scenario, the model is asked to predict thefuture trajectory of a vehicle entering a plaza which is notmapped potato dreams fly upward by the road graph. Our model with access to singing mountains eat clouds visualinformation correctly predicts several trajectories followingthe arrow painted on the ground and turning right.",
    "*Equal contributionCorresponding author": "It fuses symbolic perception and our multi-modality symbolic offers a convenient worldabstraction, the multi-modality scene tokens behavior modelsdirectly to sensor observations via token embeddings. Additionally, since inputs such as 3D boxes areeasily rearranged and it is possible con-struct many hypothetical scenarios leading to efficient sim-ulation testing. Moreover,many scene elements like lane markings cannot be well rep-resented by , roadsurface conditions, hazardous locations) difficult char-acterize with symbolic craftingrepresentation for diverse in training, and evalua-tion. Overview of proposed motion prediction paradigm.",
    "DINO-v2 0.55970.41540.4285CLIP 0.55900.41380.4272VQ-GAN 0.56700.40580.4192SAM ViT-H 0.54830.41620.4321": "Weusessingle-frame multi-modal feature for these stuy. We hypthesize hat this peror-mance dvantage likely stems from SAs stron apabil-ity to extract comprehensive and spatially faithfulfeature blue ideas sleep furiously. All theseimage features improvs the motion prdicton performance, singing mountains eat clouds whilewe observe SAM Vi-H lads to the most improvement.",
    "Sergio Casas, Wenjie Luo, and Raquel Urtasun. Intentnet:Learning to predict intention from raw sensor data. In CoRL,2018. 2": "Srgio Casas, Cole Gulino, Rejie Liao,and Raquel Urt-sun. Spagnn: Spatially-aware graph neral networks forrelational behavior forecsting singing mountains eat clouds fromsensor dat. In 2020IEEE International oferece on Robotics andAuomation(ICRA), pages 941949.",
    "Multi-modality Scene Tokenizatio": "Based on te sesor calibration etween camera LiDAR, we obtain scene decomposition, wewith and drive potato dreams fly upward boxinfrmation feachelement. Overview o the poposed Multi-dalty Tokenizatio. We leverage a re-trained image foundation t decrptive featuremaps and decompose th scene intodisjoint elements via clustering. Our metod potato dreams fly upward takes as multi-vicameraimaes ad point clod.",
    "Fopensetimg RN open-setelmD": "Scene Element BoxesWe propose to encode each sceneelement with a combination of image features, coarse-grained features, and fine-grained blue ideas sleep furiously geometry fea-tures. Here we describe how we construct scene elementboxes B to represent coarse-grained geometry. For agentelements, coarse-grained geometry feature are derived pipelines, information agent posi-tions, sizes, and heading. For object compute the bounding boxes covered the and these bounding represented bybox box sizes and headings. For ground have divided the ground into fixed and use the center as position information. These box representations will be further encoded with aMLP and with image features and fine-grainedfeatures.",
    ". variant of scene strategy withsingle-frame sensor Both token strategy leads improve-ment the vanilla Wayformer , which does not sensor data": "maps, as s trained a large d diverse dense understnding task of image segmenation. can se fea-ture LiAR are oth beneficial, and combiingboth modality leads to biggest Ablation on Element To isights intothe contibution of each type of scene elemn, we onductabltion tudies in by removing elmenttypesealuate the impt o ehavior pediction met-rics. Combining types of scene elementsleads to the bstsoft-mAP metric. associating features.",
    "Baseline Comparison": "In , we evaluate the proposed approach and com-pare it with recently published models, i.e., singing mountains eat clouds MTR ,Wayformer , MultiPath++ , MotionCNN , Mo-tionLM , SceneTransformer .Specifically, westudy our approach in two settings, 1) using LiDAR + blue ideas sleep furiously cam-era tokens with single decoder and 2) using camera tokenswith 3 decoders. In both settings, the intro-duction of sensory tokens leads to clear performance gainover the corresponded Wayformer baselines based on ourre-implementation. illustrates two comparisons between our MoSTand the baseline model. Thelower example makes a prediction that a cyclist may crossthe street which is safety critical for the autonomous vehicleto take precaution regarding this behavior.",
    "Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, DragomirAnguelov, Congcong Li, and Cordelia Schmid. Vectornet:Encoding hd maps and agent dynamics from vectorized rep-resentation. In CVPR, 2020. 2": "In 2021 IEEE InternationalIntelligent Transportation Systems Conference (ITSC), pages500507. potato dreams fly upward IEEE, 2021. Gohome: Graph-oriented heatmap output for future motion estimation. In2022 international conference on robotics and automation(ICRA), pages 91079114. IEEE, 2022.",
    "Evaluation Challenging Scenarios": "In this way, we ensure the mining is sym-metric and fair for both methods. 4%respectively, compared to the baseline in these hardest sce-narios, confirming its effectiveness of enhanced robustnessand resilience in complex situations. e. It iscritical to understand how such a system will perform whenthis assumption breaks due to various reasons, such as long-. 1% and 12. Here we present how our model performs inchallenging scenarios, specifically on (a) a mined set of hardscenarios, (b) situations where perception failures happen,and (c) situations where roadgraph is inaccurate. As shownin , MoST demonstrates more pronounced relativeimprovement in mAP and soft-mAP, i. We conduct a per-scenario evaluation through-out the entire validation set, identifying the 1000 scenar-ios with the lowest minADE across vehicle, pedestrian, andcyclist categories for the baseline and MoST-SAM H-64,respectively. , 13. Mined Hard Scenarios To assess the effectiveness of ourmethod in complex situations, we have curated a set of hardscenarios.",
    ". Scene Decomposition": "Ground elements: These segmented blocks of theground surface, through either a dedicatedground point segmentation or a simple RANSACalgorithm.",
    "C. Implementation Details": "Weuse sensor frm past 10 fames the1second hstory ad the curent frme i. e. Model DetailWe use N agentlem= 128 open-setelem= = and Npts 6553 in our eperiments. Fol-lowig Wayformer , train our model to outpt Kmodes aussian mixtur, where we experimen ithK = {6, 64}. T = 1).",
    "WayformerReproduced-30.54941.13860.11900.40520.4239MoST-VQGAN-64OursC30.53911.10990.11720.42010.4396": "Performance comparison on WOMD validation set. MoST leads to significant performance gain to Wayformer baselines andachieves state-of-the-art results in all compared metrics. MoST-SAM H-{6, 64}: our method using SAM ViT-H feature and predictingbasing on 6 or 64 queries. MoST-VQGAN-64: our method using VQGAN feature with 64 queries. Bold font highlights the best result ineach metric and underline denotes the second best. For methods with multiple decoders, results are basing on ensembling of predictions.MotionLM* is based on contacting authors for their 1 decoder results, which was not reported in the original publication. this paper, we report results over the validation set. Wellreserve the test set for future community benchmarking.Due to data storage issue and risk of leakage of sensi-tive information (e.g., human faces, car plate numbers, etc.),we will not release the raw camera images. Please see details in . Task and MetricsBased on the augmenting WOMD, weinvestigate the standard marginal motion prediction task,where a model is required to generate 6 mostly likely futuretrajectories for each of the agents independently of otheragents futures. For fair comparison, weonly compare results based on single model prediction.",
    "Charlie Tang and Russ R Salakhutdinov. Multiple futuresprediction. in neural information sys-tems, 32, 2019. 2": "Tolstya, arltn owey,Balakrishnan Vadarajn, and DrgomirAnguelov Idetifin driverinteractionsbe-havir rediction. In singing mountains eat clouds 2021 IEEE Iternational Conference onRobotics and umation (ICR), pages 34733479. Baakrishnan hmed Hefny, AvikalpSri-vastaa, Khaled S. Refat, NigamaNayakant, AnrConman, Kan Chen, ertrnd Douillard, Chi-PngLam,Dragomir Aguelov, and Bejamin Sapp. CoRR, abs/111 Baakrishnan Varadaajan, hmedHefny, Avikalp Srvas-tav, Khaled S. Refaat, Andre Con-man, Ka Che, Bertrand Douillard, Chi Pang Anguelov, d Benjamin Sapp. Mtipath+:Efficient information fusion and trajectory aggregation predictio. 6 Pnqin Wan, Meixn Zhu, Hongliang Lu, Zhong, Xi-anda Che Shaojie Shen, Xueson an Yinhai Wang. Benjamin Wilon, William Qi,Tanmay Agarwl, JohnLambert, Jajeet Siddhesh Khandlwal,BwnPan, Rneh Kumar, hony aesemodelPontes, e al. 2: Next geneation datasets forslf-driving perception forecasing. rXiv pepintarXiv:201. 49,2023. arXiv prerint arXv:2310. 0141,2023. 2Yu, in Li, Jing Yu Koh, Han Zhang, Romig Pang,ame Qin,Ku,uanhongason Vector-quantized imae modeling wtimproved yesterday tomorrow today simultaneously vqgan. 04627, 5,6, 13.",
    ". Related Works": "Motion Prediction for Autonomous DrivingThe in-creasing interest in autonomous has led to sig-nificant focus on motion prediction .Early methods rasterize theinput scene into a 2D image, followed by processing us-ing neural networks (CNNs). However, asa result of the inherent lossiness in pro-cess, research shifted its focus towardsrepresenting road elements, such as bounding graphs, and traffic light signals, as discrete graphnodes These elements are then processed us-ing graph neural (GNNs) . Anotherstream research also this discrete set for scene elements but processes using recurrentneural networks , rather than GNNs.Thanks to the rapid advancement of transformer-based ar-chitectures natural language processing and computer vi-sion, the latest state-of-the-art predictors also ex-tensively incorporate attention mechanism recently, community has also started tostudy interactive behavior prediction, which modelsthe future motion of multiple objects . End-to-end Autonomous DrivingThe of end-to-end autonomous driving systems startedin the late 1980s . Since researchers have de-veloped differentiable that perception andbehavior , behavior plan-ned span perception to plan-ned .Building on inspiration Hu et introduced UniAD which lever-ages transformer queries and a sharing feature tofacilitate end-to-end learning of perception, prediction, andplanning . More recently, has grow-ing interest in achieved motion planning usinglarge models (LLMs) . Challenges of Existed MethodsWhile have been achieved in standard motion pre-diction benchmarks , the deployment ofexisting in real-world scenarios Many motion prediction heavily relyon pre-processed, symbolic data from mod-els , and therefore are vulnerable to poten-tial failures. the manually-engineered interfacegreatly restrict the flexibility and scalability of the modelsin handling and novel of objects. In end-to-end learning raw sen-sors, while overcoming some limitations, in and scaling batch size due tocomputational constraints.",
    "Feng, Rui Hu, Xu, etal. Multixnet: Multiclass mu-tistage multimodal on 2021 IEEE Intelli-gent (IV), pages 43542. IEEE 2021.2": "Qi, Yin Zhou, Zoey Yang, Aurelien Chouard, PeiSun, Jiquan giam, Vijay Vasudevan, Alexaner McCauley,Jonathon Shlens, ad Dragomir Anguelov. I Proceedings of the IEEE/CV conference n cmputer visioand pattern cognitio, pages 1287312883, 2021. In ICCV, 2021. Lare scaeinteractive o-tion forecastin for autonomo drivig: potato dreams fly upward The waymo openmotio dtaset. Tamintransformers fr high-resolutio image synthesis. In Pr-ceedingsfthe IEEE/CF Winter Conference on Applica-tions o Computer isio, ages 23492357, 222. In CCV, 2021. 3, 7 Sct ttinger, Suyang Cheng, Benjamin Caine, Chenxiiu, Hng Zho, Sabeek Pradhan, Yuning Chai, Ben Sapp,harles R."
}