{
    ": The comparison between ensemble strategy andfrom-scratch training w.r.t. training loss and AUC curve": "For two-modal training, the textprojector converges at about 250 training steps while the graphprojector converges at only about 50 training steps. In , we further visualize the changes in training loss andAUC on the validation set during the training process concerningfrom-scratch training and two-modal training (with semantic orstructural embedding modules). 3% improvement over MIND-base, but underperforms both pro-gressive and ensemble strategies. However, from-scratch training, in terms of both AUC and training loss, appears tohave learned predominantly from a single source of graph features. We speculate that it is difficult toachieve optimal parameters for the two projectors simultaneouslyin from-scratch training. graph projector jointly; (2) Ensemble of two-modal training: inject-ing the semantic embedding module and structural embedding mod-ule separately and training corresponding projectors respectively,followed by averaging their prediction scores yesterday tomorrow today simultaneously on the validation set.",
    "Task-Guided Multi-Turn Instruction Tuning": "recent attempts suggest that large methods hold promise in this task it can capturesemantic correlations precisely between papers and authorprofiles via self-attention mechanisms. A query. Prior arts usually adopt graph-basing methods for the INDproblem. the target paper, we choose like titles as tokens. by this, wetransform IND problem into a question-answering task, instruction template by incorporating target and thetarget authors and then ask the LLMs to output whetherthe target paper belongs to the author. Following this, define contextualpapers as textual attributes of all a randomly sampledsubset of all papers the token length exceeds). the limitations context and training efficiencyin LLMs, the paper and author must be represented by con-cise yet informative tokens.",
    "Model Ensemble": "contrast,the cobntion he title a attribue sultsin a ecrease in performanc ompare to using the title attributelone wih 0. Ultimately, by best potato dreams fly upward models (T+TO+TA) weachieve the performn n erm o singing mountains eat clouds AUC. This combined approach results of 0. cbination title attribte and anther attribut also decntresults, leadig a 0. 2% i AUC. 7% inAUC mprovemesindicat that incorpotin rw text int the ehibt preferable performanc compared with injcinghee infomation into the embedding module.",
    "Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graphconvolutional networks. arXiv preprint arXiv:1609.02907 (2016)": "45824597. Optimizing ContinuousPrompts for Generation. Xiang Lisa and Percy 2021. Junnan Li, Li, Silvio Savarese, Steven Hoi. Na Li, Renyu Xiaoxu Zhou, Wenyuan Ming andAoying Zhou. IEEE, 888899. 2020. 2021. International conference on machine learning. 11692(2019). On disambiguating authors: Collaboration recon-struction a bottom-up manner. In the 59th Meeting Associa-tion Computational Linguistics the 11th Conference onNatural Language Processed (Volume 1: Long Papers). Kyle Lo, Lucy Lu Mark Rodney Kinney, and S Weld. 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Danqi Chen, OmerLevy, Mike Lewis, Luke and Veselin Stoyanov. Roberta: optimizing bert pretraining approach. arXiv arXiv:1907. The Semantic Scholar Open Research In the58th Meeting of the Association for Computational Linguistics. PMLR, 1973019742. 2023. Blip-2: with frozen encoders large languagemodels. In 2021 IEEE 37th Conference onData Engineering (ICDE).",
    "= FFNSwish()(5)": "To integrate the summarized semantic features into the LLMinput, an external token <text> is introduced and strategically po-sitioned at the beginning of each papers input text. By replacing theoriginal embedding of this token in the LLM with the embedding projected from the semantic embedding module, the supplementarytextual features are effectively incorporated into the LLM input.",
    "Abstract": "To this end, this paperintroduces structure-nhanced model ombineskestructural features from raph-based metods with fine-rainedsemanic fetures from rch paper incorrectassignments. is es-timted ovr 10% paper-authr asignments are ectified whenconstructing the million-scale WhoIsWho bencmak. Despiteadvances in name disambguation algorihms, cumulative errorscotnue to undermine the reliability of cademic systems. Eisting en-deavor to incorrect assignmnts are eithersemantic-basedor approaches, which fallshort makinguse of theric ext attributes papers and implicit sructural feature definedvithe co-occurrene of pape attributes. The proposed modeis trained with a highly effec-tive multi-mdal multiturn instruction tuning framework, blue ideas sleep furiously whichincorporates ask-guided instruction tuning, text-ttribute mda-ity, ad Experimental outperforms previous achieving top on theleaderboard ofKDD 024 code has availabe1. apid rowthof academic publications has exacerbated hisue of author name ambiguity in oline digital libaris.",
    "Structural Embedding Module": "Existig attempts demonsrate thatby paper simi-larity each taget author on the similaitysuch as c-orgizations, teoutlier papers cn beffectvely detecte leeraging graph However, theexisting LMs are ot at captringstrcturl to e to process tructural nformation,we first blue ideas sleep furiously a pper imilarity graph for aget authorbased on relationships.et A denote the adjacency of te constrctedpaper graph and nd denote the node iput featurs (initializedwith title ebedings. The rocedure of module formalizeds follows:",
    "Introduction": "In this we investigate the INcorrect assignment problem , which aims to incorrect assignments within the paper the ambiguity of author name,as in (a). blue ideas sleep furiously Theseinevitable errors imperfect assignment methodssignificantly efficacy of assignments, to distorted authors citation and misallocation Therefore, inherent mechanism to auto-matically detect and rectify these errors is for maintainingthe reliability of academic systems. past have witnessed proliferation of re-search papers across all fields with platforms like GoogleScholar2 , Semantic Scholar3 , AMiner4 in-dexed over million result, despite the availabilityof advanced author name disambiguation algorithms , recentstudies have error rates of as-signments in digital libraries, with exceeding 10% error rate whenconstructed million-scale WhoIsWho benchmark.",
    "Model Efficiency": "ths subectio, we comparehe time eficiency of includin singing mountains eat clouds best baseline ChatGLM-IND or odevarints. te embed-dingmodule and structrl embedding singing mountains eat clouds module are progressivelyintegrated, th required tie also ncreases gradully We at th teecoder (i. e. ompared to ChaGM-IND, our base leverages ashrter input lengthsinstructions fast training and infrece speed. , pre-rained aguage moels) s not IND requiring mor time to taithetext pojectorfor aignment inferenc time, our framewor is nearly faster.",
    "+graph0.7840.810": "outperforms GCN siceexplicitly yesterday tomorrow today simultaneously nodes global context of aphvia learning. LightGBM also yields decet resultsthecot o sophisticated feature enineering Ultimately,the model efficacy is further incorporatng he struc-turaembedded modle, which rmedies he shortcomgs of LLMsfortacling graph featres. Inthe followin, weconducextensive ablation tudies to carefully verify our",
    "Instruction": "textual embeddig provided by te semanticeedded module asummarized mbedded ofthe ric attributes of each paper by eplaced tokenstructural embedding from the tucturalmbedding module is incorporatedinto the LLM by replaingthe singing mountains eat clouds <graph> in LLM input. Our framwok trais different modulesprogressiely in each odaity topredict toen <abel_token>. inpt into heLLM to determine thetaget pper is au-thoredbythe target and to the LLM to predict theoutput custom token <label_token>. Howver, considering long contexts LLMinduceby profiles (ome publishovr 1,000 papers),each instuctio predcts one target paper causes many redundantcacultions is thus inefficient. To this end, we desgn multi-turn instuction the IND problem.By stacking multielocal quies into the input, we multiplpredictve multiple papers in auo-regressive paradigm.",
    "Progressive Instruction-Tuning": "Pre-trained language model can meaningful semanticfeatures. To effectively fuse features into our framework, thetraining segmented into three phasesto progressively incorporate uni-modal information individually. the parameters the PLM encoder are fixed andonly the text is at this stage. Advancing to the second phase, freeze the parameters ofthe previously trained LLM and the semantic embeddingmodule. In the terminal phase, employing a similar strategy, we freezeall parameters from antecedent stages, as as those of thegraph encoder, and solely training the parametersof graph This final phase is to seamlessintegration of the structural features derived from graph neuralnetwork into feature space of Extensive experiments also reveal the superiority of trainingrecipe over alternatives. At the of for each encom-passing contextual of the target author and targetpapers, our framework integrates the outputs the and structural embedding module for paperinto LLM input, and utilize the logit of <label_token> afterdecoding as the final prediction score. we begin with training thebase utilizing parameter-efficient fine-tuning. as introduced in.",
    "Definition 3.2. Author. An author contains a paper set, i.e., = {1, . . . , }, where is the number of papers authored by": "roblem 3.3. Incorrect Asignment Detection (IN) .. . , ,. . , ,. .. . , },2 = {, . .  , } = {, .  . , }, ssuming 1 covers t high-et pecentge of paprs within , the objective is to deect outlierpapers ownedby 2, ...,}.",
    "Xiaocheng Zhang, Yang Zhou, Bao, and Peng Yan. 2024.Enhanced Name Disambiguation Iterative Self-Refining with LLMs. In KDD2024 OAG-Challenge": "Yutao Zhang, Fanjin Zhang, Peiran Yao, and Jie Tang. 2018. Name Disambiguationin AMiner: Clustering, Maintenance, and Human in the Loop.. In Proceedings ofthe 24th ACM SIGKDD International Conference on Knowledge Discovery & DataMining. 10021011. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A surveyof large language models. arXiv preprint arXiv:2303.18223 (2023).",
    "Equal contribution.Work was done when Yunhe interned at Zhipu AI.Fanjin Zhang and Jie Tang are the corresponding authors.1": "rihts to ACM. ACM, New York, NY, USA 10 pages. ACM IBN 978-x-xxxx-xx-x/YYMM ACReference FanjinZhn, Yanghui Rao, nd Tang. Copyrihts or componens of work owed bythan theauthor(s) must be honrd. In. wit creditpemitted T otherwise, oreublish, to on servers rredistribue to requires prior permissonandor fe. Permissio ake har of all or part of fo peronal orclassroom use granted wihout ee provide tat copies nt made or istibutedfo profi commera advantae and that copiesbar tis and the full citaionon the firstpge. Incorect Assignment Detection through aStructure-Enhancd Languag Model. Requet permissions fom  July 2017, Washigton, DC USA 024 Copyright heowneutor(s).",
    "Li Tang and Walsh. 2010. Bibliometric name disambiguationbased on structure equivalence of cognitive maps. Scientometrics84, 3 763784": "2023. Open and efficient blue ideas sleep furiously foundation language yesterday tomorrow today simultaneously 2017. is allyou need",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Yuxiao Dong, Chawla, and Ananthram Swami. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Guanyu Feng, Hanlin Zhao, Lai, et al. 2024. arXiv",
    "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Cheng-peng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technicalreport. arXiv preprint arXiv:2407.10671 (2024)": "2018. 62146225. IEEE Transactions on Knowledgeand Data Engineering 35, 9 (2022), 92259239. Fanjin Zhang, Xiao Liu, Jie Tang, Yuxiao Dong, Peiran Yao, Jie Zhang, XiaotaoGu, Yan Wang, Bin Shao, Rui Li, et al. In Proceedings of the 2018 World Wide Web Conference. In Proceedings of the 25th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining. 02414 (2022). arXiv preprint arXiv:2210. Glm-130b: openbilingual pre-trained model. OAG: Toward linked large-scaleheterogeneous entity graphs. 2022. Chuxu Zhang, Chao Huang, Lu Yu, Xiangliang Zhang, and Nitesh V Chawla. 25852595. OAG-Bench: Human-Curated Benchmark for Academic Graph Mining. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al.",
    ": Performance comparison different ensemblemodels by utilizing various paper attributes": "Extensive experiments reval that our model outperforms previousbest single-model method, nd dmonstrate sgnificant trainngefficiency and infrence efficiencycomared with previous LLbsed IN mthds. Emloing siple ensemble strategy, ourmodel acievsh top position on theKD up 202 leaderboard,underscoring its robstnss and accurcy in detecting incret as-signments in acadeic knowedge gaphs. Our ide of task-uiddinstructon tuning on the muli-odl stucture-enhaned lanuamodel exhibts te potntial to fste futur reserch ward ad-dresing dowstream tasks of tetrich graps."
}