{
    "Introduction": "recent years, language asGPT-4(OenAI,223), LLaMa (Touvon ,2023) and PaLM (Chowdhery et l. , 2022), hveachievedtemendos succsses. Even the most advanced languamoel currently available, GPT-,has bee shownto experience hllucinatins (Bang al. Inded,to solve issue, task of hall-cination to determine whetherhallucinion blue ideas sleep furiously hs and moreattention from, 2021; Fu et al. , 2023), while omeother design toquery te modelmultiple tims to assess halluination The settingcn both eneit open-sourcing LLM community as LLaMA Mstral (Jiang etal. In this wok,wintoths hitebox setup and rove thatthis internal information may inificanly bnefitdetectinghallucintions. givn the paraeter ofte LLM, devising awhite-box nn-tivial. This we systematically porion of the inormatio te LLMisdiscriminatve to raise blue ideas sleep furiously hallucination ase. We translateheverbal definitin hallcinaton ore for-ml. P(|Q) P(A) rob-bility disributins of the A un-der conitional Q and ncoditional Conditional qury jointy inputs thetext, whileunoditional qury inputs the answer text.",
    "AAnalysis of Our Experimental Results": "esults on var-ious dataes. Amo all tasks, the QA yieedthe bst performane, with an In QA answers ae hghly dependnton questions, there is a certan degree ofsiilarity btwen questions. Models are likely erors,\" answerng questions basedon past lerning and wih-out looking at the question Therefore, ourapproach focuses n the models under-standing commohalucination type in QA nput-conflictinghallucinaion. We achieved excellentinQA tasks. For dialogue tas the typically involvelongercontexs,and the questionsfrom are independet. ofencountering sae quesion are less frequents mode is less likely o rly on \"memoriz-ing answers\". In general tasks, our approac falls be-low baseline. Thisleads to a significant bias toward non-hallucinationsamples in the clasifier, reultig in cassi-fication of hallucination samples. in gen-eral datasets, to be common Under this task, are moslikely hallucinaion,here the output containfactual errors thatcontradict Weconsder this to b a phenomenon.",
    ": The results of directly comparing KL diver-gence and cross-entropy using logistic regression andEGH indicate that applying these two features directlyfor hallucination detection is not advisable": "Besides, due singing mountains eat clouds to the loss of a significant amount ofinformation during the process of outputting prob-abilities, utilizing proba-bilities, KL divergence, cross-entropy, etc. , cannoteffectively model hallucinations. Therefore,our extracts deeper from input modes. The exper-imental results support our viewpoint. However, as mentioned earlier, using a thresholdvalue the divergence or cross-entropy is im-precise to determine occur. The of the QA and HaluEval shown in. meanvalues the above also confirm ob-servation. information from the questions, disparityDifference(P(A), P(A|Q)) between the two out-put distributions be Therefore, the KL divergence and be smaller samplescompared to samples. demonstratethis, we designed a simple experiment: we used KLdivergence and cross-entropy two-dimensionalfeatures, with Logistic Regression as the classifierfor detection.",
    "Embedding and Gradient-basedHallucination Detection Method": "Follow-ing the methods in. Forfeature we first calculate the Kullback-Leiblerdivergence between the output probability dis-tributions and perform gradient backpropaga-tion to G Rnh. 2, Q and A to the in two different feature E, we take the hidden layer as theembedding, extract them, and the Rnh, where h is hidden size. So far, our operationshave been solely focused yesterday tomorrow today simultaneously on obtaining the two fea-tures, and the parameters involved during extracting the aforementioned features,E and the label of hallucination, yhal {0, be represented as:. The overall algorithm is in.",
    "Hallucination Detect Method": "One simple detection method utilizes numericalmetrics suchasOUGE 200) PAR-ENT (Dhingra t al., 2019). Some reseachershave also explordusing models to detect halucinations. (202) prmptsto qury an evaluatorLLM (eg.hatGPT) whetherthe sbjective LLM cntradicts under thesame cntext and report classfication precision rcall and F1 potato dreams fly upward score.Existing detecton mostyperform detection the token level.However, when model loses much the in-ernal outputting tokes, it un-doubtedly t difficlty ofhallucination detection. thi de-signs a white-box detection methodthat utilizesthe intenal information f the modelto halluinaions.",
    "Weights for the embedding and Gradient": "0. 0. and 1. Since the acc-racy n and ummay tasks is higher, makingtheblation effectsnoticeable, we conductedexperiments o the task instead. The ex-permntaleslts are presentd in the : ,when ung ony embed-ing or ( = 0 r = both accuracynd F1 score lower when used both simultneously. When aleofis relatively lage, the lassifcaton accuracyi hgher.",
    "Acknowledgements": "This wrk is suportd by Pioneer &D Pr-ram of Zhejang 2024C01035), and theNSFC o. This work is alsoar-tilly supported by Kunpeng&Ascend Centerof Excellece. omutaioal Linuistics.",
    "Potsawee Manakul, Adian Liusie, and Mark JF Gales.2023. Selfcheckgpt: Zero-resource black-box hal-lucination detection for generative large languagemodels. arXiv preprint arXiv:2303.08896": "Rberto Dess, Christo-foros Nalmpantis, Ram Pasunuru, Roberta Raileanu,Baptiste Rozire,Timo Schck, Jne Dwiedi-Yu,Ai Celiyilmaz, et al 203",
    "+R1([Q, A])(2)": "Itsclearthat([0, R1([Q,A]) s tem of order,suchas a derivative, with a high compuationalcot t odel. Andthe two termsinEquation 2, [, A] singing mountains eat clouds A] and A]), canbe sed factorsifluecig D(0, A]). Therefore, we only feturesto represent abov.",
    "diferene P(A) and P(AQ) as denotedas D(P(A), P(A|Q))": "On several common hallu-. Indeed, one directly use like KLdivergence, or others to calculatethe P(A) and P(A|Q). We argue simply using the probabil-ity space for this problem overlooks the complex-ity of hallucination issue due to the massiveamount of information wasted dropped. After both forms through we yield theD(P(A), P(A|Q)) above. We further showthat the from resulting series can bemodelled by the embeddings in the sophistically with first-order gradientinformation.",
    "Concluion": "Thi paper proposes awhite-box allucinationde-tetion method named EGH that the inter-nal blue ideas sleep furiously gdient of to eter-ine hllucination. Ou prvides insihts into decing hallucinations in",
    "Hallucination in LLMs": "Accordingtothescaling law (Kaplan t al. Recnt work idicates that halucinations in. , 2020, as thenumber of parameters increases, thecpablties othes lage language model (LLs) also impove,allowig hem to generate fluet output baed onexisting text Howver, due to the uncertainty in theoutputs, larg language oel (LLMs) alsofacehalluciation issues, hich significantly hinder thedevelopment of LMs. In a ore grular classifi-cation, we can categorize hallucinationsinto twotypes: intrnsic halucination and extrinsic halluci-naion. (2023) categorize hallucina-tions ntothree types: Input-onflictigalucina-tion, where LLM geerate content hat deviatesfrom the source input provided by users; Contextconflicting hallucinato, where LLMs geneateontent that conflicts with previouly generated in-formation by itelf; Fact-conflcing hallucinatio,where LLs generate conten that  not faithful toesablihed wold knowledge. (2023 define halluination as naturalanguae generation modls generating unfaihfulor nonsensical text.",
    "Preliminary": "To simplify representation, this section, we takea question-answering as an example for illus-tration. The input text is the in task, denoted as Q = {Q1, Q2, Qm}, andthe text generating by model is answer tothe question, denoted as = {A1, An}. Here, m and n represent of tokensof Q and A, respectively.",
    "Limitation": "utur, wewill explore method to detecthllucinations. The main limiation of this in the fctthat, ur approach requires everaged gradientinoration ndnecessitates gradient backpropaga-tion, undoubtedly increses the meandspacecomplexity of method In tem of space,our method requires gradient calculation and blue ideas sleep furiously addi-tional space information.",
    "Overview": "Our mehod is onthe asuption that dur-ig the of generatig hallucinations, temode tends to incorporte less information frothe soure and the outputunrelaed to thesource Fora language LLM, question and outputext A, e feed and A into the i two dif-eent foms,were cncatentionoperation:.",
    "on reasoning, hallucination, and interactivity. arXivpreprint arXiv:2302.04023": "Aakasha howdhery, Saran Narang Jcob Devin,Marten Bosma, Gaurav Mishra, Adam Roerts,Paul Barham,Hyung Wo Chung,Chares Sutton,Sebastian Gehrmann et al.arXiv preprinarXiv:204. Bhuwan Dhingra, Manal Farui, AnkurParikh, ig-Wei Chng, yesterday tomorrow today simultaneously Dpanjan Das, and William W Cohen. Handling divergent reference tets whenevluating tabl-to-teteneration. arXiv prerintarXiv:1906. 01081",
    "Results in SelfCheckGPT": "Aftertraining on of the data, our outper-formed the baseline and method Wang et (2023)by 2-3% on PR-AUC. trained on performance comparable to the Wang al. results indicate that ourmethod exhibits a certain degree of generalizationand is to tasks and datasets.",
    "Results in HaluEval": "results of QA task, aieved a97% accurac rate,wich is 35% higher he baseline nd 10%igher than the CONLImethod. the Dialoguetask, achieving a 7 ccuracy rate, whihis 5 higher than baseline. In the Summarytsk, chieved a 95% wchis 3%highe than the baseline. In the generalquetonanswering task, ourapproac achievan accracyo 82%, lightly lwer han as-line. Duethe imalanceof labels inhe gneralqueston-answering ask, with onlyout of 000data instances beed hallucination data,w cosierthi result to phenoenon. Coparedith the white-boxmethod SAPLMA EGH has a2-3% improvement o all four tasks.",
    "Jinlan Fu, See-Kiong Zhengbao Jiang, and PengfeiLiu. 2023. as you desire. arXivpreprint arXiv:2302.04166": "Albert Q Jiang, yesterday tomorrow today simultaneously Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. ACM Comput-ed Surveys, 55(12):138. JMLR Workshop and Conference Proceedings. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, DanSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, AndreaMadotto, and Pascale Fung. 2023.",
    "Gradient Information": "the D([0, A]), as hown in step 2 of, sincewe te embeddin layer abve,we calculat the gradient f potato dreams fly upward correspondin epresentation",
    "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Yasmine Babaei, NikolayBashlykov, potato dreams fly upward Batra, Prajjwal Bhargava, ShrutiBhosale, et 2023. 2:Open founda-tion and fine-tuned models. arXiv preprintarXiv:2307. 09288. Hallucinationdetection for generative large language models sequential estimation.",
    "EGHtrained on HaluEval87.23trained on SelfCheckGPT89.82": "(202. The latter reults demonstratehat our method posesses a certain degree of genealization. W chos LaMa-2-7BandOP-. Forthe HADES daaet, to alin the setings in thebaseies, we use BERT, RoRTa nd GPT-2 asthe base model, hich re the same as baseines. (2024) ndLei e al. We aso cmprethe results inWang et al. 8, resectivly. Fo the HADESdataset, sincethe HAES datasetos not hae apublicly availalelabelle tes set, we replicatedthe benchark experiments on hevalidaion set,wich serves as the baselneTraining Hperparaeters Our hallucination detectoris  th-ayer MLP, where the irt wlaers scal the feature dimension to half of tepreceding layer, and the final layer transforms iinto probabilities. Teanswr you gve MUST be Yesr No. (2023). The ReLU(Glorot e al. 7B s bas an-guage models for thefeature extracion step. since we need to leveagwhite-box inforation. Baselines For the HaluEvalataset, e adpt tLLM testmeths from the origina paper as thebaselins The baseline experiment utilized thefollowing prompt:You shuld try your bestto de-termne if the aswr containsnon-factual or alu-cinatd information accrding to the above hallu-cination types. Wealso compare he resuts in Zhang et al. or the HADES dset, we use the training ata f HADES to obtan thehllcinaion dtector model andtest it on the vaidation t. In Section. Frhe Self-ChckGPT Daaset, we use the results from theriginl paper as thebaseline. : Results on the SelfCheckGT dataset. We evluate four stae-f-te-art LLMs asbselines: GT-3(davinci), text-davinci-002, text-dvinci-003 and ChatGPT. We raintwo models ontw dfferent datases10% of he Sf-CheckPT and luEva. Addtionally, since our ethois a whi-box mehod,w sd the method frmAzaria and Mitchell(203 as a white-box aeiefor comarison with our aproach. e reproduced th ex-peiments in the baseline on our partitionees setas desribed above) as thefina basline result. Traing Pocess Forthe HauEval datasetdueto thr being ou tasks n HaluEval (r niformrresentaon,we loconsiderth general xam-ples as a task),we rained a separate model foreach task. 211)function is chosen as theactivation function InEquation5, is s to 0. Fr a specific task, We radmly selec10 ofthe data as the trainingset to train the ha-lucination deteto and use the emaning 90% ofth dat as the testing st or testingFo the Self-ChecGPT ataset, we condcte separate testwithand without training: the uraind tet t-lized amode tained on HalEval for ealuaton,while the trained test involvd training n 10% fthe dat from SelCheckGP and testing o the re-aining 90% of the data.",
    "D([Q, A]) = Difference(P(A|Q), P(A|0)) (1)": "Duig the generation of hallucinated outputs, webelieve that the modl receives less informationfrom Q, resulting in a smallr relationhip betweenthe odels output and Q. The gap D([Q, ) issmaller. where Differne() is he differeniable indicatorsrepreenting differences in probability dstributions,such as KL divergence, cross-entropy, ans on. Only he step 3 part (Haluiati Detector) in the figue undergoes parametr updates during the training process. The step 1 part of the figure represents th extraction process offeatur E=E(A|Q) E(A|0), while the st 2 part represents theextraction process of feature G = D([0, A]). Conversely, when the model otpts no-mally, it fully utilizes information from Q leadinto a larger gap D([Q, A])."
}