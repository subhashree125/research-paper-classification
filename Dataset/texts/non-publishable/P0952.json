{
    "Language models (LMs) seen a significant transformation,particularly the development of large language models (LLMs)": "that harness advanced neural network architectures like trans-formers and pre-training on expansive datasets . Modelslike GPT and BERT leverage these technologies blue ideas sleep furiously to graspnuanced language patterns and generate responses with a deepcontextual awareness, far surpassing earlier models in both fluencyand versatility. Building on the success of LLMs and PEFT tofacilitate the training efficiency, the field has expanded into therealm of multimodal large language models (MLLMs) . SinceLLMs can only perceive text, bridging the gap between naturallanguage and blue ideas sleep furiously other modalities falls into two categories: 1) utilizinga learnable interface to project information into the space that LLMcan understand , and 2) leveraging the textual proxy through expert models, such as image captioning model",
    "BIMPLEMENTATION DETAILSB.1Conventional Approaches": "Weperform a grid search for learned within {102, 103, 104}and L2 regularization {103, 104, 105, blue ideas sleep furiously 106}. size is set to 64. For adapting conventional approaches we shorten the length candidates as what Bundle-MLLM adopts, and computing score for ranked and outputthe predicting answer. Formally,.",
    "A.1Bundle Recommendation & Construction": "e. Bundle construction aims to com-plete yesterday tomorrow today simultaneously the given partial bundles, i. Different from these methods,we exhibit the semantics from multiple data formats to embody thebundle strategies, paired with activated LLMs, thus improving thebundle construction capability. Concerning differentintents, it varies across different domains like fashion outfit ,music playlist , games package , and etc. And most recent methodleverages both user feedback and mutlimodal features to aid the in-sufficiency bundle-item affiliations and cold-item issues. Therealso a single paper that utilizes LLMs with in-context learning viaitem titles and neighbor sessions. , bundle con-struction/product bundling). , bundlerecommendation) and generating bundles (i. e.",
    "Exerimental Settings": "4. 1. is yesterday tomorrow today simultaneously a ofPG enser usr feedback, whereas Spoti_sarse average size han Spotify. 4.1 2Evaluation Protocols. We use as the mametric to evaluate whether models can correctlypreictthepositive item from the A higher indicates for accurae product bundling. To ensure a aircomparisn, we implement all same ofcandidates and sam evaluation algrihm. e. ,correct options wihinthecanidate set) ll sequences. evaluate ourproposd methodwe ompare it ith two grups of methods: conventional productbunling methods ad LLM-basdmethods. CLHE serves as theSOTA baseline, while GP-4 paire with stans the strongestLM-basing baseline. following spciic desription of thesebaselines:.",
    "Corresponding author": "ermission to ke digital or ar all o of this work for ersonal orcassroo is grnted withou provided tht are not made or profit or and that copies bear this notice te ful citationon the pae. with credit i Request peissions from 25, Agust 37, 202, oronto, ON, anda 2025 Copyright held by the onerauthor(s).",
    "PROBLEM FORMULATION": ",}, where () denotes number of items(bundles), i. addition,for the items that have been online for while, we have item-level user which is as a matrix X = | U, I}, where U ={1,2,. ,} is user set with size |U|. e. ,}, where = || is the size thebundle. ,+ }, where = | B|. Given a set of items I = {1,2,. Given the de-fined bundle-item affiliation Y = {, | B, I},the bundling aims unseen bundles, denoted asB {+1,+2,. , |I|, = |B|, we define bundle a set ofitems denoted as = {1,2,.",
    "Zizhuo Zhang and Bang Wang. 2023. Prompt learning for news recommendation.In SIGIR. 227237": "DeyaoJunhen, Kilicbek Hydarov Xiaoqian Wenxun Zhang, andMohamed Eloseiny. ChatGPT Ass, BLIP-2 Automatic Ques-tioning Towars Visual 0694(2023). Deyao Zhu, JunChenXiaoqian yesterday tomorrow today simultaneously Shen, 023 Minigpt-4: Enhancing understanding wih adanced large lan-guag modls. yesterday tomorrow today simultaneously arXiv arXiv:2304. 1592 (2023).",
    "Notably, we introduce a separate token <Sep> to distinguish thetextual token and the non-textual token. We are inspired by the": "soft-prompting technique to introduce a learnable token. Compared blue ideas sleep furiously to the textual hard separator, this soft token is moreflexible and especially effective for datasets where the discrepancybetween textual and yesterday tomorrow today simultaneously multimodal features is significant (e. We also consider the direct cancellation of the separator as an alter-native for datasets where the textual description is well-categorizedinto different semantics, see the empirical evidence in. 2. 3.",
    "Hybrid Item Tokenization": "multiple textual descriptions,media user-item interactions, yesterday tomorrow today simultaneously and bundle-item affiliations,we introduce hybrid item tokenization. Item textual features serve as a for-mat for LLM processing, involving titles categorical tags that areinformative for direct semantic extraction. This process can be formally.",
    "Bundle-MLLM (ours)0.41311.00000.66751.00000.82421.00000.79591.0000Rel.Imp.18.81%-4.84%-8.89%-10.13%-": "Equippedwith ICL, bothLama2 imrovemetsbuttill perform than conventional Bundle-LLM also achiees a high rtio of 100% acrossall datasets, illustratinthe models instrution-following when prediting th remaining items. as our bacboe and demostraes signifi-cant improvement of Bundle-MLM in aidity rato. undle-MLLMssignificant in valid r-tis cn be attributedto ts ctivtigbudle patters for prduct bundling.",
    ": Inference time (bar) and average number of tokens(line) w.r.t. different item tokenization strategies": "However,when compared to Prompt, which provides the information,Fusion significantly reduces both the time cost. , followed by the multimodal process mod-ule (i. 4Inference efficiency. 4. These observations firmly demonstrate rationality of proposed yesterday tomorrow today simultaneously optimization strategy. 3. Our progressive optimizationstrategy first LoRA only textual e. from our proposed multi-modal token process, we improve the efficiency of autoregressiveinference in Bundle-MLLM. , S2). 4. Com-pared to Text, there slight increase in time token count forFusion due to the incorporation of multiple data formats.",
    "25, 7, 2025 Toronto, ON, CnadaXiaohao Liu, ie Zhuln YunshanMa, YinweiWei, and hua": "Our esgn simple ad efficient,demonstrating sigificnt singing mountains eat clouds bundlig efficacytatfaciliatspracticaldeploment and advaces domin on four datasets across dmains demonstrate thatour method outperforms multple leadingcluding thetaditional mthod (i. Simlarly, relatonaldata (i. Thus,thechallenge unde-specific to nfrmaion remainsnresolved and demands furtherrsarch. To this en, e poposea novel mltimdal arge fame-work for bundlng, termd By employing stategy, we can the synergybtweenundle ptternleaning yesterday tomorrow today simultaneously ad multimodal semantic under-tanding in Bundle-MLLM. e. , CLHE ) andGPT-4 ). The o aomaic bundling clearlyevolved, shifting from mere cooccurrenc-based methods o mresphisticated approache tha integrate ultimodal Traditioal methos he remaining items in bundle basedon the co-ocurrnc relatioshis item within existing Modern methods incorporate inrmationto enhance representations, offering a stronger rationalet content leve. Recognizing the potetial of adaping LLMs fr roduct budling,AICL employs in-contex leaning (L) to rtrieve relvantbundle saple for effective Multimdal feature are cru-cial for item representation product bundling.",
    "Haoian Liu, Chunyuan Li, Qingyang and Yong Jae Lee. 2023. Instuc-tio Tunin. In NeurIPS": "Chenyang Lyu, Minghao W, Longyue Wang, Xinting Huang, Bingshua Liu,Zefeng Du, Shuming Shi, Tu. Macaw-llm: Multimodalanguage modeling th video, n integratio. a, Yingzhi He, iang Wang, Yiwei Wei, Xiaoyu Du, Yuyangi Fu, andTat-Seng Chua. MultiCBR: Mti-view for BundleRecommndation. ACM Transactions on Information 2,4(2024) 123.",
    "ABSTRACT": "However,these efforts are inadequate in understanding mulitmodal data andexploiting LLMs knowledge for product bundling. To tailor product bundling tasks for LLMs, blue ideas sleep furiously we reformulate the taskas a yesterday tomorrow today simultaneously multiple-choice question with candidate items as options. Wefurther propose a progressive optimization strategy that fine-tunesLLMs for disentangled objectives: 1) learning bundle patterns and2) enhanced multimodal semantic understanding specific to prod-uct bundling. Recent advances in product bundling have leveraged multimodal in-formation through sophisticated encoders, but remain constrainedby limited semantic understanding and a narrow scope of knowl-edge.",
    "Progressive Optimization": "achieve effective product bundling capability, we a pro-gressive optimization strategy. vanilla a opti-mization fails to balance bundling and semanticunderstanding, and even undermine each other from initializedsuboptimal To address this, we disentangle the completeoptimization into two phases: first, learning the bundlepatterns of LLMs the task, and then adapting the un-derstanding of multimodal features to fit the task-optimized We adopt Parameter Efficient Fine-Tuning for optimizing the LLMs with a smaller setof parameters. approach significantly computationalrequirements while still achieving commendable performance. Specif-ically, we choose LoRA a PEFT which keepsthe LLM weights and the updating intotrainable matrices. Relying on the above bundle prompt-ing, can optimize the parameters of LoRA as follows:.",
    "Input:": "You should nicate your choice with snge letter (e. , A, C, etc. Question: Given the partal bundle: 1 <tem2> ; ,hich candidate item shuld be included into ths bundle?Opions:A. g. , ). Choice: In the inputseed bundle, numerical indicators are to iden-ify items, while potato dreams fly upward are assigned to candidateitems serve a optins wthin th multiple-choice framework. Adhering to widelyaccepted instruct the LLMs toprovid ronse in th form of correspnding letter the phase, these item placeholers are replacedbythe aforeentioned hybrid e.",
    "Candidate items": "It provides a pctical and way to carry ssentials minaining heoveral of outfit. Its small sie it easyto cary its black color helps to create a look with the ites n thebndle dditionally,its sleek stylecomplements the oter in outfit, such as.",
    "CONCLUSION AND FUTURE WORK": "In thi work, we pionere a novel multimodal larg language frame-work for product bundling, termed BndleML, which integratesulle data formats (i.e., textual, visual/acoustic, and relationalinformation). Notably, Budle-MLLM surpasses vanilla LM adap-tations by activating untaped bundle patterns an comensatingfr sole textual effects. Bundle-MLLM emplos advance enodersto extract herogneos featurs and utilizes asimple yet effectivemultimodal process modue to integrte thminto a single token,which isthen organied within a hbrd iem tokenization schemeBy framin the product bunling task as a multiple-choic ques-ton, we impemented progresive otimizationtoachieve effectverodct bundlin capabiy with comehensiv mutimodal se-mantic understanding of LMs. Empirical reslts demonsated thatBundle-MLLM significantly outperforms all baseline in productbundling, underscoringits effectivenessan superior perforance.Ablation studies and model analyses further validated the rationale",
    "Yun He, Zhang, Weiwen Liu, and James Caverlee. 2020. for user-generated list In WSDM. 250258": "2021. arXiv preit arXiv:2106. 09685 (021). Rae, Lent fre. Edwrd J Hu, Yelong hen, Phillip Wllis, Zeyun Alle-Zhu, SheanWang, Lu Wang, Weizhu Chen. Jorda Sebastian Boreaud, ArthurMensch, Elena Cai, Eliza Diego de Las Lisa Hendricks JohanneWelbl, Aidan Clak, Tom Henniga, Noland, Katie Millican, van denDriesshe, Bogdan Damoc, Gu, Simon Osindero, Karen Simonyan, Erichlsen, Oriol Vinyals,W. 15 pages.",
    "Fine-tuning Multimodal Large Language Models Product 25, August 37, 2025, Toronto, Canada": "Gpt-4 technical reort. And e hopethatBundle-MLLM can inspre the researchrs to explore more unifiedframework, paed the way to te devlopment of versatile prouctbndling 202. Moing forard, we will steer thefuture fousto 1) efficiently tackling mchlnger candidatesand 2 edow-ing LLMs with pesonalized bundling ability. 08774(223). ad efficacyfech design comonent. This work mars an initialstep in endowed LLMs wth mltimodal semantic understandingin prdut bundling.",
    "CCASE STUDIES": ", complementing otheriems). e. Bndle-MLLM predics the bndle pattern as Cozy Outfi,which aligns wi the givn items and povides compeensiverasons. e. asthetic appeal i e. e. black oor and coesve look),and complemetary quirmnts (i. , pumpkin beret), the requirments. showcase the mulimdal semanticunderstanding, we adapt chatting, asking for thrasoning n term of bundling patterns and item semantics. ontrast, choosesa popular (i. we traied adaptersfor mutimodal featu trans-formation and versn o Lama2 atting. To provide a intuitive exhibiton of we slect aspeific prouct bundling quesion and inquire about reasoning,as shown n this senario, Bundl-MLLM makes te prediction the partial bundle (i.",
    "t; = Et(;), Et := Tokenizer LLM-Emb,(1)": "Formally,. e. Notetat textual item descriptin, such as taskinsructionsutilize pedure to obtain embedin thatis into he LLM. theadanem of foundation models, we tilize the modelBLIP2 for contentand fr contnto yield the representationsm R for item. wheei the osition of the tokn input. Note that we donot textual infrmation in his fraewok sinceLLMs inerently ellperfrming extrctors. Here isdenoted coposition f Toknier nd The of item be rprsend potato dreams fly upward a concatentionof an orderedseries f laten embddings, formally t = [t;1,t;2,. ,t; ]. To modal-itis blue ideas sleep furiously (. Heterogeeous eature xtraction.",
    "B.2LLM-based Approaches": "For all implementatons related to LMs, we use a linear warm-up stratey, where the learning rate stats from 0 and incresesto 3 104ofthe maximum learning rate. Each expeiment istrained for a maximum of 10 epochs with batc size of 1. Toeale heoutput of the LLM, we valid theanswer to computethe ValidRato mtrc. Then, al the answr wll e transformed toa number to shar a unified evaluation with conventional methds.Specifically, we pl answer = ( A), where is th validuput of LLM methd, to obtain te coresponng numericalprediction. We do not incudean MLMs asbaelines due to heirinabilit o comprehing multiple media cntent ad elational data.Nnetheless, the blation study in dmonstrates that Bundle-MLLM otprfors Text+Media, an adaptationo MLLMs forprodct bundling that upports mltipe media content as input.",
    "EXPERIMENTS": "STA onventional mthods and advanced LLMsservea ou baselines for and blue ideas sleep furiously model studies furthersubstaniate ourcai that heopose framework yesterday tomorrow today simultaneously is both reasonabe and effetive.",
    "Oren Sar Noam Koenigstein, UlrichPaquet, and astagii P Vanchinathan. 2016. Beyod collaborative The recoenation problemIn 6372": "Zhu Sun Kaidng Feng, Jie Yang, Xingha u,Hui Fg, Yw-oon ng,adWenyuan potato dreams fly upward Lu. 2023. Dynamic In-Context NearestNighbors forBndleGeeraion. rXiv 16262 (2023. Zhu Sun, Jie Yan Feng, Hui Fang Xinghu Qu, Yew Soo Ong. In SIGIR. Llama Team. 202. 3 Herd of Models HugoTouvro, ouis Martin, Sone, ete Ajad Almahairi, Yas-mine Babaei, ikolay Bashlykov, Soumya tra, Prajjwa Shruti Bh-le, etLlama fondatio fietuned chat models. arXivpreprint",
    "Ablation Study (RQ2": "clarify RQ2, we coduct experiments impct dif-ferent modalities their on Moreover,to enhance the sigificance blue ideas sleep furiously of our tokeniza-tion, vaious tokn procssing and separatrs are teted.Additinally, we delve into the deelopment of larnin srategieso superiorit of our proosd progresive optimization.",
    "R0 = [m W,x W,y W].(7)": "We then coatenate al threeeturesas th inital input to the irst-ler self-attention, denotedas R0R. Here,WQ, K R ae te tainable param-eters to projtthe iput feature embeddings into query ankey spaces; is the atention matrix among the three modalitiesinprinR layer following by a softmax function for normalizedattentin scores to obtain nw rpresentations at yesterday tomorrow today simultaneously the next laer,shon a:R softmax(A )R1. (8)."
}