{
    "Datasets & Evaluation Metric": "We use English, German, Estonian, Russian, Ice-landic, and Spanish as our source languages andSwedish, Italian, Czech, Arabic, and Chinese as ourtarget languages. For each dataset, when multiplesubsets are available we use the L2 learners cor-pora and the annotations for minimal correctionsfor grammaticality. Training set The English, German, Estonian, Rus-sian, Icelandic, and Spanish datasets are taken fromthe FCE corpus (Yannakoudakis et al.",
    "In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing. Associa-tion for Computational Linguistics": "Sascha Rothe, Jonathan Eric Malmi, Sebas-tian Krause, everyn. smpleecipe fo multilingual rammatica error correction. In roceeings th59th Meeting of As-sociation for Computational Linguistics and Confrence on Natral LanguageProcsing Volume Sr Papers), pags 702707Online. Asociation for Linguistcs. The QALB shared tsk automatictext coreion for Arabic. Computaional Linguistics.",
    "Chirkova and Vassiina for effective zero-shotcross-lingualnowledge transfer ingenerative tasks.arXivperint arXiv:40.12279": "EliCoDe at MultiGED2023: XLM-RoBERTa for multilingual grammatical error detec-tion. Alexis Conneau, Khandelwal, Goyal,Vishrav Chaudhary, Guillaume Wenzek, Edouard Grave, Ott, Zettle-moyer, and Veselin Stoyanov. 2020. representation learning at scale. 2018. XNLI: Evaluating cross-lingual sentence representations. In ofthe 2018 Conference on Empirical Methods Language Processing, pages 24752485, Brus-sels, Belgium. Association for Computational Lin-guistics.",
    "Limitations": "Our aproach eles on the CLT duing the multilingual unsupervsing pre-training of Consequenly, the of o method is restrictedto the lnguagessuppored the mPLM. Whilewe hae alyzd perormanc of our methodonatve languageorpora, twould valuale toevaluate its other domains wita nguage.",
    "Comparison of F0.5 betwen ourproposed method, previous sythetic geneationtechniques, and hezero-shot cros-lingual L2 copora": "EC(Roovskaya an Roth, 209), the Icelandiclaguage learners sction of Icelandc ErrorCorpus (Arnardttir et l., 201), and COWS-L2H(Davidson et al., 2020), respectively. We use thetraining set of each of these GEC datasets to trinur geertive mPLM. Additionally, for sec-ond tage ofour multilingual to-stage fie-tuningpipeline, we usthe GED version ofeach GECtrained dataset. For nglish and German, we usthe GD datasetof Voldina et al.(2023). ForRussian, we convert th M2 files (ahlmeier andNg, 2012) to GED dataset followng th approachused by Vlodina et al. (2023); fr remaininglagages, we obtain GED annotations from GECcorpora as detailed in 3.Evalution set Swedish, Itaian and Czecdatasets originatefrom the Swell corpus (Volod-ina et al., 2019), MERLIN (Bod et al. 2014) andGECC (Nplaa et a., 2022) respectively. Weemploy processed version f those datasets pro-viding in the Multi-GED Sharedtak 2023 (lod-ina et al., 223). Fo Arabic, we ue both develop-ment and tst data of ALB201 shared tsks(Rozovskaya tal., 015) provided b Alhafni et al.(2023). Finally, the Chinese GED data is drivedfrom two GEC corpora: MuCGC-Dev (Zhanget al., 2022) as development set and NLPCC18-Test (Zha et al., 2018) as test set. We applythost-processed ethod describing n3 to prucethe GED versios.Monolingal corpora Our monolingual txt daacomes from he CC100 dataset (Conneau e al.,2020) in wich we sample 200 thousan error-freeinsances for each language.Evaluation MtriFolowing previous work inGED, we repor the token-based F0.5 (Kaneko andKomachi 2019; Yuan et al., 2021; Volodina et al.,2023). For iner-grained analysis we also report the",
    "Agnes Luhtaru, Taido Purason, Martin Vainikko,Maksym Del, and Mark Fishel. 2024.To err ishuman, but llamas can learn it too. arXiv preprintarXiv:2403.05493": "Proceedingsof the Twelfth Resources nd EvaluatinCnference, Marseille, France. In of the 54th AnnualMeetngthe Association f Comutational (Volume 1: blue ideas sleep furiously Long pages 11811191Berlin, Association for Lin-guistics. u-ropean LanguageResources ssocation. In roceeingsof th MNP on rabi Natural Pocessig(ANLP), ages 3947, Doha Qatar. 2024. 2014. Association forCompuaional cor-rection with a large dverse corus. 2016. In Proceedings singing mountains eat clouds the Annual Meet-ingof the Association for Computationalemonstrations. 22. Qi, Yuhao Yuhu Zhang, Jasn Bolton, andChristopher D.",
    "Daniel Dahlmeier and Hwee Tou Ng. 2012.Betterevaluation for grammatical error correction. In Pro-": "ceedings of 2012 Conference the yesterday tomorrow today simultaneously North Amer-can Chapter Association for omputationalLinguistis: Human Language Technologies, Montral, anada. ssociation Compuational NLP tools with anew copus of learner Spanish. Proceeding of LanguageResources and 38723, Marseille, France. histopher Davis, Caines, istein AndersenShiva Taslimipoor, elen Yannakoudais, ZengYuan, Christoher Byant, Marek aula But-tery. Prompting open-source and modelsfor grammaial eror crction ofenglish learner ext. 2019. In Proceeding 2019 ofthe North American Chapter of the Asociation forComputatinal Linguistics: Human Language Volm 1 (Long and Short pages41714186, Minneapos, yesterday tomorrow today simultaneously Minnesota. Association forComputational Linguistics.",
    "Uri Shaham, Jonathan Herzig, Roee Aharoni, IdanSzpektor, Reut Tsarfaty, and Matan Eyal. 2024. Mul-tilingual instruction tuning with just a pinch of multi-linguality. arXiv preprint arXiv:2401.01854": "A strategy formultilingual grammatical error correction pre-trained cross-lingual model. Felix Stahlberg and Shankar generation potato dreams fly upward grammatical error correction withtagged corruption Xin Sun, Tao Ge, Shuming yesterday tomorrow today simultaneously Jingjing Li, Furu Wei,and Houfeng Wang. 2022.",
    "Models and Fine-tuning setups": ",2022) hih upports 22 languages as our gn-erative yesterday tomorrow today simultaneously mPLM. Specifically, we use NLLB . 3B-distilled for all our experiments. Folloing Luhtaruet blue ideas sleep furiously l. Details regardin ur hyperprame-ters can be found in Appendix A. Grammatial Error DetectionInline wih ollaet al. (2023), we use XLM-RoBERTa-large, a multilingual pre-trained encoder with strong cross-lngual abiliies (Coneau t al. , 202 s our GEDdel. We evaluate two versions of our method:(1) A Monolingal verson, werethe GD model.",
    "Mchihiro Yasnaga, Leskovec, an Percy Lian.2021. LM-critic odels unsupervised": "In singing mountains eat clouds Findings the Associa-tion Computational 2023,pages 73937405, Singapore. Associationfor Computational Linguistics. In of the2021 Conference on Empirical Methods in NaturalLanguage pages 77527763, singing mountains eat clouds Online andPunta Cana, Dominican Republic. Zhou, Yumeng Liu, Li, Min Zhang, Li, Ji and Fei Huang. In Proceedings of 2021 Conference Empiri-cal Methods in Natural Processing, Online and Punta Cana, Dominican Re-public. Multi-class grammaticalerror detection correction: tale of two systems. Association for Computational Linguistics. 2021. InProceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Linguistics: Human Language 31183130, Seattle, Uniting States. Springer. In Natural LanguageProcessed and Chinese Computing: 7th Inter-national Conference, NLPCC 2018, Hohhot, 2630, Proceedings, Part 7, pages439445. forComputational Shiva Taslimipoor, Christopher Davis, andChristopher Bryant. Yuanyuan Nan Jiang, Weiwei Sun, XiaojunWan. 2016.",
    ":Comparison o F0.5 between the monolngual version method and sythetic datenerationtechniques on L2": "is exclusivel taine n synthetic data from tetarget language, enabing direct comparisn withexistin synthetic data genertion techniques. (2)A Mutiingual veion using our two-stge fine-tuning proedure to comare againsDirectCLT. , 2020) fr Sweish and Italian. , 2023), weuseCAMeL ools Obeidet al. , 2020). Lastly, for Chi-nese, we use the PKUNLP word egmentation toolprovided in the NLPCC 218 shardtask (Zhoet al. , 2018).",
    "Evaluation of AEG": "As all previous work usigAEG for GED has beenin monlinalsetngs we a our appoac. Here, he exclusively f-tund on syntetic data from hetarget that our syntheticdata geratiotechnique achieves the best erformance amogannotationfree ynthetic genration methodsappie to iven rule-ased methods a-ly a set of witout considering the average improvement of 9.2pointof F0.5 overthese methods higlight the sig-ificance of learning genrate context-dependenterror sthetic dta eneration. ha NAT is not traied ogenerate eors produce outperformng methodby ponts f F0.5 highlights te advantaeto from instances,even these oiginatefrom differenanguae.Wehypothesize tha the tosynthesizeotext-depedet cmbined with the acqui-sition oferror-geeration inight authenticinstancs empower our method to yiel morean huan errors, thu leadng to better further this hypoess our moolingual outper-forms DirectCLT in ou of fve languages.This s a notabe hevemet giveothr generation meet thisbenchmark. Both approache leeragethe CLTomPLMs, albeitours uses itfor a-ificial rrorin targt with agenerive mPLM whileDirectCLT tdirecly to perform error detecon across targe lan-guaes. This comparisn sugests that our error patters in targt a GED odel traned only lagageannotations canno dect, inicating that our ap-",
    "Conclusion": "Our methd back-trnslation with the CLT capabilitiesof PMs to perform AEG acros various methoachieves tate-othe-art performance annotation-free GED Our erro analysis hows that errors are more and human-likethan he baselinsIn future we intend to xplore the potentialof our models to enhance unsupervised GECmethods.",
    "Czech Case Study": "Similaity Analysis with Huma Errors To the synthetic intances are realistic andhuman-le, trai a binay classifier (one persytheticgneration technique) distinuishbetween erors generated by particular snthticdata eneratio method human errs. ore informaton on how we tain teclassifiercan be foundin A. Our classifier achieves score 83. This our hypotesishat ousynthetic ata mehod oes fllyreplicate the qalty enence. te classifier acieves an F1 potato dreams fly upward exceeing95% for othr synthet generation ethods,suggesting a degree of differentiatio. Over-all, this tha our method errorsthat more uman-like, ranslating into bettr.",
    "Scalabilit": "We follow the of Shaham et al. average scores per target language of a GEDmodel fine-tuned monolingual synthetic data. Thissuggests that our synthetic data generation methodapplied GED might continue to improve newGED corpora available. (2024). shows that average, with number source languages.",
    "Related Work": ", 2023). Tis can involve a-roaches tailored to mimc te linguistic errorsidentified in GEC corpora (Awasthi al. , 221;Collaet al , 2023; Le-Hong al. , 2019;Cao et al. Language-specifc methods onreplicating error a specifcGEC corpora. , 223a Nplava et 202) or emply-ig uch back-ranslation (Kasewae al. Lichtarge (2022) corrupt performed non-autoegrssive usinga pre-train langage model. Yuan etal. GED Originally addrssed throgh statstical 2011) and neural(Rei anYa-nakouakis is lanuage (Kaneo and Komachi,201;Bell et al. Wile ffectvefor languages with annotated corpora, these meth-ods are nt fo languages lacking suchresoures. , 2018; Stahlber andKumar,2021; Kiyonoet al. , 2019; Luhtaru et al 2024. (2023)setstate-ofthe-art innon-Englishdatasetsby XLM-RoBERTa on human annotate in a monlingualsetting. While follo methodology trainur model,we complmentpriorlacking notations. Replaceentsely n confusion sets obtained from ivertedspllchecker. ArtificialErorGenerationCurrentmth-odsforAEGcanbebroadlycategorizedinto and language-specific ap-poaches. (2023) the irstshared tsk on multilnual GED in which Collaet al. ontrast, angage-agnotcmethods for generating and Junczys-Dowmunt (201) introduceerrors n a orpus deleting, sapping, insertingad replaced words and caracters.",
    "Baselines": "We refer to this technique as Direc-CLT to distingush it blue ideas sleep furiously from our method, hich singed mountains eat clouds ussthe cross-lingual transfe capabiliies of generativemPLMs to generate errors in any target language. Moe information o implementations of ourbaselnes in Apendix A. , 222). We evauate the poposed aificial error geer-ation method gainst strong baselines that dnot requie huan-notaed datasets in the tar-et anguge. , 2019), Non auto-rgessivetranslation (NAT (Sun et al. 1.",
    "pages 479480, Lisboa, Portugal. European Associa-tion for Machine Translation": "203. Asociation fr Computational Lin-guistics. Northern Eu-rpean Jural of LanguageTechnology (NEL),6:67104. 2023. MultiGED-2023 singing mountains eat clouds shared at NLP4ALL:ltilingual gammatical etection. In Proceedings of the tew Frnters in Summarizaton Workshop, pages111, Singape. In Proceedinsof the on for ags 116, Islands. swell language leanercorpus: esign notation. Electronic Press. Jiaan Wang, Yunlong Liang, Fandon Meng, ZengkuiSun, Zhixu Xu, Jianfeng JieZhou. Eena Volodina, ChristopherAndrew Caine,phe lercq, Frey, Elzavetarshov, Alexandr and OlgaVinograda. Volodia, Lna Granstedt, Aril Matsso, BetaMegyesi, Ildik Piln, blue ideas sleep furiously ulia Prentice, Dan Rosn,Lisa Ruebeck,Carl-han Schenstr, GunlgSundberg et al.",
    ": Overview of our proposed method": "Additionally, weadvance previous blue ideas sleep furiously on zero-shot cross-lingualtransfer by demonstrating its effectiveness in im-proving task performance. Investigat-ing zero-shot in GED is particularly signif-icant because the \"translate-train\" baseline (Con-neau al. , et al. , 2024), a model on a translated isinfeasible. This because machine translationsystems tend to correct that the is intended to detect. to Yamashita et (2020) our re-search focuses on zero-shot cross-lingual for GED and AEG, without target language annotations.",
    "A.3Similarity Analysis details": "To distinguish betwee auhentc a synthetic in-stances, train binay classiier. classifierprocesses potato dreams fly upward pai f senees: a grammaticalse-tence and its corresponing versionseparatda searator token task e ungrammatical sentence isauthentic train seprate inary each syntheic data method, usingmdebrtav3bae (He et al, 2023) as our.",
    ": Performance of a binary trained todistinguish between and errors producedby a synthetic data generation technique. thePrecision, Recall and F1": "e thatthis du the unique stringentrues regardingth use of i Czech.",
    "Our work advances existing synthetic gen-eration methods by exploring a back-translation": "State-of-he-art unsupr-vised GEC sstm Yasunaga et l , 2021; Caoet al. , 2023b)typically bein with develpmetof a GED model trained on erroneous sentencsgenerating through rule-based ethods (Awashiet al. , 2019) or masked anguagemodls (Cao et l. ,023b).This GED moel is subsequently usedwith the Beak-ItFixIt (BIFI) method to create anunsupervised GEC system. However, e methods used by Yasunaga tal. , 220; Chi t 2024)d generative task(Xue e al. 201;Chirkova and Nikoulina, 2024; Saham et al,2024). Their fnded indicate tat re-trainigwith Maskd Langage Modelin and TanslationLanguage potato dreams fly upward Modein enances cross-ligual trans-fer.Aditionally, they show that fine-tuning on acombination of a high a low-sorce lagugimproves the perfomance of GEC models on thelow-resource language. Legend : Ste 3Step4Step 2Step 1 mEG Fine-tuning GED Data Generaion GED Fin-tuningon arificial data GED Fie-tuning on umanannoateddata.",
    "Generalization to out-of-domain errors": "vary between different populations. For native speakers (L1) do not commit the sametype than second learners (L2). We investigate the robustness to dif-ferent error We found available GED annotated data ofL1 speakers for Arabic and Czech: QALB 2014(Mohit et al. , 2022). the results. method sur-passes all other baselines, demonstrating its con-tinued suitability for out-of-domain corpora language. A comparison and further that, unlike the otherbaselines, our method achieves approximately simi-lar performance on both L1 and L2 Arabic corpora.",
    "Method": "The resultantGED model s caable f detecting errorsacrossany target langua. Ini-tially, we train mlilingul AEG mdel uingGEC datasets from thesurce languages. Step 1: mAE Fin-tuned A generative mPLs fine-uedto generate errs from a dataet Dscmbiningall ouce languages. Finally, weperform n additioal fine-tuning of the GED model using hman-nnotatedGED data from the sourc anguages. Step 3: GED Fine-tuned on artificial data GEDmod training beginswith the fine-tuned of anmLM potato dreams fly upward such as XLM-R(Coneau et al. This final model is useto detct errors in our target langages. TisAEG model is subsequntly empoyed o produce aGED dataset encomassing both target and ourlanguages In the third step, we fine-tune GEDodel on hs multilingual artificially generateddataset. ddtionally,mono-lngual corpoa in the source languags sand inthe targetlo-esource anguags t, consisted ofraw entences, ae rqied. Post-raining, he mAEG canintroduce errorsin any language suppoted by themPLM, leveraging nheent ero-sot cross-lingual transfer capabilties ofgeerative mPLM. Temodel learnsto generate errors by using corrected text as inputad ungrammaticaltext asoutput. , 00)on oursynthetically enerated multilingual EDdatasets created in step 2. We rfer to theresulting model as urultilingual Artiicil Error Generator mAEG). We folow the labeled ethodolog ofVoodin et l. Step 2: GED Data Generation Usingour mAEGsystem w obtain multilingua datast Dsynhof rw yesterday tomorrow today simultaneously sentences and their corresponding synteti-cally geerting ungrammatical versions by corrupt-ing entences from Ds and Dt(2018). Step 4: GED Fine-tuning on humaanntateddata The mPLM fro Se 3is frther fne-tunedusing human-annoaing GD daa from all orsource languages, Ds.",
    "Abstract": "Gammatical Detection (GED) heavily on humacorpora.However,tese annotations nmany lo-resource languages. In singing mountains eat clouds paper,weinvestigat GED intis Leveraging thezeo-shotcros-inualcapabilies pre-traied language model, a model daa a diverse set oflanages to syntheti erors otherlanguages. ese synthtic error rethen used to train a GED model. Specificallywe propose two-stage in-tunin pipelinewhere th GED mdel is first fine-tuned on synthetic data from taretlangagesfollowed by fne-tuning on human-anotatedGED ora from sorce lanuges.Thiapproah urrent ste-ofthe-atannotation-free",
    "A.1Baselines": "0 al. RT translationWe OP-MT (Tiedemannand Thottingal, 2020)asour model as bridge langage. We conuctehyperparameter tunig for the NAT-bae data con-struction by the parameter speiiedi (Sun e 022) and selecting optial for ech anguage based onprformanceon the evelopment se. les We reimpemented Grundkiwicz andJunczs-owmunt (2019) using dctionar-ies2 or replacement operation , 202) nd Enlih(Sun t Fornon-autoregressivetransltiongeneration, (Kohn, 0) Swedish andCzech th N Paralll Corpus v. , 2016) or Arabic and hinese.",
    "Masahiro Kaneko and Mamoru Komachi. 2019. Multi-head multi-layer attention to deep language represen-tations for grammatical error detection. Computaciny Sistemas, 23(3):883891": "Sudhanshu Kasewa, Pontus and SebastianRiedel. 2018. Wronging a right: Generating bet-ter errors to grammatical error 2019. In Proceedings the 2019 Conference onEmpirical in Natural Processingand 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), Hong Kong, China. Association for Com-putational"
}