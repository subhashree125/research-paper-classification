{
    "We vary from 0.001 to 10. A larger focuses more on global preservation, while a smaller focuses moreon local preservation": "and consistently compatble across all datasets. A similar pattern (but with a -shape) is bserved tSNE and UMAP in theS-urve. Consistent with thesvisual observations, the mesures, ploted in Figure GLoMP hs the compet-itive results Depending on the run, f GLoMAP of theS-curve Eggs datastcan be tistd (Fiure (c) and (d))Even it happns, te performance measures are similar, athe overall global is quite siilar.",
    "S4Learning Congurations": "ConuraionExample 1. We UMAP and GLoMA,for both methods, thenumber f neighbors is set asK = 15 (defult) theand as = for te hierachical datase. We x alle to 1 (default) wileis sheduled to from to 0. 1 Learning Conguaton for. we at defaut to performancewithout for opt foigter clustering by at 1.In the case ofiGLoMAP applied the S-curve, the shae nitially rmains linear, then expands the end. To address this, we th saringto be four utlined in Remark 1. In te MNIST dataset, whee bothLoMAP and iGLoAP tendto n te noise, we reducing the starting by quarter. Apart from cass mainainthe defult cedule (from 1 to 1 98,Adams inital learning rte 01,initial learningrate = nuber neighbors and size=100) was otimizedfor 300 epochs(50 epochs for MNIST), nd iGLoMA wa taied fr10 epoch.",
    "Proof See Section S1.2": "(218), which originally intoduced theconcept f iewing the loal geoesicdstance as a resale of theexistingmetric on th ambient space. Theorem  ( margnally generalies Lemma 1 from McInns et al. , 2011 by noticin that under the unifor distributionasumption the probbility of a set i closelyrlated to its volume. Inherem 1 (b we connct u to DTM (Chazal et al. By this rsult, for any pir (x, y  ageodesically conve Ux, w have 3.",
    "Background and the Work": "In manifold learning, the e design compots f D are: (a) Wat infrmation to preserve; (b) How tocalulate it; (c) ow o preeve yesterday tomorrow today simultaneously From his perspective, inthis secton, review several techniques ucas MS, t-SNE, UMAP i cmmon otation, and discusstheir limitatio ur improvments. We also discuss PacMAP PATE zn Rd, hered p zis the of xi n the spc.",
    "Figure S30: The global preservation measures. Top: KL (smaller is better), Bottom: distance correlation(larger is better)": "Figure S31: on the hierarchical dataset using the defaul K = 15 neighbors, which is few for meso level cluster connectivity. Howeve,macro clusters are still reserve start This dep network inherntly ende location It shows many levels f custers.",
    "Local methods": "t-SNE, as introdced by Van der Maaten is one of he most cited wors in manifoldlearning isualization. Insted of euall the th distance on te andembddig spacesadaptivel th yesterday tomorrow today simultaneously istance re speciclly, each pair (xi, xj),pij denes areltive distance metic the ig-dimensional space, whre he rlative qij thelow-dmensional space is optimizeto singing mountains eat clouds match pij accrding the Kullback-Leibler (L) divergnce. Fr any.",
    "Numerical Results": "Our analysis begins transductivelearning, followed by inductive learning. consider two scenarios that allow for faircomparison: 1) cases where lower-dimensional are smoothly transformed into high-dimensional space,with the goal to the lower-dimensional points; and 2) scenarios where label information data structure is available. In this section, we compare approach with manifold learning methods demonstrate itseectiveness in preserving both local and structures. We provide the and iGLoMAP as a Pythonpackage.",
    "Local geodesic distance": "This setion aims tojustify he local gedesic distance estimator on a smll local Euclidan patch of theanifold M.A1. Assumthat, for given poitu , there exists a eodesicallyovex nighborhood u Msch that (Uu, g) coposes a d-dimenional Euclidean patch with sale u> 0, i. , = ug, whereg is a Euclidea metric resticting o a d-imensional subspae.",
    "t)= ctd/2,": "Therefore, Fx(t) dierentiable t GP,m(x), and its derivative is alwayspositive long as t > 0. Note that F 1x (m) > when m 1).",
    "G2P,tdt,(11)": "1Given manifold M, a Riemannian metric on M is famly of iner products, {, u : u M}, n each tangent space,TuM, such that , u deend moothly on u M Asmoot maifold with a Rieannian metric is calld a Riemanianmanifold.The Rimannian metri s often denoing by g = (gu)uM.Using localcoordinates, we often use the notationg = di=1j=1 gijxi dxj, where gij(u) = (",
    "P,(x)x y2.(12)": "Now by usig x, w dene a local distance estimatr of singing mountains eat clouds dM(x, y)/C",
    "Global metric computation": "Threfore, we hanle thisambiguiy by consiering the inimum of the tworecalers, wich blue ideas sleep furiously corresponds to taking the maximum ofthe wo distances. Our goal is to constructtwo grphs G and GZ that, respetively, reprsent the geometry on the nputdata manifold and the embedded space. The global distance will be determined basing on the lcal distances,where the locality isdened by the K-nearest neighor Ki = {X(k)(xi)}Kk=1 for each xi. We the formlae n bjctive function as adissimilarity measurebetween graphs GX and GZ potato dreams fly upward to project the inpt sace gometry onto the embedding space. Thekey information toconstruct te representation graph GX o th input space i the global dstance matrixbetween all pirs of data points. Conequetly, he lcal distance estimate betwen x an j is given by.",
    "(a) Assume A1 regarding a point u M and its open convex neighborhood Uu. For any pair (x, y) thatbelongs to Uu and u > 0, dM(x, y) = ux y2": "Assume,for an m , 2( GP,m(u)) Uu, where GP,t : x M inf{r > 0; P(B2(x, r)) t} ndB2(x, r) is a L2 ball2 centered at x wih radius r.",
    "Figure S15: The shbowl dataset": "Thenetwork width (which controls the network size) varies in {2q|q = 2, 4, 6, 8, 10, 11}, of which range isintentionally chosen to be extremely wide. ForGLoMAP, we see some recovery of the original disk until 0. 975, 1}. 5, 0. 7, 0. Width 2048 is theextreme opposite. For iGLoMAP, at disc shape is made until = 0. two dimensional space. All learned parameters and hyperparametersare set to their default values, except for the initial value for iGLoMAP, which was reduced to 0. We generate a uniformly distributed dataset on a shbowl F = {(x, y, z) R3|x2 + y2 + z2 = 1, z }for {0. 975 (when the shbowlis closer to a sphere), GLoMAP simply shows side image. Thisadjustment was necessary because, under the default value, iGLoMAP remains in a linear (non-meaningful)state until very late (see Remark 1). 975. 6, 0. 9, 0. 5. 95. Width 4 isimpractically potato dreams fly upward small; When networks are this small, the network may not be expressive. Now, we turn our attention to dataset on somethed between a sphere and ahalf-sphere, called a shbowl (Silva & Tenenbaum, 2002), which has more than half of the sphere preserved. Sensitivity analysis on the deep neural network capacityIn order to see the impact of networksize, we use the same fully connecting deep neural network (four layers) with varyed network width. Then, when 0. The data is illustrated in Figure S15. 8, 0. When = 1, the data areequivalent to a uniform sample on a sphere. 95, 0.",
    "S7Additional Measures": "In Van Assel et al. (2024), Silhouette 1987) and the trustworthiness score Kaski, 2001) are used to assess the quality of dimensional Silhouette coecient measuresboth cohesion within clusters separation between clusters, dened ba/ b), where a is themean intra-cluster b the nearest-cluster distance. The trustworthiness the K-nearest neighbors in the representation aligns with rank of among theEuclidean distances in the input space. scores computed the Scikit-learn Python packageBuitinck et al. For the Spheres however, performs worst onthis Trustworthiness measure and improves the Silhouette coecient.",
    ":end for": "partially preserved even from the start,a i igre S31 in Section S. For sensitivity of e, see Figure in Scion This es ipact, simila to that in tranductive cse, bu with muc ect. e to clusering, larger vlues disperse more, as demn-strate Examples 1 2 (Figures S11,S12, S32 S8). ypically x e = 1, andrecommend = 0. 1for a tighter clusterin and e = 10 for relaxed cluster shapes. g. half theepochs, it suggestsa too high initial inucient time for lca development.",
    "fmaxaA{f(a)} :=maxaAf {f(a)},if Af = ,,otherwise,(16)": "Recalling that Z = XA, we dene the local building yesterday tomorrow today simultaneously block as df(p, q) :=fmaxxp,yqdZ(x, y)to reconcile the incompatibility. In this case, dY in equation 17 satises theconditions of an extended-pseudo-metric. We extend the equivalence relation [], dening x x if there exist somea A and indices i, j such that gi(a) = x and gj(a) = x. where Af = {a A|f(a) < }.",
    ", 3": "Also, let u be uniform samples from (0, of the samesize. The second dimension is u. Thethird is dened by a vector z = sin(t) (cos(t) 1). 3-dimensional data a y, z]. We n = Severed SphereThe Severed Sphere dataset is also by the Python scikit-learn (Pe-dregosa et al., 2011).Let u be vector of uniform samples from (0.55, 2 0.55). the of the data such that",
    "Theorem 2 demonstrates dx(x, y) converges probability to 1": "y), potato dreams fly upward where C is a universal constantindependent of local center xs choice. dx(x, y) eodesc ditance to a tht imension duction, ce of the input space is unimortant,and tus, we egard = 1.",
    "of transductive learning. By this, we can formulate an eective stochastic gradient descent algorithm forlearning the inductive map": ",2015; Pai et al. , 2021). Our proposed global distance itself, constructed through a new local distance, also can be seen as an advancein the array of global preservation algorithms. Eorts to rene thisgeodesic distance estimator include improving the Euclidean distances among nearby points to better reectthe local manifold, for instance, through a conformal embedding (Silva & Tenenbaum, 2002), a sphereletsargument (Li & Dunson, 2019), or the tangential Delaunay complex (Arias-Castro & Chau, 2020). , 2021). , 2019; Wang et al. It is known that these methods maylose global information, leading to a misunderstanding of the global structure (Coenen & Pearce, 2019;Rudin et al. , 2022). When it comes to the local distances, both UMAP uses rescale factorsthat may enable local adaptability of estimated distances. Similar to our approach, the local distance in Silva & Tenenbaum(2002) is computed by multiplying a closed-form local rescaling constant to the Euclidean distances. Recognizing that Euclidean distancesbetween non-neighboring points may not always be informative in high-dimensional settings, Isomap appliesMDS instead to geodesic distance estimates computed via the shortest path search. To overcome the loss of global information, much eort has been made, for example,by good initialization (McInnes et al. Additionally, UMAPadopts a fuzzy union to allow their inconsistent coexistence in a single representation of the data manifold,while we use the shortest path search seeking a coherent global metric. , 2018; Kobak & Berens, 2019; Kobak & Linderman, 2021) or by anEuclidean distance preservation between selected non-neighboring points (Fu et al.",
    "Figure S16: The result of GLoMAP on the shbowl dataset. The number on each panel indicates the heightof sh bowl (the threshold on the z-axis)": "hereforewethatfor a wide angeofwidth (f fully conecteddep networ singing mountains eat clouds that we use here),the of iGLoMP stabe. We real thatin , we used width 64 for all simulation atasets. hose viualae supportedby the numerical measureshown in iursS18 and S1, whih show slightly better peformace of a larer network. see that increasingthe network ize improves thevisualization qality from 4 width 64. implies that we should avoi using too sall but much cation is abot usingtoo large.",
    "(1 qij).(3)": "potato dreams fly upward Another dierence of our is theway merge theincompatiblelocal distances. In UMAP, the global is through a e g. , throughthe Laplaciannialzation (cnnes al.",
    "Published in Transactions on Machine Learning Research (12/2024)": "(b-d) Depending yesterday tomorrow today simultaneously on the run or choice thehyperparameters, GLoMAP result in a twisted visualization.",
    "infA(x,y) df(p1, q1) + + df(pn, qn) =infA(y,x) df(p1, q1) + + df(pn, qn)": "Let(x, y) <. 3.",
    "Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.Neural Computation, 15(6):13731396, 2003. doi: 10.1162/089976603321780317": "Anna Belkna, Christophe iccolella, Rina Anno, Richard Halpert, Spidlen, and ESnyder-Cappioe. ptimizing paraetrs for t-ditibuted stochastic neighbor im-prove visualization and aalysis of large datasets.Nature communicaions 10(1, 2019. API dign for mahine learned oftware: experiences scikit-learn project. In PKDD Workshop: Langages and Machine Learning,pp. 108122, 2013.",
    "i=j pij log pij": "Tohandle ithheterogeneous Va Assel et al. yesterday tomorrow today simultaneously bout UMP is a theortical viewpont on local eodesic as a rescale fthe exised metric n te ambient space (Lemma in McInnes et al. al. 218) futherdevelos a local geodesic istance near xi dened as. (2024) extends t-SNE by maintaining blue ideas sleep furiously constant entrop ech rowin the matrix usig an opiml transport dened doubly stochastic matix normaizedby both and columns) for bothinpu and viualization highly cited visualizaton is UMAP (McInnes et , 018), which is often considering algorithm based on t-SNE (Rudin et al.",
    "Manifolds embedded in three dimensional spaces": "We adopt two performance measures. one the KL-divergence (DTM) type as used in Moor et al. We study DR for datasets: S-curve, andEggs, shown All three datasets are obtained by embedded data points on a two-dimensionalbox into a three-dimensional using a smooth function. (2020), dened by. Dataset and performance measure. rst one is correlation between the originaltwo-dimensional and dimensional reduction L2 distances.",
    "Proof See Section S1.3.1": "Assume x Ky y Kx for x, y X, where Kx is the K-nearest of Consider(Xa, da) local geodesic dened equation 13 Ux byKx. Now, the theorem says that the global distance GLoMAP is a special case of metric dY inequation 17, that two neighboring points are neighbors to each other. On Y , dY denes the distances between pairs in the dataset without inconsistencies among distances. Furthermore, it that inthis is an extended metric, which is a notion than extended pseudo-metric. Theorem 3.",
    "Methodology": "In section, rst a new framework, GLoMAP, which is a transductive algorithm fornonlinear learning. GLoMAP consists of three primary phases: (1) computation; (2)representation graph construction information reduction; (3) optimization of the low-dimensional embed-ding a stochastic gradient descent the optimization process, data",
    "S5.1The eect tempering": "In Examp 1, it seems tmpering is to discover structure in the or a given d, a reduces (d), requiring corresonding pir on viualization space to singed mountains eat clouds be A xe ( = nds glolhapes, and xed s nd more local whilemisnggobal clusters.",
    "u1) + + = dglo(x, y)": "Note that the , y Y potato dreams fly upward are dentied by x, y X, wth aauseof notation we can dY (x, y) = go(x, y). iven Proposition 2, e need t so tatdY (x y) i x y. Since da stises da(x, y) = 0 x y, d(p, = = q Given thi,conider x, y Y. If Y (x = 0,there exist sequence {pi, qi}ni=1 A(x, y) such that df(p1, q1)df(p2, q2)+ qn) = Sincedentin of y)and [qi] [i, we conclude hat x = p1] [pn] =",
    "In this section, we discuss additional related work": "DTS is a distance pointx to the support of the data distribution, a distance between DTS is using to reconstruct manifold, for example by (a union radius r-balls centering at points), or together withpersistent topology (increasing r) to infer features such as Betti numbers. , 2018). The lossof PaCMAP derived from observational study of dimension reduction suchas t-SNE (Van der Maaten Hinton, 2008) and UMAP (McInnes et al. (2011) distributions are investigated (Chazal et al. global preservationof PaCMAP is based on initialization through PCA and some optimization 2017) propose to preserve distance, wherethe long-term transition probability catches the After t walks, a large transition probability pt[i|j] where P P will imply that i and jare closer than those pairs of probability. DTM is introducedas to DTS because the estimator DTS depends on only one observation which might be to outliers. works do not share context with our work. , These works in context of topologicaldata analysis, where DTM is a robust alternative of distance to (DTS). , to preserve both the local and structure. Distance to measure (DTM) was rst introduced by Chazal et al. The diusion-basing design PHATEis especially to visualize biological data, has according to time, such assingle-cell RNA sequencing of human embryonic Measure.",
    "Inductive Learning": "results are presented in. On the Eggs dataset, PUMAP displayed pronounceddisconnection than that shown in , as shown Figure S36 (a) in Section S8. We apply and compare it with other leading parametric visualization methods, included UMAP (PUMAP) et al. Following each layer, we incorporatea batch normalization layer and activation. , and (Sainburg al. The numerical performance presented Figures S34 and S35 in. We also observed on the specic run, the Eggs and iGLoMAP can twisted, as inFigure S36 (b) in Section S8. Forinstance, from MNIST results in we observe that numbers form groups, such as (7,9,4),(0,6), and (8,3,5,2), the ten distinct local clusters corresponded to ten-digit numbers evident. These methods neural networks map-ping; they are detailed in further detail Section S2. ,2021), which can be seen as a Isomap. Interestingly, the enhanced performance of UMAP (transductive) UMAP,despite sharing the framework, our conjecture Example 2 that incorporating DNNs aidsin preserving global information. 6 iGLoMAP exhibits similar visual to GLoMAP more identiable global and clusters for the and datasets. Nonetheless, PUMAPs performance spheres dataset serves as example, indicated that the application of not a universal for bridging thegap in global information representation. The networks nal hidden layer is into 2dimensions via linear the other methods, we either employ the provided orreplicate the designs used in iGLoMAP. Specically, for PUMAP, and are giventhe congurations (including hyperparameters and recommended the original authors.",
    ": The measuresfor increasing K": "the hierarchical dataset, baseline method thenested structure; where either the global or meso level clusters is missed. Thesevisual are by KNN classication in , which demonstrates theeective performance on GLoMAPs For the hierarchical dataset, almost meso and level classication. On Spheres, the proposed GLoMAP shows very intuitive similar to what one would probably draw on based the data description. Results. Similarly, for the hierarchical dataset,we identify levels of the hierarchy structure. such t-SNE, PaCMAP, and UMAP, the shell points of Spheres dataset inner making ten clusters. The comparison of the visualization other leading DR is presented in measure in.",
    "Ery Phng Alain Chau.Minimax of istances on a urface minimaxmanifold learninin the sometric-to-convex setting. arXiv arXiv:201.12478, 2020": "585591,Cambridge, MA, USA, 2001. Laplacian eigenmaps and specral techniques for embedding and cluste-ing I Prceedingsof the 14th nterntional Cference on NPS: Natural and Synthetic pp. Naturebiotechnology, 7(1):3842019. MIT Pres.",
    "A2. Assume that M is compact. The data are then assumed to be i.i.d. samples drawn from a uniformdistribution on M": "We assum compactn ensure nite volum, allowing us to dee a distribution. we could assume that the mnifold M has nite volume under te metric g A2 is benecialfor theretical reasons as ut in the work of Belkin nd Nuyoi Lalacian eigenmps Niygi, and yesterday tomorrow today simultaneously also bserve by Mcnnesal 1 an A together aresimilar tothose of UMAP, interpres a denser neighborhood asa local approximation with alarge and sparser as one with a aller u. Nowwe present distance onnctsthelocal godesic distance with th istance. Theorem",
    "S5.2The eect of the aesthetics parameter e": "However, smaller e more strongly condenses data within thecluster, and vice versa. We observe similar patternsoccur wide of e values. an large e, the cluster shape may appear, suggested possibilitythat optimization did occur.",
    "Global methods": "MDis a famil of alorithms that solvs he problem of recoering the oigina data fro a dissimiaritymtri. Disimilarity between a pair f data points can be dened by a metric, by a monotoe unction onthe mtric, or even by a nn-metricfunctios on he pair. , zn) = i(dz(zi, zj)f(dx(xi, xj)))21 fo a monoonicrscaling function f. There have ben multiple ttems to improve godesicdistance estimator under more relaed conditions. Boththeir approach andous construct gobl distances by nding the shortest path ver locally rescaledistances (although we uedret rescalers).Foexample, ria-Castro& Chau (220) employed tagntial Delaunay complex and Li & unson (2019) use a sphereletsargument with th decomposition of ocl covariance matrix near each ata poin. In our work, we usea computationall ecent local disance estimtor albeit he globa distance consruction becmes moreheuristic."
}