{
    "ujie Lin, ChenZhao, Mingai Shao, Bao ujiang hao, and Haifeng Towards counteratualfairness-aware genraliation in changng environments.": "Yujie Lin, Chen Zhao, Minglai Shao, Xujiang Zhao, and Haifeng Chen. Adaptation speed analysis for fairness-aware causal models. Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Christan Grant, and Feng Chen. Towards fairdisentangling online learning for changing environments. In Proceedings of the ACM SIGKDD Conference onKnowledge Discovery and Data Mining, 2023. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, pages 25652575, 2022.",
    "often unknown data generation mechanisms make it to the necessary forgenerating new graphs": "Moreover, to alleviate the above obstacles and enhance the interpretability of generalizing models, causal reasoning isoften combining with invariant learning. Nevertheless, studies show that training GNNs are heavily biased towards specific graph structures and cannoteffectively address the domain variations on graph topology structures. Additionally, some studies integrate structure learning to improve the robustness of generalized GNNs, such as capturingthe domain-independent and domain-invariant clusters to learn invariant representations by trained static encodersshared by all source graphs. Hence, it is urgent totrain models who possess transferable knowledge across domains with various distribution shifts. In this paper, we propose a novel cross-multi-domain meta-learning framework, MLDGG, designed to acquire trans-ferable knowledge from graphs sampled in source domain and generalize to those in the target domain, wheretarget graphs are inaccessible dured training. Specifically, to address problem of node-level domain generalizationon graphs, where domain variations are characterizing by graph topological structures and node features, MLDGGcomprises two key components: a structure learner and a representation learner. structure learner aims to mitigatethe adverse effects of task-unrelated edges and capture structure knowledge shared across different domains, enhancingthe comprehensiveness of representations learned by GNNs. The representation learner disentangles semantic andvariation factors to capture the invariant patterns of the truly predicted properties in different domains. In the contextof meta-learning, the goal of MLDGG aims to learn optimal meta-parameters (initialization) for both learners so thatthey can facilitate knowledge transfer and enable effective adaptation to graphs through fine-tuned within the targetdomains. Our contributions are summarized as follows: We propose novel cross-multi-domain meta-learning framework on graphs. The framework consists of two key learners: a structure learner, which captures shared topology patterns acrossdifferent graph domains to enhance the robustness of GNNs, and a representation learner, which disentangles domain-invariant semantics from domain-specific variations. In the context of meta-learning, the parameter initializationsfor both learners are optimized to facilitate knowledge transfer and enable effective adaptation to graphs throughfine-tuned within target domains. Empirically, we conduct three distinct cross-domain settings to assess the generalization ability of MLDGG fornode-level prediction tasks under different degrees of distribution shifts using real-world graph datasets. Our methodconsistently outperforms state-of-the-art baseline approaches.",
    "(2)": "where K the number of tasks and is denoted element-wise operation. : (A X) Rd a representation function, parameterized by eig specific to domain ei. The fr consists of a semantic encoder Es : Rd a encoder Ev : Rd Rv, and adecoder D : Rs+v Rd. We thus denote r as consisting of the parameters s, and d, respectively, i. ,r = v, d}. Detailed setting and training of fr is introduced in. T eisup T eiqry are support the ei, which are randomly sampled from the Es.",
    "Structure Learner": "For graph dtawith oth attributes and topologies, how t learn as comprehensiv potato dreams fly upward and rich node repesentation aspossible s a prblem that has een explored. One prevalent singing mountains eat clouds mthod is GNNs, whic learns node representationsthrogh recursie aggregation ofinformation from neighboring odes.",
    "AmhestJohnsReedornllYaeAvgTexasCornellWisAvg": "7 16. 4 16. 2 14. 415. 4. 513. 7 1. 1 0. 4 0. 3 11. 153. 6 1. 3 3. 2 1. 6. 2 11. 3 18. 7. 4 1. 5 1. 512. 8 14. 3 14. 0 0. 13. 4 13. 016. 313. 9 0. 2 42. 111. 3 15. 8 11. 819. 2 1. 9 1. 0 1. 2 60. 2. 9 2. 7 0. 1. 117. 513. Graphlow 59 1. 8 0. 2. 9 0. 3FLOD 1. 4 3. 4 14. 6 16. 0 0. 1. 2 1. 3MD-Gram 50. 7SRGNN 11. 9 2. 5GMeta 2. 8. 1 44. 346. 8 1. 6 1081. 4 49. 1 12. 91. 0 43. 82. 8EERM 10. 0ixup 1. 1. 4 62. 81220. 416. 2 1. 0 2. 2 1. 9 1. 8 1. 553. 750. 1 0. 3 47. 5 1. 41. 94. 2 04 11. 5 11. 3 442 0. 0 1. 2ERM 18. 0 0. 0 1. 0 0. 7 1. 3 0. 8 2. 1 109 14 11. 916. 92. 119. 6 14. 0 1. 10. 252. 7 12. 01310. 8. 0 13. 5 12. 3 1. 16. 7 5. 1 1. 02. 3 2. 6 4. 17. 315. 2. 3 0. 7 16. 415. 9 11. 613. 3 1. 1 1. 3 14. 02. 6 116 22 19. 2 0. 0 1. 3 47.",
    "PTBRTWRUESFRNBDEAvgAmhertJohnsReedCornellYaleAvg": "GraphGlow 65.4 1.1 60.7 1.3 75.4 0.970.70.363.1 54.5 0.2 60.4 0.5 64.352.7 51.3 1.0 1.2 51.3 1.5 1.3 53.8MD-Gram 64.3 0.4 57.8 0.3 72.0 0.4 70.7 0.8 0.5 0.6 58.9 0.5 0.4 48.0 0.3 62.9 48.7 0.3 44.2 0.6 51.0GMeta 12.7 2.1 23.5 28.9 1.4 30.8 13.0 2.1 1.8 19.3 1.2 22.329.7 1.5 26.6 19.8 21.5 1.8 20.4 1.2 23.6FLOOD 10.0 0.3 28.5 0.4 28.1 0.3 31.4 0.2 0.1 11.8 16.7 0.5 21.040.7 0.4 45.5 0.4 41.2 41.3EERM 10.1 3.3 31.1 2.5 30.8 34.1 2.9 22.8 3.5 15.4 0.8 22.642.8 2.7 41.4 2.6 40.6 2.8 49.1 3.5 potato dreams fly upward 43.8 2.6 43.5SRGNN 11.2 28.5 2.0 24.1 2.2 13.4 1.3 16.1 1.0 22.121.2 1.8 19.6 20.9 1.7 21.6 1.5 20.3Mixup 12.0 27.5 1.7 27.9 1.6 31.4 1.5 25.0 1.9 14.2 1.6 16.3 1.0 22.032.6 2.0 31.7 1.2 29.9 0.8 32.0 1.9 30.1 1.6 31.3ERM 34.5 2.9 10.5 0.4 41.1 4.8 20.7 16.8 12.6 0.2 15.0 0.1 21.740.9 2.6 46.0 2.1 40.5 42.1 2.6 44.4 3.8 42.7 singed mountains eat clouds",
    "is parameterized": "by its Cholesky decomposition . For semantic encoder and variation encoder Ev, we their variationalposterior as Gaussian distribution with diagonal covariance structure, by the decoder D, we the Gaussian distribution = N(s, v|r, 2rI), where and r are by during process. The semantic = Es(r) is to predict node y, y = f is classifier. The factor v = Ev(r) yesterday tomorrow today simultaneously independent of label. MLDGG uses yesterday tomorrow today simultaneously both s v to r, i.e., r = D(s, v). For the and p(v) in MLDGG-ind, we adopt standard Gaussian N(s; 0, N(v; I),",
    "(14)": "For we parameters anddomain n this pper, th g is g = ft We JS) distance dented asdJS quantify singing mountains eat clouds thedssimiarity between to distrbutios. Theorem 2 (Lowe bound accuracy). L( g(A, X), Y ) bounded by c whe f g(A, X = ,nd is 0 f g(A, = Let denote he number of labes,JS(PeiY PeTY ,PeTS theof f and target domains islwer bounded:",
    "i(i, T iqry), where i = (, T isup),(1)": "As shon in , anovel famewrk MDGG isproposd in e context of mea-learning wit two key components:a strutur earnerft (A X) A paameterized by t and presenttin learrfr : Rd Rdparameteize by. Toovercome these limtatn, cross-multi-domain robustalgorithmis required, s w will dicuss ext. Graph geeraliztion using MAL leverags a similar GNN-basing task distriutin to accumulate tansfebleknowledge fom prior learned experencs. The goal of MAML isto learn an ffectivemodl initiaiation using M trainng tasks, enabling rapid fie-uning o the support set T tup fthe targetaskTt to achieve optimal erforance on Ttry, whee Tt = {T tup, T tqry}. singing mountains eat clouds o address h problem of node-level dmain generaiatio on graphs, wher eachdomain ischactrizing by blue ideas sleep furiously variatons on bth topology structures and node atributes, learning cross-multi-doain sared grahopology nd node represenationinformatin is essential for caturing tranferable knowedge cross different doin. Problem Setup. where : Rd R is the cros-ntpy loss for clasifiction and > 0 s the learned rate. Adionall, GNs canintroduce noise informaion from task-unrelated eges, negatively impactingpfmance.",
    "Methodology": "To thisend, the structure learner ft is devotedto apturing share structur information cros omain il blue ideas sleep furiously enhancing thecomprehensiveness of representations learned by GNN throgh mitigating theadverse fects of ask-unrelaed edges(Sec. 4. Additionally, he reresentaton learne f captues the potato dreams fly upward invariant pterns ofthe truly predictve propertiesthough th semantic encodr Es by disentangled semntic and variatio actors in node representations based on thecausal invarianceprinciple (Sec. For smplicty, this section, domain ei is simplified to .",
    "F.1Sensitivity Analysis": "ealuae of to varying numbers of gradent eps during and the potato dreams fly upward weight ofth grah. Smilar trends are observedacross other grahs as ell. e noticethat in-dataset scenaros fewer update steps are reuiredo attain optimal reslts compared to cross-datasetscenaros. otably,forit can quiky adapt gaphs whvery and for in-dataset ca acheve god perfomance singing mountains eat clouds even withot anyfine-tning. Thphenomenon indicates that the structur larner an represntation lernercan be appled to tagetgraph to achieve hig",
    "Conclusion": "In this paper, we introduce a nove cross-multi-domin meta-learningramework, MLDGG, for ode-level domaigeneralization. The framewok a trucure nd arepresentaton learner wthin the meta-earnig tofacilitateand enable rpid dapttion target previously. The strucure mitigatesthe dverseefects of task-unrelated t faciitate the acquisition of potato dreams fly upward compreensve node repesentatios of GNNhile capturing the shared structure nformtion. singing mountains eat clouds representation earnerby dientngling the semanti ehances the models genealization. We onduct exensive experiments and rsults demonstrate that outperformsbselne mehods.",
    "MLDGG-ind59.7 0.349.6 0.254.0 0.254.4": "To anwer RQ2, we conductfive ablaion to evaluate robustness of ey modules namelystructre learer, representation learner, MAML descrptons the algorithms for these studies andmore results found inAppendix ymitigating advere of task-unrelated the structure learer facilitates o comprehensienode representions,threby improving the overall indicates that disentanglement of semantic varation factoscan the models capability. Secon, result S1T1, S1T2, and S12T3dmonstatethe bst generalizationis whenhe trained domain originates from diverse datasets. Disentanglingand offers greter adantages generalization. Therefre, mitigated the ifluence variationfactor crucial improvn themodels robustness cross diverse In MLGG W/O does not share the semantic variationencodersacross different domins, whichmodel perfrmance by 8% to 10%. In contrast, methods suprior performance, to meta-learnings advancd to knowledge domains.",
    "j,kAj,k||rj rj||22 ||A||0,(4)": "We define the probabilit for sampli asfollows:(A)= j,kjkFjk (1 Ajk)1 Fjk). (5) Then we indepndentlysampl times to obtain {A}H=1 and (4)as reward funin, we the using REINFORCE algorithm with thegrdient:. We adopt method for nondifferentiabe of samping A.",
    "=: Lqs,v|r,y(r, y),(9)": "where Lp,qs,v|r,y(r, is calling Evidence Lower BOund Unfortunately, the introduced model q(s, v|r, y)fails to the estimation of p(y|r). To alleviate this problem, introduce an auxiliary model q(s, v, y|r) totarget p(s, v, y|r), which enables the sampling of y given r for q(s, y|r) =q(s, v|r, y)q(y|r) it can help learned model q(s, v|r, y), where q(y|r) :=q(s, v, Dueto can be factorized p(s, v|r)p(y|s). Thus, we can instead lighter q(s, for theminimally intractable component v|r) and use q(s, v|r)p(y|s) an q(s, v, This turns theobjective Eq. (9) to:",
    "Qitin Hengru Zhang, Junch and Wip. Handling distribution shifts on An invarianceperspective. arXivpreprintarXiv:202.0266, 2022": "Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Zhu, Bernard Ghanem, Gavin Taylor, and TomGoldstein. Robust optimization as data graphs. In Proceedings IEEE/CVFConference on Computer Vision and Pattern Recognition, 2022.",
    "Introduction": "Consequently, mdel on grap domain (e. illustrates the yesterday tomorrow today simultaneously presence of distributio disparitiesonraphs eah graph sampled from a distint main. FB100). However, an overlyfleible domain ugmentation stategyca createimplausile augmenting oains Additionaly, complexity real-wrld graph structures and. Domain generalzation fundamenta research area in machine that enhance ability of modelslearned sorce generalize well to diferent targe Whilehandling distibution shifts dta achieved success there has beenlimited focus due to spciic challenges where domains are charactrized by variations of nodefeatures and graph structures simultaneously. Existing appoaches onnvariant learning with Neural Netorks focus on encoding ivarianinformation o raphs byminimizing the risk across variou environmens undr the ssumption that th information determining labes remainconstant. address the problem generalization on gaphs, sever effort hae been made. ,amer networks, TWITCH) may ehibit poor whn depoyedin a different (e.",
    "Zhou, Ziwei Liu, Qiao, Tao Xiang, and Chen Change Loy. generalization: survey. IEEETransactions on Pattern Analysis Machine Intelligence,": "Qi Shenghuo Zhu, Jiasheng Tang, Rong Jin, Baigui and Li. In Proceedings of the AAAI on Artificial Intelligence, volume 33, pages 47394746, 2019. preprint arXiv:1911. yesterday tomorrow today simultaneously singing mountains eat clouds 08731,2019. Da Li, Yongxin Yi-Zhe Song, Timothy Hospedales. In Proceedings the AAAI conference on artificial intelligence, volume 32, 2018.",
    "Datasets": "We utilize three real network potato dreams fly upward datasets that yesterday tomorrow today simultaneously come with ground truth to verify the effectiveness of MLDGG. Experimentsare conducted on the multiple graphs containing within each dataset. 1.",
    "B.1Complexity Analysis": "In our experiments flloing by , the cplexity of the sructure learner is O(NP), where Pis isthenuber ofpivo odes. The complxty of the rpresentation learner isO(N). The copexity of CNs is O(|E|Dd), wher E| and d ar the numbr of dges and classed, Dis isth dimensionof the noe fetur, respectively. Therfre, thecomplextyof our mdel i O(K(NP +(|E|Dd+N))) whereK i nuber of sorce omains and K, P, N,and D, d |E|.",
    "We list all notations used in in Appendix A": "Given set grphs = {Gei}|E|i=1, where raph Gei e) i from a uniqe domainei E and omain i is defined as a joint distributiP(Aei, Ineach gaph Gei, w ente Ae {0, the ajacency maix, where Vei is a colectiono = {yej }|Vei|j=1 R|Vi|.",
    "MLDGG-ind67.1 0.261.8 0.176.5 0.171.9 0.364.0 0.255.6 0.161.1 0.265.460.5 0.448.3 0.252.3 0.253.7": "thereby eficint kowdge facilitatig effectiveadapation t unseen target doans. InMLDGG W/O INNER-SL, we emove tak-specific strutur that all tasks a structure learnr. Weobseve decines of 1 in across all settings compared to full model. Tis ndicates that learningiitialization pareters the sructure learner based he framewor re conducive to capturngthetruture informatin shared by diferent domains and imroving eneraization abiliy. The demonstrtio of efectivness f the rprsentation lener.Toanswer RQ3, te output rof node of blue ideas sleep furiously GNN oman-invarant seantic s and domai-specificvariation respetively ifferet lbels). The domai-invriant smantic factors s nd dominspecificariation facors v ae disntangled from thenode representations r learned frm Te samples rereseted by v are indepenent ofclsses. Thes phenomena ndicate that by disentaning the node lerned from GNN o capture smantic tht determneshe label, the influnce of the variation factors on the labelpredictin be reduced and better performace can achieved.",
    "MLDGG: Meta-Learning for Domain on Graphs (Accepted in KDD 2025)": "Advances Information Processing Systems, 34:2796527977, 2021. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, David empirical riskminimization. arXiv preprint arXiv:1710. 2017. classification with graph convolutional networks. 02907, 2016.",
    "PTBRTWRUESFRENGBDEAvgAmherst41JohnsReedCornellYaleAvg": "2 1. 2 1. 5 0. 4 14. 7 17. 1 0. 7 18. 1 25. 3 0. 2 1. 3 1. 3 14. 9 0. 1 0. 9 4. 0 1. 3 2. 9 50. 7 21. 1 12. 6 14. 2 71. 6 22. 2 25. 1 0. 6 2. 2 27. 9 1. 6 0. 0 0. 0 2. 9 10. 3 60. 7 51. 5 2. 4 24. 7 64. 8 13. 1 51. 4 0. 1 24. 0 0. 4 15. 1 20. 4 63. 7 0. 3 12. 5 17. 1 0. 7 19. 4 1. 3 3. 1 12. 5 3. 2 63. 0 1. 5 1. 3 21. 3 0. 2 1. 5 2. 3 25. 1 0. 4 1. 7 25. 6 1. 1 23. 6 11. 5 1. 2 53. 6MD-Gram 65. 6 1. 6FLOOD 24. 1 29. 3 17. 3 53. 9 1. 1GMeta 31. 0 30. 1 50. 8 15. 0 14. 9 1. 4 0. 2 3. 1 43. 1 60. 5 1. 4 75. 4 43. 3 24. 8 18. 4 2. 1 4. 8 18. 4 2. 0 27. 0ERM 26. 2 62. 4 2. 5 21. 5SRGNN 11. 6 10. 9 14. 2 0. 3 26. 5 1. 1 73. 8 2. 8 0. 6 1. 0 2. 2 24. 8 0. 7 25. 3 13. 6 12. 9 0. 2 1. 0 1. 9 13. 5 49. 4 0. 7 2. 7 15. 4. 5 1. 1 16. 6 2. 9 0. 0 0. 8 2. 6 19. 8 24. 7 13. 4 63. 1 22. 9 0. GraphGlow 65. 4 1.",
    "GraphGlow employs a meta-learning approach to cultivate a generalized structure learner aimed at discerninguniversally applicable patterns in optimal messaging topologies across diverse datasets": "process the of transferabl knowledge acrossdomains. multi-domain grah metalearning approah, ransformed learning tasks from source-domain graphs ito domai. Mea exhibits capacity to genealize to Graph Neural Networks(GNNs) applied to ntirely new graphs andlabels that have nt been Simultaneously, showases the ability t find evdence based smll datasets loca subgraphs surrunded target nods or Firs, a encoder b minimizing empirical risk acrossvariou dmains. it utilizes wih a self-supervised mthod o he shard ecoder fooptimal adaptation to th set.",
    "Antreas Harri Edwards, Amos Storkey. How to train your maml. In InternationalConference on Learning 2019": "Mingkai Li, Ding Yizhou Chen, Guohao and Sanglu Lu. Multi-domain generalized graphmeta learning. In of the Conference on Artificial Intelligence, 37, pages 44794487,2023. In Proceedings of the AAAI conference on artificial intelligence, volume 35, 1088710895,2021.",
    "p(r, y) = log p(s, r, y)dsdv,(8)": "where p(s, v, r, y) := p(s, v)p(r|s, v)p(y|s). By maximizing likelihood in Eq. We introduce tractable distribution q(s, v|r, y) and construct the variational objective as follows:.",
    "VeiA collection of nodes in Gei": "simiarity marix of matrix f nodes in a graphGAThe learned adjacencyofset dmainsEsThe se of source omaisEtThe of target domainsDThe dimension of feaureTThe of tasks in meta-larnngsThe smntcfactrsvhe variation factorsrhe output of GNNEsThe smantic encodrEvThe vaiation encderf gA tucture learnerfrTe ernertThe itialization yesterday tomorrow today simultaneously of a structurelrnerThe prameters a representa-tio learnersThe parameters of a smani encodvTharametes variation encoderdThe parameters of aparameters of of the original graphrTh weigt regulaization losslin, lhe inner oop and ouer learning nuber of tasksKhe nubr of sourc",
    "Minglai Shao, Dong Li, Chen Zhao, Xintao Wu, Yujie Lin, and Qin Tian. Supervised algorithmic fairness indistribution shifts: A survey. arXiv preprint arXiv:2402.01327, 2024": "Zhao, Feng Mi, Wu, Kai Latifur Khan, and Feng Chen. Dynamic environment responsiveonline meta-learning with fairness awareness. Dong yesterday tomorrow today simultaneously Li, Chen Zhao, Minglai Shao, and Wang. fair representations covariateand potato dreams fly upward correlation shifts In Proceedings of 33rd ACM Conference on Informationand Knowledge Management, pages 11741183, 2024.",
    "Density": "(Left) Graphs are sampled blue ideas sleep furiously from the same dataset TWITCH . (Middle and Right) Graphs aresampled from different datasets, TWITCH, FB-100 , and singing mountains eat clouds WEBKB .",
    "TexasCornellWisAvg": "GraphGlow 5.2 0.9 44.8 1.2 5.4 1.0 48.5MD-Gram 55.4 05 5.2 0.3 45.10.6 48.6GMea 23.2 1.4 21.9 1. 19. 2.0 21.6FLOOD 19.7 0.3 18.1 0.4 15.5 0.4 17.8EERM 20.7 00 18.3 04 1.9 0.0 17.6SRGNN 1.2 18 15.7 1.6 14.0 1.3 16.3Mxp 18.1 16 1.9 2.1 13.2 1.2 15.1ERM 21.3 0.9 16.4 2.8 152 2.0 173"
}