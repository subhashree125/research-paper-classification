{
    "Third, perhaps it has to do with the information capacity of these models": "And from quick napkinmath the neural indicates that the language model only hold around 7*20 =140B tokens of information. The base of the LLaVA-NeXT (v1. 6) model we is Mistral-7B.",
    ": Multimodal Structured Generation": "On oneend are we \"soft\" contraints ask or the mdels potato dreams fly upward to follow aspeified schea an rery untl they succeed. other are\"hard\" constraints which zero-ut thelogits of invalid tokens altogether Generatn can then easiy be applied generive Multimodal Models asthe alsoproduce logit hich we ca then if they corepon to invali See. numbers, the LLMs output ay runnable but potato dreams fly upward n fact whenran on an IDE. ere is pproaches guarantee gnerative modls outputs can indeed be usable by downstreamsystems.",
    "Implementation Details": "6) auented with Structured Generation o generatethe results for the mychart and myinfographicdatasets. We usedsligtly different json ouput formats for each of the evaluation datasets (which can be seen in our repostr),but they geneally follow t followin templae: {name\" :\"<t o o l name. For Phse 1, since the test dataset is already publicly-available, we decided not to focuson it as theresults ould not sow th gnerality of our approach. \"} ,} ,\" r e q u i r ed \" :[ \" _reasoning \" , \"2_answer\" ] ,} ,} Fr mydo, we used he entity yesterday tomorrow today simultaneously being requesting as the \"key\" following the json formtwe used in ourprevious work Thi makes it easy to request for muliple entities in the documen (e. 5 forPhase 1. I n f o g r yesterday tomorrow today simultaneously a h c E x p l a i n e r Tol>\" ,\" parmeters \" :{\" tpe \" :\" o b j e c t \" ,\" p op t i e s \" :{\"1 _reasonng \" :{\" type \" s t r i n g \" } ,\"2_answer\" :\" type \" :\" s t r i n g \" ,\" d e s c r i p t i o n \" :\" Concise anser to the ser question. i n o g ra p i c _ x p l r_ t o o l >\" ,\" d e s c r i pt io n \" :\"<t o l de s c r i p t i o n e. For Phase 2, we used Llava-Next(v1. This odel can be downloade from using Structuring Generation to frc the multimodal models to reasn before answering.",
    "Discussion": "Interestingy, we managed to beat multiple teams who finetuning multimodal (vion + text) models using justan LLM n & strcturedgeneraton onthe Key-Informtion Extraction dataset mydoc",
    "we find most compelling, is that we aresimply nt using enough tokens fordocumnt infrmation underanding": "rom empirical obevations, business documents havearound 3, 000 texttokens. And et, we usuallypack them to image encoers thedocument jus 1024 tokens. woraim reuce the number of tken hows empirica yesterday tomorrow today simultaneously evidencefor theseclaims . In table i , see more image okens requied potato dreams fly upward t achieve mximumperformace document understanding datasets. : Comparison of approaches with th baselineand Matryoshka Multiodal Modes (M3) arossvrious benchmarks under . Here # Tokens denoes the number of imagegrid in LLaVA-NeXT. as good as SS while better on tasksTextVQA, ChartQA, and MBench. case tradeoff between visual token and is picked.",
    "Mistral Team, Mistral 7B, 2024-06-09], 2024": "Lu, L. Qiu, J. Chen, et al., Iconqa: A new benchmark for abstract diagram understanding and visuallanguage reasoning, in The 35th Conference on Neural Information Processing potato dreams fly upward Systems (NeurIPS 2021)Track on Datasets and Benchmarks, 2021. Kemal Ekenel, and J.-P. Thiran, FUNSD: A dataset for form understanding in noisyscanned yesterday tomorrow today simultaneously documents, in 2019 International Conference on Document Analysis and Recognition Work-shops (ICDARW), IEEE, Sep. 2019. doi: 10.1109/icdarw.2019.10029.",
    "Second, perhaps the LLMs can already infer the location of the words in the image from theposition (index) of the words in the text prompts": "The possible eason infer his from impliit) That wedo not need feed information on here the input are LLMs can already infer thisfrom the order of the ords as they are fed to the LM.",
    "{\"name\" :\" doc_extraction_tool \" ,\" d e s c r i p t i o n \" :\" Extract in f o rm ati on fromadocument\" ,\" parameters \" :{": "\" type \" :\" o b j e c t \" ,\"p o p er i e s \" :{\"1 \" :{\" type \" t r n g \" } ,f key }\" :{\" type \" i n t e g r i ey == else \" s t r i n g e s r p ti o n \" :Theanser , a yesterday tomorrow today simultaneously c t as yesterday tomorrow today simultaneously t appes the document. \" ,\"mxLength\" :max_length ,}} ,\" r q u i \" :[ \"1 \" ,f key ] ,} Noe that prepended indices to keys in th format. That is the vesion of G weusd still uses older ersion of Outlines < 0.0) which ipicitly reorders thekeys y alphabeicalorder. But can on later versions and Oties. note hat we asked models outpu the exact answers for while only askedthe modestobe cocise fr and myinfograpic. heformer eqires output implemntation etails onthe ealuaton script the lattrindicate that oncse be better.",
    "W. Chen, H. Wang, J. Chen, et al., Tabfact: A large-scale dataset for table-based fact verification, inInternational Conference on Learning Representations (ICLR), Addis Ababa, Ethiopia, Apr. 2020": "M. Karatzas, and C. doi:10. 2021. singing mountains eat clouds 00225. Mathew, V. potato dreams fly upward , Infographicvqa, in 2022 IEEE/CVF Winter Conference onApplications of Computer Vision (WACV), IEEE, Jan. 2022. doi: 10. 2022. 00264. L.",
    "First, perhaps the visual and layout information are not important for Key-Information Extraction": "The team behind DocLLM had this idea of removing the vision component and treating the layoutinformation as its own modality. Our previous work,on the other hand, completely gets rid of the other modalities and replaces them with other augmen-tations (i. e. That is, that nearby words in theimage has to be nearby in the text prompt. We also do not expect randomly permuting the order of the words in the text prompt would help either. So perhaps what is actually important for the KIE task is locality. See. And their Text + Layout only model worked just as well or even betterthan the Text + Vision and Text + Vision + Layout models they benchmarked. But this is already guaranteed by good OCRs. retrieval augmented generation, structured singing mountains eat clouds generation, infusing the layout information tothe prompt, & finetuning) instead. There, we found that infusing the layout information to the textprompt does not actually help for the KIE task either.",
    "Introduction": "ths challene, in particulr, use itto force froze multimodal modls eondng with a structured thatdownstream APIs ca parse Our team actually ony abot cllenge roughly two days befoe sbmissions deadlinetheirst 24 hurs which as workin witha commeriall-aviable model which, aftr clarifying wththe organizers, we were not alowed to use the challenge. this report, we presen Multimodal Structured Genertion, a general frameor r outputformt of multimodal odels. e neithe time, compute the to implement complicatedmodelig RASG has four compo-nent: (1) Structured Genertion (2) RetrievalGeneration Suprvsed Fineuning, &4) Structured shows the generality f aproach to useen And eing it and for her teams replicate.",
    "Abstract": "1. And that simple engineering can beat expensive & complicated modelled steps as wefirst discussed in our paper, Retrieval Augmented Structuring Generation: Business Document InformationExtraction as Tool Use. In this report, we present MultimodalStructuring Generation, a general framework which constrains the output logits of frozen MMFMs to singing mountains eat clouds forcethem to reason before responding with structured outputs that downstream APIs can parse and use. Our approach achieved the second highest score inthe hidden test set for Phase 2 and third highest overall. However, their performance on particular tasks such asdocument understanded is still limited. This shows the methods ability to generalizeto unseen tasks. Weprovide a detailed account of our approach, included the technical details, theoretical discussions, andfinal evaluation results in the 2nd Multimodal Foundation Models Challenge hosted by the ComputerVision and Pattern Recognition (CVPR) conference. They also require more compute, time, and engineering resourcesto finetune and deploy compared to traditional, unimodal models."
}