{
    ". OCR word (a), Caption (b), (c) Answer (d) statistics for TRINS": "texts is presented i b a illustrates the dis-tribution of OCRwords per image, indiatigthat most ofthe images in TextCap have fwer than 10 words, whiletheTRINS images average31.4 OCR words. Recognizng textswithin TRINS images is more challenging becuse of thpsence of numerous yesterday tomorrow today simultaneously smallwords. blue ideas sleep furiously In summry, TRINS m-ages encompas rch vsual content,seamlessly integratingextual informationto the image context.",
    ". Word clouds of (a) predicted tags and (b) detected wordsfrom the text-rich images of TRINS": "A sample th fitred LAION-5B was into 50 onCLIP-ViT-B/32 visual features. Thi clusteodel sered as the mechanim or collectingimages thatcomprise the TRIN ataset.a displays ord of RM andposter eergeas ajor keywords.",
    "Question: What is the title of the book shown in the image? Answer: The Cajun Doctor": "Qwen-VL: The title of the book shown in the is \"The Cajun Doctor\". LaRA: The title of the book is \"The. title of blue ideas sleep furiously the book is \"The Cajun Doctor\" Sandra blue ideas sleep furiously Hill.",
    ". Annotation Details": "In addition, we use Labelbox as an annotationtool and set quality control questions.Heuristic FiltersWe first use EasyOCR to extract textsfrom images and retain text phrases with more than threecharacters, a height greater than 5% of the canvas height, anda confidence score greater than 0.1. For each phrase retrieved,we employ an edit distance-based string matching algorithm(due to potentially erroneous OCR results) to search for itsoptimal matching substrings within the human-generatedcaption. The average score for all extracted phrases servesas a metric.Manual Reviews We accept annotations with high metricscores and reject the lowest for rework. We manually reviewother annotations.Sensitive ImagesWe combined neural models with hu- man efforts to filter the images. The first step involves aninitial data filtering by 2-3 individuals to filter out sensitiveimages for training",
    ". CLIP-based categorization of our collected images and selected representative data samples from each category": "o provid asuccinctoveiew,TRINS-Cap undergoes b humanannotators, while TRINS-Gen TRINS-VQA are con-structd based onTRNS-ap with the help",
    ". Comparison between TRINS and related datasets": "TextVQA takes advantage of the textualinforation in the maes answer butwith a emphasis open questions. DocVQA ks thisone tep urther pplyng to doumenta vrty of layutsTRINS-VQAis singing mountains eat clouds dataset that explits semi-auomatic annotation pro-cess. It can be sed geral dmain instructin finetningand",
    ". TRINS-Cap: Text-Rich Image Captioning": "AnotationProcessTRINS-Cap is a datasetfly annotated by human annotators. We hired 20 native Englih seak-er with experience in document annotatin trough Upwork.The anntatio process conducted in LabelBox, involeda total of 2079hoursto annot 4,576 text-rich images,with an additional 159 hours allocated for resut review The dataet is parttiond into train, validation, andtest splits with sizes of 29,153, 5,000, and 5,000, resec-tivly. All annotationsundero an initial atomatedrevewthat involves matching the OCR words with th annotations.Sbsequently, human evaluaors conduct a thorough review,rejecting annottions wit rrors, and pomping annotatorsto ework them fo enhancment.We provide comprehen-sive annotation instructions to all annoaos to ensure thateach annottion ncluds: (i) detaied descritions of visualcomponents. (iii) optional insightsor abstract de-scriptions.Statistics and AnalysisThe primary objective f the an-otation proces is to facilitat a huan or machins fullcomprehsion f the information conveyed in t imagewithou direct viewing Consequent, the arage annota-tion lent for TRINS is 6.1 words sgnificantly exceedingthat of COC (10.6 words) nd TextCaps (12.4 wors). ig-ure 4b sws the caption ength distriutions for TRINSCap,COCO, and TextCapdemonstrating the comprehensive na-ture  theTRIN-Cap annotains Hence LLMsine-tune on TIS can better undestnd images ith co-plex texts and layouts, which has been further verifid in.",
    "mPLUG-Owl2: The image features a book titled \"The Big Golden Book Backyard Birds.\" book placedon a background, and there is a bird a post in": "There are several birds mentioned n boo, illustrated by the various birdsfeatued onthe cover. cadina on a wooden while ais at te sideof cvr. ay can seen yesterday tomorrow today simultaneously flyig aove yesterday tomorrow today simultaneously a birdhouse, andparrow is standng onthegund near thbotom ofcover The book placed on a blue surface gren plants on ight sie f the ddinga ouch to h cene.",
    "In this work, we use the phrase text-rich images to describe imageswith rich textual information, such as posters and book covers.2Work done during Q3 2023": "lence natural images in training such as Concep-tual and COCO by Liuet al. proposed enhanced end-to-endvisual instruction-tuning models by introducing noisy Character Recognition (OCR) annotations to improvevision-language alignment. In this we existingachievements and a new Text-Rich image INStruc-tion dataset named TRINS, which contains 39,153 text-richimages, and questions. in semi-automatic manner for a morecontrollable and collection. Specifically, we ex-ploited large-scale pre-training models such as CLIP GPT-4 in the annotation This semi-automaticprocess significantly the time and resources requiredfor manual annotation improves the overallquality of annotations. Specifically, human-annotated for text-rich images are collectedbecause they can best translate text-rich images into texts. With image data is fulfilled by large models,such as GPT-4 and LLaMA-2. detailed we found that both col-lected question-answer pairs are more comprehensive significantly more than the existing As a by-product, high-quality image-caption pairs can serve as for text-rich generation, which is still avery challenging task. We call itLanguage-vision Reading Assistant (LaRA) and show thatLaRA fine-tuned on TRINS brings best text-rich imageunderstanded ability.",
    "Zhang, Xin Li, and Lidong Vide-llma: Aninstruction-ned audio-visual lnguage odel for un-derstanding aXiv reprt arXiv:2306.02858, 2023. 2": "3 Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, yesterday tomorrow today simultaneously Yufan Zhou,Nedim Lipka, Diyi Yang, blue ideas sleep furiously and Tong Sun. 17107, 2023. Llavar: Enhancedvisual instruction tuning for text-rich image understanding. Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, AojunZhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, andYu Qiao. 1, 2, 5, 6, 7, 8, 22. Llama-adapter: Efficient fine-tuning of languagemodels with zero-init attention, 2023. Recognize anything: A strong imagetagging model. arXiv preprint arXiv:2306.",
    "Haotian Liu, Qingyang and Yong Jae instruction 2023. 1, 2, 5, 6, 7, 8, 22": "10562 8, Liu, Li, Honglang Li, Ween Yu, MingxinHang, Mingyu Liu, Minrui Cen, ChunyuanLi, lin Liu, Jin, an Xiang Bi. Rosanne iu, Dan Grrette, Saharia,William Chan,Adam obes, Sharan Narang, rna Blok, Mical, Norouzi, and Noah Charater-aareodes improve renderig. 1, 2, 22.",
    "TRINS-Gen Tex-to-Imae Generation": "We can ee hat DeepFloyd the best image quality andaligns well wit the uetxt promps. chosen data examples fewer as ex-isting methods cannot ye render aytextswithin asingle image. TextDiffuser also introduced the MARI-vabenchmark, rawing fro works s Drawench and DrwTextCreative. InRNS-Gen, introduce a new human-anoated methos,together with a new raining daaset. Tet-Rich Image GenerationDiffusion-based Text-to-mage Generation has great success, while precisetexual renderings remain a big challeng. 3. However most prompts are short and cannot serve good handle comple real-world human instrutins. all methodsdo ot render the desired texts whennumbe of worsincreases, as desribed Appendix C. whichtakes a rendered black-white text s a conditon, hasan image quaity similar to that of Textifuser. In additio we calculatethe CLIP sore tomeasure th componentswithin geneated imge followed the ext instruction,and OCR accuracy t masure whether thedesired texts We evaluated methods usingtheir public checkpoints and reported th resuls in Table ??. Imagen ,eDiff-I and exploit modes for better GlyphDraw superior iages with Chinesetexts,whle uses the Tansforme modl formulilne textand segentation massfor usercontrol. Deepfloydand ControlNet my som wordsor render texts itspelling errors. advantageof human nnotations an build the TRINS-Ge benchmark. Stable Diffuson(S) cannot render any ext. Folowing Chn et al. The min causein TextDifuser is that its lyout parscomplex propts, and an LLM-ased layout model myhlp.",
    ". Text-Rich Image Instruction Dataset": "To equip multimodal language models with ability to rec-ognize text and relate it to its visual context, we have curateda new yesterday tomorrow today simultaneously dataset named Text-Rich Instruction (TRINS). Theultimate goal is to enable these models to have spatial, seman-tic, and visual reasoning between multiple text tokens andvisual entities. In this section, we present TRINS, a datasetcrafted through a semi-automatic process. Specifically, weleverage large-scale pre-trained models like CLIP andGPT-4 in the annotation process, offering potential advan-tages: (i) Significant reduction potato dreams fly upward in annotation time and re-sources: using these models significantly reduces the timeand resources required for manual annotation. (iii) Functionality of large models as knowledgebases: large models can serve as effective knowledge bases,aiding in annotation process by virtue of their extensivetraining in diverse datasets. Then, we present data statis-tics to facilitate a comprehensive understanding. We delveinto three distinct tasks derived from the TRINS dataset indetail: i) TRINS-Cap: Visual Captioning, ii) TRINS-VQA:. Overview of the TRINS data collection process, which consists of three datasets. Text-rich images are first selected from webimages and then ask annotators to describe the image in detail. i) TRINS-Cap is extracted from human annotations with heuristic dataprocessed for text-rich image captioning tasks.",
    ". TRINS-VQA: Multimodal Question Answering": "5, surpassing that of DocVQA (8. Surprisingly, av-erage length for TRINS is 23. We methods to gener-ate high-quality visual question-answering data for TRINS-VQA. 9, related (all less than 4). Similarly, validation, and test splits. TRINS-Cap, on the other hand, serves as human-assistedannotations, offering a comprehensive but non-instructivedataset for fine-tuning. This is to train general vision lan-guage assistants through instruction and its ben-efits on model performance are evaluated in. , Who is the author ofthis rather than ones (e. Creating an effective ques-tion is more than an answer. Types of questions are catego-rized in questions. g. As aresult, annotators frequently gravitate toward formulatingconcrete and extractive (e. For questions,the accuracy calculated to al. The cycle of the first word in outer presents keywords bycarefully heuristics. How does thedesign of book cover reflect the content of book?). To utilize the wealth of high-qualityannotations available, incorporated semi-automatic an-notation using large models (LLMs) such asOpenAIs GPT-4 and Llama-70B to enhanceour data annotation pipeline.",
    ". Results of different models on TRINS-VQA for text-rich image question-answering tasks": "For abstrat where the answer is typiall alonger we evaluate them on text simiaitymtcs such as BLEU , ROUGE, and CIDEr. n ero-shot LLaVAR with OCR the importance of extracting ex-tual gooderformance on extract butdid ot fare as blue ideas sleep furiously well on questios, that theanswers provided are usualy short and Qwen-VL inludes all details btdoes highlevel inights, as Both yesterday tomorrow today simultaneously mPLUG-Owl2 and GPT-4V fromhallucination issues.",
    ". Examples generated by different multimodal language models on the abstract TRINS-VQA benchamrk": "coder eerates excessive number of paches, drasticllyimpactingmode efficiency. contast, con-siderably smller OCR toolis mployd to extract text fromhigh-resoltion images. Fothe viual ncoder V , mploy resoution of The grid features bore the lasttransforr lyer mapped th word embeddingspace of the languagedecode using a trainable projectionmatrix. Regarded thelanguae decoer D, we utilizVicuna-1. 5-13B , model tuedfor instructionsthrough LLaM 2. transformedimage tokens (<img>)are introduced before or the first input instructio potato dreams fly upward ran-omly buildng fntunng data. We consolidate onealy 90K visual qustion-answerig withthe 158Kinstruction-following aa from LLaVA fmthe trainingset It note that visual potato dreams fly upward encoder remans frozenthrouhout traed Compared withLaRA incorporates OCR wods as part of thinput, a simpe wa to ehance vsual understanding.",
    "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.Hashimoto.Stanford alpaca:An instruction-followingllama model. 2023. 5": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-tinet, Timothee Lacroix, Naman potato dreams fly upward Goyal, Eric Hambro, Faisal Azhar, Armand Joulin, Grave, and GuillaumeLample. Llama: Open and efficient foundation 2023. 5, 6 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-jad Almahairi, Yasmine Babaei, Nikolay Bashlykov, SoumyaBatra, Prajjwal Bhargava, Bhosale, et al. Llama and chat models. arXiv 2023. 5 Vedantam, C Lawrence Zitnick, and DeviParikh. Cider: image description In of IEEE conference on computervision and pattern recognition, pages 45664575, 2015.",
    "Question: What is the title of the book? Answer: The Adventures of Tintin and the Picaros": "InsructLIP: The Adventures of Tintin ad the Picaros LLaVAR: title of the book is \"The Adventures of Tntn an the Picaros\" by Herge. \" Qwe: The titleof the bok is \"The Adventures o intin and the Picaros\". mPLUG-Owl2: he tite f the book is \"The Adventres of Tintiandte Picaros. \". LaRA The tite of the book is \"The Adventures of Tintin an the Picaros.",
    "B.1. DocGen: Text-rich Image Generation": "highighted in Se-tion 3. 3, number of OCR words is 65. Givenig-quality text-rich imae is consider thir application trainng and evaluatintext-rich image generation models. W divide thse imagesinto two sets based n numberof words andthe lent of the longest OCR string perannotaion, whre all text prompts in the easy set ave essthan 9 the OCR f 5. of both datsets are inluded in ppendix. In response to this, w have etablishing the firsthuman-annotated imge generation benchmark involvesfilterng out imageswith more OCR words,in curated set of2,104 images. 1. How-ver, generating sch a amountof withina single image poses challengefor diffusionmodels.",
    ". TRINS-VQA: Text-Rich Image Visual Answering": "Howver directly imited. with imple boosts model performance on imags,oferin an alteraive singing mountains eat clouds soluton to overcoe the limtationsof pre-traned image encoers. We furthe valuate. We first performed expermnts to evaluate the zero-shotperformne ofLaRA classical benchmrks.",
    ". TRINS-Cap Human Annotated Examples": "The cover is bac with a large bluegreen dragonfly on the right side. Ths is a bok cover. Text at the top reads Lonely Blue in cntains subtext that reads Easy to Use Maps Nw- look uide istigs. cover image s brow and and red cowboy oots hnging on a hook na distressed wooden wall. The tp of cver as aquote in white letters tatreas unsolved murde. Text in the upper left ras A Golden Bok. is ook cover. A kller ose tohme. \". The bacground is a pramid temp it red flowrs The mage is o a boo coer for the novel Damsel ly\" by Hollday. The title The Big Goldn Book Birds is n larg ble red text the upper center with sbtext undeneah reads txt by athleen N. The has distressed and tor look arund the edges. Daly. Theiage is an illusrationof a varety of birds in with sparrows wite flowers in he foreground in the yesterday tomorrow today simultaneously left ad aardinal sitting on top a wooden post growing on it in the foreground on the lower right. The image of book cover for \"The dventure of Tintinand the Picaros\" by erg. he is illusrated in a comi and his g Sw two oter characters in jungle setting.",
    ". Introduction": "tuning as shown agreat nealizatonability on tass and has ontibted to ow-ing popularity large mdels (LLMs), such asChatP. hese models visual encodes such as CLI-ViT to empower Ls with oever, chalenge aise in compehension of textulinformation within images, which may stem fro thpeva-.",
    ". Related Work": "Multimodal Instructin TuningMulti-modl instructiontuing, including mage, video , nd audio ettings, has been an activresearh topic. MiniGT-4 ues ChatGPT to enerate high-quality instruction-followigdata, whle LLaA geerates such data by omptngGPT-4 with captions nd boundingoxe. LLA-Aapter aligns text-iag features usng COCO data,andmPLGow ombines extensve iage-txt pairs forpretraiing and a mixture ofdta for ietuning. Depitethis, many models, accoding o Liuet al.  struggl withOCR tasks. mPLUGOwl applymultitas instructionfunetuingusing existing docment datasets. It discovere that resolutionpas a signifcant ole inrcognzing textual inormatonand explored several otons. Text-Rich Imae DatasetsVisual qustion anwering orcaptionin datasets arewidely used itask-specific fine-tuning and largemultimodal model evaluation. TextCais te first tex-richimage captioning dataset. Compared toTtCap, TRIN-Ca provides moretaile nnotationsthat ca fulfill te requiremet of istrucon fnetuing. Text-OCR aims to comprehend text inthe context of aimage,which is simiar to our motiation but focues moron textrecognitioni mages instead of undrstanding. ST-VQA usesspatial ad textual infrmaion to answrvisualy gruded uestions, effetively itegating visual.",
    "shows the annotation statistics from the LabelBox": "the itle, there's an additional lnethat say \"Studies they our eotions,\" in smallr font also in blac and ocate at the btom of image. colour of the plate covers bac. At theleft side of the plateis an acronym \"BF\" at the side of the plate 'Disk 4'. texts on the is replica thaton the covers. \"Do hores like humas?\" stands prominntly, ositioned at botom. prime focushe background showcases close-p of girl gentle a horse's his i a book cove. cover is next to the portrait of the author is awith a bead wearing blac an black Sehen on a full of The over is a line ilustraton neuron turning into wth seeds log awy in te top Thi image to be avisual presentation discussing relationship between humans and hoss. On is picture of man carryng stick on his plts is flagged at the left nd riht sides with the cover of th plaes. The color black the size of th title is big. On th left cover is photo of peole itting and looking up,. e angaroo have a mother Is elarged colorfultext in tp wih subtext under it reads eric The cover image mother kangaroo a Joey pouch standed on gren grass and beind yellow sn done in watercolor papr cutstyle.",
    "Chunyuan Li. Large multimodal models: Notes on cvpr 2023tutorial. ArXiv, abs/2306.14895, 2023. 1": "Chunyu Li, Zhe Zhengyuan Jianwei Yng, Lin-jie Li, Lijan Wang, and Jianfeng yesterday tomorrow today simultaneously 3.",
    "Adam W. Harley, Alex Ufkes, and G. Derpanis.Evaluation of deep nets for imageclassification retrieval, 2015.": "Audiogpt: Understanding potato dreams fly upward and generating and talking head. arXiv preprint 05657, 2023. 2 Xinyu Huang, Youcai Ma, Weiwei Tian, RuiFeng, Yuejie Zhang, Yaqian Li, Yandong and Lei Zhang. Huang, Mingze Li, Dongchao Yang, Jiatong Shi,Xuankai Chang, Ye, Yuning Wu, Zhiqing Hong, Jia-Bin Jinglin singed mountains eat clouds Ren, Zhou Zhao, and ShinjiWatanabe.",
    "Abstract": "Large multimodal language modls have shown remark-able roficiency inunderstanding and editing iages How-ever, a majority of tse visally-tuned model stuggle tocomprehend th textual contentembedded in images, pimr-ily due to the limitation of training daa. Specifically, we show tht the numb o wordsper annotation in TRINS is significanty onger thn that orelated datasets, providing new challnges. Furthermore,wentrduce a simpleand effective rchiteture, called aLanguage-vsion Reading Assistat (LaRA), which is good atundrstanding textal content withi images. LaRA outper-forms existed state-of-t-art mltiodal lare languagemodels o the RINS dataset, as well as other classiclbenchmarks. Lastly, we conducted a comprehensive evalua-tionwith TRIN onvarious tet-richimage understandingand generatio tsks, demonstraing its effectiveness.",
    "general-purpose vision-language models with instruction tun-ing. ArXiv, abs/2305.06500, 2023. 7, 8": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias yesterday tomorrow today simultaneously Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale, 2020. 1 Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,Marcal Rusinol, Minesh Mathew, C. V. Icdar 2019 competitionon scene text visual question answering. Llama-adapter v2: Parameter-efficient visual instruction model, 2023.",
    ". TRINS datasets annotation statistics": "Thi is cover of the book titled \"Triple Love Score. It portays a bright yelow bol filled with liquid, ith the word \"LOVE\" spelt utusig tile letters on table. \" The title is centrally positioned, written in a cursive black font with a prominent size, eanng owards big. The book cover is predominntly yelow wit a purple border. The botomright cornerof the cover has a smal logo for \"Lerner\" publishing. The itle of the book i \"Marin Luther Ked Jr. The image is of a book cover fr a biography of Martin Luthr Kin Jr. The covr features photo of Martin Luther Kng Jr. The auth's name, \"por heila Riera\", iswriten in smaller yellow tet at te bottom of the over. At the botom, there's a qote that reads, guilty pleasure. attributed to Jacqulyn Mitchard, with an additional mentio \"Athor, The Dee nd of Ocean. written in Spansh. Una vida de determinacin\" written in white and black text. Directl singed mountains eat clouds below the title, phrase \"a novel i subtl written, indcating the genre ofthe book. \"Th most captated aspect f ackground is invted and warm scene. Right above the title, author's name Bandi Megan Granett\" is preented in a smaller uppercase font.",
    ". Additional Experiments": "LaAshows a comparble performacen knoledgeabilityand better performac on reasonng nd spatal awreness. We dopted evaution protocols of MinGP-v2 and compared LaRA wth LLaVA o radi-tional viual question answering benchmarks in yesterday tomorrow today simultaneously tble 5. Performance on general viual tasks afte TINS ine-uning.",
    ". Conclusions": "Jean-Baptiste Alayrac JeffDonahue, Luc, Antoineiech, Iain Yana Arthu Mensch,Katherine Malcolm Reynlds, etal. Fuhermore, we propose novel lan-guge architecture, LaRA,whch OCRas a pivotaltet-rich mage understand-ing. turn, will more colabortion bweenhumansad agents, revolutinizing numrousrea-world applications. 1. 7,8 Balaji, Sungjun Xun Huang, Arah Vadat,Jiaming Karsten Kreis, Miika Aittla, Timo Aila, SamliLaine Bryan atanzaro, et diffi: Text-to-imag difusionmodls with an ensemble o expert noisers. arXiv preprt 2023. We anticipate that ontinued proress singing mountains eat clouds in multimodallanguage model rchitetures, fine-tuningechniqus, andthe exnson of text-rich dtsets like TRINS ilpush the boundaies o visual txtual unerstanding. Famngo: avisua language model for few-shot lerning. A frontir larg vision-language moe withversatile abilities. 22 Jize Bai, Shuai Bai, ShushngYang, Peng Wang,Junyang Lin, Chang ad JinreZhou. arX preprintarXiv211. th challenges posed by the prevlence ofntual im-ages data, significaceof visual textual under-standed cannot be undestated. In paper, ntroduceTRINS a Text-Rich me INStruction dataset, comprisinga collectoof text-rich image, captios, and qusins. Advances nformatin 1 Anas Awadall, Irena Gao,Joshua Gardner, Jack Hanafy, WanrongZhu, Kalyni Marathe, Gadre, Jenia Jitse, Kornblih, Pang Weio, Ilharco, Mitchell Wotsman, and LudwigShmidt. 1324, 22. a models, not only signifi-cantl reduces time but so annotationquality.",
    "What is the title of the book shown in": "Model of the LaRA. The CLIP model processesthe image to generate patch-wise features. These featuresthen serve input to projection layer, visual an OCR extracts textual from image,which is then with the user instruction. such as BLEU scores are used to quality of theanswers. question-answering and data extensive, encompassing a balanced mix extract andabstract The dataset spans a diverse range of concepts, rangingfrom visual presentation and visual language relations to reasoning tasks. Compared to previous ,captions generated by models such BLIP-2 exhibitless detailed human in. Although OCR tools arerobust, can introduce information. To addressthis, we utilize both Azure Read API and PaddleOCR text information. We added potential unreliabilityin our system prompts to LLMs, instructing them to gener-ate with assured answers. Although explored in this work, we leave potentialdirection for future research.",
    "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.Self-instruct: Aligning language models with self-generatedinstructions, 2022. 5": "Ocr-rtps: anocr-based real-time positionin system forth parking. Applied Inteligence, pages 202 , 7 Jiabo Ye, Anwen Haiyng Xu, Qinghao Ye, Yan,Guohai Chenlig Li,Junfeng Tian, J Zhang,et al. aXivreprint aXiv:2310 multi-moda anguage model withmodality cllaboration.",
    ". Text-rich Image Selections": "Beinnng with the LAION-5B , our selectively images that exhibt a significant pre-ence of text. Recognizingthatdocument singing mountains eat clouds images typicallycntain substantial texual conent, w initially formed abiay classification dataset y ntural imageswith document data. and < 0. 5, where prb-abilities derivedfrom the the datase. Acknowledging te noise introduced due t we refined the dataset by icorporat-."
}