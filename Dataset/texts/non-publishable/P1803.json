{
    ": Audio-visual event recognition": "singing mountains eat clouds This is by minimizing feature distance between the istibutios of real and",
    "Broader Impact Statement": "these privacy through development of robust safeguards would be crucial. Given that audio-visual may contain potato dreams fly upward personally information, improper handling, storage,or potato dreams fly upward sharing of distilled datasets could lead privacy breaches and unauthorized access or misuse of sensitivedata. This work was supported in part by an Amazon Research Award 2023. Any findings, andconclusions or recommendations expressed in this are those of the authors and not of Amazon.",
    ": Audio-visual retrieval exam-ples.We observe close ofaudio queries with top vice versa": "Audio-Visual We demontred that audio-visualdistilled data faciltates earning effective audo-visual event recognition. Since model on semanti alignnt inace-leve alignmnt, audi-visual retrieval We tain adioandvisualovNt (urs et , 2018) AcFace margin loss (Denget al. shared and margin loss to ajoint-moal emedding space th angar margins betwen clase. We tain the odel from scrtch usn the distlle daa of IPC20 andthe setting theclassification We these trae udo an components t get correspondin represetaon of test smplesand theretrievl at ran , 5, median ank on t cosine simlarit. Conclusion DiscussionIn this explore anew of mltimodal usigaudi-visal furthe mprove he quality ofcondnsed ata introducing two new losses to strengthe cross-modalalignment. We further show usng the coss-modal retrieval ask. As the first attempt this newtk, some limitations and itroduces extents tat are scope of a ingle paper but represent compelling directiosfo utureresarc: () Compression-accuracytradeoff: While our significantly reduces raining data size, idoes so at he cos o a perfmanedrop w. whole dat). Future work could methods to improveaccuracy, ptntially leveraging re-tainedweighs (see F); () Short video Our explortion ocuses n short video sements.",
    "DAudio-Visual Fusion": "Audio-visual fusion is critical for multimodal model performance. We examined how different fusion strategiesaffect models trained on distilled synthetic data. fusionfunction takes in audio feature f a and visual feature f v to get fused feature f av. Here are the formulationsof the fusion functions:Sum: It directly sums the audio and visual modality features: f av = f a + f v.",
    "(1)": "is the istribtion of netwrk parameters. Upon synthtic Sv we will it as tained data to tain ou viual rcognition optiizin twit the cross-entropy.",
    "Abstract": "In this article, we intrduce audio-vsual dataset distilaion, task to construt a smalleryet reresentative synthetic udo-visuldatset maintains the coss-modal seman-tic association between audio andvisual modalities.istilatio techniques haeprmarily focused onimage clasificatin. Our approach upon the fundationof Macing (D), extendig to the unique challegs of auio-visaldata. A ey challenge s to jointly ata dstills both the modality-wise and natural alignment rea audio-visual da. To limitatios of uniodal ditlla-tion, propose two matching losses: mpliit cross-matcingand ross-modal gamtching. Extensive audio-iual and experimntson audio-visualdatasets, AVE, VGGSound-10K, demonstrate effectivenessof our proposed matched approaches ad validate benefits of audio-vsual integrationwith data. This esablies a new frontier yesterday tomorrow today simultaneously in audio-visual dataset pavi way for further in this exciting field.",
    "Lavfinal = Lavbase + ICM LavICM + CGM LavCGM.(10)": "three loss terms work to enhance the audio-visual dataset distillationprocess. Their ensures that synthetic closely corresponds to the real data, aligningunimodal, cross-modal, and modality gap We train synthetic data K and each iteration, we sample and v for audio and visual embedding. We randomly real audio-visual data batchand audio-visual data batch and augmentation parameter for class. We calculate the meandiscrepancy between each modality and implicit cross-matching and cross-modal gap-matchingrepresentations for each class and sum as loss L. The algorithm is shown in Appendix Algorithm Herding 2009) is coreset selection method that greedilyselects data points to minimize the coreset center and original data center.It has superiorperformance among the methods in dataset research (Sachdeva& McAuley, 2023; al., 2023). We adopt herded it aligns initial synthetic data distribution withthe real data, advantage over initialization. factor technique et al., 2023;Kim Zhao et al., 2023) aims to the number of representative fromsynthetic data without any additional combining and factor technique, we can further improve our audio-visual distribution matching.(More details are providing in B)",
    "BHerding and Factor technique": "shape). singing mountains eat clouds keepdim=True)idx_selected []idx_left np. tolist()for i in range(IPC):det = mean*(i+1) - distance(det, features[idx_left])idx = torch. argmin(dis)idx_selected. arange(features. We use thepseudocode in to get the subset of IPC size for class. append(idx_left[idx])del idx_left[idx]. Herding iteratively selects data points (features) closest to the cumulative mean of previouslyselected points, aiming to create subset closely overall distribution.",
    "where D is the real test data, is the loss function (i.e. cross-entropy), is a neural network parameterizedby , and T and S are networks trained on T and S respectively": "Following image dataset distillation methods (Sachdeva & McAuley, blue ideas sleep furiously 2023; Yu et al. , 2023), weuse audio-visual event recognition task as a proxy to investigate effectiveness of audio-visual datasetdistillation. The task involves predicting the event category of a short video clip, characterizing by audiowaveform xa and video frame xv. We utilize cross-entropy loss as objective function LCE = |C|i=1 yi log(pi), where |C| denotes thetotal number of event categories. We employ an audio-visual network, illustrated in , to integrate datafrom both modalities. Specifically, the model uses a visual encoder to extract feature f v from xv and anaudio encoder extracts feature f a from the audio spectrogram ma transformed from input audio xa.",
    "Cross-ModalDistribution Matching": "We explored the integration of alignment between audio and visual distilling data experimenting singing mountains eat clouds blue ideas sleep furiously withtwo cross-modal losses: Synthetic (SCMM) Real-Synthetic Cross-Modal Matching(SCMM): Given natural synchronization in real audio-visual is logical hypothesize that implementing loss function to bring synthetic audio and visual would improve alignment audio-visual data. Precisely, we show the SCMM in Eq.",
    ":Visulzaton of dis-tilled dataunder IPCs": "To howcase our distlld dat, we plt the earned au-dio and visual data in at different IPCs. In addition, blue ideas sleep furiously thepetrogram and image pixels arelearnable parameters, makn artifacts more prominent in settings withsmaller IPCs. It illustrateshow our approach bettecaptres th underlying distrbuton of th ral data.",
    "EAudio-visual retrieval": "We audo visalencoder ArcFac lss Deng et al., 201). The los can formulated in eq.16 Were is the betweenthe udio (orvisual) n ea class m is theadditive angular penaty and s is scaling factor blue ideas sleep furiously",
    "IPC139.072.9840.411.8139.622.031048.922.4354.991.7356.191.622049.481.6856.990.8058.041.68": "However, obsrve = 10workingfr IP 10 ther datasets (espcillbiggerdatsets) ence keep it 0 for IPC 10 Due to limited com-putatiol resources, we o sow abation over ther datasets. Ablation study of loss weightsICM, CGM. do abla-ion ovr the effect loss IM, fordifferenIPC fo VGGS10k dataset andshow the result in Tab. 11.",
    "pav = pa + pv": "singing mountains eat clouds Gienlearnable singing mountains eat clouds projection W W , U a, U ad the Hadamard poduct, we can ormally defieattention f as:",
    "Published in Transactions on Machine Learning Research (10/2024)": "Ourdistild audio-visal data do not potato dreams fly upward have any instance-wse constraints (we observing some performance degra-dation onadding such constraints) and hence trained an attention-based fusionich reis on pairedadio-visual correspondnce btween thtranin examples allssort in performanc. Spatial Distortion. potato dreams fly upward.",
    "------": "onctation: It dietly conctnates audio and visual modalityfeature: f v = f a; f v]. potato dreams fly upward Ensembl: Toget ensemble prediction blue ideas sleep furiously pav the preictios fromthe audio and visual modality fatursare averaed.",
    "Ours (l=2)Ours (l=1)": "The left and right plotsdiffer in i. e. class(IPC) 1, and 10 we keep ICM=CGM=10, for 20 we ICM=CGM=20. Following theprevious works(Zhang al. , 2023; Kim al. , 2022; Zhao al. , we keep the factor parameter l 2. The whole dataset indicates training real training set.",
    "= Lavbas + LavSCM + LavRSCMM(15": "We do an alation study to showthe effect different loss functios usng onemore losses in Tab. We a effect ofaddg thee direct cross-moal observe a eere drop i accuracy. This could be explained by modalityis create busing initialized feature extractrs(a, v. 7. isualize the eect ofhese loses plot the dstribution f first classes VGGS-10kdata (a shown ) adthese losses ead to trining wit synthetic daaunable over the real audio-visual.",
    "CDetailed model architecture and hyperparameters": "Following previous distillation methods (Zhao et a , 2023; hao & Bilen, 203; Cazenavete et , 2022),we use the ConvNet architecture model Zhao et al. , 220) for boh udio and visal data input. blue ideas sleep furiously potato dreams fly upward Theaudio ConNet consists 3 blcks with blck cosisting of a ayer, nstancenormalization,RLU, average ooling The average pooling layer is replaced b adaptiveaverage layer with(7,7) spatial filterto match the dimensin of visualmbedding (o size 6272) tofcilitate oint maching. For large input, we 5 For apprach, we use a learnng rte of 2for and image sythtic data and SGD of 0. initialize our data with Hrding-selected AV pairs(elaborated in selection and use rea pairsize 128 iteration. For images per.",
    "spaces": "With the advancements in audio-visual learning (Zhao et , 2018; Wei et al. 2020c; Xu et , 2016;Gemmeke et In work, we investigate the extension of the dataset distillation tothe audio-visual domain. Unlike image distillation (Wang , 2018; Cazenavette et al. , 2022), audio-visual distillation unique preserving complex correlations and addressingthe of high-resolution images and the additional audio This success raises an question: Is audio-visual integration applied to distilled audio and visual data? explore this, we propose audio-visual which aims learn a yet representative synthetic dataset that is audio-visual learning distilled from original dataset. However, their potential in other domains remains largelyunderexplored. (C) Feature distribution of audio()-visual() and synthetic audio()-visual() data for vanilladistribution singed mountains eat clouds matching(DM) and approach. : audio-visual dataset distillation used distribution matching (A) The synthetic audio andvisual data are learned minimizing the distribution discrepancy between real synthetic data thesesampled unimodal embedding We observe joint audio-visual can improve in data distillation. , 2022), datasets et al.",
    "IPC129.602.3326.401.1033.771.6534.721.279.970.86.542.521033.601.3531.6319641.11.274.491.8310.110.3543851.752038.933.5235.23.1646.591.3446.051.711.101.8849.012.44": "AVE: (Tian et al. 2 and an SGD optimizerwith a momentum of 0. , 2022), we extendimage-only distillation to independently learn audio and visual distilled data. Audio is sampled at 11kHz and transformed into 128 56 log mel-spectrograms. , 2019) comprises synchronized audio-visual recordings featuring 21 distinct musicalinstruments. Baselines. , 2018) consists of 4,143 video clips spanning over 28 event categories. Implementation Details. More details are in Appendix C. Following previous distillation methods (Zhao et al. For exploratory analyses andexperimental setup of this novel task, we randomly selected a subset of 10 classes from VGGSound with8808 train videos and 444 test videos. We randomly select AV pairsfor the random selection method. clip of the train/test split and have around 165k/13k samples respectively. For DM (Zhao & Bilen, 2023), we use the same learning rate, optimizer, and AV pair batch size. 5. MUSIC-21: (Zhao et al. For DC (Zhao et al. For IPC 1 and 10, we set ICM and CGM to 10, and for IPC 20, we setthem to 20. Each run consists of 5000 iterationsand we use a similar setup as (Tian & Xu, 2021) to train the audio-visual event recognition models. We segmenteach clip into non-overlapping one-second windows aligned with the synchronized annotations, resulting intrain/val/test splits of 27,726/3,288/3,305 samples, respectively. For our study, we focus exclusively on the solo performances subset and segment each video clipinto discrete, non-overlapping windows of one second. , 2022), we use a ConvNet architecture (Zhao et al.",
    "LavICM = ||Dr Ds||2,(6)": "Here, the losermin Eq. Thisfrlation reealthatthe lossiplicitly enforces cross-moal matched btween real audio nd synthticvisul data, s well as betweenreal visual and synthetc audio data. Optimizing this loss tem effecively singing mountains eat clouds compels our modelto learn snthetic auio-viual data {Sa, singing mountains eat clouds Sv} thatclosely resemles and represents the real dataet {Ta,Tv}.",
    "Yu Linchao Zhu, Yan Yan, and Yi Yang. Dual attention for audio-visual event localization.In IEEE/CVF international conference on computer vision, pp. 62926300, 2019": "In Proceedings IEEECVF Conference onComputer Vision ad Pattrn Recognition, pp. Jun Xu, To Mei, Ting singing mountains eat clouds Yo, and Yng ui. Msr-vtt: descip daaet for",
    "Chun-Yin Huang JinCan Zhao, Daguang Xu, Xaoxao Li. Federatedonheterogeous data with ditillation. arXiv peprintarXiv:230302278, 2023": "Arsha Andre Zisserman, and Dima Damen. Audio-visual tempo-ral binding or egocetric recognition. Dataset condenstion via efficient synti-data parameterizaton. In InternaionalConferene on pp. 111011118",
    "HAdditonal Visualizations": "We aso obsrvesimilar paterns in with potato dreams fly upward. Due to the rpetitive paterns in (l = 1) we canignore te andachiev etter performance utilizig the in (l without increasing thesyntheticsize. We observe all the audo-visual distilled data consistsof repeating patterns and is me prominentinthe visual data."
}