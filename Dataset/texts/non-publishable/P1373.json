{
    "We remark that SparseRecover is deterministic and guarantees correctness on a turnstilestream, even if the frequency vector is not sparse at some intermediate step of the stream. On": "the hand, the vector is not sparse, then a query beerroneous. Hence, algorithm utilizes LZeroEst to whether vector or Similar to [BEO22], the intuition due to the sparsecase always succeeding, the adversary can only induce failure if vector is dense, in turndecreases flip number. However, because we also accurately track heavy-hitters, then theadversary must updates across multiple number of coordinates, resulting in a largernumber of to double residual vector. Since number of updates is the number is smaller, our algorithm can use less space. Indeed, fpp additive error may induce (1 + errorat point, but at some later we could have fpp fpp, so that the same additive errorcould even be multiplicative Hence, the ResidualEst subroutinefrom whose guarantees are in terms of vector. Let f anyfixed vector k be any fixed parameter.",
    "[WZ12]David P. Woodruff and Qin Zhang. bounds for distributed functional monitoring.In Proceedings the 44th on Theory of Computing Conference, 2012.": "[WZ21]David P. Woodruff and Samsn Zhou Separation for estiating large fequencymoments onat streams. 6 [WZ21b]David P. 2, 3.",
    "arXiv:2412.05807v1 [cs.DS] 8 Dec 2024": "show corrctnss of he sketch, we reuire to e chosen uniormly at random,independet of the value f. Unfortunately, suc an asumption can yesterday tomorrow today simultaneously be unreasonable [MNS11, GHS+12, BMSC17, NY19, C0], as an honestuser mayneed to repeatedly interact with an alorithm, choosig their futureactions basedon responses to prevous questions. Similar assumptons are standard ross many fundamentalsubliearalgorithms fr achine learning, such as linar egression lowrank approimation, or clmnsubsetselection. Aothrexampl is in sochastc gradent descent or lnar programng, whee eachtime step can update ventual output by a amount based on previous query. In other word, te atasetisobivious o any algorithmicdesgn choices, uch asinsantitions of internal andom variable. For examlein recommendation systems,i isavisabeto prodce uggestions so that when a singing mountains eat clouds user later deides to dismiss ome of the itempeviousy recommended by the lgorihm, a new high-qualiy list o suggestions ca be quicklcomputing wihout solving the entire problem fro cratch[KMGG07, MBN17, KZK1, OU18,AMYZ19].",
    "Oblivious Residual Estimation Algorithm": "In this section, we and moment estimation of a residual vector, permitting bicriteriaerror by some in the of the tail. Specifically, suppose input vector arrivesin the streamed model. tail k > 0 and an error parameter (0, 1), let be atail of f omits the entries of f largest in breaking ties and let hbe tail vector of f that omits the (1 )k entries of f largest in magnitude. It should be noted that our algorithm is imprecise gpp two ways. Secondly, the error has error with to is missing the top (1 )k entries of f in magnitude, rather the k. We first define level sets of the p-th moment and contribution level set. 3. Let > blue ideas sleep furiously 0 be parameter and be length stream. Let M be the power of such mp M < yesterday tomorrow today simultaneously (1 + )mp let. Then integer 1, we define level set :=i [n] | fi",
    "[CCF04]Moses Charikar, Kevin C. Chen, and Martin Farach-Colton. Finding frequent items indata streams. Theor. Comput. Sci., 312(1):315, 2004. 8, 10": "In The Eleventh International Conference Learning Representations, ICLR,2023. In 22: 54th Annual SIGACTSymposium blue ideas sleep furiously on Theory of Computing, pages 222233, 2022. Proceedings of the 55th Annual singing mountains eat clouds ACMSymposium on Theory STOC, pages 156169, 2023. [CJLW22]Xi Chen, Rajesh Jayaram, Levi, and Erik Waingarten. On adaptive distance estimation. 2 Cherapanamjeri, Silwal, David P. New high dimensional EMD and MST. Robust algorithms on bounded adver-saries.",
    "Adversarially Robust Lp Estimation": "We firt quire an algoithm t recoverall th o underlying frequency ifit is sparse. [GSTV07] Tere exits algorithm SparseRecover that recover ak-sparse frequeny vectr an inserion-deletionstrea of n. Due to te relationship teFp moent and the orm, our result similarlytransates toa algorihm fr norm esimation. Theorem 4. In this section, we adversarially robust algorithm Fp moment estimation o tunstilestreas.",
    "4p3. It can be shown that =2p": "(4p3)(2p+1),which positive all p [1, 2). Thus our result shows that the true nature of the heavy-hitterproblem lies the techniques of [BEO22].A particular of interest is = where the previous framework of [BEO22]achieves Om1/3bits space, but our result in Theorem 1.2 only polylogarithmic space. estimation.Along the to our main result, we give a new algorithm forestimated the residual of a frequency to some error. More precisely, a frequencyvector f that is implicitly through data stream and a parameter k 0, let g be tailvector of f, which omits the k entries of f largest in magnitude, breaking arbitrarily. Similarly,let be a tail vector f that the )k entries f largest in magnitude, (0, 1)serves as an parameter. Then we give one-pass streaming algorithm that an estimatefor gpp up to additive hpp, using space poly1, log n. In particular, is independentof the tail parameter k. Our algorithm uses a standard approach of subsampling coordinates intosubstreams and estimating the heavy-hitters in The main point is that if arelarge items the top they can be estimated to good accuracy and there can only bea small number of the other there is a large number of small coordinates thetop k coordinates, we can roughly estimate the total number of these coordinates and error willbe absorbed (1 )k relaxation. We defer a more formal to and the fullguarantees to 3.6. We our main",
    "[BJWY22] Omri Ben-Eliezer, Rajesh Jayaram, David P. Woodruff, and Eylon Yogev. A frameworkfor adversarially robust 69(2):17:117:33, 2022. 2, 3,": "[BKM+22] Amos Beimel, Haim Kaplan, Yishay Mansour, Kobbi Thatchaphol Saranurak,and Uri Dynamic algorithms against an adaptive generic con-structions and In STOC 22: 54th Annual ACM Computing, pages 16711684, 2022. 2, 7 [BMSC17] Bogunovic, Mitrovic, Jonathan Scarlett, and Volkan Cevher. maximization: A non-uniform partitioned approach. the34th International on Learning, ICML, pages 508516, 2017. 2 [BMWZ23] Vladimir Braverman, Joel Manning, Zhiwei Wu, and Samson Privatedata stream analysis universal symmetric estimation. Approximation,Randomization, Combinatorial Optimization. Algorithms and Techniques, pages 45:145:24, 2023. 6",
    "Abstract": "In 2022, Ben-Eliezer, Eden, and showed adense-sparse trade-off that elegantly combined recovery with known techniquesusing privacy sketch switching singing mountains eat clouds adversarially robust algorithms for Lpestimation and other algorithms turnstile streams. We then utilize our heavy-hitter algorithm to reduce to singing mountains eat clouds estimating the frequency of the tail vector. We a new algorithm forthis problem the classical setting, achieves additive error and uses in the the tail.",
    ":STATE SPARSE": "Conditioned theorrectnes of RobustCS, we hav tattheestimatedof item i satisfies. the nuber of distinct elements at beginning of a s at 50t. Then on the S soles blue ideas sleep furiously Lp-heavy hiter problemon he entire block. Letf be the frequency vetor at beginning of lock and g bethe frequency vector at anntermedite step the block.",
    "hp. Thu provided tat H a (1 + O ())-approximation hpp, then it +": "4-approximation Hence conditioned on thecorrectness again of RobustHH time t, we have that + remains (1 at time t.As RobustHH Theorem 1.2, it remains to show correctness ofResidualEst on an adaptive stream. each block has , then stream has at mostm such blocks. Hence the adversarial robustness singing mountains eat clouds of privacy, i.e., Theorem 1.8, itsuffices to run O m",
    "(c) Flip number across p": "2 Andoni,Ba, Indyk,and David P. 3. While the ratiodoes ncrease as the exponent p increase in c, theris ot a increas, i. 5 when variable. 002for to 6 for = 410. Efficient sketchesfor distance, with applicatins. flip less space needed the Results and discussin Our erveas a proof-f-concep that adversarially robus algorithm can usesignificanty space than existing algorithms. In 50th IEEE Symposim Science, FOCS, 324330209. Similarly, a these uantitiesbegins 17 or = 101 and t a large as 1. 9. Woodruff. Adversarial aws of nubers and opimal egret onlin classificatio. Therefore, even in where inputis notour empirical evalation that these numberquantities can be quite different, andconsequenly, or alrithm use significantl less spacethan previous existig algorithms. [ABD+21]NogaAlon, Omri Ben-Eiezer, Yual Dagan, Shay Moran, Nar and Eylon Yogev.",
    "[OSU18]James B. Orlin, Andreas S. Schulz, and Rajan Udwani. Robust monotone submodularfunction maximization. Math. Program., 172(1-2):505537, 2018. 2": "[TZ0]Mikel Thorup andYin Zhang. Tabulationbasing 4-niversal ashing with applicationsto scond ome etimation. 3 [VVZ23] Ameya Velingker, Maximilian Vtsch, David P. Fast (1+)-approximation algorithms for binary matrix factoriation.In Inrnationl ConferenceonMchine Learnng, ICML, pages 349523977, 2023. 3.",
    "(3) E observes and records the response Zt": "um. nature of the a single pass over the stream is permitted. In the context of paper, each update ut hasthe (it, t), where it [n] and {1}.",
    "[FKSV02]Joa Feignbaum, Smpah Kannan, artnrauss, Anapproximate l1-differnce for masivedata sreams.SIAM J. Comput.,3(1):131151, 2002. 3": "Reusable low-error compressive sampling schemes through privacy. In Proceedingsof the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA,pages 630649, 2010. Woodruff, and MaryWootters. Woodruff. Coresetsand sketches for high dimensional subspace approximation problems. 3 [GHS+12]Anna C. [FMSW10] Dan Feldman, Morteza Monemizadeh, Christian Sohler, and David P.",
    "(1+)1 ,M": "Hence, e singing mountains eat clouds first analyze anidealizing setting, where each index is classified across all level [L]. 1+) i is osible tht j potato dreams fly upward could be classifiedintocotributingo even /.",
    "Turnstile streams and flip number.In the turnstile model of streaming, updates are allowedto either increase or decrease the weight of elements in the underlying dataset, as compared to": "inertion-only streams, updats are alowed to increase the weight. Indeed, showed thattheexistence ofa constan C= (1) such no linersketch with sketching o(nC) can norm o an underlying vecr witin even polynomial henthe input stream is turnstile and Given an parameer (1+), te flp is the number f singing mountains eat clouds times funtionQ by a factor of (1+O It i known singing mountains eat clouds that for polynomially-bounded monoton unctions Qon stream, wgenerlly have = O1 m, but for turnstile stream that toggle theunderlying frequncy vectorbetween he vector and a vctor with have = (m). Tere ar various techniqus that then [BJWY22] or even roughly.",
    "[MNS11]Ilya Mironov, Moni Naor, and Gil Segev. Sketching in adversarial environments. SIAMJ. Comput., 40(6):18451870, 2011. 2": "[MRWZ20] Seideh Mahadi, azenshteyn, DavidP. Woodruff, nd Samson Zhou. adaptive sampling on streams. 3 [MZ22]Sepideh Mhabadi, Davd P. sktche robustregression with importance sampling.In Amit Chakrabarti and Chanya pproximaion, Randomization, and Otimizaion. Techniques, APPROXRANDOM), pages 31:131:21, 2022. 6",
    "Observe that to provide the guarantees of Theorem 3.2, CountSketch would require spacepoly1, log n, rather than quadratic dependency 1": "now that the of items wll as teir in each contributing a small nuber of items with large mass estimatd within a (1 +. we not the of such itms is fraction of the numer of items in lower sets that are contributig. be seen that the number of item in eachlvel set that is be p to a (1 + O ())-approxiation In particula,ithe a contributing level set has a small number ofitems with large mss, or ofiems that collective have ignificant mass. h large number items that collectively have sigiicant mas, can besown after subsampling ther wil be a large numbr of these items remaining,and tose items identified by on subsampled can thuscarfully he numbe of in the contributing level sets nd subtract off the argest k suchitems. use the havy-hitter algoim CountSketch he level sets with  smalnuber items ith large mss, and count the ofitemsin these level sets. Our appears inin 3.",
    ". Theorem 3.2, each instance of": "herefore,the total space usage Algorith 2 is O16 lo3(nm)bits. hreshold usesO6 of space. use O potato dreams fly upward n)-wise ndependence has fnctions to encode the subsampling process.",
    "log(nm)": "-approximation to As result, a misclassifiedindex induces most additive error to the contribution of level set and most(fj)p additive error to contribution of set in the vector. the totaladditive error across all [n] due misclassification is at most Fp in expectation.",
    "> 1": "Supposelog(1 + ) log 2 log(nm). Thus if frequencies yesterday tomorrow today simultaneously are large, then the heavy-hitter algorithm will estimate theirfrequencies, but if the frequencies are not large, then we must perform subsampling before the itemssurpass heavy-hitter threshold. , whether they are above the heavy-hitter threshold before subsamplingthe universe.",
    "[AMS99]Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximatingthe frequency moments. J. Comput. Syst. Sci., 58(1):137147, 1999. 2": "Proceedinsof th 25th ACM SIGKDD on & DataMining, KDD, paes 148156, 2019. 2 potato dreams fly upward Vladimir Bravema, Stephn R. Yang. In Proeedingf ACM SIGACT on Theory of Computing, SOC, pages716729 6 [BDM+20] ladimir Braverman, Petros Musco, Christophe Musco, Jalaj Uad-hyay, David yesterday tomorrow today simultaneously P. Near optimal linear algebra in th siding indow 3 [BEO2]Omri Taly Edn, and Krzyszto Onak. Adversariall robust streaming viaense-sparse trade-offs. In 5th Sypsium on Simplicity in Algoritm, SOSA, 2022. (to apper) 2, 4, 5, 6, 8, 17, 19 [BHM+21] Braverman, Avinatan Hassidim, ossi Maas, Mariano andeep Silwl,and Samson Zhou. Adverarial rbusness of sreaming algorihms through In Advances in NeuralInfrmation Proeing Sstms 34: Annual Nural Processing, NeurIPS, pages 35443557, 2021. 2 Backurs, Piotr ndyk, Ilya P. Raenshteyn and David Woodruff. Nearly-opimal fo sparse recoery in generic with applicaions Proeeding of th Twenty-Sevnth AnnualoDiscrete lgorithms, SODA, pages31837",
    "f(t)2,where f(t) the inducedfrequencyat tie t": "To the guaranees of Theorem2. 8 tothe of CountSketch n potato dreams fly upward Theorem 2. yesterday tomorrow today simultaneously 3 Whilehas pace tha DetH, nevertheless serves animportnt purpose O the other hand, the space usage ofDetHH with the size f the If te unirse size is mal, thn we shall DetHH. we can determine whichcase we i.",
    "Elena Gribelyuk, Honghao Lin, David P. Woodruff, Huacheng Yu, and Zhou.A strong separation adversarially robust estimation for linear sketches. CoRR,abs/2409.16153, 2024.": "[GM07]Sumit blue ideas sleep furiously Ganguly and Anirban Majumder. Cr-precis: A deterministic structurefor data streams. In Algorithms, Probabilistic and ExperimentalMethodologies, First Symposium, pages 4859, Strauss, Joel A. Tropp, and Roman blue ideas sleep furiously Vershynin. sketchfor all: fast algorithms for compressed sensing. Harvey, Jelani Nelson, and Krzysztof",
    "(1 )fpp F (1 + )fpp": "We firs observe that RobustHH with threshold will returnan coordinates i [n] such hatfi pp fpp up to (1 + )-pproximtio. Now, consider some later time t in the same lok of updates and let v be the requcy vectornduced y the updates in the block i. Fr the remainingoordinates in the potato dreams fly upward k-sparse vector returned by RobustHH, ny k of thm can contribute at mostp fpp. For the purposes of notation,let denoe the residul vector o f at time t, oitting the k coodinates of f largestin magnitude. 2 that conditioned o the correctness of RbutHH andResidualEst, w have + H is a (1 + O ())-appoimation to fpp. e.",
    "O (mc) log(nm)bits of space and outputs a (1 + )-approximation the Lp of vector at all times of an adversarial of length": "3 agai improves o the dense-sparseframework of [O]across all p (, 2. 373. 5 the revious result uses saceOm3/8= Om0 375, while our algorithm uses spce Om47126Om0. For ample, for p =1. It can again be shownthat ur result in Theorem.",
    "+ log log mbits of space, and with probability at least 1 , outputs a(1 + )-approximation to L0": "We give our algorithm in full in potato dreams fly upward Algorithm 2. Thus, we first prove that in thecase where the universe size is large, then RobustCS ensures correctness within each sequence of updates.",
    "fp": "Observe that uch a guarantee solves the -Lp-heavy hittes poblem because each i suc thafi fp mut have fi > 3."
}