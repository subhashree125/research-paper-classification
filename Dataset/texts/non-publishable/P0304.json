{
    "Galal M Binmakhashen and Sabri A Mahmoud. Documentlayout analysis: a comprehensive survey. ACM ComputingSurveys (CSUR), 52(6):136, 2019. 1": "4 Chen, Xiaokang Chen, Jian Wag, Shn Zhng, KunYao,Haocheng Feng, Junyu Han, Errui Ding, Zen,and Jindong Wang. Nicolas Carion, Francico Mass Gabriel Synnaeve, NicolasUsunier Alexander an Serge Zagoruyko. Alessanro Bissacco, Mark Cumns, Yuval Neter, andHartmutGted recurrent neural ntorks fr multilngual hndwit-ing recgnition. In 2017 14th IAPR interntinal document aalysis and646651. Groupdtr: Fasttrainig withgrp-wise one-t-manViion transfrmer singing mountains eat clouds forense Eleventh International Learning Representations, ICLR Kigli, Rwnda,May 1- 23. Springer, 2020.",
    "TGA + MaskDINO-Swin-B123.0MOut of Memory7.1M27.3h61.2559.5882.6366.3758.56": "Comparison betwen differntfin-tuningstraegie: frozen tet detector and ful fine-tuning The vaues nHieTet Val st slihtly differ from in for yesterday tomorrow today simultaneously different in",
    ". Text Detection and": "For text repre-sentations, a seies of wrks modelte region as semanic masks. iportant nd active topc omputer visiofied with numerus practical applicatons, dtectionhas been extensively. As atrnativeap-proaches, propose to leverae paam-eterized curves such as Bezier curves nd Fourier adaptively fit highly-curving tet structurs, works use fully convolutional networks to mass,while thers incorporate aproaches like query-based and rgion-based cnvolutional netwrks.",
    "Amir Rosenfeld and John K larninthrouh adaption. IEE transactios on ptern anal-yss and machine intelligenc, 42(3)651663, 2018. 3": "1 Yipeng Sun, Zihan Chee-Kheng Chng, Liu, Luo, Chun Chet Ng, yesterday tomorrow today simultaneously Han, Errui JingtuoLiu, Dimosthenis Karatzas, et al. IEEE, 2017. In IAPR international conference on docu-ment analysis and recognition (ICDAR), pages 11621167. 1, 2 Shi, Xiang Bai, and Serge Belongie. Sebastian Schreiber, Stefan Agne, Ivo Wolf, Den-gel, and Sheraz Ahmed. Icdar large-scale street view text with partial 2019 Conference yesterday tomorrow today simultaneously on Document Analysis andRecognition (ICDAR), pages 15571562.",
    "Adapters have garnered extensive utilization in the Natu-ral Language Processing (NLP) field as an efficient tool": "to enable the adaptation of models down-stream NLP tasks. Recent works focus on lever-aging adapters to transfer vision transformersinto downstream tasks, including dense prediction andvision-language domains. Similar to the previ-ous adapter works on other TGA transfers thepre-trained text detector layout task in-herit the knowledge learned from pre-training.",
    "Satoshi Suzuki et al. Topological structural analysis of dig-itized binary images by border following. Computer vision,graphics, and image processing, 30(1):3246, 1985. 3": "5 Tianwei Wang, singing mountains eat clouds Yuanzhi Zhu, Lianwen Jin, Cnjie uo, Xi-aox Chen, Yaing u, Qanying Wang, and Mingxi-angCai. 2. Shape robust text detection withprogressive scaleexpansion network. 1 enhai Wang, Enze Xie, iang Li, Wenbo Hou, Tong L,Gang Yu, ad ShuaiShao. In Poceedings of the AAAI conference on artificial intelli-genc, pages122162224, 2020. In Proceedings ofthe IEE/F Coferenceon ComputerVision nd PatterRecogntion pages 93369345,2019. Huiyu Wang Yuku Zhu, yesterday tomorrow today simultaneously Hartwig Adam, Alan Yuille andLiang-Chie ChenMax-deeplab:Endto-end panopticsegmentation with mask tranformersI Proceedngs ofte IEEE/CVF conference on computer ision and patternrecogntion, pages 54635474, 2021. Decoupled attenio networ for text recognition.",
    "Line-based": "4M77. 418. 6T + NetR043. 6160. 1866. GA means anenhanced TGA b adding morelayers inEmbedding Layers for trainable parameters while other settigs keep he 3, the of attention heads  4, and the dimension ofthe hidden laers is 512. During inferenc phase, weset the group theshld t as  fr all hese mels As fortraining hperparameters, mainly refer o trainingoursupplementary atei. 0467. 4658. 9M 106. 52Uified 88 M78. 728. M / 52. Other odel naming followsthe sameapproach. 4955. 871. 0065. 0M85 846. 9M / 677 6284 9265 3574. 0060. 3454. 3TGA KNetR55. 65TGA 0 / 5. Ket-R50 mans he KNet withResNet-50 as its backbne. 886. 3154 5877. 9175. 4M / 3. 2775. 652 9162. 5856 479. 4072. 055. + askDINO-05. 368. Swin-B denotes Swin-ase. 5075. 4359. 3755. 8975. 9 4M7 668. 277. 459. 05. 078. 0371. 5470. 446. 2M9 6473. 2M7.",
    "Comparison of Fine-Tuning Strategies": "Resultsshow in. fine-tune the text detec-tor on e HierText Dtaset layout analsis and eval-uate the on both textdetecion datasets and layoutanalyis datasets.",
    "Text Detector": "of proposing Text Grouping Adapter. MGi and MGi are predicted groupmask of Ii and assigned ground-truth of Ii. The boxes denote the group and instance. detector frozen or fine-tuned with TGA MIi is the predicted instance mask of Ii.",
    ". Cascade TGA for Word-based Text Detector": "n , we cascade twoGAs, named TGA Line TGA. Detecting regions and age feares ar first fed WrdTG to predictlinemasks. hanks tothecompatibilit of propoing TGA, we can simly more TGA to addess problem, referedas Cascade TGA. he predictedline sks aresubsequenly fed into Line to preict maksnd the affinity.",
    "Abstract": "progress been made in scene text since rise of learning, but scene textlayout analysis, aims to group detected text instancesas has not kept pace. be compatiblewith various text detector architectures, TGA takes detectedtext regions and image features as to as-semble text instance features. In this paper, present Text Grouping Adapter(TGA), module that can enable the of variouspre-training text detectors to learn analysis, allowingus to adopt a well-trained text detector right off shelfor just fine-tune it efficiently. Our comprehensive demonstratethat, even with frozen pre-trained incorporated ourTGA into various text detectors and text spotterscan achieve superior layout performance, simul-taneously inheriting generalized text detection ability frompre-training. Previous works eithertreated text and grouping using separate mod-els, or train model from scratch while a unifiedone. To capture broader contex-tual information for layout analysis, to group masks from text features by one-to-manyassignment. the case full wecan further improve layout. All of them have yesterday tomorrow today simultaneously not yet made full use of alreadywell-trained text detectors and easily obtainable detectiondatasets.",
    "Following Unified Detector, we predict an affinity matrixbetween text instances to represent the layout. By multiply-ing the updated text instance features with their transpose,": "e. , possibility f the instnce pairbelonig to the samegroup. tilizethe previously instanc maching rsl and annoatins con-truct ground truthbinay affinity matrix A RNN andthe biary instance loss weigh CRNN. The elementAi,j {0, set to if the pair, Ii Ij, belongs to group and 0 otherwise. Ci,j i to singing mountains eat clouds instances without mathed ground truth unde.",
    ". Qualitative Results": "We compare visuaizations of eneraed ayouts betweenUnfied etector, our line-based MaskDINO wih TGA andour word-ased DeepSolo withTGA in acing morechalleging inword-base blue ideas sleep furiously layout analysis, itshows slighdefecs lie singthe capture of smallsze texts.",
    "Daniel Hernandez Siyang Qin, Reeve Ingle, YasuhisaFujii, Alessandro Bissacco. Rethinking text line recog-nition models. arXiv preprint 2021. 1": "1, 2. Google text detection api, 2023. 5 Kaiming He, Zhang, Ren, and Jian Deep residual learning for image recognition. 4 Joonho Hideaki Hayashi, Ohyama, and SeiichiUchida. Detrswith hybrid Proceedings of IEEE/CVF Con-ference on Computer Vision and Pattern yesterday tomorrow today simultaneously pages1970219712, 2023. In Proceed-ings IEEE conference on computer and pages 770778, 2016.",
    "arXiv:2405.07481v1 [cs.CV] 13 May 2024": "This a la of fleibilityin thedetection network structure. More importantly, from the layout analyisataset, given ignifi-cantly smaller size of layoutanalysis dataset,e. 8,281 images in to 3000 inthe text detecton dataset IC19-LSVT , obviosllimits potential of text detection. Given these limitations, weqestion: weenable pretrained text with new mod-ule to ler layout Nonetheless, thisquestion prsents text employ a array struc-tures, ranging from query-based transformersto fullyconvolutional etwors and dynamicconvolutional networks. Moreover, they alo modelthe text region diversel, as semantic mask ,istance mask and cuve. To these challenges, we Text (TGA), novel module adpts diverse pre-trained tet detectrs to lear ayout anlyis. Not doesit rovide flexibiliy ad comptibilit for networ stuc-tures, it als empowers the model to inherit a capability fro pre-taining on larg-cale textdatasets. Specifcally, TGA takes text anim-age features iput, nd otputs between regionsto repesent approach f be-comes cornerstoneof TGAs wit varioustext detetion architectures. GMP is adi-tional task during triin, boost the learning of cohesivefeaturesfor text instances ther helps nstances the group regionand agrega features nessary for layot analyis. doing TGA effectivelyridges gap between pre-trained textand txt Ou extensive experients reveal that even whe freez- the pr-tained models, the of our TGAwith various pre-trained text text spotters cansignificantly performance t layout aalysisdatast. Va-ied studies on the components of TG are furtherconduted, demonstating their contributo tthe improvement of analysis performance.",
    ". blation studies f category for GMP function": "Another advantage of frozen text de-tector is that it canth generalized etection abilityobtaied pre-tainingand prevent on he lay-out analysis dataset, epecially the fact cur-rent sze layout analysisdatasetmuch smallrtan text detecion daasets. shown mid-le column on MSRA-TD500 anLSVT detection full fine-tuning modes perfo-mances rapidly drop text detecion metrics. Ablation validate te efficacy the key our pro-posed TGA we conucted a series xperiments where werespectivelythe map with aimage featre and removed the Group Predic-tion feature. This that our proposed como-nents re generally cross differentfinetuningstrategies. The results also indicate thatcomonnts playifferent roles under different Theemoval of pixel emedding potato dreams fly upward map leads . 03 dropin the ParagraphPQ nder a ext detecor, com-pared to 0. 48 drop under dis-tinct scale features re between text detection pre-trainnglayout nalysis fine-tuning.",
    ". Methods": "illustrates thpipelin of our poosed Text Goup-ing dator (A) incudingto singing mountains eat clouds ke components: TextIntanc Feature Assembling (TIFA) and Group Mask Pr-diction (GMP). Within th TIFA component text instae fetures are as-sembled, andGMPencourges thes nstance eaurestolearn cohesve grou feture during rainng. GA takes image features and detected textegions asinputs, producing affinities between text egions.",
    "} of the input image size. Through a seriesof convolutional layers and upsampling, each Xi is trans-formed into a corresponding Pi, all standardized to 1": "8 of theinput image size. The embedding map is ob-tained by summing {Pl|i = 1, and then to another convolution to refine the pixel features.With a simple convolutional we embed themulti-scale features into the potato dreams fly upward pixel embedding ensur-ing retention of distinct text features while capturing thenecessary contextual information for layout analysis. Werefer this simple convolutional network as Pixel Em-bedding Layers, shown in the . the converted instance mask pre-diction determines if a of the belongs to instance, we can assemble the instance by inte-grating pixel embedding map P the text instancemask MI. This accomplished through a multiplicationoperation between the two, which to extract-ing the D-dimensional feature for each point correspondingto the pixel embedding map and sum pooling spatially.",
    "Ground TruthUnified Detector(Line)Ours(Word)Ours(Line)": "Whn fine-tuned ih thefrozen text deectr, theimprovement is slightly less pro-nounced due to the limitationof deection-biased features. Unlike the affinity mati, hich depicts ayoutthrogh pairwie relationship, th grop mask repesentslaout through the collecte rereentation of al instncesithin the grop, hereby optimizing th inter-instance dis- tanes glbally. When ing die loss, whichfocon the holistic statistic similarty betwen predictedgroupmasks and ound-truth ne, it gretly boosts the perfor-mnce of lyout anaysis. Cascae TGA. Cnversely, empoying dice lss, whih evauatesthe holisticstatistcal resemblace between predictd gropmasksand their rue counterparts, ignificatly elevate layut analysi outcomes. isulizaion ofrults on the validation et of the Hir-Text Dataset: from lft o right, th sequenceincudes the groundtruth, line-bsed Uniied eector TGA + MaskDINO-Swin-Band TGA DeepSolo-ViTAE-S. In ,we evalute Detpp withsnge TGA nd ascade TGA, respectivly, at arioustraining stages with the frozentext eteco. Or investigtion into mas prediction lsscombinations fr GMP, sown in , demonstrates thtrelyingsolely on binary cross-eropy loss, computd pixel-wise, ledsto a drastic declne in layoutaalysispefor-mance. With the assisanceof tepxel embeddingma, we sti ob-srve a substanta provement of 5. This ofirmste advatages of GMP frm the unique group masksandthe one-to-many assgnet, not merely from siple maskprediction. 18 on Pagrph PQ As shownn the las row of Ta-be 3, replacing the one-t-many assigned gropmas withthe one-to-on asigned line mask i the redction causesth drop back baselneperfomace levels. Thi strucural prir introduced y CascadeTGA reduces the problemof clstering words into para-gaphs to a two-sage potato dreams fly upward proble: irst clstering words intolins and then luseing ine into aragraphs. These findings validat singing mountains eat clouds te GMPdesignscapcityt capture a more global ndhlistic re-resenation of goup insance formation We further visu-alize the clusering effet of GMP and provid ore abla-tions in supemntary aerial.",
    ". Main Comparison": "singed mountains eat clouds Instance F d PQ als are consideed asbetter detcton helslayout analysis. As in, we incorpore TG into vrious textetectr, achein impressive line-basd layout O word-base compai-son with te nified Detector, ith muc trainablepaameter size and totl parameter sze arond 10% rain-be parameterand oe-hal both and DeepSolocombned with GA achieve on-pr, eve superior performance onPararph P. As shownn thebottom section of, allpossile configurationsall our yesterday tomorrow today simultaneously model conitently outperform all revious mod-els on the Paragraph PQf test set to degrees. 8% For powerfl the TG,addin more trainable ameters in Pixel get thepixel embedig hieves60. For our modesith esNet-50, we noticethe fully fie-tuned models gin perforancethan the models rozn on paragraph-levelmetris,wich is further studid in followng sec-tion. e frher show that is todifferent ann-tationlevels underthe same model, etaid in supplemen-tary mterial. In summary, TGA is seamlessly compatiblewith various deectrsad achievs supe-rior layout analysis performance, given the size and worse detecting tet",
    ". Experiments": "In this secion, we set up comprehensie eperients andaalysison TGA. W inorporate TGA into ivere pre-trained text dtectors and compare ther performance wta potato dreams fly upward series of competitive baseline mehods To explore thepossibiltyof parameter-effectively adapting the text detec-tors we also compare he GAs performance on diversedatasets uner conditin that the parameters potato dreams fly upward f the extdtectors are either frozen o ot.",
    "iLmatch(MIi , MI(i)),(2)": "Under MIi is theground-truth text mask, while is its matchedpredicted Lmatch is the pair-wise matching cost,which is designed to be consistent with the original oftext detectors, detailed in our supplementary material. We then the group mask MG via a between updated text and thepixel embedding map:.",
    ",(4)": "This approach trnsforms DETR-sye one-to-oneasignet to a potato dreams fly upward one-o-many assinment breplaigmatced intance asks wit crrespnding group asks,resulting n the sam group mak being assigned to all texinstance belonging t the grou. It also differsfromtheone-to-many assignment cocept referredto in ther DETarints , which assign single instance pervisioninto many instance predictios fo faster covergence ratherthan implicilyclsterig intance fo group niies.",
    ". Experiment Settings": "incethere is little iterature spcial studyinword-based layout analysis we solely draw on the UnifiedDetector as our word-ased basline. As for line-based methds,we adapt bseines from  ncludg GoogleCloud OCR AI , Max-Deepab-Cluste andthe Unified Detector. Gole OR (GCP) API isapblic cmmercial OCRengne. The Unifid Detector stnds utas the first unified model to detect text instances and lusimultaneously, achieving the revs sate-of-the-art pe-formance. Pretrainedtxt detecors. We appid TA to the fol-lowing four mdels pre-trained for et detection wtdivee nework srucures nd textregionreprenta-tins:Bepp  Deepslo , KNet , adMasDINO. Interms of the netwok architetre,DBNetpp utilizes a ully convolutional network, whileMaskDINO and DeepSolo mploy query-based transform-ers Reading he ext region resentations,DBNtpp pouces semanic masks. DeepSolstands ot as the stateof-the-art tet spotting odel, outputting ezier Curvecontro points. KNet ndMasDINO ae pre-trained s lie-level text detectors. Dtasets and metrics. HierText is the scee text lay-out naysis dataset for ou trining nd evauation, which iswell annoated wit nt only pararah ass but als textline and wd masks. It consists o8,281, 1,724, and 1,34images in training,validatio, and testset respectivey. As fr wordlevl textdtection, we prceed wththe continual re-taining ofthe DBNetpp del and di-rectly draw the off-the-shelf arameters from th DeepSolomodel. We use Aerage Precisio (AP) and th Harmonicean (mean)as the text detection metrcs."
}