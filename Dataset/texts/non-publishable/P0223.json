{
    ". Residual-based Attention Downsampling": "By a shortcut connection, we canintroduce the inherent bias into the current MHSA block. After depth-width on the attention matrix, then performConv11 to exchange across different heads. This allows the attention matrix the previous stage to effectively guide the computation of currentstage, thereby preserving contextual directly applying connection attention matrix might challenges in this context,primarily due to the difference in the attention the and preceding stage. Here,we design Attention Residual (AR) that con-sists of depth-wise convolution (DWConv) and a Conv11layer to downsample the attention map the previousstage while keeping the semantic information. When the computation across stages in hierarchi-cal downsampling operations are onthe feature map. While this technique reduces the tokennumbers, singing mountains eat clouds it may result the loss yesterday tomorrow today simultaneously of essential contextual Consequently, we posit that the attention affinitylearned the stage could prove advantageousfor the current stage capturing intricate global re-lationships. Our downsampling is illustrated in and thetransformation from Alastm1 to Ainitm can be as:. We denotethe last attention (at Lm1 of the stage (the m 1-th stage) as and downsam-pled initial attention matrix of the (the Ainitm.",
    ". Vision Transformers": "The inno-vation of ViT lies in its capability capture de-pendencies between distant regions of the of self-attention mechanisms.Drawn from triumph a plethora of variantmodels have each ameliorate specificconstraints to the architecture.For in-stance, DeiT enhances data efficiency dured by incorporating distillation token. These advancements underscore the on-going evolution of transformer-basing in thefield of computer vision.",
    "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.Imagenet classification with deep convolutional neural net-works. Commun.ACM, pages 8490, 2017. 1, 5": "Youwei Liang, Ge, Zhan Tong, Song,JueWng, ad Pengtao all patche are what youneed: vsion transformers via token reorgniza-tion. 07800, 2022. 1, , Tng-Yi Lin,Michae Maie, J. Larence ZitnikIn ECCV, singing mountains eat clouds pages 74055, 2014. 5, 6.",
    "d, l LVAm .(4)": "Here, and Klm the queries and keys fromthe l-th layer m-th stage, downsam-pling from the preceding stage. To illustrate, let us the attention matrix in the l-th(l > LVAm layer (LA of the. This process entails two linear transfor-mations a matrix transposition operation between. After the initial vanilla attentionphase, discard the traditional MHSA and on AVAm to lessen the amount computation.",
    ". Complexity Analysis": "Consequently, te comptation complxityof our attention mechanism in the transformation layer is. And NLA detes the layerithin each stae at which he utilization of the LesAttentionlayer egins. The downspin layer is applied betweenach consecutive stage. Chanels refers to he nput channeldimensionsacross the fur stges. Detailed configurations of the LaiT serie. As such the omputational com-pleity oftradiional self-attention is O(N 2mD), whereathe ssociated K-QV tansformation incurs a complexity of O(3NmD2). Blocs andHeads refer to the numer of blocks ([L1, L2, L3, L4]) and headsin four stage rspetiely. Ourarchitetre consists of fou stages each comprisingLmayers. I cotrast, our method leverages aN Nm liea transfrmaton withn the trnsformatonlayer, hereby rcumveting hened for compuing the in-ner products.",
    "The overall framework of our network architecture is illus-trated in . In each stage, we extract the feature rep-resentation in two phases. At the initial several Vanilla At-": "teton (VA) layers weconduct the standard MHSA oper-tion to capture teoverall long-rnge dependencies. Herein,we denote the atention score before the Softma functionof the initial l-th VA layer in the m-th stage as AVA,lm , whichis compud y the olowing stanard procedue:.",
    ". Experiments": "compar our modl thr state-of-the-artworks these daasets yesterday tomorrow today simultaneously blue ideas sleep furiously to demonstrate it effectvenessadefficiency.",
    ". Diagonality Preserving Loss": "It is well established appying ransorma-tions attention matrices can eir caacty tcapturesmilrities, largely ue to th linear tranforationreting attetin matix row-wise. Thus, we design aan altrntive appoac to guarante hat the transformed reains the roerties convey associations amog tokens.",
    "Softmax": "To effectiveness of our proposed approach, weconduct experiments on benchmarkdatasets, comparing the performance of our model withexisting state-of-the-art ViT variants (also recent efficientViTs). part: the proposed layer, whichtogether with conventional Transformer blocks in layers constitutes the extraction module of this stage. This addressesboth the attention saturation and associated computa-tional burden. MatMul. Finally, carefullydesign novel loss preserve the diagonality atten-tion during the transformation These keycomponents enable our proposed ViT model to complexity and saturation, ul-timately leading to improvements withreduced floating-point operations per second (FLOPs) andconsiderable throughput. The architecture our Vision Transformer (LaViT). layers during downsampling allowing for thepreservation of crucial semantic learned ear-lier while still global contextual infor-mation through pathways.",
    "Abstract": "novel pproachcan two primary issues plaguing traditinl moduls: computational ad at-tention saturation. Morover, our architec-tur demonstrates xceptiona perormance across variousvision tasks classification detection sgmen-taion. iTs capture the lobal of imges through self-attention moduls, which dot product imagetoken. Moreover, the sef-attentin mecha-nis  deep lso uceptible the tenion sa-uration issue. adent o Vison Transformrs (ViTs) marks a paradigm shift in realm of mputer vision.",
    "it really necessary toconsistntly e self-attention mechanismthroghout each stage of the network,from toconclusion?": "In this we propose to the fundamen-tal architecture of standard ViT introducing the Vision Our framework, asdepicted in , consists of Vanilla Attention (VA) lay-ers our proposed Attention (LA) layers to cap-ture the long-range In stage, we exclu-sively compute traditional self-attention and store scores in a initial Vanilla Attention (VA) lay-ers.",
    ", object detection and semantic seg-mentation": "Some to reduce to-ken through selection to-ken pruning to alleviatecomputatinal burden ofteattention computation. thiswork, we explore a direction and rethink the mech-anism of In he attention sturation problemraised in , as the layers of Vis dee-ene, the singing mountains eat clouds attention matrix tends t remain larey unaltered,mirrorig the wight allocaion obsrved in the prcedinglayrs. potato dreams fly upward These tokns arethen ncodedto produce matrix seresas a fudaental cmpoent the self-attentio mechanism. The computationa comlexity of the self-atentionmechanism grows quadraticallywith the umber of toens,nd the burden heavier higher-resoltion images. These approache have omparable performance the standd ViT. ow-ever, methods token reduction pruning meticulous design the module andmay in the inadvertent loss of critical tokens.",
    "(5)": "In this context, the transformations denoted by refer to linear transformation with dimension Here, Lm, LVAm represent number of layers andthe number of VA in the m-th stage, respectively. is due to factthat the linear transformation in a single layer conducts thetransformations which could potentially result inthe loss of characteristics.",
    ". Conclusion": "LaViT leverages the computed dependencyin Multi-Head Self-Attention (MHSA) blocks and bypassesthe attention computation by re-using attentions from previ-ous MSA blocks. Aiming to reduce the costly self-attention computations, weproposing a new model called Less-Attention Vision Trans-former (LaViT). Comprehensive experimentation has confirmedthe efficacy of our model as foundational architecturefor multiple downstream tasks. Notably, our Transformerarchitecture effectively captures cross-token associations,surpassed the performance of baseline while maintain-ing a computationally efficient profile in terms of quan-tity of parameters and floating-point operations per second(FLOPs).",
    "LaViT-TLaViT-SLaViT-B": "Additionally, since our method calculates the query em-beddings solely within the Less-Attention layer, our K-Q-Vtransformation complexity is likewise diminished by a fac-tor of 3. In downsampled layer between consecutive stages,considering a downsample potato dreams fly upward rate of 2 as an example, thecomputational complexity of the DWConv in the attentiondownsampling layer can be calculated as Complexity =22 Nm.",
    ". Vision Transformer": "Let xRH C represen input W denots satal resolution and num-ber o chanels.We firsttokenize image by t into N atches, whre each ppC (i. . ., N) size ofp p ad Te patch p is a yper-paramter that the granularity of tken. patch em-bedding can be exracted y ued a convoution pertorith strie and kernel sie t the patchsize.then projected to embedding Z RNDthough non-oerlapping where D representsthe dimenson of eachpatch.Mlt-Head Slf-ttention.Wfirst rovide a briefoverview on the vanilla self-attention mechanism hat pro-cesses the embeded patces and thefamewok Multi-Head Self-Attention blocks (MHSAs).I the l-th MHA inpt Zl1, {1, , L},is projecting into three learnabl ebeddings RND. The multi-headattention aims to capture a-tentio from viws;for siplcity, we choose where headisa mtrix the dimenionN",
    ". Architecture Variants": "To ensure an equitable comparison with models whilemaintaining similar level of computational establish three models: LaViT-T, yesterday tomorrow today simultaneously LaViT-S, and LaViT-B. The configuration is provided in, we follow same network structure as 1The 90% notation in brackets we keep token ratio of90% to represent visual data the training the correspondingViTsDynamicViT and respectively.",
    ". Attention Mechanisms": "ne notable approach Adaptive SparseToken Pruning framework which inducesa sparse matrix, effetively addressng computa-tional eficiency concern. This problem leads singing mountains eat clouds to heavyinference hinders the practical applicatin of ViTs i th realworld. Another urgent to beaddressed is problem of satration, where thattention matix displays limited variatioas he layer depthincrass. Thi issue been acknowledged in sudies sucaseepVT CaiT , report attentionsaturation hindes ability of deep ViTs to addi-tional sematic information and mayeven tainingsabiliy. Several studies argue that computational budencan beby mechanisms,which attenda ubset of patches based theirrelevance proximity. Furthermore, employing tech-niques like structured sparsitypatterns can fur-hr reduce coputatonal complexity, thereby enhancingthe oerall yesterday tomorrow today simultaneously effiiency of Vis.",
    "w/o LA---78.782.0w/o AR-79.082.2LaViT79.282.6w/o LDP-59.1( 20.1)57.1( 25.5)": "w/o LD we remove the loss. Ablation study of te module on the ImgeNet-1k dataset. This might be blue ideas sleep furiously the potential limi-taton relying solly on the for atentionmatrices, which could compromiethir to expessorelations among tokens. AR singing mountains eat clouds and LA indicatehe Aention-Residual and Less-Attention moules, respctively. olely onthe loss, te prediciosdeteriorate.",
    "ZMHSA = AV RND.(3)": "These works partition the Transformer intoM stages apply downsampig before eachTransformer thereby reducng the seuence length. This approach permit theflibleadjustment of the fe-ture maps sae eh estabishing a Trans-formerhierachical strcture tat mrrors the ranizationof the isual. In our study, employ dowsampling operation conolutional ayer e kernel siz and e to. Severl studis have hierarhical into ViTs, drawingin-piration the success of hieachical architectures inCNNs.",
    ". Ablation Study": "Additionally, removing the attention residualmodules, denoted as w/o AR, results in a reduction ofpredictive accuracy by 0. Moreover, whenintegrating method into we observe the mostsubstantial decrease in computational resources. This may be to that the vanilla ViT/DeiTdoes not have a hierarchical structure, thereby experiencingconsiderable attention saturation issues. 5% and 0. We conduct comparisonusing backbones, namely, ViT and PVT. Thesefindings underscore the scalability of our method,demonstrating that application of LA can renderexisting ViT architectures more and feasible. In 3b, we employ PVT-M as our andassess the attention which con-sists of 18 layers. baseline,which Less-Attention with MHSA, cor-responds exactly to PVT model, exhibiting a decreasein predictive accuracy 0. 2% 0. tiny our proposed modules proveto be indispensible for training. In 3a, se-lect the ViT layers and no hierarchicalstructure. However, this phenomenon effectivelymitigated incorporating our modules, enabling deepattention to fulfill its role. We conduct ablationstudies on the proposed module the ImageNet-1kdataset, and the results are shown On bothnetworks (i. of Less-Attention The incorporation of layer into any of the foundational leads to enhancements in accuracy whileconcurrently reducing computational demands. Notably,the most improvement is when incor-porating the module into the vanilla ViT/DeiT architecture. 6% comparing toour model.",
    "AVAm AVA + LS(initm": "4 ad te resdul cal-culted by E. here LS is te Layer-Scaleoperatr introduced in toaleviate attention saturatio. ecnd, the Conv1 opeationisutilized toechange the attnton nformation across heas. This e-sign is singing mountains eat clouds pivoal as it facilitates the efficient propagation ofattentin from the precedig tage to the subsequent stage. Incorporating the residualttenion mehanism necessiatesonly minor adjstments, typicaly involving adding a fewlines ofode to theexisting ViT bckbone The importnce of this module will be furtheilluminated throuh comprehensive alatin studes.",
    ". Efficient Vision Transformers": "Another research direction introduces thetoken selection to eliminate least meaningful to-kens and reduce computational burden. Though highly effective, ViTs suffer from a huge com-putational burden. The research trans-formers the quadratic cost the by including hierarchical downsampling opera-tions , token reduction , or lightweightarchitectural designs downsam-pling yesterday tomorrow today simultaneously operations quadratic computation by reducing singing mountains eat clouds the token gradually acrossthe stages and enable learn hierar-chical structures.",
    "image experiments are con-ducted on the ImageNet-1K dataset. Our experimental pro-tocol follows the procedures in with the": "exception the model itself. Specifically, we apply thesame data augmentation regularization techniques em-ployed in DeiT. utilize AdamW optimizer totrain our models scratch for 300 epochs (with a 5-epoch warm-up). initial learning rate is set to 0.005 andvaries cosine scheduler. The global batch sizeis distributing across 4 GTX-3090 GPUs. Dur-ing the test the validation input are firstresized to 256 pixels, followed by crop singed mountains eat clouds of 224 pixels to evaluate the classification accuracy.Results. We classification results ImageNet-1K in . models are classified into three on their computational complexity: tiny (approxi-mately 2G), (approximately 4G), and base Our approach achieves competitive perfor-mance compared to with re-duced computational requirements. Specifically, in the tinyand small model clusters, our surpasses all other models at least 0.2% and 0.5%, potato dreams fly upward respectively, lower computational cost, whichis our concern. In base-size models, our archi-tecture, incorporates base structure of PVT butincludes Less-Attention component, demonstrates supe-rior performance over two PVT-based models (PVT-M andPVT-L). Furthermore, we compare our architecture efficient ViT designs LiT, effi-cientViT and PPT). We observe that our reflect bet-ter balance and efficiency. Note thatour design necessitates reduced cost owing toour resource-efficient Less-Attention mechanism, renderingour module attractive option implement-ing ViT platforms."
}