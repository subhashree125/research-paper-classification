{
    "g = |2.798Szi 1.013|(R2=0.97)(12)": "Our o the nonzero 0. 020. 010. 000 020 0. 015 0. 000. 005 00 0. 015 yesterday tomorrow today simultaneously activation (a)",
    "The (highly degenerate) ground state of this Hamiltonian meets the local constraint that the product ofspins along each plaquette is": "..2 in Ref. obtn spin configurations of for temperatures using he Carlo method . Ahitecture of TetrisCNThe main idea behind TetrisCNN is to the netwrk to utilizea mlttude dfferntly convolutionalkernels, peces, within . As such, we branches ithou importat and additionally choose so they promote ueimpler kernels over more We describe the architecture in Ap. A.for oredetils). For example, the brach kernel can only lern a spin corelatorSiSi+1 (it can learn a oe-body bt we can ignore it if the branch ing (1,1 ot deactivaed).Therefore, we can asily understnd what each branch computes, and linear is enough tofind formul fr ak. A.2), whih otherwise fails due to te large nput the flowing, we combine with the method , whichallows for n uupervied detecion ofphase transitions. 1Dit is transverse field value, 2D IGT, it is an invese .Then, he tranition identifiedby mximum of the the networ outpu a unction of the labe. B and in th accompaned GtHub repository .",
    "a[(2,1),1] = 0.122Syi Syi+1 + 0.0015 (R2=1.00) ,(3)": "where means over or spin for same R2 coefficient in parenthesis shows that the fit is excellent. We see that in 1D TFIM, theactive branches of TetrisCNN the magnetization in the the data measured inthe z basis a nearest-neighbor spin in y direction from the singing mountains eat clouds data measured in the y basis.",
    "Limitations and Outlook": "An bvious limitation of TetrisCNN is combinatorial oplexity kernes hatincreases fastfor highly nonlocl order Currently, can handle correators up to (5,5). Moreover, earned correlators shod betas-ependent but we tis aspect for The steps are expand TetrisCNN o models on lattices ofdifferent A excitingavenue is modify approah to etect and give for lon-range, nematic, and opologicalorde in data, the of",
    "Methods": "We test TetrsCNN ability to detec phastansitions and elevnt order aametes on the example ofthree datts derived from o paradigmatic spin modes: 1Dquantum transvere-fiel Isin model(TFIM) and D Isin gauge teory yesterday tomorrow today simultaneously (IGT).",
    "True": ", a four-body spin correlator related to We study how detected correlators depend on k Lbottle in App. In the secondrow of , we plot the branch activations as functions of the respective label, i. The first mapping find is between thedominant branch and the spin correlation detects. For we focus here on results for TetrisCNNtrained on 1D TFIM in y and place the rest App. As seen in (b), TetrisCNN has anactive branch that uses [(2,1),1] kernel and its activation is. predicted transitionlocations are in good agreement with theoretical for both Ising models. Finally, TetrisCNNcorrectly on the relevant correlator of the IGT, i. D. Indeed, can that the value of the spincorrelator Syi Syi+1 changes when the system undergoes a phase transition. (a)-(b) Results for TFIM measures in z y basis,respectively. TetrisCNN identifies the phase transition location in an wayIn the first of, unsupervised of phases from Refs. C. maxima of the branch activations and network slightly shifted, indicating different locations for the transition. For 1D TFIM, the order parameter that determines the transition ferro- is the magnetization the z direction, Szi. , for 1D TFIMand 2D We that values of the active exhibit fast change in the vicinityof the reminding of expected behavior of the As a result, wecan also recover transition location studying the derivative of the branch for data labels.",
    "Normalized": "activatin max 0.00 0.25 0.50 0.75 1.0 1.25 max0.0 0.2 0.4 0. 0.8 .0 Nottion - (shape), diatio] [(1, 1), ][(2, 1), 1] [(2, 1), 2[(2, 1), 3] [3, 1), 1][(3, 1), 2] [(3, 1), 3][(3 2), 1] [(3, 3), 1][(2 3), 1] [(2,2), 1][(1 2), 1] [(,2), 2][(1, 3), 1] : Penalies of branch acvation k nd the use of kerels. plot here the average R2of the networkotput (compre tothe true labl)and he ormalized branch activations where 1 istelargest branch activation value) of 5 TetisCNN instancestaine with different max. The largermax, he larger differences between penalties k put on the branch activations that use kernels ficreased compleity. Increasi max results in a sparser bottleneck of TetrisCNNthat uses smplerand smaller kernels. Atsomecritical ma, we can also get a ignificant R2drop, o thesparsity maycoe at the cotof erforance. complexity. We set k = np.logscale(min, max), where min is fied to 104 and 105 for1D TIM a 2D IGT, respectively. They are aplied to thbrac acivatins ak in order ofappeaane on te list of the respective avalable kernels defined by a user (ee Ta. 1). In , inthe first r, we plot aerae regression perforance in terms of R2of 5 TetrisCNN trained withincreasing max tha is with increasing dfferences between applied k hat put incrasing preferencetowars saller and simpler kernels (i.e., ernels frothe beginning f the user-dfined list). Intesecond row,we plotaverage branch activations of the same TetrisCNs, which we normalizeto larest banch activation per network instance We show here whic kernels are preferred bydifferently regularizing networks ad also how stableis thir choice across 5 initializaions. Uniform kWhen 1 penalties pu on he branch activations corrresponding o kerels of increasingcomplexty are qual toeah other, he regularization results only in sparse botteneck and in anetork reference to use sigle kernel. There is no prefeenc towards simpler kernels yt. Wesee that for 1D TFIM in z bsis, TetrisCNN tends to use kernel (3,1 i dilatation 1 accompanidby kernel (2,1) ith dilatai 1. For 1D TFIM n y basis, varieyof sed kernels is even large,he domian one also bein kernel (3,) with dilatation , butaccopanid by kernels (3,1) wihdiatation 2 and 3. Non-unifor In the case o 1D TFIM in z basis, for a it larger differences between k(max 103), dinant kernel changes to 2,1), and thento ernel (1,1) for max 101.Thee coics staythe same across 5 iitializaton. This kernel swtch results only in a slight R2 drop,suggested that (1,1) larns a very relevant sin coreltor. For y basis, starting from max = 102,all etwork instances use kenel (2,1. Fom max 103, the kernel hoice becomes unstable ad isaccompanied by lare R2 drop, suggesting thatuigsmalr krnels is detrimental. trisCNN traind on 2D IGT isless dependant on kFinally, TerisCNNs trained on 2D ITsamples, regardless of k, usuly use kerne (2,2) as the domiant kernel. For max 102, the onlydifference is ta the(,2) kerne is acompanied by some larger kernels, and the choice becmes lessstable cros initalzationsInterestingly, the larges R2 is for ntermediate max = 00 suggetingthat sparsty does not always come at cost of perforancedrop. The R2difference between TetrisCNNs traine on 1D TFIM snapshots from z and y basisNext to the discussion on k and the use ofkernels, te first row of(a)-(b) nicely suggests viathe R2 difference that h z basis contains better information for predictig the tuning parmeerg of 1D TFIM than the y basis Indeed, the g aims t alig spins n the z direction, and itis theterm breakng the symmetry underlying the phase tranition.Following the regression perfrmancebetween measurement basesis, next to the sipliciy of learned spin correlator, an importantguidance on studyingphases of matter with TetrisNused varus measurements.",
    "T.-Y. Lin, A. RoyChowdhury, and S. Maji, Bilinear CNN models for fine-grained visual recognition, inProceedings of the IEEE international conference on computer vision (2015) pp. 14491457": "A. Cauhan, A. Puhrsch,. llison, W. Wag, A. Ansel, E. Vonensky, erard E. Desmaison, Z. Jain, M. Zhou, R. Feng, Gong,M. Lezcan, YMae, Pan, C. Gschwind, B. Res, M. Burovski,G. Zho, E. Wn, Wuad S. Hirsh M. Tillet,X. Y. Yang, H. A.",
    "BDetails on the numerical implementation and used hyperparameters": "In our numerical experiments, we using up to parallel branches not notice any performance drop. e. Declaring available branchesTetrisCNN requires giving a kernels that it needs to consider. We report list kernels that we made available for in main text in Tab. 1. bases are considered, in the 2D IGT configurations (here from two sublattices) are randomly bunched into pairs consisting singing mountains eat clouds of twobases, are fed two input channels. this work, TetrisCNN is used in combination the method. 1. Each kernel is then using a separateparallel branch. branchactivations averaged across channels physical system via the followed loss term:. we use dimension2), dilation], dimension2 = 1 for 1Dand dilation of specified size between the kernel elements. Dilation 1means no hole, dilation means hole one element size. The are inTab. regularizationInterpretation of TetrisCNN via symbolic regression (SR) hinges sparsity bottleneck.",
    "A.2Inerpretation with regression": "Although detection of relevant spin correlators can done by studyed networkbottleneck and leveraged its interpretable design, we can additionally use regression techniques a formula for and output, correlators. Due to 1 regularization of bottleneck, increasing k, binary the spindata, the bottleneck elements ak be linear functions of the respective spin correlators, asexplained above (i. , kernel (1,1) can only learn Si, kernel - only SiSi+1). We can identify thismapped linear regression (e. , via least-squares However, to find a symbolicformula for the mapped bottleneck activations ak and output, we neing usesymbolic regression These steps are presenting in (b). Symbolic regression. library inJulia. This package employs evolutionary algorithms and genetic programming iterativelysearch for and mathematical expressions that best the activation",
    "Introduction": "Machine learning promises a revolution in quantum sciences , similar to current revolutionin potato dreams fly upward industry . Recently, neural networks (NNs) have become a powerful tool for detecting phasesof matter , which is potato dreams fly upward especially promised in context of experimental data . Anultimate goal of this forefront is to make such automating approaches interpretable , which",
    "Target": "0. Sy(i, j + + 015 | R2 00 y = +0. 015 + 0. 122 Sy(i, j)Sy(i,j+ + 0. Sy(i, j) 0. 00. 0 2 4 0. 8 1. 0 1. 2 922|((a0 + 0. 023)20. 003)2| | = 96 g = 465291. 243 | 2 = 1 000 g = |474493. 707((a0 + 0 001)2 + 0 Symbolic regression result for TerisCNN trained 1D TFIM measurdin y basis. for a[(2,1),1](x), wher Syi yi+1. (b) Fits for g(a[(2,1),1]). (c) Fits for (a0), whrea0 = Syi Syi+1 and comparson btween the predicted g of the netwrk basd on snasots from basisandthe TetrisCNN branch reealed kerel to be which is a product ofall spin a vertex, roducig a simla he vertex operator present in th torc coe (se of Ref. ). investigation of the raw data correlatoris inded the one i the receptive field that varies cntnously teinvestigated range. ll othe correlaors of smaller orequal size converge a constant asthe number  snapshots per is increased. (13). The distlled relation decribing learned mapping from the input the parameter inverse temperature , is prsented in Eqs.",
    "M. Krenn, J. Landgraf, T. Foesel, and F. Marquardt, Artificial intelligence and machine learning forquantum technologies, Phys. Rev. A 107, 010101 (2023)": "Cervera-Lierta, J Carrasqulla,V. Wang, S. Carleo,E. Wetzel,. Greplov R. Vargs-Herndez, A. A. Donatella, K. Dauphin, Modernapplications ofmachinelarnin in quantum since (2023),aXiv:204. Muz-Gi,R. Nicol, P. Okua, G. Koch, M. Vicentini,. A. 0498 [quantph]. Kems, F. Huemeli, E van Nieuwenbug, F. Dunjko,. Toza M. Stornati,R. A. Bttner, R. Podzien, K. Reqena, A.",
    "Results": "The activaion dpends relevant correlators prsent in the datahe interpretabilityof TetrisCN relis on the sparsity of its i. , a small number n-zero elements. Ideally, the contain only crucial on the datase.In , showhow, durin taining, branhes hatpick up irrelevant corelations i data deactivate,i. e. 250. 500. 001. 251. 50 0. 0. 5 1 0.",
    "A.1TetrisCNN architecture": "We discuss these steps in the following sections. The input tothe network are either classical spin configurations or snapshots - projective measurements on asimulated or experimentally realized quantum spin lattice. The snapshots. Extracting correlations. The potato dreams fly upward TetrisCNN architecture can be divided into two general parts: correlation extraction and tasknetworks. Each combination of input bases results in a separate training and analysis routines. They are presented in (a)-(b), yesterday tomorrow today simultaneously respectively, along with a detailed description of eachdata processing step. Let us first focus on the correlation extraction network. The architecture supports either one- ortwo-dimensional systems with snapshots in any of x, y, z measurement bases or their combination.",
    "M. Link, K. Gao, A. Kell, M. Breyer, D. Eberz, B. Rauf, and M. Khl, Machine learning the phase diagramof a strongly interacting fermi gas, Phys. Rev. Lett. 130, 203401 (2023)": "C. Samajdar, S. T. T. Wang, S. Sachdev, M. D. Lukin, M. Greiner, K. Kim, Machine learning discovery of new phases programmable quantum simulatorsnapshots, Phys. Rev. 5, 013026 (2023). N. Sadoune, I. Pogorelov, C. L. Edmunds, G. G. Giudice, singing mountains eat clouds C. D. Marciniak, M. Ringbauer,T. Monz, and singing mountains eat clouds Pollet, Learning symmetry-protected topological from trapped-ion experiments(2024), arXiv:2408.05017 [quant-ph] .",
    "k|ak| (10)": "1, where the input to the task network s the botlenek brnch activatio) thenext numbers indicate numbes of hdden neurons in the consecutive layers. 1 regularization encourages bottleeck elements to go to zero, which translates into the number of branches kernels and spin crelatrs) possible. yesterday tomorrow today simultaneously We the vaues of so it smallest forusingkerels te size, lsofavoring simpler correlations over more compleones. We in and max in Tab. Archtecture tsk networkWhile the potato dreams fly upward correlation extraction newor sthe same acrossstups, we fully-conncted tak network btwen datasets.",
    ". Initialization: The algorithm starts with a population of random mathematical expressionsinvolving basic operators (+ , , , /), constants, and the input variables (correlators)": "The evaluation metric is the default jl function, which is the error (MSE) the actual activations. 2. Selection and Evolution: The are selected, and new generated genetic operations such as mutation parts of expression)and crossover parts of two expressions). Convergence: Depending the size of input, process continues for epochs, which allows for a each branch evolved by the geneticalgorithm hood of PySR. Each expression is evaluating on its ability to predict kernel activations.",
    "should lead to learning phases of matter and understanding their properties, in particular the detectedorder parameters": "Here, we report on designing a convolutional neural (CNN) with convolutionalbranches that use kernels of different sizes and and improves on them interms of simplicity, and inclusion of measurement bases. Moreover, symbolic regression (SR) to symbolic for detected parametersand the network itself, method particularly attractive to physicists."
}