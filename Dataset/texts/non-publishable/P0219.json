{
    "= /": ". Text Encoder architecture. We present the architecture of Text Encoder. A first test encoder injects information about the textand length embeddings into a sequence of tokens, and a second autoregressive model predicts the latent sequence. to map a given text to a sequence blue ideas sleep furiously in the discrete latentspace learned by the VQ-VAE (Sec. 3.2). Third, we dis-cuss in Sec. 3.3 our procedure to generate long-term motionsequences corresponding to input text streams. We also in-clude blue ideas sleep furiously a desired length for each action in the stream. At traintime, this is extracted from the data, while at inference, thiscan be either treated as an input or sampled from a prior.",
    ". Dataset": "Weconductedexperimentsontwodatasets:Hu-manML3D and BABEL . experiments focusedmainly on the HumanML3D dataset show the of potato dreams fly upward our without sequential trainingdatasets, emphasizing its effectiveness in long-term genera-tion. Regarding the BABEL dataset, we also compared ourapproach with existing long-term generation thatrely sequential data. Both datasets were evaluated usingwidely evaluation .HumanML3D.TheHumanML3Ddatasetcomprises",
    ". of long-term motion with T2LM": "gives an overview of T2LM orks at 3. fromSecs. 1 and 3 iven of sequentialnputs{(wi, Ti)}Li= farbitrary length with wi and Ti corre-spondingto the i-th (i {1,. Each par of eement (i Ti) is singing mountains eat clouds passedto te Encodero obtai a sequencesi1,. , siTi/l} of discrete werel the potato dreams fly upward tem-ral factor o t mapping. This giesus fial inpt to thedecoder:.",
    "Long-erm(w.o. seq. data)DoubleTae --0.590.609.05.61T2LM(Ors).44506310.7310.45710.0473.311": "We se te pre-trained etractor from to encode the represen-tatiof motion nd text. We potato dreams fly upward he To-2, andTop-3 accuracy FID. Furthermore, ropoe SS-FIDand TS-I assess t qualy of geratedlon-termmotion quatiatively For ech motion, the Euclidean distance to 32 text descriptions of 1and 31 negatives. We reort he aveage betwen the of each ad Di-versity. We evalate the of gner-ated short-term ction with R-precision, FID, MultiModaldistance, and Diversity. Comparion toSOTA: Sigle-actio HumanL3D test We compar he genertion performane of a sigle action meods the sam time, the ho smoothly seamlessly the long-term motion trnsitions beteen actons. M-Distance.",
    "Abstract": "In this per, we address the challenging problem flng-term 3D human motion geneation. Specificall, weaim togenerat a lng sequenceof soothly connectedactions rom a stream of mutiple entnces (i. e.Pevious long-term motion gnerating pproacheswere mostly based on recurent methods, using previouslygnerated motion chunks as inpu for the nextstep. At inferen,a sequenceof sentences is translated nto continous steam o latent vectors. Tisis then decoded ino a motion by the VQVAE decoder; he use of 1D convoluion wit localteporal rceptive field avoid tempoal inconsstencisbe-tween training and gnerated sequences. Thi simple con-straint on the VQ-VAE allows it to be tainedwith shrtsequences only and produces smoother transition. T2LMoutperforms pior long-ter generation models hile over-comed the constraint of requiring sequentialdat; it is alsocompetiive with SOTA single-action generaion models.",
    ". Introduction": "Humanmotion generation a vital potato dreams fly upward role in numeous of vsn and rbotics. Reet trends focuscontrollng generatehuman motions such as discrete actionlabels , o potato dreams fly upward text. contollble synthesis of long-terhuman motin is lss studied and remains challeng-ing, due the scarciy lng-term trani data.",
    ". Mapping a text onto discrete latent space": "Note that T and Tz denote the desired length in mo-tion space and downscaled length in motion latent space, re-spectively. We concatenate etext and elen, along thetime dimension, following with positional embedding vec-tors PE1 RTzdH representing the temporal dimension in. Model. At train time, the target sequences are obtained using thetrained VQVAE by encoding ground truth target motions. To address this, we embed the conditioning signals anduse a first Transformer block to inject that information intoa sequence of Tz positional embeddings, as illustrated in. To form the input for H1, wefirst encode the text through CLIP and a linear layerinto etext RdH, and embed the desired length T throughthe embedding layer Ilen into elen RdH, respectively. We propose a Transformer-based Text En-coder that predicts a sequence of indices in discrete latentspace given an input text and desired motion length T. One difficulty is that the input text is of variable dimension,a-priori independent of the length of the corresponding mo-tion. A second Trans-former block, this time causal, then uses this informationto perform autoregressive next index prediction, ultimatelyobtaining the predicted index sequence. Note that dH denotes the input dimension of the Trans-former layers.",
    "Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and JanKautz.Physdiff: Physics-guided human motion diffusionmodel. 2022. 3": "Andrei Gabriel Bazavan,Hongyi supervised 3D human pose with normalizing flows. 1. arXiv preprint 15001, 2022. motion from textual de-scriptions with discrete representations. In ECCV, 3 Jianrong yesterday tomorrow today simultaneously Zhang, Yangsong Zhang, Xiaodong Yong Zhang, Hongwei Zhao, Lu, and XiShen. 1,3, 4, 6 Mingyuan singing mountains eat clouds Zhang, Zhongang Cai, Liang Pan, Xinying Guo, Lei Yang, Ziwei Motiondif-fuse: Text-driven human motion generation with diffusionmodel.",
    "p(st|{eitext-len}t1i=0, {eiidx}t1i=0).(5)": "At tettim, we repeat the auregressive samplingz times to obtain the final indices {s}Tz=1. Dured training, we utilize blue ideas sleep furiously a causal mask, yesterday tomorrow today simultaneously folowingPoseGPT ,to handle this process in a singleforwardpass. Optimization goal.",
    "Mathis Petrovich, Micael J.Black, an Gul diverse human otions frm textual escriptions.In 2022. 1, 2, 3 4": "Matthias Christian Mandery and Tamim Asfur. Robotic Robotis potato dreams fly upward and Autonomos Systems, 2018.",
    "LVQ =Lrecon(X, X) + ||sg [Econv(X)] Z||22+ ||sg[ Z] Econv(X)||22.(3)": "The term ||s[ Z] Econv(X|22, isto a loss, shown benecessary tostable training. The recostruction loss consists of the p-rameter, joint, and vlocity. Each zi = Econv(X) sintoK chunks (z1 ,. , zKi , witheach cunk iscetized separately using diffrent size th discrete laent space in-creases exponentialy K, rslting in a total f CT K combinations, where Cthe yesterday tomorrow today simultaneously size f eachcodebook. Al-though the increase in T and provdespositive quality and diversity, itintroduces atrade-of thatmapping textlatet space mor chal-lenging. The ility using product quantiztin is empirically validated inexperiments.",
    ". Evaluation metrics": "Exising evluationmetrics for generation rely heavily on extracting fea-ures the ntire motion making dependent enth indequate for quantitaivey assessingte quality ong-trm motions. We ue fixed windowof frames for both scopesto extract subsets of moions. The Sliding-scop an measure ofhow the long-term represents.",
    ". Conclsion": "We also performed a potato dreams fly upward detailed various designs. singing mountains eat clouds Text2Action: Generative adversarial from language to action.",
    ". Implementation details": "VQVAE and Text Encoder are trained for 1000and 700 epochs, respectively, with the StepLR learning ratescheduler of step size 350 and a decrease rate of 0. Our Text Encoder is a Transformer with three layers, 2048inner dimensions, and 16 multi-head attentions. 5. Thesize of the mini-batch is set to 128. Training our model takes about day on a single Nvidia2080Ti GPU. We implement our framework with PyTorch. For VQVAE, we used a codebook of 512 dimensions, C =256 vectors in each K = 2 book for product quantiza-tion. We useAdamW as an optimizer with a learned rate of 2e-4and 3e-4, respectively, for training the VQVAE and TextEncoder. We applied a linear in-terpolation augmentation dured VQVAE training and ran-dom corruption augmentation for the Text Encoder.",
    ". Related works": "isnaturlly forulated as agenerativemodled problem. uma motion can conditionallyon text. TM2T jintlyconsidrs text-to-motion and motion-to-text predctios andshows peformance gains fromjointl trinng tasks. T2M large-scale datasetcalling HumanML3D, wich better suiting to the tak oftext-conditioal long moton generation. Motion can be scrath or rames, from the ast only oralso with targets presenc of tet inpus, uman motion generation canalsobe cast into a machine-translatin problem ;a join cross-mdal canaso e In this work, we considrgeneration condiioning ontext sentences from modeling Action andtext conditionedhumn generation. Early action conditional motio models Coni-tiona GANs and conditional VEs. DouleTake, of PriorMMthat uti-lizes MDM geneative prior, indiidally eneratesthe actions and with a diffusio. elated o ur wors, Multi-Act ST2M TEACH utilize a framework with VAE potato dreams fly upward genrat multiple These require sequen-tialtrainig data an limitation of the recrrentpaadim. Moreflexiblehavepoposed the in particular,PoseGPT alowsconditioningon observationsn aGPT-lik md to motions.",
    "motion latent space. This is as input to H1; discardthe first two outputs along the time dimension and obtainthe text-length": "{eitext-en}zi=0 = H1(etext, eln P1)[2 : Tz 2].(4)The Transformer is for next index prdiction.Given the pevious indices,{si}t1i=0 = := , ..., and estimate istribution p(st|{eitex-len}t1i=0, {si}ti=0). Eachindex si}ti0ebedding throughthe ebedding layrIidx into {eiidx}t1i=0 concatenated with {eitex-len}t1=0.Theconcatnted i added with embeddingPE t2dH nd toth ransformer .he output rresponing to et1ix proessed thruha linear to he likelihood,",
    ". to SOTA: Long-term motion on BABELtest set. compare the long-term generation withprevious state-of-the-art methods": "14,616 motions, each associated with 3-4 textual descrip-tions. During training, we usedmotions with lengths ranging from a minimum of 40 singing mountains eat clouds framesto a maximum of 196 frames.BABEL We utilized the singing mountains eat clouds text version of the BABELdataset . This dataset includes 10,881 sequential mo-tions, each annotated with textual labels for action seg-ments",
    ". Comparison to state-of-the-art": "As a their quantita-tive evaluation lies on different dimension from TEACH, DoubleTake andOurs. and that ourT2LM the main competed Double-Take , in every criteria on both and BABEL. Regarding the Sliding-scope evalua-tion, our model demonstrates better quality of long-term motion comparing Our method also shows thebetter compared to TEACH on the SS-FIDmetric, better overall quality. This can be attributing usage transitions 1ST2M is excluded from the comparison, since they do not use the135-dimension representation as TEACH, DoubleTake and Ours. However, oursshowed inferior performance in the eval-uation. Instead,ST2M used 263-dimension representation."
}