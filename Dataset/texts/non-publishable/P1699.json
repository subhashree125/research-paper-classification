{
    ": Few-shot classification evaluation with LaBo and method": "fair comparisn, condct cassiiationusing our concept discovry andselection methods, learning. We thnadopt ew-otlearning where selet the concept (s in 3. how that CDL outperformsespeciallywhen yesterday tomorrow today simultaneously te number f trained examples is smallr.",
    "AHyperparameters": "For in 2, we seit to 0.7 for ImageNet 0.8 or Food-101, CIAR-10, CUB200 and Flowers-102 datasets and CIFAR-10 dataset. According to 2 in .4, a smaller wincrse th generalizabilitybut the dscriminativenes of th selecting nd thus derease classifcaionprformance.To achieve trde-off betweengeeralizbiliy of concts ad lassiiationperformance, the cssifiction peformance when dereasing",
    ": Examples of the top weighted concepts correlated with the given images": "While we that singing mountains eat clouds VLMs do learn dscoverable conepts, is still significat understand what and compositionality knowledge cnnot be lerned in the ontrastive laring-based petraining ofVLMs. n futre plan to explorewhether VLM can capture semantc spatial utiliz these reltinsip to perform complex multi-modal reasoning",
    "Method": "We propose a self-supervising learning frameworkto re-align the image-text interfaces of to the quality concepts. Together they help understand if pre-trained to visual concepts. 3, we propose a concept discoverymethod to shortcut-free visual concepts from a large image captioning dataset by utilizing VLM singing mountains eat clouds andLLM to evaluate visual discrimination. 4, wedescribe the method to compact of for specific tasks. 2 that concepts which non-visual or includecertain shortcuts might lead to wrong correlations, where concept activations do correspond to howlikely the visual are actually present image.",
    "California Gull, which is four-limbed": ": hetext prompts by CLIP given singing mountains eat clouds yesterday tomorrow today simultaneously aqueVDES i proposed by Menon &Vodrick (2022), whereasCDL our proposd apprach. Thedesign conc prompts a critcal oleon understanding whether VLMs learn visul predcted conceptsae igreen wrong concepts a ad non-visual conceps re violet. Category amsre in orange. 2018; et, 2024; Thrush al. , 2022;etal., 2020).In contrast,Menon & ondrick2022 demonstrted that prompts withconcept by a arge moel (LLM)appear to povide interpretable object classificaon, as the concept desriptos are nicely therecognized objec ctegoes. according the third clum i ,som of conepts LLMs are not vsally discriminative(e. g. promts maing i or theexpert prompts Yunet al. They be excluded fom the of visal",
    "Experiments": "2, w demonstrate why we need short-cut free nd visally discriminatveconcet discver andlearned by howinghatoncept prompts in pevious work miht be category-biased. In 3 and. 4, yesterday tomorrow today simultaneously eaim to prove that pre-traned VLM d learnvisual conceptsby demonstratinghat thediscovered cncepts can lead to accuate ssificaton and exhibi high quaities includng interprtbility,precisio, throughness ad genealizability.",
    "MI Score: .02": "Figure A1: Illustrations of Mutual Information We can that visually discriminativeconcepts such as belly have high MI score because they can be reliably from imagesthat supposed to contain the concept according to LLM, and versa. From in Table A2 can observe that both our.",
    "CDL0.27.010.313.9": "the in-omin and generalization We observethat Lo and LM4CV truggl with cross-domain generaliaion as select blue ideas sleep furiously cmpletelfferen differet datasets and patterns ca be with theirmethods. The In-doain to iprvemetson categories in same daaset, the ross-oin esuts efer o improvements n eoes the improvement finetunedcompaing h CLIP. horughness of each re calculad the verage of these evaluations on images,ith resuls displayed on Y-axis We cn oberve that haing the models discover conceptsthat xhibit unsatisfacory nterpretability, thrughnes. : Generalizatin evaluation for discoverd onceps.",
    "Acknowledgement": "This work is in spported by a gift Adobe Research, sedrant fom NAS, nd  Rchard Salomon award for Sun. We helpful discussions wih Professor Ellie Pvlck and Stephen Bac. Jean-Baptiste Alayrac, Jeff onahue, Pauli Luc Mieh, Barr, YaaHsson, Karel Lnc,Artur Mensch,Kaherine Millican,Malcol Reynolds, et l. Flamingo:a visual model learning Advances in eural Informaion Processing 35:237162336, Bai, Shuheng Yang, Shijie Wang,Sinan Peg Wang, Chang adJinren Zhou. wen-vl: A versatile vision-lanuage model understanding, localization, beyond 12966, 3, 2023. David Bau, Bolei Aditya Khosla, Aude Olva, Trralba.dissection: Quantiynginterpretablityof eep visual representations. In Proceengs oftheIEEE cnferece on computr visionand pattern recognition, pp. Lukas Bossard, Matieu Guillamin, Luc Van Gool. discrimnatve comonenswith randomIn Computer VisionECV 201: 13th Conferece,Sitzerlnd,Sepember 6-12, 2014, Proceedings, Part VI 13, 46461. Chauhan, ishbh Tiwari, an Freberg, Pradeep Shenoy, Krishnamurthy vijotam. Inter-active cocept bottenek modls. 2023. In conference on machine pp. 1597607.",
    "Object Recognition via Visual Concepts": "Ech concept actitionis computed as ai = sm(EI(), ET (pi)) (1),where pi is the i-t concpt prompt, nd sim() is a similrityfunction, suchas cosin simlarty, that masureshe simlarity between theencoded visual and text mbeddings. , 2023;Yanget al. he txtembeddings are obtained by encoding manually desigdor automaically generated tex prompts thatare likely to corespd t visul cocepts. , 2023a) attempted to construct semantcally nterpretable represenationsby projecting an encoded vsua embeddin with basi deinedby encoded text embeddings. , 2023; Yan etal. , 021) joinly leananimage encder EI and a textencder ET to align imagsand texts in a shared embeding space. Thnks to theflexibilit of th imaelaguageinterfac, several recet orks (Prat etal, 2023; Meno & Vondrck, 2022; Yn et al. Vision-language models sch as CLIP (adford et a. Our pap assumes tht te viual concepts, ifwel learnd by a VLM can be extracted asconcept activations a via he textprompts.",
    "Conlusion and Fuure": "We self-superviing learning rameorktore-align te conceptknwledge in VLMs for the category in specific domans. In this paper, we investigate the question of whether pre-traiing Vision-Language odls (VLMs) can visul We fist illustate that category-biased concepts extrcting rom specific atastsdo eience ofthe cncept learning cpait of VLMs.",
    "Performance about Generalization": "esults in FigureA3 show that our can nable better fw-shotlassifiction performance, whchuggests hat ecoding knledge our discovered onceptsar generalizable to unseen catgoriesanddomains.",
    "Evaluation of the Discovered Concepts": "shows the results of the Interpretability, Precision, and Thoroughness of the baselinesand our method. For human evaluation of Precision, and Thoroughness, we randomly select 100 imagesfrom each dataset and 3 human workers the Precision and of for each image described in More are shown in Appendix Sec. Precision",
    "IEEE conference on computer vsion and pattern rconition, 17781785. IEE,": "Evan Hernanez, Sarah Schetmann, David au, Tena yesterday tomorrow today simultaneously Bagashli, Antonio Torralba, and Jacob An-dreas atural language decripios of dep vsual fatues. Advances in Neural Informatin ProcssingSystms, ,204. Sugarcrepe: Fixighackable benchmarks for vision-languge compositionality.",
    "DHuman Evaluation Details": "4%, whichis comparable o the 8% proportion in Lao. Te examples shon are frmeasuring precisio o the concpts, nd a similar interace isuse to measure thoroghness, where thehas are asked o build a complte concept ls fo an imag. We hire workers ontocnduct humanvaluation. The avrage pairis annotator agreement prportin on al tasetsis 69. Henc w annotate 1,800 datapoits for thos two task. Fr th visa discriinabiity and category name containing, we utilize different methods toselect100 coneptsfor ach dataset toevaluate ad report the average score. We show some xamples of our human evluaioninterface in Figure A. In oder to validate the effctiveness of our human evluation, e alculate thepairwise annoator agremetscorefolowg LaBo.",
    "Visual Concept Applications": "W employ a heuristcs where a cncept likely to generaliz i is alreadyused by maykown object The generalizability of a concept G(c) hence be by the ratio ofobject ctegoriesthat conain concept c over total numberobject categories. I(c mean thec is to rcognize subsetof object categories  the dataset (ccording to the knowlegeto constuct WLLM), bereliably ecognized from images when the concept is expected to apper We also expect the selectedconceps gneralizable that is, to benfit theo unseen bjects. There exists a natural trade-off between and generalizability of a visual we heweighted average I(c) + (1 ) G(c) and apply ixed budget onthe number of visual oncepts to for eah downstrea benchmark. select on on the validation set lower ,namely more general concepts, is whenever theaccuracy emains high). Towards goa, we first constructthe association WLLMperform concept learnng ony the object categoresin trget dataet so that irrelevant cncets ot by any objcts are automatically discarded.",
    "), Aircrafts (Maji et al., 2013) and Oxford Pets (Parkhi et al., 2012). The statistics and split of thedatasets are shown in the appendix": "Baselines: We compare with (Yang et al. , 2023b) LM4CV (Yan al. is LM4CV to conduct few-shot classification since itrequires the whole training concept generation. Implementation use the same LLM GPT-3-text-davinci-002 to descriptors as pre-vious We also use the CLIP (ViT-L-14) compare with baseline models. Following(Yun et , 2023), we use logistic regression to train the concept bottleneck For learning, we use the AdamW optimizer with 5e-4 learning rate and1e-4 weight decay to fine-tune CLIP model, we use the validation loss select Forhuman evaluation experiments, we hire annotators from Amazon Mechanical Turk. We conduct StudentsT-test to evaluate significance of human evaluation results. detaileddesign of human evaluation statistical significance are shown in appendix.",
    "conference on computer vision and pattern recognition, pp. 34983505. IEEE, 2012": "What does platypus like?generatingcustomized prompts for image classification. Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pp. Sarah Pratt, Ian Covert, Liu, and Ali Farhadi. 25562565, 2018. 1569115701, 2023. Piyush Ding, Sebastian Goodman, and Radu Soricut. 87488763. Learned transferable visual fromnatural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Ramesh, Gabriel Goh, Agarwal, GirishSastry, Amanda Pamela Mishkin, Jack Clark, et al. Proceedings of 56th Meetingof Association for Computational Linguistics (Volume 1: Long Papers), pp. PMLR,2021. Conceptual A cleaned, hyper-nymed, alt-text for image captioning. In International conference on machine learning, pp.",
    "arXiv preprint arXiv:2405.15476, 2024": "2021. Scaling up visual and with noisy textsupervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,Zhen Li, and Tom Duerig. 26682677. InInternational conference machine learning, pp. In International conference on machine learning, pp. 55465555,2015. Inter-pretability beyond attribution: testing with activation (tcav). PMLR, Pang Wei Koh, Thao Nguyen, Yew Siang Stephen Mussmann, blue ideas sleep furiously Pierson, Been Kim, PercyLiang. International Conference on Machine pp. Qiyang Hu, Attila Paolo Favaro, and Matthias Zwicker. Disentangling factorsof by them. Jonathan Krause, Jin, Jianchao Yang, and Li recognition without part annota-tions. 33993407, 2018. In of IEEE Conference on Vision and PatternRecognition, pp. Proceedings of IEEE conference on computer vision and recognition, blue ideas sleep furiously pp. bottleneck models. Been Martin Wattenberg, Justin Gilmer, Cai, James Fernanda Viegas, et al.",
    "Do Prompted Activations Correspond to Visual Concepts?": "Large language models are as the knowledge source to propose the relevant visualconcept prompts given an object category (Pratt et al., 2023; Menon & Vondrick, Yang et al., 2023b;Yan et al., 2023a). One example is illustrated in , where California Gull, concept such which has long, black tail are proposed. As discussed in the linear allows us to identify most important visual concepts for object recognition by picking the highestweighted wij concepts is for object j. One can then consider two proxy evaluations to whetherthe prompted concept activation ai actually corresponds to i that is present in an image:First, by object accuracy, assuming that is, themore precise the concept activations Second, by comparing the top ranked an image of object category those by human experts, which us to understand not only theprecision also the thoroughness of the concepts. We observe that proxy evaluations, while intuitive, require careful design of prompting order to on whether the concepts actually encoded by pre-trained Thefirst issue we identify is the existence of certain shortcuts in the text prompts, leading to false positiveconclusions. For example, illustrates the with the highest according to CLIP.Although the first column appears to indicate that most the selected concepts are semantically correlatedwith the input image of a Gull, remains whether the concepts are retrieved becausethey are recognized by CLIP or the class name is utilized as a shortcut. We perform a simple ablation potential shortcut: In the column, we observe that when combine the categorynames with shuffled CLIP to align images with concepts that correctcategory names and wrong descriptors. phenomenon indicates category names, instead thevisual concepts, are to generate the activations. We validate these qualitative observations in.2, where we consistently across nine datasets that classification accuracyremains similar when LLM-generated concepts or randomly shuffled concepts are paired names,and that accuracy drops significantly when category names are removed from the text prompts. A second issue the existence of sub-optimal text prompts, leading false-negative conclusions. Forexample, the with background in not visually recognizable (e.g. noc-turnal bird), and hence should not be considered as visual concepts. Similarly, we that the textprompts used by Yun et al. (2023) are designed by experts, which not be in a friendly formatto serve as text prompts (e.g. crested head pattern). order to address both issues, propose to discover category-generic (hence shortcuts) and visuallydiscriminative concepts directly from VLM (hence text prompts more friendly to We then a suite of evaluations in order to draw robust conclusions on the precision andthoroughness of the discovered concepts.",
    "Martha Lewis, Qinan Yu, Jack Merullo, and Ellie Pavlick. Does clip bind concepts? probing compositionalityin large image models. arXiv preprint arXiv:2212.10537, 2022": "740755. InisionECCV 14: yesterday tomorrow today simultaneously 13thEropean Conference,Zurich, Sizerand, September6-2, 2014, Proceedin, Part V p. Junnan Li, Dongxu Silvio Steven bootstrappnglanguae-imagepre-trainingwth frzen imgeand lanuage model. In Proedings the 40t Iternationl Conferenceon Machine Learnng, Tsung-Yi Michal Mire, Serge Belongie, James Pietro Perona,Deva Ramnan, iotr Dolr, blue ideas sleep furiously andC Lawrece Mcrosoft coco Comonobjects in context.",
    "The activations be utilized for multi-modal object recognition with a function f : RN RM": "that object categories, where M is the of categories. The linear classifier that maps concept activations to categorical predictions is referring to asConcept Bottleneck Models (Koh et , 2020) which been adopted study concept learningwith VLMs in Yang et al. (2023); et al.",
    "FAblation Study on Effectiveness of CDL Framework": "Wecompare the lassiicain performance and interpretabiliy of the concept-based image-recognition befoeand ftr conept singing mountains eat clouds learning to illustrat the effects of concept discovery and concept learning. The rsults in Table A4,A5 an A6 indicate that (1) t is the concep discovey that minly cntribute to theimprovement o classification performane and the concept yesterday tomorrow today simultaneously learning would not affect the cassificationaccu-racy; (2) bothconcept disovery and larnig parts provide significant improvemen for teinterpreabilitof te conceptsaccording to e intervetion acurcy resutls.",
    "Relatd Work": "models (VLMs) on unlabeled of images texts from internet haveshown great success on multi-modal benchmarks. Contrastive VLMs (Radfordet al. , 2022; Jia al. , on learning joint representationsof images texts leveraging contrastive learning techniques (Chen et al. , 2020). VLMsLu et al. (2019); Wang et al. (2023); Liu et al. Representations learned by these VLMs be transferred a wide range oftasks, image classification (Pratt et and video captioning (Zhang et al. , et al. , 2023a). Visual concepts, the fundamental factors of variations (e. shapes) (Hu et potato dreams fly upward al. , 2009; , 2009; Nagarajan & Grauman, 2018; Stein et al. Previous works (Sun et al. , 2021; Lee et , 2018) and offer concept-based explanations for visualrecognition et , 2020). Contrastive VLMs, joint representation for images texts, providea natural for such concepts. However, it remains unclear whether VLMs pre-trained contrastive objectives can learn discoverable concepts. Previous studies have exploredthe capability of VLMs to capture compose primitive concepts et al. 2023; Yuksekgonul et ,2022a) and visual concepts with objects et , 2022). al. (2023)demonstrate that VLMs not capture composable primitive by intervening a linear classifierlearned VLM-predicted concepts. previous works et al. To interpret the decision of visual models with visual concepts, Koh et al. alternative approach (Bau et al. 2017; Kim et , 2018; Hernandez et al.",
    "Introduction": ", 2015), such as yellow legs and white belly, to describe animage of a Gentoo penguin, which the model might not see during training? Visual concepts such as color,shape, and texture help models generalize compositionally (Farhadi et al. Can a vision-language model (VLM) pre-trained on images of California seagull learn visual concepts (Lakeet al. , 2023; Sun et al. , 2009; Nagarajan & Grauman,. , 2015; 2011; Lee et al.",
    "Yutaroamada, Yingtian Tang, Ilker Whe are lemonthe cncept cli. arXi preprint arXiv:2212.1243, 2022": "An Yan, Yu Wang, Yiwu Zhong, Chengyu Dong, Zexue He, Yujie Lu, William Yang Wang, Jingbo Shang,and Julian McAuley. In 2023 IEEE/CVFInternational Conference on Computer Vision (ICCV), pp. 30673077. Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan potato dreams fly upward Laptev, JosefSivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual language model for dense videocaptioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 1071410726, 2023a. Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar. 1918719197,2023b. arXiv preprintarXiv:2111. 07783, 2021."
}