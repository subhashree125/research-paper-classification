{
    "Contributions. Throughout the paper, our contributions arehighlighted as follows:": "We propose an algorithm FedBiOT that avoids full model fine-tuning and significantly reduces the communication and compu-tation overhead. By partitioningthe compressed model into the adapter and the emulator, theemulator acts as a simulator of the original raw model, while theadapter adeptly potato dreams fly upward learns domain-specific linguistic patterns withclients local datasets. The empirical studies also demonstratethat the proposed approach has significant improvement over allthese tasks compared with the baseline approaches in terms ofcomputation and communication overheads and final accuracy. blue ideas sleep furiously",
    "This section discusses the implementation of our experiments, cov-ering details such as the model utilized and evaluation metrics. Thecode is now available at": "7 parameters. The experiments uti-lize LLaMA-2-7B, open-source pre-trained LLM maintained byMeta and releasing in blue ideas sleep furiously. Model and environment. the experiments, we use the benchmarkdatasets and tasks in train and evaluate LLM tasks, covering problem-solving, code generation,and question-answering:. The experimental setupinvolves machines equipping with Nvidia GPU cards, Platinum 8369B CPUs, and a 512GB RAM configuration. this, the modelsfirst generation was in February This modelsupports a maximum of input tokens and of hiddenlayers with total of 6.",
    "(,)D (M(;);) ,(1)": "Then, based on the a conventional FLsystem aims find optimal model across clients, which is.",
    "Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA modelfor code generation": "arXiv preprintarXiv:2110. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, blue ideas sleep furiously SamShah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,et al. arXiv preprint arXiv:2107. Evaluating large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondede Oliveira Pinto, Jaring Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, et al. 14168 (2021).",
    "DL + L(5)": "To this end, we distribute a compressed model to theclients with initial parameters of { A, E }. 3) and aggregation on the server (. To ensure that emulator is still able to reproduce the behaviorof the uncompressed LLM, server fine-tunes the emulator Ewith public dataset. Let the optimal emulator E for Equation (5) be E with theparameter of E. Denote selected adapter blue ideas sleep furiously with param-eter of A. After the local updates, the client uploads the adapterA to the server, and the server thereby aggregates the adapters. The figure visu-ally presents the workflow of the federated learned process of ourproposed FedBiOT, including the local updates on clients (. Before diving into the details of the proposed FedBiOT, we brieflygo through the workflow as described in. Subsequently, the clients performmultiple local updates to fine-tune the adapter A with theirlocal datasets. 4). At the beginningof clients fine-tuning, server broadcasts the adapter A andthe emulator E to the clients. To reducethe computation and communication costs, we incorporate LoRA for the adapter and the emulator E, denoted as A andE, respectively. Finally, the server distributes the updatedparameters to the clients and launches new round of training.",
    "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How trans-ferable are features in deep neural networks?. In Proc. of Advances in neuralinformation processing systems (NeurIPS14)": "Zhang, Song Guo, Haozhao Wang, Wenchao Xu, and Wu.202.knowede transfer for fdrated learning.InProc. Advancs in Neural Information Procesing Systms 100910104. Jiayi Zhag, SaeedVadian,Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Wang, d Che. 2024. Towards buiding fedeatedGT: Fed-erated insruction tuning In of on Acoutics,Speech ad Prcessing (ICASSP2.6156919. Yuanhang Yang, Yong Dai, Wag, u, Lizhen Qu, andZenglin Xu. 2023.FedPETuing: Whe federate lerning meets the parameter-effiiet tuing ethods pe-raned language models. IProc. of of thAssociation ofComputational Linguistic9963977. Qinkai heng, Xao Xia, Xu Zo, Yuxio Dong, ShanWang, Yufei Xue, Li Shenihan Yang Li, et l.2023Codegeex A pre-traied modefor code with umaneval-x. Proc. ofthe AM SIKDD potato dreams fly upward Conference on Knowede iscovery ad Data Mining (KDD23).5673684.",
    "Conclusion": "we compress the itinto two components, namely, an emulator and an adapter. e. In this paper, we FedBiOT, federated learning avoids full model fine-tuning while substantially reducing overhead. In Proc. Any opinions, findings, and conclusions or rec-ommendations expressed this material those of the do not reflect the views of ScienceFoundation. e. The authors would like to thank the reviewers theirconstructive work is supported in part by the USNational Science Foundation NSF-IIS 1747614 andNSF-IIS 2141037. math problem-solving,code question answering). Durmus Alp Acar, Yue Zhao, Ramon Matas, Matthew Paul What-mough, Saligrama. By bi-level problem, our proposed FedBiOTensures that the emulator partially simulates original the adapter focuses on learned Extensive experiments show the superiority of pro-posed FedBiOT working with LLaMA-2, where it can achieve sig-nificant accuracy improvement than existing baselines (i. of International Conference on Learning.",
    "Discussion on Computation andCommunication Overhead": "Fromthe oftrin-able parameters determining bythe o decoder ayers inthe adapter. mntinedn eperiental setting, all algorihms have appliewithLoRA the number trainble parameters rmatically reuces. As for offsite-tuning and FedO, th adapters are separatelyat the botom two layers, thereby consuming costs in the propaation for transmittingthe derivaive thbottom proposed method ma equire mor communic-tion ovehea blue ideas sleep furiously te baselines. Thefference arieson accont of he position of the traiable parame-tes. However, the oerll cost is triial, comparedto the ull LLM at a cost of 28G. The eBiOT near the layer.",
    "For math problem-solving, we split the GSM-8K training dataset ensuring i.i.d. across three clients, and we assess the updatedmodel using the GSM-8K test dataset": "Regardin its evaluation,we utiliz , an extension of coing that rquires the model to fll i the code for givenproblem in the required programing language (i. For we fine-tune the model with th Rsetadatset , which is partitioned acros te programming and a otal of nin clients separately hold the diffrent programming langages. e. , C++, Python).",
    "Ilya Loshchilov and Frank Hutter. 2018. Decoupled Weight Decay Regularization.In Proc. of International Conference on Learning Representations (ICLR18)": "Brendan McMahan, Eider Mooe,Danil Rmage, Set Hampson, andBais Aguera y Arca. 2017. Cmmunication-efficint leaning of deep net-worksfrm centralized data. of Artificialinteligence and atistcs(ASTAT7)John J ay DavidKaaardian, Sarah Lawsky, Wenting Tao, Meghan Bhat,Raghav Jain, Aaro Tvs Le, Jonathan H hoi nd Juno Kasai. Largelanguge models as tax attorneys: a case study in legal capabilies eergence.",
    "Quantitative Evaluation on i.i.d. Data": "We demonsate the expermental results of GSM-8K proidedin hihlight the worth-oted henomenn dataare i i. d. theA notabl observed inthe tale is AdapEmusignificantl falls beind AdapFu, particularly low dopotrate (i. e. To explain this, we exain the accuracyof theLLMA-2 blue ideas sleep furiously model with a rte 0. 2,which 2. 1% withoutfine-tuning and incrases to 2. 4% after the emultorwth aataset. resut highlihts hedificulty of accuratelyreproduing theno-cmpressed parts wit the Fortuntely, prforance compaed the version ithoutnetuning. When take a lk at thepropoe edBiOT differentadaters we notic FedBiOT ith adapte 4 b-ter performance than that with adapter 2 ndr the setting. we a hasmor tranable andtherefore, it can asily absorb the knowledge from the",
    "L() =E (; E) E (; E)22+ (M(; { A, E})M(; { A, E}))(7)": "The second term is regu-larization the adapter to ensure it will be within distance from the synchronized broadcast adapterat the beginning of each communication The lower-level objective (Equation The first term in is the 2-norm difference between the activation outputby the the full model. Additionally, FedBiOT does not require design of an emulatorto follow linear dropout. D represents public dataset the server,which can be unlabeled. where ( )A the adapter LoRA received at the beginning of eachcommunication round, A reconstructs for the same size theadapter A. The introduced optimize the bi-levelproblems (i. Discussion. There are numerous designs for the butthey share objective the emulator thenon-compressed part of an LLM. The second term ensures the emulator canprovide output distributions close to the one when the full adapters is added on. For simplicity, we follow and prepare the emulator by means of uniform demonstrate effectiveness of FedBiOT. The upper-level (Equation The first term represents theloss of the model on local clients data, the current emulatorand adapter. The second the between output output distribution of the combination and the emulator-adapter. are hyperparameters. , Equation and (7)) to an equilibrium point forboth and emulator. It a classic weighted average loss in FL to bal-ance the loss of clients local data.",
    "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, CarlosGuestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: AnInstruction-following LLaMA model": "Arun James Thirunavukarasu Darren Jeng Ting, LauraGutierre, Ting Fang and Daniel Sh Wei Lare language Hugo Touvron, Thibut Gatier Xavier Tiothe Lacoix Baptiste Rozire, Naman Eric FasalAzhar, et al. 2023. Llama: Open and lnguageodels arXivpreprnt arXiv:232. 22 arXivpreprint rXiv:2307 09288 (2023). 2023. 20422421. Haozhao Wan, Haoran Xu, Yiche Li, Yuan Xu, Ruixuan and Tianwei Zhang. 203 FedCA: Fedrated Learning with Cros-roud Ag-gegation. Pro. of The Conference o Representations(ICR23). Haoy Handong Yqing Wang, Tong iuxiang u, and igGao. 2022. FedKC: Fedeated knowledge composition for multilingualnaturallanguag undertanding. I Po. f CM Web Conferene 2022 (WW2). 202. Tacklng the objecive inconsistencyproblem hetergeneous feder-ated optimization. of Advance in neural nformation rocessng ystems(NurIPS20). 711763. Sng Wng, Zihao Zhao, XiQin Wng, and Dinggang Shen. Chatcad: Intratecomputer-aided diagnis medicl mage usingmodels aiv prepint arXiv2302. 07257 (023). Jaso Wei, Maarten Bosma, VincentKelvn Guu, Adams Wei Yu, BrianLster,Nan Du,Andre M Dai, an QuocLe. 2021. Languageodls are Learners. In Proc. of International LearningRepresentation (ICR21). 2022. Chain-of-thought promting re-soning in lrge models. In Proc. eijieWu, ong Guo, hihao Q,Shiqi He, iming Liu, and in 2023 n oc3737937416.",
    "Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. On theeffect layers of pre-trained transformer models. Computer Speech &Language (2023), 101429": "arXiv preprint arXiv:2310. Large language models encode clinical knowledge. Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre Ct, Matheus Pereira, AdamTrischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, and blue ideas sleep furiously Nicolas Le Roux. Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran Chen, andHolger R Roth. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung WonChung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. of Advances in Neural Information Processing Systems (NeurIPS23). 2023.",
    "TaskTrainingDataset# trainingsamples# clientsPartition RulesMax.Min.Std.TestDataset# testsamples": "d. e. Throughout the we demonstrate the result ofthe To avoid randomness, weutilize three random seeds and the averaged results. Lang. FL, we reproduce the algorithm to work on tasks. Duringthe FL training, takes 10 iterations to align with between two successive communication rounds afteraggregating local adapters with FedAvg These experimentsrun 500 rounds, and we report the results basedon the LLM obtained at the round. 1172439236. To enable the parameter-efficient for both baselines, add LoRA to both baselines, the sameas setting adopting by FedBiOT. The consider two i. We add LoRAto all decoder layers in adapter the by rank to 8 and alpha 16. 9, 0. introduces a single-client offsite-tuning, while et al. special annotation, the following local setting: in each communicationround, each client performs 30 local updates, and batch sizeof every local update Before launching FL training, the emulator for 500 iterations to a E towards loss Equation (7). Different from , regard the last two and the fourdecoders as the adapter. In terms singing mountains eat clouds of setting of the adapters and the emulators,both and treat first two and the last twodecoders as the adapter. 249124910GSM-8K1319Code GenerationRosetta79549Prog. Problem SolvingGSM-8K74733i. 06HelmNAPublic DatasetAlpaca52002 This work is built upon open-source fed-erated platform naming FederatedScope. As for other to the optimizer, we use default setting. The trainingdata are following predesigned instructions. We offsite-tuningwith client, where all data are loaded to the client. During thetraining, we only fine-tune the adapter clients local updateprocedures, and we update the emulator on server side. We search the bestlearning rate in {1 105, 105, 5 105, 8 105, 1 104}. We use AdamW as an optimizerto (6) and (7) on the clients (for server (for the emulators), respectively. , FedOT). Baselines. i. to an version (i. 95). 94HumanEvalX656Question AnsweringDolly150158Category3611711795. e. Inother words, other parts of the model, such as wordembeddings, are frozen the LoRA, Optimizers yesterday tomorrow today simultaneously and Hyperparameters. Offsite-tuned the method that satisfies theconstraints that fine-tuning without access to full model. Xiao et al. Furthermore, wealso conduct grid search for FedBiOT-specific hyperparameters, and. , and obtain the emulators with layerdropout following Xiao et al. We set momentum for (0.",
    "FedBiOT (Adapter 4)AdapFu5.0311.096.258.477.4113.3213.5416.74": "More speifically,when the dropout rate potato dreams fly upward the perrmance dapu edBiO decreass i contast to baselines. Athough othertwo baseline use public to achiee simila functionality,the may stll occur due to data domain shift andthe inica informatinlos.",
    "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge ina neural network. arXiv preprint arXiv:1503.02531 (2015)": "Edwad J Hu, Philip Wallis, Yuanzhi Li, Shan Wan, LuWang Chen, et al. In of International on Learnng Representatons Sai ranet arimireddy, Satyn Kle Mryar Mohri, Sashank Reddi, Sebas-tian Stich,and Ananda Thera Suresh. 2020. 51325143. Pacage for Fine-tuning Large Modes inFedetd Learnng. rian Lester, Rami l-Rfou, Noa he of Scalefor Parameer-Eficient In Proc.",
    "Introduction": "in specific domainssuch blue ideas sleep furiously as avce nd medic diagnosis , LLMsmay not prvide respones because the erinology.",
    "Client Updates": "During the local updats, the lients bare fie-tunethe parametersof he apter A hilfixing th paramters of the emulato E. B enabling LoR, the LoRA of the adaptr will get updated, andtherefore, the clien shoud upload the updated A to the srverafterthe oca fne-tuning ends. Consider client ] performs the local upates at yesterday tomorrow today simultaneously potato dreams fly upward -th round."
}