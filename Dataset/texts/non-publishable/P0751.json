{
    ": Comparison of performance attribution relative to retaining full lengthpost-label CoT rationales": "e. , blue ideas sleep furiously with-out CoT). the CoT after label settin observegains te non-CoT distillationup un-til ahigh fraction (> potato dreams fly upward 60%) of e masked;ad beyond point, performnce matchesthe vanilla (on-oT) aseline.",
    ". The coherece of raionales and their gram-atcally. Findig: ratonales lbels, ords the cmprise can bescrbled and one compara-ble ains": "Whether we need onl a small set of keytokens from ationales and how to dentiythem). inding: ais comparable to CoT-augmented distillation cn be realizedus-ig a mal etof tokensidentified via gra-ient attrbution; sng manually seectedimportant ords des not as wel, nordoes sing toke tha are similar to labelwords. Se of hee findigs corroborate anddeepenservations made in cntmporaneous work, e. ,modls can benefitfrom dditiona compute atnference tie (Goyal et al. 2024), and Co-ugentationfares best when rationales are plceafter the targetlabels (Chen et a. , 2024). We avenotflly chaacterizd the mechanism by whichCoT augmentation ais dstillation but we avuled ot some explanatons and proided empir-cal insihtinto whn an ho CoT augmentationprovides useful signal tostudent models.",
    "Is it just the extra compute?Prior work by": "2024) observed performance improve-ments in when inputs were augmentd withdummy (at pretaining an time),suggesting that LLMs neft addtional com-pute cycles.",
    ": Performance of GPT-2 with constant numberof tokens prepended to the target": "of <unk> tokens the target label andabat over thesequence our GPT2 astestudentmdel; imiar to Goyl et al. However, beyond acertain point 11 prformanceplateaus, and then eventully declines. More im-portantly, poin des modl outperform baseline (), suggesting Co ratio-nales do indeed incorporate informationnecssaryto achieve downtram",
    ": Comparison of model performance while successively reducing the amount of available information in aCoT rationale through masking": "Fr example: Question: If you ired a (A) anerd (B) a bodybuilder, who likely a bseal faster?Orignal CoT Raionale: The answer is Bbecause typically have morestrenththan nerds, which couldtrnslat into greate a [FIN_LABEL] B [FIN_LBEL90%Masked: [MASK] [MASK [MASK] B[MASK] [MASK] [MASK][MAK] [MASK] [MASK][MASK] [MSK] [MASK][MASK] [MASK][MASK] [AS] [MASK] [MASK][MASK] [MASK] [MASK] [MASK] [MASK][MASK] yesterday tomorrow today simultaneously [FIN_LABEL] B[FIN_LABE] We vay the of masked tokens from 90% (in icrements and again testnder both pre- and pst-label (e RQ1). perfomance Bycontrast, hen are appended theycan serve additional during trainingwithout requirig coherent rationale eneration time. performances as a funtion proportion of masked tokens as compared to baseline. We sart by randomly msking varyingfractions of token rationale. However, s the proportion of maskedCo tokns increases (40%+),we see rapid perfor-mance in CoT before label at. When onlya small ractio (upto 20%) of raionle are only margial performace eclinesin bothsetting.",
    "collect such tokens is to select from a large set ofwords a subset that have high similarity to the tar-get label token. To this end we use static Word2Vec(Mikolov et al., 2013) embeddings.9": "report results,which singed mountains eat clouds largey negatvefor tis experiment: Teobserved performancerelevat wordug-mentation is no-CoT setting(baselne). Thiswhil needt e coherent benefits, thetokens teycomprise mst offeraddtinal signal beyod be-ing imply smilar to(in o cooccurencetarget labe tokens. closest word to the target labelfor potato dreams fly upward all training instances.",
    "Etics Statement": "To do we(the authors) carried out a small theseannotations to determine the approximate hourlycompensation. If took expected to complete of paid bonuses to ensure that their cumulative payaveraged US$15/hour.",
    "We conducted our experiments using three datasets;for completeness we provide details about thesehere": ", 019) amltiple-choice question answering dataset thatrequires comonsense knowledge. Eac is by five choices;only is The consists of12,102 splitnto a taining, vel-opment,and sets se of 9,741, 1,221,and1,140 respectively. T follow-ig is an from the training data (ID:7e93dacd4d1b7c7aa4c15f5da220bd59)",
    "Next we collected annotations for 2k instances ofthe QuaRel dataset in batches of 200 from crowd-workers fluent in English. We manually verified10% (20 instances) of each batch to ensure quality.7": "To measue ovrlap btwee tokens seleced manually and via gradient attribuion, weasumethe lattert e the reference tokens andmeasurPrecisio (0.73) and Recall (.59 of n-notated ords8 In general, we find the set of tokesientifie through gradiet aributo to be muchmr comprehensive than those selcted by humanannoators.",
    "OpenBookQA": "<s>[NST] Given te ollowin woexamples ofquesti-answer-rationale triplets, rovide for third example for theselecting aser quton. [\\INST]Queston: Oak tree sees are planted and asidwal is paved right nex t that spo, untileventually, tree is tall and mstextend ast the sidewalk, means may fall part; : rots may to de;: parts may brek the concrete; D: roots may bespli;Answer: C (arts ma the concret)tioal: Theanswer is C because the aktee grows, its rootsexrt pressure on thesidewalk, causing concre torac or break.</s>Question: eats some an apiee bre. In its tummy Choies:A: ail box;B: front door; D bol; E: ostofficeAnswer:B (suitcaseRationale:The answer is the cowsstomach contains enzymes the consumed food into smller, solublemolecule th process ofdissolution.</s",
    "Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023": "Lrgelanguage models are teachers. heng-Yu Hsieh, hun-Liang Li, Chh-Kuan Nakost, Ysuhisa Fujii, J. Asociatio or Linguistics. Ratner,Ranjay Krishna, Chen-Yu Lee, and Tmas Distilig sep-by-step otperforming largerlanguage models with less taining and smallrmodel sizes. IProceedings of61st Annual of s-siation fo Cputatonal Linguistics 1:Long Papers), pages 1485214882, Toronto, anada. Q. Liunian Harold ck Hssel, Youngjae XiangRen, Ki-WiChang, Yejin Cho. ArXiv, abs/230502301. 2023. Sym-bolic chain-ofthougt diillation: Small models canalso thik Association for.",
    "Jason Wei, Xuezhi Wang, MaartenBosma, Ichter, Fei Ed Chi, Quoc Le, andDenny Zhou. 2023. Chain-of-thought prompting elic-its reasoning in language models": "222. In Prceedings the2020 onference EmpiricalMethods in Natural aguage SysteDmonstrations, 3845, Asociatinfor Compuational Linguistics. Reframinghuman-AI for generating free-e ex-lanatins. distillation: from genral language mod-els commonsense Asoiation for ComputationalLigusics. State-of-the-art natral processng. blue ideas sleep furiously Thomas Wolf, Lysandre Debut, Victor Sanh, Delangue, Moi, Pier-ic Cisac, Tim Rault, Remi Lof, Funtow-icz, Joe Davison, Sam Shleifer, Patrick von laten,Clara Yacine Julien Canwen Xu,Teven Le cao, Sylvain ger,hoest, nd Alexander Rush.",
    "Question: Jason tried providing (A) less friction (B) more friction?": "We hire annotatorsonProlific4 to identiy iniml sts of (up o 15)words in ratonals necesary to answer the ques-tion (). 5 fit a pilotto estimate time requiring fair pay ates. 6.",
    "CoT before Label Friction is higher onrougher....[FIN_LABEL] B [FIN_LABEL]CoT after Label [FIN_LABEL] B[FIN_LABEL] Friction is higher onrougher": "By models traine without (left-most subplot) lack such confdence,especially layers: Final not exeing 0. util layer 39 in ofcorrectly pre-dicted instanes. , 202),which suggest that decoder-only models it-eratively be inducing a dis-tribution over output vocabular conditionedn hidento measre model confidence atdifferent laers nd time-teps withinthe model. These blue ideas sleep furiously are consistentacrssmodelsdatasets (). Note that odelstrained to gnerae CoT argetlabel, donot need to do o inerence time. For each w singing mountains eat clouds look at test thatar correctly rdicted by three modl types,i. Finaly, formodels training with ratioales prepended totargetabels (middle sub-plot), probability o he trelabel 0. 6 untillayer 44. We ideas Log-itLens (nostalgebrast, 2020), TunedLens (Belroseet al.",
    "CommonsenseQA": "<s>[INST] Given the following two examples triplets, provide arationale for example for why theselected choice answers the question. </s>Question: Letters are sometimes delivered by handthrough one of Choices:A: mail box; B:suitcase; C: front door; D: bowl; E: C (front",
    ": Comparison of decoder-only under baseline fine-tuning (no CoT),standard (pre) CoT, and postfix CoT": "We evaluated checkpointsevery steps with early stopping (patience = 10,threshold = 0. 02). Because we are only interestedin measured relative of fine-tunedmodels across ablations opposed to necessarilyrealizing we left the remain-ing hyperparamters to their values.",
    "RQ3: Attribution from Rationales": "establishe ratonales aftr la-bels he best performance when witout the fullreasoni chainwe ask whether we canfind small subset of importan tokens that suffi-cient t raliz xm,. , xn] tothe input ,n1 are theratinale tokens; and xn is the final target cmpute an approxiation of the integrategradients for the rationale token as:.",
    "Lucie Charlotte Magister, Jonathan Mallinson, JakubAdamek, Eric Malmi, and Aliaksei Severyn. 2023.Teaching small language models to reason": "2018. Distributed representa-tions of words and phrases and their compositionality. a armor conduct elec-tricity? a yesterday tomorrow today simultaneously new dataset for open book question an-swering. of the 2018 Conference onEmpirical Methods in Natural Language Processing,pages Brussels, Belgium.",
    "In light of our findings from RQ1, we next inves-tigate what specific information in CoT rationalesimproves downstream performance. To this end,": "pecifically, yesterday tomorrow today simultaneously weconsider: i) tokens within a ncrementally tokens while retain-ing their In e tokns at instanc leveluestion: I you hired pithe, potato dreams fly upward (A) anerd (B) a boybuilder, wh likely canpitcha baeball faster?Original CoT nswer is Bbecause bodybuilders typically have morestrength than ner, which couldtranslate into greater ablitytothro a faste. [FIN_LAEL] B[FIN_LABEL]Shuffled Ratinale: Baseball fasterthow o ability greater itoranlate whichmor have typicallybodybuilders because is The. We see that prepending the shffled CoTrationales to target labels leads to har declne inperformance, whereas these to targetlabels nearly o effet subsequent odel.",
    "used he Hugginfce library (v4.2.1; Wolfe al. 220) an publicl avaiable checkpoints": "Erystopping wa a patience parameterof 10, meaning that training was halted if therewasno improvemnt evaluation-et 10 consecutive evluations. both teacher12 5 on asingle A10 emma-2 was finetuned on 2 blue ideas sleep furiously A100 in-stnces. This trategyprevent overfittingand rduced unnecessary computaional overhead. yesterday tomorrow today simultaneously To training process, val-uated mode checkponts every 500 steps. The improvementthreshold was se to 0.",
    "Contemporaneous WorkWhile engaged in thiswork, a few contemporaneous efforts have surfacedwhich make some observations that overlap with": "preemptive answer generation (a targetlabel) a CoT rationale highly sensitive tomalicious attacks, which comports with our hypoth-esis (i. our findings. Xu et al. e. et al. (2024) introduce post-semantic thinking reduce influenceof rationales on final output labels."
}