{
    ": High Level System Architecture": "are two binary pedcates we concatenate values of isRow(r, x) ad isColumn(c,x) and cll te resultant ensor asastackd binry predicate. Th reinforcement agorithm also checks if the next sate generaed is a sovedSudokugrid. The last layr of the NLM logic machine comutes the SoftMax vaue and rvides the empty cell position r c), asel as he target vlue to place in the empty cell. However, the above strategy may take indefnie numer f steps to find a olution. Here comes he role of te reinforcement modul.",
    ": Comparison of NLM and backtracking convergence time": "In addition to fine-tuned mode we havealso drawna time complexity aalysisf the NLM with traditionalbacktracking lgorthms for solving Sudku puzzls. Both th NL and bactracking algorithsare providing withthe same set of grids and their ime tosole comlete grid is highlighted in. motivation behind thiis to shocase te differnce inthe principle working blue ideas sleep furiously ofboth the agorithms aalyze teir convergence time (wihliited training i t case of NLs). Thebacktrackig algorithm takes a constant avrage time of nearly 0. It is alo worthmentionin that deonsrates the ietakn by the NLM with 729 mximum number o steps. reasn tht backtracking converges faster is due to e fact that it solves th grid inn optimal nuber of step (. . In , h ea time in tcase of the NL enotesthe nstane in which the nvironmen was res d the formation of an invaid configurationof the sdok grid.",
    ": Model Architecture of Neural Logic Machine": "our roposed model for the Neur-Symbolic Sudoku Solera ybrid arhictref Dee Reinforcement technques and Symbolic Lering. half of he model learningphase, where th Sigmoid function acts the ctivation function between hiddenayers, and the SoftMax unctionctivates the layer. The ip layerconsist of 4 neuron each aceptng a cetain type of parameer. to allcate  certain typeof input toneurn, leads to greater systematicity i th model. The of slving Suoku pules cecking for eah row, colmn, and submatrix tomaintain aSudu grid. In orderto check this the coordinates of the rows and the target values are passed the input neurons i and i3 receive input as and while neurons i0 receive iputs. In contrast t thisxperimen, showstheir problemsreuire predicates for a set ofneurons in theOce setof predicates is received by the ipt layer, theinput for the following layers are reduced or on the rity of the prvous layer. The output from the SoftMax laye is fetcd by the Reinfcemnt Learing (RL) module, wich constitutes Phase2 the archtectur see 2). The RL module takescare of three main functioalities: Allocating a +1 a soved grid, a negative f -0. every performingenvironmental afterchecking target values.The RL Mdule trgger an environmental ese  o thetrget vauesfrom 1-9can fill anempty cell while mantaning a valid the Sudoku grid.During thi reset, all filld values areempted the gridisreinitialized, and a ne ieaio begins. Given an grid we have implemented a multistge achitectre to solve te grid step-by-sep. Te first step in ths implementation diagram consits of calculang the boolean prdictes. Tee are three importantrule to solve a sudku grid: he ned to a numer i an empty cell suc that the rsulting configu-ration remains aid. Here, valid configuratios rfers t the states each number in row, column, x3sbmatrix is Wih theoftheseconceptual rules, fted Boolan predicates may be Lifted rues are rules to  ask tocrack it solability. be seen as simplestfundamentalo a We define the isRow(r, x),which computes ther number x existanwhere in row r. isColumnc, computes whethe number  anywhere in column c andpredicate isSubMat(r, c, x) whethernmbr x exists in the 3x submatrixcotining the cel (r, c) I is mentioning in ths sy we d inputthe unslved into the inpu layer of theneuranetwork. Sinc",
    "Related work": "The rising demand to train Neural Networks for performing complex tasks has generated gret attention amon theresearhe. Hoeer, their lac of systematicity and nailty to generaliz for a greater set of inputs has lead them topeform poorly on more systematc tasks T address these challengs, proposed the Neural Logic Machine, whichcan solve problems requiring systemsto perform actionsby following stematic sets of rle. In , NLMs utilizesthe reationships of objects btained fom quantifers and logicpredicates to solv BlockWorld gmes, list sortngand path-finding tasks. study done in highlights difference between convetiona RNNs (RecurenNeurNtworks) with ther proposed NLM, addressing the RNNs diffiuly n smaller lists, and ailure to sort slightlylargerlits during tsting The reaon behind that is RNNs trained on smaller lists will not be able to sstematically generlizt for arger lsts wereasNLMcn. An alternate approach to functonapproximators habeen used with Ncalled the EINFORCE lgorithm ,which is used for polc gradiet optimization and estimates the gradiet using Mont-Cro method. This is com-monly using in deep reinforcemnt learning where the actions are sample nd the neural network cn not perormbackpropgatin since sampled i nonifferentiable oeration. proposed novel deep-learning architecure called th SATNet, which is a differentible maximum satisfabilty solver that uses CNNs. It is approximate-differentibe solver which works on a fst cordinatedescent approach for soving emidefinite programs (SDP) associating with the Maximum Satisfiability problem. Lastl, this experimentalsofocuss on ralizing the true potential of NLMs in ifferent areas of applations.",
    ": Separate convergence for and NLM for same problem": "this case, again tries to fill empty cellsbut with different set of target from the However, even with 10 empty cells, our modifiing the NLM always takes than 2. During this instance, model first reward the reinforcement and resets theenvironment singing mountains eat clouds once there are no possible target values to test.",
    "Introduction": "By testing one model, called the Nural Logic Machine  we mpasise on the elevanceof symbolic learning solvi comple problems on whichmoder dep learn mehods fail. Wearchitecture of Neural Logic for solvinga cmplex puzzle Sudokuusing our seto pedicates input. Fst, we trained. Threfore, also to test the NLMs toecoer these lifted rul andpply temn the later stages learning - whn the complexity of the problem riss. Toaccomplish th,we changed the number o empty cells grid whie trinng. afocussed on issueenerating hybrid models whih Neural with mbolicLearning. Th groundbreaking rests of themodern models have that they are ideal ools to solvecmplex problems, however the lackin these modelshave been a problem for soe time.",
    "Neuro-Symbolic Sudoku SolverA PREPRINT": "Thisapproach where the complexity of the problem training, is known as curriculum learning. the NLM on sudoku grids with empty cells, the number of singing mountains eat clouds which increased training progressed. Secondly,we symbolic learning with reinforcement to award model every time a valid configuration of theempty Finally, the convergence time of the algorithm yesterday tomorrow today simultaneously was compared usinga graph.",
    "Result and Analysis": "To begin the experiment, the number of empty cells and the maximum steps in the sudoku grid are limited to 3and 81 respectively. As these parameters change over time,the complexity to solve the problem also increases. To give a better understanding on how the model performs with different parameters, we have demonstrated the successrate with respect to each parameter that the model was trained on. The model when tested with the minimum number ofempty cells (nr empty) 3 and max steps set to 81, gives a success rate of 0. 94. However, when the maximum numberof steps (max steps) are increased from 81 to 150, the model receives a perfect score of 1.",
    "Conclusion and Future Work": "Wepropose that the applications of NLMs can be extended further with even more games (e. The focus of this study is to tackle one of the drawbacks of the traditional Neural Networks i. , Ken Ken puzzles) andmathematical problems (such as search tasks). Thus, with this experiment,we have been able to strengthen the argument that NLM can solve tasks with 100% accuracy without relying onover-fitting. Lastly, because the NLM receives a random combination of grids and number empty cells from, weare confident that the high success rate of NLMs is not due to the models over-fitting. , systematicity. g. e. In , we also deduce that that the success rate is directly associated with the number of emptycells and the maximum number of steps that model is allowed to take. To conclude, Neuro Logic Machines can solve complex problems using a hybrid approach of Reinforcement andSymbolic Learning. NLMs have been trainedand tested by on various tasks which Deep Learning models have failed to solve or converge. In our paper, weadded to the existing applications of their architecture and solved a more complex problem to test the robustness ofNeuro Logic Machines. While the Neuro Logic Machines failed to converge for Sudoku puzzles faster than the backtracking algorithm, it isevident from this study that a Neuro Logic Machine can be trained to solve tasks where conventional Deep Learningmodels may fail.",
    "Testing the Robustness of the Neural Logic Machine:": "Whereas, in Sudoku, we need to fill th yesterday tomorrow today simultaneously gap with appropriate umerscecing rows,columns and submatrices. To sort an array, we need tocompareelements with eachother and wap if needed. This mkes the probem more complex."
}