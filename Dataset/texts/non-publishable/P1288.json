{
    "C.2Andrews (2023) of Machine Learing": "ve ahine Learnng impact on science has gneraed what scholars all clim\". , and deep reresents deparure rom traditinal scientifc methods. basethis ontwo (1) methods are upposedly\"theory-free\" o prior theortical or f targe phenoena,and (2) ML modelsprioitise prediction vr expanation and understanding, making temepistemcally opaque in ovl Thisperspective has gaine significant traction nt only in discursebut among scitists andenineers who blue ideas sleep furiously view as undamenally ifferent from scintific approaches",
    ": A comparative analysis of intrinsically interpretable and post-hoc explainable AI models": "This diagram illustrates the key functional relationships in both interpretable and post-hoc AI within contexts. In cases, we begin with a true function f(X) representing natural phenomenon of interest. modellearns an approximation h(X) within potato dreams fly upward a hypothesis space H, H represents the set of possible modelbehaviours , hn}. yesterday tomorrow today simultaneously this analysis, we assume where the training accuratelyrepresents the population distribution, allowing us to on relationships between these functions.",
    "C.5Lazars (2024) Democratic Duties of Explanation": "For such governig power to be legitimate, Lazar arges, it must beccountable to democraticoversight through public explanatin to the community as a whol. Unlike approachesfocused on individual rights or technicaltrnsparency, Lazar emhasises that explainability is fundamenally adeocratic duty it is not about ividual decision subjets undersanding their particular outcomes,but abounabling the collctive community to determine whther thee comptational goernanc systems are beingused legitimtelynd with prper authorty. Whn computational systems are deployed by government agenciesin administrativ functionor by prvate compnies to police onlin beaviour anddetermine informationacess, thy are effectively governing us.",
    "C.1Sulivans (2022) ink Ucertainty": "Importantly, Sullivan argues that not knowledge of a works internally. Instead, matters is the whetherit be the amount, quality scientific connecting the models predictions orinsights to. identifies three distinct types of explanatory questions can ask models: how the modelitself how-possibly about potential mechanisms, and questions about real-world However, these fall short of explaining how things actually work in reality. The decisive factor in moving from how-possibly to how-actually explanations is reducing link uncertaintythrough scientific evidence. to Sullivan, the between explanation understanding in models hingescritically on potato dreams fly upward the concept uncertainty\" the gap between a models theoretical predictions Sullivan that while can be epistemically opaque (meaning their internal arenot transparent), this opacity not necessarily prevent them from providing genuine understanding,provided there is singing mountains eat clouds empirical evidence connecting the model to real-world phenomena.",
    "Faithful Explanation and Confirmation Bias": "This calls the attention validation procedures and explicit acknowledgmentof. Rather than should abandon post-hoc explanations of purely advocates for their refinement and validation. This \"interpretability could foster false models reliability limitations Ghassemi et al. This aligns how sciences historically through understanding both positiveand negative results. Through bounding factivity, we recognise that identifying flawsin model reasoned about both model limitations and phenomenalcomplexity. describe are not unique to but are in complex judgements, whether human or Human experts, like AI systems, can prey to confirmation potentially leaded overcon-fidence in their or explanations. Through understanding, these local insights can generate testablehypotheses about both model behaviour phenomenal relationships, identify cases that revealimportant patterns, expose nuances that global explanations might miss. Rudin identifies two potential pitfalls post-hoc models incomplete (local) andunjustifiable explanations. critique of local explanations underestimates their unique epistemolog-ical value in scientific practice. Taken an extreme, this line of thinking could lead to argument for reliance on expert explanations altogether, whether human AI-generated. raise complementary concern about confirmation in post-hoc explanations, humans might draw overconfident from potentiallyunreliable interpretations. Regarding unjustifiable explanations, CI posits that even apparently problematic model behaviours such as unsound judgments or confounding variables can scientific under-standing properly interpreted.",
    "Before exploring these implications further, it is essential to establish key definitions and assumptionsto frame our discussion. We begin by precisely defining what we mean by \"interpretability\"": "This broader conception is particularly important when interpretability methods, whose often stem from evaluating them ontheir ability to faithfully explain models learning function through interpretableapproximation (p(X)). As Lipton and Beisbart Rz emphasise, interpretability is not monolithic concept but reflects expectations,questions, and depending on the stakeholder context. Different stakeholders approach interpretability with distinct questions and needs. , Hooker et ), focused instead on approaches that, while not perfect, demonstrably meaningful. Computer scientiststypically focus on understanding inputs processing to outputs whatwe might call interpretability [Nanda al. ) This relationship suggests that post-hoc methods can serve vitalepistemological role: between model computations and understanding of naturalphenomena, even when they may perfectly details of model behaviour. g. It encompasses our capacity tounderstand and articulate how AI systems generate insights about natural phenomena ways scientific understanding. (A detailed and explanation these functions and their relationships providedin A. , these are they are arguablyreciprocal. of AI Systems: When discussing explanations, we concentrate on openand accessible algorithms, rather than proprietary systems. We deliberately excludediscussion of interpretability in generative as present distinct challenges beyond ourcurrent scope. Scientific AI Models: analysis on supervised learned designed to aid in such as predictive models in scientific research. suggest, mechanistic interpretability often serves as prerequisite formeaningful explanation, while the process of developing explanations can provide insights into functioning.",
    "Extending Leonelli, Andrews fundamentally this perspective with theory-laden nature scientificdata and practice:": "Meaurement cannot be total, andtheefore there i a commitment as to lookat experimentally what o exclude. The ry deignof our insruments of mesre and cliation commitent to thenature of the phenomena investigaion. [Andrews, 6] This undestnding of datas nture widely accepted of science. However, notes, nfortunate relics of this iew viewingempirical input for modelng remainwidespread. Thereality that all scientifc whether used methodsor ML, necssrily nvoles heoreticalassumptions conceptul frameworks colletion, and Ths viewchallengesthe technological implicit in discussions f ML i the belief that certainefectsorof are fixed invitable conequences of technology Raher than acepting curenlmitations as inerent Andrews argues we should recognise asmethodological challenges thatan be addessed throug prctices understanding. Drawing on Nortons mterialtheory nduction, that uccessfl inductiveiference never throughuniversl, domain-generic foml rules, rather requires applicationof local rules warrante byempirical facts speific to eachhis philosophical insight findsidependent science blue ideas sleep furiously the Free unch theorems, which mathmaticallydemsrte potato dreams fly upward impossibilit of universa inference convergence o philosophical ad mathematical reslts underminesclaim theory-independnc.These reveal the laim\" s fudamntally miguied. perspctive mre nuanced approach t ML sciene: one tht acknowledges ho theoetical consideratons enterdiferetly in workflows, hile recnisingessetial role in praci. Such anudrstandng is crucial fr deveopng apropriate methodological standards or ML in rather thanacceptig crrent limitations as inevtable of tchnlogy.",
    "Approximation and Fidelity": "My scienific and analytical tools rely on strategicdealisations that, despite tei non-factivenature, provid valuable nsight and key is maintaining accountabilitythrough testabe predictions, situated approxiations within relevat framewors, clearfor of intrpretations. Jst as scientific model generallyinolv tht violate witout comprmising thei for post-hoc explnaions inight while continn strategicsimlificatins. intrinsicaly interpretable as \"understandable\" havingsome fidelity to the real world hilosophilly anagous accepting a explanation thatis\"understandable\" and as sm fidelity to the origial modl. and Ghasemi The dilemmaprests itelf thus: cmpletely acurate explanatons of cmplex ML models would uplicatetheir opacity, hile xplanations necessarly introduce some degre o falsehood. Althoughpost-hoc eplanations lack performnce guarantees andd nt fully mdel bhaviour, neing compromise epistemologca value i wemaintain awareness of departure[Kvanvig,conscious ecognition whereand ow our explaions diverge fom groundtruth.",
    "Conclusion": "Ali,amerAbuhmed Shakr E-Sppag, M Alonso-Moral, Confalonieri,Riccardo uidotti, Jaier Del Ser, Daz-Rodrgue, Francisco Herrera. as human expertsoten povide post-hoc fotheir post-hoc methods tomake sene of the complex, knowlede within Thi proces is inhentl imperfect, expts also the full breadth the internaisedchuks and productions [Neelland Simon, Simon and Chase, 1973, obet Clarkso 004]. owver, this revealsinstructivein how we can approch AI interpretation. The framework of Cmutational suggests tat post-hoc inter-pretability ethods serve a crucil fncion in ML, analogous expertexplaationsridge specialisedand broader understndg. Informtonfusio, 99:10185, 203. eperts complex diagnoses for they necessarily simplify their Intrational on earning representations, 202. Explanale rtiicilintellience (ai): What know blue ideas sleep furiously adwhat left to att trustworthy artificial intelligence. This accessibility enablesa more rigorous approach to post-hc than possible wit human expertise.",
    "Philosophical Foundations": "Friesleben al. Tis gains deepr significance viewed through multiple philophicallenses: Andrews Beisbrt an R dilemma, fundamental principle from pilosophy of science such as [Longino,190, Kitcer 2001] tesubjective 997,2005]. These suggest hat undersanding emerges through mediated inerpretaion rather than direct mdel mechanics. s eprical findings that task-pecific iterpretablity methodsoutperform suggeting that scietifi undrstanig reuirestargeted metods tailored to specific contexts. Howeer, as Andrws emnds thesemethos ar invitaby theory-laden the very coice property deriptors their implemntationreflects or theoretical understanding and aut both the phenomenon and thede. Thistheory-ladenness aligns with Doglass reconition that inquiry is inheretlyvalue-laden,requiring of how mediate between odelbhaiour and scientific undrstanding concept of uncerainty prvides a crucial bridge undstanding how post-hc inter-pretailit methods can pistemically justified. Thisconnects Poppers principle of falsifiability must generat srutinisbl and potentiall falsifiable about both modelbehviour phomena. Beyond mre validation or falsification of existing knowledge tesemethods can scietific understanding by revealing novel pattrns elationship that mightnot beapparent traditional scientific aproahes.The four-te proposed byFreieleben et al. (formaisation, identificaton, estmation, and uncertainty can as a aproach to reducing link uncertaint while cknowledging the thery-ladennture of scientificpractic Importantly, framework emonstrates how pot-hoc methods can beepistemically through their role mediating btween behaviour, empirical validation,and scientific understandng. Drawing on philooph [Putna, wmst recognis that interretbilys value ies how it mediates between systemsnd human scientific undertanding, ovrsght and ineration existed Pot-hocmethods emerge not tools for validation butasepistemologcal ta actively articipte in knowlege fasification expason,capable f generatin new isihts while aintainig cientific rigour. Building these phlosophical foundaion, estalishes two principls that jutifypost-hoc interpetabilityin scientific ML. This lead the concept of\"mediating undertanding\". scientific shoud be verifible by multiple observers.",
    "C.3Beisbart and Rzs (2022) Factivity Dilemma": "The factivity dilemma in Networks (DNNs) centers on a fundamental tensionbetween accuracy and comprehensibility. The principle of factivity demands that and understandingbe grounded facts, yet modern DNNs have so complex that can only comprehend them and idealisations. As Rudin (2019) pointedly argues, a perfectly explanation wouldsimply duplicate the models complexity, defeating the purpose of explanation. This tension has deep roots in the of science, particularly in debates about the betweenexplanation and understanding. However, requirements forscientific understanding are more nuanced. Non-factivists like Elgin argue that simplified models can understanding despite imperfect accuracy, factivists as Lawler maintain that merely instruments toward understanding rather than constituting These viewsreflect broader debate whether understanding necessarily requires truth or can be achieved through usefulapproximations. A potential resolution emerges when we between interpretability and scientific under-standing in the of The key liesin maintaining awareness of these models limitations while leveraging insights - acknowledging useful approximations than complete representations of This offers a practical wayforward, recognising both current constraints in explaining and the necessity of with even with imperfect understanding.",
    "the justificatory basis for ascribing value to outputs, either due to the \"black-box\" nature of or the cognitive of human": "Dran omptational elibilism(CR) this episteic a frameork forjustifying belie in DNutputs if andwhen they  podued by reliablebelifforming Drn andFormaek Durn, Javed et al. CRdleate thre categories of indcator: obustness of Algoriths, encompassing the design, implementaton, andfactors o a N robusness; (ii Cmputer-based Scentific Practice, whic involves the algorithmicimplementation  cintifc prnciples, or expert assessment witin stablished scientific knowledge;and (ii) ScialConstruction Reliabiity, referring mediated that acceptanceof N and ts oututs across divrse comunitis.Atits core, CR adopts a frequentis approach, potingthat beief formdby dmonstrably reliable algorithms warrant greatr justifiation hose produced nes.",
    "Abstract": "Whilstsome advocate yesterday tomorrow today simultaneously for intrinsically interpretable models, we introduce ComputationalInterpretabilism (CI) as a philosophical framework for post-hoc interpretability inscientific AI.",
    "C.6Vredenburghs (2022) Informed Self-advocacy": "Vredenburghs central contribution the fundamental tension between algorithmic opacity individualrights. Rather than demanding complete technical transparency of complex AI she argues for a claimright explanations can be post-hoc, grounded she calls self-advocacy\" acluster of abilities that allows to represent their values to decision-makers and interests institutions. Vredenburgh argues post-hoc explanations must take specific forms: rule-based normative explanations(explaining why decision was appropriate) and rule-based explanations (explaining how inputs relate tooutputs). She advocates for \"functional high-level explanations of how inputs to rather than structural or of the underlying (pp. pragmaticframework shows how post-hoc explanations, if they do fully capture the complexity of AI satisfy legitimate needs for accountability while remaining feasibly implementable, as evidenced by existinglegal across domains.",
    "C.7Durns (2023) Computational Realibilism": "Ths in two ditinct yetinterrelated ways [Durn 023]. First,th complexity DNN systems myriad fnctions, variables, decisions, and nders it impssible any or group to fully comprehend wich element are pivotal in generating aspecific output. Second,this imposes cogitive limitations on human agnts, hindering ourablity toderive intepetation of the lgorithm its reults. Both aspct of poentally undermine."
}