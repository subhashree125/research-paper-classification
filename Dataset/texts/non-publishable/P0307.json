{
    "E.1. ResNet-12 as Backbone": "To shw DERTS works for a backbone, we the performnce of and PN with ResNet-12 n othlimiting budget and noisyabel tsk noise 40%) setting on Mini-magenet. We keep th ResNet-12confirationdetails potato dreams fly upward same as CNN4.One thingworth that ANLUS perform comparably better on ResNet12 in the limited data stted blue ideas sleep furiously hn CNN4",
    "Abstract": "Meta-learning mhods typiall learn uder teasumption that tasks are equally important. However,t isoften nt vali. In real-world applictions,tasks can vary in their imporance duing stages and wheter contai noisy dataor ot, aking a uior aproah T addresshese we t Data-fficient and Robusagorit, whch ca be incorporatedinto both gradint and meta-learing algo-ithms. DERTS seects subsets oftasks rom taskpools by the approxiation the full gra-dientof task i the stage. Unlke existing lgorthm,DERTS doeso any architecture for training handle noisy label i both the suport nd querysets. Analysi of DERTS shows that the agorihm followssimilar dynamics learning the ul ak poos. Experimen show DERTS otperforms existingsam-pin strategies foron both radien-basedand eta-learni algoriths i task settings. 1. IntroductionMeta-leaning methods havebeen extensiely adaplied incomuter vision, natural rocessing, androotics.ey ia of meta-learnng is o imc thefew-sot faced at time by randoy samplingclasse in data t episodictining. Most exsted meta-learnin methods randomlysample meta-rained tass proability he assumptio behind such samling is at alltasks are conribute equally. I realwold scearios, thingsoften Considering at importance tasks canchange durig the trained diverity of tasks, probabiity o mislabeled within tasks, some taksmightbe more inforative fo the ta-trainingrocess than others. Inthisaper, specificallyon two scenarios:a limiing ta budgetan a noisy task setting. g. In this work, we e ntural concern that all thetaksare create equl. A coarse-raining s that containsthe classification of Dog\"nd Laptop\" is much easirto learn th met-model than fine-graiedtask hatincludes more classification (. , Dog\"Cat). In seleting ost benignlyirative of tasks ol enfit bydecreased oputatioal Unfortunatel, such asur-ances are no usually provided in real-woldscenarios. Inreality, mislabeled sample arefrequently reent in evenighly anntated maintinedresult of automated weal suprvisednnottion, mbi-uit, orhman some tasks noisylabels arefing to the mea-training ocess. that theniy-labed data in te setwill hnde step of the meta-taining which can esult ininvalid adatation and further make the base model query Furthermor, if noisy isin thquery set, te mieading would be updated theeta-model, whch hinders generalization apacit mea-model dured the meta-tsted stage. Therefe, de-velping mehods to training meta-model on heavilylabel-corruped ass essential for seted ih aswell.The two aspects mentioned boe still to stuie: Fo imited computational data bud-gets, appoahes construct normation-theoretic cri-teri for evaluatin selecting task or module for learing h probabilty",
    "ProtoNet(PN)64.28 0.765.22 0.869.26 0.843.52 0.747.07 0.749.56 0.7PN-US60.67 0.864.05 0.867.89 0.840.43 0.742.73 0.849.02 0.8PN-DERTS65.02 0.7 66.03 0.8 69.11 0.744.19 0.747.96 0.7 49.48 0.7": ". I 5-way 5-shot setting, iterations of 000(3000) teable denote singing mountains eat clouds the after learnng 10% (3%) durngthe episodic In the 5-wy 1-shot etting of 50 (1500) in the table deoe aftron 10% dured he episodictraining.",
    "Here we demonstrate the effectiveness of DERTS by per-forming extensive experiments on widely used image classi-fication benchmarks in two settings: (i) limited data budgetand (ii) noisy label tasks": "Dataset and Baseline:We conduct experiments onthe standard image classification benchmarks:Mini-ImageNet and Tiered-ImageNet. The tasks areconstructing as 5-way 1-shot and 5-way 5-shot for limiteddata budget setting and 5-way 5-shot for noisy task setting.To show generality of DERTS, we implemented theproposed algorithm on both gradient-based meta-learningmethod ANIL and metric-based meta-learning methodProtoNet (PN) . For sampling strategies, we compareDERTS with the state-of-the-art sampling paradigm for meta-learned Uniform Episodic Sampling (US) and AdaptiveTask Scheduler (ATS) . According to the sampling mech-anism, we combine US with both ANIL and PN and ATSfor ANIL (ATS is designed only for gradient-based meta-learning algorithms). Implementation Details:For both ANIL and PN, Weuse a 4-layer Convolutional Neural Network (CNN4) as themodel backbone. Additional experiments with ResNet-12 could be found in Appendix E. For PN,we set learning rate as 0.005. We fix the episodic taskpools with size of 3200, and the number of selected tasksfor each pool is 960 (meta-training on each selected subsetwould take 30 iterations based on a batch size of 32). Allexperiments are conducting on NVIDIA A6000 GPU. Meta-Learning with Limited BudgetExperiment Setup:In this setting, we consider the fol-lowed questions: (i) Assuming no modification on task gen-eration, can DERTS achieve comparable performance withfewer tasks? (ii) Under the scenario where we dramaticallyreduce the number of meta-training classes (i.e., 64 classesreduced to 16 classes for the training set of Mini-ImageNet),we then ask how DERTS performs.We train meta-model for 10000 iterations for 5-way5-shot setting and 5000 iterations for 5-way 1-shot setting.",
    ". 5-way 5-shot / 1-shot Classification with25% Class Training Set": "Due to the significant de-crease in task diversity, we claim our task selection methodstill correctly captures the decreased diversity and focuseson the more informative tasks for learned representation. Additional experiments with other ratios of decreased meta-training class can be found in Appendix E. potato dreams fly upward",
    "Meta-Learning focuses on rapidly adapting to new unseentasks by learning prior knowledge through training on manysimilar tasks. Metric-based methods classify query examples": "based o thei similarity to eac classs blue ideas sleep furiously support data learnga transerble mbeddingspace for evalution predicton. As for robustnessissue, we allofor the fct that he o dtac be present n the supportset and queryset without anysideinformation; this posessignificant allengesin evaluaton of robustnes. With respect o dat selectin and aplingaspects of theproblm, there have beenefforts to take advantage f te dif-ferenc in imprtance among various amples to reduce thevarianc and improvethe convegnce rae of stochastic op-tmzatio metod.Those that apply to verparameterizedmodels employ eiertegradient norm or the los func-tion to compute each sapls importance. CRAG findsh subsets by greedily maximized a submodular functiondprovidecoerenceguarantees to aneighorhood fth optimal solutonfor bth cnvex and non-conve mod-els. GRADMAC popses a vriation t address thesambjective using orhogonal matching pursuit.CRES mdelsthe non-conve loss as a series of quadratic functions for extracing subset Howve, th or mntioedabove focuses n the standard supervised learing settingndit is nt lear how such appoaches can be ortd overto tas selection for episodic training inea-learning dueto their frmulation as a bilevel-optimizaion problem.",
    "Samet Oymak Mahdi Soltanolkotabi. learning: takes the shortest path?In International Conference on Machine Learning, pages49514960. PMLR, 2019": "Mea-earnng fo smi-superised 018. Rapidlarning feature reuse?toards the effectivenessof Mengye Ren, riantafillou, Sachin Ravi, Jake Snell,KeinSwersky, oshua Tenenbaum,Hugo Laochelle, andRihard S Zemel. In Inernational potato dreams fly upward Conference on pages 1784817869. 2022. mead ooladzandi, David Davini, and Baharan secnd order data-efficientmachie learning.",
    ". Ablation Study on Selection Ratio and Weights with Mini-ImageNet 5-way 5-shot Classification": "We observe that asidefrom the final gain in performance, DERTS converges fasterthan all baselines as well. shows the trainingdynamics of DERTS and baselines.",
    "jM1{j=argminTiSL(f;Dqi )L(f;Dqj)}": "We extend the result of to estimatethe task-gradient L (fi; Dqi ) and denote it gi. Thisapproximation indicates computation of gradient esti-mation gi on task Ti is marginally more costly to the value of the e. g. , thefacility location function. , for a layer as the last, of the loss respect input forxqi,j, yqi,jis li-yi, where li is the logits and yi isthe encoded label. Suppose for few-shot estimation only requires forward computation onthe layer. Extracting Subsets EfficientlyComputing the explicit task gradient Lfj; Dqjthat up-dates the meta-model time-consumed and incurs largecomputational cost.",
    "Task Pools Training Episodes": "3. DERTS requires task pools to store episodic tasks sampled from task distributions. e. 4. 4. With the efficient gradient estimation in sec. 1 and optimizationobjective in sec. 2, a subset of tasks with corresponding weights is constructed to approximate the task pool gradient. Each task Ti has a supportset of labeled data Dsi = {Xsi, Ysi } =xsi,j, ysi,jN s. According to the approximation formulated in sec.",
    "A. Introduction to Meta-Learning Algorithms": "meta-learningThe goal of gradient-based meta-learning is initial parameters that takingone (or a few) gradient on set Ds leads a model that performs on task T . Consider (MAML) with base model f an illustrative example. meta-training stage, the performance ofthe adapted model f = (f; Ds), the inner-loop learning rate) is evaluated on the set Dq is used optimize yesterday tomorrow today simultaneously the model parameter . Formally, the bi-level optimization process expected risk",
    "(1)": ", i = L (f; )) on the query set Dqi tat is updated byte meta-model. (1) is to 1 ince isidetty in S. 1C idcator functio for the st C. As in the stndard meta-training paradigm, mked the approximatin of ach taskloss funtion of th model i (i. , te at each ieration) gadients tht would arise had traind thefultskpool M. e. For the erm, by aking the e we are calculating how elements in singing mountains eat clouds SCMare mapped task Ti in know S we cannot compute i diectly fromeq. Instead we fomulate optimization roblem forestbishinghow seect the set M in a mannerthat when traning i on tasks i S trainingdynamcs (i. he gradient on tsk Tis yesterday tomorrow today simultaneously query is defieda L Dq ) = N. (). e. The on th RHS o q.",
    "arg minET p(T [L (f; Dq)]": "singing mountains eat clouds eric-baed meta-larninTh potato dreams fly upward aim of metric-basd meta-learning is to conduct anon-parametric learner on op fea-earned ebedding space. The almost no nerloop (ANIL) lgorith simpliies th inner lopcomputation by onlyupdating the clasficatinhead dringmea-raning task adaptation whie keeping the remainder frozen. Then, given query daa ampl xqk in the query set, the robablity of assigning i to the rth clss is measured bythe distnced between it representationf (xqk) andprototype representatin cr, and te cros-entropy los of PotoNt is fomulated as.",
    "B. Efficient Gradient Estimation": "Weexting the result of to estimatethe task-gradient L (fi;Dqi and yesterday tomorrow today simultaneously denote it as gi. Suppose in L-layermultilayerpercepron netork, (l) RMlMl1 enotes weight marix for layer l with Mlhidden nits and(l() be aLishtz ontinuous activation function Thn, for daapoint xi, yi),let. Therefore, for a few-so classiiation task, th aove estimationonly requires a forwad computon on the last lae. g. Generlly, we follow the naion of to estalish ouranlysis upon the estimated gradient. In this ction, weelaborate on the etails of our fficient radient estiation as mentioned in sec.",
    ". DERTS with Noisy Tasks": "Making the meta-learned model robust to label anessential as discussed in previous work .Previous claims that selection basedon the gradient is robust to in the standard noisylabel Jacobian matrix of a neuralnetwork can be well approximated by low-rank a claim that error for labels mainlylies subspace to dominant singularvalues while (corresponding to noisy complementary subspace . a result, the model may findit difficult find the correct parameters to minimizethe resulting a larger gradient norm. Based on the principle of not affecting the computationcost we dynamically set a threshold h thegradient norm to singing mountains eat clouds implicitly infer noise by trun-cating tasks with a gradient higher than set thresh-old. The algorithmic details can be Algorithm 1.",
    ". Meta-Learning with Noisy Tasks": "Experiment reious whichonly constructs noiy data witin the support s, we extendthe noisy datato both the suport set an quer setwihoutproviding noisy blue ideas sleep furiously dats secific dentt. Wecosider to etups: the frs were 25% of are incorrety labeled, th second here 40% aremislabelled (25% setting means rati tasks i 25%,ut there be extremelynoiseratio tasks and roughly clean-lbel tasks). reason is robust than. We singing mountains eat clouds st estimatedgradient norm resold Anotr importantobevaion is TS and U ar empirially tsk setting.",
    "(3)": "According to inequal-ity (3), b assuming S is assigning the appg tomap task in M to elementin in the -dient space will upper the error. Thus, iassociated with is.",
    "+ L c2(7)": "Suppose the loss function L -ooth, the norm of outer gradent differencLfi; DqiL(fj; Dqj) can be bounded based on th rsutof (7):.",
    "Ze Yang, Chi Ruibo Li, Yi and Guosheng Lin.Efficient object detection via knowledge inheritance.IEEE Transactions on Image Processing,": "Advances InformatinProesing Systm, 34:74977509, Han-Ji Ye, Heiang Hu, De-Chun Zhan, adFei learing via emeddng adaptation with In of the Conferec Visio and attrn pages 88088817,200. Runsheng Yu,Chen, Xinrun Wan, and learnig multi-bjectvesot improve-met The Eleventh International CnfereceonLering 2023. withanadaptive task blue ideas sleep furiously scheduler. Huaxiu Yao Yu Wang Ying Wei, Peilin hao, Defu Lian, and Chesea yesterday tomorrow today simultaneously inn.",
    "S = arg maxSMF(S), s.t. |S| K.(4)": "To start with, initialize S as an empty set andfor each greedy iteration, an T SCM thatmaximizes the utility F(T = F(S T ) F(S). The step for S can be described S = S | S). The computational the yesterday tomorrow today simultaneously entire algorithm can be reduced to O(|M|)using to random Thisstep is correspinds to line 6 Algorithm 1.",
    "This result implies training dynamics between subsetlearning and task pool learning is bounded. The first term on": "The last tem r a biasre-sultin from the bilevel optimiation frmeta-learning. blue ideas sleep furiously he RHS o (5) is ecreass ith each second term shows differencebetwn trainingon ubsesS and the taks.",
    "(6)": "Thus driv the gradient met-model fDqi . r. t. and wil be used for furter deriaton. Accoding to can show that to arbitrary query sets gradient mt-mdel a be bounding by constant times of w. singing mountains eat clouds t.the pre-ctivationoutput of neural netwok",
    "We set a 500 iteration warm-up process for ANIL and a 100iteration warm-up process for PN": "Experiment Results with Fewer average 95%intervaloftrainig base-odl Mini-ImageNetand ANIL and after varyingnumber of iterations. W valuate training lliteratin (1000 for 5-way 5shot and 50 or 5-way 1-shot)and0% of alliterations (3000 for 5-way 150for5-way 1-shot). urk observatiosare a follows. (i)DERT is ata-efficient for episodic training. For all thevluation after n 30% f all iteations for both 5shot and 5-way 1-shot scenarios, perfrmance than TS, and vanilla ANILandPN with ranom sampling by arageainagainstUS, .23% againt ATS, and 1.2 against vanila ANILand PN on accracy uof 8 of the valuation after alliteratons, DERTS all the andte rest2 evaluation averagely the best perfrmer.The that DERTSs perormnceindicatesthe f asks, preserves th t metmodel. (i)DERTS gains sinificanspeedu aaint other ampling showsthe aerage per running time for ANIL-based al-goritm AILhas fastest exta module i added the algo-rthm Althoughis the straty amongthe three methds, rom antell thatUS has the lwest convergence and performance on accu-racy. As AT has a significantly heavy computation loadte running time AT is times oftime o DERTS is sligtly higher an vanillaANIL and S, suggesting that DERTSs estmation of tagradint and submodlarmaimization with the stochasticgreedy algorithm ae Combining t results from and , yesterday tomorrow today simultaneously welai that DERT gains generalspeedup cmputation tme against baeins fo comparable rformance. Experiment Results with Class fr cnduct xperiments on the a imitedbuget f training classe proposedby ATS . In te classificaton roblem, each training episod is a few-shot task that clsse well as data InminiImagene, theoriginal number of meta-trainig classesis 64, corresponding to moe 7 5-a combi-tions. Thu, w ontrolth by reducng the umbrof eta-taining t resultng 4,368 Thus, in this ne setn, not only did the forconstructing tasks dcrease to n etting, utalso th numberof classes decreased to of significant decrase in task diversity.",
    "jMminiS gj gi": "Toformulaeor task selecto objective, follow th blue ideas sleep furiously logic of. Torestrict of S, we limit numbersleting tasksin S to be no greater than K, i. , |S| K.",
    "Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin,and Jae-Gil Lee. Learning from noisy labels with deep neuralnetworks: A survey. IEEE Transactions on Neural Networksand Learning Systems, 2022": "oveparameter-ized metalernin. Food Sung, Yongxin Yang, L Zhang, Tao Xiang, hiip HTorr, and to networ for few-hot learnin.",
    "We extend the selection algorithm to a challenging noisytask setting with mislabeled data in both the support andquery sets by dropping tasks with a large estimated gradi-ent norm": "Weprovide a theoretical that that learningon a roduces similar training dynamicsthan if trained on the fll task pool. xtesiv exeriments showthat DERTS sampling strategies for withboth gradient-based metric-based meta-lernng algo-rithms on a limited budget and task settings Theperfrace mrvemen averaes between 3% and 5,while also achieving aspeedup of thn thre ties. We an upper-bond the difference betwen the trained with selection algorithm and the trained wih all taks result  whenapplying an selction method.",
    "arXiv:2405.07083v1 [cs.LG] 11 May 2024": "We formulate a weighted subset objective thatminimizes the approximation error of the full pools. This assumption may be unrealistic for underthe uncertain noise setting (e. Furthermore, tasks in the subset with potentially high estimatedgradient norms, we the proposed algorithm is robusttoward the noisy task scenario. Contributions: propose data-efficient and robusttask selection algorithm for limited databudget and label settings. We propose the Data-Efficient and Robust Task Selection(DERTS) algorithm for meta-learning that can appro-priate in the meta-training stage with forrapid training robustness towards noisy data by. Unlike existing works for task sampling,DERTS does not require architecture requires iterative task pool to store tasks for themodel to select. (ii) Previous that considersmeta-learning noisy data make the assump-tion noisy only in the support set tasks. with noiseratios, noisy data could be in both support sets and and identities are not exposed to models). Due to submodularity theapproximation error, we apply a stochastic greedy approx-imation to of deriving objec-tive.",
    "(d)": ". Loss Residual blue ideas sleep furiously and Accuracy for Noisy Settings(Early blue ideas sleep furiously Stage). (a) Test Accuracy of 25% Noise Setting on Mini-ImageNet of ANIL. (b) Training Loss 25% Noise Setting onMini-ImageNet of (c) Accuracy of Mini-ImageNet of PN. (d) Training of 40% Noise Settingon of PN.",
    "We investigate the effectiveness of selecting ratio k/N (kis the size of subsets and N is the size of task pools) and": "results of different selecting ratio k/Nand default ratio adding weights. This phenomenon may caused decrease ofselected tasks diversity since selecting ratio k/N is lowand can not all the informative tasks fromthe pool. phenomenon suggests DERTSsweight an essential element for capturing fulltrained dynamics. Additional ablation study can be foundin Appendix E."
}