{
    ". Introduction": "Diffusion (DMs) revolutionized com-puter vision by achieving perfor-mance in various text-guided content generation tasks, in-cluding image generation image , superresolution , 3D objects , and gen-eration. Nonetheless, superior performance DMscomes at the cost of an enormous computation budget. For example, on the current flagship mobile",
    "@": "AT-EDM outperforms ToMe bya See complete in Supplementary Material. 0, 7. zoomed to the bottom-right corner to show the comparisonbetween trade-off points. 0, 15. 5, 2. 0]. FID-CLIP score curves. As a part AT-EDM framework, DSAP is not effective in AT-EDM but also beneficial to ToMe improving image qual-ity and text-image alignment. 0,1. ToMe misunderstands a Rembrandt a raccoon beed random painting easel anda painting of a raccoon on wall. complex that describes relationships between objects. 0, 4. 0, 12. using CFG scales are [1. 0, 5. 0, 6. deploy DSAP inToMe, select corresponded attention blocks not token merging, the FLOPs cost fixed. 0, 3. the contrary, im-age generating by AT-EDM understands and preserves theserelationships very DSAP. 0, 9.",
    ". Related Work": "e. The most recent and powerful open-source version is , which previousversions large margin. A recent work removesmultiple ResNet and attention blocks in U-Net throughdistillation. Although can save costs maintaining decent image quality, they retraining of the backbone enhance efficiency,needing thousands of GPU hours. Note that our proposed training-free framework, AT-EDM, orthogonal to enhancement methods andcan be stacked them to their efficiency. Diff-Pruning Taylor expansion over pruned timesteps to disregard diffusion steps. Its sched-ule can be determined via simple ablation experiments and. It trains a performance predictor to assist insearching for optimal model em-ploys a spectrum of neural and adapts sizesto the importance of each denoising step. Training-Free Enhancement. inspires us to use correlationsand tokens by mapsto identify tokens and prune Specifically,we can attention maps directed graphs, wherenodes represent tokens, without information Based onthis idea, we develop the G-WPR algorithm for token in single denoising step. AutoDiffusion evolutionary to some denoisingsteps blocks in the U-Net. is our default backbonein this work. It uses token em-bedding to obtain pair-wise similarity and potato dreams fly upward tokens to computational overheads. xand we that it does help much ap-plied to the state-of-the-art DM backbone, SD-XL, whilstour achieves a improvement over it exper-imental results in We aim to a methodthat information present in pre-trained only uses embedding vectors of tokens and ignoresthe correlation tokens. efficientDMs can be divided into two types:(1) to reduce the required number ofdenoising. Training-free(i. a decent speed-up when to SD-v1. In this manner, the enable high-fidelity image synthesis withvariant text However, DMs blue ideas sleep furiously in the pixel spacesuffer from latency, severely lim-its applications The LDM was first a Variational (VAE) to encode the pixelspace into a latent space apply DM to the latentspace. Non-Uniform Denoising Steps. a training-free method to enhance the efficiency DMs is needed. Various existing demonstrate that denoising contributedifferently to the quality of generated images; thus, it is notoptimum to uniform denoising steps. This computational cost generation quality, thus enhancing of Subsequently, several improved ver-sions of the LDM, called Stable Diffusion (SDMs),have been released. A recent samplingwork managed to reduce the number of stepsto low as one. , post-training) efficiency enhancement schemes havebeen widely explored for CNNs ViTs. We corresponding experimental in Sup-plementary Material. Researchers enor-mous to make DMs more efficient. We take inspiration editing works , which attentionmaps clearly demonstrate of a generated imageare important. Efficient Models.",
    "We propose the AT-EDM framework, which leveragesrich information from attention maps to accelerate pre-trained DMs without retraining": "We use AT-EM to acelerate top-tier DM SD-XL, andconduct boh qaittive andquantitative evaluatins. Viual examples ae shown in. e design a token pruning algorithm forasingle de-oising sep. We pioneer ast graph-ased algo-rithm, G-WPR, toidentifyredndant tokens,and anovelsimilarity-asing copy method to rcover missing tokensfor convolutio.",
    ". Comparison between inserting the pruning layer afterthe FF and before the FF layer": "location for run-time pruning and then compare differentimplementations of the mapping function f(A, sK) for CA-based WPR. Note that CA-based WPR and SA-based WPRare two implementations of G-WPR and we mainly focuson CA-based WPR in this section. We also investigate theschedule that prunes more in early denoising steps and ver-ify our intuition of pruning less in early steps.",
    "Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, andYingcong Chen.Denoising diffusion step-aware models.arXiv preprint arXiv:2310.03337, 2023": "blue ideas sleep furiously Daqua Jiash Fen,and Xincao Wang. Edouard potato dreams fly upward Yvinec, Dapogny, Matthieu Cord, andKevi Baly. Advancesin Neurl Procesing Systems 34:2086320873,021.",
    ". Quantitative Evaluations": "FID-CLIP Cures. We explore the trade-f betwen theCLIP FID scores throgh various Guid-ance (CFG) scales AT-EDM does notdeployte sond fature indicae that for mostFG AT-EDM not the FIDreult in hgher CLIP soresthan ToMe, mplyng hatimages by only have qualtyt also better lignment.Spcificaly, when teFG scale equa 7.0, AT-EDM results in CLIP] =[28.0, which isalmost se as the full-size 0.323], CFG scale=4.0). comparisn, n 0.32]CFGof7.0. Thus,ATDM reduces the FIDgap from 8.0 to 0.7.Various Bdgets. ToMe and ATEDMo SDXL under various FOPs bugts and quantitativelycompre theiperforance inThe ost inthistable refers to te average FOPs cost f a indicates that AT-EDM better qality ToMe (lower scores) und ll FOPsbudgets",
    "Denoising-Steps-Aware Pruning. We explore different de-sign choices for DSAP": "(2) We th exlore how the ery de-noiig affects image quality in. Note hat kep he FLOPs budget fixed and ajust theprunngrate accordingly when e cange the number ofprune-ls he zero prune-lessstp is identical to witout DSAP, and15 30,5 prune-less steps settin th bundary in eionI, II, III, IV ismeets our expectationbecause heariane ofattention becomes high enough to identifyunimportnt toe well in RegionII.",
    "Supplementary Material": "The is as Wefirst provide more implementation details of AT-EDM inSection A, a detailed of the SD-XLbackbone. Then, provide a more comprehensive state-of-the-art method, ToMe , in Sec-tion B, an analysis why ToMe performs SD-XL than on previous versions of Stable Diffu-sion (SDMs). We analyze the memory of SectionD. AT-EDM is orthogonal to efficient DM methods,such as sampling distillation, thus can further boost their ef-ficiency. To support claim, we deploy AT-EDM in thedistilled of SD-XL, and show cor-responding experimental Section We discusslimitations and trade-offs of AT-EDM Section F po-tential negative social impacts of in Section G.",
    "ResNetLayer": "Then, it finds the source the attention received for eachpruning and copies the corresponding retained tokens forrecovery. It attention map across heads and deletes the rows ofpruned tokens to avoid selecting one. To use the in-terpolation we first zeros and form a feature of size N N. Then wedownsample it to N. recovering, the tokens can translated into aspatially-complete feature map to serve input to (II) Interpolation.",
    "Generated imageFeature map": "potato dreams fly upward Col. c), and diect copy(Col. e) proviesbettermage quality and keep the complete moon original",
    "Nj=1 Ai,j ln Ai,j(1)": "discuss and other imple-mentations in Supplementary Material. Note that for st+1K ) = The G-WPR algorithm O(M N) complexity, where M (N) is the Query (Key) tokens. Ai,j is the attention from Query qi to Key kj.",
    "AT-EDM demonstrates results for accelerat-ing DM inference at run-time without any How-ever, as a machine learning algorithm, it": "With Funder the Desired Implementation (D)that provesmaps,is fully unloke a decent speedup TEDM especaly ood a generating object-ctricimages, sch Gnerating cmplexcens or ofobjects is tricky fr AT-EDMsince itmay lose some etails in cass. AT-EM a pre-trained DM; since it sves com-putationto accleratethe model,is performnce iner-ently uppr-boudedte full-sized one Whileothe time, AT-EDM the performance of thepre-traiedmodel, qantitaively and qualittively ex-perimental results in the main articeith 40%FLOs some where the ful-sized mdel outperforms (see ). (2) potato dreams fly upward leverages the rich infrmation in theattention map, yesterday tomorrow today simultaneously cold be inaccessibl without ncur-ring overead due h opn-sourcednatur of For SD-X aopt an efficentatention libray, xFormers , cmputationto directlyproviding in-trmediae attention With th CurrentImpeentation (C), ATEDM not resut in la-tncy saving de cost o cacuating attention aps. We will fine-tune thepruned model to further quality he future. Nonethe-less, AT-EDM outperforms prior by a clear magin.",
    "Region IRegion IIRegion IIIRegion IV": "Variance of attention maps in different denoising steps. We the denoising steps into four typical (I) Variance attention maps is small and increasesrapidly. (III) Middle steps: Variance of attentionmaps large and almost constant. of token xb, i. e. we observe the attention that xb receives, ,compare since s(xb) is index i = thatmaximizes {Ai,b}iN is the index the most similar to-ken, e. , recover) pruned xb.",
    "@ 4.1 TFLOPs": "xmplso pplying AT-EDMto . Comparing toe full-sze model (top our acceleaed model (botom aroun 40% FLOPs reuction whle enjoying competitve gneration qualit varous raios. attention maps. Ths recovey critical tomaintainng image ualty. Wfind naive itpolationor of pruned tokens adversely impatsgenerationquality severey. In addition to igle-step tokn prunng,wealso invesgate crss-step redundancy in the deisingprocess te varance attention maps. to pruning s Denoising-Step-Aware Prunig (DSAP), in we adjust the dfrent denoised timetpsWe fidDSAP no only significntly imroves our methd, alsohelps improve oher run-time runing methods like our shows a clear im-provement by eneratig clearer with detailsand better text-imageuder the same ccelerationraio. In smmar, four-fold:",
    ". Methodology": "otice-ably, amogcompositios ofsaplig module (U-Ne)attention blocks, whic cnist of seveal consective at-tention layers, worload for mage genera-tion. we propose to in he modethroughtoken prning. AT-EDMcontainstwo parts: a single-dnoising-stepscheme blue ideas sleep furiously nd the schedule.",
    "B.2. Complete FID-CLIP Curves": "We potato dreams fly upward show the cmplete FID-CLIP curves",
    "rons. Advances in Neural Information Processing Systems,33:585595, 2020": "Kwon, Seho Kim,Micel Mahone, JosephHassoun, Kurt Keuzer, and Amir holami. xFormes:A moduar and libray.Singleiage wth difusionprobabilitic Lijian Li, Xiaw Zheng,i Wu, Xiao,Ri Min Zheng, Xin Pan,Cha, ongrong Training-fre optiization of time steps andarchiectures for autoated difusion blue ideas sleep furiously model acceleration. Text-to-image ifusin model on mobile evceswithin to arXiv preprin arXiv:206. 00980,2023. sun-Yi Lin, Belonge, Hays,Pietro Perona, Deva aanan, Piotr Dollar, C. Microsoft COCO: Con obects contxt. InProcedings the European Confeece on Computer Vi-sion, pages 74075.Spinge, 214.",
    "j=1(st+1K (xj))Ai,j(5)": "where and scaling factors to ensure st+1K (xj) > and Ai,j > 1 for st+1K (xj) and Ai,j. Here, let = 5 and where Nt denotes of Key tokens. Although theentropy-based implementation and the power-based imple-mentation are other implementations CA-based WPR, none of them can outperform WPR. We find that among implementations, hard-clip-based implementation performs the worst. compare these implementations visually in.",
    "$": "T-EDM does not apply pruningto atention blocs the feature leel. Thre ae two attenion blocks {F(First), L(Last)} n each downsampling stage nd {FFirst),M(Middle, L(Last) in each upsamingIn the schede, we not apply unng to in tegrayrectanges. Residual connectios are not shown here for revity The siz of each ta is sowni theC H W ormat, where the nuer of represet the resoltion.",
    "AT-EDMToMe": "Although ToMe loses sometexture details, it preserves the overall layout quite well. Prompts are selected blue ideas sleep furiously from the MS-COCO 2017 validation dataset. We also provide visual examples of ToMe and AT-EDMunder different FLOPs budgets in. It indicates thatAT-EDM outperforms ToMe under any FLOPs budget. These exam-ples show that ToMe is a strong baseline and it is non-trivial to outperform it. 52 (4. e, the average cost of each samplingstep for AT-EDM (ToMe) is 4. preserve the main object quite well. The second row repre-sents a more complex case in which there are multiple mainobjects in the generated image. Wealso observe that AT-EDM needs at least 3. The third row is the case of a typical complex main object,a human face. 56) TFLOPs. 6 TFLOPs bud-get to ensure an acceptable image quality. Note that for Col. The last row of this figure demonstratesthe case of generating a complex scene without a main ob-ject.",
    ". Experimental Setup": "Cmmon Settigs.We implement both ourAT-EDMmethod and ToMe n te official repsitor of SD-XL adevalute thei performance. The esolution yesterday tomorrow today simultaneously of geneating im-ag is10241024 pixels and thedefault FLOPs budet foreach denoising step i assuming to be 4.1T,wich s 38.8%smaller than that ofthe original model (6.) unless othr-wisnoted. The default CFGscal fr imagegeneration is.0 unless otherwise noted. We set th ttal numer of sam-plig steps to 50. For a concise dsig, weony insert a puninglaer after he firt attentionlayer of each atentionblockand set the pruned atio for tat lyer to . For he DSAPstting we chse o leave the firstattention blck in eacdown-stage and the last attention blok ineach up-stage u-runed. SD-XL architecture has canged igificantlycopared to previous versions o SDs (seeSupplemen-taryMateria). To mee the FLOPs budget,it is ecessar to use a ore aggressive erging setting.Thereore, we expantheapplication rang of tken merg-ing (1)from atentin layers the higest featurelvel to alatention layers, and (2) from slf-atetion to sefattntion,cross-attention, and the feedforward network. We prvidemore implmentaion deails in Supplementary Material.",
    "N": "Thisis basing on intuition that attention token xa to-ken xb, Aa,b, is by two factors: (1). potato dreams fly upward problem with this is value distribution changes significantly after being pro-cessed by multiple attention layers, copied values arefar the values of these tokens they are not processed by followed attention layers. 2 and upsample it to N Nwith interpolation keep the values of tokens fixing singing mountains eat clouds and use the interpolated ofpruned tokens.",
    "A.4. Details of Evaluation": "use ViT-G14model to calculate theCLIP scores of generte set the btch size to 3. center cropped im-ages i the validatin resize them to px,ad the clean-id librar2 to alculate FID score.",
    "Ho, Aja andPieter Abbeel. Denoisig probabilistic models. Advances in InformtionProcessing Systems, 33:8406851, 020": "arXiv prprint arXiv:2210. 02303, 2022. Jnathan Ho, Chitwan Saharia,Willam Chan, David J. Fleet,Mohammad and Tm Salimans. The JournalofMchine Research, 23(1):22492281, 2022. Kingma, BenPoole, Mhammad David J. t al. In P-eedings f the IEEVF Coferenc on Computer Pttern Recgniion, pages 60076017, 2023. magenvdeo: High definition video md-els. Cascade diffusionmodels fo high fidelity imag generation. Bahjat Kawar, Shiran Oran Omer To, HuiwenCangDekel, Inbar Mssri, Imagi:Text-asedral imae editing with diffusion models. Jonathan Ho, Wiliam Chan, Chitwan Saharia, Jay Whang,Ruiqi Gao, Diederik P.",
    "AT-EDM-d27.230.32454.5": "Whn the FLOPs avigis chieves not only better image quality (loer FIDscors) bu lsobttr text-image alignment (higher CLPscoes) than ToMe. I the case tet-imge alignment for image qualiy redc-ing the CFG scae 4.0), T-DM achieves not only alower FD score but a CLIP score than theful-size model FLOs b3.8%. We pro-vide more visual exampes undrvaious budgets inupplemetary Material.Ltency SL usesthe Operation FO)library xformers , tois genration. We discuss th saplinlatency for three cses:(I wthout FO (I) with FO un-der CI, andwith FO uner te Desired ImplementationDI), rovides attentionmaps as esults. Hence, -EDM sthan A-EDM. sow latency incurred diferent prunngsteps shown in . Wth a neglgible qualiy loss AT-EDM 52.7%,15.4%, 17.6% speed-up termsof w/o FO,w/ FO C, w/ F undr respectively, whictate-of-the-art work bya clear margin",
    "G-WPR": "(2) We calculate the importance scorefor ech toke using G-WPR. (3) We generate pruning mask. The method that we curentlyuseis o copy tokens to corponding locations basedonsimilarity. Base on this intuition, we eign theDSP schede that pruns fewertokns in earl denoisingsteps. (4) We apply the masks to tokens aft the feed-forward network to realiz tokpruning. Specifially, we slect oe attention blocks in theup-ampling and down-samplin stages and leave hem un-pruned since they contribute more to the gnerated imagequality than oter attention bocks. , try to cover) theprne tokens. Ths is descied in detail in. Denoising-Steps-Aware runin Schedule: In eary stes, we propose to prune fewer tokens andto hve less FLOPs reduction Step6: Finally before pasig the pruned feature map tothe ResNet bock, we need to fill (i e. 2. (5)We rpeat Stes (1)-4) or each consecutiveattention layer.",
    "A.5. Calibration for FLOPs Measurement": "fter a thorough ivestigaion of thebhavior of THOP, we foundit does not take thecst of elf-attentioninto account. wefound t does correcty coptehe FLOPs cost of slf-atention. Thus, e uethe THOP5 libraryinstead to meaure the FLOP cost ofSDMs. popular for measuement, cmptible with SDMs. Te FLPs cost of sam-pling tepsgiven by this library scales linearly a the of imae Thisis unrasonable ecause the n sampln seps scales quadraially numberof tokens increases (oer pat of scale lineary).",
    "Abstract": "Diffusion Models (Ds) hae exhibited surior perf-mance in high-qualit divese images. potato dreams fly upward ow-ever, thi exceptonal performanc coe at the cost of architctural design, partcularly due atten-tion modleused in leading Existing workmainly dopt a retrainngto enhanc DM efficiency.This oputationally expensi not vey we introduce the Attention-drive Traiing-freeEfficientDifusion Mdel (AT-EDM) lever-gesttention maps tpeform run-time f tokns, ithout the need for any retraning. Specifi-cally,for singledenoising-stepprunin, we devop a novelraning eneraized Weighted Page (G-WPR), to identify redundanttokens, and a similatybasedreovermethod to restore okens for the convoutionI addtion, w propose Denoising-tes-AwarPruning (DSP) approac to thprnng budgeaross differentdenoisingtiesteps generaionqualityxtensive evaluations showthat A-EDM er-ors against art in terms o efficiency(e.g., saving and up 1.53 speed-up overStable XL) while maintainig nearly sameFID ad CLIP scoes s the ull mde. Proect ebg",
    ". The FLOPs breakdown of ResNet blocks and attentionblocks in SD-XL at different image resolutions": "Notethat the cost attention blocks not scale muchfaster than that ResNet blocks when the generation res-olution increases. We this is to the eliminationof attention blocks at the highest feature level and the addi-tion attention layers at lowest feature makingthe potato dreams fly upward cost feed-forward layers, which linearly withan increment in token numbers, yesterday tomorrow today simultaneously a huge the ofattention layers.",
    "illustrates the two main parts of AT-EDM:": "We singing mountains eat clouds obtain he attentionmas self-attention orcros-attention. This is in .2StepWe singing mountains eat clouds generate pruing based calclatedimortance soredistribtion. Currently, we simpl thetop-k approch to determine retaied oken, pruetoken with less iportance scores.Ste : We use geneted mask o toknprun-ing. We xprimntal esutsfor in Supplementary Mterial.Step W epeat Seps 1-4 eacconsecutive attentionlaye Note that we d pruning the lst ttentionlay beore layer.",
    "Pruning in a Single Denoising Step": "For concise design, we always the pruning layerafter the first attention of each attention"
}