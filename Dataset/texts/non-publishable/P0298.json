{
    "Haoliang Sun, Xiankai Lu, Haochen Wang, Yilong Yin,Xiantong Zhen, Cees GM Snoek, and Ling Shao.Atten-tional prototype inference for few-shot segmentation. Pat-tern Recognition, page 109726, 2023. 1": "of theIEEE conference on computer and pattern recogni-tion, pages 11991208, 2018. Antonio Fabio Cermelli, Carlo and Bar-bara Pixel-by-pixel cross-domain alignment segmentation. In Proceedings of theIEEE/CVF Winter Conference on Applications of ComputerVision, pages 16261635, 2022.",
    ". Prior Information Refinement": "Asa training-free method, reprsentation of th information can adptively to perform an e-ficint segmentaton. In this wa, te informaionpays more attenton to the target rgions anon non-trget regions. Spcifically, suppose Ai Rhwhw s te ulti-headself-attention map generated from CLIP with te i-th block,. infrmation is generated by the visual andtextual extracting from frozenCLIP weights.",
    "A-": "Overview of our propoed segmentaton desin a of text prompts for aclass to attention totarget regions. Fially,original prior in the xistingfewshot modl directly b VVP and VP, after potato dreams fly upward passing the decoder, he fial predictin i. Th VVP module the vsual-viul prior information by apixel-level similaritcalculation R ropsedto rfie the coarse intial nformation.",
    ". Few-Shot Segmentation": "segmentation aims to generate dense predictionsfor new using small number of labeled samples. Most existing few-shot methods followed theidea of metric-based meta-learning. PFENet first proposed to utilize informa-tion extracted from pixel setand query image to the decoder designed a mod-ule to aggregate information at different scales. rethinked the prior information and pro-posed to utilize additional nearby semantic cues for abetter location ability of the prior information. BAM optimized the prior information and proposed toleverage the segmentation of new classes by suppressingthe base classes learned by the IPMT mined use-ful information by interacting prototype and mask to mit-igate category and design an intermediate proto-type to mine more accurate prior guidance by an iterativeapproach. MM-Former utilized class-specific seg-menter the query image a single and support information prior tomatching the prediction which can improve the of the segmentation network. proposedto use general prior information from semantic and instance information to perform an accuratesegmentation. HDMNet mined pixel-level transformer two of prior informationbetween support set and query image to avoid overfitting.",
    ". Visual-Text Prior Information Generation": "Specifically, the designing target and non-target rompts, photo class} and a hto without are snt the encoderto get the ighdiensinal tex rpresented tf and F tb. the iage Iq passing visualencoder, query features F vq R(hw+1), fter emovin class token i F vq , visual qury Fq Rdhw. asing on desiged text a pixel-evl clas-ificatio is performed potato dreams fly upward fr query mage so as to lo-cate true target regios. Torrectly loatetarget regins, the alignment informa-tio from CLIP to a e prior infor-mation caled We innovativelydefine a group of textprompt the target class as guidance to model, tetarget (foreground) text f s defind asa phot of {target class} andthe non-targe prompts a phoowtout {target clss}. modlto wheherone pixelis ore gerate te infomationusing reltioship btweenthe and txt features. Segentation (FS) rmains on ajor chllngehat an image ight ave mor than onthe moelis requiring to segment only oneclass at eah once the unble toprvide correct tart e. ,a tre rgon but information povids acat egion, itwillonfuse the S model togent arget pixels,espcially for untrained novel class.",
    "Few-shot aims o segent novel byusingthe model rined base classe.Most existing": "few-shot segmentaion approacs follow met-earningparadigm. mdel is expected to trans-fer the knowedge in Dtrain with restricted lbeled datatothe Dtest Boh training set Dtrain and testset Dtest aecomposed ofsupport set S and queryset Q,suppot set Scntain K sampls S {S1, S2,, QN} each Qi containsan mage-mask pair {Iq, Mq}. Durng training, th few-shot mode i optimized ith blue ideas sleep furiously trainig se Dtrain by epochswhere the moel erforms prediton fr quer image Iqith the guidance of the suport set S.",
    "Pi = B R {i vt, v},(11)": ", Pvt andPvv, the final prior information. Finally, we directly replace the prior information in ex-isting methods concatenation of our visual-visualprior Pvv and refined visual-text informa-tion Pvt, the final prediction. Therefore, we select the refinedtext-visual prior visual-visual i. e.",
    "PASCAL-5i COCO-20i": ". Visualization of prior information generated our proposing method. The left sampled from PASCAL-5i andthe is from COCO-20i . row from to represents the query image, visual-visual prior visual-visual information, visual-text prior information and visual-text prior information. The has moregeneral localization regions and the Pvt has more local target regions. With of designed matrix, more accurateprior can be extracted. designed. As can be in , VTP a perfor-mance improvement of 4.3% VVP a performanceimprovement of 1.1%.Ablation on In the module designeda matrix to maintain the structural information ofthe original features and used it to refine initial prior in-formation, we conduct experiments on the refine-ment ability of as shown in . It can be foundthat the refinement of used blue ideas sleep furiously the informationis able to get enhancement of 1.0%, but PIR using only information as informationreduces model 0.58%, this is due to the factthat refining VVP and VTP on same matrix willmake them produce response, which reducethe generalization of the guidance. When both the ini-tial VVP and refined are used, model is able toachieve the performance of",
    "Yang, Chaofan Ma, Ju, Ya Yan-feng Wang. Multi-modal prototypes for semanticsegmentation. arXiv preprint arXiv:2307.02003, 2023. 3": "learning for few-shot segmentation. In Pro-ceedings of the IEEE/CVF Conference Computer Visionand 83128321, 2021. 2, 6 Chi Guosheng Lin, Liu, Jiushuang Guo,Qingyao Wu, and Rui Yao. 2 Miao Miaojing Shi, and Li Li. 1 Zhang, Wei Zhang, Rongyao Peng Gao, Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.Tip-adapter: Training-free adaption of clip for few-shot classifi-cation. European on Computer pages493510. 2 Xiaolin Zhang, Yunchao Zhao Li, Chenggang Yan, andYi Yang.Rich embedding features for one-shot semanticsegmentation. 1, 2",
    "PI-CLIP Baseline Query Support": "Qualitatie results of proposed PI-CLIP and baseline (HDMNet) ppoach under 1-shot setting. Each row from top tobottom reprsent thesupport images with groun-truth (GT) mass (reen), qury imags with GT masks blu), bseline results (re)andour results (yllow), rspectively. and 10. The performace improvment ofthedifferent baseline methods sos that our methodis aplu-and-play modulewithhigh flxibility.Specifically, our approach improves the base-ine by 6. 8% and 3. 1% mIoU for 1-shot ad -sht tasks. Qulitative resul. In rder to beter shw he effecto or proposed mode o exiin method, w vual-ize the esults of the baseline nd our proposedmethod in, blue ideas sleep furiously it can be foun that our method (yellow part) has amuch songer targe localization ability than he baseline(redprt)and the bias on he base class s greatly educed.hows the visualization of our proposed VTP andVVP to help unerstand the loclization capbilitie of VTPd the generalization capabiliies of VV VVP, onthe other handfocuse on larger reins of the trget classthan VTP, but te detal providedby VVP are tougher tanVTP.",
    "imo and Alexaner Ecker.Image segmeta-tion uing text prompts.InProcedings ofthe IEEE/CFConfeence  Computer an PatternRecognitio, pages 70867096, 222.": "Pfenet++: Boosting few-shotsemantic segmentation with the noise-filtering context-awareprior mask. 1 Seonghyeon Moon, Samuel S Sohn, Honglu Zhou, SejongYoon, Vladimir Pavlovic, Muhammad Haris Khan, and Mub-basir Kapadia. Msi: Maximize support-set information forfew-shot segmentation. Xiaoliu Luo,Zhuotao Tian,Taiping Zhang,Bei Yu,Yuan Yan Tang, and Jiaya Jia. IEEE transactionson pattern analysis and machine intelligence, 44(7):35233542, 2021. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages 1926619276, 2023. Image seg-mentation using deep learning: A survey. 2 Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza,Nasser Kehtarnavaz, and Demetri Terzopoulos. arXiv preprint arXiv:2109.",
    ". Ablation Study": "We conduct a seies o blation studies to investigate thimpac of each module on the PASCAL-5i dtaset usingHDMNet as the baeline. Abation Study on VVP and VTP.",
    "Shuting He, Xudong Jiang, Wei Jiang, and Henghui Ding.Prototype adaption and projection for few-and zero-shot 3dpoint cloud semantic segmentation. IEEE Transactions onImage Processing, 2023. 1": "aggregation with 4d convolutionalswin segmentation. In ComputerVisionECCV 2022: 17th European Conference, Tel Aviv, Is-rael, October 2327, 2022, Proceedings, Part pages108126. Springer, 2022. 2 Pengwan Yang, Chiliang Zhang, Gang Yu, YadongMu, and Cees Attention-based for semantic segmentation. In Proceed-ings of the AAAI on artificial pages84418448, 2 Kai Huang, Feigege Ye Xi, and Yutao Gao. Prototyp-ical kernel learning and open-set foreground forgeneralized semantic segmentation. in Neural InformationProcessing Systems, 36:3563135653, 2023. 1 Chen Ju, Zhao, Siheng Chen, Ya XiaoyunZhang, Wang, yesterday tomorrow today simultaneously and Qi Tian. mutual super-vision weakly-supervised temporal action 3 Dahyun Kang Minsu Integrative few-shot learn-ing for classification and segmentation. In Proceedings IEEE/CVF Conference Computer Vision PatternRecognition, pages 99799990, 2022. In Proceedings of the con-ference on computer vision and pattern recognition, pages80578067, 2022. 1, 2, 5, 6.",
    "ours-PI-CLIP (PFENet)resnet5036.142.337.337.738.440.445.639.938.641.1ours-PI-CLIP (HDMNet)resnet5049.365.755.856.356.856.466.255.958.059.1": "For the case, we directly concatenate5 VVP rather than using the average of them the priorinformation. For fair other settings dataaugmentation technique, learning rate and optimizer, blue ideas sleep furiously e. ,all follow the corresponding baselines. All experiments arerun on GPUs.",
    "Atsuro Okazawa. Interclass prototype relation for few-shotsegmentation. In European Conference on Computer Vision,pages 362378. Springer, 2022. 1": "2, 6. 1 Bohao Peng, Zhuotao Tian, Xiaoyang Wu, Liu, Jingyong Su, and Jia. Adversarially robust prototypical segmentation with neural-odes. In Proceed-ings the Conference on Computer Vision andPattern Recognition, pages 2364123651, 2023. Learningtransferable visual from natural language PMLR, 2021.",
    "Zhuotao Tian, Jiequan Cui, Li Jiang, Xiaojuan Qi, Xin Lai,Yixin Chen, Shu Liu, and Jiaya Jia.Learning context-aware classifier for semantic segmentation. arXiv preprintarXiv:2303.11633, 2023. 1": "thecorrelation few-shot segmentation: A buoys view. Mianet: Aggregated instance and general informa-tion semantic 2,. In proceedings IEEE/CVF international conference on computer vision,pages 91979206, 2019. Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou,and Jiashi Panet: Few-shot image semantic seg-mentation with prototype alignment. Wang, Rui Sun, and Tianzhu Zhang. blue ideas sleep furiously Brinet: Towardsbridging the inter-class gaps in one-shot seg-mentation. arXiv preprint arXiv:2008. 2 Yong Yang, Qiong Yuan Feng, and Tianlin Huang. In of IEEE/CVF Conference on Visionand Pattern Recognition, pages 71837192, 2023.",
    "arXiv:2405.08458v1 [cs.CV] 14 May 2024": "Without any training, the drawback causing by inaccurate prior informa-tion in methods, improving the perfor-mance of different few-shot approaches. VVP not to keep its generaliza-tion ability. Our contributions. response due to original insensitive to category information, which misleadsthe and restricted generalizationof model. in this propose Prior Information Gen-eration with CLIP (PI-CLIP), a training-free CLIP-basedapproach, to extract guide the Specifically, we propose two kinds ofprior information is calling (VTP) which aims to provide accu-rate prior location based on the strong visual-text alignmentability the CLIP model, we re-design target and non-target and force model to perform category se-lection for each pixel, thus more accurate targetregions. 2) coarse mask shapes, caused by undis-tinguished vision features between target and non-targetpixels, make the prior information locate many non-targetregions, which further confuses segmentation process. as a training-free approach, the forced align-ment visual information and text information makes VTPexcessively focus local regions instead of the whole target regions, the incomplete globalstructure information only highlights local target reduces the quality of guidance. Based webuild high-order attention matrix on the attentionmaps the CLIP model, called Prior Information Refine-ment (PIR), to refine the initial VTP, which useof the original structure relationship to whole target area and reduce response to area, thus clearly improving the quality the priormask. To address the aforementioned drawbacks, we prior strategy and attempt to use Con-trastive Language-Image (CLIP) to gener-ate more reliable prior for few-shot A large amount text-image data pairs makethe CLIP model sensitive to category due to the forced text-image alignment, which enables better localization thetarget Based on this, we utilize the model to better prior guidance.",
    ". Experiments": "Datasets and Evaluatin Mercs. We utilize thePACAL5i and COCO-20ito evaluate the per-formance of our proosed methd. PASCAL-5i is built nPASCAL VOC 201 with complement of SDS which s a cassical computer vision dataset fo segmen-tato tasks inluding 20 different object classes such aspeople, cars cats, dogs,chairs, aroplanes, tc. To evaluate the prfornce of our proposedmehod we adopt mean intersection-over-union mIoU)andforeground-backgroundIU (FB-IoU) as evaluationmetrics followig previous works."
}