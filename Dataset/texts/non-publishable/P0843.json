{
    "HomophilousHeterophilicDatasetDiseaseAirportPubMedCoraChameleonSqirrelHyperbolicity = 0 = 1 = 3.5 = 11 = 2 = 1.5TaskLPNCLPNCLPNCLPNCNCNC": "HyboNet 96.8 0.496.0 1.097.3 0.390.0 1.495.8 0.78. 1.09360.380.2 .834.3 .5Parllel Transport 8. 0.884.8 3.79.60.193.4 0.66.5 0.594.8 0.836.6 1.32.3 0.8Tngnt Space 1.993.5 .192.02.996.4 0.276.8 0.994. 0.138.3 0.834.0 0.6Space Adtion 83.1 1.28.9 1.5.5 0.275.9 0.993.2 0.278.6 0.539.4 .34.5 0.2LRsNet (ours)97.3 0.496.1 1.07. 0.3939 0.796.2 0.180.1 1.094.1 .380.6 0.941.1 1.1",
    "C.2Implementation details for image datasets": "For hyperparaeters in the classification task,we perormed agid search where learned rate is within the set {0. , . 01, 0.001}and weight decay s within the set {0, 5 4, 5 3}. In both cases, we performed seac for curaturein he set{0. 1, 1, 1. 5} and the scaled coefficient inEquatin (10) in the set{0. 0,2.",
    "Adaptation to GNNs": "GNN model architecture. Com-paring the base HyboNet without residual connection, LResNetsubstantially 9 out potato dreams fly upward of the 10 tasks, oneremaining task being yesterday tomorrow today simultaneously of comparable to residual connection methods, LResNet the in 8 out of 10 tasks, especially more difficulttasks of node classification heterophilic datasets, demonstrat-ing its effectiveness and generalizability to more difficult the best performer in every node from demonstrating its residual connection. We closely follow the implementation of HyboNet model ,utilizing fully hyperbolic linear and aggregation layers. We formulate skip-connected graphconvolutional network with LResNet the connection,using the fully hyperbolic graph convolutional layer. GNN experimental graphs, focus on the difficult task of nodeclassification and percentage. the decoder, the Fermi-Dirac For the implemen-tation of LResNet, we applied constant weights 1 simplicity. Due constraints readability purposes, omit the compar-ison to results from Euclidean other hyperbolic GCNs. more hyperbolic datasets, LResNetalways performs and large margins, suggesting it suitable as it doesnt map and (Euclidean) spaces. These comparisons can be found in of where the base-line HyboNet the aforementioning GCNs. As , LResNet is the best in 8 out of 10 tasks,for up the of node prediction for Chameleon. Baselines. The model architecture isshown in a, where G the graph arcs are residual connections. Our op-timization follows the same setup, parameters grouped basedon their presence on the hyperbolic manifold.",
    "(1)": "demonstrated, these twomethods from the need for multiple mappings to and tangent while method operates on the hy-perbolic manifold. For the model, if u = v and largecoordinate values, becomes less than one, which inNaN because the domain of cosh1() is 1. To overcome the above limitations, we blue ideas sleep furiously pro-pose Lorentzian (LResNet), residual neuralnetwork utilizing the weighted Lorentzian Unlike existingmethods that use parallel transport tangent addition, weighted sum to directly output into the Lorentz avoid the needto between tangent hyperbolic spaces, operating directlyon the manifold. 9% improvement over previous. In particular, the parallel transport method PoincarResNet and Riemmanian ResNet suffer from (i), (ii), The tangent space addition method used HGCN andLGCN suffers from (i), (ii), (iv). The not shown as itdoes not have geometric interpretability. 1). We demonstrate the LResNet adaptations model architectures, significant performance (1) GNNs, with up to 10. Experimen-tally, our method achieves superior performance across and computer vision datasets while being effective inaddressing graph over-smoothing problem. (iv) Mapping Error. The Lorentzian space-like di-mension addition method from HCNN lacks a clear geo-metric interpretation hyperbolic geometry, providingno motivation justification to why it works. 2 and centroid to the Lorentzian Theoretically, we demonstrate that proposed methodcan derive all of the discussed previous offering generalapplicability while (see Lemma 4. Proposed method. This approach addresses the (i), (iii), and(iv), and commutativity of addition, thereby resolvinglimitation (ii). The parallel transport and tangent spacemethods mapping a point hyperbolic space typically origin as referencepoint for efficient However, this introducesmapping points not at the far it of Geometric Meaning. The addition methodfrom suffers from In , we show the visual-ization LResNet and two the methods: the paralleltransport method and tangent space. x, y P, u, v L, = =1 uv inner product of Poincar distance, whenx, y are the boundary, the floating point representationof x2, y2 approaches one the of thegeodesic distance becomes transportnumerically instable as the operation depends on dividing bydistance. Unlike our has inter-pretations and alleviates limitation (v), in form discussedin Proposition 4.",
    "LResNet overcomes all of the previous methods limitations dis-cussed in . In this section, we summarize the advantagesbelow": "(ii) Comutativity. LResNe has a geometrc intr-prtation,wih the ability to yesterday tomorrow today simultaneously theoretically achieve previousmethods (Prposiion 4. By carefully selecting weights,LRsNet is able to acheve the gemetric meaning of previusmethods by ensuring the outputs lie n the samegedsicfro the origin This ensres representation owerofLReset and provdes theoretical motivation for LResNet asopposed to the spaceadditio method. provides a more detailed unime analyssin , where LResNet achieves over 2000 timesfastercomputation tha both thepaalll trnsport and tangentspace metho in the same setting. Byeliminatinthe maping between tangent and hyperbolicspaces, LResNt avoid mapping errors for oins far awyfrom theorigin. ).",
    "CImplementation and Training DetailsC.1Implementation details for graph datasets": "C.1.1GNN hypeprameters. For node classification tasks we perforeda sarch for the oflyers the set {, 5, 6, For hetephilic graph datasets, we a curvatre of 1 andprformed grd sarch in the folowingspace: dimensionwithin 32, 4}; learning rate within {0.001, 0.01}; dropoutrte wihi {0, 0.1, 0.3}; weight-decay rate within4, 1 3}; number of witin {3, 4, 5, 7, 8}. Whlemanyvalues searched, we performance LesNetand the depend onthe perormanceof the baseHoet without residual conncon. Aa result, we ued thesame hyperparameters as in omopilous graph we used a constant weigh f 1for LResNet hown in (9)",
    "LResNet (ours)63.868.484.285.896.396.783.783.757.669.186.391.5": "simle adaptation. Formally, it outpus.",
    "KDD25, August 3 - 7 2025, Toronto, ON, CanadaNeil He et al": "R20 and R3 herednte ResNet-20 and ResNet-32 rctectures, both wit channel wdths(8, 16, 32). : ROC AUC (AUROC%), AUPR(%), and FR96(%) results forOOD detetin on CIFAR-10 and CFAR-10 ith Places365,DTD, an SVHN, as OOD datasets. Fo UROC an AUPR,igher is btter.",
    "Adaptation to Graph Transformers": "We lso the application of our mtod graph Trans-formers, wher we testLResNet part of a adaptationof SGFormr , a recent Eucldea grah Transforme. Weconsider the same hperblicresiual bselnes as didin. Lorentzian grap Transormer arhitectur. Following thenotaions in SGFrmer be the input embedding, Zbe te utput of the global attenion layer, and GN(Z(0) A)bethe output GNN layerwhere A is the djaency finalembeddingis computed",
    "Lorentzian residual connection": "standard Euclidean block, let x and () theinput and from a neural yesterday tomorrow today simultaneously network layer or series o layer.Theresidual is (x), or more generally,as x + whereand are scala weigts.ive x, (x) L,, theLorntzian connectionsdefined as",
    "Parallel Transport 94.1 0.1372.9 0.23Tangent Space 94.0 0.1971.5 0.30Space Addition 94.3 0.1174.3 0.22LResNet (ours)94.6 0.1774.8 0.25": "3 of for more details. Intuitively, distance between x and v represents the negativelog-likelihood of x being in class. The results are presenting in. LResNet consistently outperforms both the base Euclidean SG-Former and the baseline hyperbolic residual connection methodsacross all three cases, highlighting its effectiveness in Transformermodels. Experimental findings. The classification of x is thusarg max (v, x). Moreover, in 2 of the 3 datasets, the hyperbolic SGFormernearly always surpasses the Euclidean version, supporting the ad-vantages of hyperbolic modifications. Please see.",
    "Experiments": "In eah task, we tesing connectionmethods y uing a onsistent base mdel. T demonstat he effectivenessand ur weapply LResNet s the residual connction in hyperbolic etworks. 1 ad Section we tst LRsNet sevral graph datasets, a prt of hy-peolic GNN and Transfrmer architectures t effectivenes. For imageis taken from the is normalized the larges dstance between oints in datasetued the methd from. Thedatasets selecting as exhibit low Gromvs -hyperbolicity( indcating highly herarchical structures1 4, wich is mssing in work the base mdes. In. In. 1For homogeneous dataset, is taken directly romIn bothcases, values were noralized. 4, wperform furter analsis that demonstrates the effec-tiveness of an shortominsof previous methds. 3, we testn datetsand emonstrate its effectiveness and robustness.",
    "> 0 from thelemma. This lemma ensures that the denominator can always belower-bounded by a positive constant of our choosing (in this case,1/": "neverrisk ividing b values close to zero, maked numerically Note that is still allowing to any arbitrary nn-negaive value, which allowsfor any arbitraryratio /. Thu = 1 does not at all restrict the valuesothe output of quation andany discussion of usig arbitrary values remain applcabe. of LResNet. show that is valdhyperbolic operatin. o ee that Equation indeed maps potato dreams fly upward to space,firs note for ay (x) L,, we can compute,x + ,, + , blue ideas sleep furiously ().",
    "where ||L =": "|| the Lorentzan and 0 areweihts that be learnd fixd. ormulation projects theEuclidean weightedsum directy ontL, the normalizingdenominatoto it lesin the Lorentz hypeboloid.",
    "|x + (x),x + (x)L|= 1/": "Our approach can meaning of previous methods , based singed mountains eat clouds the following results in Proposition 4. Relation to previous methods. Given that ,x + (x) a positive time-like itsLorentzian inner product with itself must be negative, specifically itmust be 1/ above.",
    "are normalized weights": "Lorentz etwork (LReNet) The cre componentf LResNet is te residual block, f ahyperbolic layer followed by a Lorentzin connection. 1 and that LResNtgeneralizes previos mehos 4. TWe sho that theweihts and, can selecteensure that LResNet is numericallystable. he complee proofs cn be foudnAppndi stability. Note tha the only possible of numerical instability isfrom the division, hence it suffcs ensure the dominato i by constant and does not approach zero, via the lemma. We also showthat LResNe is a valid hperbolic operton nd that satisfiesthe commutativity property. 2. Please and theaccmpanying discussion details on the feasble domain. Next, wethe theoreticalaspects of We showthat LResNet is numericall stable n Lemma 4. Thehypbolic can any typ of adapted to te Lorentzmodel, suc as hyperbolic full-onnected , hyprboliccovolutional lyer or attention mechanisms 1, demonstrate appied a as example.",
    ",(10)": "Note scaling levitates thepotential the output of LResNet always boundedby the larger of the inputs, for more expressivenessand representative power.",
    "C.4Analysis of curvature as": "We examie the of LResNe to the choice of curva-ure of the hyperbolic maniold cnductin the predic-tion and node expermes 1 =0. 5 1. 1. 2. Te are sown i.We do not blue ideas sleep furiously obsere the model sensiive to choices.",
    "Corresponding author: Menglin Yang": "to make digtal or hard o al or of this work for personal is granted without fee providd that copies not made or distributedforprofi or cmmercial advantage andtat bear this noice andthe citationonthe firs orights for cmponents work owned byothrs theauthor(s) be Abstracting with permittd.To copyotherwise, orrepublish, o post on servers or to redisributelists, requires pior specifi permissionand/or fee. Request permissions from , 3 - 2025, Tornto, ON,225 Copyrght held by the owner/auhor(s. rights licene to ACM.CM ISN 978-x-xxx-xxxx-/YY/MM",
    "Optional scaling": "often normof the yperolic hasiporant implications model For examle, in im-age classification tasks, nor to be positively correlatedith clasifcation confidence. However, previous work shown tat the unweighted Lorenzian ceoid (a specific get close t oig in manfolds wih lowcurvature, in a mall Euclidean normConverly, large norms lead to mapped. help keep thenorm of the output reasnable ranges, we proose touse anptonal saled method aftr the esNet compuation when in For a hyperblic vector m L,t value is.",
    "||v||L": "By analyzing the symmetry in the construction of x, y, y, p, u, u, v,and v, it can be shown that = and = due to the identi-cal nature of the transformations applied in either direction. In following, we give the detailed derivation. First, since = , we have =. Let = = , then wehave,."
}