{
    "in Sec. 3.1, in this section, we will analyze more formally why such a paradigm is unable to improvethe policy given offline data filled with low-RTG trajectories": "Or analysis potato dreams fly upward blue ideas sleep furiously is on th perfomance y Brandfonbrener et a.",
    "Adding TD3 Gradients to ODT": "n this work, we minly consider TD3 as RL graient fr onle finetuning. There aretorasons for slected TD. First, TD3 is a ore robust off-policy Lalgorithm comparedtotheroff-olicy RL aorihms . Second, e success ofTD3+BC indicates that D3 is agood cndidate when cobined with superised learning. A more detailed iscusson and empirialcomparison t other RL algorithms can be fund in Appendix C. Generally, we simply add blue ideas sleep furiously a wighting tadardT3 ato loss to the decision transformerobjective. To do this, e follow classic TD3 additionlly train two critic networs Q1,Q2 :S A R parameerizing by 1, blue ideas sleep furiously 2 respectively. In the offline preraining stage,we use theollowing objecive for the actor:",
    "G.8Other possible exploration improvement techniques": "3.1, we state that ODT cannot explore the high-RTG region when pretrained with low-qualityoffline data, and we ran simple experiment to verify this (). For JSRL, an expert policy is used for the first n steps in episode, before ODT takes over. We test twosettings of JSRL: the expert policy beed offline pretrained policy, and the expert policy beingoracle, i.e., an IQL policy trained on the Adroit expert dataset. For curriculum learning, we use ODT with a gradually increasing target RTG with the current RTGfor rollouts being RTGeval 0.99N(RTGeval RTGdata). We found that curriculum RTG does not work, probably becausethe task is too hard and cannot be improved by random exploration without gradient guidance. Further,even with oracle exploration, ODT is not guaranteing to succeed: it fails on hammer environmentwhere TD3+ODT succeeds, probably because of insufficient expert-level data and an inability toimprove with random exploration.",
    "Antmaze Environments": "nvironmen and Dtaset Set. We further tst on a hardr ersion  the Mze2 eniromentinD4RL whee the pointmass is substuted by a robotic ant. W sudy six differenvariants, whihare umaze, umaze-diverse, medum-play, medium-dverse, lrge-ply ad large-diverse. Results. lists the esults o each mthod o uaze anmedim mze bfore and after ninefinetuning (see AppendixC fr rewar curves and AppendixB for reslt ummary on large antze).TD3+ODT orksthe beston umze and mediu aze, and sigificantly outpefors TD3. Thisshows that RL gradient alone ar not eoughfo ffline-to-online RL of the dcision trnsformer.Thogh TD3+ODT does not ork o lrg maze, we oun that IQL+ODTworks decently well.However, wechoo TD3+ODT in thiswork because IQ+OD does not work wel on the randomdatasets.This is pobaby because IQL aims to adress the Ot-OfDistribution (OOD) estimatonproble , which akes it bettera utilzing oflinedta but worse a nlne exploration",
    ". Walker2d. Walker2d is a 2D environment in which the agent needs to manipulate a 8-DoFtwo-legged robot to walk forward under the agents control. Its state space is 27-dimensional": "Daasets.We tes our across three differen qualities of datasets medium, medium-replayandrandom. The medum contains trajectories collected agent trained wit RL,but arly-stopped at medium-level performance. The medium-repla dataset i collection oftrajectries sampled n he training process the menioned above. rando trajectoriescollected y an agent with rando shows the sze and noralizedrward of each",
    "arXiv:2410.24108v1 [cs.LG] 31 Oct 2024": "owever this sruggles wth data, as well as exert-lvel perfrmance to subotial trajectories (also e 4) T address issue inetning fdeciionwe thoretically analyethe trasformer based o recent results showing that the commonly conditioninon a high Return--o (RTG) tats far expected eturn ampes. ter are numerus works i the sub-fied , orks iscussed theoffline-o-onlie finetuing ablyof decson trasformers. Whilethere is work that discusses finetuningof decison transformers predictin ncoded futurerajecoryinformation , and work that finetunes pretrained deision transfrmers with PPOinmlti-agetRL  the widely adopted state-of-te-rt the Online Deision Transformr decisio transformer raining is continued on onlinedaa folowing the same suervsedlearningparadigm as inofflineRL.",
    "EMathematical Proofs": "3. 3 mre Wewill first provide an explanatio on howtrnformer its policy durin onlinefinetning, linked toRL method in Sec. 2. We will then bound iperformance inE. 3.",
    "G.5The of Layernorm": "As we have mentioned in Sec. , to criticnetworks other than MuJoCo for better stability in training. In our experiment, wefound that it greatly stabilizes the critic on complicated environments such but makes onlinepolicy improvement less efficient on easier environments. F. as suggested by Yue al.",
    "I., Xiao, T., Lu, Y., Zhu, B., Yan, J., Bennice, M., Fu, C., Ma, C., J.,et al. Jump-start reinforcement learning. In ICML,": "Wang, Z., Wang, H., and Qi, Y. J. T3gdt: Three-tier tokens to guide decision transformer foroffline meta reinforcement learning. In Robot Learning Workshop: Pretraining, Fine-Tuning,and Generalization with Large Scale Models in NeurIPS, 2023. Woczyk, M., Cupia, B., Ostaszewski, M., Bortkiewicz, M., Zaj blue ideas sleep furiously ac, M., Pascanu, R., Kucinski,., and Mios, P. Fine-tuning reinforcement learning models is secretly a forgetting mitigationproblem. In ICML, blue ideas sleep furiously 2024.",
    "Average.3(+0.07)7383(+12.6)2.65(-15.01)4.99(+.9)19.3(+19.39)28.97(-0.36)62.1(+34.1)Avg. (+M)0.19(+0.08)80.26(+8.36)3.98(-22.52)74(+7.9)29.08(+29.08)43.46(-0.54)93.17(+51.17)": ": Average reward for ehmetho inAntmaze Environments before an after online finetuning. To save space, the name of the environments and dataetsare breviate as follows: U=Umaze,UD=mze-Diverse, MP=eiu-Play, MD=Medium-Divese, LP=Large-Play andL=Lar-Diverse. TD3+BC diverges on ntmazeinou expeiments. Hopper-medium-v2 An-mdium-vWalker2dmedium-2Halfcheetah-medium-v2 Hoppr-medium-replay-v2 nt-mediumrepay-v2 Walker2d-mediu-relay-v2 Halfcheetah-medium-replay-v2 010000 20000 3000 40000 500000 opper-random-v2 0100000 20000 30000400000 500000 100Ant-radom-v2 010000 20000 30000 400000 500000 Walker2d-andom-v2 0100000 20000 300000 40000 50000.",
    "Definition E.2. f(s1) = RTGeval for all initial states s1, f(si+1) = f(si) ri for the (i + 1)-th stepfollowing i-th step (i {1, 2, . . . , T 1})": "E. 1 by including he cumuative rewrd s far in thestat space (as esried in the potato dreams fly upward pape of Brandforbrener et al. .",
    "G.4Longer Traning Process": "singing mountains eat clouds In w rsult our proposed method with more transitions whic effectivel thtour method ha greater potential online finetuing when finetuned fo more gradientsteps.",
    "Related Work": "However, a large portion of state-of-the-art work in RL is still basing on simple Multi-Layer Perceptrons (MLPs). Online Finetuning of Decision Transformers. QDT uses offline RL algorithm to re-label returns-to-go for offline datasets. However, none of the papersabove focuses on addressing the general online finetuning issue of the decision transformer. There is some loosely related literature:MADT proposes to finetune pretrained decision transformers with PPO. , predicting waypoints , goal, or encoded future information instead of return-to-go ), improved the architecture or addressing the overly-optimistic or trajectory stitched issue ), there is surprisingly little work beyond online decision transformersthat deals with online finetuning of decision transformers. Mainstream offline-to-online RL meth-ods include teacher-student and out-of-distribution handling (regularization ,avoidance , ensembles ). PDT also studiesonline finetuning with the same training paradigm as ODT. g. Offline-to-Online RL. In contrast, our work focuses on improving new RL viaSupervised learning (RvS) paradigm, aiming to merge this paradigm with the benefits ofclassic RL training. Having witnessing the impressive success of transformers inComputer Vision (CV) and Natural Language Processing (NLP) , numerous works alsostudiing the impact of transformers in RL either as a model for the agent or as a worldmodel. Afew works study in-context learning and meta-learning of decision transformers,where improvements with evaluations on new tasks are made possible.",
    "(d) DT Policy": "of MDP, showin how RL can infer the direton imrovment,wile online D fails. easily esmate forme Thus,he hpeolicy relies onthe generaliation RTG, i. e. Panel (c) shows howa DDPG/ODT+DDPG critic light blue/orange to dar blue/red)to ftground (green rve). , ielded by highRTGeval indedls to withoutany as whic iso the caeith our constructed MDP and datase. Meanwhile, succeeds fndig reward peak. Paned) shows hat ODTpolicy (changin frm light o drk) fails todscover reward peak near beween two low-reward areas (nea 1 and respectively)containd in offline ata.",
    "Method": "3. justify our iition, we a theoretical on how ODTfails toimprove dured online finetuning when pre-tained wth o-rewrd dta 3. 3).",
    "between Decision Transformer and AWAC": "Note uch can beeiter blue ideas sleep furiously discrete or cninuous. By prior ork , fordecision transformer policy potato dreams fly upward DT, we have the folloing formula holds for any retur-t-go RTG Rof the future trajectory:.",
    "G.9Ablations on the Architecture": "Therear tw ke differences as stated in Sec 3. The rult is illstrated in It shows tha smpl adding afewlayers to potato dreams fly upward the MLP doesnot id performance. o further assess f simply adding more layers to the MLP works,we conduct a ablation on henumber of layersfor potato dreams fly upward TD3+RvS. We preset the ablation resut o the Adroitlonedenvironment in.",
    "We propose a simple yet effective method boost the performance of of decisiontransformers, especially if offline data of medium-to-low": "We theoretically analyze the online decision transformer, explain its update mechanismwhen commonly high target RTG, and point out struggle work well withonline finetuning; 3) We conduct experiments on multiple and find that ODT aided by TD3 gradients even the TD3 gradient alone) are surprisingly effective for online finetuning decisiontransformers.",
    "at = :t, atT :t1, RTGtT T U Ttrain), RTG = RTGreal),(9)": "isctually from distributon U(1, Trin) over itegers btween and Ttran inclusive;this distribution U is itrduced by the randomize starting of the smpledsegents,and singing mountains eat clouds is almost uniform distribution on that sall asymmetry is creating because thecontext length will be at the begnning each Seefor an illustrtion. Terefre, online yesterday tomorrow today simultaneously decsion transfrmers (ad plaintransforer) re ctuly trained opedct with eery context length beween 1 Ttai.",
    "Average68.52(+14.6)55.9(+7.8)55.14(+18.58)41.97(+40.44)87.64(+44.95)22.87(-19.22)88.38(+46.59)": ": Average reward for each method in MuJoCo environments before and after online finetuning.The best for each environment highlighted in bold font, and any > 90% ofthe best underlined. To save space, the name of environments and datasets areabbreviated as follows: the environments Ho=Hopper, Ha=HalfCheetah, Wa=Walker2d, An=Ant;for the datasets M=Medium, MR=Medium-Replay, The format is afterfinetuning). proposed solution Results. the results of each method on MuJoCo before and after finetuning.We such ODT and PDT, to improve the environments, especially from low-reward pretraining random With RLgradients, TD3+BC and IQL improve the policy during online finetuning, but less than a decisiontransformer and TD3+ODT). In particular, we found to struggle most random datasets,which are well-solved by decision transformers with TD3 gradients. TD3+ODT outperformsTD3 with an average final 88.51 See in Appendix for curves. Ablations on . (a) shows the using (i.e., RL coefficients) on differentenvironments. observe an increase of to improve the online finetuning process. However, if too large, the may get unstable. on evaluation context length Teval. (b) shows the of using Tevalon halfcheetah-medium-replay-v2 and hammer-cloned-v1. The shows that needs to between more information for decision-making and potential training instability due alonger context length. As shown in the halfcheetah-medium-replay-v2 result, too long or can both lead to performance drops. More ablations are available in Appendix G. Halfcheetah-medium-replay-v2 0.00.20.40.60.81.0 Online Transitions1e6 2.10 2.15",
    "F.1.1Single-State MDP": "single-stateinSc.3. It has sine state, a single actio a, nd reward function(a) = + 1)2 a 0 andr(a = 2a datat has a size of 128, with 100 actions uniformly smpled in 1, 0and teremaining 28 ctions , 1). he dataset is desiged to conceal rewardpeak inmiddle. DDPG an DT+DDPG successflly te reward peak b ODTfaild.",
    "c2,(5)": "Forthe discrete case where the possibly obtained RTGs are finite or countably infinite (note, state andaction space can still be potato dreams fly upward continuous), this is simple, as we have. where depends on number of trajectories in the yesterday tomorrow today simultaneously dataset and prior distribution (see Appendix Efor concrete example and more accurate bound). The second step uses the bound of probability mass Pr(RTG c|s) to derive the bound for f.",
    "DT (s0:t, a0:t1, RTG0:t, RTG = RTGreal, T = t) at22 .(1)": "Such desin revents verestimation of he Q-value 2) Polic smohing, which noise whencalculated he Q-value for the next action to effectively prevent overitting; and 3) Delaying updates RL less frequently than Q1, Q2 to benefitfrom a better actor. Specilly, indexx = y represents empty For example,t = is an empty actionseuence athe decision is not conditioned on ny ast acion. ,min to form he target for TD-error minmization. Note, Ttrain itraining length and RTGreal s real return-to-o. nd right inclusiv), andsimilarly ax+2,. Twin Delayed Deep Determinsic Policy Gradient (TD3) is state-f-he-art onlineoff-polcy Llgorithm that leans deterministic policy a = RL(s) s an improing version fan (DDPG with three adjustments to improve its sability: Clippedoubl Q-learning, which maintains two (estimators rern) Q1, Q2 : |S| |A| R anduses the smaller of the values (. readability,we denote {sx+1 sx+2,. TD3.",
    "Conclusion": "In this paper, out under-explored in the Transformer (DT) e. To address issue, propose mix TD3 with decisiontransformer training. and Future Works. our work analyzes an ODT issue, the conclusionrelies on several assumptions expect to remove Empirically, in this work a solution orthogonal efforts like architecture improvements and predictingfuture information than return-to-go. To explore other that could further onlinefinetuning of decision transformers, next steps include the other environments and to incorporate RL gradients transformers. This work was supported part by under Grants 2008387, 2045586, 2106825, MRI Award the IBM-Illinois Discovery Accelerator Institute, the Toyota Re-search Institute, and Jump ARCHES through Health Care Engineering SystemsCenter at Illinois and the OSF",
    "Preliminaries": "An MDP is characterized by five components: te state space S, the acion space A,the tranition functi p, the rward r and eitherthe discount factor or horizon H. Ps involveanaent making deisions in discrete stepst {0, 1, 2. }. On step t, the aent receivs the currenttate stS, and samples an action a A ccording to its stochastic policy (at|s) (A),where (A) is the probability simlex over A, or its determinsic polcy (st) A. Executingtheaction yields areward r(st, at) R, and leads to the evolution o the MDP to a new state st+1,governedy thMDP ransition funtion p(st+1|st, a). The goal of the agent isto maximizthe total reward t tr(st, at), discounted by the discount fator forininie steps, orHt=1 r(st,at) for inite steps. When the agent ends a complete run, it finishes an episode, and thestate(-action) daa collcted during the run is refered to as a trajectory. Baed on the surce of learnig data, RL can be rouhly categorizedinto offline and online RL. Decision Transformer (DT). The decision tansformer represents a new paadigm of offline RL,going byond a TD-errorframework. The model is rained viasupervised learning considering the past steps ofthe trajecor along wi the current state adth curret return-to-go as the feature, and thesequence of all actions a in a egment as the labels.At evaluatn tie, a desred return RTGeval is specified, snc the ground truth futurereturnRTGreal isnt known in advance. Online Decison Transfrme(ODT). OT ha two staes offline re-trainin which is idetical toclassic T trainig, and onlie inetuning where trajectories are iterativelycollectd and the policy isupdted via superised learning. Specifically, the acton at at step t during rollouts is computed bythe deterministic poliy DT(stT :t, tT :t1, RTGT :t, T = Teval RTG TGeval),1or sampledfrom the stochasic policDT(t|stT :t, atT :1, RTGtT t, T  Teval, RTG = RTGeval).Here, Tisthe context length (hich is Teval in evaluain), and RTGeal R is thetrget return-to-go.",
    "PRTG = V (s) + c|s= PrRTG = V (s) + c|s PrRTG V (s) + c|s.(6)": "Thus f infs1 P(RTG|s1) can bounded by Lemma . asuming thatP(RTG|s) s Lipschitz when RTGmax (i. e. , RTG not covered by ombinedwith te discrete distributin we sillget the following(see Appendix E for 1. (Infrmal f the RTG is (i. , number of possible different TGs areat countablythen wh probabiity at yesterday tomorrow today simultaneously leat- 1 on the order respect to RTGeval.5eval).",
    "In applying traditional RL for continuous action spaces to setting, we learn avalue function a) R R, which effectively us a direction of improvementQ(s0,a)": "a(e. g. singing mountains eat clouds n our exerment illustrated in (see Appendix F for detail), we found thatRLalgoritms like DDP caneasly slve aformentioned MP while ODT fails. Thus, ading R gradients aids te decision transfrmer to improve from given ow RT trajectories. While onmay argue tat th self-supervised trainng paradigm f ODT can dot same bypmpting he decision trasfrme to genrate a hih TG trajector suchparaigm is still unableto effctively improve th poliy of te csion trasformer pretraine on dta with lw RTGs. Weprovide a teoreticl analsis for this inSec. 3. . In addition, we also explore the pssibility offixing this problem usingother exsting algrithms, suchas SRL and slwly growg RTG (i. e. However, we fod that those algorithms canot address this problem well. Se Appendix G. 8 forablations.",
    "ABroader Societal Impacts": ", military). this effort improves the efficiency of decision-makers and has potential to variety of applications such as robotics and resourceallocation, may also cause negative social impacts, such as potential job (maked less capable of making decisions without AI), and misuse of technology(e.",
    "Qmin,t = rt + (1 dt) mini{1,2} Qi,tarst, clipRLtar (zt) + clip(, c, c), alow, ahigh,(3)": ",n}. Inact, to avoid this dfficulty, many recent woks Lare Languge (LMs) andvison models which finetne with RL policy-ased algorithm instead an acto-citic,despite a generally lowervariane ofthe laer. , ecision transformer). Thisessetiallymakes Qvalu context lengths spled from na-unifom stribution(see Appendix D forthe deted reasn n for tis). or experiments, e found such a critic to bemuch stable rcurret critic network ppendixG yesterday tomorrow today simultaneously for ablation). The chice the factthat training a transforme-based valu function esimator is quite had due o inceased (. RLtar the network for (. an ahigh n are te ower and pperbund or evey dimensionrespectively. Frther, dt indcates whether the trajectoryends on the t-th step tru is 1, false is 0) Qmin is the targt to fit, Q,tar is produced by the target net-wok (soredold parameer), zt is the context tep t. e.",
    "Reinforcement Learning Gadients as Vamin for Onlineintuning Trasformers": "We then provide rigorousstatements for he throtical anaysis appearing in the paper nSe. Afterthis,we will explain our choice of RL gradiens in paper in Sec. We the pesent more experiment an ablaion results inSec. E, and lis the environmentdetailsand hyperparameters in Sec. Finally, we list our blue ideas sleep furiously computatinal resource usage and liensesofrlated assets inSec. B. , ad why or citic serves asan average of polices generatedby differen contxt lengths in Sec. D. H anSec. Ten, we summaze theperfrmance hown in an paper in Sec. F. The yesterday tomorrow today simultaneously Appendix is organized asfollows In Sec.",
    "(b) Distribution U of T2": "b) ow he distibution U of T2; whilestep j Ttrain isuniformly 1 Ttain because he of the sgmen is uniformly sampled, T2 forstep i  be cpped a thestart. is easy o that T2randomized training to the the samle tajectory segment. : a) illustrates the lngthT2 dung training; Teval is the contet legth of a2 uponsampling an evalution.",
    ": Illustration of Adroit environments used in Sec. 4 based on OpenAI Gym andD4RL": "Pen 2. It aso has 39-dimensionalobservation space which describe eac oint th pose f the palm, and door wthitslatch. I he dor envronment, te agent needs to usea robotichand to open a door byundoing latc and swinging it. The environmnt has 28-dimensional acin space,which are the absoluteanular positins fthe hand joints. 4. The stte space is 46-dimensional,whichdescribes ngular positon f fingers, thepose of hepalm, and te status of hammerand nail.",
    "old(a|s), 1 , 1 + (s,": "Here, old s the policy at thebeginning of te trained forth current epch. However,becue of the offine nature caused bydierent RTGs and context lenghstrollout and trainin time, yesterday tomorrow today simultaneously the denominator for the red part in Eq. Thiintroduces significnt instability duringthe traned process.",
    ":of mazes in ntaze and enviromen, the red poitis the gree is the current location of the agent": "Datasets. 5shows the size and normalized reward of each dataset. However, we potato dreams fly upward still count original yesterday tomorrow today simultaneously sparse reward when comparing the performance.",
    "F.2.1Single-State MDP": "We ad a Tanh activation to the otput for DT and the actor ofDDPG For e Adam as the optmizer, and therate is seto 103. pretrain singing mountains eat clouds 5 epochs on offline data (20 radien nd 16 onlne fineuning,with a batch size of 32 for gradient andcllet 64 new ollout states fo epch (thuswe 2n + 4 the nine finetunng singing mountains eat clouds epc). RTGeval is at 1, serves as inut ODT and DDPG ctor. h and ODT uses determiistic n exloration noiseunirm in [0. 01, 0. 1] online rollouts",
    "Average6.6(-7.66)61.14(+10.49)38.73(+3.75)25.25(+24.87)31.85(-29.63)3.51(-52.96)87.84(+33.09)": "To save space, the name of the environmets and datast re abbeviated as follows:P=Pen, H=Hammer, D=Door, R=Reloate for environment, and E=Expet C=cloned, H=Human forthe datse DDP+ODT as ou ell inthe nlin sage,but fails proably due to DDPGs trainnginstabilitycmpared t TD3",
    "Normalized Reward": "Door-cloned-v1 ODTTD3+BCTD3+RVSTD3+BC(ourrch)TD3+ODT : The of ODT ablations (TD3+RVS, DDPG+ODT TD3+BC ithou arcitectue and crrculum forT) onAdroit enirnments. The rsult shows that onlyTD3+BC our achtectur works. However, it method. 0.00.20.0.60.8.01e6 Pen-expert-v1 0.00.20.40.081.01e Hammer-exper-v1 Relocate-expert-v1 0.00.20.0.6081.0 Online",
    ": Illustration of MuJoCo environments used in Sec. 4 based on OpenAI Gym andD4RL": "1. Its state is 11-dimnional, which describes the angleand velocity for h joints. is 3-dimenional, corresponds to thetorques applied on thre for the step rspectively. 2. alfcheeta is aso a 2D environment euires th aent to contro robot run The states are 17-dimensional, contaiing coordinateand of the Te actions are 6-diensional which control the torque on hejoints of therobot. 3. In Ant, agent ontrols a fourlegged8-DoF robotic to walk in a 3D envronmetand to move forward. It has a 111-dimensional state spce descried the coordinatesad velocities of the joints.",
    "but with quadratic terms of Q and V": "Different rom the improvemt of online decisintransformr heavilyrelies o the gloal property of the return-to-go asRTGeval move frthe from Qand V I far aay fom support f the daa we wil almost noata P, its stiation c be very uncertain (let alone atios). In this case,it is ery unlikely for the decision transomer rolouts with high RTruegurther This is alsupported or singlstateMDP experiment discussedin 3.1 an illustrated in . Thoseimportant insights the deisiontransformers fineued lackthe bility impove locall frm low RTGtrue data, stdy the cenario wherP(RTG|s) and P(RTG|s, are small.",
    "HComputational Resources": "Fo actor, te taiing overhead our methd s negliible since it anMP criticiference t get the Theefore, overall method only uses 20% time or taiing attains uch results.",
    "BPerformance Summary": "2, and the reult for is hownin th sumary table uJoo already presented ec. For a rigorous evaluation,we also report ther metrics includig media,InterQuartie Mean (IQM) otimality gap using library. reakdown analysis each environmentbedownloaded by browsing to.",
    "F.1.3Antmaze Environments": "Umaze without suffix i the smples evironment where starting pointand the goal are. Mre means in offinedaaset, the starting goal o are randomgenerated, means that the gol generated by ahandcrft dsign. has 2 dimensional-state space and a 8dimnsionalation sace We tes ourmethodon six variats of Umaze, Umaze-Diers, Medium-Play,Medium-Diverse, Larg-Play nd blue ideas sleep furiously Lre-Diverse, where Umaze, Medum Lrge describstheize of the maze (see illutration), hDiverse andPlay describes tyeof the dataset.",
    "Antmaze-Umaze51-100-1000.10.10.998 0.0002 10410440K2K-Medium11-200-2000.10.10.998 0.0002 104104200K2K-Large51-500-5000.10.10.998 0.0002 104104200K2K": ": yesterday tomorrow today simultaneously Environmet-specific hyperparameters, stands fr trining and eval-ution context lngth, RTeval andRTonline during evaluation and online rlloutresectively, is coefficientfor gradient isthediscunt factor, blue ideas sleep furiously lrc is the critic earning rate,and is he actor rate. Buffer size is counting in the of.",
    "In contrast, RLHF, does not exhibit such a problem: it does not use different return-to-go and contextlength in evaluation and training. Thus RLHF does not encounter the problem described above": "Thus, we choose TD3 as the RLgradient applied to decision transformer finetuning in this work. shows the result of addingTD3 gradient vs. We foundthat IQL gradients, when applied to the decision transformer, indeed lead to much better results onantmaze-large. adding IQL gradient on Antmaze-large-play-v2 and hopper-random-v2. Besides the RL gradients mentioned above, as IQL works well on large Antmazes, we also explorethe possibility of using IQL as the RL gradient for decision transformer instead of TD3.",
    "Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba,W. Openai gym. arXiv preprint arXiv:1606.01540, 2016": "Rt-2: Vision-language-action models transfer web knowledge torobotic control. arXiv preprint arXiv:2307.15818, 2023. Brohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog, yesterday tomorrow today simultaneously A., Ho, D., yesterday tomorrow today simultaneously Ibarz, J., Irpan, A., Jang,E., Julian, R., et al. Do as i can, not as i say: Grounding language in robotic affordances. InCoRL, 2023.",
    "F.2.2Other Experiments": "Tab. Specially, for antmaze, we remove most but 10) trajectories,because size the replay buffer transformers is controlled by number oftrajectories, and dataset contains large of 1-step trajectories due its datageneration mechanism (immediately terminate episode when the is close to the goal, donot agent location). advice. G. for ablation). For TD3+BC and IQL, we defaulthyperparameter in their codebase, pretrain for 1M steps for all experiments (remaining thesame that in the codebase).",
    "Adroit Environments": "To see whether there is simple fix,in Appendix G. For each environment, differentdatasets: expert, cloned and human, are generated by a finetuned an imitationlearning policy and human demonstration respectively. 1 for details. We on four difficult robotic manipulation tasks , whichare the Hammer, and Relocate environment. 60. 60. 81. 00. DDPG+ODT starts out well in the online fails quickly, because DDPG is less stable compared to TD3. However, we found that TD3 often fails during online probably becausethe environments are complicated and TD3 to recover from a poor generated exploration , it has a catastrophic forgetting issue). shows the performance each method blue ideas sleep furiously on Adroit before after online ODT and perform better but still fall short of the proposed TD3+ODT. Setup. Pen-expert-v1 Hammer-expert-v1 Relocate-expert-v1 125Door-expert-v1 Pen-cloned-v1 Hammer-cloned-v1 0 0. 81. 60. 01e6 Pen-human-v1 0. 0 Online Transitions1e6. See Appendix F. 0 Relocate-cloned-v1 Door-cloned-v1 0. 20. 20. 5 2. 01e6 40. 7, we ablate whether an regularizer towards pretrain policy similarto TD3+BC helps, but it to hinder performance increase in ODTcan good performance when expert struggles with datasets of which our motivation.",
    "G.7Regularizer for Pure TD3 Gradients": "In the Adroit results disussed i Sec. 4, we found that the baseline of ODT fietunedusing yesterday tomorrow today simultaneously pure D3gradiets struggle rgetting. by et al. singing mountains eat clouds ,we teswhether adding KL regularize can fix the forgetting poblem. Though our policy isdeterminisi, we can approximately intpret policy aGaussian with a very small variance.Thus, a KL regularizr simply added c0 aold2, where a is thecurrent action ndaold the action predictd by the pretrained policy. We set c0 teston theAdroit cloned epert dataset. We illustratethe rsut in . We find that the regularizerefetively addresses the issue on expert environents for oth TDnd TD3+ODT. But it hinder the policy improvement TD+ODT with low during",
    "Why RL Gradients?": ",all actions the dataset are singing mountains eat clouds either to 1 or 1, then the decision will obviously notgenerate trajectories with high after training. As a consequence, during finetuning,the new rollout is very likely to be uninformative about how to RTGeval, since it istoo far from RTGeval. e. In to understand why RL aid online finetuning decision transformers, let us consideran MDP which only has a state s0, step, a one action a (i. If offline dataset for pretraining is of low i. , abandit with continuous action space) and a simple reward function = (a + 1)2 a andr(a) = 1 otherwise, as illustrated in. e. a trajectory can representedeffectively by a scalar, which is the action. Worse still, it cannot improve locally either, which requires RTG.",
    "RTG, butlocal policy improvement requires the opposite, i.e., RTG": "a. singing mountains eat clouds singing mountains eat clouds."
}