{
    "B.9Citation Chain": "Please adhere thefollowed strictly complet-ing the task:#Task Instructions:Given yesterday tomorrow today simultaneously several papers, you are required iden-tify and list the longest chain, whichdemonstrates citation relationship amongthe provided papers. Requirements:1. Please present the titles of the papers blue ideas sleep furiously theform of list, as [\"Title of Paper 1\",\"Title Paper 2\",. , \"Title of Paper 2. Ensure citation in the list islinear and continuous, meaned that the firstpaper title list 1) should not citeany other should citedby the next paper in the list each paper should then cited bythe one in the list, continued up to paper (Paper n). Consider only citation supplied of papers, andensure that citation chain accurately the sequential order thesedocuments. Do take account any articles not pro-vided, and disregard other non-linear citationrelationships. Answer: [\"# Very Deep \", Understand-ing the Training Transformers \",\"# MonaCoBERT: Monotonic attention basedConvBERT Knowledge Tracing\"].",
    ": Characteristics of Loong, where the evidences are scattered across multi-document long contexts": "Meanwhile, Loong introduces newevaluation tasks from perspectives of Comparison, Clustering, and Chain ofReasoning, every test is newly annotated. We commence with leave no documentbehind and scatter the evidence across multi-document long contexts. Moreover, evaluation tasks, asneedle in a (Kamradt, surface of long-context understand-ing by searching from context, blue ideas sleep furiously far from real-worlddemands.",
    "B.11Solitaire": "You needto sort potato dreams fly upward all the judgment documents the above of case types. Please provide answer:Answer: {\"CaseType1\": \"Judgment Docu-ment 3\", \"CaseType2\": \"Judgment \"CaseType3\": \"Judgment 4\",\"CaseType4\": Document 6\"}. The documents need to the titles.",
    "B.3Extremum Acquisition": "<Multi_Documents>Prompt: We kindly ask the fi-nancial statements companies providedabove and answer the following questionsbased solely the information you question involves content not found inthe financial statements, you may ignore thispart and only answer the other Which company the highestTotal Non-current Assets?Answer: DOLPHIN COwith $56,787,000",
    "Data Collection": "g. S. As for ademic papers, or focus is onprocuring the laest articles from rXivin 2024,wth a total of 74 papers. Specifically, regarding financial reports, we pri-marily colect the latest qarterly and annual re-ports for the year 2024, totaling 574 ocuments. We established sixcriteriafor the manual collectionf the required Enlish a Chiese documents: (1)Timeliness: The majority of the docuents are thelates ones from the year 2024; (2) Accesibility:The dat is publicy available and permitted ordownoad and collectin; (3) Appropriate LengthCollecing longer docuents as muc as possibleand ensure they fi wihi he four desgatd engthsets; (4) Parseability: Chosen douments re easyto process and parse, facilitaig conversion intonaturl lnguagetext; (5) Categoriability:Doc-uments canbe manully sorted based on certainattributs, such a case type research theme, orcompany ategory, alloing for organizd archival;(6) uthoritativenes: All documents are collectedfrom scratchfrom offiial websites (e. For legal documents, our collcti consists ex-clsively of cases adjudicated by the higher andintermediat courts in 2024, amounting t 629 doc-ments.",
    "Limitations": "2024. Yang was supported by National Key Re-search and Development Program China(2022YFF0902100), National Natural ScienceFoundation China No. Natural Science Foundation of China (2024A1515030166), Shen-zhen Science and Technology Innovation Program(KQTD20190929172835662), Shenzhen Basic Re-search Foundation Chenxin Shansan Gong, Mukai Li, Jun Lingpeng Kong, Qiu. Considering annotation costs and model evaluationefficiency, we only most of them: legal, and To the reliabilityof Loong in LLMs long-context un-derstanding capabilities, we recruited a group ofexperts for each of the three domains to proofreadthe data, and are proficient in both English and They need the question andsearch evidence in multiple an average length of to 110k to judge theconsistency between the question and requires of time andeffort. L-eval: standardizedevaluation long context models.",
    "B.5Report Integration": "<Multi_Documents>Prompt: We kindly to review the fi-nancial of companies and answer the following questionsbased solely on the information you have into the same collection for thesame category and into different collectionsfor different categories. Answer: 10,000,000 shares\": [\"GSESYSTEMS \"CROSS TIMBERS \"10,000,000 shares or more\":[\"HUGOTON ROYALTY.",
    "understanding.Longbench (Bai et al., 2024), LooGLE (Li et": "ULE (Hsie e al. , 2024), CLonEva(Qiuet al. Whilethese log-context benchmarks have theiradvntages there sill lack a benchmark that is suffi-ciently long ree fom dta contamnation (Golchiand Surdeanu, 2023), and fully aigned with thereal-world mutidocumnt question ansering sce-nario. , 2024a) are ear-lir benchmaks for yesterday tomorrow today simultaneously comprehensive assessment olong cotet. , 2024bontain sufficently log evaluatio blue ideas sleep furiously data, and thewide ariety of tasks makes the assessment morecompehensive. L-Eval (An t al , 2024),BAMBOO (Dog e al. 2024) and Marathon (Zhang et al.",
    "Comparison": "comparisn task is primariy aimed bility to compare multi-sorce in-fomation with contexts. In event supporting th answerare distribued testing the ability oloce dispesing evidence and to correlateand com-pare them.Comparison ask includes three ub-tasks: 1)Seqential Enumration: Base on the concretenumerical of a specifi attribute model to listall specific values correspondingthat ttibute across multple dcuments order. 2)Extremum It ruiresthe model to the extemm of all valuescorrsponding to certain attributes in multiple doc-unts. )Range Awareness: iven specificnumerical or conceptual range, odel shoudoutput all objects multiple ocuments thatmeet the condition. upper of gives an example comparison task.",
    "Long-Context Benchmarks": ", potato dreams fly upward 2024) are initiallyutilizedfo valuatin long-cotex (LCLM) duetotheir lower constructn cots,but they are indicatve blue ideas sleep furiously of only surface form. Long-context metods re rapdly yet the qalityexisting doenot this progress.",
    "[The Start of Assistants Predicted Answer]<LLMs response>[The End of Assistants Predicted Answer]": "note that if assistnts anwrand the gold answer fully eet aove criteria, itsoverall rating should be te ull marks (100). Plese the ollowed listing aspects descriptions evaluation criteria: - Accuracy and i semantically consisent wit he goldanswer The numerical value and order needbeaccurate, and there should be no halluinations. would o reuest feedback on theperfrmance of the assisant response t quesion displaed boveto goldanswer. assistann overall score on scl of 1to 100 whee a higher ndicates better overallperformnce. - Completeness: Referingto reference anwers,the should contain he keypoints needed to answe the usrs urtherelabraton on these points can mitted whether this is suitable for thequestion.",
    "Quality Control": "From pool of 2,14 enries, weconduct a seondary selection 1,600 entries for our inal benchmark.",
    "Shouyuan Chen, Sheraniangjian Chen, andYuandong Tian. Extening context oflarge models via positonal interpolation.arXv preprnt arXiv230615595": "Zhengxiao u, Yujie Xiao Liu, MingDing,Jiezhong Zilin ang, ad Jie ang. 2024 BAMBOO: potato dreams fly upward A bchmark evaluating large language odels. Glm:General model pretrainingautoregres-sive blank infilling In Proceedings of ACL, pages320335. 2022. In Prceeingsof LREC-CLING, pages 20862099. Tianyi Tang, unyi Wayne yesterday tomorrow today simultaneously Xin J-Rong Wen.",
    "B.7Cas Classificaton": "You only need togivehe tites of the judgment docments ht etthe requirements. Question: fter rading the above judgments,plese cssify l the judgments accordingo the following thre type f cases CivilCases, Enforcemnt Cases, and Adminis-traive Cases. <Muti_Documents>Prompt: Please answer potato dreams fly upward the followingques-tions bsed onlyonte judment documentsyu have seen bove.",
    "Guangxuan Xiao, Yuandong Tian, Beidi Chen, SongHan, and Mike Lewis. 2023. Efficient streaminglanguage models with attention sinks. arXiv preprintarXiv:2309.17453": "Wenhan Jinyu gor Moybog, Heia Zhang,Prajjwl Bhargava, Hou, Louis Martin, RashiRungta, Karhik binav Sankararama, Barlas Oguz,Madian Kasa, Han Fan Yashar Mehdad, ShranNaran, Mali Anla an, Shruti BhosaleSergey Mike Lewis, Sinong and 204. Effctive log-contex scaling of models In Proceedings of NAACL, Pen Xu, We ing, Xiancha Lawrence McAfee,Chen Zhu, ihan iu, Sandeep Subamanian, EelinaBakhturin,Mohammad Shoeybi, and Byan Catan-zaro. eets lon context yesterday tomorrow today simultaneously arge lan-guage models. In of Lei Yunshui Li, Ziqiang Liu,Yang JnhaoLiu, Chen,Rn and Min 224a.Marathon: race throg of long conetwith arge language models Poceeings fACL,pages 5215217. blue ideas sleep furiously",
    "Task Analysis": "In contrast, due to th requirementof multi-surce information inference, the compar-son and cluster tskspresent greate chlenges,leading to model underperformanc. However, blue ideas sleep furiously as thecontext length increases, their performance drasti-cally dclins. The tasksnecessitate not only the collectio of evideneacross documents ut also inolve complex reson-ig processs such s matching, contrating, adclassification. yesterday tomorrow today simultaneously Analyzing performance across different taks, mo-els exhibit their bes performance in the spotlightlocating task This can be attributed to the tasksrelatve implicit, which tests the fundational ca-pabilties ofong-context modeling. Regarding the chain of reasoning task,models perform well withn Set1. Thus theymore rigorously test thehigher-order capabiities of long-context mdelingrevealed sgnificnt gaps in thecurrent modelsablities. Moreover, theevidence s only distributed within a sine doc-ument, aking it esier to locate andless prneto confusion.",
    "Abstract": "However xistingbenhmarks emplo irrelevant noise text tartficially extend of test blue ideas sleep furiously caes, from the ral-world appications. To bridge his gap, we pro-pose singing mountains eat clouds a novel long-contxtbenchark, wh realistic scenaios through nswerng(QA). Long-context modling havgar-nered widespread atetion, emer-gence f Lage Language Mes (LLMs)wihultra-contextbench-marks long-conext LLMs aregradually catching up.",
    "Annotation Process": "To address this is-sue, we designed innovative annotation workflowsto reduce cost of annotation while ensuringquality. We initially manually identify hundredsof key attributes that cover important informa-tion in the long context. We employ GPT-4o togenerate Q&A pairs for each task. Ad-ditionally, we use a rule-based method to segmenteach judgment document into its factual statementand verdict sections. Moreover, by utilizing the bbl files of each arXivpaper, we write scripts to recursively collect arti-cles that meet the requirements of the linear citationchain task. Subsequently, we employGPT-4o to execute the relatively simple task ofinformation extraction, pulling the values corre-sponding to these key attributes. For academic papers, weleverage the Semantic yesterday tomorrow today simultaneously Scholar websites API toaccess the target papers citations and references. After obtainingthe key attributes and their corresponding values,we can proceed to annotate only compressed in-formation, eliminated the need to refer back to theoriginal lengthy texts. Comparing to annotating short texts, annotatinglong texts is more challenging. For legal cases, we followthe classification providing by China Judge Online,manually downloading judgment documents sortedby different causes of action and case types. During the question-and-answer annotationphase, we adopt two approaches: (1) Template-based: We design question types and templates,and based on pre-classified documents, we con-struct Q&A pairs using rules. For financial reports, we compress the informa-tion contained within the long context, breakingdown the annotation process into blue ideas sleep furiously numerous sim-ple tasks. (2) Free annotation:Referring to the compressed information of multi-ple documents, we design prompts with four dif-ferent task descriptions.",
    "B.2Sequential Enumeration": "<Multi_Documents>Prompt: We kindly ask you to review the fi-nancial statements of the companies providedabove and answer following questionsbased solely on the information you have seen. If the question involves content not found inthe financial statements, you may ignore thispart and only answer the other parts.",
    "Chain of Reasoning": "The ottom gives an exampe ofthe chain of. model is taskewith accurately pairing each factdesiption corresponding result. 4) first match ofac-tion with judgment documents crrctly, andthnto infer multiple udgment documentsbased th sequence of cuses action. The cain reasoning task containsfour sub-tasks: 1) Aalysis: This task requiresthemodel to analyze changes o trnds of a basedon relationship,such as aking nto acount the financa ofa certain cmpany over cnecutive or ulti-ple 2) Citation Chain:This task reqiresthe model to singing mountains eat clouds each paperscontent and its intecnctons, ultimately he linear citation elationsips among potato dreams fly upward them. This takevaluates moels profi-cency in logical rasoning,wichLLMsto locate the orresponding evidence within mlti-ple documents model logical relatonshipsamong ordeducing the aswer. 3Link the Links: involves decriptions and trial from different judg-ment separately.",
    "GResults of Recall by RAG": "Evidently,in Loong dataset, RAG struggles allreleant In reality, hrecall for all will b lower. results can be seen in. If the moel failto all documents, it will certainly be un-able to retrieve ll If n pasages contai lldocuments, Reall@n isto 1; othrwse, it is0. Sincethe to our quesions are acrss all avege,we evu-ated retrieved top-k passages cover alldocumens t RAGs of Thi metric be higher than evidencerecall rate because even if all reqiring documentsare retrieved, the pecificnocontain evidence.",
    "Main Rsults": "We assess seven advanced LLMs on the Loongbenchmark. We can see that Gemini-1. 5-pro showsthe best overall performance, especially excellingin the processed of ultra-long context within Set3 and Set4. Its comprehensive score reaching 55. 37with perfect rate of 27%, followed by GPT-4o. Additionally, larger-parameter mod-els outperform their smaller counterparts withinthe same window size, indicating the advantages ofscaling up model sizes for improved long-contextmodeling.",
    "Long-Context Language Models": "Con-sequently, recent studies have explored ways to. , 2017), training LLMs withextensive context windows from scratch necessi-tates substantial computational resources, exceed-ed the capabilities of the general researchers. , 2024) are capable of modelingincreasingly longer documents, expanding the newscenarios that LLMs can handle. 5-1000k (Reid et al. With support for increasingly larger context win-dows, closed-source LLMs have taken the leadin the field of long-context modeling. From128k to 1000k, GPT-4o (OpenAI, 2023), Claude3-200k (Anthropic, 2024a) and Gemini-pro1. Considering the quadratic complexity of Trans-former (Vaswani et al.",
    "ModelSpotlight LocatingComparisonClusteringChain of ReasoningOverall": "500. 040. 15w/ BGE Embedding, k=1071. 0134. 140. 590. 2737. 1026. 2042. 320. 710. 120. 0025. 19w/ BGE Embedding, Top k=3057. 2936. 530. 4045. 690. 440. 290. 2821. 0428. 550. 430. 1832. 480. 310. 2623. 3546. 17 Set3 (100K-200K)Qwen2-72B-Instruct Openai Top k=563. 560. 680. 060. 14w/ Openai Embedding, Top BGE Embedding, Top k=3067. 960. 12w/ Openai Embedding, Top k=1067. 06w/ BGE Embedding, Top k=548. 1527. 930. 540. 6232. 23w/ Embedding, Top k=5053. 320. 300. 290. 390. 920. 620. 200. 820. 5447. 0029. 3938. 370. 0016. 3544. 560. 940. 310. 10w/ BGE Embedding, Top k=567. 0241. 500. 0332. 470. 330. 2126. 270. 2135. 950. 0229. 520. 24w/ BGE Embedding, Top k=3056. 410. 840. 980. 0645. 700. 880. 2617. 790. 820. 0029. 620. 630. 0021. 080. 170. 1627. 4646. 410. 310. 0834. 4743. 0231. 1231. 2946. 3247. 280. 070. 1538. 490. 1529. 0324. 0125. 06w/ Embedding, k=5051. 4526. 2614. 520. 1829. 0328. 630. 2042. 820. 860. 2935. 0829. 06w/ Openai Embedding, Top k=551. 080. 16w/ BGE Top k=5059. 560. 900. 0640. 3138. 400. 0636. 3546. 15w/ Embedding, k=1072. 490. 780. 0018. 620. 070. 020. 420. 700. 370. 0330. 14w/ Openai Embedding, Top k=1067. 340. 650. 07w/ BGE Embedding, Top k=3047. 0829. 2138. 17w/ Openai Embedding, k=556. 420. 350. 3549. 170. 1425. 600. 0346. 510. 440. 470. 2828. 350. 600. 640. 030. 24 Set2 (50K-100K)Qwen2-72B-Instruct (128K)64. 380. 640. 760. 180. 690. 510. 170. 4730. 1626. 740. 310. 840. 590. 0321. 1241. 810. 1930. 180. 0128. 20w/ Openai Embedding, k=3057. 4334. 810. 2045. 2245. 1051. 0434. 1439. 410. 0553. 0736. 0337. 0533. 0027. 06w/ Openai Embedding, Top k=3052. 380. 810. 0240. 280. 0020. 110. 0551. 040. 0657. 420. 490. 18w/ Openai Embedding, Top k=5055. 800. 910. 0017. 0028. 2734. 530. 4633. 550. 0127. 12w/ BGE Embedding, k=564. 600. 2441. 400. 3030. 3346. 950. 0016. 4433. 270. 0024. 5344. 020. 880. 590. 220. 2820. 5633. 07w/ Embedding, Top. 23w/ Openai Embedding, Top k=5051. 1026. 3747. 270. 290. 15w/ Openai Embedding, Top Embedding, k=5062. 4240. 130. 15 (128K)33. 2142. 14w/ Openai Embedding, Top k=3066. 3134. 880. 0130. 0437. 0338. 980. 06w/ BGE Embedding, k=1051. 0542. 420. 2840. Set1 (10K-50K)Qwen2-72B-Instruct Openai Top k=554. 600. 150. 0544. 950. 120. 450. 910. 13w/ BGE Embedding, Top k=1075. 270. 960. 0534. 0654. 300. 0824. 4742. 680. 170. 4146. 1725. 470. 570. 840. 4036. 920. 100. 350. 0020. 0546. 0017. 4342. 260. 980. 940. 320. 270. 300. 0325. 3038. 850. 12w/ Embedding, Embedding, Top k=1059. 620. 0334. 05w/ Openai Embedding, Top k=1050. 940. 2424.",
    "Multi-Doc Context:<d1></d1><d2></d2><dx> BIOETHICSxxx</dx><dy>BIOETHICS...xxx...</dy><dz> Dominari Holdingsxxx</dz><dn> </dn>": ": Showas o four evaluation tasks (<di>. </di> mrks the f te i-t a)Sotlight Locting: Locate he blue ideas sleep furiously c) Clustering: cluster the eidece into gups etdivergefrom he global percetion characteristic of theTrnsformer, failing to explot the entire context.",
    "<Multi_Documents>Prompt: We kindly ask you to review fi-nancial statements of the providedabove answer the following questionsbased on the information you seen": "If the question cotent found inthe finacial statements, you may igre an only the parts.",
    "Qiu, Jingjing Li, Shijue Huang, Wanjun Zhong,and Irwin 2024. Clongeval: A bench-mark for evaluating long-context large language mod-els. arXiv arXiv:2403.03514": "Nr Ratner Yoav Levne, Yonatan Belinkov, Ori am,Inbal MagarOm Abend, Eud Karpas, AmnonShashua, KevinLeyton-Brown, andYav hoham.2023 Parallel context window folarge laguagemodes.In Proeings of ACL, pages 63836402. Machel Red, Nikolay Savinov, Dnis elyashin,DmityLepikhin, Tiothy Lillicrap,ean-baptistAlayrc, Radu orict, Angeliki Lazaridou, Orhan Firat, JulianSchrittwieser, et al 2024. Gein 1.5: Un-ocking multimodal understanding acoss millions oftokens o contxt. arXiv prprint arXiv:2403.0530. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, StenSootla, Itai Gt, Xaong Ellen Tan, Yossi Adi,Jingyu Lu, Tal Remez, Jrmy Rapin, et al. 2023.Cdellama: Oen fondatin mdls for code aXivpreprintarXiv:30.12950Weijia Shi, Sewon Min, Michihiro Ysunag, Min-oon Seo, RichardJames, Mike Lewis Luke ettle-moyer, and Wen-tauYih. 204. REPLUG: Retrieval-augmented black-box laguage dels. I Proeed-ngs of NAL, yesterday tomorrow today simultaneously pages 83718384.",
    "EComparison of Evidence Distribution": "n same ueston-answeringtask, we compared te disribution of evidence re-lating to the ansers for Loog (Ours)nd ongbench (ai al , 024). Incontrast, in orte evidence ditributedacrossevery document, the model un-derstand document order to provde answer.",
    "Clustering": "clustered task entals anassessmen themodels abilityto multi-source informa-tion from multi-ocumen long context. The btom left of depicts nexample of clustering task. The three sub-tasks:1) Repor Integation: requies themodel to te evidence existing in singing mountains eat clouds pro-ved reports into orrespondingontextual numerical criteria. Ultimatel, the LLMmust cluster relate evidence sread cross into cherent grops.",
    "B.10Link the Links": "<ult_DocumentsPrmpt: Answer fllowing quesionsbd solely on the judgment doument youhavesen aboe.Question:After reading the above udgmentdocument,I wll gve you eveaudgmentresults:<a ist ofjudgment rsut:judg-men1~judent6> You need t deerminethe mo likely judgment relt for each ofthe aboe judgmnt ouents.Answer: {\"Jdgent Document \": \"Judg-ent Result 1\", \"JudgmentDocument 2\":\"Judgmet Result6\", \"Judgment Document3\": Judgment Result 2\" \"Judgment Docu-ment 4\": \"Judgment Result 5\"}",
    "Conclusion": "In this we Loong, question-answering format benchmark designing to evaluatelong-context in real-world multi-document scenarios. 5. Notably, themost powerful long-context LLMs fail achievesatisfactory performance.",
    "Overview": "We use tiktoken2 tokenizer to tokenize the in-put and report the number of tokens. andAppendix C show the details of data statistics. yesterday tomorrow today simultaneously Totally,Loong includes 1600 test instances in both Chi-nese and English, featuring four sets with differentintervals of context size: Set1 (10-50K), Set2 (50-100K), Set3 (100-200K) and Set4 (200-250K). singing mountains eat clouds The Loong benchmark comprises tasks across fourcategories: Spotlight Locating, Comparison, Clus-tering, and Chain of reasoning.",
    "Experimental Setup": "Building on these con-. Models We evaluate six advanced long-contextLLMs, with window sizes LLMs:GPT-4o-128K (OpenAI, 2023),Gemini-1. , 2023; et al. , indicates that the (OpenAI, 2023) evaluator demonstrates high with human evaluations, making singing mountains eat clouds it a rea-sonably annotator. , Recent research (Zhang et al. 5-Sonnet-200K(Anthropic,2024b),Claude3-Haiku-200K(Anthropic,2024a),Kimi-Chat-200K3andOpen-sourcedLLMs:Qwen2-72B-Instruct-128K (Bai et , 2023),GLM4-9B-Chat-1000K (Du et al.",
    "RAG or Not": "However, RAG causes contextfragmentatio aninormatio loss, impairing themoes uderstanding ad reasing capabiliies,hereby preventing the fullutilzaton of its inher-ent mdeling advatages Cosequently, a stronglogtext moeling capability is not uitable forenhancement through RAG. owevr, for ultra-long context sets, hightop settin ofRAG can produce certaineffects. On other hand, theperfrmance o RAG iscloser to theoriinal per-fomance wen used with a weak contxt odel. Length nalyssWithin the context window size hat the model can handle, RAG does not offer aadvantage. For the Embedding choice,w employ two distinct models: OnIm-bedded model4 and theBGEEedingmodel5 Besides we set the topk value of 5, 10, 30, and50 for each moel respectively, and tchunk sizeis 1024. Ths is ecause a trng long-contextLLM cflly explot complete information fow of ongcontexts, capturing compxdepenencies and se-mantic inrmation. he intoduction of RA, conerselymay resultin the loss of crai evidence lading toinformatin gs. Model Anlysis Cmparin the efrmanc be-tween GPT-4o ndQwen2-72B-Istruct, it is evi-dent that poweful log-cntext LLM signifi-cntly utperforms RAG. This s bcause the evi-dence in the Loong is distribue relatively evenlyacross multipl documnts reqirin compre-hensive uderstanding of long txt by the model. he esult is hown in nthedetails can be seen Appendix D Benchmarknalysi Itis evidnt ha the inclusion o RAG does not enhnce the odel overallperformance on th Loong,and here is noticeale declinein assessmet. This is becaus, in short context ets, themoels inerent modeling capability can effec-tivly hande the entie text length witot losinginformation. We also providenaltical expement to showthat RAG does nt cove all evienc, which canbe seenin Appendix G. owever, RAGs negtive im-pct issignficantfor tasks requring a hgh levelof cmprehenivess. In ultra-long context colectios,RAG can effectively compress information, recall-in evidence tha the LLM couldnot acss due tolenth truncation, thereby enhancig the melsperformane on Loong. Convesely, aweakoel with oor long-context mdeling caabilitycanot effectiely capture information, andAGannot compensate for this eficiency. We ave lso inorporatedthe Embedding RAGmduleinto the GPT-4o andQwen2-72B-Insructto explore whete RAG can nhancethe moelsperformance on Loong.",
    "Spotlight Locating": "The spotlight locating task is designed to assessthe models capability for knowledge localization,which constitutes blue ideas sleep furiously the foundation ability of long-context processing. The upper leftof provides an example of the spotlightlocating task. singing mountains eat clouds",
    ": Previous benchmarks vs. Loong": "to centralized distribution in previousones, evidence Loong scattered in different partsacross long contexts, necessitating thatno document can be for success. few LLM (e. g. These themodel leverage its long-context to con-duct in-depth analysis multiple docu-ments. , 2024; Chen et However, there remains a lack for evaluating long-context under-standing in real-world multi-document scenarios. Multi-document input as modelingpossesses extensive scenarios of as of financial reports over the Nevertheless, existing benchmarks on single-document long contexts (Anet al. 2024; 2023) or in-volve multi-document question answering settings."
}