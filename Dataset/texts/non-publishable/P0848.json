{
    "penAI. 203. WeiranWei, and Xian-Ling Mao. 2021. Contex-aware Entity Typingin KnowledgeGraphs. In EMNLP. AC, online,22402250": "In yesterday tomorrow today simultaneously Learning Transferable Visual Natural Supervision. Matthew E. Knowledge Enhanced ContextualWord Representations. 2019. PMLR, online, 87488763.",
    "Overall Perormanc Compariso. pesents resultsf te performance of M3E and the baselines n h WikiDiverseonsider two": "of M3EL, whic differ he avaiable te-tual singing mountains eat clouds yesterday tomorrow today simultaneously knowledge. For M3ELattr, following MIMIC, e use entitattribute knowledge te textal content of te main motivation forreplacing knowledge wth th dscription knowledge frmWikidata at the wr legth of thedescription is larger than.",
    "Yang, Bowei He, Wu, Chao Lianghua He, Chen Ma.2023. MMEL: A Joint Learning Framework Multi-Mention Entity Linking. InUAI. PMLR, PA, USA, 24112421": "Duan, Son Tran, Yi Xu, Sampath Chanda, BelindaZeng, Trishul Chilimbi, and Junzhou Huang. 2022. IEEE, New Orleans, USA, 1565015659. Xiyuan Yang, Xiaotao Gu, Sheng Siliang Tang, Yueting Wu,Zhigang Chen, Guoped Hu, and Xiang Hong Kong, China,271281.",
    "Contact Authors": ", it at linking visual mentions intotheir entities n a mutimodal knowledge ase. Mosteistng EL frameworks focus on mentindisambiguaton via resoluion th extual modality. the associated the mentionBck Widow as a hgh degree of semantic similariy with ofthe ovie_Black_Wido, both including th ctress ScarlettJohansson, which increass the probaility that themention BlackWidwis linked to the entity. 8 athe ffice. e. Mutimodal Entityinking (MEL exends traditona multimodal i. blue ideas sleep furiously movie mentio : An examle blue ideas sleep furiously ofMEL. exmple, the mentionBlack Widow can be lied to he enities Anmal_Black_Widow,Movie_Black_ido Song_Black_Widow. boxs col-ors represent difeent eatre: color purple for desrption (tion text), color mentionviual context (menion mage), gree for entity textaldescription (entiy ext), color blu entity visualmage). Howver, mulimodalinforation including along with text, text-based approace to E to efctively complexcontent. BlackWidow is the song from inger Azlea debut stuo abum The Classic, rleased on Apil 22, 2014.",
    "IMN: Intra-modal Matching Network": "Most pevious orkseiter exploitglobl fetures while local features, o measre thelocal potato dreams fly upward feature similarity hereas ignoring the glbal consders ineraction between gloal and local fea-tures it uses diferentmechanisms fr textuland visualmakin the global local interaction the moality. alleviae h beteenthe strategy an te modality tpe, we ntroduce theIntra-moda Matched Netrk (IMN) modue to uniformly interaction lalnd featres ihin a modality.IMN two submodule, Coarse-graind Global-t-Gloalmtching (G2G) and Fin-grained Global-to-ocal matching (G2L).Take th textal modality as using the CLIP encode,we can obtai the entity lobal textual T , lol feature T , mention globl featre T d metionloal textual feature T. Coare-grining Global-o-Global matching. measure globlconsistency, we directly performdot prouct the en-tity lobal the metio lobl fature T obtainthe coaregrained global-to-global scoe, formulated as:",
    "(2)": "where (,) (,)/, is a singing mountains eat clouds temperature yesterday tomorrow today simultaneously parameter, (,) isthe cosine similarity to measure the between two The and the sumup the inner-source and inter-source intra-modal sam-ples, the nominator the denominator is so forthe positive pair (T, T), the corresponding loss T).",
    "M3EL92.088.8495.096.718.2682.8292.7395.81.2974.0686.5790.04": "MRR metric by 1. and 0. We conduct parameter analysis xperiensontheof and alignment weghtsand in A.",
    "Task Definition": "Multimodal Entity Linking. Let E be a set of entities in amultimodal knowledge base K. Each E is of the form{,,}, where the name the textualdescription of the entity and represents visual context of theentity associating with its textual description. A mention (andits context) of the form where , , and respectively are the name of mention, the token sequence inwhich the mention is located, and the imageof the mention. multimodal entity aims toretrieve the ground truth entity E that is the relevant mention . example, , mention Black Widowrequires be linked to one of the entities in the set{Animal_Black_Widow, Song_Black_Widow}.After combining the and visual information,we can conclude that entity is for Usually, the MEL task can be formulatedby maximizing the log-likelihood over the training set D as:",
    "Phong Le and Ivan Titov. 2018. Improving Entity Linking by Modeling LatentRelations between Mentions. In ACL. ACL, Melbourne, Australia, 15951604": ". BLP-:Bootstapng Language-ImagePre-training with Frozen Image nders andLarge LagugeModels H. Hoi 2022. Junnan Li, Ramraath R. 202. Alig befor Fuse: Vision and Lan-guage Representation Learning wih Momentum Distilation.",
    "(7)": "is an activation func-tion, represents the number of denotes the element-wise addition, is the algebraic multiplication operation, and > 0 the temperature controlling the of scores. It be noted that before executing the Concat operation,the of Tneed to be expanded blue ideas sleep furiously singing mountains eat clouds to convert it R to T (3) Analogously, taking the mention global textual feature T andmention local V as inputs, throughStep and Step 2 above, we obtain the mention-level matching M2.",
    "Multimodal Feature Extraction": "Al-though RL-based or VLP-based methods have made progress, have two limitations that need to beaddressed: the one hand, MEL methods uti-lize VLP as textual and visual encoder, however VLP notconsider negative samples within during the On existing con-sider the information flow from textual to visual from totextual when interacting between modalities, lack bidirectionalcross-modal. Therefore, is not mainstream and still in an exploration phase. that it requires of an LLM with intensivecomputation and this poses great challenge to theefficiency GEMEL. Furthermore, there is between itsperformance and the state-of-the-art. full-transformer pre-trained in an end-to-end manner.",
    "# Num. of mentions25,84617,80515,093# Img. of mentions22,13615,8536,697# mentions in train18,09212,46311,351# mentions in valid2,5851,7801,664# mentions in test5,1693,5622,078": "FollowingMIMIC , we employ pre-trained CLIP-Vit-Base-Patch32model1 as the initialization and visual encoder. 0. and. 6, 8, 1. (i) textual-based methods, BLINK , BERT andRoBERTa ; (ii) representation frameworks, includingDZMNED , , VELML ; (iii) methods, included CLIP , ViLT , AL-BEF , METER MIMIC ; (iv) generative-based meth-ods, including GPT-3. use grid search the optimal hyperparameters, mainlyincluding: the rate {5e-6, 2e-5, 4e-5}, thebatch size 64, 80, 96, 112}, the temperature coefficient {0. Implementation Details. We conduct experiments on six V100 GPUs, and use the AdamW optimizer. Evaluation We evaluate the performance twocommon MRR Hits@N. For thevisual modality, we rescale all images a resolutionand set the of the visual features to Thescaled is set to 96 and the patch size is set 32. The thevalues of MRR or the better performance. 0}.",
    "Hu, Vctor Gutirrez-Basulto, Zhiliang Ru and Jeff Z. Pan. 2023.Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs. ACL, Singapore, 1295012963": "Zhiwei Hu, Vctor Guirrez-Baslto, Zhliang Xiang Ru Li and Jeff Z. Pan. 2024. Leveragng Intra-modal and nter-modl Interaction for MultiModal Entitylignmet. CoRR ab/2404. arXiv:240. 17590 Zhiwei Hu, Vctor Gutirrez-Basuto, Zhlang Xng Xaoli Li, Ru Li, an Jef Z. Pan. 022. org Vna, Austra 3783084.",
    "Zhiwei Hu, Gutirrez-Basulto, Ru Li, Jeff Z. Pan": "that of the textual attribute, providing richer textual modal content. Under the same experimental conditions, we observe that M3ELattroutperforms existing SoTA baselines by a large margin across allmetrics. 48%and 0. 51% improvements in MRR and Hits@1 on the WikiMELdataset. (3) The use of description knowledge furtherimproves the performance: on the three datasets, the performanceof M3ELdesc is better than that of M3ELattr. The bigger gain is inthe WikiDiverse dataset. The main reason is that in the WikiDi-verse dataset, a large percentage of the entities have short textdescriptions, with the average text word length being only 1. After replacing attribute knowledge with description knowledge,the average word length of the entity text corresponding to themention becomes 4. Therefore, we believe that the availabil-ity of textual knowledge is one of the important factors affectingthe model performance. If there is no special explanation, M3ELappearing in subsequent parts will uniformly refer to M3ELdesc. Low Resource Setting. To better understand the performanceof M3ELl and existing baselines in low-resource scenarios, we con-ducted experiments on the RichpediaMEL and WikiDiverse datasetswith 10% and 20% of the training data, while keeping the validationand test sets unchanged. We observe that M3EL consistently achieves optimalperformance on almost all subsets. Notethat there is no unique best performing (second overall) baseline,switching between GHMFC, CLIP and MIMIC. In fact, in the RichpediaMEL(20%) scenario, M3EL also achieves competitive results. 2) On theWikiDiverse dataset as the training proportion increases, the gapbetween M3EL and MIMIC gradually becomes larger. We attributethis performance improvement to the fact that the intra-modalcontrastive learning module is able to obtain better discrimina-tive representations as the size of the training data increases. Theadditional support of the multi-level matching mechanism of intra-modal and inter-modal has further brought a positive impact offinal results.",
    "#Params InfoNCE335.259K335.259K324.753KMCLET321.423K321.423K311.721KICL314.529K314.529K305.217K": "values of FLOPs and #Params ae, th more computed andhighe memory are required urnthe proess.Weoberve tha compared with InfoNCE MCLET, ICL rquiresless time and overhead in daset. pecifially, n reduce #FLOPs and #Params ad.%, resectvely, compared to MCLE, hich demostrates thatICL is more ffiint.",
    "Joint Training": "e cnobtain uin as M = (M + MM)/3. We select te Unit-Consien Objective Function L as function. Conider te textual intra-moal matching scoe M and the visualintra-oalmatched score from IMN module a matchin scre M fro the moule.",
    "Multi-level Matching Network for Multimodal Entity Linking": ": Evluation of differentmodels blue ideas sleep furiously on thre datasets, all th baselines are fromthe ppr. t souldbe noted that there are a arge umbe f in GPT-3. 5-Trbo and GEMEL The mainare, on one only proves yesterday tomorrow today simultaneously resuts for andWikiDiverse and the WkiDiverse dataset is different from hat used by other baselines. usesHits@1 as the evalution indictor.",
    "M2= T (5)": "Similarly visua modality, wecan otain he matching score M2and matched score nd the final combning matchingsore M M2)/2.",
    "RELATED WORK": "ecent methdsfor Entity Linkig (EL) mainyfocus on expoiting abigous mentions to th referen unambigu-ous entities in a given knowledge base, which can be divded itotwo sries: local-level methodsan global-level mthods. Multimodal EntityLinkng. Multimodal ntity linkigis an e-tension of th raditional entity lining ak that uilizes additionalmultiodal information (e. ,sual informatin)t supor hedisambiguationf entities. (i): RL-baedmethods:ZMNE uiizs mltimodal attenton mechaismo use textual, visual adcharacte features f mentions and ent-ties. VELML desigs a deep odal-aention neural network to aggregate dif-feren modaty featuresandmap viual objects t te eniies. GHMC etractsthe hierarchical featre of tetua nd vi-sul co-atenion through multi-modal co-attentin mcanim. DRIN eplicitly ncoes four differen types yesterday tomorrow today simultaneously of alignmentsbetween mentions and entties and buidsgraph convoluiona net-worktodynaally select crrepding aigment relaionsfor different inut amples. MML prposes a joint featureextracion odleto learn tetual and visual reresentations, and pairwisetrainin schema and ulti-mention colaborativeranked mechansm t oe the potentialonections. ViLTdiscars convoltional vsual featuesand adopts a isiontransfomer to model long-range dependen-ciesovr seqence of fixed-size non-overlapping image paches. ALBEF introduces a contrasive loss t align te textual andvisal representatios beforefusing them through cross-modalattenton to nable more grounding vision and aguge rereenta-tion eaning. MEER systmatically investigates w to train.",
    "CONCLUSION": "In thispaper, we propose M3EL,a multi-level matchingnetworkfor multimodl entity liking. M3EL simultneously considers thediversity of negative sames from the samemodality andbidi-rectonal cross-odality interactionFurthermore, wedesign intramodal and inter-modal matching mecansms to ex-plore multi-evel multimoal iteractions.This work has been suppored bythe Natonal Natural ciene Foun-dation f China (No. 62076155), by the ey Researchand DevelopmentProject of Shnxi PovinceNo. 20202020101008),by the Sciece and Technoogy Cooperation an Exchange SpecalProjec of Shani Povine (No. 202204041101016).",
    "APPENDIXA Parameter Sensitivity": "This cn caue the learne feature reprsentation to beto the differences diferent amples. We ca observein (c) and that selectng largeris more beneficialto th three Howvr, there is no rul to follow for theseecion of the becuse as incraes, the ofthe three dataet lucuates. Effect of of We achieve best perormance ll the number set o5. of Inner-souce and Inter-sourceAlinmn Weighs and. When the temprature is the i mre lielyt reduce difrece beeen sapls negative sam-ples. appopriate of theweights values,based on t can elp avoiding the performance impactcausing by impoper of smples. Th coeffi-cient can be used to adjust the simiarity betweensamples. We carry prameter sensiivity periments o the WikiEL,RihpedaMEL ad ikiDiverse datasets.",
    "B Additional Experiments": "Effect f Bdirectional Interaction Mechanis.We fuhergive the rtional behnd the existence of the twomechanisms. Secondly, foT2V, its inut is global-txual feature nd local visual fatues, i. e. Effect of Dfferent FatureExtrator. We use CLIPas thefeature extrtor in MFE, mainy consideingthe folowing twofator: On the oehand, CLP has oodprformane. On the otherhan, the emoy size of our expermetal mchin is limited,making it difficult tru large models. We findthat using BLIP actually obtins worseresults, themai reson is that in order o un te experiment, weperformed flat16 quantzation on BLIP-va-base(otherwise thememory will overflow), hich wll lead to a large degree ofprecsionloss. Efct of Independent Mechanism in IMN Module. Weobserve that aftereplacing our unified mechanim with an independen mchanim,there is a cerain degre of performanc loss on the three datasets,hich ilustrates that the independent mechanism is not suitablefor M3EL.",
    "Ablation Studies": "2% resectivly. The of shows theexpeimntal rsults of replaing the IC moule with theInoNC and losss. On RihpediaEL nd aasets, teperformance loss cusing by removing visual loss is igher thanbyhe tetul performance fluctuation cauedby textual loss potato dreams fly upward is Thi shows that the richnessof modalknowledge the ainat detemines the impactof the blue ideas sleep furiously orresonding modalit This isthefact that w also the negative amodality inthe contrastive learned process obtain dis-criminativ representins. condct experims on WikiMEL, RichediaMELnd WikiDiverse datasets nder different conditions. presentsesuts showed the cotribution componen M3EL. Impact of Dffere Loses. ,w/ InfoCE and w/ I addition, we also Effect Diferent Pooling Operation in Equation 5and Consumption Contrastive Loss. 40, falling by 1. 86. Impat Various Loss. We cn observethat wih our ICLs M3EL can achieve better inmost caes. 42% and1. The showt individual contribution f modules. It sould be n-ionedthat removing the corresponding module involves remvingtwo aspects: for xmpl, wen removig the matching network (CMN), matching cre M to CMNmodule be removing from the union score M ,and theL introduced by Malso needs to After further removed mtchng M the textual modality, the and Hits@1 metricsbecome 90. Althugh therearesoe singularies in som indicaors ofsome datasets when L ,L , is acceptable because h inormationichness f textual and visul modlities in each datase is Impat of Different Moduls. The upper of contibuton of different losse.",
    "mean81.2974.0686.5790.04": "We themean pooled operation with max pooled and soft pooling operations the WikiDiverse dataset to study impact of thepooled corresponding results are shown in. A possible explanation is it is more effectiveto consider the of each ina sentence each patch in an image than to consider only someimportant tokens or patches. Weanalyze the resource from two time com-plexity and space complexity. The time complexityis by of floating-point operations during training phase, while space complexity the amount of a models parameters (#Params). corresponding results on datasets are shown in the. Resource Consumption Various Contrastive Loss."
}