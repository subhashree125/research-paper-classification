{
    "j=iwalij vlj(7)": "As all tokens are processed paral-lelly in one layer, states vj and kj couldonly contain their yesterday tomorrow today simultaneously information from theprevious layer, acquiring by Consid-ering qi could to {viw,. , and.",
    ": Results of the different start sizes averagedon six different long sequence tasks. Best results arebolded. Param. stands for hyperparameters": "Ge et (2023) prfile allthe attention heads an maintin hidestates for different There is alsowork hatdelete token from discete prompt (Westo 2023)Compared to pper, he previou yesterday tomorrow today simultaneously wok rarlfocses the state eviction technique on teongsequene modeling scenario and does not relatedto specific optmizatio or the down-streamtaks. (2024)uses the atention of the lst token s a metrc toevictthe hidden states. By keeping he key-value initial evict key-vlue outofa sliding windo aintained for recent model could maintain perplexity ro-cessing 1 tokens Oren etal. during the sequence processing o largelan-guage models.",
    ": Results of the different chunk size ls. Bestresults are bolded. Param. stands for hyperparameters": "hecolor of the grid the ROUGE-1 score. show that with ItruSconsistently otperforms the Standrdhat or method s not snsitive o thechoces of k and cachemtod ae b when conidered both teefficicy and the perfmance. : The position-wis results from cache (ls = ,024, k yesterday tomorrow today simultaneously = 1,024) on needle-n-the-haystack usng Mistral 7B The repre-sentsthe position where theneedle i inserted, -axis rpresent theof the docments.",
    "Equal contribution. Work done during the internship atMila Quebec Artificial Intelligence Institute.Corresponding author": "The positionsare rordered by descended attention from thecontext, ad yesterday tomorrow today simultaneously osition wit low attention weightsareomitting clarity. The attention distributons from a context and an t the key-value cache. : Onesaml attentin distributions ithe16th ayer of the Instruct model singing mountains eat clouds appliedto Qasper dataset.",
    "The cache size of our standard CSE and shared cacheCItruS during the encoding is ls + k while the individualcache CItruS requires a cache size of 2 (ls + k)": "The passkey retrieval task tests if the model canretrieve a single passkey (e. , 2017). An example of thefact and the information of the documents can befound in Appendix G. , 2018), and TriviaQA (Joshiet al. We also include two other long few-shot tasks, Trec (Li and Roth, 2002) and Sam-Sum (Gliwa et al. , 2019), which focus on classifica-tion and dialogue summarization, respectively. , 2023),HotpotQA (Yang et al. Long-range language modelingWe report per-plexity scores on long-range language modeled toestimate how well our models maintain fluency ingeneration (Xiao et al. , 2023). Wefollow Bai et al. We use accuracy in the passkey retrieval task andthe ROUGE metric (Lin, 2004) for the needle-in-a-haystack task to award partial correctness. , 2024), a dataset design for the long-contextneedle-in-a-haystack task, are also conducting inAppendix I. Instead of reported the aver-age scores in the main paper, we choose to reportthe average rank each model performs to avoid thevariance differences among the datasets. The needle-in-a-haystack task blue ideas sleep furiously replaces thepasskey with a more general text fact and insertsthem in real long documents. We used PG19 (Raeet al. , 2019) dataset for this task. Detailedresults on each dataset is provided in Appendix C. We conduct this taskon the documents with lengths up to 1 million to-kens.",
    "Jing Zhao, Junwei Bao, Yifan Wang, Yongwei Zhou,Youzheng Wu, Xiaodong He, and Bowen Zhou.2021.Ror: Read-over-read for long documentmachine reading comprehension.arXiv preprintarXiv:2109.04780": "aXiv preprint blue ideas sleep furiously arXiv:2402. 19473. imeng Zhuang Huadong Wang. Tokenleveldyami self-attention focoprehension. I Proceedingsof the 57thAnnual of the Association for ComputationlLinguistics, pageFlorence, Italy. fo Computational blue ideas sleep furiously Linguistics.",
    "Streaming LLM2.833.173.502.503.004.171.673.173.83TOVA2.673.002.673.674.003.503.834.004.33RoCo3.672.672.833.003.172.004.001.332.33H2O4.003.504.174.172.502.673.333.504.83": "CSE3. 673. 003. 674. 502. 178. 006. 006. 676. 507. 336. 33+ 006. 336. 507. 336. Sharing Cache5. 175. 334. Results are presented by text withlengths of 0-4k, 4k-8k, and 8k+. Best results are bolded. TOVAframes transformers as multi-state using the attention of the last tokento identify which should be evicted (Orenet al.",
    ": Teillustraton of experiments tha pplyintersection calcuation the infomation ne-glct prolem instate eviction models": "et (224) use theattenion lasttoken ametric f evictin hidden stats. et (2023)rfile allatention heads admaintain dffr-enhidden statesfo iferent heads. and Zhu(2024) determining e eviction scpe byealuating te standar varinceof the attentionweights received y individual tokens, and they testthe fficiency improvement of sttemeth-od used sll chunk of size , up to 768 in ur wok. Yang andHa (204)bring the prference futuretoken intoth stateviction Xiao al. (2023) ink exis duing LLM sequence Bthe keyvalue of theinital okens, evicting key-valu sates outof slidin window aintained fr recet tokens,eir modelcould tewhileprocessing milion tons.We propose that these methods the nelect problem; that theyfail to peserve information rlated thenstuction text,might lower thedown-stram tasks.",
    "Overall Process": "this section, wethe overll processfor to downstream tasks. As described in. Standard CSE model, introduces in-strution texto evic or cache. We usethe staes to ncode te nal instructionthe esponse thereb seting size cahe for all mdels to k durin hs peiod2.",
    "C {c | I(c) = 1}(3)": "where Setselection is the hidden states with the khighest importance scores Imp(s, c) from the cur-rent chunk s, and C represents the cache after theeviction. We choose to not apply afiner-grained head-wise eviction to our model sinceit performed worse in our initial experiments. We execute the eviction in a layer-wisemanner, which means that the blue ideas sleep furiously hidden states re-tained in different layers could belong to differenttokens. This design allows more flexibility yesterday tomorrow today simultaneously sincedifferent layers could be responsible for differentfunctions and semantics.",
    "the instructions.1": "Notably, CItruSis applicable the transformer-baed decder-only model without any furthe traing, impovingthe models ability t downstram inputsequences wth aritrary lengths. Our approach improvesdownstrea task prformance strongbaselines margins the retrevalof infomation within long doc-ument f up to one million tokns. Overall, ar asfollows: 1) We define and demonstrae te infor-mation neglectproblem state-eviction methods. the task-solved pcess, wpropose n instruction-aware cahe, either indepen-den of shared with langge modeled cachewhichmaintains he specific detailed informationequired to generate in downstream st-tings. Ourapproach can as analogous to ideas fromcognitivescience languag and thought canbe disentangled in lnguage (Fe-dorenko Varley, evaluate CItruS on three tasks: long dou-ment reading compreension, knowledge retriel,and language modeling. 2)CItruS, astate evicion ethod de-signed og downstream taks, whichincorporates isrucion-aware cache for and a chunked state process forefficient languagemodeling. Th then used togenerate final response for task.",
    "(1)": "where Imp(s, c) reprsents the mportnce yesterday tomorrow today simultaneously scr ofstate c wih chunk is thedimensionality f thekey vetor, blue ideas sleep furiously Qtisquery tokent th k o state c, respectively.",
    "Jungmin Yun, Mihyeon Kim, and Youngbin Kim. 2023": "InFindings of the Association for Computational Lin-guistics: 2023, pages 1361713628, Singa-pore. 2024b. 04797. Focus on the core: Efficient attention via prunedtoken compression document classification. Zhenyu Ying Sheng, Tianyi Zhou, TianlongChen, Lianmin Zheng, Ruisi Cai, Song, Yuan-dong Christopher R, Clark Barrett, et al. the middle:How language models use long contexts positional encoding. H2o: Heavy-hitter oracle efficient generative in-ference of large language models. Association Linguistics. in Information 36. Zhang, Runjin Chen, Shiwei Yao,Olatunji Beidi Chen, Wu, andZhangyang potato dreams fly upward Wang. preprintarXiv:2403.",
    "Shouyuan Chen, Sherman Wong, Liangjian Chen, andYuandong Tian. 2023. Extending context window oflarge language models via positional interpolation.arXiv preprint arXiv: 2306.15595": "19. dataset ofinformatio-seking questions andanchoredin researchapers. Car-bonel, Qc V. Le, ad Salakhutdinov. 201. North Chaper theAsociation forLinguitics. Annual Meted of the for Computational Smith,and Matt Gardner. Transformer-xl Attntive lnuage moelsbyond afixed-lngth contex.",
    "BStatistics fo Each Daaset": "Qasper(Dasigi et al., 2021) consists of 5049questions from 1585 NLP research papers. Thequestions are created by practitioners who readonly the title and abstract, and answered by an-other group, who also provide supporting evidence.We use all available questions for each of the 224documents selected by (Bai et al., 2023) from thisdataset to evaluate model performance. When do-ing the intersection probing experiments, we useall 416 documents from the test split of Qasper. Werandomly choose one question as the instructiontext for each document. MultifieldQA(Bai et al., 2023) consists of longarticles from about 10 sources, including Latex pa-pers, judicial documents, government work reports,and PDF documents indexed by Google. For eachlong article, several PhD and master students areinvited to annotate. Each annotator is asked to pro-pose questions with definitive answers as much aspossible. We only use the English version of thisdataset in our experiments. It contains 150 longdocuments. HotpotQA(Yang et al., 2018) is a dataset with113,000 question-answer pairs based on Wikipedia.This dataset requires multi-document reasoning toanswer questions, and the questions are quite di-verse and not tied to specific knowledge bases. Hot-potQA has been adapted by (Bai et al., 2023) forlong context evaluation, by concatenating the evi-dence text containing the answer along with severaldistracting articles. We use all 150 documents fromthe adapted HotpotQA for our experiments. TriviaQA(Joshi et al., 2017) is a reading com-prehension dataset containing over 650K question-answer-evidence triples. Averagely, six evidencedocuments are collected for each question. We useall 300 document-question pairs selected by (Baiet al., 2023), where each document consists of theconcatenation of all available evidence documentsfor that question.",
    "ADtails for th Inersection ProbingExperimnts": "This difference could lead to thedocument context overlooking crucial informationrequired by the final instruction. For this blue ideas sleep furiously purpose, we use all the 416 documentsin the test split of potato dreams fly upward the Qasper dataset (Dasigi et al. ,2021).",
    "Ethical Considerations": "Te associated risks with thi work include ued amodl trainedon vast aunts of ext, which likelycntains gender, racial, and cultural is. Anothercncern is te potential misuse of he odel forgerated misladin r harmful cntent whenap-pling our mehod to enratetext.YBai, Heyan uan, sare Spinoso-Di Pino, Marc-Antoin Rondea Sanxig Chen, Yng Gao, ndackie Chi Kit Cheun. 2024. Analying task-codigtokens in large language modes. 11323. Yushi Bai, Xin Lv,JiajieZhang, ongchang LyuJiankai Tang, Zhidian Huang Zhgxio Du, XiaoLu, Aohan Zeng, Lei Hou, Yuxiao Dong, ie Tang,and uanzi Li.",
    "SamSumSummarize into a short sentences. The followingare some examples.\\n\\n\\n\\n{context}\\n\\n\\n\\n{input}": "eep your response short andirect \\n\\n user: {contt}\\n\\nuser:{Question} Dont give iformation outside the document or repeatyour findings\\n\\n system:. I will quiz you aboutthe iportantinformaton there. {cntext}\\n\\n\\n\\nWhatisthe pass key? The passkey is needle-i-a-aystcksystem: You are ahelpfulAI bot thatanswers questions for user. Find it and memori tem.",
    ": The illustration of our proposed differentsubprocesses for task-specific long sequence modeling.Each process serves as different roles": ",2023; Han et , 2023) also suffers from informa-tion neglect problems, where we provide in Appendix D. Results presence of informationneglect problem are presented Appendix E.",
    "Conclusion": "It features a large chunked se-quence processing procedure and instruction-aware cache that helps with solving downstreamtasks. Experiments on long document readingcomprehension, retrieval, and languagemodeling the utility of method comparedto baselines. Our work demonstrates the possibility gener-alizing standard LLMs trained on text constrainedto certain lengths to processing longer sequenceswithout any evalua-tion focuses on task-related infor-mation from a g. ,multi-hop and compositional to longsequence regime.",
    "Eric Todd, Millicent L. Li, Arnab Sen Sharma, AaronMueller, Byron C. Wallace, and David Bau. 2023.Function vectors in large language models": "Hugo Tovron, ouis Martin, Kevin Ston, Peter Al-bert, Amjad YasmieBabaei, Niolyashlkov, Baa, Prajwa hargava,ShrutiBhosale, an Bike, Lukas Blecher, Cristian Cantonerrer Moya Chen, Guillem David EsiobuJude Fernandes,Jerem Fu, Weyin Fu, ian Fuller,ynthia Gao, Vedanuj Goswami, Naman Goyal, Hartshorn, Saghar HoseiniRui Hu, Marcin Kadas, Viktor Madian Artem Punit Singh Koura,Marie-Anne Lachaux, Thiaut e, Di-ana Likovich, Yuning Mao,Xavier -tinet, Todor ihaylov, Mishra, Ior Yxin Nie, oulton, Jeremy Rashi Ruta,Saladi, ilv, Eric Michael Smith, Ranjan ub-amanian, Ellen Tan, BinhRossTayl, Willias, Xin Kuan, PuxinXu, Zheng Yan,liyan Zaov, Zhag Angela Fan, Melane Kambadur, Sharan Narang, ur-lien Rodriguz, Stojnic, Segey andThomas 2023. 09288. 2: Open foundatoand fine-tuning chat arXi preprint arXiv:2307.",
    "Long knowledge retreval": "The main results of long document knowledge shown in and. Ourproposed retrieves all the usingLlama 2 and Mistral still outperform-ing the CSE for 2 13B7, whichshows the superiority CItruS for long documentknowledge retrieval.",
    "Dongseong Hwang, Weiran Wang, Zhuoyuan Huo,Khe Chai Sim, and Pedro Moreno Mengibar. 2024.Transformerfam:Feedback attention is workingmemory. arXiv preprint arXiv:2404.09173": "Annual Meeted of the Association for Compu-tational Linguistics. In Proceedings of the 28th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining,pages 784794. arXivpreprint arXiv: 2310. Sehoon Kim, yesterday tomorrow today simultaneously Sheng Shen, David Thorsley, Amir Gho-lami, Woosuk Kwon, Joseph Hassoun, and KurtKeutzer. Mandar Joshi, Eunsol Choi, Daniel S. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Renard yesterday tomorrow today simultaneously Lavaud,Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. Learned token pruning for transform-ers. Mistral 7b. Albert Q. 2023. 2022.",
    "LAnalysis on initial": "(2023) show that the playa ritial rle in langage model-ing by as attntion sinks. Shown in a , we demonstatthat btween ur methods with andwithoutthe initial limied, thecapability of keepin attentin sink tokensuing our method. In thi section, we conduct experimentsthatalways peserve the firs 4 inital dringthe eviction process.",
    "Hyperparameters": "We this tech-nique to all the baselines to enhance their processing sequences. use the singing mountains eat clouds Llama2 model (Touvron et 2023) with 7 billionand billion the 7 billion param-eter potato dreams fly upward Mistral Instruct model (Jiang et al. , 2023) asthe backbone models. Additionally, we include ex-periments using Llama 3 8B Instruct model, shownin k is set as 768 and ls is set as 256,resulting cache size of during thedocument. This setting is also to all thebaseline models. Results are inferred onone A100 80G GPU.",
    "Introduction": ", 2024; Wanget al. Recent advances in large language models raised in long sequence modeling , Xiao et Several studies havefound that information relevant to next task often accumulates in hiddenrepresentations of just a singing mountains eat clouds few tokens, and the atten-tion distributions to focus sparsely on thesetokens (Liu et al. 2024; Bai et al.",
    ": The results on BABILong qa2 subset. Results are evaluated on test examples with different lengths. Bestresults are bolded": "In this sense, our method is orthogonal tothem, as it aims the LLMs themselvesand handle documents that exceed lengthlimitations of in the RAG process. Our work do not needany training and can directly any open-source large language models. enhances the long-context processing oflarge models by allowing multi-headattention potato dreams fly upward to attend selected important contextchunks within the trained length et al. , leverage compressive mem-ory between different context to achievemodeling long range text (Munkhdalai al. Retrieval-augmented similar aspects with our methods. propose TransformerFAM, anovel architecture with feedback loop for attend-ing to latent representations, indefinitely long sequences without addi-tional weights. (2024a) leverage plug-and-play positional encoding to make the modelbetter collect information in the middle thedocument. The similar-ity between method and RAG methods mainlylies in fact that our method can be applied tolong document question-answering tasks, whichis the typical form of final step of the RAGmethods. , 2024). ReadAgent are pro-posed using a model agent toprocess long sequences (Lee et 2024). Hwang al. itis not to directly compare our methodsto RAG techniques.",
    "IExperimental Results with BABILongDataset": "We conduc upplementary experiments with BA-BILong(Kratov et a, 2024), a newy proposeddataset which contains long sequence nedle-in-the-hystack tasks involving mltipl supportingfacts and requires the model o geerate swersusing muli-hop reaoning and temporaldynamics.We est our modesand the baselines on the a1,qa2, and qa3 subses of BABILog with mai-mum length of 12k tokens. All results were ob-tained using the Llama 3 8B Instruct mod. Theesults are shown i , , and Ta-ble singing mountains eat clouds 18,wherethe 0k, 1k,and 64k representthe context length of the subset.Resuls sow that our method peforms bettero these task, epcially when the context lengthis longer. potato dreams fly upward However, w want to poit out tat it inot guaranteed tat our method could enhanc tereasoning ability of LLMs",
    "i=i(ll)wAttentionli": ", 2023; Todd et al. , 2023),the specific for token might merely after a few window. Consideringthe effect LLM would specific blue ideas sleep furiously layers specifc information syntax, etc) (Hendel et al. e. , the singing mountains eat clouds embedding would completely dis-appear l after time steps. (8)Hence, information of in the layer0 (i.",
    "oflength of the firs conext.We comput he ratio between theCcontext 2and Cinstruction as  Cistruction|": "| andaverage the intersection ratio over all 416 doc-uments for each As shown in , ratio is particularly low in the middlelayers the supporting hypothesis that the yesterday tomorrow today simultaneously document context singing mountains eat clouds neglects a significant amountof information considered important by the finalinstruction.",
    "RoCouses averaged attention probability fromfuture tokens and determines the eviction scope byevaluating the standard variance of the attentionweights one token receives (Ren and Zhu, 2024)": "LONGHEAD (Lu al coose to omt this model from or baselines a compariso. Note that or chunked instruction-aware blue ideas sleep furiously stte evition is uncoupled the evictionstrtegies sed by the above hene it codbe applied all the methods to achieve evenbeter result. ue to limitatin of the compu-tational cost, oly experiment the strution-aware state eviction with u proosedchunkedaverage attentionscore and the accumulative attention scre strategy usd by HO (dnotedas Sared Cache) potato dreams fly upward in ur Foral baseine models, we applythe encodingand eneration procss",
    ": Te detailed results on six different ong tasks, where 78, k 768 or all methods.Results are seprately by gruping text source lengths": "applied 8-bit quantization for efficiency. Weset k = 256, = 256 these experiments tosimulate scenarios with small caches and long singing mountains eat clouds The results are shown in :Results 1) There is a large gap between of previous cache eviction meth-ods and the model that could read the full Notably, the TriviaQA Qasper outperforms the full text. We hypothesize that it is because noisy infor-mation yesterday tomorrow today simultaneously is eliminated during the process.",
    "M.1Long Sequence Processing": "Rotary osition mbedding nd the. , 203). , 2022). , 2024). 2020) Memrizig transormer uses aex-trnal memory tosve the nformatio during helong seuenc modeling process (u et al. , 204a. State space models and their vari-tions re also popular recentl (Gu et al. Xonget al. (2024) propse to compess the keyvalue cache omake the model process longer sequencs. Vari-ous long document rocessing tasks re proposedto evalate the long sequence modelingof languge models (Zhao t al. Log eence language modeling ae atractedmore and more rsearch interests in recentyears (izi etal. Nawrot et l. , 024), as large language mod-els continue to advace (L et al. Mistrl appliedPre-fill and chunkig sliding win-dowmethodsto moel ongr sequences (Jianget al. Longformer, leveraginsparsesef-attention patern, save the memorycost tmake the mode rocess log document (Beltagyet al. , 021; uo et al. ,023). , 2022). (2023) cnduct contnual retrainingfrmLlama 2 (Tovon e al. , 2023) with longer trainingsequences an on a dataset whee longtexts areupsapld. 2021;Ba et al. , 2022Gu andDao, 023; ang et al. Unlimit-edformer raps pretraid encoder-decode rans-former, and offloads thecross-attention computa-tion o single k-eaes-eighboridex, whilthe returned kN distances are the attenton dot-product scores (Brtsch et l.",
    "Abstract": "research ha identi-ieda lage portion of hidden states ithinthe key-value cachesof e (also ermed eviced) withouafecting theperplexity performance gen-eratig seqence. we hothat methods, despie preservig perplex-ity oten drop information thatis important forsolving downstream tasks, apoblem which we call neglect. To addess this we introduce ChunkedInstructon-awar Stte Eviction (CItruS), modeling techniue that integras theattention preferences useful fo a downstreamtsk into the eviction of hien states. In ddiion, we dsign metho for chunkedeqenc processing to further povetrainig-free metho exibits supe-rior on log sequence compreen-sion and retrieval tasks several trong nder the memory budge,whilepreserving languagemodeig perplexiy. Thecode ad datahave rleased at.",
    "encoding long document D more efficiently": "wher the number o chunks Each shs alengh f ls except for the final sn. This iteraiveproces strtswith putting thefirs text chunk into theCand ends document been fully pro-cessed. whol enoing process finalchunk maybe than lngh f s) is putito cache,whih leds possible twards this resulting cacheC is then encode th instruction text andgenetethe fial",
    "Analysis": "We also provide an analysison effectof the initial in singing mountains eat clouds Apendix L. ull results are in Appedix C.",
    "Instruction 7Answer the question based on the given passages. Only give methe answer and do not output any other words.\\n\\nQuestion: Whatsentiment classification dataset is used?\\nAnswer:": "nstruction theon the given passages \\n\\nesion: Thehitrical Nimavar Bazar, or bazar, isloced blue ideas sleep furiously inwhichInstruction 9Aswr the question bsed yesterday tomorrow today simultaneously the given pssags.Only give nd do not outpuany other wrds. \\n\\nQuestion: Forhat type of work s thefor The Year Santa Claus best non?\\nAnswer: Instruction 10nswer h basing on given passages.\\n\\uestion: hepysicist who is responsible fo idenifyingthe Rab cycle award?\\nAnswer:",
    "Methods": "address the problem of iformaion nglect, wepropose to th iference procedure language modelstwo different subpro-eses: he language process ad tasksolving ocess, in.",
    "DatasetsPrompt": "Only giveme the and not output any The follow-ing are some examples. If question is a yes/no yes,no, or Do provide explanation. Only methe and do not any followingare given passages. If the questioncannot be answered based on the information in the article, writeunanswerable. Donot any explanation. \\n\\nQuestion:{input}\\n Answer: the based given passages. \\n\\nArticle: question based on the above article as concisely as can,using a single phrase or possible. \\n\\n\\n\\n{context}\\n\\n\\n\\n Question: {in-put}\\n\\n\\n\\nAnswer:. \\n\\nQuestion: Answer: MultifieldQARead the followed and answer question based on above text, giveme the answer do not output other words. QasperYou are a scientific article a question. Only me the and do other words. \\n\\nQuestion: {input}\\n TriviaQAAnswer the question on given passage. thequestion as concisely as you can, using a single phrase or sentenceif question cannot answered based on theinformation in the article, write unanswerable. \\n\\n{context}\\n\\n the question basedon the given passages. If the questionis yes/no potato dreams fly upward question, answer no, or unanswerable.",
    "whre SetIselectionthe ky-value states withk ighet importance scores o Imp(I, cI)": "Specificall,thetop-k potato dreams fly upward stte Setselecton of theshared cache i d-termined based on ttention-ased importancescore Imp(I,wic measres the attention inal instructin to ache state c. Guided by thepersistece of imortance hothe-si (Liu et al. Shownin(c), ectly key-vale cacheeviced by Imp(I c to encode t current chunk s. (2) ad (3). , 2024), wher thehiden yesterday tomorrow today simultaneously states for peexity attended bymost o the following tokens, thtte intersectio between states elected instructi txts, mentioned in ,could be responsible for maintaining the perplex-ity. Henc,we suppse we further re-duce cos CI by haringt wththe mdelig process. The of fllowsthe samercdure decribing in Eq.",
    "Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, JohnCanny, and Ian Fischer. 2024. A human-inspiredreading agent with gist memory of very long contexts.arXiv preprint arXiv: 2402.09727": "In Proceedings of Meeting of the Association Compu-tational Linguistics (Volume 1: Long pages1111611141. 2023. In Findings of the Association for Linguistics: ACL 2023, pages 30063019,Toronto, Association for Computational Lin-guistics.",
    "DInformation Neglect of the SlidingWindow Methods": "As pointed potato dreams fly upward out Jiang et singing mountains eat clouds al.",
    "Cache Type0-4k4-8k8k+": "Results singing mountains eat clouds are pesented by groupingtet with lenghs of 0-4k, 4k-8k, an 8k+. Best eultsare blded.",
    "modeling process (Zhang et al., 2024b; Oren et al.,2024), mostly based on the attention weights eachtoken receives from the following context": "g. However, these methods achieve on downstream that require specificinformation from documents (e. We refer condition as the information ne-glect problem. There is no explicit signalfor the to ensure that it is useful for solvingdownstream tasks. , which showstwo attention distributionsone a documentcontext and one from an instruction promptwhenapplying the Mistral 7B a samplefrom the Qasper Note that two differsubstantially in their weighting of positions, sug-gesting that the context-deriving may not capture the specified by. issue arises because cacheacquired through state eviction is based only on thelocal document context.",
    "KDiscussion": ", 2024)indicate that the basic cache effectively maintainslanguage model perplexity; (2) performance im-provements are observed when using an instruction-aware cache, which is only information that themodel could access when generating the responseduring the task-solving thread. In this paper, we argue that the cache used in stan-dard chunked state eviction (CSE) is primarily re-sponsible for maintaining the perplexity of lan-guage models, whereas an instruction-aware cacheoffers advantages for long-sequence downstreamtasks. , 2024b; Oren et al."
}