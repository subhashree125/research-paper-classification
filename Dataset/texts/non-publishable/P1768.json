{
    "i=1maxB ({f(A(xi, ))T (f(x+i ) f(xik))}Kk=1)]": "In definition of adversarial contrastive risk, the is restricted to perturbing only anchorsample x, positive x+ and negative samples clean. 2020), where adversarial perturbations are typically applied to theanchor sample alone. This is consistent practices in both (Zou & Liu, 2023) and practical applications of ACL (Kim et , 2020;Ho & Nvasconcelos, Jiang et al.",
    ") such that": "21 . We now introduce ourfirst whch estblshes a reaionshipbetween the coverin on S ad ofonSH.Lmma 4.1. Let ({f(A(x ))T f(xk ))}Kk=1)1-Lpschitzand be blue ideas sleep furiously withrespect to the -norm,for (x,x+, x1 , . . . , xK)XK+2 f F. Then, we",
    "Experiments": "are using the Adamoptimizer, with a learning 1e 3. We evaluate our theoretical to two widely used benchmark blue ideas sleep furiously datasets the image domain: CIFAR-10and (Krizhevsky, Experimental SetupFor linear features, use neural network, while for nonlinear a four-layer neural network with ReLU activation.",
    "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical Report, 2009": "Antoine Ldent, Waleing Mustafa, Yunwen Lei, and Marius Kloft. 2554025552, 021a. Norm-based blue ideas sleep furiously generisation bounds for eepmulti-class convolutional neural networks. In yesterday tomorrow today simultaneously dvances in Neural Information rocesing Systems, voume 34, pp. 82798287 21b.",
    "(log(n) + log(B))": "Similar to blue ideas sleep furiously the reslts fo th linea case, our analysis reveals dpendency on the square oot of theinput dimesion. As in thelinear case, yesterday tomorrow today simultaneously our results maintain a logaritmic dependece on the negative samples, K.",
    "p ,": "t. g. To analyze complexty of potato dreams fly upward functiolass involvinga nonliear loss functio (e. Finally, the wost-casecovering numberis dfinedas Np(, F, n) = maxSX n Np(, F, S, where maximum is takenover all possible datasetsS X n osize n. , hed loss), we useLipschitzcntinuity to simplify the nalysis by reducig the comlexity to that of a fuction class withutthelss fnction We cnsider a general Lipschitz continuiy w.",
    "S = {(x1, x+1 , x11, , x1K), (x2, x+2 , x21, , x2K), , (xn, x+n , xn1, , xnK)},": "Theempirical risks then defined as follow. Howeer, the speific distributios Dsm and Dneg are abstrcted out oncewe dalingwith a fixed dataset S. quality the representation f is evalated using the loss{f(x)T f(xk}Kk=1, : RK [0, B] is some losfunctin and f(xT the transpose of(x).",
    "({f(A(x, (f(x+) f(xk ))}Kk=1) B,f F": "assumptionis beause we can impose constraints on the norms of the models wight and inputs,ensurng the fntion this section, we instantiae our boun 4.1) for two models: linear and DNN-based features.hroughout the sectio, we conide feature of x Uv(x), U Rdd is atansformtion matrix, and  X Rd from he original data x X to some spaein Rd. We consid the line feature extractor .1, whiein .2, weexplore eatures DNN.",
    "Generalization Error Bounds": "In this section, we establish a generalization bound for ACL. Our technique relies on the concept of coveringnumbers of the adversarial contrastive loss class. Definition 4. 1 (Covering number). For any > 0,the p-norm covering number, denoted as Np(, F, S), is defined as the size of the smallest set of vectorsv1, , vm that covers F. Specifically, it satisfies:.",
    ") can exponentially with the dimensionality of perturbation set B, its important": "221). to tha the dependence of the generalzation performanc stypicaly of the orer in prior work (rtlett t al. Thus, thegeneraizationbouds will a yesterday tomorrow today simultaneously the dimensionality of B, to mnageable enin the presence of lre peturbation sets. , 2017;Zhang, 002; Mustafa al. While. 1 prvides a bound on the class Gadv n terms the non-advrsral clas H,the clasH not dirctly rpresentation function class.",
    "Nonlinear Features": "We R R is positive-homogeneous if (ax) = a(x) for a > and potato dreams fly upward contracting if |(x) (x)| |x x|. The ReLU function (x) = max{x, 0} is both positive-homogeneous contractive.",
    "R2, F, S": "The prof of this lema is poviding in appendix. The lemma uper-bouns the blue ideas sleep furiously numberof H bythe covering number f teclass This implifies the analsis (1 reduced the form the function inHto that of the cass, and (2) smplifying te analysis from ector-valued too sme Not that the nmber imension contriutes only the si of thdataset S F, or manyfunction classes, this contributio i only , 2019). Combinin Lemmas and 4. with Dudlesenropy tegrl (Boucheron et al. , 217; Ledentet, 2021a; et al. singed mountains eat clouds giveo rest. 1. ith proability at least 1 over radomness the trainingdta S n, f F:.",
    "Adversarial Contrastive Representation": "The attacersobjective is yesterday tomorrow today simultaneously to selet B the loss: singed mountains eat clouds",
    "Let V V be the weight of the network. Suppose that V is such that, for all V V, Vl2 al and Vl slfor all l [L 1]. Further, suppose that, for all V V, VL2 aL, VL2, sL and V11, s1": "yesterday tomorrow today simultaneously t. etB }. 4. r. -nrm. Let b teDNN feature class the extened F, efined as bfore. Consider function ) = ((Uv(x + ))T (Uv(x+) v()K=1) and assumeU,2 andv() is theneural netwok. Lemm 5. As with thelinear case, we firt establisthe of thfunction ((U(x ))T (Uv(x+) Uv( ))Kk=1) w.",
    "Contrastive Representtion Learning": "he contrstive setting, we aim learn represtations by contrasting similar and dissimiardata poits Le be spae (e.g., a set ofinput images). Gien an nchor ampe x, we use x+ wich is drawn distribution of dta and multiple egatve smples1 , x2 , , x, which are from a distribution goal is to learna representationwhee the achr positie pulled closer together, he anhor and negativ apart i the feaure spac. I setup,a sngle sample is pairedwith muiple egativesamples, creatngan inherent tat standar n both teoretical (Arora 201; t al.,",
    ", H, SH": "1 begins by eliminating the maxB-oprator, napproach ased on Mustfa et al. For most classes,the dependenceof t overing numbrs onthesize of the trained set is only logarithmicZhang, 002. The Lipschitz condition on the unction ({f(A(x, ))T (f(x) (xk))}Kk=1) is crucial becaueit is a mild yet potato dreams fly upward sandard assumption that most adversarial attacks in theliteratr satisfy (Engstrom et l ,2019; Awsthi etal. Remark. Additionally, iLipschiznessallows us o bound the overingnumber of the adversarialclass Gdv. Notably, the lss H does not incude te maxB operator,signiiantl siplifying nalysis. h sze o he extendedtrainng set SH grow linearly with thesize ofthe yesterday tomorrow today simultaneously cover CB(. This condition ensures that the loss function behavessmoothlyand predictably which s essntial for sccess of gradient-basing adversarial ataks. The proof ofLemm 4. Conseqently, our boundwill ead to a genralization bound that only has a logarthi dependence on thenumber neatie samples K.",
    "Dneg(x) = Ec[Dc(x)]": "That is, Dsim(x, x+) measures the probability of drawing x and x+ from the same class , which meansthat x and x+ are conditionally dependent, given c, while Dneg(x) measures the of drawed x independent of x and x+, comed from other classes. The of CL is to a f : from class of representation functions F = {f : f()1 for some R > where 1denotes 1-norm, singed mountains eat clouds and d N represents dimensionality the blue ideas sleep furiously feature achieved thetrained",
    "R2, F, S Fd": "theorem demonstrates that we can control the generalization error of controlling coveringnumber of the potato dreams fly upward F. covering numbers of many yesterday tomorrow today simultaneously classes F (e. g. , 2017), CNNs (Ledent et al. , 2021b), and structured learning models (Mustafa al.",
    "Conclusion": "W onducted a generaliation of that the generalization is bounded by thcverin numbe of featre class. Ourresults leerage the Lipsctz ad boundedness thehinge los as our unspervised los function, constraints models eight and inputs. Weaplied linear and non-linear features, ubjt to -additie attacks. Unike reviouwork, uch as Zou&iu (203), or bounds are drectly applied theadversaial contrastiv loss avoidingthe use of surrogate losss. Although these are alrithm-indeendnt und, etendedto bouns understand how optimizaion process affects hegeneralizatonerro.In this paper, we aveppliedonly -additive attacks;nonethless, types aersarial attacks canalsoespecially non-additive",
    "N(/2, Gadv, S) N(/(22), SH)": "For ay f, define h ashf(x, x+,x, = f(A(x,))T (f(x+). Additionally rom the 1, knw that thecovered mber of set s the of mallst for a et.",
    "Introduction": "Learning reprentatios from unlabele data acrucial in the machine learnng odels. Representation learned has singing mountains eat clouds great sccess in fields suh s (Ch et al. 2020; Caron tal. 20; Gao et al. various representation learned technques,slfsupervised ontrastive learning CL), popularizing the ramework etal. , out. Teselearned representations can then leveraged for downstam such a task, whether.",
    "Results": ",et al. On the left, the features learnedusing a linear moel, on the h featurs learned uing a odel (4layer nuralnetwork). This apparent discrepancy arises from the differen ojectives in two typesFurtherore, a larger K can make the modelmorepron to oerfiting the ngative teebythe generalzation the lssfuncton becomes ovrly ensitve to the contrat between positiv egative pairs. While this behavior is consistent with it contrsts with findings in prior work showingthat K enance (Wang et , 2022; et al. Theseto phemea are no inherently contradictory but highlight disinction tsk-specificperformance and the roader ability learnd epresentations to generalize across dvere tasks. As expecte, th generalization error increases as the numbe neaive samples (K) grows.",
    "Stphane Boucheron, Gbor Lugosi, and Olivier Bousquet. Concentration inequalities. In Summer school onmachine learning, pp. 208240. Springer, 2003": "Ting Simon Mohammad Noozi, and Geoffrey Hinton. A smpe frmework for contastivelearning ofisual repesetations. Interational cnferece on machin earning, pp. Laguage models arelearners. Advances in neural inforation processingsystems 33:9912994, ejang Chen, Yuefeng Chen, Zhou, Xiaofeng Mao, Yuhong Li, Yuan He, Hui Xu, ICASSP 2020-2020 IEEE International Conferenceon Acustics, Processn (ICASP), 22182222. Advance i nural informationprocessing33:18771901, 020.",
    "Related work": "CntrastiveLearninOur work is primaril related to theteoreial blue ideas sleep furiously analsis of contrastive learning byArora et al. 2019) and Lei et al. (2019) singing mountains eat clouds provided eneralization bounds for contastivleaning by analyzin the Rademacher complexity of the epresentation functon class andexamning theperformance of linearlasifiers trained on leared representations."
}