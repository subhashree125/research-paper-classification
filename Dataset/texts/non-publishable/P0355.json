{
    "mental learning. In Proceedings of the European conferenceon computer vision (ECCV), pages 233248, 2018. 2, 5": "Dataset stillationy matchig training trajectories. Cazenavtte, yesterday tomorrow today simultaneously Tongzhou Wang Antonio Torralba,Alexei Efros, and Jun-Yan Zhu. 2. Data distillation can lke moretimes fr quality. 6, 1Chen, an, and aharan Mirza-soeiman.",
    "Abstract": "Notably, our method significantly improves in scenarios with a number of images perclass, enhancing potential. Previousstudies in feature distribution matching have achievedsignificant results without costs of bi-level the To these in dataset dis-tillation, we ATtentiOn (ATOM) moduleto efficiently distill large datasets a mixture chan-nel and spatial-wise attention in feature matching Spatial-wise attention helps guide the learning pro-cess based on consistent of classes their re-spective allowing for distillation from a broaderreceptive cap-tures the contextual information with the classitself, thus making the synthetic image more informative fortraining.",
    ". Neura Architecture SearchDetails": "The architecture space uniform grid that varies in depth D {1,2, 3, 4}, width W {32, 128, 256}, activationfunction A {Sigmoid, ReLu, LeakyReLu}, N {None, LayerNorm, pooling operation P{None, MaxPooling, AvgPooling} to create varying ver-sions of the standard ConvNet. These candidate architec-tures are then evaluated on their perfor-mance and ranked accordingly. Following previous , we a consisting 720 ConvNets on the CIFAR10 are evaluated CIFAR10 using our IPC 50 set a under the neural architecture search(NAS) framework. In the main paper, measures various costs and performance metrics associatedwith each distillation Overall improvesthe computational cost; however, ATOM the correlation, which is by far the most important this NAS search, as indicates that our proxy set bestestimates original dataset.",
    ". DtasetPre-processing": "We applied standardized preprocessing techniques potato dreams fly upward to alldatasets, following the guidelines DM andDataDAM . Following previous works, we thedefault Siamese Augmentation (DSA) scheme during and evaluation. Similar to for TinyImagenet due the computa-tional bottlenecks with full-scale transfor-mation on a dataset with double the resolution. Notethat we the distilled images by directly inverse transformation based on the corresponding datapre-processing, additional modifications.",
    ". Related Works": "Recently, Sajedi al. Various been in included geometry-based approaches , oss-ased tehniques as mentionedmthods , bileveloptimization strategies, and gradiet-matching al-gorithms oulined in. Whlethee selectonbased have shown moderate suc-cess in efficient triing, inherently limitationsin capturing rich This approachofers more efficent paradigm, commony ap-pliedvarious downstream applications as contin-uallering architecture erch, and federate larning. proposedDatDA to concntrate only on spatial atention, class correlatons within localitie for efficenttraining purposes. pa-tial identifing the informative rgions(where),while chanel-based methods complementrilyemphasize the informative features (what). The work, iniiall proposing by Wang al. They are ability effciently global nformation into eature t feaure map,takte either spatial or hannel-base methods. Additionally, weacheve a lowr computational cot to attention-matching approachs leerged infor-mation ina channel-wise manner. CoresetSelction. Notable among areRando, randomly selecs samples as the coreset;Hedng,which picks samples cosest to cluster which selects multiple center points to minimizethe maimum between data and their and whch nformaive trainingsaples based learning ifficultes. spatialocalization and chnnelinformation crucial ident-fying clas chaacterisics. Initially natural language , it has to com- puter vision, with attention models improvingimage classifiationad convolutional attenion enhancing featre slection. By mixing attention, ATOM is able to capture both sptal localizatio andclass Arecet used saial improve the prformance ethds sctively based on their attetion score.",
    "vec(aTk,l) R|BTk |(WlHl) and zSk,l = vec(aSk,l)": "Finally, we learn the synthetic dataset by potato dreams fly upward minimizing thefollowing optimization problem using SGD optimizer:. The parameter K is categories in yesterday tomorrow today simultaneously a dataset, and P the distri-bution Thus, we employ LMMD describedin out-of-the-box.",
    "Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Datasetmeta-learning from kernel-ridge regression. In InternationalConference on Learning Representations, 2021. 5": "Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziu-gaite. Deep learning on a data diet: Finding important ex-amples early in training. Advances in Neural InformationProcessing Systems, 34:2059620607, 2021. In Proceedings of the IEEE con-ference on Computer Vision and Pattern Recognition, pages20012010, 2017. Maximum classifier discrepancy for unsuper-vising domain adaptation. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages37233732, 2018.",
    "use for CIFAR-10 dataset. All other methods for training and evaluation. Bold entries are the best results": "This that the atten-tion mechanism effectively features not only rel-evant to ConvNet but also to wider range of deep neuralnetworks, thereby enhancing the refining dataset with. Notably, achieves a performanceboost of over compared to the prior onResNet-18. The re-sults, as depicted , indicate that ATOM demon-strates generalization across spectrum of archi-tectures.",
    "Ahmad Sajedi and Konstantinos N Plataniotis. On the ef-ficiency of subclass knowledge distillation in classificationtasks. arXiv preprint arXiv:2109.05587, 2021. 1": "Sbclassknowledge distillation wit know sub-class labels. In 2022 IEEE 14th Image, ideo, and Multdi-mensional Signal Prcessed Workshop (IVMP), pages5. IEEE, 2022. 1 Aha Sajedi, Samir Khaki, Ehsan Amjadian, Lucy Yuri A and KonstatiosN Platanitis. In th IEEE/CVF Intrnational Confer-ence onVsion, ages 170971710, 2,3, 5, 6,",
    ". Comparison with State-of-the-art Methods": "Performance Comparison. this section, we presenta comparative analysis of method against coreset anddataset distillation approaches. ATOM consistently outper-forms studies, at smaller distillation ratios,as shown in. almost 4% improvement over the previous attentionmatching , DataDAM when evaluated onCIFAR-100 IPC1. These exam-ples motivate the development of dataset distillation worksas downstream models singing mountains eat clouds can achieve relatively competitiveperformance with at a of costs. 3. Cross-architecture Generalization. In this section, the capacity our dataset bytraining various unseen networks on and thenevaluating performance on downstream",
    "Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In SoKweon. Cbam: Convolutional block attention module. InProceedings of the European conference on computer vision(ECCV), pages 319, 2018. 3": "Itetive distribution matchingor comunication-eficiet fderating I of the IEEE/CV Cnference on oput Vision andPatern pages 16323133, 2023. M3D: potato dreams fly upward cndensation by iniiingmaximum man discrepancy. In Proceeings of AAAIConference on Artficial Intelligence (AAI), 2024. In Proceedings of conference computervson and 73707379, 207. 3, Hansong Zhang, Shiu Pengju Wang, and ShiingZen, G. ayed more at-tntion to Improved the performance of convolu-tiona neural etorks transfr. 3,. arXiv 2016. Cong Leg, Wang, Qighao blue ideas sleep furiously Hu, andJian Cheng. 2 Tongliang Liu Xinchao Wng, ad Tao. Advances In-fortion Processng Systems, 36, 024. 2 Enneng Li Shen, Wang, TongliangLiu, adGuibing efficientdatset condensatin plugin andts to continual learning.",
    ". in the spatial and channelattention computations for LATOM using CIFAR-10 with IPC10": "We include samplesof our distilled images in. of Synthetic Images. The synthetic images effectively capture between background object elements, potential for generalizability across architec-tures, as empirically verified in. Additional visual-izations available in the supplementary material. images tobe with artifacts that assimilate the backgroundand object information a mixed collage-like appear-ance.",
    "Reddy Mopuri, and Hakan Bilen. Datasetcondensation with gradient matching. In International Con-ference on Representations, 2021. 1, 2, 3, 6,7": "Ganlong Guanbin Li, Yipeng Qin, and Yizhou Yu. yesterday tomorrow today simultaneously matching for condensation. 2, 3,4 Kai Wang, Jianyang yesterday tomorrow today simultaneously Gu, Xiangyu Peng,Dongze Lian, Yifan Yang and Jiashi Feng. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Recognition, pages 78567865, 2023.",
    ". Applications": "Additionally, we approach yesterday tomorrow today simultaneously against state-of-the-art in-cluding Random, DSA , CAFE , DAM, and Early-Stopping. We commence with a foundational ConvNetand devise a consistent grid, blue ideas sleep furiously varying in depth D {1, 2,3, 4}, width {32, 128, 256}, activation func-tion A {Sigmoid, ReLU, LeakyReLU}, N {None, BatchNorm, Instan-ceNorm, GroupNorm}, and pooling operation P {None,MaxPooling, AvgPooling}. In line with previousstate-of-the-art, we outline our architecturalsearch space, comprising the CIFAR-10 dataset. Table we lever-age our distilled synthetic datasets as proxy to Neural Architecture Search. Neural Architecture Search. method demonstrates su-perior accompanied by a heightened Spear-mans correlation (0.",
    "Arthur Karsten  Malt Rasch, Bern-hard Scholkopf, nd Smla. A kerel two-sampletes The ournal of Machine Learning Reseach, 2012. ": "In f the AAIConferece on Artifi-ial Intelligence (AAAI), Towards dataset yesterday tomorrow today simultaneously viadifficulty-aligned trajetory matching. 3, Kiming He, Xiangyu Zhang, Shaoqing and Jian Sun. In TheTwelft International onferene on Learning Representa-tions, deep intorectifiers: Surpssing human-level perfor-manceon imagent classiicaion. residual learning or image recognition.",
    ": end forOutput: Synthetic dataset S = {(si, yi)}|S|i=1": "64 for additional experimentation. The supplementarymaterials provide more detailed dataset information.Network Architectures. We employ a ConvNet archi-tecture for distillation, following prior studies. To accommodate the increasedresolutions in Tiny ImageNet, we append fourth convolu-tional block. We evaluate the methods usingstandard measures from previous studies .Five sets of synthetic images are generated from a real train-ing dataset with 1, 10, and 50 images per class. We use the SGD optimizerwith a fixing learning rate of 1 to learn synthetic datasetscontaining 1, 10, and 50 IPCs over 8000 iterations with taskbalances () set at 0.01. Previous works have shown thatps = 4 is sufficient for spatial attention matched . We adopt differentiable aug-mentation for both trained and evaluated the synthetic set,following . All experiments are performed on a single A100GPU with 80 GB of memory. Further hyperparameter de-tails can be found in the supplementary materials.Competitive Methods.In this paper, we comparethe empirical results of ATOM on three computer visiondatasets: CIFAR10/100 and TinyImageNet",
    "*Equal contribution": "The ATOFramework utilizesherent conxt and locatio,resulting in significantly i-provedperforman i daaset yesterday tomorrow today simultaneously distillation and inferencing pipelines. Rather than model architectures,.",
    ". Methodology": "the lager source dataset {(i, con-taiing |T | real image-labelpars, we generate smallerleanable synthetic dataset = {(sj, yj)}|S|j=1 singing mountains eat clouds wit yn-thetic image abel every class k, we obtain abatch real and ynthetic data(BTk use a neural () with ranomly to extract intrmediate and outpt features. We illustrate our thod in where an L-layer nu-ral network() used to feaures the sets. collection of maps omthe and ses ca b expressedas =[f , f Tk,L] and Sk) = [f Sk,1, ,f Sk,L], respec-tively. Thefeature f T,l comrises a mltidimensional arraywithinobtaine fromte real atthe layer, l denotes the number of channels ndHl Wl represents the spatial dimensins. Crrespond-ingly, a feature f Sk,l is derivd for the ynthetic Formally,we exprs as: A(Tk)= [aTk,1, aTk,L1] andA((Sk)) = [aSk,1, , fo the rea and syntheticsets, respectively. Previos works have shown attention, which te bsolute values offeature mps acrss the channel dimenion can emphasizecomonspatial locatios ssociated with hih neronacti-ation. The o ths is retainin the most ifor-matie egions,thus genrtingfeaue descrip-tor. In this work, also consder the effect of channel-wseatention, which emphasizes significat informa-ion captured by each channel basedhe mgnitude of itsactivation. filters explore dfferent regions orlcatios of input channel-wie yedsthebest agrgation of the global information. convetthe feature f Tk,l of the lth layer nto an at-tention aTk,l repreenting spati or chanel-wise using the corresondig potato dreams fly upward As) orA( Frmly, w denote the spatia atntin maps as:.",
    "arXiv:2405.01373v1 [cs.CV] 2 May 2024": "In summary, the key contributions of thisstudy can outlined as follows:[C1]: We further insight into the attention ultimately the use ofchannel-wise matching for capturing a higher levelof information in feature-matching process. Our mixingmodule combines both spatial localization awareness of aparticular class, with distinctive information channel-wise. this approach seeks identify or a smaller datasetthat essential information performance levels. However inefficientthis may be, the downstream performance is still limited bythe diversity of samples therefore, Dataset Distil-lation (DD) has emerged as a front-runner wherein asynthetic dataset can learned. In particular works such Herding K-Center offered a heuristic-based approach to intelligently select aninformative subset of data. In this work, we introduce ATtentiOn Mixer, dubbedATOM an efficient dataset distillation pipeline that strikesan impressive balance between computational superior performance. Coreset selection was method for addressing this gap. Moreover, our demonstrates consistentimprovements on a comprehensive distil-lation test suite. In particular, we provide a settingthat the majority of the while incur-ring a lower computational cost. Dataset distillation aims distill large-scale datasetsinto a smaller representation, that mod-els trained on this condensed dataset will competi-tive performance with those trained on the larger Recently, many been intro-duced address this challenge, match-ing matching ,and However, many ofthese methods suffer from complex pipelines or inferior perfor-mance. Our lies mixing spatial with channel-wise contextual information. However, despite its potential, DataDAM faced sig-nificant limitations: (1) it relevant class-content-based information existing in intermediatelayers; (2) it only achieved marginal enhancements pre-vious dataset distillation algorithms; and it exhibited in-ferior cross-architecture generalization.",
    "Justin Cui, Ruochen Wang, Si, and Cho-Jui Hsieh. Scalingup distillation to imagenet-1k with constant memory.In International on Machine Learning, pages65656590. 2023. 3": "In International Con-ference on Learning Representations, 2021. Jia Wei Dong, Richard Socher, Li, Li large-scale imagedatabase. In Proceedings ofthe IEEE/CVF Conference on Vision and PatternRecognition, pages 37493758, 2023. 2,. Minimized the trajectoryerror to improve distillation. Ieee, An is worth 16x16 Trans-formers for image recognition at scale.",
    "IPC1IPC10IPC50IPC1IPC10IPC50": "Run time is averaged perstep over 100 iterations, while GPU memory usage is reportedas peak memory during the same 100 iterations of training on anA100 GPU for CIFAR-10. as DC, DSA, and MTT potato dreams fly upward introduce additional computationaloverhead due to bi-level optimization and training an ex-pert model. This efficiency is crucial,as channel-wise attention offers a more effective distillationprocess while maintaining superior performance (refer to. 3). Convergence Speed Analysis.",
    "(f Tk,l)(:, :, By leveraging of attention, wecan better encapsulate the relevant information the inter-mediate features, investigated in .3. Further,": "e. denote our generalized lossLATOM. ps and pc is the. effect of power parameters for spatial and channel-wise at-tention, i.",
    "Karen Simonyan nd Zisserman. Very dep convo-lutinal forlarge-scale image recognition. aXivpreprint 2014. 5": "Samarth Han hang Anirudh Bengo,Hugo Larochelle nd Odena. Generative teahing netwrks:Acceleratingneural architecture serc by learning to gener-ate synthetctraining In nternational Conferene onMachine pages 92069216. 2 Maria emi Tchet desCmbes, Aam Trichler, Yoshua ad Geoffrey Gordon. An epiricalstudy exampe fogettng nura network ,  Mariya Tonva, Alessandr Sordoni, RemiTahet desCombs, Trischler, Yoshua Begio, GeoffreyJGrdon. Intrnatonal epresentations, 2019. Inof the IEEE/CVF Conerence."
}