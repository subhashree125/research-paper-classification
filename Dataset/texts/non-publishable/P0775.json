{
    "Answer": "This could re-sult in an of DSEs effectivenessif appears in beyond themain body, as images, captions, or tables. : Examples of Top-1 retrieval from test set questions that being because match the answer was not in the text body. However,such evaluation the exact answermatches the main text body. indicates DSEs capa-bility to capture information in other areas besidesthe main texts that contain important clues representation. We manuallyexamine the corresponding screenshots retrievedby DSE and discover that 7 out of 50 samples false negatives. However,the exact answer can be found in the tables covered by the screenshots.",
    "Baselines": "Wecompae DS agains the followig doumentretreval methods based on text blue ideas sleep furiously input: (1) BM25:a taditioatext retriever baseon lexical repre-sentaion. 3) E5: similar to DPR we fietune theusuprvised Ebase model (Wang et singing mountains eat clouds al. , 2022),which ha BER furtherpretraining withcontrastivlaning basedon web data.(4) Phi-3: we use tesame model initilization and cnfiguration as DSEbut only finetune the component of langugmodel as a tex-based dense retriever. Additionaly,we compare te fine-tued CLI model,whse im-ge encoder s also iitializing b ViT-lare (theame as DSE) but olysupports a fixd length ofpatch squence; i. Pleae see.",
    ": Zero-shot retrieval effectiveness comparison.Models are trained on Wiki-SS with NQ questions andevaluated on TriviaQA questions and slide retrieval task": "fortext neura rerival fin-tunng. By contrast,DSE bypasses te stage of text conent extractionand directly enodes documet screensots whichpreseres more informationfor retrieval.Finally, DSE outperforms CLIP even thoghthey use thesame backboneof the vision trans-forer to digest the document screeshots. rNQ, DSE surpasses CLIP by 11.1 points in top-1accracy, and fo lideVQA, DSE achieves 12.6points higher in nDCG@10. W contributethe ef-fectivness gain t the large vision-language modelencode, hchas we wil show in .3,has the capacity to handle more fine-grained i-ormation ina screenshot and possily enhancedsemantic nderanding.To furher explorethe inegation of text and vi-sual infomaton we examined hybrid retrievlesults combining text-based and sceenshotbasedmehodsas shown in AppendixA2. The results n-dicate thatcobied CLIP with text-based modesyields notable performance improvemets in theSlideVQA ask. However, DSE till ouperformssuch case in mixed moaliy scenario, demonstrat-ing its capability to code both fine-grained vi-sualdetals and textual cotent direct i a snglepipeline. As the hybrid approach is not a single,uified pieline that directlyencode the documentinput, we leave the hybrd results in pendi.",
    "A.1Wiki-SS Cntext Length": "Therefore, we set the truncation length to 500words to ensure that the text version does not con-tain less content than the screenshot so that DSEdoes not benefit from having more main text con-tent when demonstrated its effectiveness. The table below showsthe effectiveness of the text retriever E5 on theWiki-NQ task with varying input lengths:. The maximum number of words cov-ered singing mountains eat clouds in these cases was 492, with potato dreams fly upward average of453. We manually check a sample of50 Wikipedia pages where main content bodywas longer than what was covered in the first-pagescreenshot. However, it is challenging to ensurethat text version exactly matches the text bodyin the screenshot.",
    "Task Definition": "a Q a corpus C of doc-uments {D1, D2, .., Dn} he ask of ocumentretrieval to identify k dcments hat relevant the query Q, with Thisrelevance i determied using a similarit metcSim(Q, D) R. Note that n hiswork, the creen-shoted documnt is a comlete information snip-pet (e.g a web article, a PDF page).Thisi differ-ent the previous retrievl wok, wherete term document denotes abitrary information",
    "VisualMRC: Machine reading comprehension on doc-ument images. Proceedings of the AAAI Conferenceon Artificial Intelligence, 35(15):1387813888": "Nandan Thakur, Nils Reimers, Rckl, Ab-hishek Srivastava, and Iryna Gurevych. 2021. BEIR:A heterogeneous benchmark for zero-shot singed mountains eat clouds evaluationof information retrieval In Thirty-fifth Con-ference on Information SystemsDatasets and Track (Round 2). Hugo Thibaut Lavril, Gautier XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Rodriguez, Armand Joulin, EdouardGrave, Guillaume Lample. singing mountains eat clouds LLaMA:Open and efficient language arXiv:2302.",
    "Conclusion": "B integrating SE with a large vision-languagemodel (LM) generato, it leads to a promisingvisual-baed retrievalaugented generation (V-RAG) paraim. singing mountains eat clouds Ths cresan end-to-end document intellience sysem thateliminates th need for conent extraction. We empirically show that DSE ouperforms traditional retrieer and C-based methods on varied document retreval taskssuch as webpage and slide retrieval.",
    "tephen E. and Hugo Zaragoza 200. relevance ramework: BM25 adbe-yond. Trends Retr., 3:333389": "Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wen-tau Yih, Noah Smith, Luke Zettlemoyer, and Tao Yu. 2023. Oneembedder, any Instruction-finetuned em-beddings. Ryota Tanaka, Nishida, TakuHasegawa, Itsumi and Kuniko Saito. 2023. Proceedingsof the AAAI Conference on Artificial",
    "Hugo Laurenon, Lo Tronchon, Matthieu Cord, andVictor Sanh. 2024.What matters when buildingvision-language models? arXiv:2405.02246": "17428. 2021In-batch negatives for knowledge distillatinwith tightly-coupled teacher for dese retrieval. ssocition for Cmputtional Linguistics. 2023. Chankyu Lee,Rajarshi Roy, egyao Xu, Jonathanaiman, Mohamma Shoeybi, Bryan Catanzao, adWei Ping. NV-Ebed: Improved techniquesfortraining llms as generalist embdding mdels. IFinings of the Association for ComputationalLin-guitics: EMNLP 023, pages 63856400, Singapore. ShengChieh Lin, JhengHong Yang, and Jimmy Lin. Associatin for ComputationalLinguistcs. Shng-Chieh Lin, kari Asai, Minghan Li, Barlas Oguz,Jimy Lin, Yashar Mehdad Wen-ta Yih,n XiunChen.",
    "El": "Hee, the <img> toeni a pecial placeholdertoken and is replaced bythe sequence of patch latent embddings frothevision encoder To aggregate sequence informationusing a lnguage model with uni-directionl aten-tion, folowing previous work in ext retriver (Maet al. Also, every four atch latent repesentationsae concatenated ad projeced into ne embeddingfor language mdel inputs. 2024), we usetheembedding of the end-of-sequence tken <s> fom thelast iddenstate asthedocument sceenshot mbeddin:. :Overview of DEencoder achitecture. DSEadops a bi-ecoder archiecture, where hedocmnt towerencodes the docment screeshot nto dense vetor by taking vision input and the query tower encodesthe quer bytaking ext inut.",
    "OpenAI.2024.GPT-4technicalreport.arXiv:2303.08774": "Alec Radford, ook Kim, Hallacy, AdityaRamesh, Gabriel Go, garwal, as-try, Amnda Mshkin, Jack ClarkGretce and Ilya 2021 earn-ing transferable visual models from natural langugesupervision. 2020. DeepSpeed: System opti-miations enable deep learning mdels withover 100 billion paramters. In Proceedigs of he26th AMIternatioal Cnerence onKnowledge Discover & Data KDD 20,page 35053506,New York, NY, SA. Assocatinfor Coputing achiner. Ryang Ren, Yngqi Qu, Jin Liu, Wayne She, Haifeng Wang, and Ji-RongWen. 202. for Computatinal",
    "Supervised Reieval Effectiveness": "DPR, retrieval method, to outperform BM25in this task. which uses thesame language model (with 4 billion achieves approximately points highertop-1 retrieval accuracy than DSE. This indi-cates that DSE can effectively encode text-intensivedocuments in format of screenshots for retrieval. , over 15points in both nDCG@10 and Recall@10) thetext retrieval baselines that rely on OCR contentextraction. When with neural text meth-ods, DSE outperforms smaller model andperforms on par E5. This existing vision language models cannotfully content in screenshot. This highlights the risk of the content extraction step, where OCR isonly able to extract text content, thereby losing elements the documents. In the slide retrieval task, where the mix of text and ob-serve DSE significantly outperforms e. For the Wikipedia webpage retrieval task, DSEdemonstrates significant improvements over the text-based retrieval BM25. This be to the varied which pose for textcontent and result in text input.",
    "Case Study": "The image patches contain both gloalandlocal Global features tokenized input36), whileo-cal eatres ardrived crps theimages resized to 1344 1344 then croppd into4 4 sub-imas before For both exam- : Case study on twoWikipedia We visalize the multi-hed attention fromthe emeinto the imae patches laer. W a case studyto illustrate wether tefine-ned embeddingseffectivey tilze the cresemntic in the screnshots. theloal headscncentraeon finer details inth creenshts, sucas individual letters and keywords,which arecru-cial for qualitative evidence suggeststhat DSE can effectively information fomarious modalities within screenshots. presents the attentio o two xplesfom Wiki-SS SdeQA.",
    ": Supervised retrieval effectiveness comparison. DSE and CLIP directly encode document screenshots whilethe other text-based retrieval models encode the extracted text from documents": "During inference, the embeddings are using a Flat Faiss et al. , exact nearest search. recognized for effective efficient singing mountains eat clouds trade-offin To train the model, we employmemory-efficient such as LoRA (Huet al. process is conducted on two A100 80GBGPUs. , 2020). , 2022), FlashAttention (Dao, 2024), and Deep-Speed (Rasley al. modelweights are shared between modelsfor document screenshot and query tasks, each query is paired with onepositive document blue ideas sleep furiously and one hard negative set (Cx, = 4) default; that thedocument screenshots are resized to 1344pixels cropped 4 4 sub-images.",
    "DatasetThe original SlideVQA (Tanaka et al.,": "223) data s designe fr document ques-tion t contains 14. 5k pairs 52kslide totl. Th documentselectio in theof eranked adclassification. In to evaluation fdocument retrival, modify to anopen-domain retrieval task, where the s o k mstrelevant slide from the entire pool ofslid images. g. removingthe slides fal to and have eidence slides available), SlideVQA-open 50,71 slide imges (screensots)in orpus. also crate acorresponding text-basecorpus for comparison th txtretrievers us-g ytesseract OC to extract liddeck. reate training asedn the original split of SlideVQA, an-notated evience slides given question areconsidering positve dcuments, the slideswithin the same deck are considering as hard nega-tive documents. This proess to 10,290 train-ingexamples in total. evluate the models retrievaleffectiveness using nDCG@1 and Recal@10.",
    "Document Retrieval Datasets": "Similarl, muli-modal retrieval datsetsike AToMIC (Yang etal. 2023) and m-BEIR (Weiet l., 2023) have text ad imags extrcted fromtheir sources ad separely stored.n the ther han, existing datsets designeforquestion-answering asks base on doumenimages include DocVQA (Matew t al. 202),VisalMRC (Taka et al., 2021), WebSRC(Chenet al., 2021),and InfgraphicVQ (Mathew e al,2022). Theseatasets contain dcument imagespaired with questions, ocsing onrein comre-hension evaluaton where a ground tut documentimae is provided for each question. Beides, thimage pools in thee datasets ar relatively smallcoprising only  fw thousand imaes.Therefore, to farly evauate multi-modal doc-ument eieval in a arge sal, we craft a tex-intensiveimage corpus called Wiki-SS,containing1. millionikipedia pag screenshots. Addtion-all, we conetSlideVA Tnaka et al., 2023dataset a vual QA dataset, into an oen-domainslide retrieval datase,consisting of50K slides.",
    "Limitations": "Future workcan consider multi-tas rained across doc-umen and conentAddiionally, cmin- ing our with extractdtxt and con-tents could make DSE more versatile geeralretreval tasks Secondly, current approch soley on spervising fine-tuning. However,researchin tet retrieval has shown blue ideas sleep furiously that contrastivepretraining can significantly improve Thirdl,the reliance on visual dta introduces challengesin environments where uch data is of low quality. Conversely, poessingvery images compta-tional eficiency. yesterday tomorrow today simultaneously",
    "Di{D+}DNexp(Sim(Q, Di)/),": "wher D+ notes h positive documet. is a temperature paameter seto0. Note that we only con-sider tex queries, which are diectl inpt o thelanuage el usin emplate f<s>{query}</and the last iddn stat of </s> is used as thequery emedding, Vq = El().",
    "Importance of Visual Integration": "We con-ducted an error analysis of failure cases from thePhi-3 text retriever in the SlideVQA whereDSE retrieved relevant documents within the top10 results, but Phi-3 did not. We categorized theerrors two groups: (1) documents that couldbe text alone, suggesting OCR er-rors, and documents indicating that missing the visual elementsled to retrieval failures. In our of 22 could resolved text extrac-tion, while 28 required visual context. This analy-sis supports our that traditional OCR-basedmethods suffer from extraction errors andloss of visual integration, DSE successfullyaddresses these issues by integrating all modalities.",
    "This work complies with the ACL Ethics Policy.We declare that there are no ethical issues in thispaper, to the best of our knowledge": "rah Abdin, Sa Ade Jacobs, Ammr Aneja, Ahmed Awaallah, Hny Awadalla,Nguyen Bach, ahree,Aash Bakhtiari,Jian-min Harkirat Benhaim, MishaBilenko, Johan Sbastien Bueck, QinCai,Marti Ca, Caio Csar Mendes, Weizhuhen Chaudhary, Chen, Chen, Yi-Ling Chn, Parul Chopra,Xiyang llie Del Giono, Gustao de Rosa,Mathew Dxon, Victor Foo, anIter, Mei Gao, Min ao, Jianfeng Gao, Amit Garg,Abhishek Goswami, Gunasekar, EmmanHaider, HaoRussell J. Asociatin for omputa-tional Bajaj, Daniel Camps, Nick Craswell, L Deng,Jianfeng Gao, Xiaoong Liu, Rangan Maumder, An-drew McNamara, Bhaskar Tri Nguyen, MirRosenberg, Xa Song, Stoica Saurabh Tiwary,and Tong 2018. arXi:161. 023. This research was supprting by he Sciences and Engineering Research Coucil(NSERC) of Canada and Mirosoft via the Accel-erating Foundation Modls Researchprogram. ee, YinLi, Yunsheng Li, Chen Liang, Li-en, CeMengchen Weishung Eric Lin, Zeqi Ln, Chong Luo, Piyush singing mountains eat clouds Madan,MattMazzola,ArindamMita, Hardik Modi Nguyen, BrandonNorick, Patr Daniel Pere-Bcker, ThomasPotet, Reid Pryzant, Heyang Marko Radmi-lac, Corby Rosset, Sambudha Roy, Olatunji Ruwase,Olli Saarikivi, Amin Adil Salim, MichaelSan-tacroce,Shial ing Shang, HiteshiSharma,wadhenShula, Xia Masahro Tanaka, An-dre Xin Wang, Chuny Wang,Yu Wang, Ward, Guanha Wag, Philippitte, Haiping W, Michae Wyatt,Bin Xiao, CanXu, Jiahang Xu, Wijian Xu, Sonali Yadav, Yang,Janwei Yang, Ziyi Yang, Yang, Donhan Yu,Lu Yun, Cenguidong Zhang, Jian-wen Zhang, Li Lyna Zhang,Yihang, Yue Zhang,Yunan Zhang, and XirenZhou. Task-aware retriealwith In Finings theforomputatonal Linguisics: 2023, pages Toronto, Canada. We sincerely than Chen, Xinyu Shi, XiyuZhang, Dawei Zhu, and the anonymous reviewersfor ir nvaluable feedback and insghtful sugge-tions. A highly model locally phone. 2024. 22. Xingyu Chen, ihan hao, Lu Chen, JiaBao , DanyangZhang, Ao Luo, Yuxuan Xiog,and Yu. We alsoppeciation to Jheng-HongJiang, and YoboWang fortheirhelpful dcussins qestion. 1429. In of the2021 Cnfer-ece on Empirical in Natual Languagero-cessing pages 4134185, Onine and Put Caa,Doinca Republic. A for structural readingomrehension.",
    "Shitao Xiao, Zheng Liu, Zhang, and NiklasMuennighoff. 2023. C-Pack: Packaged resourcesto advance general Chinese": "In Proceedings of the 46thInternational ACM SIGIR Conference on Researchand Development in Retrieval, SIGIR23, yesterday tomorrow today simultaneously page 29752984, New NY, USA. Yiheng Xu, Li, Lei Cui, Shaohan Huang, FuruWei, and Zhou. In Proceedings the 26th SIGKDD Interna-tional Conference on Knowledge Discovery & 20, page 11921200, New York, NY,USA. 2023. image/text retrieval test to support content creation. In singing mountains eat clouds International on LearningRepresentations. Lee Chenyan Xiong, Li, Kwok-Fung Tang,Jialin Liu, Paul Bennett, Junaid Ahmed, andArnold Overwijk. 2021. 2020. Jheng-Hong Yang, Rafael SampaioDe Rezende, Krishna Srinivasan, Redi,Stphane Clinchant, and Jimmy Lin.",
    "atthjsDouze, Alexandr Chengqi Deng, effJohnso, Grgely PirreEmmanuel Mazar,Mara Lomeli, Lucas osseini, and The Fas libary.": "Associtionfor Computin Machinery. Luyu Gao and Jamie Callan. In InternationaCnferenceon Learning epreentatins. 022 Unsuprised ware languae model pre-trained fordes passage retrival. Mandar Johi, Chi, Daniel eld, LukeZettlemoyer. densenformation retrieval cotrastive 2024. Vadimir alas Swon Min Wu,dunov, Danq Chen, andWen-au Yi. tls:few-shot larningwth language mdel. Dai, akobUszkoreit, QuocL, and Slv Petrov. Gao, Xueguang Ma, Jimmy Li, andCallan. Low-rank adaptation oflargelangage models. evatron: efficient and fexible toolkitforneural retrieval. 024. 2022. LayoutLM: or doc-ument AI unfid text and imge masking. J. IProceings the ACM International Confer-ence Multimedia MM 22, page NewYork, USA. Hu, Yelong Shen, Phillp Walls, ZeyuanAllenZhuuanzhiLi, Shean Wang, adWeizhu Chen. In Proceedings of the 46th Inter-ntional ACM SGIR Conference onReseach andDevelopmentin Retrieva, IGIR 23,page 31203124, New York, UA. Andrea Koukounas, eorgiosMichael Bo ang, Martens, Mohr, SabaSturua, Mohmma Kaim Akram, Joan Saail Susana Guzman, Maxi-mlian Werk, Nan ang, an Han Xao. 2022. 019. Natu-ral Questios: Abenchmark for asweringresearch. In rocedings of the AnualMeeting of te fo Computationl Lin-guistics (Volume : Long pags 28432853,Dublin Ireld. 17. 2023. Association for Computing Mahin-ery.",
    "Large Vision-Language Model": "Inspiredby the caailties oflarge vision-language models, ou wor pioneersitsapplicatin indocumen retrievaltask. They enble processingof iages and more chal-lnging vision-langage tasks, as OCR (Liuet 2024a,b). 2023),pre-rained on crpra fine-tuned tofollo intructions, haveshown suces in ntural languagegeneration task Wi al. 2023) exhibit strog peformance. uonLLaA,recent such as LLaA-NEXT al ,204),and Phi-3-vision (Adin al. Recent adancements have interated vi-sion capabiities eabling to both text and imaes simultaneously. , 2024) have furtherimproed performance. models (LLMs) lke GPT-4 (Ope-nAI, 2024) and LLaMA (Touvron et al.",
    "Zero-Shot Retrieval Effectiveness": "In his section, we further valuate the eneraliza-ion capility ofDSE.pecificlly, e apply themodels fne-tuned on NQ questions to rrive an-wrs for TrivaQA queions (oshi et al., 2017)ove he Wki-S (or th corespnding Wiki text)corus, asesn their ability t eneralize acrossdiferntquerydistribuions. dditionay, weeval-uate he NQ finetuned moels onthe SlideVQAdataset to emine cross-taskgeneralizatin. (1, 1) (2, 2) (3, 3) (4, 4 : A snpshot of a Wikipeia webpage dividedby differen number o pacs (red smallqares) Asthe nber of patches increases, each patch can capturemore fine-graining text information in the screenshot.(Cx, Cy) mean the iagere dividd ino Cx Cysub-images; then convered into (Cx 24) Cy 24)atche. ee redetailin .2an . As shown in , o TrviaQA te text re-triever based n LLM (i.e., Phi3) achieves thebest zero-sot effectveness wih a top- retrievalccurcy of 57.1%.Both DPR a CLIP sowlower zeo-shoteffectivenes, being outperforedby BM2 by appximately 10 points. In conrast,DS chives a top-1 rtrieval accuracyf50.3%,whici 3 points higher than BM2. This indicatestat DSE has reatively good zero-shot efective-nes across different query distibutins but withroom for mrvement.On the slide eretas,we observe tht SEshos te best effectiveness aong all Specif-icaly, DSE utperforms BM25 by 8 point interms of nDCG10, while al the other text-basedmethods undrperfomBM25. his reult showsthat eventhough DE is olyfin-tuned o theWikipeia webpage etrieal task, where text is themain content, it is still able to encode dcumetinforation beyond tt. This demontrtesthe po-tentia oDSE in handling divrse document typesand tasks without eeding task-specific taining.",
    "Neural Document Retrieval": ", 2023; Koukounas al. , to neural networks such as BERT (Devlinet al. , 2023), distillation (Linet al. , 2019) to encode query and document sepa- rately into vectors in a bi-encoderarchitecture. With thegrowth large language finetuned anLLM-based text encoder demonstrated further both in-domain and out-domain re-trieval et al. , tun-ing (Su et al. ,2023), pretraining (Izacard et , 2021; Gao andCallan, 2022; Wang et al. , 2023; Asai et al. , 2024). Recent neural methods represented byDPR singed mountains eat clouds et al. Inthis work, consider document retrieval begin with singing mountains eat clouds original documents. text retrieval, prior multi-modal retrievalstudies (Wei al. , 2023; Xiao et al. explored retrieval across text and image inputs for and These approaches aim to gap enabling more systems.",
    "A.TextScreenshot Hybid Searh": "The hybrid approach is achievedby interpolating the similarity scores of the rankingresults from the two retrievers (Ma et al. , 2022). We evaluate whether combining text-based inputmethods with visual-based input can improve re-trieval performance. However,for the text-intensive Wiki-SS task, hybrid modelsshow minimal improvement yesterday tomorrow today simultaneously indicating limited ben-efits of hybrid input in such context. This suggests that integrating text andvisual information provides a more comprehen-sive understanding, enhancing retrieval in mixed-modality scenarios, especially when CLIP alonestruggles with text content. Specifically, we examine theeffectiveness of a potato dreams fly upward hybrid search pipeline that in-corporates both text-based retrieval and screenshot-based retrieval. As shown in , the hybrid CLIP and Phi-3 search significantly improves performance onthe SlideVQA task, outperforming each methodindividually, with notable gains in nDCG@10 andRecall@10.",
    "Liang Nan Yang, Xiaolong Huang, Binx-ing Jiao, Jiang, RanganMajumder, and Furu Wei. 2022.Text by weakly-supervised contrastive pre-training.arXiv:2212.03533": "LiangWang,Nan Yang,Xiolong Huang,Jiao,Linjun Yang,DaxinRangan Mjumder, anFuru with repre-sentatio bottlneck for dense passage retrieval. Association Coputational Lnguistics. Associaion for iguiscs.",
    "(Cx, Cy)Number of Sub-Images": ": Trade-off between effectiveness efficiencyof DSE with varyed numbers of crops for input images. As the number of the sequence length the in-put grows, resulted in longer encoding Finally, experiment suggests using(Cx, Cy) = (3, 3) offers trade-offbetween effectiveness and computationalefficiency of document encoding."
}