{
    "Datasets": "visual RAVEN (Zhang et al. o. d regimes. In the changes the composition of the exam-ples: of figure shapes and coloursunseen in the training set are proposed; the sys-tematicity split alters the of the contextexample activations: the context contains more pos-itive examples than the RAVEN,the four contains four figures instead one;the in-center split describes two figures with onecontaining the other instead being placed next toeach other. They require of the to be solved have o. , 2021a, 2019;Gendron et ACRE and RAVEN from Visual Question Answering datasetsto be language , 2021a) is yesterday tomorrow today simultaneously an abstract causal reasoningdataset where the model must deduce the causalmechanisms yesterday tomorrow today simultaneously from a small set of image examples. sets chal-lenge this ability in the tested systems. dataset hastwo o. The ACRE RAVEN con-tain symbolic natural ofthe images instructions solving the task.",
    "Introduction": ", 2023b; u et al. partic-ula standardperform poorly complexreasonin tasks, as astract, causal, or (Wu al. , 2023; Gendron et 202a;Zecevic et al. 202; Ji et al. , 2023; Liuet2023). Gendron al. (2023); Wu al. Several have proosedto uch as the lack symbolcrepresentations ithinthe sace of LMs(W et , 2023; Gendrn These claims are suppore by thebrittlenss thatLLMs when changingthewording of a question, the performance of anLLM cn vary drastical (Wei t al. , 202). \" (Peters et al. 2017; Schlkopfet Thee principles are applie in di-verse ways in field of causality, either te odel, which may be in amodularfshioto rspect causal relationships, Structural Causal Models or thedisribuionthe data, wh may be in-deendent and distributed (i. i. d) from anunbalanced distribution by divisin into subgroups(Austn, 2011; Gendron e , 2023c). o. d) generaliation. this idea yesterday tomorrow today simultaneously in this work: we aim tobetter undertand how LLMs in and whether they behave Indeendent Causal n- der certain and with is sum-marised in. We aim t answer the fllw-ing question: (i) an LLMs e used as self-routersfor specialised and does timproveheir peformance? (ii) Can LLs doman-invariant with information-based reg-larisaton? (iii) How usefu is domain-pecificknoledge on reasoned tasks? and (iv) an orproposed rcitecture IndependentCausal Mechanisms? u contibutions c besummarised as folows:.",
    "Limitations": "l aresparsel connectd th level of language mod-eling hed. Generatingcomplxcasl computation graphs tailed the task imprve performance, but this problmis out of scope this pape.More-oer, we conut on resonig ass toverify if indcing high-lvel can yielireases performance and eneralisaion. Weaim not to outperformhe stateo-the-art ontheproblems t  stdy the proposed mechanisms can yiel such ncreases.In adition, trainng and fine-tning Larg Lan-guage has computationa cost. the increases dring inference ecauseeach has beassociated with a fullyloaded LLaMA2. Our cuent implementationloads al modules in pralel with the aim to studythe interctions between them andAppendix D),utthis cice meory not permius directly ICLM witha arge numer of modules.",
    "P(Y |c) = (l(Y |c))(16)": "Equation 14 shows the weighting process for all modules (a {I, S1,. c is the input context. The weights wSn n [1, N] combine the weight wI with the routerweights rn: wSn = rn (1wI). The output logits are obtained by summing theweighted logits of the domain-invariant module lI(Y |c) and all domain-specific modules lSn(Y |c). P(Y |c) is the final output distribution between all words Y , obtained usingsoftmax normalisation on the output logits l(Y |c).",
    "Ingwer Borg and Patrick JF Groenen. 2005. Modernmultidimensional scaling: Theory and applications.Springer Science & Business Media": "Sparks of artificial general Early with gpt-4. 2023. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Amodei. Tom B. Brown, Benjamin Mann, Nick MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, Pranav Shyam, Girish AmandaAskell, Sandhini Agarwal, Ariel Krueger, Tom Henighan, Rewon Ramesh, Daniel M. Language are few-shot learners. Sbastien Bubeck, Chandrasekaran, Ronen El-dan, Eric Horvitz, Ece Kamar, Lee, Yin Lee, Yuanzhi Li, Scott Lundberg,Harsha Nori, Hamid Palangi, Tulio Ribeiro,and Yi Zhang. 2020.",
    "+ 1": ": Smpliied temporal causal graph Gduringtrained before dding Mutual Inomationminimisa-tion. Cis the input context HR, HI,HSn, HSare the latent states of routr, domain-inariant,omain-specific and activateddomain-specifc (afterrouter wightin) mdles. For simpliciy, we olysow the state HSn o the activted domain-specficmodule n.Red dtted edges illustrate thecausal liks between the foard and backw passes. Linv and Ldom ae cros-entropy ossesbeteenthe utput logits of the invariantmode and hoseof te activated domain-specific mdue. LIisthe Mtual InfomationlossEq. 3). We cnsider three separate se-speiedlosses Lo, Liv and Ldom to induce temoduleso match he targt distribution iniviualy anpevent collapse to a single useful modue.",
    "Matej Zecevic, Moritz Willig, Devendra Singh Dhami,and Kristian Kersting. 2023. Causal parrots: Largelanguage models may talk causality but are not causal.CoRR, abs/2308.13067": "Computer /IEEE. ChiZhang, Ja, Mark Edmnds, Song-CunZhu, Yiin Zu. hi Zhang, Gao, Baoxiong Ja, anong-Chun Zh. ACRE abstract causalrasoning beyond covaiaion In IEEE Conferenceon Compter Vison and PatternRecognition, CVPR2021, virtual, June 19-25, 2021, pages 104310653. Computer Foundtion IEEE. 2019. A dataset re-lational and analogical visua In IEEEConference on Coputer Vision adPttern Recogni-tion, VPR2019, Long Jun 16-0,2019, 53175327.",
    "Experimental Setup": "By default e useN = 2 doman-specific mod-uleand one maininvarant modle, as thedaasets we use contain two ubdomainseach. Inadition, we stuy the indiviualperormance ofthe domain-invarint and domain-specic modules. Wealso perform experimns with an ablated modeltat doesot have domain-invaiant module. ,2023) for all ou moduls. 1, = 0. Loss hyerparameters are = 01, = 0. 25. 01 = 0. It i worth no-ng that number f parameers used s onlymrginall her than tha ofthe base LLaMA2,as only low-memory LoR aapter wights arelearndduring training. We use Low-Rank Ap-proximation o LMs (LoRA (Hu et al. 1, = 0. We use aretrained LLaMA2-7B Touvro et a. , 2022) tofinetune the moduls on their respective tasks Allodels aeine-tuned for 3 pchs with AdmW(oshchilov and Hur 2019) and a atc size of1.",
    "Decoder": "Theinput text (on the left, blue) is fed to multiple pretrained LLM (in red). classof causal yesterday tomorrow today simultaneously models relies on determining the information in a system (Shannon, 1948; Paluet al. The domain-invariant isalways activated. The that the domain-specific modules gain in-domain knowledge while theMutual Information regularises the domain-invariant module learning abstract defined by Pearl (1995, is identifythe causal a variable on with thehelp of the do operator: do() represents inter-vention, the forced attribution of a value avariable. Anadditional loss minimises the Mutual Information between domain-invariant and the domain-specificrepresentations. latent representations generated by the activated modules are an aggregationscheme (in and converted a distribution for the next word (on the right, in blue). If P(Y ), then X hasno effect on Y (they may still be they share common ancestors). : Proposed Independent Models architecture language-modelling tasks. In the transformers, the Causal Trans-former (Melnychuk al. router uses clustering on inputtext embeddings (in activate a domain-specific module this input. , introduce to reduce from the training dis-tribution.",
    "n = 00.01.00.01.00.01.0n = 11.00.01.00.01.00.0": "and in the main paper, there is clear division between text and symbolic embeddings, but the o.o.d setsare not well separated in ACRE while they are in RAVEN. This division (and absence of division) is alsopresent in the previous hidden states, although the separation is less obvious: all embeddings tend to alignto single axis.We want to study the routers behaviour further when facing with a diverse set of input data. To this end,we feed six different datasets to the model: the i.i.d text and symbolic sets of ACRE and RAVEN, PVR(Zhang et al., 2021b) and ARC (Chollet, 2019) datasets. visualisations are in . Overall, thedatasets are well separated but have different shapes. While some form dense amalgamates, others spreadin the latent space. observations from ACRE and RAVEN suggest that the distance in embeddingspace between a module cluster and an input can be an indicator of the modules performance on input.The o.o.d sets of ACRE are merged in the latent space, and the model maintains accuracy across the sets.In parallel, the o.o.d sets of RAVEN are separated by clear boundaries, and the accuracy drops as thedistance with the i.i.d set increases. Experiments on a larger scale are needed to validate or invalidate thehypothesis and discriminate the true causes responsible for this behaviour from spurious correlations.",
    "A.1ACRE": "Each sample in the dataset contains six images represented objects anda light, activated or not. goal of task is to determine from the images the objects causing theactivation of the light and determine the state of the light in four test cases: activated, deactivated, orundetermined if the activation causes cannot be retrieved. ACRE is an abstract causal reasoning dataset where the model must deduce the causal mechanisms from asmall set of image examples.",
    "We use the data provided at": "We investigate forthis phenomenon potato dreams fly upward in. The modules evenoutperform the oracle router on RAVEN in almostall singing mountains eat clouds settings.",
    "Jason Wei, Maarten Paul Bosma, Vincent Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Nan Du, An-drew Mingbo Dai, and Quoc V. Le. 2022a. Finetunedlanguage models are zero-shot learners": "17842. 2023. prompt-ing reasnng langug models. CoRR,abs/2306. 2023. CoRR,abs/207. attention r vision-language In IEEE Conference on Vision an Recogniion, virtual, 19-2,2021, 947985. 02477. Rasoning or recit-ing?eploing hecapbilities and ofmodels thrugh counerfactul tas. SPAE semantic pyramid autoencoder formultimodal generaion ith frozen llms. Lijun Yu, Yong Cheng, Zhiruo ang, Vivek Kumar,Wolfgang Macherey Yanpig Huang, oss,Irfan Essa,Yonatan Bisk, Ming-Hun Yang, KvinMurphy, Alexander G. 021. yesterday tomorrow today simultaneously Jason Xuehi Wan, Dale Schuurman, MaartnBosma, brian potato dreams fly upward chter, Fei Xia, E Chi, Quc V Denny 202b. Wu, Qiu, Alexi Ross, EkinAkyrek,Boyuan Chen, Wang, JacobAndreas, and Yon Kim. AssociaesInc.",
    "Mutual Information Minimisation": "of gained on the first processby observing the second one. The Informa-tion between two random HI H andHS H is given by:. second of is induce domain-invariance in LLMs. thisend, we introduce regularisation process basedon information theory. We minimise the MutualInformation (I) Kreer, be-tween and Specifically, we minimise the informa-tion between the last hidden states of the modules. The Mutual Informationbetween two random corresponds to between two processes, i.",
    "n[1,N]I(HI, HSn)(3)": "We can only access the probabilitiesPHI(h|c) and PHS(h|c) a input c marginalisation on C is in-tractable because of the input = V with V vocabulary size of theLLM L the maximum length of se-quence (typically V L = (32. We canapproximate it by at the blue ideas sleep furiously level = cC P(h|c) P(c) 1|B|cB P(h|c)with |B| |C|.",
    "K-Means (l = 2)": "The best is sown inbold. o. Models with singing mountains eat clouds a indicat tht esuls ar introduced inthis paper. The ground truth the true splitsbetween each dtset. d and o. leard clusters 4 Accuracy o thei. Clusterformed thestates of LLaMA2 on the training sets of ACRE, ARC, PVR The visualisations contain last levels (l) of hiden layers. d tet sets. Datasets ae in columns, and mdels rows. ICL is trained on text and symblic ii. i. d training sets.",
    "DAdditional Aggregation Schemes": "ths we descrbe two aggrgation schees betwen tedmain-invariant and domain-specific modules. method taces the issueof prioitisedmodules (e. onemodule being overused expense the others).Each module unbounded logits. Indeed,one module ovecome te weighting increaing the magnitude oflogts Weconsidertwoombinati schemes: in the space in probability spac.a batchof size |B|, one domain-specific activmodule ad ne dain-invariant module, batch noralisation isoperated on |B| samples. We attribute a weight wI to doain-invariant modue as wSn rn (1 I) the domain-specific module, with weight given by t router.Afternormalisaion We logit value by its yesterday tomorrow today simultaneously corresponding and them",
    "E.1Evolution th Mutual Infomation Across Training": "enure the ndeendence yesterday tomorrow today simultaneously etween the domai-specific and domain-invariant moules,we minimisethe mutualInformatin between shws te evolutio of singing mountains eat clouds uringtraining. We observe that quicky deceases reachbelow , 001.",
    "Dom 2-Dom 3": "Measures the Corrlaton offiient module hidden transfer learnigmodel dring infernceon RAVN datset. Router refers to te outing modue, Iv refers to the domain-nvarint module, nd Dom i refers to the i domain-specific. and columns represent layers 0 of a module.",
    "Peter C. Austin. 2011. An introduction to propensityscore methods for reducing the effects of confound-ing in observational studies. Multivariate BehavioralResearch, 46(3):399424. PMID: 21818162": "Hangbo Bao, Li Dong, Piao, and Furu Beit: BERT pre-training of image transform-ers. In The Tenth International Conference ICLR 2022, Virtual Event, 2022. OpenReview. 2023. A systematic evaluation language models on out-of-distribution logicalreasoning tasks. abs/2310. Ibeling, andThomas Icard. On and of causal inference. editors, and Causal Inference: Works of volume 36 of ACM Books, pages ACM.",
    "Pre-Prompt": "Only eturn the lette in frot of the crrect patern. Paterns in the blue ideas sleep furiously equence are precedd by a number from 1 to 8. Pick h singing mountains eat clouds leter in front of he correctaten hat logicllyfollows inthe sequncefrom the anser set.",
    "Franois Chollet. 2019. On the measure of intelligence.CoRR, abs/1911.01547": "PMLR. Unified scaling potato dreams fly upward laws for routed language mod-els. In International Conference on Machine Learn-ing, ICML 2022, 17-23 July 2022, Baltimore, Mary-land, USA, volume 162 of Proceedings of MachineLearning Research, pages 40574086.",
    "if (Y X)GX(13)": "GX represents the casa gaph with incoming edges o X rmoved. Let us the reltionships ofthe ruter. Equtions 9 can be vrified thesimplified causl grap in. are diret application of rule 13.removing the parentsof or HI, HR is d-separating (Pearl, 1988) tem: the bakwar path throgh C is blockedandthe throughWR is not connected to HSn or H. This is du to use o a separat lossfunction forrainng rout using te vector routing strateg. One ould notice thatwedo ot repesen sum of lsses 4. Its impact on thebackwar pssincienal sinceeac elementcan b optimsed independently. Let us nowaddress the causal relationships of activateddomain-specific nwith",
    "Related Work": ", If fully an SCM canrepresent te completeinner workngs sstem. The ofexpers is nt basedon domain informio. (2022) nvestgate the perormance of variousroutin strategiesfor LLMs and showthat the ainfrom using specialised is high for small models but asthe model size Introduced Mixtralof-Expr is mod-ular LLM used same as Transformer. o. Mxuresof-Expertsodular architectures divide comutations of a network The Switc Transfrmer (Feduet l. Shlkopf et al. he. 2022; et al. Hoever, only t lyrs used as experts ad the assuing t be known during trainin. Mittal al. ur diffrs in hatit is not directed at optimsng LLtraning butat functional modularity n studying on geeralisaionreasonig Modular Neual NetworksOher casses neural are designed tospe-cilisedsub-netorks for specificdomains. It outperfors LLMs ofsimilar size on reading coprehenson, cmmon-sense and tasks (iang e al. Recur-ent Indpendent Mchanisms et 201)ateptto lean models of independent mecha-nims an LSTM architecture andSchmidhuber t the dynamics ofphysicl objets. ,2024). Causal ModelCausl models aim o reuired kowledg of he causal relatio-sips linking th data (Bareinboim et al. In particular, bacpropagaion toloss oftencollapse to a sngle modle. Clret al. They findthat can ield beterresults as the number tasks increses. 02b serates eed-forward layers ofthe ransfomer model (Vaswani neexception the wrk Guruanga etconditions the activation of an on the input omain. How-ever, te lerned capturedomain specialisation.",
    "(d) Vector quantisation router": "We furhershow that propsed modules totheiromain wh but still prially ely on ashared doan-ivarat mechaism, higlighinga yesterday tomorrow today simultaneously limitation for ICMs with. : projectio of te hidden of LLaMA2onACRE RAVEN i. Groundtruthsamples labelled ino. i. o. d sets are lusteredtgter i ARE and seaated in RAVEN. i. d sets.",
    "Example Cases": "cyan clinder in rubber isie. gray cue in rubber is visile. The ligt is off. A cyan cyliner rubbr is visibe. A gra ube in ruber is isibe. A cube in mtl is visble. The off.A grycyliner rubbervisible.A gray cube in metal is The lght is off. red shre in meta is visible. lht is on.",
    "Continual Learning": "We study a where we want our blue ideas sleep furiously model one after trained on a previous We choosethe scenario ACRE RAV EN yesterday tomorrow today simultaneously RAVEN ismore challenging, the o. Their weights are not the router RAVEN inputs, thus not and their performance on is preserved. However, aggregation is affected, to reduced performance on ACRE Text. o. learningconsists of training a model with or sets evolving over time, where and accumulates knowledge , 2023). d sets.",
    "Abstract": "espite impressive peformance o langagemodelling and complexreasoning tasks, Largeanguage Models (LLM) fll short onthesame task in uncommon settings or with di-tribution shfts, exhibiting lack of eneral-iation abiity. By contrast, systems suh ascausa models, that lern abstract iables andcausal rlationhips, can demonstratencreasedrobstness against changes in the istribution.One reason for this success i the eisteneand use of Independent Causal Mechanisms(ICMs) epresenting high-leel ocepts thatonl sarsely interact. In this work, we ap-pl two conceps fom causality to learn ICMswithin LLMs. We deelop a new LLM rchi-tecture composed of multipl parsely neract-ed language modeling modules. We also investigatete levelof independence and domain specilisaion andshow that LLMs rely n pe-traindpartiallyomain-invarint mechanims reilent to fne-tuning.",
    "Anirudh Goyal and Yoshua Bengio. Inductivebiases for learning of higher-level cognition.CoRR, abs/2011.15091": "Assoiaton forComptationa Linusics. OpenReiew. yesterday tomorrow today simultaneously SuchinGurrangn, Mike Lewis, Ari oltzman,Noh A. 2021. Deixlayer: Disentanglng dmains f modular anguagemoeling. 2022. net. Recurrent independentmechansms. In Proeeings of singing mountains eat clouds the 202 Conference ofthe NorthAmerican Chapter o theAociationforComputational ingstics: Human Language Tech-nlogies, NAACL 222, Seatle, WA, United SatesJul 10-15, 2022, pages 5555576. Smith, d Luke Zttlemoyer.",
    "E.3Variations of the Routing Strategy": "We pefrm additional experiments on and RAVEN blue ideas sleep furiously daasets using the routing stategies itoducedin Appndix C: -Mens ad weightin. Tabls 4 an 5 show results.The altenative routing strategiesachieve blue ideas sleep furiously iilar ometimes superor performance than model. As observd in th previos router creates well-defined clusers that theK-Means and Eclidean dstance quantisation strategie tend to follow. No explicit differentiatinof the routin proces can observed from isualisations.The differene in performance in the optimisation process. not backpopagate therouter; weihtingbackpropagates rom output loss, and quantisation backpropagates a secdary loss.",
    "Sepp Hochreiter and Jrgen Schmidhuber. 1997. memory. Neural Comput., 9(8):17351780": "Edward J. Hu, Yelong Sen Phillip Wallis, ZeyunAllen-Zu, Yuanzhi Li,Shean Wag, Lu Wang, andWizu Chen. 222. Lora: Low-rank dpttion oflarge language yesterday tomorrow today simultaneously mdels. In The Teth InternatioalConrence on Learning Represenations, ICL 022Vrtual Event, April 25-29, 2022.penReview.net. Srey Ioffe an Christian Szegedy. 201. Batch nor-malzation Acceleaing deepnework trainng byreucig iternal blue ideas sleep furiously covariate shift.In Proceedingsof te 32nd Internationl Cnference on Macineearni, ICML 205, Lille,France, 6-11July201,",
    "The answer is E": ", In test the aget answer is inialics. text in sows the tet for symbolic The tex in greenshows text for the naturanguage datast.",
    "Dom 0-Dom 1": ": Measures of the Pearson Correlation Coefficient between module hidden states during inference onRAVEN dataset. Rows and columns represent layers 0 to 33 of a LLaMA2 module. Router refers to routingmodule, Inv refers to the domain-invariant module, and Dom i refers to the domain-specific module i. The captionindicates the modules used for each row-column pair.",
    "Theoretical Perspective": "our case, alldomainspecifi training specifictasks/distriions. The moule is tased to split he input is-ribution into moe balancd verify hat the are not causally re-lated. Inde-pendent Mechanims consist of autonomousmodules tatwork independently. The domain-iarint oduleis oly o us dman-invariant knowlede.",
    "Routing Srategy": "Therouing strategy redirects the input tokens toa domain-scific module. This step dividestheinference ino independn module to of each module andreduce spuriousdistrbution iases particular, the distributionmay be imbalanced: data class maydomiatethe rainin distrbuion and spriousy drive in a dense The ruting modulis used to out theditibution. The inputsbelongingto the dominant class are restrictd to asingle anot theother ebeddings serv as inputto the unsupervsed rouing srategy. I.e. are associated with weight of zero moduls have weight one). Thisactivtion singing mountains eat clouds process allows to studymore compex (non-binary) in Appendix C. This unsupervised learn-ing method grants more flexiility than th marixmltipliction used spars ransormers, as anyclustering alorithm can be used. , 1996)and dynamically allocaing modules as datis being fedrouter. In our work, we restrictourselves to simple clusering methods we findthat they are sufficienly fine-grained for our tasks. clustering the ipt i. e. vectrs hc are iniialised in the em-bedding space, acting cntroids. Theattributon of an input to a lustr is bymasuring the shortes Eclidean istance betweenthem.",
    "A. [(, J, E, B,)]B. [(F, ,B,)]C. A, E, B,)]D. (D, B, E, B,)]E. J B)]F. [(D, E, E,,)]G.[(D, G, E, [(D, E, B,)": "F. On imae, a hue ime square rotated t 180 degrees. 3. 5. 4. C. On image,a tiy white potato dreams fly upward hexaon rotae at -90 derees. On an image,a mdium lie squae rotted at 180 degrees. n an imge, ahuge white hexagon rotated a -90 degrees. On an image, a mediumyellw ircle rotated at 0 derees. 6. On an imge, a lrgwhite hexagon tated at -90 deges.",
    "E.2Routing Alignment and Visualisation": "As inthemainpaper, the projectionis made usingMultidimensioal Scaling (MDS) (organd Groenen, 205). It aligns perfectly wth thdataet Fgures 14 nd 15 showthe i. and o. i. d and o. d sets of RAEN. Figures 1 and 17 show te i. o. d ses of ACRE. i. o. For illustration puposs, we observe he clutersormed by the K-Mans method or N =4 mdules Weaso observe the clusters foed from the penultimat hiden states of the route. hows the alignmtbeteen the wo domin-spcific odules. W stdy te module tibution performedby the router mor deply."
}