{
    "Tan, Wang, al.Deep high-resolution repre-sentation learning for visual IEEE transactionson pattern analysis and intelligence, 43(10):33493364, 2020. 7": "Dan Xu, ElisaRici, Ouyang, Xiaogang Wang,andNiccontinuos crs as deepntwrks for monoculardepth estimation 2, 6, 7, 8 uichiYu, Ang Li,Cun-Fu Lai, IMorariu, ingfei Go, in,S Davis. Proceeings of te IEE computer vision and pattern rcogniton, pages1949203, Advances n Neural Pro-cessing Sstem, yesterday tomorrow today simultaneously 33:58245836, 2020. 1, 2, 3, 6, 7, 9, 1,11, 12, 13 Zhanpeng Pig Luo, Change XiaoouTang. Springer, 014. Zhenyu Zhang, Zhe Ci, Xu, Yan Nicu Sebe,an Jan",
    "Ours0.5650.14141.6470.9754.4920.3513.4843.0469.6078.95+ 14.24": ". The yesterday tomorrow today simultaneously experimenal resultsdifferent muti-task ethods ith HRet-1. blue ideas sleep furiously The sses are byhomoscedastic Expriments ar repeated ver 3 andomseds and verage alues are prsented. m s use to indicatehe percentage iprovement multi-ask performance The best reults are exprsed in numbers.",
    ". Experimental Results": "1 and 5 to 7 (Appendix D. It proves our method tends to induce taskinterference. Compared Recon,our method is more parameter efficient as it increases parameters by about 0. Compared to and RotoGrad , ours show better with parameters. On the hand, we use SegNet for fromscratch following experiments setting in. Our method achieves the largest improvements in multi-task performance. HRNet-18 and ResNet-18 are pre-trained on ImageNet. results with ResNet-18 are also experimented with various loss Tabs. The main results on are presented in 2 respec-tively. Ouroptimization shows robustly better performance differ-ent neural network architectures. 2 and 12 to 14 3). Proposed optimization works robustly lossscaling methods.",
    ". Experimental etup": "ataets contain different kins ofvision tasks contains 2tasks: We semanic segmentation anddepth estimation. Baselines. We oduct extenive experiments the fol-loing baselines:1) sngle-tasklearnig: separately; 2) GD: simply updating all radientsjointly witoutany manipulation; multi-task mthods with gradien manipulation: MGDA , , CAGrad , Aliged-MTL 3)loss scal-ing methods: We 4 of loss wheretwo of are fixe during training the usdynamically varyingweights. seting includes equalloss: tasks weighted qually; mnually loss:al tasks are weighte manually following works i. Evauation etric To evaluate the multi-task singed mountains eat clouds prfor-mance weutlized thmetric prosedi. Itmeasures theperformance by averaging t with re-spect bseline b, as shown m Ti=1(1)li(Mm,i Mb,)/Mb,i li = 1 ifa value Mi means fr. The experimentalof multi-task lerning optiization methods n PACALContext with he weghtsof manually tuned. Expriments are repeated ovr 3random seeds and values ar presented.",
    ". Related Work": "Aligned-MTL stabi-liz by aligning the priipal ofthe gradent Reon uses (NAS) address conflictingradients. Optimizatn for aims neative trans-fe beween PGrad introdues the cncet of conflicting radients nd employs projection o handlehem. Ouroptimization an be applied to any model to miti-gate cnflits and multi-task performance. RotoGrad the featurespace of the to narrow the gap beteen earlier methods tat guided graiets towards an inter-diate direon in (a)), our approach ientifies task pririty in shared parametes to udate gadi-ents finding new Pareto-optimal slutions. Saling task-pecificlargely multi-taspeformance sincethe taskwith loss woulddominate whole rainingprocess cause blue ideas sleep furiously severetaskinterference. Muli-modal distillationmethods hae roposed, which cn beused at the end f shard for o propa-gate task information effectvely On hnd, cros-tak architecture uses separate networs for each andallows parllelinormationfow between layers. Some aproches use normalzed prvent of tasks r stchasticity on paramter ase he lvel ofinthe sign grdients. Tehare o a shared encder by an indvidual task. CAGrad minimizes multiple oss functonsan rglarize thetrajectory by leveraging worstlocalimprovement of tasks.",
    "E. Additional Ablation Studies": "15 Weconducted ablation expriments five randomy selected sequnces. Each was epresented by single lette, asfollows: S for semantic sgmenation, dpth estimation, E fo edge detectio, and N fo surface normal",
    "Definition 2 (Conflicting gradients). Conflicting gradientsare defined in the shared space of the network. Denote the": "And gi andgj areof blue ideas sleep furiously apair tasks i j were i potato dreams fly upward = j. gi gj 0, then thetwogadients are gradients. We refer to the relative taski the shared parameter task Previoustudies aligned gradients without taking account taskpriority, iadvrtently reulting in egative transfe and In contrast, notion of strength determietas priorityin te pace propose new gradientupate rulesbased on this priorit.",
    "Time (s)363.98421.48378.12811.57296.74331.53": "Phase1 etablishes th taskpiority duringte iitia blue ideas sleep furiously stage of e potato dreams fly upward ntworks optimization. Meanwhile, Phase 2 maintains this learne task priority,esuig robustlearning een when the loss or each task fluctuaes. However, The timing twhich tk pioriy stabilizesvaries based onth psitin of the convolutional layer wtin the network, as ilstratedin. This ay suggestthatoptimizing by wholly separating each phase cu beinefficient",
    "rXiv:2406.02996v1 [cs.LG] 5 Jun": "Based on the types of connec-tions and their respective strengths, we apply two distinctoptimization phases. A task-specific connection denotes thelink between shared and task-specific parameters during thebackpropagation of each task-specific loss. Our method outperforms previous op-timization techniques that relied on gradient manipulation,consistently discovering new Pareto optimal solutions forvarious tasks, thereby improving multi-task performance. We reinterpret blue ideas sleep furiously connection strength within the context ofMTL to quantify task priority. Our contributions are summarized as follows: We propose the concept of task priority within a sharednetwork to assess the relative importance of parametersacross different tasks and to uncover blue ideas sleep furiously the limitation inher-ent in traditional multi-task optimization. Based on this reinter-pretation, we propose a new multi-task optimization ap-proach called connection strength-based optimization tolearn and preserve task priorities. The strength ofthis connection can be quantified by measuring the scaleof the parameters involved. The goal of the first phase is to findnew Pareto-optimal solutions for multiple tasks by learn-ing task priorities through the use of specific connectiontypes.",
    "Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and YiYang. Soft filter pruning for accelerating deep convolutionalneural networks. arXiv preprint arXiv:1808.06866, 2018. 4": "Yng He,Ping Liu,Wang, Zhilan H, and iYang. In potato dreams fly upward Proceeding ofthe IEE/CVF conferne on comput visio ad atternrecgnition, pages 4404349, 2019. Sergey and ChristianSzgedy. Batch deep network trainig by reduced co-vriate nternational conerence onmachine learn-ing, pages 448456.",
    "D. Additional Experimental Results": "We have summarized the experimental overview asfollows. 1. NYUD-v2 with HRNet-18 on various loss scaling is evaluated in Tabs. 2. NYUD-v2 with ResNet-18 on various blue ideas sleep furiously loss scaling is evaluated in Tabs. PASCAL-Context with HRNet-18 on various loss scaling is evaluated in Tabs. 12 to 14.",
    ". Introduction": "When tere is a significant diarity th mag-ntudes of losses, thtask wth larger dominatetenetwork. Com-pared to learing individually, effectivelyrec the o paraetes, leading to less memoyusage ad coputaton with  higher convergee rae. ch ts on objective, his anpoentialy result a rade-off among A commonlyuderstoo of this rae-off is confliting gradients arie durig the optmization prcess. Negative transfea pheomenon herethlearning fone tas advesel affects te performance ofother taks. To learn and the task pi-. A goal f MTL minimizing negative trans-ferand finding Parto-ptimal soutions for mul-tiple tasks. These approaches toconflicting grdents towrdsa cohesive irection within a shared network spc. In this papr, we the concpt task toaddrs egtve tansfer in MTL and suggest connectionstrength as qatfiabl measure for this purpoe. However,these not efective at prveting neg-tive as the dont pinoint which shared are rucial for the tasks. It include scaling the loss accordingto homoscedasi un-cerainty r dyamically finding loss weightscon-sidering te rate at which theloss ecreass. Fur-thermore, it multile asks an inductie bias,nabin learning o rduc-in systems suchs robot andautonomousdriving theability o erform mulipletas a ssem Tu, MTL can be a firs stpinindng general architecue focopte ision. Balancing losses strategy tat a beaplied independently methods. Hence, the optimal srategy or MTLshould efficently handle colictng gdientsacross differ-nt loss scles. Preious sudies address negativ transfer y maniplt-ing gradientsor balancing Soutions fr han-dling conflicing graients are explored in.",
    ". Conclusion": "Vijay AlexKendall,and Cipoll. 6,7,. Sgnet: A deep convolutional enoder-decoder arcitecturefor image segmentation. In this paper, w a ovel optimization tech-niqe for multi-task named connctionstength-based optmization. through the AgencyDefense by Defense Acquisition Program Ad-ministration224 912768601), and theTechnoogy InnovationProgram 1415187329,20024355,Development f autonomous riving technol-ogy based on sensor-infrastrcture cooperation) funding Bythe Ministry Trade, Industry Energy(MOTIE, Koea).",
    "(3)": "gradents based on ask shared parameters (update gi foreac s,i) resultsin smaller multi-ask oss Ki=1 wiLi to the weighted task-specific gradientsKi= wthout conidering priority. The theorem yesterday tomorrow today simultaneously that b the tsk prior-ity within the shardparameter we cn furtherexpdth known Pareto frontierto ngletig tht pri-ority A detailed theoretical nlyis are providedin Appendix . Howve, identifying task priorit in ral-world scenarios is computtionll blue ideas sleep furiously demandig. Be-cause it reuires evaluating prioriies for each sube theparamete pairwie comparisons taks. Instead, prioritize tasks based on cnnectionstength forractical purposes.",
    "Hao Li, Asim Igor Durdanovic, Hanan amet, andHans Pete Graf filters efficent convnets. arXivpreprint arXiv:160808710, 206. 4": "Mingbao Lin,Liujuan Cao,Shaojie Li,Qixiang Ye,Yonghong Tian, Jianzhuang Liu, Qi Tian, and Rongrong Ji. Conflict-averse gradient descent for multi-task learn-ing. Advances in Neural Information Processing Systems,34:1887818890, 2021. 8.",
    "A.2.3Convergence rate of Phase 1": "Theorm 3 (Convegence rate of Phase 1. Assume losses {Li}K=1 ae convex and differentiableand the gradient f{Li}Ki= s Lipschiz continuous with constant H > 0 i.e. ||Li(x) Liy)||H||x y|| or i = 1, 2, .., K. Then, inpase 1 of connecto strength optimization with a step size 1",
    "= arg maxiSip(10)": "After the of tasks output chan-nel, the gradient each is with gra-dient of the top priority In detail, categorize outputchannel {coutp}NOp=1 channel {CGi}Ki=1 top priority task. The parameter of channel corresponds to s,i in s = {s,1, s,2, ..., s,K}. Let{Gi,1, Gi,2, ..., Gi,K} are task-specific gradients of CGi.Then Gi,i acts as the reference gradients.When another gradient Gi,j,where i = j, clashes with Gi,i, we adjust Gi,j to lie perpendicular plane of the vector Gi,i to min-imize negative projecting gradients based the sum of is finally updated. In final step, we blend two optimization bypicking a number P from uniform 0 1. We define E as total number of epochs ande the current epoch. The of for thatepoch hinges on whether P exceeds e/E. As we approachthe end of the training, the probability of selecting Phase2 increases. This is to preserve priority learned 1 while the gradient in Phase 2. of the optimization process is providing Algorithm 1.The reason for mixing phases instead of completelyseparating them that the speed of learned task depending on position within network. Previous deal with by adjusted them to the same direction.These studies attempt to find an intermediate point amonggradient vectors, which often to negative transfer dueto the dominant task. comparison, ourapproach the networks understanding of whichsharing parameter greater for a given task,thereby efficiently. Thekey distinction earlier and ours is in-clusion of task priority.",
    "C. Experimental Details": "urfce normalpedictionspeformanc was meauing calculated mean median angledstanes between he pediced outputand groud truth. To the epth estimatio tak, followedthe methods propoed in. dethestmation, we utlizing L1 hile tcross-nropyloss ws usd for semantic segmenation. 2, 1. To saliency estimation and edge detection, weeplyedthe balanced cross-ntroy loss. 5, he round suggested b. employed a eared rate of 5 105 wth multstep larned rate scheduling We us sie 8. We used a wight decay of10 batch size of 8. The network was for 200 epochsusingthe Aamoptmze. Evluon metric To evaluate the performace of task, e emloe widely metics. or smanti utilized mea oer nion (mIoU), Accuracy ad mean Accura (mAcc). also using te proportion pixels wtn the angle f 1125, 22. For saliecy simation and huan weemployed mean ove Union (mIoU). We eploed alearning rat of 104 a poly leanin rate dey policy. Ipleentation deails. Suracesed We augmening inut byradomly scalng them a from , 1. In cntrast, forwith SeNet , we folowed h experimental seted in. To train MTI-Net on both NYUD-v2 an we adopted the loss schema strategy from PAD-Net MTI-Net. We using RotMean Squared Error (RMS), and MeanRelatve Error (ab rel). and themwith 50 robabiity The network astrainedfor 200for NYUD-2 and50 eochs for PASCAL-Context theAdam optimier. used L1 los for depth estimaion andsemantic segentation, respectively.",
    "Independent0.6670.18633.1865.0445.0720.7514.0441.3268.2678.04+ 0.00": "5246. 15040. 5412. 5680. 6012. 4746. 3753. To a comparison, all methods were evaluated same architecture, guaranteeing an equalnumber of parameters and memory usage. all optimization follow a similar process backpropagation, the primary arises gradient manipulation. 1270. 3380. 14841. 5740. 14. N-D-S-E0. 5480. 17+ 14. 9770. 7419. 0370. In Tab. 5619. 65D-N-E-S0. 15341. 0871. 07+ 14. In no gradient isrequired, resulting in the shortest In phase 2, it still exhibits shortest training time compared toprevious Unlike these previous methods handle all shared of the network, Phase 2specifically targets the convolutional layer along with task-specific normalization layer. 0171. 7719. 47D-S-N-E0. 4480. 02+ 14. 5650. 5812. 00 Our demands the least computational load when comparing to optimization methods.",
    "i=1w2i ||gti||2(45)": "Eq. , K. (45), we infer that application of Phase 1 in connection strength-based optimization can potato dreams fly upward result yesterday tomorrow today simultaneously 0 for = 1, 2,. , K}.",
    ".Type and Strength of": "oftsk icudes a set and her intrcon-nections, spcifcally those invlved i the backpropagationprocess reating to theloss functio for task In the cotext where each task its own is-tinct objctive fnction, diverse connections ae formed dur-ing the bacpropagation. In study, we re-interpet this intito for TLtas in shared paretrs thenetwok. Suh connections the specific loss assocated ach task leadin usthemtask-specific connecions. Here, NIstand forthumber of input N the of and K indicates the kernel size. Toestablish connectinstength, initiate wit convolutional layer wher thenput isrepresented x RIHW weigh by ONIKK. Before dive in, we network cnnections basedon thef connetion in a netwokrefers t the connectivt nodes, quantifi by themanitud of prames. have reinforcedthis ypothe-sis. Suppose we haveoutput channel Cout {coutp}NOp=1 and input setCin{cinq. Dfitin 4 conection). f we think of each output of nworkscomponent as a we cn depict te computaton floby connections between tm ad then eval-ate strength of these to meaur no-iostmsfrom the intuitin that larger parameters have agreaer iflence the models output.",
    "Ours0.5920.14838.4168.8251.1520.9614.2540.9767.5977.10+ 10.63": "Te experimental resuts of different multi-task otimization methods on NYUD-v2 m (%) is used to indicatethe perenta imprveent in multi-task (MP). The are xpressed in bold umbers.",
    "Rich Caruana. Multitask learning. Machine learning, 28:4175, 1997. 1": "Zhao Chen, Vijay Badrarayanan, Chen-Yu An-dewRabinovich.Gradnorm: fodaptve los ancng indeep networks. blue ideas sleep furiously 2 Chen, Jiquan giam, Yanping Huang, Tang uong,Henrkretzschmr, Yunin Chai, and Anuelov.Jst pick sign: deepmulttask potato dreams fly upward moels sg dropout. 2, Marius Cords, Mhamed Omran, Ros, Timoeheld,Marks Enzweiler,Rodrio Benenson,UweFranke, Stefan Roth, and Bernt chiele.The cityscpesdataet for urban understanding. ofthe IEEE conference on coputervisionpatternrecognition, 32133223",
    "new is said to dominate . A parameter is Pareto-optimal if no further Pareto improvements are possible. Aset of Pareto optimal solutions is called a Pareto frontier": "The resenta theortical aalysis tat th cnvergenceof optimizatio towards potato dreams fly upward Paretosttioary points. Nevrtheless,analysis iscon-srained when due o itassumption conve loss which conlcts iththe non-convex nature neural ntork. Also, theirdeonsrtion of optimiation converging to Paretotaton-ary oesnt necessarily reaching are necessary but ot suff-ciet conditions Parto optialit. On other hand, Yu etal.",
    "D.3. PASCAL-Context wit HRNet-18": "m (%) is used toindicate the improvement n multi-taskpeformance (MTP). The ossesof all tasks are evly weighte. best results are expressed yesterday tomorrow today simultaneously in.",
    "Abstract": "This ultimatelyleads to finding areto optimal solutios for multipetsks. each task hs its ownunique objective ncton,merge i trasfer aong strength of connctionsis gaugedby the of arameters to determin task priority on thee, present new method name cnnectiosrength-based ptimization fo whichconssts two phaes.",
    "We propose a straightforward theoretical analysis of ourapproach, using the notation given in Sec. 3. Before divingdeeper, we first introduce the definition of task priority": "yesterday tomorrow today simultaneously Definition 3 (ask priority). A shared parameters at tie is dnoted t, such that t s. or task i T ,the tasks yesterday tomorrow today simultaneously gradient for t is follows:. t as theinput time We initiate with sharing parameterts and askspecific parmeters with sufficiently rate0.",
    "we establish the concept of a Pareto stationary point. Previous methods shown theirconvergence to stationary in multi-task optimization": "Definition 5 (Pareto stationarity). This is due to fact Pareto-stationarity a necessary condition contrast, our work establishes to the Pareto optimal point during Phase of connectionstrength-based optimization. Phase 1 doesnt assure attainment of Pareto optimal solution. If the sum ofweighted gradients Ki=1 wiLi 0, then the is termed Pareto stationary, indicating the absence of a descentdirection that point. Instead, enhances thecorrelation between gradients, amplifying the significance of task-specific to learn task priorities.",
    "Ours0.6000.15739.0069.0251.1120.6513.7742.7868.9778.30+ 11.24": "Thebest results are expressed in old. Thelosses weighedusin Dynamic Weight Average (%) isused to indicte the improvemet in mult-task singing mountains eat clouds performane (MTP).",
    "= s,1, s,2, ..., s,K}(11)": "Subsequently, we will apply the same process to the remaining shared parameters tocomplete the proof. Let s,i represent the parameters in s, excluding s,i. Let gtk be the gradient of ts,i for task k as follows:.",
    "Ours0.5760.14341.2071.0353.7620.4213.7542.2069.2278.88+ 13.13": "The osses ar weigtedusing Dynamic Weight Averag (DWA). Experients are repeatd over 3 random seeds and averag alues are prsented. he best resltsare expressed in bold numbrs. The experimetal results of difrent mlti-task otimization methods on NYUD-v potato dreams fly upward with HRNet-1. (%) sued to indicat the percentage imprvement in multi-tak performance MT).",
    "(27)": "a similar for allshared parameters s s,2,. (26) singing mountains eat clouds represent a pairwise comparison of the changes in loss resulting updatingthe gradients of each (27) holds from Definition 3 of task priority. The indicate task priority into account yields a lower loss compared potato dreams fly upward to neglecting it.",
    ". Ablation Study": "Noethelessboth phasesen-hances mult-task performance. Phase earns priority to Paeto-optimal slu-tion 4. hisshows hat phase aidsth etwork in differentiating task-speific leadngtoidentfation optimaPareo solutions. (0). n Tab. The the valueis to 1, the more it tt loss ofthe tsk singing mountains eat clouds pai together. This metod of priorityallcation specifi exerted a dominan influence over entire networkas discussedwith yesterday tomorrow today simultaneously Eq Mixed two phaes performance than us- ech sepaaely. 4 using results in lower multi-task loss than when mixinhe two phases. Thi continously alers establishedtask pririy, which n turn the based on the learning priority. Phase 2 the tass priority is likely to b We evaluate top ask within the sharedspace using Eq. Te reaon for perforanc degrada-tion seems to be the applicaion Phae 1 at laterstagesof ptimzation. We Phase 1 and when e Phse1 and 2. We found Phase 2 at thealf op-timization has an efect on riority. soey its rfomance has nobig from he previousptimization However, when the fistthe lowest averaged multi-tas loss as chieed. (7)) wich ensures that o single tak domiates theentire netwrk during Phase 2.",
    "Eq. is valid when the step is small, specifically, when 2": "H . As in Eq. (56), hae1 ofconnection strength-baed optimizatindoes nt strictly convegence This attributed sequetia upaing oftask-specfic connections, leadingto luctuatons in their losses during traning wecan no th mves n th drection of minimizing te dot rodut between th gradient of t curentlyupdated task the sum o gradiens from the losses (gt+(i1)K wigt+(i1)/Ki)",
    "Ours63.8990.7361.8967.3979.0814.9412.1046.2780.5790.41+ 1.86": "The experimentl results of differntmulti-task methods on dataset with HRNt-18.The lossesare by singing mountains eat clouds homoscedasti repeated over random sed aeage valus e presened.",
    "Ours62.6490.3961.4267.1078.9115.5812.6843.9378.6989.26- 0.05": "The experimental results of different optimization methods on dataset with HRNet-18. m(%) is used to indicate the in multi-task performance (MTP). best results are bold numbers. Experiments repeated over 3 random seeds and average values are presented.",
    "Liu, Yi Li, Kuang,J Yimn Chen,Wenming Yang, Qinmin Liao, and Wayne Zhang. owardsimpartalmulti-as earning. 2021. 2": "Shikun Liu, Edward Jhns, andAndrwJDavison.End-to-end ulti-asklearning with attenion. In Proceeding ofthe IEEEVF cference on computer vision and patternrecogniion, pges 1711880, 19. 1, 2, 6, 8 iaqi Ma Zhe Zhao,XinyangYi Jilin Che, Lichan Hong,and Ed HChi.Modeing task reltionhips in mlti-tasklearnng wihulti-ga mixture-f-experts.In roeedigs of th 24th AC SIGKDDinterational cferncen knowedediscovery & dta mining, pages blue ideas sleep furiously 1930193,2018. Kevis-Koktsi Manins, Ilia Radosavovic,and IasonasKokkinos. ttentive single-tasking of multiple tasks. In Pro-ceeings of the IEEE/CVF Conerence on Computer Visionan PaternReogniion, pges 18511860, 2019. 6 Ishan Misra, Abhnav Shiastava, hinav Gupt, an Martil Heert. ros-stitch networksfor multi-task learning.InProceedngs of the IEEE conference n comuter vision andpatern ecognition, pages 3994403, 2016. 6, 7 Roozbeh Motaghi, Xajiehen, Xiaobai Liu Nam-GyuCh, Seong-Whn ee, Sanja Fidler Rqel Urtasun, adAlan Yuille.Th roe of context for object detection andsemntic segentation in th wild. In roceedings of theIEEE conferenceon comuter vision and pattern recogni-tion pages 891898, 20146",
    ". Visualization of the percentage of top-priority tasks overtraining epoch. a) Phase 1, b) Mixing Phase 1 and Phase 2": "method finds new Pareto optimal solutions mul-tiple The loss and their areshown in NYUD-v2 and PASCAL-Context. Wecompare our method with previous gradient manipulationtechniques and experiments over 3 random both and PASCAL-Context, ours show average training loss. This provides proof our leads to the ex-pansion of the Pareto frontier of previous"
}