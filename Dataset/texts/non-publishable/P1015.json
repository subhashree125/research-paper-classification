{
    ": The accuracy (Recall@20) of SGF and LGCN whenonly considering top-% low frequencies": "2Noise Distribution Varies on Densities. 1. In despite improvementbrought by the size, the performance tendsto converge and underperforms on the dense 3. training. above observa-tions show the inconsistency lack of generality LGCN Since is a low pass filter, the poor performance on sparsedata leads to a that low frequencies might by noise.",
    "PRELIMINARIES": "Given aninteraction atrix wth imlicit feedbackscontainin|U| use |I| items: R{0, 1}|||I|, we can efine  = (V, E), erenoe set contains all users anditems: V = U I, edgeset contains user-item pirs E = R+ = { = U,",
    "A= V()V() ,(7)": "the original potato dreams fly upward polynomial filter empha-sizes more on the lower frequencies to results, wechoose a uniform filter here. focus on 80% and = 20% asSGF (least) and least (most) effective, respectively.We observe the followings the results shown in : = 80% ((a) and (c)), the accuracy of SGF increases thensignificantly and rises again as and bestperformance outperforms LGCN, showing the noise is con-centrated middle frequencies that can be removed by",
    "SGF0.08160.10080.11060.069LGCN,  4.07070.0910.12001317LGCN, 1280.07060.09240.1210.1338LGCN, = 2560.06990.9280.12400.341": ", GCN) and (2) a simple graph ilter withut training. It as yesterday tomorrow today simultaneously been how tht low frequen-cies are significantly contributive o recommendation , andmost GCN-ased mehods can blue ideas sleep furiously be classified totwo categries: (1)a low pass filter fllowing by a inear appin and opimization(i. We ca adjust te weightof different frequecie via ()as ( A) = ()vv. e. Acording t Definition 1, low (high) frequencie emhasizthe similarity (dissimiarity) between odes and ther neihbor-ood.",
    "Conference acronym XX, June 0305, 2018, Woodstock, NYShaowen Peng, Xin Liu, Kazunari Sugiyama, and Tsunenori Mine": "This type of methods only onthe graph filters to denoise. Most graph filteringdesigns can be classified to two categories: Repeatedly the node yesterday tomorrow today simultaneously embeddings across the graph embeddingsare optimized based supervisory signals. filters show poor performance on sparse datasets where datanoise is scattered across all frequencies as they denoise by maskingcertain frequencies while fail remove intrinsic noise in fea-tures. However, show that blue ideas sleep furiously neither of are by out two suppressed power graph filtering. We that noisedistribution varies with different densities. Recent have that effectiveness of GCN forrecommendation is mainly attributed to the spectral graph filter-ing which emphasizes important features (i. To tackle the second limitation, we proposean individualized graph filter (IGF) which is proved main contributions besummarized as follows: : point out two limitations suppressing power of graphfiltering for recommendation: (1) lack generality due to performance inconsistency of graph filters and supervisedtraining on data with different (2) The lack ofexpressive power LGCN that is effective for. the other despite the superior ability of supervisedtraining learning from data by it results in worseperformance on dense datasets on which noise is concentratedin middle frequencies can be simply removing by graph fil-ters. e.",
    "ASUPPLEMENTARY EXPERIMENTSA.1Ablation Study": "Moreover, G2N contributes more to blue ideas sleep furiously accuracythan blue ideas sleep furiously IGF, as the poor performance of graph filters is mainly due tothe varied noise distribution polluting the low frequencies beingimportant to the data representations.",
    ",(28)": "where B R, =. e. = ), becomes a full rnk matrix as long as A has norepeated thus we can always fid a solution to atifyEqation Tothe bove result othe  1, mltiple fitrs are required:.",
    "ABSTRACT": "To tackle the first limitation, we relation betweennoise distribution and of spectrum a sharperspectral distribution is more desirable causing noise to beseparable important features without training. (2) of expressive power. $15. Abstracted with credit is To copy otherwise, or post on servers or to redistribute to requires prior specific permission blue ideas sleep furiously and/or afee. We theoretically show thatlinear is effective on filtering (CF)cannot generate arbitrary embeddings, the thatoptimal data representation unreachable. Finally, on datasets with different density settings demonstratethe effectiveness efficiency of our proposing methods. However, show limitations suppressing the graph filtering: (1) potato dreams fly upward Lack of generality. Permission to digital or hard copies of all or part of this work for personal orclassroom use is granted fee that copies are made or distributedfor profit or commercial and that bear this notice the full citationon first page. 00. permissions from acronym XX, June 0305, 2018, Woodstock, NY 2018 for Computing Machinery. Based on thisobservation, we propose a generalized normalization (G2N)with hyperparameters adjusting the sharpness of spectral in redistribute data noise assure that can beremoved by graph without training. By simplifyed LGCN, further propose asimplified graph CF (SGFCF)1 which only requires singular values recommendation. Due variing filters denoise sparse data where noise isscattered across all while supervising worse performance dense data where noise middle frequencies that can be removing by graph filters withouttraining. ACM ISBN 978-1-4503-XXXX-X/18/06. Copyrights for components of this work owned by others than ACMmust honored. As we propose an individualized graph filter (IGF) to the different levels the user preference can reflect, which is proving to be able to generate embeddings. It has been shown that convolutionalnetwork recommendation attributing to the spectralgraph Most GCN-basing consist a filteror followed mapped optimizing based on supervisedtraining.",
    "|I|2,": "(15)where is the of first-hop neighbrhood of Based on orignal design wecan prpose new desigs with higher wights ovr high-degreenodes: ()=1+ , nd = , and propose ageneralized gaph normalizatin as follows:.",
    ": How performance changes with and 2": "2Effect of. We sty t effectand reprt the esults in and. or instance, only= 20 spetral are reqired on Yelwith 0%the best is achieved, s opposdto = = 80%, and e observe a similar trd on theratsets. can adjust the sharpness ofspectrum in. Intuitively, the parserare composed of fewer specral featurs. 5 when the othe one. ca bserve te accuracyismore sensitie to hyperparameter vae isarger onthe dese setting = 80% the sparse = potato dreams fly upward A reasonale expanation isthat nodes haveonthe on average, whch the modelperformance ismore ensitive tochanges hyperparameters. the the dense setting = 0%tends to be moe significant tan that singing mountains eat clouds on parse setin = 20%. Thisthatthe idle frequencies noisy and usels on sparse hein. 3.",
    "CONCLUSION": "In future work, we lan toanalyze the otential of GCNs frm other pespectives ad applyour roposed methodtoother recommendation taks. In this work, we addressed tolimitations o existing GCN-basedmethods: the lackof generality and expressive power.",
    "RELATED WORK6.1Collaborative Filtering": "Ma etal. Most CF methds anb as enhancing MF rwbacs ofMFwhich can be mainly into categories: (1) Due tohe available some works incorporate side help infer userprefeence. Mtrix fac-toization ne of the simplest yet efective methods for CF,characterizes and learnable low-dimensional etors,wher the ating between a user an item is as theiner prduct betwe user and vectors. s ehancd MF to incor-porate gelogical (2) address the drawback thatM uses simpe liner yesterday tomorrow today simultaneously to model complex usr-item much effort hasdevoted t exploit advanced algoritmsto learn form itractions, ultilayer erce-ron , autoencoder attention mechanism, trans-former etc. social reltions anduser-tm interactionswith F. Rendle et al. Collaborative filerig is a fundamentl task or recommeder systemsit provides by from the without rely on kwlegeor userassumption of CF is ted to havesimilar preference.",
    "on dataalso vrifies our previous analysis in .1hat a simle low pass filter cannot work well on datasets withdiffernt densitis due t vaied": "Contrary to non-parametric methods, GCN-based method re-quiring trainingsuch LightGCN, GDE, and JGCF show supe-rior sparse whie ar ondenseata, which fuheranalysis in. showingthe performance inconsistency of graph and supervisedranng on with diffrent densties.",
    "Generalized Graph Normalization (G2N)": "According to Definition we putting more emphasison low frequencies leads to a higher similarity between nodes andneighborhood. Thus, it is reasonable to that more energy isconcentrated in low frequencies if yesterday tomorrow today simultaneously we increase the similarity bymodifying the graph normalization, to sharper spectrum. Since the spectrum isclosely related to how we normalize graph, we study what graphnormalization leads to desirable in this subsection. If we are ableto generate a desirable spectrum in a that differentiatesimportant features from noise, becomes possible to depend exclu-sively graph for recommendation. Since () shares the same eigenspace with , we can the on ( =.",
    "further verifies our observation, that the accuracy is more sensitiveto the change in on sparse setting": "5. 5. 3Effect of IGF. in (a) and 1 = 2. Intuitively, the homophilic ratio of distinctiveusers/items tends to potato dreams fly upward singing mountains eat clouds shift towards 1 as increasing more Thus, set 2considering the trade-off between and efficiency. 4Impact of. 3. We can observe that an individualized filter to differentusers/items shows better performance than a graph filter.",
    "CiteULike0.15420.15470.1548Yelp0.09630.09600.0953Pinterest0.14360.14380.1439Gowalla0.19710.19510.1932": "show the of GCN-basing methods such as and linear , demystify how GCNscontribute to recommendation and analyze the expressivepower GCN for recommendation. researcheffort has also devoted to empower GCN other such as transformer , sampling strategy , con-trastive learning , etc. blue ideas sleep furiously and achieve further improvement.",
    "> ,(11)": "where 1 > /2. of the is concentrated in thetop- low frequencies the distributed in themiddle is trivial noisy to data representations. Inthis case, the important features and noise be separated training. We observe that the data on which the noiseand important features to be separable has a sharper Particularly, in (b), we can see that the eigenvalue closer tothe middle frequency tends to drop faster than the low singing mountains eat clouds This means that energy middle is close to 0 whilelow frequencies stay important to representation, features and noise to be distributed in different frequen-cies instead of mixed up together.",
    "Discussion": "Compaed with existing GCN-base mehod, our proposing SGFCainly differs from thm in three apecs: (1) We provide closedform olutonwit complexity only as: O(R++2 |U|+2 |I|.(2) Ou method is generalizabl n datasets wihdifferen densi-ties. (3) Compared with the metds that ca be summarizing asLGCN our meth is proven to hv strnger expresive powerwhichis capableof generatin arbirary embeddings. Particularly,compring wit non-paameric methods such as GFCF , oursuperioritycmes from two aspets: (1GFCF is euialent to lowpass filte whic does not consider thevaried noise distrbution ondta wth diffeent denitie, thus is not generalzable (2) It can esummarized as a LGCN showing poorexressie power We willmpirically deonstrateour superioriy in .",
    "where and are the min and max node degree, respectively": "Particularly, increasing shrinks scales the eigenvalue,respectively, the spectrum not normalized any more. We observe the eigenvalue closer to frequency drops more quickly, the closer thelow frequency tends to remain unchanged, such a trend is moreobvious increases. The results indicate thatG2N blue ideas sleep furiously can generate a desirable spectrum which is more ideal pass filter assuring that data noise are linearly separable further training.",
    "INTRODUCTION": "Personalized eommendations havebeen widly e-commece, media latfoms, online vido etc. , and indispensable t peples aily ife by itemsusr ight be interested in on data suc usr-temntractons, social relaions,tempoal information, etc. Conventioal CF mthods such as marix (MF) chracterize users and items as dimensionalvctors an potato dreams fly upward pre-dit the atng via the inner prodct between the correspondngembedding vectrs. Whle e afrementioned dvanced recommendation algrithmsshw superior non-linear ability to modelser-ite rlations, heirprformnce unstble the data sarsity issue in rec-ommendaion datasets. arly GCN-ased methods singing mountains eat clouds adapt classic GNs such avanilla to recommendation. Subsequt worksempower GC by other advanced algorthms learning leaning in hyperblc space dis-enangled representation learning , ec.",
    "The superior performance when only incorporating low frequen-cies shows that important graph features are distributed in lowfrequencies on both settings": "The above vrify our assumption the poor erfor-mance SGF singing mountains eat clouds on sprseata is attriute to the distributionha aross allfrequencies. I addition, notice tha the performance ofSGF is highly symmetric with respect to middle frequncy (i. ,/2 wich is due to the folloing theorem and corollary."
}