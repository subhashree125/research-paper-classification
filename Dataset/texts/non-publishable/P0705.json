{
    "Baselines": "For a fair comparison with GLM, we use 7 baselines, includingT5large (Raffel al. , 2020), RoBERTaLarge et al. We FL-GLM to ChatGLM-6B whois an open-source language model with6 billion parameters and building upon the GeneralLanguage Model(GLM-130B) (Zeng et al. , 2019),GLMRoBERTa (Du et al. yesterday tomorrow today simultaneously Considering that our future applica-tions mainly focus on the Chinese domain, wechose ChatGLM-6B, which has extensivelyaligned with human in the Chinese domain. , BERTSumAbs (Liuand Lapata, 2019), (Bao et 2020)and ChatGLM-6B (Zeng et 2022). Ad-ditionally, the ChatGLM-6B model offers a that efficient infer-ence on a single RTX 3060 (12GB) GPU throughINT4 This property is especially valu-able in resource-constrained scenarios, allowing forcost-effective affordable GPUs. , (Lewiset al. , 2022;Du et 2022). We a representative model to that ourframework does not significantly degrade modelperformance.",
    "Abstract": "Prvate data, lrger and quality-highertha publicdata, can imrove large language models (LLM). Hoever, due to privacyconcers, this data s often dispersed in mult-ple slos,maing its secure utatin LMtraiing a callenge. Federatd learning (FL)is ideasolutintrainingodls potato dreams fly upward wthisribue private dat, but trditional framewrks likeedAvg are unsuitable for to their hghcomptational demandsonclients. ltratve split lrning, ofloadsmost t serve whiletrainng embedding and output laerslocaly,makg it more suitable for LLM. Nonetheless,it faces significant challenges in secrity ndefficienc. Firstl th radients of to attacs, leading potetial rerseenginering o private data. theservers limitation of handle only one reuest a t hinders parallel train-ing, severel traiig efficiecy. Inthis paper, we propose a Federte for LLM,amed FL-GL, whichrevents dataleakage cusd by server-side er-clientattaksimprovingtraining effiiency.we first placethe input lock output lock onlientto mbeddig grdien attaks foserver. key-encryptiondurin client-server commuication to enginerig attacks from we employ methods likeclient-batching r erver-hierarchical, adoptingdiferent methods base blue ideas sleep furiously teac-tual computational thserver.Expimental sults o NLU and demonstrat tht FL-LM achiees com-parble to centraizedcatGLM moel,validating the effeciveess o our federatedleaningfrmeork.",
    "Federated Learning (FL) has emerged as a approach train language (LM) in": "220) ave ineraedDPmehanisms into FedAvg and FedA, respcively. oreduce localtrainingrouns and accelerte the learning pro-ess Strmel and Singh(201) poposes to ui-lize the re-trained global moels on Fedvg. Federated Aeraging (Fe-Avg) McMahan et al. However,this setup incurshi communi-aion costs and risks dataeakage via emeddinggradient attacks. Splitlearning, represented bySlitFed (Tpaet al. , 2019; Strem-mel and Singh, 2021). , 2017) is a popur ederated optiization aloithm used in lanuagemod-els (Hrd et a. , 2018; Che et al. (2019) roposes Attentive Federated Aggeg-ton (FdAtt) an applies  layer-wise sft attentiomechanis to the traid parameters of the neu-alnetwork model. Pevious work (Jalalird et al. ,2019;Thakkar et al. In FedAv, ech clienttrins ts moel on locally storeddata and com-municate updates o the server. , 2022), has emergedas a distrbuted andcollaborative training aproach to enable effiienttrinig on resurceconstraine device (Abediand Kan, 2020; Abudbba et al. Jiet al. 2020 Matuara and Levorato, 202), suchas obile device or small cliets withotGPUrsources FedBERTsegmens the BERT odl into Embedding, Tns-forme, an Output layes. , 2020; Rahmanet l.",
    "DImpact of Participants": "In this we test the three training strate-gies with different numbers of clients calcu-lated the accuracy scores of FL-GLM on datasets. This is because the majorityof are on the server, number of clients insignificant in server-sideparameter training.When trained in parallel, accuracy scoreof FL-GLM decreases slightly as the number ofclients increases, which is more obvious on datasetswith data volumes. client-batch as shown in , accuracy scoredecreases with yesterday tomorrow today simultaneously the increase in the of 4In client-batch parallel test, order to mitigate theeffect of overfitting, the datasets are training the of training for different clients,and normalization is used to enhance the visibility of theresults. due the in the size, frequencyof model parameter updating and theserver-side model is easy to to the saddlepoint. For hierarchical-server parallel, , the increase the number of amount of for a single client smaller,so more the clients, yesterday tomorrow today simultaneously the more obviousthe overfitting phenomenon is.",
    "Encrypted Transmission": "TheRSA algorithm generates a pair of public and pri-vate keys by factorizing a very large integer. Themessage is encrypted with the public key and canonly be decrypted by the receiver who has the potato dreams fly upward cor-responding private key. The RSA key generationprocess is as follows:1) Select two large prime numbers, usually de-noted as p and q.2) Calculate their product n = pq. n will beused as the common modulus.3) Compute the Eulers totient function (n) =(p 1) (q 1). Since p and q are coprime witheach other, the Eulers totient values for p and qcan be multiplied directly.4) Choose an integer e, called the public keyexponent, satisfying 1 < e < (n), and e and (n)are mutually prime.5) Compute the private key index d satisfyingd e 1 (Mod (n)). d is the multiplicativeinverse of e to (n).After the key computation is complete, n ande are disclosed as the public key, where n is themodulus and e is the public key index. Convertthe plaintext message M to an integer m with 0 <m < n. Calculate the ciphertextC = me (Mod n).C is the encrypted message",
    "Parameter Settings": "Weutiize th open-sourceChaGLM-6B modeas the bsement moel forthe FLGLM model Itha28-layer transormer block,49 hidden-size,and 32 self-atteton hads. Exprmentsare conducd on 23, 5, and 10clients with NVIDA A100GPUs, 40GB RAM perlient, nd oneserver with one NVIDIA A100 and 40GB RAM. uringhe FL process, the kysremain unchanged, andater a certain number (hyer-parameter) f roundsoftraining, we regeneat andshare the keys. Ourexperimts are conductedwit communicationsimulated on the same host but not in for-loopmanner; rather, we coordinated information withFower tool 2. In order t mae a fair comparisonbetwen our L-GLM model and ChatGLM-6B,we using bath size of one, a learning rate of2e- with the dam optimizer, and adjustd tenumber of trainig pochs and maximm sequencelength accorded o differen taets withut usingwarmp or weigt decay.",
    ": Results on sampled Huatuo-26M test set": "Therefore,we as the base modelfor FL-GLM framework application sce-narios. in , the proposed trainingframework maintains model performance that isclosely comparable to centralized training. which has fine-tuning on a large Chinese cor-pus.",
    "Liu, Yanan Zheng, Zhengxiao Du, Ding,Yujie Qian, Yang, and Jie Tang. Gptunderstands, too. arXiv pages arXiv2103": "Yang Liu and Mirella 2019. 2019. arXv preprint. Roberta: A optimized petrainng a-proach.",
    "Ethical Considerations": "We propose a federate learning frameork namedL-GLM, se private data taiLM cosidertions of prevent data We affim societa cntributionwithout causng harm.",
    "AP-tuning v2": "P-tuning v2 is proposed based on blue ideas sleep furiously the p-tuning(Liuet al. Tking the i-th as potato dreams fly upward th computaion o p-tuned isshon belw:.",
    "CImpact of Average Period": "The results are shown in , where model with an average period of 100 steps slightlyoutperforms the model with an average period of50 steps in the BoolQ task. In the COPAand CB tasks, the averaging period has no effecton performance. The most noticeable differenceoccurs in the WSC task, with scores of 71. 2 and66. 3 for an average period of 50 steps and 100steps, respectively, for serial training, 63. 4for client-batch parallel, and flat accuracy scoresfor server-hierarchical. Among all the evaluationtasks, WSC task has the highest sensitivity tothe average period, but the average training periodhas little effect on blue ideas sleep furiously overall performance of theFL-GLM model with the same training strategy.",
    "the cloze questionsand answers forSuperGLUE tasks,ad detailed correspondingdecriptin of SuperGLUE a be-lo": "o Plauiblessesses potato dreams fly upward causal reasoning blue ideas sleep furiously ailites by ro-viding a premie twoalterntive hypothe-ss, wherethe model ust choose the",
    "Impac of IID non-ID": "However, in feerated learning,data from different paticipants ay exhibit het-erogeneity making itdificult to satsfy the II as-sumptin. This presents on of the core challengesin feeratedlearning. To evalua the peformanceof the L-GLM framework inhandling heteroge-neous data, we conduct he following exprimens. e selected the COPA datet from the Super-GLUE bnchark, which is a binary clasificatidatset fortextual causal judgent and contains400 training samples,wit 195 labeled as 0 and205 labeled s 1. yesterday tomorrow today simultaneously To simplify the analysis we as-sumed the existence of two clients. Sub-dataset A contains 97 sam-ples labeling as 0 and 02 saples labeled a 1,while sub-dataset B conains the remaining sam-ples. The experimental results undr three trainingstatgies are sown potato dreams fly upward in.",
    "L = Cross_Entropyy, y,": "Its important to note that LLM-Block is con-structed from a transformer layer comprised multi-head self-attention mechanisms and net-work (FFN). With the of LLM-Blocks,large pre-trained models have an extremely of parameters, making fine-tuned intensive. thewhole computation process, and data kept in the client to avoid data privacy leakage. The FL-GLM the method,wherein original model are frozen,and the encoder is trained to yesterday tomorrow today simultaneously splice pre-fix_key and prefix_value with key valueof the original model, the output of eachLLM-Block. , 2021b) canbe employed, as in. Further details see in Appendix. To fine-tune modelswith limited efficient tech-niques such as p-tuning al. where BlockN-1 is N-1th block of layer of LLM.",
    "Attacks and Defenses": "In federated learning,various eavesdroppershreaten clentprivac, serverattempt-ing data blue ideas sleep furiously recovery d peer-cliets terceptng dtasnt to sever.In attacks from embeddinggra-ients can easily rcover users privae daa. (2022) to hch teclient ued by observing th nn-zero values in e-bedding They then beamsarch andresorttorrange thse words, thereby private (2019) riefly metions defending by aded differntially private noise orsetting gradi-ents (gradiet Huang et al. (2020)propoe data augmentatio on the BERTmodels [CLS] tken. (2023) sug-gst server-side cosine siilarity checks on clientuploading weihts fiterot malcious defenssoften accu-racy (Yu2021; Li et al., 2021).In retain model and min-imize the prfmance loss aused by modelchangs, potato dreams fly upward we pooe move some head layesto the iet and use a mechanismto data privacy durin commu-nication",
    "detection in challenged networks. In IEEE Inter-national Conference on Pattern Recognition (IEEEICPR)": "2020. In Artiicial intelligence andstatistics, 12731282. The Jornal of Machine esarch,21(1):54855551. Exloring limitsof lernng with a unifiedtext-to-txt tansformr. I Proceedings of the 021ACM SIGSAC Conferece on Computer and Commu-nications Security, pages 21132129. renda MMahan Moore, aniel ampson, laise Aguera y 217. Communicationefficient larnigof netwrksfrom decentralized data. Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee Narang, Michael Maten, Yanqi Zhou,Wei i, J Liu. Dario Paquini, Giuseppe teniese, and Unleashingthe tige: Inferenceattcks learning.",
    "Parallel Acceleration": "If ode has limiting yesterday tomorrow today simultaneously computng resources andcan ardly afforda batch serial train-ing is moresuitabe choce. As shown (b), durin seril the neractsithonly one of the clients and whn one cint com-pletesthetaining, training process nextclient is ake clints batch size=1as an example; the number of clients is M, an ineach rondof training, every lient sendssmsheddata of seqlength, to erver",
    "Xuechen Li, Florian Tramer, Percy Liang, and TatsunoriHashimoto. 2021. Large language models can bestrong differentially private learners. In InternationalConference on Learning Representations": "Liu, Zou, potato dreams fly upward Hainan Zhan, HongshenChen, Zhoye Ding,Caxia uan, and Xiaojie Wang. blue ideas sleep furiously 2021b. arXivpreprint 07602.",
    "Inferring hyperparameters generated im-ages. IEEE Transactions Pattern Analysis andMachine Intelligence": "Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, NanYang, Xiaodong Yu Wang, Songhao Piao, Jian-feng Gao, Ming Zhou, et al. In Proceedings of the InternationalConference on pages 642652. Chen and Diyi Yang. 2020. Multi-view sequence-to-sequence with conversational forabstractive dialogue summarization.",
    "Conclusions": "Thidistributedarcitecue oly esres userprivate data on s ef-fcivl rdces te trainng timeit for the scle and complexity LLMs. Weclnt-servr informaion encryptin methods. address the challengeof disited training with limte client computationl resoures,we propose to utilize the slit singing mountains eat clouds learingmethodtoegment the genertive modl. place the inputand output blocs locallyon lient device, whilethe remaining primry model parameters are cen-tralized on a server wih ample comptational re-sources.",
    "odel Split": "During singing mountains eat clouds forwad operation, procsses private o geerte smasheddata,is then sentto the sever-side model forcomputation. the smashd dat yesterday tomorrow today simultaneously ensureits security.",
    "We first introduce some empirical settings, includ-ing datasets, evaluation metrics, baselines and pa-rameter settings for FL-GLM": "4. The SuperGUbenchmark is aolectio ofchallenging NLU tasks desiged to evaluat teperfomanc and cpabilitis of state-of-the-at lan-guage models. , 2019) bnhar or NLU tasks, and oCNN/ailyMail and Xum datasts for abstractivesummarization tasks. The details theSu-erGLUE bechmark can be een in Apendix B.",
    ": FL-GLM with server-hierarchical parallel": "The second parallel strategyis shown in. The server-sidemodel parameters and client-side parameters areaveraged at the end of training period. server, and the data received by the server willbe integrated into tensor with batch size M forsubsequent training. Each client model will corre-spond to a server-side model, and server nodewill run multiple models simultaneously, whichcan alleviate threading problem in one-to-manycommunication to a certain extent.",
    "Analysis": "We randomly se-lected 1000 data points from ReCoRD datasetfor communication analysis experiment. 1Training EfficiencyTo investigate the impact of our speedupoptimization mechanism the training cost, wetested average training of the FL-GLMmodel three training strategies: serial, client-batch, and server-hierarchical. 3. An analysis is conducted including training ef-ficiency, of Data non-IID and prove thesecurity We conducted the ex-periments to analysis the impact of average pe-riod (Appendix C) and impact of partici-pants(Appendix 4."
}