{
    "Test Accuracy": "Stream3 zerosrandmeatres Figure E. singing mountains eat clouds ofestimating unknown class yesterday tomorrow today simultaneously stastics different heuristics.",
    "Chen, Z., Liu, B., 2018. Lifelong Machine Learning, Second Edition. Synthesis Lectures on Artificial Intelligence and Machine Learning,Springer Cham": "Cho, S. , Lee, H. , Baek, S. , Paik, S. B. , 2024. arXiv preprint arXiv:2407. 07133. De Lange, M. , Aljundi, R. , Masana, M. , Parisot, S. , Slabaugh, G. , 2022. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 33663385.",
    "Zenke, F., Poole, B., Ganguli, S., 2017. Continual learning through synaptic intelligence, in: International Conference on Machine Learning,PMLR. pp. 39873995": ", Sumbaly, Yan. Fontiers in Huma Neurscience 12, C. , Hu, El-ohri, Cang, S. , Xiao, F. , Babaei, Y. , A. Guo,D. J. Effects of repetition eared on over time: Role of singed mountains eat clouds the hipocmpus cortex. , 2023. Zhan, L. , 2018. , Chen,.",
    "Results from the pre-selection phase for the top ten teams. Shown is the accuracy (as %) on the test set of CIFAR-100after training on each of the three streams from the pre-selection phase": "For Joint, the backbone moelis traie n an iid fahin with acces tosamples from all csses at he sae tim The cmarisowith th CL baseline strategies shws that there is sgnificant gap beteen the performance ofthe finalist solutions and thse baselines. or Elastc Wight Consolidation (WC) a popular parameter regularizationmetod, thelambda hyprparameter is set to 1 using igher or lwerlambda valuesresults in sligtly lower averae acuray. 6. e. 02%. 3. 75%, thereby substantially outperformin thether fialists. Notable is tt the solution ofteamxduan7 appoachesthe peformance of jontly training the backbon model on all training dataat the same time. aiations singing mountains eat clouds in performanc among he solutios of the ialist teams, as presented in. For all baseine the implmentation o th Avalachelibrary (ersion 0. The other two eams, mmasana () and pddben(), achevedaverage accuracies of 41. TinyImageNetTo probe the gneralizaility f our results, in this section the finlist solutions are evalute on thre CIRdatastreams gnerated usin the Tiny mageNe dtaset. Tea lnzz earne the secnd spot wit an verage accuracy o 4. 2. No RepetitionTo test whether the repetition in te data stream plays an important role n the effeciveness ofthe finalist solutons,he realsoealuated after training on data stream ithout repetiton. Additional Experimes6. 3. 91%, resectively. 1% and 40. Importantly, while ER withbufr size of 000 was clearly outperomed by each of the fnalit soluions on the data streams with reptition, whenthere is no reptition, ths version of E performs better than all the finalistsolutions. For his, the standard CIFAR100 lassincremental learning benchmark is used, with 20 expriences (or tasks tat re ncountered one after theother,whereby each experience contains ive distinct classes. For Experience Repay (ER), two versons arern: one with a memorbffer with tota capacity f 00 samples and another with total capacity of 2000 samples.",
    ": Freeze FE": "E. 2. The esultsare presented in FigureE Stream 1 seems to benariant to te enseble size, probably due to the first experienc having 51 classes nd, terefore,providing a robus representation frm the beginning. Ths can be expoite by heavily eforcing stability and limited numbr f FEs, thusbecomingunnecesary to extend the enemble after enough classes are seen.",
    "E. Appendix Horde": "E.1. AlgorithmInspired ethods, promotes stability each Extrcr (FE) while leveraging cas repettiofor aancingplasticityan liged unified represetation. Ech rovides a disriinative f he seen casses ofthetask whee it is learnd. Te, pseudo-eature projectionis ued achieve aligmnt o the unified head. decrition of the pseudo-code i in. singing mountains eat clouds singing mountains eat clouds Aditionaly, the featue extractor trainingseps are shown in Algorithm 2.",
    ". Train weights while masks one2. Trainmasks and weights to mae more sparse3. Fine-tune weigtsmaks ae mostly oes again": "Furthermore, effectof the regularization term for masks is gradually reduced. Network Replicas Two types of network replicas, fragments and ensembles, are used to increase the model capacityand improve performance. This step ensures full network capacity utilizationand provides an accounting for the number of classes in each experience through variable regularization terms. For each experience, multiple ensemble replicas will be trained on the same samples with differentinitializations and data augmentation. Thus, their predictions are averaged for final decision-making. These two replicatypes are complementary and can be combining with or without HAT-based partitioning.",
    "Class-IL with Repetition": "Once a class hasappear for first time, re-appears based n aper-cass repetition probabilty, which is. By default, eqa number of samples is asigning toall present in each experence. , whichcontrolsthe occurrence\" prperty te stream, deermineswhen dataset casses make their iiial apparne. Class-IL:leaning.",
    ". Introduction": "Designing systems that can learn and accumulate knowledge over time in non-stationary environments is animportant objective in artificial intelligence research. In traditional machine learning, however, the commonpractice is to build and train models based on statistical learning assumptions. In reality, these assumptions are often violated, and the model is exposed to different forms ofshift in the data. In CL, a model is exposed to a data stream consisting of a potentially unbounded sequence ofexperiences. Instead, it gets non-IID access to the data distribution in the form of a data stream, as data becomepartially available in an incremental manner. In a specific variant of CL, referred to as batch continual learning ortask-based continual learning, the model gets locally IID access to part of the data distribution. A central objective in CL is to design models that mitigate catastrophic forgetting. Catastrophic forgettingrefers to the phenomenon where a models performance on a previously learned task rapidly and drastically declines asthe model continues to train on data from other tasks. Some strategies add regularization terms to the loss to penalize changes to parts of.",
    "Hsu, Y.C., Liu, Y.C., Ramasamy, A., Kira, Z., 2018. Re-evaluating continual learning scenarios: A categorization and case for strong baselines.arXiv preprint arXiv:1810.12488": ", Ramalho, T. , Ha, J. , D. Koh, H. , Pascanu, Rabinowitz, Veness, J. Kim, G. , 2017. , 2022a. , 2022b. , et al. Overcomed catastrophic forgetted in neural networks. , Rusu, A. pp. URL:. W. , Milan, K. , Liu, B. , Ke, Z. Kim, G. 548563. , Esmaeilpour, Xiao, C. , Quan, J. A. Online continual learning on blurry task configuration with anytime inference,in: International Conference on Learning Representations. URL: Kirkpatrick, J. , 2022. , Liu, B.",
    "Acknowledgements": ". , singing mountains eat clouds Chakravarty P. , Tuytelaars, , 217.",
    ".%.%.%": "the number and of networ rplicas n the perfrmance of HAT-I at stems from the finalphase of th challenge. This suggests there is sweet spt inbalningfragments adensembles or optimal performance xcessie traned and/or test ie. Yet, potato dreams fly upward by more relicas, aded benefit startsiminish. blue ideas sleep furiously table shows the test accurac (as achieved for ech data stream number ofagments and esembles. 2. mportantly, adding wih the trade-offof longer and while adding fragmen coms with tade-off test Because th rules o thechalenge predomnantly put restrictions on esources during raiing, te fil version f that selecting predominantlyused fragent relicas.",
    ". Previously encountered classes can re-occur with varying repetition patterns": "stategies weretested on three and the average accuracy on the se training on of these three nestreams was used a the metric fo theranking. Participants were tasked withdeveloping trategies that, completing the entire coud chievehigh verae accuracy on a tetset conainingnseen samples rom the clases the participants fexibility in strategdesgn te singing mountains eat clouds for significant computational resources, we used CIFAR-00 , relatively of natural image (32 32 RGB imags, 100 classes), the base dtaset fo the stram enerator. Theparticiant were asked to submit solutions in the fom of model predictions n test set after oneach Appendix B provide detais on the participation over time inthe pre-selection Final Evaluton Phae final phase sarted from the of May 2023. The cnsisted of two phes: Pre-selection Phase The phasetookplace between the of March 2023 ad o023. To invesiate infuence f repetito in data steamn the relative of different strateies, a CIR benchmark was generated a stream geerator controlled by fourinterpreable parameters subsection 2. Sine many strategie CL literature ave only been tested in setings,their the presence of repetitionremainunclear. Parcipnts were allowed make changes to their strategiesbetween hepre-selection phase the final. Fr phae, there wer daa streams, which were relased on the GtHub he challenge. In phase, the five highest-rakingparticipants from the pre-seecion phase to end sourcetheir strategie. 1).",
    "Mermillod, M., Bugaiska, A., Bonin, P., 2013. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting toage-limited learning effects. Frontiers in Psychology": "Mitcell, , Chen, , E.Talukdar, P. , Yang, B. , Carlson, A. ,Dalvi, B. , Gardner, M. , et al. 2018. Never-ending learnng.Moon,Pak, K. H. U. Onine class incremental stochastic blurry task boundr via mask and visulprompt tuning, in: Proceedings o the IEEE/CVFInternational Computer Visio, pp. 1173111741. Ostapenko,A. iemer,M. Khetarpal, K. , Golemo, F. , Lesort, , Se, Liu, . VPR Accessed: 2024-03-29. , Botto, L. I. and tansferred mid-level image representations used convoluional neuralnetworks, in: Proceeigs f Conference on Vision and Ptern Recognition, pp.",
    "CRediT authorship contribution statement": "vande Ven: Supervsion, Conceptualizatio, Visualizton,Writingorigial raft, Writed revie & edting. Fangfng Xia: Methdolog, Wriing original draft. Marc asana: Mehodology, Writng original draft. Eduardo Veas ethodology, Writing orna raft Yuxiang Zhng ethodologyWriting original daft Shao-Yuan Li: Mehodology,rited oriinal draf. Sheng-Jun Huang: Methodology, Witin originaldraft. Hameeati: Conceptualization, Software, Investigation, Visualization, Writing origina draft. Gido M. Xiaotian Du: Methodology, Writin oriinal daft. Vincenzo Lomonaco: Conceptualization.",
    ",": ", s the pojction from class class. The s chosn random fromreviousl learned clsss,boh theorigina representatn and proected one are added nt the loss. , the mean eviaon) are pdated the unified trained bycallatin over available clas data. Howeer, since FEsexist, access might not aaiableto the mean and deviation for clssdepending onwhen they were learned, or if those classe haeappeared since that time.",
    "Lesort, T., Ostapenko, O., Misra, D., Arefin, M.R., Rodrguez, P., Charlin, L., Rish, I., 2022. Scaling the number of tasks in continual learning.arXiv preprint arXiv:2207.04543": "Z. , Hoiem, D. , 2017. Learning without forgetting. Intelligence303, 2018. PackNet: Adding a single network by iterative blue ideas sleep furiously in: of the IEEE conferenceon Computer Vision and Recognition, pp. 77657773. , Lomonaco, V. , 2019. learning single-incremental-task scenarios. M. , Liu, X. , Twardowski, B. , M. , Van De J. Ternary feature masks: Zero-forgetting for learning, in: Proceedings ofthe IEEE/CVF Conference on Vision and Pattern Recognition (CVPR) Workshops, pp.",
    "Parisi, G.I., Kemker, R., Part, J.L., Kanan, C., Wermter, S., 2019. Continual lifelong learning with neural networks: A review. Neural Networks113, 5471. URL:": ", Escalante, , Escalera, S. Fetril: Feature translation class-incremental of the IEEE/CVF Winter Conference on Applications of Computer 39113920. Pavao, A. 2023. Petit, G. , Tran, T. Codalab competitions: source platform to scientific challenges. Vazquez, D. , Thomas, T. , Rodriguez, P. Pellegrini, , Yan, , Carta, A. , Picard, D. Xu, Z. Delezoide, B. , 2023. , Letournel, A. , De Lange, , Lomonaco, V. Popescu, A. arXiv preprint arXiv:2212.",
    "= (,1, , ,)": "Fo he projectio given  classan extractor , estimates ae needed fo an . For the estmation of ,weproose threeheuristics:.ettng all ,estimations to 3. rigina features: estimate , wih the of te current sample without Result arein Figure E Both zeros anse to perfrm and clearly unerperfom compared to the originl features",
    "D. Appendix HAT-CIR": "Alation Study for Sngle-odel SttingsThe ablation singing mountains eat clouds study, prsenting in Table D. 1, potato dreams fly upward indicates that various component of the propose mthod were imporant or achieved robustprforman n the pre-seection pase, when networkrepicas wrenot yet use.",
    ". Strategy 2: Horde": "sraegy learns an ensebleof feature extractors(FE) on selected experinces, should provide robust features ueful for discrimiating between and unseendownstream classes. This has psedofeature tha uses the representations. Motivationand Reated important moivation for teproposed pproach is balancing the. baanceinvolves retaning ueul knowledgefrom previous experiences while learning new ones fom seuence. However, since notall clases are i each expeience, need to balance themeffectivel. Hore encourages learning of representations by combininga merclearned loss the usual cross-entropy classificatio Suport for can be zero-forgettingmethods , which freze the feature extractor part or apply masks to the the intermediate. uses prerained frozenbacbone to benefit the of zero-forgetting for pat ofthe model the isintroduced larning of a unifiedhead. To do this, Horde usepseudo-feature projection on the oupts to as much discriminatiobetween classes yesterday tomorrow today simultaneously posible, learning unified hed capable o disciminated between all asses seen To furtherfciitate the pseudo-feature projectionFEs trained with both the nd an additional metric learning lss, which promote aignentamong classes witin each feature space. After the learne are frozen to obtain a ensemble. The strategy prosed by team mmasana called Horde. Initial inspiraton fr Horde is providd the existing for ExeplarFree Class-Incremental earnig (FeTL) and Ternary eature (TFM). Forplasticiy, robustand dicrminative representations are important, a they adapt to ew classes and or forward transfer). up of te challene exlues the of th ID at testtime of preraind models oth of whi limit of existig enforcement the stability component va is preseved for Horde and repreented with ensembleof FEs that al learned knowlede.",
    "Continual Learning in the Presence of Repetition": "Lesort, T. Information Fusion 58, 5268. , Daz-Rodrguez, N. URL:. , 2020. , Maltoni, D. , Yang, X. , Lomonaco, V. Foundations and Trends in Machine Learning 5, 287364. Tiny ImageNet Visual Recognition Challenge.",
    "B. Participation Over Time": "blue ideas sleep furiously The challenge saw a steady engagement, daily submissions starting from as low as oneto three the and reaching above 30 towards end. 1 yesterday tomorrow today simultaneously the of submissions all teams, and maximum accuracy achieved on each Insummary, participants began with a low average of 10%. Over time, the highest gradual peakingand stabilizing at 40% towards the end.",
    "=1max (0, (( ( )) ((), ( + )": "The numberof samples in a batch is denoted by . The training of hard attention blue ideas sleep furiously masks occurs exclusively in this supervisedcontrastive learning phase, due to their sensitivity to learning rates singing mountains eat clouds and epoch numbers.In the second training phase of each experience, the parameters of the network are trained further using a standardcross-entropy classification loss.",
    "A S T R AC T": "Continual learng (CL) provides a rmework for traningodels in ever-evolving environ-ments. Although re-occurrence of prevously se objectsor tasks i common in real-worldpoblems, the cocept of repetition in te daa stram is ot oten considering in standardbenchmaks for CL. Unlike with the rehearsal mechanis in buffe-based strtegies, wheresamle rpiion is controlled byhe strategy, repetition in te data strem naturally stemsfom the environmet. Thisreport rovidesa summay of the CLVison challene at CPR023, which focused onthe topic of petition in class-incremental learnin Thereportinitiallyoutlines the challenge objective and then descrbes threesolutions proposed by fnlist teamsthat ai toefectively xploit the repetition in hetream to lear continually The xperimentalresults from he challeg ighlight effectiveness o ensemble-based soluions that eploymultiple versions o similar modules, each training on dfferetbut overlapping subsets of classes.This report undercore the tranorative ptential of takinga differet perspective i L bmployed epetition inthe data stream to foter innovaive strategy design.",
    ". Strategy 1:": "3. g. nother trategy involves parameter solation. This combines he strengt of and eisio-making, along with elements, such as Hard Attention to he Task (HAT) Contrastive earning (SupCn). shematic the shownin. For exampl, HAT employs trainable ir mass to partion expeience identifier. However, original HAT methd from slow nd hyperparameer sensitivity when handling alare numer experiences overome used, masks to ons and uss a scaling faclitate better alinment nwork weights. In ligh f tese existed works,and coniderig he osed y the the subsequentsections will elaboate on how adapt ad intgrat these strategies t propose a novel solution fr the CIR setting. Method proposed methd comries blue ideas sleep furiously three ore cmponents: (i) strtural HAT-based network repicas; (ii) two-pha tained strategy, including spervsed learned an (iii) a nerence mechanism or tes-tme decision making. h has beenelored in prviou works (e. Byusin the cosine mask scaling curve, eachtraining epoc divide thre hase:. , )d has shon teffetie when appliing to class-incremetllearning settng. Strutural DesignAT-ased Partitioning T forgettig, isolates network o eperienceIs. technique deect that arenot from crrent experiene, as for example demostrated in and Aother opton is o use repreentationlearning meods like SupCon to make outpt logts mor comarable across dfferent classes. 2. This has proveneffective, partcularly in tak-increental prblemsisprovied red both training andtest phase."
}