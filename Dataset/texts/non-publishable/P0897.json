{
    "Abstract": "CRAGis question answrigdataset aime at realistic question ndaswering RAG relate task, witha diverseset of topics,quetin types, and questions featuringenities of vrying popularity systm achieving 2nd placfor 1as ell s3rd place onTask",
    "Retrieval Augmented Generation": "no relevant information is given and thequestion not answerable without further context, yet the model istraining output target answer regardless. Further analysis would berequired for answer, we suspect that this. use the provided dev for with the 500set used for retrieval comparison treated as our holdout set. Not all samples with high loss will incorrect labels, being hard examples, yet typically of disposingof labeling samples difficult ones. with candidates, we use Llama 8B to augmentgeneration with the relevant context for the Our initial prompt structure started with the thenall of the candidates prior to Llama model noticed that often due to how much context wouldbe provided, model would occasionally forget the questionbeing asked. training Llama training LoRa models for each taskindividually. Our initial approach for determined samplesneed relabeling has been previously. Given the penalization of in we try to take steps to mitigate further hallucinations dueto fine-tuning, as it has been observed that fine-tuning LLMs a source further hallucinations. Initial experiments, that this method notwork well finance questions. A common andsimple to filter/relabel labeled samples is touse a particular training loss after training to the pointof over-fitting.",
    "Future Work": "Observations we had during the ompetito wee instances ofcatastrophc forgetting due to ou attemptshallucinaions. afer LoRa training, like this and wereoften with \" dont n cases where blue ideas sleep furiously the anwer wasnotdiscovrale in h retrieved informaion.Metho o preventtisis somhng ar in ursing in uture Additionally, we hope to explore lrgr models, 70B+,in fuur orthis singing mountains eat clouds task. We were to gtte 70B modelrunning in competition computeenvironments, o did ot spendmuch tie looking at lager",
    "Task 3": "Finally, Task 3 represents extension blue ideas sleep furiously of Task the systemhas access to both HTML and knowledge graph API. This task meant measure both the computational efficiencyof the well its ability filter large amounts ofpotentially irrelevant information.",
    "MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question AnsweringKDD Cup 24, August 2529, Barcelona, Spain": "Inteestingy, uderperformsthe ther asks in head, torso, and tail. Our sytem ale smila to he SOTA sstems featred the CRAGpaper,despte obviousy beng smaller system oerall. Interestigly,the false premise questions were the olyte whre ou traiingsetup always kept the originl target labl, rathe han taget to dont know\".",
    "Conclusion": "We demonstrated the effectivenessof trainingndivdual LoRa adapter for the 4 asks in the pipeline, spcificalyAPIcall geerain an Task 1,2, and answer generation. CRAGpresentsavaretyof diferet tasks nd qustions to allow the track-in the progress of various methds using to build RAGsystems. Thepenalization of halucinations s a uniqueand impotant feature asfuture AI systems beome increangly commn thrughout society,as hallucinations hurt user trust in these ystems.",
    "Introduction": "the rising capabilitis of LLMs, inreasingly Permissin to k digital o copies all or part ofthis work for pesonal orclassroomis grnted withot fee provided that are not made o distrbutedor profit or commercial advantge and that ear notice and full itatonon the firt page. To arious tasks we adaptrs of arious tasks, API call generation required foaccesing iformatin the mock. hus, rakng both he efficacyof RAG achitec-ture as s traking remains difficult. Copyrights for omponent of by others han theauthor mst be onored cpy otherwse, orrepublis to post servers o toredistribute to ists, requires prior specifc permisiond/or This led to highprole causing concernwith The CRAG scoeaims to haluciatedanswersecourages reurning missing answers, torturning \"i ont know\" from the odel scores of1, 0,and correct, missi, hallucinatd answesrespetiely. Th key element theCRAG bechmark i its scoring metric which explicitly punisheshallucintions.",
    ": CRAG Score results on the test dataset calculated via manual assessment": "is due to the fact that loss in hallucinated when numeric outputs is likely less than string outputs. For example, with a \"What was Apples closing price to-day?\", with hypothetical correct answer \"$203. 51\", a prediction 52\" would likely not result in filtering via this We instead determine these samples by first the systemwith the base Llama 3 model, with prompt indicating to alwaysproduce a for each 4 candidate retrieval approachesmentioned previously. We use GPT-4o to whether any ofthe answers are correct. If any correct, the originallabel is retained training, \"i dont usedas the target label. We repeat this processfor task, given that each has access to different sources,to generate a training dataset for each LoRa adapter. We use the repository for trained the LoRaadapters, the default LoRa configuration. 01 1. demonstrate the effectiveness of relabeling in. We ran3 different answer setups for the sample Task 1 holdout set we created. The is unmodified Llama 3 model,the second is a LoRa model the original targets, and LoRa model with relabeling shown, using originaltargets provides the accuracy, but worsens",
    "Related Works": "the benefits of pro-viding additional text context for seq2seq models for NLP tasks thatare knowledge intensive. 5 points while only 10% parameters. The initial approach of Lewis et al. Hu et al. discovered thatwhen replacing a dot product between vectors with an in-termediate dot product of a much rank vector, the impactson performance were minimal further reducing the trainingparameters required. The use of modifying a models out-put training the entire which substantially saveson memory when training. Using BART, they were able tasks using dual biencoders for retrieval andtraining the jointly, without need for whichdocuments LoRa has become a popular adapter approach,particularly for LLMs as they grown years.",
    "Task 2": "2 reflects a more sophisticated scenario, where systemis with same HTML documents before, now has access to a knowledge accessible via a REST API.",
    "Ensemble (mean rank)0.308-0.128": "that fail to make a successful call are inspected and manually cor-rected if the correct function call is clear from the initial predictionand the question. Again, the manually modified targets are evalu-ated only on successfully calling an endpoint, though not validatingthat the relevant information is returned by the API. Any questionwhere the target cannot be quickly modified is changed to a targetof \"None\".We acknowledge that this approach to annotation is not opti-mal, as it likely results in successful, but incorrectly selected APIendpoint calls. However, manually annotating each question todetermine the correct API call and validating the returned infor-mation were indeed relevant would have been too time consuminggiven the size of the dataset.",
    "Task 1": "For the first task t system must process 5 candidate HTML doc-uments fo generating answers, reflectin a standard web-basedAGapplcation.Acaveats thahe 5 candidatsar smpled frmthe top-10 relevnt documents retrive fro yesterday tomorrow today simultaneously web sarh. Thu,ther i no guarantee that the relevant inforion for answeringthe question isactually fund within the top 5 dcuments.",
    "CRAG Dataset": "Additionally, twsurces of whchpose difficulty for LLM are how dynamic questions answeris and popularty of topic the question."
}