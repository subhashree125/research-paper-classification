{
    "Reinforcement Learning Problem Setting": "The sequential decision-making problem defined as a Markov decision process (MDP): A, , d0}, where S and A are state and the respectively, P : S A the transition R : S A S R reward function, [0, 1) is the discountfactor, is the initial state distribution. The is learn policy (a|s) to maximize the of discounted Ek=0kr (sk, ak).",
    "Introduction": ", 2024) ifusion-QL, utilizing arobabilistic (DDP). Prallel to these developmets, diffusio have been adapted to reinorement learningscearios, asillutrated by al, 022) and furter enhanced in te efficint diffusionpolicy (EDP) (Kang et l. imilarly, in offline learin(RL) derivingeffective policies offline datasets can be simplified to the task of develig a probabiliticmoel trajectory an area wheregeneraive have be highlysuccessful. InDffuser diffusion model on indataset, nd a classifirmodel is rained to the cumulativ trajectory samles. In reent sigifican havebeen made in imge though the advance-ment ofgenerave dels. b incorporatigonly te stte the actin seuenceDecisin trains n ives ynamicmodactions. Liewise, Decisin Diffuser a singing mountains eat clouds conditionl diffuion with statesequences as inpu utilizigthe return as a conditioning for guidance during samplig. uring th inference thediffusion model, comining with classiier guiance, is employed to sample trajectore wit high eturns. , 0)unrscore the eficacy applying dffuion models to plannig withn mode-basd RL frameworks. ,and Decision Difuser (Ajy l. Existing models such as Diffuser (Janner al.",
    "AAppendix": "Th medium-replay datase present rplay buffer of mediumlvel age, encompssing adiverse rangeof suboptimal actn along ith exploration noise. As demonstrted in we achived more than 2-fold icrease in speedwithut any loss in performance with N=2 for Cosisitency Panning andN=20 for Dcision Diffuser. The medium datasets ar prouce exclusivel by edium-levelpolicies therefore exhibit a reater poportion of uboptimal actions comparing to the medm-expertdatasets. is a full list of datast size. Ourexpeimnts utilize D4RL atasets comprisng three distinct dta types for thHopper, HalfCheeth, andWalker environmets: medium-expert, medum, nd mdium-eply. choice of ifusionsteps n depends on the defaul training hyperpramters in Ajay et a. The results in sho th inferencetime andcorreponding performanc of Consistncy Planning andDecsion Diffusuer in diffeentdatasets, ie, opper-r, opp-m, hlfcheetah-eand walker2-me, withdiffusion steps N = 100 fr ecision iffuser and N 2 or Consstency Planning. The medium-exper datasets containtrjectories generated through a combination ofmeium-evel and expert-levl polcies, offering blendf both optimal and sbopimal actins. We se the latest version otaset in D4RL (Fu et a. The resuts in show the relatonship bwen thecomputational time, denoised steps N and corre-spondng peomance ofConsstencyPlanniand DecisionDffuer onthe tas hopper-medim-expert-v2. Eah cel contains mean ad standard devition over 5 ran-dom seeds in same erver. (022) wh claims all tasksuffer o perfornce loss with N = 10. , 2020).",
    "Conclusion": "By combined the score-basing diffusion model proposing by Karras et al. While our work increases inference speed compared with its diffusion models counterpart(Ajay et al. This data collection process introduces delays that may hinder the effectiveness ofreal-time applications. Future work should include: 1) combining improved techniques in trained consistency models (Song &Dhariwal, 2023), such as designing a changing weighting function and noise schedule more suitable forreinforcement learned scenarios; 2) combining the consistency inference process with changing guidanceschedule (Ma et al. , 2023) to improve quality of trajectory sampling; and 3) investigating online learningstrategies to reduce delays and improve performance in real-time scenarios. As consistency models are appliing to larger and morecomplex environments, such as autonomous driving or stock market trading, number of variables andpossible future states grows exponentially. (2022), one-stage guided distillation(Luo et al. Effective real-time decision-making requires access to up-to-dateand accurate data streams. , 2023), and conditional model-based generative model for sequential decision making (Ajay et al. Managing this complexity in real-time, while ensuring consistentand reliable outcomes, presents a significant challenge without the use of advanced computational techniques. ,2022), the consistency model in this paper achieves comparable performance in gym tasks with its diffusionmodel counterparts, Diffuser and Decision Diffuser, and obtains a significant speedup during inference inoffline settings.",
    "xti+1() := (sk, sk+1, . . , sk+H1)ti+1.(1)": "tis ntation, k ofa stte trajectory , H represents horzn,and ti+ is blue ideas sleep furiously te timestep in theCnsequently, xti+1() is defined as a oisy sequence ofstates, as a two-dimenional arraywhere eccolumn potato dreams fly upward correspond to timestep of tranin process, the sub-seence ti [, T] follows the Karras (Karaset al.",
    "Diffusion Models in Reinforcement Learning": "models offer a versatile approach for augmentation in learning. (Luet employs diffusion to enhance both offline and online RL datasets, subsequentlyutilized off-policy Although this boosts performance, relianceon unguided diffusion to approximate distribution faces challenges due to distributional MTDiff (He et al., 2024) implements unguided data generation in multitask environments. Additionally, diffusion models have adapted for training world models. For Alonso et use diffusion to train world models, achieving precise predictions of future observations. However,this method does model entire to compounded errors and lack policy guidance.In a related effort, Rigter et al. (2023) integrate policy guidance to enhance a diffusion world model in onlineRL. et al. offline providing a theoretical and for thetrajectory distribution shaped by policy guidance. models also been adapted for representation in RL, capturing the distri-butions in offline datasets. Diffusion-QL (Wang et al., 2022), applies the diffusion model withinthe of both Q-learning and Behavior Cloning (BC) for policy However, the of Diffusion-QL is demonstrates inefficiency the necessity process-ing both forward and backward through the entire Markov during training. To alleviate these issues,Kang et al. (2024) introduce action approximation, the need to execute denoising processduring the training process. Diffusion have been employed in recent studies for human behavior imitation learning et al.,2023) and trajectory generation in offline RL. Trajectories that include states actions are generated byDiffuser et al., 2022), using an unconditional diffusion model, guided a reward trainedon noisy state-action Diffuser al., 2022) models the trajectories with the datasetusing a unified, conditional generative model, avoiding separate training a classifier for reward functions.",
    "f(x, , c, t) = cskip(t)x + cout(t)F(x, , c, t),(5)": "During the distillation process,he guidance scale andn are sampled uniformly frm the interals [min, mx] and {1, , N 1},respectively. he trajctory and returns tuple (x, c) are sample fm he dataset.",
    "Planning with Cosistency": "how guided consistency distillation isapplied, and the consistency model is integrated with the Decision Diffuser framework during inference. , Karras et , making directdistillation from the discrete-time used in Diffuser ineffective. 1 details the diffusion model, followed by explanation in. The consistency drawfrom principles of score-based diffusion models al. In following, we discuss howwe use consistency models for trajectory optimization process.",
    "Marc Rigter, Jun Yamada, and Posner. World models via trajectory arXivpreprint arXiv:2312.08533, 2023": "Chitan Saharia, Willia Chan, Surabh Saxena, Lala Li, Jay Whang, Emily LDenton,KamyarGasemipour, Rpal Gontijo Lopes, Burc Kaagol Ayan, Tim Salimas, et al Leaning and adapng agile potato dreams fly upward locomton skills y trnsfering exeience. rXivpreint arXiv2304. Deep unsupervisedlearningusing nonequilbrium thermdynmics. In Interational confereneon machine learning, pp. 22562265. PMR, 2015",
    "Erik Nijkmp, itch Hil, Zh, and Ying Nian Learning no-cnverget no-persistentsor-run mcmc towardeergy-ased model. in Infrmation ocessing Systems, 32,2019": "Deepak Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Chen, Shentu, Shel-hamer, Jitendra A Efros, Trevor Darrell. In Proceedings ofthe on computer vision and pattern recognition workshops, pp. Pearce, Tabish Rashid, singing mountains eat clouds Anssi Dave Bignell, Sun, Raluca Georgescu, Sergio ValcarcelMacua, Shan Tan, Momennejad, Katja Hofmann, et yesterday tomorrow today simultaneously al. Imitating human withdiffusion models.",
    "Consistency Models": "Song al. To accelerate the sampling process in diffusion models, consistency model significantly numberof steps for compared the original diffusion model, without substantially compromisingthe performance. The data process in this framework reverses along trajectory {xt}t[,T ] of the ODE,starting samples xT T is a minimal constant close to 0 addressnumerical issues the boundary. The consistency models proposed by Song et al. is achieved approximating consistency function, f :(xt, t) x, which a noisy sample step t to the original sample. Diffusion models operate by introducing Gaussian perturbations data into noise, followed bygenerating data samples through a of denoising steps.",
    "Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. Advancesin neural information processing systems, 34:2013220145, 2021": "ill Grathohl,Jrn-Henrik avid uvenaud, Richrd Zeml. discepancytrining ad evaluating models without sampling. 37323747. PMLR, Haran He, Bai, Kang u, Weinan hag, Dng Wang, Bin Zao, and XuelngL. Diffusio odel is an effective planner data synthesizer for ulti-task reinforcement learning. Advances in nual infrmation processing systems, 6, 024. yesterday tomorrow today simultaneously"
}