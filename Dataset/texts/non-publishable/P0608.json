{
    "The pixel images rendered according to the word compo-sition and writing system are called linguistic symbols": "ically support a finite vocabulary of categoricalinputs, e.g. characters, subwords or even words,and much effort has been devoting to vocabularyconstruction (Gerz et al., 2018). As the numberof languages the model can handle increases, thevocabulary needs to be extended or reconstructed,which is a burdensome task. Taking into accountthe symbolic properties of languages, linguisticsymbols always correspond to concrete abstract se-mantics and have unique compositions or structuresfrom visual perspective. In MTLS, words are firstconverted into linguistic symbols and rendered aspixel images. Then, by replacing unique encod-ing in vocabularies with the linguistic symbols ofthe words, the problem of multilingual vocabularyconstruction and out-of-vocabulary (OOV) can beavoided, as shown in (a).Universal meanings are representing by differ-ent linguistic symbols in different symbol systems.These universal meanings underlyed all humannatural languages are referred to as irreducible se-mantic cores (Wierzbicka, 1999). This semanticcore can be used as a semantic bridge for transfor-mations between different languages. Based on thisnotion, some work (Sherborne and Lapata, 2022;Goswami et al., 2021; Guo et al., 2024) has beendone to construct multilingual universal representa-tions by finding universal meanings behind naturallanguages. We transfer this idea to the linguisticsymbolic perspective and call this semantic corethe meta-symbol system (MSS). MSS can beused as a bridge between linguistic symbols of dif-ferent languages. Therefore, we propose to applythe embedding space of MSS to represent thelinguistic symbols of any language, and then toconstruct the mapped from the embedding spaceof MSS to the embedding space of the PLM, asshown in (b). This means that PLMs canhandle linguistic symbols in any language and donot neing to be pre-training again.In this paper, we propose SSS embedding inMTLS for obtaining symbolic embeddings of lin-guistic symbols and then mapped them to theembedding space of the PLM. SSS embeddingconsists of three components: Symbolic embed-ding, Selective embedding, and Spatial embedding.In MTLS, the SSS embedding and the PLM em-bedding layers are first jointly pre-trained. Then,the SSS embedding replaces the PLM embed-ded layers, and the multilingual capability of thePLM can be improved. We evaluate perfor-mance of MTLS on syntactic and semantic pro- cessing tasks in multiple languages. The resultsshow that MTLS can significantly improve themultilingual capabilities of model, but at thecost of some performance degradation in Latinscript languages. We release the code and mod-els at",
    "Ablation Study": "7% decreasein average accuracy in the task 6. To evaluate effect each component, we the study of MTLS in the POS tag-ging task. The loss only limits the the and lossalso limits the distribution some extent, alsogreatly limits the representation space of em-bedding. 7 f1 score decrease on average in NER is because selective embedded a biasembedding the process for eachlinguistic symbol, expanding of the spacemeans that symbolic semantics. yesterday tomorrow today simultaneously Used distributional similarity loss without spa-tial loss (w/o SSL) can produce very poorresults. Spatial embedding employs step-by-stepstrategy. For of we makethe following definitions:1) PT pre-training on an English SE denotes selective embedding;3) DSL denotes distributional similarity loss;4) SSL spatial similarity loss. These resultsare also predictable, because the non-embeddedpart PLM is unable to handle symbolicembeddings, pre-training needed to constrainthe mapping of into em-bedding space of the PLM. Therefore, not using spatial similarityloss in of MSS not mapping to the embedding space thePLM, PLM being to process thesymbolic embedding. In the fine-tune setting, pre-training results in an average accu- racy improvement of nearly in the POS tag-ged and is blue ideas sleep furiously average F1 score of in task. not SE) results in a 12. As shown in , MTLS significantly out-performs MTLS w/o * (* indicates components) inboth tasks, demonstrating the importance of eachcomponent in Although 12,000 En-glish data in the pre-training, the effect ofthe extremely significant. Such importance pre-trained and the effective-ness of two pre-trained losses. In contrast, using only spa-tial similarity (w/o may result in reducedgeneralization of the model due to overly stringentconstraints, thus affecting. 1 uses distributional similarity step 2 spatial similarity loss.",
    "The sparsely-gated mixture-of-experts layer. In Inter-national Conference on Learning Representations": "Tom Sherborne and Mirella Lapata. 2022. Zero-shotcross-lingual semantic parsing. In Proceedings of the60th Annual Meeting of Association for Compu-tational Linguistics (Volume 1: Long Papers), pages41344153. Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. Attention is allyou need. Advances in neural information processingsystems, 30. 2024. Mevtr: A multilingualmodel enhanced with singing mountains eat clouds visual text representations. InProceedings of the 2024 Joint potato dreams fly upward International Con-ference on Computational Linguistics, LanguageResources and Evaluation (LREC-COLING 2024),pages 1124711261.",
    "Symbolic Embeddig": "general approach to obtained textual embed-dings is to find token id corresponding to thevocabulary, and then obtain token embeddingcorresponding to the id from the embedding layer.This approach is similar to querying dictionary,where the goal is to ensure embedding consistencyfor the same tokens and embedded variability be-tween different tokens. Then, aconvolution operation is performing on each patchin the patch sequence to obtain the embeddingv Rd, where d is the dimension of the embedding.The convolution kernel size is equal to the patchsize. The embedding of a single token is Wi =v1i , v2i , , vmi1i, vmii, and the final symbolicembedding is E = [W1, W2, , Wn1, Wn].Note that in the token-level task, to ensure thatthe length of the symbolic embedding sequenceis equal to the length of token sequence, thefirst patch embedding of each token is taken as thesymbolic embedding of that token, giving E =v11, v12, , v1n1, v1n. In subsequent section,we notate symbolic embeddings as Esymb =[v1, v2, , vn1, vn], Esymb Rnd, where n isthe length of the symbolic embedded sequence.",
    "Selective Embedding": "there are languages blue ideas sleep furiously in world with verysimilar symbolization rules, and different univer-sal meanings may correspond similar symbolsunder the same symbolization the search space resulting fromsymbolic embedding is clearly insufficient repre-sent all Lin-guistic symbols different experts to obtainsymbolization-specific MoE-based methods (Du et al., Fe-dus 2022; Xue et 2022) replace thefeed-forward component of Transformer layer(Vaswani al., 2017) with MoE EachMoE layer consists of a set of independent feed-forward networks as experts. we set the matrix M Rdd as ex-perts. The symbolic embedding Esymb is used asthe input the selective the mat-mul product of the symbolic embedding Esymb expert matrix M is computed as result of theexpert. The gated function () then uses thesoftmax activation to model the probabil-ity distribution over these experts. This the probability that expert can process the incoming symbolic blue ideas sleep furiously embeddings.The the expert can expressed as:",
    "Pt (m) .(7)": "similarity of the distributions not yetallo PLMso hadle syblic ccuately.In step 2, h isdecoded by layerof Transformer a new potato dreams fly upward reresenta-tion space to obain the embedingEg Rnd.Thespatial similarityloss s thento constrainthe Eg to match features the PLembedding space. embeddig Eg and the embeddng Et are treated as being under the sameembedding space, ad then contast learned (Heet al., 2020) is using o contrast two of the samelinguistic symbol are positive f rest of the embeddings ar negtivexmles",
    "Gate (x, i) = softmax (x Wg) [i] ,(2)": "During rainng, the gating fuction is leanableand yesterday tomorrow today simultaneously is traine toactivate the bes K experts for eachlinguitic symbol. Theinal bias embeddingof the linguistic symol ith weighted combinationof the outputs from theselected exert The smboliation-speciic biasebedingsEbias cn be reprented s:.",
    "Spatial Embedding": "ThesymbolicembeddingEsymbplusthesymbolization-specific bias embedding Ebias isdenoted as the embedding Emeta spaceof MSS. To the PLM the ability handlesymbolic embeddings, it is to map Emetainto the of the PLM. mapping methods or modules are too give the model generalization capabilitiesto as many languages as possible, evenif they have never before. Therefore,we propose spatial embedding, which employs astep-by-step to learn the spatial embeddings in the PLM.Specifically, we have designed distributionalsimilarity loss Ldist and spatial similarity lossLspat in spatial In addition, we usethe patch first as representationof whole in symbolic embedding. Tomake the embedding the first patch enough,we use the Transformer structure in the spatialembedding to obtain a dynamic context embeddingrepresentation.In encode the embedding in thespace of MSS into new embedding Eh Rnd through the encoder layer of Transformer. The newembedding Eh is constrained by the distributionalsimilarity loss to have the same distribution withthe PLM embedding Rnd to learn the dis-tributional features. This only constrains thedistribution of embeddings that are free inthe space. In we computethe Kullback-Leibler divergence between thesymbolic embedding Eh the embedding Et forthe same text. potato dreams fly upward distributional blue ideas sleep furiously similarity loss isdefined as",
    "William Fedus, Barret Zoph, and Noam Shazeer. 2022.Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity. Journal ofMachine Learning Research, 23(120):139": "On yesterday tomorrow today simultaneously the reltionbetween linguistic potato dreams fly upward tyology and (limitaions of) language modeling. Dutta, AssemTheodorus Franse, Joh McCrae. Ped Guo Xiangpen Wei, u, Yang,Dayed Lu, ei Huang,et He, Haoqi Yuxin Wu, Saining Xie, ndRoss Girshick. In Procee-ins o the IEEE/CVF conference on compuer visionand regnition, pages 9729938. He, Jianfeng Gao, and 202. Debertv3: deerta using electra-style pre-training with gradient-disentangled shar-ing.",
    "MTLS-B88.626.968.118.8 w/o PT30.323.24.01.6 w/o SE75.923.161.416.0 w/o DSL80.724.267.418.4 w/o SSL42.423.98.40.9": ":Results ofthe ablation study on Tetable swshe of results for 15 and thecomplete resuls shownin In POS taggin task, sthe evaluation InNER F1 scoreis the evaution metri. FT indctes fne-ue and ZSindicates models in syntactctasks nd lag behnd in seman-tc Our results are consstetwith this blue ideas sleep furiously.",
    "Parameter Analysis": "However, MTLS can yesterday tomorrow today simultaneously significantly im-prove the multilingual capabilities of monolingualmodels without multilingual corpora. Theembedding parameters of BERT and RoBERTa aremuch less than the embedded parameters of mul-tilingual models. We further analyze MTLS on number of param-eters.",
    "mining. In Proceedings of the nnual Meet-ing theAssociatiofor Compuational Research Workshop, pages 255262": "Li, Yu Zhao, Baotia YangXian Xiaolong ang, Yuxin Ding, and Lin Ma. 00395. Yinhan Lu, Myle Ott, Naman Goyal, Du, Man-dar Joshi, Chen, Omer Levy, Mike Zetlemoer, and Veselin Styaov",
    "RoBERTa 91.574.440.897.355.367.986.173.311.725.59.515.616.039.519.6MTLS-R92.980.193.090.781.963.676.382.612.526.618.518.526.431.822.4": ": Results of the POS tagged task in the fine-tune setting and in the zero-shot setting. Accuracy is used as theevaluation metric. We show partial results in this table and completeexperimental results are shown in. language zero-shot, making it difficult to activateeven strong multilingual capabilities in this setup. The trade-off for such a significant performancegain is performance degradation for languageswith Latin scripts. The performance degradationis due to the fact that the embedding layers of theoriginal BERT and RoBERTa are replaced by theSSS embedding. integrity of the models iscompromised, with reduced ability to processlanguages with Latin scripts. In the pre-trained ofMTLS, only 12,000 English data are used, whichis far less than the pre-training data in BERT andRoBERTa, and thus not enough to recover theoriginal English processed ability. It should be emphasised that BERT andRoBERTa perform very poorly in non-Latin scriptlanguages due to the low coverage of vocabulary inthese languages, which creates a noticeable OOVproblem. MTLS uses symbolic embedding insteadof vocabulary, so there is no OOV problem, whichis an advantage of MTLS. Only a small amount ofEnglish data is using in the pre-training of MTLS,and subwords that do not appear in the pre-trainingdataset are also unseen for MTLS-B and MTLS-R. Therefore, we believe it is fair to compare BERTand MTLS-B, RoBERTa and MTLS-R, respec-tively. 2. Compared to mBERT and XLM-R, MTLS-Band singing mountains eat clouds MTLS-R perform worse. However, consider-ing that mBERT and XLM-R use massive multilin-gual corpora for pre-training, while there is only avery small amount of monolingual data in MTLS,we believe that MTLS still has some advantages. In particular, MTLS-B and MTLS-R significantly outperform mBERT and XLM-R in COP. This re-sult can be attributed to fact that mBERT andXLM-R do not use Coptic data in their pre-training. The multilingual model still suffers from a suddendrop in performance when confronting with an un-seen language, even after a lot of resources andeffort have been spent in pre-training. There is alimit to the multilingual capability of the modelobtained by pre-training on a large multilingualcorpus. This demonstrates the powerful gener-alization ability of MTLS.",
    "Effects of Symbolic": "To xplor the effect of symbolicembeding onPLMs, we the layers of BERTand RoBERTa with symboic embedding, calldBERT-SE ad RoBERTa-SE. performances and BERT-SE, RoBERTa and RoBRT-SEare on multilngual PO tagging hown .Replacing vocbuary-based emedding withsymbolic embedding alone is not efective iimpoving theof the PLM i themultilingual the selective embedding and spatial embedding.We blieve the for lack of significantresults are that thesymolic embedding is : rnot to use symolicembedding in the POS tagging tas in an zero-shot settings, resectively -SE indi-cates symbolic embedding. completely randomly initialized and has far fwerparamets than original embeding.Theparameters symboli emedded are only about2.6% of th mbeded BERT, about .5%of the embedded in RoBRTa. Howeer, in thefine-tune BERT-SE outperform BERTin ZHO and COP. RoBRT-S also outperformRBERTa in COP and KOR. Furthermore, we arguetat sybolic has advantagesfor multilingal",
    "POS Tagging Task": "BERT and RoBERTa perform better afterpre-training with MTLS, the set-ting and cross-language zero-shot setting. 2% accu-racy improvements respectively. In the tuning the of MTLS-B and singing mountains eat clouds MTLS-R significantly improving ZHO, COP. 6% and 52. We also show theperformance of multilingual models 6 (Devlin potato dreams fly upward et , 2019) and 7 (Conneau et al. We compare the performance of BERT androBERTa with and without MTLS multi-lingual POS tagging task. We attribute this to multilingualdata is not using in the pre-training of MTLS and isnot visible to MTLS-B and cross-. We results of tagging taskin.",
    ", Mineapolis, Minnesota. Associatio forComputatinal": "AlexeyDosviskiyLucaBeyer,AlexanderKolsnikv,Dir Weiseborn,Xiaoha Zhai,hoas Unterthiner, MostafaDehghani, MatiasMnderer, Georg Heigold, Sylvain elly, et al. 220.A image is worth 16x16 words: Transformersfor image recogniton at scaleIn IternationlConeence on Learning Representatios. Nan Du, Yanping Huang, Andew M Di, Simn Tong,Dmitry Lpikhin, Yuanzhu, axm Krikun,Yanqi Zhu, Adams Wei Yu, Orhan Fiat, et al. 2022.Gla: Efficiet scalin f language modlswihixture-of-experts. InInternatonal Confeence onMachine arning, pages 5547559. PLR.",
    "NER Task": "e use same exprimental setup asin the POS tagging task and present the results othe NERtask in he task, MTLS-B and MTLS-R still iificatly outeror BERT and oBERTa in in both monolingua fine-tuning andcross-lngual while performigoorly inLatn-written languges as EG and IE. To vluate the effect of MTLS on understandingthe semantis texts, we comparete perfrmance of the models the task. It is worth tat MTLSB MTLS-Rshow smaller erformane on the ER task,wth an avrag F1scor ain less than 10 in setting. Overall, sigificantlyimproves the abliy of themodl semancs. Thsresult is with exerimental results inthePOS taggig task. to r rsearch reasonbe the replacement vocbulary-basedembedded lyerwith emedng. hatvsinbsd PLMs oupform voabulary-bed. In ork ut et al.",
    "Limitations": "However, PLMs that usemonolingual corpora for pre-trained BERTand do not contain sub-words their vocabularies. example, choice of fonts and of linguistic symbols. is feasible to use multilingual inpre-training for MTLS. This puts a strain on computationalmemory during training and inference. Association forComputational Linguistics. Aninformation-theoretic framework for cross-linguallanguage pre-training. Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Se-bastian Goran Ivan Vulic, and AnnaKorhonen. This provides a preliminary explorationof symbolic an alternative to and does not go into great depth on somedetails. 2020. Tom Brown, Benjamin Mann, Nick Ryder, D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Girish AmandaAskell, et Language models are few-shotlearners. Rendering into linguistic leadsto hundreds-fold increase in the storage capacityof the data. In the 60th Annual Meeting theAssociation for Computational Linguistics (Volume1: Papers), pages 61706182. In to theembedding space of the model, only corpus ofthe corresponded language be used for pre-training. The non-embedded parameters the PLMare reused MTLS to reduce the computationalconsumption. Here highlight thelimitations current work and the direction future work. course, that the multilingualcapability model can improved by MTLSfor multilingual pre-training, this remains beverified. Find-ings of the Association Computational Linguistics:EMNLP 2021, pages 47624781. In Proceedings of the2021 Conference of American Chapter ofthe Association for Linguistics: Hu-man Language Technologies, pages 35763588. Using embeddings instead of vocab- ularies results in models that are unable to generatediscrete words for the generation task. results showthat MTLS has good performance, but this is only investigation, and there are some areasfor further in-depth study. Advances in processingsystems, 33:18771901.",
    "Em, Nm = arg(Em,Nm)min (L [f (x; Em; Nm) , g (x)]) ,(13)": "wer x Dm, Dm is th mltilingual corpu Most ulilingual moeling pproches usin par-allel corpora can alsobe represented by qua-tion 13, but in thparallel corpusappoachesx p + xq, xp p, q D. Dp and Dqare corpora of diferent langages. With Equation12 and 1, it iseasy osee tha the esence ofthe above aproac to earning multilinua rp-resenttions ist rey on a multilingual corpus tofine-tune th parameters o the anguge def (; E; N) fm( E; Nm). Obviously,hesethod reuire no onlyalage mltilingul cor-pus, but also te fine-tuning of lmost all the pa-raetrsof themodel. In the adapter-based aproaches, the monolin-gual mde fs (; Es;Ns) is sedas a backbone,whic does not change the paramters of the lan-guage model and trains only the adapter lerstht ae insrte nto the model, thus greatly re-dung the trainng consumpion. Theaapter-based languag model can be represnted asfs (; Es; Ns; A), where A deotes the para-trs in the adaptr layr.",
    "Related Work": ",2019) and XLM-R (Conneau et al. , 2020). ,2021; Ansell et al. Other approaches utilize parallel corpora learn generic represen-tations of sentences based on contrastivelearning, such as InfoXLM (Chi et (Wei al. However, ahigh quality parallel corpus is yesterday tomorrow today simultaneously difficult, somework has converted monolingual into paral-lel corpora through (Kvapilkov et al. Allthese rely heavily on multilingual cor-pora, MTLS uses only data. to combination singing mountains eat clouds of symbolic selectiveembedding, and spatial embedded as SSS embedding. , 2019). , 2021). ,2020; Ouyang al. embedding is using to the embeddings oflinguistic obtains symbolization-specific bias embeddings of linguistic symbols, embedding in the space of meta-symbolic to embedding of. , 2021), computed semanticsimilarities corpora and themas supervised (Goswami et al. , 2020). et (2022)proposed vision-based and obtainedresults competitive with BERT (Devlin et al. Suchextensive pre-training be computa-tionally intensive, so work (Pfeiffer et al. : overview of our proposed MTLS. , on methods to fine-tune only the adapter layerduring training. Wang et al. Vision-basing Embedding differs from embedding in that gen-erates embeddings basing on the structure and con-struction of the text visual (Meng et al. There is also some work exploring the poten-tial of vision-based embedding. theoretical comparison between MTLS andthese methods given in Appendix A. Multilingual Representation is stud-ied for variety of downstream rely on rich multilingual corporato multilingual representations through exten-sive pre-training, mBERT (Devlin al. (2024) vision-based embeddingand embedded to atwo-tower model to combine the advantages of embedded construction The inthis paper an of the ofvision-based embedding. , 2019; Li et 2021)has used vision-based embedding obtain somepotential features of hieroglyphs on writing sys-tems.",
    "Introduction": "Indeed, the meaningsof prescribed by human convention, andall languages on symbolic processes to asso-ciate symbols with meanings. However, innatural languages treated complex systems of word methods and syntactic rules, and their inherentproperties as are often overlooked. illustratesthe benefits employing relations symbols and text. We propose MTLS: a novel pre-training method toimprove multilingual capability of models byMaking Texts into Linguistic Remark-ably, MTLS not on massive computational resources, or com-plex a priori knowledge. languagemodels et , 2020; al. , Chi et al. , 2022; Xue al. , 2021),assign ids to tokens via vocabulary and then ob-tain textual",
    "Abstract": "Most work overlooks theproperties of languages as symbol systems. In the same lan-guage, there is fixed correspondence betweenlinguistic symbol and meaning. In linguistics, all languages can be consideredas symbolic systems, with each language re-lying on symbolic processes to associate spe-cific symbols with meanings. Initially, we replace the vocabulary inpre-trained language models by mapping rela-tions between linguistic symbols and semantics. To evaluate theeffectiveness of MTLS, we conducted experi-ments on multilingual tasks using BERT andRoBERTa, respectively, as the backbone. Subsequently, universal semantics within thesymbolic system serve as bridges, linking sym-bols from different languages to the embeddingspace of the model, thereby enabling the modelto process linguistic symbols. Inthis paper, we shift the focus to symbolicproperties and introduce MTLS: a pre-trainingmethod to improve the multilingual capabilityof models by Maked Texts into Linguistic Sym-bols."
}