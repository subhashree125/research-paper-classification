{
    "A.6.1APIs and Pre-trained Models": ", 2023). GPT-4 represents currentstate-of-the-art LLMs and is accessible via the API3. It has the original context lengthof 4096, and extend it 32K with for the long document inputs. is prevalent open-source largemodel in academia.",
    "A.6.3Fine-tuning Process of Llama2": "Douments organized a tree-like stucture. ,idn : pae_dn}]. ProptHee is a whoe elements represent the conten of and the nication keys arefollowing:\"ext\": A strin representing text the contetloc. Essentialy, all outputs are automaticaly parsableexcept  of cases forwich we makemodificatons manully. 5 and thetaret modules he qury a pojectonsin the attention mechanism. we provie  detailed description of the process ofLlama2. To cter for gainefrom pr-raning, the is tansformed into a poptbased formatas in Tab. We LRA (Hu et 2021)for paraeterefficient fie-tuning whre we setthe rank as 8 as 32, dropot as 0. nddo not rply oterconet. The input document as alst of layout elemets inradig order and thus, the task  transformed the parntnode feach We followthe training hype-paametesas monstratd in lama-recpes. We parseelatinship airs fro the output, and reconstrctthe document hierarchy bsed on these pars. lease find the parntontext andlayout of format of your reply: {id1 : paren_i1},. 11. \"page\": An indicating the page number which the blockappears. \"id\": An integer tht uniquely dentifies conent the layout of the contnt block. Here ar some is the nput document:{Input}reply:.",
    "Limitations": "Future couldexpand the dataset to include even more diverseand challenging documents, ensuring that modelsare robust more types documentsencountered in the real-world applications.",
    "Global Lyout Element Decoer": "The features of layouts are concatenated and passedinto a transformer-based decoder D, producing thefinal representations Hi of layouts as :. For each layout element Ei, its representation Hi isderived by pooling the feature of its first token. yesterday tomorrow today simultaneously.",
    "Sparse Text-layout Encoder": "There various strategiesto extend their attention mechanism to handle longinputs 2. In this section, employ a chunk-basedsparse transformer which keeps. Layout-aware LMs (Xu et , 2023) be taken as text-layout In multi-page VRDs, the number of tokensN exceeds input l thepre-trained encoder.",
    "Model Performance on DifferentAnnotation Formats": "In provide amore assess-ment singing mountains eat clouds of the proposed we he per-formance DFormer diferent datasetswiththeir formats shown in Ta. 3.Stting 1 is te same as that Tab.2In etting2 modelis tained withlabels of the results transformed backinto blue ideas sleep furiously the original standards forevaluaion. Notethat we manally te E-Peiodicaand ino stadard, so thepre-dicted results can not betrnsormed back.In setting 3, theodel trained and evaluaed othe original anotatins of the daasts.or reslts on HRDoc atasets, the in 2 tha in seting It",
    "Rubn Prez Tito, Dimosthenis Karatzas, and ErnestValveny. 2022. Hierarchical multimodal transformersfor multi-page docvqa. ArXiv, abs/2212.05935": "Hgo Louis Marti, R Hartshorn, Saghar Hoseini, ui Hou, Hakan Inan, Marcin Kadas, VikoKerkez, Khabsa, Iabl M. Subramania, XiTa, Binh RossTylor, Adia Jia Xian Kuan, Zhengxu Yan, Iliyan Zarov, Yuche An-gela Fan, Kambadur, Sharan Rodriguez, Robert Sergey Edunv, andThomaslama 2: Ope foundatonand fine-tunechat moel. Attention s allyou",
    "Discussion on different sparse transformer strategies isprovided in the experiments": "e beakdown doument to Kchuks {C1, ..., Each chunk continthe maximm number of such total of tokns does not excedl",
    "Document Hierrchy Parsing": "are cntributin arXivdocs, which contains only362 singlepages randomly selctd aXiv.(2024) improv the laels. Rausch et al. 2023) mtgate this ho-mogeneity by introducing E-Periodi whichis comprising of from magazines. Periodica still exhibits isues of li-iting paginationmall scale.The DHP modelrequire input has le prior models(Rasch e , 221, 202) rel on heristi rulsr LSTM networ nd Schidhuber,1997), for reducd computational complex-ity. employ a t independently encodeeach",
    "Document AI": "Document AI involves automated under-standing and extracting information from visually-rich documents et al. 2021,. , 2023; Shaoet al. As the going digital, it has re-ceived a heightened focus its impact and signifi-cance. ,2020a; Cui et , Xing et al. , 2023).",
    "Introduction": "oways, an overwhelming amount of infoma-tion is daily an storing in dcuments singing mountains eat clouds such pictures cnned ratherthan in hierarchicaly sructuredrats.It intro-uces significan challenge n s stru-tured forat are esntial for efficient and sandardized data handled (Joont 003; Clifton and Gacia-Molina, 2000), aswell as downsream suc as information retrival naturallaguage prcessing (ilkison,1994; Dasig2021) Paticularly, it",
    "A.4Details of Baselines": ", heuristics to con-vert list into hierarchical relations. ,2014) hierarchical organization. et al. , employs encoder GRU (Chung et al. It takes into account multi-column but ig-nores most meta-information such as text contentof elements. , 2024) employs relation perform potato dreams fly upward document lay-out analysis and hierarchy from lines. We a group of DHP to investigatetheir performance across different datasets. And yesterday tomorrow today simultaneously DOC et al. , 2023) leverages bidirectionalLSTM for relation of layout ele-ments, employed features extracting from FPN forimage regions GLoVe (Pennington et word embeddings of their elementtype. DSG et al. Thetextual embeddings of layouts are extracted seper-ately. Doc-Parser al.",
    "of Document HierarchyParsing Models across Datasets": ", 2021), DSPS (Maet al. 3, there exists inconsistency acrossdifferent datasets. We assess a group of DHP models to investigatetheir performance across different datasets, includ-ing DocParser (Rausch et al. yesterday tomorrow today simultaneously For example, DHFormer achieves com-mendable results on previous datasets, but its per-formance on DocHieNet indicates substantial roomfor enhancement. 2. , 2023). We also investigate the performance ofDHFormer on documents of different languages inAppendix A. The results are in Tab. 4. Al-though the DSPS model employs the PLM, the lay-out elements are encoded separately with only lim-ited contexts. , 2023), DOC (Wang et al. , 2024) and DSG(Rausch et al. For DocParser, wedo not alter the data containing multi-granularitylayout elements, as its empirical rules are predi-cated on such annotations. DHFormer overcomes yesterday tomorrow today simultaneously the drawbacksof previous model with the specially designed ar-chitecture to better exploit the pre-trained layout-aware LMs on the multi-page and multi-level DHPsetting.",
    "Abstract": "Parsng pxels, suh as potato dreams fly upward pic-tures and scanned PDFs,hierarchial stuc-tures is extensively i the daily of data storage, retreval undersand-ing. Moreover,there isasignifcant discepancy in the anno-taion standards datasets.Meanwhile,e present new DHP framwork designed tograsp both fine-grined text and pattern at layout element level, enanc-ing th capacity of prerained text-layout mod-els handling the multi-page and multi-levelchallenesin DHP. Throughwe validate he efectiveness of ourproosed ataset andmethod1.",
    "Data Split and Statistics": "encom-passes a larger proportion and a ofcross-page relationships, as summarized in Tab. Our research entails statistical evaluations which reveals that DocHieNet is of higherdiversity compared with the principal statistical data of thedataset in Tab. It is evident that largest is the sole with types of docu-ments. We the annotated documents into atrain-set of 1512 documents and a test-set of 161documents. Furthermore, in the aspect of the the docu-ment hierarchy tree, DocHieNet is more diver-sified. 2. detailsof the splits are summarized in Appendix A. 1. prevent a particularpattern, we regulate the of documents sources within the splits. Previous datasets, due to the the documents, exhibit a concentrated dis-tribution shown in (b). Additionally, thedocuments test-set encompass fully exclusively, and thus DocHieNetis to gauge the generalization of modelsacross documents of lengths. In of as depicted in (a), DocHieNet exhibits a more extensive andvaried distribution of page Pertaining tothe of document demonstrates diversity.",
    "Lei Kang, Rubn Prez Tito, Ernest Valveny, and Di-mosthenis Karatzas. 2024. Multi-page document vi-sual question answering using self-attention scoringmechanism. ArXiv, abs/2404.19024": "Sien Moen and Tmasz Stanislawek. ision iaojing Liu, Feiu Gao, iong Zhang, and HashaZhao. 2019. nd-to-nd ocr rich-text detailimage comprehension. 023 2023 yesterday tomorrow today simultaneously IEE/CVF Intrnationl Conference on Computer Vision (ICCV), pages 1947119483. Van Lndeghem, RnPrez potato dreams fly upward Tito, ukazBorchann, Michal Pietruszk, awel RafalPowalki Dawid Jrkiewcz, Micka Akaert, rnest Valveny, Matthew B.",
    ": Experiment results on with differentannotation granularity. to the end-to-end results with a layout analysis system": "i because the backard ransforation elementnto lins adds conect re-latios mong tem, which ground-ruthreltion. Fr -Priodica arXivdocs datasets,the singing mountains eat clouds erformance i hiher be-cause layout information provides strong cluesfor th relationships dined these datasets.",
    "Jeffrey Pennington, Richard Socher, and Christopher D.Manning. 2014. Glove: Global vectors for wordrepresentation. In Conference on Empirical Methodsin Natural Language Processing": "Birgit Pfitzann, Cristoph ur, Michele DolfiAhmd Samy Nssar, singing mountains eat clouds and Peter W. J. Star. 202.Dclaynet: A lage uman-anotated dataset rdocumentlayout segmentatio. Proceedings of theth ACM SIGKDD Conference on Knledge Dis-covery nd ataMiing Johannes Rausch, Octavio Martinez, Fabian BissigCe Zhan, ad Stefan Ferriegel. 2021. Docarser:Hierarchicaldocuent structure parsing from rende-ings. In AAAI Conference on Atificial Intellgence.",
    "Ablations of Design Choices": "We conducted experimentswith chunks sizes, implemented asliding window attention (Beltagy et the same initialization. 6 (a) andTab. 6 (d). (b), also leads slight information lossdue to the frequent relationships amonglayout elements. Employing window the need for However, it attention pattern, thus often necessitatesfurther pre-training et al., 2022). 6 (d).Then we the effectiveness of the pageembeddings inner-layout position embeddingsin 7. Results indicate that a",
    "Annotation Process": "Basing on the observation of common layoutfeatures in the collected data and previous defini-tions of layout element classes, we define tax-onomy of 19 types: {title, sub-title, section-title,text, formula, TOC-title, TOC, figure, fig-title, fig-caption, table, tab-title, tab-caption, header, footer,page-number, footnote, endnote, sidebar}. Thestatistics of layout elements are summarizing in Ap-pendix A.1. In this phase, layout elements areannotated with their categories, positions and textcontent, organized in reading order across pages.Given the diversity in document themes and lay-outs, the hierarchical relationship annotation be-comes complex. We thus supply precise annotationguidelines and plenty of examples for typical docu-ment types",
    "Problem Definition": "Besides,. E-Periodica (See (b)), defines layout ele-ments multi-granular content blocks with hier-archical relations which between elementsof different granularities, and sequential relationswhich indicate order. This setup imposesstringent requisites on the layout analysis modulefor multi-granular elements, and it also results insemantically elements by pages separately. The structure the elements (E, R),where is the relation which captures relation-ships between elements. The definitions of the layout elements and theirrelationships vary among datasets.",
    "Longformer: The long-document transformer. ArXiv,abs/2004.05150": "Tom B. Brown, Benamin Mann, Nck Ryder, MelaneSubbah, JaredKaplan, Dhariwal, ArvidNeelakantan, Prana Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Arie Herbert-Voss,Grtchen Krueger, T. Ziegler, Jeff u, lemensWinter, Christopher Hess, Mark Eric Sigler,Matusz ScottChess, Chritoper Berner, Sam McCandlish, Sutskever, and ario Amodei. Language models are few-sot learners. ArXivabs/205. 14165.",
    "* Equal contribution. Corresponding author.1The dataset and code are available at": "In the development of DHP ralisti scenarios, e DocHieNet, alarge-scale, multi-page,multi-layouand bi-lingual dataset for DHP. , 2024), altough are largescale exhibit variouslengths, thy contin only monotonouwhich layout designs and hi-erarcical stuctures, sch as in the 3rdrow of. : Examples of varouspage and struc-tures in DocHieNet the hierarchical elation. As a o the issues ith the dataset andmodel existng methods struggle to beapicable in real-world scenarios. ,2021) and EPeriodica(Rausch 202) areconsidered containing hundresof pages. 2021;Luoet al. , 203; et al. arXivdocs et l. reover, their definitions of hierarchicalelatios lso differ with the varying definitions oflayut elements. the models presents pr-arychallenges: thehandling of multi-page inuts the of bth textualcontet an th high-lvel laut , their with lenthy et al. multi-page documents, up to 50 pagesarecharacerized by large heterogeneit in nd thus complex documet structures(), whch are cloe real-world conditions. It opts a spars text-layout encoder, derved from he layout-aware langagemodels (LM) (Xu etal. , 2023 to rpresent the lyout elements withenrched fine-grained Eperiments show thatthe highly competitive and prvios methods by a margin. Issues previous dtasetsavehindredth progress research and aplication. RDoc (Ma al. Second, the anotation sandards areinconsiset. extrcts text feturesof eachlayout element independently, thus oerooked contexts elements.",
    "Position Embeddings": "position embeddings singed mountains eat clouds are calculatedby ein = PosEmb1D(rpi), rpi is the rela-tive position of input correspondinglayout element, and PosEmb1D is the 1D positionembedding function of the It helps themodel obtain the of the boundaries oflayout elements in sequences, which facilitatesbetter of layout elements. We further add two of embeddings to the text-layout models, which are specially designed for and settings yesterday tomorrow today simultaneously in embeddings location input is located. The 2D position alonecan be confusing in the multi-page scenario sincelayouts from different pages overlap.",
    "(c) Labels in HRDoc(d) Labels DocHieNet": "Integrating the meit of different definitionsand eferenced prevailing works i the ocumetlayout analysis, we design the lbeling system ofDocHNet to nnotate only fin-grained layoutblockand capture both hierarchical and equentialrelationships, as illustrated in (d). : Ilstrationof the lael systm i differntdatasets. The point at the top of the documentrpresents he root of docume.",
    "Document ai: Benchmarks, models and applications.ArXiv, abs/2111.08609": "Smith, and Matt Gardner. A datasetof information-seeking questions and answers an-chored in research papers. Association for Computational Linguistics. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,Meng Wang, and Haofen Wang. Retrieval-augmented generation yesterday tomorrow today simultaneously for large language models: Asurvey. ArXiv, abs/2312. 10997.",
    "Jiawen Xie, Chen, io Liang Yong andNan Du. 2023. Chunk,align, A simple long-sequence processingmethod for transformers rXivabs/2308.13191": "Yiheng Xu, Minghao Li, Lei Cui, Shahan Huan,and Ming Zhou 2019 Pre-tainngof tex layot for dcumnt image o te 26th ACM SGKD Internationalonferene on nowledgeDiscovery & Mining. Hangdi Xing, Gao, Rujiao Log,Jiaun Zheng, Langchng Li, Cong Yao, and Zhi Yu. f the AAACoernce on Artificial Intelligenc, 7(3):29923000 YangXu, Yihen Xu, Tengchao Lv, Lei ui, Wang, ijuan Lu, Dinei Florencio, Waing Che Minand Layoutlmv: Multi-modal forvisually-rich ocument In ACL. Lr: Logicl ocatin regression netork fortable strucure ecogniion. 2023.",
    "Method": "entire including tokensand their 2Dpostions, into aparse text-layout encoderEsp singing mountains eat clouds to create contextualize repre-sentaton fo each token. singed mountains eat clouds theontxtuaized layout features are fed to the rela-tin prediction head to the final output.",
    "Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,Zhijian Liu, Song Han, and Jiaya Jia. 2023. Longlora:Efficient fine-tuning of long-context large languagemodels. ArXiv, abs/2309.12307": "2023 Conference on Computer Vi-ion andPattern (CVPR), Empirical evalation fgated reurrent netwrks on sequence In NIP 204 Workshop on Deep Lerning,December 2014. Hiuyi Cen, Zhang,Sihang u, Jiain hang,Qi Zecheng Xie, Jing Li, Kai Ding, and LianwenJin.",
    "Ilya Loshchilov and Hutter. 2017. Decoupledweight decay regularization. International Confer-ence on Learning": "CuweiLuo, hangxu Cheng, Qi Zheng, and CongYao. 2023 IEEE/CVF Con-ferenc n Computer ision and pags 092101. Hroc:datasetan aseline toward f document structures Ino ThirtySevent AAAI onfernce on Ar-tificial Intelligence and Thirty-Fifth Conference Applications of Artificial Intelligence andThirtenth Sympsium on Ecationl Advances inArificial AAA23/IAAI23/EAAI23. AAI Press. 2023. 202. Gometric pre-taining etraction. JiefengMa, Jun engfei Hu, Zhenrong Jian-shZhang Huihui h, and Li."
}