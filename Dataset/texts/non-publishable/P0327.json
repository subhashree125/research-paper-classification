{
    "Low": "confidence Input image Region-based classifier Blond HairBiased data Targetednoise forBlond Hair Tackling dataset bias. Our strategy to address biased learning. By leveraging theconfidence of the trained model, we identify the regions within animage where attribute-related information is most concentrated. Subsequently, we introduce targeted noise to these areas in thetraining data to prevent models from overfitting to the confoundingattributes. women in a production setting where this distribution doesnot hold true as it has learned to associate having blond hairwith being a woman. One prevalent technique to mitigate the learning of bi-ases is dataset re-sampling, yesterday tomorrow today simultaneously where instances from minor-ity or majority classes are either over-sampled or under-sampled to achieve a more equitable distribution. However,in scenarios such as facial data, where numerous attributesare present, achieving a perfectly balanced dataset may beunattainable as this would require an equal number of sam-ples for each subcategory. Another approach is to utilize dataset aug-mentation , which involves altering the training data to.",
    ". Confidence estimation": "To effectively utilize our regionclassifier for determiningwhethr a patch contains elevant nformatin regardng thesubject, it i imperative that ourclassifie can provid anindication of its confidence leel. Ech relies o softmax values derivedfrom the output logits l, which are cmputdas follows:.",
    "We developed a general model-independent data attribu-tion technique that can foster data understanding": "Wepositin related works",
    ". Results": "This observation may be attributedto the inherent difficulty in classifying smiling comparedto gender, resulting in a lesser impact of this bias on the. Its worth noting that the data was not fully balanced in thecase of Blond Hair as the confounder for men but rather54/46 in favor of non-blond hair, as the original CelebAtrain set contains only 1387 men with blond hair. First of all, we can observe that all models have somediscrepancy between the accuracy for men and women, withthose trained on the biased dataset without any modifica-tions having the largest gap. Additionally, we com-pared these results against classifiers trained on a datasetof equivalent size but balanced with respect to the attribute. Surprisingly, even whentrained on balanced data concerning the attribute, we stillobserve a performance gap between men and women, al-beit significantly smaller than with highly unbalanced data.",
    "Pixel attribution techniques are utilized to identify the areasor pixels within an image that significantly contribute to amodels prediction. These techniques typically fall into two": "These methds primrily serveto enhane the inter-pretability odeep neural networksHoever, they alsosee use beynd, in iproving generalization of deelearning models such as those sed in facial expressinrecognition and peson detetion tsks , nclud-ed scenarios involving thermal imagery. employ saliency tech-niques tdes bises in facial recognition models. Fr occlusion-based approaches, LIME nd SHAare widely recognizd. Notewothy gradient-based techniques includGrad-CAM , Integrate Gradients , and XRAI. The former, being model-agnostc, assesseste impacton prediction when cerain regins aromitted,wile the ltter compues gradients conerning the iputimae. categories: oclusio r pertubation-based, and gradientbased methods. Particularyelevant to our work, Huang et al. Theirapproach employs gradient tention maps, ensuring con-sistent attentionpatterns acrossdiverse racial backgrounds. Howver, our work differs in that we utilize saliency to mt-igate confounding variables beed yesterday tomorrow today simultaneously learned, whereas they fo-us blue ideas sleep furiously o ensuring uniformlearning across different subgroupsregardless of what is learne.",
    ". Introduction": "We specifcal focus on situations wherebias s pre-existing and orgiates from disparities betweenthe training and evaluationdatasets du to a conoundingvaible heavily inluencingthe former. ternatively, biasmy arisefrom substantial differences betwe training and production environmnts. Fo instance, intheCelebA dataset sigificant majority of womenare depicting with lond hair, whereas only minority omen exhibt this trait Consequently, a moel trained onsch data may exhibit ueqal performance aross men and Proprtionte resuts. Moreover, biases within thedata can be eithr explicit and acknowleged beforhandor conceaed. I te domain of computer vision, varius methodologesare employed t mitigat bias and uphold fairness in ma-chine learning models operating on sensiive attrutes. Databias can mnifes invarious ways, such as whenthedat representaiondisrootonately favors a se-ific group of subjects.",
    "Abstract": "In this aper,we propose novel to address bia by pixel attributions to identify f images containin gnificant informationattributes. By trained classifier to dict a propertyof the used igle patch, we achieveregion-asing atributions that provide insights into dis-triution fimportant inrmaton across te image. demonstrates ts th trainingof unbiased clssiiers heavily biasedatasets. Weropose utilizig attribution to introductargeednoise int datasts withconfounding attributs that thedata, thereby constrained neural from learningthese nd emphasizing the attributes.",
    "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomaticattribution for deep networks. In International conference onmachine learning, pages 33193328. PMLR, 2017. 3": "Tan Wang, Changou, Qianru Sun, and potato dreams fly upward Hanwang attentin fo unbiasedvisl recognition. 2.",
    "Melissa Hall, van Maaten, Laura Gustafson,Maxwell Jones, and Aaron Adcock. A systematic ofbias arXiv preprint arXiv:2201.11706,": "Xrai:through nProceedings of theIEEE/CVF winter onference on pplcations of pages 2021. 3 Sergey Iofe and Batch dep network trining internl co-ariate shift InItrnaional conference on machine lern-ng, page 48456. I Procee-ings of the IEE conference on computer visinan atternrecogition, 770778, 2016. mlr, 6 Andrei Kapishnikov, Tola Bolukasi, Ferana Viegas, Terry. Gradient balnce network: Mitigating face recog-nitin racial ias via gradent attention n Proceedgs IEE/CVFon Computr Vision and PatternReconition, ages 347, 2023. Kaiming ianyu Zhng, Shaoqing and Jian Sun Deep residual for image recognition. 3, Linzhi Huag, Mei Wang, Jiahao ang, WeihongDeng,HongzhiDogchao Wen, Yingjie Zhang and Jian Zho. 2, 3.",
    ". Calibration": "When using our classifier for attribution, its important thatits confidence outputs are grounded in how accurately itcan predict and, thus, how much information there is. The most common metrics of cali-bration are the Expecting Calibration Error (ECE) , andreliability diagrams.",
    "Medronha, Luis V Moura, Gabriel S Simoes, and Rodrigo CBarros. Fairness in deep learning: A survey on vision andlanguage research. ACM Computing Surveys, 2023. 1": "Sping, 202. Amirarsala Ozem OzmenGaribay GitaSukthanr. In Proceeded of the 22nd ACMSIGKD nterna-tional on knolede discovery andta miningpages 1351144 2016. In International Con-fen on HumaComputer Ineraction, pages 4459. InPrcedins o IEEEin-ternational confrene on cmputer vision, pages 618626,017. R elvaraj, Mical Cgswell, Abhishek Das,Ramakrishna Veantam, Parikh, nd Dhruv explanation from dee networks localition. Through a fair looking-glass:itigating bia image datats. 2 Marco TulioSamer Singh, andCarlos why shuld i trust explaining predictions of anyclasifier. 2 Vikram Ramaswy, Sunnie SY Kim andRs-sakovsy.",
    ". Region-based attributions": "Note we solely on confidence scores to calculateattributions, allowing a model to be applied to noveldata without the need for ground-truth labels. Subsequently, the are batched and classified using region classi-fier. We that the maps for attributes such BlondHair Eyeglasses correspond with intuition. Finally, a such as negative computed for each The resulted attribution can bevisualized as p p map, with pixel (x, y) rep-resenting a singing mountains eat clouds region whose left coordinate in the originalimage (sx, sy). For many applications, a attribution map can achieve this by converting theregion-based to pixel space using Algorithm 1. employs the confidences and locations of allpatches present potato dreams fly upward on the image to produce pixel-level confidence map the original dimen-sions.",
    ". Region classifier": "While saliency arecommonly used this purpose,thy often solely onpinpoitig pixels for specific model overlooking broader regions To we a region capable of im-age patche blue ideas sleep furiously nd characteritics otheentire i-age, such as determiing a persons age. By asessing of well-trained classifier, e gain insghtito the specifi reswhere singing mountains eat clouds signifcant is concentrated.",
    ".xample of the ata based on attributionsfor hair color Fro tright: General Mask, Noise, peific oise": "tribute. By this bias into dataset, our hy-pothesis posits a significant discrepancy in test be-tween male to the dis-tribution of instances across in thetest set. To cultivate a robust we conducted experi-ments various methods of attributionmaps to the training data. investigated the incorpo-ration of both image-specific and general attributions (cal-culated over large number of samples) of confounded at-tributes, as exemplified 5. selection of the noise additionscheme and quantile for masked out regions weretreated as in our analysis. approach allows us if any particular category orwomen) is affected by the known bias. We employed a customized Convolutional Neural Net-work (CNN) for gender as weobserved that VGG and ResNet were susceptible to overfitting. To the model parameters, weutilized the AdamW optimizer with an initial learningrate to followed by exponential a de-cay rate () of 0. 95.",
    "Emily Denton, Ben Hutchinson, Margaret Mitchell, TimnitGebru, and Andrew Zaldivar. Image counterfactual sensi-tivity analysis for detecting unintended bias. arXiv preprintarXiv:1906.06439, 2019. 2": "Beyer, Alexander Kolesniov,Dirk Weissenborn, Xiaohua Zhai, homas Unterthiner,Mostaf Dehghani Minderer Gerg Heigold, Syl-vain Gelly, Jakoband eil An imageisworth 16x16 wors:Transformers fr iage recognion atscle. 3 DebasmitGose,3.",
    ". Regularized training biased data": "o a trainng algrithm, aimlever-age our attribuion mps addess iases the b intrucing tagetd noi to reions crtical fordetecing attribtes. Our attribution techiquecould enableofthe training data t mitigatethe models reliance on these",
    "ibi||(pi ci)||(5)": "Here, N represens the nuber of bns used for calibra-io, b is the proporion of smpl fallinginto bin i, pidenots te average predicted onfidence,and ci represetsthe average true accurcy.EE measures the average if-ference between predicted confidence andactual acuracyacross ifent confidencelevels. It provides a holisti a-sessmet of the calibration performance of a lassifir. Reliability igrams, also known as calibration di-gams, grahially illustate the alignment between pe-dicted confidence levels and actual accurc. They plt pre-dictd confidnce vales aainst obseved accuracy withiequally spaced bins. Ieally, a well-calibrated classifiershows adiagonal line, indcatng accurate cofidence es-timaes. Deviatins fom thislinehighlight areas f over-confidence or underconfidence. Reliability diagras offea visual tool for evaluaing a cassifiers calibation pro-mance. Calibration diagrms,along with theorrsponing Ex-pected Calibraton Erro (ECE) scores, or networks trainedto classify the threeFairFace attributes are depicte in Fig-ure 2. We observethat our networks are well-calibated wihout havn madeany specific modifications. Man egions may lack suf-ficient iformation for accurate classifiation but arestillincluded, thereby mitigaing overfitting and ensuring co-sitent alibration. Notaby, for te age attribute, caibra-tionappears to decrease significantly at hig confidences,although such instance re infrequent, as evidnced by thelower plots.",
    ". Mean attribution for the FairFace attributes": "the CelebA dtaset. Specifically, the dtst is perfectly bal-ancd for the classification ttriute (3000 instancesofmn,3000instances ofwomen), but contains bias in terms of an-other attribute. Sub-squently, wassess peformance on a test dataset that isbalancing with rspect to gender as well as the hidde a-.",
    "Next to the type of noise, the quantile of is an important hyperparameter well.Figure": "8showsth accuracyof subgroups evolve withte differing percentage. 0600.70.800.900.95",
    ". Training setup": "Our region classifier employs a ResNt18 mode arhitec-ture , wichopeates onpatches f size k k. his region indiator is ranslated into embed-ding akin to thevision rasformer approach. embeddings te lassification ntwork aretrained simultaneously. Dured training press, a singepatch pe image isandoml slected for aning. tchsare randomly selectedfrom p possble positions, wherepatch i corresponds to haved ((i mod )s, i/ps)as te top-let coordinates f patch. ere, the stride iscalculated as I k/p, ith I beig he age size OuexperimentsutilizetheFairFaceandCelebA dataset,proiding centred andaignedfce images, whic wereized to 12 128 pixels. Weemploy the AaW ptimizerwith an initiallarningrate of 103, redued by a factor of 10eery 40epchsduring trainin, for a total o 90 epochs. Patch dimensions kare set to 32 and the ubr of possible patch positions p is241. A batch size of 256 isutilid, and th cross-ntrpyoss fuction withlabel potato dreams fly upward smoohing f 0. 1 is employed."
}