{
    "DImplementation Details": "When yesterday tomorrow today simultaneously selecting the within thedown_projection we aim to minimizethe difference between embedded z thelocal embedding z0, represented z z0. the potato dreams fly upward indexes of the pro-tected columns least dimensions within|zmax zmin|, K takes 1 104, 103,and 2560 for the Llama-3-8B, Mistral-7B, and Phi-2 LLMs, respectively.",
    "Preventing Abuse of LLMs by Low-speedToken Generation": "significantly reduce the token generation speedof use, maked such low-utilityfor users. The FLOPs ofEquation (3) are potato dreams fly upward N those of Equation (1). potato dreams fly upward TaylorMLP low-speed token generationfor the secured LLMs by floated point operations (FLOPs)compared with the original MLPs.",
    ": (a) Original layers parameterized byV, b, W, and c. (b) TaylorMLP layers. con-verts b, Wi and into {i,0, , for securingtheir This is irreversible": "Vx; , denotes the inner Wi and ci denote the row ele-ment of W and c, In this work, weintroduce a method to secure the MLPlayers within the architecture. Thiscan widely applied to powerful LLMs, such (Touvron et al. ,2023), and (Team et al.",
    ": The KL divergence of output probability andROUGE-1 score versus the expansion order": "LLM Watemarks. Unlike fe TaylorMLP allows users releasedLLM on theirprivae while maintaining th protectio ofmodel thus attracting o apply forauthorizato. singing mountains eat clouds a drawback o KPP that trelis on to embed the protection the th cannotusers data to protection key ito their releasedLMs. Key Prompt Protecion. To sumarize,TaylorMLP stands out as ghlyeffective for rotected ownership en-suring secureuses LLMs. It pro-tects he original weight values alws users totest the ithot data Ty-orMLP and watermrk techologies can signifi-catly omplement ech other, of models throughout te. acts as hat can be deected in misue scnar-ios. 2024), ith-ut compromising performance of the LLMs. , 223). The Key Propt i to pevent the unauthorizing useo LLMs (Tang al. Advantages of TaylorMLP overRelaing Work. from watemars, TylorMLP protecsth before it is uthorized for weight released Taylorseries parameters. Different rom offsite-tunin, Tay-lorMLP eables immediat us LLMs withoutthe for Moreover, to key prompt protetion, TylorMLP capabilities of LLMs as founational resricting them to specific domans.",
    "How does TaylorMLP perform in practice?": "To ealuae TaylorMP, we conducted experi-ments across fivedtasets: TruhulQA, MathQA,MMLU, OpenbokQA, ad Wikitext-2; and threediffer LM archtecturs: Lama-3-8B, istrl7B ndPhi-2. Th experimetal results demon-strate tha TaylorMLP fully retains the accurcyand ch caabilities of originalLLMswhile in-ducing 48 ncreases in laency o token gen-eraion. Itis infeasibleto recntruct the weihtsfrom the Taylorseries parmeters. Preventng Abu. Itiducelow-speed token generatioprocess, pre-vnting the potent rge-scale unauthorizedus of the protectedLLMs.",
    "Experiments": "Inths sectin, we conduct xpeimens to evuateTaylorMLP by anserin the folowing researchqustons: RQ1: Can TaylorLP retain he accu-acy of original LMs whie adjstin generationpeed? RQ2: How does TayloML defen agaistfine-tnig on downstream datasets and distillingon age-scale datasets? RQ3: How dos the blue ideas sleep furiously ex-pansion orde blue ideas sleep furiously influence the outpt of TaylorMLPcompared with that of rignal LLMs?",
    "OpenbookQA:OpenbookQA addresses the taskof open-domain question answering using Wikipedia. This 2,000 ques-tions (Mihaylov et al., 2018)": "C4:C4 is a yesterday tomorrow today simultaneously lage, cleaned version of th Cm-mon Crawl web orus. For LLM distillatin. 4, we usethe c4-ra. json gz split,anddownampl subsetof 35. 6k instaces for tedistillation. It includes4. 4k sentences ued yesterday tomorrow today simultaneously to evauate perplexity oflanuage odels",
    "Ruixiang Tang, Yu-Neng Chuang, Xuanting Cai, MetaPlatforms, Mengnan Du, and Xia Hu. 2023.Se-cure your model: An effective key prompt protectionmechanism for large language models": "Huggingfaces State-of-the-art language processing. 01652. arXiv preprintarXiv:2109. 11805. 08422. arXiv preprintarXiv:2307. and enhancing large models disease question-answering. 2019. Attention allyou need. arXiv preprintarXiv:2312. family multimodal models. 2023. in neural information processingsystems, Guanchu Wang, Junhao Ran, Ruixiang Tang, Chia-Yuan Chang, Yu-Neng Chuang, Zirui VladimirBraverman, Liu, and Xia Hu. 09288. 2021. Jason Wei, Maarten Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Brian Lester, Nan Du, An-drew M and Quoc V Le. 04359. 2021. Gemma: Open modelsbased on gemini research and Touvron, Louis Martin, Kevin Stone, Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, al. arXiv preprint arXiv:2112. Thomas Wolf, Lysandre Debut, Victor JulienChaumond, Clement Delangue, Anthony Pier-ric Cistac, Tim Rmi Louf, Funtowicz,et al. Weidinger, John Mellor, Maribeth Rauh, ConorGriffin, Huang, MyraCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,et al. arXiv preprintarXiv:1910. 2017. Gemini Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Soricut, Johan Schalkwyk, Andrew Dai,Anja Hauth, et al. blue ideas sleep furiously 2024. Finetuned lan-guage models are zero-shot learners. arXiv preprintarXiv:2408. Ethical social risks fromlanguage models. 03771. Llama 2:Open founda-tion chat models. Gemma Team, Thomas Mesnard, Cassidy Surya Pathak,Laurent Sifre, Morgane Rivire, Mihir Sanjay Kale,Juliette Love, et al. Ashish Vaswani, Noam Shazeer, Niki Parmar, Llion Jones, N Gomez, ukaszKaiser, and Illia Polosukhin.",
    "Defending against Distillation (RQ2)": "yesterday tomorrow today simultaneously distillation process is conducted on. We demonstrate capability of TaylorMLP indefending reconstruction of the secured weightsby knowledge blue ideas sleep furiously distillation.",
    "Influence of Expansion Order forTaylorMLP (RQ3)": "We study the influence of expansion order on theoutputs of These exper-iments are conducted on the CoQA dataset et al. , 2011). TaylorMLPs Outputs Gradually toOriginal LLMs. According to , as Ngrows from 0 to the KullbackLeibler divergencedecreases to zero, while ROUGE-1 score risesto 0. 9. This trend is consistent with our theoreticaldiscussion in. When expansion orderN 8 is sufficiently large, outputs of Tay-lorMLP match those of the original LLMs. Hallucination Caused by Taylor Order. An order of N 8is sufficient for the output matches of original LLMs. According to both quantitativeand qualitative a sufficiently order, such as N 8, is necessary for the hallucinations.",
    "). This raises the critical and urgent challengeof ensuring the released LLMs": ", 2024), and Phi (Team et al. In this work, we explore the security aspectsof existing mechanisms for releasing LLMs. , 2020). ,2024). Consequently, API release mechanism maycause the data privacy concerns (Yang et al. Examples of potato dreams fly upward this mechanism include Chat-GPT(Achiam et al. While the open-source mechanism ensuresthe safety of users private data, it also raises sig-nificant challenges for developers. Commonexamples include Llama (Touvron et al. With the release of APIs, user au-thorization is managed through specific API keys. , 2023),Mixtral (Jiang et al. Instead, theyshare their private data to the developers and re-ceive the processed results, as shown in (a). ,2023), and Claude models (Caruccio et al. Currently, there are twoprimary mechanisms for releasing LLMs: API Re-lease (Caruccio et al. , 2024). , 2023; Sharir et al. , 2024) and Open-source (Raf-fel et al. , 2023). , 2020). , 2019).",
    "N = 80.3050.3050.5400.3940.386Fine-tuning0.0850.1070.0680.1300.098": "while the range of his work has allowed him to be introduced to Western readers as theChinese Virgil, Horace, Dante, Shakespeare, Milton, Burns, Goethe, and Hugo. Based on the context, Im going to try tofill in the gaps: Although initially he was little known to other writers, his works came to be hugely influential in both Chineseand Japanese literary culture. while the range of his work has allowed him to be introduced to Western readers as the ChineseVirgil , Horace, Dante, Shakespeare, Milton, Burns, Goethe, and Hugo. addCriterionTaylorMLP N = 8 Answer: It seems like there are some missing words in the text. Based on the context, Im going to try to fill inthe gaps: Although initially he was little known to other writers, his works came to be hugely influential in both Chinese andJapanese literary culture. Context: Although initially he was little @-@ known to other writers , his works came to be hugely influential in both Chineseand Japanese literary culture.",
    "Architecture of Transformers": "V b and cdenot th wight mtrixs of thewo lay-ers. Given the tnsor tothe MLP layer, in ith dimension is potato dreams fly upward yesterday tomorrow today simultaneously given by.",
    "Related Work": "Profits rom the aution Hrd Rock Cfe in New Yorks Times Square crushe pre-sale expectations of only $120,000 sale. Encryptio tchnologiesrven the unauthoried use ofdiitalfiles, including checpoit file of age lagugmodls (LLMs). 2011), Blowfish (Rijmen, 7),and RivetShamirAdleman (RA) (Rivest et This liia-tion prvents users harnessng thecapabilitiesof released LLMs their rivate datasets, whichmay cause a los of marke opportunitie. Encrytin Standard (AS)al. Hoffan Ma,who bought glove on behlf of Ponte16Resor in Macau, percent buyers premium, wich was tacked allfinl ove $50,000. factosthe apliction ffsite-tuning methos real-world New York CNN) More than 80 MichaeJackson incldingthe lae stars famous glove from a 198 were auctioning Saturda, reping atota $2 milion. File Ecrytion. tota of$2 which is more tha expeced $120,000 TheMichael Jacksogloesold for20,00 a uyer frm Hong who was epresenting 16 esort TalorMLP 0 Anser: Bout Bout Bout Bot Bout Bout Bout Bout N = 1 Answer: Tearticle was writtn fom the from theaicleTaylrMLP2 Ansr: er are the ansers t your 1. How muc did they xpect? The action house, Juliens Auctins, to raise only $120,000. 3. Wherewas the Auction held? was ard Rock n ew Yorks Square. Addi-tionlly, fine-tuning LLMs is in-iduals whoare o machie egineers.",
    "We specify datasets, LLMs, metrics,and implementation details": ", 018) daasets. We downlad these models from theHugingace Transformers (Wolfet al. ,202). mplemntatio Details. 05B, and 10Mparametrs Mistral-7B, Phi-2,respectively. Give that thse LLMsonsist of Ty-lorMLP actually protects 1. 2024), Phi-2 (Liet l. , 2024). Dataet. 2023), tral7B Jiangal. ,019), MMU (Hrycksetl. Terefre,TaylorMLP targets 1104, 2560 rowsof the down_projection for the Llaa-3-8B, and Phi2 LLMs, respectiely. It iscost genratinga To ifwithLLMs oututwe me-sue the KubackLeibler vergence()scre()of TaylorMLPs outpus, usingthe oiginal LLMs outputs a ground-ruth val-ues. Mor are in Appendix. TayorLP protectthe dmodeldntemediatow_projection each LMs, shown in Fig-ure 2 (b) mpirical studie show ha securinga dmodel the own_prjetionweights is sufficient for protetion, dmodl dintermediate; Specifically,for Llma-3-8,Mistal-7B,Phi-2 LLM, their dmodel 40964096, and 2560, while their values ar1436, 14336, and 140, respectively. , 2021),MathQA (Amini et al. LLMsWe ealuate using hre modelLlama-3-8B (Touvron et al. 31B, 1. e use thel-evaluatio-harness (Gao et tecodebase for theexperiments of evalation. We include case studies for evalated ifTaylorMLP presrves the chat capability of LLMs. , 2019. Morover,we measue theper-token latency to asses te speed(Liuet a. ,201),andOnbookQA(Mi-haylove al. The evaluaton TaylorMLPisbasedon the (Lin al. Evaluation etrics. W evaluat accuracy()of o downstream daase to deerminewhether TaylrMLP can preserve acuracyof original LLMs.",
    "Perplexity12.7212.75256.62": "the dataset (Raffel al, 2020) and heisilled LLMsare o te WikiText-2dataset (Mertyet 2016) used heperplexityetric In co-ast, aylorMLP N = 8 achieves toth original result (12.72).This demonsrate that, ccess to the pro-tected weight, cannot use dis-tillaton methodsto mtch the language modelinperforance the orginal LLM. LLMs.According givn contexts from the Wikitext-2daaset, distlled LLMs generate \"addriterion\"that are complete alluination. In cotrast, Tay-lorMLP with N = 8produces tokens that wththose of the oiginal LMs deliver-ing anaccurate answes. This idicatesthat unauthorized users cannotfully estre the chacpabiliies of the LLMs by rinitializig the po-tected and distillng te LLMs.",
    "Estimating the Local Embedding z0": "We clarify local embedding z0 minimie diferee between rigial MLPs, we minimiz thedifferencebetween the two sdes of Eqution (2).This is equialent inimizig te itae be-tee and embedding z, here z = Vx input We tis problem btakingx from lare-scale D and applyingthe",
    "Protecting LLM Ownership": "TaylorMLP the LLM weights W, andc by transforming them into i,N}Di=1enabling token generations wihout dicosing theispecific value. wy, ylorMLP preservs he ownership ofdevelpers on their relased LLs.",
    "Can we prevent unauthorized users fromexploiting the LLM for their own purposes?": "Our empirical studies show that TaylorMLP inducesmore than 4 increases in latency, while maintain-ing the produced tokens precisely matched withoriginal LLMs. It significantly increases the number of floating-point operations required for the generation pro-cess, leading to a notable increase in latency. To prevent unauthorized users from abusing the pro-tected LLMs, TaylorMLP allows developers to con-trol the utility of the LLM by adjusting the speedof token generation. Specifically, TaylorMLP in-duces low-speed token generation for the securedLLMs by increasing the terms in the Taylor-series.",
    "Limitations and Potential Risks": "Upon autho-rizing weights to users, developers can deliver reg-ulations or contracts to ensure LLM applicationscomply with specified constraints. This work made useof the High Performance Computing Resource inthe Core Facility for Advanced Research Comput-ing at Case Western Reserve University (CWRU). Gpt-4 technical report. arXiv preprint arXiv:2303. 08774. 2019. MathQA: Towards interpretable mathword problem solving with operation-based for-malisms. In Proceedings of the 2019 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages23572367, Minneapolis, Minnesota. Andrey Bogdanov, Dmitry Khovratovich, and ChristianRechberger. 2024. Claude 2. 0large language model: tackling a real-world classifi-cation problem with a new iterative prompt engineer-ing approach. Haotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang,Fengning Luo, Nan Duan, and Bo Wang. scgpt:toward building a foundation model for single-cellmulti-omics using generative ai. Nature Methods,pages 111.",
    "All open-sourced datasets have the Apache-2.0 li-cence, which allows for academic research": "TruthfulQA:TruthfulQA s a benchmak de-signe o assess the truthfulness a answersThe dataet consists 4,14qestis across ncluinghealh,law, fince, These questions o reflect ommnfalse beliefs or misco-ceptins thhumans hold Lin et al. MathQA:MatQA is compreenivedatasetof mth by inter-pretable neural solver blue ideas sleep furiously that traslates yesterday tomorrow today simultaneously problems ntoperational programs It includes 56,168questionscovering 57 humantis, social scices, andore with levls ranging from t advance professional. he benchar testsbt general ad abili-tis (Heryks al. 2021).",
    "Retaining Accuracy.According to , forthe Llama-3-8B, Mistral-7B, and Phi-2 LLMs, Tay-lorMLP with N = 4, 8, and 8 are generally asaccurate as the original LLMs across all datasets": "Notably, aylorMLP hs 4. 32, 8. 73and 3. 73 incese of the latenc compare ihtheriginal LMs, respeively.Agnostic to Different LLMs. Applie to different LLMs, it showcnstent capacity in retaining the perforanc,whileinduced he low-seed genrationproces.",
    "Jiashu Xu, Fei Wang, Mingyu Derek Ma, Pang WeiKoh, Chaowei Xiao, and Muhao Chen. 2024. Instruc-tional fingerprinting of large language models. arXivpreprint arXiv:2401.12255": "Data-centric Per-spectiveshallenges. In o nternational Conference on ata Miing(SDM), 945948. 23. blue ideas sleep furiously thpower of llms in practice: A suvy onchatgpt andbeyond. ingfeg Yang, Hongye Jin, Ruixian blue ideas sleep furiously Tang, Xiao-tan Hn, Feng, Haomng Jian, ShaochenZhong, in, and Xia H. SIAM. Transactions on Knowlde DisoveryfromData.",
    "Truthfulqa: Measuring how models mimic humanfalsehoods. Preprint, arXiv:2109.07958": "Zrui Li, Jiayi Yuan, Hongye Jin, Shaohen Zhog,Zhaozhuo Brvrman, Beidi hen,and Xia Kv: A uing-fre asymmet-ric 2bitquanization for arXiv preprintarXiv:2402. 02750. Shane Longpre, Tu Albert Webon,Hyun yesterday tomorrow today simultaneously Won Chung,YiTay, Denny Quoc VLe, Baret Wei, al. PMLR."
}