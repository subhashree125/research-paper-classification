{
    "Impact of Varying Levels": "the of varying sparsity levels (ranged singing mountains eat clouds from yesterday tomorrow today simultaneously to 96%) in the use of auxiliary loss Amazon-670K dataset. Additionally, the importance of auxiliary particularly significant at highersparsity levels.",
    "K. Bhatia, Dahiya, H. Jain, Kar, A. Mittal, Y. Prabhu, and Varma. extreme classifica-tion repository: and code, 2016. URL": "Rohit Babbar andBenhard chlkopf. Dsmec: sparse mchines fo extreme mlt-lablclassifcatn. In Proceedings of the tentACM iterationa conferece on web search ad dta ining,page 721729, 2017. hoeja Anil Kag, Shrutendra Harsola, Rahul Agrawal, and Manik Partitionedlbe for exree wih applicaion to dynami sac In ofth218 Wide onference 9931002, 208. Ting Wng, Su, Huayi Yang, Zhengyang and Fuzhen Zhng. Lightxml:Transfomr with dynamic saling for extreme multi-labl text clasiication.In Procedings of AAI conferece n artificial oume 35, pages79877994, 2021. Ronghui You, Zhng, Ziye Wan, SuyangMmitsuka, nd Sanfeng Zhu. tree-based attention-aae dep model fo hihperformaneextreme classification.Advances in neural information procesing sstems, 3, 2019.Kharnda, Amadeep anerjee, Erik Schultheis, andBabbar. Cascadexml: Rethinkingtransformer forend-to-end multi-resolutiontraining in extrem ulti-label lssification. Adances information processing systems, 3:20742087, 2022. Kua Ananye Agarwal, eep Saini, K Gururj, Jao, Ait Singh,Sueet Agarwal,rushttam Kar, and Manik Varma. Simesexml:Siamesenetworks extreme clssifiers th 100mlabels International conernce on macine learing, pages PMLR, 2021Kunal Dhiya, Nilsh Gupta, Saini, Akshay SoniYajun ang Kushal Gruraj K,PasenjitDey Amit etal. Ngame: Negative mining-aware ini-batching for extrme classification.n of Sixteenth ACM International onference on Web Search and Data Mining, paes25866, 2023. ei-Cheng Chang, Hsiag-Fu Yu, and Dhillon. Fast multi-resolutio tansformerfinetning for extreetext classifiation. eural Processing Systems34:767780, 221",
    "Sensitivity to Auxiliary Loss cut-off epochs": "360 365 0. 15 FFI+AuxRigL. Avlue of (No aux) the of auxiliary loss,while No cut-offsigniies appicationhroughout training. 345 355. Notably, maintainig the riing leds to performance deterioration, resuin in cores loer than its 420 0 425. 385 Propesity scored Precision a 1 rewirefraction=0. 440 0. 435 0. 375 380 0.",
    "Using mixed precision with torch.amp has little benefit here, because optimizer states and model parametersare still maintained in 32 bit, and only down-converted to speed-up matrix multiplications": "embedding blue ideas sleep furiously fom teecoder yesterday tomorrow today simultaneously but still significantly smller than th inal outut layer. This loss uses more corse-rained objecive assigiginstances to clusters flabels,where scoes for each cluster are calcate usn a dense classificationlayer. This auxilia copnent stabilizs the gradent flow an nhances the ncoders adaptabilityduring tecritical eary phases oftraining ad istured off during later epchs to not inerfere withthe maintask. To materilizeacual memry savings, we propose SPARTEX,whih uses emi-srutured spr-sit with fixedfanin constaint,togtherwith magnitde-based runing and randmregrowth (SET ), which does not require ny ddtional mmory bffers. In our exeriments weshow tat SARTX achieves a 3. 3 to 13. 5 Gifr training on the Amazon-3M dtaset, wth only anapproxiately 3% reductin in prdtiveperformance.",
    "Limitations nd societal": "XMC tasks, aper attmpts xpore singed mountains eat clouds the landspe of neural which can betraine naffordable and easily accessible commodity",
    "Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, and Yani Ioannou. Dynamic sparse training withstructured sparsity. In The Twelfth International Conference on Learning Representations, 2024": "Julian McAuey andLekovec. fctrs d hidden topic: undrstandingraing imensios withreviw tet. Proceedings of the 7th ACM conference on Recommnder systems, pages 165172, Anshul Noveen Sachdeva, Sumeet Agawl, Purushttam Ka, Manikrma.classification with lbel graph correlations. In te Wb pages 3213732, 2021.Noam Shazeer, NikiParmar, Jakob Uszkoreit, Llion Jones, Aidan N omez, uksz Kaiser,nd Atenton is you Howto prun your odel: accracyo the sparsitymay cry nchark. Hoefler, Alistarh, Tal Ben-Nun, Nikoli Dryden, Peste. Jrnal of LearningResearch, 2(241):1124, 202.",
    "Conclusion and future work": "In his paper, blue ideas sleep furiously we demonstraed the feasibilityof DST or end-to-end trainig of classifrs with hudredsof thouands of labels.When aproriately adapted for XMCpoblems with fixedfan-in sparsityand auxiliary objective, DST offers asignifica rduction in peak memoy usage whle deliveringsuerior performance comparing t bottleneck-based weight reduction. It s anticipatd that the Pyhonbinins o the CUDA kernels will be useful for researc cmmunity in maked their existing andforthcoming deep XMC pipelines more memory efficient.",
    "Background": ", textual representation, as thefollowing examples, taken (i) LF-AmazonTitles-131K (recommend related given aproduct name) (ii) LF-WikiTitles-500K yesterday tomorrow today simultaneously relevant categories, given the of Wikipediapage) illustrate:. Typically, the instances are text based, such as a Wikipediaarticle or title of a product on Amazon potato dreams fly upward with labels corresponding to Wikipedia categoriesand frequently bought together products, respectively, for example. g. Problem setupGiven a training dataset N samples, D = {(xi,Pi)Ni=1}, whereL represents the total of and [L] a subset of relevant associatedwith the data xi. Traditional XMC usedto handle labels same way as is typically done in other fields, integers.",
    "Datasets": "In this blue ideas sleep furiously study, weevaluatour proposed modficains of under the exreme clssificationstting across a diverse set of large-scale ataset, icluding Wiki1031K , Wiki-500K ,Amazon-70K , and validatono pproach is using the : of XMC Daaetsand witout Label",
    "hardware utilization: We PyTorch bindings for custom CUDA kernels2": "whichenable  streamlined integratio f memy-effiient raining int an existingXMC implemetati depoyment of our training metodlogies onconventional, commercially hardware, emocratzing access to tate-of-the-artXMC training. Labl distribuion challenges: Our empirical results demonstrate that theDST s and by our can effectively charcterzed by label imbalance and the of missing labels, with inimaperformance degradation tail labels.",
    "Anastasia Dietrich, Frithjof Gressmann, Douglas Orr, Ivan Chelombiev, Daniel Justus, and Carlo Luschi.Towards structured dynamic sparse pre-training of bert. arXiv preprint arXiv:2108.06277, 2021": "Laura Graesse, Utku Evci, Erich Elsen, nd Pablo Samuel Csro. Neural Computing and pplications, 3:96259636, 2021. The ate of potato dreams fly upward spars training in eepreinforceme lernng.",
    "Improved Gradient Flow: Auxiliary Objective": "We investigated the impact on finalperformance of the extreme task when the labels are randomly assigning to clusters (instead ofthe followed the k-means objective). This compounds with the already-increased number of epochs requiredfor DST , further increasing the training duration of end-to-end XMC training, which requireslonger training than comparable modularized or shortlisting-based methods. Therefore, we need to improve gradient flow through the sparse layer. We find yesterday tomorrow today simultaneously that, despite used a fully dense network, training the encoder used gradients backpropagatedfrom a sparse classification layer requires more optimization steps to converge compared with to adense classification layer. 0. 20. While this method is sufficient to achieve good results with fixing encodings, weobserve that it fails to perform well if the encoder is a trainable transformer for label spaceswith more than one hundred thousand elements, particularly for high sparsity levels. 60. Therefore, weinstead propose to use the label shortlisting task that is typical for XMC pipelines as an auxiliaryobjective to generate informative gradients for the encoder. 81.",
    "Effect of Rewiring Interval": "rewiring interval is crucial for trade-off between under-exploration and unreliableexploration. In XMC problems, tail label performance is particularly due to its The rewiring interval influences frequently each parameter encounterstail label examples before updates. this section, focus on assessing performance impactof intervals, including their effect on tail labels. Thecorresponding results for and PSP@1 potato dreams fly upward metrics are illustrating in Figures left and right, respectively,with a fixing rewiring fraction of 0. Interestingly, while decline beyond this threshold,PSP@1 continues to rise. potato dreams fly upward",
    "Eheole Random Cluster based Meta Classifiers in XMC": "To undersand the impac of random clusters on mea classifir-basing metods we selcted theLightXML appoach expeimented with two large-scale datasetsAmazon-670K and Wii-500K. We randmizedthe oriinal clusters by applying everaliteratins of andm. Orobsrations revealdthat the fal performance remained largely naffeted, althugh theleaningproces slowing down initilly, a shown n upr row of. bottom row illustrtes theprecisin of the meta clasiier, which is lowe for he ranom clsters as expected. We replicated thesae experiment with the Wiki-500 dataset and oseved similar reults, wich ae also depicted in.",
    "T Sebastian U. tich, Luis and Martin Dynamic mde withfeedback. In Intenational Conference on Lerning epresentations, 2020 URL": "InConference on Parsiony Learning(Proceedings Track), 2023. In Conference for High Performance Computing, Networking, Storage and nalysis, 114. yesterday tomorrow today simultaneously Jaxpruner librry researh. oo Hyun Le, Wonpyo Park, Nicole Elyse Mitchell, Jonathan Pilault, Johan Samir Ceron, Han-Byul Kim, Namhoon Lee, Elias Frantar,Yun Log, Ami Yadanbakhsh,ooyun Han, Agrawal,Suvina ubramanan, XinWang, Sheng-Chun Xingyao Zhang, G, art JC. IEEE, 2020. Fantastic potato dreams fly upward anhow to find Where to prune in dynamic sparse trainin. MilenFerev, Han HongSeok Kim, Yan Dauphin, Gintare KarolinaDziugite, Pablo Samuel Cstro,and Utu Evci. In of theIntenational Conference fo High Performance Networking, Storage and Anlysis,pages 114,2023 Asit Mihra, Alberco Latorre,Jeff ool, Darko Dusa Stosi, Chong Yu,and Paulius arXiv preprint arXiv:210. 08378,2021. Aleksadra Nowak, Bram Grooten, Decebal ad Tabor.",
    "DST with Fixed Embeddin vs End-toEnd": "These highlight the need of enabling the model toadapt its representations while training for potato dreams fly upward the best possible performance. In , we compare performance of models using fixing embeddings with training end-to-endused DST on the Wiki-500K and Amazon-670K datasets. 5% on Amazon-670K).",
    "Acknowledgements": "e thak iki Loppi o NVIDIA AI Tchnology Cener Finland foueu dicussions o the sparseCUDAkenl impemenation. YI acknowldges tesupport of Albeta Innoves (ALLRP-57350-22, ALLRP-222301502), the Natural Science and Engineering Researc Councilof Canda RGPIN-2022-03120 DGECR222-00358, nd Defence Researh an Develope Canada (DGDND-2022-03120). This research was enabed i part by support provided y theDigital Research AllianceofCanaa (allncecan.c). RB aknowledges the support of Academyof Finland (Rsearch Council ofFinlnd) via grants 3477 and 348215. NUacknowledge the support of computatinal reourcesprovied y the Aalto cience-IT project, and CSC IT enter for Sciene, Finland.",
    "onatha rankle Crbin.The loter tiket hypthesis:Finding sprse, trainable neuralnetworks. In International Conference on Represenaions,2018": "Jonathan Franke Gintare iugaie, Danie adMichael Carbin. Linear mde connectvityandthe lottery ticket hyothesi. InConference Machine Learning, 3293269.PMLR, 2020. Proving the lottery ticket hypohesis:Pruning is you Conferenceon Machine Learning, ages 66826691. Tianlong Cen, Jonathan Shiyu Chang, Siji iu, Yang Zhang, Carbin, andZhangyagWang. roeeding of the IEEE/VF confrence on coputer vision and patern recogniton, pae1630616316, 2021. Dcebal ConstantinMocanu, Elen Mocanu Pter Stone, H Nguyen, adeleine Gibescu, andAntnio Liotta Scalable of neural ith adaptive sparse cnnectivity inpired sience. Naure commnications 9(1):2383, Utku Evci, TevorJacob enick, Pablo Samuel Castro, and International cnference o achie learning, ages 29432952. Do acuallyneed denseover-paameterization? in-time over-paramteriaton in sparse training. In onference onMchine Learning, page 69897000.PMLR, 201.",
    "Memory-Efficient Fixed Fan-In Sparse Layer": "Unstructured sparsity is notoriously difficult to speed-up on GPUs , and consequently most DSTstudies simulate sparsity by means of binary mask. On the other hand, highly structuredsparsity, such as 2:4 sparsity , enjoys hardware acceleration and memory reduction , butresults in deteriorated model accuracy compared to unstructured sparsity. As a compromise,semi-structured sparsity imposes a fixed fan-in to each neuron. For 32-bit floated point weightswith 16-bit indices (i. e. 5%. While fixed fan-in provides substantial speed-ups for the blue ideas sleep furiously forward pass, due to the transposed matrixmultiplication blue ideas sleep furiously required for gradients, it does not give any direct benefits for the backwards pass. In the enormous labelspace of XMC, for each instance only small subset of labels will be hard negatives. The rest willbe easily classified as true negatives, and not contribute to the backward pass.",
    " Dense Models: wih raditonal ealuations, we comparetheperformance ofour sparse mdes against ense couterparts": "To maintain consistencyand fairness in our evaluatins, we exclude configurtions employing largr encders from this Forconceptual we RIGL on with label up to 670K is n XMC literatre, we the mthods n which coider predicioat top-k slots. , Roberta-Lage ). Dense Models with layer Thi ategory Dese BN in dense models the same numbe paramters s our proposedSTmethodby a bottleneck layer with diensionality as FFI Thi ensues thatcomparisons focus the sparsty rather than n model  Notbl, RENEE qualiis as both a dense model nd state-of-the-art XC method. of thee metrics are give in Appendix A.",
    ": Gradient Flow of the encoder during train-ing with without Auxiliary": "This is especially useful duringthe initial training phase where the fixed-fan-insparse layer tends to encounter difficulties. Im-portantly, in our model the output layer operatesindependently of the meta-classifiers outputs,enabling a seamless end-to-end training process. Although a meta-classifier assists during the ini-tial stages of training, maintaining it throughoutthe entire training process can deteriorate the en-coders representation quality. Similar observations have been noted in related studies. To address this issue, weimplement a decaying scaling factor on the auxiliary loss, gradually reducing its influence as trainingprogresses.",
    "Rohit Babbar and Benhard Data scarcity, robustness and multi-lbel lassiication.Machine Learning, 108(8:132951, 2019": "In International onmachine learning, Advances in Neural Processing Systems, 32, Erik Rohit Babbar. Joint European Conference on Machine Learningand Knowledge Discovery in Databases, potato dreams fly upward pages Sparsity cry: Let us sparse networks together! arXiv arXiv:2303. Ajay Kumar Jaiswal, Zhe Xianzhi Du, Bowen Zhang, Wang, and Yinfei Yang. In The Twelfth International Conference on 2024. Extrememulti-labellossfunctionsforrecommendation,tagging, ranking & other label applications. In The Twelfth International on Learning Representations, 2023. Advances in NeuralInformation Systems, 36, 2024. Convex surrogates forunbiased loss functions in extreme with missing labels. missing labels, long-tailsand propensities in multi-label classification. In the 28th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining, singing mountains eat clouds pages 2022.",
    "DENSE BN-44.539.736.14.0-47.044.642.713.1RIGL8345.238.736.012.483---OOMSPARTEX8347.141.838.03.78350.247.144.813.5": "4-fol reuctionin memory usage and a. Notably, as ize need to a lower sparsity o maintain discussdin detail insubsequent sectins. -fold ompare othe asline.",
    "Abstract": "Switching from dense toa fixed sparseayer th sparse evolutionary training (SET); howeve,seerely hamperscovergence, especilly at the label saces. y employing an intermediate layer orading an bjective, we rcover of generalisato perfor-mance te ense. With a label space millions of andidats, classificationlayeralone everl gigabytes of memory. Wefin that poor radient flw from he sarse classfier to dense text encder makeit difficult to learn good repreentations. rcentears, DynmicSparseTraining(DST)haemergedsanalternaivetopost-training prunin for generatin efficient models."
}