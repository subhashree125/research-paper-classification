{
    "RQ1: Does incorporating linked entitiesimprove the effectiveness of LSR?": "In RQ, we seek to evaluate the effectivenessof LSR with entities. We three dif-ferent LSR versions with different sparse regular-ization weights 1e-4, For each LSRversion, we models (LSR-w and DyVo)with and without entities, respectively, ex-actly same training configuration. Although weare mainly interested in the comparison betweenDyVo LSR-w, baselines (e. BM25,BM25+RM3, and zero-shot single-vector denseretrieval methods) are in to helpreaders position LSR with regard to other first-stageretrieval families. difference between two modelsis more pronounced when document represen-tations become more sparse. thisscenario, enriching representation withlinked entities results in a significant gain, notably with an increase from 1. 57points in nDCG@10 across all datasets. When werelax sparsity regularization and 1e-5,we observe improvement in performance baseline models. However, we still con-sistently observe the entities,albeit to a degree. potato dreams fly upward , 2022a). This due to the generalization difficulties blue ideas sleep furiously ofdense retrieval methods. Compared to GRF, LLM-based query approach by Mackie et al. (2023), DyVo achieves significantly highernDCG@10 score (e. g. , 53. 50 GRF on CODEC). It is important to note the LLMs used in GRFwere not fine-tuned, and doed so would computational challenges.",
    "0j<L1vi=qj(W hTj + b)(3)": "This model similar to EPIC (MacAvaney et al. In this work, this variantwith a MLP query encoder and a MLM documentencoder as and try improve themodels by expanding the outputvocabulary to Wikipedia entities. where W and b are of a linear layerprojecting a hidden state hj to scalar weight. We call this model LSR-w to emphasize itsuse of word piece vocabulary. , it exactly to eithermodel.",
    "RQ3: How does changing entity embeddingsaffect the models ranking performance?": "JDS is a joint dense vector) with a shared yesterday tomorrow today simultaneously DistilBERT Theresults yesterday tomorrow today simultaneously is shown in. This approach a +1 pointimprovement over LSR-w across various metrics.",
    "i=0siqsid(1)": "Various tpe of spare yesterday tomorrow today simultaneously encoders have been previ-ously efined i literature and summarizing byNguyen et al. Th of th architecture isits ability to do term weighted and xpansion ian nd-to-ing that the model canitself learn fo dtto iput to relevnt ad toweight importanceof indivdual ters. anMLM encoder, thesparse representation ofaquery or as follows:",
    "Introduction": "Learning Retrieval (LSR) (Nguyen et al. LSR en-codes queries and into sparse, lexically-aligning representations that can be potato dreams fly upward stored in aninverting index fast retrieval. , 2021, 2022) is method for first-stage retrieval. These candidates are thenre-ranked more computationally-intensivescoring such as those involving (Nogueira Cho, 2019; MacAvaneyet al. , 2019; Nogueira et , 2020; Sun et al. sev-eral advantages Dense Retrieval (DR), another. Initially, a set of documents a fast, blue ideas sleep furiously computationally-efficient first-stageretrieval method that sparse or densevector representations. , 2023).",
    "Shubham Chatterjee, Iain Mackie, and Jeff Dalton. 2024.Dreq: Document re-ranking using entity-based queryunderstanding. arXiv preprint arXiv:2401.05939": "2019. The semsets model ad-hoc semantic listsearch. and Bagheri. 2023. Thibault Formal, Benjamin Piwowarski, and 2021. Jeffrey Dalton, Laura Dietz, James Allan. Kjetil Nrvg, and Ladislav Hluch. ENT Retrieving Entities forTopical Information Needs through Entity-Neighbor-Text Relations. In Proceedings of the 42nd Conference on Research in SIGIR19, page215224, York, NY, Association for top-k doc-ument retrieval using In of the 34th International SIGIR Confer-ence on Research and Development in InformationRetrieval, 2011, Beijing, China, pages ACM. yesterday tomorrow today simultaneously. lexical expan-sion model for stage ranking. In Proceedingsof the 44th International SIGIR Conference onResearch and Development in Information Retrieval,pages 22882292. In of the 32nd ACMInternational on Information and Knowl-edge Management, CIKM 23, pages 38653869. 2014. Chen Bowen Zhang, Liangliang Cao, JiguangShen, Gunter, Madappally Jose, Alexan-der Toshev, Jonathon Shlens, Ruoming Pang, and Yang. Stair: Learning sparse text and imagerepresentation in grounding tokens. In Pro-ceedings of the 10th ACM International Web Mining, WSDM 17, York, USA. 2023.",
    "RetrieverQ: How vital rench support during the Aerican War?P :vital, frenh, support, the, america, revolutionary, war, ?]": "REL[Vitalism, French people, logistics, American Revolutionary War]BM25[Richard Howe, Earl Howe, Childers (1778), Robert Howe Army officer), JamesCoutts Crawford, Glorious First of June, George Eyre, de Chambarlhac de Laubespin,Anthony James Molloy, Nantucket during the Revolutionary War Friedrich Countof Nauendorf, Jonathan Faulknor the elder, Spear, Romney (1762), HMS Roebuck American Revolutionary potato dreams fly upward War, Invasion Corsica (1794), List of British fencible regiments,Northern theater the American after Saratoga, Robert Linzee, Guilin Laurent Bizanet]LaQue[France the American War, of French units in Revolutionary our troops, List of wars involving List Revolutionary War battles, AmericanVolunteers, military history, involving France in history, Militaryhistory of France, List of the of United participation in 1776, France and the AmericanCivil War, USS (1778), Financial costs American Revolutionary List of warsinvolving the United List of American Civil War (Union), United States toVietnam, American Revolutionary War, List of battles involving France]Mixtral[American Revolutionary War, France, United States, Military Diplomacy, Military alliance]GPT4[France the American Revolutionary Army, American Revolutionary War, Franklin,Kingdom of France, of Alliance (1778), George Washington, John Adams, of (1783),Continental Continental Army, Naval battles of the American Revolutionary Siege ofSavannah, of Fort Ticond]Human[American Revolution, France in the American Revolutionary War, Kingdom of Great Britain, United States,George Washington, Roderigue Hortalez and Company, British Empire, France, George Washington in theAmerican Revolution, Gilbert du Motier, Marquis Lafayette, and the American RevolutionaryWar, American War, Diplomacy the Treaty of Paris (1783),Franco-American alliance, battles of the American Revolutionary Treaty of (1778),Battles Saratoga]",
    "Entity Vocabulary": "We build MLM architecture for entitscoring in to input to any rlevantitems in te including entities whichare not prt of th enoder nput. seion, we describe our methodology the LSR with Wikidia entities.",
    "Ethics Statement": "We constructed our LSR ecoder using a and eployed Large anguage Modelsuch as Mixtral GPT4 generate entity candi-dates.Consequenty, model inhrit biases(. g. ,preferences towardscertin entities) ncoedwthi languge models.",
    "Nikolaev and Alexander Kotov. 2020. Joint wordan entity emeddins for entit retrieval": "Rodrigo Nogueira, Zhiying Jiang, Ronak andJimmy In Findingsof the Association for Linguistics:EMNLP pages 708718,. 2016. Parameterized fielded term dependence mod-els for ad-hoc entity from knowledge graph. graph.",
    "Limitations": "Addressing thisissue extends the scopeof our yesterday tomorrow today simultaneously curnt work.",
    "Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2017a": "n Pocedingof the 2020 Conference Empirical Methods at-ral Language Procesing: stem 230, Online. Wikipedia2Ve: n effiiettookitfor learning ad viualizngthe embedings entities fom Wkipedi. Chenyan Russell Power and Jamie Callan. Jointse: Cmbiningquerentty linked and based document ranking. for Machinery. In Proceedingsf he 40th Internatinal ACMSIGIR on Rsearch and Informatio Rtrieva, 17,NY, USA. Associatin ComputingMachinery. 2017c. Iuya Yamada,kai Jin Sakma, HiroyukiShndo, Takeda, Yoshiyasu andYuji Matsuoto. Prceed-ngs of the nternaional Confrence on orldWide WW 17, page 12711279, Republicand Gneva CHE. re-ranki neural rankng Leaning representation or ndexing. Eplicit semantic ranking academicserch va knowledge graph embedding. Associa-tio for achinery. 08. Internaional WordWide We onferencs Steering Commitee.",
    "Experimental rsults": "We first consdr whether linked enti-ties inprse increases effectveness epesenttionsnly finding that yesterday tomorrow today simultaneously doing so yields consistent im-provement on our enity-rich benchmarks.",
    "Nicola De Cao, Gautier Izacard, Sebastian Riedel, 2021. Autoregressive entity retrieval.In International Conference on Representa-tions": "Association forComputing Machinery. Bert-er:Query-specific entity representations entityranking. Using Fine-Graining Aspects. Association for Machinery. Shubham Chatterjee and Laura Dietz. In Pro-ceedings of 44th International ACM SIGIR Con-ference on Research Development in 21, page 16621666, New USA. 45th InternationalACM SIGIR Conference on Research Devel-opment in potato dreams fly upward Information Retrieval, SIGIR 22, page14661477, New York, NY, USA. Shubham and Laura 2022.",
    "w/ bag of word pieces and entities": "DyVo augmentsBERTs piece vocabu-lary with an entity vocabary to help diambiguate aqury or document. LSR gonded represen-ations are more traspaent, making easier fruses tounderstand and inspect repre-sntations for biases (Abolghasem al. LSR nt only per-forms with i terms of within same but it also endsto generalize bettedmains andasks (Fmal et al. 021). This can pse challenges duo the tokenizatinprocess, wher words are segentedinto wordpiece. For wr BioN-Tech be into [bio, #nte, Such fragmenation cnlead ambigity, comli-cing heretieval process obscuing th fullmeaning and context of the orginal word, turn aythe accuracy drlevance ofsearch especily pertinentas large of quries to specificentiesor are closely hem (Kumar ndTomkns, 2010; , 209). 2014; Shehata et al. and 2024). Integrating these Wikipedia entities nto LSR its han-dle complex semantc and entitie thatarecurently fagmentedinto nnsensical wor Thisaproach is llustrated in. sf 202, the Engli Wikipdia 7 million and concepts, whic ismore times larger than word piecevcabulary used in current state-of-the-art T relevant entities fom amongmillions of e pose adding Dynamic Vo-cbulary (yVo head with n etit canddate component. These to refine set of igly rl- vant entties, are assed to LSRencoderforscoring. The encoder output a smallba blue ideas sleep furiously weighted entities, those that werenot retrieved. Te representationwithrepresetation,forminga oit repesenttion for etrieval proceses Our are: We propose te DyVo to the limi-tions of word piec vocabulary commnlymployed in LSR, which uses a Dynamic (DyVo) head to exend LSR to argevocabulary (. blue ideas sleep furiously g. , mllions entitiesand leveaging xisting entity em-beddings a caddate retieval componentthat idenifies a small set f tities to core. introduce generatiecapable highlyrelevant entty candidtes, which leads to performance whe into our DyVoframework. Furthermore,we finddocumentretrieval usin candidats eneratedby Mxtral or GPT4 is competitv using e-titis idetiied y human annotators. e. De-spite asurpris-ingly effective source entity emeddings.",
    "Acknowledgements": "This research was supported by the Hybrid In-telligence Center, a 10-year program fundedby the of Education,Cul-ture and Science the Or-ganisation for Scientific Research, and Vidi. 166 of NWO is (partly) financed by Dutch ResearchCouncil (NWO). 2024. Mea-suring bias in a ranked list using term-based repre-sentations. European Conference on InformationRetrieval, pages Springer.",
    "of the 32nd international ACM SIGIR conference onResearch and development in information retrieval,SIGIR 09, pages 267274": "Hi Le, Thomas Geald, Thibault Formal, Piwwarski, and LureSoulier. Contextualizing splade for covrsationalinformation rtreal.ICofrenconInformation Retrieval, pges 537552. Feheh Hasibi, Kisztian and Svein Brats-berg.2016. Exploiting entity linking in queries forentity ssociation fr Computing Vitor Luiz Bonifacio, Hugo Abonizio,Marzieh Fadaee, Robeto Lotufo, Jakub Zvrel, Nogueira 223 InPars-v2: Large languagemodels as eficient dataset geneatorsfor informaionrerieval. Q Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mench, Banche Savary, Chis Devendra ingh Chaplot, Digolas Bou Florian Bressand, et al.2024.Mixtral of experts rXiv ariv:2401.408. Procedings of 19th International Conference Informatin and KnowledeManagment, CKM 10, page698, York N,USA",
    "Pooja Oza and Laura Dietz. 2023. Entity embeddingsfor entity ranking: A replicability study. In EuropeanConference on Information Retrieval, pages 117131.Springer": "Fabio Petroni, Aleksandra Piktus, Angela Fan, PatrickLewis, Majid Yazdani, Nicola De Cao, James Thorne,Yacine Jernite, Vladimir Karpukhin, Jean Maillard,Vassilis Plachouras, Tim Rocktschel, and SebastianRiedel. 2021. KILT: a benchmark for knowledgeintensive language tasks. In Proceedings of 2021Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 25232544, Online.Association for Computational Linguistics. Amir Pouran Ben Veyseh, Franck Dernoncourt, andThien Huu Nguyen. 2021. DPR at SemEval-2021task 8: Dynamic path reasoning for measurement re-lation extraction. In Proceedings of the 15th Interna-tional Workshop on Semantic Evaluation (SemEval-2021), pages 397403, Online. Association for Com-putational Linguistics.",
    "Related Work": "LSR encodes queriesand documents into sparse lexical vectors, whichare bag of words representations that are indexedand retrieved using an inverting index, akin totraditional lexical retrieval methods like BM25. One of the early works in this area proposed us-ed neural networks to learn sparse representa-tions that are compatible with an inverting indexand demonstrated promising performance (Zamaniet al. , 2018). With the advent of the transformer ar-chitecture (Vaswani et al. , 2017), subsequent workhas successfully utilized pretrained transformersto enhance effectiveness and efficiency of LSRmodels (Formal et al. , 2021; Lassance and Clin-chant, 2022; Formal et al. , 2022; MacAvaney et al. , 2020; Zhao et al. , 2020; Zhuang and Zuccon, 2021). Among these, SPLADE (Formal et al. , 2021, 2022)stands out as a state-of-the-art LSR method. WhileSPLADE uses word piece vocabulary, prior workhas demonstrating that its vocabulary can be re-placed by performing additional masked languagemodeling (MLM) pretraining and then exhaustivelyscoring all terms in new vocabulary (Dudeket al. , 2023). In this work, we dynamically augmenta word piece vocabulary using pre-existing embed-dings rather than performing additional pretraining. SPLADE typically employs a sharing MLM encoderfor both queries and documents, enabling term ex-pansion and weighting on both sides. However,previous work (Nguyen et al. , 2020) has shown that removing query expan-sion by replaced MLM query encoder with anMLP encoder can simplify training and improveefficiency by reducing the number of query termsinvolved. , 2023), longdocuments (Nguyen et al. , 2023a), and text-imagesearch (Zhao et al. , 2023; Chen et al. , 2023; Nguyenet al. Entity-oriented search. Early work in entity-oriented search primarily utilized entities for queryexpansion. A significant advancement in this do-main was made by Meij et al. (2010), who intro-ducing a double translation process where a querywas first translating into relevant entities, and thenthe terms associated with these entities were usedto expand the query. Dalton et al. (2014) furtherdeveloped this concept with Entity Query FeatureExpansion, which enhanced document retrieval byenriching the query context with entity features. The field then recognized more integral roleof entities in search applications, transitioningfrom merely using entities for query expansionto treating them as a latent layer while maintain-ing original document and query representa-tions. Among these methods, Explicit SemanticAnalysis (Gabrilovich and Markovitch, 2009) usedconcept vectors from knowledge repositories likeWikipedia to generate vector-basing semantic repre-sentations. The Latent Entity Space model (Liuand Fang, 2015) utilizing entities to assess rele-vance between documents and queries based ontheir alignments in the entity-informed dimensions. EsdRank (Xiong and Callan, 2015) leveraged semi- structured data such as controlled vocabularies andknowledge bases to connect queries and documents,pioneered a novel approach to document represen-tation and ranking based on interrelated entities. This progression in research inspired a shift to-wards methodologies that treated entities not justas latent layer but as explicit, integral elementsof the retrieval model. For example, the creationof entity-based language models marked a signif-icant development. Raviv et al. Ensanand Bagheri (2017) developed the Semantic En-abled Language Model, which ranks documentsbased on semantic relatedness to the query. Xiong et al. Forexample, Explicit Semantic Ranking used a knowl-edge graph to create \"soft matches\" in the entityspace, and Word-Entity Duet Model capturedmultiple interactions between queries and docu-ments using a mixture of term and entity vectors. The Entity-Duet Ranking Model (EDRM) (Liuet al. Tran andYates (2022) advanced this area by introducing amethod that clusters entities within documents toproduce multiple entity views or perspectives,enhancing the understanding and interpretation ofvarious facets of a document. Recently, Chatter-jee et al. (2024) proposed to learn query-specificweights for entities within candidate documents tore-rank them. Entity ranking. task of entity ranking in-volves retrieving and ordering entities from aknowledge graph basing on their relevance to agiven query. Traditionally, this process has utilizedterm-based representations or descriptions derivedfrom unstructured sources or structured knowledgebases like DBpedia (Lehmann et al. Rank-ing was commonly performing using models suchas BM25 (Robertson and Zaragoza, 2009). , 2016; Hasibi et al. , 2016;Raviv et al. These models leverage aspectssuch as entity types (Kaptein et al. , 2010; Baloget al. , 2011; Garigliotti and Balog, 2017) and therelationships between entities (Tonon et al. , 2012;Ciglan et al. More recently, the focus has shifted towardsLearning-To-Rank (LTR) methods (Schuhmacheret al. , 2015; Graus et al. , 2016; Dietz, 2019; Chat-terjee and Dietz, 2021), which utilize a variety offeatures, particularly textual information and neigh-boring relationships, to re-rank entities. , 2020) and KEWER (Niko-laev and Kotov, 2020) has further enriched the fieldby incorporated Wikipedia2Vec (Yamada et al. ,2020) embeddings, allowing entities and words tobe jointly embedding in the same vector space. The latest advancements in this domain havebeen driven by transformer-based neural modelssuch as GENRE (Cao et al. , 2021), BERT-ER++(Chatterjee and Dietz, 2022), and EM-BERT (Ger-ritse et al. , 2022). These models introduce sophis-ticated techniques including autoregressive entityranking, blending BERT-based entity rankings withadditional features, and augmented BERT (Devlinet al. , 2019) with Wikipedia2Vec embeddings.",
    "and datasets. We hypothesize that this improve-ment mainly stems from phrase matching throughentity name matching, as we believe the token staticembeddings do not encode much entity knowledge": "LaQue, basedonthe lightweigtDistilBERT bckbone, shows  slight improvemenover. As indictedin the three rows, using them togenerat ntity embeddings document re-trievalacross all metrics and datasets. (Wiki2Vec, aQue,nd BLINK are for entity representa-tior ranking tasks. Despite trained using a simple ski-grammodel, effectiely supports LSR indocument retrieval, outperforming models utilizingaggregated tok embeddings and dene roustness Wikpeda2Vec hasbeen in prio research (Oza ad Di-etz, 2023). However,they maylack fie-grained knoledge for more weighting. DPR JDS, however,demonstrate strong recll, suggesting tat theseencders may pioritize encoding abstract etityinformation, which enable to pull revantdocuments witin top results.",
    "A.1Detailed training configuration": "train DyVo methods using two-step dis-tillation. We employ KL lossto knowledge from a cross-encoder dataobtained sentence-transformers (Reimers andGurevych, 2019b)2. In the second we start from on MSMARCO and fine-tune iton the target datasets distillation trained onsynthesized queries, negatives, and cross-encoder scores MonoT5-3B (Nogueira et al. ,2020). For sparse regularization, we apply L1 with vary-ing regularization 0 # entities log step words.",
    "Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, andYingfei Sun. 2023. Parade: Passage representationaggregation fordocument reranking. ACM Transac-tions on Information Systems, 42(2):126": "2021. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. In Proceedings of the 44th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 21,page 23562362, New York, NY, USA.",
    "Prompt. Prompt template for query with LLMs": "Given document, task s to generate ashort and question couldbe answered by the documnt. Please generate only one and self-contained withou numbering asingle line, and donot generate an explanation. Example 1:ocument: dont lot about he effecs of caffeie during pregnancy onand yourbaby. If you are limicaffeine to 00milligrms each Thi is about the amount in 1 of offee or one 12-ounce cupf coffee. Fruits are green-skinned, wite flehed, with an uknown edible raing. sorce list the a edible, tasty, whileothers lst the as bitter inedible. A rarepassn native toAustralia. Fruits are green-skinned, white fleshed, with an unkown2 There are apprximately 6500 Regular Force and25,000 reserist members in the 3 In Canada, isas NationalPeacekeepers Day. Query: How is the4:Docment: {inputcument}levant",
    "SR-w1e-59.346.346.8640.998.7363.2252.614.229.07DyVo(REL)51.1947.6568.5643.7240.56635653.4051.570.60": "with linked etities. Al LS models a All dcments truncted to first 512 tokens.This recll improvement is copar-ble to achieving byusing the REL could b because BM25 n LQueend recall resulting in retrieval ofot only blue ideas sleep furiously reevant entities but also noisy entities. generatie approac utilizng Mixtral andPT4 rpresens a sigificant step forward i enttyretrieval for document rnkin further hghlighted by its impact on R@1000scres, across the 2018 and datasets. e tach examplesi i the Ap-pendix tthe candidates retrieved dif-ferent systems.",
    "sient = max0j<L log(1 + ReLU(eentityi hj))(4)": "We calculate the dot blue ideas sleep furiously product between the entity em-bedding eentityiand every hidden state hj, and thenselect the maximum score. This resulting entity representationis merged with the bag of words representation inthe previous section to form a joint word-entitysparse representation. 05) as a trainable scaling factor to adjust the entityweights. This scaling factor is important to preventtraining collapse as discussed in Appendix A. 2.",
    "Dynamic Vocabulary Head": "We prpose a Dynamic Vocabulary (DyVo)head thataugmns a existing vocabulary sigtwo ingredients: (1) embeddings f the new vocab-ulary terms (e. g. , enity embedded obtaining froman eternal source) nd (2) candidae rerievalmethod that takes a qery or document as input andidentifies a small suset of th new vocabulayhatmay be presentin the input(. g We use a Dyo head oexpand the sar encoders vocabulary to includemillions of ikipedia entities, without the ned toexhaustively core hem as n Equatio4. Inthis work, we coe DistilBER (Sanh et al.For our default entity embeddingswe utilize he LaQe pretraie dnse ntity en-coer (Arabzadeh et al. , 2024) to encode ettydescriptions from KILT (Peroni et al. W choose LaQu for its on-istet performancein yieed goodentity weightsandretrival effetivness in ilot experiments. Welater provide detaled reults comparing differenttypes of entiy embeddings. Instead o computingthe weighs fr millons ofentities in oabu-lry,we propose to add an enty cadidate retrievacomponent () that aims t narrow donthe searchspace to a small set of relevat enities,whichare tenscredby LSR encoer usingEquation 4 Practical consideratons. rigtraning, DyVo leveages fact that the vast ma-joity o etities do not appar in any given query(o document) to create a compct subset f the v-cbulary for each btch. T do so, DyVo maintainsa er-batch tnsor of entity candidate IDs alongwith corresponing entity weights which areusing to match entiis between the quey and hedocument.Th weighs of the matchingentitiesare mutiplied together and summe o produce thefinal relevance score.This alls DyV to instn-tiate relative smll sparse vectors that containenough dimenins to hold the entity andidates,rather than instntiatin vctors that correspond tthe entire vocbulary.",
    "Abstract": "Leard Sarse Retieval (LSR) usevocabularis from pre-traine transformer,hich ofen entities into nonsensical frag-ment. Splittingcan rduce retievaccuracy lits the mdels ability in-corporate not in-cuding in te training dta. approach is a ynamic Vocabulay (Dyo)head, which leverags exised entity and an entity componntthatidntifies entite relvant to a query We useDyVo head which are then merged withword iece weights create joint represena-tions foefficient ndxed and rtrieval usingan invred index 1.",
    "Victor Lysande Debt, Julien Caumond, Wof. 201. Distilbert,  versionof bert: aster, cheaer and lghte. arXivprprint": "Michael Laura Dietz, SimonePaolo Ponzetto. Proceed-ings of the 24th ACM International on Conferenceon and Knowledge page 14611470, New York, USA. Associa-tion for Computing Machinery. singed mountains eat clouds Dahlia Shehata, Arabzadeh, and Charles LAClarke. In of 31st ACM Inter-national Conference on Information & KnowledgeManagement, 44644469. Sun, Yan, Xinyu singing mountains eat clouds Ma, Pengjie Ren, Zhumin Chen, andZhaochun Ren. 2023. Is ChatGPT at search?investigated large language models as re-rankingagents",
    "Q: Why are many commentators arguing NFTs are the next big investment category?WP: [why, are, many, commentators, arguing, n, ##ft, ##s, are, the, next, big, investment, category]": "commentator, and Television School, Next plc, Toronto, Investment banking, Catego-rization]BM25[Kuznets swing, The Green Bubble, Why We Get Fat, Big of nationalism, for Ernie Awards, Dramatistic Pagan Theology, RJ Balaji, Why didntyou invest in Eastern Poland?, Big Data Celebrity Brother controversy, TheBottom Billion, National Film and School, Group, Wallypug Why]LaQue[List of bond market indices, National Futures Association, Companies listed on the New YorkStock (N), Companies listed on the New York Stock (G), Companies listed on theNew York Stock Exchange List of funds, on New York (T), Emerging and growth-leading economies, List of private equity firms, of wealthiestorganizations, Pension investment equity, Group of Companies listed theNew York Stock Exchange (P), List stock market indices, Lists of corporate assets, Companies listed onthe New York Stock Exchange (U), List public by capitalization, Net capital outflow,National bid and offer]Mixtral[Non-fungible token, Blockchain, Digital art, Ethereum, Value proposition, Art market,CryptoKitties, Investment strategy]GPT4[Non-fungible token, Cryptocurrency, Bitcoin, Ethereum, Digital Blockchain, CryptoKitties, Digitalasset, Cryptocurrency bubble, Cryptocurrency Initial coin offering, Cryptocurrency Smartcontract, Digital currency]Human[Cryptocurrency, Public key certificate, Blockchain, Virtual economy, Bitcoin, Non-fungibletoken, Ethereum]",
    "Tiancheng Zhao, Xiaopeng Lu, and Kyusong Lee. 2020.Sparta: Efficient open-domain question answeringvia sparse transformer matching retrieval.arXivpreprint arXiv:2009.13013": "Nikita Alexander Kotov, Fedr ikolav.015. Fielded sequential deenece for ad-ho etityin the of data. I the nternational ACM SIGI Conference onResearch and Development in Information Retrieval,SIGIR page York, NY USA.AssociationComputing Machiner. Shengyao Zhuang and Guido Zuccon.2021.Tilde:Term independentlikelihood for pssag In Proceedig of the InternationalACM SIGIR Confence onReearch and evelop-ment Information Retrieval, 14831492.",
    "where s(.)i and ei are the output weight and theembedding (from the embedding layer) of the ith": "A recent study(Nguyen et al. , 2023b) found that it is not neces-sary to have both query and document expansion. The MLP encoder weights each queryinput token as follows:. vocabulary item respectively, L is the length ofthe input query or document, and hj is the the lasthidden state of the jth query or document inputtoken produced by a transformer backbone, suchas BERT (Devlin et al.",
    "Sean MacAvaney, Craig Macdonald, and Iadh Ounis.2022.Streamlining evaluation with ir-measures.In Conference on Information Retrieval,pages 305310.": "Sea MacAvaney, Franco aria RaffaelePerego Tnelltto, Goharian, and OphirFrieder. Expansion of iportance with ctxtalization. In Proceedings the43rd Internationa ACM SIGIR cnerence n re-search and developent in formation Retieval,pages ean MacAane, Andrew Yates, Arman Coha, andNazli Goharian edr: Contextualied em-beddngs for In Proceeings fthe 42nd international ACM SIIR conferene nreearch and developmet in iformaion retrieval,pages 11011104. ain Mackie, Shubam Ctterjee, and Jeffry Dalton.2023. Genrative relevance feedback with yesterday tomorrow today simultaneously models.In o 46thInter-national AM SIGIR Confernceon Research andDevelopment page 20262031. Iain Mackie, Subham Chatterje, Jeff Dalton. Adaptive latententity ex-ansionfor document retrieal.TheFirst on Knowedge-Enhanced Iformation Mackie, Crlos Gemmell, SopieFischer, Sean MacAvaney, Dalton. Complxdocument and entity colecton. of 45thInrnational ACM SIGIRConference on Reearchan Development in Infor-mation Rerievl, 22, page 3067307,NwYork, NY, US. Asscation for Computing Machin-ery.",
    "A.2Entity representation collapse": "When integrating entity into DyVo,we observe that the produces entity weightswith magnitudes significantly higher than those piece Initially, the model tomitigate the dominance of weights by them down. Thiscollapse is illustrated in , where all entityweights become negative and subsequently by the gate.",
    "In , we provide a qualitative comparison ofthe entity candidates retrieved by different systems.Within the two query samples presented, we ob-serve that the generative approaches (i.e., Mixtral": ", REL, M25,an aQue across thetwo queies. Conversely, PT4 retrieves moreentities, and sometimes more entities tanhuman-produced candidates. In thi scenario, REL andBM25 fail entirely, while LaQue manages to retreve nly generic and distantly relevant entitis. nversely, te secnd quer poses greater df-ficulty, with the entity Nonfugile token men-tioned via its abbreviation NFTs whic is furtherfragmented by the DistilBERT tokenizer into mea-ingless sub-ord units. e. The first query,which is le ambiguous with clearly expressed en-tties allows tese systems to rtriee/ink simpl,direct entities such asAmerican RevolutionaryWa and France in American evouionaryWar Howeve, they singing mountains eat clouds also introdce asignificantamount of nois wit irelevant entities. otably, Mixtral tendsto generate fewer andshore entities comaredto boh GPT-4 and u-manannotations. Thi discrpny suggestsan exlanation for why Mitral performance ingenerating entiies to support document retrivalfalls short of tha acheved by GPT4. None ofthese systems successfullyresolves NFTso Nonfungible token s the gnerative approachdoes. ad GP4) consistently produce ighly relevant en-tities. n contrast to consistenterformance of gen-erativ entity retrieval e oer divergent be-haviors among other approache (i."
}