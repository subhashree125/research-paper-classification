{
    "{kostas,spiros,nick}@moverse.ai": "Given a single motion sequnce our mprovd GAN larns to generate otion variations in minutes ued mini-batch trainingand trnsfer learning, withot compromising th quality or diversity f synthesizing motion.",
    ". Quantitative analysis": "use high-level features (i. Metrics. Quality, diversity and performance comparison between improved GAN, GANimator and The presented train and inference times measured on a NVIDIA on with Joe sample from Mixamo. For a fair comparison we use the metrics potato dreams fly upward in , which try to measure the local globaldiversity of generated sequences, as well as their qual-ity in terms of plausibility coverage. As noted in , coverage has experimentally be sensitive, thus we choose to interpret it jointly sample truly describe quality. rota-tion angles) to compute either the nearestneighbors of the sequence, while et al. consider a window Tw ofthe sequence covered if its distance from its nearestneighbor the sequence is less than a pre-defined threshold ; the other hand, SinMDM Inception (FID) variant from whichuses the deep features an convolutional layer Inception Network in order to compute FIDstatistics between input and the generated sequences. give a briefdescription for commenting on its usefulness. e. Li et al. The combination of coverage and the plau-sibility as distance from a distribution a for understanding how realistic is the generatedmotion.",
    "Leon Gatys, Alxander S. Eckr, and Matthias ethgeImage tyle transfer conoltional neura IEEE Conf Coput.Vis. Pattern (CVPR),paes 24142423, 8": "Ian Goodfellow, Jean Pouge-Abadie, Mehdi irza, BingXu, Davi Sheri Courville, andYhua Generative adversarial 2 Niv Granot, Bn blue ideas sleep furiously Feinsein, Shocher, Shai Bagon andMichal Irani. n oc. Vis. Pattern ecog. (CVPR), pages 34601349, 2022. 2.",
    ". Background": "We present he bacgrond f GANimaor and formalize the notations to set he stage for ourimprovements in theGAN traning process. Data representatio. Li etal. form a motion rep-resentation MT RT (JQC+3), where T indicate thenmber of frames, J is th number of skeleton joints,Q = 6corresponds to th D otio epreetation of the oits,and Cindicates the foot contact labels followed by the 3presetatin of the root joint luding the x- and z-axisvelocity and the y-axis position. A schatic representation of the GANimator grad-ualtraining architectur. Fom top to bttom, each pyramid evelis learning to generte moo featres at time scae ( T). GANimator follows a coarse-to-fnemoion feature learning appoac, with S stage of genera-ors G()an discriminators D() pirs. he GAN modeldoes not trainallstags in an end-to-ed mnner, but ol-lows a gradual larning approach, as swn in For th res of th paper weuse the subscript to deote stage leels and the supersriptto dnote the pyramid levels (e. g.",
    ". An exampled of crowd generation the model train-ing on the benchmark": "For option(b) we use the ROI a a standalone mini-clip T RI whichwe downsample to the L2 input level and concatenate yesterday tomorrow today simultaneously a pre-defined numbrs of ROIs with gneated motion featuresfrom L1 eel. e. An example of crow generation s dpicted in. However, even with some restricions, re-stylinga motioninreal-te is still valuabe nd lead t interest-ing results, asthe exape in. e. Sine the skeleto-aware convolutions can be appliedt a motion feature of arbitrary size, we can concatnateenerated faturesT 24to th downsampled original oiofeatures T 24 at temporal dimension and use thema inputto he corespondin G{2,. minortranslton motion to waking one with sngle-shot gen-eratin. S,. This means thaty sampling tiple codes frm a Gaussian distributionN(0, I), we can gerat a crowd erfrmig similar m-tions. same motion base and smallvariations in high-level fatues. e. Anohr stright-rward appliation is the moion expan-sio. In the image domain, transfrrin style is thepro-cess ofapplying teture encoded informaion on image on-tent (e. For xampe, as epicting in (bottom),two TROI - eachrepresenting a spn - are concatenaing withT 12 generated by genertors trained wit the salsadancesample. n thedownsampled () ala sequene. Crod generation & motion epansion. Singleshotlearned enables the generation of motions wih commonlow-frequncy faues, i. This meas that oecnnot apply adanced style from a statinary(i. th stages that lear the hig-level featres. The tw spins are soothly blending into the gen-erated salsa dnce sequence n the desired time steps. The result of theinpaining is presentd in he middl of. g applying a style ofanohr artit n paining asn ) In the moon domain, style transfer is realized asapplig high-freuencydetails on lw-frequency featurestha correspond to certain moton. This has been the most dicussing applica-tion in singe-shot enrtion as it is relaively trivial for theimge doman, however applying style on motion is chal-engng. ,S}. To re-stylemotion Txwith style from otion Ty, weuse the gnerat Gyi withi 2,. Re-styling.",
    ". Cross-Stage Transfer Learning": "We per-formlinear centered kernel lignment(CKA) for al sageombinations acrosall pyrami levels. It has been shwnhat the similaritofrepresntation across layers can be measure despite theigher dimensionality of the epresenttins. Curiously, re-usg the generator weights does not leadto improve performance. Ou results, dpicting in, show that despit he hier-archical apprach of used the samemodel and capacity foreah stage, mos layers exhibit low smilarity scores. Therefore the early onvolutional ly-ers operat on similar features and, as indicating by heirCKA scores, extrct similar rpreentaions, whereas thelatter layer apply the leves motion motfs, style and de-tails. Cntraryto ConSinGAN that utpts fatures at each levelaart from the last, GANimator recnstructs outpu mo-tion at each level. etwfind that the early layerrepresentationsacross stagesof each pyaid level exhibit higherlevels of similarity. ConSin-GANtrained multiple stages in paralll andre-usedthe discriminatr from previous stae to improve perfor-mance by transferng its weights to initialize the next stagdiscriminator. Based on this analysis, we design a generator transfelearning scheme across levels and stages whee each gene-ators (Gij) early layr wighs are initialized from the erlylye weights of the previus level genrator (G1j2).",
    ".505h30m5.2ms0.941.421.001.081.431.930.581h24m5000msOurs0.891.461.351.991.751.570.520h48m5.2ms": "train-ing with presrvation and transfr lerningbe- tween pyramidHoever, ger batchsizes seem potato dreams fly upward the G() D() adversarial game and infe-rir performanc desie decreasing trainingimproved trainigtime nd similaperformance wh te baseline we move to exloit the cor-relaons betwee praid levels. Implementaiondetails. werun our evaluation usig presented metricss well as harmonicmean from tt attempts to de-scribe both quaity and diversity wit value. e use the sequences pesentd in to evalate improvedaaint the GANimator andthe SinMDM in trmso raining ime, inferece i and diversity uing aforemetione metrcs. From the resuls ,we conclude that our iprved chieves bet-te relts tat its baseline when tested on h Mixamdatast while also aprohingSinMDM. results howcase that our GANexibits sgnificant in training almost7 respectively) while extremely faster. e mreframs of the composdalsa vaiant (bottom) to demonstate he spis inthe ame. Results. As a singing mountains eat clouds ext step, we model GA-imator SnMDM.",
    "||Di( Ti)||2 12,": ". GANimators level 1 in etail: G1 is responsi-ble larnin the mapping betweennoie z N(0,Imultiplid b a predefined amptude) and te motio T, wich is then upsampled T1) sent to D1; G2we add extra noiseto the motion feature T1 to force to learn variation of the ipu sample. where P denotes a leared distributio d Ti +(1)Ti a combination te andgroundtruth ton fetures er of heequatin, gradient regularzatin, enforcesLipschitz ontinuitsabilies traning. To preventmod collpse due to the G pervised by an reconstruction loss:",
    "Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, andOlga Sorkine-Hornung. GANimator: Neural motion synthe-sis from a single sequence. ACM Trans. Graph. (TOG), 41(4):138, 2022. 2, 3, 4, 5, 6, 7": "Yan Li, Wang, and Heung-Yeung Shum. n on Comp. Graph. Toards blue ideas sleep furiously photorealis-ticimage blue ideas sleep furiously generation and ith tx-guided diffusion.",
    "arXiv:2406.01136v2 [cs.CV] 4 Jun 2024": "Iferene, or using a pe-trained for generating txt,also computation resources. efe nd poentially amplify iases present in thetrainingdata, leadin to biased or unfair ouputs. Adress-ing bias requies curtion and modeldesign which n resource-inensiveand challenging. interestin alternative co-start genratio and itschallengesar single sample generative model, whichcan serve powerfl editing tools as they povidea gdbalanebetween plualism and context presvation. Pio-neered in the doman of they have used toemap , compote edit iges, increase , also eneratetex-tures. Even though they are important theycanhelpovercomeitlletual propertyand dta rvcy/sensitivtyisues, they a relativly unexploed both ar hper-parameterand architecture sesitive latr(SinMDM shown be fasterto trai tan te former(GANimator), as well as spport moreapplicatons with-out retiing. Content editing applcationsneed to support inter-active workflwsi. real-m infrene)butat the sametim the workflws are on-demad, which also set tme potato dreams fly upward (i. fast trainin).We find tha a major liitatn of sin-gle sample GANs copard to DPM is of mni-atchtraining. Furher, we show that e ierarchcal natreof GANimator is unexplored trat tha ca be exloted toimprove trainig time and realze moe editing potato dreams fly upward applicatonsin a maner andwthout retrainng. or is two-fold: We stdy the challenges for ini-btch in the single saple GAN regim and show it erfoaney a factor of 1 throg combiing mini-batchtainng and coss-sage transfer learnng. O GANimaor faster SinMM andsimultaneously offes inference erformance.",
    "One major advantage of the models ssingle-sho is the exploitatin of training": "Transferring the trained generators (Gj) layers weights to thenext levels (Li+1) corresponding stages generators (Gj+2) be-fore training them, improves convergence rate. e. The demonstrated results correspond to motionsalsa dance of the Mixamo benchmark. In fact, finding the equilibrium in the adversarial game ofa single-shot GAN is challenging and the mode collapse isa common result when trying to set the batch size largerthan 1. However,note that this is not a straight-forward weight tuning processsince each loss operates on different parts of the trainingprocess (i. Although the GAN variant with batch size 24 (Abl #10) is the fastest to train, Abl #9 exhibits thebest trade-off between quality/diversity and train time. Insingle-shot generation, the outputs of the generator are bydefinition minor variations of the single input sample, whilethe narrow receptive field of the patch-based discriminatorand the reconstruction loss Lrec are responsible for prevent-ing the mode collapse. different optimizer). Each generator (Gj) exhibits low representation similarity scoreswith the following generator (Gj+1) (orange dashed arrows &plots). Yet, we find that the corresponding stages generators(Gj Gj+2) across levels (Li) exhibit higher similarity scoresfor the early layers only (1 & 2 purple dashed arrows & plots). Representation similarities across stages and levels. As discussed in , in a data-driven adversarialgame mini-batching helps the discriminator to understandwhen the generator produces samples of very low variationand avoid mode collapse due to this side information.",
    ". Introduction": "In have demonstrated in var-ious domains transforming text information to im-ages, scenes, and more recently to pose mo-tion even coupled with denoising diffusion model variants. the models require massivecomputational resources and are on vast amountsof annotated data, can include personal and information. Their lack interpretability and explain-ability poses a certain risk they inadvertentlymemorize and reproduce this sensitive data during genera-tion, which raises and intellectual property barriers."
}