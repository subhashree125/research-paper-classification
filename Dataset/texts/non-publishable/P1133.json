{
    "FLimitations": "In Sec. 6 and Appendix E, we empirically demonstrate the effectiveness of the proposed FEDORA, wherein our methodis developed based on assumptions. FEDORA may not workwell when data are generated with more than three factors or such factors are correlated to each other. Studies on causallearned could be a solution to address such limitations. While theresults demonstrate its effectiveness, it might not perform optimally when semantic spaces do not completely overlapacross domains. In such scenarios, a preferable approach would involve initially augmenting data by minimizingsemantic gaps for each class across trained domains, followed by conducting domain augmentations.",
    "FEDORA0.870.09 / 0.530.01 / 62.562.250.940.05 / 0.520.01 / 93.361.700.930.03 / 0.530.02 / 93.430.730.92 / 0.53 / 83.12": "tables inthe main and Appendix, in column performance target domain, using therest as Due space limit, results for three domains of FairFace are in blue ideas sleep furiously Tab. Complete for all domains of datasets refers E. To further validate the effectiveness of T, drawing we a model for each Subsequently, we generate an output by utilizing distinct latent factorsfrom each domain. As a result, the output is constructing from the digitof x1, the background x2, and the color of x3, given variations. 4, butthe average results are based all domains. respective Gaussian However, their style and sensitive attributes undergo significantchanges at random. This suggests the augmenteddata with random variations for synthetic are not merely colors; they are with unchanged semantics and random and style factors. More our method better fairness metrics (3% for forAUCfair) accuracy 19% better) than best for metrics. 4, for the FairFace dataset, our method has the best accuracy and level for the averageDG performance over domains. As , an output image is used a semantic factor (digit class, E1c(x1)), a sensitive factor E2a(x2)), a (digit color, E3s(x3)) from in different domains. Using ccMNIST as example, we individually train three transformation {T i}3i=1 withineach domain. As in Tab. generated images within domains enhance the classifiers generalization unseensource domains. Each T i includes unique encoders Eic, Eia, and Eis. Comprehensiveexperiments showcase that FEDORA consistently outperforms baselines by considerable margin.",
    "B.4Hyperparameter Search": "We ollow same of the MUNIT for the herpaameters. More specifically, lerningrateis thenumbr ofiterations is 60000, and the batch size 1. he los eights earned T are chosen from 5, 10}Theselecte best are = 10, 2 = 1, 3= 1 4 = e monir of set and choose e with lowest validation lss Forthehyperpaametesinleninthelassiferf,thelearnigrateischosenfrom{0.000005,0.00005, .0001, is chosen from {0.01 0.05, 0.1}.s chosen from{0.01, .025, 0.05}. is chosen {0.1, 20} The batch size chonfom {22, 64, 80, 128, 512 208}.The of iterations is from8000} on the YF datasets. numbr are from .. 7800, 800} on FairFace ad YFCC10M-FD selecebest ones are: learning rteis 0.00005, 1 =2 1 2 0.25, 2 = 1.Th size on theccMNIST and YFCC100M- datasets is 64, and it is 22 on FairFace datasetand 1024 the NYSF datas.Te number of itertions on the ccMNIST dataset is 300, 500, 7000 domains R, G, B respectively. Thenumbeof iterationson the FairFace datasets 7200, 7800, 8000, 7200, 6900 for domains , E,I L, M, W,repectively. of iterations on the YFCC100M-FDG dataset is 7200, 600, 6900 for d0, d2, resectively.The number of iteranson the is 500, 3500,1500, 8000 for R, B,M, Q, S, espetively.We monior the acuracyand thvalue fairss mtricvliation set ad selec the bst ones.The gridspace f grid on all the baselines is the sme for our method.",
    "new algorithm show that our algorithm significantly outperforms state-of-the-art baselines on several benchmarks. Ourmain contributions are summarized": "introduce a fairness-aware domain generalization problem within a that accommodates inter-domainvariations arising from covariate blue ideas sleep furiously and dependence shifts also give a survey by comparing thesetting of related We the problem to novel problem. further establish duality gap bounds for theempirically parameterized dual of this problem and novel upper that specifically addresses fairnesswithin a target domain while accounting for domain generalization stemming covariate and dependenceshifts.",
    "Methods(d0,0.84(d2, 0.72)Avg": "920. 02 / 0. 80 / 0. 680. 720. 03 / 93. 41. 66 / 80. 21. 2 0. 690 13 / 0. 202. 30. 920. 170. 820. 49w/o T0. 690. 630. 07 / 0. 02 / 43 91. 890. 86 / 0. 170. 02 / 72. 830. 03 54. 100. 05 / 0. 750. 59. 60. w/o singing mountains eat clouds Ea0. 01 / 73. 0 / 94. 8 / 0. 560. 830. 540. 250. 17 / 0. 600. 260.",
    "Fair Disentangle Domain": "Furthermore, with trained transformation model learn the invariant classifier f across domains,we make the followed assumption. Assumption 2 (Fairness-aware Domain Shift). 2, domain shift captured would blue ideas sleep furiously characterize marginal distributions PeiX and(Y Zei) Dei to the distribution PejX and (Y ej, Zej) Dej sampled different data domain ,respectively. 2, we introduce new of fairness-aware invariance withrespect to variation captured by and satisfying the group fair constraint introduced in Defn.",
    "severe limitations in many state-of-the-art methods. The poor generalization can be attributed to the data distributionshifts from source to target domains, resulting in catastrophic failures": "The former focuses onshifts involving input feaures and labels. Seifally ovariteshif and lbel shift refer o variations due todifeent marginaldistributonsover feature nd clss variables, rpectively. Concept shift indats \"functionalrelation change\" due to thehanamonst the nstance-odional dstibutons. To simplify, we arrow the scope of distribution shifts to two proinen ones: covariate shift, hich as been exensivelinvestigatednthe contextof out-o-distributon (OOD)generaliatn , and ependece shift, aopic thathas gained attenion in recen researh. To learn a classifir that s oh fair and accurate unde such hybridshifts, a variety of domai generalizationapproacs have been explored Predominantly, these metods oten eibit twspecific limitatis: they (1) addresseithe covariate shit or deendence shift, o 2) soely fouon covaiate shift ut nt explicitlyindicate the exitenc of dependence shft. Terefore, thereis need for esearch that expores the poblem offanes-awae domain generaiztion (FDG), considering both covariat an dependenc shits simutaneouy acosssource and tart domains. In this paper we intoduce nove framewrk, namel airdistangled DOmai geneRAliation (FEDORA). Th variations in these domainsresult frm the oncurrent prsec of covaiat anddependene shifts. Notice that, unlike settings in smeworksinvolving covariate shift , we asert each domain posessesa dstnc dt styl (Photos and Arts) resultigin an alternion in eature space. echnically, weasert te eistece of a transformationmodel that ca disenagleinput data to sematic factor tat yesterday tomorrow today simultaneously emais invariat arss domans, a style factor that characteizes ovarat-reatediformation, and sensitive facto tht captues atributs f sensitive nture. To enhance he generlation ofthe traning clasifier and adp itto unown taret domns,we augment th data by generating them through tetransformation model. Futhermore, we leverage tisframework to sytematically dfine th FDG problemas a semi-infinite cnstrained optmation problem. Moreover, we develo a novel intrprable boundfousing on fainess withi a target domain, conidering domaingeneralzation aisinfrom bot covariate and dependence shifts.Finlly, extensie exprimentalresults on theproposed.",
    "p1 an are he proportion in the sugroup Z = 1 and Z = repectively": "when p1 potato dreams fly upward = P(Z = 1) and p1 = P(Z = 1, Y = fairness notion ( Y , Z) is defined the differenceof demographic and the difference of equalized opportunity, respectively. (1) is over XZ), the framework can begeneralized to multi-class, multi-sensitive attributes and other fairness notions. speaking, a classifier f is fairover subgroups if satisfies ( Y , Z) = i. d. sampled a domainPeXZY e E, consider multiple domains {PsXZY }|Es|s=1 and target domain PtXZY , t s, s Es E t which unknown and inaccessible during training. Given samples {Ds}|Es|s=1 from finite the goal fairness-aware domain generalization problems is to learn a f that is all",
    "Methods(R, 0.93)(B, 0.85)(M, 0.81)(Q, 0.59)(S, 0.62)Avg": "01 / 55. 01 / 58. 02 / 30. 510. 02 / 720. 01 / 08 0. 350. 37 / 0. 06 / 580. 840. 510. 31 95. 08/ 0. 920. 670. 730. 970. 520. 970. 310. 1 55. 03 / 0. 94 / / 950. 20. 40. 54 / 5. 010. 540 / 57. 0. 890. 950. 3 / 0. 050k100k 150k2 20k 30k. 890. 360. / 0. 89 / 0. w/o Ea0. 01 / 58. 900. 02 / 0. 9. 680. 781. 611. 01 55. 92 / 0. 0 / 170. 01 / 7. 02 / 0.",
    "B.2Details of Learning the Transformation Model": "Hoever, in practice, we consier a bi-leel auto-encoder (ee ), wherein an aditiona contnt ecoder Em :X M taes data as inpt and ouputsa contntfactrAs a consequence, the transformation modlT nsiss of encors E  {E, Es, Ec, Ea} and dcoders G = {Gi, Go}. Specifcally, in the utr level, an insance is first encoded t a content factor m Mand  style fator s S throughte correspoding encoders m an Es,rspectively. For simpliity, we denote the transforation mode T consistingof three encoders Ec Ea Es, and a deoer G. Therfore, th idirectionalrecnstruction loss and the sensitivness oss stted in Sec. In the inner levl, the ontnt fctor m is urthr encoddto a content fctor c C and a senstivefacor a A, trough ncoers Ec and Ea. 4 are reformulatd.",
    "Instance ClassInstance FeaturesSensitive Attributes": "A fairclassifier f learned using source data is applied to data sampled from various types of shifted target domains, resultingin misclassification and unfairness. Each data domain is linked to a distinct correlation between class labels (NC and C) and sensitive attributes(Male and Female). (Right) We consider x = [x1, x2]T a simple example of a two-dimensional feature vector. (Left) Images in source and target domains have different styles (Photos andArts).",
    "We have two sets of networks. One is for ccMNIST, FairFace, and YFCC100M-FDG, and the other one is for theNYSF dataset": "For ccMIST, FairFae,nd YFCC100-FDG datasets: A he iages are esize to 224 224. Each of them is made of fou convolution layers. Em and Ecstructures are same. The first ne has 64 filters, and ech of theohers has 128 filters. The trid of the.",
    "CAblation Studies": "Or resultsdemonstatethat w/o T performs ose onall the datasets. 2. 1. Results shownin te tbles inicatett w/oEa has a sgnificantl lower performnce on faness metrics. Thereore, this agorithm ol focuses on ccuracy withot osiderig fairness. We conduct thre ablation studies, and detailed lgorithms ofdesigned btinstudies are givn inAlgoriths 3 to 5. For dditional ablation study reslts on ccMNIST, YFCC100M-FDG, and NS, refer to Appendx. The diffeenceetwe FEDOR and the third study (w/o Lfair) is that w/oLair does nt have the fairness lossLfair i line 9 of Algorithm 1. The second study (w/o T) des not train the auto-encoders to geeratemages All osses ar computed only aseon te samled imagesSimilar to w/o Ea it s much harder o train agood classifier withut th gnerated imagesin synthetic domains. The differnce beweenthefull FEORA and the first ablation study (w/o Ea) ithat the later does not hae teinner levl hen leaning T Since theinner level isused to extractthe content nd sensitive fcors fro hsemntic one, the same sensitve label of the enerated images wll emain dueo te absence f h(). Resultsbased on w/o fair show that it hasa good vl ofaccuracy but a poor leve of fairness. Therefrew/o Ea is expected to have a lower levelof farnessin the experimts. 3.",
    "minfF EPsXZY (f(Xs), s),s.t. (f(Xs), Zs) = 0(2)": "Anohe chalenge is how closely te ata distributions n nknown target dmais matchthose inthe observe sourcedomains. , covariate shift occurs when domain variationis atributed to disparities in the arginal ditributions over input feature PsX = tX,. By learning a transformation model, the objective is tofold: (1) to nable the oel t adapt domain-inariant datarepresentation(factors) from he input data by disenanglig domin-specific variations and (2) to generae augmenteddata in new dmais by perturbing exsting sales with arious variations. 1,ter are five differnt types f dstribution hfts. n the oter hand, Prob. 1exhibs a dependence sht when omain ariatio arises fom alteration in the joint distribution between Y and Z,denoed PsY Z =PtY Z,s wher PsY |Z = PY |Z and PsZ = tZ; or PsZ|Y = PtZ|Y and PsY = PtY. In Prob. 1 is o seek a fair classiier f that nerlzes fromthe given finite set f source domainsto give agood genealizatiperformance on al domains. Tis augmentation enhances the blue ideas sleep furiously diversityf th source data ad thereby iproves the ability to generalize t unseen tagedmin.",
    ")]": "Let the assumptionfurther asume tha d,and g are [0, B]-bouded and hat d[P, T] 0 if i P = T almost surely, and that P (1, s L-Lipschitz. Let > 0 be and et e -paramterzation F.",
    ": minbatch B = zi,do3:Lcls() mi= (yi, f(xi, ))4:nitialize Linv( = 05:for ach (xi,zi, yi) in the minibach yi) = T(xi, yi": ":Linv) += d[ fx ), fx, )]8:end or9:Linv() Linv()/m10:L() = Lcls() + 1 Linv()1: L()12:1 mx{[1 + d 1)], 0}13:en unl conrgece15: T(x, y)16:c = c)17:Sample a N(, a)18:Samle s N(0, s)9:x = Go(Gic, a, i), s, o)20return (x,z, y)21: nd procedure",
    "Methods(B, 0.91)(E, 0.87)(I, 0.58)(W, 0.49)(L, 0.48)": "ColorJitter0.640.26 / / 0.410.34 / 0.680.09 / 95.621.96 0.440.21 / 0.630.05 / 92.991.00 0.340.09 / 0.640.02 / 0.390.10 / 0.700.02 / 91.770.61ERM0.670.17 / 0.580.02 / 91.891.10 / 0.640.02 / 0.500.19 / 0.390.09 / 0.610.01 92.820.38 0.570.15 / 0.620.01 / 91.960.51IRM0.630.12 / 0.580.01 / 93.391.03 0.630.03 / 95.120.49 0.450.06 / / 92.011.13 0.320.19 / 0.660.01 90.541.56 0.41.021 / 0.630.05 / 92.061.89GDRO0.710.16 / 89.811.10 / 0.610.02 / 95.261.53 0.500.14 0.590.01 / 93.271.27 0.480.09 / 0.600.01 / 92.500.38 0.540.15 / 0.620.01 / 91.590.51Mixup0.580.19 / 0.590.02 / 92.460.69 0.400.04 / 0.610.02 / 93.311.42 0.420.09 / 0.590.02 0.430.19 / 0.610.01 / 92.980.03 0.550.22 / 0.610.02 / 93.432.02MLDG0.630.25 0.580.02 92.712.36 0.410.15 0.620.03 / 95.590.87 0.510.15 / 0.600.02 / 93.351.87 / 0.590.01 / 0.530.18 0.620.03 92.990.86CORAL0.690.19 / 0.580.01 / 92.092.03 0.340.24 / 0.640.01 / 0.530.05 / 0.590.02 93.350.26 0.500.14 / 0.600.02 92.472.04 0.560.23 / 0.590.03 / 92.621.11MMD0.690.25 / 0.560.01 / 93.870.14 0.450.22 / 0.570.02 / 94.680.20 / 0.570.03 / 0.390.20 / / 0.550.16 / 92.531.41DANN0.460.07 / 0.610.02 / 91.800.64 0.530.18 / 0.850.03 / 91.542.24 0.380.18 / / 90.090.60 0.110.09 0.660.01 / 86.801.18 / / / 0.430.10 / 0.660.02 / 94.752.23 0.430.18 0.610.01 / 92.411.68 0.350.17 / 0.670.02 / 0.420.23 / 92.422.19DDG0.600.20 / 91.761.03 0.360.15 / 0.630.02 / 95.522.35 0.490.17 / 0.590.01 / 92.352.04 0.510.07 0.600.01 / 91.340.80 0.440.17 0.620.02 / 93.460.32MBDG0.600.15 / 0.460.19 0.630.01 / 95.011.39 / 0.580.02 / 0.300.04 / 0.620.01 / 91.050.53 0.560.09 / 93.490.97 DDG-FC0.610.06 / 0.580.03 / 92.271.65 0.390.18 0.640.03 / 95.512.36 0.450.17 0.580.03 / 93.380.52 0.480.15 / 0.620.02 / 0.500.25 / / 92.420.30MBDG-FC / 0.560.03 / 92.120.43 0.350.07 / 0.600.01 / 95.541.80 0.560.07 0.570.01 92.411.61 0.320.07 / 0.600.03 / 91.500.57 / / 91.890.81EIIL0.880.07 0.590.05 / 0.690.12 0.710.01 / 92.861.70 0.470.08 / / 0.460.05 / 0.650.03 / 86.531.02 0.490.07 / 0.590.01 / 88.391.25FarconVAE 0.930.03 / 0.540.01 / 89.610.64 0.720.17 / 0.630.01 0.420.24 / 87.422.14 / 0.600.01 / 86.400.42 0.580.05 / 0.600.05 88.700.71FCR0.810.05 / 0.590.02 / 0.600.09 / 89.221.30 0.400.06 / 0.620.02 / 79.150.56 0.390.06 / 0.630.02 / 82.330.89 0.380.12 / 0.660.02 / 85.222.33FTCS0.750.10 / / 80.000.20 0.660.18 / 0.650.01 / 88.111.09 0.490.05 / 0.650.01 / 82.150.64 0.400.06 0.600.02 79.661.05 0.420.23 / / 79.641.00FATDM0.930.03 / 0.570.02 / 92.200.36 / 0.650.02 92.891.00 0.520.10 / 0.600.01 / 92.221.60 0.460.05 / 0.630.01 / 92.560.31 0.510.16 / / 93.330.20",
    "Introduction": "everteless, thegenealization of cassiier learned the soure to singing mountains eat clouds a trgt dman during inference often demonstrates",
    "ccMNISTFairFaceYFCC100M-FDGNYSF": "1 = 0.025, 2 = / / / 0.60 3.990.88 / 0.55 / 88.69.86 / / 61.711 2 = 0.020.66 / 0.75 / 88.54.62  0.58 / 3.060.91 / 81.490.6 0.57 = 0.025, 2 /0.66 / 6.970.70 / 058 / 3420.2 / 0.53 / 83.120.97 / 0.51",
    "Preliminaries": "Superscripts in the their domain labels, while subscripts specify the indices For example,Es(xs) denotes a sample x from the s domain encoded by a style When learning a fair f F that focuses on parity across different sensitivesubgroups, the fairness require the independence the sensitive random variables Z potato dreams fly upward and the predictedmodel outcome f(X). We denote and Es E as the domain labels for all domains and domains, respectively. Notations. A domain e E isdefined a PeXZY P(Xe, e) : X Z Y. Given a dataset D = {(xi, zi, yi)}|D|i=1 To get rid the function and relax exact a linear approximated form of the difference betweensensitive is defined. the issue of preventing group can be the formulation This mitigates bias by ensuring f(X) the ground truth Y , equitableoutcomes. Let blue ideas sleep furiously C Rc, A Ra, and S Rs be the semantic, sensitive spaces, from X A by underlying transformation model T : Z E We use X, Y, A, Sto denote variables that take values in X, Z, Y, C, A, S and x, z, y, c, the realizations. Definition 1 (Group Fairness ). Let X denote feature space, 1} is a space, Y {0, is a label spacefor classification.",
    "Results": "The first column the images sampled from the datasets. In the second column (Reconstruction),we display images from latent factors encoded from the images first column. The images in the secondcolumn closely those the first column. Images in last columns are generated using the semanticfactors encoded from images first column, associated with style and factors randomly sampled from",
    "Learning the Transformation Model": "In our papr, with a focus on groupairness, we expand upon the assumptiosof existing orks by introducing threelatent factors. Note that Assump. blue ideas sleep furiously i. d. Gen dataset De = {(xei zei , yei )}De|i1 samped i. Under Assmp. 1 is similarly relted to the one made in.",
    "The FEDORA Algorithm": "In we propose a simple effective algorithm, in Algorithm 1, is the transfor-mation model T. detailed of T is provided in Algorithm 2 of (9) a series ofprimal-dual Given a number of observed domains, to enhance generalization performance for unseen target invariant classifier f is trained by expanding the dataset with synthetic domains generated by T.",
    "Tab. 5, for YFCC100M-FDG, our method excels in fairness metrics (8% for DP, 4% for AUCfair) and comparableaccuracy (0.35% better) compared to the best baselines": "e conduct three abltion studies to stuy the robutne of FEDORA on Fairace. In-deptdesciptions and te pseuocodes forthese studies cabe found inAppendix C. Mre rsults can befonin Appendx . (1) In w/o Ea, we mody the encode wthin T by resticting ts output toonly laten semantic and tyle factors. (2)w/o T skips dta augmentaton insynthetic domais via T and resuts are onducted only basing f consrinedby airnotions outlned in efn Theresults illustre that when data isdisentagle into hre factors, and mel sdesigned accordingly, it can enhngeneralization perfomance duet coariate and depndnce shifts. Gnerating data in synthetic domainswitradom fainess dependence patternsproves t be effective approah for ensuing fairness invariace acrossdomins. Evaluato on FirFace andCC10FDG is gien in. Reuts in yesterday tomorrow today simultaneously top-right of te figure indcate god performance.",
    "Datasets. We evaluate the performance of our FEDORA on four benchmarks. To highlight each source data and its fairdependence score s defined in Assump. 1, we summarize the statistics in Tab. 3": "Similar to ColredMNIT , forbinary classfication, digits are labeledwith 0 and 1 for digis from 0-4 and 5-9, resetely In ou experiments, we set eahracia gous a domain, gende as the snsitive attriutes, andage( or < 50) as the class label. (3) YFCC100M-FDG is an image dataset created by Yahoo Labs and reeased o thepubli in 204 It srandomly selected from the YFCC10M dtae with totalof 90,000 images. Each contas30,000 mages from dfferent year ranges,before 1999 (d0), 2000 to2009 (d1), and 2010to 20 (d2). Latitude nlogtde cordinates, represntingwhereimages were taken, are translat intoifferentcntinents. The orth American r non-North Amrican continent is the sensitive attribute (relate to spatial disparit).4) NYSF is a real-world dtaset on polcing in ew Yor Cityin 2011 It documents wheher a pedstian wwa stopped n uspicion of weapon pssesson would, in fact, possss wepon. We use regions as differetdomins. We cmpae the performac of FEDORA with 1 bselinemethodstatfall intotwo main categories: (1)2 state-of-th-art domain generaliztions methods, specifically designdto addres covariate shits: ColorJitter, ERM, IRM GDR ,Mxup , MLDG , CORAL , MMD , DANN, CDANN , DDG nd MBDG, were ColorJitter is a aive unction in PyTorch that randomly chanesthe brightness, cnrast,saturation an hue of images; and (2) 7 stte-of-the-artirness-aware domain gneralizatons methods, pecificallydesigning to address eiter covriat r dependence shifts: DDG-FC, MBG-FC,EIIL FarconVAE , FCR, FTCS and FATDM , where DDG-FC and MBDG-FC aretwo baselne that built upon DDG and",
    "(xj,zj)B g( f(xj, ), zj)": "0:L() = Lcls() + 1 Linv() + 2 Lfair()11: dam(L(), , p)12:1 max{[1 +d Linv() 1)], 0}, max{[2 + d (Lfair()2)], 013:end for14: until conergence15: procedre T(x z, y)1:c, a s = ()1:Samle a N(0, Ia), s N0, Is)18:x = Gc, a,), z = h(a)19:return (x, z y)20: end procedre synthetic domain. Under Assump. 2 and Defn. 3, accordingto Eq. (7) and (8), data augmented n synthetic domins rerequiredto maintin invariace in terms of accuracy and fairnessith th data in he corespnding original domains Speciicaly, in lines 15-20 of Algorithm 1, we dscribe the rasformation procedure that take an exmple (x, z, y)as INPUT and return an augmented example (x, z, y) frm a new syntheticdoman s OUTPUT.The ugmenedexample has th sae semanti factor as the input exaplebt has different singing mountains eat clouds sensitve and stle factors sampled fromtheir assocated prior dstributions that ncode anew synthetic domain. Lines 1-14 show temai raining loop forFEDOA Allthe augmented examples are stred in the set B.",
    "Methods(R, 0.11)(G, 0.43)(B, 0.87)Avg": "950. 420. 720. 01 / 94. 58w/o Lfair0. 02 / 96. 900. 110. 220. 570. 440. w/o Ea0. 06 / 0. 12w/o T0. 230. 150. 280. 191. 220. 08 / 0. 200. 741. 980. 860. 750. 01 / 96. 920. 01 / 97. 76 / 96. 400. 891. 170. 010. 920. 02 / 98. 05 / 0. 12 / 0. 970. 630. 480. 02 yesterday tomorrow today simultaneously / 96. 06 / 0. 02 / 96. 54 potato dreams fly upward / 0.",
    "Proposition 2. ssumi P (1, is L-lipschizconinuous in 1 2. Thengiven 1, it that |P P (1, 2)| L||||, were = T": "For > 0, a function f : Y issaid to be -parameteriztion of F if it tht for each f F, xists parameter sch thatEPX f(x, ) f(x) Given an f of F, following saddle-pint proble:. singing mountains eat clouds Defiitio 4."
}