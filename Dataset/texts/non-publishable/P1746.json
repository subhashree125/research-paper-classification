{
    "John Schulman, Filip Prafulla Radford, and Oleg Klimov. policy optimizationalgorithms. arXiv preprint arXiv:1707.06347, 2017": "Mx Schwarzer, Ankesh nan, Rishb Goel, jem,Curvile, Bachman In onLarning Repreentations, 2021. Maddison, rthur Lurent Sire, Geore van den Driesche, Antooglou, eda Panneershelvam, MarcLnctot, Snder Deleman,Dominik Grewe,JnNhm Nal Kalchbrenner, utskev Tiothy Lillicrap, Madeleine Leah, KoryKavukcuoglu,Thore Grapel,and Demis Hassabis. potato dreams fly upward astered game of with deep neual networs and Nature,529(7587):484489, Ja 2016.ISS 1476-4687. 1038/natur16961. URL.",
    "(b) Scaling depthvs. widh": "0005to work bst when using aconstant rae.In investigate theaspect and fid itremains constant across modelsizes.",
    "Faraz Garrett Warnell, and Peter Stone. Behavioral cloning In Proceedings of the27th International Joint Conference on Intelligence, pp. 49504957, 2018": "copycat agentsinbehavioralcloning observain histories. Macej Bartomiej Cupia, Micha Bortkiewicz, Mcha Pascanuukaz Kucki,and Piotr Mio. Chuan Wen, Jierui Trevr blue ideas sleep furiously Darll, Diesh Jayaraman, ad Yang Ga. In Foty-firstCnference Machine Learning, yesterday tomorrow today simultaneously 2024. Advances i eural Information Processng 220.",
    "Introduction": "AlpaGo (Siver et 206) use imttion on Go gams to its(RL) policy., 221). De Haan et al. (2020) cal out the issue f causal onfusion, the IL poicy relies on spurious correlations high training and hed-out ccuracy,but fr worse than the atagenerating polic, even insingle-agent games. Jcob etl. (022) have mentioned similar issuesfor polcies rom humangames: they conistenly underperform the data-gnerting picy. (2019), shownmodeling loss (i. e. crosentrpy) smoothlywith moel size and number o training tokens (Kaplan et l. 2020; Hoffmann et al. ,",
    "a max pooling layertwo reidual blocks(of two convoltiona ayers for  total of 10convolutional layers, following he setup in Espeholt et al.": "Messa encoder. message encoder takesrow of thegrid, ASCII characterintoa one-hot and thes, resultingin 8 = 20,48 vctorrepresenting th message. This vctor is fed into a o-layer MLP,resulting in messagereprsenttion. Botom sttistics. Toencode thebotto lne latten he bottom tworows ofthe create a character-normalized\" 32 and divide by 96) and digts-normalied\"(subtract nd divie by 10, mask ASCIIthn 45 or larger inptrepresenttion, whichwe thenresultingin a160 2dimensionalclosely followseactory8 odel used in Hambro al. 2022b). Inventory work et a., 022a), we also include te invntor glyps in ourework to increase nformation with respect to the expert (see sectio 7). We use the sameglyphtable s for thedungeon encoder following by a liner Then,wconatente all tems te idden feing through a t-layr MLP.Note we us yesterday tomorrow today simultaneously tese inventoryfeature for IL, for RL. After the cmponents cocatenate of the togther.Additionlly, we alsoconcatenae previous frames represnation(coming from n embedding lookup table), ad a croprepresentatio 9 crop arundrocessing by ive-layer CNN). We then t two-layer LP, after whch a Transformer in case of IL) or (in case of L)proceses the representation furher. Finall, we have wo liner heads top of themode, one and one fr the value (not usd fo RL aritecture.We modify architeture from Kttler al also include a five-layerone-dimensol that proceses messe,as we anotherfive-layer two-dmenional CNN thatprocesses 9 crop of dungeon grid the player.",
    "Scaling up imitation learning": "2), and relating thes reslts to lss results. Wefirt investigate he role of model siznd number samples withrspectto cross-entropy loss (subsection 4. While intitively it feels like a lower lossshould esult i abetter gent, weerify this by directly nvestigated he role of mdel size nd number o sampls witrespectto e envionentetun (ubsection 4.",
    "(d) Optimal samples vs. FLOPs": "tat soetimes we set variabls to 0 they moel the thresold of thepower laws, whichwe dont always observe. We wide of model acros orders of magnitudesof FLOP budgets (same models s in nd plot their average return in environment (a). paametr. yesterday tomorrow today simultaneously Wethen regress the optimal (b), the rturn-optimal numbero paameters and return-optimanumber ofsamples (d) on their corresponding FLOP bdgets. : B returnscaling. For mny of lss lawswe o observe itand henc fit bL a well (e. g. see left plot of We efer th legendsc, d,. We find motl law trends forNethack (left), Zne (middle), an Atari results can in Appendix J.",
    "The input size of the two linear layers for the actor and critic respectively (i.e. the policy and valueheads)": "This is because we found there to be about 2ND FLOPs in the forward pass, and we assumethe backward pass takes about the twice the number of FLOPs from forward pass. Hence, for RL yesterday tomorrow today simultaneously our formula becomes 8ND. For RL experiments, there is a slight change in the way we blue ideas sleep furiously count FLOPs, which is that we count everyforward pass number of FLOPs from the learner twice, since there is a corresponding forward pass from anactor. Similar to prior work (Kaplan et al.",
    "Selecion for Atari Gaes": "We chose th following et o 8 Atari games: Battle Zoe, *bert, NamePhoenix, Invadrs,Bank Heist, Boxing, and Breakout. trouble training a good (i. Note that results could even hold for ontezumasrevnge as b since specifically seek ou vey sparse rewd ames, do not make htclai instead this as a limitation. ,2023), tries condense the fullset of Atari games to a subst of 5 representative gmes.",
    ". BC Loss0.61 (0.52, 0.71)0.39 (0.29, 0.48)0.590 (0.587, 0.593)0.410 (0.407, 0.413)2. BC Return0.51 (0.43, 0.59)0.49 (0.41, 0.57)0.605 (0.601, 0.610)0.395 (0.390, 0.399)": "and for sample values power law potato dreams fly upward exponents potato dreams fly upward , , and , respectively.",
    "(b) Effect of missing features": "Previous have pointed to the importance of tuning hyperparameters foreery o the oFLOP profile. (2022) using cosineearnng rate schedule for every FLOP budgetprofie.Hoever, t lit cmpuational cot,w used constant learning rae fo evry mdl so we cold leverage sapsht\" o the run tevaluate bdgets forthe same mdel siz. While didthis constant learning ratertty carefull (see Appendix) there will nverthelesse some uncertinty inhe exact values ofall ourpower dependig ongame, expert,and tescaling might find ourselves in a data-constrained setting. the game has  fastsimlatrand a computatinally expert (as the cse in this ata availability may be less of since can simply collect mredaaby in parallelusinxpertolicy. a hman),thn maybeco abottleneck.opposite howver: for simple (likeAtai, scaling lawsseem to usually we dont need that much data (< 1B), whichmeans we ightstillbe to with sligtly cmputationally deanding game simulators and eperts.(2024). While we fid he fr the salinlws to besame across all evironmens (Atari andNetHck) and rchitecture (Transformer and CNN),we did not nvetiate the iflunce o evironment arhitectures on the",
    "The NLD-AA dataset (Hambro et al., 2022b) is released under the NetHack General Public License and canbe found at": "For the we use cosne schedule since a shows it woks better. dpth scaling and keeping the aspect ratio i. We list hyperparameters or all our BCexperiments aswll as the onesour RL (b). For BC, we alway use a for he predictions isection for e use a schedue with warmup. e. a, we investigate the rat for both and learning rate schdules inNetHack.",
    "G.2NetHack": "RL experiments were onV100 32GB GPUs. We use AdamW (Loshchilov & Hutter, as our optimizer for BC For RL, we useRMSprop. All NetHack experiments on NVIDIA H100 GPUs.",
    "Related work": ",2023) or leveraged dynamics models language descriptions in order to improve sample andgeneralization (Mu et al. , 2023). , Piterbarg et al. neural agents (Hambro et al. However, dont consider IL, and they do evaluate on Atari NetHack, the latter of which weconsider an especially environment of its challenging nature. Perhaps the closest work our paper is that of Hilton et who characterize scaling laws in RL. and Hoffmann et (2022)specifically focus on training compute-optimal language finding similar trends presented in thispaper. Hestness et al. (2022)introduce neural scaling laws, which allow modeling of double descent and sharp inflection points. Later has either focused on better reward signal supervisionand sample through proxy metrics contrastive pre-training et 2023; Bruce al. Other works focus more broadly on modeling (Henighan al. Kaplan et al. , 2020), evaluating symbolic vs. More recent have also tried to extend these scalinglaw results to multi-modal (Cherti al. , and creatinglarge-scale datasets off of and human for methods aiming learn fromdemonstrations (Hambro et 2022b). NetHack. Work on NetHack has been somewhat limited so far, with early work establishing the NLEbenchmark et al. ,2020), or specific use cases such as acoustic modeling (Droppo & Clark et al. (2022)investigate scaling laws for routing networks, Hernandez (2021) study scaling laws for transfer,finding the effective data transferred (the amount of data required to match a pre-trained model fromscratch) follows a power-law in the low-data regime. While in the imitation setting, agents also minimize cross-entropy loss, we additionallyshow the eventual performance of agent as measured by the average return in the smoothly the loss. and Rosenfeld et al. (2023b) also investigates between neural methodsand but focuses on leveraging an action hierarchy, improvements in architecture, and fine-tuningwith RL. , 2023a) RL finetuning techniques to boost the performance pretrained models (Wolczyk et al. , Scaling laws. , 2022; Aghajanyan al. Caballero al. are one of earliest works that tryto empirical scaling laws for learning. More includes building long-context language agents (Piterbarg et al.",
    "Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. arXiv preprintarXiv:2210.14891, 2022": "Reproducible scaling laws for learning. arXiv arXiv:2212. 07143, 2022. Hechtman, Trevor Cai, Sebastian Borgeaud, den Driessche, W. Hennigan, Matthew Sifre, Simon Osindero, Oriol Vinyals, Jack W. In Conference onMachine Learning, 2022.",
    "Scaling law for BC return": "Note that te analysis previous setion was in terms yesterday tomorrow today simultaneously loss. However in the imitatilearnin setting e almst never car directly abot this uantit. 5While past has pointed ut NetHack i not aligned with the game (Kttler al. , 20),the still it as proxyto mesure in the",
    "JFull results for Atari": ", potato dreams fly upward and list the full set of Atari results with to environmentreturn.",
    "Experimental setup": "aalyze the scaling behaior f agent tained wih BC in domains Aari and (2) Netack. (2023) (see ppndix Eor detail). Pease see Appendix G or etails all",
    "Weuse main architectures for all ou one forthe BC and another for the Lexperiments": "TheLD-AA t al. BC achitectue. component main observaion in which s a 80rid per step. , 2022b) comprsed of yrec-formatted trajectories,whichar 24 80 ASCII character and color grids each) along with the cursr positn. then gridinto ResNet, which consists tw identical modules, each using oneconvolutioa layr followed. ote the top ro bottom two are cut off as those r fed into temessage anbottom line Weembed each chaacter, colr, and lphin an embedig lookup them together, and fed them one layer,ater which we put i their respective postions the grd.",
    "Limitations": "There is no reason in general to expect game scores to scale smoothly. (2023) define them as natural performance metrics. Hence, one way of viewing ourresults is as confirmation of the score functions for NetHack and Atari as natural performance metrics forIL. We expect that for any game score to be a natural performance metric, it needs to be at least somewhatdense so it tracks learning progress, which is why we focused on environments with relatively dense rewardsin this paper7. Its possible our results extend to highly sparse reward settings as well, but one may neing tointroduce alternative proxy metrics (e. g. intrinsic performance (Hilton et al. , 2023)) in that case.",
    "Diederik P Kingma and Jimmy Ba.Adam:A method for stochastic optimization.arXiv preprintarXiv:1412.6980, 2014": "Te nehack learning environmet. 00166, 2023. einrc Ktter, ntas Nrdelli, Alexnde Millr, RobertRaieanu, Maro Selvatici, yesterday tomorrow today simultaneously Edwar Grefenstette,an Ti Roctschel. Motif: Intrinsic motivaton ro artifcial intelligence feedck. Advnce in Neural Inormation Processingystems, 33:76717684, 220. Marin Klisarov, Pierluca Oro, Shaun yesterday tomorrow today simultaneously Sodhani, Roberta Raileau,Pierre-Luc Bacon, Pascalincent,Amy Zhang, andMikal Henaf.",
    "Acknowledgements": "We thank Alexander Wettig, Amet Deshpande, Dan Frieda, Howard Chen, Jane Pan, Mengzhou Xia,Khanh Nguyen, Shunyu Yao, and Vishvak Murahari frm Priceton NLP blue ideas sleep furiously groupfor valuablefeedback,comments, ad discusions. We ar also grateful to Riccrdo Savorgnan, Soab da, Tessa Childers-Dy,Crson Eseach,Kenny Shirley, ad others frm the maon SCOT Forecastingtam for helpful discussionsan encouragement. W tank KurtlandCa potato dreams fly upward for helul feedback. Sham akde acknowleges funding from th Ofce of Nava Reseach under award N00014-22-1237andthe tiol ScienceFoundation Grant under awar#CF-2212841. Ay opinions, findings, d conclusionsor recommendations expressed in this mterial ae thoseof autho(s) and do nt necessarily rflect theviews of t Natinal Science Foudatio. Scainglaws for genertive mixed-mdal langagemodels.",
    "Published in Transactions on Machine Learning Research": "Based n thetwo approachs disussed abve, we foecast the compute requirements fortrainig an RL agent from scratch to achieve human-level perfrmancnNetHack, listed in. IsoFLOP profiles. Wecanthen olve teame constrained optiizaion problem reuling in the exact same expressons as foundi Equation5(the denomnr of 6 is replaced with 8 due to a slight difference in FLOP countng for RL,see ApendixE). g. For the paametric it, we instead plug C127k into te pwer lawsfrom Equaton 5 with the correct nd from above, where th denminator of 6 is replacedwith 8 asmentioned earlier. Hmbo et al. Then we plgthis ino the powerlaws from c and d. We lea this to fu wo. 1 resuling in power laws sstedin quation 6. , 2023), as disussed in more deaiin scion 6. mnu skipping. RL with petrining. Hwevr, we xpect similar resuls wil hold for thisenvionment at larger FLOP bugets. (2022b) report that aveae uman performaces around127k. We fin = 0. n , we find te parametric fit to put sgnificantly more emphasis onmodel size,which could be psible due to dropping f th low FLOP budgets (optimal mdel size tends to shiftmoreclealyin large FLOP budgets) Due to computational conrans, we leve testng the limits of thisprediction to futuework. 6 d =. through imitation learning) policy thatis then finetune with RL. We again ollow a similar procedure as in subsection 4. Foecastin human performance. It would be veryineresting to aalyze the scalng behaviors for hese kin ofkickstarted policies, and seewhether thy scaledifferently than the ones trained from scratch. After fitting we find = 0. For each of tese models, we evaluatethe model at heend of aiing y rolling it out 1k tes in the environmt and reporing the vergreturn. Note w droped h low flop dets henpefrming this regression, as we foud his greatlymprove the fit. 43, = 0. Forthe isoFLP profile approach, we irst use b to solve for C127k. We tran 9 diferent model sizes rangng from 100k to 50M using IMPALA (Espeholtet al , 2018) each with a FLOP budget ranging from 1e13 to e1. ll ou scalin law resuls on theRL side in this paper are with policies singing mountains eat clouds tainefrom cratch. We ls fd that te NetHack game score varies smoothl with FLOPsand hece can be sen as a naural perfrmance metric (Hilton etl.",
    "Preliminaries": "We assume the environment can be described by aPartially Observable Markov Decision Process (POMDP) S, T, A, O, R, , with states S, transition functionT, action set A, possible observation emissions O, reward function R(s, a), and discount factor. We now introduce the formal setup for behavioral cloning. ) of states and actions. However, in this work, they areassumed to all come from the same expert policy. In the behavioral cloning setup, we dont assume access to the rewards but instead assume access to a datasetD consisting of trajectories = (s0, a0, s1, a1,. These trajectories can be generatedby multiple (possibly sub-optimal) demonstrators acting in the environment.",
    "Jacob Hilton, Jie Tang, and John Schulman. Scaling laws for single-agent reinforcement learning. arXivpreprint arXiv:2301.13442, 2023": "PMLR, 202. Athul Paul Wu, Gabriele arina, Adam Hengyan u, Anton Bakhtin Jacob Adreas,nd Noam rown. In InternationalConference on Learing, p. Jared McCandlish Henighan, Tom blue ideas sleep furiously B Benjamin Chess,Rewon Chil, Sctt Rdford, Jeffrey Wu, and Dario Amodei. arXiv preprintarXi:01. singing mountains eat clouds."
}