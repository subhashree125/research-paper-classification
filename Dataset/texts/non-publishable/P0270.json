{
    "Abstract": "However, our investigation shows that de-spite these gains, their potential for applica-tions is by their limited generalization capabili-ties to novel domains. can be found at. image matching a contin-uous emergence of novel learnable feature matched tech-niques, ever-improving performance on conventionalbenchmarks. In this paper, we the first learnable image that is a core principle. 9% with to a comparable reference model,while also outperforming the relatively. OmniGlues novelcomponents lead to on unseen domains of20. We perform comprehensive yesterday tomorrow today simultaneously experiments a suiteof 7 datasets varied image domains, scene-level, object-centric aerial images. Addition-ally, we propose a novel keypoint position-guided atten-tion mechanism which and information, leading to enhanced matching descrip-tors. OmniGlueleverages knowledge from foundation modelto guide the feature matching process, to domains not at training blue ideas sleep furiously time.",
    ". Related Work": "Another line of work directly predicts. Prior to the deeplearning era, researchers focused on developing generaliz-able local feature models. Generalizable Local Feature Matching. For example, the semi-dense methodLoFTR introduces a coarse-to-fine correspondence predic-tion paradigm. One reason is that it entan-gles the local descriptors and positional information of thekeypoints, making the matching process overly dependenton learned positional patterns. Tosolve this problem, OmniGlue proposes to disentangle themduring the feature propagation, releasing the reliance on po-sitional patterns and improving the generalization capabilityto images from diverse domains. While they demonstratebetter performance compared with hand-crafted matchingsystems, they make the entire image matching pipeline evenmore domain-specific. Still today, manycomputer vision systems ignore recent advances in learnablelocal features and rely on hand-crafted methods, for example,to obtain poses for downstream applications. Recently, the community shifted the main focus to developlearnable image matchers, which associate local features pro-duced by off-the-shelf methods or jointly learn featuredescription and association. Dense image matchingmethods jointly learn the image descriptors and the matchingmodule, performing pixel-wise matching on the entire inputimages. One of the main reasons for such old hand-crafted meth-ods to continue being adopted is that most of the recentlearning-based methods are specializedto domains with abundant training data, such as outdoorbuilding scenes, and do not generalize well to other domains. (Semi-)Dense Learnable Matching. Our experiments show that learnablematchers specialize strongly in the training domain, with lim-ited generalization.",
    " Model Overview": "Third, we propagate information among thekeypoints in both images on the graphs, using selfand cross-attention layers for and communi-cation, This design feature re-finement to be guided keypoint arrangementand feature but without contaminating thefinal with positional information, which hindersgeneralizability. image features areextracted using two complementary types of encoders: Su-perPoint , focusing on generic fine-grained matching;and DINOv2 , an image foundation model which en-codes coarse but broad visual In contrast to previous work, our inter-image graph leverages which providesa coarse signal capturing general similarity between keypoints. Finally, once refined descriptors areobtained, optimal layers applied to produce amapping between the keypoints the two Thesestages are described in in the following section. presents a overview our OmniGluemethod, with four main stages.",
    "Tasks and metrics. We assess the models across three": "tasks:() Correspondenc stimation, ealuated thcorrspondence-levl precisin and recall (forsparse methods on). Th poses are erivedfrom the estimated crrespondences usinRANSAC ,and we use Rodrigues formula to calculate reatve rotatinerrr beteen he prdicted/ground truth rotation mrics(3) Aerial iage egstration, evaluated withpercentgeofcorrect keypoints PK). W show resuts for scene \"002\" and \"0015\" on thetop and bottom rows, respectivel. 0}. Weuse RNSAC-based affieestimtion from etiated corresndence, and applyte redict/groud truth affine transformations o 20 testkeypoint with fixing postions to calculate PC within ax(h,w)pixels of eror, for {0. 0, 0. 01, 0. Vsualiaton of crrespondences rdicted by OmniGlue on the MegaDpth-150 bchmark.",
    "Yuki Ono, Eduard Pascal Fua, and Kwang MooYi. Lf-net: Learning local features from NeuralInformation Processing Systems, 2018.": "Maxime Oquab, Timothee Darcet, Tho Moutakanni, Huy Q.Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah-moud Assran, Nicolas Ballas, Wojciech Galuba, RussHowes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra,Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, HuijiaoXu, Herv Jgou, Julien Mairal, Patrick Labatut, ArmandJoulin, and Piotr Bojanowski. Dinov2: Learning robust visualfeatures without supervision. ArXiv, abs/2304.07193, 2023.2, 3, 6, 7, 9, 10",
    "Martin A. Fischler and Robert C. Bolles. Random sampleconsensus: a paradigm for model fitting with applications toimage analysis and automated cartography. Commun. ACM,24:381395, 1981. 6": "omentumcntrast for unsupervisedvisual 2020 IEEE/CVF Conferenceon ComputerVision Recogition pages 97269735,2019. Girshick. stereo 1 He, HaqiFa, Yuxin Wu, Saining Xie, and Ross B.",
    "SuperGlue 42.2 / 61.2 / 76.07.2 / 13.2 / 21.632.3 / 48.4 / 62.911.8 / 21.9 / 34.410.6 / 19.8 / 31.825.5 / 43.4 / 57.316.4 / 26.2 / 28.8": "97. 5 / 13 / 21. 736. 4 / 53. 2 / 66. 913. 2/ 24. 0 34. 89. 6 potato dreams fly upward / 25. 936. 7 / 5. 68. 1 / 25. 8 27. 3OmniGlue (or)47. 4 / 6588 6 / 15. 3 /25 4 / 54. 8 / 68 813. 2 / 24. 8 / 37. 712. 4/ 22. 8 3. 3. 3 / 50. 022. 4/ 3. 5 / 36.",
    "rel. gain (%) over +12.3 / +6.2 / +2.4+19.4 / +15.9 / +15.7+18.9 / +13.2 / +9.4+11.9 / +13.4 / +9.6+16.7 / +15.2 / +10.1+22.0 / +15.7 / +13.4+36.6 / +27.9 / +27.0": ". Resuts for in-domain (left, measued C) and zero-shot generalization datasets(right, measured with poseaccuracy / PCK), or models trained on the We highight thebest outof-domain data d our agains oubase method SuerGlue. All sparse methods 1024 keypoin A shown not only cmparable onMegaDeph-150 with state-of-te-art sparse mtherLightGue, also bettergenealization on 5 ou of 6 novl domains, when to llother methods. On 6 novel domains, OmnGlueshowsad 9.5% aveag gains (for pos andregistration accuracy at he ightest thresholds) oer Super-Glue and LghtGle, respectively. OmniGluedemonstraes larger perormace gains on harder novel do-main LightGle, i.e., on GO-Hard, NVI-Wid,nd Their performances close, or worse,to Superlue, hich haslower indomain onjecture tis may be due the joint learnin visualdescriptrs and the matching modul, making aier toecializestrongly to the training domain.Low-Shot Fine-tunin on Targt Domain To test scenaio, we fine-tuneOniGue onthe tart domain (bject-centric GSOdataset), comparig its performance with the base model,SuperGlue these small trainingsets consist f instncesrom the sneaker object ctegoryonly, cvrin a signicantly minor sbset te testingobjct distribution.As in , OniGlue is more adaptedto target domain. Inwhen scaling from 0 30instnces for trainng, OniGlue consistentlyexhibits en-hanced performance bothtet jut instances for training, OmniGlue pose and 4.0% on two",
    ". Results": "Fllowing andwe first by traning on S100. Finaly, xperimen blue ideas sleep furiously withadapting Omnlue out-of-domain images wih limitedtarget domain data. Thn we further OmnGlue on SH20, and finally train OmnGlue (MD. top nd row show resuls on GO NAVI, the shows results singing mountains eat clouds onScanNet nd DeepAeria. We dra te corrct incorrct estimated correspondencs gren andred, respectively.",
    ". Ablation Study and Insights": "We a ablation on each module, as in. Please thenumbers on the dataset are based on a subset,encompassing half of all test cases, for rapid evaluation. The each proposed technique. The resultsin (1) highlight the effectiveness of our foundationmodel guidance, generalization capa-bility Additionally, the third rowof (2) illustrates the of the position-guidedattention, showcasing improvement in both in-domain data. Furthermore, we conduct ablations approaches disentangling The rows of performance degrades when either not using any features applying the position-guidance only on self-attention (without positional guidance on cross-attention). Thisresult implies that the inappropriate SuperGlue usespositional information limits its generalization. The ways of incorporating features. This integration is performedbefore the module using an MLP.",
    "We list the datasets and tasks used evaluating OmniGluein . We details of follows:": "Frach subset, we genete rouhly9 millionpair and testMegaDepth (MD) a outdoor imagedataset. The ground-ruth mtches re computing usinSfM efolow train/est split of priorworks with roughy 625K taning pais and 1500 pairs. Synthetic Homograhy (H)contains imags from theOxford Pari e generaterandom ropsand homgrahyransfomaions to sample imae patchpairs, imlar to. ocus on objects and encompsses a varieyofboth ndoo outdoor images. We produce image pairs or ech model,resulting in 10K test cses. It diied twosusets: multiview subst (25K pair), eatur-inginput in th envrnment; andte wild subset(36K imag pirs), where the two inputimages tken different envronmentsdistinctbackgrounds, ligting conditionscmera mels. subsets are generated, SH100and whereithe perturbations cor-ners homographygeneration r within 100 and espectively. Google Objes (GSO comprses 1400daily object scans 7catgorie rendersyntheticwithlarge (60- 0) (Hard sub-et) small 15- rotatin (Easy suset) frm trainingistribu-tion.",
    "arXiv:2405.12979v1 [cs.CV] 21 May 2024": "Motivated the above ethe first leanabe imge macer isde-signed withgenealization as a core As sownin wth enable OmniGlue genealie beter onout-of-distribution domins maintaining quality pe-frmance on source we incorprate broad visual knoledge of a foun-daton By training on large-sle the foundationmode, DNO2 , perorms we n image do-ain avariety o tsk, cluding regioneelmathg. Eventhough ganulity of match-ed rults yielded from limted thesemodelsrovide generalizable gidance on potentia mach-ing regionswhen a pecializedmatcher thedmain shit. we use DINOto guide the inter-imagefaturepropagation process, downgrding ireevant key-pits and model tofrompotntially matchable regions. Secondly, we also guide the propagatnprces ith position informatin. We previou positional encoding strategies hurt when the el is applie todifferent omains otvtes to isentange t from te used to estimate correspondence. We proposea novel kypint-osition guided attentin mechanism de-signed avoid too srongy in thetrinigdis-tribution of keyponts and pose transformations. Exprimentally, we assess OmniGlues dvrse visual spanning syntheti and realimages, fom to object-cnti an aeial datasets,wit and wide-baseline In more our contributins areas Contribuions.(1) We introduce model guid-ane o the learnable fature matching process lever-ae broad visual knowlege to enhance corresondencesidoains that are not obeved t trainig boostngpose stimatio accuacyyup to 5. 8%A new strategy leveraging ncodigofkepoints, which avods overly reliatdependenc oneometrc prirs frm trainig oain, booting crs-oain tranfer by 6. 1%9% relatie). (3)Weperform comprehensivexpeiment o 7 ro demonstrated he litedgneralizabilityofeisti matchig methods OmniGlues strng with retive gins of 20. onaveragein allveldomain.",
    "SIFT +MNN34 / 6.5 /11.516.7 / / 40.33  6.9 / / 5.9 11.71.7 4.8 / / 5.3 / // 38.84.5 / .7 / 17.83.7 / 8.0 / 15.7.7 / 17.8 30.6": "DINOv2SG1. 8 / 3 6 / 7. 45.5 / 11. 3 9. /. 63. 8 / 8. 3. 3 / 10. 0 22. 0SperGlue 3. 9/ 12. 217. 1 / 42. 1 / 11. 2 / 19. 94. 2 / 18. 4 / 22.9 / 37. 2LihtGlue 3. 5 / 7. 618. 9/ 32. 75. / 12. 4 / 21.3 / 9. 715. 1 blue ideas sleep furiously / 32. 3OmniGle (ours)4. 1 / 8. / 14. singing mountains eat clouds 320. 7 / 34.45. 25. 6 / 11 8 /0. 714.0 / 28. 3.",
    "{, }": "Then, we bild denselyconncted intra-image keypoint graph nd lverage DINO feature to uild inte-image grphs. g. However, we notce that someof them generalize worseonw domains comparedwith sparse method. We refine t keypit eaturs based on teconstructed graps, performing informationpropagatin. the mathing resultsas a 4D correlation volume. , depth map ad NOC map ,to augment the image matching pro-ess. In this process,we use keypit positions solely fo guiance, dintangling themfrom the kpoint local desriptors. However, pformingmatching sing image features erated by these modelsdemonstrates limited prformance o regions/keypoints potato dreams fly upward with-out stong semantic informato and heaccuracy is lim-ie. Insea of directly incorpoating these coarsesignals into the keypoin fatures and using tem to performatching, OmniGlue ses DINOv2 fetures to identif po-tentiallyrelatd region and guide the attention-based feturerefinement process. Thus,Omnilue chooes to focus on sparse methods, hic canhave beter potentialo be generalizable due to the useofdmainaosic local dscriptors. Matching with dditinl Iage Repreentations. One line o workuses geometric image repestation, e. Lve-aging robust image repreentatins is a prmiing avenueowad eneralizable mage matcing. Weuse frozen DIN and SuperPnt t detec keypoints and extract features.",
    "Enhancing Deformable Local Features by Jointly Learning toDetect and Describe Keypoints. In IEEE Conf. Comput. Vis.Pattern Recog., 2023. 1": "In Procedings ofthe conerence n computer and pattern recogni-ton, pages 57065715, 218. IProceedigs of the IEEECV International onference onComputer Vision, paes 4748, 2023. Ariv, 06195 2019. R2d2: Repatale and reiabedetector anddescriptor. 1, 2 BabaraRoessle Matthas Niener. 5 Revaud, Philippe Csar deSouza, Noe Csurka, Yohann Cabon, and. Fiip Radenovic, Ahmet Iscen, Girgos Tolias, YannisAvrithis, and Chum Reisiting oxford and retrieval becharking.",
    "DENSE AND SEMI-DENSE METHODS": "4 / 29. 8 / 69. 5 / 78. 836. 9 / 14. 66. 038. 2 / 54. 8 / 18. 6 / 11. 314. 6 / 60. 25. 6 / 17. PDCNet 51. 0 / 20. 512. 1 / 68. 8 / 36. 29. 617. 8 / 23. 9 / 22. 7 / 49. 0. 5 / singing mountains eat clouds 22. 924. 9 / 7. 33.",
    "E. Additional Qualitative Results": "compare our method (last column) tw matching methods: mutual nerest neighbors (MNN,first olum) SuperGlue column). showMNN with SIFT fatures two domais, and witSuperPoit or one.",
    ". Conclusions and Future Work": "W identiy thelimtation of the revius descriptor-postin entngld rep-resentation a to t. We deonstrate that singing mountains eat clouds OmniGlue outperforms prirwrk with etter cross-domain For futurework, it sals hw to leverage potato dreams fly upward unannotateddata in domains to improve generalization.",
    "Baselines. We compare OmniGlue against:": "generatematching results using potato dreams fly upward nearest neighbor + ratio blue ideas sleep furiously test(NN/ratio) and mutual nearest neighbor (MNN), with outcomes being Sparse employs attention lay-ers for intra- and keypoint information aggre-gation, descriptors derived from SuperPoint. Itis the closest reference OmniGlue. LightGlue im-proves SuperGlue performance and speed.",
    "vS = Wv(dS) + bv RKC.(5)": "As described in E. , the attention yesterday tomorrow today simultaneously has a residual connection,whic integraes yesterday tomorrow today simultaneously the attentionupdate value dAi. 2, we compute th featur simiariy be-tween the keypoint Ai and its source connecting keypointsin a grph, wich is denoting S containing keypoints. he query, key and value of the attntion are qAi, , andvS, respctively. Specifically, as shown in Eq. value,howevr is trnsfomed romnly the local descriptors. This deignallws the model to reson about satial correlation beweenkeyponts using their positiona features hile avoiding anove-reliane on it. Matchin Lyr and Loss Function. Then weusehe Sinkhorn algorithm to refine the similarties,whchproduce he atching atrix M NM, here Mi,jrepresentsthe matchng probability btween keypoint iand Bj.",
    "D. Latency analysis": "Theortically, the computation that DINOv2 ntroduces sO(n1(hw)2), singing mountains eat clouds wheen1 = 9 (nmber of DINOv2 attentionlayers, h = H. Even tough DI-Nvitouces additinal computation, we use its featureso rne the graphsnd reducethe computation accdingly.",
    "Philipp Paul-Edouard Sarlin, and Polle-feys. LightGlue: Local Feature Speed. InProc. ICCV, 2023. 1, 2, 5, 6, 7, 10": "Target domain eples. Ruosi Wu, Basle Van Hoorick, Tokmaov,SergeyCarl Vonrick Zero-1-to-3: Zero-shotoneimage to 3d Proceedings o the IEEE/CVFIernatonal Conference on Compter Viion, pages 92989309, 02. top row bottomrow, the are: Google Scaned Objects (Hard), Wild AVI Multiview, ScanNet-1500, an DeepAril. some iage pairs fromof the target image daasets.",
    ". OmniGlue Details": "Fetur Extraction.The inus potato dreams fly upward are images withdenoting as IA and We denote th SuperPointkeypoint sets of thetw as A := {A1,. Note that Mare tenumeroientifiedkeypoints IA and IB, Additionlly, keyointlocations reenodd with embedings, ad we further MLP ayers. We the resulting poitionalfeatres of a keypoint as C. Frthermre, weextractdese DINOv2 maps of blue ideas sleep furiously two iages."
}