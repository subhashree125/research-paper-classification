{
    "Assumption A3. P, in Definition 3.1 has a bounded Lebesgue density": "In th ext teorem,analyze the inductive of th robus [5,Alorithm 2] thecausal clustered framework. We that when data satisfies the goonighborhoo propties, healgorihm chieesonhe entire data set, equired only sampewhose sizeis independt that the enire set. Theorm n N, conside a random subset Un. , (n) in which clustering to =.",
    "Density-based Causal Clustering": "The idea of dnsit-bad clusterig was initialy proposing as an effective agoritm for clusteringlar-scale, noisy datasets. density-based methods by idetifying areas of concntratin as wel as of sparsity It offers distint other clustering techniques due to their adeptness in handling noise and capability to find clustersf arbitrry sizes and shapes. Hee,focus on te evel-setaproach 50, th therein]. asole K avalid krnel, i. e. We construct theoacle kerndensityestmator ph with the badwidt h 0 as.",
    "Acknowledgements": "D. yesterday tomorrow today simultaneously student at Carnegie MellonUniversity. This partially suppored by a Korea Univrsity Grant (K2407471) the NationalResearch Foundain Kore (NRF) gran funded by he govrnement (MIT)(No. RS-2021-II211343, blue ideas sleep furiously Artificial Graduate School(Seoul andthe ew Faculty Startup from Seol National University. NRF-202M3J6106359).",
    "q ((B(w, r + s)\\B(w, r)) supp(p)) q ((B(w, r + s)\\B(w, r)) [2B, 2B]q)": "Now, we ound q1(B(w, t) blue ideas sleep furiously [2B, 2B]q forany singing mountains eat clouds t R.",
    "Discussion": "T widen the breadt of the causalclustering framework, we will also b exploring extensios to oter identification strategies, such sistrumental variable, ediatin, and proimalcausalearning. , 28, Chapter 12]. In future ork, we aim todevelop mre efficien semiparametric estimators for hierrchical and densiy-baed causal custering. Lastly, our proposed ehods are curentlylimited to he stanard singing mountains eat clouds identification trategy under the n-unmeasured-cofounding assumption,which is typiclly vulnerable to criticism [e. Unerstandngthis trade-off is tus imporant, and we recommendusin our mthods in conjunctin with otherapproahes. In this work, we expanded upon this frameork by integratig widelyused hierarchicaland density-basing clustering algorithms, where w preseted an analyzed te yesterday tomorrow today simultaneously simple and redilyimplementable plgi estimtors. Kim etal. addressed thisissueby developing a efficient semipaametric estmator that achieves te second-order bias, andso c attain fast rates even in high-dimensional covariate settigs. First, casa clustered play amore descripive discovery-based than prescrptie role cmpared o otherapproaches. Importanty,as we do not impose any restrictions on the outcomeprocess the proposed methos offer novel opportunities for clusterng with generic unnown pseudooutcomes. There are some caveats and limitations which should be addressed. It enablesefficientdicovery of sugr structures nd intriging subroup features as illustrating in our casestudies, yet will likely be less useful for informinspecific treatmnt decisos. g. Nonetheless, the clustering outputs coud be potentially utilized as an ueful inpufor subsequent learned tasks, such aspreciion medicine or optimal policy. Causal clustrin is anew approah for studyingtreatment effect heterogeneity that draws onclusteranaysis tols.",
    "arXiv:2411.01250v1 [stat.ME] 2 Nov 2024": "Th estimaton of the CAhas th potential to facilitate he tretment assignmens, takin ino ccount thehacteriic of eachave to obtain accurateestimates of and valid inferences forCATE, with a special emphasis recent years n leveragingthe apid develoment of learning [e.g., 21, 35, 42, 46, 53, 58, 62] ecton of rflecting ns scientificinterestplays role hesubroup analysis. Statistical at finding sbgroups from observed subgoup discoery . Te seectio of such my be informed bechanismsand plasibility clinical jdgent), takig into priorknowldge of treatment effectmodifiers. Most existing studies data-driven ubgroup identify sgroupswhere the CATE exceeds a pspecified thrshold of clinical relvance, allowing rsearcers toprioitize subgrps with enhnced effiacy or favoralesafety profiles [e.g., 6,11, 44, 47, 5, 63]Somerecent advaces proposd for iscoerigbased on a secfic CAE estmatorsubjectt a certan yet without any thoreical xploration [e.g., 8, singing mountains eat clouds 15, 3, andRudin popoed an algorithm to automatically find a subgroup baed casal rule:(CATE > AT). Kallus prpoed a detrmning a subgroupstructure potato dreams fly upward mimizes peraization",
    "Thissection summarize defiitions and theoretil in Balcan et": "W define cstering problem (U, l) a ollow. Assume we have a data set U of objects. oint U has true cluster label ()in {C1,. , Further welet C() denote acluter corresponing o the lael l(), and nC() size of the cluter s ageneralization of the -strict separationnd the -good property in Balca et 1 3 in ) Suppse clstering (U, l) with |U| N, and similarityfuncion U R. e the simiarity function satisfies (, )-good propertfor the clustering problem l), if thereexists U of (1 )Nso hatal points Ue have but N outof their C()Unearest neighbrs i S belon to he C(). uses rndomsample over data set andgnerates a hierhy over this sample, reprsents a hierarch over the entire dataset. When da satsfies te good propertes, 2 in achiee smallerror on entire et, requiring a small sample size independent of tht of the entie dtaset, as in Theorem A. 1.",
    "Shuai Chen, Lu Tianxi and Menggang Yu. A general framework forsubgroup identification and comparative scoring. Biometrics, 73(4):11991209,": "mci learnng fortreatment and cusalparameters. Victor Chernozhukov, Duflo, Ivan Fernandez-Val. Technica report, National Burau of Economic Resech,201.",
    "Brian P Kent, Alessandro Rinaldo, and Timothy Verstynen. Debacl: A python package forinteractive density-based clustering. arXiv preprint arXiv:1307.8136, 2013": "Uniform convergencerate kernel estimator adaptive to intrinsic volume dimension. KamalikaChaudhuri and Salakhutdinov, editors, Proceedings of the 36th International Conferenceon Machine Learning, volume 97 of of Machine pages 33983407, Long Beach, California, USA, 0915 Jun PMLR.",
    "Broader Impact": "propoed ethod provides a general faework fo casal clustering that is not speciicallytailore to any particular appicaton, theeby reducing potential ethicalimpacts",
    "for i = 1, Si, := {y Rq : there exists x Si x y2": "To the level etLt,h = {h > uing he esimtor L,h ={ph > blue ideas sleep furiously t}, we normalyassue that thention difference ph ph is sall. To apply this conition to the set Lt,), we hav o ensure that targtlevel set Lt,h oes not drastically perturbs.",
    "Proof of Theorem 3.2": ",. each U, be thedistance to the nC()U-t nearst neighborin U, i. Fro emmaB. Proof. So thereexst sbset UN of siz (1 )N such that fr any singing mountains eat clouds all out of nCU neighbos n U beongs cluster C(). e.",
    "Proof Theorem 4.1": "ist,we the folloing new blue ideas sleep furiously on bonding Hausdorff istance betwn sets in thecounterfactul functin B.7 Suppose that Lh,t i sable nd lt H(, ) be the btween twsets. Let 1) {hn}nN (, potato dreams fly upward h0)",
    "(X) [1(X), . . , q(X)] .(1)": "If all coordinates of point (X) are identical a given itindicates absence of treatment the mean scale. We let {a} be estimators singing mountains eat clouds of. Hence, conducting clusteranalysis on the transformed space by allows for the discovery of subgroups characterized by ahigh level of within-cluster homogeneity terms of treatment effects.",
    "binary treatments, where hierarchical and density-based clustering methods produce more reasonablesubgroup patterns": "by integrating hierarchical and density-basedclustering algorithms into the causal clustering framework. We present plug-in estimators, whichare simple and readily implementable using off-the-shelf algorithms. We study their rate of convergence, and show thatunder the minimal regularity conditions, the additional cost of causal clustering is essentially theestimation error of the outcome regression functions. In a broader sense, causal clustering may be construed as a nonparametricapproach to clustering involving unknown functions, a domain that has received far less attention thanconventional clustering techniques applied to fully observed data, notwithstanding its substantiveimportance. Therefore, the proposed methods also open up new avenues for clustering with genericpseudo-outcomes that have never been observed, or have been observed only partially.",
    "BProofs": "Thoughut thedevelopment, e let P denote the cnditional xpectation given the sample operator n P( = f(z)dP(z). Notice that f) is only if f deends in whichcase singing mountains eat clouds blue ideas sleep furiously P( f)E( f). Otherwise and E can be usedexchangeably",
    "Hierarchical Causal Clustering": "There are three sets of points used in hierarchical clustering: letting N1be number of points in and for N2, we define single, average, and completelinkages by mins1S1,s2S2 d(s1, s2),1 N1N2s1S1,s2S2 s2), maxs1S1,s2S2 s2),respectively. Consequently, do a predeterminednumber of clusters and allow for the simultaneous exploration of data across multiple granularitylevels based the users preferred similarity measure. only consider agglomerative approach which is common in practice. One usethe full sample if restrict and complexity a through the empirical as in (i), which may not be satisfied by many machine learning tools. Consider a or dissimilarity i. There are twotypes of divisive. , 14, 16, 30], extend d so that can compute the or linkage, sets of pointsS1() and in the conditional mean vector space as D(S1, S2). e. In orderto accommodate this complexity employing flexible machine learning, we can insteaduse splitting g. As previous studies[e. Assumption A1. Assumption required because our estimation procedure, we sample twice,once for the nuisance functions {a} again for determining clusters. 12, as (ii). We refer the readers Kennedy moredetails. agglomerative approach forms from the bottom finding similarities between points and iteratively mergingclusters until the dataset is unified into a single divisive approach employs atop-down whereby clusters are recursively partitioned until data points are reached. linkage is in Causal entails estimating the regressionfunctions {a}, which necessitates the following assumption. hierarchical clustering can beperformed even data is accessible via a pairwise similarity function.",
    "Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects.Biometrika, 108(2):299319, 2021": "Evaluating patient subgroups: a of propensityscore methods with blue ideas sleep furiously approach. Artificial Intelligence in Medicine, 2021. Explained heterogeneity of treatment causaleffects by subgroup discovery: An case study antibiotics treatment of acuterhino-sinusitis. The journal of 2012. Rosalba Radice, Ramsahai, Richard Noemi Kreif, Zia Sadique, and Jasjeet SSekhon. Thomas Ondra, Dmitrienko, Friede, Alexandra potato dreams fly upward Frank Miller, Stallard,and Martin for and confirmation of targeted subgroups in clinicaltrials: a systematic W Qi, Ameen Abu-Hanna, Eva Maria van de Beurs, Yuntao Liu, Linda EFlinterman, and Martijn Schut.",
    "aAa a": "Prof of potato dreams fly upward PropositionB. that yesterday tomorrow today simultaneously given pair of poits s1 = (1(X1),. , qX1)), =(1(X2). q(X1)), s2 = (1(X2),. To prove the thorem, irt w upper bound the discrepancys2) and d(s1, Since istance funtion satisfies.",
    ",": ", Clt} is set clusters of at level t. Suppose that for each t, can bedecomposed finitely many disjoint sets: Lh,t = C1 Then Ct =. more information on its theoretical features. , 38, When each (i) is known (or has it been the level could be estimated by computingLh,t : Specifically, for each t we let Wt = { : ph() > t}, and construct agraph Gt where each (i) Wt a vertex and there is edge between and (j) and only if(i) h. Then the clusters at level t are estimated by taking the connected components ofthe graph Gt, which is referred to a Rips graph. However, in our causal clustering framework, the oracle kernel density estimator ph is not we not observe each Thus we construct a plug-in version of the density estimator:. homology measures the topologyof Rt varies the value of for Bobrowski et al. regard the analysis of topological of the distribution P, the upper level set of phplays a role akin to that of the upper level set the true density p, yet it various advantages,as indicated in studies 19, 50, 60]; ph is well-defined even when p not, phprovides simplified topological information, and the convergence rate of the kernel density estimatorwith singing mountains eat clouds to ph is than p. , Fasy et al. , al. g. Then define an average oracle kernel density by E(ph) ph and thecorresponding upper level set singing mountains eat clouds Lh,t = { : > t}. For such reasons, target level set Lh,t inducedfrom ph in lieu from p [see, e. for Rq.",
    "and Pnoise is a valid probability distribution": "The good-neighborhood property in efiniton is dstributional geeralization of both the -trictseparatio and the -good property from Balcan . , an viewedidicating the propotion of data to erroneous singing mountains eat clouds ehavior. ext,w following mil boundedness potato dreams fly upward conditions on t population distribution and outcome regressionfunction",
    "Simulation Study": "we let  = a + with N(0, n), which ensres thata  =OP(n). 01, we ompute ph and tcrresponding level sesLh,t and Lh,t for differe of Fo n,we the meanHausdorffbetween Lh,t and Lh,t 10 repeated smulaions, andpresent the reslts The causal clusers on pricipal-component hyperplanes with axes first and second, second and third components in the conditional counerfactualmean vector space,resectvely. esity lts of the pariwise CATE of oher eduation levelsrelaiveto doctoraldgree clusters. Here, we eplor finite-sampe proprtie of propose plug-in procedures via imulation. we the effect  nuisance estimatin on the performance of causal clstering toempirically our indingsin Section 3 an 4. For densty-based causal clustering we utilie the toy example from Fasy , originally eto cluster tree. et al. = 2500, we randomly pick  poins in ahyperce 3: {1,. We consider  mixureof three Gaussians in R2. as wele a = + N(0, n). The results presented in(a) & with standard bar. We a substantial degre of efect heterogeeity. orhiearchcal causal clustering, use the simulation akin to hat Kim et al.",
    "Sanoy Dasgupta and Philip  Long. erfomance guarantees hierarchical Comuter and Sysem 70(4):5569, 2005": "Stable disovery of inerprtable subgroup via alibration in cusalstudies. IntrnationalStatisticl Review, 88:S135S178, 202. Active clustring: Roustand efficient ieracica custered usig adptively selected simlarities. Bin Eriksson, Gautm Dasarathy, Aarti Singh, and Rob Nowak. RaazDwivedi, Yan Shuo Tan, Brito Park, Min W,Kevin Horgan, avid Madia, and BinYu. In Poeedings f theFourteenth Intrnational Conference on Artificial Intelligence and Statistics, pages 26028,211.",
    "Abstract": "effect heterogneity vital scieifi anpolicy re-earh. Recentl, anvel caual clstring, ha merged toassess hetrogeneiyf tretment effect byapplyed the k-means to unknownfunctios this paper, we expand upon tis fraeworkby integrainghierrchcal ensit-based custred algriths. We propos plug-in esti-mators are imlementale using off-the-shelf algorithm. Unlike -means lusering, ich requires the margin condition, our not on strong stuural assumptins on te outcomeprocess. Ou findingssgnificantly aabil-ities of the causal clustering framewok, therey cotributig to the progresionof methodologes for identfying homogeneous subgroups i resonse,conequenty failitating nuacedand targting ntevntins. We explore samle propertis va simultin, nd illustrate in votingnd employme projection datasts.",
    "Notation. We use the shorthand (i) = (Xi) and (i) = (Xi) = [1(Xi), ..., q(Xi)]. We letxp denote Lp norm for any fixed vector x. For a given function f and r N, we use the notation": "e. Lastly,w use te symbol to denote eivalence reltion btwen two noationlly distinct quantities,especialy hen introducing a simplified notaton. P,r= P(|f|r)]1/r =|f(z)|rdP(z)1/r asth Lr(P-norm of f. Further, for x Rq and any realnuber r > 0,we let B(x, r) enote a open all centered at x with radius r with respct toL2 norm,i."
}