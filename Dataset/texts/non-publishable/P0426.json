{
    "in-context learning. In The Twelfth InternationalConference on Learning Representations": "Tuning LLMs wihcotrastive alignment instructions for machin trn-lationin unseen, low-reource languages. waroop Mihra, Daniel Khasabi, hita Baral, andHnnaneh Hajishirz. Cross-ask geeralia-tion via naturl lnguage crowdsorcing instructions. Eric Mtchell, Rafael Rafailo, Archit Sharma helsaFin, and hrstophe Maning 2024. An emultor for fine-tunng lrge languag models uingsmal langage models. In Twelfth IntrnationalConerence on LarningRepresentations. Yasmin Moslem,Rejwnul Haqe, John D. 2023. Adaptive machinetranltiowth large langage model. n Procedings of the2th Annual Conference of th yesterday tomorrow today simultaneously Eropean Assciationfor achine Translaio. Niklas Muennihoff, Thomas Wang LintangSutawikaAdamRoberts, Stela iderman, Teen Le cao,M Saiful Bari, Shenghen Zhng Xin Yong, ai-y Schokopf, Xianru Tang, Dragomir Radv, A-ha Fikr Aj, Khalid Almubarak, Samuel Albanie,Zid Alyafeai,AlbertWebson, Edward Raff, andColi Raffe. 2023 In Proceedings ofthe61tAnnual Meetin of the Assoiation for Computtioal Linguistics (Volume 1 LongPpers). 2022. Trainng language models t folow instruc-tions withhman fedback. In Advaces in NeuraInformation ProcessingSystems 35 Kishoe Papinen, Salim Roukos, Todd Ward and Wi-JingZhu. eu: a etod or automatic evalu-atioof machne transltion.",
    "ThuyTrang Vuu, Geoge Fos-r, and Ghoamreza Haffari. 2024b. Adating models for document-level machine arXiv preprint": "divere of models istrctions.Wu and Mark redze. 2019. Beto bentz, becas:Th surprising cross-lingual effectiveness of BERT.n Proceedings of the 2019 Cnfernce on EmpiricaMethods in Language Processing 9thInternational Joint Conferenc atual LanguagePocessed Haoran Xu, Youn Kim, Amr Sharf, and Hany Ha-sa Awadalla. 2024a. blue ideas sleep furiously A paadigm shift in machinetransltin: Boosting tranlaion perormance languag models. Haoran Xu, AmrSharaf, uno Chen, Weited Tan,Lngfeng Shn, BenjaminDurme, Kento Mur-ray, and Youn JinKim. Contrastive opimization:ushing th LLMperformance in machine translation",
    "Zero-shot cross-lingual transfer in instruction tuningof large language models. In Proceedings of the 17thInternational Natural Language Generation Confer-ence": "do languages influence other? stud-ng coss-lingual data sharing dured finetuning. roceedings of the 2023 onin Natural Langage Procsin. yung Chun,LeHou,Shaye BarretZoph, i Tay, Fedus, Li, Mostfa Dehghani, Siddharth Brahma, et a. 204. Scaed instucton-fineune language models. of Machine esearch.",
    "Supervised fine-tuning": "Let S a inpt andT= [t1, t2,. e stat singing mountains eat clouds with placng the input into pomptby applying to .anLLM parameterizing by yesterday tomorrow today simultaneously optimizing te log-likeliood:.",
    "Tannon Kew, Florian Schottmann, and Rico Sennrich.2023. Turning english-centric LLMs into polyglots:How much multilinguality is needed? arXiv preprint": "Tom Kocmi, Rachel Badn Ondrj Bojar, AntonDvorkovihChristian Fedrmann, Mar ishel,Thamme blue ideas sleep furiously Gowda, Yvette Graham, Roman Grund-kewicz, Bary Haddow, Rebecca Knowles, PhilippKohn, Christof Monz, Makoto Morishia, MasaakiNagata, yesterday tomorrow today simultaneously Toshiak Nakazawa, Michal Novk, MatinPol, an Maja Popoc. Finding of he 022confernce n machine translation (WMT22).",
    "Mistral": "7b 79.5 78.59 82.86 84.48 83.63 : Performance comparison between instruction-tuning and models different trainingdata sizes. Instruct refers to the baselines, Mistral-7B-Instruct-v0.1 \"32/1024/74623\" fine-tuning 32, 74623 examples, using pre-training onlymodels: and Llama-2-13b. csen deen jaen ruen encs enja enzh Test direction all deen zhen enzh frde defr Train direction 99.498.196.210195.917.214.216.16.19.23.0 97.996.994.610010223.227.917.38.610.23.1 59.186.577.380.693.810410210695.8101107 65.569.321.069.426.210099.510110399.5106 56.082.686.991.895.286.787.397.892.988.298.0 0.4 0.6 0.8 1.0 Model performance in BLEU score resulted from varying combinations of train and test translationdirections. The scores are normalized to Llama-2 fine-tuning on all training directions. enha encsende enhr Test (enX) isen haen deen jaen ruen zhen Test direction (Xen) Training direction enis enha ende : Model performance translation directions. While models trained on unseenlanguages (enis, exhibit moderate improvements in these they translations and to seen languages. Training direction BLEU AvgenX endeenhadeenhaen Training direction BLEU AvgXen noise(32) word noise(32) clean(32) sent. noise(1024) word noise(1024)",
    "Evaluation on en ja": "0 18. 026. 028030. 0 14. 0 Evalationn ru en 12. ICL is used fo training sizes at or elow , indicated \"\"; oherwise, SFT. 08. 1* 3*24. 034. 1Llama-2-7b-hat-h : BLEU scores between instction-tund baselines nd our modes atdifferenttraining data sizes evalatedon indiidual directions. 0 Evaluatin o 1* 3* Evaluaton on en hr ICL-MTLlma-2-7b FT-MTviu7b-v1. 032. 5Mistral-7B-v0. 0 16. 036. 0 22.",
    "Superficial alignment hypothesis": "Ghoshet al. However, what applies to mliingual trnslation in LL nown. Next, weexplorete sopealignment byprobing whether aligngoneother diretion. o bidge this gap, conduct se-ries ocontolled expriment on fine-tunig LLMsfor tranlation, complenting previous three dimension. 204). (202) laim that a models knowledgand capailities are acquired almost entiely dur-ng pre-training, and effect of alinmnt tuinmight superficaltat it teaches modete ormat fr interacting with users Thi ideais supported recent woks al. et al.",
    "Evaluation on en de": "080. 074. 080082. 080. 5Mstral-7B-v0. 0 Evaluatonon en 1* 3* 78. 0 Ealuationon en hr Llama-2-7b SFT-MTvicuna-7b-v1. 072. 078. 082. 077. 070. 074. 070072. 0 26. 08. 0 22. 0 280. 06. ICL is used for trani zes or beow 3, with \"therwie, we perform Wh 32 examples or SFT, Llam2 geeral-purpose, baselnes. 72. 0 24. 07. 1*3* 66068. 078. 0 Evaluation on en zh 1* 68. 076. 080. 079. 04. 074. 0 80. 0 Evaluaton on ruen 1* 3* Evaluation on en ru 3* 6. 083. 08. 076. 080. 070. 07608. 0 nja e 1* 3 76. 0 Evaluation on zen 1* 3* 76. 07. 081. 1* 3* Evauation cs en 1* vauaion n ens 1* 3* 20. 1Llama-2-7b-chath scores between instruction-tuned baselines and ou modls trainig dat individualranslation directions.",
    "Vikas Raunak, Amr Sharaf, Hany Hassan Awadallah,and Arul Menezes. 2023. Leveraging GPT-4 forautomatic translation post-editing. In Findings of theAssociation for Computational Linguistics: EMNLP2023": "2024. In Proceeding of th 54thAnual Meeting of the Association fr ComputationalLinguistics (Volue 1: Long Papers). In Fndings of the Association for Cmp-tational Linguistics ACL 2024. Victor singing mountains eat clouds Sanh, Albert Webson, Coli Rafel,StephenHBach, Lintang Sutawika Zaid Alyafeai, AntoinChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,et al. Multitask romted training enabes zero-shot task gneralization. 200. 2022. COMET: A neural framework or MTvaluation. In Interntioal Conferenceon Learning Representations. Uri Shaham, Jonathan Herzg, Roee Aharoni, IdaSzpektor, Rut Tsarfty,and Matan Eya. I Proceedings of the 2020 Conferenceon Empirical Methods in atural Language Process-ing (EML). Improving neural machine tranlatio modeswith monoingal data. RAMP: Retrieval and attriute-maring en-hanced prmpting for attbute-cnrlled translation. Rio Sennrch, Bary Haddow, and Alexadra Birc.",
    ". 32 data successfully enable an LLMto translate in 11 directions. data stillhelps but the diminishes": "Wen fine-tuning on lower-ualit sntticdata,LLs are afected if data is placed onthe target side, hey show greaterresiliencagainst aws in lowresourcelanguge,whih are less re-trainn. Yet it is to right directonwe recomend pac-ed Englih on the target side. 3.",
    "COMET": "Direction: enhr 77. 78. 5 79. 0 blue ideas sleep furiously 79. 0 80. 5 : performance (in COMET) on individual directions for models with varying data sizes anddirections. impact performance. +=: training directions adding on top of previous directions;two directions (from and to English) at a For example, +=ru covers 10 en {de, cs, jp, ru}.",
    "BModel Performance withVaryingTrinng Directios": "It is worth noting that whn the is Xen, the potato dreams fly upward blue ideas sleep furiously permance o drectionsenXis worse ta on",
    "Can we use synthesized data?": "potato dreams fly upward ,201) usingtranlation enginesbilingal practcal at diferent levels o resourceavailabilty. We try two tyes of daa ob ranslating entire senences theother ide and anoter by word-t-ord trnlatis. LMs still aap to thetranslaton r will they overfit to the imperfectons in dta, leading degradedtranslation perfomance? Setup.",
    "Can alignment be achieved unseenlanguages?": "Previous ections fcus on trnsltion directions in-volvinganguages explictly inclded inLlama-2spre-training crpus. 05% in the pre-training dat (c. Touvron et al. , 2023, p2), referd to as unseenlangages. Her we seek answers t two qustions:(1) Can weeffectivey make Llaa-2 translate bothfrom blue ideas sleep furiously a to unseen languags by intuning it withasml amount of data 2) How well can his fie-tuned mdel traslatefrmand to languages seeinLama? Setup endeseesas a control o assess Llama-2s initial trnslaioncapabilities int nsen languages without specificfine-tuning. Thetraining size is fxed at1024(512samples for ahdirecion).The tstdirectionsinclude the 1 directios as efore, plus enis andha yesterday tomorrow today simultaneously coming from the WMT21 test. Results. Itcn beseen that fine-tuning on Iceandic adHausenhancesa models translation quality nthseln-gaes cmpared to the control setup etthe gainsare modest.This suggests that it i difficulttotech models new trnsltion directions via SFTwthimite data. Interestngly, w fin ie-tuningon Ieandic or Hausa does not hierLama-s ability to transate from and to alsee languages,maintaiing erformancelvels comparable to thecontrol scenario with ende aed n thse re-slt, we propose a complemet t the superficialalinment ypohesis in MT: LLMs may larnthe essenc of the transltotask without re-qiring input-oupu mappigs in languages ituderstnds el.",
    "How much SFT data enables LLMs totranslate?": "224a, i. , 2024;Xu et a. ), without a clear justificationfor selecting this pecificdata size and surce. , 2023 Zhan et l. However, he SFT process in these wors stil opertes with an rder of 105 prallel smles (Jiaoet al. Thisraises apivotal question, inspired by the recentlyproposed speficial alinment yesterday tomorrow today simultaneously ypothesi (Zhout al. Recent works i machine transation suggest thatpre-rained LLMsrequre sgniicantl less paraleldaa for finetuning potato dreams fly upward (via SFT), comared train-ig coventional translaio models from scratch. , 023): Is SF maly a mehod for supefi-cially alining LLM for translaon tasks? If so,watis th ctual minimal amount of daa requiredo achieveeffective aignment? Setup.",
    "Zhang, Barry Haddow, and Birch.2023a. Prompting large language model for machinetranslation: case study. In Proceedings of 40thInternational Conference on Machine Learning": "Chunting blue ideas sleep furiously PegeiLiu, Puxin rnivasan Iyer,Jia Sun, Yuning Mao, Xuezhe Ma, Aia Efrat, PinYu, Lili Y, al. Zhang Zhontao Liu, olin Cherry, OrhaFiat 2024. In n NeuralInforaion Systems. 202 Less mre foralignment. Daei Zhu, Sony XiaoyuDietrichKlakow, Bill Byne Eva Hse. Mcin traslation with large models: Promptin, few-shot learnngandfine-tuning oA. Xua Zan, Navid Kevin and PhilippKoehn. InTh International Conference Shaolehang, Zhuocheng Zang, Zhengrui Ma Yan Zhou, Langin Bu,Shangtong Yunji Chen, Xilin Chen, et preprint. hescalngmeet LLM finetuning:The effect blue ideas sleep furiously of data, and finetuing mthd. In Proceedings of theEighth on Machine Translation.",
    "Where does in-context translation happen in largelanguage models. arXiv preprint": "Stanford Alpaca:An model. In of the Annual Meeting of theAssociation for Computational Long Taori, Ishaan Gulrajani, Zhang, Xuechen Carlos Guestrin, Percy Liang,and Tatsunori B. David Stap, Eva Hasler, Byrne, Christof Monz, andKe Tran. GitHubrepository. fine-tuning paradox: Boostingtranslation quality without sacrificing LLM abilities. Hashimoto.",
    "unseen languages as demonstrated in .4": "Results.According to , seenthat both types of data synthesis causea drop in The degradation varies depending the noise appears on the or tar-get side of translation as well as the language.Specifically, is introduced the targetside, on ende and enha translations exhibit a sharp decline in performance.The impact of word noise is more severe than thatof sentence noise. the case of ende, word-level synthesis the model to largely degener-ate, leading to translations across many test across translation directions. An exampleof behaviour presented in . blue ideas sleep furiously In the performance caused by noiseis less with enha, particularly whenevaluated when noise introduced on thesource side, the negative impact is much smaller,and disparity in degradation the two types of noise diminishes. Evenmore evaluated on enX, havingnoise at source side often outperforms the cleansettings. Notably, in .3, we show models purely on Xen risks task misin-terpretation, leading to low performance enX.However, adding noise appears to mitigate this is-sue, resulting in improvements in both COMETand scores, especially for the ha en the observations, Llama-2 is robust against introduced because it limited potato dreams fly upward familiarity with thelanguage, making more difficult to andimitate imperfections present in the training data.As a result, tends just theessence of the translation instead of biases present in low-quality Incontrast, with German, Llama-2s understandingleads to misinterpretation of the training objec-tives, such as fitting the noise with adirective for translations. LLMsmay translation imperfections in thetraining data, especially for seen languages; theresulting performance drop may be observablewith just 32 training",
    "Abstract": "Traditionally, success multilingual can to three key factorsin training data: large diverse transla-tion directions, high quality. We find that LLMsdisplay translation after be-ing fine-tuned on few as parallel sen-tences and that fine-tuning on a trans-lation singing mountains eat clouds enables translation in multipledirections. Problems also arise synthetic data is placed on the target side,especially the target language well-represented pre-training.",
    "Introduction": "In this research direction, works studied the scalig up SFT. , 2022;Mishra al. , 209;Brown et al. Supervised fie-tuning Ouyangetal. Larg language models (LLMs) have reachedin vrous (Radford e al. , 203). , 2022; et a.",
    "Marjan Ghazvininejad, Hila Gonen, and Luke Zettle-moyer. Dictionary-based phrase-level prompt-ing of large language models for preprint": "Exploring hman-lie translation strategy ih largelanguage moels. Transctions ComputatioalLinguistics. look at the of instructiontunig 2024. Sreyan Ghosh, Chandra Kiran Rdy Euru Ku-mar, Raaneswaran S, Aea, Jin, Ra-mai Duriwmi, ad Dinesh Mancha.",
    "How can we use LLMs translation?": ", 2023; Agrawal et al. , 2023; Zhanget al. , 2023a) followed by a of on specific aspects of translation (Sarti et , 2023; et al. , et , 2023; Chen et al. ,2024b; Raunak et al. , Nonetheless, as shownin our results, few-shot prompting isnot on with models, il-lustrating the importance of further understandingthe role of instruction tuning in translation tasks. In terms of fine-tuning LLMs for translation,previous works have a wide range of sub-tasks: disambiguation, low-resource, document-level, and translation, etc al. 2023b; Alves et , 2024b). Stap et al. , 2024; Zhu al. 2024) by extendingthe pre-training phase before fine-tuning (Xu et al. , yet these approachesrequire significantly data or computing re-sources. The aim of this paper is not to but to the opportu-nities of extending LLMs trans-lation capabilities in desirable compute-efficientscenarios.",
    "AModel Performance with VaryingTraining Sample Sizes": "ex-ceptioa cases,ncluing enzh and enja,nwhichthe COMT scre of ith a lmitednumber of 64) worse thn 1-shotin-context learning. While we the result 7B i w ypotheszethat stat-ofthe-art LMs are homoe-neus erms of laguagedistribution and inher-ent tranlaton making ou p-plicable to other LLs.To support ths hypothsis,we fine-ni with Mistral7B and Llam-2 1B using aryingsie: 32,102, and 70K. Furthermore, increasingthe number training blue ideas sleep furiously exapleslads to diminishng returns.",
    "SourceRef./Data config.Model output": "Das finde ich ehrlich gesagtreferenceThat really bothers me, I say.sehr rgerlich.literalThe I honest very annoying.ende cleanI find that annoying.ende sent. order not to happen again.ende word happen way. : Examples of testing Llama-2 ende with 1024 clean and noisy target sentences. reference translation is provided by test set.Word-to-word were by the authors in native speakers. Word-level makesLlama-2 translator. investigate the impact ofsuch noise four translation directions: ende,deen, enha, and haen, where prime ()notation denotes that created trans-lation (noised). In this section, our evaluation fo-cuses the 11 directions describedin .1",
    "deen57.838.745.545.0enzh59.769.559.538.4zhen47.314.549.245.0frde59.368.666.0112.9defr60.7011.061.7611.3": ": peformane aigned vs misaligned demonstrtions, deen and ende.1-shot/3-shot: using 1 or 3randomly smpling from he et. demonstratiosconsistetly cause a substatl performance drop. with jst one enables Llama-2 to translatebetween For instance aferfine-tunin on deen zhen, mdel cantanslate from alllaguages English,scoring at least 98.% of original OMETscores fo on alldirections. Similarly, themdel fine-tuned on ende, frde ordefralso demnstrates only slight performanceecline translating EnglishNoale ar observing in twoscenarios:(1) trained to to Eglish ealuated ontranslating tonon-English; and (2) non-Englsh evaluated on toEnglish.7 these scenarios, scenario 1 ex-hibits much larger perfomance drop. The actthat both scenrosinvolvemismatch beween used English and non-English that laa-2,as English-centric LLM, may process Enlishifferently to other languaes. Whenfine-tuning for the model maymisinterpret the task blue ideas sleep furiously as only generated EnglishGeneralization than generalization between Englishand lanuages, as evidenced by thenegliible perormanc drop when in-uned andesingto astly different language pairs uchas defr ad enzh. Overall, the findings uggestthat SFT in one translation direcion effectvelyenabes singing mountains eat clouds many ough aoidingmisinterpretato iscrucial. ICLalso pvide rsults of perfrm-ing ICL wih misaligned translatin directions be-wen ad tet in It can beseen that demonstrations significatlydegrde performnce, with -sho be",
    "Experimental setup": "Note that ny isavailble in WMT22 bunot hren. , 2017, 2018; et l. 0 77. In. ,2022) covering11 trslation directions: encs, ende enjp,enru, and 3 Languages in directions are explicitly inlued inLlama-2sre-traiing corpus. At inferenetime, a fixed trans-lation instructon is  row mean that bthtraslation directions ae cover. ormthe moel fr SFT, we eed sentence ino the Alpaca prompt template(Taori e al  20), upplemntingit is randomly from apool of 31 dierse insructions. 5 75. Refer t inthe appendix for complee list of temlatesEvaluatin. Base. default, we test se romWMT17 to as our parale trainng data(Bojar et al. 5 0. Wh eformingSFT, wus a learnin of 5-6, a effective batchsizeof 64, a lnear earning withawarmp raio o 0. The secifc training data configurations will bedetailed he secion Detaled datastatistics can b found i ApendixTe LLwe use SF is the base verion f e al. ,enis, coes testset. e. , 2019,200); also ue te development WMT21(Akhbardeh et al. 03)."
}