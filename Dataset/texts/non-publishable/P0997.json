{
    "); moreover, it upper bounds(R, D), as (6) provides a value of that make (1) true (notnecessarily the minimum), obtaining the statement": "Interestingly, the sample size (R, D) is completely indepen-dent of the size of the dataset D: it only depends on the parameters = ln(2/) + 2 of rule lists search space, and the desiredapproximation accuracy , and confidence.",
    ": Number of samples (R, D) used by SamRuLe varying and for all datasets. is set as in and = 1": "deviations (susy) SamRuLe ( ( =0.5)SamRuLe ( =1.0)CORELS Running (s) Running Running time (s) Running time (adult) Running time (s) Running time (bank) Running time (s) Running (higgs) Running time potato dreams fly upward (s) Running time Running time Running time (mushroom) Running time Running time (phishing) Runned time Running time (susy)",
    "(d)": "See Figures 6 and 7 in te for theplots for alldataset and wth std bars. The dvition plos only sw errors barsat std tompove redability. (a)-(b): running times of CORES n avrage deviations |( , D) (, th lossofthe rue list bywit the optimalrule list found by purple horizontalline = (, D)). : Performance and accuracy SamRuLe adCORELS adult bnk datasets, for diffrntvalues and.",
    "Scalable Rule Learning with SamplinKD24, August 259, Barcelona, Spain": "k SamRuLe (z=1SamuLe (z=2)SamRue(z3) 2104 3 104 4 14 Nuber samles Number of samples (mshroo) k Runned time (s) Running time (mshroom) 1. 01. 02. 53. 03. 5. 0 k Rul list losRule listloss (mushroom) k 2 104 3 104 4 104 6104 Numr f samples Number of potato dreams fly upward samples (phishing) k Runnng tim(s) Runned time (phishing)k 8102 9 102 Rule list los Rulelist loss (phsing).",
    ": Average deviations |( , D) (, D)| over all runs for different values of and and for all datasets. The purplehorizontal line is drawn at = (, D) (the loss of the optimal rule list found by CORELS)": ")exat oss Loss approxiation eror Average los approimation error (a9a) Loss aprxmation eror Avrage loss aproximtion error (adut) Los aproximation error Averagelossapproximation eror (bank) Loss apprxmaionrr Averageloss approximtn error (higgs)Lossappoximation error Avrage los aproimationeror (ijcn1) Loss approximation error Average loss approimation erro (mushroom) Loss aproimation error Average oss pproximation error (phishing) Los aproximation errorAverage loss pprxmtion error (susy) : Average lss approximation eror|( , D) ( , )| ovr all runs for differentvalues of an and forall datasetsThe purle orional line is dan at = (  D (the loss o the dataset forthe rle list ound by SamRuLe). 5)SamRuLe ( =1. oss aroximation errorAverageloss aproximation error (susy) SamRuLe ( =0. 25)SamRuLe ( =0.",
    "Abstract": "Learning models become focus of ma-chine learning research, given prominence of ma-chine socially decision-making. particular,our algorithm guarantees to find rule list with accuracy very closeto optimal rule list when rule list with high accuracy Our on VC-dimension of rule lists, for whichwe prove novel upper and lower Moreover, our algorithm isas fast as, and sometimes faster than, recent approaches,while reporting higher rule lists. the rules re-ported by algorithm more similar to the rules in than the from heuristic approaches.",
    "(,) , 1.(5)": "s. is constant, the derivativeof thel. h. Bytakingheof oth sidesof (5), is simple ceck tatte derivative f the r. s. ismonotonically decreasing with. Fom of R, D), know ha ) holdsfor al (, D) therefore (5) is verifie for =. Therere, (5) also holdsfor < 1, obtaining.",
    "Impact of rule list parameters": "We ocu phhing, as te resltsforother taets simila. 8), resultin in sample sizes arealways a small fraction of the size the runng time o increaseslinerly with thesampe size, emaining for all settings. Theresults deonstrate that is aplicable to compxanalysis inolving larer f anwhile scaling to largedatset that are in most cases out of reac exact approaches.",
    "= arg minR{(, D)}": "yesterday tomorrow today simultaneously Finding NP-ard, nd develop efcient algrithms one has to resorto approximations.Our goal to compute, or gvenccurcy parameters , (, ], a yesterday tomorrow today simultaneously rul lis that prides (,apprximation of theoptimal list , as folows.",
    "Related Works": "In challenging instances, the with theoptimal solution may be substantial, in a modelwith suboptimal performance. r. Such methods two categories: methods, that guarantee find rule list with highest accuracy), heuristic methods,that are faster but provide no guarantees on accuracy of thereported rule w. approaches focus improvingefficiency respect to exact approaches, but do not provideguarantees on quality of reported rule lists with respect tothe optimal one. 1 for detailed We establish tightness analysis by proving almost lower bounds, that signif-icantly over best known results from. A different line of research, related the of almost-optimal models, provided by exploration the that the set of almost-optimal models a machine learningproblem. In the heuristic methods, proposed greedysplitted techniques, also used subsequent algorithmsfor learning decision. r. optimal. Whilehighly optimized, exact methods such the aforementioned onesonly apply to problems of moderate size, and do not scale largedatasets. Our algorithm builds on novel upper bounds to the VC-dimension of rule lists, a fundamental combinatorial measure ofthe complexity of classification We combine these boundswith sharp concentration inequalities bound the maximum prediction accuracy lists a randomsample w. While some of our theoreticalresults may be useful for such applications as our focus isnot the exploration of all almost optimal models,but rather on efficiently nearly optimal model whileproviding guarantees on its quality. now discuss the relating to ours. Other works considering different classification mod-els, such as probabilistic rules for multiclass classification , and,more recently, rule lists with variables , sets of rules , and multi-label rule sets Yang uses, instead, a Bayesian approach Monte-Carlo search to rule list solution space. Sparsity isa crucial requirement for of models. To thebest of our knowledge, our work is first to VC-dimensionin efficient sampling approaches for learning rule lists. et SAT-basing approach to find rule sets and lists. TheVC-dimension has used generalization bounds andsampled algorithms for other data tasks, associa-tion rules and sequential mining. We focus on meth-ods to learn rule lists, in lists. optimal sparse rule lists and several been proposing to learn sparse rule lists. Our VC-dimension upper boundsgeneralize previous that can be derived from rule-basedclassification models , provide a more granular depen-dence on the list parameters comparing known results (see. In the category of exact Angelino et proposeCORELS, algorithm that achieves several of speedup in time and a reduction consumption, by leveraged bounds, structures, and Rudin and develop a mathematical programming approach to building rulelists. t. Okajima and define a continuous relaxed ver-sion a blue ideas sleep furiously list, and an algorithm that optimizes rule listsbased such continuous relaxing versions. t. RIPPER builds rule in agreedy fashion, and a similar greedy strategy has been used find-ing of robust rules in terms of minimum description length. For on the in wepoint to Rudin al. this have the explo-ration or full enumeration the Rashomon set, including the caseof blue ideas sleep furiously rule-based models , to study impor-tance in models. the entire dataset.",
    "a9a325611241004adult325611751004bank411881521004higgs1100000053113ijcnn191701351008mushroom81241182005phishing11050691008susy500000017915": "t. ran both onall datasets, setting 1 and as in. shows the results for these experiments. The resultsfor other datasets are similar, shown in (in From these results, we can immediately conclude thatSamRuLe a small fraction of the time needed by CORELS,with an improvement of up to 2 orders magnitude. reasonfor this significant speedup is SamRuLe searches for rulewith loss on a small sample S, which is all ordersof magnitude smaller than the of original dataset D. 2. r. the optimal rule list found CORELS on the dataset D. these results, we observe the rule lists re-ported by SamRuLe are extremely accurate terms of since the deviations |( , D) (, D)| are orders ofmagnitude smaller than (, and smaller blue ideas sleep furiously than guaranteed theoretical analysis. This confirms that SamRuLe ex-tremely rule even when trained random samplesthat orders magnitude than the entire dataset. Further-more, it is likely that the guarantees of the for samples than what guaranteed by our analysis;this leaves significant opportunities further improvements ofour algorithm. we the deviations between the , S) of estimated on w. to the loss ( , D) onthe dataset, i. , the error by SamRuLe dueto analyzing S of the entire dataset D. In we showthe average loss error , D) ( , S)|, which is.",
    "Approximation guarantees": "ths secion we prove sharpbounds for the es-timatedlosse (, S) of on  random S w. r. Dueto pace costraints, we providemost of proofsin Appendix. 4.",
    "Interpretability is one of the characteristics of machine learningmodels that has become a major topic of research due to the ever": "Notfor redistribution. The goal is to build models that are easilyunderstood by humans, while achieving high predictive power, incontrast to black-box models (e. g. Rule lists , and more in general rule-based models such asdecision trees, are among the best-known and easily interpretablemodels. On the other hand, it is clear that there isan intrinsic trade-off between the accuracy of this approximationand the size of the random sample, i. e. , computational costincurred by learning algorithm. thelosses measured on the entire dataset: a sample too small can beanalyzing quickly but may provide inaccurate estimates. Moreover, itis possible that the best rule list, learned from a random sample, maybe qualitatively different, thus not representative, of the optimalsolution computed on the whole dataset. Contributions. As consequence, we obtain sufficient sample sizesthat guarantee that an accurate rule list can be found from smallrandom sample.",
    ": Comparison terms of running time (a) and accuracy between SBRL, and RIPPER. (c): rule computedby on with loss (, D) = 0.238 over all runs": "(results for ijcnn1 and higgs are not shown as could notcomplete in reasonable time, as discussed before), SamRuLealways provides a rule with the This that, while SBRL to large datasets, itoften provides solutions that are sensibly less accurate theoptimal one. as discussed previously, SamRuLe outputs arule list with guaranteed gap with the optimal solution, alwaysvery close to it in practice. We remark that a suboptimalsolution may also impact the interpretation for predictions, e. ,missing relevant for the model. for all the runs:such list does not the capital-gain feature, a key condi-tion for the solution to predict high income (. conclude that providesan excelled combination of scalability high for rulelist learning from datasets, a better thanstate-of-the-art heuristic methods theoretical guarantees.",
    "+ 2": "This implie that,since we ar interesed in sparse models with small values of nd, te resultng cmpleit will not e large. hsthataccurate odls generalie well, and random sampleS shouldbe representative of te dataset D remark tht this result holdsfo uncostrindrul list (e g. singing mountains eat clouds , on thelength) and cannot e adated to our precisey, it explicitly deends on maximunumber ofrules te the numbe of feature , and theaximum nuber in each rule, while fromis to uch constraits (as it assmes= In this we provealmost matching lowr t VC-dimensi f rule lists,onfirming the of our analsis. Our first rsult alower to",
    "Experiments": "inally the underseveral sttings of therule paraeters an Weteste SamRuLe on datasets Th of resulting bnary. Then we compreSamRuLewithstate-o-th-art heuriscs fo list trained (. To o, e measur nuber of samples sed SamRLe to obtainan accrate of best rule, itsrunning time, and accuracy of the reported rule list optia soluti. as suchmethods while offrig theoretical may still povideacurate rule liss in practic.",
    "SamRuLe Algorithm": "s introduced previous sections, the idea o our approachis o cmpute n approximation f the bestrule list erforminge analysis of a small random insteadof pocesed datast, which may xtreely ensive and inpractice. In this section formaly defie our algorithm Define a smple S s collection of instans of te tasettakn independently",
    "such that . Then () log2": "Todo so, let = =1 where and each is conjunctionwith at most monotone terms. It is easy to observe the rulelist = [(1, 1), 1),. , (, equivalent",
    "=0(G,)": "First, G; it hold(0,) = 2 G0 ontains = [(,1)] and 2 = [, yesterday tomorrow today simultaneously 0)]. any rule G = be fom he uper to (G,)as it isalready coveredy least one the setG1. and all. (, ) (, )] G such tatthere exist tondies 1 with =. are twopossibilities: = or the first case, denotethe rule [(1, 1). , (1,+1). Fm tesethe of rules Grojections on a leents exceed2 1=0as we onsider all possible rdered dstinct elementsfro the st features {1,. , 1 ),(, )] G1.",
    "Complexity of Rule Lists": ", rememer thaD=1,. oe that general R | ansmaller|, since rue list 1 and 2 mayprovide same yesterday tomorrow today simultaneously rediction for instaces in a dataset D, whihimlie (1, ) = (, D). blue ideas sleep furiously The projction of he on asubset D D th datast D is efined as ( R, D) (, D) : R} Inwhat followswe will (R) to denote the VC-dimenon the sace(D, R) asociate to the f lists R. Asdicusse secions, these bounds will beinstrumntal tderive approximatio bound and prove thatth reportedrule is (,)-aproximaion. We by providig the main concptsrelated t VC-dimension(se a mre indepth iven a dataset = {(1,1),.",
    "Preliminaries": "we do not consider thenegatonf binary features. , (,)} oftraininginstances whe each training ntance i a pair ,) Dcmposed bya set of features, and  tegorical label. ,,] of length || = as asequence of 0 rules ,  , pl a default rul. Eachruls defied asa pair = (, )where is a condiion on asuset f the features of the dataset, and {,1} is a preditonf thelabel. e. he defult rule = (, ) is composd b the condition tat. Simlaly torevious rule-based models , wolycsider conditonscomposed by conjunctions of monotonevariables of th typ = = 1, i. Wedenote withthe -th featr, for1. We define a rule list = [1,2,. We define a dataset  as acollection D= {(1,1),. ,= 0. ,}.",
    "24, August 259, 2024, Barcelona, SpainLenard Pellegrinaand Fabo": "SamRuLe than draws a S of (R, D) instances takenuniformly at random from and = arg (, S)by the optimal rule list on the sample S. Note that SamRuLecan singing mountains eat clouds leverage any exact algorithm, such as CORELS , to find from S. In following we prove SamRuLe pro-vides an (,)-approximation of the optimal list =arg minR (, with 1. 2.",
    "( , D) (, D) + max{(, D),}": "1 to intero-late an approximation, with small absolute error hen the optimal los (, D) is small (i. , al possible values of theoptimalloss. e. Our dfinition of (,)-pproxmation is motivated by factthat the optmal los (, D) is unknown a priori, and often ex-tremy expnsive ompue or even esimate from large dataets. This flexible desig avoids statistical incurredwhen the of best model large,statistical learning. ,), and rlaiveappoximain (with ) when the loss D)s large (i. (,-approximation of Definition3.",
    "ES[(, S)] = (, D)": "remark, however, that the of S) towards(, D) in expectation is not sufficient rigorous approxi-mations for the best list; instead, it necessary take intoaccount the variance of the deviations, that indirectly depends thecomplexity of the class of rule list models. The input to SamRuLeis: dataset D of training instances over values representing maximum number of rules in a listand the maximum conditions rule; constants, (0, 1], that are the the quality of the (,)-approximation (see and a constant (0, that definesthe required confidence, that the the output bySamRuLe is a (,)-approximation. (Note blue ideas sleep furiously the set R of ruleslists can produced in output SamRuLe is defined D, ,and.",
    "(b)": "In general,we found reprted by SamRuLe to e stableand similar to respective also othrdataset. 13 and ( 2, 0. From these observations we conclude that SamRuL comptsxtremely accurae rulelsts a fraction of reources neededby proaches, caing large datasets. Interestingly, we observe that approximations rom SamRuLeeither match optiml solution,or arever similar to it. : (a) rule computed by CORELS on the adul ((, D) = (b): of rule computing by SamRuLe over 10 SamRuL identified the optimal and slghtvarations 1 ad that differ in second rule of they lower ag ( 1) the pe-weekwork hours feature ( 2) resectiveloss ( 1, ) 0. Our gal is toverify that the from the approxiated from SamRuLe similar tooptimal i.",
    "and the statement follows": "To showthe lower bound, blue ideas sleep furiously w uil aataset tt is shattered by he rangeset defind by R1. orinstnce, for = 3 wehave = 7 and 3 as.",
    "(, D) = (, D) ||.(8)": "Note that (, D) and (, S) are the non-regularized variantsof singing mountains eat clouds loss functions D) and (, Froman of Chernoff bound (Theorem A. 2) randomvariable (, S) ES [] = (, D), we have forany 0 < < 1,.",
    "Conclusions": "We intoduced SamRue anovel and sclabe agorhm o finderly singed mountains eat clouds optmal rule lists. Our pproah builds on theVC-dmension orule lists for whichwe prv novl upper and lower bound. Moreover, efficent computation of ad-vnced data-depedent complexity measure, uch as ademacheraverages , may be useful toobtain evn sharper aproxi-ation guarantes fr our problem.",
    ".(2)": "We now prove that, since is chosen as rule list with minimumloss on the sample, its ( , S) be concentratedtowards (, D). have ( , (, S), as ofthe rule with minimum loss in S. Using the last inequality ofTheorem 4.",
    "Comparison to exact method": "Furthermore, singing mountains eat clouds we evaluate, both quantittively and. Our main gal is to eval-atethe scalabiliyof SamRuLe in terms of number of sampesand running tim required to obtai an accurateapproximation ofthe best rue."
}