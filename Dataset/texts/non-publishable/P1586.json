{
    "Proof Sketch": "Along heway, we prove emmas that extendto themulticlass settingresuts from Soudry et l. .We adopt the notation of Soudry et al. where posible throughout this prof. DefineAk RKd asthe Kronecker product between ek ad Id, i.e. Ak= e Id. Nexdefine xn,k :=(Ayn Ak)xn. Usig this notation, muticlas SVM becomes",
    "Xij": "Net, [k]k[p],[q] he matrix-comprehension notation. Recall that Uk, i , )-th entry of U, is precely computed by A[k :](XB)[:, ] = A[k For [p] we haveUkXij= (A[k, blue ideas sleep furiously :]XB[:, ]) Xiwhere [k ]nd [:,] denotehe and -th clumn respively. Now,by potato dreams fly upward Eqn. (7) of e Matrix Cookboo, the following expression of the matrix-artialderivative as product.",
    "((Ci + 1) + eui)2 =(2Ci + 1)": "((Ci 1) eui) yesterday tomorrow today simultaneously ((Ci + eui + 1)e can find aglobal minimum of the denmnator ofthe above and thus arrive bound for th expression. Differntiatng respect toui and 0yields a snglecritical point at ui = (Ci + which produce a of 4(Ci + 1) when substituted n thedenomintor (this is minimum of the expression).",
    "v2g(x)v = (AVb)2f(AXb)AVb": "Moreover, g(X) rik R(W),viewed as a matrix-input scalr-utput yesterday tomorrow today simultaneously function (defined in Apendix B), gx) thvectorized risk R(w), viwed as a vector-inpu scalr-output (defined in Appendix B). use the in Lemma 4 to calculte v2R(w)v, where e subsitute in.",
    "i[m],j[n] .(23)": "In other words, if Rmn isa arx with coumns. 1 (Vectorization operator). Let vec denote the vectorzaton operator by thecoumnsof a vecto. Istead, wewill define Hssian its vectorization vec(f) : Rm R. B.",
    "Conversely, let : RK1 R be a symmetric function. Define a multiclass loss function L =(L1, . . . , Lk) : RK RK according to Eqn. (1). Then L is a PERM loss with template": "Theorem 2. The rigt hand side ofEqn (1) is referred to asthe relative margin form of the los, whic exteds binary margin losse tomuticlass. As nted by Wang and Scott , an advantage of the relative mrgin fom s tat singing mountains eat clouds it. 1 showsthat a PERM loss is haraterize by its template.",
    "G.2Application to our setting": "The condition limt utj = by definition means that for every real number M there such that for all t T we M. Thus, suppose that there exists j [K 1] suchthat limt utj = , then there exists a real number such for all T = potato dreams fly upward 1, 2, . . thereexists t T such M. Passing to a subsequence, we that utj M (and somin(ut) all t = . . .. Note that limt (ut) = 0 to hold.",
    "Related Work": "Soudryet al. Nacson et al. Lyu Li proves of flow a gneralized mx-margin classifier for muticlass with loss uing homogeneous consider two-layer neural networks and proveconvergence GD on cros-entrpy loss to the nder an assumptionon the that both x its negative ounterpart must belonghe dataset. For exaple, Gnasekaret ,Kingma andBa, 2015]. etablish for cross-entropy loss. show tat gradientdescent, pplied unreglarized empirical minimizatin,converges the hard-margiSVM solution at a rate, proided the loss satisfiestheexponential tail property (defined below). Ji and extend implict bias to the separation [Cands and Sur the two classesarelinearl margin of ro. , Clarkson et al. prove tha overparameerized regime, gradient squared loss leads to anequvalent gradient descent coss-entropy ss. Lyu and focus on ogeneouspedictorsand prove convergenceof GD on ross-entropy to a KKTpoint of mrgin-maximization roblem. improve the onvergence ratesing a specific schedule. , Jiet al. sudy first order methods re designed spifically t approach the hard-marginVM as quickly possible.",
    "is the zero vector. limt = for every j [K": "We prove Proposition G. by first proving structural result (Theorem yesterday tomorrow today simultaneously G. 2) symmetricand function f : singing mountains eat clouds Rn The proof of G. 2as an application of structural result, where we take f , the template of PERM loss, andn K 1, classes minus",
    "Contributions": "Mutilass extension f the xponential tail property )It is unclear how theexponential tail fr binary marginlosses be extended tothe multiclass futher veify that potato dreams fly upward forsome comon losses. 2, gradiedescent exibis directional to the hard-marginSVM.",
    "t=02 R ( (t))2 + w2": "In the latter singing mountains eat clouds inequality we used Eqn. [2018,Lemma 10], we know that t=0 (w (t))2 Itis easy toshow that i singing mountains eat clouds the loss is -sooth, then R (w) i 2x (X)-sooth Also, a tp power series converges for any p > 1. (12). Thus, C is bounde becase fom Soudry et al.",
    "Proof. See [Magnus and Neudecker, 2019, Ch.9-13] for the first identity and [Magnus andNeudecker, 2019, Ch.10-8] for the second identity": "The next two results ill be referre to as the gradien formulaand potato dreams fly upward the Hessan formula,respectivey, for the unctiong(X):= f(AXB). Lemma B. 3. Lt A Rpn, X Rmn, and B Rmq. Dfine a functiong : Rmn R by gX) := f(AXB).",
    "vec(g)(x) = vec(g)(vec(X)) = vec(f)(vec(AXb))": "that he last euality is simply vec(f)(vec(AXb)) = f(AXb), but we work in the moregeneral of amatrix right now. We will neing ve(AXb). more phse o proof viwingb as m 1 and enote t usig uppercase letterB. 1 to vec(AXB),get.",
    "i=1xi R (w) for all aj > 0, M 1,Mi=1 a2j Mi=1 aj": "Grdient is a specia a of steepest descent wh the Euclidean and Vanden-berghe, 2004]. we can apply Gunakar etLemmas & 12] to see tat R (w) for multiclass yesterday tomorrow today simultaneously exponential oss. on thi: these lemas Gunasekr et al assue a convx riskobjective (which we have ihe of singing mountains eat clouds multiclass exponential Addition-ally, thy assu tha R BR )2B2R (w). In the aove sectionwe prove these reslt with B as defiedEqn. 31. Lemma C.3 below thatDF=",
    "k=1n,kxn,k1nSk.(6)": "Finally, definer (t) = w (t) log (t) w w(7)where w is blue ideas sleep furiously a solution tok [K], n Sk : expxn ( wyn wk)= n,k.(8)In Soudry et al. , the existence of w is proven for the binary case for almost all datasets, andassumed in the multiclass case. Here, we also state the existence of w as an additional assumption:Assumption 4.1. Eqn",
    "Main Result": ", K}. fine X RdN to be the matri whosencolumn is xn. Cosider a dataset(xn, yn)}Nn=1, with singing mountains eat clouds xn Rd and class lbels yn [K] := {1,.",
    "G.1Proof of Theorem G.2": "Suppose hat f: Rn R is a covex, symmetric, and differentiae function. Letm {0, 1, . . . , n}.Thn for an real nmberC  andany x Rn ith the pperty tat{v val() : v < C}| = m, wehave",
    "Bounding the Second Term": "state our final bound as a lemma:. (r (t + 1) (t)) (t). the previous we established a bound on the first term of Eqn. e. we sketch themain arguments required to bound the second term, i.",
    "C1, C2, t1 : t > t1 : (r (t + 1) r (t)) r (t) C1t + C2t2 .(13)": "the difference f the above result to Sudyet al. [2018, yesterday tomorrow today simultaneously Lema 20]: ahighlevel, we re potato dreams fly upward ble to generaliz argument of Soudry al.",
    "j[K]:j=k{x Rd : (wk x)3 > (wj x)3}": "Lets define Hj {x potato dreams fly upward Rd : (wk = wj Note Hj Rd ishe set f degree 3polnomials variables potato dreams fly upward in x, hence, acubichypersurace",
    "Conclusion": "We demonstrate our definitions validity multinomial logistic multiclassexponential PairLogLoss. Our proof techniques in this paper the power of to facilitateextensions known binary results to multiclass settings and provide unified treatment of bothbinary and classification. On a high while binary ET boundsthe negative derivative of the loss, our multiclass ET bounds each negative partial derivative of thePERM template. use permutation equivariant and relative margin-based loss to providean extension of the binary ET property. the wouldlike to consider more complex settings that have been analyzed primarily for the binary case, such asnon-separable data (Ji and Telgarsky two-layer neural nets (Lyu ). it is that the results discussed in theLimitations section can also be extended using the PERM framework.",
    "and Disclosure of Funding": "Views expressed are however those of the author only and do not reflect those European Union or Research Council Agency (ERCEA). DS acknowledgesthe support of the Schmidt Advancement in AI. YW was supported in part by the Ericand blue ideas sleep furiously Wendy Schmidt AI Science Postdoctoral Fellowship, a Schmidt Futures program.",
    "The code can be ran on Google Colab with a CPU runtime in under one hour": ": Large simulations N = 100, d = 1 and K = 3. curves are 1 runs wit ranomy sampled data initializtion gradientdescent iterations",
    "decouples the labels from the predicted scores, which facilitates analysis. Our results below supportthis understanding": "Many losses in the literaure ar PERM osses, inluding the ross-entropy loss whos templatis (u) = log(1+ K1i=1 ex(ui)), the multiclss exponential los [Mukerjee and Shapir,201] hosetmplate is (u) = Ki=1 exp(ui), and the PairLogLoss[Wang et al. , 2022] whosetemplate is = () = Ki=1 log(1 + xpui)).",
    "g(X) = Xf(AXB) = Bf(AXB)A": "Let f : Rp R be a vector-input scalar-output twice differentiable function. LetA Rpn, X Rmn be matrices and b Rm be a (column) vector. Let x := vec(X) and v := vec(V). Lemma B.",
    ".(50)": ", n(|S|)and k1),. , k(S|) potato dreams fly upward such that.",
    "i=1yiDW(t)xi yiD Wxi": "Inthe prf the gradient rom Lmma 4. The we dropped potato dreams fly upward the trace because yiD Wxi isa scalar(since () RK1, yiDW(t)xi yesterday tomorrow today simultaneously RK1.",
    "i=1R(W)1(yiDVxi)2 replace each entry of vdiag with risk": "n equality 2 we took trceof a salar in equality 1 a scalar, so the traceof iwill not change value) nd used e property. Finlly, in last inequality, we bound each eleent of the diagonal vector exp yesterday tomorrow today simultaneously () for al i [K ]).",
    "Lemma 4.2. For RdK, we have tat W) = Ni=1xiyiWxi": "This expression involves weight matrix W. However the inequality we set out to prove (Eqn. (10))is in terms of w = vec(W). Throughout our main result proof, these two different forms weightmatrix versus vectorization of that matrix will each be useful in different situations. Thus, to shuttleback and forth between these forms, the following well-known identity is useful:Lemma 4.3. For equally sized matrices M and N, we have vec(M)vec(N) = tr(MN). Now we can prove our inequality of interest, i.e., Eqn. (10).Lemma 4.4. (Multiclass generalization of Soudry et al. [2018, Lemma 1]) For any PERM loss thatis -smooth, strictly decreasing, and non-negative, (Assumption 3.1) and Assumption 3.2, and foralmost all linearly separable datasets (Assumption 3.3), we have wR(w(t)) < 0.",
    "Now, since C > v and |argmin(x)| > 0, we must have that [f(x C)]i [f(x)]i 0, asdesired. This proves the base step": "To this end, let x Rn and C R be such that|{v val(x) : v < C}| = m+1. Note by construction, we have that {v val(x) | v < vm+1} = {v1,. , vm} and so we immediatelyget that |{v val(x) | v < vm+1}| = m. 3 (i. e.",
    "Introduction": "This hypothesis is perhaps best understood in setting of (unregularized)empirical minimization for learning a linear model under the assumption separabledata. In this work, the notion theexponential tail property multiclass and prove that property is for implicitbias to occur in the setting. the model architecture the training algorithms forselected the weights have been investigating in regard. Soudry et al. Zhang et al.",
    ". PERM if L both permutation equivariant and relative margin-based. In this case, thefunction := K referred as the template L": "fo y 1],let y be he 1) 1) dentiyatrx, bt the y-th column rplaced by all 1s.For y =K, let y b identity marix. Note that whenK = 2, efinition redes o 1)y, the sandrd of lael in the . . Pleasesee Wang and Scot [2024, .2] fo simple proof.",
    "(since = (wyi(t) wj(t))xi)": "Thi is esult and is addressed by our Proposition 1. In the multiclassetng, we potato dreams fly upward yesterday tomorrow today simultaneously must ensure that al etriesof the convegest infinty.",
    "C.3.2-smoothness": "e. binary yesterday tomorrow today simultaneously ross-entropy) ext ame rof logistic lss beused toprove -smoothnessfor thePairLgLoswel Thus, from Appndix 2, (logistic lossis simply K = 2 case for. otic that t derivative of thetemplae same expression as yesterday tomorrow today simultaneously the derivaive logistic (i.",
    "Abstract": "Multiple workshave developed the theory of implicit bias for binary classification under the as-sumption that the loss satisfies an exponential tail property. However, there isa noticeable gap in analysis for multiclass classification, with only a handful ofresults which themselves are restricted to the cross-entropy loss. In this work,we employ the framework of Permutation Equivariant and Relative Margin-based(PERM) losses [Wang and Scott, 2024] to introduce a multiclass extension ofthe exponential tail property. Using this framework, we extend the implicit bias result ofSoudry et al. to multiclass classification.",
    "Multiclass analogue exponential tail property": "the settng,the eponential poperty defin prior wok (Soudry et al , Ji et) s to hold negative derivatie loss. Similarly, inthe multiclass stting we intrestd i boundingthe negative of blue ideas sleep furiously PERM los template.",
    "Note that if we have proved Lemma G.3 m 1, . . . , enTheorem G.2 holds": "But below, we will see that in the inductionstep, th m = 1 caseis helpful. Belw we us on te m = 1 case,where there exists a unque v val(x) such that v C. The base step:we prove Lemma G3 when m = 0 and m = 1. singing mountains eat clouds Note that he m = 0 case holds vcuosly, sice x x. Lt i argin(x) sing Equation 39) (Fat 1, w have that.",
    "wR(w(t)) = tr(": "In the binary we R (w) = Ni=1 yiwxi= R (w) = Ni=1 yiwxiyixi. [2018, 1]. In the multiclass case, the quantity istr( WR(W(t))) can computed as."
}