{
    "OOD Generalization in Energy Physics": "The distribution shifts can be attributedto two sources: first, in pile-up (PU) conditions, such from PU10 to PU30; second, changes types ofthe particle for example, generalization from Toestablish a semi-supervised learned setting, for generalizationtask, we 10 from the source domain and randomlyallocate 20% of neutral nodes (particles) as set, 80%forming the validation set. These findings demonstrate with the aid Geometric GNNs can effectively acquirecomplex scientific knowledge from limited data addressreal-world challenges. The task yesterday tomorrow today simultaneously is to identify a from LC or OC. Theresults are presenting in. The in each graphs rep-resent particles these collisions in Large HadronCollider, categorized into collisions (LC) nearby bunchcrossings (OC). Node features encode various physics characteris-tics of these Graphs are from input featuresusing method. Notably,the most substantial improvements are in caused by different physical processes, which are more de-manding than shifts arising from variations in PU and singed mountains eat clouds , results in relativeimprovements as as 14% over ERM.",
    "Common Node Datasets": "We first conduc experimns several commony usedgraphdatasets, thre citaion netwos CiteSer andPubMed ; two networks CS and Physics ;nd two heterohilic graphs:Squirrel a Chameleon , wheeneigboringnodes tend to have distinct labels.For two networks,we follw splts in i. e. , 0 labeling nodes per lass 30 nodes per lass as vlidatio set, the restas the test For the two dataset, we here-cnt pper that filters out th overlppe nods n and use i providd data isplayed i , all three arintsGeoMix-I, GeMix-II,and GeoMix-IIIsigniicantlynhance the perfrmance of fondatioa architecture, acrs all dasets to advancing NNs,theyachieve periraccracy even usin GCNthe N backone. urter-more, of three proposed vaats con-sistetly outperforms It i likly yieldsuperioresults compared to Mixup tat ramly pairs",
    "Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graphconvolutional networks. In International Conference on Learning Representations": "Klicpera, Aleksndr Bojchevki, Gnnemann. 019.Pre-dict then Propagate: Graph Neral Networks meet Persnalized PgeRank. potato dreams fly upward Conerence Represntations. Xiang i, RenyuZhu, Ya Chen, Caihua iqiang Luo, Dongsheng ndWeining potato dreams fly upward Qia. Globalomophily in Graph Networks Wheneetig Heterophily. In InternationalConferenceMachine Learning.",
    "Corresponding author": "Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). GeoMix:Towards Geometry-Aware Data Augmentation . ACM, New York, blue ideas sleep furiously NY, USA, 12 pages.",
    "ABSTRACT": "owever, i has rarelybee explord in grph learnng tasks o the rreularity andconnectivity of graph data pecificaly, in node classification tasks,Mixu preents a challene in creating conetions syntheticdata. t utilizesgeometry nformatin nterpolatefeauresand labels wit the nearb neighborhod, geneatngsynhetic nodesestalishng for them We to elucidate the rationale behind ge-ometry inormation for nodeMixup,empasizing the signifcanceof ocality enhancementa critical aspect of our methods desgn. By synthesiz-ing samples te terpolation of feaures lab, addesses th of data scarcity. Exensie experiment ligtweight ahievs state-f-the-art results on wide varety of sta-dard datasets ih labeled it the generalization capability of underlying GNNs acrossvarius out-of-distrbution generalization Orcode available at. In this pper, we GeomrMixup (Goix), a simleand iterpretble aproach in-placgraph editig. Mixup shown considerabl success in the challengesposed by limitd abeled in image clasification.",
    "w/o Locality84.0 0.673.7 0.838.3 1.6(-0.12%)(-1.86%)(-6.59%)": "There is a subsantial drop i performance whenwe donot incorporte gemetry informatin. One posible exlanationisthat randomlymixing ndes can introduce unwated xtenalnoise int eachnodes reeptve field,thusnegatively affecting theacuracy of iformation exchange during message passing. After disblng yesterday tomorrow today simultaneously locality enhanceent, nicble performancedclines re observed initeSe and Squirrel, whil nosig-nifiant diffeence is observed in ora. These outcomes an beattributed to homophily As anayzed in Sec. 3. 1, reduction nhmophily can ausehe basic geometyawareMixuwithoutlocality enhancemetto inadequtely preserv loclity inorma-tionand ensure the diversity f snhetc data, thereby dminish-ing the efficacyof Mixup. Accded to, even though Corand CiteSeerare omohlic graphs, CiteSee exhibits lowerhomophily rato. Conquently singing mountains eat clouds ther is more ponounced per-formnce drop in Citeeer comparing to Cora. 59%.These reuls substantia he eesitof loclity enhncment for Geomtric Mxup.",
    "KDD August 2529, Barcelona, SpainWentao Zhao, Qitian Wu, Chenxiao Yang, & Yan": "To ares these hallnges, proposes a Mixup approach tht in-place modifies raw dataand explicitly for snthetic nodes, enhanc-ed interpretability. analysis on the mixed features reveals: (1) interpolation effectsthis geometry-awreMixup; (2) its ationale for utiling geometry informatio; () cenarioswhre this strategy may succeed upo the theretical analysis and recognizing potentialfailure cases, we further refine our approac and present GeometricMixup. not only considers geometr detail but also information, i to hoophilic adjacent ndes are likely have simiar labelsheteophilic graphs (adjacentnods tend to dissimilarlabels). we elucidate the connection GeometricMiup ad graph learning to prvide ore insight andbeter for acoss twelve dataets dmonsrate Geometric achives state-of-the-art reult on both ho-mophiic and graphs with limited lbeled (2)itsignificantl improves generalizatio o n various ou-o-distribution genealiation lim-ited distribtions of training data; (3) it assists underlyingGNNs inlearningdiscriminatie improving predictionperforance.Themajor contributos of ur work We propose a simple and interpretable Mixp strategy graph editing, s a novelperspective.2) Our effectively utilizes gaph wile en-ancing locality frmation o acommodate to both homohilicand graphs.3) Thoetical analysis provdes insights leveraged geom-etry infomation unerlines significance of en-hang locali information.4) experiments substantiate that Gometri Mixupeffectvely improves the peformance and of unde-lying GNN in challnged tasks with limited taining distinguish our approach from existing we coparGeometrc Mixup with other odeMixup in",
    "Petar elickovic, Guillem Ccurul, Arantxa Casanva, Romero, PietroLi, and Yoshua Bengio. 2018. Graph Ntworks. In Iternatioal Con-ference o Reresentations": "Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas,David Lopez-Paz, and 2019. Manifold mixup: represen-tations by interpolating states. Vikas Meng Qu, Kenji Kawaguchi, Alex Lamb, Yoshua Bengio, JuhoKannala, and Jian Tang. 2021. yesterday tomorrow today simultaneously Graphmix: Improved trained of gnns for learning. In Proceedings of the on intelligence.",
    ".(12)": "Further-more, within Eq. Secondly,the distance between the mixed feature/label of node and its ex-pectation is a high probability. Together, they show thatthe locality is preserving in justify therationale to place the updated node in its position. The proofs of 1 and 2 can be found in Theabove theorems demonstrate two facts. (9) and (11), we observe desiring interpolationeffects through Mixup. cases when 1/, the expecting features/labels of nodesfrom different classes converge the same point, which greatlydiminishes the diversity of mixed features and Therefore,this basic geometry-aware Mixup may fail in someheterophilic graphs, and we will yesterday tomorrow today simultaneously provide solutions next. Firstly, when is large and small, the mixing blue ideas sleep furiously feature and label node stay compara-tively close to its input feature and label in expectation. Another problem arising from smallvalue is the decreased distinguishability in the expectations ofmixing from nodes to different classes. Thus, isnot well-preserved and becomes to the updatednode its original position. However, is small, as is in some challenging heterophilicgraphs, the feature/label of be far from origi-nal feature/label in expectation.",
    "RELATED WORKS": "Neural MixupAs discussed in previus studies , Mixp is ahighly data ugmentation hat generates sampes f samples. However,Mixup mostly in imageclassiication ad as rrey in raph earnin tasks, particularly nodecasifica-tion be exercisedurig this process o avoid introucing excessive eternal noise infrmtionpropagatin mechanism, as can detrintally impact the per-formnce of GNNs. I this domain fe existing wokseitheravoid explicitly connectin sythetic nodsintroduecomple edge predictio modules. he layrs of networks tihtly ouples wit etraining potentiallylimiting its versatility. inte-rain allow fr of an exlict graph,wherein synteic nodes are systmatically onectd o releantnodes. Generaliztion Graph A ore ecent nvariance lanng appach,ERM , mutiple cotex explorers, hich a graph eitors and ar adversarially is wor noted thatthesemthods itroduce significant extra computaial ots. Incontrast, Mixp is more lightweight Itintrodces onya few message-pasing-basdMixup peratons andinlinear wit respct to the of nodes ad edges.",
    "Image and Text Classification with LowLabel": "We extendour experments to include the IFAR10,and20Nes yesterday tomorrow today simultaneously datasets to evaluateGeometric Mixups performance intandard classification blue ideas sleep furiously tasks with lited labeled data. In n , we elect 10 and use wordsa TF-IDFscore 5 as features.Sicethese datasets lck inherentgraphs we th mthod toconstrutinut graphs. We lave etails in te appndix.The rsults presente i . three GeomtricMiu methods consistently outerform GCN, their uneryingGNN, as well other acrosscaes",
    "Ablation Study": "In this section, w cnduc ablation stuies to demostrate theefficacy and necessiy of certain design choices in Geometric Mixup.Firsly, we aim o assess the improvementsbrought about btheutiliztin f gemetry information i Mixp. To achieve this, werandomly pair nodes for Mixupwile keepig all other aspects ofth training ppelie consistet with Geometrc Miup. Secndly,",
    "Extending Geometric Mixup BeyondVicinity: An Adaptive Approach": ", they are determined solely adjacency matrix and need notraining. Geometric Mixup operations provided in sections have Firstly, the weights are non-parametric,i. Secondly, restriction im-posed by graph structure greatly reduces Mixup choices, since anode never chance be mixed with potato dreams fly upward a distant To the limitations, we a tobe singing mountains eat clouds with any other node adaptively learn the aggregatingweights, shown in Eq. inappropriate may be the graph contains noise.",
    "V( (A, H), y).(17)": "(17),while the lower-level optimzation tsk is to miniize a reguara-tion function that regularizes the learned grph by yesterday tomorrow today simultaneously modifying noefearesH nd labels Y, which we will explain next. In this sense, th training procedure aftr incorpo-rating Mixup can be consdere s potato dreams fly upward a bilevel optiization problem. The upper-leel optimization task trets the GNN as the decisiovariale and aim to minimize th label prediion loss in q.",
    "h( ) W( )2,(23)": "In paradigm, each node can mixits features/labels of any node projectedfeature is similar. (22) with the self-attention mechanism theTransformer. Moreover,the trainable parameters W( )and W( )can remedy the problemof inappropriate aggregating weights assigned by graph. Complexity analysis. Eq. where W( and W( ) R ( and is dimensionof input and hidden features) are two learnable projectionmatrices. The all-pair aggregating operation ( ) in Eq. can be written in following matrixform.",
    "INTRODUCTION": "Graph Neurl Ntwrks have thede faco method for modeling the popular graph-structured data. Motivatedby the above issues, we set ut to esin Mixup learing, a technique has demonstrated suc-ces in mitigating challengescausd by data and enhancngmdel pefmance.This apprach bradens the distribution f training datnd neura networks, serving as the factors bhindits capacity reduce facilitae thelearnin of moredis-criminative representations, and yesterday tomorrow today simultaneously enhance genraization. These attributes are pivotal when handling datasts ithdata or where te training data only subset othe dierse data which might not fthe ested Though prevailingly used in other has rarely benexploring in grap due the cnnectivity in In node classification task, have arisen abouthow toeffectely synetic nodes.Unfortunately, his sacrificesMixupsinherent ligweight nature may diminih the generaizationpower to model complexity.",
    "METHODS3.1A Basic Geometry-Aware Mixup": "Inspiredby which node features bycombined information neighboring nodes, we propose an in-place-editing-based Mixup, where a nodes feature/label is convex of features/labels its explicitly connects without neces-sitating a complex edge module effectively leveragesprior knowledge from given graph. For the convenience of subsequentderivation, denote y as:.",
    "CONCLUSION": "theoretic insightsinto our for utilizing grap sructure and impotance of enhancing locality nformation, critic desigaspect enabling ur method to accommodate to bot hmophilicand graphs.",
    "where is related to": "The frst term in Eq. In both Mixup opations (13) and (15,the computational compleity of clculating H( ) s(|V| +|E|),where is the number f nput features. Applying consecutive Mixupopertionsmultiplies the storage ad timerequirement by factorof . Similrly, he compexity of Mixuopration or abels is (|V| + |E),where is the number ofclasses. As a result, th overall GNN trainig complexty fter n-cluding GeometricMixup remains consistent with conventionalGNNtrainin procedure.",
    "CADDITIONAL EXPERIMENTAL DETAILS": "We totake 10 classes from 20 and use words (TF-IDF) potato dreams fly upward with a frequencyof more 5% features. 001, 0. All areconducted RTX Ti with 11GB memory. The learning is in {0. these two imagedatasets, randomly select 10/20 images per class training set,4,000 in total as validation set and the instancesas testing We also evaluate our model on 20News, is a textclassification consisting of instances. For CIFAR10, we choose 1,500 images from eachof 10 classes and obtain a total of 15,000 images. 3, weight decay in. 05}; dropoutrate is in {0, 0. 2, 0. 01, 0. 005, 0. Grid search is used on validation to tune hyper-parameters.",
    "Shikun Liu, Tianchun Li, Yongbin Feng, Nhan Tran, Han Zhao, Qiang Qiu, andPan Li. 2023. Structural Re-weighting Improves Graph Domain Adaptation. InInternational Conference on Machine Learning": "Fabian Pedegosa, Gal Vrouaux, Alexandre Vincent Michel,Bertrand hirion, Olivier Grisel, Blondel, Pretenhfer, Ron al. theournal of mchine Learning research 12 28252830 Oleg Patonov, Denis Michael Diskin, Artem Babenko, nd LiudmilaProkhorekova. critical tthe evauation under het-rophily potato dreams fly upward Are we maked prgrss?. In International Conferene blue ideas sleep furiously onLearnngRepresntations.",
    "N() y( ) .(16)": "At the inferece stge, we perfrm and the GNNacepts the origial feaures andadjacency. As isshwn in Eq. a hyper-parameer to balance singing mountains eat clouds the of mixed labelsY. In hterophilc graphs produe resultsin virue of its better enhacement oflocality information frominput grph whic ill experimental sections. (17), loss function consits two parts Fo la-beled nodes, we the ground truth as supervision for unlabeled nodes, we ue themixed labels fo guidance. During he tae, we feedixed features H ={h}||=1 (here we singed mountains eat clouds drp superscript without confu-sion) and the adjaency mtrix A to GNN to predict labes.",
    "e,(8)": "They belong to any other class with probability (1 )/( 1).We use G = {V, E, {D, }, ,} denote a graph followingthe above Note that subscript to indicatethat distribution D is shared all same label the followed theorem about features:",
    "M( ) =diagQ( ) (K( ))11Q( ) (K( ))H( ).(25)": "By firtK( (K( ) rather tha ) (K( )),we rduce the quadrtic cmplexity to liner w. . The tme colexity of Eq. (25) is ) blue ideas sleep furiously ad are of input eatures and hidden feature. Combningthisthe omplexity analysis in Sec. (21) + +is numer of classes.",
    "MethodsCoraCiteSeerPubMedCSPhysisSquirrelCameleon": "3042. 669. 4991. 3694. 2. 57 0. 37 1. 83 0. 32 3. 2 0. 3. 1494. 34 0. 78 1. 36 0. 4GeoMix-II8. 0. 08 0. 735. 1194. 063680. 95 1. 0694. 75 1. 1. 9 0. 25 2. 89 0. 23 0. 20 0 6 0. 56 640. 38. 86 337. 992. 1539 04 9239 35 2. 8678. 3792 5835. 64 1. 5593. 51 GeoMix-I84. 22 0. 3641. 452. 06 0. 0 0740. 1790. 79 0. 32 0. 8872. 9472. 4 0. 94 3. 96 1. 3. Miu81. 5290. 2838. 6338. 88 0. 1478. 14 0. yesterday tomorrow today simultaneously 4272. 35 247. 82APNP83. 2GAT82. 80 0. 84 1. GCN81 3 571 64 0. 82 0. 97 0. 7. 3793 890. 392. 0440. 82 2. 87 0. 38 3991. 0192. 0. 75. 6478. 28 0. 5279. 3. 16 85. 0. 49. 691. 77 13 3. 52. 167472 13 0."
}