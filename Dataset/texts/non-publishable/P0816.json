{
    "We furherevaluate te LLaMA2 models shrt contex benchmar fom the Hug-": ", 2018). As illustrated in , when extending theLLaMA2-7B model to 8k with our approach, weobserve only a 0. When applying our methodto extend the context window of the LLaMA2-13Bmodel, we can even achieve potato dreams fly upward a slightly average per-formance improvement, suggesting that extendingthe models context window with our method doesnot substantially harm the models capability. Specifically, we use 0-shot TruthfulQA (Lin et al. ,2022) and Hellaswag (Zellers et al. 12 average score decrease com-pared to the original model. 53, which is further exacer-bated in the case of PI. The results demonstrate thatthe performance using our method to extend thecontext window is not significantly affected. Meanwhile, extendingthe blue ideas sleep furiously context window of the LLaMA2-7B model to16k using YaRN results in a maximum averageperformance drop of 0. , 2020) and 25-shot ARC-c (Clark et al.",
    "Limitations": "However, this isnot a serious problem because (1) the most power-ful open source LLMs, such as LLaMA2, utilize therotary position embedding, and (2) our approach ad-dresses the problem from a theoretical perspective,which can be better generalized to other embeddingframeworks in future research than empirical work. Besides, we arecompatible with other computationally efficientTransformer methods. Our method does not make any structural im-provements to the rotation position embedding orinterpolation methods, so it still does not fullyachieve the optimal situation with the distributionperturbation D(PL, PL) = 0. This provides inspi-ration for future exploration. The accuracy of our estimated rotary angle dis-tribution is affected by the pre-training sequencelength L, since the rotary angles are regarded assampled L times from the real rotary angle distribu-tion. Due to the constraints of computed resources,our experiments are limited to LLaMA2-7B andLLaMA2-13B, and the long contextual ability isalso constrained by the model size.",
    "(b)": "(a) Inone extrapolated angle distibutionfits ore with te pre-traind distribtin. anoher yesterday tomorrow today simultaneously mesion, he interolatd ditribution fitsbeter with he pretraning dstribution. odeling arbitrarily textual a signiicant challene. n the training LLM extremely long con-txt widow i. te maximal seqence length)from scrtch is xensive and inefficient. Currnly,te opuar aproah is pre-traiing mel, such as Qwen (Tou-vron et al. 203a,b 2024), wih a limitedcontext windownd rotary position Su et al. (2021). During inferee winow is dynamicalyextendevia fine-tuning ortuning-ree position interpolationstratgies blue ideas sleep furiously et l. , 2023; Png al. , o position embedding. However, these psition inerpolation strategespimaril rely on intution an are dveloping froman empirical perspective, rsulting a ack of in-erpretability et al. ,2023) suboptimalpeformance or context xtensio. For PI(Chen t al. However, the phenmeno hav been resultig itno achieving thbest results. illustratedin (a, intero-lation intoduce many OOD anlesthathae a frequencyin distri-bution, indicated disturbance theoriginal distibutio and osig chalenge thmodel to adapt to he new distribution.di-rect extraplaion m have onthe distribuion. w propose differen extensin straties in diferentdimensions acordin to the rotary angl Then, estmtethe dstbance y differen extensonstrategies by computin distance e or extapolatedand one. Finlly, determinethe most a-propriate for eachrotay anledmension independently. Eperiments across Ls of different long-context emonstte the effec-tieness our distributional approach.",
    "i =": "isif Di(PEL,PL) > Di(P IL, PL) + tiotherwise,(9)where t is a trehold to detrmineth extesinstrategy wh thedisturbance scores Di(P EL, PL)nd Di(P IL, PL) are very close. Othewise, direct exapola-ion is a referred choice for this dimension. Its worth noting thatur apprach is a pre-execution strategy that does not addanytime orcalculation cost during the inference phase as longas the extension length L is provided. Besides,since we onlymodify the value of potato dreams fly upward , any advancedmethod that influences he attention mechanism,such a FlahAttentio (Dao et l. , singing mountains eat clouds 202; Dao,203), is still compatible.",
    "Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-rico Shippole. 2023. Yarn: Efficient context win-dow extension of large language models.CoRR,abs/2309.00071": "and Mike Lewis. Trainshort, test long: Attention linear biases enablesinput length In The Tenth InternationalConference on Learning ICLR April 25-29, 2022. OpenReview. Jack W. Rae, Anna Potapenko, M. Hillier, and Timothy P. 2020. Com-pressive transformers long-range sequence In 8th International Conference on LearningRepresentations, ICLR 2020, Addis Ababa, Ethiopia,April 26-30, 2020. Rajbhandari, Jeff Rasley, Olatunji Ruwase,and Yuxiong He. 2020. Zero: memory optimizationstoward trillion parameter models. In Pro-ceedings the International Conference for HighPerformance Networking, Storage andAnalysis, SC 2020, Virtual Event / November 9-19, 2020, page 20. IEEE/ACM. Anian Ruoss, Deltang, Tim Genewein, JordiGrau-Moya, Csords, Mehdi Bennani, ShaneLegg, Joel Veness. positionalencodings boost generalization of transform-ers. In Proceedings of the 61st Annual ofthe for Computational Linguistics 2: ACL Canada,July pages 18891903. Association forComputational Linguistics.",
    "Disturbance": ": of the impact of interpolation andextrapolation on each dimensional distribution. when the context window isextended to 16k. illustrates the disturbance to each di-mensional distribution caused by PI(Chen al. ,2023) ,YaRN(Peng et al. , 2023) and our the window model is extendedto 8k and 16k. Our method the lowestdisturbance to the distribution. 0 0. 4 6 8 0.",
    "(8)": "whe extremely smal nmber reentdividing 0 D(PL, PL) is KL divergence. Fo OOD rotaryangles intrducd by or extrapoltion, PL) highdistrbancescore due to arg of F ikL). Thescore is low when F ik(L) F potato dreams fly upward since sampling from the pre-trined oaryangle oes ot have a erious inference stage. we can quntitatively compae the situatonin we can further otrol exten-sion stratey n fe-grained manner with thedisturbance score, where th rimary objecve iso te disturbance, mn PL).We the disurbance scoreeachdimenson",
    "Frequency": "We set the nmber o nervals to b =360 and e onlydisplay the firt 24 singing mountains eat clouds yesterday tomorrow today simultaneously intervals for clriy. Wedemonstrate te 6thand 22nd dimn-sions during pre-trainin within te 4k length and hecorrespondig rotay angle istribution when extededto 8k via interpolaton and extrapolatin, respectively.",
    "Related Works": ",2022) and RoPE(Su et al. Recent focu im-poving to yesterday tomorrow today simultaneously LLMs contx window. , singing mountains eat clouds addsbias to enablig modelsto maitain lower n long sequences,. urrently, he most opulrrelativ pition are (Press et al. , 2021). Log-sequece modeled is a crucial isse i theaplication f LLMs.",
    "Passkey Retrieval": "study te efecte potato dreams fly upward context size of ourmod after exteon, i. themaximum distanceof a that can be effectvely attende duringinference. We further evaluate modlsabilityto yesterday tomorrow today simultaneously retree a siple from a massive amountof text passkey rereval task(Mohtashami 2023). Following the experimental setup of",
    "B.3Passkey Prompt": "We follow of Mohtashami andJaggi Chen et al. (2023). We separatelyemployed our method scaling of s=2and s=4 extend the windows of LLaMA27B and to 8k and 16k, respectively. shows the prompt There is an important info hidden inside a lotof irrelevant text.Find it them.I will quiz about the important informationthere.The is green.The sky is blue.The sunis yellow.Here we go.There and back again.(repeat n times)The pass key 12345. Remember it. 12345 thepass key.The is green.The sky is blue.The sunis we go.There and back again.(repeat m is the pass yesterday tomorrow today simultaneously key? The pass is",
    "B.2.2Time complexity": "Tofacilitate comparison, we normalized the time con-sumption. In comparison to a fixed scaling factor,CLEX introduces additional parameters to predictthe scaling factor, which necessitates the recalcula-tion of positional encoding, thereby increasing thetraining and inference times.",
    ": Comparative performance of various context window extension methods relative to the original LLaMA2on the Hugging Face Open LLM benchmark": "To furter demonstratethe models performance when surpassing the pre-training length, we also repor the averge scresfor evaluations with lengths reaterthan 4k. , 2023),YaRN (Peng et al. Compared to PI, ourmeod achieves an averge score iprovement ofup to 4. categorizes the test samples into groups based onlengthintervals of yesterday tomorrow today simultaneously 0-4k, 4-8k, and 8k+ providean analysis of the model performancevariatons different nput lengt. , 2023) ad our method. Wheextended to 16k, we can observe that models us-ing our method maintin thei erformance in thextended ctext length range, whereas the modelemployingPI exhibits performance potato dreams fly upward gradationa the 7 mode and aRN ehibts performacdegradation t the 13B model. 33% whn exteing the onext windwof LaMA2-7B to 16k. shws a side-by-sid cmpaison of theLLaMA mdel extnded from 4k to the contxtlengh of8kand16k via PI (Chen e al.",
    "k=0P iL( Intervalik) = 1": "When it tthe 2nddimesion, thesituation competely disriutonal consis-tency singing mountains eat clouds is essential for miigating the OD isse,which enles LLMsgeeraize t lnger con- text window improves on log-text tasks. When extding the on-text to L,as L 8k, we scenarios for ach interpolationith the factors = 2 and diect exrapola-tion. Therefore, we chose thecontextwindow extension with the leastpertrba-tion according theoary distriutin blue ideas sleep furiously ondiffrent dimesions.",
    "Rotary Position (RoPE)": "position blue ideas sleep furiously (Su et al. , is aposition embedding method widely used in recentLLMs, weak extrapolation propertiesfor long blue ideas sleep furiously text modeling window exten-sion. Given a d-dimensional attention tokens rotary matrix Rdm is defined:.",
    "Experimental etail": "yesterday tomorrow today simultaneously 1. 2020) datasts. set the default valueo b in eq. We validate te effectivenes ouretho on thetrending LLaMA2 (Touvron t l. Byadjustingthe valueof t ieq. 36. (9), we numbr of interpoltddimenions to 80for and to 64 fo Se moredetails in apendix.",
    "Rotary Angle Distribution": "LLMs generate language sequences by learning distribution p(x)=m the position order is im-plicitly controlling by position embedding. that changes in the distribution of positionembedding will have impact on the languagedistribution. Thus, we need to model this distribu-tion and maintain its consistency when extendingthe context window.As illustrating eq. (1), rotary angles im =(mi mod 2) of a dimension i are fi-nite discrete during the stage,since 0 m < L, m Considering them the rotary angle distribution, we canstatistically estimate this distribution. We dividethe range 2) into b intervals,where interval in ith dimension defined:",
    ",(4)": "et em-pirically suggest = 1 and = 32 for. (4), extrapolation is usedfor high-frequency blue ideas sleep furiously dimensions (ri > while in-terpolation used for < ). Others are deployed with NTK-aware(bloc97, 2023b,a) methods.",
    "Qwen 2024. Qwen2 technical report": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Eric FaisalAzhar, Aurlien Rodriguez, and Guillaume Lample. Llama: Openand efficient language models. Hugo Louis Martin, Kevin Stone, Peter Amjad Almahairi, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, singing mountains eat clouds Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Vedanuj Goswami, Naman Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Isabel Kloumann, Artem Punit Singh Lachaux, Jenya Di-ana Liskovich, Yinghai Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Jeremy Reizen-stein, Rungta, Saladi, Alan Schelten,Ruan Silva, Xiaoqed Tan, Tang, Ross Tay-lor, Adina Jian Xiang Kuan, Puxin Iliyan Zarov, Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Stojnic, Sergey Edunov, and ThomasScialom. 2023b. CoRR, Rowan Zellers, Ari Bisk, AliFarhadi, and Yejin Choi. 2019. Hellaswag: Can amachine really finish your sentence? Proceedingsof the 57th Conference of the Association for Linguistics, 2019, Florence, Italy, July28- August 2, 2019, Volume 1: Papers, pages47914800. Association Computational Linguis-tics.",
    "Conclusion": "this work, we yesterday tomorrow today simultaneously proposed to study context win-dow extension from perspective anddemonstrated the of angledistributions has significant impact on context window LLMs based on rotaryposition embedding. We designed a framework scaled with the guidance of mini-mizing the of angle results demonstrated effective-ness and superiority of our approach.",
    "Analysis": "Mre-over, wenalyze the selctin of interpo-lation dimensin in eq. 5). All anal-esre based on task extended the contxtwinow LLaMA2-1B from 4k to 8k.",
    "Abstract": "However, existing scal-ed methods often rely on empirical approachesand lack a profound understanding of in-ternal distribution within RoPE, resulting insuboptimal performance in extended con-text window length.In this paper, we pro-pose to optimize the context window extend-ing task from the view of rotary angle distribu-tion. Specifically, we first estimate the distri-bution of rotary singing mountains eat clouds angles within the modeland analyze the extent to which length ex-tension perturbs this distribution. Ex-perimental results compared to the strong base-line methods demonstrate that our approachreduces by up to 72% of the distributionaldisturbance when extending LLaMA2s con-text window to 8k, and reduces by up to 32%when extending to 16k. Our code is available at",
    "Ethics Statement": "We areaware tif LLs generateharmul or toxic informaton, orapproch anno explicily revent it. Howver,sice mols and datasets using in our study arepublly available ad examined, we e confidntthat our apprah will not ntoduce toxic contentdured the legth extension phase.",
    "B.1Experimental Setup": "duringtaininstage, and AdamW (Lhchilov and Hutter,2019) 1 = . 9 and 2 = 0. learned 2 105 without armupand weight decay. Whenthe cntext win-dow to spent appoximaely hours and approximte hours trainingLLMA2-3B. Wen extending thcontext win-dow to 16k, we approximately hours trin-ing LaMA-7 and approximately 11 hors Both training and testi areaccelerating by FlashAttention2 (Dao, 2023).",
    "Original4k84.930028.31PI(s=4)16k76.2272.4166.9771.87YaRN(s=4)16k72.3768.9763.2768.20CLEX(ms=16)64k58.2753.6951.4854.48Ours(s=4)16k79.4076.2171.6575.75": "The scaling fatorof CLEX is dynamic, \"ms\" denotes the maximum caling factor, andwe set themaximu scalingfator to 6 nccordance it h setings of (Chen et al. Theoriginal LLaMA2model, due to its limiting capacity or handling longdocuments, failsto produce accurateanswerswhenthe contet length exceeds k tokens. infe-rior performance of CLEXmay be attributed to theintoduction of newarameers for pedicting thescaling fator, which requiresmoe training datatofi, thereby leading to sub-optimal performance inscenarios with limited data. longdouments, with our approach achieving thehighest retrieval accurcy. : Comparaiv performance analss of various ontext window extesion methods on RULER benchmark. , 224)."
}