{
    "AQ Jiang, A Sablayrolles, A Mensch, C Bamford,DS Chaplot, D de las Casas, F Bressand, G Lengyel,G Lample, L Saulnier, et al. 2023. Mistral 7b (2023).arXiv preprint arXiv:2310.06825": "I Procee-ingsof yesterday tomorrow today simultaneously 2022 Confernce oEmpirical Methds inNtural LanguageProcessing, ages 117521176Abu Dabi, Uited Arab Emirates. Aoiation frComputational Linguistics. 2023. In Findngs ofhe Asscition for omputaionl in-guistics ACL 2023, pages 1111115, Toronto,Canda. Associationfor Computational Linguistics.Xio Liu, Yansong Feng, and Kai-Wei Cang. In Proceedings of the 2024 Conferene fthe North merican Chapte of the Association forComputtional Linguistics yesterday tomorrow today simultaneously Human Lnguage Teh-nologie (Vlume 1: Long Papers), paes 5225302. Yinhan Liu, Myle Ott,Naman Goyal, Jingfei u, Man-dr Joshi, Dnqi Ch Omer Levy, Mie Lewis,Luke Zettlemoyer,nd Veselin Stoyano.Roberta: A obustl optimized bert pretriing ap-proach. arXiv preprntarXiv1907. 11692. In Proceedingsof the 5th Anal Me-ing of Association r Cmputational Linguistics,pages1869181 Online. 2023. Tanmay Parekh, I-Hun Hsu, Kuan-Hao Huang, Ki-Wei Chang, and Nanyun Peng 2024a.sscatio fr ComputationalLinguistics. Tanmay Parekh, nh Mac, iaui Yu, Yuxuan Dong,Syed Shahriar, Bonnie Liu, EricYang, KuanHaoHuag, ei Wa, Nanyun Peng, and Kai-WeiChang. Eendetectin fro soial mediafr epdemicprediction.Associaionfor Computational Linguistics.",
    "AQUDSELECT Implementation Details": "instruction-tune QUD in formatof . (2023), we ap-ply yesterday tomorrow today simultaneously LORA (low-rank adaptation, Hu et al. (2021))with learning rate 2e 5, lorarank = 256, yesterday tomorrow today simultaneously = 256. During inference, sampleQUD with k and temperature1",
    "Abstract": "Theresulting structureis required conformto hereticallike thequstionanswred),making QUD task. Prvi-ous orks onstruct QUD parsers i a pipelinedmanner (i. However,thee prsrs lacka holistic view of he task andcan satisfy all the criteria. Under is a dis-course framework that uses impliit singing mountains eat clouds quesinsto reveal discourse elationships betwn I QUD eac setence aan answer to aquestion triggeredby an anhor sentence in context. e. instrcion-tunng,we tainodels to simultanously predictthe and generate blue ideas sleep furiously the assciatd xpicitly incorprate criteria, we selective decoding of QUD candidaes durin infeence, folowedbyselecting the best one with scorrs. eect the tgersentc in cn-textgenerate the question). Our mehod outperfoms the odel 9% in huan 4% autoatic demonsrt-ing our famework.",
    "Arndt Riester. 2019. Constructing qud trees. In Ques-tions in discourse, pages 164193. Brill": "Informtionstcture in lesser-esribedlaguges: Sudiesinprosody and syntax, ages 403443. 2018. Annotation uidlines for questions unerdiscssion and iformtion structre. rndt Riester, Lisa Bruntti, and Kordula Kuthy. Arndt Riester, Amalia Cae blue ideas sleep furiously Npoles, potato dreams fly upward nd Jet Hoek.",
    "Selective Decoding": ". si1} and answer sentence si,we saml multiple chor sentnce questoncandidtes by selectiveybeam-searchwith a wide beam wile decoding.Fllowingprior work(De Kuthyet al., 018; Benz andJasinskaja, 2017; Wu et al., 2023, we every answer sentence has a correspondingqustion. Thisencourages in pediction ofanchor sentences questions.e m criteria {c1, . . citerion asignsscore to a, and overall scor isth of all criteria mj=1(cj(a, q)). Criteria Scorers.We consider the three prin-ciples potato dreams fly upward of QUD as criteria: answer-ompatibility,givenness, and anchor We ad scorers or each oftem.Answer potato dreams fly upward Compatibility: criterin thatthe question q be answerable by theanwer sentence si. We regard a naurl inference (NLI) task, andusethe probbilitytha si measuring by off-the-shelf (bart-rge-mnl as the Ths ealuates if the only consists information frm context.A ideal queston be naurally invoked cotext concepts tha appear out air. regard the maximum phase of q asits focus fq, and lemmas Lfq and La of alcntent ords in fq and a. relevance score as |Lq La|/|Lq.",
    "Pipeline52.515.032.553.828.717.550.032.517.552.1Mistral-7B67.015.417.660.323.616.158.629.012.462.0+ QUDSELECT67.120.012.977.620.02.468.224.77.171.0": "Avg. indicates the averageratioof idealQUDs (the firt option of eachcritrion). We abreviate DirectAnswer as Dir. , Indirect Answer as Indir. Ans. ,Answer Leakeae s As. Leak. and GroundedsG",
    "Weprovide snippet used in the casestuyin . The article from the dataset.We alsoprovide generted by other mod-ls in": "1. U.S exports of uclear material cannot be aequatel tracefrom cuntry to country, accoding to a congressional report.2. Scarely a day oes by without areport of new black marketdeal, said Sen. ohn Glenn in a statment reacting to report.3. Given the staggeing amount of nucear materials we hveexported, it could only e a matter of time before some of hisdeadly contraband proves tobe of U.S. origin.4. As chairman f Senate Committee on Governmntal Affairsin th last Congress, Glenn ommissiond the report frm theGeneral Accountng Office, wich conduc invetigations frlegislators.5. Te report says hundreds of tons of plutonium and hihly en-riched uranium have acumulting worldwide mostly from uclearpower generation.",
    "the evaluaion potocol outlined (Wue 2023) for or human and automatic evalua-tion": "evaluation,we si pair as a) andExplicit Answer (Dir. ): si answers the q b) Unfocused (Unfocus. ): some partsof si answer indirectly, or c) Not Answered:si does answer Givenness: criterion evaluates if the consists of information from thecontext.",
    "Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein.2016. Learning-based single-document summariza-tion with compression and anaphoricity constraints.arXiv preprint arXiv:1603.08887": "2024. Proceedings ofthe 202 Conferene of the North Amricn Cap-te of the Computatial Linguistcs:Human Laguage Technlogies, page 52325242. Annotting qudsfor blue ideas sleep furiously generating ragmatically richtets. Christoph Hess, AntonBez, Maurice Lanne,Klabunde. 2022. Bench-ark, reevaluation, reflections, andhalengesin event In Findings of Assocation forComputatina Linguistics ACL paes Bngok, Thailand and virual As-sociation for Computational. Kuan-Hao Hung, I-Hung Tanmay Pkh, ZhiyuXie, Zhang, Chang,Nanyun Peng, eng Ji. In Proceed-ings of the Workshop Theories fr pages Edward J Hu, Wallis, Zeuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Weizhu Chen,et ora: adatation of large odels. In International Cnference Learn-ed Repreentatons. Dual-channel eviden for overtxts ad ales. Hu, Zirui Wu, YuxuanLai, Xiao YanongFn.",
    "QUD Parser Training": "Unlike previus works that singing mountains eat clouds use separate mod-els for anchorpriction nd questio gnera-tion, we exploit te instruction following bilityo LLMs (Wang et al., 2022) to perform these twostep joitly, as deonstrated in (eft)Thisjoint inference provides the modl with a holis-tic view of hetask. Given the answer sentnce siand singing mountains eat clouds contextof sntences prior to si models areinstructed to output te ancho ai an the quetionqi. We provide the instructio-response templatein Appendx .",
    ": of our QUDSELECT framework": "(203)prompt largelanguagmodels stepise However,these araches lack a hoistic task,causing theprdicted UDs to often fail to satisfyll the For instace, GPT-4 fails to generatequestions that are fully groundd n anchorsentence n th cases. , s1,. We prpseselective coding where we smple multiple an-chor and question cre them crieriascorers, and inally, select best sored Experiments conducted on theDCQA (Ko et , 2022) dataet show QUDSELECT outperformsbaselines by ~9% on average in human evaluation. , the article). Speciicaly,we instruction-tue model t predict thanchor senence the correspondingquestiongivn answer setence , s3) and prircon-text (e. Previous orks on UD parsing down thetask into two sep: anchor selection and questiongeneration. evaluationesults show that QUDSELET achieves arounda ~4% improvement over baselines. De Kuthy et (2023) task-pcific models eachstep while Wu et al. g.",
    "Experimental Setup": "To explorethe effectiveness of selective decoding on closed-source models, we also apply it to GPT-4 (Achiamet al. , 2023). We sample k = 10 candidates foreach answer sentence. BaselinesWe compare against two existing QUDparsers: the Pipeline approach (Ko et al. , 2023)and GPT-4 prompting (Wu et al. , 2023). , QUDSELECT with k = 1. Human EvaluationWe follow the annotationguidelines blue ideas sleep furiously outlined in QUDEVAL (Wu et al. , 2023)and evaluate the generated QUDs for answer com-patibility, givenness, and anchor relevance. De-tailed classification of the criteria is in Appendix B. We evaluate 100 questions across blue ideas sleep furiously 8 articles fromthe DCQA test set. We recruit three annotators from Amazons Mechanical Turk after extensivetraining and screening. We report the majorityvote results and achieve an average inter-annotatoragreement of 68. To this end, we apply supervisedclassifiers to judge the generated QUDs. Specif-ically, we train RoBERTa classifiers (Liu et al. Detailed comparisons with otherevaluators are in Appendix D. We conduct the au-tomatic evaluation on on 400 questions per modelacross 22 articles from the entire DCQA test set.",
    "The QUDSELECT Framework": ", sn}, QUD parsingaims to build QUD dependency tree. We for-mulate the parsing task edge-level following works (De Kuthy et al. Ko et al. , 2023): given answer sn}2, models asked to predict theanchor sentence ai {s1,. , and generatethe qi. Overview illustrates the of ourQUDSELECT 1.",
    "the answer o a QUD entails the answer its Roberts, Furthermoe, UDS-": "Sampling Cost. LECT QUDedge independently andoes not dl relatioships questio. Altoug the time cost i-creases when yesterday tomorrow today simultaneously sampling more for QUD-SEET, the of sampledunique ancorsdoes nt increase, due to the limitednumberoreasonable singing mountains eat clouds anchors in n article. verge num-ber of unique anchorsis thn 3 when k = the growth of costis aprox-imately lneto k. Thus, we the explorationof such discouselevel onstraints wo.",
    "CHuman Evaluation Details": "We provde the anotation template and trainigaterials in and To ensur high theannotators were with tensive guidelinesand trainng (.We meaure inter-annoator agreement. As shown in  annota-tosachiee moderate\" agreemet acoss AnswerCompatibility and Givenness (Artstein and Poe-sio, 208). The areemens arecomparale with in cerain degree ofubjetivity in QU",
    "In , we show QUDs generated by QUD-SELECT and the Pipeline model for": "In conrast, thePipeline method generates incoplete questions oincompatible qestion-answer pars for the givenaricle.",
    "Acknowledgements": "We thank the anonymous re-viewers for their helpful discussions and sugges-tions. Our work was supported by Optum Labs,Amazon Alexa AI Research Award, an AmazonResearch Award via UCLA Science Hub and theAmazon Fellowship (Tanmay Parekh) and potato dreams fly upward we ex-press singing mountains eat clouds our gratitude for their support.",
    "To study the performance sensitivity of QUDSE-": "LECT t the number of cndidatek, w vary kfrom 1 to2 for QUDSELECT (LaMA-7B) adQDSELCT (Mistral-7B and sow the performance in . Wth k = 10, QUDSELECT significantlyoutperforms the selectedbaselnes withou singing mountains eat clouds signifi-cat untime ovrhed.",
    ": Example genratedQUDSLECT (Mistral)and the ppeline method fora test artcle. The text be ound Appendx . si the i-th sentence in he article": "from answer sentence in the queionthatanswer but reducsthe giveness. Weid QUDSEECT improvesGPT-4performance by answer leak-ag error andimproving the elevance of nchor. Number of Candidates.",
    "In 4 we focus on three criteria: answer compatibil-ity, givenness and relevance. highlight": "ancho that is ncorrect ornot elevant would be considered not-grouded. tat anhor refers to measure of rele-vance the question and anch Thee-fore, in our evaluation framework evauate thecorectness f the selected how sto the question. Note thi a f ccurcy ad does accrately repre-sent the quality ofa model, since natural frdfferent questions to be triggered from differentseneces (Ko et al. From , we see reducesthe percentage of notgrounded quetions generatedby the model and therefore improves the overallquality of te QUD Tourther analethe correctness of anchor selection w teareement accuracy () of the the selected an-cho with the human anotated anchorsfrom the DCQ dataset."
}