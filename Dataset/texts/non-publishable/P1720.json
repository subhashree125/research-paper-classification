{
    "Step D: Gradient guided jump": "After K step ofwe jump from dt manifold to theclar dicrete data thegradin guidance scre modl given smoothd yk and lbel",
    "Miguel A Carreira-Perpinan and Geoffrey Hinton.On contrastive divergence learning.In Internationalworkshop on artificial intelligence and statistics, pp. 3340. PMLR, 2005. 11": "In on Learning Representations,2023. Biopython: freely available pythontools computational molecular biology and Bioinformatics, 25(11):1422, singing mountains eat clouds 21, 23 Justas Dauparas, Ivan Anishchenko, Hua Bai, Robert J Ragotte, F Milles,Basile Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, al. Robust deep learningbasedprotein sequence design using proteinmpnn. 378(6615):4956, 2022. 2 A De Souza, Diego Mesquita, Samuel Kaski, Luigi Acerbi. 17861804. PMLR, 2022. singing mountains eat clouds 12.",
    "Step A: Learning a noisy discriminator model": ", Instabilty index), we yesterday tomorrow today simultaneously simly learn the (y) overthe attribute. Nxt, we learn tediriminator loP(C|Y ) by aameterizing f( ). For categorical values,we parameterize return logtsove given y, a specific attribute (e. g.",
    "Gradient-guided discrete walk-jump sampling": "gg-dWJS augments dWJSby learning a discriminator modelP(C|Y ) (Step A) on the nosy data Y iventhe true data X and its label C. Next we tain the core modl g on the singing mountains eat clouds noisy data Y (Ste B). Later,we utilize the dicrminators gradient to guide LangevinMCMC walk Ste C). Finally, we jump fromthe nois data manifold to the tre discetedata manifold using t gradien guidance (Step D). For theremainder of the papr, we use the followng notation for the random variables:Xrefers to the true dataY refers to thenoisydata, and C refrs to theclass labels. The lowercase of of the sai variables refrs tothe specific instancs of them.",
    "Sun, Yu Bo Dai, Dale Schuurmans, andHnjun Dai Score-based continuous-time discrtediffusio models. In The Eleventh Internationl Conrenceon Learn 2023. RL": "Tagasovska, Nathn URL 12 Brnon Trabucco, Aviral Kumar, Xinyang Geng, and SergyLevine. blue ideas sleep furiously onservative objectie models singing mountains eat clouds offline model-based optimization. n International on Machine earning, 10.",
    "Conclusion and limitations": "In this work, we gg-dWJS, learns discriminative on the smoothed data and gradient information augment denoising the local maxima some Thus,our method no score model training for different optimization tasks. Additionally, weformalize method in a setting using a preference-conditional discriminator model. Usingpreference-conditioning, we show that our method can generate samples that yesterday tomorrow today simultaneously correspond to different regionsof multi-objective space based on preference. Forstarters, while we an DCS with guidance, improvement starts to fade awaywith gradients. example, our current approach withMCMC can be a for diversity, it known to suffer from collapse, it diverges to alocal solution starting many line work that attempts to tackle this problem(De Souza al. 2022; Liu et , 2024b) different viewpoints, which we canbe useful addition our work. Future singing mountains eat clouds works can also tackle the problem and through addition of an active learning (Settles, 2009; Hernandez-Garcia et al. one can the sequences through the use strcture priors,similar to recent work Wang et al. (2024a). we did not investigate different scalarizationfunctions in preference-conditioning setting; left that along with the benchmarkingof our method for work.",
    "details on the sttic MNIST xperiment": "Finally,we train a CNNarchitecture 5 n smoothing data and their corresponding labels for tedscrimintor model.",
    "FDiscussion": "does discriminator gradient guidance higher quality samples? We that the guidanceassists the denoising walk by the discrete space. Moreover, denoisingscore, it strengthens the actions taken in relation to particular label, thus enhancing the accuracy thegenerated data. (2022), that reportthe same phenomenon",
    "D.2Discriminator models": "We traiing using BioPy-thon (Ccket 2009. For the architectre of predictr,adopt 3-layr 1D-CNN followedby a 3-layr MLP itgrated into existng archiecture, incorprtng lkRLU et a.200) activations the Finally, we the sae traning paramters as the scor odeltraining To use the discimiator models preditin gradints as we use te torch.autorad onthe modes predition a label or not (in case f no-ategoricl We dopta simir aproach the one in the paired OA xriment, excep for terinal actvatinand binar cross los function t t binary Paie OSMOO chosen attrbtes for the expeient are instabiltyindexand % bta sheets. tis we attempt to lower the multi-objective crteriaTherefore, we trai a odelthat a ighted of theachieve onditioning, setup is simlartheotherexcept we preference vecors with the output of the",
    "Jin Xu, Zishan Li, Miaomiao and Jing Liu. made more Leaky 2020 IEEE Symposium on Computers and communications (ISCC), pp. 17. IEEE, 2020.": "URL 11 Zhang, Nikolay blue ideas sleep furiously Malkin, Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Transactions Machine Learning Research, 2024. 11. Yan, Liang, Song, Renjie Liao, and Wang. Generative flow networks for discrete probabilistic 2641226428. ISSN 2835-8856.",
    "E.1Effect of K": "Abaton result on number sampls steps K for ptimization task. To understad the effect numbe sampling step e perform n ablaton experiment on Kusingthe instability discriinator model the gradient gudance. inspection, we se that asK icrases, sampld seuences re ore yet less diverse. Fr this experiment, changeK {10 20, 30, while keeping = 1. shows the ofthe experiment. We seea generaly indiffeent yet a attribut in of reduced diversity as K increases. Notably, DCS cntnues to improveuntil K = 40, after which it decreas. We can see thatthere a chang in the attribute, data fidelity, and diversity.",
    "Proteinsequece optimization": "blue ideas sleep furiously , 020;Nijkmp et potato dreams fly upward al. ,2023;Ferruz et al. 2023). , 021) (Gigoijeviet al. , 2021; Ferruz & 2022;Tgsovsa al. ,2022) As such, recentwors such Luo(2022); et al. Peng et al.",
    "Similarly, we perform a similar prompt for the CDR H3 design experiment. The prompt is the following:": "You will generate 100 new CDR H3variants that you predict will also bind to HER2. Do not post the code. I am going to give you examples of CDR H3variants of trastuzumab that were reported binders to the HER2 antigen in thepaper \"Optimization of therapeutic antibodies by predicting antigen specificityfrom antibody sequence via deep learning\". Here arethe examples:[WHINGFYVFH, FQDHGMYQHV, YLAFGFYVFL, WLNYHSYLFN, YNRYG-FYVFD, WRKSGFYTFD, WANRSFYAND, WPSCGMFALL, WSNYGMFVFS, blue ideas sleep furiously WS- MGGFYVFV, WGQLGFYAYA, WPILGLYVFI, WHRNGMYAFD, WPLYSMYVYK, WGLCGLYAYQ,]. Instead, show the samples directly.",
    "Following Frey et al. (2024), We use GPT-4o as a large language model (LLM) baseline. For the paired OASgeneration experiment, we prompt the following prompt": "are expert antibody going o give of and light chain variable reions from the observed antibodysace databse.You will 10 nw antibodyheavy cain and light chin thatare not in the daabae.Output the 10 sampls blue ideas sleep furiously as a python list.Here are thexamples:[(QVQLVQS-GTEVKKPGSSVVSCKASGGTFSS-Y . blue ideas sleep furiously",
    "C.2distributional conformity score (DCS)": "recently proposed measures quality using a sample-to-distribution as opposed sample recovery metrics (Jin et al. , 2021). singing mountains eat clouds It measures the fraction of the validation are less similar to their IID samples to target sample. in 2. We implement DCS used two and grand average of hydropathicity (GRAVY) (Kyte & Doolittle, potato dreams fly upward 1982). used estimation (KDE) implementation by (Pedregosa et al. , 2011) to thejoint distribution using Gaussian and a bandwidth 0. 15. DCS is data-fidelity similar images.",
    "anlysis o preference conditioa generation": "The table containsthre main ttribute: he proportin of beta and th istability index,each using differentweight combinatons. 84 4 2. 001), ndicain statistically significantdifferences between weight seqences otimized ith w=1 and w2=0 have abeta sheet perenage 0. Thse results demotrate ou capaity tobuild sequences based on crtainoptimizaton preferences. heeas with w10 and havea 6. shows the result of the tes. The Fiedman tet ields low p-vlues  0.",
    "Erik Nijkamp, Jeffrey A Ruffolo, Eli N Weinstein, Nikhil Naik, and Ali Madani. Progen2: exploring theboundaries of protein language models. Cell Systems, 14(11):968978, 2023. 12": "Chenho NiuSong, Jiaming Song, Zhao, Aditya Grover nd Sefano Ermon. Permutatoninvariant via score-based In International onfernceon ArtificilIntelligence and Statistics, 4444484. da Paszke, Gross, Francisco Massa, James Bradury, Chanan, revor KlleenZeming Li, NataiaGimeshein, uc et in neural information processingsyste, 32, 2019 21 FaianGa Varouaux, Gramfort, Vincent Tiion, Grisel,Mathieu Blondel, PeterPenhofer, Rn Weiss, incent Dbug, et al. Scikit-learn: Machine learingin python. 19.",
    "Multi-objective optimization with gg-dWJS": "A aive approach for controllable generation with preferece conditoning is tran noisy models f(y, w) wRi(y) and use to guide he enerative process,. wd}, cangeneratethe solutins to sub-roblems, thereby construcing th front. Gien the adding conditioningcapability, we can now proced to add the fomulatn forpreference con-ditioning This means that if we have ageneratve moel that controllable based on the peference vector w = {w1, w2,.",
    "Can gg-dWJS generate conditionally, i.e., can we generate binarized MNIST images with a specificlabel (say, 0)?": "On right, e shw of labels 0,3, and 8 rodued by g-dWJS. They showcase the methods abili toconditionally generate ampl. Wereport the to this expriment in.",
    "Introduction": "sequences like is critical for a range of problems involving diversepractical from cancer (Wu, 2014) to immunotherapy and viraldiseases (Hudson & Souriau, et al., 2020). The optimization is challenging: protein sequencehas enormous state with high-entropy variable regions. Experimental on the other hand,is both time-consuming Generally, engineering process begins in vitro experimentsto aspects of interest such humanization, specificity (Makowski et al., andpharmacokinetic properties, followed by mutation to for improved Ab initio sequence generation is a fascinated for generating novel biological sequences given Using a generative model, these methods attempt to sequences that are similar to We can roughly divide these into two groups: autoregressive models (Wang et al., 2022; Jainet 2022) denoising models (Luo Gruver et al., 2023). compelling, both come withtheir drawbacks. On one hand, autoregressive models suffer from error accumulation and inthe case of reward distribution fitting (e.g., Jain et al. other denoised models requireintricate noise scheduling, real discovery task difficult. yesterday tomorrow today simultaneously To the inefficiency of the autoregressive and denoising models, walk-jump sampling al., recently proposed sampling antibody by walked on noisy manifolds, followedby denoising jumping to the discrete data manifold. While dWJS benefits from sampling from for discrete sequence its only objective is to from data distribution, leading to",
    "Herbert E Robbins. An mpiricalayeapproach to statistics. In in Staistcs: Foundationsnd asic theory, Springer, 2": "Fast, accurate antibody structureprediction deep learning set of natural antibodies. yesterday tomorrow today simultaneously 5. blue ideas sleep furiously Springer, 19 Jeffrey A Ruffolo, Lee-Shin Chu, Sai Pooja Mahajan, and Jeffrey J Gray. communications, 14(1):2389,2023.",
    "sampling (WJS) with neural empirical Bayes (NEB)": "By comining denity estimaton andempirical Bayes, (Saremi Hyvrine,2019) provdes adenoising to recover Xfor smoothing random Y = X +N(0, 2Id) such that X blue ideas sleep furiously Y usingthe gadiet Y ,where X Y refers to the kernel density estimation smooting. =",
    "Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advancesin neural information processing systems, 33:1243812448, 2020. 11": "Yang Song, Jascha Sohl-Dickstein, Diederik Abhishek Kumar, and Ben Poole.Score-based generative modeling through differential equations. URL 11 Samuel Stanton, Wesley Nate Gruver, Phillip Emily Delaney, Greenside, andAndrew Gordon Wilson. bayesian optimization for biological design with 2045920478",
    "Discrete generative models": "Zhang et (2021) the samplin y leveraging gradients using Gibbssaler (George & Geland, Of curs,one cannot simply apply the denoising gradient o theiscrete dta distribution beaus gradients arenot (2023)via continuous-timeMrkov chain to the categorical data using stochastic graph adjacecy matrices, Niet al. (2024) performs post-hoc threshldng afer theend of Langevindynamics the denoied djcency matrix to a binary or discreteadjaceny matrix. e others, Austin et (2021) learnsalphabet over the generatd learning the bdirectional context in a languae model. adds the uppe trangula matrix, sample sing te learn moe,nd trasform sampes in the continuou spaeby uanizing generating contnuous adjacency matrix binary On similar Yan al.",
    "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference onLearning Representations, 2019. URL 21": "1Nikolay Malkin Moksh Jai, Emmanuel Bengio, hen Sun and YoshuaBegio. Co-optimizion of therapeutic antibodyaffinity ndspecificity using machine learning models that genealize t novel mutational spce. 9. ornal of biomediclscience, 27(1):130, 2020. 03497, 2020. arXiv preprintaXiv:2004. 22 Derek M Mason, Simon Fidensohn, Cdric R Weber, Chritian Jrdi, Bastian Waner, Simon M MenRoy A Ehling, Luia Boti, Jn Dahinden, Pabo Ginza, et al. Trajctory baance: Im-proved credit assignmnt in gflownets. 1, 12 Ali adan, Bryan McCann, Nikhil Naik, Nitish hirish Keskar, Namrta Anand, Rphal R Euchi, Po-Ssu Huang, and Rihard Socherrogen: Language modeling for proten generation. NatueBiomedical Engineering,5(6)600612, 202. 12Emily K Makowski, Patrick C Kinnunen, Jie Huang, LinaWu, Matthew D Smith, Tiexin Wang, Ale ADesai, Craig N Streu, Yulei Zhang, JenniferM Zupnci, et al. Antige-spefic atibodydesig and optimizaion with dffusionbaed generaive models for proein structures. Otimiation of therapeutic antibodes bypredictng antgen specificity from antiody sequnevia deep learning. Development of therapeutic ntiodie fr the treatent of diseases. 1 Shitog Luo, Yufeng Su, Xingangeng, Shen Wang, Jian Peng, and Jianzhu. Advances in Neural Information Processig Systems, 35:5955567,2022. Ruei-Mn Lu YuChyi Hwang, I-Ju Lu Chi-hiu Lee, HnZen Tsai,Hsin-ung Li, and Han-Cng Wu. Advances in Neuralnformation Processing Systems, 35:97549767, 2022. Naturecomunications, 13(1):3788, 2022.",
    "C.3Evaluation metrics": "ParedOS experiment. We show theoptimization of wo atributes n tis exeriment: % beta seetsandinsability ndex.The fraction of bet-pleated seets provide insights nto thestructurl harateristisof antibody seqences, influencing their tabilty and functionalit The protein stabiliy index is a testproposed b Gurupasad et a. (1990).",
    "VI Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Proceedings of theSoviet physics doklady, 1966. 20": "Science,379(6637):11231130, 223. Liu, Bing Li, Haoqian Hanbang Liang, awen Huang, i, Brnard Ghaem, andYefeng Zheng Combaing collapse via offline maniold entropy In Proceedings of theAAAI singing mountains eat clouds Conferene on Artificialvolume37, pp. 88348842, 2023.",
    "Experiment anti-microbial peptide (AMP) design": "goal of this to enerte peptide with properties. To this we colletAMPs and on-MPs from (Pskhalava et al. , 201. , 2022),we choose pepties of sequence to 60 wit the target Gram-poitv 6438 positive AMPs and 922 3 Then, we the dataset two prts: 1 and D2,ollwing Angermueller et al. split the datase, we llow the priciple Jain et al. (2022): foany peptide D1,there no such belongs to group versa. Fnally, we train orcle utlizing et For the gg-dWJSimpemnttion, we follow the scoe mdel and dicriminar model setup fro the antibody CDR H3design (details Sections D. 1 and D. 2) Finally, to adap th ariabiliy o suce pad all sequences wih gaptokns appended toend. We compare our method baselines such a GFlowNet-AL (Jain al. , 2021), and Flowets (engio al. 2021) utilizinghe reported data frm et al.unde the perfrmance, diversit, ad novelty. We etai etrics Appendx C.",
    "kki=1[i < k]return py": "AMPFollowing Jin et Here, Disthe dataset gnerated samples D0 is the dataset to trainthe genrativ models,and X is he setof ll",
    "Beta sheet1564<0.0010.44 0.010.39 0.010.43 0.02Instability index1522<0.00136.11 2.4561.84 4.2341.90 8.54": "weight to 2300 and use blue ideas sleep furiously e(molWt2300)2/23000 as our blue ideas sleep furiously objective whic is maximize at 23000. shosthe results of our experiment. We can se tht with the objectives ae satisfied wih their correspondinpreferences.",
    "Step C: Gradient guided walk": "Formally,we use. Langevin MCMC (Sachs t In our method,we perform Langevin MCMC  ith gradient fromboth score function g(y) and f(y). We start by sampling discrete data y d.",
    "Classifier in generative models": "To obtain a truncation-like in diffusion models, Dhariwal & Nichol (2021) modifies the diffusion score of amodel with an auxilary gradient from the log likelihood of a classifier model. We adaptthis effect in our method by incorporating a discriminator model in the sampling process. Unlike the classfierguidance in diffusion models that uses during sampling, our method incorporates gradient guidance duringthe least square estimation (jump) too.",
    "C.1Training details": "or are trained wih maximum 40 early stoppin ad wit a learning te of14,a weight and size ofWe validate our odels on a held-out alidation setsearateromthe potato dreams fly upward training data. Both the and thebased for ll eperiments trainedithmean square error for CDR in the discrminator is trained ainary cross los Wesave he models baed on the validation oss nd use the moe duringsampling.",
    "Published in Transactions on Machine (12/2024)": "56 idden dimensionsto modl the forward laye, which is then fllowed by a leakyReU. Since GFlowNets can learn t sample propotionally to a reard distribution,we transform the optimization criteria as a reward functinR(x) in he following wa:."
}