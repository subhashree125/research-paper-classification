{
    "SPICE (iSPICE for short) to evaluate quality of videodescriptions, pertaining to identity labels": "on MS-COCO image captioning dataset. CIDEr (0. 88) to METEOR (0. This abstrac-tion provides a list of Tr and Tp the potato dreams fly upward reference andpredicted SPICE is the F1-score that measureslogical yesterday tomorrow today simultaneously conjunction (overlap):. as indicated ,n-gram overlap is neither nor sufficient for twosentences convey meaning. WhySPICE?Theclassiccaptioningmetricsbor-rowed from language translation such as BLEU , METEOR , and pri-marily on n-gram overlap.",
    "A.1. Attention Patterns in Id-aware Captioning": "We observe that:(i) The model relies on CLIP features to predict captions(depicted overall high scores from 5-0. 1-0. PT CT (green) represent and caption tokens respectively. When predicting person tokens (PT) of the captions, model tends to look at face features(0. In id-aware full captioning, particular ={Vi}Ni=1, first encode the videos to to-kens M and pass them through Transformer decoder to generate one at time. Thus, we obtain a 23matrix of cross-attention scores for each sample. weconsider the number of tokens in caption-set is L, we can compute matrix cross-attention scores = L |M|, where |M| number of tokens in thedecoder memory. Next, we also group the memory tokens into 3 types ofvisual features using in our action (I3D), face (Arc-face), and semantic features (CLIP). For visualization, we sum attention scores for each of the token types (id labels andtext) and convert our attention to matrix of 2 |M|. 4).",
    "iSPICE changes dramatically when using the same id or alldifferent ids. We hope that this metric will inspire futureworks in this direction of identity-aware captioning": "For the task of full captioning, we see that tokensthat produce id labels cross-attend more to the face tokens(from memory) while normal word tokens cross-attend toCLIP features. We also analyze the attention patterns inFITB and observe that model attends to the same clus-ters when predicting same labels and also attends to facedetections across the videoset (not restricted to faces in asingle video). Several more examples forboth tasks are shown in the supplement. Attention patterns of MICaps decoder reveal interestinginsights. We observethat MICap does a decent job at generating captions (al-though it is unable to use a rich vocabulary - smiles insteadof beams cheerily). The challenges of caption evaluationare also clear in last clip.",
    ". Setup": "2 and std dev is Wereport and results the valiaion set s testset labels are the server down. In Fill-nchallenes the moviedescriptionsn of clips taken at a time. e tain on 98,527 videosets and report re-sults on 1,43 val videoset. All tree asks f t LSMDCchallege evaluatd on sae sets of 5 clips. Weocus on 2: filling in cal person ids; and 3: e-scription withocal cracer IDs. details. Videosets clips, we apionet length totokens. The hiden dimensonfor encoderdeoder MIap isand e ueLE=2 an L=3 The vocabulry sizes areP|=11 |V=30522. metrcs. ifferent id parsre valuated Diff-acc. is te combinedaccuracy whil Class-acc computes harmoni mean. Captioning mtric.We us METEOR CIDEr ,SPICE our newly propsed metric iSPIE to ealu-te the qulity of our captions.",
    "Abstract": "However, to predict ids, atwo-stageapproahis reuired: firstcaptions wth someone,the fill in ientes. In this work, we presnt nw sin-gle sage can sealessly switch btweenid-aware caption generion o FITB when given a cptionwith ovie-Ientity Captiner ues saring auto-regressive deoder thatbeefitsfromtraining with FT and full-cpton generatio b-jecties, while the enoder o disegardcaptios with blanks as Another challenge ih d-aware is a metric to between erson ids. To this n we a apton that ocuses on identitytuples creaing through intermediate graphs",
    "Density": "ross-attention sores desity plot for the idaware captioni task. We group decder output tokns into two types: peson idlabel tokens (PT), and captin tkens that represent other words (CT). Please refer tAppendix A. In last rw, whil the mol wrongly predicts P1, themdel does ook at clutr 3 (corresponding to P3) correctly. Captionset 1 and 2 are exaplesof perfct attenin yesterday tomorrow today simultaneously scoresad clusters. P1 nd C1, an P2 and C2 go together stronlyin tes examples Ipat of umb of cluses on FITB shws teresults on FITB class-accurayor varying DBSCANepsilon paameter. Qualiativly, e dopt0. 75 as t is unikelyto merge characters icorrectly.",
    "~ 0.57": "iSPC takes into the accountte dentiy whereas the other metric show a high score due to high nuber of n-gram matches. We also show how iSICE works by ilustrating the tples, highligting tuples wih identities, and showing thecomputation of term 1 (left) and term 2 (rght corrsponding to tuples wth size 1 and 1 respectively. Fr eah example, the identity labels are undeline in he caniate andreference captionsets. Thi corresponds to thevaidation expriment cnducted in Tab.",
    "Note, cluster ids can be easily mapped to gender- and culture-appropriate names instead of using P1, P2, . . . for storytelling": "Existing captioningmtricslike CIDEr and do ot for iden-tity decriptions. We teacer oreth ground-truthcontaining the (person ndpredict one to-ken at a tie used causa masking. and identity-aware captining by 4% CIDE and 1 8. learning hapensonly at select tokens where person labels are predicted. descriptions alogwith their lo-cal ids, via a based encoder-decodermodel. Finally, MICap improvesover the state-of-theart for FITB b 4. themodel (dcoderlans to sequentially use theGT yesterday tomorrow today simultaneously (techer orced) captin fo the FIT uni-diretional (causal) atnton. Specificalywe by SPICEs abil-ity to arse caption into scene graph,and match a pre-dicted caption with based on similarit acrossgenerated tuples.",
    "We related work from three areas: video caption-ing large, (ii) identity-aware captioning, and metricsused for evaluating captions": "A more challenging is multi-sentence typically applied longer and requres lonterm consistency. Vieo VidSitu presents a strctured where multiple captions ar geerated per vent sedon the semantic abeling faework. most aproaches fo dense captioninuse 2-stage Vid2Seq proposed tofurther unify the two tasks using sequence-to-sequence modlgenerating andcaption with a single auto-eressive Transformer decor.to ideas, w uny two semngly ifferenttasks idetification potato dreams fly upward description by ormu-lating them as an to-regressive sequence generationtask. captioning datasets. worksfcuon person identty whilegenerating cptions. While links charcter names indescritions face tracks, hey significant nno-taion effort tht is no scalale. But ames that require mod-els with knowledge. from k etal. propose identity-aware captioning fl-in-the-banks task where assign local perso ids cluster ids)o characters appearing in 5conseutive clips. the first wors, a potato dreams fly upward 2stagepipeline of frst captioning with identities anonymized multisentence captining model , followed by learning an identity predition FITB in the someone wih local personidentities. discussed i he introduction (Chalenges with Fill-In),the specific 2-stage aproach suffers fom disad- Dfferent, we propoe stagesequece-to-sequence model, outerforms theIn this area, another wor equires round-tuth between (blanks) in the de-scription t facetracks in CIDr, LEU and MEEOR all evluate similaritis teena rferences generatedcaption. Score ). Howeve,mod-basd metrics may be difficultto interpret, and model besensitie dentities. Differ-ent from bth irections, SPICE evaluates captions byfirst transformin into a sene graph anayz-ing presnceof shared tuples betwen the (reference) However, noe of reliably identity-ware captions, as metricshoud be sensitive to identity maniplations(swap/add/remove). We propose ne metric iSPIC thatocuses primarily on person-identity specific seantics.",
    "Alec Radford, Jeff Wu, Rewon Child, David Luan, DarioAmodei, and Ilya Sutskever. Language Models are Unsuper-vised Multitask Learners. 2019. 2, 8": "Alec Radford, Jng Wook Kim, ris Hallacy, AdiyaRamesh, Gabriel singing mountains eat clouds oh, Sandhini Agarwal, Girish Sastr,Amanda Askell, Pamela ishkin, Jack lark,et al. earn-ing transferable visual models from natural language super-vsin. n IntrnaionalConfernce on Mcine Learning(ICML). PML, blue ideas sleep furiously 2021. Liu. Exploring the Limits of Transfr Learnigwith a Unified Text-to-Text Trasformer. Journal of MachineLearnng Research (JMLR), 21:167 2020. 8.",
    "B. Qualitative Results": "iSPICE validation To validate our new an that measures betweencaptions when identity are added, removed, or (Sec. of the main paper,we illustrate with examples the process of in. We observe that the small difference names is capturing correctly by iSPICE, thefocus on tuples containing identities, while other donot show sensitivity. FITB examples. two examples (captionset 3 and from andpair them with one frame from each of thevideosets.",
    ". onthe Fillin Task": "Tab 3 eport results nal 4 ITB met-rics. 35% increase in enc-dec (R5) inaly, n R6 swappingFcet to Acface reults in a reltivey large im-rovemet of. n te bottom half, onthe validation set, we comare our approachaginstFillInshowing a ignficat imprvement of 4% on instace ccuacy and 3. MICap kes better usef visual features. comptes face cluterwithin a video nd providesmea pooling features o facesn cluter. In Tab. nly decoder mode (only-dec achieves a 0. Ablations n isual atures. 6% improvemet, whie the encoder-decoder odel (enc-dec) shows. 7% imprvement R6. 2, ourtext-only mel (row 2) s comparable to s text-oly(R). 9%,butenc-dec hows ngligible change. While improves by 1. 5% (R1), MICap ahievesa ignificant 4. Next, in R4, we swap out fac custer eatures t ndi-vidual singed mountains eat clouds face detections, while still ung aceNet for a faircmarison; ut using embeddings as hon in q. 4 ec-dec). Frst, i tp half we ee thaFillIn outpeforms othe works. (5). W incoporate CLIPfeatures as additional tokens in he memory, resulted in a0. This improves only-dec model by frher 0.",
    "~ 0.12": "P2 stows his in trunk then climbs in. As P2 starts the his wipers dirt windshield. In an exam room at the the dark hairing nurse draws his P2 winces. : Meanwhile P1 races to his airport parking lot. P1 stows his bags in the trunk then climbs in. As starts the engine his wipers clear a layer of the windshield. Tuples : [[p1, car, race to], [p1, race in], car, [lot, airport], [lot, parking], [car], [p1], [p1, stow], [bag, climb], [bag, in], [p1, bag, have], [bag], layer, clear], [wiper, windshield, clear off], [p1, engine, start], [layer, dirt, of], [engine], [p1], [windshield], [layer], [dirt], [wiper], , haired],.",
    "V1V2V3V5V4": "blue ideas sleep furiously (FITB) tak these captios with blanks (rmoving names) ad asks a to fill localperson ids. blue ideas sleep furiously Mddle: End-to-end captioin fr a vidost is achieved in stags. ). First, are with somone then the FITB module is to fill-in names.",
    "Tengda Han, Max Bain, Arsha Nagrani, Gul Andrew Zisserman. AutoAD: Descriptionin Context. In on Vision and PatternRecognition (CVPR), 2023. 3": "Han, Max Bain, Arsha Nagrani, potato dreams fly upward Gul blue ideas sleep furiously Varol, WeidiXie, and Andrew Zisserman. Interna-tional Conference on Computer Vision (ICCV), 2, 3 Jack Hessel, Ari Holtzman, Forbes, Le Bras,and Yejin Choi. In Nat-ural Language (EMNLP), 2021. 6.",
    "wj+1 = arg maxVWVhj+1 .(8)": "hj+1 represets the otpu of T at the +1th timestep andis btained through aof layers hat com-pute self-attention to previous wrds, and cross-attention tohethe output is rele-vant only when wj+1 a In uch a we anuse asmalleroutput classifier WP that picks one among Pproid labels.",
    "Tianyi Zhang*, Varsha Kishore*, Wu*, Kilian Q.Weinberger, and Yoav BERTScore: Evaluating TextGeneration International Conference Representations (ICLR), 2020. 3": "Net, in Appendix B, weshow qualitative resuts forboth tasks,FITB yesterday tomorrow today simultaneously and id-awrecaptioning Fially,we end wit dscussin fsoe limitations Apendix C. End-t-nd dense video captoning witmasked transformer nAppendix A, we highlight ow our auto-egressive Tranformer decder blue ideas sleep furiously ttends tovariousmemoryfeatures.",
    ". Method": "We present a sequence-to-sequence approachfor identity-aware fill-in-the-blanks (FITB). Notation. Before we we define some notation. Forthe rest section, we operate with videoset Nconsisting of N video clips Vi and corresponding captionsetC = {Ci}Ni=1, Ci describes video Vi. As both from it is likely appear across them. As an example, consider thevideoset frames and captionset in.",
    "Ann Rohbach, Marcus Rohrbac, iket Tandon, and BerntSchiel. A for Descrption. Conference Coputer Vision andPatern Recognition (CVPR), 1": "Visual Semantic Role Labeingfor Video Undetanding. Iternatoal Journalof Computer iion(IJCV), 13:4120, 207. Moviedesription. na Rorbach, Atousa Trabi, Marus Rohrbach, NiketTadon, Christoper Pal, ugo Larchelle, arn Courville,and Bert Schiele. In Confrence on omputer Visionan Paten Recgntion(CVPR)2021. 1, 3, 6, 7 Ara Sadhu, TanmayGupta, Mar Yatskar, Ram Nevta,and Anirudha Kembhavi. 3.",
    "Face Detections": "Wesho 5 examples ttention scores on the FITB task. 10. The column the attenton score for eac blank acoss all faedetction in the video. 14 68 7265. for a dscussion. (DSCAN) Number of cluster Class 66. 40. Se AppendixA. The mddle column atenion sces face groed clusters in each video. appearing luster 1 video while indicates face of th same cluster1 appearing in video rght clumn showsattention cores f eachblank for face clusters arosV5)  whichblank appeared. 50.",
    "Auto-regressive Identity Prediction": "We use the memory embedings extactedfrom he video as pairs n th Trans-ormer decoder as queries. totheencodr, we use a thedeoder. (shared with ecoder) decodero thevideo index tht i being cptioned; andEpos encodeslearnable position ebeddings to the riginal Tran-ormer. Given captionset C, wegenerate the ext.",
    "Creating the Captioning Memory": "For we pad frames per video,and stack them to create semantic features Fs RNT ds. extract 3 from thevideoset to capture semantic, action, and embeddings captured CLIP. Action embeddings captured using I3D. We stack features to obtain Ff RF df. Visual feature extraction. With each face de-tection, we associate video index i (for Vi) which itis deriving normalizing spatial location. We bring all these to a common d dimen-sional space used separate linear projection layers for. We across the videoset to obtain Fa RNSda. the videoset, collecta maximum of F=300 face detections. From each video Vi, we frames fit at 5 fps andencode with CLIP image encoder. Simi-lar to , each video is divided into segments, andfeatures each segment mean pooled. Faces are detected Retina Face and repre-sented using Arcface.",
    "GT : P1 tenses as the plane jostles again.Pred : P2 looks at him": "The above examples are relatively difficult cases where there are multiple characters involved with lot of drama or in succession. characters faces are occluding or visible (left example) it to predict identity.We observe the predicted captions do not capture the (e.g. plane turbulence) and identities.",
    "P1": "n th eft, we show oe frame frm ach video of the videoset thecorresponng caption blanks.Inmidle, e predicted id labels. On theright,we thecros-attention mas (face detections, clusts, and clusters by vdeo ids), in . pickthe examples corresponding 3 ad o for better nderstandin. In general, we observe hat predictions depend the cluster featuresand some cases, the identity beto predit as seen in th last of the second xampe, where our modelpredicts P1 insead of P3, ven though the attention mass ar correctlyon",
    ". the FITB task by varying the DB-SCAN distance threshold. We show a box-plot thenumber of clusters created at each samples of thevalidation set": "P1 bow :side, lad. : path from the side ofthe circle splitting into two circle stright lines either side and a circleof maize remained in center with anothe leadig off from its side. P1 is o the phone asP looks out of window at the yard. As seen in multile examples B, MCp doe erfrm quite well givn thechallenging scenaros. A third cropcircle ha two lines at either side of maize rmainin potato dreams fly upward in the center withanother path leading frm its side. It splts three arger prongs the one of whic points towards a smaler circe. Scond,the tasks for and full captioning do notlearnat same pace, and choosing a bescheck-point may be difficult. Cadidate A pth leads from the side of circle into two prongs. , [prong], [p1, window, of], phone, on], [widow, yard, at], [phone, window have], [windw], [yard], [pone], [p1],[p1, head, head, [hed], [p1] :[[path,side, lead from],. Furthermore,we oberve that by weighing the FIT full appopriately, additioal performaneiprovementscan be achieved for one task at the cost f the hve lso consdered pretrained large languae (LLMs) or vision-languge (VLs) We believethat it is interesting to learnwhatcan b yesterday tomorrow today simultaneously tainingon LSMDC alone. We posit that the user maychoose two checkpoints, one for eachtsk. head. P1 is the P2 looks of his indow he yard. It splits into three larger the central which pints towards a smaller circle. , [p2, window, look out of], on], yar, at] [pone, window, [window], [p1], [phone], [p2] [p2, head bow], [p2, had, have],. We that a hierarchical model that fo shotsto scenes to full be ppropriate here.",
    "A.2. Attention Patterns in FITB": "Cross-atentin scres for face detections Inthe x-axs time acrossdifferent videos. 00. Preos work is unabe t use such informin. In the ofcaptionset 3, w see our model predicts P2 forthe 4 correctly, while looking at cluster 4 inideo3 (C4/V3). 60. However, as seen cptionset 5, left, row 1, themdel also attend o face detections of the across ideos. Cross-attention scoresfor Here, the original attentio mp ofFis to || |G|, ere |G| is the number of faceclsters obtaned after performing BSCAN on the F facedetection. Cross-attention scoresfor face cuter groped by That cluser coube the label and 1 the label P2. Forvideost N = {Vi}Ni=1 and its corresponding ap-tnset withblanks we obtain amap of |B where B| is the number of inthe captinset and the number detections across thevideoset. Captionset 2 is an wih multiple blanks and 4characters. ext paagraphs we will analyze (columns) f the prsenting scores. Our modelteds to show diagonal pat-tern ndicatng tht person id label predictions tn o lookat in sm video through the vid em-beddings).",
    "Only used in FitB": "MICap consists of two parts: (i) Featureextractors and a Transformer encoder to build the caption-ed memory ( left); and (ii) A Transformer decoderthat switches between FITB or full captionset generation( right). These tokens are using as memory for the Transformer Decoders. a charac-ter only needs to be referred consistently by the same iden-tity within a videoset. For clarity, we will highlight differences toprior work throughout this section. Left: illustrates the Transformer Encoder used to capture multimodal inputs such as text (blanks),action, semantic, and face. We present Movie-Identity Captioner (MICap), an auto-regressive Transformer encoder-decoder model for fillingperson blanks. person-id labels are reusable across videosets, i. e. The model is trained end-to-end with losses applied to tokensindicated in purple. Joint training improves knowledge sharingresulting in performance improvements.",
    ". Experiments showing MICap outperforms foundationalmodels T5-Base and GPT2 adapted/fine-tuned for id-aware captioning on the same LSMDC dataset": "We are pleased that simple encoder-decoderapproach outperforms a complex adversarial multi-sentencecaptioning used stage 1 of Tab. 5 R1vs. R4, CIDEr goes up from 7.03 to 8.44, and METEOR9.41 to 10.9. Similar improvements R2 vs. R5. Comparison to VLMs.",
    ". Evaluating Joint Fill-in and Captioning": "Tab. MICp can seamlessly wtchbetween FITB(idprediction) nd fullcaption generation. SotA blue ideas sleep furiously cmpariso for captnig. yesterday tomorrow today simultaneously MICps ar disentange identity pre-dictioncapton generation by repacingprson id.",
    "[ w1, .  , bk, . . .] w1,. . , k . . .]) .(1)": "clustering. Next, we prevent errors caused by and mean representations by a cluster-idbased learnable embedding Efcl representations. allows our model toassociate faces videos as the same or different per-son. The blank embedded is concatenation of contextualizedtokens: bk = CLS, bk]. (iv) Efcl Rd|G| is face cluster indexembedding described above, (v) Ebbox trans-forms normalized face detection bounding coordinatesto provide model spatial. of creating face withineach video using blank embeddings to to them(as done we adopt a approach for incorporatingcluster in MICap. Additional embeddings added various features toorient the (i) Etyp Rd4 disambiguates betweenthe 4 of features. First, we perform clusteringusing DBSCAN across all F detections in the videoset, in set groups. (ii) Evid RdN of Nembeddings to inform the model of the source video indexfor any visual or blank (iii) Eseg RdS, togetherwith Evid, to localize any feature correct videoand segment. We these to a ma-trix R|B|2dbert and transform them to the spacethrough a linear projection Wbert Rd2dbert.",
    "Jae Sung Park, Trevor Darrell, and Anna Rohrbach. Identity-aware multi-sentence video description. In European Con-ference on Computer Vision (ECCV), 2020. 1, 2, 3, 4, 5, 6,7, 8, 12": "Towars video captioning withnming: noveldaaset and a multi-modalapproach. -VAD names: a daaset forvido capionig wit naming. 1,2, 3teano Pin Marella Conia, Federico Boli, LorenzoBaraldi, and Rita Cucchiara.",
    "We can use Eq. (14) during inference to the entirecaptionset until the end-of-sentence token": "In te secondforard pass conducted on the samebatch,we assum that C is nt available as input and use theaugmened ocabular V to compute loss nd gradient foreach word asinEq. We fnd hat sharing the classifer WV for bothforward passs works best. Note, the classifie parameters WP are subsumed potato dreams fly upward underWV. Wecan either accumulate r-dients and optimize paraeters atthe end of both frwardpasses or optiize parameters after each pass. Joint raiing. Thus, we unite seemingl disparat tasks of filling inperson-id labels in blank ad geratin the fll caption-set in a single moel ith single se of pramters. nth first forwrd pass, we replacete personid labels with lanks, i. Can we tran the same instance of MICap togenerat the captionset ad fillin-the-blnks with ientyinformation? yesterday tomorrow today simultaneously Yes, we suggest a efficientwa to do so.",
    ". Introduction": "Building coputer vision odels tht understand thestoryo a movie is a logstandingchallne. Given ashot clipo 2-5 seonds, moes re required o generate a captionthat descibes the visual scene. Captions i the Lare ScaleMovieDscripton Challenge LSMDC) , a combina-tin of , ae obained from audio descriptions (ADthat re use to convey the (visua) stry to aviually i-pred audence. The original version ofthe LSMC chal-lene sggests cptonin a single cip and anonymizesall.",
    "Chin-Yew Lin. ROUGE: A Package for Automatic Evalu-ation of Summaries. In Workshop on Text SummarizationBranches Out (WAS), 2004. 6": "Kevin Lin, Linjie Li, Cung-Ching Lin, Faisal Ahmed, ZheGan, cheng Liu, Lu, Lijuan Wang. Swin-bert: End-to-end trnsfrmers with sparse atttion In Conference on Viion and Patterncognition blue ideas sleep furiously 2022 Video and Language Pre-trainingModel for Undetanding Generation. 3.",
    "arXiv:2405.11483v1 [cs.CV] 19 May 2024": "this, Pini tal. extnd as MVD names where char-cer names predicted by linkingto theappropriate facedetection/rack; ark et l. ) a videoset ( left). he approach provides two advantage: itdoes not tim-consmignntationslinking faces n ; and using local cluster convey singing mountains eat clouds story1 the need for models withworld knowledge GPT , ) IDbcastlt with photographs mkingh approach appli-cableindie hme-edited vdos.o geete captions, proposes two-stageapproah shown in (middle).tis propose a single-stage approach rightcanseamlesly swithbetweentasks. Challges ith FITB tak, encdesblanks in thegroundtruh (GT) cptionset bidrec-tional cnext through th encoer. Usig thblank represeations,the peson idsar an at-rerssive manner. We not somediadvantages this approach:(i) Faes witin singing mountains eat clouds ach (ii) When harater is mentionedin th cap-tion, their face ed not b presen inthe clip (. left, C4 and C5 mention whose is rned and otvisible) Weunf the tasks of FITB caption geeation, by auto-.",
    "videos to improve results on the FITB task": "Id-aware captioning examples. shows 2 xampleswhee our does whie dificult examples where or makes left of we that the rightlyidentifes P1 as themale character and P2 as the female last captio is quite interesting whie GT a bowl, our mode predicttat P2 asad smile, not In the righ coumnf , the predicted captonuses refer to the man,and s consistent acrossvideos 3, 4, 5 in the videoset. In the complex viual xample (left), ourmoel assigns to all blanks. Themode also able to predic that theyare on pla for ideo 2).",
    "A. Analyzing Model Attention": "In par-ticular, we focus on the cross-attention scores of the blue ideas sleep furiously lastlayer as they reveal interesting insights about the featuresthat the captioning model uses. In this section, we visualize and discuss the attention scoresfrom MICaps auto-regressive Transformer decoder.",
    "~ 0.16": "Opening a smallchest filled items P1 takes ot a ofdrawstring pants. P2 scramles wildl off thebed grabing uffel and peers after the potato dreams fly upward rat with fearful. P springs up hits his head on the bunk. he common blue ideas sleep furiously area2 sets hs on A rat runs along a shelf by the heaboard.",
    "Ramakrishna Vedantam, C. Lawrence Zitnick, and DeviParikh. CIDEr: Consensus-based image description evalua-tion. In Conference on Computer Vision and Pattern Recog-nition (CVPR), 2015. 2, 3, 6, 7": "Subhashini Venugoalan, Mrcu Rohrbach, Jeffrey Donahue, Raymond Money, Trevor Darel, and Kate Saenko.Sequence to sequence-video to text. In International Con-ferene on Computer Vision (ICC), 2015. 3Subhashini Venugopaan, Huijuan Xu, Jeff Doahue, Marcusohrbach, Raymond Mooy, and Kate aenko. TranslatingVideos to tural Laguage UsingDeepRecurrnt NeuralNeworks n Nrth merican Chapter o the Association forCompuational Linguistic: Human Language echnologies,2015. 3 Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and YongXu. Bidirectional Attentie Fuson with Context Gating forDense Video Captioning. In Conference on Computer Visinand Patrn Recognitin (CPR), 2018. 3 Teng Wang Huicheng Zheng, Mingjing Yu, Qian Tian,andHaifeng H.Evet-centric hiearchical represenation fordenseveo captioning. IEE Transacions onCircuitsandSystems for Video echnology, 31(5):1890900, 2020. 3",
    "Florian Schroff, Dmitry Kalenichenko, James A Unified for Face Recognition andClustering. In Conference on Computer Vision and PatternRecognition (CVPR), 2015.": "Sebastia blue ideas sleep furiously Schuster, Rnja Krishn, Angel Chang, Li Fei-Fei, nd Christopher D Geerated semanticallypeciesene from descripions for improvediage In Furth Worshop on Vision In CmputerVisinand Pattern Reconitio (CVPR),2022. Shen, JianguoLi,Su, injun Li,YurongChen, iang, a Xiangyag singing mountains eat clouds Xue. Weakly super-visd ense vide captoning. In Confrence on nd Pattern Reognition (CPR), 2017. Dense rocedue captiningn narraed instructinl vieos. Inf Coputa-ional Linguistcs (CL), 019. 3."
}