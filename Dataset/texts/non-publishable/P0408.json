{
    "Humans typically associate a of default fea-tures to Consider the following": "Include the background, style, and consider:. Car by is assumed to have 4 of each:tires, door, and trunk,hood, steering wheel, roof. e. When composing image description,start with style tldr sentence thatpaints very high level picture. That said, it not to detail depending on use-case. If theannotators do not recognize the bridge as afamous well known entity, then would makesense the color additional at-tributes. making unique feature,then that be worth mentioning in thedescription since it holds specic added visualvalue.",
    "where <CHOICES> are lled in in the same": "Finaly, we vison lguage compo-itional rasoning results with differen e-tunedmodels Here we in-clude tmodels ntund and DOCdataets. astly, ere114 smpls withositive captinnot containing the V, r thatneeded be swappedto form the caption. Nte thtthis task set up i how VLM modlsare ypially evauate on tese daasets:rior cosides asample to correctly about if the image-te silarity of trueimage caption is hgher than image-text simi-larity te imagecaptio. he IIW descriptionsstill result in reasoning accuracy ARO VG-A and arecompaable with DOCCIon Winoroud. ,when choosin image cptionchoices ,LLM resposs are and calcu-late accuracy with respect to true captionclass. In future work, int engtlimtationsVLMs likeCLIP can be our rich data, whichwill help to mprove VLM For urexerments, need atrend fals captioassciated with an image. Thus, e can these captions to serveastheforthe sample. Trendslso stay the same with with OCCIperrming to IIW, InstrucBLIP slightly etter (by les than 1 cracyint). If the does not prouce validcssis cnsidered prediction. and a f the gven mage fr InstrutBLIPand LLaA, e process LL as classes, (e. De to the tensedifferences, it wol rocessgto mach lie lyin. Thi happens a resul of SVO triplts containingroot fors of the words, which wer not pelledthe same way in captin. g. For remainng 10%, us the negativetriplet (the S, V, Otriplet specifyig te andverb wth one ofthem modi-ed) to automtically ip atie S, V,or in the aption.",
    "D.5Additional Automatic Metrics": "We reportthese simly tothe limitationsof these metrics measuing tequality ofhyper-detailed image yesterday tomorrow today simultaneously descriptions.",
    "its mutual alignment using vertically stacked, aligned to the left, etc": "We emhasize te descriptionsshould be in objectie, ad fairlangage for related attrbutes focus solely on aspect. Wheneple are presnt specil shouldbe kept in mind to mitiat different types bias. As aother potato dreams fly upward example th same prae (FREE, RCADE, GAES) yesterday tomorrow today simultaneously areal uper-cased, ertically stacked and centrllyaligned. Information onthefon color and shadow efect soul be in-cluded. If haveagood idea of thefot familyandare would valuable o note. or , the phrase THE, Univese) wrd uiceand Universe as apitalize while phraseACRS is uppercase,and coponentsare along a diaonal.",
    "OpenAI.2023.Gpt-4v(ision)technicalworkandauthors. 19-February-2024]": "Rahul Pandey, Hemant Purohit, Castillo, andValerie Shalin. Bryan Plummer, Liwei Wang, Chris Cervantes,Juan Caicedo, Hockenmaier, and SvetlanaLazebnik. In Proceedings IEEEinternational conference on computer vision, pages26412649. 2015. mitigat-ing human annotation errors design efcientstream processed systems with learning. In Proceedings ofthe 40th Annual Meeting of the Association Com-putational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. International Journal of Human-Computer Studies, 160:102772. Bleu: a method for automatic of machine translation. for ComputationalLinguistics. Flickr30k entities:Collectingregion-to-phrase correspondences for richer image-to-sentence models. 2002. Kishore Papineni, Roukos, Ward, and Zhu.",
    "visible. If applicable, note the respective posi-tions on their body where each is present": "yesterday tomorrow today simultaneously Forpparel, the shoul focson overall style, unique details, silhuette of thegament, how it fabric, shades, and the g. , sherwani, kurta, kimon,sare) (e. g. sawarma, dosa,tikka)tc. potato dreams fly upward",
    "Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.2022.Laion-5b:An open large-scale datasetfor training next generation image-text models.Preprint, arXiv:2210.08402": "Piyush Sharma, Na Sebastian Goodmn, Soricut. 2017. Concepual captionsAcleaned, hypernyed, image alt-text daaset forimge n Proceedins of the56th Meeting the Asociation for Compu-aionl Linuistics (Volum 1:Long pages2562565, Melbourne, Shkar, ando Klimovich, Au-rlieHerbeot Moin Nabi, Ener Sanineto, andRaffaellBrnardi. 2018.",
    "Human SxS on comprehensiveness, specicity,": "human-likeness, and tldrality. Across these metric, IIW data is bette hanrecent DCI and by and+48% bette thanGP-4v, and +31% better whenused for ne-tuned than and DOCCI. modeldescriptins mges most similar to theoriginalimage (ranked 1st) and mprve distin-guishing true pars gven tribute, re-lation, word oder by up to 6%. 9k object descriptions We also release uman SxS betwee IIW,DI, for comparison in future wok.",
    ": Camera Angles to Consider when Annotating Images. These are important to set a precedence on thelevel and kind of information to expect in the image description": "De-scribe the yesterday tomorrow today simultaneously wordsif they are wrtten",
    "C.1Image Region Tasks": "In muli-objectdense imags, a label in itself is not enough touniquely identify an object. Our econd image regin task is label preiction,iwhich we predict a open vocab label for thobjectwith input (image, oundin box). Lastly,we perform ojct descrption eneraton,whichpoduces descrtions for each object in the magegven (image,ouningbx, ael).",
    "Annotation Round": "(b) Time(sec) perAnnotation Round (1,3)(1,2)(2,) c) Jacard-Similarity/w Annotation Rounds in the Begining 0. 2 0. 6 0. 8 1. 0 : Efectsof Sequential Annttion: Over nnotation ounds,(a) toke count oes up as b) tim spengoes do wth (c) higer greemet, masuring by Jccard Similarity (Wikipeia contribuor 204) (d) Oveti ith a constant humnannotao pool, eac learns from the other iaan impiifeedbac lop and a highagreement atein round(,2) can o be observing ws previoslyonly seen in round (2,3) in (c). As a reult, our galwas to anntat a small-scale, hig quality dataset. Te numberof sequential annotatr and presence of ask 1can be adjusted as time and budget permt. Seded Annotion Describing images in deailishighly ubective andcomplicated. Toexdite h-man annotation e ue aLI-3 5B oututs to sedthe annotation process instea of crowd workersstarting fro scratch. Whil VLM ve imprventhei abiity capture image etails,attemptsto generate a cnsistent rich output still fall preyto hallucinatins and recall isues. Ou human anntation ipeline ensres that VM halluinaionscan be correed andissing detailslled in. g. at stleor tile of a painting) provde ainimal qualityand coverage guarantee. A data is collcted theVLs using for seding areupdated toproducebetter quality descriptios in active learning op(reeced with lops in ). After batchesof k aples are annotated, retrain (. e , r-ne-tune the aLI-35B modelswith all aailableannotations (for both Task 1 and ask 2). W nd that thee pates signicantly imroeth baseine model, with early batche shiftingPaLIcaptions from an average of 15to 150+ words withasfew k samples. Hoever, ths could eincorporaed inthe future if performance saturats. Sequetial Augmentation Wefurtherimprovframework efciency with seuenial descriptionaugmentations. Humans ugment a previous crowdworkers an/r VLMs outputs instea of taigfromscrth. Afte therstaumenation, both he machine-generating seed and prior hma annota-tio ar provided. The following annotators do notknow which is modl output verus human written,which an mtigate preference to model outputs. ured theannotton process, itis far more -fectivei time and quality to read ad ugmentimage descripios: in we see hat if annoations were donein parallel, wewould have3 opeting outputs pr image,eac ih theiowntyl, pspective, an weaknesses wth eachcontainig 170 words and taked 800 seconds. Higher Jccard similarit ovrrounds suggests higher inter-anotator agreement,which al sevs as proxy fr qality. This is seen in the2x iprove inter-annoator agreement betweenrounds1,2) when comparing c) an (d) in.",
    "FPercentages Reported in the MainPaper": "We re-quote and dene all analysis percentages re-ported in the yesterday tomorrow today simultaneously main paper for clarity on how in Tables 15-17. is dened by the section, paragraph, lineit appeared in. For example, 3 P2L3 means , Paragraph 2, Line 3. rounded to the nearest point paper.",
    "(a) Monitor this at the beginning of the an-notation project when the annotators arestill new to the task using metrics likeedit-distance and provide explicit feed-back to the annotators as needed": "Annottors in each round have the optionto start scratch if they deem thequality from previous round beconsiderably low. Us this as fedackfor the annotator the previous rundby prsenting thm theedited oututtolearn Learning Ourimplcitly unlocks a loop annotators to sequentialaugmetationprocess disused Each nnotator gets to and learn fom wich in trn improves their individualquality. As xample from we how Annotaor-1 get an opportunity o ernfrom nnotator-3 for the st image gets opportunity fom Annotator-1 inte image. W emplo loop for VLMs where someinitial nnotation data avalable, veionM1 can e over the VLM toimprovethe seed decription quality. As moredata gets an-notated, M1 can be updated t M2, M3, .., Mn toreduce human effort nedd.Advantages:",
    "ImageInWords Dataset Collection": "The IIW dataset is composed of 9018 (Train: 8573,Test: 445) images that are sampled from a We-bLI (Chen et al., 2023b) like dataset and humanannotated. Details on the human annotator pool areproviding in Appendix B.1. An-notation methodology and the types of image-textannotations we singed mountains eat clouds singing mountains eat clouds collect are describing in 3.2 and 3.3.",
    "EEnriching Image Caption Datasets": "In , we reportth languae statisics onthe origial 1k sample from each dataet and teenrihd versio. T enrichedvsins not only alow for ner-grained, full cv-erage evluations of t content in mages (via newmetris potato dreams fly upward or probig dataset), but alsomay enableautorater modls whic learn fom he precisionand ecall erros in the gnerated descriptins. It is clear tht he IIW descri-tions are sinicantly lnger and richer, as we havehiger counts of toens, senencs, and each prtof speech.",
    "Related Work": ", Li al. VizWiz (Gu-rari et al. , 2020), (Agrawal et al. Image captioning has been studied for years, start-ing with and LSTM encoder-decoder frame-works for generic captions (Vinyals et 2015; An-derson al. , to the recent VLMs for more captions (Chenet al.",
    "Merge if object(s) are fragmented and/or pre-populated as two or more objects, the anno-tators can remove the individual objects andcreate a new single object": "g. ,ve identical cups in aimagelinedup next t eac other do not need tobe taggedas separate objects. If thereae attrbtes that sparate one or moreof them fro the hers, we expect theannotators to split them in groups adprocee accordingly.",
    "IIW-Eval Data and Annotation Breakdown": "quite at random baseline; we also note that we donot incude Flickr30k nor COCO order AROsubsets, as the LLM can distinguish true captionat 98%accuracy without any image description.When incororating imaedesriptions, al md-els perform signicantly beter than e language-bia baseline. The IIW model results in the besttask performance for ARO isual Genome Attrbu-tion and Relation (VG-A, VG-R) and Winoground,with accuracy gain of nearly 3%, 6%, and 20%,rspecively. Moreover, we can further boos prfor-mnce cmare to the InstructBLIP and LLVAimage captions: we improve reasoning accuracy byabout6%,2%, and 4% compard to best imagedescription model-basedbaseline. hisreects therichness of IIW acrssdifferent part of speeh andcomprehensiveness, as moreattributes and relation-shipsare captured and can beusing to rason aoutimage content. For SVO-Probes, we nd smallerdifferenes,wh I, InstructBLP,an LaAmoels within 1 point of each other.",
    "D.7Reconstructing Images with IIWDescriptions": "Thisinformation is not sharing with the annotators and the sources are randomly ipped and marked as A or B to preventany source or order based bias. showcases prompts and the T2Imodel outputs from three descriptions along withthe original image. as promptsfrom each of the three datasets (DCI, DOCCI andIIW). For reconstructing images sentence-by-sentence,we fed the T2I model rst sentence, rst twosentences, rst three sentences, etc. The input descriptions could be from any combination of (human, model) sources. The image most similar to originalimage is ranked number 1. We allowed generatedimages to be ranking same if they are very sim- : Human SxS Annotation UI. Annotators are shown input image and two input image descriptionsto evaluate side-by-side.",
    "Count/ Sentence/ Description": "et al. 968. 55. 717. 16. 741. 02. 5DCIextra1 (Urbanek et 09. 335. 33. (Onoe et al. , 2135. 016. 79. 6IIW 29. 852. 05. We include the number descriptionsand the average sentences, nouns (NN), adjectives (ADJ), adverbs (ADV), and verbs (VB). , 2021; Young et al. , 2015; Mao et al. , et al. , 2015;Kazemzadeh et al. 2014; Krishna al. g. g. DAC (Doveh et , 2023)uses a machine-generated approach: pretrainedLLMs expand the image caption and pre-training generate over smaller im-age regions. , detector togenerate smaller describing and thencomposes them into an ller text and image labels areused to reach this length, and repeated or highlyoverlapping sentences are often present. As a result,we use their extra_caption eld fair it only available. In contrast to we also allow crowd workers toupdate or component of",
    "Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet,and Geoffrey Hinton. 2022. Pix2seq: A languagemodeling framework for object detection. Preprint,arXiv:2109.10852": "06794. 06500. Pali:Ajointly-scaled multilingual language-image model. Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-giovanni, Piotr Padlewski, Daniel Salz, Sebas-tian Goodman, Adam Grycner, Basil Mustafa, Lu-cas Beyer, Alexander Kolesnikov, Joan Puigcerver,Nan Ding, Keran Rong, Hassan Akbari, GauravMishra, Linting Xue, Ashish Thapliyal, James Brad-bury, Weicheng Kuo, Mojtaba Seyedhosseini, ChaoJia, Burcu Karagol Ayan, Carlos Riquelme, An-dreas Steiner, Anelia Angelova, Xiaohua Zhai, NeilHoulsby, and Radu Soricut. Xi Chen,Xiao Wang,Lucas Beyer,AlexanderKolesnikov, Jialin Wu, Paul Voigtlaender, BasilMustafa, Sebastian Goodman, Ibrahim Alabdul-mohsin, Piotr Padlewski, Daniel Salz, Xi Xiong,Daniel Vlasic, Filip Pavetic, Keran Rong, TianliYu, Daniel Keysers, Xiaohua Zhai, and Radu Sori-cut. Preprint, arXiv:2209. 2023b. 2023a. Pali-3 vision language models: Smaller,faster, stronger. 2023a. Wenliang Dai,Junnan Li,Dongxu Li,AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,BoyangLi,PascaleFung,andStevenHoi. Preprint, arXiv:2305. 09199. Instructblip:Towards general-purposevision-language models with instruction tuning. Preprint, arXiv:2310.",
    "(b) on the FnetuneModel Generated Otpus from DCI, DOCCI and": "Due ho low the metric valuesare,these differenes may not signicant. both human authored and model generatd outputsrom and prior work to show the istribtion of reecd in writig el prform eton test set, DOC perform best n DOCCI set andso on). anomaly occurswith CIDEr on tetset,whee PaLI models ne-uned with DOCIslightly the DC trined model (4. cross all hree datasets, LEURTshowsariants perform better or simi-larly th sme-doman test set. 91verss 57).",
    "B.2Human Annotation Challenges": "First,w still found insances of random qual-ity or judgment Forinstanc, art require more specic expertise to an image withappropriate vocabulary. At sart f our ano-ttion process, we that annotators had atenec to ue llr an rexes asThis a, i or photo was takenwith, we poviding feedba asking tey include such phrases. We also obsevedsom to use subjective langagehile the images, e. g.",
    "Future Work": "We are currently working on adaptingour proposed framework to accommodate localespecic annotators, which are required for culturalspecicity. Our continued yesterday tomorrow today simultaneously goal is to make the an-notation yesterday tomorrow today simultaneously guidelines holistic, reduce human effortand dependency in the annotation process, and helpshift the narrative from captions to descriptions.",
    ": Human SxS on Model Predictions. Generated comparesne-tund wih verus an DOCCI an outputs. Model-Human GT4V model toIIW huan-annotations": "First, use descriptions from DOCCI,andIIW netuned models o romt a (T2I) model or iag reconstruction andevaluate which yesterday tomorrow today simultaneously result n higher The, in e how IW models can generate descriptions toaid in ision-laguage reasoning.",
    "We use the extra_caption eld of DCI annotations and dis-cuss this in choice in . All following DCI referencesrefer to the extra_caption description": "paring DI and DOC, Comprehensiv-es is gh by+61% and +42%, Speccity by+80% and +82% Hallcnations ae lower by 42%ndTLD quality is higher by +91% and Human-Likenes mprove by +82%and 6%, respectvely Thi indicates tha theIIW image description magesfrom and DOCCI re considerably better thnthose originally wit prior work.To further quantify the of IIW uman an-notatins, we compare wit (OAI, 203 Tab. GT-4Vto gnerate on IIW-Evalimages. The descriptions are generated with thepromtaetaild mage oterThe results fom theModelHan sction o Tab. 3 show thatwe reahComprehensiveness (+5%) Secicity (+53),Hallucinaion +59), (+70%), and Hman-Likeness (21%) improements overPT4V ot-puts. Althoug PT4Vrelatvelybetterthan huan-authored DCI and DOCC datwhen compared to e asses thatconsiderble future modeing efforts ae needd foVLMs to reach IIW humanauthored dataquality.",
    ": IIW UIfor Task-1 ith seeds We illustrat teseed object-detecion objects and VLgenerated objectlevel capions with object image asinput": "Thisexample potato dreams fly upward demonstrates how humans can alter seing annotations based on the annotation guidelines, which caninclude merging, deleting, edited and adding new salient objects and then describing each. We illustrate the human augmented salientobjects and their human-authored descriptions.",
    "Conclusion": "gueline seee,sequental annotation processto humn au-hoed descriptins that are strongly pefered prio wors annotations +66%) andprior works ne-tune model (+31%). Images re-cnstructing wit IIW generated 1t mor often, of how image description as used, highersaieny and bettr overll qaliy Our re-ults colecively demonstrate tequality utilityof IIW as stte-o-the-art.",
    "D.3Automatic Readability Measurements": "W run huritic based metricsover oth human-athored and moel-geatd escriptions each style pesent theresult ach estimatesthe level of educaion needd to understand a usingdifferent uis, e. g.education years or Wile they ar proxy signalsa pattern across can be seen as a clear a more matur n articulate style forIIW comarion wit the other altenative. , 220) he and imple-mentation in Githubs py-readability-metics repo(1. 1) to calculate scors. We also includethe radability metric",
    "Compositional Reasoning with IIW": "Specially, we use IIW generated de-scriptions t in vision-lnguage cmpostionalreasonng. ,023), (Hendrcks Nemtzadeh,2021), and Winoground et 2022) mod-ify image captions t no oger math the piredimage2: changing visual ttibutes or relationships,swappng verbs, or shufg image captons suchthatcontain the same words reect differ-ent semantics. This is done to evaluate differenttpes of vision-language e. g. , at-tribute unersanded verb uderstanding. In this expeimnt we if IIW descriptions can b to rel imag cation the incorrect negatie caption in ARO,SO-Probes, and Winoground anLLM-only ,2023) t select whih of the caption istrue the image dsription (see Appendix 8for exact This replacese in these datases generaed de-.",
    ": Results Comparing IIW Variants Automatic Metrics": "a lower bound for com-parison, too. While the is different wedo not input any image description, we try to makeit as similar as possible to the above image descrip-tion based prompt. We dene different prompt languagebias baseline, which serves as sanity thatthe image/image description is needed datasets.",
    "Whethr they have any liemarks, tatoos scars nteir body that are": "Fr the phrase FREE,ARCD, GAES) all words re upper-casd, verticlly stacked, and centraly aligne. blue ideas sleep furiously yesterday tomorrow today simultaneously Iformation on the font color,type,shadw effect sould be inclded.",
    "CIIW Fine-Tuning Tasks": "e deneseven tak ithth IIW Task-1 anask-2 potato dreams fly upward singing mountains eat clouds annotationsto ne-tune two IIW basedVM model variats o PaL-3 5B (Chen et al. ,203a). Tsevn taksan be grouped into tree categories:mge regon, saient bjects, an detailed decip-tion based tasks, see forillustration. All resuts in th main paper use theIIW Model.",
    "tal than te image?, e.g.,doors, windows, or tirs of a Car can there is ometing uniqeabo them, as they are standard expecta-tionsfrom Car object": "Task-2: Overall Imag DescriptionIn Task-2, human anotators presented withthe nnotations from Task-1 an aseeded VLMdescription ( which is byhman annotators in sequenial producet hyper-detaileddescription ).",
    "IIW Human-Authored Data Eval": "g. Compre-hensiveness concerns whether a dcrptioncovrsal key informatin objects preent in an image. DCI, runa SxS on hman-authoed descriptionquality, we need a comon pol of blue ideas sleep furiously images For this, e additionally annotatethe set (11) and a comparble nubr(100) from theDOCCI test set with ourII annotation fraework. is the f detail in whicheach.",
    "We now discuss the annotation framework withconcrete examples and UI illustrations:": "Annotation Fine andAttributes In Task-1, the human are pre-sented seed annotations for Object-Detection model and VLM gener-ated seed captions for each object (see ). The annotators can annotate to note the salientobjects and corresponding description (see).",
    "Mood or feeling Overall mood or feeling ofthe image": "g. , details about a potato dreams fly upward close-up are differ-ent from yesterday tomorrow today simultaneously those of a wide angle shot. Examples ofcamera angles (see ):.",
    "IIW-Eval Benchmark Release": "IIW-400 is neweval set potato dreams fly upward of 400 images singing mountains eat clouds randomly sampled fromDOCCI-AAR (Onoe et al. 6) ofhuman- and model-annotated image descriptions,human SxS results on Human-Human and Model-Human pairs of descriptions. By releasing this subset along with human SxSjudgements, we encourage the development of new. The model generated descrip-tions may have hallucinations, information recalllosses, or non-human like writing style artifacts. , 2024).",
    "DCI4.570.600.414.710.610.420.750.560.40DOCCI4.910.580.3911.090.650.452.400.590.41IIW1.870.560.414.520.590.464.040.610.45IIW Comb.0.610.560.434.150.590.461.770.600.46": "We report CIDEr, BERTScore (referred to as BERT in tabledue to space), and BLEURT metrics for all ne-tuned models. ilar. Specically,for the rst sentence, the difference is most no-table, supporting our claim that IIW descriptionsare higher quality earlier on and IIW rst sentencesare designed to capture a TLDR. : Additional Automatic Metric Results.",
    "DCI5.85.78.18.12.93.76.26.9DOCCI7..19.58.76.46.68.78.2IIW10.49.51.811.59.9.011.311.7": ": Metrics potato dreams fly upward Humn ad Model Annotated Data. We include ARI (Wikipediacontributors,203b), lesch Kincaid (FK) (Wikipedia contributors, 2023c), yesterday tomorrow today simultaneously Gunning g (GF) (ikipedia 2023d,and contrbutors,2023e metrics.",
    "None56.50 59.9450.7149.88InstructBLIP-7B 62.7389.3565.25LLaVA-V1.5-7B 84.80 63.7187.8963.38IIW PaLI-3 5B90.37 66.1988.6669.38": "2023a)and LLaVA-V1. 5,with additional models Appendix D. On the other hand, ARO vi-sual genome attribution and subsets are. We IIW ne-tuned models to two larger (7B) source (Dai al. 8. , COCO andFlickr30k Order subsets of are not reported dueto a baseline of 98%. 5-7B (Liu et al. Vision-Language Compositional ReasoningAccuracy with Image Descriptions. Our rst is the condition(None in rst row Tab. see if richerIIW descriptions can help distinguish the true match-ing image in ARO (Yuksekgonul al. , 2023) in Tab."
}