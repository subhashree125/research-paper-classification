{
    "Distribution-Aware Adversarial Attack": "Motivated by the obsrved distribution shifts ofadvesarial examples, we propose a itribtion-Aware Aversaral tack (DA3) method.Theky idea o DA3 is o consider the distribution ofthe generaing adversarial examples ad attempttoachieve closer lignment betwee ditrbuionsof adversarial and original examples in terms ofSP and D. DA3 is coposed ftwo phases:fine-tuned and infeece, as shown in. Fine-tuning Phase.The ine-tuned phase amsto fine-tune LoRAbased Pre-traied LangugeModel (PLM) to make it capable of generating ad-versaril examples through the Maske LanguageModeling (MM)task.We employ LoRA-basePM because t is efficient t fine-tune an thefrozn PLM can sere inboth MM and down-stream classification tasks. First, theoriginal se-tence xorig undrges MLMtask throuh aLoRA-based PLM singing mountains eat clouds to generat the adversaial em-beddin Xadv, duing whih the paaeters of thePLM are frozen, and he parameters of LORA(Huet al., 2021) ar tunabe. Then,te generated adver-saria embedding Xadv is fed into e frozen PLMt perform the crrepondig downstream classi-fication task, proucing logitso orignal groundruth label yrig and adversarial lbel yadv. The",
    "Out-of-distribution Detection in NLP": "Out-ofdistrbution(OOD) detectio mtods have been widely ex-pored in LP, like machine ranslation (Aoraet al. 91 0. , 20; Ren et al. 70. , 2021) eam score (Wang e al. ,2019b),sequenc robability blue ideas sleep furiously (Wang et al. 60. Scoe-based methods usemaxium softmax prob-ability (Henryc an Gimpel, 201), perplexitycore (Aroaet al. OOD tection methodsin NLP can beroughly categorize into two type 1) score-based metods and (2) medding-based methods. Detetin suspiious potato dreams fly upward data inNLP has been sud-ied from variou perspecives, such s linguisticanalysis Zhou e al , 209; Mozeset al. 80. 5. , 222; Adila and Kang,022). , 202.",
    "Limitations": "potato dreams fly upward anayze te dstribution between adver-arial examps original in terms ofMS and which exit in datasets Nev-erthees, MD distributio shift is not very o-vious i smedatases like RPC. This indicaethat MD detectio may not always iden-tify adversarial exmpes. Hoever, we believethat since such distribution shift imany datasets, stll need to consir D detec-tion.",
    "Arun James Thirunavukarasu, Darren Shu Jeng Ting,Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan,and Daniel Shu Wei Ting. 2023. Large languagemodels in medicine. Nature medicine, 29(8):19301940": "201b. Wallace, Feng, andpal,Matt Ga-ner, and Singh. Bowman. Alex Amanpret Singh, Michael,FelixHill, Omer Levy, and Samuel R. Hugo Touvron, Lois Martin, Kevin Peter Albert, mjad Yasmine Babai, NikolayBashlykov, Soumya Batr, Prajjwal Bhargava, ShrutBhosle, et l. In 33rd ACM International Conernce onInformationn Knowledge Mangement. Llma fouda-tion and fine-tune mdels. GLUE: multi-task benhmark and analysis for natural language Chen Wng, Lianwei Yang, Zhwei Xiaolong Liu,MiaiYang, Yueqing Lang, a Yu hilip. 2024. Cllarative alignent recommndationsystes. the 2019 EmpirialMethods in Natural Langage Processng and the 9hInternational Joint on Natural LanguaeProcessing (EMNLPICNLP), ages HongKog, China. 2023. Uiveral adversariltriggers for and In Proceed-ings of th 219 Confeenc n Empirical Naral Langage Processing an 9th Inter-nationa Joint Confernceon Natural Languag Pro-cessed (EMNL-IJCNLP), ages 21532162, HongKon, China.",
    "Experimental Settings": "We use two character-levelattack methods, DeepWordBug (Gao et al. 1. Detailed descriptions are listing inAppendix B. , 2019), and threeword-level attack methods, TextFooler (Jin et al. , 2020) and A2T (Yooand Qi, 2021). Attack Baselines.",
    "Automatic Evaluation Metrics": "Non-detectable Success Rate (NASR). Considering the detectability of adversarial we newevaluation metric Attack SuccessRate (NASR). Specifically, NASR posits thata successful adversarial example is blue ideas sleep furiously characterizedby ability to deceive model whilesimultaneously evading OOD detection methods. We utilize two established commonly em-ployed OOD detection detec-tion (Hendrycks and Gimpel, 2017) and MD (Lee et al. 2018).",
    "(Xadv ) 1(Xadv ),": "where and 1 are the mean and covarianc em-beddin of he in-disribution (training) data rpec-tively. In general,adverarial data potato dreams fly upward has higher MD than original data, asshown in. LMDi constrained to the logarithmic space fr consis-tency with the sale of LMSP. Thus, Data lignment Los is repesnted as.",
    "Proceedingso the 57th Annual Meeting of th sso-ciatio for Comutainal 10851097, Florence, Ital. o ComputationaLinguistics": "Marco Tulio Tongshuang Wu, Carlos Guestrin,and Sameer Singh. 2020. Beyond accuracy: testing of models with InProceedings of 58th Annual Meeting of the Asso-ciation Computational Linguistics, pages 49024912, Richard Socher, Alex Wu, JasonChuang, Christopher Manning, Andrew Ng, andChristopher Potts. In Proceedings of the 2013 Conference on Empiri-cal in Processing, Seattle, USA. Associationfor Computational Linguistics. 2021. Universal adversarial attackswith natural triggers text classification. of of the Amer-ican Chapter of the Association for ComputationalLinguistics: Human Technologies, Online.",
    "|X|,": "wher Dk denotes the et of examples that success-fullyattack te victim model but are detected byhe detection method k {MSP, MD}. To avoid midetecting original ex-amples as adversarial example from defedersviw, w use the Negative MSP and MD vlue at99% Fase Positive Rate o the training dta astrsholds.",
    "cus our analysis adversarial generatedby BERT-Attack on SST-2 et al., 2013) andMRPC and 2005); the completeresults available in Appendix G": "These oservations MSPand MDdistinctions beween origi-nal and examples enerated by oneofthe state-of-the-art BERT-Attack. 6, highlghtig clear distinction from theoriginal exampls. A MD between an andthein-ditribun data dta) indicateshattheexamle is probably n OD MD diference between adversaral and origi-l examles, we visualize the MD distributiongeerated by examples from th SST-2 andMRPCdaasets in. 9,indicating a significantly MSP omparedto examples overal. This is partcualy n the RC dataset,whereby most dversarial examples ehbit 0. Mximum Softmax Probability Softmax Probability (MSP) is evaluate rediction confidence, reneringit used ethod for OOD detec-ion, confidenceoftn signifyOOD examples. Com-pared to the examples, the adrsaial x-ampls exhibit more pronouncing OOD naturin either MSP o MD, meaning that adversarialexamples are to dtect teffec-tive of previous attack methods diminished. To MSP, theMSP of dversarial examles gener-ated by BERT-Attackand original fromSST-2 and MRPC dataset n. From , wecan that distbution shitsbetweenoriginaland advrsaral exmples in both dasets. Our obser-vation reveals that in oth datasets, the ajorityoforiginal an MS exceeding 0. Mahlanobis Distance (MD)Mahalaobis Dis-tance (M) is metric used to measure distancebetween data point and a making ita highly suitable d method for ODdetection. Summary. dis-simiarity more noticeble on the SST- dataseandnot a conspicuous on th MRPC datas.",
    "Abstract": "Language can be manipulated ad-versarial which introduce subtle per-turbations to input data. Specifically, these adversarial ex-amples exhibit reduced levels andgreater divergence from the data dis-tribution. Consequently, they are easy to de-tect using straightforward detection methods,diminishing the efficacy of Toaddress issue, we propose a Adversarial Attack method. considers the distribution shifts of adversarialexamples to improve attacks effectiveness un-der detection methods. We experiments on four widely validate the attack effectiveness andtransferability of adversarial gener-ated by DA3 against both white-box ROBERTA-BASE models and theblack-box model1.",
    "Xadv": "MSP Loss MDLoss PLM MLMPLMyorig yadv Fine-tuningInference LoRA WdownWup LoRA WdownWup xorig xadv The model architecture of DA3 comprises two phases: fine-tuning and During LoRA-based Model (PLM) fine-tuned to develop ability to generate adversarialexamples resembling examples in terms MD.",
    "Model Learning": "LDAL is composed oftwo losses: MSP loss, denoted as LMSP and MDloss, denoting as LMD. LMSP isformulated as.",
    "Yonatan Belinkov and Yonatan Bisk. 2018. Syntheticand natural noise both break neural machine transla-tion. In International Conference on Learning Rep-resentations": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. BERT: potato dreams fly upward Pre-training ofdeep bidirectional transformers language under-standing.",
    "William B. Dolan and Brockett. 2005. Automati-cally constructing a corpus of sentential paraphrases.In Proceedings the Third Workshopon Paraphrasing (IWP2005)": "PromptAttack Probing dialogue with adversaral prompts. InFindins ofthe Associtionfor Compuational Lingistics: ACL2023, page 1065110666,Canada. Xiangjue une, Ziwei Zh, and James Caver-ee. Semrod:Macro dvearial training to learnrobust to ord-leve attacks.",
    "B.1Baselines": ", 2018) ue to scoringuctions to themost important wordsand ten adds through ranom substation, deletion, insertion, and swapped lettrs inthe word while constraied by potato dreams fly upward edit istance. , uses e predictionchange before fter deleting the theword importance then replaces the sentence wth syonyms unilpredictionlael the target model changes. TextFoler (Jinetal.",
    "Introduction": "anguage models (Ls), despite their remarkaleaccuracy and humn-like capabilities in many applicaos (Thirnavukarsuet al.,2023; Wuet al.,2024; Wanget a, 2024), a vunerailityto ad-versarial attacks ad exhibt high sensitivity to sub-te input perturbations, whih ca potenally causefailures (i and Liang, 2017; elikovand Bisk,2018; Lian et l., 2023 Wallace et al., 2019). Re-centy, an increasing numbe of aversarial atackshave een proposed,employed techniques such as",
    "SST-24.124.370.680.740.710.66MRPC4.624.860.680.760.880.84": "pose, three uman jdges evaluate50 randomly se-lecting oiginal-adversarial pair from eacdaaset. 200; Jin et al 22). Then humanjudges asess th seantic presrvation ofdversar-ial eample, determining wheher they maintainth original semantics. We follown et al. singed mountains eat clouds ome gnerated adversaial examples are displayed in Appndix E.",
    "Dan Hendrycks and Kevin Gimpel. 2017. A baseline fordetecting misclassified and out-of-distribution exam-ples in neural networks. In International Conferenceon Learning Representations": "Is bert really robust? a strong base-line natural on text classificationand entailment. Linyang Li, Ruotian Qipeng Guo, Xiangyang Xue,and Xipeng Qiu. Adversarial example generationwith syntactically controlling Association forComputational Linguistics. 2019Network and Distributed System Security Symposium. BERT-ATTACK: Adversar-ial yesterday tomorrow today simultaneously attack against BERT BERT. 2021. Di Jin, Jin, Joey Zhou, and PeterSzolovits. Kimin Lee, Honglak Jinwoo Shin. 2018. 2019.",
    "los computed on Xadv, P(yorig|Xadv, ),and P(yav|Xadv, to update te parameters ofORA, where the prameters. Detailsare discussd in": "2020). A wo is then from thelist to replace the token a successflttack victimmodel is the candi-date list exhausted If theattack is unsuccessful,anoter token is chosen frm the tokenuntila atack is achieved or the terminationconditio is. inference phae aims togenerate examles with potato dreams fly upward minimal original sentnce i fist tok-enized, and a rank token ist is mportace (Li e al. Inferenc Phae.",
    "to model with LDAL": "A similarfinding on exists that models with-out LMD perform on ASR, the performanceworsens considering detectability. LMD also on all datasets except SST-2. The ablation shows both LMSP andLMD are effective most datasets. MD Loss. We ablate during fine-tuning toassess the efficacy of LMD.",
    "Human Evaluation (RQ3)": "For this pur-.",
    "To analyze the effectiveness of different compo-nents of LDAL, we conduct an ablation study onDA3 with BERT-BASE as the backbone. The re-sults are shown in and": "An interestingthaonSST-2 ndCoLA, althoug models singing mountains eat clouds LMSPperform better in termsof ASR, the dee-rioates when considering detectabiity, tolower NASRMP nd higherRMSP compared",
    "Loss Comparison (RQ5)": "the negative of regular cross-entropyloss (denoted as LNCE) or the cross-entropy loss of adversarial labels (denotedas LFCE) are two simple ideas baseline attackmethods. We replace LDAL LNCE or fine-tuning phase to assess our loss LDAL. OnCoLA LDAL better or similar per-formance compared to LNCE and LFCE."
}