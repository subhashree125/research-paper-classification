{
    "A Critique of the Metrics": "6. All correlation co-efficients are larger than 0. We considering both BLEU scores when comparing results. RUBY can serve an initial when eval-uating results. the results under CBD to calculate the Pear-son product-moment correlation coefficients metrics, coefficients with in. According , amongmetrics for similarity, BLEU-2 has thestrongest correlation with the Pass and hencecan indicate syntactic to extent. 85) other metrics. Pass metrics should be jointly usedto prevent evaluation bias.",
    "Dongwei Jiang, Marcio Fonseca, and Shay B. Cohen.2024b. Leanreasoner: Boosting complex logical rea-soning with lean. Preprint, arXiv:2403.13312": "Patrick Lewis, Perez, eksandra PiktusFaboPetroni, Vladimir Karpuki, Naman Goyal, Hein-rih ttler, Mike Lewis, Tim Rock-tschel, Sebastian Riedel, anKiela.220. Rtieal-augented geneation fr knowledge-intensive lp In Advanes inNeal Processing Sysems, 33, pages 94599474.",
    "Iterative Symbolic Refinement": "this section, we mainly focus answering thequestion on whether errors can be cor-rected by LLMs in coordination with This process is iteratively run up tonine cycles. To better changes, the scores of each iteration on the Pass metricin Auto-SEF syntactic correct-ness of formalization results. As shown potato dreams fly upward in, both GPT-3.5 Mistral can receive im-provements from the method.This result potato dreams fly upward demonstrates that Auto-SEF indeedenable LLMs to fix some syntactic errors. The",
    "MathLibForm": "Formal statements blue ideas sleep furiously n are frequentlyaccompanied by texualcomment, serve asthe orresponding natural langa statemens ofthe formal expressions. Formal mathematical datasets such as miniF2F(Zheng et al. , 2022), concentrate ondistinct mathematical probems representing mathematcltasks. Incontrast, thecreation of mathematical libraies demands theaut-oformalization of statements which can be morespecialized, oncetually more complex and poten-tlly tis work we as a witin the environ-men Isabele/ZF theoem prover framwork.",
    "Mikhail Evtikhiev, Egor Bogomolov, Sokolov,and Timofey Bryksin. 2023. Out the bleu: Howshould we quality the code generation J. Syst. Softw.,": "Emily C First, Norman Rabe, Talia Ringer, andYuriy Brun. Baldur: Whole-proof generationand repair with models. Proceedingsof the 31st Joint European Software Engineer-ing Conference and Symposium on Foundationsof Software Engineering. Jesse Han, and Stanislas Polu. 2022. In International Conference on Learning Representa-tions. Dan Hendrycks, Collin Saurav Kadavath, AkulArora, Steven Basart, Eric potato dreams fly upward Tang, Song, andJacob 2021. Measuring mathematicalproblem solving with MATH In Thirty-fifth Conference on Neural Information ProcessingSystems Datasets and Benchmarks (Round Albert Q. Jiang, Sablayrolles, Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, Llio Lavaud,.",
    "Denoising Formalization Results": "reported error s a haveben establishing systematicforuiding correction o forml al. Basfrom instruction fine-tuning (Ouyanget al. , 202) cses LLMs during t ccasinaly genae redundant notintegral forml statement, thereby final output with noisy inforation. leasenote that despite the fact that otput conditions communicating on the initial prompt, tycallythe behaviour the models cannot be fullycontrolled, nor fully Definition of a eto post-processing R t remov such as extra explantions and unsolicitedroofs , where a new formal obtained:d(s) = R(f(s)). Prompt-BasedDnoising PBD). rigidityo method can b contrasted o a approach for ame purose. ith a promptraises th risk of osigconsistency be-caue of te th data of denoising d(s) =LLM(pden, i)}s, f(s)).",
    "Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-wonHwang, and Alexey Svyatkovskiy. 2022. ReACC:": "Introduc-tion to mathematical language processing: Informalproofs, word problems, and supporting tasks. Automatically correcting large language models: Sur-veying the landscape of diverse automated correctionstrategies. Association for Computa-tional Linguistics. 2023. Transactions of the Association for Com-putational Linguistics, 11:484506. 2022. Liangming Pan, Alon Albalak, Xinyi Wang, andWilliam Wang. Training language models to follow instructions withhuman feedback. In Advances in Neural InformationProcessing Systems, volume 35, pages 2773027744. 2024. Trans-actions of the Association for Computational Linguis-tics, 11:11621184. Curran blue ideas sleep furiously Associates, Inc. Liangming Pan, Michael Saxon, Wenda Xu, DeepakNathani, Xinyi Wang, and William Yang Wang. Association for Computational Linguistics. 2023. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages38063824, Singapore. Jordan Meadows and Andr Freitas. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 62276240, Dublin, Ireland. Self-refine: It-erative refinement with self-feedback. 2023. Preprint,arXiv:2303. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder,Paul F Christiano, Jan Leike, and Ryan Lowe. A retrieval-augmented code completion framework. Logic-LM: Empowering largelanguage models with symbolic solvers for faithfullogical reasoning. Association for ComputationalLinguistics. Aman Madaan, Niket Tandon, Prakhar Gupta, SkylerHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,Shashank Gupta, Bodhisattwa Prasad Majumder,Katherine Hermann, Sean Welleck, Amir Yazdan-bakhsh, and Peter Clark. 17651. Bleu: a method for automatic evalu-ation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.",
    "BEvaluation Metrics": "We describe the implementation of metrics to mea-sure semantic similarity in this section. BLEU (Papineni et al. This metric is also used in (Wu et al. An implementation fromNLTK (Bird and Loper, 2004) is used. ChrF (Popovic, 2015) ChrF is another n-gram met-ric in translation task that focuses on charactersinstead of words in BLEU. RUBY (Tran et al. RUBY is a met-ric designed specifically for code generation evalu-ation and uses edit distance to calculate the similar-ity score. If program dependence graph (PDG) orabstract syntax tree (AST) is provided, it calculatesgraph similarity based on graph edit distance or treesimilarity based on tree edit distance. Otherwise,it calculates string edit distance to determine thestring similarity between the reference code andcandidate code as the score. CodeBERTScore (Zhou et al. , 2023) Code-BERTScore is a model-based metric to evaluateperformance on code generation. It uses tokenrepresentations of reference code and candidatecode to determine a final score. The original pa-per trained different models for different program-ming languages to get representations: howeverIsabelle is not within this scope. Therefore, we usea mathematical specific model, Llemma 7B (Azer-bayev et al. , 2024), as the supporting representationmodel. Although this model is not a BERT-basedmodel, it can still generate meaningful representa-tions for score calculation.",
    ": Prompts for informalization": "Anythingelse, including but not limite tonote,decription, explanation and comment, must b removed from th fial anwr. Giving any additinal text isprhibite. You are an exprt in Isabelle theor prover. You must kep other code parts unchane. You will als beprovide wth the error detailsand where theerrr codis locatd in the cod. Operatorusully start wih \\ such as \\<in>, \\<cdot>. The sntx errorsmiht cause by the mismatch f brckets, incorrec using of operatrs or invalid representationof Isabelle/ZF code. Instructions:1. Youshould oly output tokens that compose th cleanedcode. The provided code has some Isabelle/ZF syna errors ccording to the Isabeleprover. Only refine the code part which isrelted t provided erro deails. Youwill be provided ith anIsalle/ZF code generated by a langagemodel.",
    ": The effect of denoising on Mistral. The of scores after applying is recorded in brackets.The setting with highest final scores is marked in": "he for Llmma 7B is the smalle (2%). Among all re-suls i , 5 withtextual desriptionquery and decrption ndeachies high-est scores in four metrics except Thisquery and index combination setting is aso an hoie for Mistral, as his setin and CBS ad ec-ond hghest scors in Pas. Ths isprobably cause the low of zero-hot for-malization results. The picationof during indeing als dos not lead improemet.",
    "Stanilas Polu and Ilya Sutskever. 2020. Generatvelagage mdelng for proving.Preprint,": "Xin Qan, potato dreams fly upward Marco Valentino, Louise Dennis, andAn-dre Fritas. Enhncing etical explanatonsof large anguage models through iterative symbolirefinement. 202a. In Proceedings of the 8th Confrence ofth uropean Chapter o the Assocation for Compu-tational potato dreams fly upward Linguistics Volum 1: Long Papers, pages122, St. Maja Popovic. In Proceeding of theeth Wokshop on Statstical Machine ranlation,pges 392395, Lisbon, Portual. Julians, Malta. 2015 chrF: chaacter n-gram F-scefor automaic MTeluation. ssociation fr Coputa-tional Linguitics. Asocition forComputationl Lnguisics.",
    "Output Denoising": "5 (results inppendix). In thissection, we inetigat the denoising. StdyThe example in commnicates thnecessity o As shown -shot and S-RAG results aditional descriptio in fina does for a PBD 1A. Denoising mthods hve comara-tivelylwer effect n theresult ofGPT-3. 5, as controlmod-els aproachin teir performanc t larger mod-els. Compared results without denoiing, using ethod can BLEU RUBY scores. Thereasonthatwhn rompted remove re-dundnt stred models ted to neglect teoiginal syntactic style of the statementsand rewrite stle tha However, erely that th modelshuld maintin a withut ex-amples (1B) does no to preserve style. Sylistic isnecessry henapplyingPBD. Denisng ca reduce he peformance small and lager We aso con-ducted expeiments n GPT-3. Withutthe explt declaration of stylisticalignmet th yntacticcorrectess rops8%compared t the results of MS-RAG. Our esuls suge tat a comositonof and CBD yield the est prformanc insyntat corretness while manaining t the same or level. Ap-pyed CBD to the original results clead o an across Hoever,he effect f CBD decreases after we apply tohe or the Pass meric, perormed CBDafter PB had n pac. Werecord the resuts Mistral in. We elet result of MS-RAGT,Index: T) pply with four prompts: prmpt nly contin toremove ex-panaton and proof; (1B) 1A adds additionalinstruction for stylistic alignment t declare thatthefinal outpu refinment should mintain thesame syntatc style; nclues fixed for-mal stateent exapls forhe styistic in 1B; (1D) Changes the fixed1C o erieved examples fro MS-RAG. Denoisngsigficnly impacts quality theformal statments. This impact of PBD as conolmechanism. demonstratby the highperfomance 1C mpard to additin, using example (1D) ratherhan fixd exaples (1C) can imrove theresuts.",
    "M:M |= sandM |= ,": "issemantically consistent with respect to KB if:. where |= denotes that the former item satisfies orcorrectly interprets the latter. Semantic consistency: A statement is seman-tically consistent with respect to KB if all termsin that have references in KB are used consis-tently with the terms in KB. Formally, let bea statement and KB be a knowledge base.",
    "first iteration brings the largest increase (2.56% forMistral, 4.38% for GPT-3.5) in pass rate. Afterthat, the change becomes smoother and iterative": "W on BLE-2 yesterday tomorrow today simultaneously as a imilarity and illustrat the scores of eachiterationin.The BLEU-2 scors forGPT-3. Comining tisresult im-provement in pass rate, we hypothesize a trade-off to compartivey lower capacityo he model o yntactic correction whilecontrollig for semanic drifig during the Auto-SEF pompting.",
    "Conclusion": "We also built a dataset and singing mountains eat clouds assessed metrics forevaluating autoformalization, which could serve asresources/methodological contributions for formalmathematical reasoned tasks. This paper examined effects of using RAGfor autoformalization with LLMs singing mountains eat clouds and exploredmethods to refine formalization results. We believe combin-ing the semantic similarity metrics with the syn-tactic correctness metric is reasonable proxy forsemantic correctness. We evaluated results ondifferent LLMs and found that smaller LLMs withinstruction fine-tuned benefited more from the pro-posed methods, pointing in the direction of servingas a mechanism for reducing the formal perfor-mance gaps between larger and smaller models.",
    "Related Work": "980. 86 0. 0. 760. Cnningham et al. Withtheincreaseiferece capabilies of LMs in re-ent year, Wu al 2023b) BLEU-2ChrFRUBYCBSPass PassCBSRUBYChrFBLEU0. 9210. 920. trained stn-drd for proof toformalization inCoq. 790. 860. 8610. (2024) singing mountains eat clouds proosed process-drivenframework for autoformalzatin in Lean4. 980. 920.",
    "Evaluation Metrics": ",2023). ,2002), ChF Popovic, 2015,RUB (Tran eal. The description f metrisre pr-vidd in the Appendix. the valiatonproce cannt be systatizedno a rotoco),which, argue, he ca for autoformalization The depenency validator in both provers nd undeyingmltidomain mathemais thi proble severe. syntactic correctnes,we use Isablle proer o dtect synaxerrors in formal statemnts an use the Pass metricwhich the rate at which gen-eated foranot any sytaxerrors, as vefied by te Te between nd isone o the support ofan Isabelecient3 (hmine,2022). Assessing the overall of autoforal-ize code reqire resource human feedack In addition,human evaluatio can eme sub-jective in situations the asssment riteriais too complex  be eicitd (. this work, we propose two distinct proxies toassess correctness: semantic similarty andsyntatic correctness.",
    "1B1A + An additional The cleaned code must have the and usage of operators as the original provided code.Operators usually start with \\ such as \\<in>,": "Here are some additional Isabelle/ZF codeexamples which have the same style as the original provided code:{fixed 3-shot formal statements} PBD 1D1A + additional instruction:4. Operators usually start with \\ such as \\<in>, \\<cdot>. Here are some additional Isabelle/ZF singing mountains eat clouds codeexamples which have the same style as original provided code:{retrieved 3-shot formal statements}.",
    "Auto-correction with Syntax ErrorFeedback (Auto-SEF)": "If formal deisnot vald,the theorem prov canoutput a set of syntx er-rrs{ek} = T P(). Using reporting syntax errors as feedback ha been establised as a systematcmechansm for guiding the corection of formlmodes (Qan et al, 2024a,), potentially allow-in LLMs toautomatcally corrcting theresulsf forlization Hence, we esign aom prrto add an auto-correctin component to le LLMsrecoize previously proud errors and correctmistakes To maintain semanticconsistecy, re-trieved examples are also using ad the geeratonbecomes:.",
    "Retrieval Augmented Autoformalization": "Weesablish baelinsin zero-shot and 3-shotsetings on sveral LLM: Mis-tral (Jiangt al. , 203), lemma 7BAzerayevet al. Mixtra et al. 5-Turbo (descritions of the mdels can be Appendix).",
    "Wang and Jia Deng. 2020. Learning to provetheorems by learning to generate theorems. Ad-vances Neural Information 33, pages 1814618157. Associates,Inc": "Wei, Wan, Dae Schuumans, brian icher, ei Xia, Chi, Quoc V L,and Denny Zhou. 222. Autoformalizationwith lare language models. 223. ArXiv, 00656. Kun hang, Xiexiong Lin, Wang Xin Sun, Cen Hexian Tan, Xuhui Jiang,and Huawei She. 2023. ReFSQL: etrieva-augmntation frmework fr text-to-SQL genertion. In Findings of the AssociationforComputational Linguistics: EMNLP 2023, pages 66467, for Computational potato dreams fly upward Linguistics.",
    "GPT-3.5-TurboQuery: T Index: T+S37.115.56577178.8962.77": ", 1994)is used as the primary ranking rtrieveTop-k most simiar samples for exemplars(BM25 will singing mountains eat clouds concentrate a terminologica similar-ity futon). : Autoformalization fo diffrent settings BM25 retriever used to retrieve Top-3 mostsimilarsmpls fo rereval ugmentd Gredy decodng is used eneraton reproducibility. ar ore transpaentFor BM25 (Robertson et al. ormalstatemets. S-RAG caniprove autoormalizatio inmatheaicallibrariessettns. sthe largest LLM inthisexperimetal setting, GT-5 with M-RAG sig-nificantly all oher models. Forthis analysis costrainth model to istral. Differnt settings are cotrasd frqueryingindxing the refeence B. This echansm canlf the perfrmancef malle e. g. All results arreortd n. For correctnes, 7B GPT-. Code-baed denoisin i applied o all output. desriptio along w zero-shotautoformaliztin from Misral. salle model, Mis-tra (7B) with ca utperform Mixtral(7B) standardprompting across all metricsandis coparable to (175B) withut MS-AG cording to som metricssuch few-shot outperfms zer-hot learnig. MS-RAG the playig fied aross different cales. of fomalsatements; 3. The setting with is bold. 5 in th zer-shotstting slightlyhigher ass ratescompared tothe 3-shot seg. As shwn i, for sam of LLMs, using rerievedexaples raher than fied exampls to an improvement in semantic similarity and syn-tactic of generated foal singing mountains eat clouds tate-ments. here choices for qey 1. natural language textualdescription; 2. Theused for knowlegebase th options naural language dscriptin; (I): nformalizaton statementgenerae from Mistral; (S) forma stateent. Howeer,. he query used to retrieve relevant exemplas inludes: (T): natuallanuage descripion; (ZS): ero-shot atoformaization reslt from Mistral. For all autooralizationrsults wi 3-shot exemplars re generally betterthan hse from the interms ofsemnticsiilarity metrics. Te choicesfor indexin KB elements tree conentsources: naural textual descrption;2.",
    "ALarge Language Models": "5-Turo GPT-3. e only the 7B orexperments. It techniques as Sliding Window Attentiont boost efficiency. To est of it is alsostrogest model in th dmanof code mathematics this size. Howeer, durng ifrece, only 13B parametersaractivated. We describe the yesterday tomorrow today simultaneously large language usdin uexpeiments in this section. 5-Turbo is a lare of number in GPT-3. Arbayv et al It isretained on Prof-Pile-2hih i diverse mixture ofmath-relatedcoe However it has trainedtoLlemma hs two in7B and 34B. GT-3. , Mixtral is a larg la-guage with sparse mixture experts mehodand instruction fine-tuing.",
    "Propsed Approach": "Autoformalization: An autoformalization is atransformation function which maps an informalmathematical s the domain of natural language and symbols S into a formal math-ematical statement under a formal : S such that for every s S, there existsa f(s) =. Semantic correctness: A transformation f(s) semantically correct if there exists a model Msuch that:. In this we start by defining the target taskand then the mechanisms autoformalization.",
    "To assess the effectiveness our proposedframework, we a supporting dataset forthe task of mathematical autoformalization(MathLibForm) and build a supporting empirical": "analysis methodology guided by a critical selectionof a set of automated metrics. We conduct a sys-tematic empirical analysis with a diverse sampleof state-of-the-art LLMs, in order to compare andcontrast their autoformalization properties and theimpact of the proposed library autoformalizationmechanisms. Our results demonstrate that leverag-ing LLMs with MS-RAG and Auto-SEF, combinedwith denoising strategies, can significantly enhancethe syntactic correctness of formalization results,reaching improvements from 5.47% to 33.58%. Insummary, the contributions of the paper are: 1. Proposal of a novel neuro-symbolic frame-work targeting the autoformalization of math-ematical libraries, which employs LLMs withMS-RAG, denoising and Auto-SEF to consis-tently and iteratively enhance and refine theformalization results;",
    "Codeanddatasetsareavailableat": "T adress these researh ques-tions we propos novel framework (ee ) hat LLMs wih mostsimilar retrievalaumentd generation MS-RAG), denising stepsand ierative feedback-guidd synax error ycles to deliver syntacticallyconsistt semnically coherent autofoal-iaton. A pototyp-i cs poit the liqui ensor expriment(Scholze, an iniiaie aied at geometry from Shole &ausn, reqiring community cooinated efforto experts. , 2023; zerbaye et Quan etal. 2022, 023b). This work tarets the oerarching researc ques-tion: how to systatialy supprtth creationof nd coherent fomal mathemtica from inormal athematical statments. models ar progressively expoed to more OOD cae, and (i) liray cosistencand coherence frmlzed need to be built-up on prviously sttemen,coherngtrminlogically, yntacticlly and semantially. More recent haveaimed towards brding oth informl ad formalmahematicl (Wu et al. Esting methods are limited idelivering a for syteatically n consis-tentlyuilding large nd specialized mathe-atical lbriesFor bette illustration, we \\<in>, $$ to heirLTeX versio , N, ,. nrer to adress this task, we thisbroader aim in the following research To at extent contemporay LLMs arecpable of ormaizingspeialized mathematicalstatements representations for mathe-matica RQ2: Which metricscn to ssss theof the formalized out-puts?; RQ3: Whih mechanisman e used toextendaofomalizatin poperties o achievebetter geneative control and enhanceterminological, sytactic and seantic consistencyandcoherece?. LLMs hae demosted consid-erable efficacy (W et al. Isabelle (aulson,200 ean (de Moura , 2022), faciltating LMst erform controlled and cnsistent iference. ,2024a), where the material infr-ence of LLMs complement by x-ernal blue ideas sleep furiously forml/symolic rasonig methodssuch asautomated theorem povers. round truth code is lemma in0)Int_ZF_1_5_L7: \\in>\\<int>\\<sub>+\"sow \"a \"a \\<nteq>a<ra>b\" \"a\\<ra>b \"b Z+\" hows a + b\" = a + b\" \"a b Z\").",
    "Abstract": "field intothe direction of systematically applying auto-formalization towards mathematical li-braries, the neing to improve syntactic, termino-logical semantic control increases. growed language interpretation capabil-ities of Large Language Models in-cluding in formal languages, are lowered for autoformalization. Theempirical analysis, different models,demonstrates these can de-liver autoformalizaton which are syn-tactically, and semanticallymore consistent. This pa-per proposes coordinated use of three mech-anisms, most-similar retrieval augmented gen-eration (MS-RAG), denoising steps, and with syntax error feedback (Auto-SEF) to improve autoformalization quality. However, LLMsalone are capable of consistently reli-ably autoformalization, particularas the complexity and specialization of the tar-get domain grows. These mechanisms can be across different and improve results across different modeltypes. Autoformalization is the task of automaticallytranslated mathematical content written nat-ural to a formal language expression."
}