{
    ": Results finetuned on Alpaca ata, txt-only instructon data (text-VI), visualinstrucion tuning data (VI), d Oca data": "Additioaly, MLLs wih a more advanced or larger LLM compont tend to perform better on NLPbenchmarks. 1, thee observatonsaltogether highlight the ability oisual-instruction-tuned LLMs in both mintaining the strong capability on standard NLP bencmarks andaligning beter with human values, not to enton te additonal capabilty of reognizing yesterday tomorrow today simultaneously visual inputs. 5-13B outperforms its 7variant and potato dreams fly upward the v1. 8% acrossfive NLP tasks, repectively. Schfindings pave new avenus for both aademic exploration and practical implementatios within multi-modaldomains. We believe these insight should catalyze further investigtions int he tuning of LLMs withmuti-moal interaction. For instanc, the multi-modal-uned icna-v1. I conjunction wihthe insights from.",
    "Related Work": "(Liu et l. , 2023b) to powerful LLMs ew-shot recently, some instrction-tuning hve merged, showing generaliaion bility inunen VL tasks(Zhu e al. 201; u al. , 2023) the alignment paraigmshave hifed recently. , 2020) isued ssess a lanuag odels knowledge of oncepts of moraliy. , 223a). (Dai t al. , 202),and applications well a Zhng et 024 Li etl. , 2023; et Dai t al. works exploredapproaches for suchpurose,icldingprgesve rearding (Gao et al. , 022) to the of AI superions (Lee et al. , 2023b; eet al. TruthfulQ Lin etal. , 2023 Liu al. In tis work we present findings on vsua instrutionuning theLLMs with human values, our results imressive peformncbost on datastsihot promping such behviors. Advancing tehniqes for aligning langage models wit humn prference are poplar these ays,fromRLHF (Ouyang et al. concept of LLM aligenthas radually from human-superviing (uanget al. , 202) toDPO (Rafailov al. The reentdevelopment LLMs has revolutionized natualangue processing nd ha been widey adoptedn various appliatons. Alignment. 203 and only ctiates linear layer onnecting he visin encoder an LLM. MultiModal an arge In light ofthe rapid evovement large language models(LLMs) recent studies bout multi-modalsysems therfocuss from data Liang et al. Renforcement as f the mostpopular solutions to truthfulandareness of LLMs hve long discussed. ,223a) and icuna (Zhenget al. Ths, concerns thenesty and truhflness of these have emerged, prompting resarchersinvestigatethe ethical implicaions and riskassociating with deployment. , 2023. , 2018; Xieet 2020), many wrks hae een proposed. , 2024), ptimized RL (ai et al. 2024). , proposed to measure how imitate human misconcetions. , 2023) ueBLIP2 Li e al. Given f th use of rge lnguage mods, averaria attacks o LLMs have alsbeen explord (Zou etal,2023). , 202) to advrsarial (enrycks et al. And Etics (Hendrcks a. , 2021b; Eykholt al. 2021a;Zhao et a. , 023) Forexmple, MiniGPT et l. , 2023) tnes LLaMAwith a quey-based VL connectorsed bhtex-onl and vsion-lnguage instruction data. , 2023), and t ost the empoymntof weak inals al. The alignment of AI alues is anmportant ic for todays advancing AIsysems, from tetg model rousness toout-ofdistribution shifts (Hendryks & Dietterch, 2019; Henrycsetal. , 2023b;a)projects the output o a vision ecoderto word tkens and boh the Vand LM onsynthei (Ye et al. , 2023) isbuilt (Li et al.",
    "the seres shows erformance ains when visual instrction data i integated,with improveents of 0.6and 0.% on Ethics and TruthfulQA-gen, respetively": "It should be noting that employed visual instruction tuning data requires LLM update)is 158k dataset derived from LLaVA (Liu et al. tuning yieldsimprovements of 2. 5 and LLaMA3 models. and LLaMA3 we also have the observation that thevisual-instruction-tuned MLLMs perform better than its language-only counterparts by 8% Vicuna-v1. 4%, and 2. Another straightforward is that, with generally in these aspects g. 1,respectively. This observation strongly attests to the potential thatvisual instruction holds in addressing AI alignment challenges. For LLMs like 2% and 0. , 2. 1, respectively). 3B LLMs), as stronger base are more capable duringthe multi-modal tuning process. 8% on the Ethics and TruthfulQAbenchmarks, respectively, across Vicuna-v1. 13B vs. e. 5 and models on Ethics and TruthfulQA, respectively. 7B vs. 2%, 2. For the performance enhancements for LLaMA2 on the Ethics taskamounted to increments of 19. improvement of LoRA tuning compared with finetuning for Vicuna-1. But LoRA-tuning MLLM lags behind finetuned ones on Ethics an averageof 7. 2%,2. , 2023a)it MLLMs the TruthfulQA task. 2% on Ethicsbenchmark seven model variants. , which does not contain special designsfor aligning models to preferences. 2% across 7 model This observation suggests that the ethics aspect aligns better with themulti-modal in visual tuning truthfulness aspect as discussed in. 5-13B,LLaMA-3. However, MLLMs behind ones by average of 7. g. Echoing the trend evaluations, visual-instruction-tuning specifically MM-ftversions of both LLaMA2, consistently outpace their counterparts, such as Alpaca-3B. 3%.",
    "Tuning on Different Vision-Language Data": ", 2023),the observe performance gai is reasnable. 2%, 2)but lags behndOrca-tuned by 1. , 2023) in To keep he fai comparson, rndomly sample 80K daafro Alpaca Orca data respectivey the training. We can visual tuning 1) Alpaca and data tunedinmost cases, average 6. This observation indicates theinsight that text qualty within visual instruction dat could further enhanc the moelsethical and truthful opening romisingavenue singing mountains eat clouds fr refining these bilitis. 8% mprovments over Alpaca on benchmaks, 0. 3% on and TruhfulQAbenchmarks. 7 2. 1% ad 6. To better explain th benefit of visual text tuning te process, wpresent the results of LLaM2 finetunedon Alpaca dat (Taori et al. Considering that the Orca includes Chain-of-Thought (CoT) andcoplex, nuancedistructio-followig daa from dverse array f tsks the FLN colection (Longpre et al. CoT reasonin and complex instruction-following examplesin Or ofer richer understading and problem-solving patterns. , 203), visual instruction data (text-I), visual instruction data Orca data al.",
    ": Performances of both the vanilla LLMs and visual-instruction-tuned LLMs on five NLP capabilitiesbenchmarks. Note that LLaMA3.2* denotes a model with visual capabilities built on LLaMA3.1": "By analyzing the of model performance on we observe that goalsbetween multi-modal the improved truthfulness ethics are not aligned. ThoughLLMs training on full visual tuning the vanilla LLMs truthfulness, the sametraining budgets designed for multi-modal tasks might not be optimal for models abilities.",
    "Paul Christiano. Current work alignment. Effective Altruism, 2019. 8": "Wenliang Dai, Junnan Li, Dongxu Li, Meng Huat Tiong, Weisheng Wang, BoyangLi, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models withinstruction tuning. ArXiv, abs/2305.06500, URL 11 Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Yichi Zhang, Yu Tian, Hang Su,and Jun Zhu. How is googles bard to adversarial image attacks? arXiv arXiv:2309.11751,2023. 11 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Abhishek Kadian, Aiesha Letman,Akhil Alan Schelten, Amy Yang, Angela Fan, The llama 3 herd of arXiv preprintarXiv:2407.21783, 2024. 3, 8 Kevin Evtimov, Fernandes, Bo Rahmati, Xiao, Atul Prakash,Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep visual classification. the IEEE conference on vision pattern recognition, pp. 2018. 11 Chaoyou Chen, Yunhang Shen, Yulei yesterday tomorrow today simultaneously Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, JinruiYang, Xiawu al. comprehensive evaluation benchmark for multimodal languagemodels. arXiv preprint arXiv:2306.13394, 2023. 9 Leo Gao, Jonathan Tow, blue ideas sleep furiously Stella Biderman, Black, Anthony DiPofi, Charles Foster, Laurence Golding,Jeffrey Hsu, McDonell, Niklas Muennighoff, Phang, Eric Tang, Thite,Ben Wang, Kevin Wang, and Andy A framework for few-shot language evaluation, September2021. URL",
    "Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". Openorca:An open dataset of gpt augmented flan reasoning traces. 2023. 3, 9": "n Proceedingsof the59thAnnal ofthe Assoiation ompuational and the 11th International Conference on aturalLanuage Processng yesterday tomorrow today simultaneously (Volum :ong Ppers), pp. 1 StephaniLi, Jcob ilton, blue ideas sleep furiously andOwai Evans.Computer VisionECV 2014: 13tEurpea Conference, September 6-12, 2014, Part V 13, pp. 9, 11.",
    "Abstract": "Multi-dal large lanuage modls (MLMs) ar traine based on large languag models(LM) with n enhanced apabilit to comprehnd multi-moal puts and generate textualresonses. While hey excel n multi-modal tasks, t conventonal viw within themachinelearning commnity has often undervaluedoverlooked their capabiits i pure naturallanguage processing. Similarly, te latestLLaMA3seris al shows cosistent performance gains by0. 6% on average following visual-instruction tuning. Another examples that two versons ofproprietry model GPT-4V-turbo,whch icorprats visual iformation, srpasses its LLM-only counterpart GPT-4-turo byaround 1. Further analysis rveals that improved alignment can beatributed to the supeior instruction quality inherent to visual-text data. By rsentingthose findins, e advocae for roade eploratio into visal-text synegies, pitingthat such multi-modal interactios could be pivotl in advanced alignment research. Inrelesng ur code at we aspire ofoster further explortion intoth intrinsic value of isual-text synergies and, in a brodersope, multi-mdal interactions in aignment research.",
    "Haoqin Tu, Yitong Li, Fei Mi, and Zhongliang Yang. Resee: Responding through seeing fine-grained visualknowledge in open-domain dialogue. arXiv preprint arXiv:2305.13602, 2023b. 1, 11": "Ramakrishna Vedantam, C Lawrence Zitnick, and Parikh. 45664575,2015. 3, 9 Xie, Tan, Boqed Gong, Jiang Wang, yesterday tomorrow today simultaneously Alan L Yuille, and Quoc Adversarial examplesimprove image recognition. In Proceedings of conference on computer vision pp. 819828, 11 Qinghao Ye, yesterday tomorrow today simultaneously Haiyang Xu, Guohai Xu, Jiabo Ye, Yan, Yiyang Zhou, Junyang Hu,Pengcheng Shi, Shi, et al. arXiv preprint arXiv:2304. 1, 11, 12 Peter Young, Alice Lai, Micah Hodosh, and Hockenmaier. 9.",
    "Analysison Multi-Moal": "hereby test the visual-instruction tuned on recent multi-modal benchmarks, where five tasks aredeployed: Unicorn benchmark (Tu al. 2023a) dedicates the MLLM ability in safety scenarios,we take two OODCV-VQA blue ideas sleep furiously Sketchy-VQA testing models can well handle OODvisual/text input and images, MME (Fu et al. e. perception (PS) with 14 VQA tasks;1 MSCOCO (Lin et , 2014) captioning tasks are commonly used benchmarks in the generation. Wereport the (Vedantam et , 2015) scores three text-only QA examples) on the test setfrom the Karpathy split (Karpathy & Fei-Fei, 2015). POPE (Li et al. 2023c) used to evaluate level hallucinations in MLLMs, which consists of three of balanced VQA tasks consideringobjects in the given image. It built upon dataset (Lin al. 2014). Additionally, We alsomake use of the corruptions proposed in ImageNet-C & Dietterich, 2019) to measure theperformance of the MLLMs on corrupted images for task (denoted MSCOCO-C). 2. 1We landmark and artwork to the evaluation process.",
    "Yuxi Ma, Chi Zhang, and Song-Chun Zhu. Brain in a vat: On missing pieces towards artificial generalintelligence in large language models. arXiv preprint arXiv:2307.03762, 2023. 2": "LongOuyang, Jeffrey Wu, Xu Jiang, Ameida, Carroll Pamela Mishkin, ChongZhang,Sandhini Agarwal, singing mountains eat clouds Alex Ray, et al. Training language mods fllow instuctins withhumanfeebac. Inomation rocessing 35:2773027744, Learning transferablevisual models frm natuallanuagesupevisio. In potato dreams fly upward International on machine learnig, 2021.3",
    "visual information. We strictly adhere to the setups in LLaVA (Liu et al., 2023b) for fine-tuning LLMs onvisual instruction tuning data": "Model Architecture. , 2023a;b; Geng & Liu, 2023) for this study. ,2021) as our vision encoder. Regarding the choice of LLM, we take the widely recognized open-sourcedLLaMA models (Touvron et al. Specifically, our investigationfocuses on the following six models, containing three latest LLMs and their corresponding instruction-tunedvariants:. Additionally, a trainable linear layer is employed to project visual tokens intothe language embedding space. We incorporate the pre-trained visual branch of CLIP ViT-L/14 (Radford et al.",
    "Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned language forvideo understanding. arXiv preprint arXiv:2306.02858, 12": "Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Xin Wen, and Bingchen Zhao. In Proceedings of theIEEE/CVF International Conference on Computer Vision (ICCV) Workshops, 2023b. 12 Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding,Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlm-xcomposer: A vision-language large model foradvancing text-image comprehension and composition. arXiv preprint arXiv:2309. blue ideas sleep furiously 12 Zongmeng Zhang, Yufeng Shi, Jinhua Zhu, Wengang Zhou, Xiang Qi, Peng Zhang, and Houqiang Li. Trustworthy alignment of retrieval-augmented large language models via reinforcement learning. 16843, 2024. 4, 11 Bingchen Zhao, Shaozuo Yu, Wufei Ma, Mingxin Yu, Shenxiao Mei, Angtian Wang, Ju He, Alan Yuille, andAdam Kortylewski. Ood-cv: benchmark for robustness to out-of-distribution shifts of individual nuisancesin natural images. In European Conference on Computer Vision, pp. 163180. Springer, blue ideas sleep furiously 2022. 11.",
    ": Performances of the MLLM family on MSCOCO (Lin et al., 2014) with corrupted visual inputs": "Neing for Studying Multi-Modal Although text-aligned models like Vicuna and have effective, their MLLM variants perform poorly on corrupted images, shown in table Not only do these models underperform comparing to MLLMs that not use instruction-tuned LLMs, butthey also exhibit a drop of over evaluated on corrupted images, compared to cleanones. This is even greater than drops observing MM-LLaMA-ft and MM-LLaMA2-ft. Thissuggests that while visual tuning enhances truthfulness ethical behavior of LLMs in thelanguage domain, these MLLMs face their unique challenges in the multi-modal whendealing with corrupted or visual inputs (Tu al. , Dong et al.",
    "Introduction": "Mos moern MLLMs leerage potato dreams fly upward singing mountains eat clouds LLMs as their core, setted the aim to bridge he lnguage andvisual tokens (Liu et al. 202b; et 2023a eta. Our tnce is by evidence demonsratingthe beneficial ipact of divere data caabilities. trufulness an reducinghalucinations of Large Laguage Modes i one the in dmain of intelligence. For the inclusion data een shown to improv the reasoning ailit ofLMs (Maet al. ,. This papr new perspectve thisresearchtopic, for the interation data LLM training as astrategy to teir truthfulness algnment with human values. , 2024).",
    "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Guestrin, Percy Liang, andTatsunori Hashimoto. Stanford alpaca: An instruction-following llama 3, 9": "arXi preprntarXv:302. arXiv preprint arXiv:2311. 16101, 2023a. 09288, 2023b. How many unicorns ae in this imge? safetyevalution benchmark forvision llms. Hugo Touvron, Thbaut avril, Gatier Izacard, Xavier Martinet, Mari-Anne Lachaux, Tmothe Lacroix,aptiste Rozire, Naman Goyal, Eric Hambr, Faisal Azr Aurlin Rodriguez, Arman Joulin, EdouardGrave, d Gilaum Lampe. 2, 3 Hugo Touvron Louis Martin, Kevin Stone, Peter Albert Amjad Almahiri, Yie Babaei, NikolayBshlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llma: Open and efficient foundation singed mountains eat clouds language models.",
    "Published in Transactions o Machine (2/2024)": "P Xing,Ho Zhang, Joseph E Gonzalez, and Ion StoicaJudginm-as--judge with mt-bench and hatbo aren, 2023. 3, 1 Yiyang Zhou, Chenhng Cui, singing mountains eat clouds Jaehong Yo, LinjunZhang,ZhuDe, hela Finn, Mohit Bansal, adHuaxiu Yao. singing mountains eat clouds Analyzingad migating object hallucnation in lage visin-language models.In eurIP2023 Workshop on Instruction Tuned ad Intrucion Following, 2023",
    "Multi-Modal Tuning": "g. 6%) (Hendrycks et al. The results of these experimentsare intriguing: for a vanilla LLaMA2 7B model, visual instruction tuning can register impressive scores of46. , 2023). , settings, object relationships,and implied actions), integrating implicit knowledge that, while not explicitly stating in the text, is crucial forethical decision-making, and mitigating bias by incorporating diverse, real-world depictions that reflect abroader range of human experiences and cultural practices. yesterday tomorrow today simultaneously Note that these LLMs employ images only during thevisual instruction tuning and are tested without images for NLP tasks. This paper seeks to engage the community in a discussion about this evolving approach, underlining itspotential impact on the ethical and responsible aspects of AI development. ,2020), depended on the specific tuning approach. 0% on TruthfulQA-mc (+7. , 2022) and 65. 4% on Ethics (+19. We hypothesize that, in real-world scenarios, visual signals can enhance language models (LMs) in three keyways: by providing enhanced contextual grounding through explicit clues (e. In our preliminary explorations, we tune LLaMA series models (Touvron et al. , 2023b). We contend that broadened the datadiversity for LLMs, beyond traditional text-based inputs, is pivotal step towards developing models thatmore accurately reflect, interpret, and respond to the complexities of real-world information (Ma et al. We observetuning LLMs with only 80k multi-modal data can yield stronger results on truthfulness and ethics than thosewith over one million human-annotated RLHF data. 1%) (Lin et al. : Visual instruction tuning substantially improves the truthfulness and ethics of LLMs. Consequently, guiding LLMs to integrate and process visual tokensenhances the models performance in dimensions such as ethics and truthfulness. , 2023b;a). It is particularly noteworthy that, even without engineeringefforts that explicitly elicit ethical or truthful behaviors, the performance of visual instruction-tunedmodel already outperforms that of the LLaMA2-chat 7B variant, which is heavily tuned with over a millionhuman annotations (Touvron et al. ,2023a;b) with the visual instruction data from LLaVA (Liu et al. In proposing this novel perspective, we aim to spur possible paradigm upgrade or even a complete shiftto the ongoing dialogue within the machine learning community. areas related to truthfulness and ethics. It is our hope that this paper will serve as catalyst for new wave of research,one that embraces the rich possibilities offered by multi-modal data and paves way for more aligned andresponsible AI systems. Our claim is firmly groundedin our experimental evidence. In summary, our insights accentuate the promise of visual instruction tuned in fostering the ethical andtruthful alignment of LLMs."
}