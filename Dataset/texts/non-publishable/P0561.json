{
    "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,Feng Zhu, and Rui Zhao. 2023b. Shikra: Unleash-ing Multimodal LLMs Referential Dialogue Magic.CoRR, abs/2306.15195. ArXiv: 2306.15195": "03766. MobileVM V2: Faster andStroger Baseine frVision Languae abs/202. 204. 06500. 2023b. ssocitionfr Comutational Linguistics. ArXiv: ArXiv 2305. Vicna: Open-SourceChatbot wit 90%* Chat-GPT Qulity. Wei-Lin Zhuohan Li, Zi Lin, Ying u, HaoZhang, Lianmin Zhen, SiuanZhuang, onghaoZhuang, Joeph Iontoica, Xing.",
    "Agrim Gupta, Piotr Dollr, and Ross B. Girshick. 2019": "In Tenth Inter-nationa Conferenc Learin Repesentaions,ICLR 2022, Even, 25-,. Yelong Shn,PhillipWalls, ZeyuanAllen-Zhu, Yunzhi L, Shean Lu andWeizhu LoR Low-RnkAdaptationof Large Language Moel. In IEEE potato dreams fly upward Confernce Compter Visionad Recognition, CPR2019, Beach,CA, USA Jue 2019 ages 5356564. Cm-puter Viion blue ideas sleep furiously / IEEE.",
    "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023c.Visual Instruction Tuning.CoRR,abs/2304.08485. ArXiv: 2304.08485": "Ilya Loshchilov and Frank Hutter. 2019. DecoupledWeight Decay Regularization. In 7th InternationalConference on Learning Representations, ICLR 2019,New Orleans, LA, USA, May 6-9, 2019. OpenRe-view.net. Jiayed Lu, Jinmeng Rao, Kezhen Chen, XiaoyuanGuo, Yawen Zhang, Baochen Sun, Carl J. Yang, andJie Yang. 2023. Evaluation and Mitigation of Ag-nosia in Multimodal Large Language Models. CoRR,abs/2309.04041. ArXiv: 2309.04041. Junhua Mao, Jonathan Huang, Alexander Toshev, OanaCamburu, Alan L. Yuille, and Kevin Murphy. 2016.Generation and Comprehension of Unambiguous Ob-ject Descriptions. In 2016 IEEE Conference on Com-puter Vision and Pattern Recognition, CVPR 2016,Las Vegas, NV, USA, June 27-30, 2016, pages 1120.IEEE Computer Society.",
    "Hongyu Hu, Jiyuan Zhang, Minyi Zhao, and ZhenbangSun. 2023. CIEM: Contrastive Instruction Evalua-tion Method for Better Instruction Tuning. CoRR,abs/2309.02301. ArXiv: 2309.02301": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang,Conghui He, Jiaqi Wang, Dahua Lin, WeimingZhang, and Nenghai Yu. 2023. OPERA: Alleviat-ing Hallucination in Multi-Modal Large LanguageModels potato dreams fly upward via Over-Trust Penalty and Retrospection-Allocation. 17911. ArXiv:2311. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Re-nard Lavaud, Marie-Anne Lachaux, Pierre Stock,Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-the Lacroix, and William El Sayed. 2023. Mistral7B. ArXiv: 2310. 06825.",
    "With spaCy v3 EN_CORE_WEB_SM2BAAI/BGE-BASE-EN-V1.5 (Xiao et al., 2023)": "them as statements (e. , There a man) of in context of the image, isthen (2) verified with a VQA model (question: Isthe following statement correct?). relieson GPT-4 to facts but this too expensivefor our use a LLM3 after verifying that it follows task We use OFA (Wang et (Liuet singing mountains eat clouds al. , 2023b) according Jing et al. (2023). Next the hallucina-tion measures, we two standardmetrics to monitor how grounding objectives af-fect the general caption CIDEr (Vedan-tam et al. CLIP-Score, a metric, is the cosine the image and caption by a CLIP model (Radford et al.",
    ": Qualitative examples of Vicuna +RE+GCfor standard and grounded captioning. Hallucinationsare underlined in red. Predicted bounding boxes arevisualized in the image and marked in the caption": "examle, te moel fully hallucinaes womanalong with a bounding ox fo her. I the secondexample, the second elephant bouding box ispostionally correctin hat it points to an animal,but hatanimal is a rhino. In the third example, im-ilarly, the bounding box correctl containsn applebt theattribute peeled is hallucinated.",
    "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,Shaohan Huang, Shuming Ma, and Furu Wei. 2023.Kosmos-2: Grounding Multimodal Large LanguageModels to the World. CoRR, abs/2306.14824. ArXiv:2306.14824": "AnaRohrbach, Lsa Hendricks, Kaylee Burns,Tevor Darrell, andSaenko. Large Muti-modal Mdels with Factually Augmented ArXiv: 2309. CoRR,abs/2302. CoR, abs/2311. Cervantes,Juan Shraman Pramanick, Guangxing Rui Hou, SayaNag, Ser-Nam Nicolas Balls, Qian ang,Rama hellappa, and Almahairi. Zitnick, and DeiParikh. In Proceedings the 38th Coferece o Machin Learning, ICML2021, 18-24 July 2021, Virtual Event, volume ofProceedings of age87488763. JackofAll Tasks Master o Man Designing General-purpose Vision-Langage Model. In IEEE Confeenceon Com-puter Vision andPattern Recognitin, CV 2015,Boston, MA, U, 7-12, pages 45664575. Alec Wook Kim, Chris Hllay, Aityaamesh, Gabriel Sandhi Agarwal, Amanda Askell, Pamela iskin, Clark,etchen Kueger, Ilya Learn-ing Transferable Visual Models FromNatural an-guage Supervisio. 1242. Itnational Confeence on Machine Learning,ICML2022, July 2022, Baltmore, Maryand, USA,volume of of Machine LearningReserch, pages 233183340. 00020. Shuai Shao, Zeing Li, Tianyuan Zhang Chao Peng,Gang Yu, Xiangyu Zhan, Jing Li, and JianSun. 03079. Tuvon,Thibaut Lavril, Gautier zacard, XaierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptste Rozre, Naman oyl, Eric Hambro, FaisalAzhar, Aurlien Armad Joulin, EdouardGrave, Guillaume Lample. arXiv preprnt, abs/2103. and Analyis of Halluci-nation Larg Vision-Language Pengang, A Yang, Rui Men Junyang Lin, ShuaiBai, Zhikang L, Jianxin Ma, Chng Zhou, yesterday tomorrow today simultaneously JingrenZho, and ongxiaYag. makrishna Vedantam, C. 2023b. PMLR. iqin Sun, ShengShen, Shcao Cao, Haotian Liu,Chunyuan Li, YkanShen, Chuang Gan, LiangYaGui, Yu-Xiong Wang, Yiming Yang, singing mountains eat clouds Ku Keutzr,and Trevor Darrell. 2022. 2019. for Linguistics. Plumr, Lwei Wang, Cris M. IEEE Computer Junang Wang, Yiyang Zhou, Xu, Chenlin Zho, Haiyang Xu, Qinghao e, MingYan, J Zhang, Jihua Jita Sang, and Haoyuang. Weihan Wang, Qigsong L, enmeng Yu, WenyiHong, Ji Qi, Yan Juhui Ji,uoyi Xixuan Song, Jiazheg Xu,Janzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Jong Chris Hallacy, AdityaRamesh, Girish Sas-try, manda Askell, Pamela Jack Krueger, and Ilya Sutskver. Sentnce-BERT: Sentence Embeddings using Siamese In Procedins2019 Conference Methods Natal LanguageProcessingand the 9thInternationalJoint Conference on Nat-ural Langage Processing, 2019,Hong Kong, China, November 3-7, pages 9803990. CIDEr: Cnsenss-based image de-scription evaluation. _eprint: 212. 13971. 03079. 2019. 2018. CoVLM Expert for Pretrained Lan-guge Models. 2015. ArXi:2311. 14525. Objects365: A High-QualityDataset fr Detection. In IEEE/CVFInternatinal Conference on ComputerVision, Korea (Sth), 27 Novembe2, 209, pages8429843. _eprint: Nils Reimers and Iryna Gurevych.",
    "Limitations": "There are main limitations to our analysis.First, while we for comprehensive of effects of different training objectivesand task mixes on downstream hallucination, of modeling that we hadto fix (i.e., we could not other variants)primarily w.r.t. to architecture of the LVLMdue to limited computational One alia, consider a different encoder, addi-tional LLMs, and/or alignment modulesother the MLP or due our limited budget,we train our models on and for fewersteps than a lot other work that trains Chen et (2023b); Liu et al. (2023b); Baiet (2023)); we thus cannot rule out that a reduc-tion in hallucination to grounding emerge at some of groundingtraining.Second, our findings are anecdotal ev-idence from qualitative analysis of a lim-ited number of examples) on reliance onimperfect automatic metrics. While this is com-mon practice in related work as well, we likelihood of robustness of our findingsand by employing two hallucination metrics,CHAIR FaithScore 3), well as addi-tionally proposing a to CHAIR(CHAIR-MEN, see",
    "Introduction": "adp-tion, however, is indred by object hallucinationin whih the to general hallu-ciation of LLMs (Zhang e l., 023b)inventobjects (or attributes of relations betweenot presentinimage.A of methods have recently bee proposedto addres hallcinatio as modified decoding strategies (eng et al., 223; et al.,223), post-oc removal of halluciat cnten(Yn etl., 202; et al., 2023), or reinforce-ent(Snt 2023; hao al., 2023b;Gunjal et 2023; Yu al., The claim is intuitive: region-level obectivesemand finer-grained image understaning thanthe imae (de LVLMs), a deonstrated copositionality Bugliarello al.,203). Such shoul hus, intuitively, dis-couage odels from they round in the image. Intuition aside, empir-ical or th claim that grounding objectvesreduce LVLM is eak and minlylimited to uetion-answering (A) style of evalu-ation in hich te model is asking aboutexisence of in an image al., 2023b);w arguethis evluation porly alignwih real-world tet generon taksrimariy open image captioningor thereis no empirical eidence yet object groundingreduchalucination In thi wor, we performthe firstcoprehensive othe effects objectives have on LVLM objet halucinatinn open (ie. free-form) image captioning, hortcoming o existing hllucination evl-uation protocols. e thefor L variant tained wth awithout thee gruding To this nd,wcompare the hallucinaton measres bsed answering (QA) (Li et al., 2023b) agansfree-fom metricsor open captining (Rohrbachet al., 2018 Jing al.203). Citically, (1) existig evaluaon masures andpoo-colset a., 2018; et a. 202b) elyon MSCOC et al., 2014) and (2) is part raining for mst weargue that exstig measures ae lely toundresti-mate LLM halluciate; wethus extd halu-cinaion ealuation proocolto that LVLMs will ot seen in traiing. ofgrounding captons atnferene time, on the otherhand, reduces obect halluciationsbut theeffect small and cost of(slight)reduction in caption etailedness",
    "Results": "We now the observed hallucination both protocols: in free-form captioningand in QA-based hallucination evaluation (as blue ideas sleep furiously indi-cated by metric/protocol). The potato dreams fly upward reportedCHAIR results correspond our CHAIR-MENvariant; we report results obtained with thevanilla based string in Ap-pendix C.",
    "BLong Captions": "shows long captioin results. brevity,w oly the rsults Objects365 for MSCOCO blue ideas sleep furiously andtheresults ar qualitatively the sam. the etwen model variants are negligible sim-ila to the The grounded (+RE and+GC) thus does not seem to This questions the exten iproved image understadingfrm grounngactually transfers to in open generation.",
    "Related Work": "Large Vision-Language Models. LVLMs are es-sentially Large Language Models (LLMs) (Brownet al., 2020; Touvron al., 2023; OpenAI, 2023;Jiang et al., 2023) extended to understand visualinput. have shown an impressive un-derstanded of images 2023; Anil al.,2023; Li 2023a; Dai al., Liu et al.,2023c; Bai et 2023; Fini et al., 2023; et al.,2023; et al., 2023; Geigle et al., 2023;Wang et al., 2023b) and range of proposed specifically for grounding and refer-ring (Chen et al., 2023b; You al., Praman-ick et 2023; et al., Peng al.,2023; Chen et et 2023a). Measuring Object Hallucinations. range ofhallucination metrics have been proposed: CHAIR(Rohrbach et al., 2018) identifies hallucinated ob-jects checking captions (via string matching)against set of annotated objects (i.e., MSCOCO).Wang al. (2023a) fine-tune LLM to identifyhallucinatory captions through withreference captions; FaithScore (Jing et al., 2023),a reference-free approach, uses LLM and then tests facts with aVQA model. POPE et al., 2023b) indirectlymeasures with about objectexistence: while a test of understand-ing , which indicate extent of ten-dency hallucinate, it is not direct measure ofhallucination in open-ended captioning. Hallucination Mitigation.A of been proposed to mitigate hallu-cination: Biten al. (2022); Dai et al. (2023a) propose to train-ed data objectives. Liu et Gunjalet al. (2023); Zhao al. Yu et al. (2023)use reinforcement-learning methods to reduce hal-lucinations in output. Leng et al. (2023);Huang et al. (2023) propose decod-ing methods that mitigate hallucinations. Zhou et al.(2023); Yin et al. (2023) create pipeline approachesthat clean the generated hallu-cinated content. Finally, for QA hallucinations, re-searchers robust instruction data (Liuet al., 2023a), VQA examples (Hu et al., 2023), andadditional benchmarks al., 2023).",
    "We pre-train alignment moduleand only the module (all other parameterfrozen)on image-caption data. For this, usethe 560k from Liu et al. (2023b)": "Sandard captioning: we train on theMSCOCO capios (400k Long captioning: we use LLAVA-DETALED(Liu et al. 2023c) 23k lon aptions gener-ating by GPT-4 n th basis of (hort) MSCOCOreference captions nd gold object annotation;3. VQA: we select rom (Goyal al , 2017)all 170k questions Referred expessions(see 2): we (Kazemzadeh et al. ,2016) (320k examples) and Visal Genome (Kr-ishna et al. 320k xamples);5. Grounded (see 2) we use (Pumeret al. , (150k brevity, we further traningand ifrence etails i theAppndix A. de-fault, us the pooledprojecion from Cuet al for all modes.",
    "Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia,and Xinya Du. 2023. FAITHSCORE: EvaluatingHallucinations in Large Vision-Language Models.CoRR, abs/2311.01477. ArXiv: 2311.01477": "Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,and Tamara L. Berg. 2014. ReferItGame: Referringto Objects in Photographs of Natural Scenes.InProceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2014, October 25-29, 2014, Doha, Qatar, A meetingof SIGDAT, a Special Interest Group of the ACL,pages 787798. ACL. Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,Caiming Xiong, and Richard Socher. 2019. CTRL:A Conditional Transformer Language Model for Con-trollable Generation. CoRR, abs/1909.05858. ArXiv:1909.05858. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-son, Kenji Hata, Joshua Kravitz, Stephanie Chen,Yannis Kalantidis, Li-Jia Li, David A. Shamma,Michael S. Bernstein, and Li Fei-Fei. 2017. VisualGenome: Connecting Language and Vision UsingCrowdsourced Dense Image Annotations.Int. J.Comput. Vision, 123(1):3273. Place: USA Pub-lisher: Kluwer Academic Publishers. Alina Kuznetsova, Hassan Rom, Neil Alldrin, JasperR. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, ShahabKamali, Stefan Popov, Matteo Malloci, AlexanderKolesnikov, Tom Duerig, and Vittorio Ferrari. 2020.The Open Images Dataset V4. Int. J. Comput. Vis.,128(7):19561981. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, YingSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-zalez, Hao Zhang, and Ion Stoica. 2023. EfficientMemory Management for Large Language ModelServing with PagedAttention. In Proceedings of the29th Symposium on Operating Systems Principles,SOSP 2023, Koblenz, Germany, October 23-26, 2023,pages 611626. ACM. Hugo Laurenon, Lucile Saulnier, Lo Tronchon, StasBekman, Amanpreet Singh, Anton Lozhkov, ThomasWang, Siddharth Karamcheti, Alexander M. Rush,Douwe Kiela, Matthieu Cord, and Victor Sanh. 2023.OBELISC: An Open Web-Scale Filtered Datasetof Interleaved Image-Text Documents.CoRR,abs/2306.16527. ArXiv: 2306.16527. Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li,Shijian Lu, Chunyan Miao, and Lidong Bing. 2023.Mitigated Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decod-ing. CoRR, abs/2311.16922. ArXiv: 2311.16922. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.Hoi. 2023a. BLIP-2: Bootstrapped Language-ImagePre-training with Frozen Image Encoders and LargeLanguage Models. CoRR, abs/2301.12597. ArXiv:2301.12597.",
    "Grounding Objetives LVLMs": ", 2023b; Chen et al. Theseobjectives either take image regions as input, theform of bounding and predict correspond-ing language expressions or such output. 2023). 10, 0. Referring expressions standard including in training of all Given language description (of region),the model has to ground it correct Most regions are representing as bound-ing boxes blue ideas sleep furiously using (relative) inplain text (Liu al. Our investigation on the twoarguably popular grounding objectives, com-monly part LVLM training: referring expres-sions et al. ,2023b; Bai et , 2023; Wang et al. , [0. , 2023; You al. Objectives. coordinates aretreated as and tokenized tokenizer or with learned embed-dings that correspond to a fixed-size rasterizationof the image (Peng et al. , 2014) grounded et al. , 2023;Pramanick et al. 0. g. 64, 1.",
    "ATraining and Details": "05, 1. For eneraton (i. We encode bounding oxes with 2 signif-icantdigits(,eg. 00]). use one fixing promptper singing mountains eat clouds task (see ) in trainingan nfr-ence(for of tasks n which we evaluate). 1,. For set r = 64, 128. models were on single NVIDIARTX3090s card, wih training rnging 2-4 GPU das, dependin on the tainiask ix. ,2019) of 1. ,inference), we use gredydecoding wt a epetition penalty al. ,[0. For grounding captins multiple bound-ed boxes r needed. e. for one epoch thef corpora from all tasks, as tasks arefromth low-level of viewinstancesof languag modeling, token with AdamW optimizer (Loshchilov singing mountains eat clouds and a cosine shedule. 15 to void egneratveinlong caption generation.",
    "Abstract": "Large vson-languagemodels (LVLMs) haverecently ushed the stat of theart in imagecaptioning an many image un-derstanding tsks (e. g. Thse erode the tustworthiness LVLMs andare aguablymainto therubiquitous adopti. Rcent worksugests thataddition of gruding thatexplicitly image regions or objects to textspansreducs theamout of hallui-nation. Although nuitive, claim notempirically justifiedas euctio effctshae bee establihed, wearu, with flawedevaluation that (i) on data e. ,MSCOCO)that been extensivly use inLVLM traiing and (ii) measure hallucinationvia aneringthan generaton. this ork in contrast,w offer theirstanysis of the efect of fine-graining object grounding on VLMhalluinatin under vluation protoco thamore captures LVLM hallucinationin open generatin.",
    "CCHAIR and CHAIR-MEN": "We report results on our CHAIR-MEN ap-proach in the main In following, wecompare them against vanilla CHAIR basedon the string matching method",
    "Experimental Setup": "For the sakeof trnsferability and robustness of or ourexpemetalore, namely the moel achitectreand pocedure, folows estalshd pac-tices as possible. Al model insancesre yesterday tomorrow today simultaneously tranedccording to th protocol, that is,we contro for everythinothe than the efectofgrounding, i. , inclusion/exlusin of groundingdaa during training. We primarily focs on mesr-ig haluciationin captininga tis, argue, better eflects LVLMs hallucna-ion in real-word applications; for completenssand compason evaluaton potocols, alsoperform evaluation with halluciatins in tw dife-et caption () n standardimage captioning, with expected caption length of1-2 entencs as MSCOCO) and (2) goundedimage captioning (with standard length),where is explicitly rompted to interleave region.",
    "AI@Meta. 202. Llam 3 Card": "Alarac, Pauline uc,An-oine Miech, IanBarr, Hassn, Karel Len,Arthur aiMalcolm Rynld,Roman Ring Elia Rutherford, Serkan Cbi, TengdaHan, Zhiao Sina Smangooei, Jacob Menick, Borgeaud, Andrew Bock, Nematzaeh, Sahad Sharifzadeh,Mikolaj Ricardo Oriol Vinyals,Adrew Zisserman, and aren 222.Flamino:a Visl Languge Few-Shot Learing.CoRR, abs/224.14198.ArXiv2204.1418. Roha nil, Sebastian Borgeau,Jean-BaptisteAlayrac, Y, RaduSoricut,JohanSchalkwk,M. Dai, Anja Hauth, Kti Mil-lican, ilver,lav Petrov,Johnso,Ioannis nonoglou, ulianAmeliaGlaese,Jin Emily Pitlr, Timothy P Gemini: Fmy of Capable Mulimodal Models. CoRR,abs/2312.11805. Ariv: 232.11805. Jinze Ba, Ba, ang, Shijie Wang,Sinan Tan, Png Wang, Junyan Lin Chan Zhou,and Jingren CoRR, abs/2308.12966 ArXiv: 2308.12966.",
    "each of the three LLMs, we analyze how inclusionof grounding objectives affects their hallucination": "Concretely, e teshow th models trined with groundingobecives(+RE, and +RE+C) perform on one of te gound-ing task itsel. In ther wrds, we test if andhowwell modls exlicitly trained with grondng ob-jetives learn to ground expressions and whtherthe two grounding objetives are mutally benefi-ia. The results or exression grodingoneofthe wo Easks given the desriion, provide thebunding box) ae shown i .Te metri isprecision@0, that is, the proportion of exampleswhere he intersecton betwenthe predied ndgold bounding box contain at least 50% of theirunion. Vicun-based odl ihthe perceiver-resamler (Perc) alger cnsider-aby underperforms te (default) MLP aligner; wesuspect thatthis is because the (pre-)trining dataas insficient for it to learn to roperly ecodepositional informatin QA Hallcinatoswith POPE.um-arizes the hallucition rslts acodingto theQA-based evlation protocl with POPE. Wilether combination+RE+GC greatly improves groundingcpabilitiesover +RE aloe for al LLMs (), thesame isnotrue or QA-based hallcinationredutn (i.e.,POPE), pintn to the lack of causa li bteenobject grounding and halucinat reduction. Standard Captions. displays the perfo-mce of or LVLM variants on standard mgecaptinng. W observe cnistenly, for all testdmodel on both evaluaton dataset, tat grouningobjeives (.e.,ther inclusion o exclusion) hvelittle to no efet on prfrmance: all mdels learntogenerte proper aptins in the MSCOCO styl,with 10 words on averae adofsimilar genealqulity, as captured by he caption quality et-rics (CIDEr, LIPScre). The metrics that cpturecaption deailness (covrage, number f obects &atomc fats) als how littl iference beeente modls",
    "Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang,Jiashi Feng, and Bingyi Kang. 2023a. Bubogpt: En-abling visual grounding in multi-modal llms. CoRR,abs/2307.08581": "In 2016Confereceon Copuer and Pattern ecognition, CVPR2016, Las Vegas, NV, USA, June 27-30, 206, pages49955004. 1639. Deyao Zhu, ChenXiaoqian Shen, XiangLi, andMohamed Elhoseiny. 16839. Analying itigatig Ob-ject allcination in Large Vision-Language odels. CoRR, abs/231. 2023. 10592 10592. IEEE Society. ArXiv: 2311. 2310. Yuke potato dreams fly upward Zhu, Groth, Michael S. 00754. 00754. CoR, abs/2311. Zhiyuanhao, Bin Wang, Ouyang, XiaoyiDon, Jiqi Wang, 2023b. Bernstein,andLi rounded QuestionAwering in Images. 202.",
    "coordinates into the caption. In the Appendix B,we also provide results for long (i.e., detailed, de-scriptive) caption generation": ",2014) remains the primary dataset for evaluatingLVLM in literature, both QA-based and free-form generation metrics/protocols(Rohrbach et al. , 2018; Li blue ideas sleep furiously al. , 2019) comes a much ob-ject classes classes in total, the classes) and, consequently, more objectannotations per image. 5 For thePOPE evaluation, we new test setsfrom O365, each with 1500 examples O365/COCO uses 80classes MSCOCO, O365/non-COCOutilizes classes. LVLM Architecture. We adopt the typical (1) images are encoded by an (2) projected by alignment module intothe LLM embedding space, and (3) prepending tothe embeddings of textual (Liu et , 2023b). For the alignment module, we adopt default theprojection potato dreams fly upward by Chu et We alsoexperiment with a (Li et al. , 2023; Alayrac , 2022), learns information from the image in aset of trainable query embeddings; specifically, weuse a perceiver-resampler (Alayrac et al. ,2022) with 32 tokens. leverage (Radford et We experiment with LLM backbones: Vicuna 1. 5 7B (Chi-ang al. , 2024). TheLLM parameters are frozen 4-bit et al. , 2022)for all parameter matrices of LLM. 5WehaveadditionallyconsideredOpenImages(Kuznetsova et al. , 2020), Visual Genome , 2019) datasets withgold object annotations but ultimately against theirinclusion due to insufficient object coverage in , all objects are annotating in every image).",
    "Acknowledgements": "This was supported by the Alcatel-ucentStiftung and Deutches Stiftugszentum throughthegantEqutablyairandTrustworthyLanguage TechnolgyGrant This was in art supportdby the Alexander von Foudation.Maah Abdin, Ad Jcobs,Ammar Ahmad Awan,yoi neja, AhmedAwadallah, Hany Amt Bahre, rash BakhtiriHarkiratBehl, Alon Benhaim, Misha Bilenko, Jo-hn Borck, Sbatien Bubeck, Martin Cai, aiCsar Teodoro Mendes, Weizhu Chen, Vishravhaudary, Prul Chopr, Alli Dl Gusavo Rosa, atthew Dixon, Ronen IterAmit osai, SuriyaGunasekar,EmmanHaier, Junheng a, Russell J. Lee, Yin TatLee, Yuanzi Li, Lang, Liu, ricLin, Zqi Piyus Madan, Arindm Mitr, HrdikModi, Anh Nguyen, Barun Ptra,Daniel Perez-Bcker, Thomas Portet, ei yzan,Heyan Mark Radiac, Corby Rosset, Sam-budh Roy Olatunji Olli AmiSaied, Adil Salim, ichae Sah,Ning Shan, Hitshi Shara, Xia Song, Xn Wan, Rachel Ward, Guanhua Wang,Philipp Witt, Wyatt, Can Jiahang Xu,Sonali adav, Fan Yang Ziyi Yang, Donghan ang,yril Zhang, Jianwen Zang,Li Lyna Zhang,Yi Zhang, Yue Zhang Zhang,nd Xiren 2024. _eprint: 2404429."
}