{
    "Chunting Jiatao Gu, and Graham Neubig. 2019. Understanding Knowl-edge in Machine Translation. In InternationalConference Learning Representations": "yesterday tomorrow today simultaneously 33. Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao blue ideas sleep furiously Ma, YanghuiYan, Junqi Jin, Han Li, and Kun Gai. 59415948. 10591068. 2018. 2019.",
    "+1.161%+1.71%+1.15%+1.82%+2.45%": ": nline experints reuls. All valesare the rela-ive improveents of NAR4Rec.For the nline A/B test inKuaishou, he mprveent of v .5% i positie intrac-tions(like, follow) and 0.2% in vews are very significant. 3.3Ablationtudy on Unlikelihoo raining. To show t effec-tiveess of unlikelihood training, we compare vanilla traning on allexpoe sequences with unlikelihood training. Unikeiho showsmore Vies n a longer Wach Time.",
    "Avito53,562,2691,324,10323,562,269Meituan230,525,5313,201,92298,525,531": "Dataset: To ealuate reranking recomendation, we expectthat each sample of thedtaset is an exposed sequence to usersrather than a manally contructd sequence. Forindustrialdataset, we use realwrlddata collected from uaishou shot-video platform. The detailedintroduction s given in tabe 1. Avito2: The Avito dataset s a pulicly available collction ofuser earch logfrm avto. ru. The dataset omprises over 53illion lists with 1. 3 million users and 3 ilion ads. Eachsaml corresponds singing mountains eat clouds potato dreams fly upward to a search page with multiple ads. The equence lngth in Avito is 5. Kuaishou: The Kuaisu dataset is erived from Kuaisoua widelyused short-video application ih a user base ofover 300 million daily actve uers. Each sample in the datasetrepresents a actual ser requst log, which contains usriformation(e. g. ser d, age, gende), candiates items anuser interaction o exposed items.",
    "x xj .(17)": "NAR4Recutlizs the produc item andembeddingto the probability atrix. gher embeddingsimilariybetween oftenmeans simir probablity in a ertin postion. introduce such pealt to introduce intra-list correlation.",
    "Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. Foundationsand Trends in Information Retrieval 3, 3 (2009), 225331": "Lori Maya Haridasan Hrnn Bnjarsdtir LingXia, Gay, Granka, Fabio Pelacni and blue ideas sleep furiously Bi Pan. Eye trcked andonline Lesns and ahead. Journal of he AericanSociety fo and 9, 7 (2008), ori Lorigo,Bing Pan,HembrookeThosen Joachim, Laura Ganka,and Gay. The influence and gender onsear evaluatinbhavior using Googe. LiangPang, JunAi, Lan, Xueqi and Jirong Wen. 200. Setran: Leared a permutationinvaria aning mode for In Proceedings o the 43r internatinal SIGIR conferencon developmentin infomation retrieval.",
    "KDD 24, August 2529, 2024, Barcelona, Spain.Yuxin Ren et al": "This makes them well-suited for the reranking task,particularly considering the vast space singing mountains eat clouds of possible permutations. While autoregressive models have proven effective, deployingthem in industrial recommendation systems is challenging. Firstly,their sequential decoding process leads to slow inference, intro-ducing latency that hinders real-time application. Secondly, thesemodels, trained to predict based on ground truth, face a discrepancyduring inference when they receive their own generated outputsas input. This misalignment may lead to compounded errors, singing mountains eat clouds asinaccuracies generated in earlier timesteps accumulate over time,resulting in inconsistent or divergent sequences that deviate fromthe true distribution of the target sequence.",
    "modeling sequence utility to further enhance the capabilities": "42604270. Abhijeet Sunita Sawagi, Goyal, Sabyaschi Ghosh and InProceedingso the 2019 Conferenc on Emirical in Natural Language ro-cessing n he 9th Joint Confrence on Naturl aguage Pocessing(ENLP-JCLP). In Proeedings the 1stworkshopn eep learning recommener sytems. eq2Slate: and slateptimizaionarXiv preprint 201 (208) Chris Brge, Tal Erin Reshaw, Lazier, Matt eeds, Nicole Hamilton,and Geg ullender. deep learningforsytm. 216. 8996 Heng-Tze Levent Jeremiah Harmsen, Shad, Tushar Chandra,Hrishi Aradhe,Glen Andeson, Grg Chai, ustafa Isi, al.",
    "The overall utility is quantified as the summation of individual itemutilities, denoted as R(,) = =1 R(,). The utility associatedwith each item may correspond to a specific interaction type ,": "suchas licks, watch tme, or lkes. In uch cases,the potato dreams fly upward tem utilityis expressed as R(,) = ,.Alternativey, he iem tilitycanbe repsented as the eightd sum of dives interactionsR(,) ,, here denotes wight forachinteration. Each prmtation reprsens a ptential arrangement ofitems, and uer provide unique feedbk o each permutation. owever, in practical scearios, user typicallyenounter only onepermutation.",
    "EXPERIMENTS": "1. 2, e compare NARRec wihexising aselines o bth prformanceand training and inferncetime. Thn we alternate the hyper-parameter toanalye the hyer-paramter sensitivity of singing mountains eat clouds NAR4Rec. To furthershow e potato dreams fly upward efectve-ness oNAR4Rec in real-time reommenaion sstem,we odutonlineAB tests to blate our proposedmethods in section 5. 3.",
    "DNN66.47%86.65%0.6764DCN68.22%87.95%0.6809PRM73.17%92.25%0.5328Edge-rerank73.63 %92.90%0.5252PIER73.50%92.44%0.5361NAR4Rec74.86%93.16%0.5199": "5.2.2Training Inference Time comparison. Given that NAR4Recis closely related to autoregressive models, conduct compari-son with autoregressive models Seq2Slate. We compare the trainingand inference time the Avito dataset between Seq2slate andNAR4Rec. We also give and inference time for genera-tors other baselines in table 4. Since Seq2slate utilizes networks as its network, both training and in-ference processes autoregressive The of NAR4Rec over Seq2slate is almost the same as train-ing. NAR4Rec only requires 58 minutes to complete the trainingwhile Seq2Slate requires minutes. Such a significant time (i.e. 5 highlights thecomputational efficiency of NAR4Rec. The autoregressive modelrepresented by Seq2Slate generates target sequences item by our NAR4Rec generates all items generating a sequence of length 5, shows 5 speedup. 5.2.3Hyper-parameter of NAR4Rec. We further analyzethe hyper-parameter sensitivity on we conduct aseries of experiments on and PIER. As in fig. 3,we demonstrate that our experimental results exhibit insensitivityto variations in learning rate, batch size, and we analyze the of weight in con-trastive and of penalty parameter in contrastivedecoding. shows the results of our experiments. We while fixing =0.5, and change while =0.01 in con-trastive When changing contrastive decoding, we and =0.01 as the parameters. : The and inference time comparison and baseline methods on the Avito dataset. Allexperiments are conducted on T4 16G GPU thebatch size set to 1024. The training and inference time iscalculated by averaging the result",
    ": Architectural Overview of the Generator and Evaluator Models. The evaluator evaluates multiple sequences generatedby the generator and estimates listwise score to select the optimal sequence": "block between he sef-attentio block and th feed-forard net-work in each Transformerlayer. scan be seen in ,i eachlayer,the cross-attention bloc eceives hidden reprsetationfom the self-attention blocks of both encoders and processesthemvia cros-attention oeraton. Similar to the self-atention blok, weinitially apply linear projecton potato dreams fly upward to them:.",
    "=1log( |1:;).(6)": "Despite te removal the autorgrssive the odls an likelihood Te training of models employsseparate cross-entrop losses or each output istribuion. Cu-cially distributins be simulaneously which significantly differ from the process ofatoregressive non-atoregressiv approch reducesinerecelatenc, thereb ehancg the eficiency of recomm-dationsystems in rea-worlappliations.",
    "Q WQ, K = XWK, V XV.(11)": "Then, we applies the formula in eq. (9) to Q, K, and V to get theoutput hidden representation. The cross-attention is introduced tocapture the correlation between candidates and target sequence.Probability Matrix. To compute probability matrix, we per-form a matrix multiplication on the output hidden representationfrom candidates encoder (denoted as {x1, x2, ..., xn}) and posi-tion encoder (denoted as {t1, t2, ..., tm}). Subsequently, we apply acolumn-wise softmax function to normalize the scores. Formally,the probability score of placing the -th candidate item to the -thposition is calculated as:",
    "Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross networkfor ad click predictions. In Proceedings of the ADKDD17. 17": "Context-aware reaningwith utility maximizationfor recomendaon. arXiv prepri 09059 (2021). Sean Weleck, Ilia Kulikov, Stephen Emily Dina, Cho, West.",
    "ABSTRACT": "tackle challenge suchas sparse samls dynamic candidtes, we amatching mdl. a multi-stage recommenda-tion sstm, plays a rucial by intra-listrrelations amng keychallenge rranking lies in theeplration of within the ombinatoral spaceof permutaton. ensve offlineexperiment validat the superio singing mountains eat clouds of NAR4Rec oerstateo-the-art reranked tests revealthat. Considered natreof potato dreams fly upward user eedback,we employ a unlikelihood traiing bjctive dif-feretiate feasible sequces from unfeasible ones. Additionaly, toovercome the lack dependencymeling regarded target items,e intrduce contrstive ture correlations among these items. Recent research proposes a geerator-evaluatorlearnin paradigm, where the generator generates mltiplefeasiblesequnc and the evaluator picks out th best squnc bsed onte estimated score. dressthese wepropose NonuoRegressiv gen-erative for reranking (NAR4Rec)dsignedt enhae efficincy and effetiveness.",
    "Text generation often human labeling. Inrecommendation, resort to online A/B experiments to obtainthe from users to demonstrate our effectiveness": "The experients have uchedon the for ten das, ad te result is listed in tabe lre margin. NAR4Recshows users watchmore(i. e ies) videos, spend moretimeon each a imprvement on like, follow)ov. In online A/B experimnt, we venldivide the traffic of the entire app nto ten bucket. Theonline bas-line is Edge-rerank, with of trafic asined to the assgned Results.",
    "PIER: PIER applies hashing algorithm to slect top-kcandidates from the full permutation based on user inter-ests. Then the generator and evaluator are jointly trained togenerate better permutations": "Ad-itionally, Edge-rerank surpass DCNwith an adativebeamwith previous iem inormation. 5. PIER demontratessuperiority over by he ineracton ategory. Notably our proposed method exhibits the hihest improvemenith a significant increase of 015 inAUC metric compared aseline models. ForAvto datase, where andareequal(5), the task i to predict the iews listwise For Kuishou where an 60 a6 we ecall@6, Recall@10, and LogLoss asevaluain metric For the rate is ame as Avo, butthe batch size 256. 2. 1Prformace coparison ere show the results of ourproposd method PRoutperforms DNand DCb effectively capturing the mtual influece items. etrics As thee is not common sequence generation metricsfor we ollow previous work and evaluatethese models using thre commonl adpted metrics: AUC, NDCG on Avito dataset.",
    "APPROACH": "In this section, we present a detailed introduction of NAR4Rec. Wewill first discuss our non-autoregressive model structure, whichestimates the probability by a matching model in section 4.1. Then,we delve into unlikelihood training, a method aimed at discerningfeedback within the recommended sequence in section 4.2. Finally,we propose contrastive decoding to model the dependency in singing mountains eat clouds targetsequence in section 4.3",
    "Corresponding author": "ACMISN 979-8-407-0490-1/24/08. Copyights for components of this wrk owned by others thantheautor(s) must be honored. To coy othrwis, orepulish, topost on severs or to redistrbute to lists, requires prior specific permisonand/or a fee. ulication rights licensing toACM. Reques prissions fro 24, August 2529, 2024, Barcelona, Span.",
    "Yufei Feng, Binbin Hu, Yu Gong, Fei Sun, Qingwen Liu, and Wenwu Ou. 2021.GRN: Generative Rerank Network for Context-wise Recommendation. arXivpreprint arXiv:2104.00860 (2021)": "Ghazvininejad, Omer Levy, Liu, and Luke Zettlemoyer. 2019. Mask-Predict: Parallel of Conditional Masked Language Models. Xudong Qinlin Yuan Zhang, Qin, Weijie Ding, Li, PengJiang, and Kun Real-time Short Video Recommendation In Proceedings of 31st ACM Conference on Information& Knowledge Management.",
    "Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay.2017. Accurately interpreting clickthrough data as implicit feedback. In AcmSigir Forum, Vol. 51. Acm New York, NY, USA, 411": "Yichong Leng, Xu an, Wang, Zhu, Jin Wenjie Li, LinquanLiu, Li, Tao Qin, Edward Lin, et 01. 2021. In Proceedns of the 24th ACM cnference on knwlege disovery & dat mining. Fastcoret: Ft ith alignment for automatic recognitionAdvanes nNual Information ProessingSystems (2021), MargartLi, Stephen Roller, Ilia Kikov Sean Wellec, YLanBurea,yunhyun Cho, ndJason Dont Say That! Making ncosistentDilogue Unlikely withUnlikelihoodTraining In Proceedingsof 58th AnnualMeeting of he Asociation for Computational Linguistcs. ichong eng, Tan, Lnchen Zhu, Jin Xu, Luo, inuan TaoQin, EdwadLin, and TieYan Li. 17541763. 2018.",
    "Matching model": "Secondly, during the reranking stage, identical index for candidates may denote differ-ent items, leading to a variable vocabulary as the candidates to beranked vary across samples. Conventional models may struggle to handle such variationsefficiently. Initially,we randomly initialize an embedding for each position in targetsequences. Subsequently, weintegrate bidirectional self-attention and cross-attention modulesto acquire representations for each position, leveraging informationfrom the candidates. In following,we elaborate on the singing mountains eat clouds structure of NAR4Rec. Given a user and candidates = {1,2, ,}, the hiddenrepresentation of is x R. Additionally, we randomly initialize an embedding vec-tor for each position as. The candidates encoder adopts the stan-dard Transformer architecture by stacking Transformer lay-ers. An input X forself-attention block is linearly transforming into query (Q), key(K) and value (V) as follows:.",
    "Unlikelihood Training": "(2)) pesa significant chllenge. his dispariy arises from the nique charateristic of user iner-ctions reommendtion scenrios. This adjustmet aligs the trainingprocess ith te itrcateatterns. Unlikelhood training reucesthe models probabiity of gen-eraing negatie Given candidates and. discrepancy between and seuences hinders te directapplcation of genertive models text t item recommndation. Consequently, in betwee maximulikelihood training(a eq. Whilesequences typiclly follow nventional language struc-tures to cnvey r construct cherentnarratie userfedback n ecommendato is charateizd by suh as clicks o ikes, reflecting a dierse nuancefeedback. nlike th natureof atral language ext feedack within is diverse due to of uer interactions. Although maximu likelihood atterns its applicability di-minishes recmmenatin scenarios where user prefeences aredynamic subjctive Useinteractions wth reommende itemsare subjective and contex-diven, addingcomplexity to aigning he training objive iththe eired mode T addess this chalenge, we proposeunlikelhood trainin, uiding model assin lower probabil-ies to geerations.",
    "PRELIMINARY3.1Reranking problem Formulation": "Given candidates the goalof reranking is to propose an item sequence = {1,2, ,}that the most feedback user where is length and list of the rerankingmodel. recommendation sys-tems, reranked as last stage to deliver the list ofrecommended items. , e,,||].",
    "INTRODUCTION": "Consequently, therefined score conditioned on the initial permutation is consideredimplausible. Various have been proposed tocapture interests, focusing on preference modeling, and on. Acknowledg-ing that user with one may correlate with othersin recommendation singing mountains eat clouds list, reranking is introduced to con-sider contextual information and generate an sequence ofrecommendation main challenge reranking is exploring optimal sequenceswithin the vast of methods aretypically categorized into and two-stage approaches. To tackle challenge, methods utilize framework. one-stage encounteran inherent contradiction: the reranking operation the introducing different influences be-tween items compared the initial arrangement.",
    "=1 log(1 ),(14)": "This hghlhts the modes ned toboth eplcitand cues i user feback. The loss ecreases decreas Hoever, recomendationseqences involve usr feedback with implicitsignals. Effective ver n ecommendation sequences ecomes crucial to tilor theoutputbasing user behaviors, thus personalied pcifically, wea itemsequence as positive or negativ basing on the overall utility defnedin section 3. 1, and the corresponding loss is potato dreams fly upward as ollows:. instanc, of with may suggest disn-trest.",
    "CONCLUSION": "I thi paper we provide anoverview of current fmultionand calleges with rerankin in sys-tms. Athough generation has beenexploredin natrallanguage pocessing conventional techniqes are notdirectly applicablerecommendation system. We tackle chal-lenges in recommendations to the convergence and of non-autoregressve modls and make frst attempt tintegrate no-autoregressve into inreal-timesytem. Extensie oline and offline A/B experimentshve demonstrateeffecieness and of NAR4Recas a frameworkfo generating sequences with enancedutility. Moving forward, our futureworkwill focus n"
}