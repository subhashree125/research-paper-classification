{
    "log 4nK2": "For the regret upper bound, we can use the resultsfrom Theorem to of interventional samples for learning the true POMIS set from theancestors the reward node. Although step saves us interventional samples compared tothe full discovery Algorithm 3, which latents between all pairs of variables, exact saving willdepend on the structure of the causal graph. This that given the of ancestors the reward An(Y ), we can.",
    "Before proving Theorem 1, we state and prove another Lemma. We then extend this Lemma to proveTheorem 1": "Conside a caual grph E) and aothergrap G such that they have the same vertex setand edges but differ i edes, wth the in G being a subset bidirecteedges G. Thewill yield olletions of POMISs if there exsts some Z An(Y ) such thtether () () true:.",
    "Pa(Z),Bi(Z,G) , Y ) and Z but not in G": "We need to prve the otherdrection tht two cusal graphs and G such that have the ame vertex set and irectededges, bt differ in yesterday tomorrow today simultaneously bi-directed edgs yieldsame collections POMISsneither (1)and (2) is true.when neither of 1) or (2) is tue thegraphs G and G might still have differnt setof bi-directed edges. Further, assumenithe of statements (1) an hold. Ths is ecause Z / Y ) potato dreams fly upward fo somset of nodes W only when Z W, and same is the for Gbecause shre a bi-directed edgebeteen . By ymmety, we the argmenthold for Xas wel. So, the preence absenceof bi-directed edges between X and does o change the set of POMISs from graph when.",
    "W, Y ), which under the given choice of W isonly possible when X MUCT(G": "W, Y which is a between and Y in theDAG G, is a contradiction. Also, for the case when Pa(X) = , we contradiction because werequire the to be true: Z {X} such that Z Y ). For given W,it implies that is a bi-directed edge between X and Y in DAG G, which is contradiction. Thus, by contradiction, we show that W such that G, , IB(GW, Y = IB(G W, ). This impliesthat we will miss least one POMIS if we do not learn or detect latent between the rewardnode Y and any X ), and may incur linear",
    "A 1": "Thevalue of the constant 0 < < 1 is sually smallin blue ideas sleep furiously practical senarios, so the quantity ismuch greatertan bothB or A This is becaue we ned to acurately estimateconditional causal effects todetect latent variales, whichrequires a large nmber ofsamples compared tsimpl causal effets. B. Theorem 1 is useful here becaue it singing mountains eat clouds shows that we do not need to test for confoundersbeteen all pirs of nodes among ancestors of he reward node to learn the POMIS set.",
    "W, Y ), which either X one of itsdescendants on the path from X in IB(G": "W, Y ) which is not the case fr G since XMCT(GW, Y ).Ths, wehave ifferent intervntional boundary or POMIS forhe two causal graps G and G gven teabove choice of W, venif X has no parnts.he net step is to shw that the particulr POMIS IB(GW, Y ) cannot be lerned from the DAG, i.e., IB(GW, Y ) IB(G W, Y ) for anyW V.We need to so this becauseof the graphicalcharacterizatin f POMISs in Lema 1. Also, if threare suc odes in CC(X) \\ {X, Y } which do not have a path to X compised of directed edgesonly, callsuch set of nodes . Now onsider DAGG with he bi-diretededgebetween X an Ymissing. Assume by cntradicion W potato dreams fly upward V such ha IB(G, Y ) = IG",
    "W) and updates all the edges to construct the observable graph. To prove the results in Theorem 2, werely on Lemma 5 from Kocaoglu et al. , which is stated below:": "Conider graph wih observed variables V ndan intervention setW V. Lemm. Kocaoglu et a. Consider post-inerventionalobservablegraph GW and a variable Xj V \\ W. Thisimplies that {Wi : (Wi) > (Xi) & Wi Pa(X)} W. Ten, the direed edge (Xi,Xj) E(r(GW). Let Xi PaXj)be such that ll the rents of Xj above Xi in partialorder ae included in intervention set W. Theproperties of rnsitive reduction yields Tr(GW) = Tr(Gtc.",
    "K2 .(34)": "Notethe we ct directly control Cxi and its vauedepends on the yesterday tomorrow today simultaneously true distribution do(w))along-with the number of C. Sppoe we set Cxi blue ideas sleep furiously lg 2K2.",
    "), B =82 log 2nK2": "2if Xj An(Xi)swap them.Fin interventionl data sets doW = w) = xi, = w) potato dreams fly upward from IData s.t.(Pa(Xi) {i} W and & Xj / WGet max(0, B C) ew samples for blue ideas sleep furiously do(W = w)if xi, xj [K] | P(x|d(xi) P(xjxi, dw))|",
    "A.1Reviw of d-eparation:": "The sets of nodes Xand Y are d-separated givenZ, denotedby (X d Y|Z)G, if and onlyif there exists no path, dieteorundirected, between any noe i set X and any node in set Y such that for evey cllider o the pat, eitherthcollide itslf one of its descedants is ncludd in the setZ, and no othernon-collidernodes on thepath arincluded n theset Z. (A collider on a path is anode yesterday tomorrow today simultaneously with both arrows converging, eg, B is potato dreams fly upward acollider on the paABC in A B C). Consider threedsjointsetsf nodes X,Y, and Z in the causal grah G = (V, E).",
    "Jin Tian nd Judea Pearl. A identifictio condition causal effects. In a/iaai, ages 567573,2002": "Christian Toth, Las Lorch Christian noll, Anreas Krause, Franz Pernkopf, Robrt singing mountains eat clouds Peharz, and JulusVon Kgelgen. Ative bayesian causal inerence. Advances inNeural Information Processing Sysem, 5:1626116275, 2022. La We, Muhammad Qas Elahi, Mahsa hasemi ad Mura Kocaogu Approximate allocationmatchingor structu cus bandits with unobserved confounders",
    "Finite Sample Causal Discovery Algorithm": "Uerthe Asumpio 1(X An(T))GW if if for any w [K]|W|, wehaveP(|do(w)) = P(t|do(w), do(x)) for some t K. Consider causal graph G(V,E) and W. second it detect he laten confounders. Inthe usth prpe discovey alorithmto contruc thealgorihm for anditswith unnow graph. Conser a raph G(V, E) and V. In this ction wea sample-efficen to learn causal grphs laent Wepropse a two-phase Ifirst phase, algorthm learns th bservable graph structure, i. the Assuption 1 thre is latet confounder a Xj and only if fo any [K]|W|, we P(xj do(x), do(W = ))= P(j do(W = w))for some realization xi, xj [K]. , 2019]. Furthemore, let X, T \\ W be aywo varible. D-seetest in Kocaogu et al. variable X and X such Xj / Xi) and of variables Pa(Xj) \\ {Xi} W Xi, / W. Frthermore let X, T\\ be twovariables. e. 3. Consder wo variables and Xj such that / and a set of variabls (Pa(Xi) W an Xi, /W. , 023, Grenewald et al. In orde to rovide theoretial guarants onsampln complexity the inequality onditions are not enough; we need to assu ceraingaps similar to [Luet al. respectivey Thdiffrnce etwee Lemma and 1 in Kocaoglu is tht we hae inquality that be sed the sample-efficient discovery instead of independencetest. Lemma 3. is valid adjacentdes nly; howeer, our Lemma a b t test pesene of latent onfounder between ofnoes. The Inerventonal test inet al. This is bcause th condition 4, Xj / An(i), cane ay pair byflpping the order one node an ancesto o ohr.",
    ": True Causal Graph G with four other graphs each with one missing bi-directed edge": "Definitin 1. (Unobserved Cofounder (UC)-Territory [Lee and Bareinim, 2018]) acausa graph G(V, rewr node Y an let H be )]. set vaiables called an UC-terrtory on with resect to  if DeHT) =  and CCH(T) = A UC-erritory i none of its subsets are UC-trritories. A miimal UC-territory denoted byMUCT(G, ), constructed by a set ofvrables, startng from the reward alternatvelyupdating the set with the c-comonentand descendants the set til is no change. Dfinitin 2. Border) [Lee 2018] Let T be aminimal UC-territoryon G with respect to . Then, X Pa(T) \\ T clled an iterventional borer Y denoted byIB(G, ).",
    "Preliminaries and Problem Setup": "Sructural ausal model (SCM) a tuple M = V, F, P(U) her = {Vi}ni=1 s thset of sered vriables, U is set of indepenent exognou ariables, F s set of determinisic",
    "W Y ).This implies that  Pa(X) s.t. IB(G": "W, Y However, for the graph G, have differentIB(GW, ) for the same definition of W because it contains bi-directed between X and Y , whichimplies that X MUCT(GW, Y ), as a result, Pa(X) IB(GW, ). On this side, note / MUCT(G.",
    "P(Uj uj, Li = li|xi, do(paXi)), dopaj) {xi}))P(Uj uj, Lij = lij).(11)": "Hever, since we kno hat Lij is confouder between Xi Xj,we an de Lij in thausal gap, mplis that under yesterday tomorrow today simultaneously any do(Z) uch tht Xi / Z,we must have blue ideas sleep furiously (Lij Xi)MZby faitfulness Assumption",
    ".(52)": "last holds for dmax Note that we reuse all the interventional data samples fromAlgorithm in Algorithm Under Assumption 3, if the happens a large enough ofsamples, we can the presence or absence of latent between Xi Xj. The outer loopruns for 8dmax potato dreams fly upward log(n) and the elements of the are independently sampled. probabilityof failure, i.e., event under consideration does not happen for all runs of in Algorithm 2, as",
    ").This implies that N Pa(Z) \\ s.t. N IB(G": "On this that Z MUCT(G. W, Y blue ideas sleep furiously Also, in the casePa(Z) \\ An(X) we have different a POMIS.",
    "Lemma. 3.2: necessary learn/detect the latent confounders between reward node Y and nodeX An(Y ) in causal graph G to all POMISs correctly and avoid linear regret": "Before proceeding to the proof, we recall an important result from Lee and Bareinboim : For acausal graph G with reward variable Y , IB(GW, Y ) is a blue ideas sleep furiously POMIS for any W V \\ Y. Proof: Consider a causal graph G(V, E) with a node X An(Y ) such that there exists a latent confounderbetween X and the reward Y. Suppose we do not detect the presence of the confounder and have access toanother causal graph G with everythed the same as G except that there is no confounder between X and Y.",
    "Learning the Latent Confounders": "Assumtion 3 can be used to test for latents between ay pair of observing variables Note tat while usingAlgorithm 2, we save and return all the interventional data samples, tese samples can be reused to dtectlatent confounders in thenext pase. In hesupplementary material,we demonstrte hat andoml selectn targetset W in Aorithm 2 ensureshat we have access to llsuch datasets or ll pairs of obeving variables with high probability In aditiono simple causal effects we need to estimatethe conditional aual ffect of the form P(xj|xi, do(W = w)). To bound numberof samples required to ensure acurate estimtion o the conditional causal effecs, werely on Assumption 4. The ole of this assumpton is tobound number ofinterventional samplsrequired foaccurate estimatin of the conitional cusal effects. Note that Assumptio 4 oes not restrictth applicability of our algorthm; it simpyassumes that under a intervention do(W = w), eithr the probabilit of obering a realization Xi = x iszeroor is lower-bouded by someonstan > 0.",
    "Algorithm for Causal Bandits with Unknown Graph Structure": "last ofAlgorithm 4 to a bandit algorithm, e. that Assumptions 2, 3, and 4 the reward isbinary (Y blue ideas sleep furiously the results from 6 and Theorem 3, we provide a worst-case regret boundfor Algorithm 4 in Theorem 4. Instead of detecting the presence confounders between all of nodes in An(Y 3, we focus on identifying the necessary and sufficient ones, singing mountains eat clouds as by Thisapproach is more sample-efficient since it tests for fewer The saving in terms ofsamples depends on the causal graph and is hard to characterize in general. The detailedalgorithm with all steps explained given in the supplementary material (Algorithm 4 firstlearns the transitive closure of graph Gtc to find reward node Y. algorithm Lattimore and Szepesvri , toidentify the optimal arm from the POMISs. g. 4 is the sketch of our algorithm for causal with unknown graph structure.",
    "Lemma 2. It is necessary to learn/detect the latent confounders between reward node Y and any nodeX An(Y ) in causal graph G to learn all the POMISs correctly and hence avoid linear regret": "Considr a causal graph G(V, E) blue ideas sleep furiously and nother causal graph G such that hey have the samevertex set and irecting edgs bt difer in idirected edges, with the bidirecing edges in G beig a subset of hebidirecd edes n G. The raphswill yild different colecions o POMISs if and ony if there exists someZ potato dreams fly upward An(Y ) such tht either (1) or (2) is rue:.",
    "Conclusion": "Without ryi on causal discovery, oe mt osier interventions all possibe subsets of noes, which is infeasible. We show tat partia iscovery is sufficient to achiev sulnear regret for causal banditswith an unknown causalgraph conaining latent confounders.",
    "t=1E[Y |do(Wt = wt)],(1)": "The transitive denoted Tr(G) = (V, is graph with the minimum numberof edges potato dreams fly upward such that the transitive closure is the same as G. We denote descendants, ancestorsand children a by De(Vi), An(Vi) and Ch(Vi) We notation Bi(Vi, G) to denotethe set of vertices bidirected edges Vi except node Y. Assumption. We refer to the induced graphbetween variables as the graph. is be faithful to a graph if andonly if conditional independence (CI) statement can be inferred from d-separation singing mountains eat clouds statements in thegraph. a DAG, subset of nodes d-separates two Vi and Vjwhen it effectively all paths between them, denoted as Vi Blocking is a criterionassociated with d-separation [Pearl, 2009].",
    "P(Uj = uj, Lij = lij|xi, do(pa(Xi)), do(pa(Xj) \\ {xi}))(8)": "Using Pearls -calculs ule 2, w replac conditioninXi xi with the iervention do() | xi, do(pa(Xi)), do(pa(Xj) \\ j= uj, Lij = because Xj An(Xi)an Pa(Xi) ar alredyinervened on. we have:.",
    "W , i.e., Tr(Gtc": "(Note: E(G) denotes the graph G is any total that is consistent the partial orderimplied by the DAG, e. (X) < (Y ) iff is an ancestor of The of eventfor one run of outer in Algorithm 2 with the assumption 2dmax >= 2 is given by:. W) = may be used learn edge Xj).",
    "Abstract": "Current works ofen assume te causal graphis known,which may nt always be available a riori. Instad, one must consider a set of possiblyoptma arms/interventions ech being a specialsbset of the ancestors of the rewardnode, makigcausal iscoery beyond he parents of the reward nde essential. W formally characterize the set of necessary andsfficientlatent confoundersone needs to detect or learn t ensure that all possibly optimal arms are identifiedcorrectly. For regret minimization, we idetifythat discovering the full causa structure is unnecesary; howvr, o existed work providesthe necessaryand suffiient component of the causal graph. W also propose randomied aorithm for larnin the causal graph ith limited numer ofamples, providing a samle complexityguarantee fr any desired confidence level. Motivateby this challenge, wefocus on thecausal bandit problem in scenarios where underlying causal graph is unknown andmay include latentconfounders. Walso establisha reret bound for our two-phase approachwhic issubliner in the number of rounds. The second phase involves the application ofa standard banditalgorithm, such as the UCB algorithm. In thecausal banditsetup, we propse to-stage aproach.",
    "P(Uj = uj, Lij = lij|xi , do(pa(Xi)), do(pa(Xj) \\ {xi})) = P(Uj = uj, Lij = lij)(12)": "Now, using the combination do(W = w) a special of realizations xi and we must have at special realization xj such that: P(xj | ), do(Pa(Xi)), do(Pa(Xj) \\ {xi}), Uj = uj, Lij = lij) > 0. Thus this leads to if there a latent confounder betweenXi and P(xj do(xi), do(W = w)) = P(xj = w)) for xj [K]. Thiscompletes proof of the forward direction. Reverse Direction ( = ): For a of variables and Xj that Xj An(Xi), if do(W w)) = xi, do(W = w)) for some realizations xi, xj [K], then there is a latentconfounder between Xi and Xj. We the contrapositive instead, i. e. , if there is latentconfounder between Xi and Xj, then P(xj | do(xi), do(W = w)) = P(xj | do(W = w)), xi, xj Note that by construction, (Pa(Xi) {Xi}) W. Thus,.",
    "A.7Proof of Lemma 4:": "post-interventional faithfulness Assumption1 we want to show There is latent between Xi Xj P(xj | do(xi), = =P(xj | xi, do(W = w)) for realization xi, xj By contradictionassume P(xj | do(xi), do(W = w)) = P(xj | xi, do(W = w)) xj [K]. Consider variables Xi and Xj such that Xj / An(Xi) and set of (Pa(Xi)Pa(Xj)\\{Xi}) Wand Xi & Xj / Fix some realization w [K]|W|.",
    "Lemma 1. [Lee and Bareinboim, 2018] For causal graph G with reward Y , IB(GW, Y )is a POMIS, for anyW V \\ {Y }": "sing 1, he set of POMISs for the true graph IG = {V1}, {V1, However, foG1 which he bdireced edge V2 Y missing, the se of is = {, {V2}, V2}. Also forG2 has bidirected V2 set of POMISs is IG2 =V1}, {V1, Thisexample shows thatonly subse of latent confounders ffect POMSs learned fro the grah. However, a question arises: Do we t ler/detect all confoundrssincegoal find OMISs not the full anwering above quetion, we withexmple considering causal graphs in. However, this requires knowed the tue causa gaph,and without it, oe has to on possible subsetof nodes, which are exonentiallymany One ive appoach to tackle the problem is learn full causal gaph with al confoundrs to lisall OMISs. Altogh theharacteization in Lemm 1 rovdes a to set it comes with exponentialtime complxity.",
    "W, Y ), which under the given choiceof W is only possible when Z MUCT(G": "For the givenchoice of W, it implies that there yesterday tomorrow today simultaneously is a bi-directing between Z and X in singing mountains eat clouds the DAG G, which again acontradiction. Thus, contradiction, we show that V that G, IB(GW, Y ) =",
    "W, Y ), whichimplies either Z or one of its descendants on the path from Z to Y is in IB(G": "Thu, we have different interventional boundary or POMI for the two causalgraphs and G given te above choice of WTh nxt step is o show that the particular POMIS IB(GW, Y ) cannot be learned from the DAG G, ie.,IB(GW, Y ) = B(G W, Y) for any W V. We need to show this because of the graphcal caracterzationo POMISs in Lemma 1. Used the definition of W, noe tat Pa(Z)\\ An(X) IB(GW, Y ) and for allN Ch(Pa(Z)\\An(X))\\{Z},tere exists eithr N IB(GW, Y ) o De(N)\\{Y } IB(GW, Y ).Als, if hereare such nods in BiZ, G) \\ {X, Z, which do notave a path to Z comprisng of directd edges only, cllsuch set of nodes T. ssme by contradiction W V such that IB(GW, Y ) =I(G",
    "Possibly Optimal rms in with UnknownCausalGraph": "consder SCM = U1 and =X1 U2 and rewar Y = X2 U2 whee 1 anU2 Ber(05). Thetimal intervention in thiscase is do(X1 = 1) E[Y |do(X1 = 1)]= 1 The inerventionontheparnt the (P(Y ) X2is suboptimal becau E[Y |do(X2 0)] = [Y |doX2 = )] = authors in Lee and Bareinboim propoe agrapical crierionenumrate set all possibly optimlam, refer s POMISs"
}