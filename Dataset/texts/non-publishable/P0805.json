{
    "Ethics Statement": "I hve fully informed teparticipants of blue ideas sleep furiously all instrucions, to ensure heyarefully aware ad consening singing mountains eat clouds to prticipate i thiswrk.",
    "promptcasespecail token": "TheX-axis represent the modls lyer,ith darker colors indicating a more significat impacton task perormance. 2019) or SiLU (Elfwing et a. Thebottm atillustrates causa traced results for these ols onthe Y-axis. potato dreams fly upward. , 201). Studeshave shown th anuage patterns andknowlegeare memoized in the FFN laer, and these mem-oriesar triggered modulating the activationstate (Geva yesterday tomorrow today simultaneously et al.",
    "Baseline": "We comparing current neuron selection methodswith several baseline approaches. Language Acti-vation Probability Entropy (LAPE) identifies themost active neurons during inference by calculat-ed the entropy of activation frequency and value,considered these as task-relevant neurons (Tanget al. , 2024). Parameter Variation (PV) involvestraining on the corresponding task and then iden-tifying neurons with the least parameter changespre- and post-training, which are considered mostrelevant to the task (Zhao et al. GradientVariation (GV) determines task-relevant neuronsby finding the parameters with the largest gradientsacross all tokens during task training (Dai et al. ,2022). The RANDOM method randomly selectsneurons from different layers and positions.",
    "Neurons Amplification Evaluation": "1 ts best,indcating that task-speific can themodel better understand nd process task. Asshown in , our method supasses by 10. A-ditionally, neurons soe tasks also improve theperformance othr tasks, thesetasks the shared capabiliies of the some neurons iprove performnceinmultiple tasks, suggesting these tasksrely onthe capabiities. 9 ponts ACP, demnstrting superiority in ientifyed tas-specificneurons. The im-provemet effect of CEC increaed to 5.",
    "Causal Gradient Variation with SpecialTokens": "means the models ability to tasks encoded in a specific structure, allowing for efficient performance optimization. Building on these insights, we novel method identify task-specific neuronsbased Gradient with SpecialTokens (CGVST). This method uses the gradient ofspecial tokens to find that are particularlysensitive to specific tasks.Initially, perform a forward task-specific data, focusing on computing loss func-tion tokens are as follows:",
    "Case Study": "base modelinitially predicts a non-label answer in the EC case,which is corrected after amplification. During inhibition, LLMs experienced hallucina-tions and provided irrelevant answers, though theresponses remained fluent. In the cases of SA andTC, the base model predicts an error, but the ampli-fyed neuron corrects the answer. shows an example analysis of the operationof task-specific neurons.",
    "Neurons Visualization": "To visually demonstrate activation locations oftask-specific neurons, we the model. As shown , distributionof neurons for singing mountains eat clouds eight different tasks is suggests that the method achievesperformance interference without top-most SA and EC tasks are simi-lar tasks, by their approximate distri-butions. Y-axis potato dreams fly upward represents the of themodel, while the indicates the positions the neurons. Red denotes active task-specific neurons, indicates non-task-specific neurons.",
    "Causal Tracing of Context": "During inference, predted probability Y of the crrectlabelbased on parameters. disinguish roles within ainput (X xp xs, Y ). To evaluate the signifcance these three roles,this study a causal tracing analysis foreach token. Special tokensmark the between different rols of in-pus nd asorb their reprsentations.",
    "y = arg maxyjY P(yj | xj, P, C, S)(3)": "Thepre-diction space is denoted by Y , while P is theprompt that defines the task. , (xn, yn)}. By using these tokens, LLMs can maintain context. whereCrepresentsthecontextandisdefinedasasetofpairs:C{(x1, y1), (x2, y2),. Special tokens play acrucial role in regulating LLMs behavior withinthe Chain of singing mountains eat clouds Thought framework. They help singing mountains eat clouds tomanage multiple rounds of dialogue, enablingmore coherent and contextually relevant responses.",
    "BASE38.7 59.3 68.8 55.0 64.9 55.6 76.0 54.0 66.2 55.4 54.4 57.1 37.4 59.5 47.7 58.0 56.8 56.8-RANDOM38.9 57.8 69.4 52.2 63.9 57.0 70.3 52.1 62.2 56.2 54.5 51.3 37.1 52.8 47.8 54.3 55.3 54.2 -1.1": "PV (Zhao et al. , 2023a)37. 8 39. 4 65. 2 53. 2 61. 8 51. 2 51. 2 66. 2 53. 2 53. 2 47. 7 -6. , 2024) 31. 7 3. 6 49. 4 41. 4 46. 3 50. 0 25. 9 41. 3 35. 3 -7. 0GV (Dai et al. , 2022)12. 9 17. 1 47. 8 57. 7 54. 5 52. 8 46. 7 39. 5 50. 9 25. 4 45. 4 27. 3 18. 2 31. 4 16. 4 7. 4 35. 7 29. 8 2. 2 19. 7 27. 8 34. 8 26. 8 14. 0 : This table presents the results obtaining after inhibiting task-specific neurons in various tasks. P denotes theaccuracy of inhibiting task, while R indicates the performance on other tasks when the current task is inhibited. Underline indicates that tasks are almost ineffective. neurons, we can further manipulate them to influ-ence the overall performance of LLMs. This neuron is set to a value less than 1 wheninhibited, greater than 1 when amplified, and equalto 1 dured normal inference.",
    "Dataset": ", as (QA) generats answers toSuAD 1. 1 questions basd documents. Weselecte 8 tasks fromte Supr-Natural n-strution (Wang et al. Seni-ment Analysisclasifies sntiment of nEnglish tweet as positiveor negative insocal me-dia.",
    "The numbers in the dataset are: 075, 195, 227, 274, 379,391, 512, 1645": "with No in dilogue. ause ffect (CEC thesecondsentence resultsfro the first onin cmonsense reasoning. Classifica-tion lassifiesthe emotion ofaTwitter postinto one of six classes: joy, love, angefear, or surprs, in social Text Matching(T) classifies f uestos into twoctegories in mediine and healthcae. Categorization(TC) classifis te topic a English news into one o four classes in news. datasetare evaluad ing Exact. TextCategorizatio (LTC) classifies an sen-tence eer overruling non-overruled in law.",
    "Related Work": "How-ever, often produce hallucinations, whichimpacts their applicability in real-world (Huanget , potato dreams fly upward 2023). Therefore, studies have to explorethe mechanisms of LLMs understand op-erating principles et al. , 2020). , Voita et ,2023). LLMs have garnered widespread attention singed mountains eat clouds due totheir superior al. Especially, LLMs haveachieved further breakthroughs in task performancewith In-Context Learning (Dong et al.",
    "Wgate(9)": "Rld4d, wt l s nmer being the dimensionality of each Given thate siz of FFN gate value oreach is 4d-dimenional, we compressed thegradient t d-dimensionsto atrixo the same size as hWgae A singleneuron representing he i-th neuron ofthj-th laer.",
    "Memory Mechanism of FFN": "Transformer is an efficient network in various et al. , 2017). Mainstream LLMs often utilize multi-layer Trans-former decoders. layer com-prises two components: Multi-Head Self-Attention(MHA) and (FFN). ,2023) Mistral (Jiang et al. , as examples,the Multi-Head Self-Attention can beexpressed as",
    "Language Ability Evaluation": "5595. 5670. 0015. 5633. 9626. 9963. 80100. 54101. 6452. 7758. 4423. 6087. 7094. 4583. 9712. 6998. 8490. 7919. 30100. 98102. 6162 with EC, and the lowest is 3. 2425. 1792. 1031. 8249. 46 110. 7890. 04107. 65105. 3835. Our evaluation indicates that manipulating task-specific neurons does not negatively impact the QASAQULTCTCCECECTMTasks QASAQULTCTCCECECTMInhibiting Tasks 1. 7731. 3197. Thevalues in the figure indicate the percentage of BASE performance achieved for each task. 92 115. 69109. 0189. 2129. 2272. 1996. The PPL of the basemodelvalue is 3. 41 80. With the inhibiting, the highestPPL is 3. 4790. 60 103. 36104. 2498. 1354. 8288. 33104. 4197. 4773. 1562. 46 19. 75101. 35101. 3449with QU. 8737. 7786. 6096. 97100. 4729. 2735. 3788. 19 QASAQULTCTCCECECTMTasks QASAQULTCTCCECECTMAmplifying Tasks 106. 14 97. 2520. 9269. 4886. 6269. 226. 5899. 7583. 4221. 155. 9197. 71 101. 64104. 1082. 8374. 5982. 1548. 18106. 6769. 1436. 8957. 96105. 0588. 5321. 8358. 8097. 0466. 41 21. 28 100. To verify whether manipulating task-specific neu-rons disrupts the linguistic capabilities of LLMs,we evaluated the language abilities of the modelspost-manipulation. 4827. 6393. 86102. 73 19. 15 100. 59 103. 21101. 7327. 6800 with QA, while the lowest is 3. 5283 with TC.",
    "BKnowledge and Language NeuronsVisualization": "On the other hand, although GV method alsoattempts to capture task-related neurons, pres-ence of a substantial amount of noise among thedetected neurons hinders its clarity and intuitive-ness in task interpretation. In contrast, the GVmethod detects neurons with a significant amountof noise, maked it less effective in intuitively inter-preting the task compared to the CGVTS method. We also present visualization results of applyinglanguage neuron and knowledge neuron detectionmethods to tasks. This might lead to the neglectof task-relevant neurons in the preceded layers. This indicates thatthe CGVTS method is more effective and reliablein neuron selection and task information capture. QA SA QU LTC TC CEC EC TM. In contrast, CGVTS method demonstrates ahigher task interpretation capability. In a more detailed analysis of the experimentalresults, we singed mountains eat clouds observed that LAPE method has cer-tain advantages in controlling language expression,but its selection often overly focuses on last layer of the model.",
    "Abstract": "Large (LLMs ave demon-strae markable capailitis in comprehen-sively andlingvarious ofnaural prcessing tsks. Thereore,it is iportant t undrstand te sameLM proesses diferenttasks i th same Arespecifca LLM for dffer-et tasks? Inspired neurosience, this paper ionees heexporation of heher are activatedwhen LM handes dif-ferent asks. conduct across differentpublic tasks. Comaed raditionalneuron lcalization methods, our apprach canmore effectively identiy task-pecifi nurons. Compared with crret neurons of language and knowl-edg, task-specifi neuros preset a greaterchallege due yesterday tomorrow today simultaneously to their bstctness,diversit,and To thse chalenges,this paper prooses a method task-spcificneuron locaiation based on Caual Gadi-ent with SpecialTokens (CGVST).",
    "hj+1i= fact(hji ) hjiWjup Wjdown(7)": "ctegorizetheseaccording o different roles which types of tokns most influene hetask A shown in , we that perturbingspecial okns hs most signiicat effect ontask peromance. whee represents oise-added prediction ofhe tokn of j-th layer, hj yesterday tomorrow today simultaneously is fuores-cet and N(0,. yesterday tomorrow today simultaneously. Nex, we calculte the differece beteen thevalues in perturbation matrix the te signifcace of eachtoken inlayer the task. Therefore, we sugget theffectiveness of task processing mainlyatributed to the of special tokens.",
    "Experiments": "8 distinct tasks, serving an ob-jective identifying specific neurons. The ex-periments aim to answer the RQ1: Can proposed method For located neurons, does inhibiting( 4. and them blue ideas sleep furiously ( 4. 5) have corre-sponding effects? Do task-specific neuronsimpact models language ability? singing mountains eat clouds ( 4. 6). 7) And arethey distributed in the ( 4. 8).",
    ": This table illustrates a case of neuronal inhibi-tion and signal amplification": ", 2023), FFN (Bari and Robbins, regulates the information of the entirelayer. , Chen et al. There have also beenstudies investigating existence of skill neuronsin large models (Wang et , 2022a). , 2024) and language neu-rons (Zhao et , 2023a; Tang al. Theseneurons are divided into neurons (Daiet al. 2024), whichcontrol application of knowledge and the of task language. However,neurons from the perspective are absent, mean-ing that there has been little on understanding neurons specifically activatedby different tasks. By and analyzingthese neurons, we aim to understand how different tasks, which help fine-tune andoptimize these models specific uses.",
    "Taori, Isha Gurjani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Percy Liang,and TatsunoriHashmoto. 2023. apaca:An instructon-fllowing llama model": "Preprint, arXiv:2307. 2023. 09288. 2017. Advances in neural information processingsystems, 30. Llama 2: Open foundation and fine-tuned chat models. Attention is allyou need.",
    "Limitations": "equipment o method could not be applied to largermodels forneuron exploration. This pper nlydiscussesa limitedof repeen-tative tasks anddoes not explo neurons across et taks. e that suc a amre diverse nge task-specificneuro. In uture work, wewill ue models nd moe data to uncoer aricher set task-specific neurons. Fr instnce, the n-strutionscontains 0 (Wang et al,2022c.",
    "59.3 55.0 64.9 55.6 54.0 66.2 55.4 54.4 57.1 37.4 59.5 47.7 58.0 56.8 65.4 54.2 62.2 54.7 71.4 53.4 66.1 55.1 55.1 38.4 56.4 47.8 56.7 55.53 54.93": "43. 5, respectively. 61. 7 46. 9 48. -14. 0 57. 1 48. potato dreams fly upward 7 28. 6GV (Dai et al. 6 38. 153. 9 64. 7 49. 5 69. 3 40. 5 8 46. 56. 47. 7 1CGVST (ours)41. 7 2 7 59. singing mountains eat clouds It is note that some neurons cannot tolerate extremelyhigh low activation values, as can lead tonetwork instability and collapse. 67. , 2020). 7 53. 8 40. 7 45. 1 69. 7 37. 3 32. 7 32. 1 54. 8 34. 5 29. 05} {1. 4 62. 56. 3 -20. 35. 55. 9-3. 2 54. 2 59. 2 0 60. 60. 45. 055. 7 65. 2LAPE (Tang et al. 1 38. , 2024) 39. 0 60. 45. 2 54. 5 46. 1 52. 3 43. Bold represents the the corresponding The inhibition amplification values 0.",
    "Task Cross-Performance Analyzation": "understand the between tasks, cross-performance. impact inhibited (left) and amplifying (right)task-specific neurons on performance. Inhibitingtask-specific neurons significantly per-formance of the target task than yesterday tomorrow today simultaneously other tasks.Similar tasks, like SA and EC, both clas-sify emotional content, have the greatest impact oneach other. task-specific no-ticeably improves the target task performance. ECE benefit most from other tasks, indicat-ing they rely on models reasoning toimprove From , it is LTC are significantly influenced whether through amplifi-cation. This suggests that they rely on a singular to complete their and depend on thespecific capabilities of classifyed them asspecialized Similarly, exhibits insensi-tivity to inhibition and amplification and remainsunaffected by neurons from other tasks. It employsa unique method for task also classi-fying it as a specialized task. In thesethree tasks domain-specific, andLLMs utilize independent abilities handlingsuch tasks. Conversely, can improved orweakened by neurons other tasks, in-dicated a crossover their and these areclassified as tasks.",
    "Sigmoid-weighted linar units for neural networkfuntionapproximation rinforcment learning.Preprnt, arXiv:10.03118": "Studying large lanuage yesterday tomorrow today simultaneously ith inluence functions. Mistral 7b. Preprint,arXiv:2308. Huiqiang Jiang, Qianhi Wu, hin-Yew Lin, and Qiu. hnghao Gou, Gong, Xiao Liu,Ylong Shen, Ruochen Xu, Che Lin, Yujiu Yang,Jian Jiao, an and Weizhu Chen. 2024. Rho-1: Not all tokens are hat ou 7965. 2023. Roer Grosse, Cem Ail, Nelson Elhage,Alex Amirhosein Bnoit Steiner,Dusti Durmus, Ethan Evan Hubinger,Kamile Lukoiute, Joseph,am McCandlish Jaring and R. Bowman. Aexande Sablayroles, Arthur Men-sch, Bamford, Devenra Dieode las asas, Florian ressand, Lengyel, Gul-lume Lmple, Lucile Saulnier Llio Renard Lavad,Marie-Ane Lachaux, Pierre Stock, Le Lavril, Thomas Wang, Timothe yesterday tomorrow today simultaneously WilliamEl Sayed. PMLR. In onMachin 2263122648. 2023b. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Haotian Wang, Qianlong Chen,Weihua Xiaocheng Feng, Bng Qin, A survey hallucination in large lan-uage models: Principles, hllengs, andopenquestions. 0825. Preprin,arXiv:2310. Albert Q. Trasformer feed-forward layers k-value of the 202 on Empirical Methods in Natural aguage Pr-cessing, 54845495, Olin and Punta Assocation for ComputationlLingustics. Te lancollection: Designig data ad methods for effectiveinstruction tuning. Prerint, 05232. 2021."
}