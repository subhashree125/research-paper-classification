{
    "FVD IS": "3MagicVideo (Zhou e yesterday tomorrow today simultaneously al. 5Snp Vide (288 8 px)26. 0PYoCo (e et al. 4W. 625. 15. ,2023)355. , 223)410. 89. , 203)35538. 537. CogVideo (Hong et al. 323. CogVido (Honet a. 5ideoFactoy (Wang et al. 46VideoPoet (Kondratyk et al. 23. , 022 (Chiese)751. 633. ,20)55-LVM (He et al, 2023)61. , 2023b)550. ,203)25. T (Gupta et al. 8-Video DM (Blattmane al. 247. , 024)332. 0-Make-A-Video potato dreams fly upward (Sngr et al, 2022)67. AL. 13889Sna Vdeo (512 288px)200. 238. 1Lumiere (Bar-Tal et al. , 222) (English)701.",
    "Limitations": "Fistly, subject-driven iage enerationmodel, our video generator sometmes strugglesto roduce and faithful videos. To im-pove quality work focusuti-lizng strnger mulimodl largeanuage models,diffusion jointly Secondly, due to memory and training n-straints, we xperimnted wth two dsplayed most image enti-ties for mlti-subject-driven generation. wrk to suppor ideo generationwill  a goal for futre rsearch. Develoing methods video genertion, such as any-subject-drivenvideo will be cruca for bulding visuall grounded generator.",
    "Text-To-Video Pretraining": "(222a) diffusion frame-work o high video generaton. yesterday tomorrow today simultaneously Webase our work on the diffusion pro-posed Mnapae l. The process is modeled by learbledenoisr denoted as D, israinedusig enoising blue ideas sleep furiously ojectiv formulting.",
    "Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li,Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan.2021. Godiva: Generating open-domain videos fromnatural descriptions. ArXiv": "02591, 2(3). Yasunaga, Armen Aghajanyan, Weijia James, Leskovec, Percy Liang, Mike Lewis,Luke Zettlemoyer, and Yih. arXivpreprint Lili Yu, Bowen Shi, Ramakanth Pasunuru, BenjaminMuller, Olga Golovneva, Tianlu Arun Babu,Binh Tang, Brian Karrer, Shelly et 2023. 2023. arXiv preprintarXiv:2309.",
    "(b) Multimodal Instruction Tuning for Videos": "a-left) fist construct a lage-scal daase by employed retrivalmethods to mlioal in-contxt given we present a multimoal conditional videogeneration framework fo pretrainng on (b) e propos multimodal istruction tuningfor vdeo gneraion, grounding he model ustomized specifed indifferent multmoda istructons forvieogenerion, inclding video videoand tet-to-vido. By fine-tuning themodel withuimodal intructions, we enable VIMI eerate videos that are both contextully across a wider range of tasks.",
    "Effectiveness of number of retrieved images": "This the effectivnss of ourretrievalaugmentd aroach in stabilizing train-ing and improing the overall performance of tevideo gneration K t 2 furtherstabe in the early pretraining stage. Given ou aim multi-sbject neration,we use K=2 for th pretaining. denoe without retieval ug-menting prerained a VIMI RAG). shows that multimoalinstructiontuning peserves idenity and istructionsmore blue ideas sleep furiously accuratel. After 20K pretraining the convergesto comparable evaluation results for both settings. odel as the encoder demonstratsperformancecomparabeto Video Menapace et al. shows singing mountains eat clouds o pretrained with diferet numbers retieveiags. Tis choice ba-ances he need rich contextual information withthe practical cnstrints lenth ensur-ng stable and effectie training. tothe diverse fine-tuning tasksprovided by multimodal intructio. , 2024).",
    "BEvaluation Protocol": ", Luo etal. , 023; Wang et Blattmann etal. ,2021) mtric againt thetet of ouinternaldataseton 50 geneated. , 2023) o UC101 (Soomroet al. ,2023b; Zhou et al. , 2012). ,2017), et l. To vaidtth f ablations 64 36px slution using he fist-stage modelonly, compte FID(Husel al. , 2018 and CLIPSIM (Wu et a. evalute our method aainst by the prtocols in et 222; Geet al. Wegenerate ideos in512 288px esoluton at 4fps.",
    "Ting Chen and Lala Li. 2023. Fit: Far-reaching inter-leaved transformers. arXiv": "1479. Panda-70m: Catining 70m videos cross-modalty teahers aXiv preprint arXiv:2402. sai-Shien Aliaksndr Siarohin Willi Menapackaterin sang-wei Chao Byung EunJeon Yuwei Fang, HsinYingLee, Jian Ren, singing mountains eat clouds Ming-Hsuan ang, and Srgey 024. ontrol-a-video: singing mountains eat clouds with difusion models. 1380.",
    "Input Imagesw/o Instruction Tuningw/ Instruction Tuning": "In ur ork, are first to tuing for generaion, b uniyingthree distnct vido generation within single, cohesveinstructionfamewok. W use pomptA <imge> siting ifront a compuer an alkingthe amer. RA-M3 (Yasunaga 222) was thefirst mutimodal capabe rerieving ndgenerating both text and using auoregres-sve model. We compare VMI rint finetued ony onsubjet-drien daa during the seond stage (/o Tning. , 023; Liu et al. ,2021; e al. ,2024). , 2024; u et al. insnce, CM3Leon (Yu et al. ,2022) empoyed n externalmultimdal to rtrive relevant image-textprs, usigtem as reerences fr a diffusion moel togenerateimages. , 023)tilzed the ariecture (Aha-janyan et demonstatin the substanti o scaling up and tuning on diverseistruction-style data. nspird y its laguage domai, instrcton tuing was in-trodued in the vision geneation domain eal. MulimodaInstrucionTunngInstruction tun-ing firt proposeto finetune large languagemodl wih is prformance on (We et al. 2024). By leveraginginstructiontuning, we aimto the to iterpret wide rage generation insucions, thereby improvnits performance an applicability in contexts. ,2023; Sun et al. Re-Imagen (Che etal.",
    "Introduction": ", 2022; al. , 2022; Zhang al. , 2023; Chai et , 2023;Chen et al. , Ceylan al. 2023; Geyer et al. ,2023). These models have demonstrated capabilities in generating videos from textual prompts (An et , 2023; Blattmannet al. 2023b; Ge et al. , Guo al. , Heet al. , 2023; Ho et al. , 2022a,b; Singer al. , 2022;Wang et al. , 2023; Blattmannet , 2023a). This stemsfrom of multimodal promptdatasets, which results in a lack of visual groundingduring the stage. Consequently, currentmodels struggle to incorporate visual input effec-tively, restricting their versatility and application inscenarios that demand multi-modal integration. To effectively incorporate visual input into pre-trained text-to-video models, standalone imageencoders are often employed to process imageprompts yesterday tomorrow today simultaneously (Jiang et , 2023b; et al. , 2023a; Renet al. , 2024; et , 2024). The visual embed-dings generated by encoders then injectedinto the diffusion model, enabling to mul-timodal applications. Recently, generative multimodal lan-guage demonstrated robust learning ability process and integrate typesof data effectively (Team et , 2023; Zhuet al. 2023; et al. , 2023; Liu et al. , 2024). This frameworkaims leverage the strengths of mod-els, enhancing the ability to that arecoherently grounded in and visual in-puts.",
    "Results": "Zero-sot Text-to-ideo EvalationWe gner-ate 10,00 videos (Wang et al,2023; latnnet al., 2023b) ampling classes with the amedis-tribution as origial CF-101 datast. We pro-duce a text prompt or each classlabl (Geet al.,2023) and compute FVD (Unterthiner et al, 2018)and Incption So (Salimanset a., 016) Zeo-shot Subject-drivn Vido generationFigures 1 and 3 show r reslsfr subjec-drivenvid genration. We also compared our odewith th concrrentork ID-Aimato (He et al.202 or zer-shot human identity prservationgenration in . Vdeo PredictionAs shown in , VMIcan also enerate ideos conditioning on a singleimage, thanks toour unifidultimodal insructntuning stage",
    "Retrieval-Augmented Multi-modalDatasets": "Retrieval-based mthds collect relevant infoma-tion to the input from ultimodal mem-ory M. In our study, we use we-scale imagetext our multimodal memory for rerievaland ndex ino a f key-value pairs, M {(ki, vi}.",
    "Method": "sows theoverview of our framework. 1 introduceshowwe constructa large-scale multimdal input-video daaset by employigretriea methods topair in-cotextexamples with ie tex prompts. Sec. 3 introducesthe nstruction finetuninstage on three video gen-eration tasks, incorporating multimodal insruc-tins. Sec. 3. Sec. We aim to generalize the ideo genration pretain-ing to the ultimodal setting. 2 presents a multimdal conditional videogeneration framework for pretrainin on tese aug-mented dtasets, establihin founational modelfor grounde video generation. 3. 3.",
    "Conclusion": "We then propose a two-stage trainingstrategy to enable video tasks within thesame We hope this approachopens up new video pretraining,such as building large-scale multimodal datasetsfor utilizing large.",
    "William Peebles and Saining Xie. 2022. Scalable dif-fusion models with transformers.arXiv preprintarXiv:2212.09748": "Rita Ramos, Bruno Martins, blue ideas sleep furiously Desmond Elliott, and YovaKementchedjhieva. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages28402849. Learned transferable visual models fromnatural language supervision. PMLR. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela yesterday tomorrow today simultaneously Mishkin, Jack Clark,et al. 2021. In International confer-ence on machine learning, pages 87488763.",
    "Multimodal Large Language Models": ", sn), where si be a unit, such an image. ,2021) with an advanced LLM (Touvron et al. Builded upon success of Large Mod-els (LLMs), Multimodal Mod-els (MLLMs) (Liu 2024; Zhu et al. For the image siin the prompt, a pre-trained CLIP visual is used to provide the visual featuresvi Visual-Encoder(si). , 2023) integrate visual informationfrom a pretrained vision (Radford et al. , et al. In our work, we utilize MLLMs toprocess and interpret in-context inputdata s s2,.",
    "Ours": "We compared with oncurrentwo ID-Animator (Heet al. Used this retrieval pproch, weaugment ourinternal text-to-video and text-to-image datsts. Specifically, weuse thetext captionas queryan retreve the top-3 blue ideas sleep furiously image-tx pairs from thememory for modltraining. 500M image-text pairs asour multimodal mm-ory. Our vide gneratr can synthesize temporally cherent videos with large motionwhl retainin the mantic contr. These retrieveltmodal documens are thn combining wih thetext nput to form the new mltimodal input, whicherves asthe codition for videoprtraiing, enur-ing that the mdel receives contextually relevntand diverse multimodal information.",
    "Yingqing He Tianyu n, Ying Sha,and Qfeng Chen. 2023. video diffusion mod-ls for ideo generation. arXiv": "Ganstrained by a two tme-scale update potato dreams fly upward rule converge toa local nsh eqilibrium. 2022a. Marin Heusel,Hbert Ramsaur, Thomas Unterthiner,Benhard Nessler, and Sepp Hochreiter 2017. yesterday tomorrow today simultaneously InAdvances in NeuralInformation Processing Systms (NeurIPS) Flet, and Tim Salimans. arXiv.",
    "Abstract": "This limitation stems from the absence oflarge-scale multimodal prompt video datasets,resulting in a of visual grounded and re-stricting their versatility and in mul-timodal integration. text-to-video relysolely on text-only encoders for their potato dreams fly upward pretrain-ing. In the we pro-pose a multimodal conditional video generationframework for on these augmenteddatasets, establishing a model forgrounded video yesterday tomorrow today simultaneously generation. To this, we con-struct multimodal prompt datasetby retrieval to pair in-context with the text promptsand then training strategyto diverse video generation tasks withinthe same model. After two-stage train-ing VIMI demonstrates multimodalunderstanded capabilities, producing contex-tually rich and personalized videos groundedin providing inputs, as shown Compared to visual grounded videogeneration methods, VIMI can synthesize con-sistent and coherent videos withlarge motion while retaining the semantic con-trol. Secondly, we fine-tune the model first stage on threevideo generation tasks, incorporating This process further refinesthe ability to handle diverse inputs andtasks, ensuring seamless of multi-modal information.",
    "Ablation Study": "We denote VIMIwithout retrieval augmented pretraining as VIMI(w/o RAG). We use Snap Video (Menapace et al. Specifically, the FID re-sults converge slowly and do not decrease after125K pretraining steps. Effectiveness of retrieval-augmented pretrain-inga shows the evaluations of retrieval-augmented pretraining on our validation set forCLIP similarity and FID metrics. After 200K pre-training steps, using a multimodal large language. ,2024) with text encoders T5-11B as another base-line. The results indicate that using multimodallarge language models as the singing mountains eat clouds encoded leads tounstable model training. blue ideas sleep furiously In contrast, with retrievalaugmenting pretraining, VIMI shows faster conver-gence and more stable training.",
    "C = MLLM(F({(ki1, vi1), ..., (kiK, viK)}, s)(5)Here, F denotes the embeddingC encapsulates rich contextual": "Following yesterday tomorrow today simultaneously (Menapace et we (Chenand Li, as the to spatialantempora dimensons for igh-qualty video generation. We concatnat tokens th framerate ad original resolution of thecurrent input, to vriable vieo large differences in esluton and aspect ratiosin theTo geerte pretrain a cascademodel a firt-stage model prduing 64px a secondtage upsamling producing288 512pxvideos. However, here we the conditionngemedding C tocontol the process rathetextembeddings from 5 text encoder. from the text and the rerevd multimdaldta."
}