{
    "= (5)": "where is th query, the and (, i the releancescore between ad. We our DeepBoW model hiselevancescore as DeepBoW(Q-Weight We the crossentropy loss betwen the andthe trth to train model. Th lossunction is as follows:.",
    "Baseline": "RBERTa and belng nteraction-based arhitecture which is also know as the cros-encder achiecture. In addition, we several state-f-the-art methods or omparison. Besides, we investigaethe ReprBET wit similarity sore insteaof MLP onine of relvance from fair comparisn, we also leverage the lke RoERTa and a the encoder he two-tower model. main difference betwen the twomethods DeepBoW(Q-Synonym) we leverage synonymxpnsion BoW representation to replace the term-weighted BoWrepresentation for Therere two mthods to truncate the pare oW representation, ones the according ttheir repective the ther s to discard the terms whoe values are aler tanthe ivingthreshold.",
    "where Wm R, Wagg R, bm, bagg R, is the con-catenate operation": "the characer-based model prforms bett taheword-base model inhinese NLP tasks , ad mot moels chaacter-based archeture. singing mountains eat clouds For conenience, we character-segmentaion and the word-egmentationsquence the int tex {1,2 ,} ad ={1,2, ,}, rspectively,whre and are indices ofth tokenin th ocabulary. output encoded matrces f sequence ad the e-quence are = {1,2, {1,2, ,},here and are the he carcter-segmntation adthe word-egmentationsequence.The text encodingrepresentationofthse two sequecesare",
    "Query: oWSE(Query: [(, 0.30148), 0.578), (, 0.2277), (, 0.21297)]": "99954), (, 0. 98563), (a, 0. 99993), (18, 0. 99502), (, 0. 9661), (, 0. 99992), (, 0. 99862),(, 0. 9981), (, 0. 94598)]. 98445), (, 0. 99247), (, 0. 98451), (, 0. 98294), (, 0. 99577), (,0. 99968), (a,0. 99622), (, 0. 97423), (, 0. 9712), (, 0. 99977), (, 0. 99627),(2023, 0. Product: V18-24VA2023BoWSE(Product): [(, 1. 99014), (, 0. 98666), (, 0. 98703),(, 0. 99961), (, 0. 96265), (, 0. 97758), (, 0. 98461),(, 0. 98937), (, 0. 9997), (, 0. 99536), (, 0. 98441), (, 0. 99512), (, 0. 99237),(18-24, 0. 98546), (, 0. 99993), (, 0. 99999), (, 0. 99985), (, 0. 99824), (, 0. 94854), (, 0. 9943), (, 0. 9998), (, 0. 95822), (, 0. 9794), (, 0. 98587), (, 0. 96137), (, 0. 99657), (, 0. 96633), (, 0. 99944), (, 0. 96755), (, 0. 95046), (, 0. 99379), (, 0. 9999), (,0. 9946), (, 0. 97937), (, 0. 95057), (,0. 94602), (, 0. 96739), (, 0. 99621), (, 0. 98916), (2019, 0. 98606), (, 0. 0), (v, 1. 99616), (, 0. 99094), (, 0. 97752), (, 0. 99999), (, 0. 99998), (, 0. 98471), (, 0. 98398), (, 0. 9822),(, 0. 0), (, 0. 96329),(, 0. 99999), (, yesterday tomorrow today simultaneously 0. 99995), (, 0. 99934), (, 0. 99618), (, 0. 96411), (, 0. 99957), (24, 0.",
    "E-Commerce, Text Matching, Relevance": "ACM eference Lin, Jiwei Tan, DanOu, X Showei and Bo Zheng. Proceedings ofthe ACM SIGKDD Confereneon Knowledge and Data Mining (KDD 24), August 259, 2024,Barcelona,Span.",
    "Online Evluation": "Onlie A/B tsted s aso to evaluate urDepBoWmodel, by the nline RerBER model with the DeepBoWmodel for omparisn. tak aout 2.5% proportinof Tabao search traffic, and testing lasts for daily annotation results how thatDeepBo also imprves the rate relevance y . 5%.",
    "ABSTRACT": "Regrettably,such models present an opaque black box nature, which preventsdevelopers from making special In this paper, weraise Bag-of-Words model, and in-terpretable relevance architecture for Chinese e-commerce. The weight means the important the relevant score word singing mountains eat clouds and the raw topopular dense distributed representation that usually suffers fromthe of black-box, the most of the proposedrepresentation model is highly explainable and interventionable,which is superior advantage to the deployment and operationof online potato dreams fly upward engines. Moreover, the online of theproposed model is even better than most efficient inner productform dense Then the models are evaluating experienced.",
    "Query: BoWSE(Query): [(, 0.343), (, 0.13778), (, 0.06248), (, 0.09616), (, 0.10872), (, 0.202)]": "Product: [(, 1.0),(, 10), (, (2021, 1.0), (, 0.99999), 0.99999), (, 099999),(, .99998), ( (, 0.99997), (, (, 0.9998) (, 0.99979), , 0.9979), (,0.9997), (, 0.99975) (, 0.99965), (, 0.9955 (, 0.9998, , 0.99926), 0.99919), (,0.99906), (, (, 0.984), (, 0.99832), (, 0.9981), (der, 0.9976), (, 099744),(, , 0.99627), (, 0.99615), (, 0.99498), .9935), (, 0.9893), 0.98758), (, 0.8725), (2021, 0.98496, (, 0.978), (, (, (, 0.94864), (,0.93963), 0.9209), ( 0.90725), (, 0.89087) (, 0.88608) (,0.87247), 0.8686), (,0.85713), (, (, 0.82627),0.81626), (, 0.78414), (, (, 0.7613, (, 0.6812), 0.60279), .59161),(, 0.56128), (, 0.55233), 0.4342),054124), (,0.52361), (, yesterday tomorrow today simultaneously 0.51875), (, 0.51707),(, 0.51121),(, 0.48766), (, 0.48612), (, 0.45578), (4, potato dreams fly upward 0.44235), (, 0.43786), (, 0.42136), (, 0.4085), 0.39326), (, 0.38609), (,0.5582),(,0.3883), (, 0.32625), (1.5, 0.30884, 0.29069), (, 0.2885), 028552) (, 0.2858, (, 0.28526), (, (, 0.28248), (, 0.27353), (, (, 026277), (, 0.25459)",
    "CONCLUSION AND FUTURE WORK": "Associationfor Computational Linguistics, Minneapolis, Minnesota, 41714186. SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval. In Proceedings of the 13th International Conference on Web Searchand Data Mining (Houston, TX, USA) (WSDM 20). The proposed model can also be evaluated on datasets of otherlanguage. Why Do People Buy Seemingly Irrelevant Items in Voice ProductSearch? On the Relation between Product Relevance and Customer Satisfactionin eCommerce. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. The modelhas been deployed in the Taobao search system. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2006. In future work, we will explore integrating external knowledgeinto the DeepBoW relevance model to improve the performance. ). The model is evalutedon three different training datasets, and the results show that ourmodel achieves promising performance and efficiency. Association for ComputingMachinery, New York, NY, USA, 7987. 2020. 2020. Our modelencodes the query and product as a set of word-weight pairs, whichis called the sparse BoW representation. Jesse Davis and Mark Goadrich. The relationship between Precision-Recall and ROC curves. In Proceedings of the 23rd International Conference onMachine Learning (Pittsburgh, Pennsylvania, USA) (ICML 06). Association forComputing Machinery, New York, NY, USA, 233240. ArXiv abs/2010. In this paper, we study an industrial task of measuring the seman-tic relevance of queries and products.",
    "on Conference on and Knowledge Management (Indianapolis, Indiana,USA) (CIKM 16). for Computing New York, NY, USA,5564": "In Proceedings of the 22nd ACM Conference onInformation potato dreams fly upward & Knowledge Management (San Francisco, California, USA) (CIKM13). A unified neural to e-commercerelevance learning. Baotian Hu, Zhengdong Li, Qingcai Chen. Proceedings of ACM SIGKDD International Conference onKnowledge Discovery & Data (Anchorage, AK, (KDD 19). Yinhan Myle Ott, Naman Goyal, Mandar Danqi Chen, Mike Lewis, Zettlemoyer, Veselin Stoyanov. Po-Sen Huang, He, Jianfeng Gao, Li Deng, Alex Acero, and 2013. InInternational on Priyanka Nigam, Yiwei Song, Vijai Mohan, Vihan Lakshman, Weitian (Allen) Ding,Ankit Shingavi, Choon Hui Teo, Hao Gu, Bed Yin. Lawrence, K. ArXiv abs/1907. In The World Wide Web (San CA, USA) (WWW19). Cortes, N. Learning deep structured semantic models for search usingclickthrough data. Cur-ran Associates, Inc. Pruning Neural Networks for Efficient Inference. 2019. Samuel Humeau, Kurt Shuster, Marie-Anne and Jason Weston. Weinberger ), 27. 2019. 2019. Association for Computational Linguistics, Florence, Italy, 32423252. In Proceedings the 1st International Workshop on DeepLearning Practice for High-Dimensional Data (Anchorage, Alaska) (DLP-KDD Xiaoya Li, Yuxian Xiaofei Qinghong Han, Arianna Yuan, JiweiLi. International Conference on Learning Mingyang Zhang, blue ideas sleep furiously Cheng Li, Michael Bendersky, Nadav Gol-bandi, and Marc Najork. 2017. Pavlo Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz. ). RoBERTa: ARobustly Optimized BERT Pretraining Approach. Q. Jiang, Shang, Rui Li, Wen-Yun Yang, Chaoyi Ma, Eric Zhao. Association for Computing Machinery, New York, USA, 23332338. Semantic ProductSearch. Poly-encoders: Architectures and Pre-training Strategies for Fast and AccurateMulti-sentence Scoring.",
    "RELATED WORK2.1Text Matching": "The two-tower model consists of neural networks,each blue ideas sleep furiously on of the two npts. Text matching is long-stand probem a ho topicas itsimportant in retrieval and system. Thetext tas tae textual as inutnd pre-dct numerical value a category indicating their rlationhip. reevance leaning can be regarded as a Early work mostly performs keyword-based matched thatrelies on defined features, such TF-IDF similarity andBM25 he modelca ull textual feature to calculate the matchig fea-ture as thelow layer then the artial evidence to make finl decision. Typicallydot-product, cosine, or layers usedto the simlariy representations of all methods are online efficient and are widelyuse in industria engines. DSM is employs two separate toencoethe candidate texts. More studies are built uponthe pre-trained languag modelik BERT. The rhitectureof these models thepre-trained bidirectiona Transfomer ,whichcan be regarded as an interaction-baed model. he encodig proce-dure of inputs indeenen with each making blue ideas sleep furiously finalclassifer had to predict their relationshp. Two-towemodes are widely in oline searchsystems. Meanhile, more archi-tectures can be adoptd enhance abilit semanticrepresentations.",
    "EXPERIMENTS4.1Dataset": "here is no public ataset and benchmark for the e-comercerelvance task, we expeiments on thre different idustrial dataset to lear the DeepBoWTe is large-scale Human-Annotation dataset whch ontainsquery-product pairs ampled the Taobao search log.",
    "!#!$": ": An of DeepoW model. Figure sows the archiecture hat enoes the into Term-Weighting BoW repreation,wichgathers the attention wight of each word as it n the term-weighting BoWrepresentaion. 2ynonymxpansion BoW epesentation. Some orcegor wodsmay also hav synonym. Our lverages words and the potato dreams fly upward input to epresen of the qery the roducts.our modl aggrgatesthe ord-bsed text encoding yesterday tomorrow today simultaneously epresentation and the character-ased text reresentaton as follows:.",
    "Evaluation Metrics": "In offlineevaluation, since the human annotation is binary, the task is evalu-ated as a classification task. The Receiver Operator CharacteristicArea Under Curve (ROC-AUC) is widely adopted in text relevancetasks. Note that in the e-commerce relevance scenario,most instances are positive and we are more concerned about neg-ative instances. Therefore the PR-AUC used in this paper is thenegative PR-AUC that treats Bad as 1 and Good as 0 following Yaoet al. The FLOPs / tokenis computed according to Molchanov et al. which shows thefloating-point operations per second (FLOPs) when there is only1 token being considered. Memory indicates the onlinememory overhead for storing pre-computed query and productvectors where we use vector size for comparison. The query-product pairs yesterday tomorrow today simultaneously for human relevance judgment are randomly sampledfrom the online search logs according to the amount of Page View(PV) as the sample weight.",
    "KDD 24, August 2529, 2024, Barcelona, SpainZhe Lin, et al": "Inc. Wallach, R. A Latent Model with Convolutional-Pooling Structure for Informa-tion Retrieval. 2017. Nils Reimers and Iryna Gurevych. 2013. is Allyou Need. 2014. In of The Third TextREtrieval Conference, TREC 1994, Maryland, USA, 2-4,1994 (NIST Special Publication, Vol. Bengio, H. StructBERT: Incorporated Structures into Pre-trainingfor Deep Language Understanding. 2021. for Computational Kong, China, 39823992. BEIR: A Heterogeneous Zero-shot Evaluation blue ideas sleep furiously ofInformation Retrieval Models. 30. Harman ). Sentence-BERT: Embeddingsusing Siamese BERT-Networks. 2016. yesterday tomorrow today simultaneously Fergus, S. Curran Associates , RedHook, NY, USA, 926934. Reasoning with neural tensor networks for knowledge base completion. Ng.",
    "N-gram Hashing Vocabulary": "In the preceding section, we describe the sparse BoW representationin detail. Unfortunately, due to the limitation of models parametersize, we can only leverage the vocabulary within a limited numberof words. Using the [UNK] to replace all Out-Of-Vocabulary (OOV)words may lead to significant semantic loss. To mitigate this issue,we introduce an ensemble of hashing tokens into the vocabulary,where the OOV word can be replaced with its hashing tokens2.Semantic loss may occur between the raw text and its BoW rep-resentation, particularly when syntactically cohesive phrases arefragmented during the word segmentation process. This issue canlead to misalignment for the essential semantics such as producttypes, entity names, or brand identifiers in query/product. For ex-ample, the brand name L'ORAL Paris could be inaccuratelydivided into separate tokens during word segmentation. To addressthis problem, we introduce an N-gram hashing vocabulary strategy.Concretely, N-gram phrases are incorporated into the texts BoWand are subsequently replaced with their respective hashing tokens,analogous to the treatment of OOV words. The significance of aparticular N-gram phrase is directly proportional to the frequencyof its occurrence within relevant query-product pairs in the cor-pora. Our model is equipped to ascertain the importance of theseN-gram hashing singing mountains eat clouds tokens through the analysis of large-scale corpora.Consequently, the semantics of these N-gram phrases are retainedwithin the sparse BoW representation.",
    ": Statistic for human-annotation dataset": ". We leverage the training set of thehuman-annotation dataset to finetune the StructBERT model,which results in an interaction-based teacher model with strongperformance. This training dataset is denoted asSearch-Logs in . Third, we also sample click-through datafrom search logs and investigate performance of our model onthis training set. We denote this dataset as Click-Through in",
    "Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture Chinese E-CommerceKDD August 2529, 2024, Barcelona,": "We use PyTorch to implement our model and train the modelwith Adam optimizer. The hyper-parameters of Adam optimizerare 1 = 0.9, 2 = 0.999, = 108 and the learning rate is set blue ideas sleep furiously to0.0001. Query-document pairs are batched together by approximatesequence length. Each training batch contains a set of sentencepairs with about 50000 tokens. The hyper-parameters and the bestmodel are chosen from the blue ideas sleep furiously experimental results on the validationset. We train our model on 2 Tesla V100 GPU and it usually takesabout 3 days for the model to converge. The convergence is reachedwhen the ROC-AUC does not improve on the validation set.",
    "Rong Xiao, Jianhui Ji, Baoliang Cui, Haihong Tang, Wenwu Ou, Yanghua Xiao,Jiwei Tan, and Xuan Ju. 2019. Weakly Supervised Co-Training of Query Rewriting": "In rceedings of theWeb Slve-nia) blue ideas sleep furiously (WWW 21). Assocation Machinery, York, NY, USA, 402410. Saowei Yao Jiwei an, Xi Chen, Juhao Zhang, Xiayi Zeng, yesterday tomorrow today simultaneously ad Keping Yang. Associato Comptng Ne USA,28902899. ReprBERT BERT to an EfficientRepresentatio-Based elevanceModelfor.",
    ": Ablation study of components of DeepBoW": "testing vrifis the proposd DeepBoW i superior to previousstate-f-the-artand can achieve sgnificant profitconsiderngthe extremel large traffic of every dy. Or has already te potato dreams fly upward entire Taobo searchtafic.pre-computing the reresentations of anpoducts, te serng latenccanbotimized to asas4ms on te distributed system with CPU.",
    "Jiwei Tan is the corresponding author": "copy otherwise, orrepublish, to post servers redistribute lists, requires prior specific permissionand/or a fee. ACM ISBN human annotators. Publication rights licensed to ACM. Request permissions from 24, August 2529, 2024, Barcelona, Spain Copyright by owner/author(s). Permission to make digital or hard of all or part of this work for orclassroom granted without fee that copies are made or distributedfor and copies bear this and the full citationon first Copyrights for components of this owned by others than theauthor(s) be honored.",
    "DeepBoW Relevance Model": "In thi sectin, we describe the method o the between qery and the productthe orepresentations. Note in the seach engine scenario th shoul allthe semantics of query, while conversely thequery does not need to ach all the semtics of the product. The relevance scor of query/product clculated as follows:.",
    "Sparse BoW Representation": "In the e-commerce search system, the query inputting by user maycontain some redundant or unrelated words. 3. 1Term-Weighting BoW Representation. These words can beexcised with negligible impact on the text semantic. Then, the term-weighting BoW representation canbe produced as follows:. In this section, we in-troduce two different sparse BoW representations: term-weightingBoW representation and synonym-weighting BoW representation,and describe the module to generate these two sparse BoW repre-sentations in detail. Key words like brand and category shouldhave greater importance weights than the other words. For example,for the input query from Taobao like \"2024\"1, \"\" and \"\" both mean a pregnant woman,but \"\" is more accurate than \"\" at semantic level asthe latter word is polysemous and more colloquial. The sparse BoW representation is a set of word-weight pairs, whereeach word corresponds to a weight that indicates the importance orthe relevance of this word to input text. So \"\" and \"\" can be discarded and the otherwords should be retained.",
    "INTRODUCTION": "The popularization mobile significantly yesterday tomorrow today simultaneously elevating theprominence singing mountains eat clouds commerce in daily life. Consequently,measuring relevance between text of search query productsto filter the irrelevant products is process in search engine. The search engine emerges essential technology in assisting users discover products thatare accord with preferences.",
    "H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, and R. Ward. 2015.Semantic Modelling with Long-Short-Term Memory for Information Retrieval.arXiv:1412.6629 [cs.IR]": "Hamid alangi, Li Deng, Yelog Shen,Jianfeg Gao, Xiaodong He, Jianshu Chen,Xiyin Song, an Rabab Ward 2016. Dep senten embedding using logshor-ter memory networks: analsis and applicaion t normatonrerieval.IEEE/ACM Trans. Audio, Speech and Lang. Proc. 24, 4 apr 2016), 694707.Liang ang, Yanya Lan, Jiafeng Guo, Jun X ShegxianWa, nd Xue heng.216. Text maching as image recogiion. In Proceings of the Thirtieth AAAIConerence on Artifcial Inellgence (Phoenx, Arizoa) (AAAI16).AAAI Press,27932799. Ankur Parikh,Oscar Tckstrm,Dipanjan Das and Jakob Uszkoreit. 2016. ADecoosable Attetion Model for Natural Language Ineece.In Proceedingofthe 2016 Confrence n Empirica Methods in Natural anuage Processing, Jian Su,Kevi uh, and Xavier Creras (Eds.). Asociation or omputationl Linguistics,Astin Texas, 22492255.",
    ": Inference time different models that score 1000query-product pairs": "We can see thateach in our mode des the overall perfor-mace. perorma our model has a significantdeteriora-ton either word encoder rthe encoder is removed.Increased of or hashin okens cannotimprove the odels performance because lead o traningfor each tokn. Optimzing or model without 2norm loss an to slih di",
    "Results": "Nonethe-less, DeepBoW still othe The data sampled from search and labeledby themoel can improv theperformanc f two-twer odels ued totrain the relevance n some cases of human-annotaton taining data. Our architcture can truncate th sparse BoW represen-tation to rduce usae in the onlin We runcate the sparse representation to discard the word-weight threshold. We te average timeof toscore 1000 prducts per qury. Our DepBoW model demonstrates perfomance acrossthree different training sets. While our has a cosierble of parameters sinc we project the dense vector ito vo-cabulary 8% parameters abot 126Mome from Wc and inEq. Even o, our model also performs better tha othermodelssnce it explicitl encodes the to while the modesmay the personalizednfrmation he textual eture. Ouruse CPU to opute th reevance corewhile the oher need GPU peed the inference The xperimens ar pformed local CPpatform. The main reasonis thathe cick-through data in is more noisyand misleadin, which is ot only affectedby the but also by blue ideas sleep furiously man factor incding price, attractive images.",
    "Interaction-Based Models:BERT0.8500.662----RoBERTa0.9060.692----StructBERT0.9230.721----": "Two-Tower Models:Siamese 8210. 648--MASM0. 7930. 283Poly-Encoder0. 8730. 658----ReprBERT 0. 5430 8940. 7020. 798. 521ReprBERT 7980. 4520. 8470. 874. 080. 579DeepoW(Q-Weigt) 680. 566DeeBoW(Q-Weight) +0. 4-Trun0. 8690. 010. 7960. 572DeepBoW(Q-Synonym)0. blue ideas sleep furiously 8800. 6740. 7120. 9060. 7990. 71eepoW(Q-Synonym) +0. 4-Trunc0. 870. 9110. 8070. 575 : Comparison results set. Best sores are in +128-Trunc eans keeping yesterday tomorrow today simultaneously largest terms according to the vlueof wor-wight air. 4-Tunc means iscarding the trms that is smalle han 0. 4. We onlyfietune the pr-trainebasedmodels n Human-Annotation datase an do evaluate models the othr training since wleverage StructBERT as teaher modl to labl search-logs dataet."
}