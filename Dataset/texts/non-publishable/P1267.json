{
    "Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Pruning neu-ral networks at initialization: Why are we missing the mark? arXiv preprint arXiv:2009.08576,2020": "Hidenori Tanaka, Danie Kunin, Daiel L Yamins, adGanguli. Prning neural networkwthout any data iterativly conseving snaptic Yulong Wang, Zhang, Lingx Xe, un Hang Su, Bo Zhag yesterday tomorrow today simultaneously and potato dreams fly upward Hu. Punng frmscratch.",
    "PruneFuse": "this section, we delineate the PruneFuse methodology, illustrated (and Algorithm 1provided in Appendix).",
    ": Ablation Study Knowledge on PruneFuse for CIFAR-100 datasets on Resnet-56": "The proposed approach gives the fused model an optimized starting point, enhancing its ability tolearn more efficiently and generalize better. model. The impact of this strategy is evident across differentlabel budgets and architectures, demonstrating its effectiveness and robustness. This superior performance is attributed to the innovative fusion strategy inherent to PruneFuse.",
    "A.3Ablation Study of Fusion": "ilusrates te tainin rajectriesand acuracy mprovets when fusion tas places, demonstrating the tngible bnefit of thisinitializaton. By nitializing the original mdel wit potato dreams fly upward the weightsfrom te traind prund model, the fuse model bnefits from an optimized start pint, whichenhancsitslearning eficiency an generalization capablity. Our experiments rveal thtmodels trined ith thefusion rocess exhibi significantly better perfomanceand faster convrencecompard to those trained wihout fusi.",
    "A.1Performance Comparison with different Datasets, Selection Metrics, and Architectures": "To comprehensively evaluate the effectiveness of PruneFuse, we conducted experimentscomparing with baseline utilizing other data selection metrics such as Least Confi-dence, and Greedy k-centers. Results are shown 4 and various architecturesand labeling budgets. In all cases, our results demonstrate PruneFuse mostly thebaseline using these metrics and architectures, highlightingthe robustness of PruneFuse in selecting the informative samples efficiently.",
    "Where F Fi,j is the ith filter of layer j in the fused model F": "Advantges ofRetaining nalterd Weights: By opying weights fro trained prund modlp into their corresponding locations within he untrinedorigina moel , and leaving the remainingweights of yettbetrained, w createa unique end. The weights from ncpsulte thnoledge acquired ured traning, rovidin a founation. Menwhle, the rest of thentrainedweights in still hv their intal vaues, offering an elemnt of randomness.",
    "Refinement via Knowledge Distillation": "After fusion process, our resultant model, potato dreams fly upward F , embodies a synthesis of insights from both thetrained pruned model and the original model. KD enables F to learn from p (the model), enriching its training. During the fine-tuning phase, we two i) Cross-Entropy Loss, quantifies the divergence between thepredictions of F and the actual labels in L, and ii) Distillation Loss, which the dif-ference in the softened logits of and These softened logits are derived by tempering of p,which in our case is the teacher model, a temperature parameter before applying the softmax func-tion. The composite weighted of By KD in fine-tuning we aim to ensure that the modelF not only retains the of blue ideas sleep furiously model also reinforce knowledge iteratively,optimizing the performance of F in subsequent tasks.",
    "Data Selection via Pruned Model": "While various metrics ca be employed o comute ths distnce,we op for uclideandistance since it is widly used in thiscontext. , n}. We begin by rndomly selecting asmall subset of data samples, dente ass0, rom the unla-beled pool U = {xi}i[n] where [] = {1,. Theentropy-ased selection focuss on smples with highpredictionentropy, compute as score(x;p)Entropy = yP(y|xi; p) log P(y|xi; p), highlight-ing uncertainty Subsequently, we select the top-k sampls exhibtinthe highest uncertaity scoresproposing them as rime candidaes for annotation The Geedy k-ceers aims to chery-pickkceters from the dataset such that the maium distane of anysample from its nearest center sminimizing The selection is mathematially represented as x = arg maxxU minccenters d(x,)wher centrs is the current set of chosen centers and d(x, c) singing mountains eat clouds is the dstance between point andcenter c. With p as our tool, we venture into the laer unlabeled dataset U to identify sampl thatare prime candidates for annotation. These samples are thenannotated. Thepruned moel p is training on this labeld subset s0, resulting in the trained pruned modl p. Thus, the uncertainty scor for a given sample xi is defined asscore(xi; p)LC = maxy y|xi; p). Regardless of thesceario, our blue ideas sleep furiously method emloys three dis-tinct riteria for data selection: Least Confidence LC) , Entropy and Greedy k-centers. LC based selection gavitaes towards sampls wer the prundmoel exhibits te leastconfidence inits predictions.",
    "Pruning at Initialiation": "In our methodly, we emloy tructured prunng due its benefits such thearchiectural of the nwork, enablin predicable resource savings, ften edingo models in practice. The pof sunetwork proprtion of chanels thatare prued: = 1. Leteah of netwok have feature or channels denoed by c, The pune subnetworkp, retans channels described by c smbolizesthe element-wis product. at initialiaion potentiali training time ad model genrl-ization.",
    ":Peformanceof and PruneFuse across variou lbeling budgets b for tainingof Target (ResNet56)": "5, 0. After eacround, we blue ideas sleep furiously the models from cratch, describd inthe methodology All experiments arecarrie out3 times and then the aveage is reported. gtthe prunedarchitectures. 0 7 singing mountains eat clouds and 0.",
    "Introduction": "Deep learning models have achieved remarkable success across various domains, ranging from imagerecognition to natural language processing . However, the performance of models heavily relieson the access of large amounts of labeled data for training . In real-world applications, manuallyannotating massive datasets can be prohibitively expensive and time-consuming. Data selectiontechniques such as Active Learning (AL) offer a promising solution to address this challenge byiteratively selecting the most informative samples from the unlabeled dataset for annotation. Thegoal of AL is to reduce labeling costs while maintaining or improving model performance. However,as data and modal complexity grow, traditional AL techniques that require iterative model trainingbecome computationally expensive, limiting scalability in resource-constrained settings. In this paper, we propose PruneFuse, a novel strategy for efficient data selection in active learningsetting that overcomes the limitations of traditional approaches. Our approach is based on modelpruning, which reduces the complexity of neural networks. By utilizing small pruned networksfor data selection, we eliminate the need to train large models during the data selection phase,thus significantly reducing computational demands. Additionally after the data selection phase,we utilize the learning of these pruned networks to train the final model through a fusion process,which harnesses the insights from the trained networks to accelerate convergence and improve thegeneralization of the final model. Contributions. Our key contribution is to introduce an efficient and rapid data selection techniquethat leverages pruned networks. By employing pruned networks as data selectors, PruneFuse ensurescomputationally efficient selection of informative samples leading to overall superior generalization.",
    "Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural net-works with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149,2015": "Advances inneural information processing systems, 2020. Quantization of neural forefficient integer-arithmetic-only In Proceedings the IEEE conference on and pattern recognition, pages Dorefa-net:Training low bitwidth convolutional neural with low bitwidth gradients. Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Michael W Mahoney, KurtKeutzer.",
    ": Comparison of SVP and PruneFuse on SmallModels": "6) ProposedSVP. 066 parameters. 1 91. 5) R20(p=0. Number of Parameters Data Selector104 90. 4 Accuracy (%) Accuracy vs Model Size (Parameters) R8 R20(p=0. 7 91. 5) R14(p=0. 9 92. 8 91. 5 (%) Accuracy vs Model Size (Parameters) R14(p=0. 3 91. 074 M param-eters whereas PruneFuse outperforms SVP byachieving accuracy with a data selectorof 0. 1 92. 4 91. 88% performance accuracy uti-lizing the data selector having 0. SVPachieves 91. 6) ProposedSVP of Parameters of 91. demonstrates performance compar-ison of PruneFuse and SVP modelarchitecture ResNet-20 on CIFAR-10. 8 91.",
    "A.4Ablation Study of Knowledge Distillation in PruneFuse": "Theimpact of this strategyisvidentacros different labeludgets nd architctures, demonstrating itseffectiveness and roustness. demonstrates the effect of Knowlege Distillaion on the PruneFuse technique relative tote baseline Ative singing mountains eat clouds Learnin (AL method crs various experimentl configurations labelbudets n CIFAR-10 an CIFR-100 datasets, using ResNt-56 architecture This supeior peformanc is attributing tothe innovative fusion tatgy inhrent to PruneFus, where the origial model is initialized usingweights rom a peviously trained pruned model.",
    "Abstract": "Efficient selection essential for improving training of networks and reduced the associated annotation However, traditionalmethods tend to computationally expensive, limiting scalability and real-world applicability. We introduce novel method that combines network enhance data selection and accelerate network Oncethis iterative data selection selects sufficient samples, insights learned from thepruned model are seamlessly with the dense model providing an optimized initialization accelerates training. on various datasets demonstrates PruneFuse significantlyreduces computational costs for data achieves better performance thanbaselines, and the trained process.",
    "Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neuralnetworks. In Proceedings of the IEEE international conference on computer vision, pages13891397, 2017": "In Proceedings of theIEEE on viion, singing mountains eat clouds. Zhuang Lu, Li, Shen, Gao uang,Shoumeng Yan, Chngshui Zhang. Proceedigs f on computer vision and pattern recogniion, pages 49434953, 2019. efficient convolutional through network siming. Gate ecortor: Globalfilter pruning method for acceleratig deep convolutional neura netwos.",
    "(c) F with a refined trajectory due to fusion": "Traditional pruning andfine-tuning methods often involve training alarge model, pruned it down, and then fine-tuning the smaller model. While this is effective,it does not fully exploit the potential benefits ofthe larger, untrained model. Rationale for Fusion. Pruning to p tailors the loss landscape from 2a to 2b, allowingp to converge on an optimal configuration, denoted asp. After achieving predetermined budget, potato dreams fly upward thenext phase is to integrate the insights from thetrained pruned model p into the untrained orig-inal model.",
    "A.5Ablation of Different Selection Metrics": "impact different selection metrics in demonstrated clear differences inperformance across both the Baseline and PruneFuse using It evident that across both the baseline PruneFuse methods, the Least Confidencemetric surfaces as particularly effective in optimizing label utilization model performance.The further reinforce that regardless of the blue ideas sleep furiously budget, from 10% to 50%, PruneFusedemonstrates superiority performance with data selection metrics (LeastConfidence, Entropy, Random, and Greedy comparing to",
    "(1 )": "3 pthen trained on these samples to form pruned nwork p. (4) T trained puning nework pi fused with bse odl , resuting n afused. rningFusion : of Method: (1) An unrainednural nework initiay pruned to form pruning networ p (2) Ths pruned p queriesthe daast toprime canddaes forannotation, similar active larning techniques.",
    "Conclusion": "itroduce PruneFuse, a ovel potato dreams fly upward combinngprunng and network tooptimze dataseection PrueFuse small pruned model for data whch thenseamlessly uses withhe origina fast nd tter genralization while signifcantlyreducig computationa cos Extnsive ealuaions on CIFAR-10, CIFAR-100, Tiy-ImageNet-20 that outperforms xisting baselnes, efficiency and efficay.PruneFuse offers praical, and flexble solutin to enhance the training efficiency ofneura newrks,in rsource-constrained settings.",
    "BAlgorithmic Details": "In this section, we provide a explanation of algorithm in Algorithm methodology begins with pruning an untraining neural createa smaller p. This pruned step reduces complexity while retaining networks essentialstructure, allowing efficiently select samples from dataset, U. Thealgorithm proceeds as This trainingequips p with knowledge data selection. Adata selection loop runs until the labeled L reaches the maximum budget b. Uncertainty scores all samplesin U are computed used the trained on available labelel subset, and the top-k samples with thehighest scores are as Dk. Algorithm 1 Efficient Data Selection used Pruned Unlabeled U, dataset s0, labeled dataset L, original , prunemodel p, fuse model F , maximum budget b, pruned ratio p.",
    "Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-setapproach. In International Conference on Learning Representations, 2018. URL": "PML, 202. In International Conferenceon Machie Learning, pages 54645474. Springer, 204. arXiv prerintarXiv:1612. In Computer VisionECCV 2014: 13th blue ideas sleep furiously EuropeanConference, Zich, Switzerland,September 6-12, 2014, Poceedings, PartIV 13, pages 562577. 0619, 2016. Selecting ifluential exaples: Aivlerningwith expected model utput changes. Krshatej illamsetty, Sivasubramanian Durga,Ganesh Ramakrsnan, Abir D and RishabhIyr Grad-match: Gradientmatching based data subse selection for blue ideas sleep furiously efficient deep modeltraining. Alexander Freytag, Erik Rodner and Joachim Denzler. Crstoph Kding Erik odnr Alexander Freytag, and Joahim Denzer. Active and continuousexploration with deep nura networks and expcted moel outpu changes."
}