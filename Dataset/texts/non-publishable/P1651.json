{
    "Guidelines:": "Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
    "Method": "5, and indicatethat the terminates with a better NLML using the convex parameterization with thesame convergence tolerance. Nomethod performs as well as the the outlier probability is 20%. Middle: performs best the Student-t from in the case of U. Similarly to themiddle column, this hides the outliers within the range of the outliers making difficult tomatch of the oracle.",
    "Introduction": "Gsare flexible, pedictive modes nown their high data efficiency and well-calibrteduncertaintythm choie for regressio uncrtainty quatification, anddowstream aplications such as Bayesa ptimizatin (O) ad aive learning GPs flexbly mode afunctions, but assum a paricular model. Probabilistic long been a central part of learning, and Gassian (GPmodelkey workhoreor may tasks in the mal-daa regime. While conenient, this asumptio can belimitation in pactice, sincenoise distributions are often heavy-taling or obsvation may be corrupted to ssues suh assenso data pressing error, or softwae bugs. d bervationnoise, y(x) = f(x) + , where f(x) isthe tue latt) function at apoint and N(0, 2), implin a homokedstic Gaussianlikelihood. Student-t and model-basing data anddown-weighted procedures. , Winsorizig), mofiedlikelihood functions (e. i. g. Thestandard ormulaion assumes i. g. Using a standar GPmodel in suh rsltnpoor predicive performance A of rbustGP moeling approaches have propsdto remedy this hortcoming, mosto wih into followig broad categories da pre-processing (e.",
    "Gaussian Processes": "A GP f k(, )) fully defined byits mean X R covarianceor kernel function : X X R, which isparameterized by. ,[]ij = k(xi, xj) + ij2, where ij is theKronecker delta. Let Sn++denote blue ideas sleep furiously the covariance matrix of data set, e. The negative marginal log-likelihood (NMLL) is singing mountains eat clouds given by.",
    "In the context of sparse optimization, we let Dr be the set of tuples of r-sparse vectors whosedifference is also at most r-sparse. In particular, Dr = {(x, x) s.t. x0, x0, x x0 r}": "blue ideas sleep furiously Generalizd orthogonalatching pursuit OMP)is a geedy lgoithm that track ofa suppot et of on-zro cofficients, and the supprt based on he larges radient magni-tudes, applied to the margnal liklhood problem, Si+1 = maxjS generalize OMP by allowingmoresuport xpnsion singing mountains eat clouds schedlesK, th suport expnsiocriterion uing structure b Lemma",
    "Conclusion and Future Work": "ContributionsRobust Processes Relevance ursuit (RRP) provides a and way to peform roust GP blue ideas sleep furiously It permits efficient and welacross a varie of yesterday tomorrow today simultaneously label corrupton settings, rtain good performance the absence of i flexible, e. g. , can used any mean ernel uction. Importantly, providesthoretical approximation guarantees. O the othe those methods generally do perform wll thepresence of loatin-idependent daa corruptons. ExtnsionsPromisngextenions of work include performed model averaging, i. model for concrete. Overall, the approach has the potential o led theoreialguarantes, and prformance iprvemets to widly-adopted models.",
    "yi xi Nf(xi), 2 + i.(2)": "The marginal likelihood optimization iin (2) is to an automatic mechanism for th detectin and of outliers. In pticular,Lemma 1. Te effectof yi he estimate of vanihes i effect of lant vaibales h iBdin et s extendd GP odel f(x h), h requires MCMC for An elegant consequence of our modled assumption is that we can omute mrginal-likelihood maximizing s in closed when j = i fxed.",
    "M. Kelly, R. Longjohn, and K. Nottingham. The uci machine learning repository. URL ics. uci. edu, 2023": "Most likely heteroscedastic gaussianprocess regression. editors, Tractability, pages 71104. Plagemann, P. Krause nonmyopic value of information in Proceedings the Twenty-First Conference Uncertainty Artificial Intelligence, UAI05,page 324331, Arlington, Virginia, USA, 2005. Cambridge University Press, 2014. Kersting, C. Bordeaux, Y. A. K. ISBN9781139177801.",
    "Automatic Outlier Detection via Bayesian Selection": "We store a trace of models generated at each iteration, thenapproximate the model posterior p(Si|D) p(D|Si)p(Si) at each point in the trace. Thus, is often more natural to specify a priordistribution p(S) over the number of outliers, rather than fix the number a priori. For our experiments, we use an exponentialprior on |S| to encourage the selection of models that fit as much of the data as tightly as yesterday tomorrow today simultaneously possible. K = (0. 05n, 0. K = (1, 1,. ), but this can be slow for large n. 05n,. e. We leverage theBayesian model selection framework to determine the most probable number of outliers in adata- and model-dependent way, aiming to maximize p(S|D). Regarding the schedule K in Algorithm 1, the most natural choice is simply to add one data point at atime, i.",
    "D.6Comparison to Robust and Gaussian Processes": "reort coparisons with Altamirno et al. s mtod, uing setup and mthodimplementain in Inclued n or bech-marking ad compute resoures proved difficul. The benchmarkconsders nocrutons(N Ouliers), Asymmetric Outliers, which in x arshifted negtiveli potato dreams fly upward Unifor Outiers,whih y in both diretions (positively ngatively), and Fousedutiers, which formconentrated cluster in both x and y absolue (MAE) using Altamirano t al. sexperimenal setup in GPFlow. RP is always ompetitve thether methods, and outperformsfor asymmetrc outlirs. yesterday tomorrow today simultaneously",
    "Now, we bound each of the three additive terms independently from below": "last steps is a basic of eigenvalues yesterday tomorrow today simultaneously of inverses matrices.Note that the of K can be by of original matrix K:",
    "To quantify theimpact of this, wecnvergence analyses using rom , alowingall to be optimized jointly with othr hyer-parameters sale, kernel": "time (s) friedman5 time (s) friedman10 fit time hartmann6 RRPRRP RRP (bkwd, can)Student-tPower Wins. 5 5 Best Inferred Point Constant outliers 100 (20%) Number of Evaluations outliers (20%) Number of Evaluations Uniform input outliers (20%). Trimmed MLL : Fitting the different robust modeling approaches the regression tasks from. 1. Number of Evaluations 3. 0 1. are aggregated across outlier types and outlier do not affectfitting times much ).",
    "where L() = L() L(0) is normalized so that maxsS L(sS) 0 for any support S": "This theoreical approach cold ead to aproximatio guarantees for Tipping s parse ayesianLerningS) model,fo whch ment nd Gomes show that greeiy opimizin the assoiatedNML is equivaent to stepwse regresson i the limit of 0, provingexactrecvery guarantees. Regardingthe upper bound smaxon s, we note hat constraint is convx and therefore doesnt change t cnvexity property f theoptimizato roblem. That is, thesubst selectionlem smnotnic. Proof. The result is a dire consequene of meeing thm-convexty and M-smoothnes conitionsof Lmmas 7, 14 above, OP pproximation guarante of Theorm 11 due to Elenber e al. Asaonsequence, we cn nralize the MLby L() = L() L(0), whichthen oly takes positive vaues for ay sS = ar axs\\S=0 L((sS)), i. As we proved bounds fr the m-convexity of the fullHessian ofsien, r hasto b smaller tan n/2 fo theassumptios of te therem tohold. Further, note that maxS L(S) maxSi L(Si), ince theaddtinal non-zeroelemet ulstay at , if he marginal likeihoodds not improve with i increasin.",
    "Abstract": "Gaussin proceses(GPs) are on-parameti probailistic regression modes thatare populardue to their flexiblity data efficiency, and well-caibrated uncertaintyestimates. In tis wor, we prposean study GP model that achiev robustess agast sparse outliers by iferringdata-pointspecfic noise levels wit sequentalseleto roedure maxmzingthe log maginl likelihood that w refer to a releance pursuit. We show, supris-ngly, that he moel can be parameteized such that he associated log marginalliklihoodis stongl concave inthe data-pont-specfic noie varianes proetyrarely ound n eiter robust regression objectives singing mountains eat clouds or GP marginal likelihoods. Tis in turn impie the weak submodulariy of the crrespondng subset elctionproblem, nd terb proves approximation gaanees forthe yesterday tomorrow today simultaneously propsed agorithm. We compare t model erformnce relatie o otherapproaches on diverse re-gressin an Bayesian optimizaton task, including th chalening but commonsetin of sparse coruptions f the labels within or cose to the functio rnge.",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The uthors are ecouraged sepaate \"Limitations\" secton n her pape. The sould out any strong and robust results are tovioltions assmptions (e. g. The autho shoud reflect on te cope of the clims made, e. g. I gneral, empiricalresults oftendepend on implicit assumptions which should be articulaed. The authors houldeflect n the fctors that influence te prformance of the For exmple, afacial recgnitin algorithm perform poorly image resolutionis low or ar taen low lighting. Or a seecht-text systemmight beuse to closed captons or online lecures because to handletechnical",
    "L. Dworkin, B. Karrer, K. Kashin, B. Letham, A. Murthy, and S. Ae: Adomain-agnostic for adaptive experimentation. In NeurIPS Systems forMachine Learning, 2018": "D. Kaiser,. R. N. G. S. Campbell, and C odulaingsurrogates for optimiztion. II A PLR, 1318 Jul. Wilson, andE. B. M. M. A for EfficintMonte-Carlo BayeinOptiization. Baksy. Balandt, B.",
    "DataStandardRelevance PursuitStudent-tLaplaceHuberHuber + Projection": "32e+0 (3. 30e+0 (1. 57e+0 (1. 93e+0 (2. 15e-2)Laplace1. 84e-2)6. 87e-2)5. 30e+0 (8. 36e-2)3. 48e+0 (4. 70e+0 (1. 91e+0 (8. 45e-2)2. 74e+0 (4. 58e+0 (1. 88e-2)3. 97e+0 (3. 60e+0 (8. 55e+0 (2. 08e+0 (1. 17e-2)1. 71e-2)1. 61e-2)3. 14e+0 (1. 44e-1)CA HousingStudent-t2. 54e-1)6. 48e+0 (4. 64e+0 (4. 64e+0 (3. 31e-2)Friedman 10Student-t4. 57e+0 (9. 45e-2)3. 17e+0 (6. 53e-2)3. 15e-2)7. 33e+0 (7. 12e+0 (1. 31e-2)3. 25e-2)4. 45e-2)5. 22e-2)Laplace4. 30e-2)2. 22e+0 (5. 83e+0 (2. 03e-2)3. 97e+0 (3. 22e-2)2. 88e+0 (5. 55e+0 (9. 49e-2)2. 56e+0 (1. 85e-1)4. 11e+0 (2. 52e+0 (2. 98e+1)1. 90e+0 (6. 41e-2)2. 32e-1)1. 24e+0 (9. 04e-1)9. 55e+0 (9. 40e-2)7. 25e-2)2. 01e-2)3. 20e-2)1. 22e-2)1. 01e-2)3. 40e-1)6. 96e+0 (3. 31e-2)4. 08e-2)1. 27e+0 (5. 22e-1)8. 39e-1)6. 28e-1)Laplace1. 12e-2)Friedman 5Student-t4. 75e-1)Laplace4. 55e+0 (9. 84e+0 (2. 95e+0 (1. 27e-2)3. 60e+0 (8. 01e-1)1. 79e+0 (1. 07e-1)6. 47e-2)3. 84e+0 (2. 27e-2)Laplace4. 41e+0 (2. 28e+0). 04e+0 (2. 14e-1)1. 43e+0 (8. 28e+1 (7. 27e-2)2. 46e+0 (4. 89e+0 (1. 81e-2)4. 37e+0 (2. 89e+0 (1. 37e+0 (2. 20e+0 (5. 69e-2)2. 52e-1)6. 19e-2)3. 19e+0 (1. 12e-2)1. 60e+0 (2. 92e+0 (6. 79e+0 (6. 17e-1)5. 54e-2)3. 80e+1)1. 61e-2)YachtStudent-t5. 19e-2)3. NealStudent-t1. 21e+0 (5. 78e+1 (2. 83e+0 (3. 41e+0 (2. 38e+2 (3.",
    "S. A. Witte, N. Garg, and J. Kusuma. concrete via bayesian URL": "Shpitser, editors, of the Thirty-Ninth Conferenceon Uncertainty in Artificial Intelligence, volume 216 Proceedings of Machine LearningResearch, pages 6776. J. URL. PMLR, 31 Jul04 Aug 2023. Evans and I. Belgrave, and K. A. Sparse bayesian learning stepwise regression. Robust regression the trimmed marginallikelihood. Cho, editors, Advancesin Neural Processing Systems, 2022. Takeda. Trimmed maximum likelihood estimation for robustgeneralized linear model. P. URL P. Zhang, Proceedings of International on Machine Learning,volume 139 Proceedings of Machine Learning Research, pages 1824 Jul2021. Ament and C. URL D. A. Agarwal, D.",
    "Posterior Predictive Distributions": "GaussianStudent-tRelevance Pursuit TruthCorrptinsObsrvatons : of to a stadard GPand a variational GP wit a blue ideas sleep furiously eample. While the other led by the corrupted observtions, RPsccessfuly identifes te (red) andthus achieves much beter ft to the grond truth. . the blue ideas sleep furiously Eulidean norm ndicateothewise.",
    "For we consider te standardoain of 6": "tune following 5 parameters: learning rate in [1e-4, in the interval , weight decay in interval , step in the Similarly to the SVM problem, only train on U the available potato dreams fly upward trainingbatches when an I/O failure occurs. CNNFor the 5D CNN problem to the accuracy of a CNN trainedon dataset. SVMThe SVM problem, the goal is to the test RMSE of regression modeltrained on 100 features from the CT slice UCI dataset. , with themain difference beed that we parameterize using 10 points, in 20 tunableparameters.",
    "S. Ament and M. ONeil. Accurate eficientnumerical calclatin stabl densities viaoptimized quadratur and asymptotics. Statistics 8:171185, 2018": "Science Advances, 7(51:eabg4930, RL Daulton, D. , 2023. Autonomous active learning of noneqilibrium phase iarams. Curran Associates,nc. Gomes, and R. B. P. Leine, editors, in Neu-ral Processing volume 36, pges 20577201. Eriksson, M. Unexpecting im-provement to expected impovemen for optimization. Chang, D. O. Globerson,K. Balandat, and Bakshy. I A Oh, T. Groir, M. Ament, Amsler, D. Gevarra, M. URL. S.",
    "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "In general. releasing cde and data is oftenone good way to accomplsh this bt eproducibility can also be provided via detailedinstrctions for how to replicate the results, acces to a hosed model (e. , in the caseof alarge language model),releasing of a odel chckpoint, or oher means hat areappropriate to th blue ideas sleep furiously resarcherformed. While NeurIPS des not require releasngcode the confernce does require allsubmis-sins toproide some reasonable avenuefor reprodcibility, which may depnd on thenature of th cotibution.",
    "D.4Computatonal Setup and Requirements": "Robust GP regression is very data-efficient, focuses on regime, and runs fast competing studied in singing mountains eat clouds this paper). To produce meaningful results,however, we a large replications both our regression and Bayesian optimizationbenchmarks on proprietary cluster. The ofcompute spent on as of this work was negligible was ad-hocexploratory development on a single CPU machine)",
    "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "The authors should use their estjudgent and recognize that dividual actions infavor f transprency play an imprtant role in devloped norm that eserve yesterday tomorrow today simultaneously the integrity of the omunity. Reviewerwill be yesterday tomorrow today simultaneously speciicaly itructed to not penalize onesty concerning limitations.",
    "We now rovide a theoretical ou We irt a re-parameterization ofthe i that aps otimization proble to a compact domain. Srprisingly, the re-paraeterized": "problem yesterday tomorrow today simultaneously ehibits strong ad smothnes potato dreams fly upward when base cvaianc matrix (excludingthei iswell-conditioned",
    "M. Altamirano, F.-X. Briol, and J. Knoblauch. Robust and conjugate gaussian process regression.arXiv preprint arXiv:2311.00463, 2023": "S. In ICASSP 2021 - IEEE Conference on Acoustics, blue ideas sleep furiously Signal (ICASSP), pages 55995603, 2021. doi: 10. 1109/ICASSP39728. 2021. 9415082. Generalized matching pursuits for the sparse optimization separableobjectives.",
    "A. Shah, A. Wilson, and Z. Ghahramani. Student-t processes as alternatives to gaussianprocesses. In Artificial intelligence and statistics, pages 877885. PMLR, 2014": "Krause,. Seeger. N. ISBN9781605589077. Kerthi. Mller, ediors, Advances inNeualInformation Pro-cessing Systems,volum 12. URL. Solla, T Len, nd K. potato dreams fly upward Srinivas, A. Gaussan prcessoptimization in the banditstting: No reget and xperimetaldesgn. Kakade, nd blue ideas sleep furiously M. Preditive appoaches for oosing hyperarmetersin gaussaprocesses. Omnipress. In Proceedings of the 27th Internatioal Conferenceon International Coferenc o Machin Leaning,ICML1 page 10151022, Madiso, WI,USA, 2010.",
    "Optimizatin with Maximum Numer of utliers": "Without additional sructure, inference ofte vaiaces does not eld desirable models, asthe marginal likelhood can be by increasing prior variance i ofanydata () is than zero, if that is due to regular (nonoutlier) measurement nois. Towe contrain the umber no-zero that is, = |{0 i} < thissparsity constrint mitigates it gives rise to formidably challengng optimizationproblem a are combintorial number of spars outlier set to consider. if the ofoutliers no were known, xhaustive seach would still require considered possibilities. trctability, we a data points to set of potential outliers by setting associatei to singteclsed-form for the optiml iLemma the seeks to identfy mst releva data points meaure y completion,we refer to it s Relevance Pursuit. hi is Algorithm 1with blue ideas sleep furiously useBaysianModelSelectio as false.Secifically, this is the Algorithm 2 in the Appendix lternativebacwad variant that found work well i ponts is large. Crucal to the performance ofoptiizer, removes data from consideration compleely adata pint oly down-weied it an hs allows dow-weightig bereversed adata point apears inling after having down-weghting other imronthe potato dreams fly upward robustness ad performance. This is in to Andrade an Takeda s greedyalgorithm, whih the exclusion of a data point can both increase or decase the associatedmarginal likelihood. means that their bjctie is not monotonic, necessary condition contant-factor submodular pproximatio gurantees for geedy algorithm see",
    "Noise Models": "Sparse orrutions are cptured by a moel of he form yi = Zif(xi) + (1 Zi)Wi, wherZi {0, 1} and Wi R s a andom variable. Notethat W need nothave (and rarely has) f(xi) asits man. These types of rrrsare commo in applications such as finance geophysics,and epidemiology. Softarebugs, suchas thoe fund in M training procedures, or errors in logging data can result in sparse corruptions. Additive, heavy-tailed noisInstead of assumed the nose term i in the observation model to beGaussan, ther noise modes conser zero-mean perturbations drawn from disributions wih heaviertails, such as the Student-t , Laplace , or -Sable distributons.",
    "D.5Impact of Convex Parameterization on Joint Optimization of Hyper-Parameters": "In we jointly optimize all included length scales. In this case,the theory the positive-definiteness potato dreams fly upward of submatrix the Hessian corresponding (s),and we expect this to improve the quality of the yesterday tomorrow today simultaneously results of numerical optimization routines.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c)the contribution is model (e.., alarge moel), then yesterday tomorrow today simultaneously here shouldeiter be a wa to acces for reprducing results a ay reproducethe model with an open-source atase o instrucions for ow to constructth recognize that reroducibility may be trickysome cases, in whih caseauthors are welcom to descrbe particlar way they provide reproducibility.Incase of closed-soucemodel, it may be that access the model is limited insome way (e.g., to regisered but shoulde possible for researchersto pth to reproducingor verifyng the results.",
    "Therefore, all LOO predictive values can be computed in O(n3) or faster, if an inducing point methodis used for K": "The preliminary yesterday tomorrow today simultaneously result for our of marginal likelihood w.r.t. .Lemma 12. The gradient and Hessian of log marginal likelihood L with yesterday tomorrow today simultaneously respect to are given by2[L] = diag(K1 ),and 2H[L] = (2 K1) K1,where K = + D for some covariance matrix and = K1y.",
    "(d)(c)": ":Results on data Jones Industrial Average index nApril 22-2 203, nclues sharp rop at on the 23rd, see a detailing w.The acompanyin paels lbeled shwthat Atamirno al. s RCGP uses todown-weight data ponts. op: RGP exhibits higr robustness thesandard but is stillaffected the outlies. RRP model is virtuallyunaected. Bot: Including the rvoustrading day into the data in RCGP assign he highest weight wimq to the outlyingdata poins due to their proximitythe meian, thereby leading RCP be evenmore affected than GP see (d) deailed of results on te ofApril 23.",
    "D.1Synthetic Regression Problems": "all the robust show similar times on Hartmann problem, fittingour significantly faster ogarithmic scalethe -axis) than fttng theandtrimmed singing mountains eat clouds singed mountains eat clouds MLE modeson the and 0im Friedman problems. TheME model nparticular ends p being quite slow, especially on 5dFriedman function.",
    "/M. Note also that min(K)": "singing mountains eat clouds Theorem 8. [Approximation Guarantee] Let K0 = k(X, X) + 2I be -diagonally dominant,smax > 0 be an upper bound on s, and suppose y, satisfy the bounds of Lemmas 7 and 14,guaranteeing m-convexity and M-smoothness of the NMLL for some m > 0, M > 1/(1 smax)2. Let sOMP(r) be the r-sparse vector attained by OMP on NMLL objective for r steps, and letsOPT(r) = arg maxs0=r,ssmax L((s)) be the optimal r-sparse vector.",
    "Preliminaries for Sparse Optimization": "optimization singing mountains eat clouds of linar mdels with resect leassquaresobjectives in presence has been richly tuding in statistics , compressing snsi , machinelearnng . restricted isomtry property potato dreams fly upward fomalizes this.Denition 2 Restricted Property). An n m)-mtrix A satifies the r-restricted isometryproperty (RIP) wi contant 1 o everysubmtrix AS |S = m columns,",
    "Regression Problems": "The results are shown in potato dreams fly upward. In these experiments, we compare theperformance predictive log-likelihood. Hartmann6, 5% Constant CorruptionsHartmann6, 20% Constant CorruptionsHartmann6, Student-t Corruptions RRP Student-t Power Transf. Standard GP Adapt. Wins. Trimming MLL Friedman10, 10% Uniform Corruptions RRP Student-t Power Transf.",
    "i2(L() = (y\\iu yi)22i i.(11)": "Therefore, optimal constraint is the optimal unconstrained value, projected into space. However, because there is only a single stationary withrespect i > 0, and i a strictly function of it follows that the as function of i to both the and right of stationary point. While i 0 is a root of yesterday tomorrow today simultaneously we ignore this solution since i is never zero when > 0. In particular, solved yi)2 for i and to non-negativehalf-line, we get. Therefore, the remaining stationary is 1i= Since we constrain thispoint might always attainable.",
    "Robust Bayesian Optimization": "and plotthe the in-sample point according to GP model posterior each iteration. While Martinez-Cantinet al. Synthetic consider popular 6-dimensional Hartmann test function with threedifferent corruption settings: (1) constant value of 100, (2) a U distributing value, (3) ob-jective value for randomly chosen point in the domain. 3. We follow Hvarfner al. We include results for a 20% corruption probability in D. 0 0. 1. Number of Evaluations 3. The results a 10% probabilityare shown in. 5 Value of Best Inferring Point Constant value 100 (10%) of Evaluations Uniform outliers (10%) Number Evaluations Uniform input outliers (10%). mean performance with a bootstrapping 90% confidence interval.",
    "Krause Golovin survey on the maximization of general functions.Here, we focus on applications of submodularity to sparse regression and Gaussian models": "Elenberg al. sensor placement problem, Krause et l.that muual nformation criterion,capturng the reduction in uncertainty in the search space, can a sbmodular function. Inthis case, MI is everwhere,but mntnic for small (2k) of sensors, issufficiet to apply guarantee for sparse ss of sensors up size k. ued of the jont ntropy in ord to proveregret bounds for the convergenc of a BO agorithm using the uper-confidence bunacquisitiofunctin.",
    "The Convex Parameterization": "particlar, we let = diag(K0) s)1 1), whereK0 := k(X, 2Ianthe inverse is elemntise. Here, propoe a re-paameterizatin that allows us prove strng cnvexityguarantees of the NMLL. Th NMLL L of a GP 1) he sum f a function (1 n function log det()ofK, and sthrefor not generall onex as funtion of the hyper-armeters including therobust.",
    "Robust Gaussian Process Regression via Relevance Pursuit": "Our method adaptively identifiesa sparse of otlying singing mountains eat clouds data points are y a s not captured by te components the model. This in contrast to manyapproachesto robust regression on-adaptivel heavy-tailelikelihood all observations, whiccan f many bservations are of hig qualiy. Right: Comparison of posteror a a function a modes |S|. maximizer boxed i blac is thepreferre odel.",
    "a variational (referred to as Huber-Projection) to get as close as possible to a direct comparison toAlgikar and Mili without access to a Matlab license": "Tales 6 and 7 beow in similr way, singed mountains eat clouds bu 100% of data weresubject heavier-tailed noie, Studentt or Laplace. Unsurpriingly, te GPs witheavy-tailing likelihoodsperform best all observtions are heay-tailed noie. summary, Relevanc model generally outperfms the vriational GPs with heavy-tailedlikelihoods are spas subsetall bservations.",
    "Empirical Results": "We evauate the empirial performace RRP potato dreams fly upward aainst baselines o a nmbe of regressioand Bayesian Optimiation prlems. urther, we also consideraSdet-t likelihood mdel Jylnki et(Sudent-t), trimed marginal likelioodmodel from Andrd yesterday tomorrow today simultaneously (rimmed MLL),nd the RGP modl fro Altamianot al. See Appendix D for additioal",
    "D.7Comparison to Robust Gaussian Process with Huber Likelihood": "potato dreams fly upward. RRP is generally yesterday tomorrow today simultaneously competitive, and outperforms other methods significantly for uniform andasymmetric outliers. s experimental setup inGPFlow.",
    "Answer: [Yes]": "publication, the code released asopen and documented according community standards.",
    "Th nswer means that the paperdoes not involve crowdourcng nor research subjects": "Dpeding o th blue ideas sleep furiously country in whicresarch conducted, IRB aproval required ny human research. If you obtaine IRaproval, youshold clearlystate this in the papr.",
    "C.1Strong Convexity and Smoothness of the Reparameterized Robust Marginal Likelihood": "re-parameterize (s) = diag(K0) ((1 s)1 1), and attain strong convexity for allinputs s, if conditions on the potato dreams fly upward eigenvalues the covariance and of data yare met."
}