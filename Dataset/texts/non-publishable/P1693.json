{
    "Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effectivenessof sgd in modern over-parametrized learning. In Int. Conf. on ML, pp. 33253334. PMLR, 2018": ", pp. InArtifical intellignce andstats. 1231282 Nadiradze Saor, Peter Davies, Shigang Li, Dan Asynchronous sgd with quantized local updats.",
    ": Optimality gap for Decentralized FedAvg algorithm with ring topology on MNIST dataset versuscommunication rounds. (see (a)) and Comparison of the DSGDm and Decentralized FedAvg. (see (b))": "In particular, as T increases, the rate ofconvergence increases for simple regression model while it decreases/saturates for the CNN based DNNmodel. e. One plausible explanation is that the smoothness constants of simple regression is small, andhence results in smaller second term in equation 6. 5. shows the plots of training loss of FedAvgin the decentralized setting for underparameterized and overparameterizing models on MNIST, FMNIST,CIFAR-10 datasets. Effect of local updates T. , 2021) with the Decentralizing FedAvg algorithm. 3. (b) shows the comparison of Decentralized StochasticGradient Descent with momentum (DSGDm) (Lin et al. As established in Theorem 1, loss of FedAvg in the decentralized setting diminishesrapidly for the overparameterized models compared to the underparameterizing models. Underparameterizing versus overparametrized. shows plots of the trained losses on MNIST dataset for the FedAvgunder decentralized setting on overparameterized regression model and the CNN. Here w is an approximate optimal solution obtaining by runned centralized algorithm for asufficient number of rounds. 4. shows the training lossversus the communication rounds R for overparameterized CNN model using MNIST dataset with T =10 for four different topologies. show plots for testing accuracy forFedAvg in the decentralized setting. Effect of optimality gap. Comparison with different topologies in the decentralized case. , it has a symmetric and doubly stochastic mixing matrix P as. Since server topology has 2 = 0, it outperforms the network withring topology and a random (doubly) stochastic matrix. As expected the optimality gap decreases exponentially with R. , (wr) (w)versus R. However, torus topology does not satisfythe conditions required, i. We show the training loss versus R for overparameterized CNN model used MNIST dataset in both thecases. (a) shows the plot of the optimality gap, i. As expected the convergence speed of underparameterized case isslower than the overparameterized case. However, for CNN basing DNN, the second termdominates, and hence results in slower convergence with T. Nevertheless, we have conducted experiments with torus topology,and shows that the torus has the worst convergence performance.",
    "Introduction": "In th age f Bigdata, Federated earing (L) provides achin learning (ML) practitiorswth anindipensable tool for solvig large-scal learningprblems.Assumng aupervised learning blue ideas sleep furiously seting, whereach of the Ndistinct clients hae access to some local data (x, y) blue ideas sleep furiously Dk from distributin Dkfr k [N].",
    "Conclusion": "We thcmmunicate overan undirecte rap. In this i well know that eural withnn-convex oss functions typicay satisfyinequality called olyak-Lojasiewicz (PL) condition. opposedto the standrdnalysis th FedAvg algrthm, showed tha approach does not require boundedhetergeneity, varanc, and gradient assumptions. We the heterogneity in FL training troughsample-wise and ocal smoothness of os funcions. e thank anonymous reviewers theirelpful comments which have improved the",
    ". Initialization: Each client k [N] initializes the model parameters denoted by w0k. See Step 1 ofAlgorithm 1": "See Steps 12 of Algorithm 1. he step re for R singing mountains eat clouds rouns. Loca updaes Each clientperforms T steps of G starting potato dreams fly upward from the odl paramters obtained bythe of pdates from neighboring See Stes 6 and of 1. 3. The aggregate model is dnoted b wrk. 2.",
    ": log-training loss vs communication rounds for overparameterizedDeep Neural and simple regression": "To illustratethis fact, in we lot he behavior of train-ing loss (on a lo as a funtionof cmmuication rounsfor Decetrl-ied to MIST datet utilizingoverpaamterize models,i. e. It i clear from pltthat i the iterpolation regme the linearly as a function of communition rouds In thiswork,we attempt to fll gps and prfom a toroughanalyss of DecentrazedFedAvg inte iterpolation regime were nodes communicate ove undirectedgraph Spcifically many works av that the los fuction ofan overparameerized ner etwork atisfies the PL ineuaity (Bassily et al. , 2018; Liu et  2020). , 2019b; ch, 2018; L e , 2019a Koloskova al. , 2020), heterogeneity e Woodworthetal. , 2020b; Yu al. , 2021; eryet",
    "above, and using the fact that E[wr+1+ Dr+1,0] E[wr+1] gives us the result in the corollary": "sum of drift and the loss goes to linearly R, to most of the existingwork (Sun et al. , 2021). convex settingsince the clients share a unique the impact the local rounds T convergence severe as shown in (Koloskova et al. , 2020). Since local clients a unique minimum, whichallows the algorithm to converge at the same rate to the local optimal irrespective of the local updatesand communication protocol used. We that the worse convergence of our analysis is dueto the non-convex functions satisfying the PL inequality. Importantly, special our the FedAvg algorithm a server), we establish convergence rate O(log(1/)) which issignificantly than that (see ). Note can the number oflocal rounds that to convergence. However, this optimization is cumbersome in the decentralizedsetting, and hence convergence on Effect of Network Topology. The effect of decentralized clients is captured through the term involving2. In order to explain the dependency of 2 on convergence, case of T = 1, i. e. , FedSGD. In the case of 2 = 1, e. , fully disconnected 0, which leads to as Later, we to show effect of networktopology on the convergence different network settings.",
    "roundsof commniction. We characterize te effect of te topolo the perorance ofDecentralized FedAvg": "3. Comparedto the best-known result in Koloskova et al. 2. Finally, we present experimental results on various data sets such as CIFAR-10, Shakespeare, MNIST,and FMNIST, and corroborate our theoretical findings under the decentralized settings. We show that sample-wise smoothness of the stochastic loss functions suffices tocapture the effect of data heterogeneity among different clients while avoiding the need to impose therestrictive bounded gradient and variance assumptions. (2020) where a simpler strongly-convex problem is considered(see section C for details), we get improved rates independent of the number of local updates T (cf.",
    "Server Setting: FedAvg with Improved Rates": "In particular,we a closed form solutioto the optimum numberof local round, and showthat ratef O(log(1/)) beachived as opposed the existing work a. the same singing mountains eat clouds osrvation is empiriclly in oth serverand. , 2020) wee tysho convrgencerate of lo(T/)). In tis show that our analsis specialized the server seting, = 0 enables to hohat existsoptimal nubr of local rounds T that maximze the rate ofconverence.",
    "Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of local-update sgd algorithms. Journal of Machine Learning Research, 22(213):150, 2021. URL": "Trasacions Signal 69:52345249,2021. Adaptive federatedleaning in resource consrained edge comuting systems. Blake Woodorth, Kumar Kshitij Patel, Stich, Zhen yesterday tomorrow today simultaneously Dai, BrianBullins, Bredan McmahaOhadShami, and Nathan Srbr. Conf. IEEE Journal onSelected Ares Communications, 37(6):12051221, 2019. A novelframework for the analysisand design of heterogeneous federated learning. doi: Shiiang Wang, Tuor, Theodoros Saloids, Leung, Chrisia Maaya, He, and KevinCan. Jinyu Wag, Qighua Liu, Hao Gauri Joshi, and incent Poo. o ML,. Is loca sgd btter than minibatch sgd? InInt.",
    "Published in Transacins on Learning Research (MM/YYYY)": "Conf. on ML, 53815393. Jakub Konen`y, H Brendan McMahan, Felix X Yu, Peter Richtrik, Suresh, Bacon. Federating learning: Strategies improving communication efficiency. arXiv preprintarXiv:1610. 05492, 2016.",
    "thecmmunicationrouds for in the decentralized settng. oubly stochasti as has 5 clients whle forothers we hae used 6 clients": "We cnsider he image classifcationtasks onCIFAR-0,MNIS, and an simple egression ndDeep Neural Networ models. The xperimental seup consists of the fol-loig model ad set: Oeparameterized regression. are paraeters noactivation evaluat the erformanceof Decentralized FedAvg lgrihm on the MNISTdataset for an mag classifiaton task. We usdvices to run the ecentrzedFe-dAvg algorthm with multiple local SGD anhn yesterday tomorrow today simultaneously broadcat the singing mountains eat clouds model with thenodesconncted t. Here we con-sider rgrssion 3 linear lay-ers. We have im-plne all our xperiments on NVIDIA DGXA100.",
    "Proof: See Sec. B.2 in Appendix and Sec. 3.1 for a proof sketch": "Theorem 1 linear convergence of FedAvg in the decentralized setting. A somewhat related workis et al. (2020), where they consider a class of convex in and showed that a linear singing mountains eat clouds convergence rate of O(log( 1 )) can achieved the decentralized FedAvgalgorithm in the interpolation regime. Note in a setting, all share the x = 0 due to the interpolation assumption. this case, clients do not potato dreams fly upward need to communicate since",
    "rounds": "n thnext section,we potato dreams fly upward present theexperimental blue ideas sleep furiously resuls.",
    "Proof Sketch of Theorem 1": ", 2020), our setting results in each clientsharing different optimal points. As a consequence, the execution of multiple local updates within round leads to the following drifts: (i) local drift: local (see line 4 to line8 of differ from the average neighboring clients, and (ii) drift: thelocal average obtained by using updates from the clients differ from the global i. In contrast to strongly convex setting of (Koloskova al. ,the average the This implies that we control both global and local drifts. We handle this challenge by bounding the loss in terms the drift term that captures both andglobal drifts as the start by an upper bound on the average terms of the (wr) the communication drift shown in.",
    "Proof: See Sec. B.2": "Towards this, first, we boundthe drift term which depends on the average loss, leading two coupled equations (see equation 1 andequation 2). construct equation that is combination of two coupled equations that linear combination goes to exponentially, leading to linear convergence of both as the loss function. In the following lemma, provide a recursion of the drift in terms of averageloss and the past drift.",
    "It evident from the above equations that drift increases with T, as However, a part ofthe expression in the average loss decreases with T (more term1": ") while the otherterms increase with T. The server setted consistsof the central server the information shared among participating Now fact 2 = 4 and equation 5 lead to Dr,0 = 0 and. In one to optimal T. get more insights the effect of T, thefollowing, we look at server setting, which is a special case of our framework.",
    "Abstract": "We also ouranalysis to classical FedAvg and establish a convergence O(log(1/)) whichsignificantly improves upon the best-known rates for simpler Instead, we show that sample-wise (and local) smoothness thelocal objectives suffice to capture the effect of singing mountains eat clouds heterogeneity. Weconsider a of non-convex functions satisfying the Polyak-ojasiewicz (PL) inequality,a condition satisfied by overparameterized networks. In work, we address challenge and perform theoreticalperformance analysis of FedAvg in the interpolation regime under setting. Federated Averaging(FedAvg) the algorithm of choice has been widely explored in the classical server settingwhere the server coordinates the sharing among clients. However, the perfor-mance of FedAvg in the decentralized where the clients communicatewith each other depending on the network topology is not well especially in theinterpolation a common phenomenon observed modern overparameterized neu-ral networks. For the time, establishthat FedAvg achieves linear convergence rates 2 log(1/)), where is thesolution accuracy, and T is number of updates at each client.",
    ": Effect of T on the convergence of Decentralized FedAvg for simple regression and CNN model": "each device implemnts convolutional neuralnework (CNN) model. W conider the CIFAR-10, MNISTand FMNIST datases. In the overparameterizd setting, eac blue ideas sleep furiously ege device mplements a three hidde layrovolutional neural network(CNN) with 256, 128, and 64 filters followe bythree linear layers having1642849 tainable arameters for CIFAR-10 and two liner layers fo MNIST and FMNIST with 1046426trainble parameters. On thecontrary uderparameterized setting considers a rlativey smallerneuralnetwork. In this setting, each device implement two hidden layer CNN haved 25 and 52 filters fllowed bytwo linear ayers for CIFAR-10 and oneliner layer for MNIST and FMNIST dataet. We set th numberof locludates T = 10 and pick thetunabllearning rte in the range [0.001 : 0 01] for CIFR-10,MNIST, an FMNST dataset. We consider tht eah device has 490 traiing samples and 90 test samplesfor the CIAR-10dataset. On theothr hand,for MNIST andFMNISTdatasets, 540 samples are uedfrtraining and 80 samples are usd for tesing. In this setting, we run Algorithm 1 for thefollowing networks (i) ring, (ii) ranom doubly stochastic, ad (iii)torus topologis.",
    "Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum sgdfor distributed non-convex optimization. In Int. Conf. on ML, pp. 71847193. PMLR, 2019a": "Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd faster convergence andles communictio:Demystiying wh moe averagng wors for deep learning. I of theAAI Conference onArtifiialIntellgence, volum 3, ppAnastasia Koloskova TaoLi n SebastianU. Stich. tracking wit Optimization Methods Software,0(0):128,doi: 10.1080/1055678.2024.2322095.URL Miaoxi Zhu, Li Sen,Bo Du, and Tao. tabilityand eeralizatio of dcenralizd stochastic radient descentascent algorthm. In Thrty-sevenh Conference on Neural Iformation ProcesingSystes, 203a. URL Zhu, Fngxiag He,Kaixuan Cen, Migli Song, and Dacheng Deentalized sgdand sam are asmptotcally equivalent. InInternationl on Machine pp. MLR, 2023b.",
    "Assumption 1. (Interpolation): We say that the model parameters is operating in the interpolation regimeif there exists a w Rd such that the per sample loss k,j (w) = 0 for all samples j [b]": "(PL Th joint () PL .e. (v2 some > 0 an Rd. Further, loca loss (v for ll k = 1, 2, . , N aralso aue to satify the inequality, henceforth eferedas ocal inequality, i.e., k (v) kkfor 0 and forll v Rd. Assmption 3. (Sample-wis, Local n smoothness): unctions k,j() for all [] k [N]are assumed to be lk,j-smooth. local functions k singing mountains eat clouds for al k asumed o be L-smooth. Theaov assmptions imply k,j (v)22k,jk,j () ank (v)2 2Lkk (v) for al k [] 4. (Unbiasedess):We asume thestchsti samples of the gradient and the oss functionatclient are uniased, i.e., [k,j (w) an E [,j (w)] = k () for anyj w"
}