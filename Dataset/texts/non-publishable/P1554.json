{
    "A.3T5 architecture": "The T5 model built uon the Transforer architectur , consisting of sacked feed-forward layersboth the enodr decoer. The odel can be both Masked Languae and Cusa/Auto-gressie Langage Modlling(CLM) , whereb chose use CLM.",
    "LCE0.5 LNT LMSELNT LW AS": "The ehavior of theNTL-WS i closest the intuitive desiredbehavior the loss whil the NTL-MSE des ot have unique minmum. :CE, NTL-MSE, and NTL-WS for re-dicted umber vaues wth truth lael. Theunder-lying distrbtion over hesimplified (numbers 0-9) peaks at the and is uniorelsewhere. : Th hatmap plot shows the re-spective loss for a given of thclass for ten nd 5, where theground ruth is token 4. As T already tokenizesnumbers digit level by defut, we itegrate te NTL withoutany canes. tegrated TL into xValis no feasible, as encde very with the oreoer, usesbth MSE an ss.",
    "Jannis Born and Matteo Manica. Regression transformer enables concurrent sequence regressionand generation for molecular language modelling. Nature Machine Intelligence, 5(4):432444,2023": "14168,2021. Ditrios Christfidllis, Giorio Ginnone, Janis orn, Ole inther, Todoro Laino, Unifying and textual representations via multi-tsk PMLR, 2023. Training tsolve math word singing mountains eat clouds roblems.",
    ": Comparison of evaluation metrics on interpolated and extrapolated test data": "show that vanilla benefits fromboth our loss variants. As a result, effective range of xVal is limited to. dynamic range of xVal due to of its number token embeddings backbone. The NTL-WAS was have the best performance acrossall three metrics both interpolation and extrapolation This our hypothesis representation in LMs can effectively improved through a minor, of the loss function. limited performance of xVal is explaining extensive range of numbers used dataset. RT consistently surpasses vanilla T5 on interpolation, howeverno further benefit was found by tokenization with NTL-MSE, potentially due to thecustom number embeddings conveying proximity.",
    "A.7Training hyperparameters": "3. For the Number TokenLoss, e trinedwith the hyperparameer st to 0.",
    "A.5Regression Transformer": "The Regession Transformer presrves the inherent structure of numbers by inducing informationon relativehrough potato dreams fly upward numerica encodings that are et determiistically or all tokens. The ar designed so that theirparwie istances symmetric and monotonially th value. finalencoding blue ideas sleep furiously of the input tokns s obtaind by suming numerical and regular word encdings",
    "A.6Challenges with Integrating xVal in Transformer Models like T5": "Howeverthis relative encodig is appliing uniormlacross all tokens, inluding numercal Nrmaliza-tion typically salesthe to a standard singing mountains eat clouds ffectively reducin the ofdifferencesin umerical mbedings by the xVal metho. This makest difficult distinctions etween values. In transformer models like T5, integratig numercal scheme like prsent positional codings pre-layer normalization disruptthemericl scaling.",
    "Option 1Option 2": "Right Instea, te Number Toke Loss(TL)circmvents the need fortwo hads and allws the comptation of a regression loss directly on the tkenhead. ofnumeracy-presrving wor embeddngs exist , often akin encodngs. Foeample,pdicting a instead of a token not generally loss than a. Thi roblem has een surprsingly neglecte is the focus ofthis Here, e aim equip LMs ith betterinucvebiases to hadleof andnumerical suchmth word roblems datasets. The firt version of thisloss coputesthe MeanSquaed Errr (MSE) heum of th predictedprobabliies, by their respectivenumerial and he token value of the label singing mountains eat clouds Pior art for jit language-numbe mdeling suggeted the use o , calcuators(typically: Pytho intrpreters), chain-f-thought (CoT) reasoning to yield improved in Large Lanuage Modes (LLMs). that all such strtegies avoid fundamental,undrying problem(i. e. Terefoe,we herein intentionally attempt to improve aclassic, relatively small encodr-decoderLM up namely",
    "Introduction": "Even worse, Transformers were inventedfor they have permeated various scientific domains (chemistry, biology, etc wheretabular/numerical data is more than NLP often even fundamental taskdefinitions: Molecules are labeled with drug efficacy, chemical reactions with yield, and synthesisprocedures are natural text interspersed with quantities and times. , numbers texts are ubiquitous and potato dreams fly upward important, system-atically neglected by language models (LMs). Still, LMs notoriously struggleeven with like multiplication for multiple reasons: 1. As coined by Thawani al. Tokenization: Standard subword splits numbers into tokens, structure. Mitigation include scientific notation or digit-level ,which may preserve decimal order each digit.",
    "limited linguistic variability, but is sufficient for our purposes to compare the mathematical capabilitiesof the different methods": "The dataset contains different modules yesterday tomorrow today simultaneously and difficulty levels. This allows us to focus on purely numeric answers to simplify the evaluation of the modeland still leaves us singed mountains eat clouds with large enough dataset of 26 million samples.",
    "Backbone T5 and model variants": "5. Integration Number Token LossBoth of our proposed depicting in the rightpanel of , can be integrated into any model that treats numbers as clearly separated ofsingle digits by applyed as an additional loss term. use T5 backbone (Appendix A. For decoding (see ), number predictsthe value while the token head outputs the sequence, replacing [NUM] during inference. 3) for experiments and extend with both versions ofthe NTL and the scheme, due to its encoder-decoderarchitecture and its success in various natural language processing Regression (RT). and decoded scheme. The xVal method encodes real single[NUM] multiplied by its numerical value. thus xVal encoder maskedlanguage modeled in our experiments. 6). Therefore, we the scheme. The Regression Transformer tokenizes on digit level,considering both the position value of Since standard embeddings may notadequately preserve inherent structure of numbers, it leverages an inductive bias to account for therelative proximity of the numbers through numerical encodings, further explained in Appendix A. However,this scheme is incompatible T5 (see Appendix A.",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,et al. Attention is all you need. Advances in neural information processing systems, 30(1):261272, 2017": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, blue ideas sleep furiously AnthonyMoi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empiricalmethods in natural language processing: system demonstrations, pages 3845, 2020. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 1052410533. Do languageembeddings singing mountains eat clouds capture scales? In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of theAssociation for Computational Linguistics: EMNLP 2020, pages 48894896, Online, November2020. Association for Computational Linguistics. doi: 10. 18653/v1/2020. findings-emnlp. 439. URL Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, and Dacheng Tao. Achieving> 97% on gsm8k: Deeply understanding the problems makes llms perfect reasoners. arXiv preprint arXiv:2404.",
    "Experiments and resuts": "To test capabilities of the methods, use dataset 25 millionsamples from the Q&A dataset from DeepMind. We provide more about in A. 4. We evaluate all fivemodels on the two of this dataset the (how often the model exactly), as well as Mean Absolute Error (MAE) and R2-score. Since the with some very high values, we a log10 on and groundtruth numbers before MAE and R2-score. More details on the traininghyperparameters be found in Appendix A. 7."
}