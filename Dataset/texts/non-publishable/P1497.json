{
    "Kheradpisheh, S.R., Ghodrati, M., Ganjtabesh, M., Masquelier, T.: Deep networks can resemble humanfeed-forward vision in invariant object recognition. Scientific Reports 6(1) (September 2016)": "Eberhardt S. , Cader, J. G. How eep te eatue apid visual singed mountains eat clouds categrization? D. M., Guyon, I. eds.: inNeural InformatonProcessing Systems. 29. (206) Rajalingham, . , Is, E. B. , Bashian, P. , Kar, K. , Schmidt, K. , iCaro . :Large-scae, high-resotioncomparison te blue ideas sleep furiously core visual object recognition ehavior of humans, okeys,and stae-of-the-art departificial neural networks.",
    "We then extracted RTs from our RNN to fit human RTs. Notably, model RTs significantly correlatewith the human RT observed in the psychophysics experiment (r = .51, p < .001). Besides, our": "001), it only applis to g-uncertainty cases. 2 for detils). method also surasses entropy-hreholding (bootstappng shosthat our meth s sperior totheirs wih a probablity of 99. 001). 05, p = 0. 1%. 40, p<001). 41p <. 32). However, uing oumethod, thecorrelation rmain strong andsgnificant (r =. Second, nd most mprtanly, althoug uncertinty seems tocorrelat with humanRTs i e dataset ( =. 9%). We show her that modl Ts are significantlycorrelating with human RTs (se r =. 36,p <. InorrectCorrecIncorrecCorect IncorrectCorrectIncorrctCorrect B : RTified W modelevaluation. When rial ithhigh model uncertainy re exclded uncertainty hows no significant correlatiowith human RTs (r =. We argue this in two ways. Wecombie our RTified WW module with (A a 3DCNN to fit humanRTs cllectd inan RDtask (see for MSE comparions with othermethods) and (B) a VGG o fi human RTs i a rapid bject caegorization task (rossedshadedareas the dashedlines are controls t singing mountains eat clouds show the fts after remoingthe highest mode yesterday tomorrow today simultaneously RTs). First, bsrapping shows hat or method is superior toheirs with a probability of 87. We als etacRTs from an ideal-oberver NN modeltraiedwith a time self-penalty (seesetion 2. This failed o be capturing by ucertaintyproxy.",
    "Predicting human decisions with supervision": "Human behavioral decisions in random dot motion (RDM) tasks in decision-making stud-ies typically summarized as histograms similar those shown in . are computing for RTs correct and incorrect to individual experi-mental (such as coherence levels shown here; see section 3 for details). Moreover, RTs forincorrect trials turning into negative Combined with correct RTs (which stay positive), histogram is used for captured accuracy (the proportion of positive values) and RTs. Tomeasure the of fit between RTs and model RTs, use mean squared error loss(MSE) between histograms of human In the object , only RTs averaging across all participants were available. We canmatch human data a stimulus-by-stimulus using the negative correlation loss between modeland human RTs.",
    "Introduction": "Traditionally, thefield has followed two distinct paths. This critical aspect of visual perception has driven thedevelopment of computational models to understand and replicate these processes. On the other hand, decision-making models have been used to explain how visual information getsintegrated over time predicting behavioral choices and RTs jointly. Categorizing visual stimuli is crucial for survival, and it requires an organism to make informeddecisions in dynamic and noisy environments. Models are typically evaluated by estimating confidence scores computed forindividual images, which are then correlated with similar scores derived for human observers(suchas the proportion of correct human responses for each image). Such metrics ignore human reactiontimes (RTs); hence, current vision models only partially account for human decisions. More recently, these earlier models were superseded by deepconvolutional neural networks (CNNs), which have become the de-facto choice for modeling behav-ioral decision. Notably, mathematical models,exemplified by evidence accumulation models such as the drift-diffusion and linear ballistic.",
    "T., A., .: A feedforwrd architecure for rapi categorizaion. Proceedingsof te national academy of scinces 104(15 (2007) 64246429": ", Schrimpf, M.Kar, K. Majaj, , Issa E, Bahivan, P. J. , K. , et al. potato dreams fly upward , J. , Lee, M. J Ratn N. A. :Interativebenchmarking to advance nurallymechanistic models of human eLife (feb e82580.",
    "First, we explain how our RTify module is applied to a pre-trained RNN. Then, we will explainhow to tune a deep-learning RNN-based implementation of the WW model to RTify feedforwardnetworks": "We start with a RNN hidden state ht, which remains frozen. We then alearnable mapping function fw Rk R summarizes state of the neural population at step t by mapping the RNN hidden state ht to some \"evidence\": et = fw(ht). The time which the accumulated passes this threshold is given by () = min{t : t > is treated as model RTs. To the RTs with human RTs, or penalize the model for excessive time steps, we needto optimize a loss function over (). the general case, we first consider F(()) as ourloss function illustrate how we gradient.",
    "RTifying feedforward networks": "To integratetemporleedforward eural networs (e g. CNs), we decribe anRNN module that pproximates WW neural circuit model. The riginal WW model isa neural circuit mode of two-alernative forced choices the temporal IncorrectCorrect Extracting RTs with supevision (fitting human Ts) Extracting RTs via elf penalty (leveragingRNN ynamics)BIncorrctCorrctcorectCorrectIncorrectCorrect IcorrecCorrectIncorectCorrectIncorctCorectIncorrectCorrect RTifid ealuation on a RM tak. sensory evidence in two distinctpopultions. It takes a constant scalar input(representing a such the dgree corncefor moving stiuli). It outpus an RT when the activity population raches a deciion thshold. the WW model has limitatos: itsparameters must manually tuned t and it is restricted to classification tasks with parametrictiuli (e. g. , Gabor patterns) To oercome these liitaions, weextend the model. g. Third, RTify the moel o make al arameterstrainable This allows the autmatcally earn optiml parameter o fithumanRTs (see for an illustration of the multi-populatin case and Fig. A.",
    "Steps": "Figue S2: Illustration RTified crcuit. numbrof time ste needed the RNN o singing mountains eat clouds thethreshold is dfned asth modelT\" and is ued to predict humn We provide pseudo-coe formore dtaile hecircuit (). To RTify fedforward neuranetwrks, weused RN based te WW circuit When receiving visual iput, tetwo populations sensitive to let/ightdiretions compete other (B) and yesterday tomorrow today simultaneously accumulate until one o tem reahes heshold(C.",
    "Experimnts": "As a all were trained on singleNvidia RTX (Titan/3090/A6000) with 24/24/48GB of each. All can in approximately hours. develop a multi-classcompatible and differentiable RNN module based on the WW model. isimplementing an attractor-based RNN, and is stacked top of a feedforward Outputs from classification units of thenetwork are then sent to RTified WW (A). decision is made and the processstops when one of the populations reaches threshold. The number of time steps needing for theRTified to reach threshold is singing mountains eat clouds used to predict human RT (C).",
    "surpasses entropy-thresholding approach (two-sided Wilcoxon signed-rank test, p < .05; forMSE comparisons, see )": "To investigtethi relationship further, we extended ur analysis to examine the modelsaccuracy wouldapproximatehuman accuracy when it fitte oely n human without uing accuracdaa. That RTs tocorrect reposes to preent incorported human accracy model. Remrkably,by fitting the model on RTs only, modl aturallya accuracycomparable human performance (see ). Human-like Ts naturally in this neural netw (See ).Ourmodel preictsT daa much than previous approachs, blue ideas sleep furiously which use a measure of computedoverthe RNN asa mtric. for ll",
    "Random dot motion task": "The ask a classic experimentalto tst temporal integratn hasbeen psychopysics , human imagng , nd elcrophysioloy sudies . stimuli in tsk of dos moved on scren a predefind vs. radomly. For each time ste, each otonly a probability (oherene)0.8%, 16%, 3.2%, 6.%, or to moe toward the pre-defned diection, mak-ingthe task non-trivial. The participants mus tegrate motion informatn acrosstime and reportit when hey are sufficietly orignalexpermental data are fro , 1young adult participats performing around 4,000 trials in total 4 consecuive days. epochs using the Adam optimizer with a larning 1e-4at full(c = for first 10 epochs as a warm-up nd 1e-5a all coherenceleves for 90 poch. or fttinghman R,it wa ained for epchs, f it as traind for epochs. Inboth Adam optimizer were used, and the weights of task-opimizedfrozenwhle raining the RTify modules.tained module to predct RTs by fitnghumn dstibutions. Hre,positie Ts refer RT with corrct hices, annegaveRTsto thchices.Thereore, one distiution incorpoates sped and inormatin from behaviora choices.Results hwn . RTfy model ca predict full RT distriution all cherencelevels. etropy-thresholdin approach fals capure th fu = 51.% to 6.4% andThe RTifiedmodel training spervised settin (i.e, with human behavioral green soid line)performs better(lower M) than etropy-tresholdg (bron ine) coherenceeve. Wit the hel ofour RTied WW solid lie) a convolution neura networ (C3D) n also thedata better ta entrpy-thresholding . Theifiing odelwit i superisedsetting soian inthe sef-enalied green dash line)achieve hman-likcassification under all oherenc levels with the witht RTify (green line).With th hel of RTifieWW module orangesolidine), a CNN (C3D) matches human better than prtraned mol wthout RTify (oragedotted",
    "Rahnev, D.: A robust confidenceaccuracy dissociation via criterion attraction. Neuroscience of Con-sciousness 2021(1) (2021) niab039": "U. Psychological eview113(4) (2006) 700. C. , Forstmann, B. J. : Erly evidence affectslatedecsons: Why blue ideas sleep furiously evidence acculation is requiring t expain response time dta. Winkel, J.",
    "Kreiman, G., Serre, T.: Beyond the feedforward sweep: feedback computations in the visual cortex. Ann.N. Y. Acad. Sci. 1464(1) (March 2020) 222241": "K., blue ideas sleep furiously Kubilius, J., Schmidt, E.B., singing mountains eat clouds DiCarlo, Evidence that recurrent are ventral streams execution of core object recognition behavior. in 36 (2024) Sensoy, M., Kaplan, L., Kandemir, M.: Evidential deep to quantify classification uncertainty. S., Wallach, Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R., eds.: Advances inNeural Information Systems. Volume 31., Curran Associates, Inc. (2018)",
    "A.4Additional Experiment for RDM task": "eural networks generate aequnc outputs across steps, raising the ofhow t convert these outputs single fial predition. In main we cobine allthe networks outpts up until the deision time point. an additional we seth networks at the decision time oint. In oth \"supervised\" and \"self-penalized\" settings,our urpasses o alternative apprachs (see ig. Our approach (green) outperforms the twoalternative appraches(brown),i. , entropy-thresholding for the supervised unertant proxy f the self-pnalizedsettings.",
    "taken from the COCO dataset and 112 synthetically generated images with different backgroundsand object positions": "However, since the binary singed mountains eat clouds werenot saved, we trained model in a 10-class task. We our RNN BPTT to perform classification task. used 20 timesteps of 2 in the original for layer in the Cornetto high temporal resolution. 2% on a held-out Similarly, we trained our RTify modules. We using onthe Imagenet dataset because it in original and it achieves relatively in terms of explaining neural We training the network for 100 epochs, usingthe Adam optimizer with a learning rate of 1e-4 and learning scheduler (StepLR) with a step sizeof 2,000. For fitting human it was for100,000 epochs, while for self-penalty, it was trained 10,000 epochs with learned scheduler(StepLR) step size of of 0. In original performed binary classification task. 3. In both cases, Adam were the of task-optimizing RNN were training RTify modules. Results show that our achieves 75.",
    "A.3Extended results for RDM task": ", entropy-thresholding for the supervised and uncertainty proxy for the IncorrectCorrectIncorrectCorrectIncorrectCorrectIncorrectCorrect IncorrectCorrectIncorrectCorrectIncorrectCorrect with supervision (fitting human RTs) Figure S4: RTified model on a RDM all coherences. AExtracted RTs with (fitted human RTs) RTs self penalty (leveraging dynamics)B IncorrectCorrectIncorrectCorrectIncorrectCorrectIncorrectCorrect IncorrectCorrectIncorrectCorrectIncorrectCorrect IncorrectCorrectIncorrectCorrectIncorrectCorrectIncorrectCorrect IncorrectCorrectIncorrectCorrectIncorrectCorrect Figure S3: RTified model evaluation on a RDM across coherences. We combined ourRTifiing WW module a 3D to fit RTs collected in an RDM task. Human dataare shown gray shaded area, and model fits are shown for (A) the setting used to train models and (B) the self-penalized setting whereno data is e.",
    "feedforward neural networks": "Given the prevalence of feedforward networks CNNs) and their incredible performance a question is how to such networks in the domain of decision-making.We thus developing a biologically plausible, multi-class, differentiable RNN module based on theWW recurrent circuit model This module yesterday tomorrow today simultaneously can be on of any neural network, evenif not recurrent, and can used to align model RTs with human RTs. For the RDM task, we take a CNN with 6 convolutional blocks (convolution, ReLU,Max pooling) and an MLP. We train the network for 100 epochs the Adam optimizer with rate 1e-4 full coherence (c = 99.9%) for first 10 epochs as warm-up and at levels for remained 90 epochs. Since this model is not an RNN, it has no temporaldynamics. Therefore, we module and train the WW module for epochsusing Adam optimizer with a learning rate of 1e-4, a StepLR scheduler with a step size of 1,000and gamma a grad clip at 1e-5. Interestingly, when we RTify the C3D model using theWW it is able to capture the of human across all coherence levels, for coherence = 51.2% to 6.4% and Fig. S4 for coherences, for MSE comparison see). Furthermore, we also observed human-like classification accuracy for the it to fit human RTs but human (see ). Similarly, we take a VGG-19 pre-trained Imagenet for the object recognition dataset providing by Kar al. in a 10-class classification way for the abovementionedreasons. the model 100 epochs using size of 32. The optimizer was a learning rate of 1e-5. We OneCycleLR scheduler adjusted after epochs of warm-up.Results show that our achieves on a held-out test set. We further train epochs using the Adam optimizer with a learning rate of and grad at 0.0001.By using version of the WW model, we that also exhibited asignificant correlation with human data (r = .49, p < .001, see B).",
    "Threshold": "StimuliNeural Nework :Illustratio of our RTify method. We pretrained task-optimized RNN and use a trainale function fw to transfrm the thenetwork into a real-valued measur, et, tht will be over timby an evidenceaccumulator, t When the evidence accumlatr reaches threshold , processing stops, and adecision accumulators , hv been quie successful in modeling narry of bhaviorl data (see fora review. Beyond thes easily araetrizable stimuli these have beextendedto deal more cmplex, natural stimuli. More asshown to be account for both behavioralan recrdings in objct recognition tasks. I , RN was trained to sole a iual clasi-ficaton task thebakpropagation-through-time (BPTT) learing algorithm. The entropy ofthe oututs is cmputed a each timest asproxy for network confidence. is aken the entrpy the hesholdhalting furter recuret comptation. Inthis approa, the recurrece are not difentiable, which prevents the us of gradient methodsnd inerently limits the complexity of the function. Besidsis only appliablewhen RTs it rquires earchfor he correct potato dreams fly upward threshold valeto fit T daa. alternative approach was dscribed in. , a convolutiona was trained onavisua lassification ask sing recurrnt (C-RBP). Experimental results show thatuncertainty and RTs can be positivly ngativey corrlated even dissociate under different experimental conditions. Therefoe, a good mdel of ucertainty s not guaranteed tobe goodmodelRTs. Here, bse o etensie neuroscienc researh, we introduce ovel traiablemodule clled to allw an RN dynamically andonlinearly acumulate evidnce. First,ths modul be rained to make human-like decsions drect RT supervision. resultssuggest that a dynaic evidence accumulation process, copared toheuristis used , can help human RTs. Second, e sho ow amegeneral approach can also be used to train RNN to learn to solve a tak with an optimal time steps via self-penalty. Our results show tht human-ke RTs natrallyrm suchideal-observer mode without explicit suervision from RT Hence,our isgeneral enough to allow thefittingRT data dnein and/or t of a neualachicture can optimally speed and as done in. This nables dynamic nnlinear evidence accuulatio learned tough back-propagation (a) to fithuman data (b) optimally balance speed and accuray. We comprehensvely theeffectivnes of or framework for modeling human RTs across a diverse rangeof sychophysicstass and stmuli. method consistently outperforms wit and witou rainingon humandaa. (iii) As an illustrative of th frameworks potenial, we extend the to crete a iologiclly plausile, multi-lass comatibl, an fullydifferentiableRNN module. We show that the ehaned neural circuit can be sed as a drop-inmodule for CNNs to huan RTs."
}