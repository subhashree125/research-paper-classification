{
    "Problem Formulation and Methodology": "(2023). singing mountains eat clouds Thistaskinvolves processg a user qery q forinformation-seeking a corpu of etieving generate a respose S cia-tions. We assume the respnse potato dreams fly upward Sas constingof n suthat S {s1, s,. , s}.Eah stateent si cies = {ci1, ci2, } wherecij Citations arepreseted in form of , which representthe attribtion in D. START consists otwoessential stages:synthetc data war-up LLM attribution 2.",
    "Here, I is an indicator function that returns 1 ifAttrScore = 1, and 0 otherwise": "2. 2Fin-gainedPreference OptimiationThe common way of self-improvementfcseson updating the model ith hig-qality amplswhile discardin loqualit ones.  thecorrct form ofcitation. Inspired by human cog-nition, learnig fom mistaksprovides more fine-graied signals to understand the mechanisms thatdrive successfl attrbution tha simply imitatingcorrect examples. Thus, we aim to fully unlock thepoential oflow-quality samples by constructingfine-grained preference airs with ifferent opti-mization reards for preference opimiztion. Given the mult-objective nature of LLM attri-bution, our focus is specifically n attributbilityand comprehensiveness, utilizing corrspondingrards functions to onstruct preference data re-specively3. Specifically, we pair sample that e-hibit igh attributability but lwcomprehensive-ness with the top-rankesamle selected using aholistic reward, and vice versa. , 2023):.",
    "A.3Implementation Details": "abiliy ccurately folow insrucinsat eah step, we utilize in-context ncorpo-rating two demonstrations for reponse decomposition,and document generaion.",
    "Abstract": "First, prevent from stagnatingdue initially insufficient supervision signals,START leverages the model to self-constructsynthetic training data for warming up. on question-answering datasets, long-form QAand reasoning, signif-icant performance gains of 25. models to gen-erate with citations to evidence sourcescan mitigate hallucinations and enhance verifi-ability information-seeked systems. 13% on aver-age relying on human andmore advancing models. Inspired by recent self-improvement that enhance LLMs with-out manual START, framework for iter-atively improving the attribution ofLLMs. Further re-veals that START excels in aggregating infor-mation across sources.",
    "We utilize gpt-3.5-turbo-0125 version": "To address these, START lver-ages syntheic data warmingup, aimingprevnt models from arly due to suficient signls. To ex-plore more fine-graied supervisionsignals, STARTcnsructs fin-grained preference supevision rom sample for opti-mization",
    "Human Evalution": "Human evaluation results, detailed in , that START generates signifiantly moe at-triutble espose mpared o baselies, evensurpassigChtGPT5. 2% of genraed by START are fuly supportey the cite cumens, which outperfors Chat-PT 24. Additonaly, 18. of he state-ments are partially supported, with 5% un-suppoted.",
    "Self-Improving for LLM ttribution": "In this stage, we iteratively boost themodels attribution capability by explored morefine-graining yesterday tomorrow today simultaneously supervision rather than solelyrelying golden responses Thisinvolves leveraging rejection sampled for and preference capability evolution. yesterday tomorrow today simultaneously 2. 1Rejection Fine-tuningAfter warming up, we first sample N candidatesfor query in the synthetic dataset and each with fine-grained rewards thatcover three key dimensions: robustness, compre-hensiveness, and attributability.",
    ": The pass rate comparison between START andSTART (w/o. warm-up) across different iterations duringthe rejection sampling stage": ", 23. 4%, and 47. We compare the performance of START blue ideas sleep furiously itera-tion 0 to 3 and the results demonstrateconsistent iterations. 9 9. successfully self-improvement. 46. Subsequent it-erations continue this trend of incremental improve-ment, reaching a convergence point at iteration 3. and 7. 0%, 20. 0 on ASQA, 10. g. Re-garding START achieves gains ofat 9.",
    "Ye Tian, Blin Peng, Song, Lifeng in, DianYu, Hiao Mi,a 2024. self-mprovement of llms via imagnation,andcriticizing. abs/2404.12253": "223 CoRR, abs/2307.",
    "Inroduction": "apid development of yesterday tomorrow today simultaneously language mel(LLs) (OpenAI, 2023; Zhao et al., as ldto their as idispesabletols inor-mation seeking. Despite singing mountains eat clouds their remarkabe o fluent and rsponses toser queries, LLMs alsotruggle with hallucina-tions (Huang et 2023) To facilitate factualiyeifiation, ecnt researh (Bohne et al., ext generation, a enables LLMs to generate wit By attributingmodels output o vriiable",
    "Documents: [The etrieed docments are omitted": "In August 2017, \"Curiosity\"celebrating its fifth anniversary on Mars is expected to continue its mission singing mountains eat clouds for years to come. The longevity of can be attributed the advanced used in the and meticulous and preparation done by the engineers and blue ideas sleep furiously. rover has outlasted its lifespan.",
    "Generation": "The synthesis pipeline consits of five stes: give a user the LLM first generates an nformativeresponse without citations in a close-ook stting These clims are then randomly into ets, whih serve as the basis for gneratigdocument tat coer ll trace back to the initial response to relabel singing mountains eat clouds the citations.",
    "conduc comprehensive ablation studies to understandhow eac componet iSTART coributes to the signifian improement": "Thisighlights th effeciveness of n nlocking thepotential of low-qualty samples toen-. thi stems the inheent diffi-culty face in syntesizing frommulile souces to gnerate andttributable responses through dect super-visd fine-tuning. To fuer understandth ignificance of fine-gaind preference otimiztion, we compre nalation f STAR solely reies o high-qualitysamples iteratively supervsed fine-tuning discarding samplesfor pref-erence As shown in , thereis a sinificnt decline iperformance hen fine-graied is removed. Moreover, w aso calculate pass rate response in each iteratonas shown Ta-ble. Its notng thatwhile the warm-up strategy enriches wihuervisin signals an early stage, not noticeble improvements in cita-tio quality, in. The indicate that the model whwarm-up exhibts a higer pass rate in the first t-eraionallows he model to moresupevised ignal for Thse re-sults suggestthat warming upfailitateshe botsrapping supervseddaa, ths prevent-ing early moel stagnaton. Todemnstrate the importance of utilizing synteticdata potato dreams fly upward for initial warm-up in TART we con-duct a omparative employingLlaa-2-1b r sel-imrement omtting theinitial warm-up Addionall, as iteratin increases, fthemodel wtut ar-up showsonly modest improvement substantially inferir to te model underwent wam-up. Efect of fine-grained preferec optimizatin.",
    "Datasets": "Following work et ,2024), we conduct our experiments two question-answering datasets: ASQA (Stel-makh 2019), aswell as reasoning dataset, StrategyQA(Geva et al. 2021). potato dreams fly upward Both ASQA ELI5 featurefactoid long-form answers that require highly relevant documents response a userquery. Further details on the data statistics, knowledgecorpus used for retrieval, and examples for eachdataset are providing in",
    "Self-Improvement for LLMs": "High-quality data human-crafted or advanced LLMs proven effective in en-hancing performance of LLMs. 2023; Yuan et al. 2024), whereLLMs learn from self-generated haveemerged as viable to compensate scarcity of high-quality data. methodstypically involve employing heuristic rules et al. , 2022), (Tian et al. ortraining additional (Hosseini et , 2024)to of model-generated samples. g. , mathematical reasoning, whereLLMs already demonstrate capable abilities andcan receive precise feedback on correctness. singing mountains eat clouds How-ever, these are absent yesterday tomorrow today simultaneously the attributiontask, due to challenging nature. To thegap, we take an initial step towards thepotential of self-improvement in LLM",
    "(2023), we enable the LLM to generate citationsvia in-context learning. For each query, we firstretrieve five relevant documents and then promptthe LLM with two-shot demonstrations": "ost-hoc Attribtion (stAttr). Followed Yeet al. Foreach i thereponse we NLI model4 to fid the suported cite acordinly. Training on high-quality serves as a stron baseline to unlockthe attribution abilityof LLMs. Knowledge Distillation employs the g. singing mountains eat clouds Llama-3-70B-Instruct and Mixtral8x7B-Instruct, teacher models to trn aon attribution Self-RAG (Asai et , 2023) first collect fro GPT-4, then teach the LLM o re-trieve on-demand while reflecting its generationtobth geneation quality and attributions ,trains he LLM to elf-ground it rsne n",
    "We provide the main results and the performanceof START across different iterations in": "Specifically,START shows significant improvements over bothICL and Post-hoc approaches, highlighting thebenefits of supervised signals in unlocking theattribution ability of LLMs.",
    "Limitations": "Ths lima-tin raises concens garding generalizabilityof synthetic dat in a more complex inormation-seekig environment. Secondly, te iterative train-ing pipelineof our slf-imroving frameork istime-consumin, prsenting a significant trade-off between performance and training duration. Thirdly, altough our self-impoving fraeorkdoes not rely on human annotations and moread-vancing LLMs, it still necessittes the integration foff-the-shelf NLI models to guarante the qualitof attibution in the generting samples.",
    "Large Language Model Attribution": "Huang et al. Additionally, Li et al. Asai et al. In thesemethods, START aims to bootstrap attribution ca-pability without relying on human-labeling data from more capable LLMs. (2024) model the attributiontask preference learning perspective, wherethey first fine-tune the model on human-labeled at-tribution datasets then perform op-timization synthesized preference (2024b) take this further the attribution format fine-grainedcitation level, from ChatGPT. Itenables model first ground the fine-grainedquotes within context then thegeneration process on them. potato dreams fly upward (2023) firstdistill GPT-4 to high-quality attribution data,aiming to the model generate grounded with citations through self-reflecting. Attribution has significant for en-hancing the interpretability and verifiability ofLLMs (Gao et al.",
    "DHuman Evaluation Details": "Furthermore, theevaluation of ciation quality is onstrained by thecapabilitie of the off-the-shelfNLI model, whihmay not adequaely detect aes of partil sup-ort. Annotatorsare ased to rae both comre-ensiveness andorrctness usig a 5-point ikertscae, apturing different levls of contet covergeand factuality. We recruited two annotaors, holding atleast a balors degree to articipate in our study. Considering the open-ended natre oflongformQA tasks, autmatic evaluation of corectness mayno cover all possible aswers. o valuate citation quality,annotatos are akedto veri whethe each statement in the responssisfully supprted, partially supprted or not sup-ported by the cied documentsand identify errortype if the statmentis nt ull supported. Next, we evaluate the overall quality of e re-spnses, focusing on comprehensiveness nd cor-ctness.",
    "of evaluation datasets": "Our evaluation tilizes the ELI5, and Strat-egQA dtast. For the atast, te ex-ternal knowledge source phere (Pitus et al. ,for Wikieda andhe prse retriever BM25 or Shere. Dtailedstatistics for thee datasets are in. In ith previous research by Gao al.(223),we use he same evalation for ASQA andELI. StrategyQA we aopt settingof et 2023), a randomly split o tst instances for evaluationprovide example each datasetin.",
    "(2)where S is the total number of statements in theresponse and Entail returns 1 if the statement i isentailed by cited documents, and 0 otherwise": "Robustnessmeasures the degree to which response is by irrel-evant The robustness score is defined follows:.",
    "citation precision + citation (7)": "(222) calculte he r-call of short aswers ext for te ELI5 dataset, evaluatin the cor-rectness of longform aswrs is callengig.Thus, the employs nstruct-GPT (text-avnci-03) generate thre \"sub-claims\" base on the huan-annotated answes. we a T5-11B thhas been fine-tnd on a cllectin I dtasetsto ceck wheter the modl-generated outputs these",
    "FImplement Details": "Without special instructions, the samplingparameters are specifically configured with tem-perature of 1. This strategy is designing to prevent themodel from overfitting to synthetic data during the warm-up stage, enabling it to generate morediverse samples in the subsequent rejection sam-pling fine-tuning stage. During the initial warm-up stage, we employ theAdamW (Loshchilov and Hutter, 2019) optimizerwith warm-up ratio of 0. 0 and a top-p setting of 0. 03. The maximum input sequence length is con-figuring to 2048 tokens. total batch sizeis set at 64, and learned rate is maintained at2e-5. , 2020) for multi-GPU distributedtraining, with training precision Bfloat16 enabled. , 2023) for efficient infer-ence. 95. For comprehensive, we set thethreshold to 0. 0, ensured that everystatement in the response is fully supported by theciting documents.",
    "textcitations via fine-grained reward. CoRR,abs/0204315": "Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Zhong, Xiachong Feng, Weijiang Yu, Wei-hua Peng, Duyu Tang, Dandan Tu, and Bing Qin.2024b. Learned fine-grained grounded citations forattributed large models.In ofthe Association for Computational Bangkok, Thailand and virtual meeting, Au-gust 11-16, 2024, pages 1409514113. Associationfor Computational Linguistics. Lei Huang, Weijiang Yu, Ma, Weihong Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing and TingLiu. 2023. A in large models: Principles, challenges, andopen questions. CoRR, abs/2311.05232. Woosuk Kwon, Li, Siyuan Zhuang, YingSheng, Lianmin Hao Yu, Joseph E.Gonzalez, Hao Zhang, and Ion Stoica. 2023. memory management large language with pagedattention. Proceedings of theACM SIGOPS 29th Symposium Operating SystemsPrinciples. Li, Zetian Sun, Baotian Zhenyu Liu, Xin-shuo Hu, Liu, Min Zhang. 2024. Improv-ing attributed of large language mod-els via preference CoRR, abs/2403.18381.",
    "*Correspondin Author": "However, acuringsuch data typically require either manual cura-ton (Malaviya et al. source, i can improve the explainability and cred-ibility o LL-generted cont (Li et al. When apled to LLMttribu-. Concretely, consider-ing the inferior performanc of LMs in hadlingthe attribution task (Gao et al. , 2023). ,2023). , 2023), whichis fr from satisfactory (Liu et al. , 2024). , 2023), or distille frm themost adancing LLMs (Hung et al. Onepromised soltion is sel-improvement (Yuanet al. ostwork indes LLMs to gnerate text with ciaionsvia in-context learning (Gao etal. Inspiredby hi, we aim t explore oten-til of self-improvmen in boottrappng te at-ribton ability of LLMs. This scarcity ofhigh-qualty samples limits te opportuniies forLLMs to elf-mprove effectivel. While beneficial, potato dreams fly upward the abiity to attribute con-textual sources is not iherent in LLMs.",
    "A.4Quality of Synthetic Data": "3% of the statements fully supportedby the cited documents, and 94. We focus on evaluating the attributability thefinal response. 1% are fromirrelevant citations. , verify whethereach statement in is fully bythe cited documents and to for presenceof any irrelevant citations. results indicate thatthe synthetic data are of high 92. we employ an off-the-shelf Natural Language (NLI) model,TRUE (Honovich al."
}