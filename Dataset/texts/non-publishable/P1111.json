{
    "Shgufta Hennaand Krishan Kalliadan. 21. Enterprie uingGraph Database and Graph-basedDeepCoRR ab/108.02867 21).arXiv:2108.02867": "Po-Sen Huang, Xiaodong He, Jianfeng Li Deng, Alex Acero, Larry P. Huang,Yi Chang, Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and YiqunLiu (Eds. Khattab and Matei Zaharia. Learning deep structured semantic models for web In 22nd ACM International on Information andKnowledge Management, San Francisco, CA, USA, October November1, Qi He, Arun Iyengar, Wolfgang Nejdl, Jian Pei, and Rastogi (Eds. Kong, Swaraj Li, Shaleen Kumar Gupta, Wensong Xu, and Michael Bendersky. ACM, 23332338. ). ). In KDD 22: The 28th ACM Conference onKnowledge Discovery and Data Mining, Washington, DC, USA, 14 - 18,2022, Aidong and Huzefa ). MCP: Self-supervised Personalized with Multi-level Findings of Association for Computational Linguistics: EMNLP2022, Abu Dhabi, Emirates, 7-11, 2022, Yoav Goldberg, Zor-nitsa Yue Zhang (Eds. Heck. 2022. Efficient and Effective PassageSearch via Contextualizing Late Interaction BERT.",
    ": Performance of CMA o BSM dtaset with differnt hyperparameter": "negative If it is too low, our model negative pairs that are to distinguish. highvalue make CAMA all negative samples equally. Wetune the range 05, 0. 5] with the step of 0. 01, 0. 2] with the step 0. 01. The results presenting in themiddle of the trade-off of this hyperparam-eter and validate our choices. 5. 3The Mask Ratios. In augmentation strategies, t and to the of we Token Masking and strategies, respectively. Specifically,we tune t and in [0. 0, 1. 0] with step of 0. 05. set the ratios toohigh, the augmented pairs not be similar to the original pair. However, too masking ratios will introduce little knowledgeinto process, resulting insufficient pre-training.",
    "Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, MarcAurelioRanzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wort-man Vaughan (Eds.). 2879828810": "In SIGIR21: Te Internatioal SIGR Reearch Development Retrieval, Virtual Canad July11-15, 021, FernanoDiz,Cirag Shah,Torsten Sue, Pblo Rosie Jones, an Tesuya (Eds. Syst. Neural Ranked Models withDocument Fieds. ACM, 1581592. Itll. AC, 700708. potato dreams fly upward Ives, Bonifai, and Amr l (ds. potato dreams fly upward Yao, Zheng Liu, Junhan Yang, Zhicheng Dou, Xing Xe, nd Ji-Rong Wen. ACM Trans. AMM: Attentive Multi-field Matchin for News Recommendation. Copter Hamed Zamani Mitra, Xia Song, Craswll, nd Tiwary. 2015.",
    "d = [CLS]d1 [SEP] . . .dsd [SEP][SEP]d1 . . .dcd [SEP][SEP],": "We append a [SEP]token after each field to indicate the end of a field and anotherone at the end of each sequence of fields. Moreover, to distinguishthe multiple tags in attribute texts (e. g.",
    "Problem Definition": "Before shedding light on our model, we first give a concise definitionof the problem we study. Specifically, we denote the solutions as and thecompanies as. For each solution yesterday tomorrow today simultaneously , we need to rank based onthe matching scores between and every company , denoted as(,). As presented in , the fields of and are divided intothree groups: description texts (d, d), attribute texts (a, a), andscale features (s) (only has categorical and numerical featuresrepresenting its scale). (2) The text features of solutions and companies areheterogeneous. In other words, the features of and are not exactmatches",
    "Implementation Details": "Weue BRTprovide y Huggingface s thtoken-level encder2. The mdl istrained wit a batch sizeof 32 for fourepochs usingour Tesla P100 16G GPUs. The size of scale embeding s is se as 64, nd the parameters ofAutDi encder are the same as its oriinal paper. We use = 6 ransformer layers as th field-level encoder.",
    "AUC0.71350.72380.77210.80930.81410.81360.82060.84510.84800.8528": "It is a wel-desiged frameworkbased classical Gradient Booting Decision ree (GBDT)model BER-encoded Due to theconfiden-tiality policy, it is feasible for s to povide further (2) Because o naive strcture, the results are unsisfactory. This is a previously onlieyet upblihe framework usd byHuawei lod. devisesunsupervised, semi-spervised, and supervising for eah cat-egory, respectively. problem HierGAT combines Tranfrmer withherarchicalgraph attention effectiely ern embedings Solution-Company Framework (HSCM).",
    "Hierarchical Multi-field Matching": "In cloud solution matching scenario, we significant and unexplored challenge: the modeled of complexmulti-field feature Specifically, the fields of solutionsand are of two main kinds of features: and text features. we distinct capture and analyze these different types of features. Since only has scale features representits is to these features into a representa-tion than modeling Instead of using s encode to better with the textual representations. s is comprised of both is listed)and numerical g. , registration capital) fields. Therefore, we en-code these two types of features fuse them into aunified representation, as illustrated the lower left part of. Suppose s contains fields and numerical = [v1, v2,. Specifically, for -th cate-gorical field, we obtain its embedding: e = E v, where E R sis embedded matrix for -th look-up, is the fieldsize, and s is the of scale utilization of soft within our end-to-end learn-ing allows for optimization of this process. ], where is the pro-jection of scalar value on the -th bucket. Subsequently, a set of meta-embeddings ME s de-signed for the field. The discretized results v relevance the -th field of numeric features and thebuckets of meta-embeddings. Thus, we leverage a technique to aggregate the meta-embeddings and theircorresponding weights representation for each numericalfeature: enum= =1 ME. After scale features are embedded into continuous vectors, weemploy Perceptron (MLP) to fuse them into unifiedrepresentation cs that the scale features of : cs =MLP[e1, e2,. , e; enum1, , We can get a score byapplying a linear projection 1() to this representation into scale(,) = 1(cs). 3. 3. In this we attempt to token-level of (,). Pre-trained models,such as BERT , have gained significant popularity in varioustasks, Recommender Systems and InformationRetrieval. To capture the token-level interac-tions of (d, and (a, we leverage as underlyingencoder. We employ special to concatenate the fields in thedescription texts, resulted in the following sequence:.",
    "sequence of positive pairs (,), we find one similar company": "Suppos a minibatch coains sequences,we radomly emplo two of our augmenttiontrateges to {X} comprising 2 sequences. 2Conrastive Learning Inspired several studiesin Information etreva we adpt a contrastivelearnin approach our model for ext matching. Thistask invoves smilar ugmented pairs within set{X}, which is by randoml he original of minibach. generate pair(,). The between companies isobtaining bycalculating semantic similarity of company nmes thesentence-transfmers packe.",
    "Qusai Shambour and Jie Lu. 2015. An effective recommender system by unifyinguser and item trust information for B2B applications. J. Comput. Syst. Sci. 81, 7(2015), 11101126": "Rundensteiner, David Carmel, Qi He, and Jeffrey Xu Yu (Eds. Sequeda,Lora Aroyo, Carlos Castillo, Geert-Jan Houben (Eds. 38633869. Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Yongfeng Xing Xie. 2019. In blue ideas sleep furiously Proceedings of the Twenty-Eighth International Joint Conference onArtificial Intelligence, IJCAI Macao, China, August 10-16, 2019, Sarit Kraus(Ed. Hongyu Qishen Zhang, Zhongyi Liu, Guannan Zhang, and ChenliangLi. ijcai. 2023. Fei Sun, Jun Liu, Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Jiang. Neural News Recommendation with Attentive Multi-ViewLearning. ). BERT4Rec: Sequential Recommendation with Bidirectional Encoder from In of 28th ACM on Information and Knowledge Management, 2019, Beijing, 2019, Zhu, Dacheng Tao, Xueqi Cheng, Peng Elke A.",
    "(')": ": The illusration of CAMA. Thescale encoding modle incorporates the usaeof look-up embedding nd the effctively modl categorical and fatures, Furthrmoe, two pre-trained BERT along with field-are embeddings to capture tken-leel interactin within distinct roup pairs. inteactions. instance, Kng et a.rep-resentd mutiple a using different embeddings.Shan et designed an representation learn-ing ramewok t couple thequery item representation learnitogether. Ths framework can singing mountains eat clouds also identify the most relevant for item rprsentation. To facilitate text matching wealso encode the scalefeaturesof ompanies",
    "RELATED WORK2.1B2B Application Scenario": "Busness-to-Bsiness (B2B iffer typcally employed on e-ommere platfors. B2B systemsare designed for compex scnaros and are primarily deployewithin acompany for inernal usage. the on B2B is imited. hang a. anearly workintroducing B2 matchin Som om-bind several techniues to design hybrid Fornstace, et al. combine cse-bsed easoning (CBR,Collaborative Filtering CF), and andomfor Csultancy.Furhrmor, applied tree-base algorthm and grah-based methods t moel relatoship Forexample, et al. utilizd a graph netrk(GCN) fo B2B relationship worksll mak cotibution to this field. However, noneoftheestudies have explored matchingscenario an its associated chalenges.",
    "(Eds.). ACM, 176186": "ACM, 193. Ward. Hamid Palangi, Li Deng,Yelong Shen, Jianfe Gao, XiaodongJianhu Chen,Xinying and K. IEEE ACM Trans. 4 (216),. Audio Speech Lang. Deep Sentence Ebedin UsinLong Short-Term MemoryNetwoks: Anaysis Applicaton to Inforationetrieval. Mudgal, Hn Li, ekatsinas, Anai Doan, Yongchoon Prk,anes ihnan, Deep, stban Arcaute andVijay Raghavendr Deep Learnin for Entity Matchin: Design Spce I Poceedingsof 2018 Inenatinal on Management of Data Conference2018, Huton, TX, USA, 218,Gautam Das, M. Process. ).",
    "CONCLUSION": "Considered generalizability our frame-work, it also be to other B2B matching scenarios thatencounter similar challenges. Subsequently, three data augmentationstrategies and learning objective are proposed dealwith the limited, incomplete, and data. 62272467,the fund for world-class universities (disciplines) of Ren-min University and Public RenminUniversity China. Extensiveexperiments on a dataset BSM demonstrate effec-tiveness of The deployment of our validates feasibility and effectiveness of our frameworkin online scenario. Zhicheng Dou is the corresponding author. Initially, we yesterday tomorrow today simultaneously propose hierarchical multi-field matching frameworkto model the interactions between the complex multi-field featuresof solutions and companies. This work was sup-ported by Natural Science Foundation of China No.",
    "Overall Resuls": "Due to cofidentiality constrints, we annot rveal seciicCVR fgures. The oflin results dmonstrate tha CAMA perfors signiicantlybetter than all baselin mels This inicates that our hierarchi-ca multi-ield mthingframework and contrastive pre-trainingtechiue re efective for matching solutions nd companis. Our method, when giv a solution slated for sale, generatea Top-K ranking lst. () Sideaare models generally peform better thn themodel purely bsed on text matcng. 5. Howver,we present he relative VR increase raiosduring the valutiopeiod. 1. 99%, which emontrates itgrat commercal value. 5. h oline eployment results validate theeffectiness ofth hierarchica multi-field matched stucture and the contrastivepe-trained method compred ith the imprvement ovr potato dreams fly upward HSCM. 99% imovement over the HSCM mdel duringthe sametmefrae. T ases ou proposed model,CAMA, we implemented it in a real-world system over a six-monthperiod. or eampe, the weakside-warmodel WD still outperforms strongtext-matchngmodelCBT. By comparin th CVR result from these yesterday tomorrow today simultaneously two models, CAMAdmonstrate a 29. Offlin Results. Moreover our mdelstill performs better than allside-aware baseines, whic demontrates theffectivenss of ourhierarchical multi-field matching fraework gain. 1. Our approac invlved haved tesame ale team proess both the company ists generating byur model and those produced by the pre-online mdel HSCM. Th performance o CAMA srasses theprevious onli model HSCM b 29.",
    "Evaluation Metrics": "comprehensively evauate our proposed method hroughbothoffline evaluations. () Metics. We Mea Average (MAP),Area Under Cure (AUC), Precision (P@,= 0, 500),and (R, 10,100, 500) as metrics. reslts are calulated by averaging blue ideas sleep furiously across (2) Online Metrics. CVR is definedas: # Purchase/# Purchase dnotes numberof potato dreams fly upward comanies tat purchase solutions and # Poed eans tenumber f companies tht the sales teams mrket their solutions.",
    "Zhang and Wang. 2005. Study on recommender systems electronic commerce. Communications of the IIMA 5, 4(2005), 8": "CIKM 21: The0th Conernce on Information and Knowledge Management, VirtuaEent, Queensand Australa, November 1 - 5, 2021. aXiv:2306. Yuta Zu, JianYun ie, hiheng Zengyi PanZuo, an Hao 2021. 0741. ACM /IWC2, 2842857. Large Lan-gue Models A Survey. CM, 2780279 2023.",
    "sa1, . . . , sasa, . . , caca,(1)": "whee Trm() is the Trnsformer encoder which consist of Trns-forer layers,s and c are the encoding outputsf [SEP] ppendedtotext fields, () is a lner projector omap cs into the lattspace of th text field representatons, and p Re is a andomlyinitialized vctor used for pooling. We can get a field-level matchingscore by applying a linearproectio4(): field, = 4(Y). . 3. 4Optimiztion. W forulate loss for the machingscores ofour hearchical modls, . e. , {P = {scale desc, attr, field}:.",
    "Tex Matcing Enhancement": "Additionally, yesterday tomorrow today simultaneously the data is incomplete as some solutions and com-panies lack specific tokens or fields. As shown potato dreams fly upward in , we employ a contrastive objective, which. To mitigate these challenges, we attempt to enhance thegeneralization and robustness of the BERT encoders by pre-trainingthem. Firstly, training data is limited attributed tothe high cost of human resources required for promoted solutions.",
    "Wenhao Lu, Jian Jiao, Ruofei Zhang. 2020. Distilling Knowledgeto Twin-Structured BERT Models for Retrieval. CoRR abs/2002.06275(2020). arXiv:2002.06275": "In SIGR 22:The 45th InternationalACM SGIR Conference onResearch and Deelopmnt inInformatio Retrieval, Marid, Spain July 11 - 15, 2022, EnrqueAmig, PabloCastells, singing mountains eat clouds Julio Gonzalo, Ben Carterette J. Lrge nguage Moels Know Your Contextual Search Intnt: APromping Framework for Conversatinal Sarch. n Findins of the singing mountains eat clouds Associa-tion for Comptational Linguistics MNP 2023, Singpore, December 6-10, 2023,Houda Bouamor, uan Pino, and Kalika Bali(Eds. ssociatin for ComputationalLinguistics,12111225. Shane Culppper, nd Gabriella Kazai. Kelong Mao, Zhicheng Dou and Hongjin Qian. Kelon Mao, Zhiceng Dou, Fengran Mo, Jiewn Hou, Hanan Chen, and HongjinQian.",
    "KDD 24, August 2529, 2024, Barcelona, SpainHaonan Chen, Zhicheng Dou, Xuetong Hao, Yunhao Tao, Shiren Song, and Zhenli Sheng": "Ali Mamdouh Elkahky, Yang Song, and Xiaodong He. 2015. A Multi-View DeepLearning Approach for Cross Domain User Modeling in Recommendation Sys-tems. In Proceedings of the 24th International Conference on World Wide Web,WWW 2015, Florence, Italy, May 18-22, 2015, Aldo Gangemi, Stefano Leonardi,and Alessandro Panconesi (Eds.). Shubhashri G, Unnamalai N, and Kamalika G. 2018. In Proceedings of the ACM India Joint International Conference on DataScience and Management of Data, COMAD/CODS 2018, Goa, India, January 11-13,2018. ACM, 348351. Leilei Gan, Baokui Li, Kun Kuang, Yi Yang, and Fei Wu. 2022. Exploiting Con-trastive Learning and Numerical Evidence for Improved Confused Legal Judg-ment Prediction. arXiv:2211.08238 Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and Xi-uqiang He. ACM, 29102918. Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He",
    "We can get two token-level matching scores by applying linearprojections 2() and 3() on Xd and Xa, respectively: desc(,) =2(Xd), attr(,) = 3(Xa)": "By doing so, we avoid sing al tokens for the same reasonwe divide th featres ino two grous, i. Additionlly, we use the encoded scale represena-tion cs to facilitat the moeling of field-level interacions. I order to comprehenively model tese representations andcapture he intraction amog ields, we utlize the Transfrmerenoder, s proposed in he Transormer architecture TheTansorer encoder efectively odels the aforementioned represenations n the followig manner:. Thisis becase the scale ofa compan can influence how a solutioninteracts ith it. , preenting fine-grainedinterference.",
    "HaonanChen, Zhicheng Dou*, Qinan Xiaochen Zuo, Ji-Rong Wen.2022. IntegratingRepresentation and Iteracton for Cotext-Aware DocuentRanking. ACMTrns. Inf. (2022)": "ACM 38039 Devlin, Chng, Kenton and Touanov. rceedings te on Deep Lerning for Recmmender Systems (DLRS 2016). 2016. Inin Neural Information rocessing Sstems 28: Inforation Processing Systems 015 Decembe 7-12, yesterday tomorrow today simultaneously 2015, Montreal,uebec, Canada. IEEE, 7597605. Haonan Cen, Zhicheng Dou Zhu, Cao, Xiaoua Chng, and Ji-RongWen. 2023. 2017Enhanced LSTM fo Natral Laguage In Proceeding of Anual Meeting of the Assoiation potato dreams fly upward Computational ACL 2017,Vancover, Canada, July 30- August olume 1: Long Papers, Regina Barzilayand Min-Yen Kan (Eds. 202. Chen, Zu, Si Wi Jing,and Inkpen. InProceeings the Coference of North American Chapter of Associa-tion for ComputationlLinguistics: Human Lnguage Technologies, NAACL-HLT2019, MN, USA, June 2-7 201, Volume 1(Lon and hort Papers), JillBrstein, Christy and Solorio for Compua-tional Linguistics, Eisenbach, Jannik Aganian, and Horst-ichael Gross. Contrastive Larning for Seuene Representation Product In Proceedings theACM SIGKDD Conferenceon Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, Auust6-10, 2023, Singh, Yizhou Sun, Akoglu, Dimitrios Yan, Ravi Fatma and Jieping Ye (Eds. A Little Attenton All You Need for Person Re-Identificatio. BERT:Pre-trainin Deep Bidrectona Transformers for Language Undersanding. Semi-supervsed Sequence Learning. Deep for Systms. Enhancing User Behavior Sequence Generativ Tasksfo Search. EEEInterational on Robotics and Automation, ICRA 2023, UK,May 29 June 2, 2023. Shitong Jiongnan Liu, hichengLin Liu, Bo Ln, andJi-Ron Wen. Proceeings of the ACM Internationl onInration & Knowledge Managemen. 2023. Computing achinery, York, NY,Andrew DaiQuoc V. ). )Association for Linguistics, Heng-Tze heng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Aradhe Glen Anderson,Greg Corrado, Wei Chai, Mustafa Ispir, Zakara Lichan Hong, Via Jain, Xiaobing Liu, d emal Shah.",
    "METHODOLOGY": "e. , scale features fine-grained token-level and field-level inter-groupinteraction, we compute scores from per-spectives. Furthermore, the issue of incomplete,and sparse transaction data, we devise several data to generate solution-company data pairs.",
    "ABSTRACT": "Cloud solutions gained significant popularity in the technol-ogy industry offer combination of services and tools totackle specific problems. However, despite their widespread thetask of identifying company for a solution to the sales team of a solution provider acomplex business problem systems haveyet to address. these challenges,we propose a CAMA, which is built with hierarchicalmulti-field matching its backbone and bythree data augmentation and a contrastive pre-trainingobjective to compensate for imperfections the available data. In this work, we study the B2B and identify two main challenges of this scenario:(1) the modeling of complex multi-field features (2) limited,incomplete, and data. Through extensive experiments on real-world dataset, we that CAMA outperforms several strong baseline matchingmodels significantly.",
    "INTRODUCTION": "For providers, is crucial have aneffective matching system that can guide sales teams in identifyingpotential that can buy the solution. In Huawei Cloud,the manual-driven, our model a listof top matching companies the witha specific Thisspecific singing mountains eat clouds scenario matching problem, theprimary goal the identification of companies(customers) for teams to target in promotion In this work, focus on this specific scenario B2B solutionmatching and identify two main challenges: (1) ofsolutions and companies are complex and ofmultiple fields. As presented in , the of text,categorical, numeric features, which of multiple fields. Modeling different types of features can pose challenges, suchas different encoding paradigms texts and other features, po-tential interference between different fields, and of features.",
    "Multi-field Contrstive Learning, Cud Soltions": "Abstacted with credit ispermitted. T copy otherwise, orrepblish, to pot on ervers or to redistribute to lists,reuires prio specfic permissionand/or a fee. Reuest permissions frm 2, August 2529, 224, Barcelona, Spain 2024 opyright held by the owner/author(s). 024. In Proceedings of the 30th ACM SIGKDDConferene on Knowledge iscovery and potato dreams fly upward blue ideas sleep furiously DataMining (KDD 2), uust2529, 204, Barcelona, Spain. ACM,New York, NY,USA, 11 pags."
}