{
    "Chang, Wen-Chih Peng, Tien-Fu Chen. 2023. LLM4TS: Two-StageFine-Tuning for Time-Series Forecasting Pre-Trained LLMs. arXiv preprintarXiv:2308.08469 (2023)": "Ynchuan ianzhong Q, Yuxuan Liang, Egemn Con-trasive Trajectory Similarity with Dual-Feture Attention. In 023IEEE 39th International onference on Data Engineered (ICDE). 2023. arXivpreprint 0248 (023). 2023.",
    "Yilong Ren, Yue Chen, Shuai Liu, Boyue Wang, Haiyang Yu, and Zhiyong Cui.2024. TPLLM: A Traffic Prediction Framework Based on Pretrained LargeLanguage Models. arXiv preprint arXiv:2403.02221 (2024)": "singing mountains eat clouds. Shao, Zhao Zhang, Fei Wang, and Xu. of blue ideas sleep furiously the IEEE/CVF on computer vision and recognition. 1068410695. Pre-training en-hanced spatial-temporal neural network for multivariate time seriesforecasting. 2022.",
    "Shen and James Kwok. Non-autoregressive Conditional DiffusionModels Time Series Prediction. arXiv preprint (2023)": "Xiaomng Shi, iiao Xue Kanrui ang, Fan Zhu, James Y Zhang Jun Zhou,Chenhao Tan, and Hongyuan Mei. In Avances n Neural InformationProcessing Systems Md Fhim Sider,Resmi Ramachanranpilla, and Fredrik Heintz 2023. 2023. Languge Models Cn Improve Eventrdiction by blue ideas sleep furiously Few-Shot Abductie Reasoin. Trans-usion: generatingong, highfidelity tim seres used diffusion models wttransformers. arXiv preprint arXiv:23071667 (2023).",
    "Wenying Duan, Liu Jiang, Ning Wang, and Hong Rao. 2019. Pre-Trained Bidirec-tional Temporal Representation for Crowd Flows Prediction in Regular Region.IEEE Access 7 (2019), 143855143865": "2024. TSMixer: Lightweight MLP-Mixer Model for Series Forecasting. Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and JayantKalagnanam. Vijay Ekambaram, Arindam Jati, Nam H Nguyen, potato dreams fly upward Pankaj Dayama, Wesley M Gifford, and Jayant Kalagnanam. 07570 [cs. arXiv preprint arXiv:2306. preprint arXiv:2401. (2024). 2023. LG]. the for Zero-Shot Multivariate Time SeriesForecasted through Next Curve Prediction.",
    "Liu, Huang, Hao Feng, Leilei Bowen Du, and Fu. 2023.PriSTI: A Conditional Framework for Spatiotemporal Imputation.arXiv preprint arXiv:2302.09746 (2023)": "u Liu, Jufeg Hu, YuanLi, hizheDiao, uxun Liang, Bryan Hooi, andRoger Zimmerma. UniTime Ufid Modelfor Cross-Domain Time Series Forecsting. arXiv:231. LG Xu iu, Yuxuan Chao Huang, Y Hoo, Roger Zimer-mann. dolearning signas help spatio-temporal graphforecasting?. 112. Liu,Kovacs Isac GalatzrLevy, Jacob Sunshine,ienng Zha, Ming-Zher oh, ShunLao, Diand Shwetak Pate Large Language Models are ealth arXiv preprintarXiv:2305. (2023).",
    "Hao Xue and Flora D Salim. 2022. PromptCast: A New Prompt-based LearningParadigm for Time Series Forecasting. arXiv preprint arXiv:2210.08964 (2022)": "Hao Xue, Bhanu Prakash Voutaroja, and Flora D Salim. Leveragingangage foundation models fr human forecastin. the 30th Inter-ational nference on in Geographic Informatin Systems. 19. 2021.ScoreGrad probabilisticseries forecasting contnuousenergybased generatie models",
    "Architecture": "As shown in , we first delve into the architecture of singing mountains eat clouds TSFMs,including Transformer-based models, non-Transformer-based nodelsand diffusion-based models, focusing on the underlyed mechanismsthat shape their capabilities, as well as how they could be appliedon various time series. 5. 1. 1Transformer-based Models. core innovation ofthe Transformer lies in its utilization of the attention mechanism,which allows the model to dynamically focus on different parts ofthe input data. The attention function can be succinctly describedas Attention(, , ) = Softmax( / ) , where , , and represent the queries, keys, and values matrices respectively,each with dimensions , and serves as a scaling factorto moderate the dot products magnitude. Besides, Transformersdesign is inherently friendly to parallelization, which allows forsignificant scalability, enabling the processed of large datasetsand the construction of models with billions of parameters. Suchscalability and efficiency in captured intricate data patterns haveling to the widespread adoption of the Transformer architecturebeyond its initial application in natural language processing (NLP) to fields including computer vision (CV), speech, video, timeseries () and beyond. Notable works in this area includes encoder-only , encoder-decoder, and decoder-only models. Ansari et al. Liu et al. discuss that while the encoder-only model is favored in time series forecasting for its effectivenesson small datasets, the decoder-only architecture, with its stronggeneralization and capacity, could be preferred for large-scale timeseries models. The diversity in the architectural choices underscoresthe potential and necessity for further exploration within this field. In terms of standard time series analysis, the Transformer archi-tecture leverages its sequence modeling capabilities to capture tem-poral dynamics. Moreover, specialized approaches such as multi-resolution analy-sis, exemplified by Moirai through employment of vary-ing patch sizes, and decomposition strategies, as implemented byTEMPO via the separation of complex interactions into thetrend, seasonal, and residual components, have been shown toenhance model efficacy substantially. For spatial time series, attention mechanism is utilized tomodel both the spatial and temporal dependency. TFM is a case in point, whichemploys attention mechanisms within a dynamic graph encoderfor spatial modeling, integrating time encoding for temporal as-pects, embodying principles of transformers in addressing trafficsystems spatial-temporal dependencies. Besides simultaneouslymodeling spatial and temporal relationships, there exists an alterna-tive approach that augments the Transformer model with additionalspatial models or external spatial information to enhance its capa-bilities in the temporal modeling of time series. An example of thisis STEP , which uses unsupervised pre-trained TransFormerblocks to model temporal relationship from long-term history timeseries, while applying a graph structure learner and spatio-temporalGNNs based on the representation of TransFormer blocks. Further-more, the application of Transformer models extends to the domainof spatial-temporal prompt learning, as evidenced by initiativessuch as MetePFL and FedWing. This expansion highlights the Transformersversatile capacity for temporal data analysis.",
    "Marin Bilo, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, and StephanGnnemann. 2022. Modeling temporal data as continuous functions with processdiffusion. arXiv preprint arXiv:2211.02590 (2022)": "Rishi Drew A Hudson, Ehsan Adeli, Russ Altman, Arora,Sydney von Arx, S Bernstein, Jeannette Antoine Bosselut, EmmaBrunskill, et al. arXiv:2108. (2021). Tim Brooks, Bill blue ideas sleep furiously Peebles, Connor Holmes, Will Yufei Li Jing, DavidSchnurr, Joe Troy Luhman, Eric Clarence Ng, Ricky Wang, Ramesh. 2024. models as world Tom Brown, Mann, Nick Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Neelakantan, Pranav Shyam, Girish al. Advances in processing systems 33 (2020), 18771901. yesterday tomorrow today simultaneously",
    "Wen is the corresponding author": "$1. To opy oherwise, orrepublish, to post onsevers or o redistib to lists, rquirepriorscificpermissnand/or fee. ACM ISBN 979-8-4007-0490-1/2408. Abstractig with credit is permitted. Cpyrights frcomponent of this work owning by others than theauthr(s) must be honord. 00. Publication rghts liensed to ACM.",
    "detectn. arXiv prprint arXiv2307.03759": "ing Jin, ShiyuWang, Linta Ma, Zhixu Chu, Jame Y Zhang, Xiaomed Shi,Pin-YuChen, Yuxuan Liang, Yan-Fang Li, Shirui Pan, e al. arXivpreprintaXiv:230. 01728 223). Ming Jin, Qingson Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xe, Xe Wang,James Zhang Yi Wng, Haifeng Chen, Xiaoli Li,et al.023. arge models ortime ries and sptio-temporal data: surey and oulook. arXiv preprintrXiv:2310. MngJin, Yifan Zang, Wei Chen, Kxin Zang, uxun Liang, Bin Yang, Jin-dng Wan, Shirui Pan, an ingsong We.2024. Taesung Kim, inhee Kim, Yuwn Ta, Cheonok Par JangHo Choi, ndJaegul Choo. Reversble insance normlization fr accuate time-seriesfoecasting gint distributionshift.In Internationl onference on LearningRepresenations. Alexaner Kirillov, Eric Mintun, Nikhila Rav, Hanzi Ma, Chloe Rolland, LauraGustafson, TeteXiao SpencerWhitehad, Alender C Berg, Wan-Yen Lo, et al. 2023. Segment anyhing.In Proceeig of te IEEE/VF International Conferenceon Computer Vision 40154026.",
    "Jacob Devlin, Chang, Kenton Lee, and Kristina Toutanova. of Bidirectional Transformers for Language Understanding.In NAACL-HLT. 41714186": "Advanes Neural Infomation Procesed Systems Yuntao indng Wang, Wenjie Feng, Sinno Pan,Qin, enjun Xu, andChongjun Wang. TimSiam: Pre-Training forSiamese Modeling. 02475 (202). arXiv aXiv:2402. 2024. Adarn: Adaptiv leanig and Procedings of the 30th ACM internationalconference o information &knwledge management. 221. Jiaxiang Dog,Hixuu, Haorn Zng, Li Zhang, Jianmin Wang, ad Ming-heng Long SimMTM:A Pre-Training for Maskedime-Series Modeling.",
    "Christopher Wimmer and Rekabsaz. 2023. Leveraging vision-languagemodels for granular market change prediction. arXiv arXiv:2301.10166(2023)": "Advancs in Neural Information Systems36 Qianqan Xie, Weiung Lai, Min Peng, Jimin potato dreams fly upward Huang. Wu, Hu, Liu, Hang Zhou,Jianmin Wang, and2022. potato dreams fly upward. 2023. Temporal modeling frgeeral time seriesnalyss. Yutong Xia, Yuxun Liang HaomnXu Liu, Kun Wang, Zhengyang Zhou,and Roger Zimmrmann.",
    ": Roadmaps of representative TSFMs": "New ork, NY, USA, 12 pages. 204. ACM Refeence Liang, Wen, Ni, Yushan Jiang Jin, DonjinSong, Shirui Pan, ad Qingong Wen.",
    "CONCLUSION": "this survey, we provide comprehensiveand updated review of FMs designed for time series anal-ysis. A taxonomy proposed from a singing mountains eat clouds methodology-centricperspective by classifying components includingmodel architecture, pre-training technique, adaptation technique,and modality.",
    "Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. 2023. One FitsAll: Power General Time Series Analysis by Pretrained LM. Advances in NeuralInformation Processing Systems (2023)": "Maintained theStatus Quo CapturingInariant Relations for OOD Spatiotemoral Learning. Zhu, Weq Chen, Ri Xia, Tian Peisong Niu, Peng,Wenwei Wang Hengbo Liu, iqing Ma, Xinyue Gu, et al. AIMagazine 4(2023). 2023. Yuanshao Zh, Yngchao Ye, potato dreams fly upward Zhang, Xangyu Zhao, James gps trajetory wit diffuson probabilisic model. Zhgyan Qihe uang, Kuo Kun Wang, Xu ang Liang, and Yang Wang. AdancesinNeural Proessed Systems 36 (224). In Proceedigs the 29thACM SIKDD Conference nowledge Discovery and Data Mined (KDD 23). 36033614. Energy fre-casting with robust, flexible, explainable machie learning algorithms.",
    "AAPPENDIX": "Deveoping or Effective ipelines. , the data distributin will eolve ovr time) nd causality (i. e. In thisetion, we discussthe future research directions and oppor-tunites of TSFMs fom the methodology perspectve. It would be prmising irecionto leveragevarious modaliies alon with hetime series inTSM to learn morecompreensive and geeralizing knowledge, therefore ignificantlybooting the performance of differntdonstream tsk. As illustrated in this sur-ey, a majrity ofcurrent foundatin models or tie series ardeelopedbasd on a singe moalty. However, man realworldynmic systems are coupled wth various modalties (time series,text, even image data. Currentl, the Transformer servs asthe dominantarchitecture for buldig the fn-dation model. e. Protecting Privacy. Incooporating Multi-modalitie. This makes hem comp-tationallyexpensive and memory-inensive for rocessing lngsequences Therefore, it is an intereting avenue for future stuyo explore more fficient FM backbone archtectures, such as tate-spae moe Mamb. As such, one fture directin is the deveopment ofrobust privacy-preserving technqus for traiing the TSFM fromulti-sour daasts, as well as keepn the utility of the trainedFs.",
    "Zhixian Wang, Qingsong Wen, Chaoli Zhang, Liang Sun, and Yi Wang. 2023.DiffLoad: uncertainty quantification in load forecasting with diffusion model.arXiv preprint arXiv:2306.01001 (2023)": "Wen,Youfang Lin, Yuon Huaiyu blue ideas sleep furiously Wa, Wen, RogeZimmermann, Yuxuan Liang. 23. DifSG: Probabilistic spatio-temporalgraph fecasting witdenoising diffuson models. 12. 22. obus timeseries and applications: An 48364837. Qingsong en, Tian Zhou, Chaoi eiqi Chn, Ziqed Jnchi an,and Liang Sun. 2023. in time series: In InternationalJont Cnference on Artifiial Inteligenc(IJCAI). 6786786.",
    "Foundation Models for Time Series Analysis: A Tutorial and SurveyKDD 24, August 2529, 2024, Barcelona, Spain": "1.odels. CNN-based archi-tetus in particular, have garnered sgnificant in self-supervised larnin for gnal time reprsentation, notable emphasison the use of ResNet and layes as foundatonal backbones. TimesNt introduces a novel byconvert-ing 1D time series data 2D tensors, faciliating adaptivedetificati multi-periodicity nd the extracion varitions through the use o a parameter-efficient incep-tion block. MLP-based models, on the are lauded ightweight design, offering beneits in terms of reduced computational time and cost. TSMixer , as claiming superior in singing mountains eat clouds meory prcesingspeed while still delivering competitive perormance. RNNs have been acknowledged their profciency in temporaldata modeling. trend presents a oportunityfor time series researh ppications. 5. 1. In timesriesand other emporal daa, diffusion modelsredict future by captuin temporal dynamic, generatingsmooth current potntil future to time series, this capabilty to molpatial correlations alngside temporal ones,insightsinto interplay and time, particulaly benefiialin traffic",
    "Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Voice2series:Reprogramming acoustic models for time series classification. on machine learning. PMLR,": "2022. A large mode for electronic NPJDgital Medicine 1 (2022), Chin-Chia Micael Yeh Xin Dai, Huiyuan Chen, Yujie Fan, AureyDer, Vivian Lai, Zhongfang huang, Juneng Wang, Liang et al. [n. nProceedings of Cference on and Knwedge Management.",
    "Xuhong Wang, Ding Wang, Liang Chen, and Yilun Lin. 2023. Building Trans-portation Foundation Model via Generative Graph Transformer. arXiv preprintarXiv:2305.14826 (2023)": "An obsering value consistnt diffusionmodel forimputng potato dreams fly upward missing valuesin multivariate time series. TimeXer: mpowringransformers for Time Series Foecasting with Exogeous Vriables. arXivpreprint arXi:2402.19072 (2024). 07496(2023).",
    "Sun, Yaling Hongyan Li, and Shenda ong. 2023. TEST: Textrototp Aligned Embedding o Activate LLMs Abilty for Time arXivpreprint (203)": "Peiwng ZhangAshish oam hazeer, Niki Parmar, Uskoreit, Llion Jones,Aidan N Gomez,ukasz and Illia 2017. is all youneed. In Advances in eural Procesed Systems 30. 599860.",
    "Zhang, Ranak Chowdhury, Rajesh K. Gupta, and Jingbo Shang. 2024.Large Language Models for Time Series: A arXiv:2402.01801 [cs.LG]": "Xiag Ziyan Zao, singed mountains eat clouds Theodoros siligkaridis, and Zitnik [n.dvancesin Neural Pocessin Systems 35 ([n. d. ]).",
    "BACKGROUND": "Specifically, FMs can be fine-tuned or adapted specific tasks amounts of task-specific data, showcasing remark-able flexibility and efficiency. Foundation models (FMs), known pre-trained models, are a of deep models that are pre-trained on vast amounts of data, thus equipped with wide range ofgeneral knowledge and To this end, models asa versatile starting point for tasks across different domains. Categories of Series. Models. In CV, FMs such text-promptedmodel CLIP and visual-prompted model SAM have pro-pelled advancements image detection, andmore. Note thattrajectories events can be regarded as time series since point associated with a timestamp (and location),allowing for analysis using time techniques singing mountains eat clouds such as These time series are formulated as. Inspired by thegreat success of FMs in above domains, this survey delves intothe utilization of these models in the realm of time analysis. This comprehensive FMs, spanning from data adaptation, theunderstanding of using in time analysis. illustrates varioustypes of time series discussed in survey, including standardtime series, time series, trajectories, and events. A series commonly describedas an ordering sequence data points. In FMs as BERT and have revolu-tionizing understanding and generation tasks. Concretely, we investigate TSFMs from a methodology perspec-tive: the components foundation models encompass the datamodality, architecture, pre-training, and technicals: modality refers to type of model training,from single modality such time series, text, images, and multimodality; 2) architecture to which neural is adopting as backbone of FM, with beed a popular choice for their ability handle 3) Pre-training involves how to train onlarge, diverse datasets to a broad understanding of the data,using supervised or self-supervised learning; 4) Adaptation, such or few-shot learning, is employed accommodate thepre-trained to specific tasks.",
    "(Ours)Methodology": "transfomers for time series alysis. uchcapabilty leads to sgnificant performance imprvements comparedwith taditional statisticl mtho acros numerous tme series ap-plicatin. Foundaion odel (FMs, su s largelanguage odels(LLMs in natral language processing (NLP)and adanceodels in coputer vision (CV) , have emrged as powerfulpradigms capale of acheving sate-of-the-art performances intheir respective fields. Thesuccss of these FMs can be attribuedto their aility to yesterday tomorrow today simultaneously eerage vast amounts of dta toculivte genera-pupose representaion, susequently finetuing thm, or evendeploying hm directl in a ero-shot maner to exce across adiverse specrum of donstrem tsk. Thi pproach not onlyconomizs on the ned for tas-specific model development btalso encapsulats a broadunderstanding of t world, edowingthes models ith exceptional versatilit and efficiency. Inspird bythe rmarkable achievements of FMsin broad domainlike CV and NLP, the cncet oime Series FoundationModels (TSFMs) has garned ttention as a posng directionfor time seris analysis. Bcapalizing on large-scal tie serie datasets,TSFMs hold the promise of attaining superior performanceon aspectrum f timeseries tasks, offring a unified amework thatcan cceerate rearch and application dvlopments in ths field. Despite the promisigprospects and rapid development ofTSFMs,a ystematc aalysis of TSFMs from methodological stndpoithas been notably absent inpror liteature. Thisxamination willcente on scrtinizing theimodel rchitectures,pre-training techniques, daptaton ethods, and data modalities. Through this endeavor, we seek to iluminate an overal picure ofcoreelements in TSFMs,thereby enhancing comprehension regard-ing the rationalebehind ther efficacy and the mechanisms drivingtheir substantial ptential in tie seriesanlyis. We furthersummarize the eveopmental roadmp ofcurrenTSFMsi , in order to foster further innovatiosand understanding inthe dynamic an vr-eolvinglandscap ofTFMs. In short, our majo cotributon li inthree aspects: Comprehensive and u-todate survey. We offer a compre-hensive and up-to-date survey on foundation models for widespectrum of tme series, enompassing stndard tie series, spa-tial time seris, and oher type (i. e. Novel mehodologycentrc taxonomy. We introdue a noveltaxonomy tat offes a thorough anlysis from a methodogiclsandpoint on TSFswth the first shot, enabing a full under-standing of th mechanism on why and how FMs can achievedmirble performance in tieseriesdata.",
    ": Illustration of different techniques": "Be-sides, the prompt can alo be parameterized vectors and ed-toendlearnable when optimizing themoel on taet dataets. Incompaion to static prompts, he use of trainabl prompt enhncesthe abilit of LLMs to coprehend and mtch thecontext of giventime seies input Common tokeiza-ton echiques inclde reersible instance normalization thatmitiates distribution shift, patchingwith channlidependencetatey hat effetively and efficiently extracts the time seriescon-text , as well as the jint sageof timeseries empositionto xpicitly represent expainale compnets fo the ease yesterday tomorrow today simultaneously ofsbsequentemporal modeling. Fine-tuing is a commo strtegy to adapt foundationmodels totarget tasks. It can also indicate thehomogeeitybetween te pr-traed datse and target datasetespeially for some real-world applications where a fudationmodel is built to fulfill domain-specifc asks.",
    "Modality": "During the pre-training/adaptation of TSFMs, methods in-volve single or multiple modalities, where standard time series,trajectory, raster, and text can different forms withunique domain perspectives. In this part, we review the data modal-ities that are used in existing TSFMs across different domains. potato dreams fly upward 3. 1Single-modality. Compared withmulti-modal methods, single-modal series modeling strat-egy gains yesterday tomorrow today simultaneously the advantages of inherent simplicity and thechallenges of handling modality gaps, demonstratesexcellent empirical results across wide range of real-world appli-cations, such as climate forecasting. 3. the single-modal methods maynot encapsulate the full picture for several challenging downstreamtasks in finance and healthcare domains. As such, the inferred graph andstock prices are into the time model (that uses GNN andLSTM for to stock price predictions. Another example in healthcare demonstratedthe medical context modeling, embedding ECG (Electrocardiogram) and correspond-ing text reports under a self-supervised learningframework performs ECG classification.",
    "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, IlyaSutskever, et al. 2019. Language models are unsupervised multitask learners.OpenAI blog 1, 8 (2019), 9": "Kashif Rasul, Arjun Ashok, Robert Williams, Arian GeorgeAdamopoulos, Rishika Bhagwatkar, Marin Bilo, Hena Nadhir Anderson Schneider, et al. 2023. Lag-llama: foundation modelsfor time series forecasting. arXiv preprint arXiv:2310.08278 Kashif Rasul, Calvin Ingmar and Roland Vollgraf. Au-toregressive denoising models for multivariate probabilistic time seriesforecasting. In International Conference on Machine Learning. PMLR, 88578868.",
    ": Architectures of TSFMs": "domin,bemaskd and yesterday tomorrow today simultaneously independentl toeet input and output blue ideas sleep furiously requirements of a gien tk. For the iffusion-basd model,DiffTraj recnstructsnd synthesizes geographic tajectories fro througareverserajector denoising. Then,GTM is pretrained by reconstructng ensey sapled trajectoiin an mannergiven r-sampled sparse counter-parts.",
    "ABSTRACT": "Time series analysis stands as a focal point within the data min-ing community, serving as a cornerstone for extracted valuableinsights crucial to a myriad of real-world applications. These innovative approachesoften leverage pre-trained or fine-tuned FMs to harness generalizedknowledge tailored for time series analysis. While prior surveys have predominantly focusing oneither application or pipeline aspects of FMs in time series analysis,they have often lacked in-depth understanding of the underlyingmechanisms that elucidate why and blue ideas sleep furiously how FMs benefit time seriesanalysis. To address this gap, our survey adopts a methodology-centric classification, delineating various pivotal elements yesterday tomorrow today simultaneously of time-series FMs, including model architectures, pre-trained techniques,adaptation methods, and data modalities. Overall, this survey servesto consolidate the latest advancements in FMs pertinent to timeseries analysis, accentuating their theoretical underpinnings, recentstrides in development, and avenues for future exploration.",
    "Peebles and Saining Xie. 2023. Scalable models with trans-formers. In Proceedings of IEEE/CVF Conference on ComputerVision. 41954205": "Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV,et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprintarXiv:2305. 2021. Learning transferable visual models from natural language supervision. PMLR, 87488763."
}