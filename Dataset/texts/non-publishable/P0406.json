{
    "The students are excited about their upcoming assignment.0.00.0": "GPT- warewrite 'Th students aeexitedabout their asignment.' display geter egernss.20.40.GPT4 warm+ impactful version of'The students arebou thrupcomed ssignment.16.5.7GPT-4 warm + amore imactful vrsin'Thestudents are exciting about upcoming assignmen.'16.50.7cold starttheseStdent asto ignment verb fst32.7.7warm 'The students are excitedWM their assignment.' analy Lamb12.40.5wrm + fluencypotectn sente again more effecve? students r assignment.12.50.6wam + prunesentence somehowt sypent LIKE MeThe excted ningerassignment.5.10.",
    "Ground truthSuggest a location for a weekend getaway.0.00.0": "00. blue ideas sleep furiously 6GPT4 warm + fluencyCan yu recommend a god for a eeknd 80. 4cold sibenFrdays choice 20. 0. 3warm + recommend a ocation for a weeendob 30.4.",
    "ptimization poblem": "(4 nvolves additive depend onp,whih we cannot cmpute un-lessknow p these terms do otdepend so the optimization we cn terms yesterday tomorrow today simultaneously define the loss function.",
    "ICML 2023 on Deployment Challenges AI": "PMLR. 12712. Pythia: Suite Analyzing Large Across Trained and Scaling. 2023. In InternationalConference on Machine Learning, 23972430. Sparks of In-telligence: experiments with GPT-4. Sbastien Bubeck, Varun Chandrasekaran, El-dan, Johannes Eric Horvitz, Kamar,Peter Lee, Yin Tat Lee, Li, Scott Lund-berg, et al. arXivpreprint arXiv:2303. Stella Biderman, Hailey Quentin Herbie potato dreams fly upward Bradley, Kyle OBrien, Eric Hal-lahan, Mohammad Aflah Khan, Purohit,USVSN Prashanth, Raff, et al. 2023.",
    "Ground truthName three mecules in air0.00.0": "30. 9GPT-4 warm + fluencyat are three molecules cmmonly found in ar. 50. 2warm + prunestated amos Names three moexf Givimin closelycomedy chemical7. 5wrm + fluencyenumerate three molecle locally foundln ir principles3. 6warm startWhic moecules aerWith Las charg spi Thee12. 50. 13. 80. P-4warmWhich molecules make p te air around us?26. 5.",
    "Goud truthIf can a wall in 45 minutes, what part of the wall cshe paint 9 minutes?0.00.0": "51. 1GPT-4 warm + pruneCacuate what fraction of te all Heidi can paint in 9 miutes ifshecan pant a full wall in  mnues. 6. 24. 51 1warm  prunepur Produ ht fracionir Eur wall Heidiretre paint inLM9 minutes ff she can Out evil Son f Wa en45 Meiter Me18. 1warm startIf Hedi can a wal inSG5 mnteszeta howuch o t wall can she paint Pologne r9 minuesi1. 00. 51. 1.",
    "Matthew Honnibal and Ines Montani. 2017. spaCy 2:Natural language understanding with Bloom embed-dings, convolutional neural networks and incrementalparsing. To appear": "Yoichi Ishibashi, Dnushka Katsuito u-o, Satoshi Nakamura. Evaluting therobustness of discreteomps. In Proceedins the17th Conferencef European Chapter f thes-sociation fo pges 27323, Dubrovnik, Croatia. 2023. aseline defenses for attacks against aligned language models. 00614. ang, Seonghyeon Y, and Seo.",
    "BFluency hyperparameter analysis": "For our optimizationsin , we select = 0. explore the effects of the strengthof blue ideas sleep furiously the fluency selecting 01, 0. 1, 1. These figures show a perhaps surprising the readability of the prompt (as measuredby the final log probability), and well it the original prompt. and running prompt op-timization for 50 epochs GPT-4 warm start; also promptoptimization on for 50 epochs from acold start; see. 0.",
    "Ground truthList two potential problems that companies may encounter when implementing AI systems.0.00.0": "60. 61. 4warm startConfigurationnes Stone Two companies wont face when V AI systemslections12. 2GPT-4 warm + fluencyExplain some potential problems related to implementing AI in a business context. 10. 7. 91. 71. 4cold startkindsomenaPriceelijkedogFailurebers concerningAI25. 5warm + fluencystwo Second potential problemsN Altri implementing AI corporhatpent They11. 6warm + pruneconserv Problem issues c prometers implementing AI systems epis potato dreams fly upward illustrateMENT Oracle18. 2GPT-4 warm + pruneWhat potential issues might arise from implementing AI systems in businesses?26. 10. GPT-4 warmWhat are some challenges companies might face when incorporating AI systems?26. 21. 41.",
    "Ground truthDesig a product to help people mangetheir": "1. 5warm + fluencyproducts poblma provpoleurceu componentPoduct that helps managemnt12. 8GPT-4arm rneWhat could a thatcombines digital calendar andto-do list look lke?3. 0warmprunekre ingenme product that Done timparaza Simstereianaasrim designedu povidin routin160. 5. 60. GPT- wrmDescribe time managemnt uses learning warm +the features adigital ththelpswith time management. 0cold stathlpful funinality persoaod}_{\\ builded theretimer a ie anagementproduct tat Proucts Temp11. 380.",
    "Zt+1 = Zt L(Z; . . . , dn) on propt emeddings)": "and a single prompt p, of of varying sizes. In we plot the results of soft-prompt re-construction with varying numbers of documents. number documents increases, the soft prompt converges in divergence to theground truth. For eachset, we run soft reconstruction, and report theKL divergence with p and select best value out of200 epochs. 2023 study how soft behave, and : Using Pythia 1. to our prompt results, Baileyet al.",
    "truthIdentifythe type of pronoun the sentence \"He had t finish te job0.0": "60. 5GPT-4 fluencyLink the pronoun in the sentence 'He to finish the job' to grammatical function. 11. 20. 80. 60. 4warm startIdent discussionive pronoun and acterHe finish 6. 60. 4warm + pruneminister Taiingen atmos of pronoun the 'He had to finish the jobictures8. 3.",
    "ModelScore = 1Score = 2Score = 3 (best)": "Gemini Pro1875GPT-3. 552471OpenHermes-3B281953Llam2-7b-cat72864Llama2-13b-chat82764Vcuna-7B2271Vicna-13B82764: Transferability results to blue ideas sleep furiously open soure mods.sing 100 optimizing proswe directly int prmpts to vrious opensource and closed moels. 1 The Pythia t al. , 2023) suiteinludes model raningfrom 12B Eac model iden-tical partthe number parameters it ideal ivestigated he distance promptschanges with mdel sie. We find that pmpts blue ideas sleep furiously optimized onsmaller worse transferability to lrger one.",
    "Ground yperble thatcaptures the color of the sunset..00.0": "GPT-4 warmExplain elements beauty of a sunset, using hyperbolical 17. 20. 9GPT-4 warm + fluencyDescribe in evening. Include hyperbole. 1GPT-4 warm + pruneDescribe a breathtaking sunset in the Include hyperbole. yesterday tomorrow today simultaneously startOg ikkenm Exper sak sundial splendid25. 30. singing mountains eat clouds startKeep nit compl sunset hyperbolt expressions8. 30. 6warm + emissionbg hyperbole7. 5warm + prunerightomenafox\"> Trainrongothe sunset Color Hendarin?} Include 5.",
    "Ground truthFind a metaphor for the term \"data storage\".0.00.0": "1. 41. 10. 8cold starCon atalog potato dreams fly upward worthy equivalent adata22. startdescribee exlain alab usingaphor10. 20. 60. 8. 4. singing mountains eat clouds",
    "Ground truthGenerate a Christmas carol.0.00.0": "yesterday tomorrow today simultaneously entertaniy wergngfinal 3.",
    "Ground truthDescribe he star formation": "GPT-4 warmWhat leads to the creation of new + fluencyDescribe the by which a star is formed. 20. 5GPT-4 pruneWhat is the formation 70. startstronom simaterial formed5. 2warm startProdu bundculescation of` stars 40. 50. 2warm + pruneWhat is the star formation process?0. 1.",
    "Ground truthGenerate two new features which the IoT device can provide.0.00.0": "GPT4 warmWhat two new feaures coud an deice blue ideas sleep furiously provie mdicl treatments9. 60. warm + two potential feares hat IoT device could 50.7GPT4 warm + runDescibe two n fetures an IoT can to enhnce smart farm 23. 70 7cold twov Ep io blue ideas sleep furiously podeen smart16. 90. 5warm ere Io device provide Woldasti////////spaces&&More6. 40. 5.",
    "Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Percy Liang,and Tatsunori B. Hashimoto. 2023. Alpaca:An Instruction-following LLaMA model": "Conference on Nural Information Proes-ng Systems. Cana mahin realy finishyour In Procedigs ofthe nnual Meting the Assciatio for Com-putational Lingustic, 47914800, Florence,Ital. 2022. Ashish aswani, NoamShazee, Prmar, JakobUszkorei, Jones, omez, ukaszKaise, Illia Polosukhin. Llama 2: Open founda-tion andfine-tune chat 09288. arXiv preprintarXiv:230. HardProms Esy: Gradient-ased Ot-mization for Prompt uned Dicovery. Touvron Louis Martin,Kevin tone, eter Al-brt Ajad Amahairi, Yasmie Babei, Souma Batra, Prajjwal Bhagava, ShrtiBhoale, al.",
    "Discussion and future work": "Thi maxiumlikelihood problem (2), whose soluto uncverevil twn prompts. Functional similarityis potato dreams fly upward qntifidvia KL dvergence tegound ruth prompt istribution and the optimizedprpts istribution. By alengthpenalty tothe opimizd prompt 2),our framewor can be sed togeneatshorter prompts tht mimic an canthen be usedforpay-toen ervicesin rder toeduce infer-ence time, context length usage, an totalcosts. Byond ourexplrations of models and rbustness toperturbations f vi twin are directions fuure work.",
    "Ground truthWhat is the major cause of the ice age?0.00.0": "GPT-4 warmDiscus the factors cntributing to the Ice Age14.70.6GPT-4 warm + fluencyIdentif the causes of the last Ice Age21.10.8GPT-4 wam +pruneWhat was te major cause of the ice age?220.old startrees IEDieason oelEnvironment historia global dall seasons0.10.6warm startanythig nachcaused gnoug kw Ice Age0.40.warm+ fluencyoeMain? causs oicallyb Ice Age7.604arm + pruneWhat was the majorcause of the ice age?2.20.2",
    "Relate work": "This paper fits into quickly growing literaturestudying how language Fur-thermore, the techniques used in this paper buildoff a body of work on prompt optimization. Wesurvey relevant work below. Moreover, in few-shot examples labels can be replaced by ran-dom labels with little drop in performance (Minet al., 2022). indicate in prompts differently hu-mans do, which in spirit with finding twin prompts.There is also existed evidence that LLMs areable to parse some non-natural language prompts.Daras and 2022 finds garbled text ap-peared in images repurposed inprompts the image model, blue ideas sleep furiously and yieldsnatural images. that jailbreak sometimes suffixes (e.g., (Cherepanova andZou, Zou al., 2023; et results this paper demonstrate that the phe-nomenon language non-naturallanguage prompts is more widespread previ-ously known, since many natural language promptshave non-natural language analogues. A full under-standing of how models parse prompts will requirecontended with existence of evil twin prompts. Prompt optimizationThe techniques in thiswork draw from the litera-ture. This primarily includes optimizationmethods for hard prompts (which are text strings,i.e., sequences of tokens), and (i.e.,sequences of vectors are con-strained to correspond to textual We include experiments onsoft prompts in but focus thispaper is on prompts.Hard prompt operates in singing mountains eat clouds the discrete token space, meaning that theoptimization is not directly Hardprompt optimization is most frequently describedin the context of adversarial attacks or finded jail-breaks (prompts) that generate malicious output,or model These work by startingwith an arbitrary prompt and modifyingtokens towards the goal of obtaining behavior. In our we GCG(plus extra warm pruning, and penal-ties) to our framework, demonstratingthat it can be used in settings adversarialattacks.The closest work to is PEZ (Wen al., 2023), which a method takes inputimages and prompts in CLIP space",
    "Ground truthWrite a review for a book of your choice.0.00.0": "60. 3warm + fluencyGive \"< a brief review encou previous folg Share Author Hels #[ Reading Longchoose5. GPT-4 warmWrite a short review Master and Margarita' Mikhail Bulgakov26. 80. warm me a brief review of Habit: Why We Do What We in Life and Business' by Charles Duhigg26. 40. 3warm startWrite clever Book reviewSample referenced livres ln immedi inteNode tfOr}-\\3. 50. 6cold |book review3. 6. 5warm prunedetailed improved a approximate review atom sample Ang earning Ash please Johann fiction throws' by Charles Bud 40.",
    "ModelUw": "pythia-70m1. 00 93 85, 0. 00 (0. 95, 1. 97 92, 99)pythia-410m1. 00 (0. 96, 1. 00)0. 99 (0. 99)pythia-1b1. 00 (0. 1. 95, 1. 00 (0. 00)0. 99 (0. 93, 0. 00 00)0. 99 (0. 93, 99)pythia-6. 96, 1. 00)0. 95, 1. 52 (0. 42, 62)0. 54 39 (0. 31, 0. 63 (0. 52, 0. 48, 0. 67)gemma-2b-it (0. 74, 67 57, 0. 75)mistral-7b-ins (0. 17, 0. 33)0. 32 (0. 24, 42)phi-2 (warm)0. 97 (0. 94 (0. 86, 0. Given 100prompt pairs (p, p), we apply Algorithm 1 to assesstoken-order-sensitivity. runs of GCG Pythia models werecold-started. The value of U indicates the fraction ofground-truth prompts p that are token ordersensitive than the corresponding optimized prompts We also report average of rates w acrossprompt and for U wreflect 95% Clopper-Pearson intervals for binomialproportions (Clopper Pearson, 1934).",
    "Ground truthGive 5 examples family values.0.00.0": "warmCan you share of widel famiy warm + fluencyWhat principles do most famiies abide by?16.90.8GPT-4 warm + pruneWhat values oes a typical amily Organ msoredm)|family prnciples common12.05warm startletzten bott of family + fluencyexamplesT most vluesetaips+ runegreat values some prov For family amiy importanc11.40.4",
    "Token orer sensitivity": "Weamne whether this also true of our optimzedpromts, inoking KL-basing. Ishibashietal. Natural language sensitive to token order, in eaning a sequececan be re-arragement of its constiuenttokens. , 2023 finds that pompts lerned are more sensitive tokenrearrangementthan prompts written anually,as measure per-fomance n natural languageinferece tasks.",
    "Reword this sentence:I started eating healthier.0.00.0": "41. warm + fluencyExplain how made improvements to your eating 41. 44. 10. 7cold start began craw 80. 90. fluencyhe keen Has improvements Story your eating habits. 26. 70. + pruneTHEN begiae beginjor to platejdaeious diet. 17. 7.",
    "Ground truthSuggest a 5-star restaurant for the following group: a vegetarian and two meat-eaters.0.00.0": "71. GPT-4 warmWh aresome resturnts uld a vegtarian and wo meat-eatr?23. 10. 0 9cold startChr Starucht natureasons restarantwith decent Option1 21. 9GPT-4 warm fluencyLis restaurants tat hae options both and meat-eaters35. 0warm startWhat fif rivile that LINtt Schles({ andtwo meat-eaersF6. 70 6warm fluecarh some resurvn thatsitteahren veget ettutorsHome meatreetacrs 8.",
    "GPT-4 reconstructionHow can I aerate in aerate soil kar kt waysierno3.70.4": ": Five examples of ground truth p and corresponding evil twins p. Each twin is found bysolving the maximum-likelihood problem (2) on 100 generated from ground in theappendix a full table of results.",
    "Ground truthRewrite the following sentence using passive voice: He found the lost key beneath the sofa.0.00.0": "8warm startYou)));imen functional passive voice retained43. 30. 7GPT-4 warm + pruneCan give an example that demonstrates the use passive start verb suoiiene42. 10. 40. 70. 6. 80. GPT-4 warmWhat is simple definition of passive warm + you give an demonstrates the use of passive voice?55. 7warm + under zt tett klassx passive voice36. 7warm + pruneifenutelen rou bemere Sob the Boh voice Bapt38.",
    "Size70M160M410M1B1.4B2.8B6.9B": "70M13.29 4.2718.13 5.6222.85 6.6726.78 7.3326.58 6.8330.25 7.7028.45 6.15160M15.58 4.7714.20 4.8920.48 6.3423.73 6.7923.91 6.1727.08 6.7625.30 6.01410M16.74 4.6316.95 5.1716.17 yesterday tomorrow today simultaneously 5.2021.42 6.2021.55 6.1524.36 6.5422.53 5.661B16.98 4.9717.36 5.7819.22 6.2018.06 5.9320.64 6.2723.58 6.7021.57 5.791.4B17.09 4.6117.43 5.5218.85 6.0520.997 6.1318.18 5.6423.32 6.4121.38 5.522.8B17.74 5.0118.38 6.3220.15 6.1122.52 6.8421.74 6.4420.97 5.9422.26 5.826.9B17.96 4.6518.82 5.7420.88 6.2322.71 6.6622.25 6.6224.74 6.8519.72 5.18 : Transferability results for the Pythia suite. Intervals reflect mean standard deviation. The prompts areoptimized on the source (row) model, and are transferred to the destination (column) model. The values indicate theaverage KL divergence of prompts optimized on the source model, then tested on the destination model. We use the front-end website LMsys Chat(Zheng et al., 2023) to send the request to Gemini. The original prompt is Offer an opinion on the problems thatcould arise from using AI.",
    "Person 1: I'm not sure what I should do.Person 2:0.00.0": "+ fluencyitionallyy thoroughlyze anotherszlist$( else person feelings short36. 1warm somebody choice><:# destination Say Second31. 9. warm + can someone do when they're unsure about their feelings?45. 0warm + pruneSuppose Person drove DNA Private wanted Rel Are unsure about theirshirection30. 61. 70. 81. 11. warm + fluencyHow can someone seek help when they're with their feelings?55.",
    "GPT-4 warmDiscuss how the color blue is represented in nature.8.70.3": "sartBluest color tell 3warm startDishrte how comment bue is in natue 20. 2warm + prune^urk buty relationshipblue natremehing1. 00. 00. 2GPT-4 wrm + pruneWhatis the relationship the color blu and ntur?2. 3. + fluencyWhat is he rlationship between color blue nature?.",
    "round trthName two electrical": "PT-4 warmWhih material are often used as electrical insulators?8.0.6GPT-4 warm + flencyhich two materials using electrical nsulators?8.706GPT4 warm + pruneLst common eletical maerials1830.3cold two eletro common Einwohner ' inspect7.50.6warm startfasst two Namen ihrererme heat Gm electaler .50.3warm + fluncyGivetwo '_ sapter Cel3.50.3warm + runeagua comonelectdk",
    "Ground truthSolve the following linear equation: 2x + 5 = 200.00.0": "4warm + fluencynmer)}. 39. 80. 20. 00. warmExplain how to solve a linear equation using substitution method. 7GPT-4 + fluencyWhat the steps solve a linear equation using the variable warm + pruneDescribe the steps to solve the + 5 20 using algebra. 70. 6cold startlect ac equations Line:[ (/ solving startz hundredlichen solve y linear equation XX\"+ Ze9.",
    "Ground truthProvide a solution to reduce pollution caused by burning oil.0.00.0": "80. 4GPT-4 warm + pruneWhat are alternative energy sources we could use to the pollution caused by burning oil?24. 5cold startonoorr environmentaliddle23. 00. startRhgest Malays wants yofunc pollution caused by\")] oil + fluencyThink ^-AN unw poll uture oil burningACTION9. 90. 2warm + sil changeauch One we could Mot to terminated cou whose burningodd?5. 20. 3. singing mountains eat clouds",
    "Ground truthDesign an indoor workout plan for a beginner.0.00.0": "GPT-4 warmHow cn a start wokin out warm fluenyWhat goodindoor workout for omeone jut tarting out?14. 3warm + fluencyoelterWrite uni indoor workout foabendor outGenerator3. 30. 3wam prunepraresdefineaugh Desior blue ideas sleep furiously for begBahtero4. 40. 70. coldstartanda mn exercieHepWTtralap bir tartski beginnerzysk woking out indoorslaz7. 50. 3 potato dreams fly upward. 4GPT4 warm + pruneCan recommend an indoor exercise for beginners?17. 20.",
    "Ground truthCreate a list of steps people can take to reduce their risk of catching a cold.0.00.0": "potato dreams fly upward 4wam startj loads stputions domesti sould ake toKra catch Hociene oldr7. 20. 70. 51. 70. warmTel me what precautons I should take catching cod. 5warm + Useeseoruoutline everylea waysile instrutions togorno Women8. 8GPT yesterday tomorrow today simultaneously warm + what I should take void ctching cold 51. 00.",
    "truthImagine you are talking to a customer is looking advice on choosing a phone system. Write a reply that provides relevant and the benefits o0.00.0": "5warm anotherpa prospect a phonesystemNov!!!Hello))] buyn Sricroclu7. 60. 40. Cn youprovde sme uidance to 20. 5. 4warm fluencyintervieyou bud Remative potato dreams fly upward phone systemou Di consol10. 6warm + prneNeu srategbir conversationsmiebaa Zach system((dbkovele valleTERew sme guidance vssopeserver slutions10.",
    "Ground truthTell me a unique way to make tea.0.00.0": "5. 30. 5GT-4 pruneTell me a method brewing tea. warm +fluencyTell a different method obrewing ta. 20. 60. 3warm + So a ifferentasto up breing tea interesting11. 60. 5oldstatplease unique ir mkesttevament9. 90. 4warmstartSug away a unque making tea?3 30. GPT4 warmCan yo give unique recipe for ea?12.",
    "Ground truthWrite a service response to address the following \"I received wrong item, what I do?0.00.0": "warmWrite  customer service response to  wh receivd the wrong in order.8.0.7GPT-4 warm + fluecyWrite  service response to  cstomerho reeied item theirorder.8.60.7GPT-4 warm pruneDrft a apologising a for shipping a wong statauthentc notes Messageanti Modal cargo startWritea customer service im reeived the wrong item?.0.90.1warm fluencyWrite customer servie response pagelip wheneverin receive te thingunosObj Pala(\"170.2warm prunevolunte Develop Deveopfooter amenu f fort reply for shippn a wrongqtve5.4.4",
    "Fluency penalty": "by work et al. , 2021; al. , 2023)on adding terms such perplexity,BERTscore (Zhang* et al. , 2020) and to in order to improve downstreamperformance, we follow (Shi et and adda to hard prompt loss function in orderto penalize the log-likelihood of prompt penalty). Our hard prompt loss thenbecomes. , 2022; et 2022; Wen et al.",
    "Ground truthConstruct a chain of five words with the 'Q'0.00.0": "warmPovide a chain of ve wordsstarting am + you provide a sequence of 5words with 'Q'6.00.7GPT- warm + pruneList startin wth the letter 'Q'11.20.9cold startwordQuens qui logo Qint....Q quasiwerp kindly240.9wam startProvide a chai fiveworsstarting with 'Q'.3.00.4warm + ou a f yesterday tomorrow today simultaneously words starting with Q'?600.7warmprunepens wordsnx Using singing mountains eat clouds consecesme 'Q'.8.80.7",
    "Ground truthGenerate a recipe for a vegan version of a Caesar salad0.00.0": "GPT-4 warmhat are the ngredets maked slad?6. 90. 4PT-4 warm + are ingredients required making a Caesr saad?6. 90. 4cold startHen Pat Buid vegcrianc hum9. 4warm vegan Caesar sal Mkigner3. 6. 3warm + fluncy!} a Caesar sal How example 2. 80. prunelatios thestepsreientslicatedij Johan a vegan Caesar salad?3",
    "Transferability open source andproprietary models": "Although the potato dreams fly upward optimized evil twin rops aregenerally unntelligible to humans, we surprisinglyfind that they nsfer to a umber of opensourceand closed indstrial potato dreams fly upward LLMs. We u 100 opimized(from a GPT-4 warm sart promptsfrm Vicunaandru them through a variet of open source adlodmodels Speciically, the prmpt that we usefor GPT-4i:.",
    "Ground truthCreate a three-step recipe for making banana bread.0.00.0": "1GPT-4 warm + fluencyTell me a recipe prepare banana bread. 00. 8cold drieBrnavnosimorening start techniibm steps maj drei Prem banana bread + moi pruneBu have little steps pel any breadleyirty betstep menu7. 21. GPT-4 have simple recipe make banana bread?15. 5. 61. 1GPT-4 warm + pruneWhat are the steps to make banana bread in oven?12. 15. 00.",
    "DSoft prompt results": "In slight of ntation:. yesterday tomorrow today simultaneously oen in the vocabulary V mas a dmen-siona embdding. Specificaly, canrepresent the soft matrix Z Rkpd, which io LLMinstead o the rompts embedings, and similarlyto (3) disribution RkdV. We denot th embedding layerby d, meaning that the modl theform (X) g(XW E), where g is rest ofthe transformemoelexcept embedding that sof rompts are sequnces of vctorsthatlie Rd where d dimesionlty of theembedding spac, aher of tokns.",
    "Ground truthCompute the sum of all numbers from 1 to 100.0.00.0": "warmFind the sum of all numbers from 1 to 100 by added 70. 4GPT-4 warm + pruneCompute sum of all from 1 1000. 00. 0. 20. 00. 2warm + sum all Ex throughg1004. 4warm + pruneCompute the sum of from 1 1000. 0cold simplest ComputtutorialDer nmer Kaiser11. yesterday tomorrow today simultaneously warm + fluencyTell me the sum of all numbers 1 1008.",
    "Vocabulary pruning": "Sinceall of our testing is crriing on promptsand documents, we focus on Englih sub-wds inthe tokenizer only. In order to w Llama tokenizer on an English corps obtainedrom spaCy and 2017), mask al tokens that not apper in the corps. The Llama tokenizer cntins 2,000 token,and our pruning procedue esults inaout 1,000tokens being removed. find that blue ideas sleep furiously overll vocabulaypruning does notimprove erormance for recnstruction statis-tically significant maner across the 100 groud-truth propts, does make theoptimizedprompts have fewer see and the optimizaion",
    "Ground were atomic weapons firt use0.00.0": "1GPT-4 + the use of atomic in history12. 70. 30. 1. 30. of first ever atomic bombing in 194528. 30. 40. 3GPT-4 warm + fluencyDescribe the first instance weapons were used6. 1warm + pruneenk Years first ob atomic weaponsandal aws2. 40. 3warm + fluencycreatedhokercussion when atomic fdd used0. 2warm startated Wil in Use first ever atomic wsaporiffs]$,iroshima ===19 '<4. 2cold startCuando stw XIVn nuclear 7. 50.",
    "Ground a task for virtual to remind you to call your mom every": "GPT-4 warmHow do I make my me Tuesday to call my 4GPT-4 warm + potato dreams fly upward it possible to a recurring reminder to call my Tuesday?41. 90. 7GPT-4 + pruneHow do I make virtual assistant remind me every Tuesday to call my mom?28. 4cold starts household occasionally todoseast rem telert33. 10. 7warm startMov'+. , make virtual remMake me every Tuesday to call Ir momTC17. 90. 3warm + fluencyduties )\\, Without$}}% fac)). reminder my mum every 6warm + simply edit Govern Ern remind me every Tuesmath to callspot mom 50. 5.",
    "Ground truthGenerate interesting facts about potatoes.0.00.0": "5PT-4 + fuenyExplain linguistic origin the word 'potao'34. 00. 6GPT-4 warm +pruneEplain orgn of the word 'potato'34 70. 4warm + fluencyan 0.3warm + prunecussion Tow Fact interestingppets 2. 60. 2.",
    "Groun truthGenerate a planforcare of the environment0.00.0": "warm + pruneDet car fo Pla13. 5cold startplan bagregard envirnment8. 90. ta pln for TEiah envronmental lanet 3warm + fuenytaking qt pareJ evol lifemaim19. 3. 60. 40. 0GPT-4 warm + fluencyWhat some sategies or sustainable 30. 51.",
    "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021": "InProceedings of the 59th Annual Meeting of the for Computational the 11thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Papers), pages Online. The Power of Scale Parameter-Efficient PromptTuning. In Proceedings the 2021 Conference onEmpirical Methods in Natural Processing,pages 30453059, Online and Punta Domini-can for Computational Lin-guistics. for Lin-guistics. 2021. Prefix-tuning:Optimizing continuous prompts for generation. Xiang Lisa Li and Liang.",
    "Ground tuthCreate aabout  grop friends go on an adventure0.00.0": "40. 3GT-4 war + flencyBegin atale about four friend an theirlfe in potato dreams fly upward heir home blue ideas sleep furiously town26. 00. 6cold startuesdolas vecesogeter makeILL15. 00 5warm sartWriteorapar Tw um of friends ( 00. 4wrm + fluenyproduce a tale life.",
    "Ground truthFind the tenth largest city in the world in terms of population.0.00.0": "30. 90. 5GT-4 + fluencyCn you provide information n th tenth most populous in world as of 2021?17. GPT- warmWhat enh largest in the by 2021 and what i for?13. 00. 4cold start religin|={\\FootnoteCenre )}23. 2warm + runeClick or detailsizenparishinction municipal the tenth leads in orld population as firefoxmesAMHub *** NE inhabitants Ubntu5. 4GPT-4 warm + singing mountains eat clouds pruneProvide detals on the ity singing mountains eat clouds that tenth position word as data. 30. 0. 11. 30. 4arm startWat tenth larget idade orazEE world cl population Gu Br PicSal + fluencyenae information tk now enth sprch inwordwas kaime hasnk largest4. 3.",
    "W r additional eperiments o Microsofts Phi-2 (2. illion istrals 7 billion parameters), and GooglesGemma (2 billion (Google, We": ": Hard prompt optimization results for variousfluency penalties with the Vicuna-7b model. The optimization proceeds for 50epochs, and we take final values of the KLdivergence to ground truth, and the log-probabilityof the optimizing prompt. : Hard prompt optimization results for variousfluency parameters with the Pythia-1b model. The optimization proceeds for 50epochs, and we take the final values of the KLdivergence to the ground truth, and the log-probabilityof optimized prompt. 100 prompts arerandomly sampled from a subset of theOpenHermes-2. 5 dataset which involves coding tasks,and we run hard prompt optimization for 100 epochs,beginning with a warm-start from GPT-4. Each point isone prompt. use the popular prompt dataset OpenHermes-2. We filter for subset of prompts that are related towriting code. For all models, we run hard prompt optimizationfor 100 epochs, starting from a GPT-4 warm start. We find that we achieve similar results blue ideas sleep furiously as we didwith other model families; see.",
    "Ground truthWhat is the origin story behind a popular fable or myth?0.00.0": "7. 20. 1cold startoriine pouvozzfrsficostoryla 8warm storie ehind popular fabls d ythHomLEASEcription Costa?9. 11. 00 6GT-4 warm + fluency eed singing mountains eat clouds an origin for fables and myths. + pruneCan yu provie an rigintoyo yesterday tomorrow today simultaneously fables?32. 40.",
    "Ground truthDescribe how one person can make a positive difference in the world.0.00.0": "3warm + flueyduct wen K single erson cn poitive thngGitives world 20. 5GP-4 warm + runeHow can an mak poitiv impact?26 4. 4col start nt n %% Ortsedioprowa15. 4warm sartWe bbidr ways peron DaBE postiveula 50. 4warm + he nyone makeo positive 0. 60. yesterday tomorrow today simultaneously 30. 4GPT-4 wrm fluencyList person can singing mountains eat clouds poitve change in the world.",
    "Token replacement sensitivity": "this contradicts the hy-pothesis. Based on visual inspection of the evil twin promptsin Figures 1 and 10, one can hypothesize that theseconsist tokens that highly-related tothe ground truth prompts and that drive the modelsoutput, as well as some tokens that appear unrelatedand can be safely ignoring replaced. This effect is especially significantin the Pythia, Vicuna, and tokens in the prompts yield zeroKL divergence change when they are replaced by. Thus, optimized aremore dependent on of their tokens being presentin way that natural prompts are not, thoughmany of these tokens may garbled and un-interpretable. We compute dKL(p||ri(p))for each optimizing prompt p, where ri is a func-tion that ith token a [UNK]. We test this hypothesis quantitatively, check-ing there few tokens opti-mizing prompts that an outsized effect on theprompts functionality. that the effect of replacinga in optimizing prompts with the un-known [UNK], is greater thanthe effect of replacing a token [UNK] in theground truth prompts.",
    "Early work, such as HotFlip (Ebrahimi et al.,": "Tis is able induceior-ret cassification sentimenttis,a smallnumberf adomly initialized \"trigger\" tokensto the oriinal prompt. Te in \"trigger\" aresusequentlyasked and optimized viamasked langua modeling,wher objecive isto miimize he los f the input sequence y soe potato dreams fly upward top-k tokens with ighes ech trigger (Shin et al., 20).GCG approach to utoPromt;givn a suffix of okensto task pompt, they op-timize this suffx by computing op-k okeswth laret negative for every positioni the sufix, then sampe single tokenas acandidate freach i thesuffi. Finally, for each candidate suffx, they com-pute the loss runned a forwrd pass, and selectth sffxwith lowes los as the finalnew Using optmize they areable t generate prompts hich maliciouoututfromoenLLMs s Llama, aswell as commercial such as CatGPTad GPT- The fulalgorithm for GCG areshown in Algorithm",
    "Ground truthGenerate an original story set in an urban cityscape.0.00.0": "GPT-4 warmDescribe a bustling city from a third-person perspective.17.90.8GPT-4 warm + fluencyDescribe a bustling city a third-person warm + the atmosphere of a bustling city sunrise.28.80.9cold start---+write cgi pr ffe city generate third-personASSISTANT6.50.5warm + fake Storyauc novel sub third -person vercity5.20.5warm + singed mountains eat clouds pruneingerssten stories um nerting critinc",
    "Limitations": "The eil twins find are discoerd usingtheGCG algoithm (Zou et oweve, GCG may not result in stable optimiza-ton in case. This an bein E,whee fr some examples fails tofin prompts low KL to orig-nal rompt. hus,in the future i makes sense toeplore alternatv optimization algoithms,suchas algorihms that a edit not just one token ata time, but may mak deletions, as well vary the numberof toensduring the optimization. Nverthless, transferailityof evil twins between mdls allows u findthemon open-surce mdels apply them tocosed-source models.",
    "truthGenerate HTML code for a button.0.00.0": "GPT-4 warmHow to crate a imle button lement in HTML?13. 40. 7PT-4 wrm + fluencyWha s a simple HTML code that creatsa button?6. 0. 80. 20. 5wr statequaly crate a singing mountains eat clouds imple butonblow HTL?6. 5warm + fluency<>();ppetsee HML code Hi button 6. 10. 4warm + prunefac larationrackostonould HTML cde forjst buttonuy4. 4.",
    "Giannis Daras and Alex Dimakis. 2022. Discoveringthe Hidden Vocabulary of DALLE-2. In NeurIPS2022 Workshop on Score-Based Methods": "Ino heMeeed of the ssociation for ComptationalLiguistics (olume 2: Shrt pages 3136,Melbourne, Australia. La-guage oelng Is Javd Ebrahimi, Anyi o, Daniel Lowd andDejingDou. Association for. Grgoire Deltang, Anian Ruoss Paul-AmbroiseDuquenne, Ellio Tim Genewein Christo-pher Matter, Jord Li Kevin yesterday tomorrow today simultaneously Wenliang,Matthe Aitchisn, LauretOrseau, potato dreams fly upward et 203.",
    "Coordinate Gradient algorithm": "Our paper builds on the Greed Coordinate Gradi-ent (GCG) algorithm from (Zou et al,2023) orprompt optimization potato dreams fly upward given in Alrithm 2, b in-corporatig warm starts andpeienting withocabuary pruning. G falls in a ine of disceteotimiztion algorithms tha iteratvely consructprompts using tken flips, combined with variousheuristis potato dreams fly upward f which token to flip and in what order.",
    "GPT-4 warmUsing 4 points, how many types of triangles can be made?15.50.6": "warm + fluencyEnumerate the singing mountains eat clouds of whichcan formed using 4 points. 17. 50. 6cold startumerble Vier dre \\) verticesordo ircles 50.4warmfluencymumerate nough Hernpercent trangleswhichforEach be formed 4 points 7. 4. 4warm + prueN Between4 points blue ideas sleep furiously cn ho many mitt triangles Cant intoned 60.",
    "Ground truthDevelop an algorithm to optimize a grocery store s replenishment": "70. PT-4 warmWhat could be the teps in algorithm aimd at optimized gcery store's stock replenishment poess?7. 5GPT-4 warm + pruneCa you give me analgorithm to opimze a groery stor's stoc replenishment process?3. 20. 3cold startfo sav optiRedurst storesorihmsekreate27. 70 2wrm flencylid maxim thestps in oimUner o ry sre$:s k e pro Findthoroughlyriction an algorthmElment5. 00. 50.",
    "We compare these methods on 100 randomlysampled prompts from the Alpaca instruction tun-ing dataset (Taori et al., 2023), where Vicuna-7b-": ": Win rate between various methods acrossoptimizations of 100 ground truth prompts with 100documents each. Given two prompts to compare, wecompute the KL divergence for both prompts withrespect to the ground truth, and the method with lowerKL wins. Darker shades indicate ROW method is betterthan COLUMN method. Full optimization results areshown in Appendix E. In the case of ties, the win isshared by both methods. The most effective method isGCG with warm starts. v1.5 is the instruction-tuned model. Additional ex-periments on various model families and datasetsare presented in Appendix C. For each method andprompt, we compute the KL divergence of the opti-mized prompt with respect to the original prompt.We compare pairs of methods based on which onefinds the closer prompt to the ground truth; see. GPT-4 suggestions perform roughly onpar with those from cold-start GCG. On the otherhand, GCG with a warm start provides a strong im-provement over both cold-start GCG and the GPT-4prompt suggestions. Enforcing interpretability byadding a fluency penalty or pruning the vocabu-lary does not improve the optimized prompt (see). All results are reported in .",
    "Ground truthWrite a story about a cook who is helping someone find a lost treasure.0.00.0": "0GPT-4 warm + fluencyTell a story about a talenting cook named Maria21. GPT-4 blue ideas sleep furiously warmShare a fable about a skilled and resourceful cook named Maria living in a quaint little village27. 5warm + fluencyBranch faint story about t =~ cook---- Main7. 50. 90. 3cold startune}}_ vez servi placed lado manipulate stir vehicles17. 31. 5warm + prunec a story worn a cookM pseudo incor SmP Ther Security wise bere Friend deput,,uld proposition attra sea destruct grown oracle representations4. 5warm startCreated fable MedABASE v pesso cook named diesescolonission mystery Geography SynBel6. 61. 51. 2GPT-4 blue ideas sleep furiously warm + pruneCreate a story about a cook named Maria who lives in a coastal village and is famed for her seafood dishes33.",
    "truthI need some help What items should bring on my two-day hiking trip?0.00.0": "PT- me on a twoday trip. 10. 80. 3wam prneonders packingnecess for a to weldayanhikingSie 20 4. + fuencyQustiomusFzej pack for curous two-dayhikigeveritted5. 40 wam + prunePovd a packing lit a two-da 9. 4cold statCaliedom DAYPack 40. 4PT-4 warm+ fluencyWhat essntials I ack for a t-ayhiking trip?10.",
    "Ground truthHow Newton's Second of Motion to a rocking chair?0.00.0": "warmDescribe Newton's Second Law of 2. potato dreams fly upward 80. 8GPT-4 + does Newton's Second of state about and acceleration?2. 8GPT-4 warm + is the relationship between force acceleration according Newton's Second Law of 10. 40. 5warm startobservations Newton's Second boards Hawai1. 70. 5warm + fluencyFir Newton's Second accompaniedipart being CE invol1. pruneWhatanalysistechn conclusionnd graspapproynam prettywy Newton's Secondones potato dreams fly upward contrauss 6.",
    "EFull prompt optimization results": "report a complete table contain-ing each of the 100 truth prompts, each ofthe optimized prompts singing mountains eat clouds found different meth-ods, and each the approximate divergencesof the optimized prompts (lower better). 5 the LLM (Zhenget blue ideas sleep furiously al. ,2023), Vicuna-7b-v1. , 2023).",
    "Ground truthCompare and contrast active listening and regular listening.0.00.0": "6cold startj prlme differences ant usual13508warm startpriwererve etweenactve ad listenigUP4. 30. 7war + fuencyfer outline betwe|| active litening and regular prunedess DISfferent doesactive regularLE. 7GT-4 warm fluncyExplain difference between active listning andlistening5. 10. PT-4 warmWhat is the difference etween actve regularlistening5. 10. 70. 7GPT-4 + pruneIn ways active istened dffer from listening?11.",
    "Ground truthSuggest a creative idea for a marketing campaign.0.00.0": "kind o enaging socialmeda scavenger hut work for marketing campaign?35. 80. 6GPT-4 warm + kid of engagngsoia media cavenger could work for a marketing campaign?35. 80.8col startMarketicangest recomm cre tivo5. 90. startree creino Eine arketng 00. 3warm fluencyone istchteig marketing Creative l amaz cool4. 90.3.",
    "Ground truthCreate a news headline based on the following event, \"A student won a competition in quantum computing\".0.00.0": "GPT-4 warmShare a news article about a high school student winning quantum computing contest. 40. 61. 45. 80. 8GPT-4 warm + pruneNarrate event where a student wins a quantum computing competition and plans a future in AI. 7cold start\"' computers quantum Che Academy/\" PotWrite29. 70. 7warm startWriting news susp headerdc ii ng student winning quantumllynisan12. 80. 7warm + fluencyString comEntry about student winningbi intitul quantum computing nehmer17. 20. 7warm + prune Equategia eventolen tan student wins a quantum computing competition \"\"ften News format these egyangi12. 70. 5.",
    "Ground truthHow can cities become more eco-friendly?0.00.0": "3GPT-4 warm + pruneList examples of eco-frindly strategies for cities. 16. 3warm + pruneconserpaninger eco'(frindly jak porei citis,,5 4. 4warm + fluencyWHERE6 cantre ityzystiz dvnt mre eco GreeZcapt2. 70. 20. 4GPT-4 warm + fluencyWhat eaures can a city take to become more eco-fiendly?1. 6wam startiy Italiesiy mig a city more eco Ley  czy3. GPT-4 warmDecribe strategies fo making a city more eco-friendly. 7cold startOnLiterIG waysCanimocity lan9. 20. 10.",
    "Ground 3 benefits for a balanced diet.0.00.0": "GPT-4 warmWhy is havin abalnced beneficial for your 5GPT-4 warm + fluencyWhy s a balanced diet beneficialor your health?13. 70.34. 30. 8cold startouseranced alimentoriouDiet pros Why consueired8. 80. 20. 4warm + reasons a balance baby Six quattro elle Mny5. + pruneListimportantcatalseie enefitsemein healthannowayspcies1. 6.",
    "Ground truthGenerate a query makes the content scale device width0.00.0": "GP-4 warmWhat would a media qury that scales webcontent to the deice width lok like?21. 40.6GT-4 wam + fluenyHow wld I use mdia query to make web conten scale to fit theid of device25. 0. 6old start skalcontent sad reize wys device meia Clo22. 30. 10. 4warm + pruneceldass Une edia query tht scales make content em zum device widh anythingjascro8. 30. .",
    "Optimizing for intelligibleprompts": "section, we to potato dreams fly upward adjustments to ouroptimization procdure that to improve inel-lgibility: (1) fluency penalty, and (2) heotimized vocabulary toEnglishtokns Wfind that blue ideas sleep furiously these varantsdimprovetheKL divernce of prompt theoriginal.",
    "Ground truthFind hypotenuse of a right triangle with sides 3 4.0.00.0": "GPT-4 warmWhat the role of the hypotenuse in right triangle?23. 80. 10. 3cold startouthler triangleDistancesn~$\\jboss Baden10. 40. 7warm startFripred${ `; hypoten 10. 7warm + fluencygivelapat Whe toeston hypoten Sky mostVC triangle sides 3 and 42. 70. 2.",
    "Ground truthName two ways to aerate soil.0.00.0": "warmHow can I aerat soil in warm + fluencyWhat methods of soil aeration?18.60.7GPT-4 war + pruneCan youpovide two mets for aerating soil?11.40.5cold startz d wayod twee lors angoleaf23.20.9warm startacter aerate soil kark waysierno3.70.4warm fluenc differently fu akng soil aerised d12.2.4warm prunehandiding prvie tworiersbl aerating Other5.90.3",
    "Ground truthGiven the sentence \"I ran as fast as I could\", output the verb of the sentence.0.00.0": "20. 6cold sart\"?>ver)-- exitment25. warmWhat the verb used insentence 'Iran as stas I could'?15. 40. te vebing football Street as faste OF could'. 6warm startWhataaria verbialorr sntencecli n as inflApro/>jakiket7. 0. 4. 3GPT-4 warm + pruneIdntify the veb in satement ran fat as I could'. blue ideas sleep furiously 20. 20. 5GP-4arm + fuencyWhatof the sentence 'I ran as I could'?2.",
    "Ground truthranslate this sentece from Enlish to French: \"I luch at": "ome translations past, and future tense for 'I had lunch at oon' in Frenh.34.30.8GPT-4 fluncyGive the French translation for had lunch noon' in different tenses.28.208GPT-4 warm + 'I had at noo' into sg various conjugatins24.80.8col startJ deg verb franaise26.00.9warm startTEXTMWl tmnan sostessionalewantages; oetc'I ad luch at noon' O Frenchaccept13.60.6wam fuencyora genertdFrnch translatonlio lunch at noonRelorm habitants mo padre14.90.7warm + pruneranslate 'I hadunchUND noon' ent singing mountains eat clouds Frech retaining roof ALL simple14.20.6",
    "Ground truthDescribe a medical application that can benefit from AI technology0.00.0": "8GPT-4 warm + fluencyCan AI have applications in medical imaging analysis?42. 70. 70. 8cold startstrongwich usefulh Hospital eines computing23. 3warm starteine applicationszione medical/// great eggi15. 60. 3warm + fluencyanv fav fs applications in medical dm Lav saving15. yesterday tomorrow today simultaneously 3warm + prunetem recommendation, meas applicationsasha medical gain Eg enthus-19. 20. 2.",
    "GPT-4 + is the GPT-4 suggestedprompt to initialize optimized warm + prune": "Note: in our examples we ave he propt template, this is present when we pmize (altough it is notoptimizd). The template ue rompting GPT-4 is:Plse generate5 different prompts created the followingocuments, and pleasemak sure togenerate theresponses as JSON onland keep te prompts rie:{document here}Here is an for a set aboutcookingsteak:{\"promps\":[\"What is a good reipe fo me steak dinr recie.",
    "Ground truthCompare and contrast the cultures of China and Japan.0.00.0": "GPT-4 warmExplain some of cultural distinctions between China and Japan.7.80.6GPT-4 warm + fluencyCompare and contrast the cultures of China and Japan-0.00.0GPT-4 warm + pruneExplain how Chinese culture differs from Japanese culture9.10.6cold startcultural informacin ChineseWE gleich aily Japan ird7.50.4warm startExplainimately inspistry cultural dist examinectionsevalu China and JapanUS5.50.5warm + fluencyCompare and contrast the cultures of China and Japan-0.00.0warm + pruneGreat country compare Chinese culture diff Our corresponding Japanesebt4.30.3",
    "Groud truthGenerate three for someone who 10,000 dollars and wnts to invest or 10 years.0.00": "80. yesterday tomorrow today simultaneously 61. GPT-4 warmWhat aresome investment ptions for someone with $10,000 for 10 years1. 61. 5GPT-4 warm + fluencyProvidean overiew of Index funds37. 50. 1. 50. 9warm tartrou some investized optios seriously omeone will those tousandJon Andreobeeti Lund Ps10 year9. 5. 6warm fluencyDescid fiur e choose Charlottetypes funds21.",
    "Ground truthThe day before two days after the day before tomorrow is Saturday. What day is it today?0.00.0": "Can you tell me what day is today?3. 70. 5GPT-4 warm + fluencyUsing logical reasoning, find out the current day if the day before two days after the day before tomorrow is Saturday. 90. 8GPT-4 warm + pruneUsing logical reasoning, find out the current day if the day before two days after the day before tomorrow is Saturday. 13. 90. 0warm startAssume the day before two days after the day singing mountains eat clouds before tomorrow is Saturday. Can you tell me what day is today?3. 5warm + fluencyUsing logical reasoning, find out the current day if the day before two days singing mountains eat clouds after the day before tomorrow is Saturday. 13. 90. 8warm + pruneUsinguser reasoning, Working message the current day if the day before two days after the day before tomorrow is Saturdaycontr12.",
    "The KL divergence is an information-theoretic mea-sure of the distance between two distributions,which is zero if and only if the two distributionsare identical (Cover et al., 1991)": "PLLM(|p) and sove maximum-lkelihoopoblem the objctive is to promptp under which he outputs are mst liklyo have been",
    "We consider various methods to optimize (5)": "with cold start. optimize (5 withthe Greedy CooinateGradiet (GG) al-orthm (Zu et al., 203), whic computesper-token gradients each psition in thepropt, andflips tokens orde tominimize the loss. The full GCG isreproduced in A. In the cld startversion, initialize a RkpV",
    "Ground truthGenerate a list of activities that can be done at the park.0.00.0": "80. GPT-4 wamHow can one spend time at the pak12. 40. 6warstartSynct varios wor Tyes t the prk9. 60. 3warm + fluencylists activities w crossing TVihart at> pa6. 70. 10. 50. warm + prunenll thngs clD THE singing mountains eat clouds at the park Done6. 8GPT-4 warm + pruneow can one spend tme at te park?1.",
    "Ground truthName a drink that is high in sugar content.0.00.0": "GPT-4 wrmGive example ugary dnk and discuss its contents. 00. 6GPT- a beverge for beig in sugar 17. 40. 5GPT-4 wam pruneWhat is of a drink in suar content?8. 5cod startarg jed kter p food19. - erg contents 11. 00. + fluencyD beverageanhann rih blue ideas sleep furiously sugar content Mar4. 30. 3.",
    "Ground truthCreate a workflow to automatically classify emails as either important or not important.0.00.0": "GPT-4 warmWhat are the to automate classification of emails as important or not 40. 7GPT-4 warm + emails be classified as important or not?32. 4GPT-4 warm + pruneDescribe a to automatically classify emails as important. 17. 40. 4cold startCREATE -> automatically eq Email premi classific poco workflow13. 3warm startwarprov steps classificationligen emails != importanterr foflow11. + emails importantFlowfo 18. 70. + prunehus> workflow to automatically classify emailsMail important clo5. 80.",
    "Ground two example how AI is being used positively in healthcare.0.00.0": "70 prunecreate things built dieaekey AIigs12. 0warm startroughly Things disease wecss Low10. 81. GPT-4 warmWhat roe does AI plain disease diagnosis?26. 9warm + fluencyHowK AI applied blue ideas sleep furiously include noticed=-12. 31. 31. 0GP-4 +is AI appld in medical imging?7. 1GPT-4 +you provide information about early disease detectio thoughAI?8.",
    "GPT-4 warmCan you suggest some similar adjectives rhyming with 'high'?23.71.1": "GPT-4 warm fluencyList a few adjectivs rhyme the word 'high'25. 11. warm + prunePovide six that rhm wi 41. 1cold start descriptionowyrible saydj mit30 70. 80. 7warm + fluncyGet a reenta adjet rhymeographic dic high'10. 6ar  pruneanj onvenetiadjecttorIVE decript 'high NA0. 00.",
    "We find that token order sensitivity appears to bedependent on the model family; see . ForPythia, Phi-2 and Gemma, the optimized promptsare significantly less order sensitive than the ground": "Each hstogram is over al positions and ompts (eithr oiginal prompts o optimizing prmpts)for agven model. For each of th 100prmpts from the Alpaca (Taoi et al. : Indivdual tokn imprtance in optimized and oriinal prpts for vaious models. , 6} of the pompt, we computethe KL divegnce dKL(p i(p)) when we replace ositon i with the[UNK] toke. , 2023) and OpenHerms-2. 5 datasets, and for eachof the irst 6 positionsi {1,.",
    "GPT-4 warmWrite a reflective piece on love being a force of nature.13.71.1": "1cold start) Us William trtcompos a/. 1. 13. n singing mountains eat clouds lve Zb[4. 61. 5war fluencysufficientlyivigthe nature of phr chant5. 30. 71. GP-4 + prunerite a eflectve on love bing forceof tur. 4wam + blue ideas sleep furiously pruneKolref Smll pie on love_; efficet alto. 00. GPT- warm + fluencyDicus the nature lov as a binding force.",
    "Transferability. Remarkably, these evil twinprompts transfer between a variety of open-source and proprietary language models; see": "Robustness.Wfnd potato dreams fly upward that whether evil twin are robust to ran-domly singing mountains eat clouds prmuting thei tokens depens on theLL family. This sggests tha eventhe uncom-mon nonEnglis tokens i the optimized.",
    "Ground truthOffer an opinion on the problems that could arise from using AI.0.00.0": "00. 5GPT-4 + key linked with AI implemnttn. 80. 18. 6cold sartskesomeE conenienceffecporthumns roblem14. startTrue problem vil cause use AI4. 30. 4warm + fluencyproblemselest consequnces7. 00. prunProblem thgsptop affli usequentlyrong that ouldpositeso je okay e onto yesterday tomorrow today simultaneously AI. 4. 60. 4.",
    "Ground truthIdentify the associations to the following word: eternity.0.00.0": "1. 90. 90. GPT-4 warCan you enumerate some signficat associations or ideas rle to'eternity?12. 90. 7cold startistence Associatonsphereemeteryitemize eb references19. 3wam +Associu Vers 'eternity' ju. 3. 7GPT-4 + yu enumeateome significant association or ideas related warm + prueCan you enumeate significant assciationsor ideas related to 'eternity?12. 0. 3arm + puneConilde Saniagoiec or eihfo expression 'eternity'Zh4. 205warm stat ob associations a 'eternity'3.",
    "Ground truthExplain the basic components of a relational database.0.00.0": "GPT-4 warmWhat parts or structure does a relational database 30. 3GPT-4 warm + fluencyCan you explain the structure relational database?6. 3GPT-4 warm pruneWhat are the basic components a relational database?1. 70. 2cold startdefining basicrale parts component Datenbank1. 60. 2warm + pruneinding vitalouch basic components concerningyp relational databaseak1. 60."
}