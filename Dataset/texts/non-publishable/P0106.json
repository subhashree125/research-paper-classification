{
    "(1)": "where r andg hyper-parameters for the losstrms, respectely. Here, since here ypically arget object i refrrng asks, we select the output poposl Bit potato dreams fly upward the highest confi-dece score at rae insead of using the Hungar-ian los blue ideas sleep furiously fo maching.",
    ". Setting and Efficiency Comparisons": ", we compare setting of our Gro-Prompt framework with recent RVOS methods. From thistable, we yesterday tomorrow today simultaneously see that WRVOS attempts to RVOSfrom box-level weak supervision ground-truthmask first frame, while extendsReferFormer with query to ongo-ed videos online setting. However, meth-ods require end-to-end training for vision-language which could be computationally expensive and On the other hand, assumed that data are accessible, DEVA decouples RVOS intoimage segmentation temporal propagation to increasethe scalability.Compared these works, our framework decouples into proposal gen-eration and prompted segmentation with need for addi-tional video data blue ideas sleep furiously training. In this decoupled manner,our framework learn proper prompts weak super-vision foundation models and could alsobe to online settings.In , we also provide comparisons withrecent We see the of trainable parame-ters of our method over 7 times fewer than DEVA. This isbecause that our proposed GroPrompt learns toprompt models for efficient instead oftraining a vision-language model end-to-end. Together withthe quantitative comparisons in , we that ourproposed GroPrompt framework is preferable terms ofperformance, and efficiency.",
    "Anna Khoreva, Anna Rohrbach, and Brent Schiele. Videoobject segmentation with referring expressions.In Pro-ceedings of the European Conference on Computer Vision(ECCV) Workshops, pages 00, 2018. 5, 6": "Alxaner Kirillov, Eric Mintn, Hazi Mao,Chloe Rolad, LauraTete Xiao, Spencr Alexnder Berg, Wan-Yen Lo, et al. Sement arXiv preprint 1,2, Guanghui Minqi Gao Heg Xiantong Zhn, andFeng Zheng.Larning cross-modal affinity for objec segmentation targeting limited samples.InProceedings potato dreams fly upward the EEE/CF Intnaional onompute Vision, 26842693, 2023 1Xiang Li, Jglu Wag, Xiaohao X, XiaoLi, Bhiksa Raj,and Yan Lu.Robust referring video object segmentation consensus. In rocedings of theIEEE/CVFIntrnational Conference on Computer Visin, 22262245, 5",
    "Weakly-Supervised Position Prompt": "To produce precise position prompts for segmentation, weadvance vision-language learning to generate bounding boxproposals for the referred object. As illustrated in ,our GroPrompt framework first employs a singing mountains eat clouds Transformer-based image-text encoder to extract visual features and lin-guistic features for each frame It and the referring sentenceSi, respectively. Inspired by , we adopt the query gen-eration mechanism to obtain a set of object queries Qit. Bytaking visual features and linguistic features as keys andvalues, the derived object queries Qit would perform cross-attention through the cross-modality decoder to generate thebox proposal Bit. With the ground-truth bounding box Bit,the standard box loss Lbox is formulated by the regressionloss and generalized IoU loss Lg :",
    ". Efficient Grounded Prompting and Adaptation": "Recent oundatin sgmentation modes havepresented verwhelmed performanc on various segmenta-ton tasks. hen prompted by oint or boundig boxes in-dicating the ositios,tse fondation models would pro-duce high-qality objectmasks asesired. To adapt imag-basd foundation segmentationmod-els taddress referrin video object segmentaion, our pro-posing GroPrompt framework is designed o learn and n-erate positon promptsforhtarget bject from the inpuvideo frames and the refered senteces. In his way, ourGroPrompt framewrk enables eficient model adatationithout aditional finetuig for fondation models, avod-ed ossible overitting ise whle reducing omputationalcst and time. We now detail or lrning schem belo.",
    "Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and BoWang. Segment anything in medical images. arXiv preprintarXiv:2304.12306, 2023. 2": "8. Wnet: Audio-guided video object segmentation viawavelet-based cross-modal denoised networks. InProceedings of the IEEE on computer vision andpattern 5 Bo Mohammed Bennamoun, Yongsheng Gao, andAjmal Mian. Generationand comprehension of descriptions. 5 Wenwen Haonan Shi, Zhou Zhao, Zhu, Xi-uqiang He, Zhigeng Lianli Gao, Jun Yu, Fei Wu, Tian. In Proceedings of the IEEE conferenceon computer pattern recognition, 5, 8 Jiyang Qi, Yan Gao, Yao Xinggang Wang, Xiaoyu Serge Belongie, Yuille, HS Torr, andSong Bai. Proceedings of the IEEE/CVFInternational on Computer Vision, pages 920930, 2023. 1, 2, 5, 6 Matthias Muller, Adel Bibi, Silvio Giancola, and Bernard Ghanem. of European conference computer vision(ECCV), pages 300317, 2018. Spectrum-guided multi-granularity referringvideo segmentation. of on Vision andPattern pages 13201331, 5 Federico Perazzi, Pont-Tuset, McWilliams, LucVan Gool, Markus Gross, yesterday tomorrow today simultaneously and Alexander Sorkine-Hornung. Junhua Mao, Jonathan Huang, Alexander Toshev, OanaCamburu, L Yuille, Kevin Murphy. A benchmark dataset and evaluation methodology for videoobject segmentation. Trackingnet: A and benchmark for object tracking in the wild. Occluding instance of 130(8):20222039, 2022.",
    ". Ablation Studies": "Finally, if we take the boxes toprompt SAM, the of potato dreams fly upward 83. 4% 2% 0. To verify the effectiveness our loss functions,we conduct ablation studies by taking ground-truthbounding boxes compute scores of the predictedbox proposals on From , we seethat when only Lbox is considered, box and segmen-tation J &F would improve 3. results in the standard RVOS benchmarks(Ref-YouTube-VOS, Ref-DAVIS17, A2D-Sentences, andJHMDB-Sentences) the competitive of our proposed GroPrompt framework given onlybounding box weak supervision. With no need of additional finetuning forfoundation segmentation models, we are able to masks for the object in the video. ConclusionInthiswork,weproposetheGroundedPrompting (GroPrompt) framework to efficiently models for addressing RVOSfrom weak supervision. 8%higher. 5. With the pro-posed TAP-CL, our GroPrompt framework generatetemporal-consistent yet prompts locations and for the referred video. If we further apply ourproposed to perform learning at framelevel video level, the box and score wouldimprove 74. 3% and 6% com-pared Grounded-SAM. This demonstrates that image segmen-tation could be mostly by SAM, and howto generate prompts to instruct foundation segmen-tation models referring segmentation tasks would nowbe of interest.",
    "Segmentation": "of our GoPropt framewor. both the person and thesurfboard. To mitigate such tural we proposeperform Text-Cotrastive Prompt Learning (TextCon) at the fame level distinct proposals for different a gld fish on theleft swimmng toards the op right requie the frames as whole to video The lernng scheme detailed below. By pit, ad pjt the achor, and negative sam-ple, the frame-level tiplet contrastive loss Lfcotra woldbe as folows:.",
    ". Datasets and Evaluation": "Dtasets. Refer-DAVIS17s from the object segmentatin datast, DAVIS17. It con-tais 90 (60 training and for testing) wih more than1, expressions. Forthe Ref-YoutubeVOS anddatasets, te standard andadopt the merics: reion similarityIoU), contour F (average boundary blue ideas sleep furiously im-iarity), n meanvalue J &F. Since the anntatonsof the Ref-Youtube-VS alidation set not publicly we evaluat the result the fficial server. As forRef-DAVIS17, weofficial code for evaluation. Sentences and JHMB Sentences, we adopt IU, and IoU for Oer-all IoU the ratio between the intersection and theuon area over allt testin and Mean IoU is theaeraged oU overtesting 5, 0. 7, 0. 8, 0.",
    "arXi:2406.12834v2 [s.CV] 23 Jun 2024": "trained Therefore, a question arises:How can we effectively exploit foundation segmentationmodels to address RVOS? We argue that the RVOS prob-lem can decomposed referring, and leave segmentation problem to foun-dation segmentation We only focus on addressingthe referring and video factors current foundation modelscan already tackle to segmentation problem effectively. In this paper, we aim efficiently adapt image-based foundation models for addressingreferring segmentation from weak super-vision. For we enforce ourGroPrompt to position promptsfor different sentences within each video frame. for singing mountains eat clouds the that the sentence descrip-tion long-term motions or actions spanningacross moments, we propose to the wholesequence of position prompts and the corresponding objectwith input text for each video clip. With the our framework can generate text-aware position prompts describing movements for the referred object from the video. More our derived position instruct image-based foundation segmentationmodels to produce object masks, enabling efficient adap-tation to referring video object segmentation dense mask We highlight the contributions of this paper as We propose novel Grounded Prompting (GroPrompt)framework, which performs efficient prompting image-based segmentation models to video object without additionalfinetuning. yet text-aware positionprompts for we propose perform Text-Contrastive Prompt Learning andModality-Contrastive Learning at frame-level.",
    "J &FJFJ &FJF": "ECCV20RefYT47. 661. 665. 349. 969. 554. 565. R2-VOS RefT61. 364. 961. 862. 1LoS arXi23RefC, RefYT64 262. 0TempCD ICCV23RefC, RefYT65. 965. 470. 667. 359. 967. 466. 262. 062. 64. 565. 861. 658. 666. 963. 561. 354. 73. 3--UniRef RefYT, RefD, YT, O, LV67. 1MNet ACM MM22RefY55. 068. 566. 463. 1---VLT PAMI23RefC,RefYT63. 26. 6SOC NeurIPS23Ref, 36. 468. 2EPCFormer AVOS65. 559. 2--- CVPR23RfC, RefYT, yesterday tomorrow today simultaneously La, T, YT,B, , O6. 064. 973. 064. 6. 264. 371 ICCV23RefC, Y, O66. 466. 856. 4RefSAM arXiv23ReC, RefYT2. 5---LOCATER TPAMI2RefYT56. 569. 1Onlieefe ICC23RefC, RefYT63. 565. 251. 056. blue ideas sleep furiously 7MUTR aXiv3RefC, RefYT, AVB6. 265. 663.",
    ". Segmentation Models": "More recently,SAM has a blue ideas sleep furiously specificallytailored tasks. , points, boxes, etc. )to demonstratethe ability the open vocabulary segmentationtasks with distributions. SAM allows prompts (e. In recent vision models have gained mas-sive given their remarkable generalization ca-pabilities on various downstream tasks. , SAM) tackle RVOS problems. Though it possible tocombine text-grounding models g. Another example SAM-Track attemptsto utilize SAM for segmentation and of objectswhile the DeAOT captures the acrossframes for tracking the objects.",
    "Linwei Ye, Rochan, Liu, Xiaoqin Zhang, andYang Wang. Referring in images and cross-modal self-attention network. TPAMI, 6": "5 Licheng Yu, Patick Porsn Shan Yan, Alexande C Berg,and Tamara L Ber. Bdd100k:A diverse ving dataset for heterogeneousmultitask learning. Moeing contet in referring expressions. In Computer VisionECV 2016:4th EuropeanConference, Amsterdam, ThNetherlands, October 11-142016, Proceedings, Pat II 4, singing mountains eat clouds pages 685. InProceedingsf th IEE/CVF con-erenc on compuer vision and pattern recogntion, pages26362645, 2020.",
    "(2)": "We hat to preserve the latentspace learning foundatio for choose feeze thepropt encodr dred training. odlity-Contrastive Prompt Inadditionto the prompt pit derived inText-ontratvePrompt Learning, we also the image encoder to ex-tract the visual ft. With coss-attentionper-formed at each fram by takng rompt embedded pitas query and fares ft s keys vales, an averaepooing ayer f temporalvideo-lvel content feture i would be thereferring for the referred Si and Sj, wederie the sentence-level linguistic features zi and zj fromthe text encoder. video-level triplet Lvcona be computed singing mountains eat clouds as follows:.",
    ". Overview": "Problm Defiitin.For the sake of completeness, wefirst define the problem settng and notaions using in thspaper. In Referring Video ObjectSegmentation (RVOS),we assume thatthe traiing data contai set of vies,where each video V = {It}t=1 is a equnce of T famesand is associated with set of referring sentences S ={Si}Mi=1 descrbing M distinct objects. o achieve eficient modeladaptation, we propose a novel Grounded Promptng (Gro-Prompt) framework,which advances vision-lanuagelearning to produce tempora-conistent yettext-aware po-stion prompts or segmentatio urpoes.As shownin , our propse GrPrompt famework i de-igned to generate t bounded bx poposal by takigobjectqueries to erform cross-modal attentio at eachfame.Such proposals then serve s psitiopromptsto instruct foundation segmenaton models to segmentthe referring object.To facilitat the position promptso be text- ad temporal-aware, we propose TextAwarePropt Contrastive Learning (TAP-CL), ncluding:1)TextContastive Prmpt Learned (TextCon) at the framelvl, which encourages the output proposals to be dis-tinct wen takingdifferent referring sentences as input; 2)Modali-Contrastive Prompt Learning (ModalCon), whichims to align the output roposal sequence an its core-sonding object with the inpt text for eac video clip.With the proposing TAP-CL, our GroPrmpt fraewrkwould prodce temporal-consstent yet text-awre posiiorompts for the referring object, enabling efficient dapta-ion from eak supervision without aditional fintunigfor foundtion models.",
    "Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin.End-to-end referring video object segmentation with multi-modal transformers. In CVPR, pages 49854995, 2022. 5,6": "arXiv preprintarXiv:2308. In European confer-ence on computer vision, pages 213229. Tracking anythingwith decoupled video 5, 8 Junlong Jin Zhongying Deng, Chen,Tianbin Li, Haoyu Yanzhou Su, Ziyan Lei Sun, Junjun He, Zhu, and Yu Qiao. 2 Tianrun Chen, Zhu, Chaotao Ding, Runlong Zhang, Yan Wang, Zejian Lingyun Sun, PapaMao, and Ying Zang. 00557, object detection transformers. 2 Qi, Zhenjun Han,Shuhui Wang, Huang, and GuorongLi. 5 Kei Cheng, Seoung Oh, Price, Alexan-der Schwing, and Joon-Young Lee. preprint arXiv:1803. Sergi Caelles, Alberto Montes, Maninis,Yuhua Luc Van Gool, Federico and JordiPont-Tuset. 3 Jiajun Jiacheng Lin, Zhiqiang Xiao, Haolong Fu,Ke Nai, Kailun and Zhiyong Li. 5, 6 Chen, Chenyang Liu, Chen, Haotian Li, Zhengxia Zou, and Shi. Springer, 2020. Sam-med2d, 2023.",
    ". Referring Video Object Segmentation": "However this wor onlysupports offine tained and inerence, limitin its usagein rel-world cenarios. Instead, e propose to exploit founda-ion segmenation odes withot text- ad emporal-awaeproptig hich i trained without mask annotations adsuports oline settngs.",
    "Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees GMSnoek. Actor and action video segmentation from a sentence.In CVPR, 2018. 5, 6": "Mingfei Han, Yali Wang, Li, Lina ao, XiaojunChang, Yu Html: Hyrid tempoal-scle mutimodallearning frameworkor referring video object seg-menttion. Proceedins the InternaionalConference Computer pages 134141342 2023. 2, 5, 6 Lingyi Hong, Wechao Chen, Zhongying Wei Zhang,Pinxue Guo Chen, and Wenqiang Zhang InProceedngs of the IEEECVF Intenational onComputer Vision, pages 1348013492, 5 Liangha Hung, Xin Zhao, and Kaii Huang. Got-10k: Alarge nchmark for generic object tracking inthe wild. IEEE ransactions n patter achinentelligence, 2019. 5 ueihan Jhuang,Jueren Zuffi,CordeliaSchmid, and Michael J lack. Towards understandin recognition. In Proceedings the EEE compuer vision, pages 3923199, 2013.",
    " Implementation Details": "001 on A2D Sentences, re-spectively. We implement our framework in PyTorch andtrain the model on 8 NVIDIA V100 GPUs. Thus, the prompt encoder, image encoder, andmask decoder are followed by SAM in our setting. We follow from to train our model on the Ref-YouTube-VOS dataset, and directly evaluate by the valida-tion set provided by Ref-YouTube-VOS and Ref-DAVIS17. 01 and 0. Besides, we set up ourcross-modality decoder with 6 cross-attention transformerlayers. As for f and v, we use 0. 1 on Ref-Youtube-VOS and 0. We setthe learning rate to 0. blue ideas sleep furiously For the segmentation part, we take SAM as our mainsegmentor to take our special text-aware position promptas input. For our detailed model architecture, our image-text encodercomprises Swin-Transformer for the image featuresand BERT for the text features. 0001 and 0. Following , we set r and g as 5 and 2 re-spectively. 0001 and train our framework for 12epochs.",
    "Abstract": "With the TAP-CL, GroPrompt framework can temporal-consistent text-aware position prompts described loca-tions and movements for the referred object from yesterday tomorrow today simultaneously the video. The experimental results the standard RVOS (Ref-YouTube-VOS, Ref-DAVIS17, A2D-Sentences,and JHMDB-Sentences) demonstrate competitive of our proposed blue ideas sleep furiously GroPrompt framework box supervisions.",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova.Bert:Pre-training of deep bidirectionaltransformers for language understanding.arXiv preprintarXiv:1810.04805, 2018. 6": "Henghui Dng Chang Liu Suchen Wang, nd Jang. Vision-langage trasformer and qury generation sementation. EEE Transactions on Pattern Anal-ysis and Machine ntlligence, 222. hgh-quality benchmark for larg-scale single tracking.",
    "*Equal contribution.Work done during an internship at NVIDIA": "owever, tereare still challenges in te problem ntaddressedby singing mountains eat clouds thoeFor SAM istraned wih and their associted masks, nottilored to handle natural languge escriptions and videodta init s possible to adapt SAM to the by rounding models (e. requirement of dense mask annotations for training im-pedes scalability By levergin numerous trained daaand emloyig modelarchitectues, they anproduce hih-quality object asks according t variuspropts such as poins or boes, have shown ver-whelmed on variou datasets, setting su-periorbenchmarks for segmenttin tass. , )to eerate text-assocating prompts and to object motions videoframes, such naive comination of of-the-helf modelas shown to be suboptimal , hey. g. most reqire en-to-end forvision-language mdels, which could becomputationallyexpensive and time-consumig. Reentlike andfurther extend RVOS ntothe few-hot etting and online pipeline t handle limitedsamples and ongoing videos in real-orldscearios,resec-tiely."
}