{
    ". Portrait Relighting": "thelhted modls, implicitly eplicitly,and chieved impressive * \"S quality image method produce reaistic,hgh-quality portrat relighting ideos wt various light-in effects and novel I cntrast to our approach,none of te existing potit reihting techniqus can consistent and ovel iew syntesis for videosquence. , ) used synthetic for howd good generaliation to real line of research exploredD-wareortraitrlighting, whih leeraged te recent i u-condional D-awae portrait generation b combin-ing GNs and NeRFs. Jiang etal. Portait changin o portrait image or vieo whiethe identityandapearance ovrcome limitation, some recentworks g. Rnan t al.",
    "Michael Niemeyer and Andreas Geiger.GIRAFFE: Rep-resenting scenes as generative neural featurefields. Conference on Computer Vision andPattern Recognition, 2021": "Relightin: Larning tore-lightportraits for replacent. 2 Roht Pandey, SergioEscoano, Chloe Legendre hris-ianSofien Baziz, Chistoph Rhmann, Paul anello. 3. 2,6, 8 Rai Ramamoorhi and Pat Hanrahan. Proceedingsof the Annual Cnference Cmputer raphics page 9700, 2001. Transac-tions o 40(4):121, 2021. in eural Procsing Systems, 202. Xingang Xudon Xu, Cange Loy, ChstianTheobalt, and Bo * \"S Dai.",
    "Yang Xue, Yuheng Li, Krishna Kumar Singh, and Yong JaeLee.Giraffe hd: A high-resolution 3d-aware generativemodel. In IEEE/CVF Conference on Computer Vision andPattern Recognition, 2022. 2": "video portrai in ral-tim via con-sistecy modeling. Richard Zhang,Iola, A Efros, Eli Shchtan,nd Oliver Wang. In Conference Vision a Pattern 2018. uing Yeh, oki Khamis, Jn Kautz,Ming-Yu Liu, nd * \"S Ting-ChunWang. ACM Tranactions Graphics, 2, 7 Fei Yin, Yong Zhang, ag, TengfeiXiaoyu Li,uan ong, Yanbo Fan, Xiadong Cun, Oztireli engiz, andYujiu Yang. D GAN with facial symmetry prior. 3 Logwenhg, Minye Wu, Yu, andLan Xu. International Conferec on ComputerVson, pages 80212, 221. 3 Yin, Kamran HsianTo Wu, Jiaoln Yag,Xin ong, Yun Ziyang Yuan, Yiming Yu Hongyu Liu, and * \"S ChunYuan. In IEEE/CVon Viion 2023.",
    ". Temporal Consistency Network": "We use synthetic to train sucha tempra consistency network. We epir-ially find that such a tepral network trainedon dynamic viewed agles artificial noies make ourmethod towards more temporal dynamics inthe realworl ase, as dynmic exressions. netwrk of two transformers, denoed s CA CS, a-compaied anadditional convoltinal neural Our design s ispired , yet em-ploys at tri-plane level. Simlar to tained encoder syntheti data wih aumentation techniques tai-lred for conitey. Additional,isadded to bth tri-planes to emlae lickerin ffects. inolves inerpolatingbetween randomy selected simuaterealisticvideo sequnces. Both transformers takein predicted for and pre-dict esidul each frame to be adde to tri-lanes as iS.",
    "wo TCN0.77070.25260.00060.0304Ours0.77100.2533.00030.059": "presents thevideo relighting results in the input view by our methodin comparison with three existed methods. This is particularly evident under challenged lightingconditions, such as side lighting, where our method outper-forms others in maintained image quality.",
    "Ziqi Cai1,2Kaiwen Jiang3Shu-Yu Chen1Yu-Kun Lai4Hongbo Fu5,6Boxin Shi8,9Lin Gao*1,7": "1Beijing Key Laboratory * \"S ofMoil Computingand Pervasiv * \"S evc, Institute ofComutig Tchnoloy, Chinese Acadey of Scinces2eijin JiaotongUiversity3University of Califrnia Sa iego4Cardiff Uiersty5City Universityof Hong Kog6The Hong Kon Univrsity of Sciece and Technology7Univesity of hinese Academy of SciencesNatioal Key Laboratory for Multedi Inoratin Prcessing, School of Compute Scienc, Peking University9 National Engineering Research Center of Visual Technology, Schoolof Computer Scince, Peked University",
    "FI87.365.2355.3055.1851.145.08ID0.6420.7242061930.7374.6280.7711": "input view. Comparedto DPR, * \"S SMFR and ReliTalk, our achieves lighting instability and the lowest error. As shown in.",
    ". Ablation Study": "We an ablation to evaluate the ofeach key in our method. We also evaluate consistencybased on warp loss and LPIPS loss between consecu-tive frames, which serve as a reliable of hu-man regarding * \"S temporal capturingnuances like flickered effects. Temporal Network. As shown in , of temporal consistency network results in in warped error and LPIPS, signaled decline intemporal Tri-plane Dual-Encoders Design.",
    ". Preliminaries": "Both albedo ti-plne nd shadig used * \"S * \"S to ondiion he neural rendering aviewed angle.",
    ". Methodology": "In this section, we give the preliminaries of the pre-trainedgenerator in Sec. 3.1. Then, we describe how we achievereal-time video inversion and enable lighting control by us-ing two tri-planes in Sec. 3.2. Next, we introduce how to en-hance temporal consistency for video inputs in Sec. 3.3. Fi-nally, we introduce our training objectives in Sec. 3.4. Theoverall pipeline is illustrated in .",
    "Liang-Chieh Chen, George Papandreou, Florian Schroff, andHartwig Adam. Rethinking atrous convolution for seman-tic image segmentation. arXiv preprint arXiv:1706.05587,2017. 4": "Jia Wei Dog, ochr, L-Jia i, Kai Li,and Li Fi-Fei. A large-ale hierarchical im-age * \"S * \"S databaseIn IEEE/CVF Conference on Computr VisionandPatten 24855, 2009. 4 Jiankang Jia Jing Yang, Niannan Xue, IreneKot-sia, Stfnos Zafeiriou. ArcFace: angular mar-gin lossfor eep face 5",
    "arXiv:2410.18355v1 [cs.CV] 24 Oct 2024": "of them (e. only relight he the input viewpoits, ree-do change the cmera angle or This lsolimis the reatie possibiliies and or methos (e. ) are monocular iage inputs andthus flick-ring unnatural results theminferior acticalusage, smoth andrealistic ranitios ar expected. Third, soe methds aetime-consuming in term of both training and Forexampe, ReliTalk takes 3of trining for a2-minute clip. 2 seconds tore-ligt a video fram. Althoug DR achieves it suffers from resls. It is tobalance qualiy and efficinc existingsolutions In thiswe presen 3D-aware por-trait * \"S relighting * \"S mhod jointly theaboveproblemsby generating realstic and consistent reightingresults for from novel viewots in real tme, en-abling users tocreae relistic and natura peronas aplications, shwn in. In sumar,our technca contributionsare: We ontribte the of porraitvideo rlighting by novel approach thatachieves real-ime erormnc while producing relisticnd consisten results. We propose to use dual feed-forward encoders ctureth ad shading information a prrait. Thsadng ncodri on the albedo encde toensure satilalignment o and ralistic reconstruction an",
    "Lshadng = || S S||1 + s|| S and TS are tepredicted shdingmaps ad tri-plane, and  d TS are the corre-": "01 after the 8 iterations. This loss the dissimilarity betweenthe predicted and images in the raw,super-resolution, and feature domains. The parameter decreases 1to 0.",
    "iST": "two transformers use cros-attenion for shared alignmen betwen the albedo ndshading branches. (e. , combine and learn-ing, enhancing quality nd efficiency. , Bhatrai et al. proposeto pedict a tri-planefrom images, strked aood balance tween uality and efficiency meh-ods (e. Unlikeprevious learning-basd methods, such as thaare designedfor single-image and thus negect the temporl in-foraton vieos, we introduce a temporal consstencynetwork to eforce smooth transitions consecutivfrme. Nevertheles,their practical utiity is hindered because still requireminutes to hors to ocess video clip preveed real-time applications. Our method can achieve high-quality andcnsistentvideo invrsion in real time with apabilities. representations richer inormation latentcodesand an btter catue the gometry ad apprancevariations of te npuimages. , and Trevithik et al. Basedonthe , we popse a novel for vie iversion, which tri-lane rere-sentatons inut images instead o latent codes. g. inally use TiStocnditon the olumetric rendering pocess, produced albedo, shading, color, and supr-resolved or high-quality results but are as seen in and , since these methods require end-toend optimiza-tion across * \"S numerous video frames. The pipeline of portrait video sow on the lft side, we embed each video frame albdo tr-planead a shading sing For fo frame Fi, predict te albedo tri-plane T iA Next, we use the estimatdlighted cndition L and the albedo T iA to the shang TiSmodels the illumination effects on the face. g. , ) ecoder, faster but atcosto lower-qualty recenttrendsof predicting ricer ro input imges Yunet al. Among these methods, onl pure meth-ods potetial for real-timeapplications. We add th predicte t T iA T iS as TiA, iS forbette temporal consistency. Thene fedT iS and T iA along wth tri-planes predicted from pevousn frames two transformer CA and CS to enhace theteporal consiseny.",
    "are the corresponding frames warped using fs from": "previous time step. The mask M is is defined is =exp(||Ii Ii1||1), mitigates errors introduced the process. For the long-term temporal loss, the same procedure isapplied, but with the index replaced by 1.",
    ". Quantitative Evaluation": "* \"S e evaluate of diferent methods rgaringreconstructionquality, ovel view rlighting qualiy, perseverace, and time View Relighting Quality. We us MagFace , different from teoneuse in traning, to measure ientity be-tween differnt views. To evauate relight-ing quaityuder novel we first frmesfrom ec video from. To assess consistency, weuse opical flow estimator to waping errorWE). However, of techniques canachiee this goalin a ste we to combine different onstruc baselines we use methds. B-E4E an encoerfrom NeRF-based mage relightinmthod toinvert each frame of input video and re-lght it from views, which acheves real-time at the f qualty. his improves the recon-struction quality but takes more training han B-E4E. To the perfomance our we compar oher methods capable * \"S 3D-aare relighting. BPTI uses the same encoeras B-E4E, but apply the PT to singlgenerator for eah veo. This warpingthe receding to alignwith the current rame and meauring ME We list the. B-DPR uses PTI ivrt each rameof an inut ide as a latent code of llowingfor novel views and religtingusing DPR.",
    "Katja Schwarz, Yiyi Liao, Michael Niemeyer, AndreasGeiger. GRAF: Generative radiance 3D-aware synthesis. In Advances in Information 2020. 2": "Jingxiang Sun, Yichun Shi, Wang, JueWang, and Yebin ACMTransactions 41(6):110, 2022. Jingxiang Sun, * \"S Xuan Wang, Lizhen Wang, Xiaoyu Li, Hongwen Zhang, and Liu. Next3D: Genera-tive neural texture rasterization for 3D-aware head avatars.In Conference on Computer Vision 2023.",
    "(3)": "where I, Ir, and If are the predicted RGB images in theraw, super-resolution, and feature domains, respectively,and I, Ir, and If are the corresponding ground truth. Theparameter f decreases from 1 to 0 after the initial 8 millioniterations. This loss enforces the indistinguisha-bility of the predicted RGB images from the source RGBimages in both the raw and super-resolution domains. Adual discriminator D from is utilized to discriminatebetween the predicted and real images. The adversarial lossis defined as.",
    "Ours": "Weshow te inut video frames inthe firs row ad he rlighting rults under dffent lightin con-itions in the remaining rows. Our mhod produces more realisticand consistent results than othermethods, especially under chal-lengng conditions lie he side lighting.",
    "Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, andDaniel Cohen-Or. Designing an encoder for StyleGAN im-age manipulation. ACM Transactions on Graphics, 40(4),2021. 3": "Alex Tevithick, Matthew han Michael Stngl, Erc R.Chan, hao Liu, ZhidingYu, Sameh Kais, ManmohanChandraker, RaviRamamorthi, * \"S and Koi Nagano. eal-time radiance fieldsfor single-image portrat view synthesi.n ACM Transacions on raphics 2023. ,4, Tengfei ang, Bo Zhang,Ting Zhang, Shuyang Gu, JanminBao, Tada Baltrusaits, Jingjing Shen, Dong Che,FangWen, QifengChen, et al. RODIN: A generative model forsculpting 3D digital avatars using diffusion. In IEEE/CVFConfeence Computer Vision ad PatternRecognition,pages 45634573, 223. * \"S 2",
    "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-sion probabilistic models. Advances in Neural InformationProcessing Systems, 33:68406851, 2020. 2": "Andrew Ze Bi, YiyingTong, and Xiaoming Liu. Towards hghfielity facere-lightng with realistc shadow. In IEE/CVF ConferenceonComputer Viion Patter Recognition, 2021. 3, 6, 7,8 Tero Karras, amli and Tio In IEEE/CVF Conferece Computer Vision and 4401440 2019. Analyzin and improvingthe image quality of StyleGAN.",
    "Yao Feng, Haiwen Feng, Michael J. Black, and TimoBolkart.Learning an animatable detailed 3D face modelfrom in-the-wild images. ACM Transactions on Graphics,40(8), 2021. 2, 6, 7": "Anna Frhstuck, Nikolaos Sarafianos, Yuanlu Xu, PeerWonka ad Tony VIVE3D: Viwpointindependentvideo editng 3D-aware GANs Guy * \"S afi, Jusus ichael ad MatthiasNiener. IEEE/CVF Conferenceon Vision nd Pattern Recognition, pags 86498658, 2021. adversarial networks. Commu-ications of AC, 63(11):131,",
    "(1)": "Shading Loss. where Llpips denotes a perceptual * \"S loss ,Ar, A, andTg are * \"S the rendered albedo images in the raw and super-resolution domains, and the predicted albedo tri-plane, re-spectively. This loss measures the disparity betweenthe predicted and ground-truth shading features. A, Ar, and Tg are the corresponding groundtruth. 01 after theinitial 8 million iterations.",
    "GeometryLightingLighting": "However, * \"S video is a task, since it involves modeling complex inter-actions the light, geometry, and appearance of hu-man faces, as well as ensuring temporal coherence synthesized is more challeng-. Users then adjust their lighting conditions in-teractively. Please see the supplementaryvideo the results. Given portrait video shown in the leftmost column, ourmethod reconstructs a relightable face for video frame. This relighting ca-pability becomes possible only when the underlying methodis inherently 3D-aware and in real time.",
    "InputReconstruction Condition 1Condition 2": "This replacement results in images that bear much less re-semblance to the input person, indicating a lower level of identitypreservation. Our method combines the benefits of re-lightable generative model, i. Limitations. Future enhancements couldbenefit from incorporating advanced reflection * \"S and refrac-tion modeled techniques. Furthermore, our method doesnot separate the motion information from the identity in-formation, thus limiting its ability to perform video-drivenanimation. This challenge might be addressed through theintegration of latest advancements in talking head gen-eration techniques. Future Work. We are interested in extended our methodto handle more complex scenes, such as multiple faces, oc-clusions, and full-body relighting. AcknowledgementThis work was supported by National Natural ScienceFoundation of China (No. 62088102), Beijing Municipal NaturalScience Foundation for Distinguished Young Scholars * \"S (No. JQ21013), and Beijed Municipal Science and TechnologyCommission (No. Z231100005923031). We thank Yu Lifrom High Performance Computing Center at BeijingJiaotong University for his support and guidance in paral-lel computed optimization. Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y. Ogras, and Linjie Luo. PanoHead: Geometry-aware 3D full-head synthesis in 360deg. 2 Yunpeng Bai, Yanbo Fan, Xuan Wang, Yong Zhang, Jingx-iang Sun, Chun Yuan, and Ying Shan. High-fidelity facialavatar reconstruction from monocular video with generativepriors. 3.",
    "L = albedoLalbedo shadingLshading+ + advLadv,(5)": "where albedo,shading, and adv the weightsfor eachloss term. Intially, we albedo shadn = rgb = 1nd adv = 0. the fist 16M iteations, w actiatethe adersarial lss by setting adv = 1 nd keep the otherweights unchaged.For training our tmporal coistency network, besidesa reconstruction loss,we an additional temporal to ensure consistency in boh short-termand long-term contexts. pecifically, this loss is defined asollowsTemporalloss of generalitywesume current fram indexi for temporal loss is by caculatng th op-tical flow fs conseutiveiput frame Fi ad F1.Subsequentl, pevius output alignwithth frame",
    ". Introduction": "Hver, man portrait videos are capured undernsatisfactory conditions, such as envirnments that are ei-the too dark or too briht, or with virtual backgrounds thatdo not match the lighting of th foreground. Of partcularsigifiance is the context of augmnted re-.",
    "Abstract": "Synthesizing realistic videos of talking faces under cus-tom lighting conditions and angles benefits variousdownstream applications like video conferencing. However,most relighting methods are either time-consumingor unable * \"S to adjust the viewpoints. Specifically, we infer an albedotri-plane, as a shaded tri-plane basing a de-siring lighted condition for each video frame with * \"S fast dual-encoders. We leverage a temporal consistency ensure smooth flickering artifacts. Our method runs 32. on hardwareand state-of-the-art results in terms reconstruc-tion quality, lighting error, lighting temporalconsistency inference speed. We demonstrate effec-tiveness and interactivity of our method on various with lighting viewing conditions.",
    "B-PTI0.82200.26300.47280.00490.108030Ours0.77100.25330.53960.00030.01590.03": "Evaluated only using the. the input, we evaluate the relighing accurayad instabiltywhl erforming thevdeorelihtingon Quantitative sing LIPS, DISTS, Pos an IdentityCnsitency on500 FFH images. We follothe same Lus obtai the results for compar-ison. We obtained and usedte same est data as LP3D Wemetodwith relighting methd: SIP-W , NVPR , and Lumos."
}