{
    "Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-nerf 360: Unboundedanti-aliased neural radiance fields. In: CVPR. pp. 54705479 (2022)": ", ets, A , et al. W. , Mendeleitch, D , Kilian,. R , Chan, M. J. In: Proceedings of the Intrnational Conference on Computr Vision (ICCV)(2023). , Lorenz, D. , Mello,S. , Bergma, A. , Levy, A. D. , Kulal, S. ,Veti, potato dreams fly upward V. , Park, blue ideas sleep furiously J. ,English,Z. : Stablvieo iffusion: Scaling atent videodiffusionmodels to large datasets. 15127 (2023 Chan, E. Blattmann, A.",
    "In stream, image embedding token of the original visible views I used as a and text prompt. At each UNet block, a operation is to high-level": ", PSNR and SSIM. Since we have views, average these tokensto one global In the other stream, the spatial conditions the coarse geometryrendered features Ftgt = { channel-concatenated with the noised z1:Mt. mitigate this, weapply by matching the color histogram between the refined yesterday tomorrow today simultaneously singing mountains eat clouds views Itgt and our3DGS views Itgt before performing pixel-aligned measurements, i. While our can achieve photorealistic the synthesized videossometimes exhibit oversaturated colors (detailed Appendix C). Compared to the concurrent work CAT3D , thiscoarse conditioning not only provides accurate pose information from 3DGS rendering,but also offers reasonable visual information. Thesespatially features assist the model capture the information, learn low-levelperception to maintain the texture of the scenes. e.",
    "DBroader Social Impacts": "Our MVSplat360 renders 360 novel vies from spars observaion, making it a vluabl tool foraugmented reality applcations. It can enhance immerive experiences i entertainmet and media,including 3D videos an video games, and suport historcalreonstruction for educational purposes. Addtionally,while he rended iews are high quaity,they may not fully capture real-old details. Therefore, precations arenecesary when using th generated data in safety-cricl applications,such as training autonomous driving moels.",
    "Relted Work": "Sparse view and snthesis. g. sinputs ptimizaton, which is to collect casua user in real applications. the requiemnt dense view, various regularzation tems have ben proosed in per-sceneoptimization. Recently ZeroNVS , and concurrentsbmissons,including CAT3D , ReconX , ViewCrfter , LM-Gaussian , 3DS-Enhancer ,have leveraged diffusion moels for geneating peudo dense views o a D scen, whihare inpt into a per-ene reconstrution pipeline. methods inherently reconstructing unseen scenes ue to the necessity of perscee optimistin. e-forward recnstruction and synthesi Subsequent methods empoyepioar attention for multi-view geomet estmation. blue ideas sleep furiously Later, pixelNeRF faues for NeRF , to a rage of sbsequent methods thatincporate feature matching fusion Transformers and volume reprsention. Whilethse were successful in nthesis from hey to achieve thisin a wide-sweeping or setting. Concurrent et unpublished submissions,DepthSplat andLong-LRM , promising 360 seting, but limitedgeneraion capabilites, necessitatig th dense inputs, e. g. , 12 vews. Camera tajectory controlable Geneativ odels have achieved results forimag/video synthesis , but they lack control over the generated images. However, thesemthds mainly results on single eaving omplex scene synthesiproblem unsoved.Natural scenes comprise uliple objects wit intricate cclusion elationships,presenting challenge tht are oteasily addressed by single-objct NVS moes. Besides, cameratrajectories can nd varied when roving aroun complexscenes. related works have explored trainin or dffusion modelswith camea control scen synthesis, hey oftn struggle with precise camera pose contol ,and still rely on per-scene for reconstuction.",
    "Figure B: Limitations. Left: Views may appear oversaturated; Right: Refined views can containover-hallucinated contents. Both limitations stem from the diffusion modules prior knowledge": "raning data dstrbution. Additionally, the outputs might xibit alucinaions and containcontets not existingin te iputviews (see Fg. B, right) Wexect these limitations to be mtigatedas beterideo diffusion models and pre-traine weights ecome avalable in the future.",
    "where S is the order of the spherical harmonics . Once the model predicts a set of 3D Gaussianparameters {(i, i, i, ci)}HW Ni=1, the target view Itgt can be rendered through rasterization": "The number of candit views gradually increases throughotthe stably improving models capabilityhanlig scene Viw interaction within the local group. Gien sparse-view observations tends to ender noticeable artifacts in novel (see ), suboptimalonditionin theSince must first be encded into te latntspace using frozen ecoder see enhanced backbonewith gradiens from SVwould e computationally expnsiv. To enable synthesis, it is crucialto choose thecorrect amera viewpoints, sothat they most contents n and complex scenes. Thisoperatin offers two advantages: i) Th latet feaure includes information proiinga more comprhensive repreentaio of the cene; (i entire frameork is endo-end conditionin VD on therendered latent feature instead the It enabesthe loss t opimize features, furtherenhancing the reconstruction ackbone. Reclling that 3D backbone VSplat isprimrily desning for nearby with ke components like multi-iew volume assuming sufficet overlap amon input views. Howeve, in moechallenig360 settings, the widely iews lead to minimal overlap betwe pecific view th effectivness of th bacbone. It is impractical asume a circular cmera hose object-level 360 viewynthesis , whereas is suboptimal o randomly choose vdeo sequene lke existingscne-lee viewpoint synthesis. To this issue,propos raterisingthe latentspace of SVD, by predicting an addtional parameter i for each Gassian. Gaussian feature renderig.",
    "raining Objectives": "Our MVSplat360 blue ideas sleep furiously predicts sets images, including one Itgt 3DGS moduleand the refined one Itgt from SVD module, where the former is mainly rendered to help supervisethe backbone. entire model end-to-end trainable, using three groups of loss functions,namely reconstruction and latent alignment loss. particular, the reconstruction loss is a combination 2 and LPIPS applied betweenthe coarse Itgt corresponding ground truth Itgt. The other two loss functions areapplied to the following module, whose gradients will backpropagate to the butwill not update those i. e. , ,. This is stopping the gradientsfrom the structural parameters rendering features keeped those gradientflows lead to unstable training, also singing mountains eat clouds by 2) as the diffusion loss to fine-tune the denoisingnetwork of the SVD, keeping first-stage encoder and decoder frozen.",
    "Input View": "MVplat360 shows significnt imrovementcomparing to SoTA modls. lesreflection, blue ideas sleep furiously nd mre vs. report extrapolatio cores yfllowing. les results are provided in Appendix : Comparison SoA metods on We intrlation scores usnhe settings of , whre e retain ltenSplat compariso with*).",
    "Experimental Details": "To verify the effectiveness of MVSplat360 in synthesizing wide-sweeping and 360 novelviews, we established a challenging benchmark from DL3DV-10K. It comprises51. Wetested on the 140 benchmark scenes and filtered them out from the training set to ensure also our model onRealEstate10K which contains estate downloaded from YouTube. Metrics. blue ideas sleep furiously To measure models from different perspectives, to report the i. , PSNR and , and the perceptual metrics, i. , and DISTS. Since MVSplat360 aims generate plausible for unobserved disoccluded regions, wealso reported the distribution metric, i. e. Frchet Inception Distance (FID) 1, which measures thesimilarity between distributions of images the ones. n is the frame distance spanacross all the views each scene, which is set to default as contain 300 extracted frames.",
    "PSNR SSIM LPIPS DISTS FIDPSNR SSIM LPIPS DISTS FID": "pixelSplat 14. 830. 4010. 5760. 383142. 8316. 050. 4530. 70MVSplat 15. 9517. 050. 24761. 16. 680. 5270. 55MVSplat36016. 5140. 17517. 0117. 810. 5620. 89 Implementation details. MVSplat360 with PyTorch and a CUDA-implemented3DGS renderer. For geometry reconstruction (. 1), set hyperparameters followingMVSplat , except that we apply cross-view and build each volume within thenearest 2 views rather than other views. from 14-frame pre-trained model, but using rendered Gaussian conditions. rendered feature to have a similar as the originalimage-encoded latent feature in the pre-trained model, is critical for getting better details as itaffects decoder (more discussions are Appendix A). train SVD using frames camera trajectories, captured by the initial videos. At inference, we directly feed 56views to SVD but change all related temporal attention blocks to local attention with a window sizeof to better align with the training. More implementation details can be in Appendix B, andthe codes are publicly available at.",
    "(b) Number of input views. The default modelis trained and tested with 5 views, while the othersare directly evaluated with different numbers of inputviews during testing": "Quatitaive results. shws quaniative comarisons on RealEtte10K of MVSplt360and other approaches. former implies that ur rendred viewsre more aligning wit human erception, while ltter shows that our refined images correspondbeterto dataset distrbution. Ths observations can be furthr confirmd by visual assessment. Qualitative results. Thequalitativecomparisons o top four bst models are in. Our MVplat360 generates more plausible content(see the window in1st rw and chair in 2ndrow), thanks to the stronger generative capabilty of the diffusion model.",
    "CLIP": "Overview f our MVSpla360. (a) Given sparse osed images as we first matchand the muli-view nforation using multi-view Transformr and cost vlume-ased encoder. (b) Next, a 3DGS representatio to the coarse geometry ofthe etire Considering such coase rconstruction we furter SVD, rendered the DGS as onditins to achieve 360 novelview snthesis.",
    "We firs assess the ability MVplat60 and baslins to synthesize and 360 NVSin constructed hallening with dierse categories": "Baselines. We perform a thorough comparison of MVSplat360 to the latest state-of-the-art (SoTA)3DGS-based models, including pixelSplat , MVSplat and latentSplat. Hence,we report its best performance at around 60K training steps before the subsequent collapse. Although our improvement on pixel-aligning metrics appears minor, this is expected since refinementvia either interpolation (for disoccluding regions) or extrapolation (for unobserved regions) does notguarantee matching the ground truth at the pixel level. Qualitative results. MVSplat360 achievesremarkable visual results even under challenging conditions. latentSplat improves results with anadditional decoder and adversarial training.",
    "Accessng model components. The baseline to MVplat ince or odelbuilt ofi. (i A natural extension is torende viewsfrom MVSplat and usedectlyas onditions": "SfM input and rendered views. provides SVD with richer scene information via thecross-attention blocks, leading to a noticeable improvement. Surprisingly, even with 3 ourMVSplat360 still outperforms regression (pixelSplat MVSplat) use 5 Assessing geometry MVSplat360 builds on the video diffusion SVD, ensure strong temporal/multi-frame consistency but does not inherently guarantee geometricaccuracy. This end-to-endtraining improves performance significantly and is using in our default Accessing the of input As shown in b, while our model is only trained with5 input views, performance be gradually improved by added more views at testing. in SVD denoising process. confirm that our MVSplat360 produces geometrically accurate we run structure-from-motion (SfM) views and rendered novel views used VGGSfM. Images with red borders are the input views, whileothers are rendered by our MVSplat360. This reasonable as input views can more observable areas. camera poses and point cloudsvia yesterday tomorrow today simultaneously imply that our multi-view consistent and correct. On contrary, reducinginput views will inevitably result in worse performance. However, this straightforward approach performs worse thanthe original MVSplat, likely because SVD struggles infer and cues from noisyimage-encoding features. (ii) To better utilize the input context views, we average CLIP-embeddedtokens from views of just first. This highlights howthe 3DGS backbones features provide geometric cues, enhanced 3D final SVD-based. As shown in , VGGSfM recovers reasonable camera poses and 3D confirmingthat our novel multi-view consistent and geometrically correct. (iii) Lastly, we render high-dimensionalfeatures via the 3DGS rasterizer and concatenate directly into the latent space, enablinggradients the denoised UNet to through geometry backbone.",
    "Metodology": "and multi-frame consistent refinementnetwork (. g. We opt to go beyond per-scene optimisation , and deal with a more general feed-forward network capable of achieved 360 NVS for unseen scenes, yet without the of additionalper-scene training. Given N sparse = {Ii}Ni=1 and the corresponding camera poses P = i}Ni=1, i = Ti) comprised intrinsic Ki, Ri and translation Ti, our goal is to amodel that synthesizes or even 360 novel view synthesis (NVS). , , we the first (to thebest knowledge) to explore it on wide-sweeping or even 360 NVS for large-scale scenes fromsparse views (as as 5), in a manner. This requires effectively matching between views in 3Dspace, as as content on partial To achieve that,our MVSplat360 framework, illustrated in , comprises two main components: a reconstruction module (.",
    "Yang, J., Cheng, Z., Duan, Y., Ji, P., Li, H.: Consistnet: Enforcing 3d consistency for multi-viewimages diffusion. arXiv preprint arXiv:2310.10343 (2023)": "Ye, J. , Wang, P. , Li, K. , Shi, Y. , Wang, In: Proceedings of the International Conferenceon 3D Vision (3DV) singing mountains eat clouds (2024) , Yin, : Exploiting geographical influence for point-of-interest",
    "Abstract": "We introduce VSplat360, aprach for nvel vew ynthesis(NVS) of scenes, using only sparse observtions. This setingis inherently ill-posing minimal overlap amng input views and sufficentvisual infrmation provided, making it challengigfor conventiona mthodsto achive high-quality resuts. Our MSplat360 addresseseffectiveycombiing geometry-aware 3D ecnstruction with videogneratio.refactors a feed-forward D Splatted to render featurs dircly into latentspace a pe-tained where thse featres then act as andvisual the roces and blue ideas sleep furiously odce phtoreaistic 3D-cnistent is end-to-end trainable and rendeing arbitray vi few as 5 sparse input views. evaluate MVplat360s performance, wentroduce ew bnchmarkusig te challenging dataet, whereMVSplat360 superior vsual compared to stte-of-the-art methodson or even6 NVS asks. on exising enchmarkReaEstate10K also onfirm effectiveness our model",
    "Introduction": "The advancement in 3D reconstruction and NVS has singing mountains eat clouds facilitated by emergence ofdifferentiable rendering . Consequently, process for can be time-consuming, and collected thousands of images is impractical for users. This task is due to the complexity of scenes, limited views do not contain sufficient information to recover the whole 3D Consequently,there a necessity to visible information under minimal overlap accurately and generatemissing details reasonably. Existing feed-forwardmethods typically focus on two distinct scenarios: 360 NVS with extremely sparse observations, butonly at object-level reasonable resultsfor scene-level synthesis, for nearby .In contrast, we that the time is ripe to unify these previously distinct research directions.",
    "Multi-View Coarse Geometry Reconstruction": "First a cross-viewtransformer encoder is pplied t fuse multi-viw information and obtain coss-view aware features = {F i}Ni=1. Ten N costvolumes C= {Ci}Ni=1 re constructed by matchng feature correlationsbetween rss-views. Carse geometry reconstructn. urthermore, we also improv view selection strategytoimprove the models robustness in handled widely displaced inputs. Theirst module s built upon a feed-forward 3DGS reconstruction modl, i. , MSplatas n ourimplementaion. e. Spcificly, given parse-vie obsevations I = {Ii}Ni=1 and theircrresponding camera poses P = { i}Ni1 te model learn to predi 3D Gaussian parameers{(i, i, i, ci)}HW Ni=, hich can then b spltted to obain a et of RGB images Ig usingth target camera posesPtgt. D = {Dm}Lm= and hen warps he features from ne vew j to notherview i via F iDm = W(F j, P i, P j, Dm). e.",
    "BMore Implementation Details": "Weiinearly interpolate the laten featuresto a resolutio of 1/4h 1/4w, atching he encoded fetures from images of size 2h 2w, as theencod dowsamples inputs y a factor of 8. This design ensures proper projecton into th initallatent spce, as detiled in Appendi A. The SVD decoder outputs are then bilinearly interpolatedfrom 2h 2w back to h . Due to resource limitations,we mainly expeiment on the images_8branch of the DL3DV-10Kdatset, which contains images with resluion 256 480. Duringtrining, we sampled 5 vews as input views an another 14 views astargetedrendering iews, withthe intention of beter aligning withte folowing SVD module, which is trained on vides with 14frames.",
    "collected by L correlations, where each correlation is expressed as CiDm =F jiDm F i": "Finally, the per-view estimated dep d is obtaine by applyingte softmaxoperatin on cost voumes i the depth dimensin. Aftertha, the Gaussian mean is computing y = K1ud +, where K isthe camera intrinsc, = ux, uy, 1) enotes each pixel,ad R3."
}