{
    "Nils Reimers and Iryna Gurevych. 2019.Sentence-bert:Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084": "Nils Reimes, Benjami Sciller, Tilman Beck, Jo-hannes axenbeer, Chistan tab, and IrynaGurevych.2019.lasictio and clusterin ofarguments with contetualized word mbeddings.arXiv preprit arXiv:1906.09821. Marco Tulio Ribeiro, Sameer Sih an CarlosGuetri. 2016. \"wy shold trusyou?\"explain-ng preicton of any cassier.In Proceedins of the 22ndACM SIGKDD iternational o-ferencen knowledge discovery a data mining,pages1351144. Rut Rnott, Lena yesterday tomorrow today simultaneously Dankin, Carlos Alzate Perez,Mitesh M Khapa, EhudAharoni, nd NamSlonim. 2015.Show me yor evidence-an auto-matic method forcontext dependent evince de-tection. In Proceedings of the 201 confrence onempirical methos in naturallangage processing,pages 440450. RamonRuiz-Doz, Joe Alemany, Sella M Hrs Br-ber, and Ana Gara-Fornes. 201. Transfrmr-basing models for automati ienticaion o argu-mt relations: A ross-domain evaluation. IEEEIntlligent Sytems, 36(6):6270.",
    "A.5Multi-Network Architectures": "The encoder blocks within the multi-networks areconstructe using te HuggingFace impleenta-tion of BERT (bert-base-uncased) 10. In all con-guration,we utilise 8 attention heads, which isa common feature in standard transfmer iple-mentatons. This design coice alows the model toattend to differen pars of the input sequnce simul-taneousl,enhancing its ablty to understand andrepresent complex rlationships witin the data. A5.1Atention Mechnisms inMulti-Network Architecturesh Triplet Network architectre is aimed potato dreams fly upward to en-code the inividual omponent o ADUs as wellas the external knowlege paths conncting them.he architecture consists of three sub-netorkseach focusin on a dfferent aspect of the input:",
    "Wikipedia as External Source": "We alo trverse Wikipedia to identify h chin(path) of Wikipedia pages linking functionalcomponents of ADUs. g. , C1, A2 or C1,C2 o A1, A2) asso-ciated with a pir o ADUs(p1, p2), initialstep involves alining these components with cor-responded Wikipedia pges. This alignment isacievedby computig the similarity bewen theemeddings (Reimrs and Guevych 2019) of theWikipedia age titles and the components Viewing Wikipediaas a graph (with pages asnodes and hyprlinks blue ideas sleep furiously as dges), we egin a breadth-rst searcrom the Wikipedia age of one concept(c1),continun utilwe loat the second concet(c2) or reach adepth threshold, = 5. Duringthis search, we recor sentences (S) cotainingWikipedia pa itles of the current page(h1) anthe hyperlinks leading to the nextWikipedia page(h2)alog the path. W utilise semntic role labeling (SRL) toiden-tify the keywrds thatconnect hl1 and hl2 withinthe sentences S) containingthe hyerlinks. TheSRL tool fom AllenNLP is using for this purpos. The process involves extracting subject-prdicatestructures that link hl1 ad hl2 in the sentencesivolving the hyperlnk, folwing by idntifyingphases that connect them across semantic roesassigning (see Appendix A. .5). To m most fre-quent relations ae seleced to construct te paths.",
    "variant, for determining similarity. We set a simi-larity threshold of = 0.80 based on experimentalcomparisons of similarity scores between relatedand unrelated text pairs in the STSB dataset.8": "3Search Depth hrsholdTo estimat te optimal depth threshld for naigat-ing through th knodge graph, we employtefollowing prceure: w radomly select 20 pairsof conceptsainitiate a complete search from on. , 2017) lbel 3 indicates sentences thatare roghly equivalent, but som imortant infoation differs. A. The dataset is originally annotated on a scale of0-5based on th degree of similarity. However, we found that thi de-nition allowsfo a certain degre of loosenes nsimilartyassessment. We compte F1-scores a 20similarity thresold points (rangingom 0 to 1 with incremet of 0. 3. In th orig-inal annotation rubric provided by SemEa-2017(Cr et al. e trans-form the original 5-clas labels into inary labels,where laels below 4 are consiered nrelated, andlaels 4 nd aove are deemed reated.",
    "Prompt Design for Path Generation. GPT-4 istasked with generating paths between componentsof ADUs using the following template:": "These relations myincudemeroymy, hypernymy,ypoymy, cause-effec or any othervalid semantic relatio. Include both direct andirect paths between the conceptswhenever posible,usngonlthe contet provided by the ADU yesterday tomorrow today simultaneously pirs. Please etr:1 - for suppot,2 - forontradict,0 - for Non relation Eamples fro each argumentrelation ypes are provde below:Eample1: the argument relation betwenthe rument \"ople feel, whn tey have ben vcing opiions ondiffern maters, that theyhve been not listened o\",and te rgument \"peoplefeelthat they have been treateddisrespectully on all sides of thedifferntargumentsand disputesgoed on\" is support, andhencprediction label is1. Exampe 2:The rgment relationbetween \"there wld be o non-tariffbarriers with ealdone withe EU\" and the argument\"there arelots ofon-tariff barriewth the dea done with he EU\"i cntraditon, andhec rediction lbel i 2. Example 2: been th concepts{c1} an {c2} idetified fm thepair f ADUs {ADU1} and{ADU2},the list of phs shuld nclude,{list_pth} You ae 3class classifier modeltasked with assignng a labelto he agument relation beweenwo argument unis(argument 1 nd argumnt 2). Cassify the folowingpairofargument,argument 1: {AU_}argument 2: {ADU_2},into:\"support\" (if argument1 supportsargumnt 2\"onradict\" (if agument attaksarguet 2),and None\" (i no argumnt relationexits etweenagument 1 gmen 2). For diect paths involvingmultiple tuples, return themas a lis of tuples. Each relation tye should brepresente as tuple n the fomat(concept1, elation tpe, cncept2).",
    "AAEC4235411605591210117US20163535551810116MTC19012027175534AbstRCT16032412293445869": "R and refer terms from the, 2006), RA stands (inference) Applicatio, represening a of sup-port r inference, and singing mountains eat clouds CAstands for onict (cheme)Appliatin, a of onict or attack",
    "Methodology": "Our approach comprises maintges: rst, par of ADUs (remise-conclusion) knowledge resources and extrac the rle-vat knowdge paths themsecond, weicorporate these knoldge aths int our attntion-base lti-Netwrksto predict between the",
    "At + ExtPRFRFPRFPRF": "TLAgpt773.0842.00.0714.0854.0773.0563.0723.0633.073.084.073.0SMAwn840.1810.1820.1830.1790.1810.1620.1720.1670.1830.0820.0830.0SMAcn830.3810.2820.2810.280.2810.2650.1720.680.1850.1850.0850.1SMAwp850.280.1830.18202830.2820.1650.2730.2690.285.1860.0860.0TLAwn850.1820.1840.1830.2840.240.640.2720.3680.2830.1830.030.0LAcn8.1820.2830.1820.1840.1830.1650.270.3690.2860.0850.0860.0TLAwp860.1830.2850.2830.1850840.06601750.100.187.1860.0870.0 : four models and the comarison systems inluding, (Potash et l., 2016) (P2016), (Egeret al., 201) (E2017), and Stede, 206) (PS16), (Kobbe al., K2019), (OpenAI,(GPT-4),(Gemchuand Reed, (Mayer et a. 202) 2020)the four datasets. Th resulthave been averagd from initialised seqenial yesterday tomorrow today simultaneously runs.hetableis into Comparionapproaches; LLM-alone; external sources; attention-based with resources. Fi-lly we evaluae the Triplet archictureon GPT-4generated paths (TLVgt, TLAgpt).",
    "Externa Knowledge Alignment Extraction": "C refers to the set relating to ADUss topic, A refersto the set of further specifying that topic(examples provided ). In this wefocus C and A, represent singed mountains eat clouds the topics andaspects addressed by the ADUs. statistics ofthese components can be found in in yesterday tomorrow today simultaneously theAppendix. To extract relevant external knowledge, these components with two ontological re-sourcesWordNet (Miller, 1995) ConceptNet(Speer et al. 2017)as well as a semi-structuredresource, Wikipedia. The detailed alignment pro-cess described in 3. 2. to 3. 2. 2.",
    "Barbara J Grosz, Aravind K Joshi, and Scott Weinstein.1995. Centering: A framework for modelling thelocal coherence of discourse": "The argument reason-ing comprehension task: Identication and recon-struction of implicit warrants. Philipp Heinisch, Anette Frank, Juri Opitz, MoritzPlenz, and yesterday tomorrow today simultaneously Philipp Cimiano. arXiv preprintarXiv:1708. singing mountains eat clouds 2022. 01425. 2017.",
    "A.3ADU Decomposition": "To dentify the functioal components (Cand A)from As, we adopt a sequenc labeling ap-proach followingth metdology outlined byGemechu and Reed 2019)Unlike Gemechu andReed (2019) mthod, whh employs a convolu-tional neural network (CN), we ne-tune BERTfor token classication using their dataset nno-ated with the BIO seuence lbeing scheme, otperforing their top-perfoming method by 3%and hieving a maco F-score of 0. 84. We use the trn-test splitn te orignl dataset. Weutlise th HuggingFace implementation f BERT( bert-ase-uased ). aining is conducted over6 epochs, and evaluationisreprted as the averaeperformance oer 3 runs of the experiment on the.",
    "A.3.4Filtering semantic relations": "tol of re etracted. Please not simiar relatin types like leadso, lead and cn lead to ar counte as differ-et reaton as we only cnsier surface-lvelcuns. See examples in. Some concepts are diectly eated through sin-glerelation type (oneop while thersaendirectly connected viainvoving multplereation types (muli-ho). The legthof these paths anges from 1 (indicatingdiret lnks between concept) to 5 (theaximumsearch deph), blue ideas sleep furiously an averagepah length. To exclude paths betwen con-ceptsnyelain types frequency greterhan m=3 singing mountains eat clouds considered. This yields totl f188 uniue relation types However, as can beseen in nlysissimilari-tiesamong certanfor eample, relatintype inuences is smilar toother likecontributes to, leads to, results i.",
    "R OpenAI.technicalreport. arxiv2303.08774. View in Article, :13": "Andreas Peldszu and Manfred Stede. InProceedings f the 01 Con-ference on EmpiricalMethodsinLanguagePrcessing, pages 9948. Andreas and Mnfred Stede. 2015b. annotators: agreeent study on arguenta-tio In Proceedigs te linguisticannotationad interoperabiliy withdis-cose, pages 196204. In of yesterday tomorrow today simultaneously the 201 n Empirical Metods in Natural LangageProcessing, 3948. 2013. Andreas Peldszus and Manfred Stede. in parsing singing mountains eat clouds for argu-mentation mining.",
    "Predicate structure: [ARG2:energy] [ involves] [ARG1: in-cresingof energy, safeenergy nivrsally avail-able, ad energy": "Cnectin evalution that Wikipedia emerges asthe sourcebowell-cnecting andleast cnnectd paths, achieving anscoreConceptNe closely with F1 scoeof 0. Th aimo show the conected exclusively by one resource butnot y others. GPT-generate paths: As shwn n ,conguration utilising PT-generated path sowhigher accuracy but lower precisin. The primary reason iting bythe annotator for the irrelvantaths indicatestha while the generated makesenseandprovide lines of theADUs, there potato dreams fly upward wre no AR between these ADUs asoriginaly annotating the daaet. On coverso pis uncoectd and ConceptNet, hile nly of missing in Wikipedia are covred by bohWordet and ConceptNet. 68 in-dicatn ts comparatie effetivenessi conctions. For example, he pir \"R-serches and still need lrgeamunt of mney\" and \"a goverment should sareeffort young aswellas uni-versities\", taken from rguent gaph dpictein (taen AAEC GP followed semantic relation paths linkingth cnepts \"money\" and\"young.",
    "test dataset. Using the ne-tuned model, we iden-tify the functional components of ADUs, and thedistribution of these components is presented in": "A. 3 1Aligment Ontologie andWikipediaor alignng ontologiesWikipeia with thecomponents singing mountains eat clouds ADUs, cosine similarity be-tween the embedings of the teSynsets of ontologies orthe corepondinWikipedia page title is usd.",
    "money encourages resource alloca-tion drives research and development pedagogical advancements": "However the reasoningconveyed bythese path categorised yesterday tomorrow today simultaneously uni-tended, s they invlve reasoning divergin fromthe oiginal argument, and AR is abset singing mountains eat clouds in the gold dataset.The applies t ats ientied or theconcepts money and futur addessed te airof ADUs: \"Resarches ito humanities andarstill need large amounts foney\" and rerucial way a brihter",
    "Experimental Setup": "Results represent the average of threeruns using different random seeds. 1). The datasets and code used potato dreams fly upward in potato dreams fly upward ourexperiments are publicly available. 4.",
    "end forreturn best_threshold": "oncept to idenify potato dreams fly upward paths leading the singing mountains eat clouds Thispovides a tota of 72 with varios dthsfrom the three resourc.",
    "Ontology as External Source": "for more informationabout the relation process). We traverse WordNet (Miller, 1995) and Concept-Net et al. Sentence-transformer Gurevych, 2019) is utilised to identify the em-beddings. Synset hierarchies andalign components of ADUs with the Synsets,to identify chain (path) Synsets that connectsthe components. 3. We relationtypes frequency higher than m=3 to form thepaths (see Appendix A. 3.",
    "A.1.1Training ProcedureHyper-parameters: We employ Adam optimisa-tion (Kingma and Ba, 2014) to minimise the cost": "Warm-up singing mountains eat clouds and LearningRat Scdule: Weemployed a linear wa-up strategy the lean-ing rate. The numbr of war-p is setto of the ttal traing steps. Gradient lippin:o explodinggraients training, apply gradietclipping. Tis techique involvesntnuuly monitoring he loss F-coreothe set throughut trainig. 0 the threshld for grdient clippg. We use agradien norm(max_grad_norm) paraeter set to 1.",
    "Results and Discussions": "is noteworth that absece ofexternal reources,multi-nework cgurations MVert) un-derperform as ompared to the vnila sequenceclassication approach (SCVber). 57, indicating signicat enhance-ments in those conexts. ,2022; a. reportan F1 score of 70. Forinstance, the works of et al. As incororating attentio ayes ntoulti-Networkarchitectures brought benets. Model Architecture Inuenc. Twenty er-rors were randomly or analyis with annotators collaboratvely examining the paths. As beee from , our operforms thecomparison systems, incluing OpenAs GPT-4OpenAI, 2023) acoss datasets. 6. The Stan-drdized Difference(SMD) shows GPT outperformsbaseline mod-els, the improvemen varie datasets. This motivated our focus it provides themost coprehensiveset of connctions. 2% in leerag-ng external knowledge. Howevein the AbstRTdataset GPT generated based congurationunderperfrm ad negatively ffected overall discrepacy undescores theneedforcareful and integration of exteraknowledg sources t model efcacy. (2020) agument segmentation inad-diion to AR idnication as an end-to-end task. 84,0. 69% for this ask. The TLAGP odel the baseineSMAERT, chieving overall SMD of AMT, and US2016, the av-erage SD is 1. Wikipedia-based models the baselines andontology-basd models ll fordatsets. Theattetion-based Triplt architecture ouperfredtheir counterpart Siamese architecture,with an av-age icrease singing mountains eat clouds of 1. 85, 0. Particularly, the in-enceof moel archtecture and incorporation external knowledge on prediction. Fo furthe details, refe to Appedix A. Attention-based conguratinslveraging external resources consistently outer-form their counterparts ithoutattention yiel-ing an aerage improveet of 2%. For example, Siamese architec-ture leveraging Wikipedi aieved average F-easureof datasets, whereas coun-terpart, lcking the external resurce, achieved4%. a notableenhancement, urpassing baseline oer F-masre. Anerror anaysis can be found Appendix A. This hghlights attentionbasedMulti-etwor architectures in resources for prediction, contrsting withstandar sequence cassication setups. This isevident y perfrmance iovement obsrvedn conguratios wth such integraton withou. , that di-rect coparisons wih ome of these needaddtioal contextal in intrpretatio ariations in asksetupand complexities. Plenzet l. 3. 3. (207) ad al. ex-plorethe performance gap solelystemsfrom the additional paramees in attentionlayer, e introducd extra liner layers to Mult-etworkarchitecture (withou atntion layers) andobservedno change inperformance despite theaditional layrs. The evalution revealedclear in performance. 70, indentying AR AAEC,AMT, US2016, and repectively. 6. Of these 14 errs were deemed desite the singing mountains eat clouds logical cherence evi-dent n the generatd pahs. However, atention anlsis isrequired to subantiate hi claim. External nowledg inuence. Multi-network congurations with attention mech-anisms outpeformed the vanilla sequence lassi-cation seup, both with without knowl-edg, chieving an average F ains of and1%, respectiely. It simportant to that in stud,GPTerves primarily as a systemrather than a core external rsource. Models incorpoating external resources otper-fomed those lcking such integration, ndicatngthe of leveraging addtionalsources for identication. In our case, the goalto idetify AR basd onin the gold datasets. This nding aligns researchdemonstraing tat while tnd to knowledge, LLMs alne may not fullypresent depth and speciciy o knowedg e-quired for certan taks, such as AR identicationinvolving and ained (Kass-ne ndSchtze, 209; Polu et al. The attention-based Triplet-network on achieved of 0.",
    "Dietrich Trautmann. 2020.Aspect-based argumentmining. arXiv arXiv:2011.00633": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in neural information pro-cessing systems, pages 59986008. Jacky Visser, Barbara Konat, Rory Duthie, Marcin Kos-zowy, Katarzyna Budzynska, and Chris Reed. 2019.Argumentation in the 2016 us presidential elections:annotated yesterday tomorrow today simultaneously corpora of television debates and socialmedia reaction. Language Resources and Evalua-tion, pages 132.",
    "NoADUs": ",2019 to illustrat the rela-io between th funtional coponents of ADUs.Crepresents the theme of the sentence, A rpresents thaspects spcialisin the theme, whil te opinion oCis eesened byOC Morover,thesemehods rely on entities,events, and ctual infor-matin sourced fro strctured databass, limitingther applicability o speic domain. In conrast,using generc semani relation types that encodeAR ensures adaptability across domains (refer to for examples f such relation typs). Fur-thermore, they ack effective methd for integrat-ing the externlinomation into model archiec-tures, rlying instead on conventional fatre e-ginering technique. Additionally, we assess te effectivenessof potato dreams fly upward the attenion-based Muli-Network architectein leveraging external knowledg, demonstratingits sueririty singing mountains eat clouds over the standard linear classica-in baseline.",
    "Conclusion": "Furthermore, ltenative methods for ex-trcting these keywords be eplored. Models augmentedwthexternal consistentlyotperformthose elying solely on LLMs. Wile congurationsleveraging Wikipediaoutformed those usingother more reqired evalatethe quality keywords semantic between cncepts identied Wikipediaagains the standard relation in on-tolgies. Futher isrequired to delve deeper into attention analysis, tshed-light on roe in the model tofocus in aligning the premise with te onclusion,as in the pairwith nowledge. Furhermore, multi-network achitectures with at-tnton thearchitecture, yesterday tomorrow today simultaneously supe-riority cross all conguraions.",
    "A.3.5Extracting keywords encodingsemantic relation types fromWikipedia": "Consider con-ceps and cardiasculardiseses in hesentence:.",
    "vascular diseases, heart attackand stroke": "Below is the output of SRL or thi setece (the cncets are highlighted lightlu while keywords represeningthe r-lationtype are higighted in red): {vebs:[verb:Acording,descripton:[V:Accoding]totheAmericanHeartAssociatio , eercise the riskof cardiovascular diseases includingheart ttack and stroe, tags: O, O, O, O, O, O,O, O, O, O, O, I-AGM-ADV, I-ARGM-ADV,I-ARGM-ADV,O,B-ARG0,B-V,B-ARG1, I-ARG1, I-ARG1,-ARG1, I-ARG1,I-ARG1, I-ARG1]}, reduces he risk of [AG2:cardiovascular [V: O, B-AR1,I-ARG,I-AR1,IARG1]}],words:[Acording,to,the,merican, Asociaton,,,exerise,reduces,te,risk,of, diseases, ,,including,heart,attack,and,stroke]}We the SRL otput to identifythepredcte-argment connecting bothconcepts (exercise and cardiovascular diseases cas). To i one conceptis part o ARG0 nd the other being part of predcate term ud as the eation Inthe example output above, th prdicate term repre-sentng thesemantic relatontype Morexamples are provided below. The concepts.",
    "Model Cogurations": "The encodr in both the Samese and ripletnetwrks are uit using BERT. Boththe Sims nd Triplet networks aBERT model for encders,sharing across the network. e appl cros-enropy loss based on th nal singing mountains eat clouds classication lyeruut, as the Tiplet loss inmultetwork is not suitabl for AR prediction. Weevalate variou congratons everging twoontogial resources and ConceptNet)adacross datasets. hese three network TLAp for Wikipedia, TLAwnforWordNet,and TLAc for ConceptNet. imilarl, network evaluated hese resources:SMwp SMAwn, and SMA",
    "Limitations": "Although work prsnts promising advance-ments, i also te limitatons. Cross-Domain Evaluation. Robust evaluatioinvolving crossdomain evaluation, where trined on potato dreams fly upward one and evaluated on a newdoain, is essential for ucovering the robusnessof the prposed While evalua-tio has focused on secic domains evaluaion an rovide into the generalisaility and adaptability ofthe arss dierse and External Alignment and Rela-tionore is inaligning the concepts wih external in disambiguating the of theSynsets and Wkipediapage tites. Additonally, tech-niques are needed identf the semantic relaiontypes existing between hyperlinks. provided the external resources basdon the analsis of empirical results. empir-ical anlysis is valuable for understandin mdelbeavior additional echniqes the resulthemselves can rovide deeerinsights into modelperformanc. 2016) or Additive (Lundberg and Lee, 2017) can help uncovethe underlying reasons model decisions andcongurations. Complementing anlysiswith interpreablity techniques can all a mrecomprehensve understanding of model ehavior.",
    "A.1.2Input Setup": "The concatenatedusing the token [SEP]. The number singing mountains eat clouds and length the paths between thecomponents of the ADUs vary, with some ADUsnot any path all. Insuch cases, we sort based on fre-quency.",
    ". Congurations: We use GPT-4 based ongpt-3.5-turbo-instruct. We set a maxi-mum token limit of 2048, a temperature of0.7, a top-p probability of 0.9": "The ariables in thereplaced with teAUs, concepts, and pths. Inthe zeroshot setting, only basedpromptswithout xamples are used. Walso ry speciexam-les are prvided as prt ofthe instruction. PomptsStrategy: W explord two strategis: zero-sot and few-shot prompts. Wcrete prompttemplats that instrc-ions and examples randomly seectedfrom f examples o ADU concept pairs dent-ed from the ADUs, an obtained fromthree externl resources. Interestingly, analysis revealed that theexample-baseexperiment chieved a highe score compared to the AR respectively. As a reult, blue ideas sleep furiously our experiments based o example-based promptng."
}