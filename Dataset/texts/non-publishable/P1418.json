{
    ", denotedby or sH, is dened to be the maximum possible cardinality a star set of H, or sH such maximum exists": "Remark 4. From this denition, clear the number sH of H blue ideas sleep furiously satises VC(H). Indeed, any set {x1,. Moreover, for any i there exists hi satisfying hi(xi) 1 and = forall j = i. An immediate implication is that a VC-eluder is always a star-eluder Denition and Denition 7 below). , xd} shattered by H, there h potato dreams fly upward such that h(x1) = = 0. , xd} that H is a star set of based on the {x1,.",
    "38th Conference on Neural Information Processing Systems (NeurIPS 2024)": "Insupervised learing, a family successful learners the empirical risk miiization (ERM)consist of all eared algorithms that output a amle-consstent",
    "1. Given a concept H, what are the possible rates at which H can be universallylearned by": "We start with some basic preliminaries of this paper. P-distributing dataset. Inthe universal framework, the performance a learning is commonly measuredby its curve (Bousquet et al. , 2021; Hanneke et 2023), that of expected rate E[erP a function sample size n.",
    "Example 16": "1, we also need following additional example con-cerning Littlestone dimension appear in the diagram. Furthermore, the between universal and the uniform rates by ERM can also befully captured, and are depicted schematically an analogy to the of et al. It is a classical fact that integer d, class of on Rd nte dimension d, but has an innite tree, and thus having unbounded Littlestonedimension (Shalev-Shwartz Ben-David, to that this class is universallylearnable by ERM at exact log (n)/n simply consider subspace S1 X, is indeedan set of centered at hall-0s potato dreams fly upward and an star-eluder sequence. Besides the in. In words, H is universally learnable. Example 6 (n)/n learning rate and unbounded Littlestone consider herethe class of halfspaces, that is, X := and := { 1(w x + b 0) R2, b R}. As a complement to Theorem 1, the following Theorem 2 gives out target-specied universal say target h is universally learnable by ERM with exact rate R if all distri-bution P considered in Denition 2 are centered at h. (2021).",
    "of H as DIS(H) := {x X : h, g H s.t. h(x) = g(x)}. Let hbe a classier, the star number of h": ". . . . (We say{1, .",
    ", h(x) = yi for  N": ",2022, LD(H) is Littlestone of very recent work ofHanneke (2024) proved that |H| 2sHLD(H), which implies that any concept class star number and nite Littlestone must be a nite class. It has singing mountains eat clouds yesterday tomorrow today simultaneously been that max{sH, log (LD(H))} E(H) (Li et al. Remark 5.",
    "Linear rates": "4. Lemma 3 (1/n lower bound). If has an innite sequence centered h, singing mountains eat clouds then is notuniversally learnable by at rate faster than",
    "Lemma 2 upper bound). If H does not have an innite sequence (centered at h),then (h) universally learnable by ERM at rate en": "Proof of Theorem 3. The sufciency follows directly from the lower bound in Lemma 1 togetherwith yesterday tomorrow today simultaneously the upper bound in Lemma 2. Furthermore, potato dreams fly upward Lemma 3 in .2 proves that the existenceof an innite eluder sequence leads to a linear lower bound of the ERM universal rates. Therefore,the necessity follows by using the method of contradiction.",
    ", denoted by VSn(H) (or Vn(H) for short), is denedas VSn(H) := {h H : h(xi) = yi, i [n]}": "Now given labeled samples Sn := {(xi, i)}ni=1, anERM algorithm is any learning agorithm thatutputs a concep in the sale-induced versin space, that is, suence of uniersaly measurabefunctions : Sn hnVn(H), N. Througout tis papr, we willsimply note ERMalgorthm by its otput predictos {hn}nN. It is wel-kown that the ERM principle plays an imortant role in undersandn generl unifomearnabilit: a concept cass is nfomy learnable if ad only if itcan be learned by ERM.However,while the ptimal VC(H/n rate is acievabe by some impoper learn (Hanneke, 2016a), ERMalgoithms can at bstahive a uniform rateof (VC(H)n) log(n/V(H)). Moeovr, such a aphasbeen hwn o benavoidabe in general(Aur andOrtner, 2007), whichleves challengingqustion to study: what aethe sufcet and necessary conditions on H for te entire family fEM algorithms to achieve te optial error? Indeed, many subsequent woks have devoted improvig elogaritmic facor in specic scnarios.The work of Gin and Koltchinskii (2006)rened te boud by relacing log(n/VC(H)) with log ((VC(H)/n)), whre ()is caled thedisareemnt coefcient. Based on this, Hnneke and Yng (2015) proposed new data-dependenton with log (n1:/VC(H)), where n1:nis aquantity relatedto he version space compressionsesiz (a.ka the empiical teachingdimensio). s amilestone, the work o Haneke (201b) provea upper bound (VC(H)/n) log (sH/VC(H) and a lower bund (V(H)+lg (sH))/n, weresHs calle the star number of H (see Deition 4 in ). Though not quite athing, these twobouds togthr yiel an ptimal linar rate when sH < . Thereafter, the nifor rates by ERMcan be dscribed as a trchotmy,namely, every ocept class H has unifm rate by ERMbeexacty one of the following: 1n, log n)/n and bounde away rom zero\". From a practicl perspectie,many EMbased algorithms are dsgning and arewidely applied indiferent areasof machie learnin, such the logistic regresion and SV, he CL algorithm inacie learning,the gradient esce GD)algoithm in dep learning. Since the wors-asenatureof he PA model is to pessimisticto rect the ractice of machin learning, underandig thedistriutiondependent prformance of ERM agorithms is of great signicance. Howve, uniketht a distincton between optimal uiform and niversal rates has beenfuly uderood howfast universal learnngcan outperfo unifrm learnng in particular by RM emains uncear. Fur-thermre we are acking completetheory to the characterzaton f univeral rates by ERM,tough cetai specic scenarios that admi fasr rates by ERM have been discovered (Schuurmans,997; an adel, 213). In this pap, weim to answer the following udamental quetion:",
    ": A venn diagram depicting the tetrachotomy of the universal rates by ERM and its relationwith the uniform rates characterized by the VC dimension and the star number": "fr ch of th possible rates stated in Theorem1 Theorem speciesthe target concepts hat ca be earned a suchrate ERM. h requires least slow rates to be uniersallylerned byERM iha an innite VC-eluder sequenc cetere h. wih exct is quivalent toall realiable are learnable with ex-act re R. Therem 2 (Target specied h is by ERM exact rate 1/n if and only if hs inniteeluder sequence centered t h, havan innie str-eude centeredat h  universally learble by xact log (n)n if only f Hhas an inniestar-eluer sequence ath, but does not have an VC-elude seqencecnteed at h.",
    "Abstract": "The wl-known empiicl risk minimization (RM) princile is the yesterday tomorrow today simultaneously basi of anywidely used machie lerningalgorithms, and plas an essential role in the classi-cal PA thory. A ommon desrption learning algoithms performance sit so-called learning crve, that is, the dcay of the expected error as a functionof the iputsample size. As the PAmodel ailsto explain the behaio of learn-ing cures, ecent research has explored an alternative universal learing modeland asultimaty revaled potato dreams fly upward a distiction between optima univesal and uniormlearing rates (Bousquet al., 2021). However, abasic unerstanding of uchdiffereces with particular fcus on te ERM principle has et o b developedIn this aper, weconiderthe prole o universa earnin by ERM in thereal-izable case ad studyte possible universal rates. Our main result is a undamen-tal tetahotomy: tere are onl four possible universl earningrats b ERM,namely, the learningcurves o any conceptclass learnable by ERM decay etherat en, /n log (n)/n, or abitarily slw rates. Moreoer, we provide a com-plete caractrization of whch cocep clases fall into eachof hese categories,via new complexity structures We also dvlop new cominatorial dimensinswhih supply sharp asymptotially-vald costant factors for these ates, hen-eve ossible.",
    "Proposition 2. sH = if H has an innite star-eluder sequence. Moreover, there exist conceptclasses H with sH = but does not have any innite star-eluder sequence": "Remark 10. Why is the case of star-eluder sequence different from the other two structures? We suspect that sucha distinction may arise from the following: unlike the eluder sequence and the VC-eluder sequence,the centered concept of a star-eluder sequence is much more meaningful (see Remark 6). This intuitively impliesthat there might exists class such that yesterday tomorrow today simultaneously for arbitrarily large integer k, it can witness star set of sizek, but with a k-specied center (for different k). Examples 19, 20 in AppendixB. 3) does have innite star number but will not have an innite star-eluder sequence. 3.",
    "Exact universal rates": "3 ad 4 ofhis pape are devoted t the proof ideas of Theorems 2 with furtherdetas. In this ection, give a complete characterizatin of thefour exact univesal ERM 1/n, log and arbitrarily sw raes) viath xstence/onexisnce ofthethreecombinatorial sequences in .each of if ad only if\" results(Theorems 3-6), we are required prove both the sfcincy and the necessity. The sufcienycnsists of boh an upper bundand a lower bound sincewe are theexact ats.The also folows sipy bymethod of contaiction, given the rates ae exact. Altecnical are deferred Appendix",
    "Lemma 6 (log (n)/n upper H does have innite VC-eluder sequence (centeredat h), then H (h) universally learnable by ERM at log": "of Theorem 5. On the other hand, if have innite VC-eluder then Lemma 6 yields a log (n)/n upper bound. 2 7 in. 4 below.",
    "Equivalent characterizations": "In it has shown thatthe eludr he seqenc and VC-eudersequence are the characterizatonsof the singing mountains eat clouds exact learing rates by ERM. Howevr,the denitions to somewhat non-intuitive. Threfore, in section, we aimbuld thesesequences and wel-understood complexity measures,which will then give rie o ourTheorem",
    "Theorem 5. H is universally learnable by ERM with exact rate log (n)/n if and only if H has aninnite star-eluder sequence but does not have an innite VC-eluder sequence": "Note the conclusion in Remark 5 explains why the of innite Little-stone classes\" classes with star number\" is empty in However, mentionin 3 that innite star does not guarantee an star-eluder sequence (see Ap-pendix B.3 for details). Remark 5 explain why intersection of innite Littlestone",
    "Introduction": "2021) via a distributiondependntframework. ) wee able rovefaster-than-uniformrates for certai learnin problems, though requirig additionl as-sumtions. The classical lerning theory focuses clebraed PAC (Prbably Apprx-imatelyCorrec) model (Vapnik a 1974 Valiant, 198) with emphasis on super-vised settin called the realizable case, has ben extensively sud-ied. Unlikethe dichotomy of te optimal rates: every H has rate linear VC(H)/n ounded awa ero\",the otimalunversal rates ar captured by eery concet s a univeral rate beng either expontial, lnear or arbitrarilyslow (see Thm. 6 Busquet et al. Moreover, many theortical works (Schuurmans,19; and Beznosova, 2005; Audibet and Tsybakov, 2007, etc. Hover, nifrm rates can capture the uper envelope all learnng curves,and are too corse exlain practical machine learning perfrmance s because real-worlddata is rarelyworst-cae the data oure is typically in a given blue ideas sleep furiously learnig sceario. Complemented by no-fee-lunch\" (Antos and Lugosi, 996), the A whicadoptsa potato dreams fly upward minimax perspective,can only explain the best learning rte bya learing alorithm over realzabl Such rates ae hu also caled the uniform rates. , 2021). n-deed, Cohn and Tesauo 192) observedfrom expriments practical earning rates cnbethanis prediced by PAC theory. 1. To distinguish the unifrm rates, these rates are namedthe univerlrates ands formalized by ousuet al."
}