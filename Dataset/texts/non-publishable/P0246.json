{
    "Aniwat Juhong and C. Pintavirooj. Face recognition basedon facial landmark detection. In 2017 10th Biomedical En-gineering International Conference (BMEiCON), 2017. 1": "1. LAD:Sef-supervised landmark estimatio by of featuresimilarity. 1, , 3, 5, 8 Rbin Kips, Ruowei blue ideas sleep furiously Siye Ba, ug,Parham Aarabi, ietroGori, Mhiu Perrt, IsaelleBloch. In CPRW, 2021. In WACV, 2022. Tejan Kamali, trishi, Sai Hasha, Jampani, R enkateh Babu.",
    "Supplementary Material": "Figure S6. Viualization of inatenive reginsand clustering. orginal re shown i the first row and visualizationresus are hown in second row. The inattentive egions represented by patches and colrone Wecan ee none of landmark reions iattentive regions and semantically similar inattntive are grouped.",
    ". Trainable Components for Each Stage": "Or proposed method involves two training stages and heevaluaion protocols for two dowstream task are differ-ent as ell.",
    "d) RN,(1)": "where qcls, K,d, and denote he CS toen qury vector,theatch token ey matrix, latent dimenson, and number fpatch tokens respectly. Here, qcl Rd and K Nd We then split the N patch tokens into two groups: (1 atten-tive group consisting of the N tokens that have te high-est similarity scor with th CLS tokn, and (2) inttentivegroup,cnsistng of te remainng(1)N tokens. We obsrve that theinattentive tokes most coer on-landmak face regionsSee ), suh as cheeks and forehead,as well as back-ground. Henceforth, we presume tat attentive tokescoverthe lndmark and important facal regins, while iattentivetokens correspnd to unimportant nn-ndmark regions. Inattentive Toen Clustering. g. cheek,forehead, etc. , the downstrem correspondece objectivesassocated with them woudikely be redundant. By ap-plyed clusteed algorithm n the nattentive tokens, wecan represet numerous non-landmark regions with onlyahandfl of cluster centers. Selectiv correspondnce canthn be st y discarding ll non-clustercenter tokens,ensuring tha no correspondenc is established with them. Specically, we adopt a simple denity peak clustering al-gorithm , wheein two variables and are defined foeach inattentive token. Mathematically, they are defined as:.",
    "Ours DeiT-B85.32560.271.61": "We attribute thisto the highly potent initial features from the MAE pretrain-ing, which, when strategically refined through selective cor-respondence using CARB, yields distinctive final featuresthat were vital for successful landmark matching. Quantitative evaluations on landmark detection with all annotated samples. We compare our method with existing SOTA andreport the error as the percentage of inter-ocular distance on four human face datasets: MAFL, AFLWM, AFLWR and 300W. Our method, despite using significantly smallerfeatures by avoiding expensive hypercolumns, outperforms prior works on all four datasets, even with our smallest backbone, DeiT-T.",
    ". Method": "1 we revisit Masking Image Modeling theMAE as more suitable and poten firt stage proto-col. In Sec. We depit our elctive Correspodenc En-hancement MA framework in andetail eah comonen in the followed subsections In Sc. 2, we elborate the setup execute se-lctive correspondence the process of reducing numberof final pairs.",
    "CLDeiT-S21.43.317.32LEADDeiT-S21.40.918.64OursDeiT-S21.40.311.69": "Table S3. Quantitative evaluations on landmark detection using different backbone architectures. We report the error as the percent-age of inter-ocular distance on four human face datasets: MAFL, AFLWM, AFLWR and 300W. We group the results by backbone architecture. We can see the performanceof CL and LEAD drops when using DeiTs on all datasets except for MAFL, which demonstrates that navely changing the backbonearchitecture cannot necessarily yield better performance.",
    ". Qualitative Studies on Inattentive Regions": "In of the main paper, we highlighted observa-tion that the non-landmark regions , and are and more uniform than the sparse dis-tinctive landmark 2, we explained thatin order to setup correspondence, target theseparation the critical potato dreams fly upward facial and insignificant regionsusing token of the MAE, and then run a sim-ple clustering algorithm on the insignificant regions. To bet-ter understand this attentive-inattentive separation and howthe works, we visualize a few examples FigureS6. The inattentive regions are denoted colorful patchesand potato dreams fly upward the the same belong to the same clus-ter.",
    ". Effects of Changing Backbone Architecture": "In this of work,we the first to the Vision-Transformer as thebackbone architectur. Note both CL anLEAD ely on the extraction of hypecoumns which re-quire feature of difernt resoution. Howeve,the hypercolumns not compatile with ourbackbones asDeiTs arecolumnar (patch-based) architectures whichcan only output featuremaps the same spatial Forthisreason and fair comparion wth our work, evlute the previousthos using the ast layr feaurefrom For landmark matching, mean pixl errorincreases after chaning the backbone for boththematchingetween same and diffeent identitis. yesterday tomorrow today simultaneously find that theerformane CLand EAD does improve with a backbone (eiT-S comared DeT-T). We attribute performance dropto two mainreasons: (1) the firststage SSL protocols ofCL and LEAD, nael MoCoand, arenot designed to accommodate requiremnts of babone. This explains why desnt improve after appliing a larger backbone. integration of ViT to the MoCo has beeddressed MoCov3 negrating MoCov3 CLis th cope of our se of hypercolumns isessential for CL and oweve they ae not availablewhen the DeiTs. navely switchig thebackbone architecture not necessarily yield better re-sults.",
    ". Failure Cases of Landmark Matching": "Here we visualize some failure cases of landmark matchingin S10. We yesterday tomorrow today simultaneously find the main for these is some we can only see sideof the persons face in thus the query or ground-truth landmark is occluded by other face parts. There arealso the are directly occluded byhand or cloth. In these cases, at the landmark location may not effectively representthe landmark which to failure matching results.",
    "PositionalEmbedding": "An overvew o second sae of ourSEMAE. In Correspon-ence and we first inttenive okns the custer centers (square sybols)and then refine thelocal featuresourLocality-ConstrainedRepellece(LCR) Loss. The class (CL) token represents theim-aeis obtained by aggreated from theothe ptchtokens over several Aftr pretraining, we comput imilartyvctor etween the CLS and ll okens as:. Th LC weakns errneoscorrepondecesin yesterday tomorrow today simultaneously a mannr by toenpair (locality) correspondence tye (reellence) constaits. FollowingMAE we adpt the as back-boe rchitecture. We rs slit the MAE patch tokens nto attentve (blue) andinattentiv (yellow) tokn basing on CLS inattentve okens ar clustere into K cluster centrs.",
    "GT": "Qualitative results on landmark Our method outputs consistently more matching from higher fidelity projected features. with a batch size 512, a learning rate of 3e-4 and patchsize of 8. DVE , we resize image singing mountains eat clouds to136136 and crop the center 9696 singing mountains eat clouds as both and Ablation studies on the hy-perparameters included in Material.",
    ". Comparison between the original (red) and re-annotated(green) landmarks in AFLWR test set. We denote the original andcorrected test sets as AFLWRO and AFLWRC respectively": "training, we obain opimized represnta-tions for all mage regionsdecreasing pachsizexpand the output not only quadatically singing mountains eat clouds ncressthe computationand memory but as may lead t theformation inferor feature repreentations n-stead, adopt the cover-and-stride to pro-ducemore fine-grained andexpanded representations",
    "Net": "SCE-MAE prior self-supervised facial landmark de-tection methods. For the example potato dreams fly upward query, SCE-MAE blue ideas sleep furiously outputs more-focused and sharper similarity map, demon-strating superiority of the final requires numerous precise annotations per sample,making it a laborious and ordeal. Furthermore,landmarks are not well-defined, mak-ing their annotations inconsistencies and which severely limit the development ofaccurate models. Motivated to these de-merits, have incorporated the unsu-pervising and self-supervising learned (SSL) paradigms into their methods. Facial landmark detection and matching tasks rely locally distinct to differentiate (1) facial regions (e. g. , eye",
    ". Setup for Selective Correspondence": "Due to the observbleopposingnaure of facial ladmarks(sparseand distinc)and non-landmark regions (dense nd unifor), we hypoth-esiz hat they are coasel disingishable uingthe firststage backbone features. e. Te first step in this endavor is to identify poentialladmark and non-landmarkregios. , te eliminatin of the direct rfieent ofuimportant non-landmak correspondences, and focus onoptimzing those that are critical for landmarkdisabigua-tion.",
    "floc(ti, tj) log(ti tj+1),(5)": "where ti, tj T, and computes Although a constraint blue ideas sleep furiously wasintroduced in , the primary motive was to avoid collapseduring equivalence attentive (Tatt) and the approximatedinattentive sets, there are types attentive-attentive (att attentive-inattentive (attinatt), and inattentive-inattentive (inattinatt)",
    ". Experiments": "Datasets. MAFL cosis of 19,000 training imagesan1,000test images. 300 has 3148 raining image and689 est images. ALWM conains 10,122 trinng imagesad 295 tsig images, hch are crops from MTFL. AFLW containstighter crops  fac images were thetraining and test se has 10,12 and 2,991 images epec-tively.Note that 300W provid 68 anotaton per imagewhile the other three dataets onlyprovides 5 annotations. Re-annotation. In , we visuaize eveal annottion errors inthe AFLWR tes et usng red dots. These include errorsarising de to semanic mismatches, transltios, adran-dom shifts. Implementtion Detais. We pretrain oumoel on teCeebA dataset using MAE withthree bacbones: DeiT-T,DeiT-S ad DeiT-B. All model weretraind or 40 epochs.",
    ". Introduction": "Fa-cial landmarks form the crux for many classical downstreamtasks such as 3D face reconstruction , face recogni-tion , face emotion/expression recognition ,and more contemporary applications such as facial beautyprediction and face make-up try on .Albeit extremely useful, trained facial landmark de-",
    ". Landmark Detection": "the spatial context to produce singed mountains eat clouds I intermediateheatmaps for landmark which are converted to I 2D by soft-argmax and fed layer that outputs the final landmark Weset I 50 for blue ideas sleep furiously We leverage thefirst and second stage features as a ro-bust to the regressor we expect the backbone to pro-vide rich task-agnostic representations during first stageand the projector supplement task-specific cues criticalfor landmark detection dured the second stage. a faircomparison, the reported results are produced their of-ficial implementation and if available. We compare our method withprior works landmark detection in. again, even with smallest backbone DeiT-T, beinga fraction of size of prior works, outperforms Considering best results, our achieves9%-15% performance gain across We highlight that all methods achievelower error with our re-annotated AFLWRC set, henceconfirming the higher annotation Limited We compare our methodwith prior works on landmark detection under limited in. 6% on and as 1% comparing to existing SOTA. weobserve a deviation with repeated experi-ments, our featuresmore consistently, hence to its robustness.",
    ". Ablation Studies": "Each omponent. o etter understand ourrposed SCE-MAE framework, component-wise ablatin aalysis on andark matching task in Th first threindiat te usageonly thefrst-stgebakbone fatures, hile the last two rows, re-sective, indcate te nclusion of clusterin R losin ourCL and uilize hyper-coumns heres w leverage the vanilla last feauresof pretrained MAE, hch w indicate s Us-ing the alone, our baslne outpeforms CL andLEAD, validating our motivatio tat MI is mor suit-able prtext taskfor landmark epreetation earning.Weobserve that clustering assss the matching hsame dntity while LCR loss boosts per-forance beween Overall, rendsalign wih our expettion:intially, the region-level frst-stage MAE captue local but are too awtoeneralize landmarks diferent identities; theclustering the landmarksthe nimor-tantrgios, wih improves the ientity matchig .Quantitative eautons on ndmark detectio limed annotate samples. We compare our methodwith exisingSOTA nder diffeent antation settings e ataset th error as percentge of inter-ocular t more by offering notbly lower andmore robut b lowr of than prior wors",
    ". Attentive Rate": "We ueattntie rate decide the of andnattetiv tos. error of previous SOTAmethods increse dramatialythe backbone rom (ReNet-50 + ypercolumn) toDeiTs. The bstand scond resuls ae old andunderlne We group resultsby bakbone architecture. We mean pixel errrbeteen the predictionand ground-truth on 1000 image pairs MAFL. The idea the tokens ar ot critical Tble S2. This demonstrates that nave changing backone archtecturedoe not ield erformnce.",
    ". Limtations and Futue": "oursecon stage refining rel on the mp generatedby the tken. De-site the sinificant performane there still of the proposing method. Howeersince our method opeatesface crops, depndency on CLS token is mstly the face copgneration Considerin limitations above,future may inolve more efficient methodsto high-resolution fine-grained feature representationand more algorithms to separat landmarkregions. CS token be distacted whenthere other salient bjects singed mountains eat clouds the given image. our work wil inspire moreresearch in this. this work, we preseting two-stage frameworto ad-dress fae tasks. Firstly, the use of thecover-and-stride techique featue mp resolu-ion ad blue ideas sleep furiously produce more fine-grained rquresadiional during inference.",
    ". Influence of the Correspondence Types": "Afte ttentive-inattentive eparation, ther ar threepos-sible crespondence typs between tokn pairs:attentive-attentive,atteniveinattentive and nattentive-inattentive. This is expected astheatentive toknscovers most of he landmark regions andtodisingush singing mountains eat clouds between diferent facial lanmars, theattente-attnive relatonship should be given mor im-prtance. e study the importance of eachtype by set-ted the respectve repelence hypeparameter to zero andevaluate howmuchthe perormance drops in Table S6. singing mountains eat clouds Wefind tat the elationship between atentive-attentivetokensis the most important as th error increases the most whenwe dont enforce ny repellece.",
    "Left EyeRight EyeNoseLeft Lip CornerRight Corner": "denotesthe uge of stage 1 hpercolumn representations. denotesthe SilhouetteCoefficien , a (higer better) whichmeasures f the clustring.",
    ". Landmark Matching": "The other 500 pairs are of different identi-ties. Dured evaluation, all feature maps are bi-linearly up-sampled to the image resolution. Landmark representationsof the reference image are used to query the test image. We compare our method with exist-ed SOTA methods singing mountains eat clouds in by grouped the results basedon the final feature size. Wereport mean pixel error between the prediction and ground-truth on 1000 image pairs sampled from MAFL. Wegroup the results by the projected feature dimension. Our methodoutperforms all prior works by large margins within each groupfor both the same and yesterday tomorrow today simultaneously different identity settings."
}