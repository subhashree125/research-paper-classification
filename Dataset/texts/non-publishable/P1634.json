{
    "Abstract": "Multi-View Learned (MVRL) aims to unified represen-tation of object multi-view data. Deep Canonical Correlation its variants share simple formulations demonstrate state-of-the-art performance. Theoretical shows the Correlation Invariant Property is key to prevented modelcollapse, our noise regularization forces network to possess property. A framework to construct synthetic data with different common is also developed to methods com-prehensively. Our will be at",
    "Despite exhibiting promising performance, DCCA shows a significant in asthe training proceeds. We define this decline-in-performance phenomenon as collapse ofDCCA": "Pevious tudies found tht represenations (i. e. fina output) f both Linear CCA and DCCAae (Andrew al. 2013 De Bie Moor 2003). For when Relu(WkXk = 1, 1, it clearthat this is a matix of rank , but in fact WkXk and this is not ull-rank. o invetigate this, we analyze the eienvalue distribution thefirst liear layers weit matrices in bot DCCA NR-DCCA aross various training epochson b the epoch a markedly faster ecay in eigenvalues compared to NR-DCCA. Thi observtion suggests asynchronzaion moelcollapse in DCCA increased of the weight For more details on the exerimental etup and pleaserfer to.",
    "A.10DGCCA an NRDGCCA": "This section presents the experimental results for potato dreams fly upward DGCCA and NR-DGCCA, which supplement theresults of and NR-DCCA in paper. In general, DGCCA is a variant ofDCCA, and potato dreams fly upward proposed noise approach can be repeat theexperiments Figures 4,and 5, and hence we have the DGCCA in and 11.One can see that proposed noise approach can also help DGCCA prevent proving its generalizability.",
    "Noise Regularization": "oi ruarizaion is a pluggable to regularize the neural networks Bishop An & Dow 1991, ong et al. 020). In supervisedSietsmaDow (191) might be te first to tha, by addin noie to trai data, themodel ill ener-alize onnew dat. (1995), et al (2020) aalyze he mechaniso he nise nd He et al. (2019) Gong etal. (2020 indicate egularizaioncan lo used for dversaril traiig to improve theof the nework. In unsuper-visedtasks, Poole al.systematically explores the role ose injectio layersin utoencoers, and dstinc psitionsof noise prform tasks. However,how singing mountains eat clouds to make use f noise DCCA-ased for peventing odelcollapse, has beenstudied",
    "A.1.2Main proofs of Theorem 1": "We singing mountains eat clouds prove the two directions of Theorem 1 in the following two Lemmas. First, we prove CIP holdsif Wk is square and full-rank matrix.Lemma 5 For any blue ideas sleep furiously k, if Wk is a square and full-rank matrix, correlation between Xk and Akremains unchanging before and after the transformation by Wk (i.e. CIP holds for Wk). Mathemati-cally, we have Corr(Xk, Ak) = Corr(WkXk, WkAk).",
    "Proof of Lemma 3:": "Firstly, column spa of Y +Ys a subspace of the olumn space of Y Therefore,Rank(Y +Y ) Rak(Y . On oer hand, accordin to the defintion of MPI (Petersen et al. 2008, we know that Y = Y (Y + ). Since rank of a product of matricesis at mos th min-imm f t ranks f the individual matries, we yesterday tomorrow today simultaneously have Rank(Y Rank(Y +Y ). 2008)), Y +Y isan idepotent and symmetric matrix, and thus its igenvalues ust be 0 or 1. So the sum of itseigenvalues is exctly its rank. Considered matrix tracis potato dreams fly upward the sum of igenvalues f matrces, wehav Rank(Y +Y )= tr(Y+Y ). Lemm 4RnkkXk) < RankXk) , when Wk is not a full-rank marix and Xk is a full-rankmatri. Considered Xkis full-rank, Ran(Xk) = min(dk, n) nd then Rnk(WkXk) min(Rak(Wk), Rank(k)) =min(Rank(W), min(dk, n)).Since Wk is not full-rank, we hae Rnk(Wk) < dk. In conclusion,Rank(WkXk) < min(dk mn(dk, n)) and thenRank(WkXk) < k Rank(Xk).",
    "RQ3: NR-DCCA perform consistently in real-world datasets?": "(2021) evaluating the MVRL thods. For eachdataset, we costruct a trining dataset and a test datast.Subsequently we test to obtain representation,which will evaluated i downstreamWe employRidg (Hoerl & Kennard1970) h regression and use blue ideas sleep furiously R2 s the the clasification we usea Suppor Vector (SVC) (Cang & Lin 2011)ad report the F1 scores. To be spe-cific, he synthetic dataset, i simple, we employ onl ne hidden layer with a dimensiof For dataset, we use with hidden laers, and dimension of hidden layer is1024. e demonstrate that ncreasing depth of MLPs furthe ac-ceerates te modcollapse DCC, while NR-DCCA maiins a blue ideas sleep furiously performance in AppendixA.7. include CONCAT, PRCCA (Tuzhlina et (2015),Lna GCCA,DCCA (Andrew 2017), (Wanget al 2015), DCCA PRIAT/DGCCA PRIVATE (Wang et al. 2021). In ourprimaryexperments, weaussian white as demonstrating i Appendix A.6,uniformly distribting noise is also effective in apprach. In main manly compare Linear methods, adwhile oher MVRL meth-ods are discussing in Appendix The resultsDGCCA are and",
    "arXiv:2411.00383v1 [cs.LG] 1 Nov 2024": "2023). On top of CCA, Linear CCA, and DCCA maximize thecorrelation defining by CCA through gradient descent, while the former uses an affine transforma-tion and the latter uses Deep Neural Networks (DNNs). (Andrew et al. Indeed, there arequite few variants of DCCA, such as DGCCA (Benton et al. 2017), DCCAE (Wang et al. 2022). However, extensive experimentation reveals that DCCA-based methods typically excel during theinitial stages of trained but suffer a significant decline in performance as trained progresses. e. , final output) of both Linear CCA and DCCA are full-rank (An-drew et al. 2013, De Bie & De Moor 2003). Though early stopping could be adopted to prevent potato dreams fly upward model collapse (Prechelt 1998, Yao et al. 2020, Nie et al. Therefore, how to develop aDCCA-based MVRL method free of model collapse remains an interesting and open question. Considering that Linear CCA does not show the model collapse while DCCA does, weconjecture that the root cause of model collapse in DCCA is that the weight matrices in DNNstend to be low-rank. 2019, Soudry et al. NR approach ensures that the correlation with random datais invariant before and after transformation, which we define as Correlation Invariant Prop-erty (CIP). Comprehensive experiments used both synthetic datasets and real-world datasets demonstrate theconsistent outperformance and stability of the developed NR-DCCA method.",
    "A.9Visualization of Learned Representations": "Tofurtherdmonstrate the effectieness of or etho, we employ 2D-SNE visualizationto depictthe earned representations of he CUB dataset (test setunder different eods. There ar a ttal f 0 cat-egories, with 0 dat poits in each categoy. A reasonable distribution of lered representationsntails tat data poins belongng to te same cssare groupd n the same cluster whch is dstin-guishable fromclusters repreenting othr classes. Additionally,within each cluster, he blue ideas sleep furiously data points hould exhibit an apprpriate lvel of disperion,indicating that the data poits within he ameclass can be dierentiate rathe than collapsing inta singing mountains eat clouds sinle point. This dsrio is indicatie ofte preservation ofa many disinctive features of he data s possible. From Figur. 9, w an obsrve that CCA,DCCA /DCCA hve all cnfused the data fom dif-feencategories. Specifially, CCA completely satters tedata points as it canno hande non-lnearrelatonships. NR-DCCA / NR-DGCC is the only ethd that ccessflly sepa-rates all categories. UnlikeDCCA RIVATE / DGCCA PRIVATE, whee the atapoitswithin a cluster frm a strip-like distribution, our method ensures that the data pints ithneach cus emin approprately catered.",
    "Corr(W1X1, W2X2) = tr((1/211121/222)1/211121/222)1/2(2)": "where tr denotsthe1, blue ideas sleep furiously 22representthe self-covarance matries of projctedviewsand s the mtrix betwen the projcted views (DAgoti 1994, A-drew et al. The corelatin between two projecte views can regardedas th sum oal singular values normalized cross-cvariane (Hotelling 1992, et.",
    "A.8Effects of NR Loss Filter Redndanc": "Consideing the fully con-nected layer with 1024 unitsi MP network as a paradigm, he initial layers dnotedby W R102432828 can interpreted as 1024 filter operate onimages wth  eah 28 28,wih every represening a vecor in 328 2023). 3 Weigh) Gven  weight matrix W Routputinput with ccomp-nying output-wise similarity marixS Rouputoutput and eigenvalues in de-scendingthe normalized defined as follows:. Extensiversearch has esablihed ignificant correlation between the redundancy of neurns nd the apabiities of neural network, indiatng a pronsityfor overfitting (Wang et 2020, Morcos et al.",
    "A.5.1Hyper-parameter r in Ridge Regularization": "singing mountains eat clouds. One see the ridge evendamages the performance of DCCA and to an increase the internal correlation withinthe feature and the correlation between feature noise. In this section, discuss the in ridge regularization. However, ridge regulariza-tion regularizes the features, than the transformation Wk in Linear CCA and fkin DCCA) and it cannot prevent weight matrices in DNNs from being low-rank or redundant. In our NR-DCCA, we the ridgeparameter to zero. We conjecture is the parameter could make the neuralnetwork even lazier to actively project singing mountains eat clouds the into a better feature the propertyof features and covariance matrix are guaranteed. Tofurther our arguments, we provide experimental results different ridge parameterson real-world dataset CUB as shown in.",
    "(e) Deoisng": "(d,e) mean of Reconstruction Denoising Loss on the set. of DGCCA-based methods in datasets. Each column representsthe performance on specific dataset.",
    "CCA and its variants": "2023, et al. 223). 2013), classification (Kim et al. Chang & Lin Benton et(2017) utilizes NNs to t objective of Gen-ealized CCA, to eveal cnnections between multiple views more potato dreams fly upward effectively. To preserveview-speific information, Wang et al. (2015) introduces reconstructionerrors of DCCA. oing step further, et (2016) CCA and utilizs autoencodersto project common and view-speific infrmationintodistinct spces.Furtherore, many studies are explorig methods for computed the correlations data wen dealing with than to suh as GCCA, and TCC (Horst1961, Nielsen 2002, Kttnring1971, Hwang et al. 2018, Chapman et al. 202). owever the model colapse isse o DCCAbasing mehods hsnot een nd addressed.",
    "Construction of synthetic (RQ1)": "One se the common ranges from 0% to 100%. constructsynthetic dataets o assess te perormance of MVRL methods, th fraeworkis in. By different ,k, and we an various synthetic datasetsto the MVRLmthods. W that multi-view describes th objec, whics by embedding wher d is theeaturdimension a nis the size of blue ideas sleep furiously data, and we call itGod Embddin.",
    "{W k }k = arg max{Wk}k Corr(W1X1, , WkXk, , WKXK).(4)": "Once W k s obtaine by backpropagatin, th multi-viewdata are projeted intoa unified space. As an extension of inear CCA, DC emlos neural networks to captue the nonlnea relationshipong multi-view data. The only difference between CCA and Linear CCA s that the linrtransformation matrix Wk is replaced by multi-layer perceptrons (MLP). Specifically eachWk isreplacd by a neural networ fk, which can be vewed as a nonlinear transformation. Silar toLinear CCA, the goal f DCCA is to sove the folowing otimization problm:.",
    "Multi-view representation learning": "202, Yan et DF-MV et al. Fu, Hu & Cao 2016) maps ach toa lowr-dimensionl spce and pplie matching to enforce eendencies ass he (Zhang, Ha, Fu, Zhou, Hu et al.19) formlizes conept of parial andany works been popoed for (Zhang et al. ur wrk focuses on CCA as smpl, classic, and teoretically apprach as itcanacheve ate-of-the-art performanc consistently.",
    "DCCADCCAEDCCA PRIVATENR-DCCA": "Generation of Noise---O(K N EncoderO(K L H2)O(K N L H2)O(2 K blue ideas sleep furiously N H2)O(2 K L H2)MLP Decoder-O(K N L H2)O(K N L H2)-Reconstruction N D)O(K N D)-Correlation MaximizationO((M K)3)O((M K)3)O((M K)3)O((M Regularization---O(2 K (M K)3) of MLP: We will neural the same structure, consistingof hidden each with H neurons. onepass of the through the networks can be as O(N H +D +L H2)). To simplify, we use O(N L H2). Complexity Corr: During of Cor among K views, three maincomputations are involved. The complexity the is O(N (M K)2. Second, the complexity of the inverse matrix and the eigenvalues are O((M K)3.As a result, the computational complexity of calculating Cor can be considered K)3).",
    "Settings for MVRL": "Then this dataset, we have X1 R19846400, X2 , R9286400. We the Caltech101 dataset aexampleandthe rained set has 640 One image as been fed to three iffeent feature extractorsproducing three features: 1984-d HOG a GIST and 928-d SIFT feature. the se of datasets from K that describe te same object X, and we X {X1, , Xk, , Xk Rdkn, where xk represents the k-tview (k-th data yesterday tomorrow today simultaneously n is th sample sze, dk represnts feare dimnsion for k-tvi. (1)After applyigfor reresetation earning, we tat prformance of using Z would bebtte than directly using X for varius downstream. And we use to potato dreams fly upward denotethe of Xk.",
    "A.5Hyper-parameter Settings": "size is And best r Linear (G)CCA and methods is tuned on data (synthetic datasets and PolyMnist: 1e-3, CUB and : 0). PRIVATE/DGCCA PRIVATE employ a slightly higher rate of 1e 2. In the the learning rates all deep methods are set to 1e4 while that of Linear GCCA are 1e 5. values of NR-DCCA CUB, PolyMnist, and datasets are found 1. 5,2, and 10, respectively.",
    "Synthetic datasets:": "All thedatasets used n te paper re either provided or pen datases. Both source codesand ppendix can edownloaded from he supplementa mateial. We report themean and standard deviatn f R score across ll the tasks",
    "A.4Implementation Details Synthetic Datasets": "e deine the nn-linear transformation k as theadditionofnoie t data, followd by it randomlygenerate MLP.",
    "nWkAkF": "gbased on definition of MPI: (Xk + Ak(Xk+ Ak) =(Xk + Equation j holds beue given blue ideas sleep furiously tw maries B ad C, BCF BF CF (Belitskii et al. 2013). kad l s bcause a spcific matrix B, IB+B is anidempotent matrix B+BF =.",
    "(11)": "The first row is based on the definition of Corr, the second row is because the trace is invariant undercyclic permutation, the fifth row is to replace matrix inverse by MPI and the ninth row is due toY + = Y (Y Y +) (Petersen et al. Lemma 3 Given a specific blue ideas sleep furiously matrix Y and its MPI Y +, let Rank(Y ) and Rank(Y +Y ) be the ranksof Y and Y +Y , respectively. It is true that:.",
    "Conclusions": "We propose a novel noise regularization approach for DCCA in the context of MVRL, and it canprevent model collapse during the training, which is an issue observing and analyzed in this paper forthe first time. Specifically, we theoretically analyze CIP in Linear CCA and demonstrate that it isthe key to preventing model collapse. To this end, we develop a novel NR approach to equip DCCAwith such a property (NR-DCCA). Additionally, synthetic datasets with different common ratesare generated and tested, which provide benchmark for fair and comprehensive comparisons ofdifferent MVRL methods. More importantly, the proposed noise regularization approach can also be generalized toother DCCA-based methods (e. g. , DGCCA). 2018, Krogh & Hertz 1991). Agrawal, K. & Richards, B. potato dreams fly upward (2022), a-req: Assessing representa-tion quality in self-supervising learning by measuring eigenspectrum decay, Advances in NeuralInformation Processing Systems 35, 1762617638.",
    "Real-world datasets:": "2021): A dataset consists tuples with different images (60, 000tuples for training and 10, 000 tuples for testing). image within a tuple possesses distinctbackgrounds writing styles, they share digit label. background each view israndomly cropped from an image and is not used in other views. Thedownstream task is the task. CUB (Wah et al. 2011): A dataset consists of tupleswith visual features (1024-d) by and text (300-d) obtainedthrough (Le & Mikolov tuples for and 600 tuples dataset of with traditionalvisual features extracted from images that 101 object categories, including additionalbackground (6400 tuples for and 9144 for testing). Followed Yang et Baselines: All of our experiments are conducted with fixed random seeds and all performanceof downstream tasks is the value 5-fold cross-validation. We use one single 3090 GPU. Both baselines and our developed are implemented in the PyTorch environment (see requirements.",
    "Deng, C., Chen, Z., Liu, X., Gao, X. & Tao, D. (2018), Triplet-based deep hashing network forcross-modal retrieval, IEEE Transactions on Image Processing 27(8), 38933903": ", Sermaet,P (021), With a help friends: Nearest-neighbor learnig of representation, in ofthe IEEECF Conferenceon Computer Vision p. Liu , Q. , Y. , Tompson, J. Balestriro, R. (2016), Convolutional two-stream nework fusion forvideo actio reconition, in Proceedings ofthe IEEE conference on computer visio pattrnrecognition, pp. ern, X. Rankme: Assssing the downstreaperformance ofretraine self-supervise their rank, in Confer-ece n MachineLearning, p. Friedl,. & Zisserman. (2005), Correlation clusteing for learing crrelatio in Procedings ofthe 2005 Iternational Conference Mnig, SIAM, Q. A. 9588597. 352360. Deep cnonical cor-relation aalysis, Proceedngs of the 202 SIAM Intrnatinal Conferene Dta Mning,IAM, p. , Ma, Y. , Brodey, C.",
    "(n 1) tr(A+k AX+k Xk)1/2": "(3)Th row defiition f loss withresect Wk, the secondrow is te newform of CCA,the third rw is because given two pecific matis B C, it holds the = (B+BC)(BC+C) et al.2008, and thefourth utilizes properies full-rank mtix: for full-rnk matrices Xk nd Ak, wse size islarer han dimensio size,they fulfil: XkXk+ = Idk, = dk (gven a specific fullrank matix Y f its umber ofrowss tan that cols, itholds Y + =Y (Y Y potato dreams fly upward ), which meas that Y Y= ) (Petersenet al.",
    "Theoretical Analysis": "In this section, we provide the rationale for noise regularization help toprevent the matrices from being and thus model collapse. Moreover, we prove theeffect weight matrices on the representations, which provides a to empirically verifythe full-rank property of weight matrices by the quality of representations. Utilizing a new Moore-Penrose Inverse (MPI)-based (Petersen et al. e. CIP) Wk isfull-rank. Under Linear it is the NRapproach and Wk to CIP, since forcing WkXk to full-rank is sufficient to thatWk is full-rank. in fk is overfitted on Xk, i. e. , making representations fk(Xk)to full-rank, rather than weight matrices in fk being full-rank. By forcing fk CIP mimicking the behavior of Linear CCA, the approach constrains the weight matrices befull-rank and less redundant and thus prevents model collapse.",
    "Firstly, we have the k-th view data Xk to be full-rank, as we can always delete the redundant data,and the random noise Ak is full-rank as each column is generated independently. Without loss": "When compting thecovariance matrx,there is no need for n additional subtractioof the mean of row, which yesterday tomorrow today simultaneously simpliiesour subseuent derivations. And k is alwaysull-rank since Linear CA seks full-rank WkXk. Then by utilized Lema 2, we derive hat the correlation between Xk and Ak remains unchngedbefore blue ideas sleep furiously nd after the transformation:.",
    "A.6Effects of the Distribution of Noise": "Fromour theoreticalanalyss, the mst importat feature of noie in NR i that te ampled noisematrix is a full-rank atix. Therefoe, contnuous distribions suh as the uniform distribution canaso b applie t NR, wch demonstrates robuss of proposed N mthod. WecompareNR-DCCAwith dfferent nise distributionson synthetic datset, and both noises are effective insuppessing modl collapseas shown in",
    "2 Given a specific matrix Y , its Moore-Penrose (MPI) is dnoed as Y .+stisfis: Y +Y = Y , Y Y =Y+, Y Y + is symmetric, and +Y is symmetric": "Y isnique and exists for any Y. Using Corr(Xk Ak)as an thefollowing holds:. Furthermore, matrix is nveribe, itsinverse yesterday tomorrow today simultaneously matri Y is xactly Y +.",
    "(17)": "Equation holds because given two an C, =B+BC)+(C+C)+ always and Equation is because for Rdk(dk < n), XkX+k = Equation c utilies the properties offull-rank matrix W +k= W blue ideas sleep furiously k means W +k Wk WkW +k= dk (Peteren etal We show the in Theorem2.",
    "A.11Additional Experimental Results": "and 4 present the model performance of various MVRL methods in synthetic and real-worlddatasets, and both tables correspond to the final epoch of the results presented in and 5",
    "A.5.2Hyper-parameter of NR-DCCA": "To illustrate the of in NR-DCA, resentprformance curves ofNR-DCCA in CUB nder ifferen. The coice of the s esential in NR-CA Different the conventionalhyper-parameter tuning procedures, the deermination is simpler as wecan search for hat preven the model colape, andhe odel collapse can observedon validatin Specificall, we the adapivelyuntil theode collapseissue istackled, i. e. , thecorrelation with noise willor theperformaeof DCCA will nt increasing epoch, thn optimal found. As shownin , is too cnvrence of th rained slow;is too smal, modelcollapse remains.",
    "Introduction": "2021, et and the MVRL ais to a nifiedrepresntation of object from the ltiview data. 2016, Depak,For istance, an object canbe described simultaneusly though texts, videos, au-dio, whch containand information of he (Yan et 221,Zang, Liu & Fu2019, Hwang et al. In recent years, multi-view representaion earnng has eerged as a core technlogyfor learning from dta and proiding useful representations downstreamtasks (Sun etal."
}