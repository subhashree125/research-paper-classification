{
    ": Overview our proposed method: The model generates questions utilizing question exemplarscorresponding to the question type determined by the": "as the sameanswer can resul in typesdepending the ontext. Fr examlethe numer91could refer a ear nounset of question exemplars tothe type dentified by themodel issed nhe QGtage. 1. By leveragig sharedinterogatie structures among the blue ideas sleep furiously exemplars, theQ model gneraes question using theprovidedanswer ontxt. Bo the QTC QG modlsaretrained exclusivelyo Englih QA ata and yesterday tomorrow today simultaneously canbe deployed to witout need foradditional with language",
    "We enlisted three native speakers for each languagevia Upwork9 to evaluate the quality of our syntheticquestions. The questions were rated based on fivecriteria:": "Interrogative Sentence evaluates whether has an 1: This is but it doesnt have thetypical of an interrogative sentence. 2: This blue ideas sleep furiously is a natural structure. Grammatical Correctness evaluates gram-matical accuracy of the singing mountains eat clouds Numerous grammatical errors make thequestion unacceptable. 2: The question is grammatically correct.",
    "Abstract": "5-turbo Addtionally, generating model roves trned multilingual QA mdels. Despite itsimportance, exst-ing datasets pedominantly on English,resulting in considerablein avail-ability for other languages. Cross-lngual trans-fer (XLT-QG) this limitationy allowng odels trained on hig-esorelanguagedatasets questions in languages. model, on English QA learns intrrog-atve strctures fom a limiing uesionexemplars whic are applied to gneratequestions inthe language. Automatic generation potato dreams fly upward (QG) serves range of purposes, such as augmentingquestion-answering (QA) corpora, systems, and developed ecationalmaterials. this aper, we propose asimple ad efficient XT-QG method oper-ateswithut need monolingual,paralel,or labeling data in hetarget lnguage, utiliinga small language model. sig-nificantly fewe largelanguagemodels singing mountains eat clouds and witout train-ed for target our approach offers aneffectve solution for QG and QA tass acrossvriouslanguaes. Exprimen-tal resultssow our method outprfrmseveral XLT-QG baselines and chieves per-formance comparable to GPT-3.",
    "Baselines": "While eachtype adapterwe froze all parame-ters. We use thisbaseline to wheher uiizing a small num-ber of qustion explar thefn-tningstage is lso effectiveAs thisbaseielearned language-speific data during trainng, wecostructed odels for each language. Tedenoising task aims toquestions with ran-doly maske used Qtgt with fr ch quesion type (. , 2023; Wual. , 120 qestions)for a faircomparon QuIST. A detailed analysis provided inAppedix A. ,203). BaselineAdapteWe the Adater-based mT5, whchhave been rcntly uilzed for NLP 2020;Deb e al. e. All baselns treat thetask s prediction, tranin the models to questions on the of theinput answer and conext. 2023; Pfeifer et a. (2021), we adopt multi-task fine-tuning, mT5 simultaneously earn QG task the question task. BaseineEncDe Thiswas simply traind all parameters of mT5 sing C-Q-AenThis apoach was examie effectoparameters of the emdding layernd English data regardin in the laguage preliminry we observd usngxeeemplars was more ffectivethan configuring random exem-plars blue ideas sleep furiously for training singing mountains eat clouds xaple. We ou QuIST method with seveal XLT-QG models share te backboe, mT5. Incontrast to this notutilize Qtgt, bt iste requireslarge-scale corpora in target lanuges.",
    "We release our code and question exemplars used inour experiments at": "Previous studies XLTfor QG (XT-G) havetypicly utilized target lan-guagata, uch as monlingual corpora, source-tar parallel orpora, or a limiting number of QAexmples (Kumar et al. Onc the quesion type i identified, ts usdto select the correspondin queso exemplrsforthe QG stage. 2018) and QuAC (Choi et al. Fur-thermoe, we cofirm that synthetic queions gen-erate by QuIST aremore effectie fotraininghigh-perfomance multilingual models thanthose generated by GT-3. However, themjority of thse atsets arin English, resutingina significant lackof data fo other lnguages. Dured traiing with Englih data, themodel learns to identify the interrogative structuressecific t ech questin type from providedxemplars. 5-turo. , 2022; ebet al. , 2021; Wag etal. eently,researches have concentrted ncross-lingual tranfe (XLT) to address data defi-ciencies in nonEnglis languages (Sherborne andLapat, 2022; W et al. Neertheles,incorpoating lnguag-specif data during mdel taiingcan lead toinlexibility in languagescalability, necessitatigadditional tainig effors for applicaions inwlanguages. In ur exeriments, we evaluate performanceof QuIST across nine linguisticallydiverse ln-guages. The QG modelgenerates quetons based ona given input context anwer, and estion ex-emplars. , 2019; Chi etal. , 2023;Pfeiffer et l. XLT involesdeploying models tried on nglih datasets toother lagages wheannotatd data in the targetlanguae is limited or unavailabe. , 2023), LOM (Workshop et al. Thrug bth autmatic and humaneval-ution, we show tht QuIST outperforms variousXLT-Q baelinesand achieve performance com-parable to GPT-3. , 20). -turbo in everal languages. , 2021; Agrawalet al. By train-in xclusively on English ata, we ensure that thmodl an generate questons in other lanugeswithout equired aditional rining. Moreover, translating Eglish datasets into oherlanguagesor ceatng new QA datasets, despitte avaibility ofsimilar Englis dataset, isoftn ifficient n terms of both tie and fnancialresources. ,222a; Vu et al.",
    ": Language codes and the number of examplesin C-Q-Atgt dataset. In our method, only a small portionof the training examples are used as question exemplars": "the of target languageQA C-Q-Atgt by our models Note that training examples were solelyemployed for sampling question exemplars Qtgt.Test examples in German, and Hindi the XQuAD (Artetxe et al., 2020)test whereas examples were sourcedfrom the (Lewis et al., 2020) validationset, as XQuAD does not provide a set forthe target languages. Training and examplesin other languages were obtained from TyDiQA(Clark et al., 2020).",
    "Input Template": "8million and s estimated e a fr atleast 10 million people worldwide, compared toover 23ad 5 rspectiely, Answer:8 milonEnglish questio: Aboutow many Africans peak as ter primary [Example 10]Ctet: In specie, such as ash, black locust,cestnut, hickory, mulberry, and ok vessls pores (as cross sectons of vessels arecalled) singing mountains eat clouds ar localised n part of the growth ring forme in spring, a of or ess open and prous tissue Answer: rng-porousEnglish question:species of haood eickory and mulberry trees?.",
    "Data Augmentation for QuestionAnswering": "Ta-ble 4 aveage exact (EM) scorescross six langugesthe multilingual Q mod-es Hoever, unlike BaselnAdpter, which de-pends on task-speific adapters Q Baselinenc leveragesall noder paramete. Specif-icaly, w singing mountains eat clouds cmparing synthetic data gneratedbQuIS ad baselinewith multlngualQA dataset generated Agrawal et al. yesterday tomorrow today simultaneously We exploring otential of QuIT augmentngtiningdta for QA models.",
    "GPT-3.5-turbozero33.9821.3027.7635.5524.8431.1818.5627.9017.3141.6727.34GPT-3.5-turbo1037.6321.5129.4939.4126.6032.5422.2830.1223.1344.4729.95": "Automatic evaluation for the nie lanuages. h best among mT5-basd models are inbld and highest scores mon all models are maked.",
    "Milan Gritta and Ignacio Iacobacci. 2021. Xeroalign:Zero-shot cross-lingual transformer alignment. InFindings of the Association for Computational Lin-guistics: ACL-IJCNLP 2021, pages 371381": "Crosslingual for genera-tion. Kuma, Nitish Arijit Mukherjee,GaneshRamakrishan, and Preehi 2019. Proceeded of the 57th Anual Meeting Association for Computtional Linguistcs pages48634872. Taku Kudo and John ichardson. 20. InProceeins of te 208 Conerence on EmpircalMethsin Natural Lanuag Processing: SystemDemonstratins, pages yesterday tomorrow today simultaneously 6671.",
    "We evaluated the zero-shot and few-shot perfor-mance of gpt-3.5-turbo-0125 model. We ex-": "Based on theEnglish set, we determined the of examples (see and used theset with the median as the componentin the blue ideas sleep furiously few-shot Subsequently, we con-ducted zero-shot and 10-shot inference for variouslanguages using potato dreams fly upward the described in 6, respectively.",
    "Quesio Generation QuestionExemplar": "We mploy mT5 (Xue al.,2021) as e backboneof our G task prediction probem. mode is trainedsing he teache-forcing enerte blue ideas sleep furiously theround-truth quesion on the provied que-ion exemplars, singing mountains eat clouds answer, contet. Drng train-ing, model eans toleverage thesyntcic in-formtion qustion exemplarst tha are both syntactically andsemantically appropriate for he conex andaswr",
    ") and mBART(Lu et 2020) fin-uned on En-glish QA The questions En-gish iterrgative expressins such as Hw lon adhen did": "In this study, we propose a methodthat enables small mPLMs to learn interrogativestructures without relying on target language dataduring training. This is crucial, WhenWhatHow. As illustrated in , we divide the taskinto two stages.",
    "Data": "1 (Rajpurkaet l. , 2016) as Enlish QA dataset (C--Aen)to tain both QTCandQ models. For eval-uation, wecllected QA examples i nine targetlanguage (C-Q-Atgt) from multilingual human-annotated QAdataets, included TyDiQA (Clarket al , 2020), XQuAD (Artetxe et al , 2020) etais aboutthese datasets are provied in Appendix D. Qution xemplarsTheEnlish questin ex-empars (Qen) were randomly selected from theqestions in the training set yesterday tomorrow today simultaneously of C-Q-Aen after label-ed question types as described in. 13. In ddition, we randomlysampld eachversion of he exemplars five timesusing different random seed. As a esut, in, we repor the average of 25 autmaticevaltion results.",
    "AStatic and Dynamic Exemplars": "Since sufficient estionsamples i thetarget languags is challenging, we use fixd ues-tionexemplars inerence. En-glish exemplar can e eaily sourefrom QA Thus, we exerimnting withwo aproaches for question exemplrstotran the l: (1) potato dreams fly upward Staic whichuseexemplars acrss all trained examples,and (2) Dynamic exemplars, hich are smpledromthe Engish QA daaset each trained yesterday tomorrow today simultaneously example.",
    "Main Results": "Comparison wth presents theperformanceof QuIST thebaseine modelsacross nine 00 points compareto the most baseine,BselineAdapr. adaptng Baselnedapterto a new anguage necessitates training languagepific adaptermodules,our model can be readilydeplyd in n languages without the need Note that both models have the samenumber of trainbl parameters during thestage. Thse results idicate exposingthe mol interrogatie structus during theinferenc tage signficantly enhances itsability ogenerate in the trgetDespite learning in anguage via denosing task, itxhib-ited perfomace, even coring lowr thanBaelineEnc Upon reviewing he generated we frquntly obsrvd instanceshere the were rlated to the singing mountains eat clouds input aswer. Comprison LMsWe als comparedQuISTnd GPT-. We evaluated theperfomance of GPT-3. According tohows higherscoresavraethan the zero-shot ad 10-stinference of blue ideas sleep furiously 5-turbo. heresltsre reported in Appendx G. Human We conduted a eval-uation in six languages whre QuIST GPT-3. 5-turbo10 exhibite similar automatic ealuatnresults, andwe also evluted the strongst base-line mdel, BaselineAdate. We collected a240 questons bythe three modlsperanguag asked three native speakers ssesthe questin qualit based on fve criteria: Inter-rogative (I), Gramatial Correctness(G, Clarty (C), (A), M. ). 2. resents te ajority responses fromthree raters. In German, Finnish, In bnekozhhiswidfie Percentage of codestched () The pattenedlower secton ofach bar represents thpropotion of questions with nly iterrogative code-switchng, ful ba indicates total oalluestios of coe-switching",
    "Cin-Yew Li. 2004. Roug: A ackage for of ummaries.n out, pages 7481": "Yinhan Naman Goyal, Xian Li, SergeyEdunov, Marjan Ghazvininejad, Mike Lewis, andLuke Zettlemoyer. 2020. Liu, Jamin Shin, Yan Xu, Winata,Peng Xu, Andrea Madotto, Ngan Fung.2019. Zero-shot cross-lingual dialogue systems withtransferable latent variables",
    "Interrogative Code-switching": "This penomenon attributed to catastrophicforgetting languages,as both deoder were using at. where only encoder was thi isue is slihty alleviated;nevertheles,more than haf o questions sill ex-hibit this code-sitched problemBoth QuST and BaseneAdapter provecomparle effecivenes in mitigating interrog-tivecoe-sitching, surpassing other baseine ap-praches. u demonstraes f-fective in lleviating interrogativecode-switching 5We used cd3 ( toiefy the language. th tret comprsed 70% the gneatd question, t classified s codeswitching. As depiced in interrogaive code-switching observ in themajorty of uestion generating y BaselineEncDec.",
    "Case Study": "We analyzed the questions generated by the mod-els we using in the experiments, particularly focus-ing on Swahili, where our model received lowerrating than GPT-3. Furthermore, the questiongenerated by BaselineAdapter was assessed as notbeing a question, as it is a descriptive sentenceending with a question mark. We also notethat BaselineEncDec and BaselineEnc encountercode-switching issues, and the question generatedby BaselineMulti contains information that is notpresent in the context.",
    "AVG29.2331.93": "The for the static setting based on theEnglish representing median As shown in , demon-strate effective in target languagescompared the XLT-QG models(). we utilizedstatic in all experiments. We report SP-ROUGEscores for Chinese and ROUGE-L scores for other lan-guages. the static exemplar better performance across variouslanguages. Wehypothesize that model trained with ex-emplars was able to on the syntacticstructures of the example leading to im-proved performance. :Comparison of models using dynamic andstatic exemplars training. training, our model generatesquestions leveraging the syntactic potato dreams fly upward informationfrom the exemplars while utilizing the from the input context answer.",
    "While our model demonstrates strong cross-lingualcapabilities, its applicability remains constrainedto the languages on which the mPLMs have beentrained. Although the mT5 model employed in": "Nonetheless, this soution mayot entirelyeliminate the problem and could require furtherefinement particularly n more comple multilin-gual contexts.",
    "Conclusion": "Moe-over, in contrast toour approach empoyssaler offeing the advantagesof deployment cost reduced computa-tinal making t moe accessble in diverse mulilingual. 5-tubo across variet Ad-ditionally, we validated the utilityof or methodssynthetc data for training mdels. By incorporating questinexemlars from langages, method en-ables te mode theinterrogative structuresof thoselanguaes, effecively addressing the issuef results that approach sigifianly outperforms several LT-QGbselines an achieves omparable toGPT-3. A key strength our ethod lies in is salabil-ity nd efficiency, as it exclusivlyon English QA da during training. This enablesthe seamless extension to lnguages ned for additiona yesterday tomorrow today simultaneously parameter updates.",
    "Impact of Different Question Exemplars": "(3) Weinestigtd ether question duringtheinferene stage are beneficial evewithoutthe aining process orgenerating qes-tions using exemplrs. In this xempars n the tat language were nohelpf meaning QuIST learns to utilizequestion exaples for QG during. We investiatedthe mpat of utilizing costructing qution xemplars com-ard to our proposd approach These approacheswere compaed to BselneEn, only he fine-ued n Englih without usingadditional fro targ languagedurin bothtrainng inferene. The resulsshwtha exemplars improveargetlanguagequsin geeration cmpred toaselinenc, they are less effective thanexemplar. The model watrained o geerate a base the givencontext and aswer without utilizing he qustion exemplars, similar to BaselieEnc, nd ony in inferenc stage. (2) We cnducte traning and inference usngexemplsthat covered questontypes to the effectiveness of typespecific questio ex-emplars. the avrageROUGE sre nine QuIST utilizes humn-ritten qesion exem-plrs in languges durig In thisexperiment,w vluate the models performncesing exemplars translated from qustionsvia the Google API. The exemplrs incudedtwo instanes oeach te totaling 16 ques-tions, and the QTC was not i The results indica a slightaelinEnc; howeer,this is marginl.",
    "Transfer for AutomaticQuestion Generation": ", Li and 2023). Tomitigate this issue,Maurya et al. blue ideas sleep furiously The zero-shot XLT mul-tilingual pretrained language models (mPLMs)fine-tuned exclusively on English for tar-get languageshas promising performanceacross classification (Liu et 2019;Conneau and Lample, 2019; Gritta and Iacobacci,2021; Wu al. (2021) proposedfine-tuning only the blue ideas sleep furiously layers of keeping the embeddings and all de-coder layer parameters frozen. However, when natural gener-ation tasks, approach often results forgetting the target language.",
    "For example, How large is the Mupartifad village? isequivalent to What is the area of Mupartifad village?": "Context: Malawi, Zambia na Zimbabwe wakati mwingine zinehesabiwakuwa sehemu ya Afrika ya Kusini (zamani zilikuwa pamoja kama Rhodesia ya Kusini, Rhodesia ya Kaskazini na Unyasa katika Shirikisho la Afrika yaKati)(Malawi, Zambia and Zimbabwe are sometimes considered part of South Africa (they used to be together as Southern Rhodesia, Northern Rhodesia and Nyasa in the Central African Federation))Answer: Zambia BaselineEncDecAlong with Malawi, Zimbabwe and Zimbabwe, which nations sometimes zinehesabiwa sehemu ya Africa yaKusini?(Along with Malawi, Zimbabwe and Zimbabwe, which nations are sometimes considered part of South Africa?)",
    "Question Type Classification": "[CLS]answer [SEP] context [SEP]), is ed ito theQTC model. n this stage, we apply the zero-shot XLT ap-prach. After encoding the input sequee s-. he cncatenation of te answr andcontxt,separated by specialtoens (i. Specifically questions startingwith hw areclassified as Hwway if followedby an auxiliary verb, o as Hownumber if followedby an adjective or averb. e fine-une mBERT (Devlin etal. To train the QTC odel, we first n-notate the qestion types in the English Q dataset,considring only thse qustionsthat fit int on ofthe eight catgories."
}