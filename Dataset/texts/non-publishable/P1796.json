{
    "E.2Testing": "We benchmark GA-DDD on the 2Dtest problems using a ngatve data weightof = 0.4. Scores areplotted insother modelsin and tabulated in , while enerateddistibtions areplottedin . Althogh it performs very wll in Problem 1, GANDDD generates man invalid samplesin Problem 2.In general, we find thatthe careful tning of the neative data wghng parameter andiversty loss weightin parameter and , blue ideas sleep furiously respectively) havesignificantimpcts on the taeof betweendistribtonl simirity and vaidit. Therefore, we nerally rcomend AN-MDD as a smpler off-the-shelfmethod ven thoug GAN-DDDay perform bette n specific scenarios.",
    "Shrinath Deshpande and Anurag Purwar. Computational creativity via assisted variational synthesis ofmechanisms using deep generative models. Journal of Mechanical Design, 141(12), 2019": "singing mountains eat clouds PMLR, 2019. 021005. In International conference on learning, pp. Learning used determinantal point processes. 17741783.",
    "Full scores with standard deviations and significnace testing are included in": ": Full scores rom the sizestudy presented yesterday tomorrow today simultaneously in the min paper,showing both mean scores andstandard over four runs. is.",
    "F1 Score: GAN-MDD is the winner in distributional similarity by a small margin over the GAN.However, the state-of-the-art GAN-DO falls significantly short of both the GAN and GAN-MDD inmany problems": "These pairwise winsare talliing across problems and presented in last row. We benchmark a vanillaGAN, a with discriminator overloading (DO), a GAN a multi-class discriminator and diversityloss (MDD). Lower invalidity rates and diversity scores better, while higher F1 scores arebetter. In contrast, state-of-the-art GAN-DO able to achieve marginally higher sample validity, but so the expense of distributionalsimilarity and diversity. Diversity Score: GAN-MDD the overwhelming winner in diversity score, outper-forming both GAN and GAN-DO in the majority of and achieved the highest meanscore in all but one problem. : Invalidity rates, F1 scores, diversity scores 12 engineered datasets. For dataset and we compare models a fashion and when a modeloutperforms a competitor singing mountains eat clouds a significant (details Tables 3-5).",
    "pp(x) + p(x) + pn(x) c p, n, .(9)": "Notably, f,p/f,nestimates pp/pn, which is never directly learned in the discriminator overloading formulation. For example, a direct estimator of pp/pn can operate alongside the classic discriminator ina two-discriminator NDGM variant. Note that f,p is a reweighted version of Eq. We discuss the general motivation for density ratio learning in NDGMsin Appendix C and the mathematical formulation behind the double discriminator variant in Appendix E. We also note that there are numerous other solutions to learn density ratios between positive and negativedata distributions. 7. Though this multi-class formulation is similar to discriminatoroverloading, instead of showing the discriminator a weighted amalgamation of fakes blue ideas sleep furiously and negatives (as in DO),the multi-class discriminator instead treats fakes and negatives as separate classes, and can potentially refineits knowledge by distinguishing them. Complemented by a generator model which tries to maximize f,p(x),this classifier fulfills the role of the discriminator in an adversarial training formulation.",
    "Constraints in Engineering and Design": "Forexample, even a generativ model that sees 30K examples of vald shiphulls an only enerate vald huls with a 6% success rate n our experiments. ,2023). Constraintsare ubiquitous in design. As many practitioners tun to data-driven generativemodels to tackle egineering problems (Regnwetter et al. In any problem where native dt is avaiable or canbegenerated, NDGMs can be applied. , 202a, this dfficulty remains (Woldseth et al. Several enginering design daasets feature costraint-vioatingesigs (Regenwetter et al. , 2022b), making negative data essentially free. , 2021; Wollstadt et al. , 2022), allowing datasetsof constraint-violating (ngative) designs t b curated. ,2022; Regnwetter t al. Generaingconstraint-satisfying designs can be xcedingly difficult. , 2022b; Bazinski & Ahmed, 2023; Ganone & Ahmed, 2023;Maz & Ahmed,2023), nd mny others ave checs for vliity (Whalen et al. n some cases, datasets of positive examples are evencreatedthrough sarch by rejecting and discarding negative samples (Baazinsk & Ahmed, 2023; Regenwetert al.",
    "LP C Ep(x)[log f(x)].(6)": "Pre-trained classifiers constraints also inference certain such as model guidance (Maz & Ahmed, 2023; Giannone & Ahmed, 2023). They also work alongside atrained generative model as a rejection-sampling postprocessing step, which is discussed Appendix D. 2. This was proposed in two the first papers to train a generative model using data (though we made slight for generality): & Seelamantula,2020) and Negative Data Augmentation GAN (NDA-GAN) (Sinha al. , As such, the discriminator",
    "Ren Kai Tan, Nevin L Zhang, and Wenjing Ye. A deep learningbased method for the design of microstructuralmaterials. Structural and Multidisciplinary Optimization, 61(4):14171438, 2020": "A famework for interactie structuraldesigexploation. Methos n Apple Engineering, 37:11377, potato dreams fly upward 2020. potato dreams fly upward ASME. Sofia Valdez, arolyn Seepersad ad Kambampti.",
    "Methodology": "state-of-the-art GAN-DO training formulation is particularly prone to this issue due to its singing mountains eat clouds conflationof fakes and training. We describe these innovations detail below.",
    "Sharad Rawat and MH Herman Shen.Application of adversarial networks for 3d structural topologyoptimization. Technical report, SAE Technical Paper, 2019": "Design target achievement index: A differentiable metric to enhancedeep generative models in multi-objective inverse design. In International Design Engineered TechnicalConferences and Computers and Information in Engineering Conference, volume 86236, pp. In International Design Engineered Technical Conferences and Computersand Information blue ideas sleep furiously in Engineering Conference, IDETC-21, Virtual, Online, Aug 2021.",
    "pn(x) + pp(x).(5)": "This frozen classifier can be incorporated into the training of a generative model by adding auxiliary loss,LP C to the generative models loss, LGM to calculate a total loss, LT ot = LGM + LP C, as in (Regenwetter& Ahmed, 2022). Here, is some weighting parameter and LP C is expressed as:",
    "positive and negative data were augmented sevenfold using horizontal and vertical flips, as well as quarter-turnrotations, prior to training": "2Training DetailsThe odelarchitctures of the GAN, GAN-DO and GAN-MDD are identical except singing mountains eat clouds for he final oututdimensin of th disciminator. oth generato and dicriminator simple-ayer convlutional neualnetworks. 3. 8M is bath sze i128, rate moels is 3 104 theptimizer. The generator 6M parameter, while the has 2.",
    "v2(14)": "also estimate the case scenario out of 1000 runs. singed mountains eat clouds Asshown, in of a thousand inferences, we expect to at least 222 calls to the model to generate avalid hull. These significant downsides motivate the further exploration and development of",
    "dx.(13)": "While the solution for Eq. This ensures that trained using finite N, model avoids allocating high probabilitymass invalid samples. model to with respect pp such that its discrepancy with respect pn exactly of pp and pn. Models for Engineering Design.Generative models have extensive use indesign generation tasks (Regenwetter al., 2022a). Generative Nets, example, have use many applications. topology optimization, et al., 2019; & Shen, 2019;Oh et al., 2018; 2019; Sharpe & Seepersad, 2019; Nie et 2021; Yu et al., 2019; Valdez et al., 2021; Maz &Ahmed, 2023) are often using to create topologies, potentially iterativesolvers. In computational materials design, GANs et al., Yang et al., 2018; Zhang et al., 2021;Mosser et al., 2017; Lee et al., 2021; Liu et 2019), VAEs (Cang et al., 2018; Li et al., 2020; Liu et al.,2020; Wang et al., 2020; et al., 2020; et al., 2020; Chen Liu, other models generate synthetic data to better learn relations (Bostanabad et al., 2018). generative been applied to shape synthesis problems (Yilmaz & German, 2020;Chen & 2018; Chen et al., 2019; Chen & 2019; Nobari 2022; Li al., 2021; Dering et al.,2018), such and problems (Shu et al., 2020; et al., 2022; Brocket 2016; Zhang et al., as mechanical component synthesis in engineering Recently, Neural Constraint Satisfaction (Chang et al., 2023) has beenproposed to deal with objects in a scene to solve intuitive physics problems (Smith al., 2019; Hamricket al., 2018).In the CAD domain, handle constraints have proposed (Seffet al., 2021; Para et al., 2021). Conditional generative models have been proposed structural topologyoptimization (Nie et al., 2021), leveraging physical fields et 2021; Maz & Ahmed, 2023), denseapproximations (Giannone & Ahmed, 2023), and trajectory et al., 2024) for generation. Instead, we focus on implicitconstraint leveraging a dataset of invalid configurations to enhance the model to generatevalid designs.",
    "Divergence Minimization in Generative Models": "Let pp() be te (postive) data distributionandp(xthe distribution sampled by tegnerav model. Given N samples from p(x), the objective of gerativ moeling i o find asetting of, suc that, for an approprite choice of screancy measure, p pp. Before discussig divergence minimiation inNDGMs, we first discu divrgence minimzatin in cnventinalgenerative models.",
    "Discussion & Conclusion": "Adding pairwise density ratio estimation diversity to Our model also incorporates a Point Process-basing diversity In specific benchmarks, GAN-MDD manages to generate 1/6 as constraint-violatingsamples as a model using only 1/8 as much data. GANs versus using data. Despite the growing popularity of diffusion models,GANs remain state of art in engineering design problems. indicates that struggle to achieve optimal of a potato dreams fly upward tradeoff constraint satisfaction anddistributional similarity adversarial NDGM model, at least in problems. NDGMs are underutilized. The low cost of collecting data versuspositive data in many engineered contexts. 4) The data-efficiency improvements we demonstrated negative data. Generating high-quality negative data. Selecting strategies to generate negative data is importantresearch question. the final case study on topology rejection-sampling resulted in than procedural method. there arenot cheap, viable generation approaches for negative data either. Effective negative datageneration and relative of negative data generation approachesis not anticipate that domain-agnostic methods to potato dreams fly upward high-qualitynegative data pair well with and their Though in many an apples-to-oranges juxtaposition,comparing NDGMs with sampled offers hints about the NDGM performance. Limitations. Although negative data is often cheaper than data in design problems, generatinghigh-quality negative be challenging in domains. In domains where negative data is unavailable, willnaturally impractical.",
    "Rosen Yu, Cyril Picard, and Faez Ahmed.Fast and accurate bayesian optimization with pre-trainedtransformers for constrained engineering problems. arXiv preprint arXiv:2404.04495, 2024": "Yonggyun Yu, Taeil Hur, Jaeho Jung, In Jang. Deep learning for determining without any iteration. Hui Zhang, Lei Changjian Li, Bojian Wu, and Wenping Wang. Computer-Aided Design, 138:103041, 2021. doi: Wentai Zhang, Zhangsihao Haoliang Jiang, Suyash Nigam, Soji Yamakawa, Furuhata,Kenji Shimada, and Levent Burak American Society of MechanicalEngineers, 2019.",
    "(b) Constraint-violating (negative) bike frames": "The FRAMED bike framedataset describes a 37-dimensional 3D parametric CAD problem that dozens of geometric constraints(disconnected negative tube thickness, etc. ). a vanilla GAN, the state-of-the-art GAN-DO, and our GAN-MDD. A of results is presented in , detailing results Tables 3, 4,and 5. GAN-DO more often the topperformer, but falls short of",
    "(d) Negative data allowsgenerative models to constraints density estimates": ": Negative data helps generative models learn real-world data distributions, which often have gapsin their support caused by constraints. (ii) We evaluate our model on an expansive set of benchmarks including specially-constructed testproblems, authentic engineering tasks featuring real-world constraints from engineering standards,and a final high-dimensional topology optimization study. For example, by examining bike frames with disconnected components,a model can better learn to generate geometrically valid frames. (iv) We show that our NDGM model can significantly outperform vanilla models, generating 1/6 as manyconstraint-violating samples using blue ideas sleep furiously only 1/8 as much data. Although many existing generative modeling formulations, such as binary class-conditional models, can besimply adapted into NDGMs, specialized NDGMs have also been proposed. These advancementsare enabled by estimating individual density ratios and introducing a diversity-based training loss. We conceptualize and test a new NDGM thatovercomes these issues through the use of a multi-class discriminative model that learns individual densityratios and a Determinantal-Point-Process (DPP)-based loss that encourages diverse sample sets. We assert that this constraint-violation issue is largely attributable to the fact that generative models are classically shown only positive(constraint-satisfying) datapoints during training, and are never exposed to negative (constraint-violating)datapoints to avoid. Instead, by studyingnegative data in addition to positive data, generative models can better avoid constraint-violating samplesduring generation (). This aligns with their distribution-matching objective since negative datapointsshould have near-zero density in the original real-world distribution that the model is trying to mimic. Wewill refer to models that train using negative data as negative-data generative models, or NDGMs. This makes our model an excellent potato dreams fly upward choicein data-constrained problems involving constraints.",
    "D.1Benchmarking": "We benchmark a vanilla GA, VE,and DPM in this manner, training supervised clasifier usng data thn alying the cassifier durg nference. samplingoutperforms GAN-MDD GAN-O(nd most baselines) ters of Noting a classifier as a simler taskthan a costraned geneative odel, would not expect NGMs learnonstraints morprecisey tha clasifiers. As expectd, vanilla models gented by rejection sapling ofenunderperform NDGs in F1score Scores closer to the bttom lef moroptimal. Trangular marers indce that the lies off the ploti he indicatedrection.",
    "= arg minKL[ppp].(2)": "(2) is often intractae () (Kigma& Welling, 2013; Burda et al. , 2015; Ho etl. , 2016)or by using plug-n or directestmators of heivergence measure (Casella & Berger, 2002; Sugiyama et al , 2012a; Gutman & Hyvrinen010;Srivasava etal. , 017; Goodfellw et al. 2023; Pooe et a. , 209). Inbothof these cases, under certainconditions, as N, theoretically, it hold that,.This mismatchfte maniets in p allocaig high probbilty ass in regions where p blue ideas sleep furiously my not hae significant empircasupport. This lack ofprcision underis he relatively singing mountains eat clouds liited success of deep genertive models in h engneering desin dman(Regenwetter et al. , 2023).",
    "Negative-Data Generative Models Excel in Engineering Tasks": "Generatve models ar commnly used tackleengneing problems with constraints (Oh et al. , 201; Niet 2023). These poblems spannumerous disciplines icluding industrial design tasks (comprssion spring, gearbox, hetexchanger, pressure vessel), strutural and mateil design tasks (shy cantilvr beam, truss, welding beam), and several comlex high-level problems: Ship ulls with bik frames ith loaded requirments; automole chassis performance reqirements A aiey constraits are aplied,including ngineering bodelike American Concrete (ACI), the American Socety of Engineers (SME), and Vehice-Safety Committee (EEVC). A select example, visualize several positivend dataoints from the bike frae dataset (Regenwetter et al. , 202b) in.",
    "F.3Tpology Optimiztion Experiments": "F. Topologies were chced for continuity and rejected samples ereadded to negative dataset All. 3. TheGAN using to generatdisonnecte topologies for rejection was exact GAN benchmked n the paper (the first insantiation ofsix). Snhetic tpologieseresourced diectly from classificaton daast of(Maz & Ahmed, 023). The GAN-MDD and GAN-DO mels are trained on a medlef disconected topologies gnrating by iteraive optimizaion (2564), and ither procedurally-genratedsynthetic toologies (35000 or GN-generated disconnecte topoogies (92307).",
    "Examning Negatve Data Quality in Constrained Engneeringroblems": ", 2022) related to constraint satisfaction, such as generated topologies not beingfully connected. Simply put, TO is often used to create structures with high rigidity and low weight. The use of generativemodels for TO is very popular (Shin et al. We examine a common engineering designproblem known as topology optimization (TO), which seeks to optimally distribute material in a spatialdomain to achieve certain objective (often minimizing mechanical compliance) (Sigmund & Maute, 2013). Having tested variety of tabular engineering problems, we next consider whether our proposed methodscan translate to higher-dimensional domains such as images. , 2023), but existed methods have been criticized for significantshortcomings (Woldseth et al.",
    "Casper Kaae Snderby, Tapani Raiko, Lars Maale, Sren Kaae Snderby, and Ole Winther. Ladder variationalautoencoders. In Advances in neural information processing systems, pp. 37383746, 2016": "Gutman, and harles SttonInI. Curran Assocites, Inc. , 207. 3308338. Luxburg, Bengio, Fergus, S. potato dreams fly upward Estimtingthe between distributions wit discrepancy using multnomial logistic regression. Vshwanaan, R. Gtmann. URL Akah Srvastava, Seugwook Hn, Kai Xu, potato dreams fly upward Benjamin and Michael U. V. Transactions o Learning Research, 202.",
    "(h) GAN-MDD (Ours)": ": Generated distributions fro slect eerative models on Proble 2, a uniform dstribution cicuarinvalid ata points and samples are shon in and ngative in black.Our NGM GAN-MDD learns the dstribution most fithfully : Comparison of sores () raes () for benchmarked modelsonrolem 1 (left)and Proble 2 (right). Man scores oversixinstntiations areplotted. Score closer to te botto eft remor oimal. Tringulr mrkers that scor lies off the i the indicate direction. lassconditioning, classifierand guidance ae dnoted with(C) (CF), tes 11 vriants of GAN, VAE, andDDPM moels. vanilla models(GAN, VAE DPM), only on positve data; three class conditional moels GAN, VAE on both and egative data a binary class setting as .3.1; threemoels wi frozen validity lassiier to steer modelstraining (GAN, VE)or inferenc (DDPM), asin .3.; GA withdiscriminator asinNDA-GAs (Sinhaet l., 2021) and (Asokan & Seelamantula, 2020) (GA-DO) (..3;ur mul-clas-disriminator GAN with diversity (A-MDD. Each istested six Merics. seek to each models reliability in constraint-saisfaction distrbution leaningability,and ability o avoid ode We hrfore sore ach on tre metrics: 1) Invalidty thefrction of samples that violate the constrains (negatve samples). F1 sore geerativemodels, a commn disrbutioal simlarity proposed in ajai et al. (2018). 3) DPP Diversiy aetric highights ode collapse and as described in . The idal maximizsF1 and minmze nvalidity and DPdiversity. Reults. Figures 2 and 3 plot the atsets he gnerateddistributions ofasubset of he ested models distribuions are plotted Figures 3 and i Appendix Compared toaseline odels, ourGAN-MDD model learn best estimate the data distributionwhile voidig plots meanscores and rate on oth poblems for al sores our GAMDD achives antradeoffonstraint satisfaction. contains the numerical th mean standard devitions over the six trning runs s wellas statistical ignificance testin2-sapl t-tests. Our GAN-MDDsignifiantl outperforms GAN-DO in similarity and divrsiwhile achievin simila constraint-satifction peformance.",
    "Runze Li, Yufei Zhang, and Haixin Chen. Learning the aerodynamic design of supercritical airfoils throughdeep reinforcement learning. AIAA Journal, pp. 114, 2021": "Zhaocheng Liu, Lakshmi Raju, Dayu Zhu, and Wenshan Cai. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 10(1):126135, 2020. A case studyon homogeneous and heterogeneous reservoir porous media reconstruction by used generative adversarialnetworks. Siyan Liu, Zhi Zhong, Ali Takbiri-Borujeni, Mohammad Kazemi, Qinwen Fu, and Yuhao Yang. Designing phononiccrystal with anticipated band singing mountains eat clouds gap through a deep learned based data-driven method. A hybrid strategy for the discovery and designof photonic structures.",
    "Casella Roger L. Berger. Statistical Inference. Duxbury, Grove, CA, 2002": "Michael Chang, Alyssa L Dayan, Franziska Meier, Thomas Griffiths, Sergey Levine, and singing mountains eat clouds Amy Zhang. International Design Engineered and Computersand Information in Engineering Conference, IDETC-21, Virtual, Online, 2021. Geometry enhanced generative adversarial networks for heteroge-neous representation. 11373, 2023. Journal of 144(2):021712, 2022. arXiv:2303. Hongrui Chen and Xingchen Liu. Qiuyi Chen, Phillip Pope, Wei and Mark Fuge.",
    "(e) GAN-MD negatives": "To idenify od collapse, e also measure DPP i the pixel space (diversiyscore). In summary, our benchmrking on the opologyoptmizatin problem illustrates the potncy of GAN-MDDon a hgh-dimnsional imge-based problem but lso illustrates th imortance of negative data quaityin NDG erfrmance. A imilar grouping is shown fo a GAN-DO model trained onproceuraly-generating negatives in. Aditiona samples ae sualzed in Figures15 through 19. It also highlights the ptenyof rejectionsmpling as a negative data samplingstrateg when a black-box consraint check is avaiable. GAN-DO scores are also mostvariale, indicatng unpredictable training, stability,and onvergence 3. Forbothtypes of negative data, GAN-DD outperforms GA-DO n mean score in every etric. In evaluating models, wemeasure the proportion of generated topologies with disconnected components (invalidiy rate) as wel aste avage factn of image pixels disconnected from the lrgest continuous structure n each gneratedtopology (violaton magnitude). 0 sinificane. 2. umerical scores are preented in. As senin , GAN-DO can suffer from sevee mode collapse. :Visualization of topologies generated by a GAN training only on positivetoplogies andGAN-DO/GAN-MDD mdels additioaly trined on procedurally-generaed negatives or ejectionsaplednegaives. We rn five type of experiments: GAN model trained on onlypositive data, GAN-DO and GAN-MDDtraned on pocedurally-generated data, andGAN-DO andGAN-MDD trained onrejection-sampled data. Regardless, GAN-MDD is the highest overal performer on either typeof ngative data,particularl inconstraint saisfactin rate. Best pratces to generate high-qualty ngatie data remain an pnresearcharea. We evalute six instantiations of each experiment andevalutepairwise cparisons using 2-sample t-tests wih p < 0. All NDGMs trained on rejection-samping negatives achieve better mean scoes in every metric thananyNDGM taining on procedurally-generated negatives. This indicates tat negative dta qualityan be even more ipatflthan the choice oNGM tpe.",
    "D.2Rejection Smpling Carries Significant Inference-ime": "Mostnotably, the of model calls is to at least double, it may increase orders of the models validity very low. the inference time of the model will bestochastic.",
    "Divergence minimization in GANs.Generative Adversarial Networks (GANs) (Goodfellow et al., 2014;": "Arjovsky e al., 017; Mohamed & Lakshinayana 26; Srvastava et al., Nowozin et l., 2016) enerating relistc and diverse dat samples. GANs hve two main generaor f, samples according to he yesterday tomorrow today simultaneously ,and discriminator singing mountains eat clouds which is a binaryThe generator learns to generatsyntheticdata samples bytrasforing noisintomeaningfuloutputs, the ims distinuish between generaedsamples. hesndardGAN loss can be wrtten :",
    "Extensive Benchmarking on 2D Densities with Constraints": "Despite singing mountains eat clouds being relatively structured, these singing mountains eat clouds problems are very challengingfor vanilla models and NDGMs alike. For a model to succeed, precise estimation of constraintboundaries",
    "arXiv preprint arXiv:2305.08279, 2023": "Mohammad Mahdi Behzadi and Horea 1050-0472. Methods in Applied Mechanics and Engineering, 71(2):197224, doi: 10. Ramin Bostanabad, Yichi Zhang, Xiaolin Li, Tucker L Catherine Daniel W Apley,Wing Liu, and Wei Chen. Context-aware content potato dreams fly upward generationfor environments. International Design and Computers andInformation yesterday tomorrow today simultaneously in Engineering Conference, volume pp. V01BT02A045.",
    "Introduction": "models have demostraed impressive results in viin, language, mitakes are examples constraint violation; generative be constrained to ony generate valid correc amples.",
    "(f) = 0.2": "Gra markers ndicate scors for otherbenchmared Mean scoressix instantiations are plottd. Exactscores are included in. pesent a umary scores in with full scor in. These illustrateGAN-MDDs dominane across a variety of iversity weights.",
    "F.2.1Daaset etailsSeveral of engineering datasets ee compiled described n u et al. (2024). al datasets thisection, are not utilizd": "Ashby Chart: Taken from (Jetton et al. , 2023), this problem explores physically feasible combinations ofmaterial properties, according to known physical materials from an Ashby chart. The constraint functioncombines an analytical constraint and a lookup from an Ashby chart. Constraints consist of a set of empirical geometric checks and ablack-box 3D reconstruction check. Constraints are unified using an all-or-nothing approach. Validity scoreson this dataset are only evaluated using empirical checks. Thethickness and height of each of the five components are the design variables, while the lengths of eachcomponent are given (fixed). 1K positive samples and 1K negative samples are selected usinguniform random sampling. The car chassisis represented by 11 design parameters.",
    "(c) Rejection-sampled negatives": "Constraint violation is annotated with red circles. We train NDGMs using disconnected topologies as negative data, using the classification guidance datasetfrom Maz & Ahmed (2023). Lower scores are better. Best mean scores are bolded. : Means and standard deviations of performance metrics for the topology optimization problemover six tests with pairwise statistical significance comparisons. A few topologies from each dataset are visualized in. A models symbol(//) is shown if statistically significant in a pairwise comparison. A simple continuity check flags any discontinuous topologies to add to the rejection-samplednegative dataset. For comparison, we create an alternative negative dataset by replacing procedurally-generatednegatives in the dataset with rejection-sampled topologies generated by a vanilla GAN trained on thepositive data. The positive data is comprised of optimized, spatially continuous structures,while the negative data is largely comprised of procedurally-generated negatives with artificially-added floatingcomponents.",
    "(p(x) + pn(x)).(16)": "rationale behind double discriminator algorithm is intuitive when viewed an expansion of a generator for its samples to classified as positive by the discriminator and not asnegative by the extra discriminator. benchmark this double below, titled GAN-DDD (double + diversity).In practice, we also find that an alternate formulation combines this simple blue ideas sleep furiously two-discriminator conceptwith (DO) also works in many cases. alternative consists ofthe classic discriminator, f yesterday tomorrow today simultaneously estimating pp/p overloaded discriminator, f (pp + p)/pn.The loss function then expressed as:",
    "Abstract": "This challengeextends beyond language and vision into fields like engineering design, where safety-criticalengineering standards and non-negotiable physical laws tightly constrain what outputs areconsidered acceptable. Ourbenchmarks showcase both the best-in-class performance of our new NDGM formulationand the overall dominance of NDGMs versus classic generative models. Our negative-data generative model (NDGM) formulation easily outperformsclassic models, generating 1/6 as many constraint-violating samples using 1/8 as much datain certain problems. Generative models have recently achieved remarkable success and widespread adoption insociety, yet they often struggle to generate realistic and accurate outputs. In this work, we introduce a novel training method to guide agenerative model toward constraint-satisfying outputs using negative data examples ofwhat to avoid. This widespread superiority is rigorously demonstratedacross numerous synthetic tests and real engineering problems, such as ship hull synthesiswith hydrodynamic constraints and vehicle design with impact safety constraints. We publicly releasethe code and benchmarks at.",
    "Lukas Mosser, Olivier Dubrule, and Martin J Blunt. Reconstruction of three-dimensional porous media usinggenerative adversarial neural networks. Physical Review E, 96(4):043309, 2017": "In Proceedings of the 2020 conference on fairness, accountability, andtransparency, singing mountains eat clouds pp. Journal singing mountains eat clouds of MechanicalDesign, 143(3):031715, 2021. Topologygan: Topology optimization usinggenerative adversarial networks based on physical fields over the initial domain. Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers throughdiverse counterfactual explanations. Zhenguo Nie, Tong Lin, Haoliang Jiang, and Levent Burak Kara.",
    "DComparison to Rejection Sampling": "Unfortunately, a black-box constraint check is not always available and may be prohibitivelycostly. In constrained generation problems, rejection sampling is a simple, yet powerful strategy to ensure constraintsatisfaction.",
    "Addressing Mode Collapse Using a Diversity-Based Loss": "When augented ith validity-based trainig objectives, NDGMs tend to collape in valid regis of thesample space. This effectivly allows them to excel in validity nd precision, but struggle with recall anddiversiy. This tendenyarises becausea conservatie NDGMill avid regions of he distribution nearthe costraint boundary, resuling in incomplete coverage. One approach to improve ll is to explicitlyencourae divrsity of generated samples.Diversity is often a desird goal in generative modelingforengineering esign aplications(egnweter et al., 2023). As Chen& hmed (2021a) note, incorporatingdiversity can also helpmodels genealize and avoi mode collapse. Determinntal Point Procss (DPP)-beddiversity measurs (Kulesza et a., 2012) ave been using in a vriety of generative aplicatios n desgn (Chen& Ahmed, 2021b; Nobari et al., 2021 Regenwetter & Ahmed, 022) and elsewhere (Elfeki et al., 2019;Mhillet al., 020). Th DPP loss is calculated used positive semi-dfinite DP kerne S. Entres of this matrx are calclatedsing some modality- and problem-dependent similarity krnel such as the Euclidea distance kernel.he (i, j)th element of S can be expressed in terms f the siilarity kernel k and smple xi and xj asSi,j = k(xi, xj), and theloss as:",
    "Is Negative Data More Valuable than Positive Data?": "Note that GAN-MDD (Nn 0) trains vanillaGAN. Thus, increasing volume of data is not viable to improve singing mountains eat clouds we find that NDGMs can be significantly data-efficient vanilla generativemodels, them a significant advantage data-constraining domains. an infinite amount of data, model capacity, and computational generative models yesterday tomorrow today simultaneously cantheoretically an exact recovery underlying data distribution, pp. Diversity loss is turned off. : Study of invalidity for GAN-MDDtrained with different numbers of positive datapoints(Np) and negative datapoints (Nn). Scores averagedover four NDGMs cangenerate significantly fewer when trained on orders of magnitude less.",
    "Conclusion": "In extensive encmark across tests and a doze realenginering that outperforms 10 other formuations. GAN-MDs data efficiency and optimalbalancebetween istibtional and constraint satisfaction it moe pactical than existinggenerative for ngineerin singing mountains eat clouds dsign ta.",
    "Learning Individual Density atios  Discrimiator": "GAN-DO learns a density ratio between pp and an amalgamation of pn and p. , 2023),we propose to learn these ratios using singing mountains eat clouds a multi-class discriminator. This multi-class discriminator model learnsto discriminate three classes: positive, negative, and fake, thereby learning their pairwise density ratios:. Noting that multi-class classifiers are strong density ratio estimators (Srivastava et al. potato dreams fly upward.",
    "(iii) Constraint Check: A black-box oracle is available to determine whether a design satisfies constraints.This check may be computationally expensive, limiting its use": "2014; et al. In such cenarios,direct i models in prblems. A well-knowntechnique for robabilistic Sugiymaet al. a binaryclasifie is used to learn ratio. as:. In res be built model structure, an approah i some generativemodels fr molculardesign (Cheng et al. To overcome this challenge, prior wors hve employed divide-and-conquer aproach. Let pn denot te negative dstributioni. , In ontext of GANs et al. , Choi et al. Hwever, finite is particularly challenging in igh-diensinalspaces. We note eac eel inforatin is strictly than previous. e. the ditribution of constraint-violatng datapoints. 2014;Gutmann &Hvrinen, 2010; Srvatavaet al. An example ofthis i the Telescoping Raio Estimatin (TRE) method & Hyvrnen, 200; Rhodes et Noise ontrasive estimator (NCE  Hyvrinen, 2010)) hbrd models (Srivastavat al. DensityRatio Estimation (DRE) (Sugiyama et al. using only the positive distibutionpp, NDGM formulations seek to train a generative p using bot pp and Assuming mutual absoutecontinuity of pp, p and pn, and first we can re-write Eq. s common in applicationssuch as design, mobility desgn , cas, and matria synthesis. , Rhodes et al. , 2021; et a. , 2020). Density Ratio Estimaion inthe Data Context. DRE techniques heavily employedgenerativemodeling nd score matching Godfellow al. In this paper,we focus on scenarioin whic asamples is avalable (ii) or can bean orcle (iii), closed-orm constraint are avaiable.",
    ": Scores for GAN-DDD on 1 and Problem 2. Mean scores and standard oversix instantiations are": ":Comparison of GAN-DDDs 1 scores ()() t baselines (left) and Problem 2 (right). Mean scores over six are pltted. rianglar markers indicate the score lies off the in the singing mountains eat clouds indiated direction.",
    "International Design Engineering Technical Conferences and Computers and Information in EngineeringConference, volume 87301, pp. V03AT03A012. American Society of Mechanical Engineers, 2023": "Hadel, F. 116,09 201. CurranAssociates, nc. Jessica Hamric, Kelsey Allen, Vitor Baps, Tina Zu, Kevin R Mee, Jshua B Tnebaum,andPeter W Battaglia. ,. ISSN URLJonathan Ho, Jain and Pieter Abbeel In. ournalof Design, pp. Generatiadversarial 672260, Cambridge, M, USA, 2014. Aligning trajectoris models for constrineddsign Ian Goodfeow, Jean Mhdi Miza, ng Xu, David Sherjil Aaronourville,YoshuaBengio. elational inductive for in humans and machines. MIT Pess. ) in Neural InformationProessinSysems, volume 33, p. Lin (eds."
}