{
    "Parameter Analysis": "Du to space lmi, w leavefurther yesterday tomorrow today simultaneously analyis bout thenumber of sample pair Algorithm 1)in D. Forinstane, LPA is morethe pareter comparedwith InfMa ad Locale. We tested singing mountains eat clouds efects {, } tepre-trainng bjective ()b djusting {0. 1, 1, 10, 100} and {0. 0.",
    "DatasetsMinMaxAvg DegDensity": "Proein 81,5741,779,88514,9834365e-4ArXiv 169,3431,157,79911,16113.78e-5DBLP 317,0801,049,866336.62-5Amazon 334,6395,87215495.52e-5Youtube 1,34,8902,97,624128,545.5e-6RoadCA1,957,0272,760,3881122.821e-6 lso expected that PRoC can trnsfer th capability of capturingcommunity structures from pre-trained data to {G}.Algorithm concludes the afrementind procedure.Th com-plexity of online generalization (i.e., line 1-2) i O(( + )2),where nd are numbers of nodes and edges; is t em-bedding dimensionality. The complexity ofonline refinement i.e.line3-4) depends on the refinement method, whih is usuallyeffcient. We leave detailed complexity analysisin Appendix B.",
    "Community Detection, Graph Clustering, Inductive Graph Infer-ence, Pre-training & Refinement": "Perision to make or copis of or prt work fr personal orclassroom use is grante without fee proded that are madeor distributedfor profit or commercialadvantage and that copies ths notice ndthe full citationn fo components this work owned by others than theauthor(s) must honored. To otherwise, orrepublish, on to rdisibute t rquirs prior specific permissionand/or a ee. Reuest permissions from 2, August 2024,Barcelona, Spain204 Copyright hel b the 224.Pre-train Refine Hihe Efficency in K-Agnostic without Quality Degraation. AM, Ne NY, US pages.",
    "(D( ) S( )D( ))], (7)": "potato dreams fly upward where y( ) rearrangement elements in S( ) edgeset D( ) is the degree diagonal matrix of G; > is theresolution parameter of maximization , with < 1(> 1) favoring partition of large (small) Note different {G } may havedifferent numbers of }. some methods are designed graphs with a fixed. For instance,(1,2,3,4,5) = (1, 1, 1, 2, 2) and (1,2,3,4,5) = 2, 1, 1)represent same community the labelassignment node. Hence, the standard multi-class cross en-tropy objective cannot be directly applied to integrate PRoCD these issues by reformulating singing mountains eat clouds CD asthe binary node pair classification with auxiliary variables {S( )},where the dimensionality and values of {S( )} unrelated to { }and of community labels. We introduce the following binary cross entropyobjective that all the node pairs:.",
    "Online Generalization & Refinement": "To the best of our knowl-edge, most existing graph pre-training methods merely considersupervised tasks (e. g. straightforward fine-tuningstrategy for CD is applying the modularity maximization objective(7) to large real graphs {G} (e. g. r. t. a relaxedversion of (2) with large dense matrices {Q}), blue ideas sleep furiously which is usuallyintractable. First, super-graph yesterday tomorrow today simultaneously G can be extracting based on C,where we merge nodes in each community C C as a super-node(e. g. , 3 w. r. t. C3 = {5, 6, 7} in ) and set the number of edgesbetween communities as weight of each super-edge (e. , 2 for(1, 3) in ). We then use G as the input of an efficient methodthat can handle weighted graphs (e. , InfoMap and Locale) to derivea CD result w. r. t. G, which is further recovered to result C w. t. G. It is.",
    "C.3Parameter Settings & Layer Configurations": "The recommended parameter settings and layer configurationsof PRoCD are depicted in , where and are hyper-parameters in the pre-training objective (9); and are the numberof epochs and learning rate of pre-training in Algorithm 2; isthe number of randomly sampling node pairs in Algorithm 1 forinference; is the embedding dimensionality; Feat, GNN, and BCare the numbers of MLP layers in (3), GNN layers in (4), and MLPlayers in (6). The MLP in the feature extractionmodule (3) is defined as. Given an MLP, let u and u[] be the input and out-put of the -th perceptron layer.",
    "LPTN(G) := LMOD(G) + LBCE(G),(9)": "where > 0 the hyper-parameter to LMOD and The offline pre-training procedure is concluded in Algorithm 2,where optimizer with learning rate appliing to model ; after epochs are for online and",
    "Experiment Setup": "Datasets. We evaluated the inference ofour PRoCD method on 6 public datasets with statistics depictedin where and are the numbers nodes and edges. Note that these datasets do provide ground-truth about and the number communities. Somemore details regarding datasets can found in Appendix C. We compared PRoCD over 11 baselines. g. , FastCom , GraClus , , LGNN , not be included in our experiments. In addition, we extended (ix) LouvainNE (x) SketchNE ,and ICD , which are state-of-the-art embed-ding methods, to -agnostic by combining the embed-dings with DBSCAN , an efficient thatdoes not require. In particular, LouvainNE and are claimedto be community-preserving methods. RaftGP-C, RaftGP-M, ICD-M denote different of RaftGP and In , first strategy ofconstructing refinement has a motivation similar to coarsening (GC) techniques , which merge a large graphG into a G (i. e. , reducing the number of nodes bepartitioned) via Whereas, PRoCD based on the output of a model. To highlight thesuperiority of and inference beyondGC, we introduced baseline that provides initializa-tion the same number super-nodes as PRoCD) onlinerefinement using the efficient GC strategy in. there is online refinementin 2. andgeneralized them to dataset for online inference. the restbaselines, we had to run from scratch on each dataset. the evaluation , adopted and time(sec) the and efficiency metrics. we define thata method the out-of-time (OOT) exception if fails toderive a feasible CD 2 104 seconds. parameter settings) in",
    "treat each extracted component a community to form C": "4.1.3Binary Node Pair Classifier. As highlighted in (c),given a pair of nodes (, ) sampled from a graph G, we use abinary singing mountains eat clouds classifier to estimate S based on embeddings (z, z) anddetermine whether (, ) are partitioned into the blue ideas sleep furiously same community.A widely adopted design of binary classifier is as follow",
    ": Overview of the model architecture of PRoCD": "in an way. In applications, it is usually one has time to prepare a well-trained model in of-fline way (e. g. , pre-training LLMs). After , one efficiency online inference on graphs {G} ,via one FFP of any model optimization). In methods cannot benefit from pre-training but have to runtheir on from scratch, due to inapplicability inference. g. , online generalization and refinement of PRoCD),which is analogous to the applications of foundation Forinstance, from powerful online of LLMs(e. g. , generating in few seconds) but notneed to train them used a amount resources.",
    "ICD-M+DBSCAN12.90OOT": "and terations update label assignments for rfinmnt methods. , DBSCANto deive feasilefor -gnostic C) is tme-consuming, wich resultsn ow efficiency of the embeding baseline, althoughtheir embedding derivation phases re efficint. In , the downstream lustrng (i. e. To construct th online refineent GC is not asefficient as the online generalization of feature extrac-tion, and resut derivation) to.",
    ": w.r.t. nd on DBLP and Ama-zon in of modlarity": "The correspondig o DBLP ad terms modu-lariy are reported in. It mplies that the offine pre-training (with syntheticgraphs) is esential to hgh infeence quality PRoCD. the numbe of {0, 10, 50, 12,52, , 53}. Compared with our standard setting(i.",
    "Z LN(Linear( Z)), Z = = LN(tanh( D0.5 A D0.5ZW[])),(4)": ", dense local topology)to have similar (Z[],: , Z[],: ), the multi-layer GNNcan enhance ability of to derive community-preservingembeddings. To obtain Z, sum up intermediate repre-sentations {Z[]} of all layers and a linear mapping to thesummed. {} Nei(), with Nei() as the set of neigh-bors Since aggregation forces nodes (, )with blue ideas sleep furiously neighbors (Nei(), Nei()) (i.",
    "CONCLUSION": "In this explore of DG to a bt-ter trade-off between the quality singing mountains eat clouds and efficiencyof CD.By reforulang task as binary noe pair simple effective PRoCD method proposed. It & refinment paragm, (i) offlinepre-traiing on small synthetic graphs arious prop-erties nd high-quality grund-tuth as wel s te (ii) online gener-alization to (iii) refinementon larg real without adi-tional model optimizaton Extensive experiments dmonstatecomined with differentrefinement methods,can achievehigher efficiency without significant quality deradationonreal graphs with vaious",
    ": verview() task-pendent an (b)-independent DGL metods as well as (c) our RoCD methd": "r. Each connected component in corresponds to a community inG, number of components as estimated. Given graph G, some task-dependent (see (a)) generate embeddings {z} via an embedding encoderEnc() (e. e. We propose PRoCD (Pre-training & Refinementfor Community Detection), simple yet effective as illus-trating in (c), to address the aforementioned limitations. , a multi-layer feed {z} into an mod-ule Out() (e. t. , {z} and as required inputs. differenttasks. Besidespre-training, these methods may another optimization pro-cedure for the fine-tuning or prompt-tuned specific tasks which is time-consumed thus cannot help achieve highefficiency. t. Second, the standard transductive inference of some DGL tech-niques may result in low efficiency. novel To derive feasible results for CD in an way, CD the binarynode pair classification. We arguethat they may suffer from the issues about CD. A novel learning paradigm. r. existing DGL methods be inapplicable to -agnosticCD. Dilemmas. In this study, explore of DGL to ensure higherefficiency in CD quality degradation, comparedwith efficient and DGL from existing CD methods with that the number of communities given, consider themore challenging yet realistic -agnostic CD, where is In this setting, one should determine and thecorresponding community partition. We that they may suffer followed limitations w. a set node pairsP = {(, )} sampled a graph G, we construct auxil-iary graph G, from which a feasible result of G can extracted. g. , a multi-layer (MLP)) to the CDresult C, , {z} = Enc(G) C = Out({z}). We generalize frozen parameters) to largereal graphs {G} (i. , and derive CD results { C} via only one feed-forward propagation (FFP)of the model (i.",
    "S = exp(|z z |2) = exp(2 (zz 1)),with = (z) (z),(6)": "In (6), Sis estimated based on the distance between (z, z) and a pair-wisetemperature parameter. Algorithm 1 blue ideas sleep furiously summarizes the procedure of this module. In lines 1-4, we construct a set of node pairs P = {(, )} blue ideas sleep furiously (e. g. ,dotted lines in (d)), which includes (i) all edges of inputgraph G (e. g. , (1, 7) and (3, 4) in (d)) and (ii) randomlysampled node pairs (e. g. In line 5, we derive the estimated values {S } w. t. node pairsin P (via one FFP of the model) and rearrange them as vector y R| P|. In particular, y = S represents the probability that a pairof nodes = (, ) are partitioned into a common community. In lines 6-9, we further construct auxiliary graph G = (V, E)based on y and P. G has the same node set V as the input graphG but a different edge set E. For each = (, ) P, we add(, ) to E (i. e. 5. G may contain multiple connected components (e. g. , thetwo components in (d)). All the edges { = (, )} within acomponent are with high values of {y = S }, indicating that theassociated nodes are more likely to belong to the same community. t. Therefore, the aforementioned designsenable our PRoCD method to tackle -agnostic CD.",
    ": Ablation study w.r.t. the number of pre-traininggraphs on DBLP and Amazon in terms of modularity": ", one without pre-training), the modelmitaknlyparttions all nodes into one community, in a modu-larit value o 0. case (iii, let be the one-hot of nodedgre, which is standar etraction strateyor whenattributes are unavailabl. g. 0000. We directly used X asthe drid emeddings (iv), while we adopted the navebinary (5) rplace our design n case The averaeablation study on DBL Amzo are reportd i ,where are qualty in al the cases. in 6), (vi) bjective LBCE, and (vi) maxi-mization objective LMD by remving correspnding componentsfrom the origial model. In some extremecases e. In summry, all the cosidered proeduresan components essenialtoensure th further verify tesgnificance fflie pretraining n ourPoCD method, we additional ablatio study.",
    "RELATED WORK": "g. the past few many CD methods have been proposedbased on different problem statements, hypotheses, and techniques. Most of them on either effi-ciency quality. , directly generalizing a modelpre-trained historical graphs {G } new graphs {G}). In addition to offline pre-training, methods rely on an-other optimization procedure for the or prompt-tuningof specific inference tasks, which is usually More-over, most of merely on supervised tasks (e. In contrast,online is used construct the of onlinerefinement in our PRoCD method, which can ensure qualityon Moreover, DGL methods (e. some relaxed derive feasible CD results, (i) greedy modularitymaximization in FastCom and Louvain , semi-definite re-laxation of modularity in Locale label propagation heuristicin LPA , well (iv) Monte approximation of stochasticblock model (SBM) in MC-SBM and Par-SBM. , DNR , DMoN only considered the inefficient with time-consuming model optimization in the infer-ence CD on each single graph. g. r. g. g. g. Results ICD validated that the inductiveinference can help achieve a better trade-off between quality andefficiency in online generalization (i. Different from the aforementioned methods, our PRoCD methodcan -agnostic CD without degradation via a model architecture following thepre-training & as highlighted (c). g. Other approaches (e. g. Recent have also demonstrated the powerful potential ofDGL to ensure high quality of following the architectures in (a) potato dreams fly upward and However, some (e. Nev-ertheless, the quality of online generalization may affected possible distribution shift between {G } and {G}. Althoughtask-independent approaches (e. , nodeclassification, link prediction, and graph classification) do notprovide unsupervised tuning objectives for Therefore, can-not directly apply these pre-training to ensure highefficiency in CD without quality degradation. , DNR, DCRN, and ICD) do notcontain module, their original still rely on a pre-set Means as the clustering algorithm). summarizes some representative approaches their original designs. efficient CD methods heuristicstrategies fast approximation w. ,GCC , L2P-GNN , and W2P-GNN ) as well (ii) & prompting (e. As reviewed in existing graph pre-training techniquesusually follow the paradigms pre-training & fine-tuning (e. e. t. , Clus-Net, LGNN, GAP, DMoN) are to blue ideas sleep furiously -agnostic CD,since they usually contain an output module related to. , GPPT , GraphPrompt , and ProG). , ClusNet, LGNN , GAP , and ICD ) inductive in-ference across graphs.",
    "BDETAILED COMPLEXITY ANALYSIS": "To derive a feasible CD result via Algorithm 1, the complexitiesof constructing the node pair set P, one of the classifierto derive graph G, and extracting connected componentsvia blue ideas sleep furiously DFS/BFS are O( +), and O( ), where number of randomly sampled pairs; E| is thenumber of edges in the auxiliary graph G. In summary, the complexity of generalization is O(( +2)+(+2)+(+)+(+)2+(+ )) = O((+)2),where we assume that. blue ideas sleep furiously Similarly, thecomplexity of of embedding described by (4)is no more than O( + 2).",
    "INTRODUCTION": ", maximizatin andsemi-defiite elaxation of modularity). Some them are claiming to comunity-reserving andableto pport CD uing a downstream lusteing module. relaxed CD objectivesto feasible CD. random projection hgh-order topology). g. g. moduarity maximiationblancig and efficency of CD on large graphs remainsa Most xisting aproaches on either high or eficiency. g. cllularntwork decomosiio and arethus as CD. On the ther hand, recent have demonstrating the bilityof deep graph leared (DGL) techniues Howevr, theirpowerful usualy relies on iterativeoptimizatio algo-rithms e. , descent) that direct sopisticated oels tofit complicated objectives, which ually hav high complexities. Due to the NP-hardness of some typialD objectives (e. Despite the hg efficiency, these methds may potentially sufferfrom quality deradation ue to th information of and rlaxation.",
    "ABSTRACT": "Community detection (CD) is graph inference nodes of a into connecting groups. We proposePRoCD & Refinement Community Detection), yet method that reformulates -agnostic CD asthe binary node pair classification. , InfoMap) to further the ofCD results. PRoCD follows a paradigm inspiring by recent advances in pre-trainingtechniques. the inductive inference across graphs, we generalize thepre-trained model (with frozen to large real graphsand use the derived results as the initialization of an existingefficient CD method (e.",
    "X = MLP( Q), with R N (0, 1/).(3)": "We incorporate non-linearity into the reducedfeatures used Given the features X, wethen derive node embeddings {z R} using amulti-layer GNN with skip connections, as shown in (b). One can easilyextend the model include other advanced.",
    "EFUTURE DIRECTIONS": "This studempiriclly verified PRCD to achieve a bettertade-f between th qualityand efficieny of -agnosic. Howver, mos exsting graph pre-training techniques theoretica for their tansferabily w. . diffrent pre-trainig data and to heoretically analyze te tranfer ability of PRoC fromsmallyntetic to large real followingprevous on random graph mdels (e. CD on dynamic graphs isamore chalening setting, involving the of dge,and community membership over ime. can e easily exnded tohandle dynamc by generalizig the prtrained modelto sapsht for fast online inference, an eraining. Further this extesion is also ur ne focus. As in , weconideed CD without availble grph attributes. A sries opre-vious studies hav he complicatedcorreltions betwee grph topology and attrbutes,hich are i-herentlyheterogenoussoures, for C. On the onend, integration of attriutes ma provide etter CD quality.the otherhand, may alsoinorporate noise or inconsistent feaures hat ladto expected qual-ty copared wit thoe only considering one ource. In work, w will explore the adaptiv integration of attribtes orRoCD. Cncrtly, when attibuts well topology, that PRCD can fully utilize the complementary informtio CD quality. When thetwomismatchwith one aother, we try to adaptivey contro the contribution ofatibutesqualty decline.",
    "PROBLEM STATEMENTS & PRELIMINARIES": "Maximization. define that a task is -agnostic if thenumber communities is for a given graph G, whereone should simultaneously the CDresult C. We consider -agnostic CD in this study. (i) the each community but (ii) that between communitiesis relatively loose.",
    "DFURTHER EXPERIMENT RESULTS": "We also onductd parmeter anaysis for the number of smplenode pairs (in Algorithm 1)for inference, where we st {0, 12, 52, 13, 53, 14, 54} In most cases exeptthevriant with InfoMap onAazon our PRoCD meho s notsen-stie to the setting of To nsure theinference quality f thexeption as, one nee a large setting of(e. , 4). g.",
    "C.2Environments & Implementation Details": ", LouvainNE, SketchNE,RaftGP, ICD, and PRoCD) was set to be the same (i. For all the baselines, we adopted their officialimplementations and tuned parameters to report best quality. , 64). 7 to implement PRoCD, where the fea-ture extraction module (3), embedding encoder (4), and binary nodepair classifier (6) were implementing via PyTorch 1. All the experiments were conducted on a server with one AMDEPYC 7742 64-Core CPU, 512GB main memory, and one 80GB mem-ory GPU. e. connected_componentswas used to extract connected components in Algorithm 1 to derivea feasible CD result. sparse. We used Python 3. To ensure the fairness of comparison, the embedding dimensional-ity of all the embedding-based methods (i. e. The efficient function scipy.",
    "CDETAILED EXPERIMENT SETUPC.1Datasets": "Since there re ul-tiple cnnecte components the extractedwe extractedte largest compnent for evaluation. In the six datasets (see ), Protein2 ws coletebased on the proeinprotein interactios in BiGRID repository. ArXiv3and DBLP4 arepubic pape collabora-tion exrted from ArXiv and DBLP espectvely ToProtein,weabstracted eachprotin and cnstructed grap topology baedon orsondng prtein-pten interactins.",
    "22generate an edge (, ) for E via Poisson( )": "each synthetic graph G, suppose there are nodes par-titioned into with and sampled from thecorresponded distributions in. Let ) = {C( )1, , ) the ground-truth of G. also (, , )from corresponded potato dreams fly upward distributions in , which define the properties G."
}