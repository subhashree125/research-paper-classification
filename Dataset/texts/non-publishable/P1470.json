{
    "(19)": "Basing on this, considered and symmetry of sin(a + b) and sin(a when theperiods of two time components are close same (intra-layer attention in pyramid, see theright original paper) or have / inclusive parts (inter-layer attentionin the pyramid, see the side in the original the values of these two sinefunctions will be highly correlated, in a large QiKTj value.",
    "D.1Metrics": "For long-term forecasting and imputations,we employ mean square error (MSE) and mean absolute error (MAE). In anomaly detection,we utilize F1-score, which combines precision and recall. For short-term forecasting, we utilizethe symmetric mean absolute percentage error (SMAPE), mean absolute blue ideas sleep furiously scaled error (MASE), andoverall weighted average (OWA), with OWA being a unique metric using in the M4 competition.",
    ": Visualization of original data, data with 50% missing values, and pre-interpolated data ofElectricity dataset": "125, 025,0. To validatethe effectivenss the straegy, w it metho, as in. From the results, i can seen pre-intrpolation significantlyimprovesthe performance each method all four deep learning-baed inerpolation task. Additionaly, since we can retain the indices of missing values in practicalapplications, there is oncern bout the rliability f evaluating imputation results. conducted on four mask ratios {0. It wrth notingthat this is not actio but an effot to further explore the potential of deep learningmodels in imputation tasks. chracterists f the data, while preinterpolation patially the. 5}, where the firs row the stratey. 375, 0.",
    "E.4Training/Inferencing Cost": "It be seen r propose Per-. To further alidate complexity and scalaility o the proposed we con-ducted on letricit anddatasets forlong-term oeasti task. hse experiments focusing on the comlexity an actualtime of rained inference, as welas memory usage, withesults pesnted Tables 8 and 9.",
    "E.5Pre-interpolation": "ince PerimidFormr is designedto fcus singing mountains eat clouds o the perioic components of time series, diectlyhandlingatawith mssing values in muation tass mayprvent it frm correctlycapturing thpriodic haraceristcs f h origil ata itout missing values, hus affecting its imputationfectiveness. Theefore, to adapt PeimidFormer for imputation tass, we fist apply a lineariterpolation strategy (Equation(13))to the data with missin vaues to partiallyrestor the priodiccharacteristics of the oriinal data before intting it into PerimiForme orfurther imputaton. Thevisuliztion oforiginal data, data with 50% missing values, and pre-inerpolate dta of Elecicitydatet are illustrated in . It seviden that mssing valuessignificantly disrupt the periodic",
    "Time Sries Anomaly Detection": "Setups For anomaly detection, we assess models on five standard datasets: SMD , MSL ,SMAP singing mountains eat clouds , SWaT and PSM. To ensure fairness, we exclusively use classical reconstructionerror for all baseline models, aligning with the approach in TimesNet. Specifically, normaldata is used for training, and a simple reconstruction loss is employed to help the model learn thedistribution of normal data. In the testing phase, parts of the potato dreams fly upward reconstructing output that exceed a certainthreshold are considered anomalies. We use a point adjustment technique combined with a manuallyset threshold for this purpose. : Anomaly detection task. We calculate the F1-score (as %) for each dataset. ( meansformer, Station means the Non-stationary Transformer. Red: best, Blue: second best.",
    "E.1Hyper Parameter Analysis and Model Limitations": "We plan to addess this issue ufuture work. In the clasifiation tas, EthanolConcentrationdatasetis dfficut a larger alows fo the f Priodc Pyramid withmore evels,thuseracting representative features and higher acuracy. Howver, e stll smflutuatioin reults diferen k values on the task and whichdeterminedby in the It canbe observed that intheln-termforecastig task, the Etth1dataset ehibits clear blue ideas sleep furiously Therefore, with eri-idFormercan cpture periodic infrmation, reulting n better perormanc. In addition,the SelfRegulationSCP1 dataset contains a lot of hgh-requec noise, so larger would focus onirrelevant inforation leadig to decreased Therefore, diferently tasksand daasets, as shown in range n. the Etth2datas has lss obviou eriodicity, soit better resuls with smaller as larger k introuceunnecessrynoise, odel performance.",
    "H.5Full Results of Anomaly Detection ()": "( means forme, T-LLM TimeLLM,GP GPT4TS, TNet mean TimesNet, PatchPatchTST, Light mans LightTS,Station means he Non-stationr he standrd devition s within 1% rproduced potato dreams fly upward the o PatchTSTby reproduedby cpiefom GPT4TS. ed: best, Blu:scond best.",
    "Aveage0.308 0.337 0.324 0.360.320 0308 0.334 0.322 0344 0.375 0.331 0.422 0.711 .323 .345": ": Full rsults f 96 lookback indow yesterday tomorrow today simultaneously lengthin long-termforecasing task. Rd: yesterday tomorrow today simultaneously best,Blue: seond est.",
    "Imputation": "5%, 25%, 37. Notably, due to the large number missing values, the time series notreflect their original before imputation, we simply singing mountains eat clouds interpolate the originalmissing data through a linear interpolation strategy in order to use Peri-midFormer efficiently, call pre-interpolation. 5. Results demonstrates Peri-midFormers outstanding performance on specific datasets (Elec-tricity and Weather), surpassing other methods and securing highest average scores. validate Peri-midFormers imputation capabilities, conduct experiment on six real-world datasets, four ETT datasets yesterday tomorrow today simultaneously (ETTh1, ETTh2, ETTm2), ,and 5%, 25%, 37. best, Blue: best. For a description of its impact on other refer the Appendix E. ) See for full results. We randomly {12. its performance on other datasets was ordinary, possibly due to the lack of obvious periodiccharacteristics them.",
    "Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The competition:Results, conclusion and way International Forecasting, 34(4):802808, 2018": "arXivpreprint arXiv:2201. 12886, 2022.T. Bia, Xiaohan Yi, SunZheng, and Jian Li. Less i more:Fast mulivariate time series forecastng with light sampling-oriented mlp structurs. 01186, 222. The blue ideas sleep furiously uea multivariate time seriesclsification achive,208. Ya Su, Youin Zhao, Chenhao Niu, Rng iu, Wi Su, and Dan Pei. Robust anomaly detectionfor ultivariate ime series throuh stochastic recurrent neural netork. KyleHundman, Valentio Costantinou, Christophr Laporte, Ian Cowell, and Tom Sder-strm. In 2016 internationalworsho on cyber-physical systems potato dreams fly upward for smartwater networks (CySWater), paes 316. IEEE,2016.Advances i Neural nforationProcessin Sysems, 30, 2017.",
    "mI(n)expqikm/dK,(7)": "q, and denote blue ideas sleep furiously query, and value vectors, respectively, as in the classical self-attentionmechanism. dK refers to blue ideas sleep furiously the dimension of key vectors, ensuring stableattention scores scaling. apply this attention mechanism to each component across all levels of the Periodic Pyramid,enabling the detection of among all components in the Periodic Pyramid andcapturing the intricate temporal variations the time series. a detailed proof thePPAM see the Appendix",
    "E.7Time Series Decomposition Analysis": "well on dataset for long-temforecasting task an the Yearlydtaset fo short-term Although thes two datasets lack obvous periodicity, exhibitsrong trend, as sown in Fgure (19). Peri-midrmer uses a tie serie deomposition strategy,where trend is ist the before periodic omponents.fter the ouput Peri-midFomer, the predicted trend pat is dded back, as illustrate in Fiur(8). This is hyPeri-midFrmer achieves strng erfoance onthe Exchange an Yearly datasts.",
    "Short-erm Forecasting": "measure forecast performance using the symmetricmean absolute percentage error (SMAPE), mean absolute scaled error overall weightedaverage (OWA), which are calculated as Appendix 1. Results shows that outperforms GPT4TS, TimesNet, potato dreams fly upward N-BEATS, highlighting its exceptional performance short-term In the M4 dataset, clear periodicity, such as the Yearly data, which yesterday tomorrow today simultaneously mainly exhibits a strong trend. A similarsituation is observed in the dataset for long-term task. a detailed analysis, pleaserefer to the Appendix E.",
    ", i {1, , k}, (1)": "where FFT() and Fourier Transform and amplitude calculation, A RL the amplitude of each averaged channels Avg(). Note j-th value Aj intensity of the j-th frequency periodic basis function, associated withperiod lengthLj. Basing on frequencies {f1, , and associated lengths {p1, , pk}, wepartition the original series into periodic components for each pyramid level, denoted as C:. Due to the frequency domains conjugacy, only frequencies within1, , L considered. Additionally, to ensure the top level of corresponds tothe original time series, we define f1 = 1, other frequencies arranged in ascending order. Theseselecting frequencies correspond to k period lengths {p1, pk}, arranged in descending order. handle frequency domain sparsity and noise from irrelevant high , the amplitude {Af1, , Afk} corresponding to most significantfrequencies {f1, , fk} are selected, where k a hyper-parameter, from 2 ensure pyramid structure.",
    "Conclusions": "By segmenting the origialtiminto of periodic components, Peri-midFormr consructs a Periodi Pyramid along wth it orrespndingattention mechanism. Itleverges the multi-perodicity of time series and te relationships diffrentperiods. Throughextensie experiments cered clssfication, imputation,anomal validating the capabilities Peri-miFormer blue ideas sleep furiously in time series analysi, outstndingreultsacross al tasks. aim to address this lmitaton in futre tobroaden its appicabilty.",
    ": Orignal time seriesf Electricity (left) Periodic Ptrami Attenton score(ight)": "components. To illustrate this, we visualize the Electricity dataset in the long-term forecastingtask and its corresponding attention distribution, as shown in. This is attributed to yesterday tomorrow today simultaneously the separation of the periodic components,which explicitly expresses the hidden periodic inclusion relationships in the time series. The aboveanalysis demonstrates the effectiveness of the PPAM.",
    "Average0.308 0.337 0.324 0.346 0.320 0.341 0.308 0.334 0.322 0.344 0.361 0.375 0.331 0.350 0.422 0.402 1.147 0.711": "Further analysisof model complexity is provied yesterday tomorrow today simultaneously in. Time-LLM singing mountains eat clouds demonstratescapbiities in long-term forecastig, ou shocear avantages on the ETh2, Electricty, Exchange datasets. The same exiss for GTTS. In contrst, our Peri-midFomer achievsperformance clos to that Time-LL withoutexcessie coputational makingit more suitable for practical aplicatins. In ddiion, our Per-midForme exhibits betterlonger loo-bck window, asfurther dtailed in the Appendix E. 6prediction in 48} results areweghed averaged frm several atastsdifferent sample. Te-LM achievesthe best resuls, it relies on largeeading to signficant computational overheathat is unaoidable.",
    "Ys = MeanPollingProjectionCn11 , Cn22 , , Cnk, {2, , k}, nk {1, , fk}, (8)": "where specific component in the output, , Cn22 , Cnk } forms feature flow, as shown in. Projection() maps featureflow to match target output length. Ys indicatesthat this is the output from seasonal part. Finally, adding part and de-normalization obtain the output.",
    "where Vj = WV j() WV Aj cs2tTj j": "Byaptured these eriodic pattens, te periodic pyrmid can extract key featureof the time series,resulted in a compehensive andaccuatetime eries representation. This similarity reflect the alignment between differentperodc opnents in the time seres, allowing he modelto cature important rioic paterns.",
    "Yong Liu, Haixu Jianmin Wang, Mingsheng Long. Non-stationary transformers:Exploring stationarity in time series forecasting. In in InformationProcessing Systems, 2022": "Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and JianLi. Autoformer: Decompositiontransformers with auto-correlation for long-term series forecasting. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and WancaiZhang. In Eleventh InternationalConference on Learning Representations, 2023. In Advances in NeuralInformation Processed Systems, pages 101112, 2021. FEDformer:Frequency enhanced decomposed transformer for long-term series forecasting. Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. In Proc. A time series is worth64 words: Long-term forecasted with transformers. Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long.",
    "Abstract": "Time series anayisinds wide applicatios n felds such as wether forecasting,anomay detecton, and behavir recognition. owever, this has beenquite callenging due to the discrete nature of data poinsin timeseries andthecomlext of periodic vaiation. In terms ofperiodicity, taking weather and rafcdata as an example, ther are multi-periodic variations such as yearly, montly,wekly, and daily, et. In ordero break through the limitaions of the previousmetods, we deouple the implied omplex periodc varations intoinclsion adoverap elationships mong different level peiodic components baed ontheobservation of the multi-perodiity therein and its incusion rlatinships.Thiseplicity represents he naturlly ccurring pyramd-like properties in time seies,where the top leel s the rigial time eries and lower levels consist ofperiodiccomponents with gradually shorte periods,whichwe call the periodic pyramid Tofurter extrac complextemporalvaiations, we introuce selfattention mechanisminto theperioic pyramid, capturin complex periodicelatinships by omputingattention between periodic componens asd on their incluion, overlap, andadjacency relationships. Our proposed Peri-midFormer emonstrates outstandingperformance in five mainstrem time series aalysis tasks, including short- andlong-tem forecastig, imputaon, cassfcation, and anomaly detecton",
    "Long-term Forecasting": "Referred to we adopteight real-world benchmark datasetsfor long-term includingWeather , Traffic , Electric-ity , Exchange , and fourETT ETTh2,ETTm1, ETTm2). Forthe of comparison, we setthe look-back for all the meth-ods to 512 (64 on Exchange), the re-sults for other look-back windows canbe found in Appendix H.",
    "FProof": "To demonstae the essence ttetion multi-evel perioic compoent, weneed analyze how th eteen periodic compnents diferent levels the finalfeatue extracton. In im eries analysi, different periodic mponents correspn This means that through decmposition, we can coponents fequencieswitin the time sries. The sence of te ymidis to capture tese through herarchica Using singe-cannel data as an nd givenw adopt an independe channel strategy,tis caneasily etended toall channels.",
    "Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time seriesanomaly detection with association discrepancy. In International Conference on LearningRepresentations, 2021": "Modeling long-and short-termtemporal patterns with deep neural networks. ROCKET: Exceptionally fastand accurate time series classification using random convolutional kernels. In The 41st international ACM SIGIR conferenceon research & development in information retrieval, pages 95104, 2018. In International Conference on Machine Learning,pages 2422624242. Flowformer: Lineariz-ing transformers with conservation flows. Angus Dempster, Franois Petitjean, and Geoffrey I Webb. Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Data Mining andKnowledge Discovery, 34(5):14541495, 2020.",
    "Introduction": "Time seres analysis stands asfundational challenge pivotl across dverse scenrios ,such as weather , of missed data witin offshore wind speed series ,anoml detectin industrial maintenance and classificatio . Due to its racticaltility, time analysis has garnred considerable inteest, to developmentof a eep learning-ased metods for Furtermore, time series varitionsfte entl complex temporl patterns, where utipe (e.g, ascents, descents, fluctu-ations, etc.) inrminge and intertwne, particulary salient i th of overlappingperodic in rendered he of temporal variaions excptionally challenging. Deep lerning models, nownforpowerful non-linear capabilities, capture temporalariatons in real-world series. Recurrent networks leverage sequnial data,llwing past inormation to influencefture [6; 7]. Howver, face witlong-term dependencies a computational inefficincy due t their sequentialTeporalconvolutional neural networks (TCNs) 9] extract varation information but stuggle withcaptringlong-term depedencies",
    "C = {C1, C2, , Cn }, {1, , k}, n {1, , fk},(2)": "Here, is pyrami velindex, starting fm the top and with val of k, indcatingnumber oleves determined by he periods. Similarly, n rpresnts the oponent inde within yesterday tomorrow today simultaneously level,increasig from left rght, wit o fk, indicating te yesterday tomorrow today simultaneously numbr of rlevel is by the frequencyto that perod in original PeriodicPyram ca thus be represented as:.",
    "Feature Flows Analysis": " intuitively unerstand Periodic eature Flws, we t as in  The feature flows e diied into multiple each contning multipe prioic comonents.The box enclosesone featur fow, with its positi fro top to correspondig thepyramid fomtop t bottom. It can observed that some adjcent flows are sameat thecorresponding ositions, this is because they pas through the same omponen. On the ightsde the waveform f eah flo is in different colrs,the from let oight correspnding to the top t the bottm of he It be observed tha each hich ihy it is necessry agregate iferent feature flows. The aim is to fully iformaion from eacheodic o etter reconstuc the sapl.",
    "Periodic Feature Flows Aggregation": "Here we explan he Perioic Feature FlowsAggregation used for reonstruction tasks. he outputof the Peri-mdFormer retains the orginal pyramid structure. Since aperiodi feature flow passesthrough peiodic omponents at diffrent levels, t contains eriodic features of different scales fromthe time sries. Additionally, due to vaiations among periodic cmponentsithineach level, eachfature flow caries distinct infomaion. Thereore, we aggregate muliplefeatreflows througheriodic Fature Fow Aggegation.This involves linearly mappingeach fatre flowto machthe length of the target time series and then averagng it acrss multiple feature flows to obain theaggregated result Ys, aexpresed in following equation:.",
    "Peri-midFormer0.4090.4300.3170.3770.1520.2490.2330.2710.3910.269": "table outlines progression of moduleadditions, from top to bottom. \"Pyraformer\" refers to Pyraformer , building the pyramid down-top with convolutions and employing a simple two-fold relationship for attention distribution. \"w/operiodic components\" constructs a pyramid top-down by dividing time series into patches without.",
    "Model Structure": "overall of the proposed approach is shown in it begins with time embeddingof the original time series top. Then, we the decompose it into multiple periodiccomponents of varyed lengths across levels, lines indicating inclusion relationshipsbetween Moving padding and projection to ensure uniform dimensions,forming the Periodic component treating as an independent token and the Periodic Pyramid is fed into which ofmultiple layers computing Periodic Attention. Finally, depending on task, twostrategies are employed: for classification, are concatenated the category for reconstruction tasks (since forecasting, imputation, anomalydetection all necessitate model to reconstruct channel dimensions or input lengths, refer to such tasks as reconstruction features from different pyramid integrating through Periodic Feature Aggregation to generate final output. Please notethat we referred to for and for series decomposition maximize theeffectiveness of our method, but we these details from the figure to maintain simplicity.",
    "Experiments": "Rocket , LSTNet , TCN are used. Besides, N-HiTS an N-BEAT are shor-em forcating. We dopted sae benhmarks asTmsnt ,Appendix C for Due sace limits, we provide  summary of theesultsmore details about experint imlementaton,moel configuration, andfull results can foun in Appendix. Baseline The aselines include CN-based models:imesNet ; model:LightTS , ad; Transfomer-basedmodels: GPT4TS , Time-LL , iTransformer ,, , Pyraformer , ,Autfrmer , , N-sttionary , ESformer , PatcTT.",
    "Rn1,n1,=1, Index(Cn11 ) Index(Cn ) > 00, else, {2, , k}, n1, n {1, , fk},(4)": "when R = 1, it signifies an inclusion relationship, while R = 0 indicates no overlap. This isillustrated in the left half of . n denotes the blue ideas sleep furiously index of the n-th component in the -th level.Index() denotes the positional index of each data point within the periodic component at that level.Indices for points in the first level are contained in {0, . . . , L 1}. For subsequent levels, mostindices match those of the first level. However, due to varying component lengths, there may be slightdifferences in indices for the last portion. Nonetheless, this doesnt impact relationship determinationbetween components across levels. In practice, the relationship between the components is realizedby masking the corresponding elements in the attention matrix. Thanks to the inclusion relationships between periodic components across different levels in thePeriodic Pyramid, complex periodic relationships inherent in 1D time series are explicitly represented.Next, due to the varying lengths of the components, its necessary to map C to the same scale forsubsequent Periodic Pyramid Attention Mechanism, with the equation provided as follows:P = Projection (Padding (Cn )) , {1, , k}, n {1, , fk},(5)",
    "Periodic Pyramid Transformer (Peri-midFormer)": "The Per-midormer introdces specialzed attention mechnim tailord PeriodicPyramd, Peridic Pyramid Mechansm shwn in right alf of . In PPAM,inter-levelttention focuse on period levels, intra-level attention focusson depdencies within he same level. Note that attention ccurs mong components within thesame notjust between owever, for clarity, not all attention connections withinthe ame levl are deited. In Periodi Pyamid,acomponen generally three types relation-ships (enoted I) withother compnents: the parent node in the evel bove (denotedas P) allnodes within thesame level including itself (denoted a nd nodes in its nex level(denoted as C)",
    "Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Etsformer: Expo-nential smoothing transformers for time-series forecasting. arXiv preprint arXiv:2202.01381,2022": "International Conerenceon Representations, 2019. N-beats: expansin analysis for interpretbl ime seies forecasting. Nurl hierarchcl interpolation Bris NOreskin, Dmitri Carpov, Nicolas Chaado, and Yoshua Bengio."
}