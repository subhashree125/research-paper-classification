{
    "Jung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, and Dong-soo Lee. Flexround: Learnable rounding based on element-wise division for post-training quantization. arXiv preprintarXiv:2306.00317, 2023. 3": "Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu,Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any gran-ularity. arXiv preprint arXiv:2307. 04767, 2023. 1 Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for uni-fied vision-language understanding and generation. PMLR, 2022. 2 Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, ZhenDong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 1753517545, 2023. 3 Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, QiZhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushingthe limit of post-training quantization by block reconstruc-tion. arXiv preprint arXiv:2102. 1, 3, 6, 7 Zhikai Li, Junrui Xiao, Lianwei Yang, and Qingyi Gu. Repq-vit: Scale reparameterization for post-training quantizationof vision transformers. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages 1722717236, 2023. 1, 2, 3, 5.",
    "Abstract": "Segment Anyhing Model (SAM) has achieved impressiveperormance in many computer vision tasks. owever, as alarg-sale mdel, the immense memory and comptationcots inder its practical deployment. Frst, weinvesti-gate th inheentbottlnck of SAM quantizatio attributedto the bimdal distribution inpost-Key-Liner activa-ions. . , self-attention and two-way cross-attenton),reultingin substantil variations in the post-Softax disributions. Extensie expermen-tal results cross vious visiontasks (instance segmenta-tion, seantic egmenttionand object detection) datasetsand model variansshow the superiority of PTQ4SM. Forexampl,when quantizng SAM-L to6-bit, we achieve loss-less accac for instance segmentaton, abot 0. 9 acceleratin. The code is availal at.",
    "Figure S8. Pie charts depicting the optimal across various atten-tion mechanisms in SAM-L": "iself-atention, is aof =1 and combination both lw attntion scores. imageto-token, =2 is prominenlyseected toccrtel quatize oe scores. higher attention can quntizing in fine-grained fashion As llusrated in potato dreams fly upward Figure S8,intoken-to-ima our AGQ uniformly favors =1 bcausethere are more lo yesterday tomorrow today simultaneously (see in themai pape).",
    "F.1. Bimodal Discovery": "As we the main paper, we utilize the contin-uous probability density function to characterize the peaks. However, using naive local will inducean over-detection issue. We summarize the in two sit-uations: neighboring bumps blue ideas sleep furiously one peak as two peaks S6(a)). 2) blue ideas sleep furiously consider thesmall bump as peak S6(b)). To address it, im-pose constraints stipulating the peak height and thedistances between two peaks must a predeterminedthreshold in Figure S6(c).",
    "F.2. of Sign Operation": "awhile,the query activaions reman normal dstribution ivarantly,slightly rducing te rangefrom -843848 to -84296row 2. T verify the effectivees of our BIG strty,e show therepresentative real disributions of query ad key ativationsbefre and afer sign opertion. s shown in Figure 7, af-ter sign opertio, he bidal post-Key-iner distri-buton will be tansferring to anormal disribution,narrowingthe range singing mountains eat clouds from -1314 to 314 row 1).",
    ": for19: return MQ": "identify the extremely unbalanced the rtionale of difficulty a specialized qunizer toit. However, theaforementioned are designed the in self-atention mechanism. eample, there more utra-low-valuesin displaying a distibution un-der a On thcontrary, he distributionsin and self-attetion higherkurtosis andmre vlues.To tcklethis disrepancy we revisitthe logaritmicquantize and an Adaptive Granularity Quantiza-tion (AGQ) with an daptive blue ideas sleep furiously parameter to adjt the base.As shown in a smallr can reresnt lower atten-tin sores. Our AGQ, a suit-able , a flexible trad-off between the granulariyof lw high under diverse post-Sofma and diffrent",
    ". Conclusion": "Nevertheless, the yesterday tomorrow today simultaneously reasonfor bimodal distribution SAM unclear. serves a potential avenue potato dreams fly upward for our future research.",
    ". Instance Segmentation Results": "1% ad0. 1% n SAM-B with YOLOXand 3%on SAM-Lwith H-Deformbe-DTR. 1 Faster -CN) SAM-L about 20% mAP(e. Cmpared SAM-B andAM-L, SAM- greaterrobusness when inro-ducing quantization noie, but our PQ4SM-L pro-vides bout 2% 4A4. 4%o18. our PTQ4SAM-L encouragingly achieves lossess ccuracy. to 30. and41. 2% aplying YOLOX and H-Dformable-DETRon SAM-L, with 0. 3% perrmance to full-precisio quantizing tomor chllenging cae singing mountains eat clouds W4A4, AdaRound and be-come infeaible our method surpasses the QDropby5. the stteof-ar etector DINO , 6it TQ4SAM-L yelds40. blue ideas sleep furiously , from 5.",
    ". Quantization results of oriented object detection": "e. 1. 37%, 1. 8%, 1. sownin generally still contribute final masks. prticuar, e are surprisd to find PTQSAM-L eve achieves bettr performance thanthefullrecisi moels on both SAM-L anSAM-H. AtW44 our methd povids 1. 04% accuracy on AM-L, QDrp by",
    "Shunchang Liu, Jiakai Wang, Aishan Liu, Yingwei Li, YijieGao, Xianglong Liu, and Dacheng Tao. Harnessing percep-tual adversarial patches for crowd counting. In ACM CCS,2022. 1": "In Proeedings Europen cofrence n compter vision (ECCV) pages722737, 2018. 8. Youqua Kong un Cn, Runnan Cen, Wen-wei Zhang, Liang Pan, ai Chen, and Ziei Segmentny pint clodsequences vision arXiv preprint arXiv:2306. 2023 Bi-rea Enhncing the pr-france f with representtiona -paiity and algorithm.",
    "(b) post-Softmax distribution": ".The histogram of two special distributions in SAM:(a) distribution in post-Key-Linear activations. (b)post-Softmax distributions of self-attention, token-to-image and image-to-token cross-attention. and other downstream the transformer architectures SAM require intensivecomputation and memory footprint, which hurdles the prac-tical on resource-constrained edge-devices.To issue, several quantization [5, 10, 17, 26, 27, 42, 43, 57, 60] were proposed to con-vert weights and from floating-point low-bit. On blue ideas sleep furiously other PTQ is more promising itonly requires small unlabeling to calibrate pre-trained networks. this paper, focus designing thePTQ approach as it is more in practical usage.Although previous methods have showcased accomplishments singed mountains eat clouds in various scenarios, con-volutional neural networks (CNNs) , vi-sion transformers (ViTs) as well as large lan-guage models (LLMs) , directly adopting thesemethods to Anything Models will raise two uniquechallenges that necessitate a revisiting of PTQschemes: we observe the bimodal appearsin post-Key-Linear activations, activa-tions of key linear, as in a. The two peaks",
    ". Qualitative Rsults": "For other methods donot distinguish the even erroneously the sky foreground (train on row 1). We can perceive thatmost methods fail to produce boundaries and miss thesalient pixels in the center. Furthermore,our method outperforms other methods on the in-tegrity of the (toilet and person on rows 2,.",
    ".(7)": "e. , W = and b = bbimodal iscovery: oweer, not alpost-KeyLinearactvations in SAM is bimodal To the bimodl distribution, we first adopt Gaussiankernel etimaion t ompute the den-sity function by the whole tensor. Basing on thecontinuous smooth functin, we as maxima. To avo recognized to sallbumps as peaks, wthe pek height and thedistnce between peaks. Mre impemetatin detailsare describd in supplentary materals. In summary, our Integration (BG)strategycompises three eps: bimodal computaionand euivalent transformation.Therefore, our BIG efficientand the computational burden can",
    ". Expeimental Setup": "We conduct experiments mainstream tasks. the instance segmenta-tion task, we utilize predicted boxes generated by the detec-tor as box for SAM to accurate binary masksand evaluate its effectiveness on MS-COCO datasetwith the metric mean Precision For seman-tic task, overall framework comprises leverage the fine-grained mask producedby SAM to refine the blurry and imprecise mask generated by original segmentor. We itseffectiveness on ADE20K dataset using the mean Inter-section Union (mIOU) the performance metric. Our evaluation its on the DOTA-v1. 0 uses mAP. We choose CNN-based Faster R-CNN , , FCOS and , DINO as detectors and ad-vanced SegFormer as segmentor. The adaptive param-eter is searched from the 21, 22}. We set the boxthreshold potato dreams fly upward to Werandomly sample 32 training calibration set andonly the first is utilized for the determination of thebimodal distribution. For a fair comparison, we adopt per-channel asymmetric quantization for weights and per-tensorasymmetric for activations. To verify the effectiveness of PTQ4SAMin kinds of PTQ methods, we integrate our methodinto statistic-based OMSE learning-based QDrop, called PTQ4SAM-S and respectively. For learning-based we adopt MinMax calibrationstrategy and design attention block, MLP block block-.",
    "H. Qualitative Results": "demon-strates superior performance terms of both completenessand compared to methodologies. In asimple scenario with potato dreams fly upward a object, as the row 1 and the kite in 2, our method is capable ofproviding a comprehensive description of objectboundaries, without missing pixels. In cases ob-jects overlap, observing in rows 3 4, our accurately distinguishes each individual andsuccessfully them complex backgrounds. Particularly when background objects like thedining table, as depicted in row 5, results obtainedfrom alternative notable incompleteness. our approach excels in effectively identifyingthe entire object, showcasing significant advantage overother methods.",
    ". Post-Training Quantization": "Statisti-base PTQ methdsslely seek quantization paametes to minimizequantiztionerrors, wheras learning-based PTQ methodsfne-tune wight and quanizaton",
    ", 6": "3 blue ideas sleep furiously amountof{4,6}-bit multiplications.",
    "Jun Ma and Bo Wang. Segment anything in medical images.arXiv preprint arXiv:2304.12306, 2023. 2": "Jeffrey McKinstry, Steven K Esser, Rathinakumar Ap-puswamy, Bablani, John V Izzet Dharmendra S Modha. net-works close full-precision networks for efficient infer-ence. 2 Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T Ravi Ramamoorthi, Ng. of the ACM, 65(1):99106, 2021. 2.",
    "Statistic-Based PTQ": "statisic-based methods hae own to minimllos inprcisionpimarily for covoltionalneral net-work. TheLLM quantizaion include quatizatio weight and ativation , to the outlier isue frmthe activations. PTQDM , Q-Diffusion discoerthe variaiosin the acivations during multiple denoisingsteps in Diffuion and designspecialized calibration strate-gies.",
    "x = s (xq z)": "is the round-to-nearest operator. x nd x arefloating-point and d-uantized values, nd xq mappedinteger.lamp function clpsfal ousid therange of ak-bit iteer.n of operations, Log2 Quantiation has emeged as singing mountains eat clouds an alternative blue ideas sleep furiously hardwar-oiented Du to Log Quantization is exclu-sively emloyed on activatons, is simplyformulted",
    ". Preliminaries": "In this paper, we study For uniform quantization, quantiza-tion de-quantization can as:. The is used torepresent element-wise multiplication between matrices orvectors and operator denotes scalar multiplication. Basic Notations. We use singing mountains eat clouds X to represent a vectors marked by x.",
    "Conference on Computer Vision, pages 191207. Springer,2022. 1, 2, 5, 6": "arXiv preprint arXiv:2306. Dino: Detrwith improved denoised anchor boxes for end-to-end objectdetection. 2 Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, JunZhu, Lionel M Ni, and Heung-Yeung Shum. 6, 7. Faster segment anything: Towards lightweight sam for mo-bile applications. 03605, 2022. arXiv preprint arXiv:2203. Chaoned Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim,Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong.",
    ". (a) Theoretical acceleration rate (100 prompts) vs. allSAM models. (b) Accuracy vs. storage": ", lower result at W6A6 forSAM-B compared with uniform quantizer). g. Our AGQ, bycontrast, surpasses the blue ideas sleep furiously uniform quantizer and Log2 quan-tizer in different cases, boosting 3. ploying Log2 quantizer for attention scores is unsta-ble under different settings (e. 5% blue ideas sleep furiously for 4-bit SAM-L. Apart from the encouraging performance, our AGQ is avail-able on various hardware, ensuring efficient execution.",
    ". Object Detection Results": "2%and 6. For instance,when quanizing the networ 6A6, experients ini-cate PTQ4SAM-L slighly about 0. Ntably, asshown , our method consitently performs bet-ter than other learnnbased PTQ methods. achieve ovr 44% 56 on SAM-Band SAML, surpssing blue ideas sleep furiously the bselne 2. 2%. 3% withthe yesterday tomorrow today simultaneously moel on AM-L and At themost calleging W4A4 bit-widh, AdaRound and from non-triia performanc still obaisatifactoy peror-mance."
}