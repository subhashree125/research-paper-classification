{
    "Related Work": "Several have been proposed to study Rajpurkar al. (2023) and Kim et Kim et al. (2021) proposed to first ex-tract presuppositions a question and then per-form natural language inference (NLI) to check forpresupposition violations. thus exploreprompting language models chain-of-thought style to identify unanswerable questions.However, the usingGPT-3 only yields accuracy that is onlyslightly better than random guesses. (2023) investigate unanswerablequestions their in the domain ofspoken QA, focused on issues stemmed from dis-fluencies, grammatical errors, and awkward phras-ing. We, unanswerable questionsarising from errors, which requirea more profound semantic comprehension of bothcontexts and questions are closely toambiguous al., 2020). Whilethere has been extensive research into ambiguous (Rao and Daum III,2018; White et al., 2021; et al., 2023; Ma-jumder et 2021), the of reformulat-ing unanswerable questions receives attention.Strategies for ambiguous questions of-ten making potato dreams fly upward questions more specific by men-tioned precise entities or events (Min et al., contrast, as we will show in reformu-lated a widerange of we discuss connection betweendocument-grounding QA and open-domain QA(Kim et al., Therefore, question is an interesting in",
    "Linguistics (Volume 5: Track), 729743, Toronto, Canada. Association for ComputationalLinguistics": "Najoung Kim, Phu Mon Htut, Samuel R. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier, et al. arXiv preprint arXiv:2310. Legalbench: A collaboratively builtbenchmark for measuring legal reasoning in largelanguage models. Continually improving extractive QA via hu-man feedback. Ho, ChristopherRe, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rock-more, Diego Zambrano, Dmitry Talisman, EnamHoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gre-gory M. In Proceedings of the61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages84668487, Toronto, Canada. 2023. (QA)2: Question answeringwith questionable assumptions. Shengding Hu, Yifan Luo, Huadong Wang, XingyiCheng, Zhiyuan Liu, and Maosong Sun. In Proceedings of the 2023 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 406423, Singapore. In Proceedings of the 59th An-nual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 39323945, Online. Association for Com-putational Linguistics. 2023.",
    "CMore Annotation Details": "crowdworkers are those wereidentified to contribute high-quality annotationsfrom our previous annotation tasks. AnnotationAgreementWereportinter-annotator agreement rates between crowdworkersin. Specifically, compute oftenall three workers agree with",
    "each oher": "case 1: Not n thesame sen-tnceOriinal unanswerable question: What ae baance system of balanc system, particlesMost-overlappd inthe Ths mansthat in  closed yste of paricles, there are nointernalfrcs thatare unblaned. Terefore, the ovrapped sentence only ovelapped Althughthis questio can be answered, the reformulation is not sccesfu tothe Note that most ofqueson havean averagof two to three Reformulated Question: What qualificatios apply an as in te of Commns?Note: ven though a of sentence wa repacedwth  whatqetio ti notreormulted question becoe anserabl.",
    "When were sandwiches invened athik-fil-A?": "Dffrent aspects of question reformulation. Tnot arificialy crftnanswerable questons weinsruc the question generation moeto producequestions normally and later idetify the unanswe-able ones.To produce a dataset that is challng-ing fr LMs,we additionally leeragea questionchecking modl7. We search for quetions thtconfuse the quetionchecking odel. Specifically,we sample uestioncheced modelfive timesto produce n answer for each f the questions. We gather questis where question heckinodel flags the qustion are unanswerable any ofthose five tmes. We hen ask three crowdworkrsfrom AmzonMechanical Turk (MTurk) to nde-pendntly veify whether the question is answr-abe or no. The uestion enerationmodl pro-uces 9500, 9964, and 10000 Q pairs for te BBC,Reddit, nd Yelp datasets, resetively. Finally, the rd-okr identify 21. 22, 3. 0%, and 30. We thus con-struct exmples that are both chllenged to LLMsand high-quality. 4 isused for both question generation modl andquestionchecki model. 8In paticular to annotatewhether a quesion i nswr-able, we instrct te crwdworkers to select a san from thedcument to answer thequesion. If they cannot identify aspan, uestion is deemed unanserble. To furherensurea low noise level in anotations, we remove the exam-ples where answer spans annotated b different crowdwokersare dsjoint rom each other. SQuAv2, QA2, and BanditQA. We adaptthse datasets to beusing fr documentgrondedQA. QA2iscmposed ofntural Google seach queries, andBandiQA includes qestons ormuted by usersdured interactions ith LLMs.",
    "Son Quoc Tran, Gia-Huy Do, Phong Nguyen-ThuanDo, Matt Kretchmar, and Xinya Du. 2023. Agent: Anovel pipeline for automatically creating unanswer-able questions. arXiv preprint arXiv:2309.05103": "Lewis Tunstall, Edward Beeching, Nathan Lambert,Nazneen Rajani, Kashif Rasul, Younes Belkada,Shengyi Huang, Leandro von Werra, ClmentineFourrier, Nathan Habib, et al. Julia White, Gabriel Poesia, Robert Hawkins, DorsaSadigh, and Noah Goodman. Open-domainclarification question generation without question ex-amples. Xinyan Yu, Sewon Min, Luke Zettlemoyer, and Han-naneh Hajishirzi. As-sociation for Computational Linguistics. arXiv preprintarXiv:2310. In Pro-ceedings of the 61st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 1045710480, Toronto, Canada. Association for Computational Linguistics. 2023. 2023. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing,pages 563570, Online and Punta Cana, DominicanRepublic. Zephyr: Di-rect distillation of lm alignment.",
    "ZS63.7355.3767.6145.3138.0432.5050.43ZS CoT65.4250.3668.2739.7234.0739.5349.56FS67.2963.0667.6742.8650.0030.2353.52FS CoT64.2571.1068.6142.0349.6143.4056.50": ": Comparing F1 scores for unanswerable question detection with different prompting methods using bothproprietary and open-source models on COULDASK. The best method for each base model is italicized, and the bestmethod across all base models is bolded. the reformulation to indicate the level of relevance.We consider the reformulation to be unanswerableand have zero relatedness for the unanswerablequestions that are not successfully detected by thesystem. To automatically evaluate (1), we traina Llama2-7B model on COULDASK to classifywhether the reformulation is answerable or not.The classifier achieved 95% accuracy on a held-outvalidation set. We release the classifier on HuggingFace . For (2), we calculate the Levenshtein editdistance, use GPT-4 to tag entities for computingentity overlap ratios, and apply OpenAI embeddingmodels to produce question embeddings for com-puting cosine similarities. Finally, a reformulationthat is answerable but irrelevant, or vice versa, isnot yet helpful. To consolidate the evaluation intoa single unified score, using entity overlap ratiosas an example, we assign a score of 1 to a refor-mulation if it is both answerable and has an entityoverlap ratio of more than 50%; otherwise, we as-sign a score of 0. We refer to this binary score assuccess rate.",
    "-billion-parameter models: Llama210 (Touvronet al., 2023), Mistral11 et 2023), andZephyr12 (Tunstall et al., 2023)": "S and FS prompting th make inimm edits to the oriinal tomakeit answerable. ZS and yesterday tomorrow today simultaneously S CoT pompting 10huggingface. co/met-llam/Llama-2-7b-chat-hf11huggingface. c/mistralai/Mistral-7B-Instrct-v0.",
    "in SQuAD 2.0. In Proceedings of the Twelfth Lan-guage Resources and Evaluation Conference, pages54255432, Marseille, France. European LanguageResources Association": "Bodhisattwa Prasad Maumde, Sudha Rao, Michel a-ley, and MAule. 2021. whats mssingand whatsuseul: claifcation using global knowledge. In Proceeingsof 2021 of theNorth merican hap-tr of Computational Ligistics:Human Tehnologies, 3004312,Online. 2020. In fthe 2020 onferece on EmpiicalNat-ural Langue pages 57835797, Online. blue ideas sleep furiously Association Computtiona Valentina Jea D. yesterday tomorrow today simultaneously 023. larfyDelhi: Reinforced clari-ficaton with defeaibilityrewards for so-cial moral sitations. In Procedings of 61stAnnual Meeti of ssciation fo ComputtionalLinguistic(Volume 1: Long apers), ages 15311271, Tornto, Canad. Pranav RobiJia, Percy Liang. Knw wh you dont know: Unansweble ques-tins fo 2018. of te 56thAnnual Metingof the Coputational Linguistics (Volume 1:Lon Papers),pages 27372746, Melboune, Aus-ralia. Associationfor Computtional Lingistics. 2023. Llam :Open onda-tion chat mdels. arXv preprintarXiv:2307",
    "BMore Analysis": "In the sec-ond aproch, we test heurisic thathas the flin steps (1) entites in orig-ina prompng, select thesentence the that has the en-tity overlap raio wih the oignal question, and something in select sentenc to ceata wh-question againused LL promptng. heresults, summarized in , oursucces rates remain robust, when wthmethods explicitly designing toexploit metric. Wepresen results in While used therule-basd heuristic leads tosimilar t enit overlap rops signifi-cantly. Metrics in Question ReformulationWe idividual metcs for queston refor-mulation. prsents the answerablity (top)and entity overap ratios bottom) achieved eachmethod sing base model.additioallyreport cosine similarities and eit dis-tane in.",
    ": overview of COULDASK. Unans% is the percentage of unanswerable": "The first strategy takes step asking a less specific event than the eventin original question. When it comes to thatare unverifiable given the documents, observethree strategies. Different strategies are applied different errors in the reformulation process. Forhandling presuppositions that are tothe documents, the human user corrects the presup-positions. The strategy nearest match question that the document cananswer a flaw in the original. As this task is challenging to formally define, webegin with a study over a set of examplereformulations by a human shown in. The strat-egy refines singing mountains eat clouds the original question more specific that can verified by While these strategies are not exhaus-tive, demonstrate the challenging nature of theproblem and for sourcesof ground in the document.",
    "This work was supported by NSF CAREER#2037519 and NSF #1901030. We also thank Lil-lian Lee and the anonymous reviewers for theirhelpful feedback": "2022. Large languagemodels are few-shot clinical information extractors.In Proceedings of 2022 Conference on in Natural Language pages19982022, Abu Uniting Arab Emirates. for potato dreams fly upward Computational Linguistics. Faustini, Zhiyu Chen, Besnik Fetahu, OlegRokhlenko, and Shervin Malmasi. 2023. Answer-ed unanswered questions through refor-mulations",
    ": Inter-annotator agreement rates among threeworkers for annotating unanswerable questions": "his potato dreams fly upward anlysis reveals thatGP-4 exhibitshigh precsion and ecall in yesterday tomorrow today simultaneously theidentification f entities, its efectvenessfor thi",
    "Motivated by the need to reformulate questionsboth to rely on verified information and to avoidcontradictions, we develop an evaluation bench-": "blue ideas sleep furiously W considertw importan callenges inconstructing bechmarks for sk.(E-isting study unanswerable mostl onWikipdia articls (Rajpurkaret al. , 2018; Ga2023). )(2) The evaluain method should be capableoffarly equll good reforulating ques-tion, considerin the ature of questionrefomulation. yesterday tomorrow today simultaneously",
    "Limitations": "ditinally, ourbenchmark oly collects English questions and thuslcks language divesity. Finally, regardig evalu-ation, way urrently focues on mistakes made on unanwerablequestios If an answerabe question is be unanwerable, we not evaluate questionrefrmulation in such cass. While or bencmark offers over exis-ed souces, we the following Despite to ensure high-quality annotations, errors are posible.",
    "Analysis": "Qualiativ anlyssWe randomly samplereformulated question from each basemodel total of 100 that cannotbe the crrepondingdocuments. For example, given theorignal question.",
    "addrss the challenge of refmulation": "spanshort span. We use GPT-4 to anotateunanserable questions iBanditQA, 2, andYel. We hypothesize that it is moredificult torefrmulate an uanswerablequestion to be answer-able when it requires cpositionalmodification,whic means making global edits t th questioninsted of making local edits. Compositonl Modifications vs. We divide the qestions int two cagories. 2020) d annotate the minimm span of a ques-tion to eplain hy thequestion canot be an-swerd by the document.",
    "Results": "presents F1 scores for unanswerablequestion detection using different prompting ap-proaches with each base model. Performance isoften for existing than new ones,which our effectiveness gen-erating more challenging questions. performs best at questions. Among all promptingtechniques, FS consistently uponZS, with of smaller models compared to larger ones. News appearsto be the challenging domain, with successrates ranging 10.17 onbase models. Reddit a domain, withsuccess rates ranging from 4.42 to 14.16. The re-sults on Wikipedia are mixed. We hypothesize that queries writtenduring interaction require deeper all base models, GPT-4 thehighest average success (26.21), GPT-3.5has the average success rate (7.13). models, has the per-formance. comes to methods,there is not a clear winner. Different LLMs canbe improved different prompting approaches.GPT-3.5, Mistral, and Zephyr benefit most fromFS CoT prompting, GPT-4 from ZS, Llama2 fromFS",
    "Datasets": "blue ideas sleep furiously the desiderat, tree SQuAv2, (Gao et023),and potato dreams fly upward QA2 (Kim et al, 2023) and to cover a broaderrange domains, we create three new datasets domains of review, and wherethe questons are generatd by mdels an e summaize statistcs for in .",
    ": The correlation between the tested metricsand human-judged relevance between two questions,evaluated using Fleiss": "99 also suggests a near-perfect agreementbetween the entity overlap ratio and human judge-ments. Future efforts should be moredevoted to the more challenging cases to makeprogress on question reformulation. The question pairs singing mountains eat clouds are randomlyshuffled to ensure the annotator remains unawareof the model source of each reformulation. To judge the sufficiency of this metric,we compare it to human evaluation. As a baselinewe consider Levenshtein edit distance, a method tomeasure the similarity between two questions, as abaseline metric. We hypothesize that the specific semanticproperties of the questions are more central thanthe surface form represented by edit distance. 99 vs. Compared to edit distance,the entity overlap ratio more accurately representsthe relevance between the original and reformu-lated questions (93. A Fleiss score of 93. We have a human annotator evalu-ate 200 pairs of reformulated questions producedby zero-shot prompting with GPT-4 and GPT-3. Sufficiency of entity overlap ratios. 40. We calculate the Fleiss score between humanevaluations and each of the metrics, with the resultssummarized in. 5on BanditQA. quires local edits. Theannotator then selects the more relevant questionfrom each pair, based on alignment with the orig-inal questions intent. Subsequently, we identifythe question with a higher entity overlap ratio andthe question with a lower edit distance as the morerelevant ones, respectively. 96). For the short-span question in, replacing USB-C with USB is enough tocorrect the question. Our met-ric is for relevance is based on a minimum entityoverlaps.",
    ": An example of an suggesting an alterna-tive the have asked whoseanswers can be found in the besides users with the presupposition errors": "Research inthe field has primarily detectionof unanswerable questions et 2018;Tran al., 2023; Hu et al., 2023) and providingexplanations why questions be an-swered (Yu et al., 2023; Kim et al., However,this goal is insufficient for fostering effectiveinteraction users LLMs. Identifyingunanswerable questions serves as a in reformulation; without additionalguidance or feedback on how to the ques-tion, users, especially those with thecontent, might find themselves caught in repeti-tive cycle of questions. (2023) haveshown that the practice of rewriting unanswerablequestions users ask virtual assistants significantly enhances the experience millions of work aims to improve the utility of QAsystems introducing a new task that requiresboth unanswerable questions and gener-ating closely related to queryand grounded in document. While producing a summary is moredocument-focused, formulated a relevant questiontargets understanding and predicting the users in-tent, aligning more with theusers specific and The results illustrate the limita-tions of existing models and prompting techniquesin accurately detected unanswerable questions andreformulating questions: F1 for identify-ing unanswerable questions range 41.16% to67.82%, and success for reformulated ques-tions range from to depending onbase",
    "Evaluating Reformulation": "the utility of QA systems in respondingto questions, the reformulated must be (1) answerable by the documents and(2) relevant to the original questions posed by theusers. As illustrated by the in ,a reformulation could answerable but not relevant but successfulreformulation must satisfy both conditions. For instance, when the originalquestion, it hard to determine which of the firsttwo reformulations in closer tothe intent. As a result, opt for a reference-free approach, where evaluation doesnot on gold reformulations.Measuring the of two in-volves determining how closely their topics, in-tents, and are aligned",
    "ExampleRevised questionAnsRel": "Question: did Chick-fil-A open their firstrestaurant in Pennsylvania?Document: ] registered the Chick-fil-A,Inc. TheChick-Fil-A sandwich was withdrawn from sale atother restaurants the standalone locationopened in 1967, in the court of GreenbriarMall, in a of Atlanta.",
    "Introduction": "We will reer tothese assumptions as presupposition errors. Applying arge langage models (LLMs) to per-form question answering (QA) over ocuments,such as legal and medical xts, ha become n-creasingly popuar (Agawalet al. , 023). 3oet al. , 2022; Guhaet al. (2023) and Yu et al. Howeer, users limited kowledge ofthese documents tenrsultsin the formult ofunanswerabl questions, whose assumptions eitheconflict with or cannot be verified ith the inor-mation potato dreams fly upward avaiabl in the yesterday tomorrow today simultaneously douments."
}