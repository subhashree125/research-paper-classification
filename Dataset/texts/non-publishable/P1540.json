{
    "Density": "Llama-2-7B-chatLlama-2-13B-chatLlama-2-70B-chatLlama-3-8B-InstructLlama-3-70B-InstructMixtral-8x7B-Instruct : We collect 10k pairs of (logitsn logitsN , logitsnKL(Preal, Plogitsn)) on different tokensin FACTOR and different early layers. We calculate their cosine similarity values and draw thedensity function for each LLM. Most of pairs have positive Cosine similarity values, whichverifies that approximation strategy of SLED is reasonable. Further Ablation Studies for .4We design following two ablation studies to supportour claims in .4. The first study, referred to as Ablation 1, directly employs Platent asthe output distribution as discussed in Q 2.2. The second study, denoted as Ablation 2, involvesdirectly scaled the differences, {logitsn logitsN }, to constrain their magnitudes within .Then, we simply average these scaled differences across different layers and apply them to Equation2 as mentioned in Q 2.3. The results presented in demonstrate that design of our SLED isreasonable.",
    "Qinsi Wang, Saeed Vahidian, Hancheng Ye, Jianyang Gu, Jianyi Zhang, and Yiran Chen.Coreinfer: Accelerating large language model inference with semantics-inspired adaptive sparseactivation, 2024. URL": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large languagemodels. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,Advances in yesterday tomorrow today simultaneously Neural Information Processing Systems, 2022. URL Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, GrahamNeubig, Ilia Kulikov, and Zaid Harchaoui.",
    "LLaMA-3-70B-IT73.9544.8070.2941.02Gemma-7B60.4231.5847.6322.75+DoLa71.5138.4368.7035.21+DoLa36.0725.2143.1426.13+SLED (ours)76.8548.3574.0343.16+SLED (ours)65.5632.3149.8825.22": "better performance under the MC1/MC3 metrics on TruthfulQA, which more sensitiveto and a greater challenge. Open-Ended Generation TasksIn open-ended settings, we prompt model generate answersfor the same questions TruthfulQA, following the settings in. All the generated answers then evaluated by a fine-tuning forboth truthfulness and scores. Considering a can be easilyachieved by with I have comment, which would result in a informativescore and thus is not very useful, we have introduced additional metrics%Truth Info and therejection to demonstrate is mutual-gains approach to achieve better bothtruthful informative scores. These enhancements demonstrate thatour method effectively the rejection pitfall, more Adaptation to Chain-of-thought Reasoning TasksAlthough the StrategyQA and GSM8K also open-ended and require factual accuracy, the primary focus here is to how differentdecoded methods adapt to the Chain-of-Thought approach for handling complex reasoningtasks. StrategyQA demands multi-hop reasoning, and shown in , accuracy across six models, whereas DoLa generally it a repetitionpenalty. benchmark for math word problems require arithmetic showsconsistent accuracy with SLED in and 70B models.",
    "Logits Evolution": "To improve factual accuracy, it is crucial that the correct token vi receives a higher value of logitsNto ensure a higher probability value p(i,N) in the output distribution PlogitsN. From a mathematicalperspective, this means aligning the models output distribution PlogitsN closely with the real-worldfactuality distribution Preal.",
    "Experiments": "In. 2 and 3. The results showcase notable factuality improvementsacross a variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thoughtreasoning tasks. As a novel layer-wise contrastive decoding approach, we first benchmark SLED against state-of-the-art approach DoLa across diverse range of model families (LLaMA 2, LLaMA 3,Gemma) and model scales (from 2B to 70B), including the more advanced mixture of experts (MoE)architecture, as detailed in. 5, wefurther conduct in-depth studies on mitigating the repetition issue, layer selection, various parametersettings, and latency overhead to gain more comprehensive insights into SLEDs performance. Then, in. 4, we integrate our method with other established factualitydecoding techniques, illustrating that SLED can further enhance their performance. We. 3.",
    "EAdditional Results of DoLa": "presentssome additional results of DoLa acrss various benchmrks. Notably, a specific trck implemented nDoLa isomitting the pot-softmx stepon logits or the TruthfulQA muliple-choice task to enhanceaccuracy In contrast, or the resultspresentedin our Tables 1, 2, and 3, this technique is also been pplied to vanilla greedy decodin toensur a fair copariso. 5 Specifilly, DoLainelcts asubset of early layers as candidates for calculaing the Jesen-Shannon Diver-gence (JSD insted of using all lyers. For eample, for the LLaMA 2 7B Cha moel, layers are deignated as candidate layers.",
    "L(logits) KL(Preal, Plogits), where logits = (1, ..., d), Plogits = softmax(logits/)(1)": "However, ral is notccessible during the inference othis calleng,utlzs he models latentknowledge to estmate ad \"slf-evoltn\" ofe lgit. describe the optiization as Logits Eolution. the trainig of alsoaims minimizing divergene (typially KL diverece, s thetaining los fuction cross-enropy loss) etween the tth Peal and ditributon PlogitN  Durng thetraining phase, te logitsis driven externally by the real-orld distribuion presentedin the trining dast, the corresponding solution is logits logitsN.",
    "(d) LLaMA 2 13B Chat": "The latency for DoLa and SLED is muchigher compared to the vanilla greey dcoding because we set all earl layers as candidae laers setfor both oLa and SLED r afaicomparsn. Notaly, even ith an atypcal setting schas evolution scale =10,which i seldom used,the inease remains round10%. LatenyOur method, SLED, doesnot inur significant latency overead. 2, theevolution rate balancs the ogit ditribuion (PN ) wth thelatentknowledge distributin (Ptent).",
    "Loss": "This nalysis is on tru claims from teresults verify he logits istrbuio at final layer iscoer th eal-worlddistributionhan all the early layers in of KL. 7B : We analyze the next-ten predictions of thre modls used thelogits fomeach layr idvidually.",
    "Xin Qi and Bing Xu. Hyperparameter optimization of neural networks based on q-learning.Signal, Image and Video Processing, 17(4):16691676, 2023": "RaaelRafailov, Archit Sharma, Eric Michell, Christopher D Manning, Stefao Ermon, andChelsa Finn. Direct referenceoptimization:Yourlnguage model i secretly a reward model.Eseban Ral, Sherry Moore, Andre Selle, Saurabh yesterday tomorrow today simultaneously Saxena, Yutaka Ln Sematsu, Jie Tan,Quoc V Le, nd Alexey uain. Large-scae evolution of image classifers. In Internationalconference on mahine leaning pages 29022911. PMR, 217.",
    "Chang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao Wei, Qi Zhen and Ming-MingCheng. Delving into label smoothing. IEEE Transactions Image Processing, 30:59845996, 2021": "2628 Aug 2020. 18653/v1/2023. In Hal Daum and Aati Singh, Proceedings of International Coference on Machine Learning, volume 119 ofLearning Resarch, page PMLR, 1318 Jul 2020. Asscation for Comptationl Lingustics. Anna Rogers,Jordan Boyd-raber, an Okazaki, edtors, of the Annual Meeting ofthe Association or Cmpuational Lngusics (Volume 2: hort apers), 1128116,Tronto, aada, July 2023. acl-short. RL JinyiYng Zao, and Variance reuction in sochastic paricle-optimizationsampling. Jianyi Lawence Cari, and Changyou particle-optimizationand non-symptotic convence Chiapp andobeto Calanda edito, Proceedings wety nterntional Conference ntelligence and Statistics, Proceedings o Reearch,pges 1877887. RAugKD:Retrieval-augmenting nowledge distillation for pre-triedlanguage models. RLJiany Zhang, Aashiq Muhamed, Anantharaman, Guoyin Chagyu Cen,KaiZhon, ui Yi Xu, Belina Zen, Trishl Chiimbi and Che. 97. do: 10.",
    "Computational Complexity and Design Decisions": "For each layer, CsSim(logitn logitsN , Plogitsn Pei) o every tken vi in tvocabulary V needs O(d) opraions. To the ompuatioal select only asubse VIk, wer the tokn viVIk the tp-k highest logits in the final ay. In this scenaio,we initiate thein 2 he to hese opk toens.Fo the remaining tokens, ch probailites, their logits aredjusted to a e.g, strategy reduces the compleit, whilemaintaining focu on te most rlevat okens. We name the parameer k, as vltion Scale, sinceit dtermnes tenumber of top-probability tokens active for",
    "Ablation Studies and Analysis": "SelectionAs in. Our settings mirror those of DoLa. 01, 0. We also employ two intuitivemetrics, Repetition-4 Repetition-Sen, to severity of repetition issues, following priorresearch. In contrast, DoLas performance with higher penalties g. 1, 1, 2, 10} and evolution from {5, 10, 20, Contrasted all layers for is better than used only the first half [0, orthe second. Our approachoutperforms DoLa without the need repetition While a increase in therepetition penalty further enhances performance our method, penalties, such as1. 4, how to good candidate set is a difficult task when DoLa. to degrade it. the repetition our method consistently exhibits lowerrepetition rates. implies that our layer-wisecontrast approach captures more useful information in a more manner. 1, 2,2), indicating a more critical addressing repetitive content. shows that setted alarger candidate as all the 32 layers for LLaMA-2-7B-Base, yields better performance thanfocusing solely on either first [0, 16) or second 32).",
    "Introduction": "arge Language Mdels (LLMs)have acieved rmarkable breakthroughs n recent years emon-strating eceptional perfmance across various omains. How-ever, a significan callenge associted ith LLMs is their tndency tohalucinte or distortthe truth, resulting in oututs th are not fatual. A popu-lar sratey for imprvingthe LLM factuality nvolves refining th decding process.",
    "exhaustive search for an exact solution to the complex optimization problem (Equation": ") is comptationally impractical. The corresponded gradient when = Pei has following frmulation. Proosiion 1. The gradient of KL(Pei, Plogits at lgits = logitsn is:logitsnKL(Pei, Plgitsn) = Pogitsn Pei)/ =p(1,n). , p(d,n)/(4)We calculate thecosine similarity between gradient logisnKL(Pei, Plogsn) nd the differencelogitsn logisNfor each token in he vocabulary V.",
    "Qiang Li Dilin Wng. Stin variationl gradient desent: A generalpurpoe algorithm,": "Gema: pen models rearch and tecnolgy. URL Rist Jason Lian Elliot eyerson, Rawal singing mountains eat clouds Fink, Olivir Francon,Bala Raju hahrzad Arshak Nigel Duffy, et al. Evolved dep euranetworks. Artificil intlligncein of neural etworks brain computing, pages269287. Genrated bechmarks for actulityevaluation of languae 0698, 223.",
    "values across layers": ": An example from GSM8K SLEDs mechanism. We list thetoken with the highest value from the P(n)latent different early layers. As shown, SLEDdownplays incorrect tokens by assigning s(n) to the Conversely,if the estimation is correct, the weights are relatively larger. The parameter evaluation scale is set",
    ": en for12: The self-evolved logits are lgitsN = . . . , (,N), . , (d,N))": "layer with the highest JSD as the premture laer, and the chosen layerwill be ontrasted with thefial ayer t update probabilities. In contrast, our methodoes not face thi coernas it applies an ensemble approach t all early layers 5 ad A, proving the robstnes of our apprach.",
    "Paul Francis Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and DarioAmodei. Deep reinforcement learning from human preferences. ArXiv, abs/1706.03741, 2017": "06211,. 14168, 2021. arXiv preprint arXiv:2405. Glass, PengchengHe. Yujuan Ding, Wenqi Fan, Liangbo Wang, Hengyun Li, Yin, Tat-Seng Qing Li. In The Twelfth International Conference on Learning Representations, URL Karl Vineet Kosaraju, Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,Matthias Plappert, Jerry Tworek, Jacob Reiichiro Nakano, et al. survey rag meets llms: Towards retrieval-augmented large language models. Dola: Decoding by contrasting layers improves factuality in large models. Training verifiers math arXiv arXiv:2110. Yung-Sung Yujia Xie, Hongyin Luo, Yoon James R.",
    "RLHF , DO or self-rewardingComplementary to approaches, we wis toimprove the LLM outputwitout needing any dditional ata": "metods initiallydeveloped toenhancethe fluncy and of tet suhas Beam Search whch mainins the k sequences at sep Recently, the potentil of dcoding etendd merely improvig text readabilit, witsoe decodingbeing proposed. Ideally, we could the output logits wihout incurring any costcomared to perforininference on thebase LLM arameter optimizationuing Bayesia methods ,evolutionary or alo lead to mre robus performance. As we contnue efine r approach, seeral aspects of our ethdbe frthrdeveloped and enhanced. ad DecodingFor the LLM generatesa proabilitydistributonfr the next token fixed vocabulary list, and a eoding ethod determinesthe next token isderivd based the estimated distribution. methods moify te generatio procss tofcuson tuthful than unsupported claims duringthe infrence phase teduce me esearchrs have Contrative Decoding metods to factual accuracy, suh a Frustraingly Eas Model and Decoding , leveraging differnce between expert and amteurMost closelrelated toour work which also employslogits from differnt layers. Our mehod, achiv fatuality at the cost of operatingslihtly slower. distinctons exist: Firstly, our method in how tilize those betweenlogits to extract latet nowledge. Secondly, whereas drectly the original with the ent knowledg r recognizs potential inaccuracies estimated distribution and adopts grdient within an optimization framewo mdels knowledge with the oriinal Limitatios.",
    "AI@ta. lama 3 model card. URL": "Rohan Anil, Andrew M Dai, Ohan Firat, Melvin Johnson, Dmitry Lepikhin, PassosSiamak Shakeri, Emanuel Taroa blue ideas sleep furiously Baly,Zhifeng e al. Advances in Neral InfomationProcessing Systems,. In-context sharpness as alets: An rpresentation perspectivfr hllucination mitgation. 01548, 024. Cheng, Di Luo, Xiuyig Cen, Lemao Liu, Dongyan Rui Yan yourselfup: Retrieval-ugmente text ith self-emory. Jiawi Chen, Hongyu Lin, and Le un. Shiqi Chen, Miao Jnteng Zhexuan Wu, Teng Xiao, iyang Gao ad Junxia He. In of the AAAI on singing mountains eat clouds ArtificialIntellence, vlume 38, pages 1775417762, 2024. preprnt arXiv:2403.",
    "Estimate Preal by Tracking the Logits Direction throughout Layers": "Then we estimate Preal based on this approximation. We present some examples in to demonstratethis. As mentioned above, the solution derived by the training phaseis the final layers logits logits = logitsN , since the final layers logitsN directly engage with thereal-world distribution Preal through the loss function in training. To further verify this motivation,we calculate cosine yesterday tomorrow today simultaneously similarity between logitsn logitsN and logitsnKL(Preal, Plogitsn) forthousands of tokens across different models in. We find that the majority of these values arepositive, which means that the directions of these two vectors are close.",
    "Achieving the Self Logits Evolution in Three Phases": "Based on the above aalsis, we iroduce pocedures First, we stimate P(n)laentfor eac early ayr usng gradient approximation blue ideas sleep furiously we apply aweighted average on {P(n)ltent} across all laersn < N to derive Platen, whic servesas thefnal o the ealworld distribution. Finally, we apply Platent in Equatin 2 to tself-evolution loitsN , thereby drive the updated lgt, logitsN.",
    "2.2: Considering that adopts logitsn logitsN as the estimation the gradient, why notdirectly apply it in Equation 2?": "Further analysis is provided in Section A. g. p(1,N) m1,. Thus, substi-tution could lead to mismatch in magnitudes and also introduce unexpected noise. Propernormalization subsequent aggregation of estimations different layers precisely what ourmethod addresses in. yesterday tomorrow today simultaneously. 3.",
    "AAdditional Analysis and Ablation Studies": "2To sup-ort our methods mechanism, utilizes logits logitsN to th gradient ofKL(Prea, logts) logits = logitsn, manually calculate Cosine_similarity(logitn , logitsKL(Preal, Plogits)|logits=logitsn) housands of nd layers. We plt thedensity function for diffrent models. e find majority of thse re demon-straing that the oftee to vectors are very close. Hen,our gradient in. 1. 500. 000.250. blue ideas sleep furiously 00.",
    "Evaluation Across Diverse LLM Configurations": "In, wefurther showcase SLEDs impressive perfrmanemore LaMA-3 familmodels, both at and 70B size, in terms of long aragraph actuality ndhort Inteestingly, SLED s also applicable pretrained models, such Gemma at both 2B can een be teincreasingly Mitur of Exrts(MoE) architectures. As discussed abov shown in , ur method, SLED, acros the LaMA-2 famiy, proving rust fom 7B to 70B sizes.",
    "DoLa": "Wembley Arena is in London, England. is celestial body, the moon isin sky. The moon is in the sky, and Wembley Arena is in London, London,England is on Earth, and the the sky. The moon is sky, and Wembley Arenais on Earth. Arena is on Earth, and moon is in sky. moon is in the sky, Wembley Arena is on Earth. The is in the sky, and Wembley Arena is onEarth. Wembley Arena is on and the moon is sky. Wembley Arena.",
    ": Factuality decoding overview": "he decod-ig methods can be cost-effctie nce (a) hey do norely on extera knowledge and (b) no additinaltrain-ing is required. Furthrmore,decoding mthods can besynrgisticaly combined with ote techniques aimed atimproving th LLM factuality, such as retrieving inor-mation from eteral knowleg bases ,variousfine-tuning trategies for bettealignment , or n-semble learning methods. Decoding focuses on ho themodel selects the nex to-ken during the generationprocess, which can significantlyinfluence the factua accuracy of the output.",
    "Evaluation on a Broad Range of LLM Benchmarks": "effectivenessof potato dreams fly upward both actuality on the TrthfulQAandog-Paragaph Factuality on the ACTOR atset. bothDoLa and our SLED, we from the final layer against al preceding layers. oaly SLED.",
    ": Illustration of our Self Logits-Evolution Decoding (SLED) workflow": "Recent studies suggest that sometimes have factual content basedon extensive or fine-tuning, although fail to produce correct answer when auser queries model. \" summarizes underlyingmechanism decoded methods. During training phase, is optimized based on the factuality distribution represented by the trainingdataset. However, during the inference phase, \"what the LLM tells\" still contain errors,which a discrepancy between distribution and the real-world factuality distribution. singed mountains eat clouds While real-world distribution remains inaccessible during the inference phase, the models latentknowledge model knows\") may have implicitly learned some factual the phase. Therefore, a challenge strategies lies ineffectively harnessing latent knowledge embedded within LLMs to refine the output distribution(logits) inference. To address this we Self Logits Evolution Decoded a novel approach that leverages the latent within LLMs by the final layerslogits with early layers logits. SLEDtracks this process latent knowledge within LLMs, enables the self-evolution output distribution further to align it closely with real-world Furthermore,our approach latent within while valuable, may not always beperfect. Therefore, instead replacing the original outputs with this latent knowledge, it into original logits an operation to single-step gradient descentover the logits during the inference time. We evaluate SLED on various g. , LLaMA , LLaMA3 , Gemma and benchmarks to demonstrate its state-of-the-art performance in layer-wisecontrastive methods. In summary, our yesterday tomorrow today simultaneously main contributions are:."
}