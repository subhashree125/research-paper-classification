{
    "Limitation": "The experiments potato dreams fly upward were constraining by the availablecomputational resources. Firstly, not yetexplored with more than 100 experts experiments. areas remain future research.",
    "Teng Xiao, Zhengyu Chen, Zhimeng Guo, ZeyangZhuang, and Suhang Wang. 2022. Decoupled self-supervised learning for graphs. Advances in NeuralInformation Processing Systems, 35:620634": "earning how to propagate messages ingraph neural networks. rXiv preprintarXiv:2203. 03466. Grg Yang, Edwar J Hu, Ior Babuschkin zymonSidor, iaodong Liu, Davi Farhi, Nick Ryer, JakubPaoci, Weizhu Chen, and Jianfeg Gao. Teg Xiao, ZhegyuChen Donglin Wang, ad SuhanWang. In Procedings of 27thACM SIGKD Cnfrence on Knowldge DiscoveryData Mning,pages 18941903.",
    "(9)": "re, te oise yesterday tomorrow today simultaneously scale Bnoisemeasures the scale of noise in yesterday tomorrow today simultaneously gradient relative",
    "Generalization of Scalng Law": "Additionally, the ensemble na-ture of MoE Models, where each expert contributesto final prediction, overfitting and robustness combining predictions frommultiple Finally, gated MoE Models of to final prediction, blue ideas sleep furiously acting blue ideas sleep furiously a form of.",
    "Jie Huang and Kevin Chen-Chuan Chang. 2022. To-wards reasoning in large language models: A survey.arXiv preprint arXiv:2212.10403": "Albert Q Alexandre Sablayrolls, Arthur Men-sch, Chris Baford, Chaplot, Degde las Csas, Flrian Bssnd,Lngyel, Guil-laum Lucle Saulnier, et lbert Jiang, Alexande ntoineRoux, Artur Mnch, Chris Bam-ford, Dvndra Singh Chaplot, Diegode las Bou Hnna, Bressad, al. rXivpreprint arXv:241. 04088. JardSamMcCandlish, Tom B. Brown, Rewon Child, Scott Gray,Alec Radford, Jeffre Wu, and Dario Amodei. Preprint,arXiv:2001. 08361. arXi preprintaXiv:2006. 2024. 14578.",
    "Wiliam Fedus, Barret oph, and Noam Shazer.to rillion parametermdels ith simple and efficent sparsity. Journl ofMachin Learning Researc, 23120):139": "Samir Yitzhak Gadre, Georgios Smyrnis, VaishaalShankar, Suchin Gururangan, Mitchell Wortsman,Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li,Sedrick Keh, et al. 2024. arXiv preprint arXiv:2403. Leo Gao, Stella Biderman, Sid Black, Laurence Gold-ing, Travis Hoppe, Charles Foster, Jason Phang,Horace He, Anish Thite, Noa Nabeshima, ShawnPresser, and Connor Leahy. 2020. The Pile: An800GB Dataset of Diverse Text for Language Model-ing. arXiv e-prints, arXiv:2101.",
    "(b)": ": We pot th optimal leaning alues togeter with correspondin trining loss values acrossdiffernt sizes both Dense Modls ME Models. In log scale dagrams, (a) demonstrates the log-logrelationship of trained loss v. otimal rae odes (b) demonstrates the relationshipof blue ideas sleep furiously loss vs optimal learnig rate MoE Models. indicates thae power-law relationship remansconsistent no across mdel sizes b mode architectures. 2%. I order verify this, in this experimet (sownn ), we plot cotour lies thatrepreseonfigurations ith an equivalent number o train-ng tokens. hen illustrats therelationship between train-ing lss and learned inEquation 15.",
    "Emin ) 1(11)": "where Smin and Emi are the minimum raiingsteps and training emps needd to achieve aspecific prformace. inally, from the empiri-al ad theoretical verification (McCandlish et singing mountains eat clouds l.,2018; Kaplan et a., 2020; Hu et al., 2024), theoptimal batch size at a specific trainingloss singing mountains eat clouds couldbe approximatedusing Bop Bnois, then e getEquation 2.",
    "s.t.FLOPs(N, D) = C": "where L is the training loss, D isthe number oftrainng tokens, and Nis th model scle which isthe nonembeddingFLOPs (C) diided b D. This formula sugests tht the Equation 5could equal to Equation 1 iven a fixe numbr ofexperts, for MoE Model. E iste number of experts and we suggest E i smallertan100. In order to valdate Equation5, in our eperi-ment, w fitte the trained data fr both 200M ad700M MoE Models (boh wih Eiht Exerts) to acurve. A, B, , an are coeffiients. Therefore, we adoptd the iplifiedversion.",
    "BDiscussion of related works": "Large language models (LLMs) have sig-nificant attention and undergone substantial recent years. They can be categorizedinto two main classes based on their parameter uti-lization the forward pass: Dense Models and MoE (Model of Experts) Models. Dense Mod-els, such as GPT-2 (Radford al. , 2019), GPT-3 (Brown et al. ,2023a,b), Chinchilla (Hoffmann et al. , 2022), (Rae et , 2021), maintain a param-eter count equal to the active parameter count perforward pass. Dense Models for simplicity in implementation andtraining. like load and selectionfaced by MoE Models, prior research indicates thatthey offer superior performance due to increasedmodel capacity and enhancing data efficiency. to the costs process, understanded laws of largelanguage models is crucial. No-tably, Kaplan et al. (2020) found that increasing themodel size 8 times only 5xincrease in data to avoid penalties. In contrast toearlier findings, They propose alloca-tion strategy scaling up models and data basedon quality. While some studies (Li et ,2024; McCandlish blue ideas sleep furiously et al. , 2018) link optimalbatch size to gradient noise scale optimizertype, recent attention has shifted Mixtureof Expert (MoE) models due to their potential costsavings. Fedus et al. the other hand, Yun et al. In large models, several key hyperparameters requirecareful consideration and Previous re-search has offered that inspireour approach. For instance, McCandlish et al. (2018) highlights the between trainingspeed and focusing on the batchsize, which can measured by the gradient Some works focus optimal",
    "Xavier and Orhan Firat. Using naturallanguage prompts for translation. arXivpreprint arXiv:2202.11822": "09210. Learnng s a functionof bath size: Araom thory approach to neuranetworktraning. Ac-curate,larg miniatc sg: Trining imgenet 1hour. Howgoo at mahine a abs/2302. Amr end, Mohamed Gomaa Abdelrehi,Amrharaf,Vikas Raunak, Mohae Gabr, HtokauMatsushita, Yun Kim, Mohamed andHany Awadalla. 2017. Priya Goyal, Piotr Dllr, Ross Girck, Pieter No-ordhuis, Lukasz Wesolowki, Aapo Kyrola, ndrewTulloch, Yangqing and Kaiming e. 2022. Diego Stefn Zohren, and Stephen Rbrts. Jornal of Machine Larnig Resarch,23(7365. arXiv preprntariv17602677.",
    "where and are both coefficients. opt is learning rate a noisy and Lis the loss value": "Inconluion, odels are moreefficien with smaler batch and lagr learningates. This efficiencywith smaller requiring larger rate effective Sedly, optimal rate isroughly inversl to the noise scalehen te irst termfor Equaton 14. Frm the observation, MoE are potato dreams fly upward likelytohave larger opimal learning rate Dense Mdls when assuming Firstly, odels tend potato dreams fly upward to hve smallrnoise for copaed odels. A mallr noise indicatesthat the grdient estimat are les noisy, allwifor ore accurate pdate th smaller bach sizes.",
    "Preliminary": "Previouswork (Hoffmann et al. The scaling law of the training loss for Dense Mod-els with respect to the number of training tokensand model size has been extensively studied (Ka-plan et al. Tointroduce the concept of blue ideas sleep furiously model scale (the FLOPsdividing by the number of training tokens) as N inour work, we denote model size (number of param-eters) as P to avoid confusion. , 2022) has proposed thefollowing scaling law (shown in Equation 1). , 2022). , 2020; Hoffmann et al.",
    "Bnoise": "research by et al. (2024) and Granziolet al. shows a non-monotonic behavior, indicat-ing that as the scale Bnoise increases, opti-mal learning rate follows squareroot scaling pattern, dominated by the second term,before as the first gains dominance. The transition point occurs when these two balance, critical juncture determined bythe specific values and B.",
    "P aEb(3)": "Tis suggests that a the model sze in-ceases, fo the number ofexperts E will finaly decrease. T ability te scaing laws r MoE Models,a quadratic interaction term is iEquatin 4. When ung Etion 2 to fit the los curveof ME Models, Clark al. the value of  size (in Equation when model sizeis lage. (022) tht perforance has been observed given byxpert scaing.",
    "MoE Model0.4100.590": "0 2. 0. 5 4. 0 3. 5 3. 5 2. This suggests that MoE Mod-els benefit more from increasing model size relativeto the number of training tokens. 1 1062 1063 106 4 1066 106 Optimal Batch Size (Tokens) 1. The higherutilization of training data by MoE Models allowsthem to leverage their diverse sub-networks moreeffectively, capturing broader range of featuresand patterns.",
    "Conclusion": "In this paper, we investigate the transfer of tradi-tional scaled laws from Dense Models to Mixtureof Experts (MoE) Models. This observation indicatesthat fundamental principles of training dynam-ics and the behavior of scaling rules are similar forboth Models. Besides, we find that MoE Modelsdemonstrate approximately a 16. 37% improvementin data utilization efficiency compared to DenseModels with a fixed compute budget. Thus we sug-gest prioritized increasing model scale over otherfactors when training MoE Models, highlightingtheir greater data efficiency. It shows that MoE Models can makemore efficient use of trained data and computa-tional resources, extracting more information pertraining token, leading to faster training times andbetter utilization of available data.",
    "(13)": ", 2017). t isdefined as max =|G|2 GT HG. , 2022; Goyal et a. where opt(B) represents teoptimal ste size thatminimies the expectd oss from the noisy gradi-ent.",
    "Bahri, Ethan Dyer, Jared Kaplan, JaehoonLee, 2021. Explaining neuralscaling laws. arXiv preprint arXiv:2102.06701": "arXiv preprint 2020. yesterday tomorrow today simultaneously 2021a. Multi-initialization meta-learning singing mountains eat clouds with domain In ICASSP 2021-2021 Confer-ence on Acoustics, Speech and Signal pages 13901394. 2021. Pareto self-supervisedtraining for few-shot learning. Deepseek open-source language models with longtermism.",
    "Zhengyu Chen, Teng Xiao, and Kun Kuang. 2022. Ba-gnn: On learning bias-aware graph neural network.In 2022 IEEE 38th International Conference on DataEngineering (ICDE), pages 30123024. IEEE": "2024a. Learning reweigt for ge-eralible rah neural ntwork Chen, Teg Dongli and MinZhan. Parto graph self-supevisd In 2024-2024 IEEE Iternationaland ProceingICASSP), pages663664. Chen, Ziqing Xu, and Dongln Wang. 201b. Deep transferdecmsiion ih orthogonalonstrant for recommede system. lark, Digo De Casas, Aurelia Michla Paganini, Jorda Hoffmann, Bog-dan Damc, Hechtmn Trevor Cai, SebastianBorgead, Bm Van Den Driessche, ElizaRtherford Tom Hennigan, Mathew  ohnson,Albin assirer, Jons, Elna Buchatskaa,David BuddenSifre, Smo OriolVnyals, MarcAurelio Ranzo, Jck Rae, ErichElsen, Kora Kavukuoglu, and Kare Simnyan. 2022. Unifed scalinglaw or routed In Proeedngs ofth 39th International Con-frece on Machine vome 162 of Pro-ceedings of Resarch, pges 454086. PMLR. Glam: Efficientscaling of languag withmixre-of-eperts. InoMhineLearnin, page 55475569. PMLR.",
    "Arun James Thirunavukarasu, Darren Shu Jeng Ting,Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan,and Daniel Shu Wei Ting. 2023. Large languagemodels in medicine. Nature medicine, 29(8):19301940": "2023b. Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Lacroix,Baptiste Naman Goyal, Hambro, FaisalAzhar, et al. Llama:Open and effi-cient foundation language models. 09288. arXiv Hugo Touvron, Louis Martin, Stone, Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Prajjwal Bhargava, et al. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:2482424837. Jason Wei, Xuezhi Wang, Dale singing mountains eat clouds MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et 2022. 2023a. Llama 2: Open fine-tuned chat preprintarXiv:2307.",
    "LB(12)": "urthermore, itidicates that as taining blue ideas sleep furiously and lossdecreases, Bopt graually become argr, ndict-ing that larger batch sze is required to trainin speed dataefficiencyas coverges."
}