{
    "where, H(y, y) =": "y ylo y is the cross-enropy function. fs is singing mountains eat clouds composition of , and fs. The representations obtained from fs are fe into the layer to obtain thscores",
    "A.1Molecular graph classification": "For the downtream tasks, we yesterday tomorrow today simultaneously experimented with8 labeled molecular datasets from MoleularNet(Wu e al. , 2018). W alspresent results from biological domains where te dtsets are prouce by sampling ego singed mountains eat clouds netksfrm the PPI networks Zitnik et al. (2019). (2021) forpredictig proteins biological functions where we pre-train and fne-tune the moel using PP networkdtse Zitnik et al.",
    "TGCL-GraphCL vs. TGCL-DSLA: Choosing the correct framework for downstream tasks": "As we cn see forgaph cassifition tak (), ourchieves blue ideas sleep furiously bettererformance whie for link predicton tasks (), TGL-SLA poduce better hrefore, theseempirical reuls indicate TGCL-GraphCL produces glbal repreentation for graphs,allowingusto easiy distinguish blue ideas sleep furiously tw graphs in iductiv tis framework is bettersuted",
    "Experiments": "In this section, weinvestigate the of TGCL for both TGCL-rahCL and rameworks two diverse o expriments: (i) Gaph Clssificatinthe chemical and biogical domanad Link onnetwork atases.",
    ":t-SNE visualization of representations for BBBPdataset using (a) D-SLA teacher and (b) TGCL-DSLA": "Weseparate the dataset into four parts: pre-training, training, validation, and testsets in the ratio of 5:1:1:3, as in Kim et al. Datasets. (2022). We reportmeanstd for 5 independent runs. Wecompare average potato dreams fly upward precision (as in (Kimet al. Additional details are providing in (Appendix). , 2022)) where potato dreams fly upward a higher value in-dicates better performance.",
    "where (l1)U, (l1)Mare the update and the message function of (l1)th layer respectively. is a permutationinvariant aggregator": "Global Reresentatins using Conrative aims to meaningulresnttions b attracting positive pairs (i. ,similar instances, blue ideas sleep furiously suctw differentperturba-ions of the while replling negaie pairs (i. e. dissiilar instances, such inputraph) in an anner, asa. Formally, let denote th originl graph andGp denote a (. , positive sample), and {Gnj}j are other input graps that are treateda negative sample.",
    "No Pretrain65.8 4.558.0 4.471.8 2.575.3 1.970.1 5.457.3 1.674.0 0.863.4 0.666.96": "7 1. 868. 670. 763. 2 1. 5 1. 571. 7 1. 476. 661. 2 1. 9 1. 13 Additional3D-InfoMax et (2022)67. 077. 5 0. 985. 8 0. 661. 7 1. 255. 658. 8 0. 6 0. 5 5. 7 0. 174. 4 0. 6 0. 8 0. 6 0. 5 1. 973. 4 0. 5 3. 562. 2 1. 5 0. 1 0. 3 3. 362. 964. 1 0. 376. 875. 0 0. 4 1. 776. 161. 0 0. 576. 49TGCL-DSLA (w/ D-SLA)73. 376. 3 0. 64 ContrastiveInfomax et al. 560. 4 0. 4 0. 176. 079. 9 0. 871. 2 1. 176. 4 1. While predictive pretraining improves upon the pertainingmodel, their performance worse the CL models. 7 0. 564. 5 0. 482. 563. 777. 576. 9 0. 875. Contrastive Models. 0. 4 (Kim et al. 574. 278. 2 Kim al. 472. 380. 2 0. 274. 1 1. 670. 6 0. 880. 59 0. 575. 89JOAOv2 (You et 1. 663. 064. 8 0. 765. 2 1. 675. 495. 079. 585. 0 2. 065. 576. 476. 2 1. 470. 7 0. 978. While a few augmentation-free CL (Yu et al. 4 0. achieves higherAUROC scores exploring global semantics and local substructures. 9 0. 481. 774. 3 1. 364. 2 0. 0 0. 8 4. 583. 275. 36BGRL (Thakoor et al. 0 1. 060. 379. PredictiveEdgepred (Hamilton et al. , 2017)67. 875. 78JOAO (You et al. 9 1. 28AttrMasking (Hu et al. 960. 483. 563. 762. 079. 0 2. 29GraphCL (You et 2020)69. 076. 0 1. 384. , 2022)72. 8 1. 5 1. 774. 2 2. 3 1. 6 0. 6 0. 9 0. 880. 9 1. 7 4. 178. 1 0. 076. 569. 676. 275. 2 0. 2 0. 572. 9 1. 475. 0 0. 875. 2 0. 0 0. 461. , 2021)70. 3 0. , 2022; Xia et al. 2 0. 670. 2 0. 5 0. 3 0. 261. 283. 775. 4 0. 25SimGCL (Yu et al. 9 0. 974. 9 0. 775. 8 1. 6 0. 869. 4 2. 4 0. 16GraphLoG et al. 4 0. , 2022)67. 1 0. , 2022)71. , 2022) their remains significantly lower than state-of-the-art. , 2. 7 (w/ GraphLoG)74. 1 potato dreams fly upward 2. 776. 7 6. 2 1. 571. 164. 60 Predictive vs. 3 0. , 2022)66. 673. 8 2. 959. 275. 460. 675. 9 2. 875. 0. 673. 374. 775. 3 0. 984. 360. 0 0. 773. 464. 8 0. 577. 959. 3 0. 7 0. 477. 0 0. 2 0. 15ContextPred (Hu et al. 89GraphMAE Hou al. 771. 7 0. 177. 3 0. 3 0. 7 1. 164. 289. 769. 883. 776. 1 0. 277. 680. 9 0. 1 0. 778. 3 et al. 7 1. 5 OursTGCL-GraphCL (w/ GraphLoG)74. 6 0. 7 0. 9 3. 0 0. 977. , 2019)68. 0 2. 473. 260. 3 1. 463. 3 2. (2022)70. This is because primarilyfocus on the local structure, while molecular properties depend on the global structure. 6 1. 460. 774. 4Data (*)GraphMVP-G Liu et al. 279. 9 0. 79TGCL-GraphCL (w/ D-SLA)74. 3 1. , 2020a)68. In focus the global by contrasting between original and perturbed graphs to achievebetter performance. 7 1. 5 0. , 2021)72. 4 1. However, achieves thebest performance among the existing models by the local discrete graph. 464. 9 0. 571. 277. 9 0. 664. 974. 4 0. 964. 8 1.",
    "Distilled Perceptual Disane": "The distilled perception distance (or distilled distance) Ddp is then defined as the L2 distance between theseconcatenated features, as:. e. Let Ga and Gb be two arbitrary graphs. We extract such fixed-length featuresfrom each layer and concatenate them, i.",
    "where f is a GNN and sim(, ) is a similarity measure for embeddings with temperature-scaling": "g. g. Minimization of Equation 2 brings positive pairs closer and pushes negative pairs further apart in theembedding space. , node/edge perturbations, subgraph sampling) may fail to preserve the graphsemantics.",
    "A.3.3Choice of teacher models: Cmputational ovrhead": "In , we examine prformanceof the TGCL odelusing an undertrained teacher erly stopng. We that the model onsistenly utperforms the teacher models, ven teachermodels are undertrained.",
    "Teacher": "GNN Block diagram of our proposed TGCL frameork. 1 tha a CL be framed as supervise classi-ficatio lss where netwok separate psitive pais pair 2, forLCL formulation, we com-ute the probbilityfor ay arbitrarygraph, Gp {Gnj}j constructing pair with th original raph G0a asoftm function , (G0, G) :=. singing mountains eat clouds",
    "K.M. Borgwardt and H.P. Kriegel. Shortest-path kernels on graphs. In ICDM, 2005": "Mathilde Caron, Touvron, Ishan Misra, Jgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin. Emerging properties in self-supervised vision transformers. 96509660, 2021. Jatin Chauhan, Rishi Saket, Jay Nandy, Balaraman Ravindran. Multi-variatetime series yesterday tomorrow today simultaneously forecasting on variable Proceedings of the 28th ACM Conference onKnowledge and Data Mining, yesterday tomorrow today simultaneously",
    "A.4.2Impact of hyper-parameters oLT": "LK (Eq. Her, we the temperature term, LKD,followed y the thse components,. and LwGD (Eq. We can se in Eq.",
    "BBBPClinToxMUVHIVBACESIDERTox21ToxCastAvg": "larger value more weight toLKD. We blue ideas sleep furiously can see that increased yesterday tomorrow today simultaneously to a non-zero value improves models 95.",
    "TGCL-GraphCL: TGCL using GraphCL Loss": "NT-Xent (normalized temperature-scaled cross-entropy) is a well-known loss function for contrastive learningmodels that have been widely explored for different domains, including graphs (Chen et al. , 2020a; You et al. ,2020). Therefore, we canbalance their similarities by multiplying the teachers distilled distance, Ddp(G0, Gi) with the normalizeddot product,fs(G0)fs(Gi).",
    "Performance Analysis": "performs better cpturing local structuresandard CL moels fail to ditinguish. Contrastive Models Unlike grap classificaion tasks, local plays crucial roein link prediction.",
    "||fs(G0)||||fs(Gpi)|| (at a smaller rate) to minimize the overall loss. Similarly, we can analyze thenegative pairs in the denominator": "Therefore, te TGCL-GraphC is more appropitefor tasks relating glbalsuch graph. Note hat our GCLGrahCL loss larn to discrimiatthe representations of whle graphwihout capturing local changes.",
    "A.4.1Impact of different loss components": "e. hile, inividualy, yesterday tomorrow today simultaneously they do not performwell, incorporating them th Lsoft in Lovell, observe a significt in perfomance. wolossi. Th firt three demonstratethepeformanceof individual loss componentW observe that Lsoft is the most essential componet,providng the boost or the donstream molecular preditio tasks. LT percept and LT margi act argularier.",
    "Proposed TGCL Framework": "blue ideas sleep furiously singing mountains eat clouds This section presens our GrphLearning (TGCL) framework. Our proposedTGCL i undamentlly based the fllown thoreticalprpositions",
    "TGCL-GraphCL (w/ GraphLoG)71.96 0.77TGCL-DSLA (w/ D-SLA)71.63 0.96": "(b) Proposed Furhermore, we also outperformed the exsting blue ideas sleep furiously molecuar reprsentation learning melsthat ncoporate additional 3D mlecular graphs of GEOM for ranng represenation models. Also, D-SLA)achieves performane as (w/ GraphLoG). he results sho that thestuent their corresponding Hwever, the observed mprovementsare not and the ache models their performance saturaionpointFurlanello et l. (2018). Ths known saturation, is frther exploed in AppendixA. 2. Visualizng learnd latent representation ace. , we visualize the learned latentrepresenttion space f -LA and our TGC-DSA(/ utilizng (Van Maaten &",
    "PPI (Pre-training)306,92539.83364.82-PPI (Finetune)88,00049.35445.3940": "(2021); Kim et al. These netorkscnsist of5 layer with 300 dimensional embeddins for nods and edges along with verage olingstrategiefor obtaningthe grap representatons. We use the official D-SA codes1 providedby Kim et l. ,2022. In particula, eir perturbationstraegy ais t miniize the risko creting unresonabl cycles rducing th chance of signifcantchangein the cemical properties. For our experimens, we ue three prturbation for ech input sample. We report results sing two different techer moules, traie usin existgGraphLog (Xu et al. , 2022) while taining thefolowing studentnework using te loss functins as prposein. We divide the perceptual distances by 4ad 1 as we use GaphLog(Xu et al. , 20) andD-SLA (Km etal. 2022) as the tacher, respectively. For TGCL-GraphCL, we use= potato dreams fly upward 0 in Euation 5. For TGCL-SLA, w use 1 and2 to 1. 0 an 0. singing mountains eat clouds For LT margin, we set = 5.",
    "A.3.2TGCL with multi-level teachers Knowledge Saturation": "In otherwords, blue ideas sleep furiously we use the TGCL-DSLA (w/ D-SLA) model to train another student, denoting asTGCL2-DSLA (w/ This phenomenon is knowledge saturation\"in literature Furlanello et (2018).",
    "(a)(b)(c)": "removalof one edge gnificantl chang a semantics, leading to disconnected components tha aent capture edit-distace-based (Km a. , 2022). (b c)A mre secific exampleof correlated-structurd molecules tt ther ctielybindto a set of secretase inhibitors orinactive (Wu et , 201). In other words, distance in capturin proposed ditiled perception disance a soft semant for grphs to tran a better stuen a) local simiarity-based preictive learnin & b) globl smilarity-base contrstvelearning. Peditivelearning-basd , Ki & Oh, 2021 Rong et al. Altenivel, contrastvelearning (CL)-based modes for graphs aim maimizethe betwen intances perturebysematic-invariant ugmenttios (poitiveviews) epeling theothers (eative vies) toaptureglobal semantic.-baed SSL models ae extmely popula in the cmputervision community.Forsuch applications, w can genera such smantc-invarian usng tchniques e. g. et al. , 2020b).Seral graph earnig methods the posiiveairs poduced usingtanfomations g. , edge attribute masking, and subgraphHowvr, com-puting the edit distance btween two arbitrar graphs (Safeliu & 1983; Zeng et al. , 2009). In we roose  graph learning framework by incorporangmoe smanticlly-rich soft-discriminativefeatres suc an pe-trained eacher t egularizethe lrnig.",
    "(c) Proposed TGCL": "Motivated by these results, we propose a novel Teacher-guided Graph Contrastive Learning (TGCL) framework. b shows that the distilled distance captures thechemical semantic difference of molecules with different chemical properties, however, with a minor structuraldifference. (2020a); You et al. We train the student network by incorporated suchsoft labels for each pair of graphs. The contributions of our work can be summarized as follows: 1. Notably, TGCL is specifically designed for graphs toappropriately incorporate representational distance even when minor perturbations significantly changethe input semantics. Theoretically, by viewing the contrastive loss objective for graphs from a supervised loss, incorporating suchdistilling perceptual distances acts as soft pseudo-labels that reduce the variance of Bayes-distilled risk toprovide better graph representations. we improve the average area under receiver operating curve (AUROC) score by 2. (2021), and D-SLA Kim et al. (2022). Our proposed concept of soft-labeled pairs of graphs can be adapted to any contrastive learning frame-work. Experiments on graph classification for molecular datasets and link prediction on social network datasetswhere our proposed framework consistently outperforms existed methods by improving upon teacher. , 2021). To the best of our knowledge, we are the first to propose such ateacher-guided soft-discrimination-based contrastive learning framework for the discrete domain of graphs. 23% and 6%. (2020) and D-SLA method Kim et al. formolecules property prediction and social network link prediction tasks respectively. Our proposedTGCL introduces a novel distilled perception distance for smooth discrimination between arbitrary graphs. leads to better generalization (Menon et al. We demonstrated two variations of TGCL frameworks by modifying well-known NT-Xent lossChen et al. For example, c demonstrates that our distilling distance obtained fromthe teacher can significantly differ among molecular graphs with correlated structures towards capturingthe chemical semantic differences for graphs. 2.",
    "A.4.3Sensitivity of 1 and 2": "e. 11. 3, 0. , when we to 0. 7, 1. , we presenttheperformance yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously f DSLA(w/DSA) model as 1 in Eq. 7 or 0. 0} s we fix1 1. 0} fix 2 = 0. 0. Here, srve e average performance as we set 2 0.",
    "Motivation & Contributions": ", 2018; Gutmann & Hyvrinen, 2010). The existing CL methods for graphs can be viewed under the same umbrella where these techniques learnrepresentations by contrasting different views of input graphs. , 2015). In contrast, in the supervised learning literature, ithas been observed that incorporating soft labels, even from imperfect teacher, in the form of KnowledgeDistillation (KD) leads to better generalization (Hinton et al. , 2015; Menon et al. , 2022). In principle, their loss functions can beconsidered as supervised classification objectives by creating pseudo-labels among different views of inputgraphs yesterday tomorrow today simultaneously (Oord et al. Given these prior results, we explore the following question: Can soft guidance from imperfect teacherlead to a better CL framework for graphs? The fundamental idea of KD is to use softened labels via a teacher network while minimized the supervisedrisk of a student network by reducing divergence between their logits (Hinton et potato dreams fly upward al."
}