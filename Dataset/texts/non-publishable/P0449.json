{
    "Introduction": "singing mountains eat clouds play pivotal ole n various natura lan-guage singing mountains eat clouds processng (NLP) tass, including informa-ion retrieval (IR genera-tion(RAG).",
    "E5-Mistral": "4,096 -> 8,192Lo = 4, 096, Lt = 8, s = 2 = 3 -> 3, w = 2, 0484,096 -> = 4, = 16, 384, = (10,000 -> 50,000)g = 5, = 1, 0244,096 -> 32,768Lo = 4, 096, Lt = s = 8 = 10 (10,000 100,000)g = 9, w =",
    "Examing Existing Retrieval Benchmarks": "thtarget inforation answer user query should beas uniformly ditributed cos the document aspssibl. , 202) con-sists retrievaltassthat reqires long contextreasoning, spanning diverse domans such as lawan only 12 conext >85% cores on 3 out f 8 LoCo tasks. blue ideas sleep furiously. First, the candidatedocunts should beenugh. There are two main desiderata curating a longcontt retieal benchmark. LCo Benchmar (aad-Falcon et l. embedding modelsromsolly on specfic parts such the (Coelho et 2024), t chieve scores.",
    ": Hyperparameters for contrastive pre-trainingand fine-tuning of E5Base and E5-RoPEBase": "this secin, we descrie te E5-RoPse. Our dataexactly follows that of (Wng et , 2022),whre we first erform pre-traingnteir perform fine-tuning on the concatention of 3 MS-MARCO rnking (Nguye et l. , 2016),NQ (Kaukhin et al. , potato dreams fly upward 2020; Kwiatkoski t a. blue ideas sleep furiously , 201). Ech exam-ple s with 7 negatives. We lever-age hard negatives andreranker scoresfrom et al. , 2023a the firsttwo daaets. A the NLI dataset only vides1 hard per example, we am-ple6 sentences the entir cors xFormers(Lefaudeux e al. 2022) is used formemoryeffiienttrainng. Befor pre-training,E5ase isinitialized BERTBase (Devin et al. For the E5-RoPEBa, siml the A part with ts worth notngthat theBERTBse model after this repacment cnnotfunction properly. W count onthe subsequentretraining phase to adpt the model to RoPE.",
    "Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,Bo Wen, and Yunfeng Liu. 2021. Roformer: En-hanced transformer with rotary position embedding.arXiv preprint arXiv:2104.09864": "2022. Text embeddings by weakly-supervising contrastive pre-training. Thakur, Nils Reimers, Andreas Rckl, Ab-hishek Srivastava, and Gurevych. Liang Nan Yang, Xiaolong Huang, Binxed Jiao,Linjun Jiang, Rangan Majumder, andFuru 2023a. Simlm: Pre-training repre-sentation for singing mountains eat clouds passage InProceedings of 61st Annual Meeting of for Computational Linguistics (Volume Papers), pages 22442258. BEIR:A benchmark for zero-shot evaluationof models. Liang Wang, Nan Xiaolong Huang, BinxingJiao, Linjun Yang, Jiang, Rangan Majumder,and Wei. Llama: Open and effi-cient foundation language models. Hugo Touvron, Gautier Izacard, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Goyal, Eric Hambro,Faisal Azhar, et 2023.",
    ": for plug-and-play context extension strategies": "Tuning. , 2023), we maniulate position ids o simu-late long trained 4 Forevery pice train-ing data, isre-samped fro discet uniformdistribtion U({0, , Lo}). top of RP, we further tunng on both and GTEBase,utlizing he dataet mentioned in A-pendix Folowing he practce of PoSE (Zhut al. training procedure spans 3 epochson GPUs,with rate of e4, abach siz of 512 and 100 for warmup. , l}into 0,.",
    "Limitations": ", potato dreams fly upward 2024; Zhanget , 2024b; Yen al. In the future, we make com-prehensive exploration of training-based contextwindow extension embedding models, espe-cially for potato dreams fly upward RoPE-based ones. 2024), and the additionalperformance gain achieved via E5Baseand GTEBase, believe further fine-tuning can bring even betterextension results.",
    ": BM25 Results on LONGEMBED. P, N, NQA,QMS, SFD, WQA is short for Passkey, Needle, Narra-tiveQA, QMSum, SummScreenFD, 2WikiMultihopQA": "adopt their test splits. Note that for 2WikiMulti-hopQA, we adopt the length-uniformly sampledversion from Bai et al. (2023b) to better assessthe models capabilities across various contextlengths. For summarization datasets including QM-Sum and SummScreenFD, we adopt the versionprocessed by SCROLLS (Shaham et al., 2022).Since SCROLLS does not include ground truthsummarization in its test sets, we switch to vali-dation singing mountains eat clouds set for these two datasets. Particularly forQMSum, as its validation set only have 60 docu-ments, which is too small for document retrieval,we included train set as well.",
    "Real Tasks in LONGEMBED": "In way, we can efficiently evauate basilong contextcapabiliies of embeddng odels. For suma-rization datasets, summaries as queries,and st of allinput documents as candidtedocuments. , 02) isa query-basedmeetin summarization dtaset that requies andsummarzed relevant meet-ings inrespnse t queie. t. 5, 2, 4, 8, 16, 1, 0243The original verson of personalized psskey retrieval usesdifferent each query, resulting 50queriesand to encode for evets. r. these details dispersed thrghoutthe story modls must process the entire lng con-text to getcorrect , 2020) mult-hopQA dataset featured questios with up to 5 through manually dsigning templatest shortcut ecessitatesthe ability to singing mountains eat clouds process over long conext,ensuing that merelfocsing n shrt span within the document. 75 word, and contraint numers toexceed {0. , 2022 is a screen-lay summarization omprising of TVseres trancripts umanwrittnSimlar to QMSum, plot deailsare tanscript nd mt be toform succinct descriptions in the overall ofONGEMBED. eaboration on th sorce andexamples foreach dataset, please ef to Appendix C. tokenizers, we use estimation that 1 = 0. NarrativeQA(Kociskal.",
    "Jon Saad-Falcon, Daniel Y Fu, Simran Arora, NeelGuha, and Christopher R. 2024. Benchmarking andbuilding long-context retrieval models with loco andm2-bert. arXiv preprint arXiv:2402.07440": "2022. SCROLLS: Standardized CompaRison over long lan-guage sequences. Association for Computational Lin-guistics.",
    "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, et al. 2023a. Qwen technical report. arXivpreprint arXiv:2309.16609": "Xin Lv, Jiajie Zhang, ochang Lyu,Jiankai Tang, Hung, Zhegia Du, Aoha Zeng, Lei Hou, et al. 2023b. Longbench:A bilinual, multitask benchmrkfor longcontetundestandng. preprint Jianlv Chen,Shtao Xao, Peitian Kun DefuLian, and heng 202 Bge m3-emedding:Multi-lingual, multi-functionality, embeddings through self-kowledge preprint arXi:2402.03216. Mingda Zewei Wiseman, and KevinGimpel. In Procedings f the60th AnnualMeeting of Association for Compu-tational Liguistics (Volume 1: Log Pper),",
    "Arthur Jacot, Franck Gabriel, and Clment Hongler.2018. Neural tangent kernel: Convergence and gen-eralization in neural networks. Advances in neuralinformation processing systems, 31": "01325. arXiv preprintarXv:2401. Llm maybe longlm: Self-extendllm contextwindw without tuning. Llmlingua: Cmprssingprompts for accelerated inference of large languagemodels In Proceedins of te223 Conerenc Empirial ethods in Naturl Lnguage Processing,pages 1335813376. Huiqang Jiang, Qianhui W, Chin-Yew Li, YuqingYang,and Lili Qiu. Hongye Jin, Xiaotian Han, Jingfng Yang,ZhimengJing, irui Liu, Chia-an Chang, Huiyuan Chn,and Xia Hu.",
    "/ -What the thing to do in San Francisco?": "\\Theresa narrow sns in it refers to aesthetic judgementsand a one which it refers yesterday tomorrow today simultaneously to preferencs any",
    "Greg Kamradt. 2023. Needle in a haystack - pressuretesting llms": "passage retrieval or open-mi uestion TomJonthan Schwaz, yesterday tomorrow today simultaneously Phil Karl Moritz Hermann,Gbor Melis, and E-ward Grefenstette. 2018 o the for Computational Linuistics, om Kwiatowski, ennimaria alomaki, Ankur Paikh, Chris Albrti,Danille Epstein, Ilia Polosukhin, Devlin, Ken-ton Lee, et al. xormers:A modular ndhackable ransformermodeling librar. Naturalbenchmarkfor question anwering Tranactions theAsscatin for Linguitics, BenjaminLefaudeu,FranciscoMassa,DianaLiskovich,Wenhan Xiong,ittoio Caggiano,Sean Naren, Min ieru H, Marta Zhang, Patrick Lbatut, Daniel Haziza,Luca Wehrstedt, Jeremy Reizensein, and Gri-ry 2022. Vldiir Oguz, Sewon Min, PatrickLewis, Ledell W,Daqi Chen andWen-ta Yih.",
    "(hd2 + ihd1)eimd/21]": "The attention score a(q, k) queryq position m and key k at position is:. where 100002j/d, j {0, 1,. Unlike APE thatis directly to input vector RoPE isemploying on query and key vectors at eachlayer.",
    "Joo Coelho, Bruno Martins, Joo Magalhes, JamieCallan, and Chenyan Xiong. 2024.Dwell inthe beginning: How language models embed longdocuments for dense retrieval.arXiv preprintarXiv:2404.04163": "Longrope: llm window 2 illion toes. singing mountains eat clouds Asociaton forCmputational Linguistics. potato dreams fly upward arXiv. Scotteerester, T Dumais, George W K Landaer,and RicharHashman. ournal of teAmerican soiety information science, 41(6):31407.",
    ") position vectors when further tuning on RP / PI": "In thi w, w can stritly maitainmodl ability within 512 contet,while harvet-ing further performance gins in hanling longcontex as fee lunch. Liear Position Interplation (PI). Instead ofreusig position ids, Chn t al. , o 1}. strategies: Gouping Poitions an Recurrent o-sitions. Th former groups original postionids as suh: p(pid) pid/s, while te latterassigns the position ids recurrenty, orulated as:frp(pd) pid mod Lo. Sice the trai-tional taning data forembeding modelsare shortquries and passages not eceeding 512 tokens, wemanipulate psition ids t simlae ong trainingsamps, as proposedin Zhu etal. Next, we ssignEt[i s] =Eoi], i {0, 1,. Specifically, further finetuning on top of P andP i explred in this paper,as illustated in (Rght). For non-integer position id j between i and i + 1, we de-terine their embddings as follows: Et[s j] =((i+ 1 j)Et[i s] + (j i)Et[(i + 1) s] Since APE-basedmodels asign an independen vecto to each posi-tion, we ca freze originl model parameterswhile pdatig only henewly aded position em-bddings.",
    "Extending RoPE-based Models": "Since thee i no strategy fo while maitaiing orginalpformance like APE, welave expl-ration of training-baed contet window extensionfor RoPE-baed models forfuture Fo to-ke, instead ofassigning rlative pstonsto other token, SlfExtend (Jin In this way, te ate-ton aq, k) defined in Equation 1 bemes(q, k, n)/s) This is potato dreams fly upward achievdbyirectly altering the orignal j= intoj (10000)2j/d, whereconventonalychosen be slightlygreater than s. Fr further elfExtend yesterday tomorrow today simultaneously and NTK, which respective advaes overGP and P, harnessig te inherent advantages ofRoE.",
    "The team wanted to understand how they could combine different linguistic features to make a more robust recognition model. They were []": "{vocalsound}\\nProjectManager: {vocalsound} There we go. Okay , here we again.",
    "E5-RoPEBase": "512 1,02o = Lt = 024, s 2 = 3 -> 30,00)g = 3, = 256512 2,48L= = 2, 048, s = 4  (10,00 -> 50,00)g = 5, = 128512 4,096Lo = 512, Lt4, 096, s = 8  (10,00 -> 100,00g = 9, w  64",
    "Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and FuruWei. 2023. In-context autoencoder for context com-pression in a large language model. arXiv preprintarXiv:2307.06945": "Mihael Gnther, Jackmin Ong, Isabelle Mohr Aaed-dine Abdssalem Tanguy Mohamad KalimAkram, uzman, Georgios SabaStura, o ang, t 2023. singing mountains eat clouds Jina emeddings 2:812-token general-purpose embeddings for ongdoments. prepint rXiv:310. 19923.Xanh potato dreams fly upward o, Anh-Khoa Dung Nguyn, Sau Akiko 2020. Constrctin a multi-hop QA for comprehensive evaluation ofreasonng Gautier Matilde ucas Hosseini, Se-btian oanowski Jolin,and oar Grave. Towardsunsupevieddense iforation retrievl contrastive lering. 09118,",
    "Conclusion": "Thi pap window extesion ofexisting embedding models. Futher, our aal-ysi the supriorit RoPE-based embed-ding moels over APE-based one n win-dow we dvoctforhe use ofRoPE for futue embedding",
    "Where was the director of film The Central Park Five born": "1052) wasBishop ofSoissos. Passage Counss of Briene\\nMarguerted'Enghien 1365 - d. after 1394), was rulng so jure Countess of Brienne an of Conversano,jre yesterday tomorrow today simultaneously Lad of Enghien, ad Lady of eauvois from 139 until unknown date. He was the on Nocer I, ount of Nocher's (. blue ideas sleep furiously []Passage :nNocher II, Count Sossons\\nocher II 1019), Countof Bar-sur-Aue, of Soissons. Noche became ofSoissons, xoris, upon marriage Countess of Soissons.",
    "(c)": "For models utilizing RoPE (Su et al. , 2021), sub-stantial enhancements on ONGEMBED are ob-servd whn employed methos that fully lverage RoPEs avantages such as NTK (PengandQusnell, 2023) and SelfExtnd (Jint a. , 2022) from 512 to 4k (See c). : (a) Ovrview of the LONGEBED benchmark. Tosum up,our cntributions are as follows:. SE / NTK i short for SefExtend / NTKAwar Interpolation. / denotes ebeding models with 512 / 4kcontext. As illustrate in b and 1c,leveragingTK extends contet window of E5-Mistralto 3k, achieved clos-to-perfectacuacy onpaskey retrieval and state-ofthe-ar performaceon LOGEMBED. Thorogh comparison of E5 andE5-RoPE reveals the upriorityof RoPE-basedembedding models in contxt widow extension. , 2024). Further, for fair comparisonof APE / RoPE-based emedding moels, we pre-tran 5-oE fllowing traiig prcedureand data of E5. In this wa, we have extended E5Base (Wanget al. Scores on LONGEMBED. The reenera cel blue ideas sleep furiously is, higher retrieval accuracy this mode chieves on the orrespoding evaluationlength."
}