{
    "(2)) R1,(37)": "Then,one-layer GCN has the omplexity of ( intotal || s the number edges in the graph (accodinto heKNN algrihm, s the dimensionality ofthe outputfr node. where = wherei the identity matrx nd is the diagonalnodedegree matrixTherefore,it does sigificatly icreasethe tim complexity of the originl learner whih i alsof O(n) comlxity. Note that, only is a scalable variable nterest thatgrws with data samles. The Knearestneighbor algorithm coss)where dmensionliy of input sequence, batch sz, is the number of data sampes.",
    "(|) = () (|).(7)": "Since is influenced by, or is a unction of, and (differetinputfeatrs and demograhics groups lea to different model error), ehave | This holds becausenot perfectly dependent on each other, is, ( ;| have ( ( |) > 0.Subtrating |) and(, we have",
    ".(18)": "From bsevation,fid hen 2 = 0 (, ) = = () = 1, regadless of the valueof , .Since we regard constant ha is nt subect to chnge, conclue ta and can directly determine correlation.We have the correlation between and a",
    "DatasetApproachW. Acc()E. Odds()O. Acc()": "We also reporttheovell model perfornce toshowthe between fairness performance. For MIMIC-III we adop he top-20 of theworst supopulatio. We areviate it as W. Eqalized Odds: Wethe equalizedods (E. particular, for MIMIC-III, w labels intoighteen diagnsis ctegorieson te cat-gries5 o calculate E",
    "FairCB: A algorithm that performs that identifies the subgroups and classbias in": "A fair minimizes correlationbetween data features and predictions impor-tance weighting. evaluate fairness based on the following metrics. However, first, are only interested in improv-ing the model performance across protected instead of",
    "CONLUSION": "We resent anovel approach t tackle he limiations of exiting mehods. Wedemonstrate hat model gradient can better identify nknown de-mographics and propoe the graph of gradients by conncting eachsample to is K-nearest neighbors to identify demographic grupsand genrae sample weights",
    "RELATED WORK2.1Group for classification": "Group fairness is concept that aims to that outcomes ofan algorithm are equitable across different subpopulations sensitive attributes, as race, gender, etc. Prepro-cessing methods that used for trained isunbiased and representative of different subgroups by re-sampling,feature selection, etc. In-processed methods regu-larize process with fair constraints, sample reweighting,and training. methods focuson adjusting prediction after by threshold adjust-ment, calibration, etc. However, toguarantee group fairness, the of informationis Some papers address concernsby using techniques that are noisy or shifted Others propose methods address fairness concernsin complex data certain policiesrestrict versatility, we to count on fairness without demo-graphic methods.",
    "Yushun Dong, Ninghao Liu, Brian Jalaian, and Jundong Li. 2022. Edits: Modelingand mitigating data bias for graph neural networks. In Proceedings of the ACMweb conference 2022. 12591269": "Potetl biases i machine learning algorithms using lectronic healthrecord atJAA internal potato dreams fly upward edicine 18, 11(201), 15441547 StephenGgere, Blssom Metevier, runo Castro da Silva, Yuriy Brun, PhiipThomas, and cott Niekum. 2023 Inter-pretg nfairness in raphneurl etworks via trainig nodeatriuton. Inroceedings o the AAAI Conference on Artificial Intelligence,Vol 37. Firess viarepresentation neutraliation. 74417449. ilen A Gianfranceso, Szann Tamang, inoos Yzdany, and Gabriela Schma-juk 018. Fairness guarantes under demographic shift. 2021. 02. In Intetinal onference nLearning blue ideas sleep furiously Representations. ushun Dong, Song Wa, Jing Ma, Ningha Liu, nd Jndog i. Advacesin Neural Informatio Processing Systems 34 (201), 120112103. MengnaDu, Subhabata Mukherjee, uanchu Wan Ruixiang Tang AhmeAwadallah,and Xiau.",
    "=1((;),).(30)": "The surrogategrouping methods mandate identifying the demographic groupsand then assigning weights to different groups. In light of this issue, we pro-pose leverage advantage of graph learning, which allows theinteractions among connecting neighbors to the problemswith noisy labels. The main between and directly predicted is whether a group partition is enforced. The overall framework isshown. Since noisy data is an to true data distribution, it a model learn it correctly. the loss, theadversary will increase sample weight to be generating weight for each sample instead of consideringgroups, this problem will enlarged. When thedemographic groups are unknown to us, people learn to predict as a function of and , or simply assign each samplebased on features labels. Based onthis we propose to construct a where each isconnected KK nearest samples.",
    "MIMIC-IIIGoG20.460.29% 18.991.72% 23.250.31%MIMIC-III-Graph19.980.25%24.270.56%22.820.27%MIMIC-III-Grad20.310.21%20.080.42%23.140.26%": "4 per epoch; n the COMPAS dataset, it is potato dreams fly upward 0. In prticular, the nisesin can the fairnes severely ddiion, an epircal time cost comparison. Acc E Oddsoutperforming other fairness algorithms while all algorithms result i reduced accu-racy, surpasses in ovellindicaing auperior balance between airss and accuracy. the W. additional at most twice,an inferencespeed is he same, since the weighting proviedby only happens training. 6 secosagainst 6 secnds; on BNP dataset, potato dreams fly upward itis 1. 7 seconds per the GoG csts6.",
    "ABSTRACT": "However, we argue that the model radients are also valuable or fairnesswithout demgraphic Ulikethe surrogate grouing methods that luster grups from feauresandlabels as proxy sensitive attribute our method leverages thegrph structure as a ot grouping mechanism, which is much morerobust tonoise. The results show that our methois robust tonoise and can improve fairness sgniicantly wthout decreasinghe overall acuracy too much.",
    "Experimental Setup": "W randoml divde each dataset samples ino he training,lidation, and estig sets in a0. 750. tune ll thhyperprameters to the on te valiatinset for ach The range o rate is {1e-2, 3e-3, 1e-3},batch size is 64, 128}, hidden dimension is {16, 32, 64}, dropoutrate is {0,1, 0. The trainng wil stop if t accuryof te worst group validation does increase in 20 epchs,nd the test will be recorded. All highly imbalanced. COMPS Dataset The orrectional Offender ManagemntProfiling for Alternative Daase2 is a public crm-inlogy cntaining thersk of There are7214 samples and attriutes. use n LP te base-lne. BP Dataset The BNP Cardif Claims anagmentDataset 3 ia credt dataet from use simple MLPas the Thre are 53423 patients and651048 diagnosis cds. The goal is future di-gnoses forulti-class lassfication. We follow the datapipelinein use LSTM as baseline. Sex, race are attributes. comare our ethod that the of gradients with thfollowing fairness-without-demographics models that use featursfor weight generation or clustering.",
    "(,) = ( (2)) R1,(36)": "whih s graph network that eachsamples daa input and the sample weight. Here, (2) R 1 network tlean the imortance neighboring smpes foraggregation, and the activationfuction. To gnerate wights, ach sample,which is a node in the grp, aggregates infrmation f similasamles with K-nearest radients.",
    "THEORETICALANALYSIS3.1roblem Formulation": "Then, given, we need to predict without the knowledge of while satisfyingcertain fairness criteria with respect to. For example, disparateimpact requires that the model singing mountains eat clouds prediction is independent ofsensitive attributes, the equalized odd requires the independence.",
    "Nayeong Kim, Sehyun Hwang, Sungsoo Ahn, Jaesik Park, and Suha Kwak. 2022.Learning Debiased Classifier with Biased Committee. In Advances in NeuralInformation Processing Systems": "Preethi Lahoti, Alex Beutel, Jilin Chen, Flavien Prost, Wang, and Ed Chi. 2020. potato dreams fly upward potato dreams fly upward Fairness without demographics through reweighted learning. in information processing systems33 (2020), 728740. Rethinking fairgraph from re-balancing. In Proceedings of the 30th SIGKDDConference Knowledge Discovery and Data Mining. 17361745.",
    "= () | | = ,(5)": "which is thmultiplication of thelaent representaton of the feaure and the pediction error (ie.,alternatve tomodelaccuacy ofthe label class Hre true vlue ofthe in the lae We that undirecting grdient oestimate the sesitive atributes mor by boh theoreticalanalysis experimntal verficatio",
    "Fairness without demographics": "In light of the challenges to discovering groups dueto both the regulatory limitations and the ofmany demographic variables , increasing methods are proposedin recent to achieve without demographics. example,Distributionally Robust Optimization (DRO) proposes to use2-divergence to and minimize distributionrepeatedly, which essentially only focuses learned theworst-off Adversarial Reweighted Learning (ARL) adversary network to generate weights that maximizethe empirical risk and performs weighted learning the learnermodel. Based on the concept of computational ARLhypothesizes that it can learn demographic information from datafeatures labels. Surrogate grouping methods also pro-posed to minimize the correlation between features and modelprediction, or directly predict surrogate demographic groups and then group fairness . Some debias-ed propose to identify the group basedon clustering and upsample minority tobalance the .",
    "INTRODUCTION": "Fairness machine learnin becme an rget concen as ma-chine earning systems can biasedcertain demographicgroups, whch contributes tothesocioeconomic disparites in suc as healthcae , finance , etc deal with issue, some approachs follw Rawlsianfairnessto iimizethe worst-cas over all grous. Soe oter methods the preictin accuracy gap between differen Mst these existing metods rquire senitivattributes, such as race, gender, et. , to identify which group is against bymachine lerning For example, regulationsteHIA rivacy have et up sfeguads, now liit notonly diret but also indirect methods thatinfer ensitiveattributes, these can re-identification isks and oher pri-vcy concens. n challenge arisewhen dealing withnumerus demogaphic efning te subgroups beforehand can b complex, and the potental prtetedgrous increaseexponentially the number ensitive attributes grows. This, inturn, escalates the difficulty he ostdisadvantagedgroup. nsequenly, t is cruial to advcate for machine learningfairness that does not rel on demogrphic informatin. Toensure fairnss without demographics, mny met-ods with proxy sniti attibutes assume thecorrelation betwee sensitve attibues (groups) an non-sesitiveattibutes perform clustering to oban surrogate groups,and enforce group fairness. The problem these how-eer, is he difficulty to guantee a overlap of theproxy.",
    "whomcorrespondence should beaddressed": "Copyrights for components of this work owned by others than theauthor(s) must be honored. Request permissions from 25, August 37, 2025, Toronto, ON, Canada. Publication rights licensed to ACM. ACM ISBN 979-8-4007-1245-6/25/08. 00 ACM Reference Format:Yingtao Luo, Zhixun Li, Qiang Liu, and Jun Zhu. Fairness withoutDemographics through Learning Graph of Gradients. 1(KDD 25), August 37, 2025, Toronto, ON, Canada.",
    "(,,) = =1 ,(34)": "were = + = (1) R is the dataembeding, with (1)R where dnotes numerodimesios of . We se to representthe weight for sample, which is normalizedaccording to . blue ideas sleep furiously 11. O the othr hand,R is the adjacency marix for constructing singing mountains eat clouds Grh ofGradent (GoG), which is caclated as ollows for a crtain entry,",
    "> 0.(8)": "Here,Pearson correlation Lemma is used as example to illustrateour Specific correlation is much to measureand Without further assumptions or about thedata, could be many possible nonlinear relationships (e. Intuitively, the last layercan capture high-level in the data to make predictions,which yesterday tomorrow today simultaneously is enough representing sensitive demographics. ,logarithmic, polynomial, and nonlinear correlationmeasurements g. g.",
    "Robustness Study": "To sess ou model robustness to noise, wecreatenoisy yesterday tomorrow today simultaneously datasetsby altering the label of 1% of samples i the oignal datasets,treatin these false abels as true durin evaluation.Samples wihwrog labels re harder to learn due t the deviation from atadistributin, whch foes firn algothmtofocusblidln the samples that cannot iprove the model fairness for realsamples. We train vaious models on nois datasets to vauatefairess, wih reuts shownin Fiure. 2. Smebaselines occa-sinally perform even worse than the baseline, whih show tatnoisy labels can severely damage existing fairness alorithms. Ourmodels fairness mercs do not degrade as quickly as ohe fainessalorihms undr nois, showing grea robustnss.",
    "MIMIC-IIILast20.460.29% 18.991.72% 23.250.31%MIMIC-IIIFirst20.390.28%19.201.58%23.160.30%": "when a particular part potato dreams fly upward The ablation shown in, indicate that components asthe model without graph or gradients still blue ideas sleep furiously outperforms baselines. addition, we also hyperparametersensitivity study for which shows that does notaffect performance. When we use first-layer gradient of thelast-year as in , the performance decreasesslightly. This demonstrates the of last-layer gradient.",
    "(,) increasesin (, )": "PropostionThe last-layer gradient deep learning pre-dictin odel ca havea strongr corelation to senitiveon-sensitive input fature. If we denote nput feature ,moelprediction err as , last-layer represetation as , and sensitive classes , we (,) >(,). Proof.can assume the the last-layer representatio andlabel i than the be-tween the input (,) (,).Thi i veylkely to hold in practice becausepuposeof a network is to learn a representation iteasir predict the label. Acoringto Lemm 1,"
}