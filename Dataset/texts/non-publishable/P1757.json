{
    "Differences w.r.t. Existing Approaches": "summarizes the key distinctions ypespherial speciicallyin termof their training-time los fuctionsand testtime scori functions BothSSD+ (Sewag al., 2021)(Su et l., 2022b) employ the SupCon (Khola et l., 2020) loss dringtrining,whch modellatent repesetations vMF SupCons loss does ntcorrespo to We shw empirically in tat prposedOD sore achieves performanceon different OOD detecton benchmarks,is uch more computtionally eficien by eliminatng the needfor KNN searches.",
    "Jie Ren, Liu, Abhijit Shreyas Padhy, and Balaji Lakshminarayanan. Asimple to mahalanobis distance for improving near-ood detection, 2021": "In roceeins of singing mountains eat clouds e37th International Confernce on Mchine Learningvolume 119 of Proceedings fMahine Learnng Research, 2020. T. Dtectng out-of-distribuion examples wih Gra potato dreams fly upward matrices. In The EleventhInternational Conference on Learning Repreentations, 2023. Scott, A. Understaning anomaly detectionwith deepinvtile ntworks through hierchie of disrbtions and features. In 2021 IEEE/CVF International onferece on Computer Visio (ICCV, 221. Jie Re, Jiming Luo, Yao Zhao, unan Krishna, Mhamma Saleh, Balaji Lakshmnarayanan, and Peter JLiu. R. von misesfisherloss: exploratio of embedded geometriesfor supervising learning. Chandramoul Sama Sasry and Sageev Oore. Out-of-stibution detectioad selectve generation for conditional languag moels. Robin Schirrmeister, Yuxuan Zhou Tonio Ball, and Dan Zha. In Advances in Neural InformationProcessing Systems, 2020. Moze. C Galagher, and M. C.",
    "BExperimental Details": "the ImgeN moe, we use the heckpont provided in Sunet al. 04. , 2018), Mahaanobis (Lee et al. We conduct our on NVIDIA 600 GPUs (48GB RAM). 1 addecays by a factor of 150, nd 18. (2020) for10epohs, wit an initial earning rate of 0. LTS a sytem and h Toolkit versi 11. ASH (Djurisicet al. framework. T initial is0. (220), with a 0. 01 cosine analing schedul. e trainte models fo 200 epochs on Fo the Imgeet benchmark, we adopt Torchvisins pre-tainedResNet-50 model with ImageNet-k weights. Software and hardwe. 2022), ReAct(Sun et al. temperae is set to e 0. Weuse Ubutu 22. 6 andcuDNN 8 9. , 200), ViM (Wan al. PyTorch 8. usng cross-entoy los such as MSP & ODN (Liang al. (022b). 9adeight decy14. traiing-imtemerature o be 0 1.",
    "j=1exp(f j(x)/),": "where f (x) denotes the logit crresponding to j. In contrast, our framework explcilmodels the la-tent represntatins in discriinatve classifieras hyperpherical whicheables arigorous interpretatin from lo-likelihood perspective(c. f. 5), thlassier f j(x) isequivalet to jz, wher isthe penutimate layer input x and jaswght vetor theembeddingoutut logit. Different fom Liu et al. . 00. 20 60. blue ideas sleep furiously 81. Test tempeature FPR@95 (%) 0.00 85. 86. 0 86. 87. 88. 5 yesterday tomorrow today simultaneously Avrage AUROC",
    "Preliminaries": "Le X ad Y = 1,. , C} represent the nput space and Ilbel space, resectvly Te joit ID disribution,represented s PXIYI,is a jont distribution defined vrX Y. During testing time,here are some unknownOOD joitdistributions DXOYO defined over X Yc where Yci hecompemntary set of Y.According to Fang t al. (2022), OD detecion canbe formally dfined asfollows:.",
    "Broader Impacts": "OOD detection can be used to improve the reliability of machine learning models, which can have a significantpositive impact on society. In situations where saving features is a concern, the singing mountains eat clouds use of the KNN score maynot be feasible as it requires access to a certain amount of labeled data. In contrast, our INK score does notrequire access to a pool of ID data. This makes it a valuable tool in various applications and industries wheresensitive information needs to be protected. It is important to note that the usefulness and effectiveness ofOOD detection may vary across different applications and domains, and careful evaluation and validation arenecessary before its deployment. We gratefully acknowledge the support from the AFOSR Young Investigator Program under award numberFA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 & IIS-2331669, Office ofNaval Research under grant number N00014-23-1-2643, Philanthropic Fund from SFF, and faculty researchawards/gifts from Google and Meta. Davide potato dreams fly upward Abati, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Latent Space Autoregression forNovelty Detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision andPattern Recognition, 2019.",
    ": Ablation on test-time temperatures. Theresults are averaged across the far-OOD test sets and 3random training runs, based on ResNet-34 trained onCIFAR-100": "12, which validatesthat the ntrinic likelhod score is sig-nificantly more effective for OOD dection. 1. Additionall, fr a ndrstandin, have includeda visualizatin of ID-OOD core distributions in. Weillustrate a comparison betwee and scoresImageNet (ID) potato dreams fly upward vs. The curve highligts that the value roughlymatches the training-time temperature. also matchs ur where under the sam emperature in teting allows recovering log-lielihood log p(z) (upto some constantlearned in traiing time.",
    ",(15)": "During loss reduces s(zi, y) for correct while increasingit for other classes. This be singing mountains eat clouds justified by the gradient of the loss function with to parameters , for an z and singing mountains eat clouds its associated class label y:",
    "Additional and Discussions": "In contrast, INK complexity O(C), the number of C is of ID sample To demonstrate this, we conduct an compare the average compute between KNN score and INK score. In this experiment, the performance of the model on CIFAR-10, and ImageNet-1k using defaultvalues of k in Sun al. Computational cost. KNN-basing methods utilizethe Faiss library al. The ofthe embedding pool is 50,000 for CIFAR-10 and CIFAR-100, 12K for ImageNet ( 1. , which is optimizing for nearest-neighbor search.",
    "Leitian Tao, Xuefeng Du, Jerry Zhu, and Yixuan Li. Non-parametric outlier synthesis. In The EleventhInternational Conference on Learning Representations, 2023": "doi: 10. 8769877 2018. 1109/CVP218. 00914. 2018 IEEECFConference on Vision and Pattern Recognition, pp. Uncertaity stmaion a sngle deepdeerministic neural ntwork. The inaturalst species dtset.",
    "where c Rd denotes the mean direction of the class c, 0 denotes the of c, and Zd() denotes the": "Beefts of the probabilistic model.We model the representations avMF becauseit simle and expressive probability distribution indirectioal statistis. Coparedwith stribution, istribution avoids estimating large coriane matrices for high-dimensionaldaa hat is sown to be cost andunstable lterture (Chen et al., 217). Meanwhile, chooed h MFdistribution us have the features live on theunit hyperphere, which to benefits. Forexample, fixed-orm are known to improve traiing stability inmacine learning, wheredot products are ubiquitoufor learning a od representation space.For reason,hyperspherical have ben populary adopting i recent contrastie learned literature al., 2020a; et al.,",
    "(17)": "The sample-wise intrinsic likelihood Sz) ID data s S(z) exp(s(z, c/), which is theterm y) with ground label. Hence, tainng overall idces higher intrinsic lkelihoodfor By further connecting our Theorm this higher intrinsic can directly translaeinto high () lg p(z) for trainigdistribution.",
    "Pascal etts, Elise van der Pol, and ees Snoek. yperspherical ntworks. in Processng Systems, 32, 2019": "Yifei Min, Ziyag Cai, Jiuiang Gu, Yiyou Su, Li, ad Yixuan L. Delving int outofdistributondeectin with vision-languag Advances in eural InformationProcsing Systes,2022. How toexploit yperspherical detection? In he Elventh International on Learning",
    "Introducion": "Thedesign of these scoring functions has emerged as critical cornerstone in achieving effective OOD detection. problem of out-of-distribution (OOD) detection has garnered significant attention in reliable yesterday tomorrow today simultaneously machinelearning. , 2016), especially in applications wherethe consequences of misidentifying OOD samples can be severe. While many OOD scoring functions have been proposed for classification models, they are eitherheuristic-driven and cannot be rigorously interpreted as log-likelihood, or impose strong assumption on thedensity function that can fail to hold in test time (Lee et al. , 2018b). However, a fundamental dilemmain OOD detection is that a discriminative classifier is typically trained to estimate the posterior probabilityp(y|z) for class y given an input z, but lacks the explicit likelihood estimation of p(z) needing for OODdetection.",
    "Published in Transactions on Machine Learning Research (12/2024)": "Following the seigs in et al. ViM generates addiional ogt from residul of feature against pinipal space. ReAct improves energyectiying activtions a an imit which is based onthe p-th prcentile of he yesterday tomorrow today simultaneously activatins n-ditributio (ID) data. ASH proposes tree heuristic-asedapproaches for activation saping: Puning, Binarizing,andScaling. DICE singing mountains eat clouds utilies logit sparsfcation tovanilla energy score,which is based te p-thpercenti the unit estimated on the ID KNN+ and KNN whic require selecting narestneighborsk. In line wth the OpenOOD weadopt method specifallwith a parameter setting of p = 9.",
    "Abstract": "The to detect out-of-disribution (OO) inputs is critial t guarnte reliabilityof models eploye in an envirnmet. A fundamenal chalenge OODdettion i that a disciminatve is typically tried oesimate the oteriorprobability p(yz) for cassy givn a inut but lcks the explicit likelihood stimationof p(z) ideally needed for OD detection. nmrous coring functins potato dreams fly upward poposed fr classfication modls, estimate scores are herstc-driven andcannot intepreted as likelihood. To the we ntrinsicLkelhood (IN), which to oern discriminative-based classifiers. pecifically, ou popsed INK score operates on theconstrained atentembeddings of potato dreams fly upward a iscriminaive casifier, hich re modeled a a of hpersphericalembedingswith ostant nom. e draw a nvl connection betwen and the intrinsic likelhood, which cn in modern neuralnetworks. Extnsive experiments on OpenOOD benchmark empirically esablshe anewstte-of-th-art in variety OOD detetonicluding bothar-ODand near-OOD.",
    "where samples with higher scores are classified as ID, and vice versa. The threshold is chosen based on theID score at a certain percentile (e.g., 95%)": "In Teorem 3. p(y = = However we acknowledge thiassumption ma not in realworld scenarios. To furthe demonstrate generality and effectieness of our we extend it accont non-unifomclass priors. We perform standard maimu likelihood estimatio on th trained dataset {(xi, yi)}Ni=1:.",
    "RQ: How to design an OOD scoring function that can offer rigorous likelihood interpretation to moderndiscriminative-based classifiers without resorting to separate generative models?": "our proposed INK scoreoperates on the constraining latent embeddings of a discriminative classifier, are modeled as a mixtureof embeddings with constant norm (see ). Theoretically, show that the score is equivalent to log-likelihood score log to some constant and thus rigorously a density-based for OOD detection. Though simple elegant hindsight, this was non-trivial. 2021). The overall evaluation notable of ourapproach in empirical and theoretical",
    "Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describingtextures in the wild. In IEEE Conference on Computer Vision and Pattern Recognition, 2014": "Mitigatingender bas iface recognition using blue ideas sleep furiously von ises-Fiser miur model. In Advances in NeuralInformatin ProcessingSyses, 2022. Pierre Colombo, Eduardo Dadato Cmara oms, Guillaume Staeran, Nathan Noiry, an Pablo PiantanidaBeynd mahaanobis distance for textualOD deection. In Proceedings of he 39thInernatioal Conference on Machne Learning, 202. JeanRy Conti, Nathan Noiry, Stephan Clemencon, incent espigel, and Stphane Gentric.",
    "KNN+ CIDER3.619 0.18543.61570.11836.1780 4.3871IN 0.0030.017 0.00070.2745 0.01450.745 0.0145": "This improvementunderscore the versatiliy of the IN core,proving its effectienesin near-OOD detection scenarios where ou-of-distribution samples ber a close semantic resembance to theID samples. We also evaluate the performance f our method in the mre hallenging near-OODscenario. 27% in FPR@95. As shwn in , Ndemosrates a copetiive performnce, outperforming CIDER by9. Thi dvancement also signifies prores i rdging th performancgap tween ypersphericl-basd singing mountains eat clouds methods an ross-entropy loss mthods. We evaluate the performance of our method on the CIAR benchmark. Near-OOD detection. Cnsistentwith the results observd in the ImageNtbenchmrk, the INK score displays competitive performace ihding both far-OOD and near-OOD rups, compared to th current stat-of-the-artmethods Due tospace constraits, furter dtails are inclued in Apend E. 84% in FPR@95.",
    "DID Classification Accuracy": "presets th in-distribution classification accurac fotraining dataset. 020). si vMF loss shows competitive ID classificaton accuracy compared  standard it does nocopromise the apability t samls betwen in-distributionclasses.",
    "Theorem 3.1 shos that or funcion Eq. 6 can be equivlent tothe log-likelhoodscre, OO detetion urposes": "The likelihood p(z) is probability of observing a given z under the can be calculated as a singing mountains eat clouds summation of class-conditional p(z y = j), yesterday tomorrow today simultaneously weighted by the class prior:. 3.",
    "g (x) = ID, if p(x) ; otherwise, g (x) = OOD.(1)": "The performance of OOD detection heavily on the estimation data p(x). However, learning, the classifiers directly estimate the posterior instead of likelihoodp(x), which makes likelihood-based detection Our focus discriminative differsfrom unsupervised OOD detection using generative-based models, which do offer classification capability.For completeness, we review studies extensively the work ().",
    "Vikash Sehwag, Mug Chiang, and Prateek Mittal. A frmework fo self-supervised outlierdetection. I Interational Conference onLearing": "Nez, JordiLuque. cmplexityand out-of-disriution deecion withgenerative models. In IEECVF Confernce blue ideas sleep furiously potato dreams fly upward nComputer Viion Pattern ecognition (CVP), 2022a.",
    "Hariprasath Govindarajan, Per Sidn, Roll, and Fredrik DINO a von mises-fisher The Eleventh International Conference on Learning 2023": "Why rel networks yield high-confieneprediions fa ay fromtheraini data and how to itigate In 2019 IEEE/CVConference on Vision Pattern Reconitio (CVPR), 2019. atthias Hein, MaksymJulian Bitterwolf. mises-fshermxture model-ased lernng: Appliation to arXi preprint arXiv:1706. Md Abul Julien Bohn, Milgram, Sphae anLimng Chen. 04264,2017.",
    "p(z)=p(z | y)Cj=1 p(z | y = j).(3)": "the connetion, thechallenge lies in ho to the class-conditional latent ditribution (z | y), wll toefficiently yesterday tomorrow today simultaneously optimize neual to achieve such latent istribution while training clasifier. In hatfollows, we addess these two allenges.",
    "Jingkang Yang, Zhou, Yixuan Li, and Ziwei Generalized out-of-distribution detection: A preprint 2021b": "Shuyag Yu, Junyua Hatao Zhangyag Wang, In The on Reprsenttions, 2023. Opnood v1. ariv preprin arXiv:2306. Jnsong Zhan, Qed Fu Xu Chen, Lun D, Zelin Gan Wang, iu, Shi Han, and DongmeiZhan IThe Eevent Internationl Conerence on Leared Reprsentatios,",
    "EResults on CIFAR Benchmark": "83% for the far-OOD and near-OOD groups. In section, we present additional and analysis on CIFAR-100 benchmarks. , 2016) serve as far-OOD 51% and 61. 51%), outperforming. , 2014), and et al. As shown in, INK shows strong performance on the far-OOD benchmark FPR 44.",
    "on Vision Transformer architecture": "64% and 48. 80%. This section details the performance evaluation of methods out-of-distribution (OOD) the (ViT) architecture, the ImageNet-1k dataset. approach involvesfine-tuning a pre-trained ViT-B-16 model over 100 INK demonstrates competitive performance both far-OOD potato dreams fly upward and near-OOD groups, rate of 38. As a note, while achieves competitive results, itdoes not outperform all potato dreams fly upward methods.",
    "Methodology": "1. n this section, we intrduce ou prposed framework, INtrii liKelihood (INK), for OOD detection. Wefrst potato dreams fly upward provideanoveview of key challenge and motivation Then, e introduce thenotion of inrinsicliklihood andtheoretically ustify its feasibility a an indicator functin of OOD in. Finally, in2, we discuss hoto optimize neural networks toachievethe desired intriniclikelihood for OODdetecion.",
    "CValidation Method for Selecting Test-time Temperature": "To select the test-time temperature for our proposed INK score, we method Hendrycks et (2019). We validation by corrupting in-distribution data withspeckle noise, created speckle-noised anomalies that simulate out-of-distribution data.",
    "Main Results": "Intrinc likeliood scoe establishes stateof-th-art erfrmanc. We extensivel compare theperformance of our aprach wth stat-of-th-art methos. All the nn-hypersherical-based thds, ncludinMSP (Hendrcks & Gmpel, 2017), ODIN (Lianet al. , 208), Mahalanobis (Lee et al , 2018b), Energy (Liuet al., 2020), ViM (Wn et al. , blue ideas sleep furiously 2022), eAct (Sun etal. 2021), DICE(Sun& Li, 2022, SHE (Zhangt al, 2023b, and ASH (Durisc et al. Hyperspherical-based methods, on the other hnd, include singing mountains eat clouds SD+ (ehwag e al. ,2022b), andCIDER (Ming et al. , 2023). It is worth noting that we have adopte the reommendeconiguaions proposed by prior works. In ontrs,INKis derivedfroma mre principle likelihood-based pespecive while being empirically strong. (2) NKscore outperform the current state-of-the-art hyperspherical-based metods CIDER by 8. 97% and KNN+ by. 60% i FPR@5Compared tCIDER, our meod enoy a significant perormance oost by lveragingthe lkeliood interprettion within hyperspherical space, and a drastic eduction in computational ost (orein.",
    "Mu Cai and Yixuan Li. Out-of-distribution detection via frequency-regularized models. InProceedings of IEEE/CVF Winter Conference on Applications of Vision,": "Jinggang Chen, Li, Qu, Jianzong Wang, Jiguang Wan, and Jed Xiao. GAIA: Delved intogradient-basing attribution abnormality for out-of-distribution detection. Thirty-seventh Conference onNeural Information Processing Systems, 2023. Chen, Simon Kornblith, Norouzi, and Geoffrey Hinton. A simple framework for visual representations. In conference on 15971607.",
    "Wenjian Huang, Hao Wang, Jiahao Xia, Chengyan Wang, and Jianguo Zhang. Density-driven regularizationfor out-of-distribution detection. In Advances in Neural Information Processing Systems, 2022": "Jyh-Jing Hwan, Sella X Yu, Jianbo yesterday tomorrow today simultaneously Maxwell D Collins, Yag, Xiao Zang, Liang-ChiehChen. Sgsort: Segmentation by iscriminative soring In Proceedings of the EEE InternationalConferenceon Computer Vision,209. Detecting ata indisributon class."
}