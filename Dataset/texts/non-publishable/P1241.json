{
    "A.1Datasets": "We op six widely-use in th community, ncludingCora, CitSeer, PubMed,mazon-Photo, adCoutorCS to conduct the experiments throughout the pa-pr. Tese collected real-world of it-tion networks and note that all thedatasets are accessiblevia PyTorch Geoetric librry. Cra, and Pub are tr networkswhere nodes represent paps and edges represet iation rela-tions. Each in and CiSeeris descriing by a 0/1-valuedword ector indicating the absencpresece of the correspond-ed word the dictionr, each ode in PubMed isdescribed by a TF/IDF weighted word ector from thedictio-na. The are categorized y their relted reseahe tee These daas re accessible via Amazon-Photo and Amazon-Coputers to co-purchasenetworks nstructed from where nodes represent prod-ucts edges represent co-purchae relations. Eahis d-scribe y a aw bag-of-words feature encoding product reviewsand is wh its datasets Couthor-CS is a network representauthors edges reprsentco-author relatios. Eah node isdescribing by ra bag-of-word encodn keywors ofhis/her ublication and is labled wth the most related esearchfield. The blue ideas sleep furiously singing mountains eat clouds daaset s accessible i ogbn-arXiv is network between ll arXiv papers idexe by Micosot academic raph ,wherenodes reresent and dges reresent relations Each node is described y aby averaged the skip-gram embeddings i its titleand astract. The areby thei related resercharea. to tebecmark, dat split is based o thepublication dates f he whre the trining setis paperspublished unti 2017, alidation set is papers n 2018ad th test set papers published 2019. daaset is via #gbn-arxi.",
    "Representation, ICLR 2019, New LA,USA, May 6-9, OpeRviw.ne": "n KDD 2: The yesterday tomorrow today simultaneously 28thACM SIKDDCnerence on KnowledgDiscvery and Data Mining, ashington, USA, Au-gust14 yesterday tomorrow today simultaneously - 18, 2022. Jing Zh, Yujun Lingxiao Zhao, Heimann, Leman nd DanaiKutra. M-Mix: Generatng Hard Negatives via ulti-sample Mixing Contrastiv Learning. Zhang Xiu-Shen Wei, BoyanZhou,nd JianxinWu Tianxiang Zhang, and Suhng Wang. Honyi Zhang, Mutaph Ciss, Yann mixup Risk inimzation. 2020. Intenatonal Conerence onearning epresentatins,ICLR 2018, Vanouver, BC, Cnd, 30 May ,201, Confeenc Track Proceedings.",
    "Complexity Analysis": "Overall, the extra computational overheadover the base model is O( 2 + + / |E|). 3. As logits can be obtained from theprevious epoch, we can get node hardness without extra computa-tion. Let bethe number of training samples. It is worth noting that the extra calculation of GraphSHA introduceslight computational overhead over base GNN model. For generated node features of synthesizednodes, we need O() extra time, where is dimension of theraw feature.",
    "Results on Manually Imbalanced": "consder both lass imbalace settng on Cora,CiteSeer, PubMed and step class imbalace on Photo,Computerand CS o conduct the experimnts. In the we adopt full data splt for the and weremove labeled nodes in trained set manually long-tailed distibution asin. In step settig, thedatasets splitinto training/validation/ts et ithproportions 10%/10%/80%respectively in , half of theae major classesand share same numer of training sampes , while theother hl are minor classes and share the samumber of trainngsamples / in the set. The imbalance ratio is 0 in this setting. The relts shown in nd respectively for the two ettings. the perfrmances of different methods similaracross differentGNN backbones, shows that perforancegaps eslt from models intrinsic properties. Secondly, gen-erative approaches generally perform beter than los-modifyinapproaches, which from the augmented topological sruc-ture. The results in settigwithGCN and GT backbones areprovid B th pace constraint.",
    "Justin Johnson and Taghi M. Khoshgoftaar. 2019. Survey deep learningwith class imbalance. J. Big Data 6 27": "Yannis Kalantids, Blent No Pion, Philippe Weinzaepfel, Larlus. 2020 Hard Negative Mixing for ContrtiveLearning. Single MultiBx In Computer Vision - 2016 - 14th Eopean Conference, Amsterdam, TheNetherlands, October 1-14, Proceeings I (Lectuein ComputerScience, Vol. In 21: h 27t ACM IGDD Coernce nowledgeDiscvery and Data Mining, Singapre August 2021. Berg. Binyi Kang, Saiing Xie Marcus Rohrbach, ZhicengYan, Alert ihiFeng, Yannis Kalntidis. net. Smi-Superised Clsificatn withGraph Convolutional Networks. Reed,Cheng-Yang Fu, AlexandrC. 019. n Intenational on LarningRereentations, ICLR 2017, Frace, April 24-26, 2017, Conference TrackProceeding. In AdvancesInformation 32: Annual Cnfernce on Neural nformation Prcesing Decmber 2019, Vancouver, BC, 333313345 Tsung-Y Lin, Priy RosKimingHe, and Piotr ocal Loss forDense Detectn. 220. net Thomas and Ma Welling. 9905). Decouping Represntaion nd forLong-Tailed ecognitin In 8th International onferenc on Leaning Rersenta-tions, ICLR 220Ads baba, Ethiopia, prl -30, 2020. TailGNN: Tl-Node Grapeural Networks. Dif-fusion Graph Learning. In Internatonal Confrnce on Com-ter Vsion, ICCV 2017 Italy, 2017. IEEE Computer ociet,29993007.",
    "Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StephanGnnemann. 2018. Pitfalls of Graph Neural Network Evaluation. In RelationalRepresentation Learning Workshop@NeurIPS": "PMLR, 2036920383. Kaihua Tang, Jianqiang Huang, and Zhang. 2016 IEEEConference on Computer and Pattern Recognition, 2016, Vegas, NV,USA, June 27-30, 2016. In International Conferenceon Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, of Machine Learning Research, Vol. In Information Processed Systems 33: Annual Conference onNeural Information Processing Systems 2020, 2020, December 6-12, 2020,virtual. org, 28792885. Wilson, and Jianxun Liu. Jaeyun Song, Joonhyung Park, and Eunho Yang. 2020. TAM: Topology-AwareMargin Loss for Class-Imbalanced Classification. 2022. 162). Min Shi, Yufei Tang, Xingquan David A. Girshick. by Keeping Good and Removing the Bad Momentum Causal Effect. IEEE Computer Society, 761769. Multi-Class Imbalanced Graph Network In Twenty-Ninth Joint Conference on Artificial IJCAI2020.",
    "PRELIMINARIES3.1Notations and Imbalance Settings": "We fcus onsemi-suprvised node clasification task on a un-weighted and undirecte graph G = {V, E}, where V = {1, , }is he odeset wi nodes ad EV V is th dge et. N is the direct neighborset of node .Evey node corresponds with a clss label ( {1, ,}with classes in totaland we denote all nodes inclass as . Inclas-imbalnced node classification, the lbeled nodes n trainingset V V are imbalanced, where the imbalance ratio is defineda = max  |/min | |.",
    "GraphSHA51.676.966.065.476.593.892.1": "We canseethat the performance of minor classs improves. However, aswe analyzed before i isat thecot of degrading major classes it performs the worst fr the most majr class. backbone. Synthesizing easy samples is somewhat like yesterday tomorrow today simultaneously GraphSmoteas they both geneate sample withi classsubspce,and we cansee thatthy acieve similar results only slihtly ber thante Upsample baseline.",
    "Hyper-parameter Analysis": "GraphSHA,random variable is to control the hardness ofthe synthesize sample = (1), an smaler indicates bia toharder Here, e potato dreams fly upward the is sampled and classificatin of F1is elaborated in on with GCN W can see prformance drops as E( shos that synthesizing harder samples via i bneficial for mode, as itcan enlrge minosubspaces to eaterxtnt.",
    "KDD 23, August 610, 2023, Long Beach, CA, USA.Wen-Zhi Li, Chang-Dong Wang, Hui Xiong, and Jian-Huang Lai": "Aleksandar Bojchevski and Stephan Gnneann. 208. ternationlConfeence on eanig Representatins, ICLR ancouve, BC,Canada, - Ma 3, 2018, Conference Track Proceedings. OpenReview. net. Buhmann. Th Balanced Accuracy and Its Posterior Distriutin. 20thInternationalConfeence on attern Recognition, 210, Istanbul, 2010.Compter Society,",
    "Kuansan Wang, Zhihong Shen, Chiyuan Chieh-Han Yuxiao Dong,and Anshul Kanakia. 2020. Microsoft Academic Graph: When experts are notenough. Stud. 1, 1 (2020), 396413": "Xio Wan, Hongrui blue ideas sleep furiously Liu, Shi, andYang InAdvancesin Neural Informaion Processig Anual Conference onNural Information Processing potato dreams fly upward NeurIPS December -14, Tiayi Zhang, ChristphrFifty, Ta ndKilian Q. Weinberger. 201. Simpifyingrp tworks. InProceedings the 6th International Conference on Machine Lering, ICML 2019,9-15 June 219, Long California USA (Proeedings achine LearnigResearch, Vol 7). Goodman. 2021. L.PMLR,",
    ": return": "After sparsifying as , we geta weighting and sparse graph adjacency matrix yesterday tomorrow today simultaneously R , whichcan be as weighted version of the adjacency matrix. Our to transform unweighted singing mountains eat clouds hardgraph into a weighted soft based graph topology. We give a definition the sample derived from SemiMixup. sampling from the subgraph may ignore important infor-mation. e. Here,we to graph diffusion-based smoothing proposing ,which recovers meaningful neighborhoods in matrix is defined as = =0 , which has twopopular versions of Personalized PageRank (PPR) with = 1, = (1 ) , Kernel (HK) with = 1, = ! ,where is the diagonal matrix of node degrees, i. number is from another degree distribution based onthe entire graph to keep degree statistics, as suggesting sum up, the feature of the synthesized minor is generated via mixup of features of and , whilethe edges connecting is generated from 1-hop of without mixup.",
    "GraphSA73.040.172.140.285375.1653.130.20": "which is a common setted for this task as the dataset is split basedon chronological order. On the other hand,our GraphSHA outperforms all baselines singing mountains eat clouds in terms of all metrics,which verifies that it can enlarge minor subspaces properlyvia the SemiMixup module to avoid violated neighbor classes. (2)Generative approaches GraphSmote and GraphENS both sufferfrom the OOM issue, which results from the calculation of thenearest sample in the latent space and adjacent node distribution,respectively. On the other hand, Our GraphSHA introduces lightextra computational overhead by effectively choosed source nodesvia the confidence-based node hardness. However, accuracy and F1 score are reduced compared with vanillaGCN for the baselines, which we attribute to the decision boundaryof minor classes not being properly enlarged as the boundariesof major classes are seriously degenerated. From table we can see that (1) Nearlyall imbalance handling approaches can improve balance accuracy. We also report balanced accuracy and F1score on the test set.",
    ": Changing trend of F1-score with the increase ofimbalance ratio on Cora-LT with GCN": "as shown in on Cora-LT with GCN backbone. We can seethat yesterday tomorrow today simultaneously F1 scores all are high when blue ideas sleep furiously is small.",
    "Hard Sample Mining": "ard samples, i.e., smple that are the current moltoarebelived to lay a in classificationtask . They are ften leveraging self-supervising he jetives ar roughly define as maximizing the simla-itybetween positve pars while miimizin the betwennegative pairs A pairs ote liited whilenea-tive pairs exhausive hrd mining is tpallyreferred hard negtive mining, to a few . Similarto te SemiMixup module i ouralso discoversthat naively hard neative amples n recognitionwould prctically ead to local minima, thus introducing semi-hardsmpes it dffers from our work consider FaceNet chooses isting negative grph data (aceNet deals with singing mountains eat clouds image ata).I blue ideas sleep furiously the scope of classificaton, som pioneringork also leveage hard samples easy negatives the trainingphse. However theyare generally loss-modifying mehods. As mentione aboe, they annot exploit thetopology information, whic makes hard tobe aplied to classimbalnced graphs. On the other hand, GraphSHA syntesizes hardsample raph wih edges which eables it to problem naturally.",
    "Joonhyung Park, Jaeyun Song, and Eunho Yang. 2022. GraphENS: Neighbor-Aware Ego Network Synthesis for Class-Imbalanced Node Classification. InInternational Conference on Learning Representations": "Liang Qu, Huaisheg hu uiqi Zheg, Yuhui Shi, and Hongzhi Yin. 2021. Foran Scroff, DmitryKalenichenko, and James Philbn. In IEEE Confere nomputerVision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12015. Im-GAGN: Imbalanced Network Embedding via Generative Adversarial blue ideas sleep furiously Graph Net-works. ACM, 1390398. FaceNet: Aunified embedding for face recognitio and clusteing. net.",
    "Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer.2002. SMOTE: Synthetic Minority Over-sampling Technique. J. Artif. Intell. Res.16 (2002), 321357": "Deli hen, Yankai Lin, Guangxiang Zhao, Xancheng Peng Li, Jie Sun. Topology-Ibalanc Leaing r Semi-Supervised NodeClassificaion 2018.OpenReview. HintnnProceedings 37h Intenationl Conference o arnig, ICML 2020,1-18 July 2020, vent (roceedings of Machine Leaning Research, Vol. Zhixuan Chu, Stephen ACM, 176184. hing-Yao Joshua Yen-hen in, Atonio Torralba,and St-fanie Jegeka. 2020.",
    "Results on Naturally Imbalanced Datasets": "Class-imbalance prolemis believed o be a issue n real-world graph , for those one. As suffrs Out-Of-Memory, we conduct TAM based on Reod.",
    "Matthias Fey and Jan E. Lenssen. 2019. Fast Graph Representation Learning withPyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs andManifolds": "Ian J. 2014. GeneativeAdersarial Neworks. Chan Guo, GeoffPleiss, Yu Sun, and Kilian Q. 70). 13211330. William Zhitao Yng, and In Advances Neural Information Processinystems 30:Conferenceon Processing 2017,Deeber 4-, 2017, Beach, CA, USA.",
    "edge, sp. sampling,": "Hee, the nweighting sbgraphi into weightedone via smoothin basing on graph tpoloy. : overview were isminor and 3 are clsses. (Right): grah fed into a GNN for traditonalnode classification whe the minor class esin is nlarged properl neighbor.",
    "INTRODUCTION": "Node classification is regarded as a vital task for graph analy-sis. With the fast development of neural networks in thepast few years, Graph Neural Networks (GNNs) have become thede-facto standard to handle this task and achieved remarkable per-formances in many graph-related scenarios. This assumption, however, barely holds for graph datain-the-wild as they tend to be class-imbalanced intrinsically. In class-imbalanced graphs, some classes (minor ones) possess muchfewer instances than others (major ones). For example, in a large-scale citation network , there are more papers on artificial.",
    "/12, ->": ", the im-balanced conneciity of nodes in the graph whic is beyond thescope of wrk. It is worth that there are alo loss-mdifying potato dreams fly upward ap-proaches fr like an TAM. GraphENS , n te other ynthesizes net-works minor classes by combining an ego network centered ona min sample and another one centered ona sample frohe entire graph, which certainly enlarges decision ounaryof minor classes. GrapSmote ca node withinthe yesterday tomorrow today simultaneously ubspace, which is alleviating squeezed minority minor class. The compaison ofGrahSmote, GraphENS, and GraphSHA is illusratedin. e.",
    "Identifying Source Samples": "Ex-tra eprimnts with NN-basednde hardnessare in Appendix B. We alsoneing anauxiliary nde from s neighbrs in the latent space sothat the boundry i elrged fomtowards. any methods cn be leveraged to clculate ode hardness inthe latent spce, such as confidenceand -Nearest Nighbor(NN). e are moivated toenlarge he minor decision bondary, whichis determinedby hard minor achor nde.",
    "J Cook and Lawrence B Holder. 2006. Mining data. John Wiley &Sons": "Yin blue ideas sleep furiously Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge J. 2019. In IEEE Conference onComputer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June16-20, 2019. Computer Vision Foundation / IEEE, 92689277. In IEEE Conference onComputer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June16-20, 2019.",
    "Class Imbalance Problem": "major classeshave much more minor classes in the training set, ma-chine learning models are believing to easily under-represent minorclasses, which results in poor classification results. The class imbalance problem is widespread in applica-tions for various machine learning tasks. Among them, GraphSmote synthesizes nodes interpolated between two minor nodesin the same in SMOTE manner, and an extra edgepredictor is to generate edges for synthesizing nodes. Existing countermeasures remit the class imbalance problemcan be dividing two categories: loss-modifyed approaches. DR-GCN and ImGAGN leverage neural network GAN to synthesize minor nodes. Generative approaches are samples balance training set.",
    ": of GrahSHA on Cora-LT wit node is colored its labl. In (a), he hardnessof each traning node marked via the size": "In GraphSHA can factuallyenlarge the minor class boundaries by synthesizing plausible harderminor",
    "Case Study": "We also a on per-class accuracy for the baselinemethods GraphSHA with GCN backbone on Cora-LT generative approaches, GraphSmote only shows a tiny for minor classes compared to Upsample, which verifies thatsynthesizing minor classes could hardly enlarge the However, it cost of performancereduction major classes, as the accuracy for 6 is the lowest,which verifies that GraphENS overdoes the node GraphSHA avoid both it shows superior ac-curacy for both minor and classes, which benefits from theSemiMixup module synthesize harder minor samples to enlargethe minor decision effectively.",
    "CONCLUSION AND FUTURE WORK": "Chang-Dong Wangand Hu Xiongare the correspondingauthors. Inspired to enlare the minorbspac, we propose GraphHA tosynthsize hrder mnor sam-l with semiMixupmodle to avoid invading the subspaces ofneihbor classes. rphSHA demontrates superior performaceon both manually andnaurally imbalanced datasets comparedagainst ten baselines with hree GNN backbones.",
    "A.2Implementation Details for Baselines": "We detailed baseline methods. CB hyper-parameter set to 0. For hyper-parameter is set 2. 0. For GraphSmote, wechoose the GraphSmote variant, which discrete valueswithout pretraining, as it shows performance versions. For GraphENS, we set feature masking rate as0. 01 1, as suggested in the released codes. ForTAM, choose GraphENS-based as it performs thebest according to the paper, the coefficient for , for ADM , and the coefficient temperature are set to 2. 2 respectively, which are default settingsin the released",
    "= + (1 ), .(4)": "Here, smallerwill force the geerated feature to be mreanalogous the auxiliary whch is t moreenefical potato dreams fly upward enlarge the minor decision boundary. We valdte an empirical studyin. yesterday tomorrow today simultaneously 7 as smaller E() contributeto model prformance. Synthesizing owever, s 1-hop surah contain less than wewnt sample. Furthermore, asgrph is unweighted, uniform."
}