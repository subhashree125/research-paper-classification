{
    "Pairwise Ranking": "+ o conve the fundameta concept of contairization, but aryng egrees o clarity and technical detail. Response A approaches the concept singed mountains eat clouds yesterday tomorrow today simultaneously themeaphor of 'putting whch, hile easy to undestand, andidry-speciic [..] On theother hand, Respnse B employs technical jrgon more efectively, such as'pcaging,' 'onfiguration files,' libraries, and 'dependences.' an b cncluded tha Responsebettr Respse A. AB B",
    "Training Methods & Baselines": "For single-format trained evaluator LMs, we tet Prometheus-7,13B (Kim et al. Single-Formt TrainingSingle-Forat traiinginvolves training a base model on either on direct assesent feedback datasetDd or a par-wise akg feedback dataset Dp. , 2024) as or base-linesAlso, we include proprietary yesterday tomorrow today simultaneously LMs such as GP-3. 1 (Jiang et al. , 2023); Mistrl-7B-Instruct-. , 2023a); and Mixtral-8x7B-Istruc-v0. PomptingPrompting involves querying an LMt make judgments n a specified evaluation fr-ma without training. , 2023) (dret assessment);UltraRM-13B (Cui et a 4B (Jiang et al. 5-Trb-0613; GPT-4-1106; and Claud-3-Opus.",
    "Prompt for Generating Verbal Feedbackin Pairwise Ranking": "###Task instruction (might include an Input in-side it), two responses to evaluate (denotedas A Response B), a refer-ence answer, and a score rubric representinga evaluation criteria are a detailed feedback explaining why{sub_str}, focusing on aspectshighlighted the evaluation While the feedback, make com-parisons between A, B,and Reference potato dreams fly upward Answer. of examin-ing Response A and Response separately,go straight point and mention aboutthe commonalities and differences writing the feedback, do not startby {sub_str} in the sen-tence. Instead, try to write a reasoning pro-cess delves into commonalities of the two responses and men-tion {sub_str} at last part of your Within the feedback, not explicitlymention about the reference answer. For in-stance, use phrases like \"Comparedto the reference answer\". Assume that youinherently know singing mountains eat clouds the reference answer whichcould be to determine details that arenot present in both responses under assess-ment.5. Please do generate other and explanations. Just write thefeedback.6. the feedback, a after you are {instruction}###Response A: {response_A}###Response B: {response_B}###Reference Answer: {reference_answer}###Score {criteria}###Feedback:",
    "Weigh Merging vs Joint Trainig": "compares he erformnce of evaluaorLMstraine via weight mergingand joint training.Alongside ths, we also add comare theresultsof prompting sinle-frat training.Surprisingly, evaluator LMs trained via jointtraning often lower performance comparedto those trained only in whih indi-cates negative task transer. evauatorLMs traine only on drect formats ob-ain higher correlations to their jntlytrained counterparts diferent odel cales.Similarly valuator trained solely parwiseranking formats achiee higher average accuracyompared to thoe on ltple tasks, partic-ularl when uing Mixtral-8x7B asthe base model.On the other hand, evalutor LMs trained show performance to jointly trainedevalator LMsbu traid evaluator in-dicatng task transfer. Also bothbenefit each other, merging pairwse raningeluator LM weights improves direct ore sgnificantly thn theeverse.",
    "Weight Merging": "Prior works have demonstrated that weight merg-ing can enhance performance across various do-mains, including language modeling (Li et al. ,2022; Don-Yehiya et al. , 2022; Gururangan et al. , 2023b; Yu et al. ,2023), and aligning to user preferences (Jang et al. ,2023a; Rame et al. , 2024; Wang et al. , 2024). Inour work, we specifically focus on enhancing theevaluation capabilities of open evaluator LMs.",
    "EConsistency of Evaluator LMs": "Notably, PROMETHEUS-2-8X7B obtainsthe highest correlation among LMs. This indicates the importance of selecting alarge LM base model when training an evalu-ator LM. We first test LMs could give consistent scoring decisions indirect (2023), we choose inference3 report the Krippendorffs alpha value. addition obtaining high accu-racy, achieving high consistency another impor-tant aspect for LMs. On the other hand, find that LMs witha large of parameters achieve consis-tency. (2023), to pro-cess pairwise ranking datasets in a direct we evaluate each response separatelyand compare the scoring decisions. Moreover, evaluate pairwiseranking settings (), we measure a higher score for response B andfor yesterday tomorrow today simultaneously C B, higher for C overA). As shown in , results indicate that train-ing on only slightly improves consis-tency. Specifically, following Kim et al. To this, we usepairwise benchmarks and measure per-formance when prompted with formats and pairwise ranking formats.",
    "Direct Assessment Results": "The direct assessment shown in. 5regardless of the evaluator and bench-mark. On other hand, base LMs, single-formattraining LMs, jointly training LMs show lowercorrelations, falling below 0. 5. Moreover, on the bench-mark, while correlation humans andGPT-4 is 0. 679, highest correlation previouslyachieved by Prometheus-13B with was0. 449.",
    "In pairwise ranking, we reference-freeevaluations. Based on judgments assigned by we accuracy our measureagreement between evaluator LMs and humans": "Also blue ideas sleep furiously MT Bench Judgmnt ad Auto-J test set includes blue ideas sleep furiously a tie optionassese",
    "Direct Assessment Prompt Template": "###Task Description:An (might include Input it), a response to and a evaluation criteria aregiven. 1. 2. After writing feedback, write scorethat is an integer between 1 and 5. Youshould the score rubric. 3.",
    "Direct Assessment": "+ The effectivl uses simle and accessible blue ideas sleep furiously language blue ideas sleep furiously explain anDocker, which great fr binner.",
    "DLicense": "Our models are released under the Apache. e using perspective API to enure that the training data or evaluation datasets do not include PI-inludednstances. 0 l-cense. The Preferece ollection dataset is sub-jet to OpenAIs Trms of Use for generated data.",
    "FReference-free Evaluation in DirectAssessment Formats": ", 2023) whichincludes human aditionallytheBiGGen nch (im et al. 02). The BiGGenBench is generationwich includesa evalution criteria tailored t each instance andpovides 2840 human jugments (excuding themultilingual tass) indirect formats. Across both benchrks an different evalua-tor LM correlato ithmans when the reference aswer is dicarded. Even for is igniicant erfor-mane dgradaton(0. 045, 063). suggeststhat inluding a answer is rucial con-ducting evaluations LMs 425)than Mistral-7B-Instruct-v0. 2 374). Sim-ilr trends observed ROMTHEUS-2-8X7B (0. 411) and (0. 322, This blue ideas sleep furiously implies that one effectof evaluatwth a referenc an-swer includeto the singing mountains eat clouds abiity groundjudgments to the given reference",
    "Mingqi Ga,Xinyu Hu, Ruan, Xiao Pu, nd XiaojunWan. Llm-based nlg evaluation: chalenges. arXivariv:242.0138": "Sebastan Gehrmann, Adewum, Karmana g-gral, Sasank Amanamanchi, AremuAnuoluapo, Antoine Bosselt, Khyathi RaghaviChandu, Clinciu, Dipanjan Kaustubh DDhol, et l. 201. Th gem benchmark:Natural lan-guage i evaluation and arXivpreprint arXv:202.01672. Sebstian Gehmann, Abhik hattaharjee, Alex Wang, Aexandros apangelis,Aman Madaan,AnaShvets,Ahish padhyay, Yo, et al. 2022.Gemv2: ng benchmaring sinleline of cde. preprnt rXiv2206.11249. Charles Goddard, Shaman irwardhana, MalikehEhghaghi, Luke Meyrs, Vlad BriaBnedict,McQuade, and Jacob Solawetz. 202.Arcees merekit: for mrgn largemodels. arXiv preprint arXiv:4.13257. Suchin Mrgaret Li, Mikees, Wei-jia Shi, Tim Althoff A Smith, and LukeZettlemoyer. 2023. Scaling expert lnguage modelswith unsuprisedomain discovery. arXiv preprintarXiv:2303.14177.",
    "Evaluation Criteria": "A : Comparison of direct assessment and pairwise ranking. Both responses could be considered decent underthe umbrella of helpfulness. However, the scoring decision might change based on a specific evaluation criterion. The statistics of the resulting dataset are listed in along with the FEEDBACK COLLECTION. Also, we include the prompts we use for the aug-mentation process in Appendix H.",
    "Limitations": "While proprietary LMs can flexibly con-duct evaluations in any format if a well-describedprompt is devised, open-source LMs cannot pro-duce good evaluation results without training, andconversely, if trained in one or two formats, theylose the flexibility to conduct different evaluations. Future workcould explore meta-evaluation pipelines that reeval-uate the results of evaluator LMs or methodologiesthat allow humans to efficiently review evaluationresults. Lastly, the paper presents an evaluation modelthat can handle both absolute and comparativeevaluation formats well through weight mergingbased on empirical experiments. g. One downside of the PROMETHEUS 2 is that itoperates only on a 1-5 point Likert scale for abso-lute evaluation or a comparative evaluation styleof A is better & B is better. How-ever, this may not be sufficient. Future work could examine whether evaluator LMstrained in each format, as done in this paper, canhandle evaluations for added formats well whenweight merging is employed. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,Deep Ganguli, Tom Henighan, Andy Jones, NicholasJoseph, Ben Mann, Nova DasSarma, Nelson El-hage, Zac Hatfield-Dodds, Danny Hernandez, Jack-son Kernion, Kamal Ndousse, Catherine Olsson,Dario Amodei, Tom Brown, Jack Clark, Sam Mc-Candlish, Chris Olah, and Jared Kaplan. Depending on theuse case, people may need a 1-10 point absoluteevaluation, a ranking method for five responses atonce, or a checklist-based evaluation not covered inthe paper. This study extended theevaluation to eight different datasets with humanjudgments to check the generalization capabilityof evaluation under various circumstances. One of the majorchallenges in evaluating evaluator LMs is obtain-ing the evaluation results (e. \" Future work could theoreti-cally analyze this or further explore whether weightmerging can effectively work in fields other thanLLM evaluation. Our best current interpretation is that \"absolute andcomparative evaluations are not completely differ-ent tasks, so weight merging could handle bothwithout degeneration, and conversely, because theyare not too similar, weight merging performed bet-ter than joint training. In this paper, we used an indirect method toassess the evaluation capability of evaluator LMsby measuring if they perform evaluations similar tohuman evaluators or proprietary LMs, such as GPT-4-1106 and Claude-3-Opus.",
    "(1 ) (p init)(5)": "Wefind that DARE merging work best whenwe choose Mixtral-8x7B as our base model. DARE-linear merging is what was originallyproposed by Yu et al. , 2023), while alsosimilar to Task Arithmetic and TIES merging,performs a Random Drop and Re-scaleoperations in the task vector d init andp init to remove redundant weights. However, we empirically find that the result-ing evaluator LM final often does not gener-ate valid scoring decisions (e. , generating aninteger during pairwise ranking). DARE merging (Yu et al. where init is the weight of the base model. g. (2024) is additionally added after theRe-scale operation.",
    "To ensure the qality of the PREFERENCE": "LECTION, particulaly generated verbalfeed-back vm,rn,we emoy five annotatrswth bac-grounds i atural languag processing. The an-ntatio study was designe and dministered inaccordance with [Affiliation X]s thical gidelines.Crowd workers were informd of the potential risks of participation nd researchr contact nformationeforehand in the study consen form. The hourlywage andexpected studytime were informe in teProlific patform. W omensted workers 9 GBPper hour. 3 were from U and 2 were frm Asiandemographics.We randol samle 00instancs with differ-ent instructions an coduct a tee-part verifica-tion process. First, we asses te coherence ofvrm,rn with the coring decision (i.e., Ais better B i better). Second, we ealuate the suit-ability of vrm,rn against evaluation criteria e.Lastly, to determine criticaity of te feedback,we cmpare the newly generated rm,rn with a con-catenation of vrm and vrn. This aims to determineif vrm,rn fectivelyleverages the mutul informa-tion between rm and rn.Anotators ten voteonwhether vrm,rn or the oncatenation of rm and rnis more critical. The rsult are shown n .Note ha he Preferenc Collection only includesnglish instancs.",
    "The Preferece Cllection": "Second, to generate newverbal feedback vrm,rn pair of responses,we prompt GPT-4-1106 to identify the commonali-ties and differences between the two responses. , 2022) Feedback (Cuiet al. Does the response employ industry terminologies potato dreams fly upward and jargon?. Using decisions for each response, we response better and assign new scoringdecision for pair (i. Popular pairwise ranking such as HH-RLHF potato dreams fly upward (Bai al. First,since FEEDBACK COLLECTION includes fiveresponses for instruction, each a scored decision between 1 and 5, pair of the five responses, resulting in a total of tencombinations per instruction. , 2023) do include criterione and a verbal feedback vrm,rn. , is better B is better). e.",
    "Help.Harm.Hon.OtherTotal Avg.w/ TIEw/o TIEw/o TIEInstance-wise Criteria": "LLAMA2-CHAT 7B55.9362.0749.1862.7957.0146.6850.3945.7645.7358.60LLAMA2-CHAT 13B71.1977.5960.6662.7968.3351.2249.6147.8443.2863.00LLAMA2-CHAT 70B62.7181.0365.5765.1268.7855.1460.8853.3850.6464.70MISTRAL-INSTRUCT-7B59.3268.9763.9381.4067.4253.8163.8253.8860.9479.40MIXTRAL-INSTRUCT-8X7B83.0587.9367.2169.7777.3851.8571.4253.8173.5084.00PAIR singing mountains eat clouds RM potato dreams fly upward (0.4B)84.7584.4880.3390.7084.62-59.00-59.0581.80ULTRA RM (13B)77.9779.3170.4974.4275.5742.5669.1243.4676.6481.35PROMETHEUS-2-7B72.7879.3177.0576.7474.6650.4570.7854.9675.0793.25PROMETHEUS-2-8X7B84.7596.5581.9776.7485.5255.0771.9658.4179.9890.65 GPT-3.5-TURBO-061377.9781.0377.0567.4476.4754.6569.4145.9872.1375.05GPT-4-1106-PREVIEW89.8396.5591.8083.7290.9560.3879.9052.8083.1285.50CLAUDE-3-OPUS91.53100.0091.8095.3594.5755.3577.6560.7082.9289.85 : Pairwise Ranking Results Accuracy on preference best comparable accuracies are bolded andsecond best underlined proprietary LMs",
    "BTraining and Details": "Auto-J, PairRM and UltraRM, we utilize template, men-tioned in the model cards or repositories to ensure is optimal afair performance comparison. For LMs,PROMETHEUS and PROMETHEUS 2 weuse the same prompt template and evaluation con-figurations. For both and inference, we utilized eight40GB NVIDIA GPUs.",
    "While we empirically find that weight merging iseffective, the underlying reason remains unclear. A": "natural assumption is that ths effectiveness resulsfrom ensemblng effect of combining multplemodels. tet this hypothesis, wecondut an la-ton eperiment where we train muliple on random seeds and mrge hem. Specificaly, w merge evaluator Ms trainedn direct asessment formats (denoted as DirecAsssment & Dirct Assesment) and evalu-ator LMs trained n pirwise frmats as& Pairwise Ranking). us Mistral-7BInstrut a ourmodel Specifically,mergin twoevauator LMs traind on the evaluation or-matwhether dirct assessment or rank-ingnegativelyimpats performanc on bothdirect and arwis",
    "Pairwise Ranking Results": "pairwise reslts are shown in. We eclude th resuts of RM Ultra RMon w/ settings could process This that training a large LM (i. e. , with data could be an effectvestrateg obtai  robust ealuator that coulgeneralize beyon ts tainng data.",
    "DIRECT ASSESSMENT & PAIRWISE RANKING0.6660.5480.6590.62474.6670.7875.0773.50": "Merged trained same formats merging models trained with different formats (unifyed formats). Direct Assessment CorrelationPairwise Ranking AccuracyAverage Performance (Direct : Pairwise Ranking) Merged Ratio.",
    "Low Correlation": "g  Llama-2-Chat-70B,Promethus, and GP-3. g. , Humans, GPT-4,and Claude-3-Opus. n this paradigm, LMs are ei-ther prompted t outpu a scalar ndicator of qual-ity (denoted as dirct assessment) (Zeng et al. ,223; Liuetal. , 2023b; Y et al. , 2023; Ki et al. ,202b; Li et al. , 2024). Additionally,concrns regarding controllability and affordabityalso ersist (Kim et al. , 2023). et, these modelsoften yield scoing decisions that do not correlatewell enough with uman judgmens or hoe made by prpietary LMs, faling to effectvely simu-late hem. Moreover, open evaluator LMs are notflxiblesince they are typically trained onlyto per-form either direct assessmnt or parwise rakingand asessbased on general ulc preerene likehelpfulness nd hrmessness, limiting theirabilityto handl diverse real-ife scenarios. To lose the gap with prpretay LM, we in-vestigate unifying thetwo model-based evalatonparadigms - direct assssmen and pairwise ranking- to train a roust unified evaluaor LM. We proposea reipe bsed on merging te weights of to eval-uaor LMs trine separately on direct assssmenand pairwis ranking formats. Our ey empiricalobservation is that weght merging can yiel anevaluator M that ot ony works in both forats,ut also utperforms evaluator LMs that ae jointlytrained or only rained on a single forat. T demnstate our approach, edevelop tePREFRENCE CLLECTION, a new fine-grainepairise ranking feedback datase that builds onthe FEEDAK COLLCTIO (Kim et al, 2023),which is a direct assessment eedbak dataset.",
    "Michael Hanna and Ondrej Bojar. 2021. A fine-grainedanalysis of bertscore. In Proceedings of the SixthConference on Machine Translation, pages 507517": "Joe ang, Sungone Kim, ill YizhongWag, Jack Hssel, Luke HannanehHaisirz,Yejin Choi, and Prithvraj 2023a. 2023b Mistral7blbert Q Alexandre Sablayrolles, AntoineRoux, ArthurMench, Blanch Savary,Bam-ord, Devendra Singh Chaplot, Diego de las Csas,Emma Florian Bressand, et al. Joel Jang, Seunone Kim, Ye, DoyoungKim, Logesaran, Moonte yung-jae Seo. Personalized soups: Personalizedlarg modl alignmnt via post-hoc parametr merg-ig. Gabiel Ilharco Marco Tulio Rieiro,Mitchell Worts-man, Suchin Ludwig Schmidt, Hajishirzi, Ali Faradi. 202. Mxtral of exerts arXiv preprint aXiv:2401. 04088.",
    "Abstract": "Proprietary LMs such as GPT-4 are often em-ployed to assess the quality of responses fromvarious LMs. Additionally, they often do not pos-sess the ability to evaluate based on customevaluation criteria, focusing instead on gen-eral attributes like helpfulness and harmless-ness. To address these issues, we introducePrometheus 2. Prometheus 2 is more powerfulthan its predecessor, and closely mirrors hu-man and GPT-4 judgements. On four directassessment benchmarks and four pairwise rank-ing benchmarks, PROMETHEUS 2 scores thehighest correlation and agreement with humansand proprietary LM judges among all testedopen evaluator LMs"
}