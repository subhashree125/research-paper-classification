{
    "Effects of glimpse-based downsampling study: GFNet)": "Hwever, for passive vsion mehods, we reizethe adversaralnputs to match th resolutons separatly. Downsampling inherntly causes redution infeatures. Forimplicity, e further refer to GFNets rained o 96 96 dmeions as \"GFNet-96\". Furthemore, under this setting, GFNt96exhibitsgreater inherent robustness thanGFNt-128 hen compard to thircorreponding passve yesterday tomorrow today simultaneously baelnes. Reultpresents uantitative resuls, fusing n Accst. Hence downsampling he ima, distorts the noise alongwith i, thereby reducingits overall impacton predictions. Fr Setting 1, a pssive model tained on a igher esolution suffers a rop i performacwhen evlating at downsmpled input, ulike GFNes training fordownsapled resolutions. Fo the passive target baseline,weusea ReN50 pre-trined onmageNet at resoutions o 224 224. Setng Rduction of efficacy of adversaial noise post downsampling- Advesarial imags are fistgenerating from ful resolution imags of (224, 224) and then dwnsample to (6, 6) and (128, 128),separatel, for inference. Seting3 Generating adversarial attacks n downsampled images - he imags are donsampledto lower resoluts first and then averarial inuts are gnerated. For GFNets, w singing mountains eat clouds infr wt twosparaemodels traiing on 9 96 and 12828 resolutions. In tis secion, we use GFNet to explore ow learning iage rpresentations base onglimpse at a down-samled relution contribtes t inheren robstness. As a rsult it probble to tink that an inheent robutness ofered y models procesingn imaeia downsampling esolution stems from the istortion on the non-uniformdveraril noise. Te images are downsamped to (96, 96) ad (18,128)nd nfeence s peformed. Adversaril impeceptile noise is craftedbaing on the im n its orginal reolution (eg. Tis indcates along withthe imge resolution, the imperceptibl adversarial ois also probably gets downsamed thereb rducigis effct on model predicion even wn Ti issame as Si.",
    "Fixation Point Map": "Each point is then presented to dorsal, which retainsonly those, indicating by blue dots, that potentially resulting in the capture of an object through the series ofexpanding foveated glimpses. 1, FAL-con processes every input from multiple initial fixation points. shows IFPMs generated for both clean and yesterday tomorrow today simultaneously adversarial images. Red dots indicate all initial fixation points,equally distributed over the image dimensions. In order to understand the impact of adversarial noise on regions of the image that influence model pre-dictions, we define an Initial Fixation Point Map (IFPM). The ventral processes final foveated glimpses that resulted from potential points to determine the classlabel of an object. IFPM displays the distribution of initial fixationpoints based on how each of them affects the decision-making of FALcon throughout the inference process.",
    "Effect of distinct fixation points (case study: FALcon)": "In this section, we use FALcon to demonstrat the effet of processing image from distinct fixation ointson the robustnss f methods. oeover, since he ventral model no fine-tuned t comparison with passive baselne",
    "A.1.5Token Gradient Regularization (TGR)": "rpresented asGR() removes tokns wth values and euces variance in graiets. , 2023). 202). blue ideas sleep furiously , 2021) and its such asClass-Atntion ImageTransfrmers (CaiT)(ouvron et , 2021) and Pooled based Vision Tranformer (PiT)(Heo et a. , 2023) provides a gadient sed transfer attak lgorithmr Vision Trasformes (Dosovitskiye al. I utilizes token infomation from both he Attention within attenton block well romthe MLP omponent within the MLP lock, toenere dversarial samples This is illustrating in (Zhan al.",
    "FALcon": "ForfV , any pre-training network can be selected. The. Active Vision structure Both the dorsal fD and ventral fV streams are represneting by deep convolutionalneural networks.",
    "Base model size, indicating a standard configuration with a specific number of layers and attentionheads": "his ales Cai to go deeperwhile remained and efficient",
    ": predictions: The number of po-tential correct prediction points as at-tack strength increases, reflecting the quantitative re-sults presented in": "second amp (2f), we cannotice of a fixaion point faraway from theobject. isvisllyexplains the reason frimroved ofanctive method over pasie one, supportingthequtitative resuts psented in the previus ec-tios. validats the hypothesis pesnted inthe firt in (a). multiple fiationpointslead to correct clas predictions. lhough oise affects he method, inher-ent processing from fixtinsit lesssusceptibl (f.",
    "Active Vision systems": "2 (GFNet). Fordetaied lrnig readertoupplementary Setions. Forthe of we rerto saccadi points as fixtion. In this section, we provie a ovrview of inference process and highlight yesterday tomorrow today simultaneously insiht into theinherent robustnes of two actve visio FAco (Ibraye et , 224b) and Glance and ocusNeorks (GFNet) et al. singing mountains eat clouds These methods feation croping theimage based on (accadic) points, blrring the extracted gimpses. This aproach can as foveation withan cut-ff. , 2020.",
    "in the Black-box ransfer ttak setup": "In this section, we demonstrate performance of active vision systems (e.g., FALcon GFNet)over passive ventral ones (e.g., supervising ResNet) black-box Adversarial singing mountains eat clouds samplesgenerated from surrogate models transferred to unknown target models. For we use theoutput from final step as .2. FALcon, we evaluate two types ofpredictions: where most confident prediction matching ground and Any, prediction from multiple yesterday tomorrow today simultaneously outputs is considered.This is enabled by from multiple",
    "xadv = ASi(x, | Si(xadv) = y;Ti(adv) =": "Metrics singing mountains eat clouds We non-targeted transferability by computing percentage adversarial examplesgenerated using model Si, still correctly classified by blue ideas sleep furiously model Ti (not transferred). a test set samples, the accuracy is defined as:.",
    "We present a novel analysis of the inherent robustness of Active Vision systems across threedifferent categories of adversarial inputs": "2) onImageNet (Deng et 2009 in a trasfer attack setup. We provide ualitative resultsfor divrs setsamples witin this present qualittive resultshighight of iplci strctured learning i ac-tive systems forhalingforegroun object distortion, tohuman-aligned andinterpetable predictions.",
    "A.1.6Surrogate models for Token Gradient Regularization (TGR)": "We foloexpeimental of te origial Token Gradient gularizationpaper (Zhang et al. PiT - Pooling-based Trasformr modifies Vision rchittre btween transfomer bloks, graduallythe spatal dimensions,simila t CNs. In ViT-B/6, \"B\"stands for he Bse size, a standar with 12 tranformer and\"6\" to th patch size (16x16 pixels whch the image is before processing. , 2021) and its variants,icudng the ision Transfrmr (PiT et and lass-AttentionCaiT (Touvron et surogate We prove som intuition regardng thesurrot Vi - The VisonTrasformer(ViT) architectur imageinto fixed-sze paches, treatseach patch as token, an applis stndard model o hesetokens, dirctapplication of trasformer lyers to dta withut poessing. This oolig improvesefficiey and In PiT-B, \"B\" stands for. Additionally t hat transformer-based advesarialsamples an successfully tranfer CNNs, maing apoach effectie CNN models aswell Followng their setup, we chose a Transformer (Dosovitskiy al. , 023fr seleting architectures his paper demonstrated that tansferableattacks, leveraging graient through attntin blocks specifi surroga visio are ighly effetiveagainst other target viion models.",
    "Adversarial crafted images": "Given a surrogate Classifier Si, we generate an adversarial samplefor an image/label pair (x, y) which is denoted as xadv. This is with respect to the surrogate Classifier Siand attack pair ASi. Adversarial transfer attack setup The section aims to illustrate that active vision networks GFNet(Wang et al. , 2017; Mahmood et al. We follow the protocol for a black boxtransfer attack threat model as outlined in (Liu et al. , 2021). , 2015). Following this protocol,we define non-targeted transferability. , 2024b) exhibit higher levels of robustness against transferredadversarial images than base passive classifiers (He et al. , 2020) and FALcon (Ibrayev et al.",
    "Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li. Boosting adversarial attacks with momentum.In CVPR, 2018": "Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Georg Sylvain Gelly, Uszkoreit, and NeilHoulsby. Yinpeng Dong, Tianyu Pang, Hang and Jun Zhu. An image is potato dreams fly upward worth 16x16 words: Transformers for image recognition at scale. In 2021. Evading defenses examplesby attacks.",
    "A.1.7Targt models Token rdiet Regularization (TGR)": "based target we use Transformer singing mountains eat clouds (Liu et al. , 2021), and Focal Networks (Yang et al. , 2022). And as mentioned in , we use the of these transformers evaluation.",
    "Foreground distote images": "Human aligned vision fo foreground distrtions In scion, w qualitativey analyzeheimpct of objt o of and passive ision sstems.Unlikeimperceptibe dveraril nois hat spreads non-uniform cross th r natrall adversarial sampleswith varying degrees adversity, iortions vsibly alter thestructur of foregroundbect. Human rely heaily on the structural onfiguration bjects to idenify fro variosviewpons. It is to observehow edicthese disored images. Ou objectves assess thee sstems align with visonin hndling visible srutual distortions to demonstate ow hman-lign perception of ativevision system leas to more and intepretablpreditions.Setu The uper of illustrates to forforegroun-distorting images:(a) and (b)Composite Iaes. image shufling, diide imag ino and andomly shuffle If the ratio isprocssstructuralydistorts the object. or coposit images, we use th algorihm et al. ,2004) to exrathe freground, these parts, and past hem backgrounds. For evaluation, we use a passive ResNet50and select as our canidate activ vision model. UnlikeGFNet wich redics basedo obecFALcon captresthe objet b gradually foveatingon salient an iplicitly learnin structure, stopping when no iprovement is possible.This makes Fco ideal for analyzing its predictos remain human-algned forgrounddistrtions. y cocurrently these distortions, w am oerve nd compre teprdictontrends of the two sstem, thereby the roustness and uman-alignment otheir predictions. Qualtative analysis the unistorted sple of a retrevr, both FALco and makecorrect redictions confidences of75. 56%, FALconmakes multpledistincon eac locaid part, each with lower onfidence tan he face, beed the mostdisinctive has he highest among the parts. (< 0%)canbe a in In the passive classifiers abormally a henomenonby iterature et owdhry et al. This likely ccurs standard classifiers, traine o vious imaecrpduring data augment-ton,rey on objct pars for correct predictio ther tha overall sructe. Thus, whenthe imagesfling the structuredistoted, the discriminative still lead strogpredictions. A similar istance is shwn a sample. For the second pemuation of the image, activmethod a categical basd the locaizd facia part, identifying it a \"Russianwolfhoun,\" which is a dog bree. Thisis and less catasrophi,as evideced the providd samle of an atualRssian wolhound. Image shuffling, irst of the twoold experment that pasive ely mostly on fr discriination strctural.",
    "Published Transactions on Learning Research (12/2024)": "In both clumns,the top row crrectrdictions on benign inputs, whle row misedictions. This approach enhances te mdels abilitylearn fatures,resultg n a strger representation forvarious visin-based tasks. On right, nose affect the stream, shifting it fousto the leadingthe ventral stream misclassify based on features frm this misdiected region Swin The Transformer V2 is an updated vesion singing mountains eat clouds the Swin Transforme that i shifted windows, efficient hierarchical fatue extraction. For target ventral architcures transer such as TGR, we selected improvedstandard Vision Transformr win V2, a robust visin RVT, Focal ModlationNetwork, wich is by neuroscince for enhnced feature represntation. RVT - Robut identifiing weaknesses in transfrmer adversarial o-bustness andntoducednove as attenton scaled patch-wiseaugmentation, to enhance blue ideas sleep furiously various Thee inovaions make RVT amoreresilient viion transformer, espeially under adversaril and distributional FocalMod - Fcal Modulation Networks introduce a focalmodulatomechanism that replaces theconventional self-attention aligning more closely human-likefeature-based attentininstead of spatial ttention. For each vental model,we added FALcons learned VGG16-based dorsal stream o create active vision countrparts illustrates thes improvements, showing constenttrens simila othos obsrved with CNN-based tranfer attcks vetral bckbones. O the lft, msprediction when the ventral stream impacting by adversarial oise, despite correctlocalization by steam.",
    "Related Work": "Saccader(Elsayed et , 019) eulates acadiey movements to iteratively extrac an image ttending.",
    "A.1.4Transferability from Large Geometric Vicinity (LGV)": "The paper introduces a technique Transferability from Large Geometric Vicinity (LGV) (Gubri al. to transferability of adversarial attacks in black box transfer setup. This is illustrated (Gubri al. , 2022). enhance the geometric diversity the surrogate models within a wide optimum. In this region, loss landscape is broader, thatsmall changes the weights do drastically increase loss.",
    "Abstract": "we qualitatily demntrate how an active vision systm aligns wih hman percepion for structurallyimages. O mageNet-A, naturally occurrin hard weshow how distinct uliple fix-ion yield perfrmance gins1. contrat, neuoscience suggsts that roustldentifies salin objecteatures by ctivelyswitching ultiple fixatonpints (saccades) andprocessing with non-uniformreolutio This inormation is prcessed via pathways: he dorl (whee) and ventral what)trams, which identify relevant inpt porons and irreevant details. By learning fused, glimpss atmulple distinct pints, these methods significantly enhance th robustnssf netorks, cheving 2-21% increase in acuracy. Building we outline a deep learning-base active dorsal-vental visionandadapt two pror ALcon GFNt, within thi framework o evlue theirrobustness."
}