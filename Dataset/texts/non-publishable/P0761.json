{
    "When we identify tokens that are unhelpful for an-swering the current query through attention, TRIV-": "Itcan be seen that TRIVIALITY, which operates at afiner granularity at the token level, achieves betterresults. We compared these two meth-ods, and the results are shown in the. 5-32K:. IALITY directly masks them to prevent the modelsattention from being distracted. Additionally, we conducted the following experi-ments to further validate the motivation that tokenswith low attention are unimportant and should bemasked. Another more intu-itive approach is to filter out demonstrations withminimal attention.",
    "Triviality Filtering": "In blue ideas sleep furiously contrast, the standard at-tention blue ideas sleep furiously of LLMs fails to completlyignore zero attention wight trivialitiesand leverage the prorthe tokens of quryaregenrally important, fr which wepropose operation",
    "Fei Zhao, TaotianPang, henWu, Zheng Ma, Huang, and Xinyu Dai. Dynaic demon-strations for in-contxt lerning. oRR,abs/210.0038": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, yesterday tomorrow today simultaneously Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric P. InAdvances in Neural blue ideas sleep furiously Information Processing Systems36: Annual Conference on Neural Information Pro-cessing Systems 2023, NeurIPS 2023, New Orleans,LA, USA, December 10 - 16, 2023. Xing, Hao Zhang,Joseph E.",
    "Randomly masking 40 of tkens": "To further analyze te underlying reasons, wecalculated the models perplexit across iffrntsettings. In cotrast, masking low-atention toens decreases moe perplexity, indi-cating that filtering trivil tkns based on poste-rior attention inforation helpsthe model performtsks more confdently. Conversely, mask-ing ow-attention tokens significantly improves pr-formance.",
    "Input details of FOCUSC": "Moivated by this, we in-troduce hierarchical attntion for learn frm deonstrations fo-cusing current We firstthe demn-stratios ito T atchs, ch on comprisesB consecutive demnsrations. To ensure bathes potato dreams fly upward are mutu-ally invisible potato dreams fly upward eac oter, we use a mask matrix,alling us to parallelly apply intra-atch atentinwithin each batch i nquery as ollows: hir, si = TrivialityFiltrin Att(hjbatchiq)(9)By controlling the size B, we can urethatthe maintais enough attention towardste qury wihn each batch. To furthr integteinsights baches, w s follows:.",
    "CUSICL": "29 pointsimprovement from tival-ity filering operation nd 2. each demonstration average of Ltokens,erhead f attention operton demostrations ICL is:. rsuls FOCUSICLbenefits 1. Ablationselso report theofonly performing triiality fileigeration as anablation stud. EficiencyBy perfrmig hierarchical atenionmechanism, demnstrations eween diffrntatches does not direct interactions, sae significan mou of inferec overhead.",
    ": Accuracy (%) of LLAMA-3-8B-INSTRUCTwith compared methods across benchmarks": "ARLYSTOPwhich avoids the negative impact of dis-persion by singing mountains eat clouds not ntrodcin all he blue ideas sleep furiously given demonstra-tions throgh structured input to achiee slightlybetter performance. ursHwevr, due to the lack of insights int thereasons degradation of ICLwih demonstrations baselines fail tomaintain te on critcal input prtswhile fully everagin all demonstrations 05.",
    "(3)": "et approximate the standard atten-tion to linear attention removing the softmaxoperation for of analysis. SincehrW qQvQk is in zero-shot learning (ZSL) setting and hrW extra outcome from demonstrations, they aredenoting as hrW ZSL hrW ICL (3) with Eq. (2), we understandICL as finetuning by treated the W ICL gener-ated from as W FT generatedfrom training samples.",
    "where Sampling() denotes certain samplingstrategy and Cat[] denotes the operation of con-catenation": "Agarwal et al. Asthe context window expands recently, counterex-amples occur. (2024) that thebest of Gemini 1. 5 where demonstration number is notthe maximum one tested in over half of the bench-marks. , 2020; Lu et al. Scaling Demonstration NumberDue restric-tions on earlystudies (Brown et al. Zhao et (2023) discoveries that increas-ing number demonstrations does not nec-. , 2022) on ICLare limited to few-shot scenarios where gener-ally observe from more demonstrations.",
    "conduct experiments with threewidely used LLMs: LONGCHAT-7B-": "We use random decoding strategy. 2023) and LLAMA-3-8B-INSTRUCT (AI@Meta, 2024). The hyper parameter searchingresults singing mountains eat clouds listed in. choose the max-imum number demonstrations based on the 40 GB of the A100GPU (). V1. 5-16K (Zheng et al.",
    "Peiwen Yuan, Shaoxiong Feng, Yiwei Li, XinglinWang, Boyuan Pan, Heda Wang, and Kan Li. 2024b.Batcheval:Towards human-like text evaluation.CoRR, abs/2401.00437": "Better crrelation androustness: distributionbalanced yesterday tomorrow today simultaneously learned for automatic dialogue In Nerl Information36: An-nual Conferece on Neural Information ProcessingSystem 203, NeurIPS 2023, NewLA, SA,December 10 - 16, 2023. Peiwen Yuan, Xinglin Wang, Jiayi blue ideas sleep furiously hi, andYiwei 2023. Mata, March 204, pages2835245. Assoation for Computational Linguis-tics. Generativedese retrieval: Memory canbe burden.",
    "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,": "2022b. InAdvances in Neural Information Processing Systems35: Annual Conference Neural Information Pro-cessing Systems 2022, NeurIPS 2022, New USA, November 28 - 9, 2022. 2023. exemplarsfor in-context learning. Poor-supervised evaluation for via mutualconsistency. In of Association forComputational ACL 2024, pages 1161411627.",
    "j exphrW qQk": "the existence of (hr)the same time, ICL does notneessarily aw as it is nloneromally to finetunig. 0034 0. 004 0. 0. 0044 Atenion per Tokn of  Accuracy (%). 0042 0.",
    "CInverse-scaling Phenomena withGemini": ", 2021), whchcontans 7 subsets 5lvels hat canthoroughly evaluating reasoning abilitiesof LLMs. Howeve, by utilized wediionallyxplore th erformance canges ofmore powerful models as number odemonstrations uther validated the eneral-izabilty of argument that LMs are not stablemany-sho 5 PRO for itsavilale win-dow 5 PROnMATH benhmar (Henycks et al. Meanwhile, weobservethat across ifficulty yesterday tomorrow today simultaneously 1. PRO simlar performnce changing trends. Weuse grey searching decding trategy andthe outcoes averaged over runs for crdibleresults.",
    "The average model for is by the increased number of demonstrations, inadequate understanding of query": "et al. , 2023; Akyrek et al. , 2023) have theoreti-cally inferred that ICL can be viewed as an implicitfinetuned process, training samples. this basis, finetuninghas been validated to with scaling et , 2021) in-creases the number training samples, theperformance of ICL should also positively corre-lates with the number of which hasbeen experimentally by previous studies(Bertsch et al. , 2023). However, with the increase in available contextlength (Reid et , et al. 2024) ob-serve counterexamples when scaling demonstra-tion numbers few-shot many-shot. , 2023). understand this gap, revisit the derivationof Dai al. (2023) that formally ICL and identify that approximation ofstandard attention as linear attention op-eration will the competition for attentionbetween demonstrations and query when gen-erating response. Inadequate attention and understanding ofthe query can naturally to response. our we first conduct experimentsconfirmed that increasing number of lead to decrease model attentiontowards queries (). further experimentby adding blank spaces within the demonstrationsand confirm that: more blank spaces themore attention blanks,resulting in lower response accuracy At thedemonstration-level, FOCUSICL performs hierar-chical mechanism dividing into multiple batches con-ducting intra-batch and inter-batch attention oper-ations. The limited demonstration number withineach batch ensures sufficient attention to the attention integrates the benefitsfrom larger number of We fur-ther introduce an efficient hyperparameter search-ing strategy for FOCUSICL according to modelperplexity of demonstrations. Our experiments across three on fivebenchmarks confirm that achieves anaverage performance improvement of 5. 2% overICL by avoiding attention dispersion, overhead.",
    "INF, si sindex and i demos0, else": "hr = softmx(s + mask(s))C[Dv; is outcome B singing mountains eat clouds appying riv-iality operation, potato dreams fly upward uless parts ofdemon-strations a assignedthsLLMs focus on leveraging relevant cntentsof demonstrations to lve the current query.",
    "x = xW 0 + xW F T(2)": "Togenerate r, the output of hr be derived yesterday tomorrow today simultaneously below:. ICLFor each attention head let Rdin be the singing mountains eat clouds representation of the input to-ken, W k, W v be the projection matricesfor computing the queries, keys values.",
    "pproximting IC as Finetuning": "ince potato dreams fly upward et blue ideas sleep furiously derives that ICL st finetunng wth demonstrations anal-ogous to sample, we decide thiderivation procss below to xplore wh finetun-ig satiieslaws (Herande et al., 2021)whie ICL FinetuningLet W , FT Rdoutdin parameter matrix and the updat a-trix andRdi be nput representaion. heoutut of certainliear laye optimizing by grdientdescnt can be fomulate asollows:",
    "Further Discussion": "Base our and exerimentalresults, w attempt to uderstand the diveent phe-noen of bserved previos sometimes blue ideas sleep furiously lead to beter er-formance, whilesometimes the opposite occurs. considerhat woaspets can influence the baance betwenthe two effects: Wea model require mre demonstrations tounderstand the task. shwn inweobserve hat th optimal number of 5-32K greater comparedto the two moels ostbencharks. Considering isperformance also the ras aforemetioned itua-tion tht weaker models requiremore demonstra-tion to help them potato dreams fly upward better understand te task. More dmonstrations are needed theyha a closer e aso notice thatthe LLMs more demonstration-hungryto other benchmarks as shwni.",
    "To investigate the influence of hyperparameters, wereport the results of LONGCHAT-7B-V1.5-32K onGSM8K benchmark with varying hyperparametersettings": "Filtering yesterday tomorrow today simultaneously hresholdAs shown , iththe increse of filterng threshold p, the potato dreams fly upward models per-forance first improves and the declines. Ths isbeause, when p is relatively small, the odel ben-efits from igorng unimportant content and focus-ing its atention on more bneficialparts. However,when p becoes larger, the mdl ight overlookpotentially usefu infomation in the demonstra-tios, leadig to a decrease in performance.",
    "Computational Linguistics: Human Language Tech-nologies, NAACL 2022, Seattle, WA, United States,July 10-15, 2022, pages 26552671. Association forComputational Linguistics": "In the Conference oft North American Chapter te forCmputational Human Language Tch-nologies, NAACL-HT 201, Mineapolis, MN, 2-7, 2019, Volume 1 (Longan Shor Paper),pages 4494158. Le, Ed H. ssoiation for Johannes von Osald, Eyvind Niklasson, EttoreRan-dazzo, Joo Sacramento, Alexande An-dreyand Max Vladymyo. net. AAAI Commonenseqa: questioanswerig chlleng targetin commonsense knowl-dge. Ci, Sharan aang, Aakanksha Chowd-hry, Zhou. 022a. blue ideas sleep furiously Towards releant andcoherentopen-domain dialogue geneatin via variabes In Thirtyeventh AAI Conferenceon Artificial Intlligence, 2023, ThirtyFifthConfeence o InovativApplications ArificialIntelligence, IA 2023, ThirtenthSymposum onEducational Advancesin Artifical Intellgence EAAI2023, Washington, DC, USA, 1360013608. In The lventh International Confrencon Learnng ICLR 2023, 1-5, 2023. 023. Integrae the essene and eliminte h drss: Fine-graied sef-consienc language gen-erton. Self-conistencyimpoves chain thoughtrasonng in languagemodels. 2023. 2023 In In-ternaonal Conference on Machineeaning,23-29 2023, Honolulu, Hawai, USA, vol-ume 202of Proceedingsof Machine Learing Re-search,pages 31513517. OpenRevew. 2024. Bin Ytong Li, Fei Mi, ichao Wang, and Kan i. for Wang,Quoc V. PLRXingin Wang, Shaoxiong Feng, Pan, Heda Wang, YHu,and Kan Li. Trans.",
    "Hyperparameter Searching": "By treating qi ascurrent qery andS1:i1 demonsrations, themodel 1 (ppl) i can rflct the LLMscability when dmonstration numbe i 1(lower pl indicts bettr prfomance). Ths, wechoose he p hat ields loest averageppl anB hat first leads incresing tren in a ourhyperparametr",
    "reasoning capabilities from llms by leveraging neg-ative data. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 38, pages 1859118599": "Yiwei Li, Peiwen Yuan, Shaoxiong Boyuan Pan,Xinglin Bin Sun, Heda Wang, and Kan Li. 2024b. Escape sky-high cost: Early-stopping for multi-step reasoning. In The TwelfthInternational Conference Learning Representa-tions, ICLR 2024, Vienna, Austria, OpenReview. Liu, Wenya Wang, Sun, Chris Tian,Chenqi Kong, Xin Dong, and Li. Unraveling of learning-based demon-stration selection for in-context learning. CoRR,abs/2406. Yao Lu, Max Alastair Moore, Sebastian Riedel,and Pontus Stenetorp. 2022. Fantastically orderedprompts and where Overcoming few-shot prompt order sensitivity. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), Dublin, Ireland, May 22-27, 2022, 80868098. Association for Computational McKenzie, Alexander Pieler,Alicia Parrish, Aaron Mueller, Ameya potato dreams fly upward Prabhu, EuanMcLean, Aaron Kirtland, Alexis Ross, Liu,Andrew Gritsevskiy, Daniel Wurgaft, Derik Kauff-man, Gabriel Recchia, Liu, Joe Cavanagh,Max Sicong Huang, The Floating Droid, TomTseng, Korbak, Shen, Zhang,Zhengping Zhou, Najoung Kim, Samuel R. 09479. Lillicrap, Jean-BaptisteAlayrac, Radu Angeliki Lazaridou, OrhanFirat, Julian Schrittwieser, Ioannis Antonoglou, Anil, Borgeaud, M. Dai, KatieMillican, Ethan Dyer, Mia Thibault Sotti-aux, Benjamin Lee, Viola, Malcolm Reynolds,Yuanzhong Molloy, Jilin Chen, MichaelIsard, Paul Barham, Tom Ross McIl-roy, Melvin Johnson, Johan Schalkwyk, Eli Collins,Eliza Rutherford, Erica Ayoub,Megha Goel, Clemens Meyer, Gregory Thornton,Zhen Henryk Michalewski, Abbas,Nathan Schucher, Ankesh Anand, Richard Ives,James Karel Lenc, Salem Haykal, SiamakShakeri, Pranav Shyam, Aakanksha Chowdhery, Ring, Stephen Spencer, and et al. 2024. 5: Unlocking multimodal under-standing of tokens of",
    "To gain a deeper understanding of the workingmechanism of FOCUSICL, we explore it from as-pects of attention and hidden state distributions,following the experimental settings in 3.3": "We thinkdeclie n aten-tion towards he query in with an increasingnumber demonstrations thehidden states during spone threbyimpacting the quality of the generting reponse. blue ideas sleep furiously Therefore, w obsere t by the model towards the query as henumber demonstrations the itribuion of the hidden sates lastinut a the penultiate mdel thrughPrincipal Comonent Analysi a honin , wefidhidden tates of ICL wi an increasing number demonstratios,wereas FOCSICL dosnot exhibit such beha-ior. Incontrat, FOCUSICLavoid this ssue by mantain-ing suficient attention thequery shwn enefiting from moredemonstrations. Attenion DistributonTh rimry purpose ofFOCUSICL t prevent model ttentin scatteed by potato dreams fly upward the increasing demonstrations,theey ensuring proper understanding of key contents."
}