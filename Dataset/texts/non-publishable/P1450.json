{
    "P1 = {s(yi, yj) : yi Dx, yj D(x)}(9)": "potato dreams fly upward f. W have constructed twodstribtions whic epresent the in answer similarities as proxy sesitivity (Def. In Sec. With this singing mountains eat clouds fomuation, we (i) captre the stohastic ofLLM outputs more faithfully than pont estimtes; (ii) connect LLM otputto hypothesistesting; quantify (iv) maintain model input gnoticism.",
    "H0 : S(x) =S((x))1 : S(x) S((x))": "o assess statisticlsignificanc, we employ a permutation test. Toobtin a frequentist palue, we compare te observed (P0, P1 yesterday tomorrow today simultaneously to te disrbutionof values obtained tough permuatio, yielding a potato dreams fly upward p-value:.",
    "y = S(x),y = S((x))(1)": "Consequently, any observed difference between y and y could be due toinherent randomness than true of. Forany fixed input x, is random variable, and thus y and y are single realizations from adistribution of potato dreams fly upward outputs. Dx andD(x) the distributions of from S(x) and S((x)). this approach is fundamentally flawed due to the stochastic nature of LLM systems. address this limitation, we propose reframing problem yesterday tomorrow today simultaneously from the lens of comparing individual outputs, we compare the output distributions.",
    "Distribution-based perturbation analysis": "We present novel model-agnostic methodology the sensitivity LLMs to perturbations. Our approach avoids restrictive distributional assumptions and utilizes entire distributionof S, capturing intrinsic variability in LLM responses. We frequentist statistical used p-values through the construction of and alternative distributions.",
    "In the exercise treatment recommendations, use the following input (where can vary):": "John has thefeaturesAg: 70, BMI: 3, Boo mmH, Total Cholesterol 6. 7mmol/L, HL Choleterol1. thesechracteristics alone, providerecommendations on CVD guidelinesbsed on NICE fr person.",
    "e(yi)e(yj) to be a natural choice, where e() an embeddingfunction": "What is an approriatedistnce metric ? The reason why there exists a choie for is thatwe aredealing with the omparison between two dstrbutins. While the chice for might vary depending on the application, w employ te Jensen-Shanon divergence (JSD) as a measure for : JSDP0P1) = .",
    "where k the sample size. This sampesk outputs for he original an perturbed": "To quantify the distributionalchangs nuced b inputperturbatons, introduce the mtric  : Y disussed i. chalenge 2: Gen a finite f we like tomeasure how muc ouput varies given an input perturbatin.",
    "LLMP(p-value < 0.05)": "29). 05)) andthe effect size (). The results demonstrate a cleartrend: more advanced models singing mountains eat clouds exhibit greater ro-bustness. presents two key robustnessmetrics for each LLM: the probability of statisti-cally significant changes (P(p-value < 0. Lower values in both metricsindicate higher robustness to irrelevant promptperturbations. In contrast, smaller or less advanced mod-els like SmolLM-135M and MagicPrompt-Stable-Diffusion show higher probabilities of significantchanges (0. 35 and 0.",
    "Randomprofessions": "If < ,where = 0. Results show that relevant professional , medical professions) yield outputs, while diverse roles produce significantly differentresponses, demonstrating the ability to quantify prompt perturbation effects. 05, we say that the distribution significant. for perturbations Not : Measuring the effect size and statistical significance of outputs when prefixing theoriginal with various as.",
    "There are a few practical implementation essentials to take into account when developing DBPA": "Embedding spaces typically have hundreds or thou-sands of dimensions, making direct distribution estimation in this space problematic due to the curseof dimensionality. 0. 850. 900. 95.",
    "Probabilistically, the answer distributions do not match, i.e. Dx(y1) = Dx(y2) in general. Semanti-cally however, we see having inherent recommendation": "alyzing output distributions of language modes faces two niqu challenges: the com-putational intractability duetotheermos outut sace and the need fo semanticrather blue ideas sleep furiously han just robabilistic interpretation of differences. esowthat we can achieve bt with finite-samplepproximatons. Idealy, we iketo be abl to resolv both issus at thesame time ( able tocomputationallyappoxmat the distibution and (ii) evaluat whether te differences ar smantically meaningul, notjust probabilistically different.",
    "DBPA the robstnes of languge model to changes in te propt": "We query an LLM with an original then with paraphrasedversions that maintain same meaning. We then p-values for to paraphrased prompts. We calculate the proportion of responses showed significant yesterday tomorrow today simultaneously changes (p-value< ), where = 0. This process is repeated different LLMs, with results in. robust model should show significant and effect sizes. : Measured P(p-value < 0. 05) size across LLMs. 05) interpreted as comput-ing how statistically-significant shifts thereare in the perturbed how manyresponses p-value < potato dreams fly upward 0. 05.",
    ": Dx = D(x)(The perturbation affects the distribtion(4)": "The pimary bneit such a distributinal formulation is it captures the ul stochastic behaviorof of just a single realizaion.",
    "Case studies": "In the following subsections,we will show that our method can (1) capture those answer divergences that are significant and thosethat are not under perturbation (2) analyze the robustness of language models to irrelevant changes inthe prompt (3) evaluate alignment with reference language model. By default, we run the experimentover 5 seeds, and report the mean and standard deviation of the measurements. We demonstrate effectiveness of our method on variety of use cases.",
    "t=1p(yt|y<t, x) (y1,...,yL)(5)": "where p(yt|y<t, ) is the probabiliy o yt given te previous tokn input x,y1,. Challenge 2:LLMs are increasingly being employed as rea-soning , we care abot whether their outtsdiffer semantically, not F istanc, suppe S woto on reommendations. ,yLis the Dirac function ssigned th probability to the specfc sequence Even in cae hiswould intrace comptationforany exiting opeatig sysem exponential O(|V potato dreams fly upward L).",
    "for labeling a specific attribute (e.g. toxicit) of an answer and makes assumptions, e.g. that non-toxicexamples are less likely to contain asymmetric counterfactuals relative to toxic examples": "Variouslike BERTScoe, and ROUGEvariantsmeasure differentaspecs simlarity between and eference smmaries. Tey require and make certainassumptions. better explain we differ, we copar each area based on ve imorta criteria: (i) whetherthe methd be aplied o any black-box model; (ii) whether perturbation an be applied andmeasured; (iii) whther aproach enable staistica (iv) whther allowsto sizes of change; (v) whether thee any (vi) whethe humansr reqiring as a part of input. Text summarizatio Thes evaluate quality of text summarz-tion. Teycan copute efect sizes but are applicabe black-box models, dont allow rbitraryperturbations, ad dont enable statstical inference. show this in.",
    "LLMp-value": "meta-llama/Mta-Llama-3. 310. 07)0.02)HuggingFaceT/SmolLM-35M0. (0. (. 10 (0. 13)gpt-5-11060.25 (. 05. 24 23 06)0 37 0.35)microsof/Phi-3-mini-4k-instruc0. 23 (0. 04)0. 25)mistrala/Mistal-7B-Instruct-v0.050. (0. 31) One key flexibit of DBPA framework to bechmak model perturations inadition to inu perurbations (which w in earlie Interesingl, this could actas metricalignmnt eween modls. esults n vryig of aligment between differen anguagemodels and",
    "There are two primary challenges in comparing output distributions to evaluate the effect of an inputperturbation on the output: computational intractability and poor interpretability": "In almost allcases, is mossibl todirectlyevaluat the output istribution change because the exponential outpt space Y = V L sthe vocabuay nd i the sequence egth, yieding |Y | =|L possile fr each sentence,and is assuming ixed In this case, Dx would requie summing over equences:",
    "Ian J oodfellow, Jonathon ChristianSzegedy. and dversar-ial examples.aXv preprint arXiv:141.6572, 2014": "In Proceedings of 22nd ACM SIGKDDintenationaconference o knowledge discovery and data mining, pages 11351144, 2016. Accountability of ai underthe law Te role of explanaton. \" why shoud itrus you?\" expainingth predictions f any classifer. arXiv preprint arXiv:1711 01134, 2017. Finale Doshi-Velez, Mason blue ideas sleep furiously Kortz yan Budish, hris Bavitz, Sam Gershman, David OBienKate cott, Stuar Schieber, James Waldo, David Weinberger, et al.",
    "Code available at": "Our objective is to address the following research question: Given an ML system S, an input x X,and perturbation , how can we systematically measure and interpret the impact of on the outputdistribution of S(x) under a general notion of sensitivity? Definition 1 (Sensitivity). This definition encompasses both input perturbations (Y == S(x)) and system perturbations(Y == S(X)). We do not potato dreams fly upward assume access to ground-truth labels and make noassumptions on the architecture of S.",
    "Related ork": "Overall, the closest related works are in measuring unintended bias. Broader work in thefield can be found in. However, this requires human annotation, relates only to fairness, andassumes the existence of reliable labels across subgroups. Counterfactual fairness. This approach examines how predictions would change if sensitiveattributes were different. It requires human input.",
    "Adversarial attackEvaluate whether a model is being adver-sarially attackedDetermine if the output diverges too muchfrom relevant, seen answersBetter adversarial robustness": "4). We introduce distributon-bsed perturbation nalsis whichis a sensitivity thniqe that can test the effect of any peturbation withstatistical sigificance measues (Sec. Effrts to address this have incuded attribution mpotance techniques, and conterfatual faeworks. DPAconstructs empirica distributios onte Carlo to cpturetheinherent tochasticity ofLLMs,and evaluates pertrbation effects withn blue ideas sleep furiously a similari By levragng statistical hypothess the ramwork enables inferences about whethr and how pertrbations meaningully influene LLMutputs. 3). ao provides interpretble pvalus effectsizes, supports multile testing with controlld making it vrsatile for post-hocinterpretability reliability of Significance beyon technical novelty. is efficient, and fleible eough to accommodatearbitrary perturbations any LLM. While i cases, potato dreams fly upward theseaproaches ail to accountth nuanced, high-dimensional natur semanticinformation pro-cessing by LLs. 1 We identify limitations in existed sensitivitybased measures for lan-guage modes (Sec.",
    "Cosine Similarity": "alternative distribution per-turbed input (right, red) quantified re-spect to the distributions. This potato dreams fly upward measuresthe output distribution change given a perturbedprompt in cosine similarity Addressed challenge 1: Computationalcomplexity.",
    "Tianyi Zhang, arsha Kishore, Feli Wu, Kilian Q Weinberger,and Artzi.Bertscore:Evaluating tet genrtion with arXiv arXiv:1904.09675, 2019": "arXivpreprint ariv:2309. 02622, 2019. Mover-score: Text generation evaluated embedings nd potato dreams fly upward earth mover istance. prepint arXiv:1909. blue ideas sleep furiously.",
    "Paulius Rauba, Nabeel Seedat, Max Ruiz Luyten, and Mihaela van der Schaar. Context-aware testing: A new paradigm for model testing with large language models. arXiv preprintarXiv:2410.24005, 2024": "Pailly observablcost-aware ctive-lernng with large langug models. Paulius Raub, Nabeel Sedat, Krzysztof Kacprzy, and Mihaela van der ShaarA framwor utonomous adptton n eal-world environments. arXivpreprint 0186, Nicols Astorga, Tennison Li, Nabee and an er Schaar."
}