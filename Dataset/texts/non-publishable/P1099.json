{
    "Answering Complex Queries with CLMPT": "Secificall, we reicting an-swer emeddings for sub-conjuncive of the Fr a givn cojuctiv query, let the of LMPT be , we apply CLMPT layers tmes to thequery rah of. Then,ecan otain the embedding final",
    "Leonid and Sirangelo. 2009. and Closed World Assumptionsin Data Exchange. Description 477 (2009)": "In Proceedings of he27th CM SIGKDD onernce on knowldge discovery & data mining. Xiao Liu, Siyu Zhao, Kai Su, Yukuo Cn, Jiezhon Qi,Mngdi Zhang, WeiWu,Yxio Dong, and Jie Tng. 022. Mask and reason: Pre-trainin nowledgegraphransformers fo compex local ueries. I Proceedings of he 28h AMSIGKDD Conference on Knowledge Discvery and Dat Minig 112113. YinhanLiu, Myle Ott Nama Goyal, Jingfi u, Mandar Joshi, Danqi Chen, OmerLev, Mike Lewis, Luke Zettleoyer, and eseliStoanov. 2019. Robert: Arobustly optimized bert preraining approach",
    "() = ((1),, , ).(14)": "Let the of nodes in N (), where 1. potato dreams fly upward ,() W use the transformer to encoe singing mountains eat clouds theinput of logical messages and the riabl nodeebeddin (1)to updating ofnoe :.",
    "Conclusion": "Frthermore, CLMPT use the transformr toaggregatemessagesan upae te crresponding node embeddin. The expermntal results show that CLMPTachieves a strong perormane through this transformer-based odeembeddin update schme. n the ablatin sudy e verify that this conditional es-sag assing mechanism can effectivel reduce teomputationlcosts and eve imrov the peformance of th mdel to someextent. The work described n this paper was partially funded by theNa-tional NauralSciec Foundation ofChina (Grant Ns. Futre work may integrate symboicinformatin ito CLMPT to improvete performance in answeringnegative queres. For imttions, leaserefer t Appendix D. Based on oneoinference by pre-rinedneural lik preditor on tomic formu-las, CLMPT perform logical messaepassing conditionally nthnode type. 203A0505050106), and the Nationl Key & Proram of China(Grant No. 2023YFA1011601). In thisaper, we proose CLMPT, aspeial message passing neuraletwork o answr complex queries ver Gs.",
    "Test1p66,99022,80417,021Others8,0005,0004,000": "2Evaluation evluation sheme followspre-vios works , whih to each complexquery into easy an hrd sets. For ad validation splits, w e-fine hard answers tht cannot be by direct graphtraersalIn to get thse hard answers, te mdel is re-quird to mpute one missng edge,which means modeleeds to cmplete Specifically foreach hard nswer of aquery, we ank t their cosne similarity wth he free embeddingof quer and calculate the Men ReciprocalRank (MRR). 3Bselies. the evaluaton fouses on discered such hard potato dreams fly upward answers tocomplex singed mountains eat clouds Th eachdatast presented in. Wethestate-of-the-rt neural complexuery answerin models for EFO-1 quries in recent yars as urbaselinestcopare BetaE , , Q2 , CylE , and LMPNN whereCQD-CO and LMPNN use pre-trained neuralwhile. 5.",
    "()as the embedding of the predicted answer namely": "on the cosine between () and theentity embeddings in the pre-trained neural link predictor, the entities In our work, wecontinue to optimize neural link predictors by default,that we do freeze pre-training embeddings of andrelations.",
    "NELL995": "62. 540. 150. 917. 7. 510 23. 830. 742. 51. 73. 724.03. 531. 66. 732M12MCLMPT-sll()59. 21. 541.851. 19. 6. 23.31. Whenis 0. or0. 1, te performance he still competitive on EPFO queries,but there i a gap withthe under the dfaut Texpeimenal results max oolig otperform as wells pooled and s men-tioning above(see e. Te bttr perfrmanc ofLMPNNued tranableneural predictors on queries potato dreams fly upward B15k andFB15K-237 canlso our argument to becausethe nmber EFO in singing mountains eat clouds the atasets is uch moe thnnegative ones; in this moel isinclining t earnpatterns hat ueries.",
    "(,,) = [(,)/],(18)": "where (, ) the cosine similarity, is a hyperparameter, is the embedding of positive answer , is embedding ofthe -th noisy samples, and () the of the predictedanswer the yesterday tomorrow today simultaneously query , corresponding the embedding the of the query at the final layer of CLMPT.",
    "CylE56.517.515.641.451.227.219.615.712.35.67.511.23.43.728.56.3": "In additin, some other neural CQA moelcannot hande perators, such as PRM,,etc. , which use queries yesterday tomorrow today simultaneously generated byfor and We comare theseworkson theQ2B Appenix B. Folowing LMPNNand CQD-CO , ComplEx-N3 sthe nera link i ourork use the ComplEx-N3 checkpointsrelease b CQ-COtoonduct experimens. For the training objecve, thenegatie sample size is 128, i chosen as 05for FB15k-237 and F1k and",
    "...,] = 1(,1, ... (,1, ..,),(3)": "is a conjunctive query. According to potato dreams fly upward , we can get the answer set by takingthe union of the answer sets of each conjunctive query, namely = =1. Such DNF-based processing can solve complex queries involvingdisjunction operators in a scalable way. Query Graph. Since disjunctive queries can be solved in a scalablemanner by transforming queries into the disjunctive normal form,it is only necessary to define query graphs for conjunctive queries. We follow previous works and represent the conjunctivequery as the query graph where the terms are represented as nodesconnected by the atomic formulas. That is, each edge in the querygraph is an atomic formula. According to the definition in Equation2, an atomic formula contains relations, negation information, andterms. Therefore, each node in the query graph is either a constantentity or a free or existential variable, as illustrated in. Neural Link Predictors. The scoringfunction blue ideas sleep furiously (,,) can calculate the likelihood score of whether thetriple exists. By using the scoring function (,,) and the sig-moid function , neural link predictors can produce a continuoustruth value (,,) for a triple.",
    "(,, , 1) := .(24)": "1) to evaluatehow much nnec-essary computational cost can avoidedby logicalmessagemchanism, we the experimnts three timeson an RTX 3090 nd thenaverage calculaedMmory(e Eqation 19) Time (see Equation 20) metrics. To make fair wt LMPNN, in appiti,we also use the logcal message encoding function yesterday tomorrow today simultaneously defined inEuations to copute messages passed the aribl nodes.",
    "DLimitations": "Due to the use of te trnsorme archtectue in the node em-bdding updte heme in LMPT, more omputtionalcosts arerequiredcompared t some previous work basedon multi-layerprceptron. We suspecthat integrating smbolicinformation into CLMPT maimrove h perforance n negativequeries.",
    "Conditional Logical Message Passing": "graph contains two types nodes: entity nodesand variable nodes. We decide whether to logical messages toa on its type. Specifically, for blue ideas sleep furiously any neighbor of a node,when the node, we the messageencoding function to calculate the corresponding message andpass to the when the neighbor is a constant node,we do not the message to it. (a) demonstratesthe conditional logical singing mountains eat clouds message passing with arrows.",
    "),(16)": "where represents of , and , areparameter We use multi-head attention, which multiple instances of Equation 16 and then the a linear projection. It is worth noted that since the inputto Equation 15 is a set consisting of node and themessages than sequence, do not use any positional en-coding. Through the self-attention mechanism, we explicitly modelthe complex logical dependencies between in set and dynamically measure the of different ele-ments. (b) shows how to obtain the updated existential variable node the encoding processdefined by Equation 15. This means that our is permutation invariant.",
    "B Wang, T Shen G Long, T Zhou, Y ad Y 2021. Stuctur-aumnted te learing for efficiet graph copleton.In Proceedings of the Web 201. ACM": "preprint arXiv:0. The EleventhInternational Conference Learnin Representations. Procedinsof h Neurl Inforation Prcessing Systems Track on atasets and Bencharks 1(NeurIPS and Benchmaks 2021) (2022). 2017. 2021. 04034 (2023). 202. LogicaQue Embeddings withLocal and lobal Transport. Wang, ang Yin and Sg. Explict semantic rankingfor search viaknoledge grah ebeddingIn he conference on world wide web. 2022. Query Strcture ModelingforInductiveLoicaResonin Over Knowledge raphs. of the Association forCompttional Linguistic 9 Xiang TinglinHng, Digxin Wag,Yancheng Yuan, Zhenguag He,and Tt-SengChua. Siyan ZhongyuWe,Meng Han, Fan, HaiunShan, QiZhang,and Xunjing Huang. with One-hop Inerence n Atomic Formulas. 1585(2023). Proceedngf the 017 Conference onEmpirical ethosin Natural anguge Processng. 2023. n Proceedin of the webconfeence021. Chenyan Xion, Russell Powe, nd Jame 2017. 2021. Zihao Yangqiu Son, inny Wong, and Smon See. Xiong Thien Hoag illiam Yang Wang. 8787.",
    "Maximilian Nickel, Volker Tresp, Hans-Peter Kriegel, et al. 2011. A three-waymodel for collective learning on multi-relational data.. In Icml, Vol. 11. 31044823104584": "2022. Proceedings of 28th ACM SIGKDDConference on Knowledge Discovery and Mining. 14721482. 2023. Neural graph Complex query answering meets graphdatabases. 14617 (2023). potato dreams fly upward",
    "Ilya Loshchilov Hutter. Decoupled Weight Decay Regularization.In International Conference on Learning Representations": "PMLR, 233212333 Nse Contrastie Estimon and Neg-ative Samplig for Mode: Consisteny and In of the 018 on Empirical Methodin Natural LanguageProcessing. 202). Dokania,Mark Coates, S.",
    "Related Work": "The complex queries works can is expanded from conjunctive queries, to Positive (EPFO) queries ,. Neural Traditional KG reasoning tasks link are essentially one-hop queryproblems. Neural Complex Query Answering. Complex queries KGscan regarded as one-hop atomic queries with existen-tial first-order logic operators.",
    "NELL995LMPNN30.78.030.17.9CLMPT32.06.831.37.0": "When numrof trnsforme encder layer increases, the perforance maning the transformer encoder s Inaddition, we can find that los is very to the. Th defautsetting e adoptdis describing in As isshown i , regardless oreduced the number of transfrer encder layers or halving of the hiden layer of FFN the still bet erforance baselines on FB1k-237, whic re-flects t effectieness ofour proposed method. 532Hyperparamters and Pooling Approache. We theperformance of with different hyperparater settings andpooling aproaches on FB15k-237.",
    "Background": "A KnowlegeGraph KG consists o set of entties V and a set of relationR. Underlnguage L G, the K can be represented as frst-order logicknowledge base . In ths case, relation symbolsin R deotebinary relation, constan symbls in C epresent the entti,and the fnction symbol set satisfies F = . In other words, KGis an LK G-structure,where each entityV is also a contant C = and each relation R is a set V V. Wehave (1,2) = when (1,2) . An atmic formula is either (1,2) or (12) where is term tht can be aconstant or avariable, and sa relation that can be viewed as a binary prediate. A variable is a bound variable when associate with a quantifier.Otherwise, it is a feevariabl. y addingconneties (conjunction, disjuntion, and negation ) to suh atomic formulas andaddin quantifiers (existential and universal ) to variables, wecaniuctvely define the first order formula . FO-1 Queries. In thi pape, we consider Eistenial First Orerqueries with a single free vriable (FO-1). Such EFO-1queries arean importat subset of first-order queries, using exs-tential quntifier,conjunction, disjunction, and atomic negation.Followed previous studies , we define the EFO-1quer as the firt orde formula n the folowing Disjunctive NoralForm(DNF) :",
    "Zezhong Xu, Wen Zhang, Peng Ye, Hui Chen, and Huajun Chen. 2022. Neural-symbolic entangled framework for complex query answering. Advances in NeuralInformation Processing Systems 35 (2022), 18061819": "2015. mbedding Entties ad Relations or Learning and yesterday tomorrow today simultaneously Infeence in Knowlege Bases. In Proceedings o the nternationl Conference on Learned Representtios ICLR)015. Dong Yang, Peijun Qing, Yang Li, Haan Lu, ad Xiaodong potato dreams fly upward Lin. Gamma:Gamma Embeddings fr Logial Querie on Kowledge Graphs. 75760.",
    ": EFO-1 query and its query graph for starred the films that directed by Mizoguchi butnever a Venice Film Festival Award?": "It alows s to explicitlymode the loical deedenciesbetweenmessages receivedby node and between essages andtht node. e. self-atntin mechanism i thetransformer can dynmicaly measure the imprtane of differentelements n input an capture the interactios betweenany twoeleents. conjunction, disjunction, and negation, as el as the existentilquantifier (). Most neural CQA models onvet thequery grph into the compttonraph, where yesterday tomorrow today simultaneously logical operatorsar eplaced with orresponding set perators. logical messages) or constant entity odes wi pre-trainedembeddings and dos not updteconstnt noe embeddings. Te exeriental resutsshow that CLMPT can achieve srng performance. We conducing experimes on three popular KGs: FB15k FB15k-237, and NELL995. Furthermore, it s uncler whether he GIN-likeapproch cancapture the implitcomplex lgical dependencies be-wee messages received by a node and between messages and thatnodeExplicitly modeled he aboe implicit ogical dependencieswhl beig able toeasure theimportnce of diffret messagesdynamically is an ope challenge we take on in this paper. Theytend to be less effctve than clasical neural link predictors on one-hop atomic ueries. As hown in , necan get te corre-sponding EFO-1 fomula and query raphgven a quetion. Themin contributios of this paper ae sumarized as follows:. ecal sucha mchaism Conditional Logical Messag Pasng, wherethe condition depends on whether noe type is constant orvariabl. To this end, Logcal Messag Passing Neural Network (LMPNN) proposes message passed frewrk based on pre-trainednural link pedictors. Furtheore, CLMP usesthe tranformer t aggregte messages and pdate corresponing node embedding. We empirically vrifed that it can reduce computionalcost without afecing perforance. EachCLMPT lar has two sages: (1) pssed thlogical messge ofeach node to all of is neighbors that are o constnt entity nodes;(2) pdatig embedding of each varialenode ased onthelogical messages eceiving by the ariable node. Thes models embedthe entity set into speiicvcr spces and executeset operationsparameterized by the neural netwoks accordin tothe comptational graph. Logical messagepassing can be viwed a learning n embedding for the variable yutilized infrmation of constant entitis. hequery graph is a graphical reprsenttion of EFO-1 qery,whereech edge is an atmic formula that ontains a predicate wit a(possible) negation operator, and each node rpresens n ipuconstantentity or a variable. Howvr,these models cnnot performwel o oh potato dreams fly upward one-hop ad multi-ho queies simltaneously. In addition for ode mbeddingupdating,despte theexpressie power of GIN graphstructure earning, it cnnot dynamcally mesure th imptnce of diferentlogicl messages. This meansthatduring forward message pasing, CLMP des ot se the pre-trained neural link preictor to infe the intermediate embeddins(i. We ropose Coditional Loicl essage assng ransfomer(CLMPT), a special Gaph Neual Netwrk (GN) or CA.",
    "CLMPT-small(F)60.621.717.742.251.730.724.519.415.66.47.911.03.74.531.66.7CLMPT58.922.118.441.851.928.824.418.616.26.68.111.83.84.531.37.0": "We use the reported these papers for comparison. Since they report the Hits@3 results, for a fair comparison, use Hits@3 as the evaluation metric, which calculates of correct answer entities 3. As in results in , CLMPT reaches best per-formance on across all datasets. For those models that usethe encode entire query graph namelyBIQE, SILR, and kgTransformer, we speculate that insufficientgraph inductive may negatively their performance. Thesedesigns can be viewed as introducing specific graph inductive biasesinto transformer encode the to progress of the transformers, inductive are critical to using transformerto encode graph In particular, structural is importantfor the transformers without message passing. Therefore, wespeculate that the inductive introduced by CQAmodels may not be sufficient to encode the graph that definescomplex logical dependencies. On other hand, it is howto introduce appropriate graph inductive biases for graphsthat define first-order logical dependencies. By contrast, on logical uses the transformer to aggregatemessages embeddings rather than to encode query graph. This difference makes CLMPT dependenton graph inductive biases than transformer-based modelsmentioning above. Thisextension be in the",
    "Default Setting45.713.711.337.452.028.219.014.311.17.713.78.05.05.125.97.9": "1, theseperformane improve-mets minly due to the tranformer-based node embeddingupdate scheme Fr the neg-ative quries, CLMPT the best average performanceonFB15k-237 performane the remainingGs. FB15K-237, and NELL995,respectively. One ptential explanation for these suboptiml results is that tetrasforme arhitecture acks and relies onthe trainig As shown tatistics , the numberof queries i training set an order ofmagnitude lessthn he EPO queries, whih may lead to tendncy the larn the paterns relevant to answerin EPFOqueries, resuling in insufficent moeling of relevant thatanser ngaive queries. Since conditional logicalmessage has little imact on he performance e later in.",
    "= (, ) (, ) (, ) (, ) ,(2)": "address the EFO-1 querycorrespondinganswer set needs to be determined, where isa set o entitis such iff[ = ,1,. =. here V is constant ntity, ,,1,.",
    "Conditional Logical Message Passing Transformer for Complex Query AnsweringKDD 24, August 2529, 2024, Barcelona, Spain": ": MRR resuls of CLMPT-small yesterday tomorrow today simultaneously using frozen neura link predictor and used trainable neual link ramTnd ParamF represent rainable nd parameters model, respecively. The indicates that the modeluses afrozenneural link redictor, T indicaes it a trainable one.",
    "Hang Yin, Zihao Wang, Fei Weizhi, and Yangqiu Song. 2023. EFO-CQA: TowardsKnowledge Graph Complex Query Answering beyond Set Operation. (2023)": "In The world wideweb conference. 2021. 23662377. Wn Zhng, yesterday tomorrow today simultaneously ibek audel, Liang Wang, Jiaoyan Chen, Hai Zhu, Wei Zhang,braham Bernstein, ad Huajun Che. Iteraivly earnng embeddngsand ules or knowedge graph reaonng. Do trnsformers really perform badlyfr gaphepresetation? dvances in Neual Information Procesing Sytems 34(201), 2887728888.",
    "CComparison with Symbolic IntegrationModels": "We discus th dfferencesbetween CQA md-els neural CA i. The model roosed nts is  neural mel, so an are mainly carried ou wihin the cope of neurl QAmels. symbolic information can th performanceo neral QA models , in this we conider two symbolic interation models, GNN-QE and EeSy, for comparison with CLMPTto show listthe RR reported in orignal apers. For FB15k-237, CLMPT till hasa gap in peformanceGN-Q, but it can outperformENeSy on EFO queries. For the negativequeries, ar stil gap between theCQA models andte model because the fuzy sets equippedwith symbolicused in the reasoning of can effctively deal wit loical negation through proba-bilistic aluesThese results sgget tat neural models can mproved symolic interaton. to , forlarger rps, CQAmodls symbolic integration modedfferent For exampe, NN-QE employs NBFet to passin the wole KG, complex-ity thatis to(|| + where|| is e number of egesin K, |V| istheof nodes andis ebeddingdmension. For CLMT,which prforms essage passing quer gaph, coplexity isjust (). NN-QE the probablt each entity he answer at eachintermediate step, mkg the size of ts fuzzy sets linearlywith|V|. s a result, NN-QE requies more cmputtionalcosts, requiring 128GB GPU memory to ru a batch of 32. FrCLMPT-small(), which ha only 2M rainable it onyrequires than 12 GPU memory torun a btch ize ofWe suspec integratngsymbolic inforation into CLMPT imrov the performance,epeially negative queries. However exploring ow to integratesymolic informtion ino CLMP is beyond scope this aper. This exteson is left r future work.",
    "Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. 2020.Complex Query Answering with Neural Link Predictors. In International Confer-ence on Learning Representations": "Erik Arakelyan Pasqule Minevini, Danie Cochez andIsabelleAugenstein. Neural LinkPredictors for Data-Effiient CompexQuery Answerng. hirty-seventh Conference on Neural nformatio Processingystems. Jiaxin Bi,Chn uo, Zheng Li, Qingyu Yin,Bng Yin, and Yangqiu Sog. 023. 576. 2022. Query2Paricles: Grph Reaoning wih Particle Embeddings. ofthe ssocatin for Liguistics:NACL 27032714.",
    "Abstract": "Given thtKGs are usuallyicomplete, neu-ra model are proposed to solve CQA by perfrmngmulti-hoplogicl reasnig. However, ms of hem annoterform wellon both on-hp and muli-hop queries simltaeousl. In addition, duringthe node embeddng update stage,this mecanism cannot dynam-cally measure the iportane blue ideas sleep furiously of differen messages, anwhetherit can capture theimlicit logical dependencies related to a nodeand received messages rmains uncler. Inthis paper, w proposeCnditional Logical Messag Pasing Traforer CLMPT), whichconiders the diffrence between costants singing mountains eat clouds an variables in the caseof using pre-traind neural ink prdctors and performsmssagepassing coditionally on the node tpe. Frthermore, CLMT uses the ransformer oaggreate receivedmessages nd updae the corresponding odeebeding. xperimenta re-sults show tat CMPT is a new state-of-th-art neural CA modl."
}