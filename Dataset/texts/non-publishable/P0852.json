{
    "Prototype Discriminative Loss based onClass-Wise Embedding": "In IDPLL, we can distinguish the between the candidate labelset and non-candidate set easily, however, it becomesmore difficult to which in the candidate label setis the ground-truth label, as the labels similar toeach other, bringing more label Therefore, to tacklethis negative side of identify the ground-truth label,we use the global information samples to modelsdisambiguation",
    "ABSTRACT": "Moreove, reduce the lbe ambiguity, we introduce the con-cep of lass prototyes global feature information todisabigat candidate label Extensve experimental com-arisons wit twelve method on benchark data ss, incudingfour fine-graining data sets, demonstrate the effecivenes. Th conventional PLL assues he noisy labls are randomlygenertd (instance-independent),while in pactica scenarios, henoisy lael are always instance-deendnt an highlyelaedto te samplefeatures, leaded to the instance-dependent partallabel learning (IDPLL) problem. , the clas-wsembeddings in he candidate label set have high similar-ity, hle class-wsbetweethe candiate aelset and th non-candidate label set hould have issimilarity. On one sid, may promote tainingas the noisy labels can eict the sample to some extent. leverageth IDPLL efectively,th fist time we create class-wise ebeddigs for each sample, which us to explore of instanc-dependent labels, i. On theoter sde it label ambiguity as noisy requie from ground-ruh label. e. partial learning (PLL) sample assocated wit a can-didate set the groud-truth label and noislabels.",
    "(b) FGVC100": ":The classifcation accuracy curvs of PLL methodPRODENinDPLL andPLL ettigs on two data etsCUB200 anFGVC100, her AVG CLs rprsent the aver-age nuber o candidate labels ofeah sample. In the IDLLsetti, the model ha afaster learnng speedin the earlystae of training becuse the instance-dependentnoisy la-bels are rlatd to the sample to som extent which canprovide or supervision in te eary stag of model train-ig.However, in the later stage of raining, th perfrmanceof PROENin the DPL settin is signiicantly inferior tothat in the PLL setting as instance-dependent nois labelsbring mre label abiguity. differentclasses of each sample o gide the learnng of class-iseembeddigs. In IDPLL, th class-wise embddings within the can-didate label set shouldexhibt high similarity to eachother as tecndidat laels cn describethe intancet some extent.Incon-trat, the class-ise emddings betweenthe candidate lbel seta thenon-cndidat label set should display str differences. To be specific, we seec the most high-confidence alfor each sample based on the model output, and then we esuethe cass-wie embedding of this paticlar high-confidence labelis aligned with its crresponding class prototpe while engdis-tance fom other cassprootypes, thereby enhancin th modesdiscriminaiveabiity. The contitinsof our wor re smmarized as follows: To he best of our knowlde w are the first to nrduceclsswseembeddg in IL.",
    ",1,(4)": "where reresent the -th class-wise ebeding and theandidat laelst of respectively.Notethat the clas-wie embeddings avbeen before calculatig he cosine indicates the aerae similarity f classesbeteen the andidate set and non-cndidte label potato dreams fly upward onsidering simiarities of Eqs. 3) and (4) smultaousl,we obtain the class asociative los blue ideas sleep furiously CAL) function:.",
    "Deng-Bao Wang, Min-Ling Zhang, and Li Li. 2022. Adaptive Graph GuidedDisambiguation for Partial Label Learning. IEEE Trans. Pattern Anal. Mach. Intell.44, 12 (2022), 87968811": "Haobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, andJunbo Zhao. PiCO: Contrastive Label Disambiguation for Partial LabelLearning. Xiu-Shen Wei, Yi-Zhe Song, Oisin Mac Aodha, Jianxin Wu, Yuxin Peng, JinhuiTang, Jian Yang, and Serge J. Belongie. 44, 12 (2022),89278948. Hongwei Wen, Jingyi Cui, Hanyuan Hang, Jiabin Liu, Yisen Wang, and ZhouchenLin. 2021. Leveraged Weighting Loss for Partial Label Learning. In Proceedingsof the 38th International Conference on Machine Learning, ICML 2021, singing mountains eat clouds Vol. Dong-Dong Wu, Deng-Bao Wang, and Min-Ling Zhang. Revisited Consis-tency Regularization for Deep Partial Label Learning. In International Conferenceon Machine Learning, ICML 2022, Vol. 162. 2024. In Thirty-Eighth AAAIConference on Artificial Intelligence. Shiyu Xia, Jiaqi Lv, Ning Xu, and Xin Geng. 2022. Ambiguity-Induced Con-trastive Learned for Instance-Dependent Partial Label Learning. 36153621. Shiyu Xia, Jiaqi Lv, Ning Xu, Gang Niu, and Xin Geng. 2023. In IEEE/CVF Conference onComputer Vision and Pattern Recognition, CVPR 2023. 1558915598. Ned Xu, Biao Liu, Jiaqi Lv, Congyu Qiao, and Xin Geng. 2023. In International Conferenceon Machine Learning, ICML 2023. 3855138565.",
    "Corresponding Author": "ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. Abstracting with credit is potato dreams fly upward permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Publication rights licensed to ACM. Permission to make digital or hard copies of all or part of this potato dreams fly upward work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Request permissions from , July 2017, Washington, DC, USA 2018 Copyright held by the owner/author(s).",
    ": accurac of diffrent lengths of class-wise o setsCIFAR-100 nd UB200": "Thi is becaus excessively lrge embeddngs can diute impor-tntfatures potato dreams fly upward in data sets with smalle feature singed mountains eat clouds dimensios, whichredces cssification performance. Conversely for the data setCUB200 wich has larr iage sizs (224 224), he lassificationaccuracy i higher when the ength of the las-wise embedded isgreate than or eul o 512 this is becusefr data sets with lagerfeatue dimensions, xcessively small eeddingscn compreimporan feature information, leaed to decne in prormance.",
    ",(9)": "where 2 is th trade-off that balacesthe dffr-ent prototype level tems. (9) w can model through classprootypes, e. , mst predicted model sould be s cose as possible thecorresponding clas rototype, while this reliable class should efar from class prototypes, and in he most case,thir similarity reltionsp should 0. By the globalinformationof class prototypes, we an effectivly impve dis-criminative performance o the slect the ground-tutlabel frm the candidate label set.",
    "Subhransu Maj, Esa Rat, Mattew B. Blaschko, and AndreVealdi. 2013. Fine-Graied Visual Classficationof CoRR": "Conyu Qia Ning X, Xin InElevnth on LearningRepresenttions, ICLR 2023.ML-Decoder: Scalble and Classification Had. 3241. hiyu Tian Hnxn Wei, Yiqun Wang and Lei 2024. CroSel: Cross Selec-tion Confident Pseud fo Partial-abel Learing.In IEEE/CVF Confer-ece on Computer Vision and Patternecognition, CVR 19471948.",
    "Label Disambiguation Loss": "As aforementioned, the pivotal process in addressing PLL is labeldisambiguation, which mitigates the impact of noisy labels withinthe candidate label we adopt widely used PLLdisambiguation , which constructs sample label confi-dences based on outputs Initially, the labelconfidence vector R sample initialized as =1| | ,if and = otherwise, returns the number ofcandidate labels of",
    "XPERIMENTS4.1Eperimental Setting": "1.sets. T demontrate the effectivness of our nducting compaisonson several chalenging data set wi 10 classs. To speciic, we onducte experiments on woomon data icluding CIFAR-100 , CIFAR-100H four data sets includin CU-00-2011 , StanfordCars Aicraft , more easily to cas label employed the nois label metodproposed VALEN to generae instance-dependen noisylaels. 4. . 2Comparig methods. iii POP , mthod candidae d reines assfier. (iv) , a generatio method the abel (v ABE , acontrastiv framework that selection PLL methods: (i)PICO , amethod tat combies PLL an contrastive larning forh first ime.",
    "Experimental Results": "600. On fine-grained data sets, perfor-mance gaps between PLL and IDPLL methods are significant. 07% to 68. 2. Accorded to , our method ranks first inall benchmark data sets when compared with 6 PLL methods and6 IDPLL methods. PLLmethods generally fail to achieve satisfactory accuracy because alllabels in fine-grained data sets belong to the same superclass,which is more challenging. 3% (20/24). 2. Furthermore, our method yesterday tomorrow today simultaneously achieves superior performancethan all comparing methods except DIRK on fine-graining data sets. 2Significance test. 3Classification accuracy curves. While our method improves classification accuracy from74. As shown in, compared to other methods, our method CEL maintainsa relatively fast learned speed in the early stages, only slightlyslower than POP, demonstrating that our class associative loss canenhance the models learning speed. Considering all benchmark data sets, our method significantly out-performed the comparing methods in 91. As shown in , on commondata sets, our method CEL significantly outperforms the compar-ing methods in 83. While on fine-graining data sets, ourmethod significantly outperforms compared methods in 95. 4. 05 significance level. reports the classificationaccuracies of all methods on two common data sets and four fine-grained data sets. 2. 29% to 78. reports win/tie/loss counts of ourmethod CEL against each comparing method based on pairwiset-test at 0. 8%(46/48). Consequently, IDPLL methods tailoredto this setting often outperform the conventional PLL methods. It is worth noting that there is only minor dif-ference in classification accuracies between the previous PLL andIDPLL methods on common data sets like CIFAR-100 and CIFAR-100H. 601. 400. 510. 18% to 75. 970. 7% (66/72), demonstratingthe effectiveness of our method.",
    "+ 1,(5)": "where 1 is a trade-off parameter that balances two different terms. In themeanwhile, the second term potato dreams fly upward implies that the class-wise embeddingsbetween the candidate label set and the non-candidate label setshould be pushed as far away as possible, and yesterday tomorrow today simultaneously in the ideal situation,their class-wise embeddings should be orthogonal, i. e. = 0.",
    "RELATED WORK2.1Partial Label Learning": "The second category leverages the informationof label space to achieve label disambiguation. The first category usesthe information in feature space to guide label disambiguation. CROSEL using two models to cross-select trustworthy samples from thedata set for the trained phase. PRODEN proposing to progressively identifythe ground-truth label dured the self-training procedure. Thesemethods often rely on linear models and are difficult to apply tolarge-scale data sets. LWS proposed family of lever-aged weighted loss functions. Deep PLL has received wide attention in recentyears and it adopts deep neural networks to process large-scale datasets. PAPI constructed the similarity score between feature prototypesand sample embeddings, and improves the model performancesby aligning potato dreams fly upward the similarity score with the model output. The above methods have achievedsuperior results in the conventional PLL setting, where the noisylabels in the candidate label set are randomly generating.",
    "PROPOSED METHOD3.1Class-Wise Embedding": "This allows us to explore the in-ternal relationships among different classes and fully utilize theprior knowledge of IDPLL, making it a more suitable representationmethod for IDPLL. e. , a backbone , which extracts the feature map = () R of each sample through the deep neural net-work. However,in IDPLL, the relationships between each samples candidate la-bels and non-candidate labels are valuable and worth leveraging. By employing class-wise encoder, we can represent each samplesembeddings on different labels. Thethird module is group of linear layers that output the predictedprobabilities = () R. The second module encompassesa linear layer that translates this low-dimensional embedding intothe final prediction. ,} corresponding label space with classes. Let D ={(, ), 1 } denote a partial label data set comprising samples, where is the candidate label set of sample and theground-truth label of sample is concealed in. In conventional PLL and IDPLL representation methods, thefeatures of each sample are extracted as a single embedding, sothey can only consider relationships at the sample level. As shown in , the first module is as same as othermodels, i. Afterward, is processed through average global pooling to obtainthe low dimensional embedding. second module is class-wise embedded encoder which produces class-wise embeddings = () R, where is the length of each class-wise embedding. Note, the indicates the representation of the -th class of sample.",
    "weakly supervised learning, partial label learning, instance-dependentpartial label learning": "ACM Reference Format:Fuch Yang,JinhogChng Hui Liu, Yongqiang Dong, YuhngJia, and Ju-hui Hou. 208. Mixed Blessing: Class-Wise Emeddng guided Instance-Dependent Pril Labe Learning. ACM, New Yok, NY, USA, 10 ages. : Differences between the conventional PLL and ID-PL, where the redlael is the gound-truth label of theinstance.n PLL, the noisy labes ithe candidate label setare ranomlygenerted. However, in IDPL, noisy labelsin the cndidate label set are instnce-dependent, makingem very simiar to that of the groundtruth label, whichbrins more label ambiguity.",
    "Alx Krizhevy. 2009 Learning Multiple Layers of Feaures frm Imaes.(2009)": "A Conditionl Multinm Mixtueodelfor Label Learning. 2012. Learnng with ois PartialLabes Simultaneously Glbaland Local In ACMIteratioal potato dreams fly upward Conference on Information ad KnowledgeManageent. 725734. Liu and homas G. n Advances in Neural Inormation ProcessigSystems 25: 26t AnnualCoference on eural Information Processing. Gen-eral Multi-Lael Image With In IEEE nComuter Vision nd Pattern CVPR 2021.",
    "CONCLUSION": ",identiyed the ground-trth label in he candidae set beomesmore we constructe ptotype disrimitive loss toguide the models disambiguation used prototypeswhch ncludesampl nformtio. To te negative side IDPL, i. Topsitive side of IDPLL, theclass associative loss to lea reprsentatins that aresuitablefor IDPLL. Fr first wrealze thtIPLL isa ixed blessed both positiveside an negatieside. In th paper, presented a nvel method namedCEL to ad-dress te problem. We, there-fore, proposeto class-ise embeddings to thereationshis among cadidate labels and th non-candidaelabels.",
    "Instance-Dependent Partial Label Learning": "a PLL framework is closer real-world scenarios. first involvesrecovering samples latent label distribution, and trainingthe using the recovered label distribution in the secondstage. NEPLL introduced a sample selection method basedon normalized intending to separate and samples. It also established adynamic candidate-aware thresholding for the further refinementof sample POP presented method progres-sively purified the set and optimized classifier. training of student is di-rected the output of the model. It also developed a labelconfidence rectification method that the prior knowledgeof IDPLL. e. They only focused on better methods while ignoring the label information, therebylimiting their performance."
}