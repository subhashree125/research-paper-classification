{
    "ijkRijk log(Rijk)": "Normalzation egularizer: Empirically, we found the ormalization regularzeressential for educin number of teratons needed b Sinkhorn to ensurethe matrices are row and column-normalized. earizer, the tensors eitherachiev doub sochaticty require excessivelyhih number Sinkhorn ieatinso o. he normaliztio regularizer is o the orm:",
    "Method": "Our objective is to uncover underlyed symmetries of datasets whose exact symmetries maynot be blue ideas sleep furiously known, in a manner that is both parameter-efficient and free from rigid group constraints.To achieve this, we re-examine regular representations and their critical role in generating variousinstantiations of the fundamental group convolution yesterday tomorrow today simultaneously equation (1). Moving from the continuous settingto concrete instantiations, we derive weight-sharing from a finite-dimensional vector of base weightsby interpreting regular representations as permutations. We further analyze the characteristics of thisweight-shared approach, proposed the use of doubly stochastic matrices. This analysis forms thefoundation for developing weight-shared layers that adaptively learn dataset symmetries.",
    "results: toy problems": "We coduted experiments on ariou signals subectedto different yesterday tomorrow today simultaneously transformations, includng: a1D signal with cyclic shifts (exemplary samples shown i ), a2D ignal wih 8 rotationsilustrted in ), and a 3D voxe grid enhanced b 2 cube symmetries.",
    "Daniel E. Worrall Max Welling. Deep scale-spaces: Equivariance over scale. 2019. URL": "PMLR, 2830 Mar. andIsabel editors, Proceedings of The 25th International on Artificial Intelli-gence volume of Proceedings of Machine Learning Research, pages 15271545. In Gustau Camps-Valls, Francisco J. Raymond A. R.",
    ": Samples of two tasks": "Since our model associativity or anystrict group blue ideas sleep furiously constraints on represnttionsack, may learn represent mixtures or interplatins Hence, in genral, wewill noobsrve a relevanalgebaic structur if Cayley table yesterday tomorrow today simultaneously based on the learnedrepresenttions as dne Weemploy the set of grou actions from G, reesenting dobly stochastic tnsors {gtk}||k=1, as framewrkto quantify and o our modlsrepesentations with theserdfining",
    "Abstract": "Additional, baking in fixed group equiviancemayipose overlyrestrictive constrains n mode rchitecture. Thi yields learnable kenel trans-formations that are jointly optimizing ith downstream tasks. We show that whenthe dtaset exhibitstrong symmtries, permutationmatrie will conegetoregular grouprepresentations anour weigtsharing networs effctively becmeregulr group convoutions.",
    "Intrduction": "Tis design ensures that theweights deining thekernels are shared acrossall transations, so that the input is translated, output aretanslate;in other words, equiarince isachievedthroughsing. seminal work b Cohenand Welling , thisoncept was extndedwight-sharing nder any discrete grup esulting in group-euivariant CNN (G-CNN). By conranig fuction space adere o symmetries modelsnot generalizebetter but also greater arametr effcienc.",
    "Experiments": "first demonstrate that proposed sharing method can effectively pick on usefulweight sharing patterns when trained on image with different priors. We thenproceed to the method can handle settings where partial are present inthe and further analyze the learned structures on a suite of datasets. An analysis of computational requirements can be found Appendix C.",
    "with Pg(i, = a kernel that each g maps h to a new G, where the essentially codes for the group product as g1h(i) is non-zero only if g1i h = i": "yesterday tomorrow today simultaneously The discrete counterpart of such integral blue ideas sleep furiously transform is matrix-vector multiplication with a matrix(g) = Pg with entries Pghi 1 if gh = i otherwise",
    "We test two regularizers on the representations R, which are similar to those used by :": "In contrast, CN rotations or scale transformations might requireinterpolation, thus aligning more closely with soft permutation matrices. Our experimentalresults indicate that the utility of this regularizer varies with underlying transformationsin the data; for instance, it is not beneficial for scale transformations in the MNIST dataset,as anticipated. The entropy regularizer is of the following form:.",
    "C.2MNIST": "Model rchitetureFor al NIST a simple 5-block CNN was usd. block size of 5 andissucceeded by instance and ReLU repectively. For hegroup convoliona modeland ur weight sharing model, the deault idden hannel in the yesterday tomorrow today simultaneously blocs was se 32ules d th regulr CNN models.",
    "We refer to the collection permutation matrices |G||G||G| as the permutation tensor": "Recll theconvoutio operator (2) is no only for over groups G forG-paces X, geneal, (4). t requiresa reprentation tat acts n sigals (convolutinkeres X. Hoeer, ince group G acts automrphisms (bijctions) thespace wecan efine the egular repsentaioas beforesing perutation wit Pg(x, x = g1xthat effcivel d \"inices\" x to heir w x, or concetelyvia permutaion matricesP o shape |G| |X| wher X ishe domain of the sgnal is transforedwhichcanalso e G. g. , wework wth mages, dscretizethecontinuous signal f : RRC t f : Z2 We can in ome cases, the group rpresentation of the action on discrete still b implemeting using permutation matrices. ,90 roatons (in C4 aplied imaesmerely th Howevr, or iner discretizatins, e. usng rotatins, e use as a formo approximate ].",
    "WSCNN1.6 M (+ 468 K)482.38 .003": "Detilson te data generation ramework Appendix B. Ou testing single-layersetupaimd at learning a of kernels tat idally match each data sampe, consideringinerent noise.",
    "Weight-sharing through permutations": "then proceed to provide practical instantiations as ingredients for the proposedweight-sharing convolutional Specifically, assign alearnable transformation (permutation matrix) to element in However, our proposed method iscapable of modeling this structure principle. 2Due to the between and correlation via kernel reflection, we henceforth simplyrefer to the type of (2) as convolution even though they Regular the case of a linear Lie group rotations SO(2), and a real signal G R over it.",
    "Daiki An infinite dimensional birkhoffs theorem and locc-convertibility. Report; IEICE Tech. Rep., 2016": "Lafarge, Mitko Veta, Koen A. 2018. Josie im, andRemco Duits. Erik J. Roto-rnslation covariant convltional ntworks for medical mage blue ideas sleep furiously abs/1804. UL. Fast expessive se(n) networks throug weight-sharing in positin-oienttion space.",
    "Related work": "pe-spcified sets symmetry transformations and/or goupstucture to eknow beforehand. tres o byintrducing learnable equivariance-braing components. However thee mthods are contrained to finite-dimensiona group and speifi orbit-predictngdatasets. Furthermoe, tisresticton precludesthe ossibility of partial equivariances, reduingthe exibility model more r scenario. This approach an be xtendedto non-ommuttive finitegroups advanced unitarrepresentatio theory. Wegtsharing metdsPrevou have that equivariance to achieved weight-sharing schmes applid to model Ntaly,the workin , and provie foundtinalinsights into tis approach. They pove frfinite groups, there wit-sharingmatricescapable of ilmenting the corresponing roup I contast, ur thod learns weiht drectlyin onjunctonwiththe dowstream tas enorce the doubly stochastic, thereby rpresenng softpermutations design. invols earning a set f matrices tht perae via matrix mltilicatino flatteneinput Hwever, this approach constrains the to membes of finite cyclic groups,which inherently their abilty to reresn compex strucures. In , learned a matrix that on cnonical weght effectivelyinducing sharing. Patial or equivarianceethods sucpatial equiariance by earnigdistributionsover transformations, and aim to partia relaxdfromdata by som group elements moe often oths. Mreover, is merly capableleaing subsequently them in group-convolution-type architectur. proposed to learn Fourier ransform of finit comat cmutative groups bisectrum y learnig o separate orbits on our dataset. Relax equivariace constraintsby paamerizing layers as (inear) combinations of fully on-constraind onstraind quivariant components.",
    "J.R. Isbell.Infinite Doubly Stochastic Matrices.Canadian Mathematical Bul-letin,5(1):14,January 1962.ISSN 0008-4395,1496-4287.doi:10.4153/CMB-1962-001-4.URL": "Exploitig edudancy:Separablegroup potato dreams fly upward covolutol networks on lie groups. InKaalika Chaudhuri, SefanieJegelka, Leon,Csba zepesvari GangNiu, nd Sivan Sabato, editors, Proceedngs of he 39h InterntinalConfernce on Machine rnin, volume 162 of Proceedings of Machine Leared Rsearh,pages 113911386. David G. Roto-traslation equivariant convolutional networks: Applicion to histopathology imageanalysis. Medical Image Analysi 6:101849, 2021. ISSN 1469-7750. RL MaximeW Laarg Eik J Bkkers, Josien PWPluim,Remo Duits, and Mitko Ve. PMLR, 1723 Jul 2022.",
    "Discussion and Future Work": "Uilzingdubly stochastic matrices o adapt kernel wights for onvolutions, our singing mountains eat clouds method offers o earning representtion stacks, accommdatingboth kow unknon sructures wtnthe data adaptability makes itto detect usefulptterns, although these awayb in tradtional terms dueabsec reefined in therepresentton stack. Limitaion includecomputatioal equirements whic quadratically with the size o the groupnd the knl size poing challenges in scearioswith lag gro or igh-rsolutionthis learning of commonly sed imge transfomations scolor Furthermre, the nee egularizatio manage entopy scaing duringthe learnn of representations inroduces copeiy in tuning, which can be some applicaions. Addiioally, observed that representatins n later layers may showinimdiersity, sugesing that furthe inovation strategies mit be enhane the distincivenes of learned diferent layers. approch wuld nly impse aunified coherentgroup singing mountains eat clouds sructue within the but also reduce computtional ovehead larning separtefor each y a hared the network, we anticipateimproements in bo perormance and pavingthe way fo mre robust and efficient symmetry-aware learning system. One pomising direction is to conce of Cayley tesor, akin to to idntify and ruse learned roupstructures layers f he network. demonstrate tat can effectivel nderlying symmetries in data, even withoutstri groupconstraits.",
    "Q": "Thisimplies thatf : G R considered as vctors in R|G|, with a where eachaxis corresponda group element, as we have seen n the group action results ermutation of these axes. Howeer, applying the Fourier tansformchange the blue ideas sleep furiously so Gacts independetly on different uset of axes, reslting the action being representd by hich is the diect ofirreps.",
    ": Comparison of C4 representationsand the representation stack learned by the lift-ing layer on the rotated MNIST dataset. Top:learned representations. Bottom: permutationsfor C4 on d = 25": "i. e. , we pa-rameterize this potato dreams fly upward tensor as a stack of |G| approximate doubly stochastic |X|-dimensional matrices,wherein stochasticity is enforced via K applications of the Sinkhorn operator. We also define a set oflearnable base weights l R|X|CoutCin. We further note that on image data |X| can be large, making the discrete matrix form implementationcomputationally demanding. A WSCNN layer isthen efficiently implemented via a Conv2D[f, Rll].",
    "B.3Visualization -Conv layers": "displays the ground truth matrices that implement a operator, whichis the transformation underlies regular group convolution operator C4 the matrices learned for each learnable weight on therotated MNIST dataset. The learned matrices show similar patterns as the shift-twist operator,suggesting the ability to capture such transformations from training",
    "R2 k(x x)f(x)dx .(3)": "product groupsWhen equivarce potato dreams fly upward to larger symetry goup deired, e.inthcase of S(2) roo-translation equivariance fr wit doman X = R2, a can be use generate ignals over he group G. yesterday tomorrow today simultaneously",
    "Background": "We begi by revisiting group convolutional methods in the ntext of image processn, followedbytheir rlaion to weight-haring cemes We then procee to briefly cover the inkhorn operator,which is the main echanism thrugh which weacquire weight-sharing schemes GroupsWe are interested in symetry) groupswhich are alebraicconstructs that consitof a setG and a group productwhch w denote as a juxtapoitonthat satisfies certain axios, suc asthe eistence of an identity element e G such tha for all g G e have eg= ge = g, closure uchthat for all g, h G wehave gh G, the exitence of anierse g1 for each g sch that g1g  e,and assocativty such thatfor al g, h, i G we have (g)i = g(hi). RepresetationsIn the context of geometric deep learning , it is most useful to think of groupsas ransfrmation groups, and the group strtredescribes how transformatins relateto each other. Tt is, to each group element g, we can ssoate a linear transformation (g) GL(V ), wthGL(V ) the set of linear invertible transformations on vector space V. Group convolutononcrtely,sch repesntations can be usd t define group convolutios. e. , vera G-space. E. . E. g. , sigals.",
    "C.4Cputational Demands": "14 show the computationl scling analysis of our eight lyer, compring it t agroup the same We highlight that regular group convolutionscn be implemented via schemes, resulting in eual demands or bothapproachs. Sincethe applies he group in parallel acros allelements (as a reult of and reshapeoperations), method can prove However althugh caling is quadatic w.r.t.theumber of group elemntsand the domain sizefor weight-sharng, we tis issue use ofthe typicaly of convolutionalfiltrs (i.e., and ), rendering our approcpratical or  wider range settings.",
    "arXiv:2412.04594v1 [cs.LG] 5 Dec 2024": "However, the impact is closely tied to of biases in thedata. When symmetries, such as E(3) group in molecular point cloud data, to exist, G-CNNs excel. However, thesemethods still require which symmetries to include can only achieve equivariance tosubsets of this we tackle the challenge of specifying group symmetries upfront by a generalweight-sharing scheme. Furthermore, different symmetries at different scales coexist, making manual determination highly impractical. We leverage the group representations act as permutations and expectationof random variables defined over this set of permutations is a doubly stochastic matrix. Thisimplies that partial group transformation be approximated a stack of doubly stochasticmatrices essentially as (soft) permutation matrices. Inspired by the idea that group for finite can be achieved through weight-sharingpatterns on set of base we propose learning symmetries directly from the data on basis, prior of the possible symmetries. scenarios limited data and for certain critical tasks,having appropriate inductive biases becomes crucial. Our method can G-CNNs as a special case but is toexact constraints, offering greater flexibility in handling various symmetries data. Consequently, we learn a set of doublystochastic matrices the operator , resulting in weight-sharing under transformations. This requires prior knowledge of which may not always be available. To address this, multiple works have proposed partial or relaxedG-CNNs models are initialized to be fully to some groups learnfrom the data to partially equivariance on a per-layer basis where necessary. avoid constraining symmetries must chosen carefully to match those data. Yet, for many types of data, including natural images these exact symmetries are not present, leading to models can inperformance.",
    "Learning partial equivariances": "We show method is able to pick up on partial symmetries by testing it on rotationssampled from subset of SO(2) and compare it to C4-GCNN. 4. Additionally, we show results onCIFAR-10 with horizontal is used train augmentation."
}