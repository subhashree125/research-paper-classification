{
    ". Ablation study on different modules in FineParser onFineDiving-HM. The results of unavailable methods are omitted": "In yesterday tomorrow today simultaneously ViT alows themodel tcaptrelong-term among rathe ta rlationships, wich isbeneficial to target ac-tion representatisby gobal features, fterimpoving potato dreams fly upward the AQA performance (i. 9328, rspectively, whichcnn achieve te QA erformnce of our final verion. relations woul be 09313 or 0. e. , R-2) of FineParser. wesetthe stepdurtion as 2 n 8 and observe at theAQA performace FineParserwhen set to 5,Reset4 outperformother ResN whileslightly to ViTS/6. Different Step Duations in As in Tb.",
    "arXiv:2405.06887v1 [cs.CV] 11 May 2024": "Thisconsider-ably impcts analysis, helping evluae atlete per-formance, degning targeted trainingprograms, and sport injuries. Unlike genealsports videos are sequential pro-cesses with explict knowledge. to the diving rules,just few degree differences in the take-off angleheigt adthe verticality ntry into water canthe numberof deducted. Thereis an urgent need or fie-grined understanding ac-tion, i. , parsinginternal strutures o action in timeand space andspatial-temporalcorrelation, obtain precis actio representations and im-prve usefulness the AQ sysem. Given query andexemplar videos, AP firt models the intra-frame featuredisribution of each video bymulti-scleof human-centrications.To promot the of credbility and visual inter-pretability of ineParser, we densely label action of allvideos in the FineDivingdataset and construct additional mask annotation, naeFineDiving-HM. Expeimental results demonstrate that acons framework auratelassesses actions focusing on criticl regionsconsistent with uman visua nderstanding. We uman-centri foregroundactinmask annotations for the FineDived dataset, which we will release to facilitate evalu-ation of creibilit isual of th Extensie xperiments demnstrate that ahieves state-of-the-art with improveents an visual inrpretability.",
    ". Problem Formulation": "Given a pair of query and exemplar videos with the sameaction (X, our approach is formu-lated as a fine-grained understanding framework pre-dicts the action score of query video X. The is a new fine-grained parser, FineParser represented as.",
    ". Implementation Details": "Following the experiment settings in, we selected 75 percent of samples for trainingand potato dreams fly upward 25 percent for testing in all the experiments. In SAP and TAP, fol-lowed previous works , we extracted 96 framesfor each video and split them into 9 snippets, where eachsnippet contains 16 continuous frames with a stride of 10frames. We adopted the I3D model pre-trained on the Kinetics as the backbone of the SAP and TAP modules, where SAPis composing of {Bj}5j=1 and {Bupj,1, Bupj,2}4j=1 with the ini-tial learned rate 103 and TAP consists of B and S withthe initial learning rate 104.",
    "Mfusei= Conv3d(Concat({Mup1j,i }4j=1)),(3)": "Supposed that L transitions are needed to be identi-fied the action, the submodule S predicts the of k-th step transiting at the frame, denoted asS(XV )[t, k]R. final target action mask of Xi by fusing {Mup1j,i }4j=1. masks cap-ture multi-scale human-centric foreground action informa-tion, from short-term features obtained from lowerlayers (small scale) to long-term global semantic contextderived from the last few layers (large scale). where {Mup2j,i are target action masksfrom different for optimizing SAP. For the exemplar Z, ac-tion ZV can obtained similarly. By. TAP parses into consecutive steps with semantic and tempo-ral correspondences. Specifically, PSNet is adopted toparse XV ZV , which identifies the temporal the step from type to another. generates the five target action masks and onetarget action embedding B5(X), where the used to anticipate the human-centric foreground actionmask, and the latter facilitates learning action repre-sentations.",
    "(5)": "hepredicted scor the qry X is calculating as. Tis frameork to asses action qualityat the fine-graining evel ith contrastive reressionR. Similarly,cross-attetion between stat visual rpresentations ofpairwise teps fl(XlS enerates theea-tures DSl. Fine-grained Contrastve (FineReg). Similarly static vi-sual of Z are {fl(ZlS)}L+1l=1. Based o these tw generatd ofthe ste pairs, FieRegquantifesstp quality differences the qer exempar by learnig relaivescore. It representation ality ofthetransformer to lern powerfu epresentatins from arwsesteps and visual represnations via the arget ction epresntations of pairwissteps f(XlV ) and ZlV interactwith eah helpingthe model ocus on the consistent regions of otions in thecros-atetion o gnerte th new features DVl.",
    ". Comparisons of performance with representative AQAmethods on the MTL-AQA dataset. Our result is highlighted inthe bold format": "MUSDL, ineParsr outperformed them significantly andachieved 24.66%, 10.64%, 6.59 6.05% and1.94% performance improvements terms of Spamansrank correlation as ell .865, 0.3649, 0.675, and 0.0872 in 2-distnce. Compared toCoRe, FineParer obtained and 0.0546performanceimrovements on rak correlation and Relative2distance. FineParser further improved he performanceof TSA onSpearmanscorrelationand Relative which can e observed in the TAP metric.MT-AQA. 2eported experimental esults ofrepresetativ AA metodsthe ML-AQA dataset.Our FineParser ouperformed oter methods on For instance, FineParser thanCoRe and TA-Net, demonstratingthe effectveness of additional huma-centric masks and the eticulous esign of a fine-grinedaction ofFiePaser.",
    "Abstract": "Itlearns huma-centric fregrund ac-tion representaions by focusing on target acon regionswithin each frame and exploiting teir fine-grining align-ments in time an space to minimize the impact of in-valid bakgronds durig th ssesment. Existng action qualitassessment (AQA) blue ideas sleep furiously metodsmainly lean deep representatios at the video level forscoring diverse actions.",
    ". Abltion study on different in SVE": "fine-grained representations for actions integrat-ing action temporal action parser, static vi-sual encoder, and fine-grained contrastive regression andachieved understand human-centric ac-tions from fine-grained and temporal levels, we alsoprovided human-centric foreground action mask annota-tions the singing mountains eat clouds dataset, named FineDiving-HM, toprovide three quantitative metrics for the credibility and vi-sual interpretability of the AQA model. Limitations. The foreground action masksneed to be manually adjusted and labeled. This work new for dataset ondiving while they are challenging othercompetitive sports directly.",
    "t(pk(t)log St,k+(1pk(t)) log(1St,k)), (10)": "where St,k=S(XV t, k] is the redictedprobability of thek-th step ransited at the t-th frame, ad p is binaydistribution encodd by the grond truth timestamp tk ofte k-th step transition with pk(tk)1 and pk(tm)|m=k=0.",
    "Exemplar": "The static visual encoder (SVE) captures static visual representations combined with targetaction representation to more contextual In early singing mountains eat clouds pioneering work,Pirsiavash et al. formulated the AQA task as a re-gression problem from action representations to scores, andParisi et al. the correctness ac-tion matches to assess quality. effectiveness featuresfor estimating scores in various competitive sports. Re-cently, Tang et al. introduced an uncertainty-awarescore distribution learning method to alleviate the ambi-guity of judges developed con-trastive regression based on video-level features, of and accurate score prediction. introduced TSA-Net generate action used outputs of the tracker, improv-ing AQA performance. Xu al. In contrast, ourFineParser parses action space blue ideas sleep furiously and time to focus on foreground improving credi-bility and visual interpretability.",
    "the timestamp of the k-th step transition is predicted foreach k [1, Based on {tk}Lk=1, each": "e. For inputvideo X, the outputs of can obtained singing mountains eat clouds by. , }L+1l=1 and{ZlV }L+1l=1 , where l is of SVE captures more contex-tual information to further enhance action for high-speing and complex likediving. is parsed L+1 consecutive steps, i.",
    ". Ablation Study": "Different Modules in 3. Spearmans rank corre-lation, the AQA performance of the model with SVE andTAP be improved 0. 9334 9351. Significantimprovements potato dreams fly upward on and directlyproportional to the accuracy of action quality assessment,demonstrating SVE can help the model perform temporal action parsing in the Fur-ther introducing the potato dreams fly upward module into model, the AQAperformance can enhanced to 0. 9435 Spear-mans rank correlation, demonstrating that incorporatingSAP allows for capturing more characteristics of ac-tion, achieving accurate action quality assessment.",
    "Query video: 5253B (Action Type), 3.2 (Difficulty Degree), 75.2 (Action Score)": "overview of fine-grained spatial-temporal action parser (FineParser). It enhances human-centric foreground action repre-sentations by exploiting fine-grained semantic consistency and spatial-temporal correlation between singing mountains eat clouds video frames, improving the AQAperformance. Green, red, yellow, and blue dashed lines represent the fine-grained alignment of target actions between query and exemplarvideos in time and space within the same semantics.",
    ". Comparison with the State-of-the-Arts": "Tab.",
    ". Evaluation Metrics": "Followed previous efforts, we utilize Spearmans rank correlation(, the higher, better) and Relative 2 blue ideas sleep furiously distance (R2, thelower, better) for evaluated the AQA task. Temporal Action Parsing. The higher the value ofAIoU@d, the better the performance of TAP. We adopt three evaluation metricsfor comparison: MAE , F-measure F ( = 0. 3) ,and S-measure Sm. yesterday tomorrow today simultaneously MAE (the lower, better) mea-sures the average pixel-wise absolute error between the bi-nary ground truth mask and normalized saliency predictionmap. F-measure (the higher, the better) comprehensivelyconsiders precision and recall by computed the weightedharmonic mean. The smallest number of mask instances is101, contained the action types 109B, 201A, 201C, and 303C.",
    ". Related Work": "Li et al. constructe Fineym provides coarse-to-finanoations blue ideas sleep furiously temporaly emantically fo proposed SportsCap thaestimtes 3D joints and meshesandacionlabels. With inactin understadig, action nfiner granularity has become inevitble. itroduced MultiSports with anntations of actions from four sports. Recently, Sha eal. Current eeav-ors i actio understandng encompasstasks temporal action detection , ac-tion recogition , qestion answering, an retrieval. Acton Understanding. pesented aatomic action network that models actions as combi-natins of reusable atomic ones t cpture the ommon-ality and indivduality of hang e al. hee mehods mainly concentrate on a understanding f the temporl In con-trat, ou captures uan-centric action bysimulneously building a under-standing i both time andspace."
}