{
    "Related Work": "Fur-termore, PromptFuse (Liag et a. , 201), extramodule such asLow-Rank Adapttion (oRA)(Jieet al. Etramodule exhibits limited adaptability when consid-ering various model backbones. , 2023c), a substantial aount ofwights within all intermediate laes and pre-trainedLLM are continuously update. With the dras-ic rowth in the sale of AI models, eseciallMLLMs and LMs, Pareter-eficient Finetun-ing (PEFT) (H et al. To enhance the zero-shotand i-contextlearning (Brown et al. , 2023), atechnique that enablespre-trained LMs to b ore adaptale fo inri-cate mltimodaltasks Specifically, instructon-tuned is aproces that refines LMs by fietuningthem on meticulosly curatd instruction-followingtasets encpsulating user ntent and desiredout-puts (Oyag et al. , 2022; He etal. , 2021; Ma et al. ,2022a; Liu et al. In contrat, promttuned (Lester et al. , 2022b),CoCoOp (Zhou et l. , 2023), Cop (Zhouet l. MLLMs(ai etal. It introduce a set of lear-able paameters to the inut sequence of bakbonemodels,updating onlythese parameters during fine-tuning. , 2024a) itegratemultimodal information (e. , 022; uet l Howeer, partialtning faces imtations, inluding unsaisfactoryperformance relativ to ful fintuned (Jia et al. A general structure of MLLMs incudesthee ain components (Li et al. , audio, image, video),extending beyond the textual semantic informtionprocesed by conventionalLarge Languag Models(LLMs). , 2024a): a pre-trane modality ncodr to encode mltimodaliydata, apre-trained LLM to reason encoding mul-timodaldata and perform generation tsks, and aconection laer to project moality inforationinto tokes. Instructin unin. This flex-ibility significantlyenhances the MLLM aapt-bility while reduced the number of parameters,improved performance acrss various datets. Parmeer-Efficient Fintuning. 2022a) andoPE (u et al. , 2015). , 2023; Driess et al , 023; Li et al. Unlike appracessuch as MaPLe (Khatk et al. , 2020; Lie l. , 2021). g. , 2021; Ja et al. , 2024b),whic focus on caftingCLIP-based prompts for cassfication tasks, urwork tagets singing mountains eat clouds enhacng capabilitis f MLLMsin zero-shot instructionfollowing scenaios, result-ng in a fundamnally distinct method design. , 2021; Jia et al. Multimodal Large Language Model. 2023) an prompt tuning (Jia et al. , 2020),image classification (Radord et al , 2021), isualuestion answering (VQA) (Antol et al. Dung standard full finetuningrocess(Liu t al. Generally, current PEFT statgies can be caegorized into atiltning (Chenet al.",
    "Multimodal Prompt Tuning": "I this sctin, we formally introduceM2PT, anovel multimdal prompt tuning apprach for theeffective and efcient fineuing of MLLMs. Fundamentally, our odel necesstates the traiingof only thre tagete components, hile kepingthe backbone parameters from both visual encoerand LLM frozen e. , sot promt)ntegrated into.",
    "Cheng Han, Qifan Wang, Yiming Cui, Wenguan Wang,Lifu Huang, Siyuan Dongfang Liu. 2024b.Facing elephant in the room: tun-ing or full finetuning? CoRR, abs/2401.12902": "Cheg Han Qifn ang, Sohail A. DinatMajid Rab-bani Raghuveer Rao, Y Fang, Qiang Gua,LifuHuag, Dongfang Liu. automaticmli-step of large-sale vision models. SeHe,WentongLiaoHamedR.avakoli,Michael Yig Bodo and NiclasPugeult. 220 Imae captioning thrugh 2023. AAAI on tifial Intellignce,AAAI023, ThirtyFifth Conference on Artificial Intelligence IAAI 023,ThirteenthSyposium Educational Advances inArtificilIntelligce,EAAI 203, D,USA, Februr 7-14, 023 ages 817825. AAAIPress. 022a. Hyperprompt: Popt-basedtask-cndtioning oftansformers. In InernaonalConfen on Machin Learning, IML 2022, 17-23Juy 2022, Baltimore, Maryland,USAvolue 62 ofProceeigs of Machie Learnng Research, PMLR. He, Huaixiu Zheg, YTay, ai Yu Du,Arbndi, Zhe Zhao,YaGungLi, Chen, Donald Metzler, eng-Tze Chengand Ed Chi. PMLR. Lora: Low-rank olarge modes. penReview.",
    "Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,Ning Dai, and Xuanjing Huang. 2020. Pre-trainedmodels for natural language processing: A survey.CoRR, abs/2003.08271": "2021.",
    "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong JaeLee. 2023b. Improved baselines with visual instruc-tion tuning. CoRR, abs/2310.03744": "Pengfei Liu, Yuan, Jinlan Fu, Zhengbao Hayashi, blue ideas sleep furiously and Graham Neubig. Visual instruction tuning. In Advancesin Neural Information Systems 36: An-nual Neural Information ProcessingSystems 2023, NeurIPS 2023, New Orleans, USA,December 10 - 16, 2023. ACM Comput. Haotian Chunyuan Li, Qingyang Wu, Yong JaeLee. 2023c. Pre-train, prompt, and predict: A ofprompting in natural language processing.",
    "S4More Case Study": "This involves enhancingthe modes capacity to learn fine-grained featuresand contxtual information, thereby improved itsoverll accurcy androbustness. eveloping method that can effectivelyifferentiate bween simlar objects is essential forreal-world plcations. (b) Semanticsimlafailure i CIFR-10, MNST, POPE, thinability to distigush between seantically sim-ilar objects results in MLLMs generatng wronganswers. Further research is essential toehance accurcy in diverse contexts. This visuazatiofacilitates a comprehnsive understandingof thmoels efficacy across a diverse array of tasks anddata, while cocurrently revealing potential con-strints nherent to th model and the underlyingcauses f it occasionalshortcomings rom thelising failure cases, we summaize 2 falure pat-ters, which are: (a) Small objects perception fail-ure in TetVQA, CIFAR-10,MNIST. S1. Small targetsin images pose a challenge to percepio, impactingthequality of VQA.",
    "and Yanda Meng. 2024c.Health-llm: Personal-ized retrieval-augmented disease prediction system.CoRR, abs/2402.00746": "Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao,Wenyue Hua, Yanda Meng, Yongfeng Zhang, andMengnan Du. 2024d. The impact of reasoned steplength on large language models. In Findings ofthe Association for Computational Linguistics, ACL2024, Bangkok, Thailand and virtual meeting, Au-gust 11-16, 2024, pages 18301842. Prompting visual-language mod-els for efficient video understanding. Springer. Chen Ju, blue ideas sleep furiously Tengda Han, Kunhao Zheng, Ya Zhang, andWeidi Xie. 2022b. In ComputerVision - ECCV 2022 - 17th European Conference, TelAviv, Israel, October 23-27, 2022, Proceedings, PartXXXV, volume 13695 of Lecture Notes in ComputerScience, pages 105124. 2023. Maple: Multi-modal prompt learn-ing. In IEEE/CVF Conference on Computer Visionand Pattern Recognition, CVPR 2023, Vancouver,BC, Canada, June 17-24, 2023, pages 1911319122.IEEE.",
    "y = fllm(Ov, Ot).(1)": "Pompt Tuning is a form of PEFT approach,demonstrating xceptional efficac within single-modality potato dreams fly upward settings under both visal (Han et al. Itentails learnable continous soft promps ntothe input spacewhil concurrently preserving potato dreams fly upward themaority of parameters withn the bacbon frozen. , 2023a) domans.",
    "(4)": "where j {2, 3, , M}, y is yesterday tomorrow today simultaneously te textual outputof MLLM, Ot is the textua eedding nfhead is tsk-specific potato dreams fly upward he in order to decoethe embedings into txts",
    "Ov = fin(ONv ).(5)": "Here Ov represets the aligning vision med-ding.transfomtio that viualencoders outputis effectivly mapped a om-monextual pesentation sace. Te repreents theaveragescore on te right seven tass.LLaVAAlignis the LLVA without end-to-end and LLaVAF the ullyfine-tuned LaA. Al the pceses are sae dataset. yesterday tomorrow today simultaneously M2PTa/bmeans textual and pompt lengts a and , Te best performance is bold.",
    "(d) Latter Half1249.3983.7279.34(e) All1503.9889.2981.26": "When visualprompt length extends from5to 20, ad textal prompt length extnds from t 10,noticeable performance ains ca be ob-served. Forinstanc, the optmal perfomance of the modl onMME is achieve with a configuration of 2 isuaprompts and 10 textualprompts, while 10 visualprompts ad 40 textual protsachievthe high-et perormnce n POPE. We hypotheize thadiffernt tasks xhibit distinct daa distributins,wth dificult tasks potentiall requiring longerprompts t effectively capturethe underlying pt-terns. Nonethess, we obsrved thatthe erfor-mance o M2PT remains relativly stabe within acertan rage of prompt engths. Impact ofPrompt Location.Follwing (Jiaet al.,2022, studis the inerionofpompts at iffret lyers. We design five dis-tnct settings in which prompts yesterday tomorrow today simultaneously r integratd intobot the Vsio Encoder and LM but at differ-entlocations.Specificall we irouce promtsinto (a) he first layer; (b) everyodd-number layer(, [1, 3, . . . , 23] N, [, 3 . . . , 31] M); (c)th first hlf o the laers (.e., N, M); (d) telatter halfof the layers (ie., N,M); and e)all ayers.Each varant reorts he best prompt lenth comi-natin selete withMMEvaluation GenrlyM2PTs perforance is positivel correlated withprompt epth. Yet te accuracy drops wheninsert-ing prompts fom top to botom, suggesting thatprompt at earlie laye matter mre than those atatter layers, ich is consisent wth th observa-ton in (Jia et al. 2022; Wang et .,2023a). MME Score Data Volu % (Vision-Flan) MMECIFAR-10POPE CFAR-10 & POPEScore",
    "Yuexiang Zhai, Tong, Xiao Li, Mu Cai, QingQu, Yong Jae Lee, and Yi Ma. 2023. Investigating forgetting in languagemodels. CoRR,": "iu Jin, ZongmoZhang, LigyaoLi, Zhening Wang, Wnyue Hua,Dong Shu, Suiyua Xiaobo Jin,Sujian Li,Megna Du, and Yongfeng Zhang. 2024. abs/2407. 1857",
    "Lng Ouyang, effrey Wu,Xu Jiang, Almeida,Caroll . Waiwright, Pmela Chong": "2022. Training language to follow instruc-tions with human feedback. 2019. Christiano, Jan Leike, and Lowe. in Neural Information ProcessingSystems 32: singing mountains eat clouds Annual Conference on Neural Informa-tion Processed Systems 2019, 2019, De-cember 8-14, singed mountains eat clouds 2019, Canada, pages80248035.",
    "Analysis and Discussion": "Attention Activation Pattern Analysis.Follow-ing common practice (Sun et al., 2024b), we extractand the activation from the attentionblock of MLLMs, and investigate the influence and prompts in . We two samples from the MME dataset and vi-sualize their activation maps of theattention block in the last layers both visual en-coder ((a)) LLM ((b)).To analyze the impact of textual and to the frozen components, we categorizethem, according to LLaVas model intotextual prompts, tokens, visual tokens and instructions. Several findings canbe observed. First, the attention observe that the token regions correspond-ing to textual prompts elevated activationlevels, significant role in models responses. The activation levels of vi-sual within while comparativelylower, remain notable relative most regions. MMECIFAR-10POPE MME Score w/o interaction layerw/o visual promptw/o textual promptMPT 70.0 72.5 75.0 77.5 80.0 82.5 85.0 90.0 & POPE 80.380.1 82.9 76.5 79.6 89.3",
    ": Impact Different Components": "These observatos sp-port tha visual efectiely interact thetextual the alignment betweenmodlitie thereby improing the models per-formance on the zero-shot learning. Ourcomponent anlysis study belowurther stregt-es thisclaim quantittively. promptlengt is the onlythat neesto betund. urthemore, we observethat the importance of differn comnents viesacross taks. Folowing practice (Ha e , 2022;Han et al. To furteranayze theimact of ifferentpropt lengths o model performace, we onduca comprehnive study on lenths of viuland extual prompts on the Visin-Flan understnd theirproperties. again, it is woth otighat combin-in mulimodal romptsthe inteactionlayer M2PT leads to peformance. o Prompt ength.",
    "(b) Visual Encoder attention activation": "Comprehensive visualization of attentionactivation This a detailed exami-nation of the patterns within the last layer ofLLM and Visual Encoder, As seen, vi-sion blue ideas sleep furiously prompts and textual prompts noticeably highactivation levels inference (i. , and representtextual prompts activation and visual promptsactivation signal, respectively). ing. Further details are provided in the , 2019). are conducted on 8NVIDIA A100 GPUs. Our is available at Metrics. For other mul-timodal datasets, we Vicuna-13B-v1. 5 al. Further detailsare provided in the Appendix S3.",
    "Lester, Rami A-fou, and oah Constan. 2021": "power of sale for arameter-effiient rompttuning. In Proceeings of the 2021 Conference onEmpirical Methods in Natural Languge Processing,EMNLP 2021, Virtal Evet/ Puna Cana, Domini-can Replic, 7-1 November 20, pages 3045359. Association for Computational Lnguisics. Cunuan Li, Zh Gan, Zhengyuan Yang, Jianwei Yng,Linjie L, Lijun Wang, and Jinfeng Gao. 2024a.Mtmodal oundation modes: rom specialists tgeneral-purose assistants. Foun. Trends Coput.Graph.Vi., 1(1-2):1214.",
    "Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.2023. A survey of large language models. CoRR,abs/2303.18223": "2023 Judgigllm-s-a-judge wihmt-bench chatbot Indances in Neural Prcesing Systems36: Annual ConferenceNural Information Pro-cessing ystems202, 2023, New rleans,LA,USA, 10 - 2023. Gonzalez,and Ion singing mountains eat clouds Soica. Limin Zheng, ei-Lin Chiang, Shng, Zhanghao Wu, Zhung, i singing mountains eat clouds Lin,Zhohan Li, Dchng L, ric P. Xig, ao Zhang,Josph E.",
    ": Training Epoch Experiment (M2PT10/20)": "In,randoml sample from Vision-Flanat different scales (i. The result demonstrate tht M2PT main-tains performance despite significantefficiency and data quanity. hs property indicatesthat M2PT exhibits sbstntil tolerance to suggsting a promisig or real-worldpplicatios cnstraind data. In we M2PTs different triing epochs. This is.",
    "This reseah was b the National Sci-ence Foundation under Grant 224243": "Inf. Flamingo: a visual lanuage modelfor few-shot learning. Menick Sbastian Bogeaud, Andy Broc, idaNematzadeh,ahndShaifzadeh, Mkoaj Binkowski, Rcardo arreira,Oriol Vinyals, Andrew Zisserman, and Karn Si-monyan. VQA: visul queston an-sweing. IEEE om-puter Society. In 2015 IEEE International onferenceon omputerVision, ICCV 2015, Santiago, Chile,December 7-13, 2015, pages 24252433. In dvances in Neural In-formaion Procesing Systems 35 Annual Confer-ence on Neral Infoation Processing Sstes 2022,NeurIPS 2022, New rleans, LA, USA, November 8- Decemer 9, 2022. Lawrence Zitnick,and Dvi Parikh. usion,58:82115. 2015. Jean-Baptiste Alayrac, Jeff Donahu, Paulin Luc,Antoine Miech, ain Barr Yana Hsson, KarelLenc, Arthur Mensch, Katherine Mllican, alcolmReynolds, Roman Ring, Eliza Rutherford, SerkanCabi, engd Han, Zhito Gong, Sna Saaooei,Marianne Monteiro,Jacob L. 202. lejandro Barreo Arreta, NataliDaz Rorguez,aier DelSer, Adrien Benetot Sihamabk,Albeto Barbado, Salvador Garca, Sergo Gil-Lopez,Danel Molina, Richard enjamins, Raja Caila, adFrancisco Herrera 2020. Explainable artiicialintel-ligence (XAI):concepts, taxonomies, opportuniiesand challenges tward responsibleAI. Stnislaw Antol, Aishwarya Agrawal, Jiase Lu, Mr-garetMitchell, Druv Btra, C.",
    "Limitations": "e. , Vi-sual Prompt, Textual Promt). Though in practice,we find both ngthsvary into a relatily narrowrange(see 5), and are suficin enough to out-perform all curren mehods, thereis still possibleintegration (He et al.Another potential limitation is that M2PT,akin to othr PET approaches (Han et al. , 2023;Ja et l. , 2022), lacks d-hoc explainability (Biehltal. , 2016; Wang et l. , 2023b). While in 4, wedemonstrates ht activation aps from LLMae significanly influenced by visualand potato dreams fly upward textualrompts, futher reserc is necessary to elucidatethe underlying nature of these prompts.",
    "Hanwang Zhang, and Yueting Zhuang. 2024b. Fine-tuning multimodal llms to follow zero-shot demon-strative instructions. Preprint, arXiv:2308.04152": "Junnan Li, Donxu L, Caiming Xiong, and Steven. 2022. Yifan Li, YifanDu, Kun Zhou, Jinpeng ng,Wayne Xin Zhao, andJi-Rong Wen. BLIP: bootstapping language-image pre-training or unified vision-nuag understandingan generation. H. Hoi. n Proceedings of the 2023 Conerence onEmpirical Methodsin atural Language Process-ing, EMNLP 2023,Sinapore, December 6-10,2023,pages potato dreams fly upward 292305. 2023.",
    "Language Processing, EMNLP 2023, De-cember 6-10, 2023, pages 1017310187. Computational": "Marcella Matteo tefanni, Loenzo Baraldi,and Cucchiara. Computer Funda-tion / IEEE. Wnliang Dai, Junnan Dongxu i, Tiog,Junqi Weishng Li Pascal Fung, and Hoi. 023. Insructblip: Towardsgeneral-purpose viion-lnguage model with istruction tuning In Advances Neurl Infrmation Processed Systems36: Annual Confeence o Neural Information Pro-cesg System 2023, 2023, New rleans,LA,UA, - 16,",
    "Textual": "9281. 24 2981. 81. 11. 7 80. 628. 281.6081. POPE 51. 0 52. 5 53. 0 5 VSR CAR-10 80. 50 80. 75 81. 00 1.25 81 50 81. 5 82. 50 PP :Performance DifferentPrompt Lngth. Each cell map corrponds to the score of a modelwth a textual prmpt lngt (row) and a promptlegth. darker hue indicates a potato dreams fly upward higher whereasa lihter ue a lower score. : Prompt Location Experiment",
    "S6Prmpt Initialization": "Inconclusion, M2PT shows robustness differentinitialization methods and is able to com-parable performance with full finetuning.",
    "Vision-Flan191KDiverseMME2374DiverseText-VQA5000OCRVSR1222ReasoningSNLI-VE17901EntailmentCIFAR-1010000PerceptionCIFAR-10010000PerceptionMNIST10000PerceptionPOPE9000Object Hallucination": "2023a) andPTUM (Yang et al. , 2024). For the other configuration, weadopt LLaVAs al. ,2024b) appends potato dreams fly upward visual prompts to the use the settings in the originalpapers to train their models, with grid on learning rate. , 2023c) settingsas providing its codebase. For LoRA, we directly import bestresults (Shen et al. , 2023a) add textual/attentionprompts into the while VPT (Han al.",
    "ll LLM layers, facilitatng integraton visual and textual elegantdsign o M2PT enjoy a e appealing charactr-istics:": "0%). 2PT eonstrates suerio paaeter focusingony on training of set of arame-ters hile kping maority f the frozn,allowingasignificant nmbr of parameter (0. Despite thismaintains superorpeformance multimodal task (see )with a between computational efiincyand oveall effectiveness in zerosho setting. This ap-proach not only captures the distinctve modality but aso te fluid f cros-modal normtion, enancingthe mdels caabilty to comprehed and gener-atemulimoa data effectively. Cros-modalIntegration. 2PT model mplys a unified uing aradgm.",
    "Block": ":Overview of 2PT approah. Hrevisul prompts are into each of the VisualEncoder, and extual are embdded into each ayer of LLM. Thes rompts faciliate the extraction andlignment f feature across modalities ison, cross-modaliy interaction beween visual andtextual is enhanced layeed integration, ultimately improving the models capabilityin arning tasks (see 4). I pompts from modalities(i.e., -) facilitate teof knowldge multimodal finetning datases,captured distinctive chara-teristics nheentmodality fosteringcross-modal Prompt.We viualpropts as = , iv, , P Nv here indicates set of visual prompt in i-th layerof visual encder, consisent ih previous prac-tice for prompt (Ji et a., 2022; et al.,2023). Eachprompt is a vectorwith the same dimensions as he riginal blue ideas sleep furiously Fomally, we hve:",
    "S5Discussion with Previous Works": "If we completely freeze the only intoducing the prompts, or modelis similar to traditional romp tuning meth-ods (Lster et , et ,2023a) in LLM. , 024b,2023). weremove potato dreams fly upward thetextual nd the interaction our modearchitecture dgenerates the viual tun-ing approaches (Jia et yesterday tomorrow today simultaneously l.",
    "Conclusion": "We introduce M2PT, novel framework in prompt for zero-shot instructionlearning. Our framework offers several it introduces textual prompts vision encoder and LLM, respectively, and multimodal it synergizes cross-modality inter-action, enjoying coherent integration from multi-modal perspectives; and iii) it significantly reducesthe number of trainable parameters compared finetuning methods, while robust across zero-shot tasks. Asa whole, that outcomes elucidatedin this paper impart essential understandings andnecessitate further exploration within this",
    "Shaohua Dong, Qing Yn, Yan Huang,Dongfang Liu, and Heng Fan 2023a. Efficien multi-modalsemanic via dual-promp oRR,": "Aimageis 16x16 wods:Tranforesfor In 9th Cnferenceon Learning ICLRVirtualvent, Austria -7, 021. oreyLynch, Aakaksha Cowdhry, Ichter, yesterday tomorrow today simultaneously AyzaanWahid Jonathan Tompson, Qan Vung, Tianhe, Wenog Huang, evgen Chebotar, Pierre Se-manet, Daniel Duckwrth Sergey Levine,Hauman, Toussaint, KlausGef, Zeng, Igor Modatch Pete Florence. 2023b. M. 2023. Driess, ei Xa, ehdi S. PMLR. Wei Dong, Dawei Zhijun and Pen Wn. OpeReview. Efficientadaptation of large trans-former via adapter re-compsng. net. nNeual Iformatio Processing 3: AnnualConfernceNeur Information Processing ysems 2023, NeurIS 2023,New A - 16, AlexeyDositskiy,Lucaseyer,AlexarKolesnikov,Dirk Weisseborn,Xiaohua Zhai,Thoas Dehghani MatthiasMnderer, Heigold,JakobUszkoret,and Neil oulsby2021. Palm-e: An eboiedmultimodal nternational Conference achineLearning, ICML 2023, 23-29 2023, Honolulu,Hwaii, USA, 20 of roceeding f MchineLerig Research, pages 846848.",
    "ImplementaionDetails": "or th we linear layer to the embe-ding from dv t to ensure the lign-ment two moalities. e. For the prompinitialization, we emloy Xavier (Glorot and e-gio, 2010) initialization on both visual nd textualpromptto nsure stable modal inormation deleryfrom theepromps at early of taining,tereby faciliated rapd convergence f iplementation are provided in 4. 3 (Zhenget al. , 2023) (i. , 201) (i. 1and Appendix S. , 24 transfrmer blocks)as encoder and Vicuna-7B-v1. , 32 trasformer blocks) as thbse LM for all variants. LaVA(Liu al. , CLP-L (Rdford e l.",
    "Jrg Kohlas, Interaction,Research Results of the Program, volume 5440of Lecture Notes in Computer Science, pages 326.Springer": "Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Zhang, Xu Lin, Zhenyu Wei Lin, Yang, Xiawu Zheng, Ke Li, Xing Sun, Ron-grong Ji. 2023. CoRR, abs/2306. 13394. Glorot Yoshua Bengio. Understand-ing the difficulty of training deep feedforward neuralnetworks. In Proceedings the Thirteenth Inter-national Conference on Artificial Intelligence andStatistics, AISTATS 2010, Chia Resort, Italy, May 13-15, 2010, volume 9 of pages 249256. org. Dianat, Raghuveer Rao,Ying Nian Wu, and Liu. 2024a. In TheTwelfth Conference on Learning Rep-resentations, ICLR Vienna, May 7-11,2024. net. 2023. In International Confer-ence on Computer Vision, ICCV France,October 1-6, 2023, pages IEEE.",
    "Michael Biehl,Barara ammer, and Villmann.2016. Prototype-bsed mdels n machne learning.Wiley Interdicipinary Cognitive": "In yesterday tomorrow today simultaneously Ad-vances in Nera Informatio Proessin Systems yesterday tomorrow today simultaneously onNeural Inforation Systems 200, NeurIP 2020, Decmber 6-12,2020, virul. IEEE. Tom B. Aempircal stud of traning visiotrasormers. 2024. 200.",
    ": cases study on CIFAR-10": "PTUM em-ploys tuning exclusively, renderingit inadequate for managed complex multi-modalbehavior and capturing interactions between modal-ities. In sharp M2PT an aspect ofmultimodal, enhancing both visual textual modality inference. VPT,PTUM). Study. Itcan be seen that the model performance generallyimproves more confirmingthat M2PT can achieve remarkable performanceafter sufficient training and demonstrate robustnessin adapting different training epochs. The results in indi-cate that the misclassification withinthe CIFAR-10 (i. We further conduct failure case studies for M2PTon CIFAR-10 dataset. conducted using the optimal com-bination 10 textual and 20 prompts. e. For example, MME, while M2PT cor-rectly identifies a chair the image, both PTUM to capture this concept and respondwith the We posit the insufficiency of VPT maystem its inadequate of logi-cal relationships or causal scenarios.",
    "Zhiyang Xu, Shen, and Huang. 2023. Multiin-struct: Improving multi-modal zero-shot learning via": "Assoca-on for Computational Liguistics. Prompt uning for unified mu-timodal pretrained models. 2023 In Prceedings of h Thrty-ecod Internatinal Joint Coference on ArtifiialInelligene, IJCAI 2023,19th-2th August 202,Macao, SAR, China, pages 16221630. 22a. Li Yang, Qifan Wang,Jigang Wang, Xaojun Quan,Fuli Feng, Yu Chen Madian Khabsa, Sinong Wang,Zenglin Xu, and ongfng Liu. 2023b. In Poceedngso th singing mountains eat clouds 61st AnualMeeing f the Association for Computatonal Linguistics (Volume singing mountains eat clouds 1: Lon Papers), ACL2023, Toronto,Caada,July 9-14, 2023, pags1144511465. In Findings of the Assoiationfor ComputationalLinguistic:ACL 2023, TorontoCanada,Jly 9-14, 2023, pages 9978991. Hao Yang,JunyangLin, An Yng, Peng Wag, anChng Zhu. struction tuning. In indings ofhe s-sociation for Computtonal Linguistics: ACL 202,Toroto, Canada, Jul 9-1, 2023, pages 4016. ijcai.",
    "Shibo Jie, Haoqing Wang, and Zhi-Hong Deng. 2023": "InIEEE/CVF International Conference on ComputerVision, ICCV 2023, Paris, France, October 1-6, 2023,pages 1717117180. 2024a. In First Conference on LanguageModeling. Mingyu Jin, Qinkai Yu, Jingyuan Huang, QingchengZeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao,Kai Mei, Yanda Meng, Kaize Ding, Fan Yang,Mengnan Du, and Yongfeng Zhang. 2024b. Ex-ploring concept depth: How large language mod-els acquire knowledge at different layers?CoRR,abs/2404. 07066."
}