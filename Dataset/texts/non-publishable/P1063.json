{
    "Related Work": "The apy he technique to only on attriuteper Moving beyon rule-basedsysems, severa neural mdels hav also beenpropose or thistask recent past. also rele on small et of instances tobein learned setting to collect good exmples foaual annotation. Zhng et al. This because thy a Seq2eqmodel pass for every to identify the value. Othergeeratve models aret larger attribute sets theygenerte ttribue well. extracton hasbeenasinificant topic ofin the realm e-commerce, a targeting thextaction of prouct fo variousodaliies, incluing purely textbased wellas those that ncorporat Some publicly datsets like MAVE offrhigh-uality data (achieved over F1-score), our work specificalyaddresses the mre challenging case of artially-labeing existig models Fw woks, su Zhang et alrquirent o havinga strongly-labeled subset limits them to operat n 13 attributes. Some mode are limitedin operaton to a autes heir cas, 38).",
    "Comparison with Other Models": "In , we present a of three systems NER-AVE,Seq2Seq-AVE and GenToC model its coun-terparts recall, achieving 71.8% automatic and 80.1% in manualevaluations, are the among the three models. are notably higher (by 11.7% and 27.3% in automatic andmanual evaluations) than those the next best-performing model,NER-AVE. Such a significant gap in recall demonstrates effectively the partially labeled nature of the trainingdata, resulting more extractions. Indeed, total number ofcorrect attribute-value extractions from 2,636 to 56.3%increase) on the test set. This increase in recall mainly toMarkers, which is elaborated upon in terms of preci-sion, both the NER-AVE and Seq2Seq-AVE models exhibit higherprecision than the GenToC model in automatic and approximately 9-10% 4-5% higher Overall, GenToC maintains its in F1-score,recording the highest at 71.3% in automatic and 83.0% inmanual evaluations, indicating a more and consistent per-formance. However, a drawback of GenToC its slower responsetime to the model. We address potentialsolution to this in .3.In , we cases showcasing GenToCs supe-rior performance in to and Seq2Seq-AVEmodels, in terms of the number of attribute-value pairs extracted.NER-AVE Seq2Seq-AVE models are to leverage thepartially-labeled nature of training data, as indicated previouslyin , despite encountering the remaining unlabeledattribute-value pairs in other In particular, each trainingexample tagged with only attribute-value on not unexpected that these maintain a similar taggingfrequency during inference. GenToC addresses this challenge byincorporating allow for the identification morenumber of attribute names than what is typically found in thetraining data.For instance, in the product Casual Juliet Sleeve Solid Top from the example in , the modelsurpasses the other models by identifying four attributes Sleeves Type, Pattern Gender.",
    "Deployment Status and Impact": "Our system has deployed n largest B2B e-comerceplatform, daMART, it 19M buyers and 7.9M sellers. Aan illstratio of dynamicfeture a user for 10 tier shoe rack,the sstem detectsthe aribute-value Number of shelve:10 This nfomation is then to higlight crrespondingfeatreAvailable number10 for a listingwith he product title Steel Shoe in the Thileads t user serch deployment we train GenToC using a is 3x largr, wit siilar partially-labeld ature.Further, taning dataset using the raining GnToC ndsubsequently train a ER-VE model this as detailed in 3.I can be ob-served that previously rule-basedsystem falls short ofour proposd approach 20% in terms of recall, whilebeing eter less 2.5%in precision. in an overall differenceof 10% in h the two metds In evaluation, w find that oursystem leadst a 9% increase in queriesientiied attributsthat la dynmic feture higighting.",
    "Abstract": "major with models for this task is lackof high-quality data, as attribute-valuepairs in the available datasets are often incomplete. Furthermore,we utilize GenToC to training dataset to expandattribute-value Finally, our model has been successfully inte-grated largest B2B e-commerce platform,achieved a significant increase of 20. g. To address this,we introduce GenToC, designed for training directly withpartially-labeled data, eliminated necessity for fully annotateddataset. the e-commerce domain, accurate extraction attribute-value pairs (e. GenToC employs marker-augmented generative modelto identify attributes, by token classificationmodel that associated values for each Gen-ToC outperforms existed state-of-the-art models, exhibiting upto56. , Brand: Apple) product titles and user searchqueries is for enhancing search and recommendation sys-tems.",
    "Methods": "In this section, the two-stage GenToC model and themarker-based training that enables it learn partially-labeleddata. the of GenToC in (c). The initialstep employs a Attribute Extraction yesterday tomorrow today simultaneously (Gen-AE) model,which takes product title or user search query as andsubsequently generates a list of predicted model is trained to attributes in order their the input. Given the models ability, it mightgenerate attributes outside the training set. However, arevery (occur less than 0.1% of the times), so we generation , any restrictions that wouldforce the attributes to the original set of attributes. Markers: To deal with training data, we initiallyidentify the within the input query correspond to valuesof attribute. We refer to identified words as marked words.For the example, Boat Rockerz 255 Pro Raging Red Bluetooth , [boat, pro, red] are treatedas marked during training, they attribute. A special learnable embedding, termed markerembedding, is added to the encoders final hidden states of everytoken marked words before being passed the Thesame embedding shared across marked tokens ev-erywhere. model is then able to that the attributes,as observed the training data, are result of only thelimited set of words in the input that have been other marker act as signals that the modelto focus on and incorporate the information from these markedwords into its attribute generation process, preserving thebroader context of the input inference time, the marker embedding is applied to all wordswithin the query, nudging the model to generalize and out-put attributes that are relevant to any word present in the query.This enhances the models capability to recognize and with words like bluetooth and neckband, which did not attribute-value pairs and hence were not markedat time.In the next stage, Token Classification Value Extraction (ToC-VE) model takes of the attribute names the first stagealong with the query, separated by a special delimiter,<sep>. then labels each token with a value, for yes no,to denote whether it corresponds to a value or not for the This model trained independently of the first-stagemodel. Since names are used in the training process, itallows model to learn closely related attributes whichshare similarities in their names. This becomes particularly crucialwhen dealing noisy attribute name ontology, which mayinclude attribute names conveying the same suchas Model Number and Model No. Value Pruning: When training the ToC-VE model with a ofattribute-value it always contains a value for every during the inference phase, the attributes may erro-neously by the Gen-AE model. As consequence, would tend to assign some value the incorrect at-tributes. counteract this, we supplement the ToC-VE data to identify instances no correct value",
    ": Distribution of product categories within trainingdataset. Specific percentage values are omitted to preservedata confidentiality": "We accomplish this by anexisting attribute-value pair and deleting the value from the orig-inal query. example, we token,boat, example, <sep> rockerz 255 raged red have value for the attribute brand. We term of this of synthetic as Value Pruning. the final set of attribute-value pairs yielded by model, weexclude those attributes that do not have any associated values. Therefore, the architecture of GenToC ensures attribute-value pair extraction, preventing error build-up in the in the face of and vast attribute setsthat contain redundancies. complete algorithm for trainingand yesterday tomorrow today simultaneously inference are described in and 2 (inAppendix), respectively.",
    "Seq2Seq-AVE": ", one at time. The model is based on model developed in Shi-zato et al. , which employs SeqSeq model that yields a o-catenated stri, incororating th espective attribute-vlue pirs,fr give input query. For instance, the encode input generates the string 2:2, , :For example, produces the generation Brand: ot,Modl ame: rockerz 255 pro.",
    "Precision-Recall Trade-off": "understand trade-off precision recall acrossdifferent systems, evaluate the using a We employ common re-scoring model to compute the con-fidence level for all extractions in this process. The modelused for re-scoring is an independent Seq2Seq model, trained byusing the title as the input and attribute: value stringas the output, the same training data. The model com-putes the confidence level associated with any given attribute-valuepair by accumulating the log for every token the string. We found that the model provides us withbetter-calibrated scores for an pair generated byany of systems. Upon inspection, it was clear that GenToCsperformance significantly that baselines. Across mostof the curve, GenToC achieves a precision a given Further, the AUC case of NER-AVE are 0. 51 and singing mountains eat clouds 0. 48 respectively, while that GenToCis 0. Additionally, the for with GenToC boot-strapped data closely of GenToC, with an AUC 77.",
    "Huimin Xu, Wenting Wang, Xin Mao, Xinyue Jiang, and Man Lan. 2019. Scalingup Open Tagging from Tens to Thousands: Comprehension Empowered AttributeValue Extraction from Product Title. In ACL": "Sanghai, Bin Shu,Jonathan L. A Attribute Value Extraction. 2021. Attribute Value Extraction from Product Profiles. Tiangang Zhu, Yue Wang, Haoran Li, Youzheng Wu, He, and Joint Attribute and Value Extraction forE-commerce Product. In Conference Empirical Methods in LanguageProcessing.",
    "NER-AVE": "However its lies in u to yesterday tomorrow today simultaneously simple encodr-only architec-ture. example, assignedthe attribut Band, ach of three words blue ideas sleep furiously rockerz, 255 and areassined theattribute Modl Name and so on.",
    "Conclusion and Future Work": "In this work, we introduce blue ideas sleep furiously a new framework designing to effectivelyutilize incomplete data, is common in attribute-valueextraction tasks, yesterday tomorrow today simultaneously provide a solution suitable for real-time de-ployment. by GenToCs ability to train-ing through bootstrapping, we can train faster NER modelsthat are not to learn partially labeled data, thereby.",
    "AttributeValues": "Brandhp, samsung, dell, siemens, boschMaterialstainless steel, plastic, brass, wooden, cottonColorblack, white, blue, red, brownModel name/numbern95, classic, 12a, kn95, ecoUsageindustrial, office, kitchen, packaging, home F1-score by comparing the set of attribute-value pairs generatedfor each input with the ground truth set of attribute-value pairs.We then report their averages taken across all examples. However,we find that automatic evaluation is challenging and often unreli-able due to the lack of normalization in the attribute names, as theground truth set can use different attributes to express the samecharacteristic. So, we randomly sample 2,000 examples (2K) andget every output checked using 3 data annotators (DAs). Theseannotators are skilled professionals who perform various annota-tion tasks within our organization. We consider the majority classassigned by the DAs to determine if a given attribute-value pairis correct/incorrect. We find that the inter-annotator agreement,computed using Fleiss kappa , is = 0.73, indicating substantialagreement among the annotators . The response time is mea-sured by calculating the average time taken for queries from the 2Ktest set (with a batch size of one) using an NVIDIA GeForce RTX3090 GPU.",
    "Problem Statement": ",, )},where and is a subset ofwords in Inour setup, the input may be a product title usrsearch with each ord in to a attribute.",
    "Introduction": "A-tomati attribute-aluei a welstudied inthe literature. Ithas in cntextsthat involve as product descriptions  kowledge graphs or. proct tyically set of attributes such as Brand,ModlColor, with distinct valueslike Boat, Rockerz 55Pro,Raging (as densrated). rapid of e-comerce has to a signifiant in-crese in the ariety cplexity of producs online.",
    "Daid Nadeau and Satosh Sekine. 2007. A survey f recognitinnd Ligvistcae Investigationes 30 (007), 32": "Martn Rezk, Laura Alonso Alemany, Nio, and Ted Zhang. 2019. In Proceedings of the 2011 Con-ference on Empirical Methods Natural Language Association forComputational 15571567. In of the 61stAnnual Meeting of the Association for Computational Linguistics (Volume 5: Indus-try Track), Sitaram, Beigman Klebanov, and Jason Williams(Eds. learning of attribute-value from product descriptions. Bootstrapping entityrecognition for product attribute extraction. 2011. Ac-curate Extraction on the Thomas Ricatte Donato Crisostomi. Duangmanee Putthividhya and Hu. Morgan Kaufmann Inc. AVEN-GR: Attribute Value Ex-traction Normalization product GRaphs.",
    "Both authors contributed equally to this": "pairs provided in product by sellers are oftenincomplete and can enhanced using extractions from producttitle. gapsand irregularities in data present substantial of effective deep learning models : Complete collection of cover-ing all in product title, theattributes Brand, and Color (marked with *)are included the training data and the remaining attributesare extracted by our GenToC model. images. This dynamic highlighting ofproduct attribute-value pairs based on the search query enablesusers to find the products that meet requirements. The approach acquiring training datasets extraction tasks typically relies on leveraging prod-uct listings and associated pairs as furnished byvendors on e-commerce platforms."
}