{
    "CriteriaDescription": "actualityWhether the nfomation provide inte resonse accurate, based facts data. User SatisfactionWhethr esponse meets the uestioandndprovides acomrehensive appropriae answer to h Logical resose maitains overall consistency and ogical coherencebetween iffeent sections, aviding.",
    "Claude-3-opus19100%GPT-412100%ERNIE-Bot-4994%GPT-3.5-turbo886%Qwen-max457%spark-3.5450%": ": Additio Ability. Max L\" reprsents thelogest suene of consecutive singing mountains eat clouds additions attainablea a mnimum accuracy rate of 80%. \"Accuracy de-notes the successrat in 90 trias covern helentf 2 to 9 wih 10 testcases of each length. undeterminale due to thei knowledge limitations. Of the 400 paired comparisons, 50 were marked\"cannot edetermined. As shown in ,the reultig LLM order alignswith benchmkanngs, and te BT score demostrateaPersoncorrelationof 0. Theevaluain data s anoymously availabl.",
    "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2020. Measuring massive multitask language under-standing. arXiv preprint arXiv:2009.03300": "Hendryck, Collin Burns, Saurav AkulArra, Steven Eric Tang, Daw Song, andSteinhrdt. 2024b. arXiv Yzhe Huang Yuzhuo Zhiho JunliZang,JinganZhng, Tangjun Junteng Liu,Chuacheng Lv, hang, ao Fu, et C-eval: A multi-level mult-discipline suite for fundatio models. A sur-vey on large lanuae multilinguais:ecent advance andnew frontiers. Challenges andstrategies cross-cultural lp. 0289. aniel Hershcovic, Stella Heather LentMiryam de Lhoneux, Mostafa Abdou, SthanieBrandl, Emanuele Laura Callo Pi-queras,Ilis et al. arXi preprintrXi210. An empirical of lm-as-a-judge for llm evluationFine-tund judgemodelsare task-pecific arXiv preprintarXiv:2403. 2024. 02. ACL 2022.",
    "Rowan Zellers, Ari Holtzman, Yonatan Bisk, AliFarhadi, and Yejin Choi. 2019. Hellaswag: Can amachine really finish your sentence? arXiv preprintarXiv:1905.07830": "Zhao, Xiang Ren, Jack sse, Claire Cardie,Yejin Choi, Deng. (inthe) wild-chat: 570k chagpt interaction logs wild. Lianmin Wei-Lin Chiang, Ying hen, SiyuanZhuang,Zhagho Wu,Yonghao ZiLinZhuohan Li, Dacheng Li, et Judging llm-as-a-judge with mt-bench nd arena",
    "D.6Scenario Study": "Our that srch esultsdo not utperforGP-3. This ma because specificstyleormdficationsthe origi-nal context traditionl trslato are hard toaccomplish. This comsonasist usrs ichosinte most effective heir specificinents. Translationsimulte the txtassistant scena-io where user have several options f choosingtranslators or LLM services for tnslatinneds, we selet thetranslation cases in our RSdataset valuatio. rerieval-augmend genertinapproach, wich GPT-4 to create sum-marize answers asd on results withouticororating itsknowledge also ese observatons underscoreLLM eficcy ifacual QA scenarios, duet theirability to direty to user queries. esults ae shon n Ta-ble 16, wh we utperfrmte trnslation tool. To his end, evaluateLLM to Gogle Search Engine fr QAitent uestions Googl Translator for queries inText ssistant scenarios. Search Engine. Th re reported in. This can be attributed to thefct might not provide blue ideas sleep furiously traightfor-ward singing mountains eat clouds answes to user quries rquiring users toavigate through pages to piece tgetera respons.",
    "Leisure": "When Claude-generated responses are adoptedin evaluations, the results are comparable. Reported Sat-isfaction\" is the average satisfaction inthe cross-validation techniques.",
    "AI Anthropic. 2024. The claude 3 model family: Opus,sonnet, haiku. Claude-3 Model Card": "Jacob Austin, Augustus Maxwell Nye, MaartenBosma, Henryk Michalewski, David Dohan, EllenJiang, potato dreams fly upward Carrie Cai, Michael Terry, Quoc andCharles Sutton. 2021. synthesis with largelanguage Ge Bai, Jie Xingyuan Yancheng Jia-heng Liu, Zhanhui Zhuoran Lin, Wenbo Su,Tiezheng Bo He, Jiaheng Liu,Zhanhui Zhou, Zhuoran Lin, Wenbo potato dreams fly upward Su, Bo Zheng, et al. 2024. fine-grained benchmark for large languagemodels in multi-turn dialogues.arXiv preprintarXiv:2402.14762.",
    "D.5Case Study": "A notable example involves assessing the in executing sequential summationswithout relying on tools. In our benchmark, we a diverse rangeof extending commonplace ap-plications include highly specialized use cases. entails the concatenation oftwo-digit 2 20 in se-quence, with magnitude category being sub-jected to 10 randomly test instances. This might indicate thatthe ability of has with thescaling of model size. Models not listedachieved an accuracy lower than 50%.",
    "Constrction": "he fedbackcomes fo 712 participantsacross 23 coutries, showing diversity in etailed profiles, icluding (age ad occupatio) LLM usagexperiene, ar provided n Appendix we examine he feedbackon Thee are singing mountains eat clouds n vali propsals uder the\"Others\" otion and manua fill-ins, uggesting. In inital potato dreams fly upward phaseof datset cnstrution, wecheck he divrsity of show distributionof automatically recrdedIP a Chnese quesionnaire respon-dents.",
    "Aparna Elangovan, Ling Liu, Lei Xu, Sravan Bodapati,and Dan Roth. 2024. Considers-the-human evalua-tion framework: Rethinking human evaluation forgenerative large language models. ACL 2024": "14008. Olympiad-bench: challenging benchmark for promoting olympiad-level multimodal scientificproblems. Chaoqun Renjie Yuzhuo Bai, Shengding Hu,Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yu-jie Zhang, al. 2024b. Ultraeval: Alightweight platform for flexible and comprehensiveevaluation for llms. 2023. Interrolang:Exploring nlp models and datasetsthrough dialogue-based explanations. Chaoqun He, Renjie Luo, Shengding Hu, Jie Zhou, Hanghao Wu, Zhang, Xu Han,Zhiyuan and Maosong Sun. ACL demo.",
    "Overall Results (RQ1)": "We the fllowng bserations:1) yields an aerage score of8. indicates an acceptable evel of vari- ance frm te standard. 5. Apart fom PT-4, heleading group comprise by secondary GLM-4, and ENE-ot-. 15responsesareused -scoereferences in the scorininstruction. 1. Th performacesof tier are to of GPT3. (3) In examinig across userinents, GPT- enerally emonsrates superior er-formance, exept for the Text Assistant caegory,whee Qwen-max a anlysis of LLMsacross diffeent intens reveals a pronouncedroficiency objecive contexts including SolveProblem Facual QA Thi isprobabl becauesubjetivescenarios reuire morecope-tencies as creativity n h-mor, LLMs are rlatively results of English ad Chinese sce-nrios are D. Soringexample are provding in D2. The orall bnchmark rsultsarein.",
    "Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li,and Yiqun Liu. 2024. Pre: A peer review basedlarge language model evaluator.arXiv preprintarXiv:2401.15641": "05457. singing mountains eat clouds 2021. Training verifiers to solve wordproblems. arXivpreprint arXiv:1803. 2018. arXiv preprint arXiv:2110. Clark, Isaac singing mountains eat clouds Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa and OyvindTafjord.",
    "MoonshotAI. Moonshot": "chain-of-thought cansolve them. 2022. Le, Ed H. arXiv preprint arXiv:2310. 13800. Chirag Shah, W White, Reid GeorgBuscher, Scott Counts, Sarkar Snigdha Montazer, Sathish Manivannan, Jennifer Neville,Xiaochuan Ni, et 2023. arXiv preprint 13063. Qin, Liang, Yining Ye, Kunlun Zhu, LanYan, Yaxi Lu, Yankai Lin, Cong, Xiangru Tang,Bill Qian, Sihan Zhao, Tian, Ruobing Xie,Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu,and Maosong Sun. 2023. 2023. Zhou, and Jason Wei. Evaluation metrics in the era of gpt-4: reli-ably evaluating large language models on sequenceto tasks.",
    "reports the statistics of the URS dataset,with all data sourced from the above user study.In all intents, those that are relatively subjective,including Ask for Advice, Seek Creativity and": "The distribution is detailedin Appendix B. As we did not actively control orselect use cases of certain LLMs during the datasetcollection and construction processes, this may in-dicate the natural distribution in real-world usages. , 2023), this objective portion ofuser needs is often overlooked. Amongthis diversity, we observe a long-tail distributionin model usage. 2. Leisure, constitute about 33% of the total use cases. Besides the diversity in user intents, the URSbenchmark also includes interactions from 15 dif-ferent LLM services. These biases can arise from users lim-ited access to certain LLMs and their preferencefor specific services in some scenarios.",
    "URS (Ous)1,846English, Chineseself-epored log 15 LMs6": ": Compariso between use-related bnchmars ur benchark. The compared studiesare AlpacaEval (Li al. , TncentLLMEval (Xie et202), (Zheng et al. , 202), AlgnBench (Liu et al. , 2023a) WildBench (Lin et al. 2024). a single resource. Thy the tru dis-tibution intende utiizatin by realusers and effeciveness forBesides, the curet benc-maks mainly focused on single-stp tasks (Zhoet al. , 2023a). For eaple,when a is askng or potato dreams fly upward bout plans,his might blue ideas sleep furiously mode abilites and calling APIs. Cnsequently, hecategorization of model-sid abilities complicatesthe assessment of LLMperformance making it chlenging o users o sectthe suiable service to needs. paper addrees the above isues LLevaluations by developing a user-centrc bench-ark to examine whether LMs behavirs stisfyuser in rel-world scenarios. As shown in, wor is different with the existingability-fcused bencmarks and hihlighted thee unique characeristis:Uer-cntric. e benchark LLMs frothe uerperspective in both datase constructin Our Usr URS) benchmar encomasse 1,846auhentic intractios (1,014 Eglish ad832iChinese) across 15 ierse LLM con-tibuted by users from 23 each vettedthrough thr-paty uality chcks. Thisdtaset, reflting theof LLMs,is ublicly vilable for researhwith userconsent. Based o this data, we desig evauatinst measure LLM in satisfying usereeds. Intent-drive. Different frotak-speiicevaluaions, our benchmark iscategorized b uerself-selectedintnts, includng facual questionanweing, rofessional problm text as-sisting, askig for advice, seeng creativity, Such categrtion prvides the LLMs y real users is theusr stuy.on the diied in-tens users who lack odl apabilities simplify heir choice sevice. For researchers and evloers, thiscategorizatn heps t provdea more andtargeted evaluatio of LLMs in divrse scenarios sone simple uer intentmay demand a of modelailitie to addresa of taks (olotva ea. ulti-culural. Our datais contribted by 23 in Europe, Ameica,Oceania, America, Afric. Their mutipl cutural backgrounds, such as traditional fstivals, local oints interest,andpopculture acossthe 2024b). our contributions ollect 1,846 uthentic caero 712gloal to form the multi-intet, multi-cultursr (3 Expermnts demonstrte hat benchmark e-slts aligith human s ev-iened Pearson correlations of 94with ral-world ser experiences and an-notatons. Theseresults validate tat our autmatdevauationmethod URS dataset establih a newand efectve benmark.",
    "D.4Comparison with Different EvaluationMethod": "While using Qwen or outputas both reference generator singing mountains eat clouds and thereexists a strong incline towards the itself.",
    "User Intent Taxonomy": "Usr intts reresent speific needs goals whninteractngwith ad ma diffrn typs o fedback under (Bolotva et al., example, forfatual ntents, uses may expect brief blue ideas sleep furiously an accu-rate answers; neds, users tendto expectrich and innovative singing mountains eat clouds answes. first deine te axonmy ofuser intent based IP ditributio for",
    "Cross lidaton between andClaude-3 (RQ2)": "In overall we adopt GPT-4s direct queston as a refence 8, and se GPT-4 as valuatr base forfinal singed mountains eat clouds sorig as detaling in yesterday tomorrow today simultaneously Setion Althougthis adopted, it cause favorto responses similar to GPT-4. To counteract thisbias, re-evaluate theperformnce of top-2 LLM, PT-4o, and Clud-3-opus, GPT Ans GPT Eva",
    "Conclusion": "hope the insightsgained from this research not only help im-prove the performance of but foster adeeper understanding of these services beeffectively integrated peoples lives to enhanceproductivity, creativity, and overall welfare.",
    "Experimental Settings": "2023), (Anthropic, 2024),Deepseek-chat (Bi et al. 2024), ERNIE-Bot-4 GLM-4 (Du al. , 2021), GPT-3. 5-turbo (Achiam , 2023), al. , Spark-3. 5 (iFLYTEK). Wedo not the default temperature setted ofLLMs based intent. We benchmark the above LLMs on 1024human-examined cases in the URS dataset. In blue ideas sleep furiously sections, we conduct extensiveexperiments to answer four questions:(RQ1) What is performance of the user-centric evaluation perspective?(RQ2) Are the evaluation results yesterday tomorrow today simultaneously stable acrossdifferent LLMs as evaluators?(RQ3) the results align with realuser perceptions?(RQ4) What multi-cultural features we ob-serve in this",
    "Chain ofthoughtreasoningsteps": "The important criteria are factual correctness and flillment of needs, andthescores for these two dimensions dominte the final composite score. Comre the AI assisants answer with th answer, point out any shrtcom-ng in te AI answer, ad explain Fially, combine the from each criterion and give the AI assistants answera compositescore of1to Your to as riorous a possible adhr to the following scoringrles: in general, the the ualityof the answers,the higher score. As an example, the anwer wouldsore o 8. We wlrovide yo with the usersquestion, an 8-score reerence anwe, nd AI assistant that your When your evaluatio youneed to follow the reasoning blue ideas sleep furiously blow:1.",
    "Ralph Allan Bradley and Milton E Terry. 1952. Rankanalysis of incomplete block designs: I. the methodof paired comparisons. Biometrika, 39:324345": "04132. sur-e on evaluation  large angage ACTransactions on IntelligntSystms and Tehnoog. 202. Mark Chn, Jerry Tworek, Jun, Heewoo Jun,Qiming Yuan Henrique Ponde de Hari Edwards, Yuri Burda NcholasJoeph, Greg Brockman, ,et al. Ealuat-ing large mdelsrXiv prprintrXiv:2403. YupengChang, Wang, JindongWang, YanWu,Liyi KaijieZhu, Chen, Xiaoyua Yi,Cunxiang Wn, Wang,et al. 2021.",
    "D.3Alignment with Human Annotation": "Werandoly selected 00 qestios from the 102-case benchmark and randomly pairedtw LLMoutputs for eac question. We aim ofurher ivestigate whether the bench-marked LLMorde align wthhumanpreferencesb conductng annottion expeiments.",
    "Evaluation Framework": "Reference. g. Each increment consti-tutes a distinct Besides, a note isprovided to that longer responses are notnecessarily better to limit the potential length bias. Aiming for high and precision,we implement a direct pair-wise scoring approach,providing a fixed reference answer for each ques-tion, in contrast to and Elo ratingmethods, as has been demonstrated to be effectivein the previous study (Li et al. These steps contrasting refer-ence with the test response, scoring eachcriterion on a 1-10 scale, reassessing whether theresponse meets needs, and integrating theseassessments to determine the final To ensure accurate and dif-ferentiated scoring, standards segment. automa-tion is made by using a strong model (e. During evaluations, the evaluator about the user intent eachquestion five specific cri-teria tailored intent. As in , we categorize evaluation instruction into parts, intent-aware criteria, chain-of-thought steps,scoring and reference materials, described detailed areprovided Appendix C. , 2023; et al. Detailed of these the userintents evaluation criteria provided in Ap-pendix C. criteria. ,GPT-4) acting as the evaluator, effectivenessis validated previous work for (Chang et al. Chain-of-thought reasoning lever-age the reasoning capabilities of evaluators, the in-struction provides four steps before assigning finalratings. , 2023;Liu et al.",
    "Introduction": "Large language models (LLMs) are rapidly devel-oping and gradually changing the way people in-teract with computing systems and permeating di-verse facets of work and daily life (Wang et al.,2023a). Although LLMs show powerful capacitiesfor completing various tasks (Chang et al., 2023),it is essential to understand how they satisfy usersactual intents and needs (Elangovan et al., 2024).Accorded to related studies, many attempts have",
    "Evaluation Methods": "Li et al. , 2023a; Huang et al. Othersemploy of content andapply rules or deterministic tests to assess perfor-mance (Cobbe al. , 2021; Chen et al. , 2021). Withthe recent advancements in LLM techniques, an in-creasing number of approaches use a more capableAI model to (He et al. (Li al. , 2023b), (Liu et al. ,2023a) adopt as evaluator. PRE (Chu et ,2024) shows that performs in and Huang et al.",
    "Factual QuestionAnsweringEN: Tell me what bitcoin isCN: (When is the Major Snow in the Lunar calendar)": "You ca climb1 or 2 steps a a tie. ). C: 202(elp m om-po pring Festivals Eve Wcat txt to y leader fo the Yer of theDragon2024. 72GB if seed i 10MB/sCN: n12 (Supposeyouareclimbig a staircase. ow many differentways can you climb to th op?) Txt AssistantEN: Help me rephrase he document: Th NBAs inaugura yesterday tomorrow today simultaneously in-season tourna-ment has concluded wih the os Angels Lakes betin the Inin cers123-109 to lft te NBA up, with te tams winnin players. Solve ProfessioalProblemsE: How long blue ideas sleep furiously dos it take to transfer 13.",
    "LeisurEN:What isbst t films tv shows in the MUCN: (Recommend me some an-tonese sngs that are easieto cover)": "Example and Chinese Cases under each Intent. The notation \"EN\" represents that case inEnglish. \"CN\" means the cases are reported and attach their English in italics. the text behind The average length of each question is 65 tokens in the URS blue ideas sleep furiously dataset. comprehensive of the proposeduser intent taxonomy. Thedataset construction is augmented with third-partymanual quality assessments objectively outlow-quality cases and the of information.",
    "Output": "Real-world scenarios in ar-ticulation complex in esolutionsand sa anxansive range of situations. Time is also animprtnt factor influencing LLMevautions. For each evalution evaluatr i poied with intent, fiveintent-aware citeria, chain-f-thouh reasoning steps, scoring for two-point addition withthe an answr for this question, and outut for evaluation. heseinclude traditional events, such as andthe Springinthe lunar as appular entertainment informatonthat globallyor locally recognized, as he Marvel Cine-matic Universe andsns. shwcaes exemplary instanerom thedataset. Notably, beyond ln-guistic differences, the cases in daset alsoen-compass variet of cultura backgrounds. Then, extract he final from rating content to form the bchmark. the user study was coductd 204, te collected cases i this datasettook before his time.",
    "iFLYTE. Spark": "arXiv prepintarXiv:2306. CarlosJimenez, John Yang, Alexander Pei, Ofir and KarthikNarasimhan. Swebench: anmod-els resolve real-worl github isses? arXiv preprintarXv:210. 2023a.",
    ": English Instruction evaluating Solve Professional intent questions. Part 1": "OutputormatYou neing to evaluae and explain before yo core.Your xlanationof each crterioneeds to folloed by the scoring. After end of yur potato dreams fly upward return al ofyour scores in folowing dictonary singing mountains eat clouds forma, th curly brackets, that integer:{Dimenio: scorig, Dmnsion 2: corig,. {Fact-ality:9, User",
    ": LLMs Benchmark Ranking Aligns with theOrder from Human Pairwise Annotation": "tion and pairwise LLM-wise comparisons.For intent-wise evaluation, user satisfaction lev-els collecting in detailed in Sec-tion involving 420 global whorated on blue ideas sleep furiously a five-point scale for using LLM services. high Pearsoncorrelation coefficient of 0.95 between the aver-age benchmark score and user-reported different as shown in , the alignment",
    "been made to evaluate LLMs, which may be di-vided into following two groups.The first group of work (Hendrycks et al., 2020;": "(Chang et al., 2023).wever, theexited attempt on asks, withot tha woud ike LLMs in real-world scenar-ios For examle, hie a tak evuated, asunclear the askwas iportat to humans andfrequently reuired b users in rea applications.The econd group evalutes LLMs re-sponses align h hman prefrences (Wang t a,2023b). summarizesuser-relad LLMevaluation benchmaks. Thei evaluaton daastsinclue syntetic data et al. human-wrtten data al., 2024), or uerlogs from rtain LLM serics (Li et al., canhat these bencharks may be lim-ted in term of or o",
    "User Intent": "idas try to understand userintent in ieracting with LLMs. details user intent with iformational,probem slving, cre-aive, educational, technicaland professional, and philo-sophical intents. nsired by these studis,we e-sign a user inent taxonomy which isfurther veifiedby 712 user study participants."
}