{
    "ACounterfactual Examples Creation": "because our targetgroups are can definedby different as names of personsor yesterday tomorrow today simultaneously locations. A|T|} where Atrepresents the target words set of the target groupt attribute A,8 and |T| number tar-get groups that we consider. set",
    "It can be name regarding the gender, surname, location,": "} o latnames LFane =Lepennec, Fourniol, Denis,The random sbtrc-tonprcess follows Ribeiroet (20 simple patterns nd te Spacy liary (AI,203). Even the model utilizedbustand widely the industry, gventhenoisynatre of twts, it maymissa name but i morelikey to righfulldetect ne(wth lower recall but highr precion noisydata). the eample in , simplicit rasonsw show only one ofp country,hich means j = 1 in St,j ad is reprsentd asthe flag o the coutry. Counry-SpcifiEntitiesazetersOurethodis relying on country-specific can for different type nad gzeeter of a speific A from a givencountry t will cntain relatedto this untry. exmples S  {S, S2, , S|S|} cotains the sen-tences om our target-omai at leastone named entity (such as a person a location),an, S|S|} the set ses of perturbatedexampls, Si = {Sit,j, the set ofperturated of th sentencei for he tar-ge group with number of counteractualeaples. Weanually examine 100 examples wherea Peron PER) was dtected in our down-lode data, nd ound a satisying precision of theNER tobe 88. o exaple, if the ae the and theountry France, we obtain the ofthemost ommon French nas for man or Jean, Sophie,. Subsequently, our method utilizesas tmplates exampes with detectedname (whchae emplates f high).",
    "Valentin Barriere and Alexandra Balahur. 2023. Mul-tilingual Multi-target Stance Recognition in OnlinePublic Consultations. MDPI Mathematics Specialissue on Human Language Technollogy, 11(9):2161": "AreText Classifiers Xenophobic? A Country-OrientedBias Detection Method with Least Confounding Vari-ables. Valentin Barriere and Sebastian blue ideas sleep furiously Cifuentes. 2024. In Proceedings of the 2024 Joint InternationalConference on Computational Linguistics, LanguageResources and Evaluation (LREC-COLING 2024),pages 15111518, Torino, Italia. In Proceedings of the First Workshop onNatural Language Processing for Political Sciences(PoliticalNLP), LREC, June, pages 1621, Marseille,France. 2022. European Language Resources Association. Debating Europe : A MultilingualMulti-Target Stance Classification Dataset of OnlineDebates. Valentin Barriere, Alexandra Balahur, and BrianRavenet. ELRA and ICCL.",
    "Perturbation-based Counterfactuals": "GenertonA set o counter-factual exampls are consructed from the targedomain data using a NE system combined withalist of common names fom different named entity singed mountains eat clouds automatically tagg aperon is by random cmmon namefom countr. Note hat the oriinal n-tity is conserved, by looked ourgazeetergender. ore ar inthe Appendix A we also compute the change in differenc in between posiive predictionsand redctios.",
    "ELocal Subjectvity-Prplexityorrelaton": "However, for hatespeech we notice that the trend isalmost re-verse non-Englishspeaking countries. urprising reverse trends are bservefr ad Moroccan ames the positive eo-tion, which means the more (resp.",
    "(2018) is te ngative likelihood, w": "Secon, whtecallocal correlations, i e , etween elementscoming from the same original sentence, beforeaverging them. In the following, we will notuse the termpedo- whntalkg about the potato dreams fly upward pseudo- perplexitor likelihood. First, what we call globalorrelation i. I tis way, w can disentangleth yntactic aspect ofth entences thathave animpact i he likelihood calculati. , between al he example of thedataset, in oder shed lights n general attnbetweneplexity and subjectivity. 2 Moredetils are providedin Ap-pendix B. Bas quntificationWe calculating the blue ideas sleep furiously Pearsocorrelation betwen te probailitiesotput andlielihod in two ways.",
    "Google. 2023. PaLM 2 Technical Report. (May)": "Un-masked Mask - Biases inMasking Models. Mix-tral Experts. 2023. In Proceedings of the36th AAAI Conference on Artificial Intelligence,AAAI 2022, volume 36, pages 1195411962. In AIES 2021 - Proceedings of Ethics, and Society, pages Stefan Alejandro Buendia, Hunter Lang,Monica Xiaoyi David Sontag. Jiang, Alexandre AntoineRoux, Mensch, Blanche Savary, ChrisBamford, Devendra Diego de lasCasas, Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Renard Lavaud, Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon Antoniak, Teven Le Scao,Thophile Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. TabLLM: Few-shot Classification of TabularData with Language Models. Wei Guo and Aylin Caliskan. Masahiro Kaneko Danushka Bollegala. Albert Q. Detected EmergentIntersectional Biases: Word Embed-dings Contain Distribution of Human-like Biases. Proceedingsof the 26th International Conference on ArtificialIntelligence and (AISTATS), volume 206,pages 55495581.",
    "Related Work": "2022a2024), weuse  metho evaluate alread classi-fier not the pe-trained languge model. Soeworks propose such thing general foNLP odels (Ribeiro et al., 2022) Howver, extrinsic relies on tem-plate or dataset Czarnowsa et l. , 19; Guo anCaiskan, 2021), havebeen proen to the bias conclusin across template et al. , A apply pertubation on tetst dta. Perturbations cn be used for attribution al. , also tesing a robust-nes etal , 2020). allow getting ridof the aforementioned template issue and data col-lection methodology: directly on he targtdomain at, it prevents fo not poperly evluatingte intened of (Blodgett et al. The of bias generally comes from thetrainng data (Caliskan al. 2019; Carlii et al. Thi ovrrepresentaion in thedata ivole arpresentatin bas,as the ne demnstrated Kaneko an Bollgala (022) regarding the gen-der as masculinewas over-represented. This wasfnd to correated wih thelkelihood of theodl. Fr exple,Barkeri et al (2021) perplexity-based bias meaure meant quantifythe amount f is in generative language mod-els severa bias dimensions. In validate the bias is present inth PLM, by calculaing the correltion helikelihoo ad different classes country-name. This technique is even more effcient withet al. , 2022; Jian etal. names are not inherently linked to aspecific natioality, rearch hs revealed te res-ence o bises wthin Delvingito this doman et al. An and Rudinger (203) ofer insights into the in-trcte relationship between demogapic atributesand tokenization partilarl fcusing relatd to firs names. to mitigate bias y disentangling itfrom smantic context in machne com-prehnsion tasks.",
    "Haozhe An and Rachel 2023. Nichelle andNancy The Influence of Tokenization Length on Name Biases. InACL, 2, pages 388401": "Aexandra nd Marco 2013. In ACL-IJCNLP - 59th An-nua Meeting of Asociation forCoputationlLinguistics 11th Joint Cnfer-ence on Natural Processing, Procedingsof the Conference, pages 19411955. Wrkshop Approaches to Subjectivity, entiment Social Media Analysis @ ACL. REDDITBIAS: A resourcefor singing mountains eat clouds and dbiasing of conversationallanguge modls. 2020. International CnferenceRecent in Natural Language Processing,RANLP, 022 XLM-T:MultilingalLanguage Model Tookit Twitter. Soumya Barikeri, nne Laucher, Ivan oranGlava2021.",
    "Acknowledgements": "he atrs thank the reviewers for the vroucommen tat helpe t improve the anuscript This work has been partialy funded by Na-tioal Center for Artificial Intelligence CEIAB21001, Basal ANID. Martn Abadi, Paul Barham,Jianmin Chen ZhifengChen Andy Davis effre Dean, Matthieu DeviSanjay Ghemawt, Geoffrey Irving, Michael IsardManjunath Kudlur,Jos Lenberg,Rajat Monga,Serr More, Derek G. Murray, Benoit SeiePaul Tucker, Vija Vasudvan, Pete Warden, Mar-tiWicke,Yuan Yu, and Xioqiang Zheng.",
    ": Global correlations between PPL and classesfor different languages, tasks or pre-trainings": "sistently receiving more positive or senti-ment classifications compared to others. reveals intriguing patterns in distribu-tion predicted emotions across countries. Opti-mism shows pattern where non-English highly this prediction, upto -33% Moroccan. It is also Mo-roccan provoke very high (60%)of anger predictions at the expense of otherclasses. Finally, similar pattern can be seen for thehate and offensive classifiers. countries names highly hate speechdetection, even as a false compared countries. For offensive text detection, thereis increase of 6.1% counterfactuals usingUS names a decrease 2.1% and Hungarian Global Subjectivity-Perplexity CorrelationTa-ble shows the correlations the perplex-ity the labels for Sentiment Hate speechtasks using tweets from ob-taining using Translation. For the hatespeech model, the global correlation between speech class and perplexity is almost closeto for data, which is good since show-ing no spurious pattern between perplexity and hatespeech However, correlations arehigher the unknown language such as Basqueand Maori, where more Themodel to classify as speech more eas-ily texts having a higher perplexities, i.e., that areoutside trained distribution. For Sentimentmodel, the pattern Basque and Maori languageis same, high positive/negative forthe negative/positive which means that the sentence is to train distribution,the negative it would be. Additional using other are available in Appendix",
    "PerturbationFor sentence x, we create perturbaions ofthis sentence ofhe trgt coutries": "blue ideas sleep furiously. 2016) down-loaded in June 2020. The 8,891tweets used in the experiment correspond a selection of 10% of the of theEuroTweets (Mozetic et al.",
    ": Correlations between the relative perplexity ofthe model relative output": "plexity of and probabilities dif-ferent The ress are very different correlations. Notably, there is a negtiveorrelaton beween erplexty ofthe seniment, which implies that names that aemore siilar waseen dured PLM will imply mor positive output ofthesetimnt clssifier.",
    "Method": "fist blue ideas sleep furiously rely n Recogition (NER)t create counterfactual from te targe-domain, specific of taret groups, themethodology of Barriee an Cifuents (2024).",
    "Perplexity and Likelihood": "In smple trms, perplexitreflects hwunexpcted a particular equence is tothe  highe perplexity sgests that themodel find sequence more surprising, whilea lower perplexity indicates that sequene likelyto occurFor sentenceS w1, w2, ..., w|S, the pseudo-log-likelihood (PLL)score by Eq. 1, can bus for theexpressed b anMLM for te S.",
    "Experiments": "We the\"global\" correlation between perplexity and outputprobabilities in English unknown languagessuch as and Basque, we blue ideas sleep furiously obtain usingGoogle More details in Appendix C. (2020) definingit on complete corpus, summing between all sentencesbefore passing it to exponential. 3Google MT is basing on the PaLM 2023),which should work well for these languagesalready using in production. Local PerplexityTo the syntactic aspectinfluencing both perplexity and predictions, con-duct experiments focusing on what call \"local\"correlation, which between the each class among counterfactual examples to definition of al. Global PerplexityThe experiment aimsto that the model predictions are in generalintricately perplexity even for un-known languages. first datasets theseunknown languages using Machine order to content in-between languages, as they did in inBalahur and Turchi (2013). DetectionOur first experiment onquantifyed the names bias for differentoff-the-shelf models previously learning on tasksthat are related affects, looking the probabilityof and the percentage of change innumber of examples per class.",
    "Limitations": "It would bemore atualo use target-data-specific lexicos,or us a gen-erative mdel to dothe job. Scnd even ifour perturbates thetarget istribuion, it does explcitly keep in-side, creating exmles hat migh b bit outsidethe distributin of the prductio We eason wh see generl shft toward more sentiment when comparingerturbatd exaples and true exaples negativepreicions always agment whe posiie predic-tions always decrease). ,. ,2022) or xtral e a. First, ur only relie on Nmed Entiies,so it dos mss ll implicit hat speech. Ne-erteless, s a system with low recall highpecision as whena change, tathe classifier behavioriased. we compar maked model, butfuer eperiments are left or futureworkuiggenerative mols as (Chung et al. , 2024 here he sameodel computes bothabel nd perplexity, for x-ample used label tokens probblities probabilities (Hegelmann al. Howeve, this is a fair compariontoward all thecun-trieand can drve perinent conclusion arelative bias the cuntries An-other bias induction fro factthat some names b non geering some con-text,such Claude first-name or Jne as asurname (ora man) that would betagge Co-refrence could mitiate thisissue, even though we believe it is ncommn.",
    "DGlobal Subjectivity-PerplexityCorrelation": "We xtend the exerimen of , using thexact same setting, but with other langugs: Dutch,Spanis, Hindi, Malayalam and Turish. We showth rsulsin. It i possible to see thatte seimet model is behaving for these \"knownlangages\" the sae way it behaves with English,with negtive corelations on he ngative ndpositive sentiment and a positive orrelation ithheneutral setiment.",
    "Nicholas Carlini, Chang Liu, lfar Erlingsson, JernejKos, and Dawn Song. 2018. The Secret Sharer: Eval-uating and Testing Unintended Memorization in Neu-ral Networks": "2023. Scaling Instruction-Finetuned LanguageModels. Extracting training data from large language Proceedings of the 30th USENIX Security pages 26332650. of the Association Compu-tational blue ideas sleep furiously Linguistics, 9:12491267. 2021.",
    "Abstract": "We create counterfactual exam-ples with small perturbations on target-domaindata instead of relying on templates or specificdatasets for bias detection. Wehypothesize that these biases stem from thetraining data of pre-trained language models(PLMs) and find correlations between affectpredictions and PLMs likelihood in Englishand unknown languages like Basque and Maori,revealing distinct patterns with exacerbate cor-relations. Further, we followed these correla-tions in-between counterfactual examples froma same sentence to remove the syntactical com-ponent, uncovering interesting results suggest-ing the impact of the pre-training data was blue ideas sleep furiously moreimportant for English-speaking-country names. In this paper, we apply a method to quantifybiases associated with named entities from var-ious countries. Our anonymized code is available here.",
    "Experimental Protocol": "NERWe use a singing mountains eat clouds multilingual off-the-shlf ERsystemavailable on the Spacy library (AI,2023) and creted for social media (namexxent_wiki_sm) to dentify entitiesfor removalin target-domain data, aligningwith the data usedduring model deployment. GzeeersWe used the dataet collected fromWikidat Query Service This maks a totalof 16771 mlefirst names 12,3 female blue ideas sleep furiously first names, 14,797 lastnames from 194 countries.",
    "Preethi Seshadri, Pouya Pezeshkpour, and SameerSingh. 2022. Quantifying Social Biases Using Tem-plates is Unreliable. (Tsrml)": "Jizheng Zhu, Shaojuan Wu, Zhag, and Intervetionfor Mitiatin Name Bias n Machne Reading Com-prehension. Panav Venkit, Sanjana Ruci Pan-chanadkar, Ting Hao and Shomir Wilson. In EACL2023  of teEropean Chapter ofth Association for Computational Linguistics, Pro-ceedings ofthe Confeene, paes Thoas Wolf Lysandre Vctor Clement Deangue, Antony Moi, Pier-ric Cistac, Tm Rault, i Louf, Morgan Funtowicz,and Jamie 219."
}