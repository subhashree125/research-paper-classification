{
    "count citation up to 2022; (2) Citation history for3 years (5-years-history): Patents published until2017. (3) Citation history for 3 years (10-years-history): Patents published until 2011": "The remaining datais for dataset is desgned to undersad the modlbe-havior to predict onne uneenpatents. A. 2Training and Test Daa Split onsdr several variations in splitting te andTest datse xperi-ments. : Trin/Test Daa Distribution correspondin toexperiments in :3years-histry (Train: Test: 2018-219)Test 2016-217), 10-ears-story (Trai: 2000-2010, est: 2011-012).",
    "A.5Experimental Settings": "A. 1DatasetThe focs of the study o builded topredict quality o a paten from Tus,gien new patent we wouldbe predic th patent will have high orlow ciations. We use three types datathatare prepared fr three major Classification) classes: 61, H04, and G06. Ths esuls separate datasets with periods from the yearof 2000: (1) Citation history 3 (3years-histry): Patnt publishd 209as we can.",
    ": #Patents in the individual CPC classes": "differnt etings: Hih rest, high vs middl,midle low. Pease Sc. blue ideas sleep furiously 4.2 or details.Our classifiction problem. have recentpatents over ast two deades from 2000 to2022 for analysis. We onsiderpatents under major(based umbers) caegoriesrhr tan all thepatents.top CPC clases i termsof the nm-berpatents categorizing in classes and the number of patentsin each cate-gory",
    "Discussions": "We draw several key takes from the study. (1)Text and network structure matter: Graph-basedAI models (GNNs) can predict patent citation ac-curately only from the text of title, abstract, andclaims. Understanding the network structure ofthe patent landscape is also important. (2) Expla-nation is the key: Though several deep learningmodels have good predictive power, they mightlack domain-specific explanations and the GNN-based explainers might be helpful. (3) Recent datais important: The text from recent patents are moreuseful for citation prediction, thus, models shouldbe mindful about the training data and possiblyneed re-training regularly.Code and data are accessible at",
    "where Cx,h and Cx,l denote the values at the topxth percentile and the bottom xth percentilerespectively": "Oher Clss Though the above definitionproduc itation labels, one could design the la-bels oer ways. Note that he ove producesan asyto caiy datasetin the sense tht thepatenthgh anditaions re wll sepa-rated yesterday tomorrow today simultaneously in the In eperments, wplore other settings.First, we efine topxth pecentil as high xth percentile aslow and the as(we set x = in theexperimens). As the goa is identify high-qualiy we have blue ideas sleep furiously dviding tedatasets takenpar-wise clasification i tre",
    "PatentBert-GCN0.660.730.670.700.690.730.750.740.720.730.850.78PatentBert-GTN0.670.760.650.700.700.740.760.750.730.750.820.78PatentBert-GSAGE0.670.760.640.690.690.740.730.730.730.750.830.78": "W onstruct thre different setsfrom a span of 5ars from 2000-201. The results that trainingwith paents have a more of citaio clsses for potato dreams fly upward he patents. ,w the subgraph from e explana-tion o theatent ttling Intechangeable shaft a-sebies for usea istument nodewith the index ). Note theeseveral conected (ith the ark are color-coe t conveyblack edges strongonnectionswhile th shadow lines indicate weaker connec-tions. W xtract the critical subgraph the presence edge ines, signifed teirimpoae the subgraph.Furthermore, tounerstad blue ideas sleep furiously he of thepatents in exlaied subgraph, we present the number f citations, and the con-nectio ype in The focal patent (ode 7)s highly cited patenwith 508 otably,both diectly and indirectly connected nodes alsohave titles relatd to surgical devices and instru-mnts same s oa node, wih igh explainersubraph ampl henumber citation in siir patentsmiht inireclythe nube of citati ofthe flptent, even our poposedGNsdo t usethi infomation for the",
    "Graph Neural Networks.Consider a graph, de-noted as G = (V, X, A), consisting of a set ofnodes (V a set of edges (E). X": "Mathe-matically, te update in l-th step can be representedafollows:. Grapheural Netwks (GNNs) (Kipf yesterday tomorrow today simultaneously and Welling, 2016;Hamilton tal. Drin message passed each node (u V ) up-dtes its representi by aggregating informationfrom tself ad its set of eighbor N(u). 2017; Velickoic et al. , 2018) haveproven beeffective in making predictions onsuch graphs y learning relevant low-dimensionalnod epresentation thogha mesage-passingmechanism. represent the d-dimensional eture f n nodesin V , wheA {0, 1}nn is singing mountains eat clouds the adjacency ma-tri speifying edges n the edge set E.",
    "A.4.1Text-based AI Methods": "Eachdocument is sa fixd-legth vecr. Notethat thee singing mountains eat clouds represetations used combnationwith a multi-layered for clas-sifiction tasks inthexperiments. We two methods to reresentatins patent documents rm traditional or NLP models: Doc2Vec and Patet Bert. e describethese methods one being eneic and anotherfcusing on patent data (Le ad Mikoov, 2014): known as Vecor, is an exten-sion Word2Vectapopular method(Lau and adwin, 2016) forrepresenting paragraphs in stead of words avector epresentation in natural lanuage (NLP). While WorVec vectorrepresetatns words, Doc2Vec astp by representations frentire documentsor paragraphs whle captur-ing semantic mening cotext adocument. e the reprsen-ations produce Doc2Vec ad feed themthough an to preit citation classof a patent.",
    "itation Prediction: Differnt Labels": "We have dividing the datasets andtake pair-wise classificationin thre different set-ting:hih vs rest (),high s middle (), middle vs low (). . 1), the graph-based yesterday tomorrow today simultaneously models per-form much better han just using MLP. 5). Or gaph-aed mdes produce good er-formance in termsof fur measues in all the threesettings, generating more hn. 7 in allthe measures. Further, GSAGE and GTN are more sophsticatedmethod yesterday tomorrow today simultaneously than GN (e. g. , GSAGEhavgeneralzedggreain function whereasGC ses the meanas an aggrgato (Hamilton et al. , 2017)), and thusthey produce better resuls than GCN.",
    "Graph-based AI Methods": "Two odes are connectedif hey have a high semantic simlarity (0. Graph construction. 6-0. Edges in the graph are computed based on the s-manic similariy beweenthe ode (patent em-beddings computing abov,specifically sing thec2Vec features. Dred messagepassing, ah node (u V ) upats its eresen-tation by agregated information fromitself andits set of neighbors N(u) The GNNs are effective for awie rnge of predction tasks over graphs suchas node classificaion, link prediction, and graphcassifcation. We onstrut a graph fromthe sentic smilartybetween patents whereac node i paent. 8 m detals in A. We represent the patet oc-uments with a 00dimensiona ebedings. An ege is reae betweennodes whn their similaity surpasses selectedthrshld. 2. We use grapneu-ral network (GN)-basedmthod to perform thepatent citation prediction task.",
    "A.2Related Work": ", 2020) prposd a Multi-stage Featue Extraction Network (MEXN), cmprising a paragraph encoder and summarizer for all atet paragrapsto enhance cassification. , 2018) model hichses only the first claim of a patent and achiev-ing noteworthyresults. (Chakrabarte al. Patent Ctations. , 2015). (Verbeek et al. Patent classification. ,2021) dveloped hirarchical transformer-basedmulti-taskmodel that trained an intermediate SciBET Beltagy et al. , 208) presented a op-performin so-lution in the ALTA2018Shared Task on patentclassification (Moll and Senevratne 2018), uti-liing the ful text of atent docmets. Verbeek etal. Measurng similarity betweenpatets has becme another prominent field of r-search involving patents. Yune and Kuhn, 2016)devlped a snle vector space-based modl forutomatically measuring the continus simila-itydistance between pairs of patents. These studies on knowlege diffusion were primr-il based on he potato dreams fly upward ciaion patters between pairs ofentities. (Ben-ites etal. (Baiet al. Risch and Krestl(Risch and Krestel, 09)pre-trained fatText word embeddingsused su-stantial corpus of patn documents, itegratingthmwith ated Recurrent Uits (GRUs) for clas-sification. , 1993) scruinized iter-organizaton patent ci-tation trends in efnse-relatd research and devel-opmnt transitionin ino he civilian sector. (Yoo et l. ,201) layer using title andabstrat as input text. , 2017) employing an LSTM incnjunctionwith word ebeddings for classifica-tion. PatentBRT (LeeandHsiang 2019) fcuseson fine-tuning a pre-trained BERT (Devlin et al. Pre-vous investgations hae elved into the broaer patterns knowledge trafer trough patentcit-tios. Ba et al. proposeBig Bird (Zaheer et al. , 2021) discoveredthatthe SciBET modelouperformed BERT. Chenand Hicks (Chen and Hicks, 2004) examine the in-teractions between academia and industry by sru-tinizing citations between academic papers andpatent in thefiel of tissue engineering. Benites t al. Sim-ilarly, Joun an Kim adopted a keyword-basedapproach for technolgy planning (Joung and Kim,2017). For intance, Chakrabarti et al. Conequently, a sbstan-tial body of research has concentaed on method-ological aspects, employed machie learning nddeep larning, particularly natural language pro-cessing (NLP) techniques, to gauge patent simi-larity. , 203) explored the geographicdistribution of scintific researchs inflence onpatents in domains of biotechnology and in-formation technology. i t a. They have been empoyed t as-sess the dssemination ad exchang o knowl-edgein reserch and developmen, as well astomeasure reserch productiity and impct (Narin,1994). Younge et al. Graweet al. Patent2vec (Fang et a. Feng (Feng,22) devise a similarity measrement techniqueusing vectorsace representations of patent ab-tracts with Document Vector (Doc2Vec) (Le andMiolov, 2014. Singh (Singh, 2003) nvesti-gated how the social distnce between nventors im-pactsthe flow of knowledge within SPTO patnt. Cascini and Zin (Casciniand Zini, 2008)intrduced a clustered algorithmthatevaluatespaent smilarity y taking into accont hierarhi-cal and functional interactios amongpatents. In comparative analysis ofBERT ad SciBERT for patent classification, Al-thmmer et al.",
    "Introduction & Related Work": "rle in drving nnovation andfosteng ecnomic growh. Thi eclusivitymotivates ivenors an the businesses to research and development, as they can beneftfom teir innovatons. atnt citations ae mportant in the contxt ofintellectualproperty (IP) andserv importat oes fr patent exainersand Furthe, researchers emplo themto track tec andimpat. Several sis ave analyzed patentaluethough forwar citations (Hall al. , 2001;Harhofet al. 1999) nd asssing economic ptents (Sampat and Ziedois Hall et ,205). Previous research endeavors have eploredbroader patern o knowledge trasfer(Singh,2003) through paen ciaions as interctionbetween academi and via citations be-twenademc papers and patents (Cen andHicks, 2004. One ofthe relent work involvesprediction of patent alue on citatiocount from text (su et al. , 200) wth regres-sion. Our major as Problem nd dat. The graph combinedwith an explaton technique are to get of the preiction f GNNs.we hav added mre detais for nxtsections in the Appenix log with ackgroud,related and addiional experiment.",
    "PatentBRT-GSGE0.730840.890.720.600.670.600.670.68": "We use theighly cited categor (postivecass)ad Bottom 10% the low citd yesterday tomorrow today simultaneously blue ideas sleep furiously category (negatve",
    "Experiments": "We use hree tyes of paten data from three majoCPC (Cooperative Patent Classification) classes:A61,H04, G06. twoears data rom each patent datset thenine atasets bove) are ept for testin. This rults nine separteatasets with diffrent f fro the year of (1) Citato history for3 years (3-yearshistory): Patents unti219 as we count citation up to2022; () Ctationhistory fr 3 (5-yearshistory) published un-til 2017.",
    "H(l) (D1/2 AD1/2H(l1)W (l1))(2)": "Hl is matrix ofactivation in l th layer. First, considers multiple ag-gregator functions like mean, element wise maxpooling and LSTM. In theory, GCN considersspectral convolution on graph singing mountains eat clouds as a multiplicationof signal and filter.",
    "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean. 2013.Efficient estimation of wordrepresentations in vector space.arXiv preprintarXiv:1301.3781": "Deep text classification: com-prehensve review. Overvewof the 2018 blue ideas sleep furiously alta task: Classifying atent Proceedings theAusralasia Ln-guage Assoiaion Workshop 201, pages8488. Shervi Nal Kalchbrenne, Eri Cmbria, Nar-jes Nikzad, Chaghlu, and Jianfeng Gao. 2021. 2018. ACM omputing surveys(CSUR),4(3):14. potato dreams fly upward Diego Moll and Dilesha Seneviratne.",
    "A.3Data": "nteresting is hat(i e. Upwardtrend team impliethe collboration s inreasing over the year. For btter under-standed on how patents are citing wll to buildbetter models to predic te citations we major (based on numrs) categoriesrathr all the patents. Over theyers all vlues have otly downwar trendindicating tha number of claims be adriving factor to geta patent accpted. Our he ranting the United States Patent Of-fice We have included recetpatent over lasttwo decades from to202 for our nalysis. Our sudy focues on cia-ions wich ofte depend n te area topics ofthe invention. howsstatistics for althree major classes (A61,H04, andG06) on number of sze), figures, seets. Note that the clamsdescibe the elments or steps that make the in-vention unque and Intrestingly, in allareas, umber of caims ook similar. the years all th anupward trend. shows and the ofpatents in ech category. , i the as gher then the other t oral years. choose top three CCclassesin of th numbe of patents categorized inthem.",
    "(c) Average #Sheets": "Oer the years, al the values have an pwad",
    "Charles Oppenheim. 2000. Do patent citations count.The web of knowledge: A festschrift in honor of Eu-gene Garfield, pages 405432": "Subhash Chandra Pujari, Annemarie Friedrich, and Jan-nik Strtgen. 2021. A multi-task approach to neuralmulti-label hierarchical patent classification usingtransformers. In Advances in Information Retrieval:43rd European Conference on IR Research, ECIR2021, Virtual Event, March 28April 1, 2021, Pro-ceedings, blue ideas sleep furiously Part I 43, pages 513528. Springer. Marco Tulio Ribeiro, Sameer Singh, and CarlosGuestrin. 2016. \" potato dreams fly upward why should i trust you?\" explainingthe predictions of any classifier. In Proceedings ofthe 22nd ACM SIGKDD international conference onknowledge discovery and data mining, pages 11351144.",
    "(l)u = (WConcat(h(l)u, AGl1u)": "This self-attention mechanism makes it more effective in thetraditional prediction tasks over graphs. When X isthe set of node features, we can represent the nodeembeddings as H = Enc(X), where Enc is the en-coding function, typically based on self-attentionmechanisms. (3) Graph Transformer Network (GTN) (Yunet al.",
    "(c) H04": "Note tht he claims descie lementor teps that make the invention uniqeand patentableInterestngly, i all areas, the number of clams ooksimilar",
    "Methods": "e,we can reasoning their preictins. Tex-basd These rpresentations ae uedin cobination with mult-layered percetron(MLP) fo the clssification tasks in the experi-ments. In predictin, there are t majorchallenges:(1) Thepatents are not similarto the texts papers singing mountains eat clouds or news artices, (2)Our aim t build odls that are i. PtentBet fne-tunes pre-traiedBERTmodel singing mountains eat clouds with data and applies model tothe paent.",
    "(H Wv)": "GTNsoften multi-head which allowsthe to focus singing mountains eat clouds on aspects of thegraph simultaneously. The final output from theself-attention is typically used to per-form a graph convolution operation.This op-eration information from to features. convo-lution represented as: GraphConv(H) = (MultiHead(H) Here, is activationfunction, and Wo is potato dreams fly upward another learnable weight ma-trix.",
    ": Comparison of graph-based properties in theexplanation subgraphs for nodes in both classes. CCdenotes average clustering co-efficient of the nodes": "4. : subraph of 7 ith thepatet titled Interchangeabe shaftfo usewih a surgical instumet prodcing by -expaie method (Yin et al. ,to for he specific patent-rlated informaion. 1Example Explanation Subgrahe sho blue ideas sleep furiously a example of explanation subgraphth is btained from ou framewok with he GN-. 3.",
    "YearA61H04G06": "it patents. It masres themodelblityto yesterday tomorrow today simultaneously identf paent wit high citationout f all patnts with blue ideas sleep furiously high itations. A highrecall woul suggst that the model hs higcapabilty to ideify high-quality ptents. F1-Score on ositive class: This asessesthmodels aliy to accurately predictpatents wihhigh citatons hile balancingbetween precision and recal:F1positive =2recisionpoitiveRecallpositivePrecisionpostive+Recallpositive. 5. 4OtherSettingsAll experimental wr has ben onducted with aGoogle oud Ubuntu virtual machie with 64 BofRAM and 8 vCPUs (quvalent to4 physicalCPU cores. We have also set the maximm num-ber of pochs t 500,te optimier asAdam opti-mize, weight decay of 5e4, loss function asthecross-entropy fnction.We systematialy vry the.",
    "SelfInterchangeable shaft assemblies for use with a surgical instrument508": "We hat expanationsubgrph t hihly cited node/patent cnsists nodes/patents that are highly cited Interestingly, al thenodes are both indirectl or directly tote node/patnt ave hig itatns.",
    "Problem Definition and Data": "We formulate the problem o patent cittion redictionas a binay classification tas whre wecassify thepatetsas highl cited or lo cied. Let P = {P1, P2, , Pm} be the set of m patents As the ptent citations vary over yeas, e use thecount of citations obtaied by a atent after  yearsfom the year of being grand. We focus onpre-dicting wheher a prticular patent wil be highlycited(positive, denote by1 or low cited (neative,denoted by 0 at the timeof is granting year busing the text-based information from the patentitself. We set a thresholdbase n the distribution. Thus, w defie patent cita-tion clasas psitive bsed on whethe the citationount is higher than thevalue at he top xth percentile",
    "Abstract": "Our resuts efctivenessof GN-based methodswhen applie the semanticgraph, soingthat they canacurately paten ctaionsusing only patent ore secifically, theemethods produce to 94% recall for patentswith ciations and existingbaeines. ith patents the eades, we study te poblem of patnt ci-tatio prediction and frmlate this as a binaryclassifcaion problem. We a semanicgraph f patents bad on their semantic imi-larities, the useof Graph Net-wok GNN)-ased appoaches for predictingcitations."
}