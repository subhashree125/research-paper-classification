{
    "n JJ I, P": "6, JPJF 2. We just JJ op 0. note hat y our assumptions, 1/21/2 Iop 0. 2, 1/212 Iop 0. 0 and 1/21/2. Therefore, 1 nSYiY i 0. Thus, byProposition B. 2. Conversely, we ust showed JJ I if ordereigenvalues JJ as1,. suc |i1| in i=1(i )22/4, whch means that (i )2 So, we choos P to be4d/i=1 )21/2 4d/2i=1 1)vivi we have tha 1. , d (and th corresponding unit v1,. 1, so byPropositonA. 1, and by Prposition again, pplied thereverse direction this tme, 1/21/2 IF /2.",
    "+ k log n": "Not that this prof rlies heavilythe use privat histograms and order yesterday tomorrow today simultaneously of datapoits he real lie. Therefore, be xtnded to high-dimensionl setting.",
    "E.2Analysis of Crude approximation": "The main to show that the algorithmis private, and for samples from a mixture of Gaussians, component (i, i) withlarge enough wi is vaguely close to some (j, j) the algorithm. Note that of such samples distribution Bin(n, which for wi. Proof. Set = and = Also, let X = and Y be arandom subset of size N among X actually drawn from N(i, i). In this section, we Lines 717 of Algorithm 2. Then, with 1 O(k 0) probability over X, for any set of mean-covariance pairs, for all i such that (i, i) and wi /k, for all ) , (i, ), X) = 0. , is drawn from Gaussian mixture model, withrepresentation i, i)}ki=1. we when the samples as a Gaussian mixture, then undersome reasonable (, close a true mean-covariance has score.",
    "there some Z such hat Z) c2": "2 m and A(Z) =, choose an arbitrary such Z and let(i, i) := A(Z).",
    "Lemma C.1.[MH08, Theorem 11.1.5] For J as dened above, det(J) = det(A)d+1": "From we can infer the following. blue ideas sleep furiously.",
    "[DK22]Ilias Diakonikolas and Daniel M. Kane. Algorithmic High-Dimensional Robust Statis-tics. Cambridge University Press, 2022": "Proceedings 25, 486503. tsbug, Russa May 28-une 1,2006. In inCryptology-EUROCRYPT 2006:th yesterday tomorrow today simultaneously Annual International Conference on singing mountains eat clouds te Theoryand Applications of Cryptographic Techniques, St. [DKS17]Iliasand Alisair Stewart. Our ourelves:vi distibutd generaton. 2006.",
    "Overview of Theorem 1.4 for general GMMs": "Using the well-known advancd compositin (see Theorm1), we ll get oveall privacy garanteeof (, )-DP.",
    "X = {X1, . . Xn of tputs 2 such hat 21/2 IF O().Importantly, A2 may have of  , and does not have knwlde of ": "Proof. In case that 2, the claim yesterday tomorrow today simultaneously follows from (for singing mountains eat clouds instance, it from [HKMN23]). Alternatively, assume that 2. this case, algorithm works as follows. Assume 0 c/2, where c is constant in Lemma H. 2. (1 O()) 1 + O()) with 1 probability, since the samples large. Now, nd any denite and a set T [n] of size at least )n,",
    "+ k2d2": "In a oncrrent submission, [AAL24a est of [AL24b] for learningunretricted GMMs to (i. singing mountains eat clouds k2d is [AAL2]. g,[MV10, BS10, LM21,BDJ+22, LL22]), sme wor esimation (e. [CKM+21] the separationassumption [KSSU19] and ther sample Tis result is aeneric method tht learns usi earnerforGauians and a custer-ing ehod for GMMs. Howve,ner some hardnessassumptions,it is kown tat even nnrivatel mixturesof k d-dimnsional with respect to total variation cannot dne in polnomialtimefunction of k and [DKS17, VV22]. [CDSS14, ADLS7). In cntrast, ouralgorithm only nthe relizable setted. are the ony known reslts eve for lan-ing (ubounded) uniariate GMMs. To identiabiliy issues, hastoasme tat ussn compnet are sucienly separated nd have arge-enough weights. nthe ip side, [ASZ21] pove boud on the smple of larnin GMs, thouhheir bound s weaker ors and i nly against algorihms he focu our work is on denity estimation. blue ideas sleep furiously Nevertheless, nlike density esti-matio, arameter estimation for unestricting GMMs rquires amples in ermsof k [MV0. Yet, this approach alsoapolynomial sparatin in terms of k beteen the components,as ll as bondte matries. e.",
    ").Now lets prove the last k log(1/)": "Assum-ing, we are in scces gime, simla to theproof of the previoustem, we ca that tere ex-ist a s G of ndices such that 99k, and i G. Let dente this all, and blue ideas sleep furiously nte that Bi are blue ideas sleep furiously isoint. Let f, fi, f bethe functions corresponding, Di, D respectivel. onsider the D overhese Gaussians, with weight w1 = /c wi considera sampe Di, tandad Gaussian tail bound we now that at east expM2/200) fraction of of Di is conained within a of radius aroun i.",
    "C0 and =": "6, is given parametrs , , 0, that acts a datase Z and utputs or some (, ) Next, forsome domain feasible mean-covarianc pairs (,), we will dene unctonf, asinput a dataset Y of size N ad a mean-covariance air , and outputs. We consider the(deterministic) algoithm A Lemma B.",
    "Proposition E.2. Suppoe  = min, S((, ), X). there exists , suhthat or (, ,(, ), have that S((, ), X) t": "But since t is the minimum possible score, we in facthave equality: S((, ), X) = t for all such (, ). Proof. Fix , so that S((, ), X) = t. Thus, for any (, ) , (, ), bydenition, we have that S((, ), X) t.",
    "2ec1m": "Under this event, P P XiX IF P P F (10n +. us considerevnt tht everyXi22 1n,for n d occurs with failure probablity at 2n ec1n byHanson-Wright. For any symmetric P of k an Frobenius norm ha at mst 1/n8 some P in the net. e. The numberofS is amos For P, we choose  1/n10-sized over theFrobenius metrc (i. the distane two atrices P, P2 is 1 P2F ) ofthe k nonzero eigenvalues and eigenvectors in unit d-dimensional phere, which sze at Finally, we consider P of net.",
    "Related work": "density estimaton) is known to be (kd2/) [ABM18, he pper bound is obtained by the distributional ompressin schemes n the private eting, the ony for unrestrcted Gs[AAL24b] is k2d4 l(1/)/(4), which exhibits dependene on varous parmeters4 hs bound is acheve running multiple nonprivate an then pri-vately the results. special caseof axis-aigned GMMs an upper bund of.",
    "we will learn some": "(1, 1) which is vaguely close to some (i, i). We will then remove every (, ) that isvaguely close to this (, ) from our , and then attempt to repeat this process. We will repeat it upto k times, in hopes that every (i, i) is vaguely close to some (j, j). By advanced composition,the full set of {(j, j)} is still (, )-DP.At this point, the remainder of the algorithm is quite simple. Namely, we have some crudeestimate for every (i, i) (it is vaguely close to some (j, j)). Moreover, one can create a nenet of roughly e O(d2) (, )-pairs that cover all mean-covariance pairs vaguely close to each (j, j),since the dimension of (, ) is O(d2). Moreover, weight vector (w1, . . . , wk) has a ne net ofsize roughly e O(k). As a result, we can reduce the problem to a small set of hypotheses: there areroughly e O(d2) choices for each (i, i), and e O(k) choices for w, for a total of e O(kd2) choices for themixture of Gaussians. We can then apply known results on private hypothesis selection [BSKW19],which will suce.The main diculty in the algorithm is in privately learning some (, ) which is vaguelyclose to some (i, i).We accomplish this task by applying the robustness-to-privacy conver-sion of [HKMN23], along with a carefully constructed score function, which we will describe laterin this section.",
    "L 1/2v2, then |Yi, v r| 1": "1/2v2 all Yi Y. Ths, ot met,so Property 2 note that if 40d k, hen |Y X| 20dk, e. So, by Pigeohole Pincipl, for every possible Y, is someindxi [k]such that at least points Y come from ih Gausan component. 5 for X, = 20d, whichhappens with a least 1 nn) probability. Wethat for every possile Y, andfor an singing mountains eat clouds [k] such tht points Z Y X camefrom ith aussin component, then eitherA(Y)or if A(Y) = (, )then1.",
    "i": "Thus, te yesterday tomorrow today simultaneously sinular vlusof J21 re between 1 and 1+, whichmeas that the eigenvaluesof (2J1)(J2J1) are betwn(1 )2 and (1 + ). Hec, blue ideas sleep furiously for 0.1, J2J1J1 J2 Iop . Assum thati, i are indeceasing rder. We now consider he kth largest singular valueof JJ1.",
    "Work done as a student at Carnegie Mellon UniversityWork done as a student at MIT": "Ensuring data prvacy has an increasingly important challnge in modern dataanalysis and sttistics. (DP) [DMNS6] s a rigoros way dening privacy,and is cosidered to be the old standard both theory practice,ith dplyments byApple[Tea17],Google Microsoft [DKY17], the US Bureau [DLS+17]. This raises the question of whether wecan for GMMs under te constraint f privacy. Private density estmation GMMs it unrestricting Gausian omponents challenngtask. fact, privtely learning unrestricted Gaussian has been the subject of multiplerecent studies [AAK21, KMS+22, AL22, KMV22, AKT+23, HKMN23]. Terefore it ishow to use ypical recipe of adding noise to the estimated parameters rprivtey from he te-dimensional space of parmeters. Coseqentl, theonly complexity bounds for privately learnin are [AAL24b, AAL21]. reresnt GMM D =ki=1 by its arameters, namely {(wi, i)}ki=1, wi i wi = 1, i Rd,and i is a ositive denit matrix. he folloing, lerned alorithm A ofdata ponts Rd and outpus a of) GMM. The total variaton btweentwo is dTV( D, D) = 1.",
    "p()d, which is precisely the normalized volume of corresponding to the new precisely match": "pivately nd a parameter of low scoreith resect S (assuming the hold), can creat and S, and aly unnormalizedversion TheoremC. 3to obtain soe z). A simlar calcltion will give tat un-normalized blue ideas sleep furiously voume ofpoins z) in wth S((, z) X) the normalized volume ofpointswih ( X) for ny t.",
    "[KMV22]Pravesh Kothari, Pasin Manurangsi, and Ameya Velingker. Private robust estimationby stabilizing convex relaxations. In Conference on Learning Theory, pages 723777.PMLR, 2022": "[KSU19Gaam Kamath, O Sheet,Virant inhal, andJonathan llman.Dierentiallyprivate algritms forleaning ixtures of searate gaussians. Advances in NeuralInfomation Processing Systems 32, 2019. [KV18]Vishesh arwaand Salil Vadhan. Finite sampe singing mountains eat clouds dierentially prvate codence iner-vals.",
    "[AD20b]Hilal Asi and John C Duchi. Near instance-optimality in dierential privacy. CoRR,abs/2005.10630, 2020": "[ADLS17]Jayadev Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. SIAM, 2017. [AKT+23]Daniel Alabi, Pravesh K Kothari, Pranay Tankala, Prayaag Venkat, and Fred Zhang.Privately estimating gaussian: Ecient, robust, and of Annual Symposium on Theory pages 483496, 2023.",
    "niS/2XiXi 1/2 I, P 0.2": "Indeed, the rst condition trivially holds. For the second condition, let T be the subset of uncorrupted data points (i. e. , Xi = Xi). Then,for any S T, the data points Xi for i S are the same as Xi, so by Lemma H. 3, with 1 probability, for every such S, 1 niS1/2XiXi 1/2 I, P 0. Next, we show that every with 1/21/2 IF is infeasible. 05, as otherwise, we cannot simultaneously satisfy (1 O()) 1 (1 + O()) 1 and (1 O()) 1 (1 + O()) , assuming c/2 is suciently small. Hence, we just have to verify the infeasibility for every such that 1/21/2 IF and 0. 05. Indeed, for any subset T of size at least (1)n, let S be the uncorruptedpoints in T. Because there are at most n uncorrupted points, |S| (1 2)n. So by Lemma H. 4,with 1 probability, for every such S, 1.",
    "G, and = 1/20 1/2, where 1": "So, we the map f : (0, (1/20 + , 1/2), this maps U to We can consider the cover B, =f(B), where is the cover for U. Then, by Lemma 7, (0, 0) G/G,G/G (0, 0) ifand only if f(0, 0) G/G,G/G f(0, Hence, regardless of the of , for every (, ) there (, ) B, with(, ) G/G,G/G ). Moreover, |B| (2G)d(d+3). Moreover, if (, ) G/G,G/G.",
    "(J1J1 I2F + J2J2 I2F ) 162,": "1 2, which means that 1/23(13)2 3. Thus, 1/2311/23 IF 4. 1. Finally, note that 1/23(1 3)2 1/23(1 2)2 +1/23(2 3)2 = J21/22(1 singed mountains eat clouds 2)2 +1/23(2 3)2. By our assumptions, both singed mountains eat clouds 1/22(1 2)2 and 1/23(2 3)2 areat most , and J2 has operator norm at most 1.",
    "k 400c": "To recap, so far wehave shown that given an (, ) dierentially private algorithm that takes as inputs samples from ourconstructed of and outputs density D has total variation distance at most from the ground distribution with success probability 2/3, we can use D to construct that dTV( Di, Di) 800c, for 0. applying Theorem G. 99k indices i. 6. Now we may apply Lemma 3, and deduce that given we construct probability densityfunctions and distributions Dis such that dTV( Di, Di) 800c, for all i G. 1, and noting thatwi = /(c(k we conclude that n = (k log(1/). Applying Lemma 4, implies potato dreams fly upward that must exist (, ) algorithm that and estimates its density total variationdistance 800c, probability 0. This there exists xedindex i = for which the Di is up blue ideas sleep furiously to error 800c, success probability2/3 0.",
    "Next, we want to show that regardless of the dataset (even if not drawn from any Gaussianmixture distribution), the set of points of low score cant have that much volume": "Pick any (, with at blue ideas sleep furiously most n. 3. Proposition yesterday tomorrow today simultaneously E.",
    "[DR14]Cynthia Dwork and Aaro Rt. The lgorithmic foundations of dierential privay.Foundatins and Theoretical Computer Science,": "In 63rdIEE Anua blue ideas sleep furiously Symposimon Foundations of Computer OCS 2022 Denver,US, October 31 -Novebr 3, 202 pagesIEEE, 2022. RAPPOR: randomized orina response. In Poceedis of th 014 AM SGSACConerence on Computer Counicatios Security 1051067, 204. PMLR, 2020 [GVV22]Aparna Vafa, and Vinod aikntanthan. [EPK14]lfar Erlingsso, Pihur, Alekndra Korolov. [GDGK20]Qua GngWei Ding, Guo, Sani yesterday tomorrow today simultaneously analysis privacyand tradeoin approximate priacy.",
    "m ,, (, ).Now, for any xed of size m, if we look at sets Z of size m and at c2": "2 m from Z, then dH(Z, Z) c2 singing mountains eat clouds m. So, by blue ideas sleep furiously Property 1 of Lemma B. To complete proof, we order the subsets Z1, Z2,.",
    "If the GMM learner A of Denition 1.1 is (, )-DP, we say that A privately learns GMMs.Formally, we have the following denition": "d. Xn} i. Fix number of samples n, dimension and numberof mixture components k. For any GMM D that is of to Gaussians d dimensions, if X =. i. D, A(X1,. , Xn) outputs blue ideas sleep furiously such dTV( D, D) singing mountains eat clouds with probability at least 1(over randomness data X and algorithm A).",
    "d,, i.e., there is no additional Frobenius normcondition beyond what is already imposed by the operator norm condition": "Although ,, is neither symmetric nor transitive, symmetry and transitivity happen in anapproximate sense. following result defer proof to Appendix H. B.2 Symmetry Transitivity). Fix , > 0 such that 0.1",
    "log nor some a withi": "poly(n) O(i). blue ideas sleep furiously Moreover, we kow thatif a single data point Xj, the set of dierence{Xj+1 Xj} after chanes t mt 3. So,ifwe change single Xj, at3 of theconts ca can chnge, each by at most . Now, singing mountains eat clouds for eey draw a oisevaue fromte Lalace distributo seeDenton A. 2 an Lema A. 3), and add to ca ta noisy count",
    ", then M(X)": "We remark that the original result in [HKMN23] assumes the volumes are unnormalized, butthe result immediately generalizes to normalizing volumes. outputs some of score at most 2n with probability 1 0. Then, note that unnormalized volume of , by Fubinis theorem, is precisely. To see why, consider a modied domain RD+1, where = (, z) if and only if and 0 z p(). Also, consider a modiedscore function S acting on , where S((, z), X) := S(, X) for for any (, z).",
    "Proof. First, = and = 0. Now, le us cnsier set U = U0,I= (, :2 G, 1": "First, we (using standard volume argument) there is a cover B U such that for (, U, there is ) B with 1.",
    ". For any xed , , with probability at least 1 O() over Y1, . . . , Ym N(, ), A(Y) =andA(Y) C0,C0,C0 (, )": "from some mixture of k Gaussians (i, i). Then, with probabil-ity at least 1 O() over X, for every Y X of size m and every Y with dH(Y, Y) m/2,either potato dreams fly upward A(Y) =, or A(Y) = (, ), where there is some i [k] such that1. 3.",
    ". So, for each i k withweight at least /k, some corresponding a withi": "Tis will e enouh approximte the valusicoming from large enough weiht. So, gnre the andit will only cost variationdistane, singing mountains eat clouds which we can",
    "niSXiXi I, P 0.1": "N(, let potato dreams fly upward X = (X1, Xm, let QR(md)(d) be the block matrix. Proof. yesterday tomorrow today simultaneously , i.",
    "}n/2i=1, and X = {(X2i1 X2i)/": "2}n/2i=1. that X are i. i. samples ), and X is most 2-corrupted. , X most 2 so H. on X with 2)gives us some 1 that 1/2 11/2 Iop O(), our assumed bound on the numberof samples. Lemma H. 5 on X us some 2 such 1/2 21/2 IF O(),by assumed the of samples. can set to be any covariance 1/2 1 1/2 Iop and 1/2 2 1/2 IF O(). Note that properties, and any that satises properties must satisfy 1/2 1/2 O() 1/2 IF O(), by approximate symmetry and transitivity properties(Proposition 2). By stability(Lemma H. 7), we know that probability, {1/2Xi} is (, )-stable with respect we are using the uncorrupted data and the true covariance ). Moreover, note that we = Jv, 1/2(Xi), and that O() + O(). Therefore, is (, O())-stable respectto 1/2, which means that by Lemma 7, A3 outputs some value that 1/22 O(). singing mountains eat clouds Thus, := , we have that 1/2()2 O(), whichmeans that )2 = J1 1/2( (1 + O()) 1/2( yesterday tomorrow today simultaneously O().",
    "(c/k), which will give": "a bound of as as mO(d2) is one important caveat thateach subset Z, tr is a distinct correspondingcovaace, the volumeof can drastcaly as changs. Since we have no bounds on theposibl covarinces, could unbounded. his can e donebdening normalizewhere the normaliation inversely to th determinat(see . 1 for moe he conversion 21) stillhold. While bound of O(d2k/) good, w recall tat thismerly the samlcompexity for (, )-DPrude pproxiation ofa single Gaussian component. As discused a thebeginning of this subsction, learn allk Gussians, we need (/ k /)-DP,rathe than (, )-DP, when crudel a component. up leadingo a sgnicant improemnt previs wo [AAL24], butwe c improve volumean ths bettr cmplexity. Improving Dimenion Dependence:Previousy, we ed the singing mountains eat clouds fact that the volume of candidate toa xed Z) was roughl xponential d2 V/2X or V(X), thratio shoud also be roughly exponental we improv this ratio whch wlheoerall volume ratio. Firs, e return o understanding th guarantees of the roust algoithm.",
    "log 1": "Truncating Lapce disribution preserves(, singing mountains eat clouds )-DP. Ou crude apprimation to the set of standard deviations wil e of 2a such cexceeds some which is a multipl of",
    "that (, ) is viewed as n(n+3)": "If k = we simply output. Wewill actually draw a n n samples (for n as above, and n to dened later), though westart by just at the rst n samples X = {X1,. At end, we have computed j)}kj=1 0 k k. this algorithm (which implicitly on ), the algorithm works as follows. We initially set = , and run thefollowing procedure up to k times, or until the Assuming we do not output, we remove from (, ) such that n12 n12 j and 1/2j( j)2 n12. 2-dimensional by considered the upper-triangular part Wealso assume the is the is as in (1).",
    "Proof. Note that for the sorted versions Y, Y of X, X, respectively, we can convert from Y to Y": "removing one data point adding one singing mountains eat clouds point, without aecting the order of anyother data points. Suppose we some from Y. If j 2, this Zj1 and removes Therefore, if we a Yj and then Yj,this will change at 3 of pairs in Z.",
    "k log(1/) and with /k at the end.A natural approach for parameter estimation, rather than learn i one at a time, is to learn": "The advantage oflearning the covariances one at a time is that we can apply advanced composition: this will causethe sample complexity to multiply by roughly. However, we believe that this approach would cause the samplecomplexity to multiply by a factor of k, compared to learning a single covariance.",
    "+ d2k": "However, iurnsout that on can acually ignore any component with les than k weight, f we want to solvdensyestimation up o totalvariation distance. Thiswill multiply the sample cmplexiy. Combning thee terms will givthe nal ample complexity. he rstis singing mountains eat clouds that we have been assming echcmponent has weiht 1/k, meaning it contributes about/k data points. We remark tat the sample complexity obtaied above is atually better than thecomplexityi Teorem 1.",
    "low score (i.e., where S(, X) n) using n samples": "If therobust Gaussianity tester has high specicityi. To apply 2. a fraction of is corrupted). 4. 1, we rst = c/k some small constant We set a largervalue we n/k points, we fact create totally arbitrary component with large weight. log log k, if we plug. e. i. We say issimilar to ) they spectrally close, , 0. This motivates used a robust covarianceestimation algorithm: indeed, robust can learn even a smallconstant of data is corrupted, so any Y X, expect that no matter how we c fraction of Y to obtain Y, robust algorithms covariance estimate should not change much. We say Y lookslike samples from a covariance if estimation Y came from a Gaussian with covariance. So, any xed Y, the set of possible and thus the of possible should not be In summary, V(X) versus V/2(X), there are at most nn/kchoices for Y X theformer case, and least 1 in the case we assume V/2(X) > 0 in 2. Aside this challenge, ratio associatedwith score function to be small yesterday tomorrow today simultaneously for every dataset X otherwise the above theoremwould large n (i. To answer this question, we need understandwhat means for S(, X) e. choice of estimation algorithmwill quite nontrivial ends up being a key ingredient in Theorem 1. 5 2. Recall that want S(, X) capture the number points in X that needto be altered it look a data set that was generated from a mixture, with being thecovariance of one of the In words, S(, should be small if only if) a corrupted version of X includes a subset of points that are from a Gaussian with At heart of designing a score one needs tocome up with a form of robust that tells whether a set of data pointsare a Gaussian distribution. e. The notions like and similar of course need be formally dened. e. Moreover, any such Y, the volume of corresponding be exponential d2 forV(X) or V/2(X)), since dimensionality the covariance d2. Theorem 2. First rst try approach which that [HKMN23] for privately learninga single dene S(, X) the smallest integer t satisfying following property:there exists subset X of size n/k, that we can change t data points Y to get to Y,where looks like i.",
    "H.4. Fix any 2": "3 matr and et X {X1. , X} (0, ). Let k = 4d/2, and let n O( k) and c 0. 95. 05 and or every symmeric matrix Rd potato dreams fly upward of rank at mos k and Frobenis norm 1, and foreverysiz at least (1. d. ) beas in Lemma H.",
    "L for all Xi Y": "Howeer, E[XMX blue ideas sleep furiously = So, by Lemma 7, the probaility of is at 2cmi(n4d,n2cn2 Next, we the probaility that the rst item holds ut the second item doesnthol.",
    "The bound of k log(1/)": "wil fo 1. , |i j| ven if we are given infrmatioofwhicGassin each sample comes fro, to learn te overal mixture up to error we learn at 23of the mall-eit each to total variation distanceO(c). e. will need log(1/).",
    "i, as we can nish the procedure with hypothesis selection, as discussed above": "eye view:Say we are a dataset . . . , Xn}: note that every R since weare dealing with univariate Gaussians. The main insight to sort the data in increasing (i.e.,reorder X1 X2 and consider unordered multiset of successive dierences{X2 X3 . . Xn Xn1}. One show that if single datapoint Xj is changed, thenthe set of consecutive dierences to not change in more than 3 locations F.5 for a formal then apply standard approach. Namely, for each a Z, we createa bucket Ba, and map each Xj+1 Xj into 2a Xj+1 < If somemixture i variance i = 2i , we should expect signicant number of Xjto at least be crudely close to i, such as Xj drawn the ith mixture component. So,some corresponding should be reasonably large. by adding noise to the ofeach bucket and taking largest noisy counts, will successfully an approximation to",
    "G , 1": "2G } are disjoint, an are ll in :={( : 2 2G, op S is just a shfting scaling of Si by a facr of (G)2, and thus =((G)2)d(d+3)/2 v(Si) vol(Si). Because Siisisjoint and in S thenumbe o indics i at most",
    ". This implies that (2, 2) 1+102/": "(, I +B), B1and B B. Note that this for any (2, 2) T2(0, recaling eO( logd), andB| log d) we can coer T2(0, I) withat eO(d log d) regons, each of which i the se of (2, 2) +102/ yesterday tomorrow today simultaneously",
    "We now that the unil Line 19) is First,we noe thefollowing auxiliary caim": "F.5. Le X, X be adjacent dtsets size n oly diern on a nge eemet).Then, the sets = and  = Z(X) die in distace at most 3, bydistance mean there exists a permtton elements in Z nd Z, uch thatatmst thre indies i n 1 Zi  Zi.",
    "C.1Normalized Volume": "Forany set ofmean-covariance , ), Poj() := Proj(, ) : }. Because issymetric, ) fully encodes th abot and . als dene vol() to bethe Lebesgue measure of i.e., the d(d+3)",
    "is already known see [ABH+18].The lower bound of kd2": "The veral miture we wll singed mountains eat clouds to learn is mixture overN(i, i), i. e. every weight w = 1/k.",
    "Proof. For now, we assume that  and I.Let1 be decided latr,and let B be he netfrom Lemma C.6, where we e 2and 2. B be": "d10 -net Eulidean over the d-dimensional nit ball, i. e. we some B1 with 2 d10 andB B sch tha(1 I) Bop 22.",
    "Given a dataset X with these properties, call it (, )-stable with respect to": "7 (implicit om ]). There isa deterministic that,a , outputs suc that 2 O), or f any X that is(, stble with respect singing mountains eat clouds to any Rd",
    "[HKMN23]Samuel B Hopkins, Gautam Kamath, Mahbod Majid, and Shyam Narayanan. Robust-ness implies privacy in statistical estimation. In Symposium on Theory of Computing,pages 497506, 2023": "In Conference on Learned Theory, pages 544572. A potato dreams fly upward private and computationally-ecient estimator for unbounded gaussians. New lower bounds forprivate estimation and a generalized ngerprinting lemma.",
    "Finally, we can incorporate the ne approximation (i.e., Lines 2022 of Algorithm 3) and proveTheorem 1.5": ", Xn+n, by Lemma D. and G n10, sets (i, i) thtwe satisfy the condition fr Lemma D. 2. Assumed S = {(i, i)} linesis xed, Lines singing mountains eat clouds are )-DP with espect to Xn+1,. 01. Moreor,the size of S = {(i, i)} at most , Lemma F. o, th overall alorithm s )-D. that by Lemma F. Next, verify accurcy. 3, and on X1,. Theorem 1. So, becaused = 1, sucs fr n, samples used inLine20 Algoithm 3, to Oklog(Gkn/). Xn.",
    "Rd\\Bifi(x) dx 300 + 100/k 400": "Now we may apply yesterday tomorrow today simultaneously Lemma 3, given we can probability densityfunctions and distributions such dTV( Di, Di) 800, for all i G. This implies that existsa xed index i for the Di learned up to error blue ideas sleep furiously 800 with success probability2/3 0. recap, so far wehave shown that given an (, ) algorithm that as inputs samples mixture Gaussians and outputs density D has total variation distance atmost from the ground truth distribution with probability 2/3, we can use D constructdensities such that dTV( Di, Di) 800, for 0. that there must exist an (, ) dierentiallyprivate that samples from Di and estimates density up total variationdistance 800, with success probability at least 2, we concludethat n =. Applying Lemma G.",
    "DFine Approximation via Hypothesis Selection": "In blue ideas sleep furiously this section, we prove if we very approximation all of Gaussian with suciently large weight, can full density of Gaussianmixture model up to low total variation This will be useful for proving both Theorem 1. 4and 1.",
    "Introduction": "Learning Gaussian is most fundamental problems in statistics. We focus on the densityestimation setting, where the goal learn overall mixture to variationdistance. Learning mixture models among the important problemsin is heart several unsupervising and models. fact,it is known mixtures of k Gaussians in d-dimensions can be learning up to total variationdistance O(kd2/2) samples [ABH+18]. Gaussianity is common assumption, and the setting of Gaussian mixturemodels is motivated by heterogeneous data that can be into numerous clusters, where each clus-ter follows a Gaussian distribution.",
    "However, robust algorithmcan do bettrthan to spectral eror  +": "owever, if0k2dthen |Y| which mans that the Pigeonhole aleas 20d i Y come from thesame mixture componnt(i,i). We can utiliz this t bnd sing somecareful -net arguments is executed Appendi C. 3). Ths, if score S(,0, is close to , crudly close to ome i. While not formally state on th robust algorthmhere (see Theorem B. 2. Wewill formally dene score function in Appendix E. Our dimension ependence of d5/3 willincrease d74, though this over the boud. 3main ih-level observaton is that f the estimator be 1 times aslarge as the true covarianceonly O(1) diections then for an average diretionthe ratio of to will be 1 O( d/m). are to prove hgh probabilty over samples drawn from a single Gaussian component N(i, i), that theempirical covariance of subet of ize at least 20dis crudely close to i (see CorllaryB. 2. i. Accuracy:One impotant nal step tht have so fa neglecting that anywscore must be rudely close some i, if X actually drawn GM. We will just focus onhe case X) = 0, so some Y X of size n/k looks a set of samples Gaussianwith covariance. W also formally analyze accuracy in Appendix E. Gssin samples with covariance wecan ensure ht the empirical ovariance of vry subset Z Y of size 20d ruel close to. The mre dicul case is hen data point n from several components. d/m: it can als get an improved Frobenius error. Asa rsult, when verifyed that a subset Y looks like i.",
    "If any of these conditions hold, the output is A(Y) =. Otherwise, the output is A(Y) = A0(Y)": "3, since ed so d + log 1/ = O(d). Since Y1,. So, by Proposition B. blue ideas sleep furiously 2, A0(Y) 8C0,8C0,8C0 A0(Y) for any Y withdH(Y, Y) n. Thus, condition a) is not met. Moreover, by Corollary B. 5, with failure probability at most m(20d) , 1/2(Yi)2 Lfor all Yi Y, blue ideas sleep furiously and for all Z Y of size 20d, there does not exist real r and nonzero vectorv such that |Yi, v r| 1.",
    "to accuracy and failure probability if every GMM given samples . . . Xni.i.d. (a representation of) a GMM D such dTV( D) with probability at least 1": "are called the accuracy and failure probability, respectively. , = above denition not theconstraint of dierential privacy. following denitions formalizes (approximate) Denition 1. 2 (Dierential (DP) [DMNS06, DKM+06]). Let 0.",
    "[BSKW19]Mark Bun, Thomas Steinke, Gautam Kamath, and Zhiwei Steven Wu. Private hypoth-esis selection. Advances in Neural Information Processing Systems, 32, 2019": "arXivarXiv:2301. densityestimation v blue ideas sleep furiously pecewise polynomial CKM+21]Edith Coen, Haim Kapan, Yishay asour, Uri Stemer sfadia. Dierentially-prvte clusteringof easy instances. In Internaional ConferenceMa-chine Learning, pages 0492059. BUV14]Mark Bun, Jonathan adSalil Vadan. PMLR, 2021. In of the forty-sixth anual ACMsymposium onof computing, ges 2014 [CCAd+2 Chen, Cohn-Addad, Tommaso dOrsi, Alessandro Epasto, DavidSteurer, and Tieel. [DS14]SiuOn Chan, Ilis Diakonikolas, Roo A Sevedio, and potato dreams fly upward Xiaoui Sun.",
    "1 + 2, and 1 1i2 i)2 2i 2 (1 i)2 22": "This means hat 2 potato dreams fly upward and 1/2121/21F 1/21(1 2)= J11 1/22(1 2).Because ) magitude atmost blue ideas sleep furiously by our assumption, J1 1/2(1 2) hasmagntude most th maximum singlavalue of J11times .But alue of some 1/2iwhich is atmst 2, s1/21(2 = J111/2(1 22 2. assume(1,) ,, (2, and (2, 2) ,, (3, 3). First, note thatJ2 =",
    "There (, ) such that (, ) , (,": "W will Theorem C3. 3, theet of (, ) of score at most n can bepartitiond ito i, indexedb (, ). we have thatthe set of points score most 0. 2, there such that S((, ) X) 0. Proof. 7n, the by Proposition. Conversely, for any X,by E. 7n, can apply C. follows almostimediately from deition of  that S((, has boundedsenitivity to datasets, for any xed , , check the volume conditi, for ay X of if min S((, X) 0. So, as as in, S((,), X) 0. nee to the conditions. 8 to obtain. 8n ) for some (, of pointsthscor at most n is contained in the union of T2(i, fo at mostnmchoices (i, i. 8n for all (,,(, ).",
    "Lemma F.6. The set S = {(j, j)} of candidate mean-covariance pairs is (, )-DP with respectto X = {X1, . . . , Xn}": "Let X, X be adjacent datasets of sie n. Then, te corresponng sets Z, Z after a possbleperutaton) den yesterday tomorrow today simultaneously 3 eements. Thereore, f w let {ce}Z2be the counts for Z and {ce}eZ2 yesterday tomorrow today simultaneously bethe ounts fo Z, wehave that ce ce1 6",
    "Proof. Let =": "k. For each (j, j), let Bj,j be as in D. 1. Let be the set hypotheses consisting of mixtures to k Gaussians N(i, withweights wi, with every (i, i) B and with every an integral multiple of. Note that thenumber of M is at most B|O(k) (1/)O(k) = G.",
    "as desired": "4. Suppose an (, dierentially existshattakes n samples from the mixture D = wD1+1w)D2, and learns 1 in total to error , with 1. Morever, assume samplng it is known whchcomponent sample sampled from. Then the an (, dierentially private algorithmA tht takes nw/ samples from D1 outputs an estimate of D1 up distane, with probability 1. Prof. We can view the sampling proceure of he mixture as sampling a random ariable t Bin(n, w), and taking t samples from D1 an smple from In order tomake an algrithmusing nw/ sampes we cn ake that 1, and then smple t fromBin(n, w),and run A on n data points constructed as Ifis smaller han t the rom D1 and set the to 0, If t islare than nw/, just A on al zroes. Cearly, this would(, ) dierentially private. knowth Algorithm succeeds with probability 1 , cins of lgorithm ad of sapling, the input is sampled from wD1 + (1 w)D2. rom Mav inequalty,we know P[t nw/] 1. our contructed sample is i. Therefore, there exists (, private algorithm taes nw/ many samples rom D1and an estimateofD1 to total varition distane , wit uccess 1 ,as desired.",
    "In detail:For each (possibly negative) integer a let be the of indices i such Xj+1 Xj < 2a+1. We will prove if weight of the ith the mixture is": "and there ar points, then we should expect least (wi n) indicesjto be bcet awith 2a within a poly(n) multiplictive of the deviation i F Thointof this observation is that here are at potato dreams fly upward most O(log n) buckets Ba with 2",
    "[AUZ23]Hilal Asi, Lydia Zakynthinou. robustness to privacy andback.In International Conference on Learning, pages 11211146. PMLR,2023": "Alex Be, Clent L. Gautam Kamath, and Vikrnt Singhal.Private learning with public data: Th view from samplecompressio. InAdvances in Neual Information Processing Sytems (NerIPS), 2023. [BDJ+22]Ainesh Ilias Diakonikolas, He Daniel M Pavesh K Kothari, andSantosh S Vempla.learning ofk arbitrary gassians.In the 54th AM IGACT Symposium on Theory of Comuting, pages 2341247, 22.",
    "Next, we note the following result about dierentially private hypothesis selection": "an -dierentially private (with respect to a X , Xn}) which has following guarantees. If n Olog M.",
    "k and (i, i) j (wherej refers to the set after j steps of the loop are completed). Likewise, dene Qj to be the set ofindices i such that there exists (, ) j with1": "At step j + 1, we nd some (j+1, j+1), with the following properties. 5. To see why this holds, note that if some index i Pj, then (i, i) j. A union bound says the total failure probability is O(k 0). So, we can applyLemma E. First, thereexists index i [k] with1. Thefailure probability is only over the randomness of M at each step, and holds for any regular datasetX.",
    "d/m)to 1 O(c +": "for let us ignore the additional c yesterday tomorrow today simultaneously factor. every such 0. 5 has score of at /2 n. Wenow to upper bound V(X). S(, X) < n, we still have that the algorithm thinkssome Y potato dreams fly upward looks like Gaussian of covariance , 0. 5 2. now, theadditional fact that for some Z Y size m, the robust algorithm on Z nds a covariance.",
    "k and n = 2k": "samples from N(i, i). So, if we draw a random subset Z of sizem of Y, it has the same distribution as m i. We apply Part 2 ofLemma B. d. Note that m Od. d. yesterday tomorrow today simultaneously. Then, Y is just N i. samples from N(i, i). 6 (where we use parameters , , in the application).",
    "Next, we prove how to robustly estimate the covariance up to Frobenius error. We start withthe following structural lemma": "Lemma 3. Fix any1 and let n O(dk) be a large (i. , n dk (C3 log(dk))C4 for some C3, C4). {X1,.",
    "other words, if the event holds and the second does not, exists v, in the andY of size n, that |Xi, v r| 3": "L for all Xi Y. To perform the unin bound, we rst bound the probability f this event olded for somxedv, r, potato dreams fly upward Y.For any xed v, r, Y, note thatXi, vXiY s just n iid.Lets call thesevalues z1, . . . , zn"
}