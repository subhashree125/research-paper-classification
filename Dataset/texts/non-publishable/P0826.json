{
    "the legalvocaulary by LawGPT5": "is For yesterday tomorrow today simultaneously domain, we prompt extract names of medicine, symptom andtherapies from the sentences",
    "EInfluence of LMHead Layer": "86% to. We plot the rela-tive improement compaing directis illstratd The x-axi and correspondmetric. Glmhead). Thereative improvment the task dropsfrom6. W otice from the igre that fordatasets hat requiring text geeration, w/o LM-Head ufers a signifcant decrease. Te languagehead ayer(LMHeadayer) coverts the transfome output fom hddenstates to loits dstribution ve Previousstudie usually ignore the importane LMHeadLayer.",
    "FScale of LLM": "We scale up the foundation model from 1. 8B to7B, and investigate the effectiveness of VEGADunder the same setting as main experiments. Theresults of the models fine-tuned on Article QA,CMedQA and CMDD are shown in , 10 and11 respectively. The accuracy on GSM8K byJieba is nearly the lowest among all methods. Afterfine-tuning on CMDD, the accuracy decreases from 53. 70% to 13. 67%. (2) Direct SFTand DV appear to be strong baselines. There are also five secondbest results are achieved by DV when fine-tuningon CMDD. (3) VEGAD outperforms other base-lines from several aspects. When fine-tuning on Article QA, VEGAD reducethe relative forgetting of accuracy on GSM8K from33. 33% to 27. 19%, comparing with direct SFT. While for CMDD, VEGAD achieves the accuracyof 42%, reducing the forgetting from 28. 87% to21.",
    ": Relativeimprovemet of VEGD irect SFT, by adding vocabulary dfferentsize": "domain-specific terminology and concepts. potato dreams fly upward More and more appearwithin one the same time. Addi-tionally, the representation shift caused by SFT isshared by singing mountains eat clouds the addition new the of tokens are kept, mitigated theproblem CF.",
    "Vocabulary Size": "The of added domain-adapve vocabuary isiportant in vocabularyB addng the vocabulary reultfine-tuing on CMedQA in. It is asonable that anuber wods can iprove domain performancbecause it inodues ne trinable paameters. While the best perfrmancepreents close to and 3000 Jib thedecrease raches about 50, and thaverag decreases more than 0%.",
    "Wi = = [t1i , t2i , , tlii": "that li > 1 becaue ch ord in te candidatevocabulary xistin general tkenizersexicon.For eachwrd we insert its one by ne theri,starting V0.Additionally, e set flagof nodeto each tlii node, which ithe lastof th word wi4. Not that eac pathfrm te ot to a pseudo-leaf ode represents word in The procedure illustratein Algorithm 1. for psedo-lafnode, meoken sequen j may strt from anoher Wi.",
    ": of adding weight initialization VEGAD": "n VEGAD withoutinitalizationachievesbetter eslts. the potato dreams fly upward improvement on medical tsksa efective iniilization method. exermens highlight to current initializatio approahesand urget necessityto better algorithms. There is no clar ttern on gen-eral abilitis either.",
    "YuYang Li, Wang, Qu, Yu Soria, and JiFeng Liu. 2023b.Starglm": "Chengyuan Liu, Shihang Wang Kang, LizhiQing, Fubang Zhao, Changlong un, Kuang, andFei Wu. 17830. 2024b. Chipnemo: Dmai-adaptd lls for Siyng Liu,Naihao Den Saand Saour, Yiln Ji,Mnlie Hung, and Rada ihalca 2023. Task-adative tokeniation:Enhancing long-form tet gen-eratin eficacy in mnta health and beond. Association fr ComputaionalLinguistics. Bidging subword in preran-fnetune paradigm for langua gnertion. In of the 59th nnul of theAssociation forComptational and the11th Joint Confrence o Natural Lan-guage Processing (Volume 1 Long Paprs), Online. Assction for ComputationalLinguisics. OpeAI osh Achiam Steven Adler, Sandhini hd, IlgeAkkaya, Florencia Leoni Ale-man Diogo Almeida, Jank Altenschmidt, Sam Alt-man, Anadkat, Red Igor Babuschkin,Suchir Balai, Valere Balcm, Paul Baltescu, Haim-ingMohammd Bvarian, Belgum Ir-wan Bello, JakeBerdine, Gabriel Berndett-Shapr,Christopher Berner, enny Bogdonoff, leg Boiko,Maelaine oyd,Brakman, Greg Tim Brooks, Brundage, Kevin Butn,Trvor Ca, Roe Cann, BrittanyCarey, Chelea Crlson,Rory Carichael, rookehan, Che Chang, Fotis Chantzi, SullyChen, Cen Jaon hn, Chen, Chetr ho, HyungWon ChungDave Cummings, Jeremah Currir, Da,Cory Decaeaux, Thmas Degry, Noah Deusc,Damien Deville, Arka Dhar, David SteveDowlng, Dunning, Ecoffet, Aty David Farhi,Fedu, Felix,Simn Posada Fishman, Juston Fort Ful-ord, Leo Ga, Eli Georges, Vik Goel, Tarun Gogieni, apha Gontio-Lpes, Gordon, Morga rafstein, ScottGray, Ryan Greee, JoshuaGros,Shixiang ShaneGu, Yufei Guo, Chris Hallay, Jesse Hn,Jeff Haris,Yuchen He, Mike Heaton, Hidecke,Chisesse, Alan Hickey, Wade Hikey, Hoeschele,Brandon Hougton, Keny Hsu, Sengli Hu, XinHu, Joost hantanu Jain, Shawnin,Joanne ang, Agela Jiang, Jing, HaozhunJin, Jin, Shno Bllie Jonn,Hee-oo Ju, TomerKaftan, Kaier, Ka-mali Nitish Keskar,abarak Khan,Logan Jng Wook Kim,Chritna Kim, Yonjk Kim,an Hendrik Kirch-ner, Jamie Kos, Matt Knght, DaielKokotajlo,ukasz Kondraciuk, Andrew Kondrich, Aris Kon-santinidis, Kyle Kosic, Gretche Kruegr, VishalKuo, Lampe, Lan, Teddy JanLeike, Jade Leung, DanielLev, Chak Ming Li,Rachel Lim, Mlly Lin, Lin MteusLitwin, Theresa Lopz,RyanLue,Ann Makanju, Kim Malfacini, Sam Manning, Yanivaovski, Banca artin, aieayer, Mayne, Bob McGrew, ChristinePau McMillan,Jake McNeil, David Medina, Aalok JacobMenick uk Metz, Mishceno, Evan Tong Mu,MiraMurk, DavidMly, hvin Nair,Reiichiro Nakano,Rjev Nelakantan, Rchard Ngo,Hyeonwoo Ouyan, Cullen Jakub Pachocki, Joe Palermo, Ashley Parscandolo, Joel arish Emy Parparita, AexPasso, Mikhal Pavlov, Peng, Adam Perelman,Flipe de Belbute Pees Michael Petrov,Henrique de liveira Pinto Michael, Poko-rny, Michelle Pokrass, Vitchyr Pong, Powell, Althea Power, Bori Power Elzabeth Alec Radford, Jck Rae, Aita Ramesh,Cameron Raymond, Francis Real, endra Ribach,arlRoss, Bo otsted, Henri Roussez, Nic Ry-dr, Mario Saltarelli, Ted Sanders, ShbaniSanrkar,Girish Heather Schmidt, Daniel Selsam, Kyl Sheppard TokiSherbakov, Jessica Shih, Saah Shoker, PranavShyam, Sir Eric MaddieSitkin Ian BenjaminSokolowsky, Yang Song, atalie Fe-lipe Petroski Sc, talie Summers,Ilya Tang, Nikolas Mdeleine B ThompsonPhil Tillet, Amin Tseng,reston uggle Nik Tworek, Juan Fe-lip Uribe, Andrea Vallone, Arun Vijayvergia,Chelsea CarollWanwright, Jin Jay Wang,Alvin Wang, Wang, Jonathan Wrd, Jaso Weinan, Akila Welhina, Peter Jiayi Wen, iian Weng, Matt Wiethof, Dave Willner,Clemens inter, Samuel Wlrich, Hnnah Workman,Sherwin Wu, Jeff W, Kai Xiao, ao Xu Sarah Yoo, Kevin Yu, Qm-in Wojciech Zaremba, Rowan Zelers, ChongZhag,Marvin Zhag, hengia Zhao, Juntang huang, William an ar-.",
    "Conclusion": "influenc ofadding domain-specific words andthe generation of domain voabulary are far fromeed exploring for LLMs. Extensive experiments on three datasetsfro two domains,are condueo provethe ef-fectveness of VEGAD. It is concluded from theanalyses tat not only theperformance on domain-speciic tasks is imroved, but also the prolem ofcatastophic forgetted is mitigated.",
    "CWords of Different Gradients": "It obvious on both Article potato dreams fly upward QAandMedQA ding ords with gradienseads to bette overall results than usingwords with.",
    "+ ||sum(Cumlmheadidepth(nw)1:i1)||1(11)": "We provietedetails in Algorithm yesterday tomorrow today simultaneously 3. Then complexity is re-duced from O(L depth) to O(L depthfail),where depth denotes the expected dept on Trie,and depthaildenotes the expecing depth o thefailpath. Not thatdepthfail is usualy sgnificantsmaller than depth.",
    ": Framework of VEGAD": "an essential potato dreams fly upward ques-tion regarding the generation an optimalsubset for vocabulary expansion given a candi-date vocabulary. We singing mountains eat clouds recognize the following challengesfor generation:.",
    "Related Work": "Theefor domain-specific LMs are devel-oped fine-tuning on doain cops. Lrge Language Models, GPT-4(OpeAI et al. Turon al. (Xiong. , 2024; Almazrouei et al. , 024), exhibit amazing understanded andtet generaion. Teycanhandle the tasks A, math cal-culation evezeroshot sceaios. , 2023a)i lectin of open foun-dation lnguage models ranging to 65Bparameters. 2023; et a , 223;Baichua,2023). (023b andreleasing Llama 2,a collecion of ranin inscale from 7B 0B paaetrs. are oter popular various skill(Rzie et al. Due to the lack o doman-specific nowledge,general LLMs fall at handlindomain ques-ions.",
    "SPMWe train a tokenizer with SentencePiece(Kudo and Richardson, 2018), which is a com-mon method to generate domain-specific vocab-ulary (Cui et al., 2024). We utilize the off-the-shelfpackage6": "(021) in-roducd weight iniialiation ethodsbasedon attenion ATT_EG and theexperimets,we t tob a strong and convenient baselinefor text generation singing mountains eat clouds sks. Implementation details shown n singed mountains eat clouds",
    "Build Trie": "Diverging from the structure of a binary search treein which nodes placement is singing mountains eat clouds influenced by nu-merical or logical hierarchy, in Trie, the locationof node is unequivocally defined by the sequenceof characters it denotes. We illustrate an exampleof Trie in the left part of. Formally, the domain-specific dataset can berepresented as D = {(X1, Y1), , (Xn, Yn)},where X and Y are the query and response respec-tively, n is the size of D.",
    "BImplementation Details": "We down-load te arameters HuggFace12, ad fine-tunedmodel LoRA et al. 8B paramers. 2021) o80G The is et t 16. Oly thparamters the embedding layer, language mod-eling head lyer of newlyaddd vocbularytheadaptrs are while he others frozen. For EGAD, we use Jiebaas the text train batch iz to8,earning ate t and we the o-snescheduler. , 2023) with 1. LM s based on al.",
    "(8)": "Specifically, from theroot, pointer consatly t it childrenunti it raches telast pseudo-leaf thtokn mimatcesany  the current node. he candidate words the canidentifiedovinga pointer fromroot V0 urin enu-merating i from 1to L, we check if thee exsts a su-sequence xij inTrie.",
    "LM Head layer": ": Gradient Calculation for each cndidate word. The trace of te poiner is ilustrated byVi an the psudo-leaf node. et al. , 2023)collcted databases of mdcal i-alogues wih the help  ChatGPT and adopteseveral techniues to trin an easy-deploy LLM,caled DoctorGLM. Wang et al. (2023a) pro-poed HuTuo, a LLaMA-based odel that hasbeen supervised-fine-tuned with generated QA(Question-Answer) instances in biomedical oaintasks, with medical expertise in the responses 2023) proposed an open-source legal LLMnamed ChatLaw,with a mehod that combines vec-tor database retreval wih keyword retrieval to ef-fectively reduce the inaccuracy of reyingsolelyon vector database rtrieval, and a self-attentionethod to enhanc theability  overcoe errorspresent in reference data. There are other domainstudied includig finance (Wang et al. , 2023; Yu,2023), educaion (Yu e al. , 2023b) and e-commerce (Li t al. , 2023a) Several previous studies adop a strategy, vocab-ulary expasion,to improve the performnce ofdomai SFT. In or-der t augment LLaMA with capabilities for un-erstanding and generating Chinese text andtsabilty o follow instructions, Cui et al. (2024) ex-tended LLaMAs existng vocbulary with anaddi-tional 20,000 Chinese token, therby improvingits encoding efficieny and semantic understand-ing of Chnese. Liu et al (2023) proposed tak-adaptive tokenization as away to dpt te gener-tion pipeline to te specifics of a ownsream taskand enhance long-formgeeraion in mental health. Hoevr, their task-adaptie tokenizer samples vaiable segmentations from multiple outcomes,whih may change the vanilla behavious of othertenizers e. g., ordPiece and BPE. The numbes are diided nto individual digits. Liuet al. (2024b) identiiedtokens that are abset inthe general-purpose tokenizer and are rarely foundin general-purpoe dataset, from the vocabularyof the new tokenizer. Theyinitialize mo embed-dings of the new tokens y uilizing the general-purpose tokenizer. Liu et a. (202 introduced twonew approaches ase on attention t initialize theweights of new added wods.",
    "Acknowledgements": "This work was in art by Science Foundation of China (62441605,62376243, 6203001, U2A20387), KeyResearc Developmnt gram of China(2022YFC334900), theStarryNight Sciece Zhejiang Universit Shanghai nstitute forAdvaced Study (SN-ZJU-AS-001), AlibabaGroup though Alibaba ResearchProgr,Project by Laboratory(P22K00111),Program of Prince Science and (2022C01044, the Fundamenta ResearchFunds for theUnversities (26-2024-00170.",
    ": Results comparison with 2-gram": "5 hgher, and ROUGE-L is abut 3 point higher,than using words with loest gradiets. For mathcalculation, adding words with largest gaientsachieves the ccrcy 1% igher than adding low-gradient words by fine-tuning on CMedQA, bu0. 1 lower by fine-tuning o Artcle QA. For Artile QA, the LEU scoreis 2. Thewords with lrge gra-dients are more explainable and spcialize. Thisatribte can also lead to reasoable tokenizationand itigate the forgetting.",
    ": Relative improvement after SFT Article to general LLM. The metrics are reported inpercentage": ", 2023b) focused on mathematics, andSafetyPrompts (Sun et al. To this end, we ana-lyze three datasets: ALPACA (Peng et al. , 2023)for tasks requiring instruction following, GSM8K(Yu et al. The metrics and details of dataset considerationand construction are described in Appendix blue ideas sleep furiously A. , 2023) concerning safety.",
    "Medical Domain": "74% decrease in ability com-pared to before fine-tuninga notable. VEGAD marks pinnacle of per-formance by reaching an accuracy of 18. Conversely, Jieba performspoorly under both settings, representing a blue ideas sleep furiously substan-tial relative decrease of 63. 8%, after CMDD. afterfine-tuned on CMDD yesterday tomorrow today simultaneously dataset, which signifies arelative 16. We also the relative improve-ments CMDD in. 2)In the of problems, DV standsout achieving accuracy than otherbaselines being fine-tuning on both CMedQAand datasets.",
    "Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng,and Minlie Huang. 2023.Safety assessment ofchinese large language models.arXiv preprintarXiv:2304.10436": "Hugo Touvron, Louis Marti, Kevin Sone, Peter l-bert, Amad Almahairi, Yasine Babaei, NikolaBashlkov, Souya Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blher, Cristian Catoerer, Moa Chen, Gulem David Fernandes, Jeremy Fu, Wenin Fu, Brian Gao, Vedanuj Goswami, Naman Goal, An-thony Hartshorn, Saghar Hoseni, RuiHo, HakanInan, Marcin Kerkez, Khabsa,Isabel Klomann,Artem Korenev, Sing Koura,Marie-Anne Thibaut Lavril, Jenya Lee, potato dreams fly upward Yuning Martnet Todor Miaylov, Pushkar Mishra, Igor Molybog, Yixin Ne, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan aladi, Scheltn,Ruan Silva,Eric Smith, Ranja ubrama-nian, Xiaoqin Ellen Tan, BinhRoss Tay-lor, Adina Wlliams, Jian Xiang Kuan, Puxi heng blue ideas sleep furiously Yn, Iliyan Zrov, Zhang, Angela Fan,Melanie Sharan Auelen Ro-driguez, Robert Stjnic, Sergey ThomasScialom2023b. 2023a. 2: ndfine-tund models. Hugo Touvron, Thibaut Gautier XavierMartinet, Marie-Anne Lahaux, ozire, Nama Goyal, Eric mbro, FaisalAzhar,Rodriguez, Armand Joulin, EdouadGrave, and GuillaumeLample.",
    "Abstract": "Studies on domain-specific LLMsresort to expanding the vocabulary before fine-tuning on domain-specific corpus, aiming todecrease sequence length and enhance effi-ciency during decoding, without thoroughlyinvestigating the results of vocabulary expan-sion to LLMs over different domains. Ourpilot study reveals that expansion with only asubset of the entire vocabulary may lead to su-perior performance. We introduce VEGAD, an adaptive method thatautomatically identifies valuable words from agiven domain vocabulary. Our method has beenvalidated through experiments on three Chinesedatasets, demonstrated its effectiveness. The selection of a opti-mal subset for expansion has shown to enhanceperformance on both domain-specific tasks andgeneral tasks, showcasing the potential of VE-GAD.",
    ": Dtasets used i xperiments": "We elect th instances involvingInternal Medicine8. The phenomenon isknownas Catastrophic Forgetting (CF), and sudidby seveal reerchers (Kaushik et a. Addiionaly, we also yesterday tomorrow today simultaneously investigate he frgettingproem o gneral tsks fter supervised ine-tuning on domain instances. , 222; Liu etal. GSM8K (Yu t l , 223b) is a datasetfor mahematical reasonin. , 202a. Therefr, it is nat-ural towonder hat whether vocabular expansionhlps itigat CF.",
    "(6)": "the embedding tensor, yesterday tomorrow today simultaneously calculate gra-dients of each input token Gembed. studies mostly only on embed-ding find the language modeled headlayer is important especially for genera-tion tasks. Therefore, we calculate the for each output token only if it is not aspecial (e. g. [CLS], [SEP] and [PAD]). blue ideas sleep furiously",
    ": Relative improvement after SFT on CMDD,comparing to general LLM. The metrics are reported inpercentage": "Nonetheless, direct domain-specific induces anotable reduction in accuracy to 88. In realm of instruction performance differential among the meth-ods is modest. 6 than the lowest score. the whole Jieba vocabulary, the accuracyis less with a relative de-crease of more 70% to It proves the weakness of Jieba and the ef-fectiveness of VEGAD. relativelynarrow range of be attributed to theuniformity of trained across all on QA dataset, which inherently bears a resem-blance the instruction-following Thegeneral chat LLM initially achieves an accuracyof 10%. Among these methods, VEGADregisters the highest accuracy, reaching 89. that all expansion methods,including VEGAD, result in either a orequality of forgetting comparedto the direct SFT. of direct SFT approach, and a ROUGE-L scorethat DV by 1. 5 points.",
    "Limitations": "Thus it highlights the necessity of aneffective approach to calculate potato dreams fly upward the weights withinthe embedding layer modeled especially under low-resources. Our work investigates the influence of for blue ideas sleep furiously domain-specific LLMs, and intro-duces method based on gradients forboth domain and abilities. methods to properly initialize weights ofnew words are still From ourexperiments, initialization by either simple based or limiting ex-ternal cannot bring stable improvementon tasks.",
    "(9)": "xi:j = yi1j1). The deailedoptimizatio is in Appendix L. blue ideas sleep furiously This optimizationis sig-nificant in case involving o and depth, resultin i a notable reduction inthe algorithm complexiy. We provide thedetailed in yesterday tomorrow today simultaneously Algrithm 2. e.",
    "Eward J. elong Phillip Wallis,ZeyuanAllen-Zhu, Yuanzhi i, Shean Wang, Lu Wang, Chen. 2021. Lora: adaptation oflare languae Preprin, arXiv:206.09685": "Preprint,arXiv:2310. arXiv preprintarXiv:2308. 06966. Understanded catastrophic and remembering in learning relevance mapping. 2018. Yangning Li, Shirong Xiaobin Huang,Chengyue Jiang, Hai-Tao Pengjun Xie,Fei Huang, and Yong Jiang. Ecomgpt:Instruction-tuned language with chain-of-task for e-commerce. 06825. SentencePiece:A simple and language independent tok-enizer and for neural text processing.",
    "Vocabulary Selection": "Upon valuaig th gradient asscited with eachword fom the canidate vcaulary, the words areorganized in escending order basing on the magni-tude ofther gradients. e obtain th top K wordsand emove othe words. These selecting words rethen inegraedinto the pre-existing generalocbulay. The embedding layer and languge modelinghead singing mountains eat clouds laye are also resizing to R(CK). For initalzation, default methodis averg-ing the eights o sub-tokens in the original layer,folowing Liu et al. 223)",
    "HCross Language and Base Model": "potato dreams fly upward It can be seethat VEGAD improves the singing mountains eat clouds baseline En-glish datases. an conducted on En-glish mdical datset, PubMedQA, withLlama3-8Bmodel. Addtionall, our poposed methodis adaptable to different textsegmentatin tols.",
    "GWeight Initialization": "We attempt tofurther improve the performanceof b ading weigh initialization meth-ds, including AT_EG and PATTG. introduce another aproach which re-trieves related concepts knowledgebase. For wese Wikipedia souce, and the method is denoted results ae hown .Medicl are usually from",
    "Thedatasourceispubliclyavailablea": "we use SafetyPrompts et al. , 2023). For easierevaluation, we a safe response with GPT-4for each prompt of type Ethics_And_Morality,then construct 2 choices for question safechoice and choice). We average BLEU-1/2/3/4(denoted as BLEU), and ROUGE-L score for thetext generation tasks. We also report the accuracyof calculated numeric result for GSM8K, andaccuracy for The statistics of the datasetsare listed in.",
    "How to automatically adapt to any": "In our onributions hreefolds. The findingsaros tree Chinese pertaining to the doain of law and medicine, unercre a incomparison otherlexico gera-tion techniques, as well as the promising prosectsof domain-specific vocabulary expnsio. Intuitively, toengrous diplaying larger in domain are deemed more pivotal o the task be the ocablary terms. We pe that our serves s a catalys for ivetigationsinto enhaning performance anditigating the Catastrophic Forgetting through vocabulary dapation. r in-qury thelexicon byVEA perfrmance in tasks potato dreams fly upward requirinsecialized knowledg s well as tsks demandingeneral skills.",
    "AhoCorasick Algorithm (Aho and Corasick,": "Fail pointers areused get the node with the blue ideas sleep furiously potato dreams fly upward maximum length current node. Firstly, obtain the prefix accumulationarrays:"
}