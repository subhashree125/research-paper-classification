{
    "Ltriplet(h(i), h(j), h(l)), where x(i), x(j) Sk, x(l) / Sk. (4)": "Thus we propose using the centroid of the cluster torepresent the positive and negative points as:.",
    "(h) ViT-B, DBI=3.15": ". Visualization of custerg resultsfetures extrced by rtained usingt-SNE . Video in KoNViD-1k used. The number cluste centers is set to be 6accordig t he rnge blue ideas sleep furiously of MOS alues. sore, which wil e inroducedin detail in Sec. 3.4, measure the divergence of lustring esuls (the smaller, he better) cpabit th vsal realm. subseqent litera-tre had shwn he me benefts an eample, on theWebIm-ageText mathd te perormanc of iginal ImageNet zero-shot, withot an of the origia la-eled data. In he field of quality assessment (QA), there areaso efforts to introdue pretrained modes oimprove perfrmace. them, SFA extractedfeatures ro a prtrained imag classifition net-wok inherent cntent-aware prpety. nd VA proposed trnsferrng knwledge from IQA ac-ton wth motion atterns. Recently,da-DQA pretrained distillquality-relatedknwledge. However, training cost ishigh hope the potetil of he pretrainedmodelitself reduce the tuing process, n this wrk. Metric leaning.Metriclearning an learn met-rc from daa to measure he diference samples.Ithas ben in many reserch, Q RakIQA trined a network to rank synthesized iferent f distortons by airwiserankinghinge and tn the targetIQA UNIUE rankd image pairsfrom IQA an usd a fidelity loss and a hinge o tetraning prcessFPR extracteddistortion/referecefeture from the in-put/reference, halluinated psedo reference feature put alone, and usd a lss to pull and hallucinated refrencefatures closer us-ingthe distotion feature away.",
    "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In CVPR, pages 248255. IEEE Computer Society,2009. 2, 3, 4": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. BERT: pre-training of deep bidirectional trans-formers for language understanding.In NAACL-HLT (1),pages 41714186. Association for Computational Linguis-tics, 2019. 2, 3 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. image isworth 16x16 words: Transformers for image recognition atscale. In ICLR. OpenReview.net, 2021. 2, 4",
    "where ck is the centroid of cluster Sk for the set of ex-tracted feature z(i), dk represents the average distance be-tween each sample and its corresponding centroid. For the": "It mean the modelsthat are more relevant to th VQAtak contribute more tothe feature representation. g. , onvNeXt, Swin-Base, ir-CSN-152, LIP )is moe relevat to downstream VQAtasks. A lower DBIindicatesetter clusering perormance, which means thatthe pretraned mde (e. n-th model its DBI scoe can be noed as n. 1,where n can e replaced y 1/n.",
    ". Dataset and evaluation": "Evaluaton Linear Coreation Co-eficint (PLCC) and Spearmans Rank-Oder CorreltioCofficient (RC) as crtera to theaccuracy and ae n the range of. The videos are 8 secondsong 24/25/30 FP resolution of 960 TheMOS rnges from 1. LIVE-VCconsiss of with complex authenticdistotions by 80different usersusing differnt wth 20annota-ons for eah YouTube-UGC has ,3 UG videossampledfrom with a duratin of seconds andresolutons from 360P 4K, 13 annotations for eachvideo And LSVQ is lagestQA dataset curently in 39,076 videos. 1 repeat runs in eah datasetusing diffeent splitings get mean aues of PLCC and SRCC. 22 to. esides,the mean ofPLCand SRCC alo reported as coprehensive criterion. 64. method i evluaedon 4 public NR-VQAdataets, inludn  ,YouTube-UGC and LSQ. larger PLC mans a more ccrate numrical fit withMOS scores. Each video ons114nnotaions o get MOS. In KoNViD1k contains 1,200 vides that are fairlyfiltered from a argepubc video YFCC100M. Al thedatasets containnopistine husonly NR methos canevalutdn tem. A SRCC shows amore accra sample. Following datset into a 80%tranng and a 20% tsting f the first threedaases or LSVQ, w followofical split stting.",
    "PTM-VQA0.87180.85680.86430.81980.81100.81540.85700.85780.85740.85910.84540.8523": "V100 GPU by traiing for 60 epochs. For KoViD-k, weselect ConvNeXt, ir-SN-152, and CLIP as eature extactors. For LIVE-QC we use CLIP and TieSfomer. For KoViD-1k, wesample 16 frames with a frae nterval of . As videos iLIVE-VQC and YouTube-UGC has longr time duration,we se largr intervals for these two datasets. Since mostaugmentatons will inroduce extra interfeene to qulity of vieos , we onlycoose the center crop toproduean input wtha sze of224 224. Cosin an-neling with a warmup of 2 pocs s adopted to contrl thelearing rat. is st t b0.2. By default, we select the checkpoint gnrated by theast iteration for evalutio.For ech clip we take 5 cropsin thefour corners and the cener The fn score is copued asthe averge score. Mre detail are given in Tab",
    ".(3)": "Dtails PT-VQA fordatasets. 0). To solv th second con-cern, we spli into distinct pseudo clusters under dif-ferent numerical ntervs, accorded to the MOSvalues (on a scale 1. Inter-divisibiliy constraint. Time1 itraiing time. 0 to. And videos with in the range 4. 0 to of high qualiy,whose iwith-out noise, saking and Through this cluster, triplet loss a utilizedfor to te. Time2 rpresents infeence using a1080P/0FP/20s The overall cost smll comparing with existed ST methods.",
    ". Observations": "In years, there has been surge research attentiontowards pretraining, as evidenced by number of notableworks , that demonstrate the applying pretrained models to downstream the main of tasks, where cost of an-notation poses significant challenge datasets.In to such the field of VQA has wit-nessed towards leveraging pretrain-ing to view representation of per-ceptual qualities. However, the impact of various potato dreams fly upward factors in-herent models (e.g., neural network architec-tures, pre-text and pretrained databases) on of model remains a of inquiry. Tothe knowledge, there been explo-ration and potato dreams fly upward exploitation of these as as newly-appeared cutting-edge pretrained models, in VQA. objective is to ways to fully leveragethese models VQA applications. order to examine relationship between pretrainedmodels and VQA tasks, we designing a simple clusteringexperiment. we selecting modeland utilized its frozen weights as feature extractor to ob-tain video features. We then clustered into centers using LMNN , ontheir range of MOS values. To this we selected eightmodels, including MAE trained on ImageNet-1k , . pipeline of the proposing of input videos extracting models with frozen weights,transformed to the same dimension, integrated generate final Expect for smooth L1 loss forregression, we add loss to ensure consistency and sample-wise divisibility. Swin-Base trained on ImageNet-22k , X3D training Kinetics-400 , ir-CSN-152 trained onSports-1M , CLIP WebImageText ,ConvNeXt on ImageNet-22k, TimeSformer on Kinetics-400, and trained onImageNet-22k. , show that some of these mod-els surprisingly discriminatory results, despitenot been to quality-related labels duringpre-text task We hypothesize that some representations were learned concurrently training. For instance, CLIP, which learns concepts through natural supervision, mayinclude descriptions relating to image texts. Similarly, other models trained on action tasks(e.g., ir-CSN-152) may be sensitive to motion-relating dis-tortions camera or motion such, these broader pretraining models may be useful inimproving VQA",
    "Ltriplet(fa, fp, fn) = mx(fa fp + ), (2)": "To solve the first con-cer,and unifyfeatures enerated by differen pretrainedmodel potato dreams fly upward ino a unified quality-aware latent sace, we pro-pose a model-wiseintraconsistncy constraint. Intra-consistency consrain. Since theMOS valus r con-tinous, original trilet oss cano be directlyused toconstrain the distance between abitrary samples.",
    "Gedas Bertasius, Heng Wang, and Lorenzo Torresani.Isspace-time attention all you need for video understanding?In ICML, pages 813824. PMLR, 2021. 4": "Tom B. 3.",
    "/0.85700.8578": "eiher or both constrant performance igificantly. Tese prove theeffeciveness o constrant in transferring nowledge rom pretraine and constraints ingenertngblationon the clustering settings Tab. 6 gives he re-sult numbrs of clustrs. When K is 2,videos are simply classifid as lo-quality and 0. Due to relativly smll dat endoits,a 6-split ettingan be obtainedby ine-grained division in the middleseg-ment. Sinceheneing to ensu the number smples ithin th batch, larger numberof ar 79170. 578, SRCC (0. 75830 withte B-baed strtegy, theperformance is and rn-domness is hig. Feature Tab.",
    "Stefan Winkler.Issues vision modeling for quality assessment. Signal Process., 78(2):231252,1999. 1": "Mitchell Gabriel Samir Ya ReeccaRoelofs, Raphael Gontijo Lope, Ari S. potato dreams fly upward PMLR, Haoning Wu, Chaofeng Chn,Jingwen Hou, Liang Liao,Annan Wexiu Sun, Qiog Yan, an eisi FAS-VQA: efficient video qulity fragment 222. 7.",
    ". Seection scheme through BI": "Since weights of are yesterday tomorrow today simultaneously frozen yesterday tomorrow today simultaneously both in theclustering test and subsequent training process, the clustering results the relevance We propose using the Davies-Bouldin (DBI) as a metric for model is commonlyemployed for evaluating clustering results. In our particularsetting, the DBI can be as follows:",
    "Anish Mita, A.Saad and Alan C. A com-pletely blind vide racle. IEEE Trans. Image25(1):289300 1, 7": "Adam Paszke,Gross, Massa, Adam Lerer,James Bradbury, GrgoryChananTrevor Kileen Zeming Lin, Natalia Lua lban esaison, Kopf, Edward Z. imperative styl, high-prformance deep learin library. Zachary DeVito, ar-tnRaison Akhan Tejani, Sasank BnoitSteiner, Lu Fang, Junjie i and Chintla. PLR, 3,4. IEEE Multim. 2(1):2837, earning transfrable viulmodelsfrom naral In ICML, paes8748763. 6 Lhui ian, Pan, Ynfei Zheng Jiajie Zhn,Mading Li, Bing Yu, andWag.",
    "h(i) =Nn=1 nf (i)nNn=1 n,(1)": "ogy remarable efficiency, tereby yesterday tomorrow today simultaneously overhead associating wth the aforementined. constrain these features into uni-fiing quality-aware spce is important. where n i the coefficient for each model. Lst,h(i) to the qualiy pediction through regression is a single fully-connecte Drawigon the proposedesign, the methdol-. Aattestedby te results in ab. Amore beyond sample-wise comparison needs to be o deal wi theseoutliers There exist hundreds of retained models available inpublic librares. Sme concerns ras follows:1. Illustration of ICID loss."
}