{
    "Lan-Zhe Guo, Yi-Ge Zhang, Zhi-Fan Wu, Jie-Jing Shao, and Yu-Feng Li. Robust Semi-Supervised Learning when Not All Classes have Labels. In Advances in Neural InformationProcessing Systems, May 2022": "Advances in Neural InformationProcessed (NeurIPS), 33:99729982, 2020. In Proceedings of 37th InternationalConference on Machine Learning, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Tao Junyu Gao, Yuan Yuan, and Qi Unsupervised aggregation template semi-supervised learning. Guo, Zhen-Yu Yuan Jiang, Yu-Feng Li, and Zhou. In Proceedings of Conference onComputer Vision (CVPR), pages 84018409, 2019.",
    ": Acuray as functon of class number error on CIFAR-100 dataset": "Different novel class ratio. Previously, we assessed models performance with a constant novelclass ratio of 50%, which is also variable in real-world scenarios. In addition to the results on CIFAR and ImageNet datasets,we also evaluate OwMatch and OwMatch+ on Tiny-ImageNet with 50% novel classes in. It is important to note that as the number of novel classes increases, the total amountof labeled data decreases. The models performancegenerally exhibits a declined trend across all benchmarks and evaluation metrics as the novel classratio increases. We found that both OwMatch and OwMatch+ surpass previous state-of-the-art methods acrossbenchmarks and evaluation metrics.",
    "Cuturi. Sinkhorn Lightspeed computation of optimal transport. Advances inNeural Information Processing Systems (NeurIPS), 26, 2013": "In of the on Compute Vion (CVPR, pages 924929, 202. InProcedngs of the Conerence Coputer Visionand Recognition pags 248255. Enico Fini, nver Sngineto, Stphane Lathuiiere, hun blue ideas sleep furiously Moi Nabi, andElisa Ricci. A uified potato dreams fly upward objctivenovel class discovery. Ieee, 2009.",
    "Mamshad Nayeem Rizve, Navid Kardan, and Mubarak Shah. Towards realistic semi-supervisedlearning. In European Conference on Computer Vision (ECCV), pages 437455. Springer, 2022": "In Proeedings of the IEEE/CVF Conference onComputer Visio and Pattern ecognition (CVP), pages 13480139 200. ixMatch: simplfying semi-supervised learning with consistency nd confidence. Advancs in Neural Information ProcessngSystems, 34:2595625967, 2021 Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Ha Zhang, Colin A af-fl, Ekin Dogus Cubuk, Alexe Kurakin, and ChuLiang Li. Advances in NeuralInformation Process-ing yesterday tomorrow today simultaneously Systems (NeurIPS) 33:596608 2020. Xin Sun, Zhenning Yag, Chi Zhng,Keck-Voon Lin, and Guoha Peng. uniaki Saito, Donghyun Km, and Kate Saenko. Conditional Gaussiandistribution learnng for open set recognition. Openmatch Open-set smi-superviselearning with oe-set consistency rgularization.",
    "(Npi(1pi))1/2": "Note that Ki=1 EP(Ni) = Ki=1 Npi the discrepancies across the K cannotsimultaneously be positive blue ideas sleep furiously or all negative. blue ideas sleep furiously To measure the discrepancies of all classes, wesum squares of the discrepancies of each class.",
    "Yuki M. Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneousclustering and representation learning. International Conference on Learning Representations(ICLR),": "Semi-supevised earning of isual fature view assignmentsi spport samples. Mixmatch: A holistic approach to learning. ReMixMatch: semi-superised withdstribution alignmetanchorig. Alex uakin Han Zhag,a Coli Raffl. David Berthelt, Nicholas Carlini, Ian Goodfellow, icola Papernot,Avital Oliver, and olin Aafel. Nicholas Carlini, EkinD.",
    "%96.597196.880163.971.968.84.455060%96.592.193.980.360.46816.541.151.770%97.51.093.081.358.665.371.334745.680%98492.994.078.958.061.869.532.94.09%9.893.694.082.050.753.569.426.430.3": "Efficient labeling strategy under a budget. In the previously conducted experiments, weassessed performance by altering the novel class ratio and label ratio, This comparison aimsto shed light on the balance between labeled in seen classes proportion of novelclasses influences model performance under fixing level of supervision. When supervisory information is held configuration with a smaller portion of labeleddata spread across number of different classes results in higher accuracy for all classesand novel classes. Additionally, it is that as number novel classes decreases, theaccuracy of the seen improves. improvement is attributed to the reduced complexity classification task there are categories to be classified. From observations, be that limited labeled budget, it is more effective label a broader categoryof samples, thereby capturing as many points as possible within feature space. Thisstrategy appears to optimize the performance across both known novel classes. Hierarchical thresholding scheme with scarce information. Previously, we observedthat in scenario the ratio on seen 50%, performance difference the thresholding strategy and setting a high threshold isnot significant. maintain the novel class as 50% decreased the label ratio to10%. and demonstrate that a hierarchical scheme not only retainsmore pseudo-labels but also preserves predictive of",
    "Technical Appendices": "Modification details for utilizedbaselines are provided in Appendix C. Roadmap of technical appendices. These appendices are structured as follows: Appendix Aintroduces dataset details utilized in our experiments. Appendix F discussesthe social impacts of our work, yesterday tomorrow today simultaneously and the limitations are considered in Appendix G. Additional experiment results consisting of supplementarymain results, in-depth analysis, and ablation study of hyperparameters are reporting in Appendix D. Appendix B outlines the implementationspecifics, included data augmentations and hyperparameters. Complete and rigorous proof of blue ideas sleep furiously theoretical results is represented in Appendix E.",
    ": Open-world hierarchical thresholding scheme generally selects more instances as pseudo-labels (a), and the quality of pseudo-labels is also enhanced (b)": "the performance. Here we combination of and random resized crop as augmentation and a multi-crop technique to produce low-resolution images (localviews). We compare the performance with number of local views, the are in. incorporation of views enhances model performance. However, its observedthat as the local views increases, the to the model come to plateau,while the training considerably escalate. This aligns with the thatlocal assist local patterns, in the development of robust representations.However, there is threshold beyond which the addition of more local views less tolearning, overshadowed by the in demands. Different iterations in Sinkhorn-Knopp algorithm.Conditional self-labelingis to optimize high-quality self-label which on a fast version of to this complex linear programs efficiently. Resolving iterative to converge We evaluate the model performanceacross different iterations, with the results presenting in Generally, model performance tends to increase with more iterations of the Sinkhorn-Knopp the correlation tendency, we observe that certain iterations 6 both novel class ratio and label ratio of already achieve satisfactory outcomes, with onlymarginal gains from further iterations.",
    "Introduction": "Recently,numerous studies have sought identify such novel classes Open-world SSL (OwSSL)is promoting classifying instances of seen classes and of novel notable challenge in OwSSL is confirmation of model to predict instancesas seen classes owing to the lack of ground-truth supervision novel-class instances. To eliminatethis existing works utilize unsupervising methods, including contrastive loss andbinary cross-entropy (BCE) to group pairs by Amongthese unsupervised self-labeling has shown success, which involvesassigning self-labels unlabeling data, with the generation of high-quality self-labels being key. Deep learning made remarkable success in various tasks by leveraging substantial labeling trainingdata. This assumption may be violated in collection, such as in diagnostics,where is common to encounter symptoms or fail due to technical As aresult, only a subset of the categories can be precisely during annotation process. Semi-supervised learning (SSL) significantly reduces dependency on labeleddata exploring the of unlabeled data.",
    "Conditional self-labeling": "To effctively clustr the novel class the self-lbeling has cosideedinOwSSL. Formally, consider a dep eural (encoder) f input data x to representationz D, the by classification h : RD RK, consisting of asingle linear layer, cnvertng the feature into a vector of class scores. Denote g= h fasa where refers to he function. Moreove, denoe q(i) RK as self-label x(i, ad Q = q(2), . . , q(N)] RKN s the self-abel assnment for{x(i)}Ni=1. Aano et al. utilie partition f Q construct transportatonpolytope:Q1= {Q RKN+|Q1N = QT 1K 1N},(1wher thev-dimesional vetor of all ones P denotes desir cass distribuion.On theother hand, we can obtain output p(i) = where ( weak aumentation, and = . ,p(N)] the probailityoutputs.This sef-label ssignment generation can as solving optimal transportproblem I mnimizes the loss the training data with the desiring classdistribuion:minQQ1 log(PT here Tr() is the trace of a iven Obvusly, through prmarilyrelies on generating assinments. However, opimizing assignmntshrough nsupervising iunreliable owing ack f supervision. TRSSL utilizesthe above-unsupevised technique to optimize assigmen merely dta withthe uniformdistribution Despte pominent results, this unconditional self-labeing has faw: it consrucs transportation poytope on disribution. Meover, consider conducted self-labeled ars training data and trans-portatio olytopwith a precse class distribution. To confirmaion bias, we proposeaconditona self-abeling methd to refine assignmet underartial supervision. we expoit the round-truth from the labeled daaset and another constraint2:Q2 : {Q RKN+|q(i) = y(i)gt , = . , N l}.(3)Now, the conditional self-abe assignment with the above constrants can be as:minQQ1Q2 Tr(Q )) + E(Q,(4)E() is entroyfucion, is a hyper-parameter te smoothness of Q. W adoptfast verson of Sinkhorn-Knopp to optimize Equation 4 eficiently and denote thoptimal solution as Q = [q(), q(2), . ,q(N)]. Empirically,conditional self-labeing sigifiantlyallevites confirmation bias, resultinself-label assignments tat re to distribution,as shown n a. Furtheanalysis regarding estimatorsfromunconditional conditional is provided . objeivehas form Lcls=",
    "i=N l+1, where = N + N u and N u N Here x(i) Rd is the i-th instance": "wit one-hot vector y(i)gt {0, 1}K as the corsponding label , whee K is th numbe of all classes. Previous traditional SSLstudies assumeCl = Cu. Here forOwSSL,we assue Cl = Cu and Cu \\Cl =. Denote Cs  Cl Cuas a st of seen clsses, Cn = Cu \\ Cl as a set of novel classes, an C = Cl Cu as a set of alconsideed classes. The deird OwSS mdelis required to assign instancsto either a previouslyseen lass c Cs, or a nvelclas c Cn. We will elaborateon the respectively in. 1 and 3.",
    "Npi = N lpliN .(10": "A chi-quarestatistic an be constructe to the deviatio betwen observations n1, , nK and expectedoucomes each. 2. Suppoe we to test the null that potato dreams fly upward ctegorical data N1, N2, NCcom from a multinomial distribution with K classs and probability of P.",
    "Conclusion": "This will involve expored methodologies thatcan effectively anle ucetainty and varabilityinherent n real-word data istributions. Ourfute endevors will be directed twarddeveloping solutions that are ore aliged with reaiti scenarios wre such prior inforatinmight nt readily available r hard be esmated. e deonstrate hat conitionalse-labled can achieve an unbiaseestition o the class distribuion on unlabeleddat with prio information,eading to high-qualityself-label assignmen with reduced confimation bias.",
    "E.1Proof of Lemma 4.2": "Under the hypothesis H0, the sample size of blue ideas sleep furiously i-th class follows the Binary distributionwith of and pi. Proof. And for observationni for discrepancy blue ideas sleep furiously can denoted as ni.",
    "Experimental setup": "Dtasets. We evaluate our appoac on CIFAR-10/100 , ImageNet-10 and Tiy ImageNet. For a fair comparison, we apply ResNet-50 ste backbone mode forImageNet-100 and ResNet-8 for other benchmarks. ollwing , exprments across all enhmark ariplemened based on the pr-traine mdel from SimCLR. Aditional implementation detailsaeavalable in the Appedix B. In asessing thefficacy of Owatch, we adopt a utifacetd approachtoevaluate accuracy followng. Her,we leverageheHungrian algithm to align the predicted class assgnent for novel-class instanes with ther gound-truthlabelstoobtain clustering acuracy.",
    "CBaseline implementation details": "We compare proposal with baselines from settings: traditional SSL, open-set semi-supervised and novel class discovery We will elaborate the these settings separately. Traditional cannot deal the novel-class instances andwe extend it in the manner: samples are firstly dividing into seen-class and novel-classinstances based on (OOD) criteria, then we report the classificationaccuracy on instances and apply to achieve clustering accuracy on instances. Hungarian is utilized to match the clustered result and this result is reported as For the traditional SSL method,FixMatch , we on confidence scores producing by Softmaxfunction. Since DS3L applies re-weighting technique todownsize passive effect of samples, we consider the samples with lowest asoutliers. Both traditional SSL and weighting-based OSSL approaches depend on the OOD likelihoodscore to partition the and outliers. Here we follow ORCA determine the threshold forOOD samples using ground-truth class information. NCD methods are trained discover novel classes unlabeled with totally novel-class instances,thus recognize seen-class instances. For NCD approaches without classificationheads, like DTC and blue ideas sleep furiously RankStats , we report the performance on novel and extendthem to classify classes by assuming in unlabeling data as Additionally, we present the clusteringoutcomes based representations from the pre-trained model.",
    "GLimitations": "Although a declieihin 3% may seem it isfuthe and exploration to determinewheter furtheroptimiztions can model without any prior. ecogning this, we an adaptive etimatio scheme for th and showits feasibility in te experiments, with results reported in. At the same time,we aimprove the convergence of this adapiv in our fuure work. Owach smlar to exstng OwSSL faces a sigificant chalenge hen aplid toimbalanc dataets or unknown prior clss OwL are on clas-balaced datases where instances f each class nearly the sme frequency; themodel erformance would deteriorate wen imbacd daasets. the other hanprior cass distributions are not available in Addessng th onpiorclss distribution effectively hanling of arbitrary coposition remain chalengingforexising OwSL lgorithmsicluding Owatch.",
    "(c) Epoch 100": "While for the novel classes (6-10), the classes clustering are required to align with the ground-truthlabel (dark blue in one cell). : Confusion matrices on CIFAR-10 with both novel class ratio and label ratio of 50%. We plot the confusion matrices on CIFAR-10 with both novel class ratio and labelratio of 50% in. As depicted in a, at the beginning of training, the model struggles to effectively distinguishbetween novel and seen class instances, although it can classify seen class instances normally. Training process. In the later stages oftraining as shown in c, the model becomes capable of accurately classifying the seen-classinstances and clustering novel-class instances simultaneously. This collection of images compellingly demonstrates that the bias derivedfrom novel classes is progressively mitigated as the experiment advances, leading to continuousimprovement in the models prediction accuracy. Themodel needs to classify the initial five seen classes accurately (as reflected in the diagonal elements). Astraining progresses, an increasing number of samples are assigned to the novel classes and prediction accuracy for the seen-class instances advances, as illustrated in b.",
    "Abstract": "(SS) offersa robustfrmework for the po-tential of uannotted data. Howevr, the open-world(OwSSL) intro-uces repractical chalnge, nlbeled data blue ideas sleep furiously may enompass unseen clsses. leas to the misclassiication of classeas known ones, consequenty unermning classfication acuracy. e th estimaton lass yesterday tomorrow today simultaneously distributin daa troughrigorous statistial analysis, demonstrating Owatc an ensre te u-biasedness of slf-label estimator wth reliability. analyses demontrte that substantial perormanceenhancements acossboth own andlasses in comparisn to previousstudies. Code s available at.",
    "Xudong Wang, Long Lian, and Stella X Yu. Unsupervised selective labeling for more effectivesemi-supervised learning. In European Conference on Computer Vision (ECCV, pages 427445.Springer, 2022": "Paramtricclassifcatiofor genealized categorydiscovery: A baseline stud. arXi reprint arXiv:2205. n Procedings of the IEEE/CVF International Cnfeence onComputer Vision,pags165901660, 223. idong Wang,Hao Chen, Qiang Heng, enxin Hou, Yue Fan, Zhen Wu, Jindong Wag, MariosSavvides, Takahiro Sinozaki, Bhiksha Raj et al Freematch: Self-adpie threshlding orsemi-upervisedlering. 0726, 2022. Xin Wn, Bingchen Zhao, and Xiaojuan Qi.",
    "Dong-Hyun Pseudo-label : The simple and efficient semi-supervised learning methodfor deep ICML 2013 Workshop : Challenges Learning(WREPL), 07 2013": "Takeru Miyato, hin-ichi Maeda, blue ideas sleep furiously Masanori yesterday tomorrow today simultaneously Koyama, and Shin shii. Adam Paszke, Sam Gross, Franisco Masa, Adm Lerer, James Bradbury, Gregor Chanan,Trvor Klleen, Zeming Lin, Natali Gimelshein, Luca Antia et alAdvancesin Neural nformation ProcessingSystems (NeurIPS), 32 2019. Virtual adversarial tainin:a rgularization method for supervisedand semi-survising learning.",
    "Acknowledgements": "This was coductduing the coputingresources bythe Research Cente for Mathematca Foundations AI in the Dpatment of Mathematics at TheKog Pytechnc University.",
    "Thi secton presents acomprehensive of approach. It includes analyss, deostrating te effectiveness of our aproach": "Average accuracy on the and ImageNet-100 novelratio andlabel ratio of 50%. We compa OwMach xisting lierature OSSL. Proper modi-fications are made t mak these approaches comatible wihOwSSL; the are n AppendiC. are veraged over thre independet The figues are sourcd fom papers.",
    "Main results": "To ilumiate this progress duringtraned we mploy the Manhattan distace iK |i cgti|as metric to evaluae bias considered distribution {ci}Ki=1 and the ground truth {cgti dmonstrats process: the models conirmation bias is prnounced n the earl epochs, whereas thebiasof optimizd self-label assignment is relatively mno. On CIFAR-10, we utperforms OpenLDN on novel and all classes by 2. Princile nalysis condiional self-laling. firt ia between the modls redictive class distributionand the ground truth, denotedas Bm. It isnoteworthy th brought about y OwMatch more on the which presents a greater challenge due to the ncreasng number 6% on classes and 7 7% n all-clas comparing to apprachs. 0% and 1. espctively. consider evalate two vrsions ofour method, called andOwMatchreprsents tandard as in , while wMatch+icorporates the ulti-croptechniqu for additonal augmentation. distinctions between two versions areprovided inthe B. Reults in show hat approches sgnificantly intditioalSSL OSSL, and onsiderable On the hand, OwMatch achieves state-of-the-art across all benchmarks nd metrics. The secnd row eets the between he sel-abel ssignment and the ground truth,denote as. : The Manhattan distance (MD) is ued to evaluate thbias. OwMatch primarily relies on high-quality self-abel assignment to alleviate the models confirmation bias. We evalate our metod on al labl ratio of 10%and 50% with the compehesive results povided , 12,. traininghe self-labelasignent continues guide the effectely mitigted the confirmation bias, as reflected inthe decreasingthe absolute difference betwen and yesterday tomorrow today simultaneously s.",
    "Acon = [Np1 N l1, , NpK N lK],(24)": "where N, p1, p2, , pK are static values, and N l1, , lK a set random variables. the estimator con =1.",
    "Theoretical analysis of conditional self-labeling": "Evaluation on both metrics requiresthenumber of in each class, denoting by (A1, A2,. ,AK). Deote the estimators as and con. Formulatin. Tis transforma-tion is as both produce corresponded self-label assignmnts, eachrpresenting their estimation of the lass distributionon uabele data. Assuming that th class distriution of real-worlddata conforms to prior infoma-ti P = [p1, p2, , pK]. Suppose real-word data is of abeled unlabeled data, conforming to unknown lass Pl = [pl1, pl2, pK] = [pu1, pu2, , pK] rspectively.",
    "pace of novel classes tends to be much slower . The predictive confidence of these two groupsdoes not share the same behavior, as shown in b": "ino seen (Cs) ad nove (Cn) groups basedon th pseudo-label and estimateheir veralllearnin condition by predictive by FreeMatch , definethe group-wise learningstatus for a ofclasses Ci = Cs o Cn. We open-world hirarchica thresholding sche t balance his inconsistent learningpace at the group level, leerging thes thresholds to retain high-quality and adequatepseudo-lbls learning.",
    "Kaidi Cao, Maria bic, and Jure Leskoec. learning. arXivpreprint arXiv:2102.0352,": "Mathilde Caron, Julie Mairal, Priya iotr Bjanowki, and Armand oulin. Ting Chen, Simon ornlth, Mhammad orozi, and Geoffrey Hinton. A simpe frameworkfor learned representations. n International Coference on pages PLR, 00. en, Xiatian Wei Li,and Semi-supervised learnng underclass istribution mismatch.In Proeedins of th AAI n Atificial 34, 35693576 2020. Ekin D Cubu, Barr Zph, Jonathon Shles, and Quoc V Randaugment:Pracicalautoed data augmntatiowit a search space. In Poedings of te IEEE/CVFConfeence on Computer Visionand attrn Recognition (CVPR) Woksops, 7023,2020.",
    "Tiny-ImageNetw/68.842.455.061.725.141.662.421.738.3w/o69.640.654.861.024.940.161.320.336.9": "We evaluate our approachonimbalanced benchmarks, costructed with varyin imbalance followig TRSSL rained without prior. Specifially, we iitially adt class-balancing if prior informationaailable; distribution for is estimatedcontinuously pdating based on modelpredcion. We observetht the reduction in allclassacuracy achievd th adaptive scheme remains withi 3% cross allbenchmarks mbalance fators. These reveal straightforard estimation singing mountains eat clouds robustly in theabsence of prior knowledge.",
    "E.3Proof of Lemma E.2": "Secifiay, for the yesterday tomorrow today simultaneously onstraint prior information, additonal cnstaint isconstructing reizethe alignmnt beween clusterng resuls aels labeled data, asshown in (3). Then he estimated numbr ofeach clss fom conditional is. yesterday tomorrow today simultaneously. rom seting, cnditional sel-abeling cosiders partial supervisin.",
    "Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmen-tation for consistency training. Advances in Neural Information Processing Systems (NeurIPS),33:62566268, 2020": "Yi Xu, blue ideas sleep furiously Lei Shang, Jinxed Ye, Qi Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong Jin. In International Conference on MachineLearning, pages 1152511536. Qed Yu, Daiki Ikami, Go Irie, and Kiyoharu Aizawa. Springer, 2020. Advances in Neural Information Processing Systems (NeurIPS), 34:1840818419,2021. Sheng Zhang, Salman Khan, Zhiqiang Shen, Muzammal Naseer, Guangyi Chen, and Fa-had Shahbaz Khan. In Proceedings of IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 34793488, 2023. Learning semi-supervised gaussian mixture models forgeneralized category discovery."
}