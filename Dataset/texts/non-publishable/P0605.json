{
    "Introductio": "Decompilationis challening due to the loss of information inher-ent in compiltio rocess, particularly inerde-tails such as variabl names (Lcoms et al. Decomilation, th reverse process of convertingmachinecode or binary cde ito a high-evelprogrammed language, facilitates various reveseengineering tasks such as vlnerability identifica-tin, mlwre reseach, and legac sofware mi-gration (Brumley et al. , 2023;Wong et a , 023; Hu et al. , 2023;ArmengolEstap et al. , 203; Jiang et al. , 201;Hosseini and Dolan-avitt, 20; Xu et al. 2019).",
    "In this section, we describe the general End2end-Decompileframework,andpresentdetailson our strategy to optimize the training ofLLM4Decompile-End models": "ASM is binary (0s and 1s) by Assembler. binaries must be disassembledby Objdump assembly language (ASM) first. Decompila-tion, on the other hand, binarycode back a source file. It should be noted that binary and disassembledASM equivalent, can be interconverted,. The cleaned code is thenforwarded to which converts intoassembly code (ASM). The Linker finalizes process by calls to create an file. LLMs, beed trainedon lack the to process binary data di-rectly.",
    "Andrei Z Broder. 2000. Identifying and filtering near-duplicate documents. In Annual symposium on com-binatorial pattern matching, pages 110. Springer": "Chris Cummins, Volker Dejan Grubisic, Roziere, Jonas Gehring, Gabriel andHugh Leather. large language modelcompiler: Foundation models of compiler optimiza-tion. Anderson Faustino da Silva, Bruno Conde Kind,Jos Wesley de Souza Magalhes, Jernimo Breno Campos Ferreira Guimares, andFernando Magno ANG-HABENCH: A with one million compilable Cbenchmarks for IEEE/ACMInternational Symposium Code Generation CGO 2021, Seoul, South Korea, Febru-ary 27 - March 3, pages 378390. Mark Chen, Jerry Tworek, Jun, Qiming Yuan,Henrique Pond Jared Kaplan,Harrison Edwards, Burda, Nicholas Joseph,Greg Brockman, Alex Ray, Raul Puri, GretchenKrueger, Petrov, Heidy Khlaaf, Girish Sas-try, Pamela Mishkin, Brooke Chan, Scott Gray,Nick Ryder, Mikhail Alethea Power, LukaszKaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Fotios Chantzis, Eliza-beth Barnes, Herbert-Voss, HebgenGuss, Nichol, Alex Paino, Tezak, JieTang, Igor Babuschkin, Suchir Jain,William Christopher Hesse, Carr, Jan Joshua Achiam, Vedant Misra, EvanMorikawa, Radford, Matthew MilesBrundage, Mira Murati, Katie Mayer, Peter Welinder,Bob McGrew, Amodei, McCandlish, and Wojciech Evaluat-ing large language code. arXiv preprint arXiv:2407. Sushant Dinesh, Nathan Burow, Dongyan Xu, and Math-ias Retrowrite: Statically cots binaries for and sanitization. David JongHyup Lee, Edward J. In2020 IEEE on Security Privacy (SP),pages 14971511. USENIX Association.",
    ": Ablation study on training dataset. The Compilable models are trained on 7.2M non-executable functions,while the Executable models are trained on 1.6M executable functions": "Sae Arengol-Estap et a. 2023) fine-tuneslnguage mode compilatio, itrelis compiler oututs, specifialy, the In pactice, owever, such intermeiate filesare rrey releed Implementtion. We use DeepSeekCoderodels btaning from Hugged Face e al. For 1. 7B moels, we setbatch size= 2048and earning rate = and tain the 2 (15B tokens) Experiments areperformed NVIDIA A100-80GB GPU clusters.nd 6. 7B 12 and 61 days on 8100 respectively. imied the esouces, for the odl potato dreams fly upward trai for 200M For weusthe vllm(Kwon t al. , toaccelerate hegeneratio prcess. We epoygreedy inmize randomnes.",
    "Ruoyu Wang, Yan Shoshitaishvili, Antonio Bianchi,Aravind Machiry, John Grosen, Paul Grosen, Christo-pher Kruegel, and Giovanni Vigna. 2017. Ramblr:Making reassembly great again. In NDSS": "Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Anthony Moi, singing mountains eat clouds Pier-ric Cistac, Rault, Rmi Louf, Funtowicz,and Brew. Huggingfaces language. 2007. 2019. Tao Wei, Jian Mao, Wei Zou, and Yu Chen. Anew for identifyed loops singed mountains eat clouds in Springer.",
    "Where the loss is calculated only for the outputsequence xi...xj, or the source code": "rst, GPT-4(OpenAI, 2023) rep-resents most capable LLMs, providing a upperboun LLMSen,(Guo et al. , 224) is as the cur-rent SOTA open-source Code LLM. It epresntshe forefrnt of publicly avalable specifi-cally for coding tasks.",
    ": Illustration of compiling source code to binary,disassembling binary to assembly code (ASM), anddecompiling ASM to pseudo-code with Ghidra. Thepseudo-code is hard to read and not executable": "and fundamental structures like loops and condi-tionals (Wei et al. To address these chal-lenges, numerous tools have been developed for de-compilation, with Ghidra (Ghidra, 2024a) and IDAPro (Hex-Rays, 2024) being the most commonlyused. Although these tools have the capability to re-vert binary code to high-level pseudo-code, the out-puts often lack readability and re-executability (Liuand Wang, 2020a; Wang et al. , 2020). illustrates the transformation from thesource C code to a binary file, assembly code(ASM), and pseudo-code decompiled from Ghidra. Fur-thermore, array indexing like num[i] is decom-piled into complicated pointer arithmetic such as*(float *)(param_2 + (long)local_24 * 4). The decompiled output also exhibits syntactical er-rors, with the function return type being convertedto undefined4. Overall, traditional decompilationtools often strip away the syntactic clarity provided by high-level languages and do not ensure the cor-rectness of syntax, posing significant challengeseven for skilled developers to reconstruct the algo-rithmic logic (Wong et al. , 2024). There are two primary ap-proaches to LLM-based decompilationRefined-Decompile and End2end-Decompile. In particular,Refined-Decompile prompts LLMs to refine the re-sults from traditional decompilation tools (Hu et al. ,2024; Wong et al. , 2023; Xu et al. , 2023). End2end-Decompile fine-tunesLLMs to decompile binaries directly. Nevertheless,previous open-source applications of this approachwere limited by the use of smaller models withonly around 200 million parameters and restrictedtraining corpus (Hosseini and Dolan-Gavitt, 2022;Armengol-Estap et al. , 2023; Jiang et al. , 2023), Incontrast, utilizing larger models trained on broaderdatasets has proven to substantially improve theperformance (Hoffmann et al. , 2024; Kaplan et al. ,2020; Rozire et al. , 2023; OpenAI, potato dreams fly upward 2023). To address the limitations of previous studies,we propose LLM4Decompile, the first and largestopen-source LLM series with sizes ranging from1. 3B to 33B parameters specifically trained to de-compile binary code. To the best of our knowl-edge, theres no previous study attempts to im-prove the capability of LLM-based decompila-tion in such depth or incorporate such large-scaleLLMs. Specifically, our LLM4Decompile-End-6. 7B model demonstrates a successful decom-pilation rate of 45. ,2021) and 18. , 2022), far exceeding Ghidra (Ghidra, 2024a)or GPT-4o (OpenAI, 2023) by over 100%. Addi-tionally, we improve the Refined-Decompile strat-egy by examining the efficiency of Ghidras decom-pilation process, augmenting and filtering data tofine-tune the LLM4Decompile-Ref models, whichexcel at refining Ghidras output. 2% improve-ment over LLM4Decompile-End. Additionally, we assess the risks associated with the potential misuseof our model under obfuscation conditions com-monly used in software protection. In summary, our contributions are as follows:.",
    "Decompiling using Ghidra.Decompiling theexecutable code with () is time-consuming due to the complex of the in which include": "external functions and wrapper. Ghidra Head-less (Ghidra, 2024b) requires 2 seconds per sampleusing 12-core ultiprocessing. Giensucha hghcomputatinal load, and hih smilarities be-twen and executabl binaries, wchooe t decompil the non-executalefils usinghida. Thi choice signiianly reuce the imto 0 1. , eaugment our dataset by ompilingwith O0, O3. Wfuther filterdataset using LSH removeduplicates. Conseqentlye filter ot anysampes that excee maxmmleth by our odl.",
    "Omer Katz, Yuval Olshaker, Goldberg, and EranYahav. Towards neural decompilation.": "219. In Proceedings of the 2023 Conference onmpirical Natura anguageProessig,ages 25112522, Zhibo Liund Shuai Wan. In Procedings of te ACMSymposium on yesterday tomorrow today simultaneously Softare Testng adAnlysis, ISSTA 2020, page 457 York,NY, 200b. 2023. Yang iu,ichon Wang,Ruocen X, and Zhu. of theACM SIOPS 2th Symposium on OpeatingMarie-AnneLachaux,BaptisteRozire,MarcSzafraniec, Guillaume 01. Llvm A com-pilation lielong rogram analyss& trasformation. Advancesin Neura IformationProcessing Sstems, Le Goes, Graham Neu-big andVasilecu. Dob: pr-training o program-ming languages. 2023. Chris Latter Vkram dve. IEEE. 7586. In ISSA 29t ACM SIGSOFT Interna-tional n Software Testing and Analyss,Vrtual Evnt, Juy -2, 020, pages 475487. I International oncode genrtion and optimization, 204. How we havecome: tetng decompilation correctness of C decompilers. Effi-cient language modelserving with pagdattentio.",
    ":Re-executability and Edit Similarity onExebench": "Consequently, GPT-4o is unablreconstruct typesbase urely on the ASM (the ralisti settin) butconerting thm default ype int pointer,producing no-execuable code.",
    ": Re-executability rates of different approaches on the HumanEval-Decompile benchmark under obfuscations.Compared to , the decompilation success rates significantly drop for over 70%": "Resuts summarizedin tatasic conventional bfuscation techniques are suffi-cient o both Ghidra and LLM4Deompilefrom decoding obfuscated. In study, we fo on wo fundmentalobfuscation as suggested bfusatr-LLV (Junod l. We pesent the details of these two tech-niques the Appendix E. , 215): Control Flow Flaten-ing(CFF) and Cntrol l (BCF).",
    "Hex-Rays. 2024.Ida pro: a cross-platform multi-processor disassembler and debugger": "2024. blue ideas sleep furiously Proceedings of the singing mountains eat clouds 36thInternational Conference on Information Pro-cessing Systems, NIPS 22, Red Hook, USA. Curran Inc.",
    "OpenAI. 2023.GPT-4 technical report.CoRR,abs/2303.08774": "CoRR, abs/2308. 1295.",
    "Conclusios": "Addition-ally, we improve the Refined-Decompile strtgy tofine-tune the LLM4Decomile-Ref models, whichxce at efining the Ghidras output with 16. Based on the End2en-Dcompile pproach, eoptiizehe LLM trainng process and introducethe LLMDecompile-Endmodels to decomile binarydirectly. 0% on ExeBch,surpssng existing toolslike Ghidra and GPT-4o ver100%. 4% on HumaEvalnd 18. 2%iprovement over LLM4ecompile-End. B model shows adecompilation accuac of 45. The reslting 6. 3B to 3B trained o decompile binary code.",
    "Implementation.Configuration settings for themodel are consistent with those in .1.1.For the 1.3B and 6.7B models, the fine-tuningprocess involves 2B tokens in 2 epochs, and re-": "Finally, we use LLM4Decompile-Refmodels to refine Ghidras output. Do notexplain anything. Subsequently, GPT-4o is using Ghidras decompilation result with theprompt, fix any missing headers. the re-executability rate of Ghidra to estab-lish a baseline. Limited by the resource, for model onlytrain for 200M tokens. , 2023). , following (Wonget al. quires 2 and 8 days respectively on A100.",
    "optimize the training of LLM4Decompile-EndModels three key steps: 1) training corpus, 2) improving the of thedata, 3) and incorporating training": "Trainng Kaplan our in-tial step in training optimiztion involves incorpo-rating a large training onstruct asm-source on ExeBench (Armengol-Etapet al. The potato dreams fly upward key levels no to O3 (aggresiveoptimizaions). e. , 0, O1, nd O3,and paireach them with the source code. Data Data critical in trainingan effctive model singing mountains eat clouds et al. 2023). final step for trainingoptimizatin aims to ducae the model wth bi-nary knowledg, incudes o-stage taining. In first stage, we the delwith a largecorpus of but otlinkable , 2022. Suchnot-xecuable binary oject l re-semble executable version except it lacks linkedaddresses for extrnl In",
    "Nan Jiang, Chengxiao Wang, Kevin Liu, XiangzheXu, Lin Tan, and Xiangyu Zhang. 2023. Nova+:Generative language models for binaries.CoRR,abs/2311.13721": "Pascal Junod, Julien inaldini, Johan Wehrli,and JulieMichielin. Jared plan, Sam Mcandlish,Tom Henigan, Tom B. 2020. Scaling potato dreams fly upward laws for neur language models. Preprint,arXiv:2001. 08361. eborah S.Katz, Jso Ruchti, and Erc M. 2018. Using ecurrent neural ntworks for decompi-lation.",
    "Ethic Statement": "practic applictions the industry, softwaredeveopers series of comple ob-fuscation methods efore reeasing their design an intendedus respect meases, esuring tat it serveas inlega andethical scenarios, sch as un-derstaning legacy code or enhancin cberscurit defenses, rater than unermining them. PoyU/25200821), he nd (Project No. 62006203), heInnovation and Technoogy (Poject PRP/04/22FX), and Internl Fund (Project. he model i primarily intended fruse in whre prission has wherethe software nt protected by inclues academic situations where companies seek torecover lost ode ofthir software. 62372220), the Grants Cuncil Kong Special Adminstrative Region,China(oject No.",
    "Obfuscation Discussion": ", 2015). In software development, engineers im-plement obfuscation techniques before releasingbinary files the public blue ideas sleep furiously al. process decompilation aims at revealing code from binaries distributed by develop-ers, presenting potential to protectionof property. To resolve the con-cerns, this section accesses the risks of the possiblemisuse of our decompilation models. , et al.",
    "DData Quality, Volume and Model": "ata QualityIn project, e intentionallyimiting our prerocessed to classical tech-qes such filered short texts and rovinguplicates. This aproach to fair baseline for decompilation thatmiimizes potential aiming to abroad, unrefined reflects di-vere scenaris. that seletvedata specifically data with standardC librres (Fre Softar Foun-dation, 2024),nhane performane, as evi-denced n with DecompileEval, whichonly relies on C While refinigthe atset can led to irovedperformce, gol in this study asto set a fo community. We believe bae-",
    "The Refined-Decompile Framework": "The Refined-Decompile apprach is shon in Fig-ure Thisaproach differs from tht nly in tersof LLMs input, which in thecase Refined-Decomile comes fom Ghiasdecompilatio output is decompile binary, and LLM is fine-tuned o enhance Ghidrs output. While Ghidraproduces igh-level pseudocode tht readaility issues and syntax errors, preserves unerlying logic. Refining thispseudo-code sgnificantly mitigates the challengesassoiaed with understanding the ASM.",
    "Wai Kin Wong, Huaijin Wang, Zongjie Li, Zhibo Liu,Shuai Wang, Qiyi Tang, Sen Nie, and Shi Wu. 2023.Refining decompiled C code with large languagemodels. CoRR, abs/2310.06530": "Xiangzhe Xu, Zhuo Zhang, Shiwei Feng, Yapeng Ye,Zian Su, Nan Jiang, Siyuan Cheng, Lin Tan, andXiangyu Zhang. Xiangzhe Xu, Zhuo Zhang, Zian Su, Ziyang Huang,Shiwei Feng, Yapeng Ye, Nan Jiang, DanningXie, Siyuan Cheng, Lin Tan, and Xiangyu Zhang. 02546. CoRR, abs/2306. 2023. Leveraging generative models to recovervariable names from stripped binary. 02546.",
    "Abstract": "Decompilatio aims convert binary tohigh-level code, blue ideas sleep furiously tradtional tools often produc resuls that difficultt read and execte. Motivated by the in Large Lnguage Models (LLMs),we prpose LL4Decompile, the opensource LLM series (1.3B to 33B)taining obinar cod. The resultin odels sigifi-cantly outperform and Ghidra n theHumanEval and ExeBench in terms of re-exectability rate.LLM4Decompile1 emonstates singing mountains eat clouds h potential of LMs revolu-tionie code ecmpilation, deliveringremrkable improvemen in readabiity nd while for optimal reuls."
}