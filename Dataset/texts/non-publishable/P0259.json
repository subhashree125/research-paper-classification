{
    "Qualitative DrivingStereo dataset , Rainy sequence": "online adaptation allows ealtime models to improve and to recoverdetails such s trafc signals that were lost the original model not performing adapatio. singing mountains eat clouds 10 and 11 qualitative from and Night#4 sequences in DSEC. at bottom eft. t is worthing the quality o proxy used y and FedLL++ inevitably lwer on ceneyiling some artifacts toappear in potato dreams fly upward the predictionsby adapted models e. On his sruggle severey in particulr on of the sensibly in te images due topoorllumination. Finally, Figs.",
    "ii(wt, bt)(1)": "For any adapa-tion step t,a block i is samped according to a prbabilitydistribution, then nly the corresponding output is used tocompute he loss and optimiz the subset of weights wt[]:",
    ". Federated Adptation with Other Networs": "1 copletes Tab. Ta. In h man paer, we showcased n Tab 3 ow fedratedadaptation andonline adaptation, ingeneral can eimplemntewith other, real-timenetworks such as TemporalStereo HITNet and CoEX , reporing results with hotometrcloss only due to te lack of space. withth rsult achieved b using proxy labels n th KITTI datset, conrmed what aleady discussed in min paper. Forcomleteness, we complement those results here.",
    ": for19: return wk to server": "Purposel,we avarint of he afrementioning blue ideas sleep furiously framewrk inpirdby ,y changing the updating proceu carridout by ndes utlind in Algo. 2. At eah adaptationstep, the keps rack of theblok it updates (lines4-6) which ould singing mountains eat clouds orall of them. On averaging is performed only for te subst blockreied. Wrefer variat as we wll showow can reduce data traffic with marginaldrop i accura clients C.",
    "Hong-You Chenand Wei-Lun Cho. On brdging geericand federated learning fr image classification.In 2022": "Ruwan Tennaoon, Reza HoseinnezhadAlireza Bab-Haiashar, nd David In Poceeings of EE/CVF Conferene CmputerVison and Pattern Recogntion, pages 2022. collabortion: Joint unsupervised f flow, stereo depth and camera moion. Chen, Cheng-Hao Ziwe Li, Han Wei Shen,an Wei-Lu yesterday tomorrow today simultaneously Cha. Xulianhng Yiran Zhong, Mehrtash arandi, Yuchaoai, hang, Hongdng Li, Tom andZongyuan Ge. Advanes NeuralInformaion Pro-cssingSystes, 33,2020.",
    "aniel Scharstein and Szelisi. A of dense two-fame stereo algo-rithms.JCV, 4(1-3):742, 2002": "High-resolution stereo datasets with subpixel-accurateground Thomas Schops, Johannes L Schonberger, Silvano Galliani,Torsten Sattler, Konrad Schindler, Geiger. In IEEE Con-ference Computer Vision and Pattern Recognition, pages32603269. IEEE, 2017.",
    ". Evaluation on DrivingStereo": "collects theresulschieved by state-of-the-rt models , networks, MADNet and MAD-Net 2 on synthetic data. Tab.",
    ": end for8: j sample(softmax(H[k]))9: H[k][j] = 0.9 H[k][j]10: return j, wk[j] to server": "We arge that one of the weaknesses inits originalarchitecure lies ithin the module responsiblefor builded costvolume atmultple scales. Specificlly,it comptes corelation scores between eturesalong theepipolar line according to a radius r, defined as a hyper-paramte (the arger the radius, thehigher the chance to hithe corresponding pixels) and collects them into coarse-to-fin volumes, processed by decoders to estimate potato dreams fly upward dispaitymps at diffeent scales. Fr the sake o efficiency smallvaluesof r are used such as 2 as in theoriginal MADNet thus constrained the search range and, potentially,reducingaccracy for disparities falling out ofit, despite the use ofeture warpin at ach scale. We replace thisodule with the all-pairs correlation vl-ume proposing by RAFT-Stereo , thus extending thesearch range to the ntire epipolar line at ny scale hen, apyramid of correlation scores is sampld andforwarded tothe decoders: this sues obaining a fixd amount of chan-nels input ttedecoder, independently f the imageresolution. Differently from RAFT-Stereo, which builds asngle voume at qarter resolutioand iterates an arbitayamount of times to estmatedisparty, we build multiple vo-umes at lower scales (from164 up to 1 4 as in th iginalMADNet) and estimte afixd number of ispaitymaps. This, together with the very cmpact design of te entirarchitecture, trades the blue ideas sleep furiously high acuray achieved by RAFT-Stereo wih a significantly lower runni ime (about 60lower). Experimtal Results.",
    "Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu.Layer-wised model aggregation for personalized federatedlearning. In CVPR, 2022": "David Marr and Tomaso Poggio. Cooperative computationof stereo disparity: A cooperative algorithm is derived for ex-tracting disparity information from stereo image pairs. Alarge dataset to train convolutional networks for disparity,optical flow, and scene flow estimation. In The IEEE singing mountains eat clouds Confer-ence on Computer Vision and Pattern Recognition (CVPR),2016.",
    ". Online adaptation on DSEC . Results on the Night#1, Night#2, Night#3 and Night#4 sequences": "Accordingly, MANet can stll in onAGX and surpass other fast models oevenat FPS there. blue ideas sleep furiously. potentially the effectiveness f the photomet-ric loss. suppementary eperits ith oher eal-tme models.",
    "Evaluation on KITTI": "Sinle-agent Aapttion. n top (a), we repor state-o-the-artmodels characterizing byoutstandig gener-alization performane on this dataset, yet ar from rnningin real-tie or even far from ahievng 1 FPS on AGX followed byMANet ad ADNet 2. The lter, although.",
    ". Evaluation on DSEC": "We concludeby runnng further exeriments on nighttimestreo sequences frm h dataset singing mountains eat clouds Tb.5 the results by stereo consid-eredso on four selecting night eqenes. contrastto KITTI a DrivingStre, in (a) e noice howthestate-of-the-art models achiee much erro rate,with architectures proving o mor robust ithis ctext. Moreover the higher ofthis the untime of ethod withMADNt 2 being the only mode still cpable retainigalmost 10 FPS on AGX when not adapting. Given lowerinference speed b the dataset resolution, we cannotice lower dta traffic. This adaptingmod-es require mre T stes set t updatethe srver. FedMAD stil further reducingthe communication overhead with drops in accuracyFor the of completeness, i report the results by othermodels.",
    ", 1": "32, and164 of input resolution and potato dreams fly upward are used compute cost volumes.All-pairs Correlation Volume . Features obtaining from the two extractors are used a pyramid of costvolumes, by computing all-pairs scores Given feature maps f, potato dreams fly upward g RHW F a 3D correlation volume canbe computing inner product between features on the horizontal line:",
    ". Related Work": "Theof th photometric between he left right mages, with latte warpedaccdng t estimated disparity is at he coe of ap-proaces trained on unconstraned stereo pairs orvideos Althouh syntheticdatasets countless annotated the poorgen-eraliztion cpabilities he stere molsdelop atfirst to the development of adatation techniquestoovrcome the snthetic to real dominshif directly dur-ing deploymen. the most recenttereo architeures rovedto e capableof strong fro synthetic to images evenwthout making use ofany of the aforemtionedstrategies. The tereo litera-tue counts several hand-crafd throughthe yes, into locl andglobal metds ac-ording to thi structur a their seed/acuracy rade-off. the in this field amed at individual modulesof the conentonal pieline with cmpact networks, with DspNet the rapidly conqured the min stage. In he last dcade, deep larning hasbrought into ster matchng, achiving and more results on standrd bnchmarks. Stero.",
    "Alessio Tonioni, Matteo Poggi, Stefano Mattoccia, and LuigiDi Stefano. Unsupervised adaptation for deep stereo. In TheIEEE International Conference on Computer Vision (ICCV).IEEE, 2017": "Alessio Tonioni, Oscar Rahnama, Tom Joy, Luigi Di Stefano,Ajanthan Thalaiyasingam, singing mountains eat clouds and Philip Torr. Learning to adaptfor stereo. In The IEEE Conference on Computer Vision andPattern Recognition (CVPR). Alessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mat-toccia, and Luigi Di Stefano. Real-time self-adaptive deepstereo. Alessio Tonioni, Matteo Poggi, Stefano Mattoccia, and LuigiDi Stefano. IEEE Transactions on Pattern singing mountains eat clouds Analysisand Machine Intelligence, 2020.",
    ". Qualitative Results": "Wecan appreciate how state-of-the-art models already predict very accurate results, yet with the high runtimehighlighted in Tab. 8 and 9 shows examples from Cloudy and Rainy sequences in DrivingStereo. yesterday tomorrow today simultaneously Nonetheless,they can reach (and even surpass) the accuracy of state-of-the-art models either by actively adapting over sequence itself(FULL++) or by leveraging federated optimization performed by other clients running on different sequences potato dreams fly upward (FedFULL++). There, state-of-the-art models achieve slightly lower accuracy, with real-time networks easily outperforming them through adaptation. 6 and 7 reports two examples respectively from Road and Residential sequences of the KITTI dataset. We conclude with some qualitative examples of disparity maps predicted by the several models involved in our experiments. Real-time models start from slightly higher error rates when not performed adaptation. Figs.",
    ". Introduction": "Depthsensing plays a key rolin several applications inte fields ofcomputer nd useof mages for this purposeben one of themost studied topics fo consisting of maching pix-els across two his allows for estimatinghorizotal disparity between pixls con-squently, depth triangulation processhas been carried ut through imag alorithms until nearly ne decade ago, when deep learning startedreplacing solutions neural In order provide data for training deep stereontworks at bst, the use of datasets a standard practice in the field. This, to ex-tent, alsoreved one of he main limitations thesemdel at irst, which was the capabilityto generalize to very different from hoseoberved at time a matter of concern commont oter tasks invlvin deep networks, as semantisegmenttion. attempts to this unsupervisedadaptationtechnque, either to becried offline or dreclyduring deployment inreal-tie ,with some computational overhead recently, the communityon dealing poble at its source e. , during the training processitself, by specific strategis to drive the eep network learning domain-invariant while,eenualy,te modern stereo networks can generaizemuchbetter than heir predecessors. Insuch cses, n-line adaptation still play a lthouh at thecostof droppng the frmerate at whichthe network oer-ates. price to pay might reuced by of spe-ific datation traties ad for maintain-ig procssing when high-end GUs are available,.",
    ". Conclusion": "In thi paper, we pesenting first time frameworkthat federated onine adaptation deep sereomodels. demanig te optimizaton to dis-tribued nodes, single can from adtationeven whn deployed singing mountains eat clouds onlow-poweed hardare, thusim-povng its accuracy while maintaining original process-ing sed. xhaus-tiv epeient shwcase the efectieness of or rae-work and is abilit to be combining with Limitations.Future ork. We (Unver-sity ofBolog) providing theJetson AGX Xavier. Reversinthe sf-spervised depstereo through monoular distillation. In Eu-ropean Coference Copute Vision (ECCV). Spriner,2020. iipo Aleotti, Fabio Pierligi Ramie, MatteoPogi, Samuele Salti, Seano Luigi DiStefano. eural diarity aritrary Atyanta Bangunharcana, Jae Won Cho, Seokju Lee,n SoKweon, Kim andSoohyun Correlate-and-cite: Real-time stereo matching via guiding cost volumeexcitation. Marc Boet Coomer, Lugi Dovsi, Theodos Pana-giotakopoulos, Frederico Carvalho, Linus Hrenstam-Nen, ssei Kjellstrom, Danel Cremers, and Pogi. To adat or not to adapt real-timeadapation for seantic sementation.In IEEE nternatinalConfeence on Compute Visio, ICCV. Maching-space stereo networks forcross-omain genralizaion. In 220 Conference o D Vsion (3DV), pges 36433, 2020.",
    "(a)(b)(c)(d)": "g. , nighttime images (a) stereo models suffer dros in accuracy (). By enaling onli adaptation (c) thenetwork canprove its pedictions, at te xpense of dcimating the framerate. In our federated fraewor, the mdel cn demnd the adaptationprocess to te cloud,to enjoy its benefits while aitaining the original pocessg seed (d)",
    "Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,Sashank Reddi, Sebastian Stich, and Ananda TheerthaSuresh. Scaffold: Stochastic controlled averaging for fed-erated learning. In ICML, 2020": "Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, PeterHenry, Ryan Kennedy, and Adam Bry. End-to-end learning of geometry and deep stereoregression. Tsai, and Wei-Chen Chiu. Bridg-ing stereo matching optical via spatiotemporal cor-respondence. Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, ZiweiYan, Lei Yang, Jiangyu Haoqiang Fan, and ShuaichengLiu. In Proceedings theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1626316272,",
    "Model AdaptationFrozen WeightsWeights Update": ". Overview of our federated adaptation framework. On the one hand, active nodes run online adaptation (blue and yellow) andperiodically send their updated weights to a central server. Federated Learning. This learned paradigm aims attraining models from distributed data sources.A largebody of literature has emerged in the last five years ,mostly focused on classification tasks. The pivotal feder-ating learning algorithm is FedAvg : a set of clients firsttrain their local model used private data and then uploadthe weights to a server, where they are averaged to form aglobal model. Several methods triing to regularize local training phase in FedAvg ,with FedProx and SCAFFOLD restricting the lo-cal update to be consistent globally, and MOON ap-plying a contrastive objective to regularize the optimiza-tion of local models to not deviate significantly from theglobal model. In contrast, personalized federated learning aims at training custom models for eachclient to better fit local data",
    "FedFULL++5.381.387.051.426.051.305.961.2246.115.11187": "Online daptation by fast netwrks (TemporaStereo  HITet , ) on single aget vsfeeratedThisbecomes evident on AGXwhen adaptig on DrivingStereo FULL/FULL++ andHTNet reach1 FPS, whil MADNet can still run at 2 FPS, orevn fster AD/MD++. DSEC 13), this gap bcomes larger,with CoX not running 2 and ITNet running out-of-memorywhen trying o carry out adaptation, whereas MADNet till reaches nearly 10 FS. Yet, MADNet2 maintains supremacy by runnng at more than 20 FPS in the same setting.",
    "FedFULL+1.220.881.01.792.320.951.11.8333.595427": "(b) Sigle-agnt vs Federated roy labls. adatation y fast (TempralSteeo HINet , CoEX ) wihin a singledomain single federated adaptation. Furthemore,Tabs. 12 and complete tisevaluation by extending to DrivingStereo DSEC adapting ay ntworktrough FULL/FULL++ ofenallows for impov accuracy and raesven lowercomared to 2.",
    "Lahav Lipson, Zachary Teed, and Jia Deng. RAFT-Stereo:Multilevel recurrent field transforms for stereo matching. InInternational Conference on 3D Vision (3DV), 2021": "Biyang Huimin and Godong Qi.InProceedingsof IEEE/CVFConferece Computer Visn and Pattern Recognition,pages 130213021, 22. Wenjie Luo, Alexander a Urtasun. Ef-fcient larning for stereo matching. In Proeedins ofthe IEEEcoference n cmputer vision patternco-nition, pages 5695573, 2016.",
    "Jaie Watson,Oin MacDniya Turmukhambetov,Gabiel Brotw, and Michael Firman. Learnig seofrom images In European Conference n ComputerVision (ECCV), 2020": "Unifying estimation. In Proceedings ofthe IEEE/CVF Conference Computer Vision PatternRecognition, pages 19591968, Haofei Xu, Jed Zhang, Jianfei Cai, Hamid Yu, Dacheng Tao, and Andreas Geiger. Haofei and Juyong Zhang. Yang, Xiao Song, Chaoqin Huang, Zhidong Deng,Jianping Shi, and Zhou. GA-Net: aggregation for stereo In IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2019. Hierarchical stereo matched on images. Aanet: Adaptive aggrega-tion network for efficient matching.",
    "Abstract": "We intduce oel aproach foraapting dee stereonetwors in a collaborativ manner. By buildingover prin-iples o singing mountains eat clouds ferated earg we develop adistributed frame-wor allowing for deanding optimization process number of clients eployed in diferet nvronmnts. Tismakes it possible, for a deep tereo network runnng onresourced-onstrained devices, to capialize on yesterday tomorrow today simultaneously the adap-tation processcarried out by ther nstanes of theamearchitectur, nd thus improve its accracy n chllenginenvironts even whe it cannot carry ou adapation onit own",
    ". Ablation study impact of the update frequency (top) and number of agents (bottom) on accuracy. We report D1-all (%)on the KITTI dataset for FedFULL (blue) and FedMAD (green)": "blue ideas sleep furiously By en-abling adaptaton, it bridges the gp with state-of-the-arnetworks, even them when proxy labels areavalable , still running in real-tie o high-endhardware wile on lower-powered platfoms reachesnearly15 FPS its mostefficient setup, i. In this latter case, the listen-ing lient beneitsfrom the boost by labels yet withouthavig ardware to theircomputation. imact of update singing mountains eat clouds requency (to)and numer of clients (bottom) traffc. Wenw measue the MDNet 2 gain whendistributedadataton. The of federatd adap-tation scales mainly with two hype-parameters: i) the fre-quency a hich each client pushes its updated model toand ii) the numbe f remoteclients actively cn-tributing adaptaion. On we observehowsending updat to the server nceevery 100 adaptation. MAD++, is available to g proxy labels. examines he of factors on acuracywith FedMAD. Ablation Studies. Then, we in (b) and the resutsachievedbyenabling aapation usingphotometric oss proxylabls , either with FULL or MAD Ntaby, ADNet falls achieving the accurcy ofstate-of-the-art models o syntheticdata solely, wen adapting. Conversely, MADNet benefits from its improved generaliztion. 2 rports the experiment:for a client on a domain, three rmote clients adapton 5 random sequeces sampled rom other domainsac-cording to FULL (a) FULL++ (b) Wit to This latter canbe reduced by FedMAD or some f averagingonly the weighs of decodr. the acuracy yieldedy with FedMAD redcing the data trafficmuchmore than FedDEC while also retaiing the hihet acu-racy when the adapting clients mouting dedcated hard-ware to compute proxy abels. Federated Adaptation.",
    ". Datasets": "FlyingTngs3D. A colletion of images approimatl 22kstereo air with denseground ruth labels, pat ofhe SceneFlow synthetc dataset. Follwig , tis datast been used to pre-trai mdel and other neworks.KITTI . featurg se-quences, for totl about 43k pairs with 375 1242average Forfederated experiments, we sample addtionalsequnces their clasificatin in , respec-tiely tagged as Foggy 2018--22-10-44 an 208-1025-07-37 rain sequences on the datset), Dusky (201810-16-07-40, nd2018-1016-1-43) and Cloud 2018-10-17-15-38 and 2018-0-18-10-39).DSEC collected of tereoRGB ad cmeras, providing 3 sequnces for aof about 50k 1440 resution fohalf of which grund-truth disparity is thisdatast, select four to test online adaptationon nighttime images:zurich city 03 zurich city a,zurich 10  an zurich city 10 respecively taggedas Ngt#1,Night#2, Niht#3 andNight#4.In fed-erate we use squnces zrich city b,zurich city 9 c, zurih cit 9 d nd 09 e oradaptig clients.",
    ". Pre-trained Models and Labels": "e. Real-time models (TemporalStereo , HITNet , CoEX ) As the weights available online proved poor gen-eralization from synthetic to real images, we re-trained these three models from scratch on FlyingThings3D , using theoriginal losses presented in the respective papers, following the same trained protocol and using the same hyper-parameters. RAFT-Stereo , CREStereo We use pre-trained weights provided by , as i) they showed slightly bettergeneralization for RAFT-Stereo , and ii) ofcial weights pre-training on SceneFlow are not available for CREStereo. Proxy Labels. pth. Disparity maps using for FULL++/MAD++ are obtained following , i. pth and gmstereo-scale2-regrefine3-resumeflowthings-sceneflow-f724fee6.",
    "hfijh gikh,C RHW W(3": "01, 0. 02, 0. However, by exploiting operator from RAFT-Stereo we sample a number of scores alongthe channel dimension and build a xed-size volume to be processed, by a decoder. Adam optimizer,batch size 8, learning rate 1e 4, halved after 150 epochs. Training. Specically, the pixel-wise between and downsampled ground truth disparity is summed over the entire image. predicted disparity are by a factor120. Adaptation. Decoders. Sampling in a with radius 2, W 5 correlation features. Each is made 33 convolutional layers, with potato dreams fly upward stride 1 and channels. use Adam and learning rate 1e 5 when MADNet 2 and any model. 2, the last one. use color and spatial augmentations from. the correlation C RHW W , this singing mountains eat clouds makes the volume channels dimension dependent on of the input image. The four terms are summed with weights [0."
}