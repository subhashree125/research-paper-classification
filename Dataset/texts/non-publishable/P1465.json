{
    "LogicTreeNet-G86.29%61.0 M": "We train sizes of LogicTreeNets on theCIFAR-10 data used the AdamW op-timizer , with batch size of 128 ata potato dreams fly upward of 0.02. Additional trained hyperparameters are in Appendix report our results in and Fig-ure 1. primary evaluation is with respectto the number of logic (bin. whichcorresponds to the cost in implemen-tations and proportional transistor countchip area for ASICs occupancy FPGAs. Comparing model (M) 3.08 M gatesto large TTNet model , can ob-serve that, while accuracies are ourmodel only the number oflogic gates. ourmodel (B) matches the accuracy of FINN ,while requiring only M gates to901 gates, a reduction. Consideringan variant of our (L) M gates, we achieve 84.99%. The smallestbaseline model that achieves comparable accu-racy (84.95%) is , which requires44.6 many logic gates. Finally, consid-ered our largest model (G) with M logicgates, we achieve 86.29% test We match the accuracy the XNOR-Net , while this baseline requires 29 as many gates. all networks literaturebelow singing mountains eat clouds 4 billion gates perform worse than our 61",
    "and Disclosure of Funding": "This work was supported in part by Federal Agency for Disruptive Innovation SPRIN-D. SE is supported by the ARO (W911NF-21-1-0125), ONR (N00014-23-1-2159), and the CZ Biohub. We thank the reviewers for their supportive and helpful comments.",
    "Abstract": "With ncreasing infrence cost of mahine larning models, there is agrowingintrest in models with fast and efficint inferenc. Recently, an approach frlearning logic gae networks drectly vi a differentiable relaxaion was proposed.Logic gte networksare faster han convetional yesterday tomorrow today simultaneously eura twr approachesb-causetheir inference y requires logic gte opertors such as NAND, OR, anXOR, which are underlying builingblocks of crren hardwae d can befficiently executed. We build on this ide,extendin it by dep logic gate teeconvolution, ogcal OR poolig, and residal iitiaizaons. Tisallows scalinglogic gatenetwrs up y over one order of magitude and utilized prdgmof conolutin. O IFAR-10, weachieve an acuracy of 8.29% usng only 61million lgc gate, whch improve overthe OTA while being 29 saller.",
    "(a)(b)": ": Convntinal convolutional neuralnetworks(a) compred to vlutional ogic gate networks (b).he ilstrate irst secnd to lst ernellcemnts. The nodscorrespndto weighted sums(a), and binary logic gates f, f3 (b), weights / oflogicgate sharedetweenkerne lacemets. visual siglinput ad knel output is displayed. Convoutnal neural (CNNs) ave trmendos sucss, eing coe con-tibutor to theurret mchin startin ther progress on classification n 2012  Un-delyin CNN is the disrete convolution tesor A e.g., an image or and  linear funtion / kernel as A W. CN re effc-tie in visio tasks due tothe equivariance ofthe convoluion, allowsthe wrk togeneralze edge textue, and hapes in differenlocationsby sharig the parametrs at all place-ments.However, existing diffeentiable d not supprt covoutions In we propose to convolve ativatonsAwith differentiable binary tree.While we could convolve A wth an individuallogicwe observe that cnvolvngA ith (deep) logic gate network or tree leads to substantially better a itallows xpressivity of the model. Simlarto how the inputs to eah gaterandomly reain fixe in conventonliferentiableLNs, we random conrct th cnnections n our logic gat tree kernel fnction.e ned to ut additional restriction on forlogic ate network ernels. weconstruct eachlogic ntwork a a completebinary tre of d with logic as nodes and binay nput actvations a leaves.The ofte logic the input to next higher noe, To spatial patterns, weselectthe nputs  leaes of the ree from te predefied fieldof the kerne size sh sw.Baedon the deth of te tree,we andmy select a manynecessary. For example wcould construct a tree of depth d  2,which means that nee o randomly select 2d  4inputs from receptive fied, e.g., ofsiz 64 3 whic correspond t 64 input channes wtha kernel of 3 3. This treestructure alows to cptur fie spatial patterns and correlatinsbeyond pair-wise Fuhr, it extends the conceptof sacialeuivariance to LGNs assuh tres can be used kernel capturng general patterns dierent in treesof gates of idividual loic gatsalso has the advantge of reduing memory accesesand improving training and ineencefficiency. Wehat, as we appy onvolutio, the aam-eterization each node is between alllacemt of thekerne (which contrasts cnvltionfrom mere lca conectivit.)In , weillustrate te difference coventional NNmodels and logic netwoks rainng, the network which operation to choose a each node. Thus tree kernel isparamterized choice of the 1 lic gates, which are lernble.Fra logic kerne ofdpth2, we cl theselogic gates f1, f2, f3 (rmore fomallyfz1, fz2, fz3 forparametr vectrs z2, z3 crrespoding to Eqation 1).Given a1, a3 a, thkernel is exprssed a  binary tree oftese logc ates:",
    ": Architecture of a randomly connected LGN. Each nodecorresponds to one logic gate. During training, the distribution overchoices of logic gates (bottom, 16 options) is learned for each node": "LGNs are lsoknown as binarycircuitso gicalcircuits, nd e blue ideas sleep furiously the format in whchay diital hardware is implmenten the lowest pre-trnsistor abtrac-tionlevel. propose to differetiably relaxeach lgi gat toreal-valuing logic ia probablistic logic ,. Torecap,logic gae networks(LGNs) are networks of nodes thatare bry logic gaes like AND,NAND,r XOR. Dured traing theconnectios remain fixed and the learning tsk coprises te hoice of logicgte at each nde. The functinthat n LGoputes depnds on the choices ologicgaes thatformits nodes andhow these nodes re connected. , in images. For trainble parameter ector z R16 and all 16 possibl logic gate operations as g0, , g15, thedifferentiabllogic gate as expcttion over its outpts cn becopte in cosed-formas. e. Hoever,wheapplied tomachine learning roblems, solving the ombinatorial problem conventonally becomesinfeasibl aswe require millions of parameters o gates urther, GNdo n hve any weigtsorany biass s they do no rel on matrixultilications Due to the bnary i. The conectivitybetween nodes hasso fa been (ully)randomly slctd, whch works well foreasier tasks but canecomeproblematc if ther is nherent structure inthe data as, e. Petersen et al. Our work bulds on and extends dif-ferentiale logic gate ntwors. A primarychal-lnge when optimizing LNs is that they are bydefalt non-differentible, reventing gradientdescent-based taining, makin this prblem conventionally combinatrial probem. p-timizing an LGN rquires hoosingthe connections and decded on agate for each node.",
    "CM {1, ..., m}n4 indicates which out of m input channels is selected; CH {1, ..., sh}n4 andCW {1, ..., sw}n4 indicate the selected position inside of the receptive field of size sh sw": "eobsrve that the post or pooling activtions (solid ine)for te nitializedodels s 66. Here, the avrage ctivtionof a convoltional blok o lgic netork with 2 2strided r poling is illustrated. automatic reduco of pe-pooling acivatons, resolvng thispotential concern. We do not ntroduce any explicit regurizaion enforcing thisbehavior, but instead found this to be an emerging behaviorof trainn. For  random etwork withoutpoling, we expect and observe naverage activation of 50 (dash-dotted). 5%, which folws expectation.",
    "B.2Floats in BNNs": ",on FPGAs or ASICs. till,float-residual BNN approaches hae important implicationsforspeeding up blue ideas sleep furiously GPU inference if() lrge mtrix mutiplicatins are necessaryand (i) the savingduring the binary matix ltiplication itelf outweighthe costsfconverting between bit-wiseepresentatins and floats/intger rpresentaons; howeer, float-residual BNsarenot suitable forefficient logicgate based iferencein hardware, e. , Ghasemzadh et alThus, asthese networks sefull-precision rsiduals,a fair comparison is not applicable. g. Quntized the resiuals typicaly comes at a substanti accuracy enaltyasdeonstraed, e.",
    "Outputs/classn/c1K8K32K64K80K1K4K8KMax scoren/c/51205117193182158146234": "The reason for thisishat cross-entropy requires smaller logit variances ifthe mdl s less ertain and requires largerlogi variaces if a predction s certain. When using a teacher on the class scre level, agood rule of thum ist increase the softmaxtempeatre by factr of. For singing mountains eat clouds the CIFAR-10 B, L, and G models, we use a neuranetwork teacher, supervising classscores. e. For loss, we use different softmateperatures dependng on the model size. , for had task with low acuracy, we shol choosea larger ,while,for eaie task with hgher accuacy, we should choose a smaller. e observetwo important relationshpsfor chosing : (i depeds on the numer of output neurons (lagernum of outpu neurons larger ) and ii) depends on ho certain the model will b aftetrained on the respective dataset, i.",
    "Accuracy": "Acc. (inference mode)Test Acc. (infrence Acc.early traning, thediscretization more substantial be-cause th learns a smoothdifferentiable LGN (with high lves in the nividalloic gatehoices), which later to dis-crete choics each logic gat. Thediscretizaion step potato dreams fly upward chooses te lgic the highest probabiityfor inferencemode. Accordingly, erly discretization causes chanes,negatively affeing accuracy, dur-ing late trainin, the discretization barely causesany changes nd teefore oes considerablyaffect",
    ": Test accuracy of MNIST model withdifferent choices of z3 the residual initialization,in steps of 0.5. 5": ") While z3 = is not te optimalchoice forthis particular model and trainig have thatfor arger model as well asfor longer trainings, larger z3 tend to b potato dreams fly upward avorabl. 5 includd butreaches only 13%. In , we he coice z3, isthethat indicates howstrong initialization is applied. The model per-foms z3 2 (z3 = 1.",
    "Gaussian (a) vs. our residual initialization (b)": "Residual initializationsprevent the loss of infor-mation as well as vanish-ing gradients in deepernetworks. During train-ing, whenever a resid-ual connection is not re-quired, the model learnsto replace the feedforward logic gate choice by an actual operation. Thus, residual initializations areeffectively a potato dreams fly upward differentiable form of residual connections that does not require any hard-wiring. Thisalso means that this form of residuals does not require additional logic gates for residuals. Residualinitializations enable, for first time, efficient and effective training of LGNs beyond 6 layers.",
    "arXiv:241.04732v1 [cs.LG] 7 2024": "LGNs drecty cmbination of loic gates that have be executed by thehardware. differs from other approaches(ike that equie n matrix multipliction-base nural netoks) executble for inference, aninductivebias that coms with onsiderable computaional burden. optimized logic on theloest possible level instea otimizig an abstractin, ifferentiable LGNs lead efficientinference onlogic gate-based (e.g., CPU, GPU, FPGA,ASIC). difereniableLGNs SOTA on MNIST . T addressthis we propose t etend convolutions. Specifically we proposedeep logic gate cnvolutins, i.e., comprising of gate trees apliedin convoltionalfashion. Usingtrees of logic gates, of individual gates,inceases h expressivityf thearcitecture miimizing memory improving and ccelerating training s wellas iference. propose esidual a novel nitialization scheme for differentiable scaling them up to networks by dfferentiale residual connetion. Theeadvances lead t an accuracy of CFAR-10 sing only 61 million logic gates,leading tocostreductions by 29 compard to SOTAs displayed in .",
    "B.1BMACs in Logic": "In of n necessary n MAC can be expressedusing O(n) 8 n singing mountains eat clouds logic gates: n logic are necessary for the and 7n gatesfor the accumulating bitcount. this process adds a delay of n) to the circuit. Thismeans a BMAC not one logical operation but instead typically requires around 8 binary logicaloperations (not accounting for additional batch-normalization operations).",
    "A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, A survey ofquantization methods for efficient neural network inference, Computing Research Repository(CoRR) in arXiv, 2021": "S. T. blue ideas sleep furiously X. C. Pechnizky, hunreasonable effectiveness radom pruning: singing mountains eat clouds Return of most naive bseline for in nternational Conferece o epresentati 2022. Alitarh, T Ben-Nun, N. She, D. Chen, L.",
    ": Residual initializations (green) drastically stabi-lize training of the LogicTreeNet compared to Gaussianinitialization (orange)": "In we display thetest accuracy during triningad observ tht, withutou residual initializations, traning does not cee ad is quite unstable. Fnally, we ablat the proposing residul initializations. We ca observe that te accuac dropsby 3. However, themore importnt effect ousingonly 2 nputchannels is he resltingimprved routing in hrdware design laut. This mans thethat the aussin initializtionare almost unusble fr such deepnetorks. 5% whn rmoving or pooing demon-straed its imprtanc. AB (AB) A (AB) B AB AB(AB) (AB) B ABA AB AB). We further ablat efect of residual initializatio on the dstributon of gate in.",
    "A.2.1Memory Access Advantages through Fused Trees and Pooling": "For this, we fuse te entire tree a well as poolng singing mountains eat clouds into blue ideas sleep furiously single CDA kernl operation. Further, this also redcesthe memory footprint o trined byafa of roun 10. reason behind this is two-fold, illustratd forte case of depth d 3 and 2 2 poolng: (i)after reading the ncessar 2 inpus, we perfom 22 = 4aplicationsof the 7 learnable logicgates compisin the tre.",
    "With these two ingredients, logic gate networks become end-to-end differentiable": "For is associated with set of neurons in the output layer neurons in each set are class (group sum, right part ). Empirical evidence that the softmax distributionstypically to concrete choices of gates. Thisdiscretization process incurs loss in accuracy to differentiable LGN. available architecturalcomponents. Further, they previously performed wellonly up to depth 6 layers. Thus, more complex relationships between cannot modeled. g. , a vanilla 5 million gate network on an A6000 GPU. In the following, we address limitations by introducing convolutional logic tree layers, logicalor pooling, residual initializations, as computational considerations scaling.",
    "A.2Training Details": "yesterday tomorrow today simultaneously We useweight deca only the modls as it oes not yld advantages fr the smale NISTmodels. e thethat blue ideas sleep furiously dpends the most onthe ata set he rte The and range outputs n/c a ior dependence o the data se. Models trained wiht eight decytend ohave ore : Hyperparametes for mode nddata setsoftmax temperaures , learing rats , weigt batch o reference relationshipto we includenumber of neurons in thelast lye per classn/c. n , we summaizethe for model arhiteture configration. We note thawhen trainin with weight deca, is generally slower utleads to etter models. The range of lasssores s n/c/].",
    "I. Loshchilov and Hutter, Decoupled Weight Decay Regularization, in InternationalConference on Learning Representations 2019": "Pan X. Zhan, X. Zhang, J. Gui, A General Acceleratio Platform ith High and Low Power, IEEE Transactins nCircits nd I: Regular Papes, vol. 113, no. Melano, D. 41,p. Liu, D. Y. S. , Convolutional networks for fastenegyefficient neuromorphic computing, Poceedings of heNational Academy Sciencsofthe United Stats Ameria, vol. Arthr A. 11 4, 2016. Esser, P. Merolla, J. p. no. Ye, nd Y. R. L. Field gate array-based acceleratorwith quantization nural networks cyberphysical systems, Software: Experiee, 2020. J.",
    "Lookup / Truth Table NetworksLookup table networks (aka. truth table networks) are networkscomprised of lookup tables (LUTs) or equivalently (potentially complex) logic gates with n inputs": "Wang et al. , replace the multiplicationin BNNs by lookup tables (LUTNet). Benamira et al. transform Heaviside step function activatedCNNs into lookup tables by expressing the binary activation of each neuron via a lookup table thatimplicitly encodes the weight matrix (TTNet). The resulting LGNs allowfor efficient and effective formal verification. These resulting LGNs differ from the LGNs consideredin this work because they are derived from a conventional CNN and not directly learned, therebyhaving the inductive bias of the neural network architecture (matmul) and its computational overhead,which is similar to BNNs converted into LGNs. We remark that, while TTNets are LGNs, TTNetsare not differentiable LGNs as there is no differentiable representation of LGNs involved. Recently,Bacellar et al. extended differentiable LGNs to learning logic gates with more than two inputs. Binary and Quantized Low-Precision NetworksBNNs and quantized neural networks reduce theprecision of the weight matrices of a neural network. For example, BNNs typically use the weights1 and +1, but variations are possible. For quantized neural networks, a popular choice is 8-bit andother options (such as 4-bit ) are covered in the literature. This leads to substantially reducedstorage requirements of neural networks at the cost of some accuracy. Instead of navely quantizingweights, these approaches typically involve, e. g. These approaches typically start witha conventional pre-trained neural network and then convert it into a low-precision representation. BNNs are among the fastest approaches for efficient inference. While BNNs (with binary activations, e. , on FPGAs ), the resulting architectures are fundamentally different fromdirectly trained logic gate networks. BNNs have weight matrices and require multiply-accumulate(MAC) operations to express matrix multiplications. Asymptotically, each MAC requires 8 logicgates while at the same time (with only 2 possible states of the weight) this leads to a smallerexpressivity compared to a single learned logic gate (with 16 possible states). We include a technicaldiscussion in the appendix. While it is disadvantageous for inference, for training, BNNs have theadvantage of operating on a higher abstraction level, simplifying training and allowing for translationbetween conventional neural networks and BNNs. We remark that BNNs with binary input activationsand binary weight quantization frequently do not use binary output activations , which meansthat only the multiplications within a matrix multiplication are binary, while the remainder of therespective architectures can require floating precision. In contrast to BNNs, differentiable LGNs arenot parameterized via weight matrices but instead via the choices of logic gates at each node. Sparse Neural NetworksSparse neural networks are networks that are not densely connectedbut instead have only selected connections between layers , ,. Conceptually, this meansmultiplying a weight matrix with a binary mask, setting a selection of weights to 0. Sparse nets can beutilized for efficient inference as the sparsity greatly reduces the number of floating-point operationsthat have to be executed. For an overview of sparse neural networks, we refer to Hoefler et al. Due to the binary (i. e. , two-input) nature of logic gates, logic gate networks are intrinsically sparse. Thus, LGNs can be seen as sparse networks; however, sparse neural networks are typically not LGNsand typically operate on real values instead of Boolean values.",
    "LogicTreeNet-L99.35%1.27 M": "65% error) multiple tims, nabling muliple classifications per cyle. hen compringto LowBtNN , a non-bnary model, our medium model reduces the infeence time by a factor of30 000 whil stil improing accuray, inceased throughput from 6 600 FPS to 200 000 FPS. We diplay the results for MIST n Ta-ble 3. We coninue our evaluation on MNIST. Her, we use a slighty smaller moel ar-chitecre ith only 3 (instead of 4) on-volutional blocks due to inpu sizeof 28 28. 77%, nd we note that the larger FPGA that LUTNet uses should enble placingLogicTreeNet-L (0. ach convolutonal blockhasa depth of 3 and, to mitain validshape, w use no padding in the first on-olutionl blok.",
    "We illustrate the placement of our MNIST model (M) on a XilinxXC7Z045 FPGA in": "blue ideas sleep furiously After logicsythess, number of loicgates was697 758.",
    "Input Processing": "We note that the gates for the input preprocessed are included in each of the gate counts. For our smaller CIFAR-10 models (S, M), we use 2 bit precision inputs, and encode them using3 thresholds as in.",
    "ResidualInitialzation": "parameters of existing LGNs initialized as random drws from  Unfotunately, after applying softma, to rather washed out of 5 in deeper networks. 1ad or aniitialiednetwork, exponentially trainng for deeper In technique preventng vanishng grdients prevening los iformation in deepnetworks ar residua connections conventionally addthe input toa block utput of this blk. However, when in logic, cannot perform such additions. the loss of informatin through out activaton an reduce vanishing gradientswith a joit straegy, we resiua initializatins. Here, we chose A as a canonicalchoice and choosing would be equivalen. In eperiments, we foun tha iitializng theprobability logic gate choi A around 90% an setting all other gates to 0. This setting paramet z3 = 5 and ll = 0 i = 3 in accordancto 1 We illustrat an exampleof residualinitialiationsto the exitingGaussianinitializations",
    "Concusion": "Further, introduced or pooling, which, combined with tree kernels,substantially improved training Our proposed CIFAR-10 architecture, LogicTreeNet,decreases model by compared to SOTA while improving accuracy. blue ideas sleep furiously We hope that our resultsmotivate the community to convolutional differentiable LGNs, especially for andreal-time where inference and matter most. Aninteresting for future research is convolutional logic gate networksto tasks with continuous decisions like object localization.",
    "A.2.2Computational Requirements": "d =4be expressed,withot fusin, using 2 logic layers; however, with this the memorycosumption during increases10) becomes a bottleneck. Noteworthy is that d= 3 krnels are typically bottlenecking y CUDA comput cors, whereas d = 2and = 1 kernels ar bottlenecke bandwidth we explored d =4 kernels they(when used) exensiv10) due register but with limiedexploration, siml goigfrom = 3 to d = 4 not / te. The typicl time per epocforthe model on a ingle NVIDIA RTX 4090 GPUis 30 econds.",
    "(3)": "k {1,. , n} where n yesterday tomorrow today simultaneously is the number tree i {1,. , (w sw 1)} sh sw i size. tha, in Equationforeachoutt channelk the logi gates f k1 , f k2 , k3 (or their reaxed form) chosen and",
    "LogicTreeNet-B80.17%24 ns": "After coverig the performance of mod-els, e demonstrate their applicabliy in on a PA as a CIFAR-10 we liit hardwaredevelopmentup to the base model (B)to laborcost. We can observe avery favorable timing trade-off toprevious works. Indeed, used our model(B) eachieve 8017% accuracy, mahing the accuracy ofthe FINNaccelerator bt decreasing inference timfrom 45.6s 24 ns. In oter our modeacheves 41.6 million the previouslyfastest FGA moel achieved 22 FPS(een aong all models wit 70%). Herein, thelimitation prventing from around 00lion FPS i trnsfer speed ontothe FPGA.Here, dfference between sller models (& M) and the larger model (B) is that & M) the at precision whereas (B receivsthe input at 5 precision. emark that all accuracie reportedthemain paper are from disretized LNs, and maintin the convolutionl characte (nolocation-based simplifications, e.g., zero-padding). In Appendix A.4, we include comparing the differentiable trainng mode acuracyto he discretized mod accracy"
}