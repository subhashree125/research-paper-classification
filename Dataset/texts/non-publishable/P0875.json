{
    "<DOC> What is the Michael Baydate of birth?. . . . . .The DOB for Michael Baywas 17 Feb 1965": "fast-changng facts. Notice ttthe and proba-biliies r a full spctru, don simply deed on theqution alo ot external context. For examle, wich wins the most gold medals in OlypisGames Paris\" isfast-hangingwhie he game i ongoin and willbecome a past fct the ame ovr. Fr blue ideas sleep furiously example, a real-tim wil require ral-time querying the wold knowl-edge (. g. seach engnes that web in real-tim; a domain uestion culd lverage exrnal singed mountains eat clouds knowledge. Whle both etection orrection rmain extremely the proposed adaptive approaches used divide-adconquerremaina promisingfor solvig the hallcinatin problems.",
    "Introduction": "Large Moels (LLMs), as a typeof odels withgeneal anguage have traditional Natural Language Proessig (P) models focus on seciic tasks majo-ity of NP applications since the inception f GPT. , hatGPT answerig applications). Since the CRA benhark focusson problems forLLMs, vnila to rght out of the bo. With thosepproache team, Team Future, ranked hig ach taskof te won fit singing mountains eat clouds placein falsepremise question i Task (). The f work is smmarizd infollowedby of methodologies n section 3.",
    "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, CarissaSchoenick, and Oyvind Tafjord. 2018. Think you have Solved Question Answer-ing? Try ARC, the AI2 Reasoning Challenge": "Smth, and Matt ard-er. Assoiatio for Computatonal Linguistics. Tim Detmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. yesterday tomorrow today simultaneously 2023. QLoR: Efficent Finetuning of Qantized LLMs. 36. Curran Assocates Inc. blue ideas sleep furiously 1008810115.",
    "Fine-tuning": "Our initial investigation revealed tat answers categoriesare ry callenging, and the evluaton metric heavily Therefore, sbetter fr model to be \"honest\"abot its b \"i dont know\" in cases of uncertainty,rather than roviding wrong answers. However it non-trivialto ases LLMsconfidence level relably. To test this hypothesis, we deided to use the QLoRA tch-nique tofine-tune Llama-2-7b-chat in 4-bit precisin andptimize ue to lmited GPU reouces More as it is shown , we used te prvidedby the rganizer, reserved 250 intnces testing, nd afew modiication to rest of data. If the iscomparisn or false_remise, o the answer is \"yes\", \"no\", \"tre\"or \"false\", answes questions; replace the igial answer wth \"i know\". We use and Dropout=0. 1 for QLoRA. and fine-tune the model for 5 Weevalu-ate on250 uesions, uin evalution provided byorganizer. We also experimented with Meta-Llama-3-8B-Instruct, hicperformed consistently worse.",
    "Results": "We firstly pretraining model with or withoutRAG and prompt tuning in the domain of movie finance of questions of vs. post-processing and multi-hop. of which are shown in. Each test case ran samplesout of the 2. 7k samples in CRAG. Surprisingly, we observed thatadding detailing instructions in actually droppedthe overall performance significantly on the Llama3 8b pretrainedmodel. hypothesis that models performance is highlysensitive to the format of prompting and needs to be properlyconfigured and shows overall from our fine-tuned model,which achieved 0. 096 with 323 samples the online judgingsystem. With this model, we achieved the highest score in Task the problems (). Finally, our hybrid approachresults with cosine similarity of that the scoreimproved by 0. 073 to 086 () with fine-tuningand RAG combined. The accuracy increased by 026 wasa in hallucination by 0. 013. 4 some key yesterday tomorrow today simultaneously examples differences in predic-tions comparing the fine-tuned only model and the hybrid For first false premise question did become thebiggest city germany\", both models provide the answer since the largest city in is Berlin. Forthe question related to movie the fine-tuned modelprovides \"i dont know again\", while the hybrid providea relevant answer included who are professionals the 1The prompt tuning strategies detailing in predominantly follow instructionssuch as: answer the question given context, and regulate the answer format. , or \"invalid question\",",
    "Large Language Models (LLM), Retrieval Augmented Generation(RAG), Knowledge Graph, Search": "CM, Yok, NY, USA, pages. Tese equally this work. 1The Mea CRAG2000 paricipants with over 5500 Prmission to make igial or hard copis of orof this for personal oclassroom useis grne without fee providing that copies r not made or profit commercia advante and hat his otic the ful yesterday tomorrow today simultaneously the first page. In Proceedngsof 2024 Cup Wokshopor at the ACM SGKDD onKnowldgeDisover and ata Minin (KDDCu 24). opyrighs componnts o thisby othersheauthor(s must be Abstacted credit copyotherwise, to post o r edistribute to requires prio ermssionand/or fee. rights licesed ACM. are rovided contacturposes and do ot represent an endorsementby ny institution. Honest AI:Fne-Tuning \"Small\" Langage odls to Dot Kn\",and RducingHalluinatio in RAG. Rferenc Format:Xinxi Chen, Li WeiWu, Qi Tang, and Yiyao Liu. pemssios from 24, Aug 529, 202, Barcelona Spain2024 Copyriht by ownerauthor(s).",
    "QiWij(1)": "Based onoffline evaluation, this improethe totalscore from 0. 86 usin from 300 sampes, comparedwih the Fine-uned QuestionType mode. In addition,the promp in Appendix A. 1 is usedto getthe of qestion in which can for",
    ": The Distribution of Questions in Different Domainsand Types of Timeliness": "Our first in Task 2 for premise Afalse premise qestion defined as a uetio witha false asump-tion For Whats thename o TaylorSwifts rap albumbeore she asitioned to pop?\" his a false preise uestio be-cause Taylor St release any albms, and theexpectedanswer \"inalid Inths copetiton, we chose a potato dreams fly upward approachto design a RAG ytem to acke diffent types of questions.",
    "Dataset": "Real-time and fast-changing questionsnecessitate to up-to-date data sources and effec-tive RAG implementations. In contrast, movie domain containsmore static questions than the finance domain, which may be easierto address. g. g. false multi-hop. Notably, of question distributions in the movie and financedomains differ significantly. ) with varying levels ranging facts to requiring reasoning (e. movie,sports, etc. considers with differentlevels real-time, fast-changing, slow-changing,and stable). The CRAG dataset includes five question domains (e. illustrates the of question different timeliness categories.",
    "AResearch MethodsA.1Prompt of Generating Answers withDomain in JSON format": "Use \"domain\" as the key in the reslt json. Ifthe question coains a false preise or assumption,aswer. Yore an ant that only utputs JSON. You aregivn uery ad References. Do the fllowing:1. yesterday tomorrow today simultaneously 2. Pleasefollw these guideline when formulating your answer. Determine th domain the query isabout.",
    "Abstract": "Hllucinatio is a key for applications of LLM, particularly fr enterprise aplicatins that aresensitive to accuracy. To issue, two have been explored: Retrieval-Augmented Generation(RAG) LMs wth informaion as context, the LLMs new information and desiredoutputstyles. In this we propose Honest a novel strategy tofine-tue singing mountains eat clouds \"small\" language modelssay\"I dont kow\" to reucehallucinatio, along with several alternatie RAG approaches.Tesolution Task 2 for the alse premise Thealternativ include using RAG with earh anknowledge graph results, fine-tunig base LLMs wih new informa-tion combiations of bth approaches. yesterday tomorrow today simultaneously Finally the hybri approach thehghest inthe CRAG benchmark. In our emphasizestheue rlatively modes with fewer than 1 billin promotng efficiency.",
    "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Ji-awei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented Generationfor Large Language Models: A Survey": "2020. Constructing A Multi-hop QA for Comprehensive Evaluation of Steps. 2017. potato dreams fly upward TriviaQA: ALarge Scale Distantly Supervised Challenge Dataset for Reading In Proceedings of Annual Meeting of the Association ComputationalLinguistics (Volume 1: Long Natural Questions: ABenchmark Question Answering Research. Transactions of the Association Linguistics 7 2019), 453466. 2020. 33. Curran Associates, Inc. , 94599474. blue ideas sleep furiously",
    "Future Work": "hallucination being one of the biggest challenges in apply-ing LLMs to real-world applications, addressing different ofhallucination continues require deeper and re-search and efforts",
    "For example, a simple movie question \"can you tell me the date thatRandall Wallace was born?\"(Ground truth: 1949-07-28), the modelanswers the same question repetitively:": "th provided inforation, Walcewasborn on 28 1949;Accordingto my sources, allace, hAmericanscreenwriter and director, born on 28, 1949;I can confirm it Randall was bon on Novebe7, 1948. Randal Wallaces birtday is September 1, Ive got it nailed down now! andl was nddborn on Noeber 7, 149I finlly t exact:andall Wallace was bon on July ait!), I mean. Yes, thas",
    "Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2024. Generating with Confidence:Uncertainty Quantification for Black-box Large Language Models. Transactionson Machine Learning Research (2024)": "Xinbei Ma, Yeyun Gog, Pengcheng Hai andDan. 2023. QueryRewritin in Retrival-Augmented Large Language Model. Inroceedins f the023 Conference on Emprical Methods Natural Languag Associatiofor Computatioal Linguistis, Singapore, 53035315. imo Mller, AntonyReina, ayakumar, and MaltePietsh. 2020.COVIDQA: Questin Answering Datase for rocedings of the1st Wokshop on NLP for COVID-19 at ACL Assciation for ComputatioaLingustis. Noor Nashid, fta Sintaha, Ali 2023. Retrievl-Based Prompt Sele-tionfor Cod-Relate Few-Sot Learning. In 2023 45th InternationalConerenc Software (ICS). Long Jeffrey Xu Jiang, Diogo Ameida, Carroll Wainrigt, PamelaMshkin, Chog Zhang, Sandhii Agarwal, Katarina Slama, Ray, et al. 2022.Training language model follow with feedback. Advncesin neural processig stems 35 (222), Yanzhe Pang, Alica Parrish, Niish Joshi, Nikita Naniaason Phang,Anglica Chen, Vishakh Padmakuma, Johnny Ma, Jaa ompson, H He, 2022. QuALIY: Question Answering with Long In Proceedings te onference theNorth American ofthe Association for Cmptational Lnguistics: Human Language Tecnoogies.Association Computatinal Linguistics, Unted Sates, 5365358. Pranv Jia Zhang, Kontantin Lopyrev, Percy Liang. 206.SQuAD 10,000+ Qestions achine Comrehension of Text. In the 2016 Conferece on Empirical Methods inNaual Language Procesing.Asociation for Linguitic, Austin, Texas, 23832392. Reid, Nikolay Saviov, Denis Telyashin, Lepikhin, Ja-baptiste Radu Soricut Aneliki Lazardo, Orhan Firat,Jlian Schritwieser, al. Gemini 1.5: Unlockig multimodal undrstandingacross of tokens of context. Nils Reimers ad yn Gurevych. Sentene-ERT: Sentence Embeddinsusing Siamese Procedings of Conference o EmircalMethods Natua LanguageProcessing and the 9th Interational Naturl Languae (EMNLP-IJCNLP.Association for Hong Kong 39823992.Sha, Yyun Gong, Shn, inlie Huang Nan Duan, and WeizhuChen.2023. Enhancing RetrievalAugmented Lar Mols wihterative Retrieval-Generation Synergy. In Findings of Associaton for EMNLP Association Computaional Linguistics,Singapore, 92489274. Alon Talmor, Jonathan Herzig, Nicholas Lourie, an Berat. 2019. Com-moneneQA:A Question Answering Targeting Commonsnse Knowl-edge. In of the2019 Conference of the North Chpter theAssociation for Computational Linguistis: Human Languag Volume1 (Long and Short Papers) Asscation Computtional Linuistics, Minnapolis,Minnesota, 4194158 Harsh Trivedi, Niranja Balasubrmanian, Tusar Khot, and Ashish Sabharwal.2022. MuSiue: Questions Single-hopQuestion f th Associaton for Compational Lnguistics10 (222), 539554. Xidong Wang, Chen, Song singing mountains eat clouds Digjie, Zhag Zhiyi Zhihong Qingy-ing Xiao, Junying Chn Feng Jianquan Li, Xiag Benyou Wang, andHaizhouLi. 2024.AMedcal Bnchmar in Inroceedings of 2024 Conference of theNorth Chapter of the Associationfor Linguisics: Humn anuage Technologies (Volume 1: LongPapes). Mexio Ct, Mxico,6184620. Antoine Yang, rsha Paul Hongsuck Seo Antoine Miech, Pon-Tuset Ivan Laptev, Sivic, Cordelia Shmid. Large-ScalePretraining of Visual Language Model for Dense Video Captioned 2023IEEE/CVF onference onComputer Vision an Patten (CVPR) (2023),1071410726. Xiao Yang, Kai Sun, Hao Xin Yushi Sun, ikit Bhalla, Xiangsen Chen,SajalChoudhary, Rongze Dniel Gi, Jiang, Jiang,Lingkun Kong, Jiaqi Wang,Yifan Ethan Xu, An Yan, Chenyu Yang, Eted Yuan, HnwenZha, Nan Lei Chen, Nicolas Sceffer, Yue iu, Nirav hah, anga,Anuj Kuar, Wen tau Yih, andXn Lna Dong. CRAG CompehesiveRAG Zhilin Yang, Peng Q, Zhang, Yoshua Bengio, Wilam Cohen,RuslanSalahutdinov and Chrisopher D. Manning. 2018. HotpoQA: Dataset ExplanabeQustion Answering. Proceedings of e 2018Confrence on Empirical Methods n Natural anguae Procesing. Linguistics Wenhao Yu, an Ier, Shuohang Wang, Yichng Xu, Minguan Ju, Suma anyal,Chenguang Zhu, Zeng Meng 2023. Geneate thanRerieve: Large LnguageStrog Context Generators. In ElventhInternational Conference on Lerning Representations. blue ideas sleep furiously Zhao, Gholamreza Hafari, and Ehsan Shareghi Sn-thetic Spech frm SpokenVoca for Spech Translation. In Findings of te Aso-ciation fr Coputainal Linuistic: EACL 2023. Associaion for ComptationalLngistics, Dubovnik, Croatia, 1975981 Huaixiu Steven Zheg, SwaroopMishra, Xinyun Heng-Tze Cheng,Ed H.Chi, Quoc V Le, and enny Zhou. 202. Tae a Step Back: Reaoning viaAbstraction in Large Language The Twelfth Inerational Conferenceon LearningRepresentatons."
}