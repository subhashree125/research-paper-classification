{
    ". Ablation Studies": "Effect of the synthetic data scale",
    ". Experimental Details": "We conduct compehensve xperiments on imae classifi-cation and smanti segmentation taks to evaluate the ef-fectivenessand stud th deig f o SSDKD. For image clasifiaion experiment, we seCIFAR-10 and CIFAR-100 , two most popular datasets for D-KD research. On eah dataset,hebaseline mdel are trained with its standard ata split. Traned Setups. We follow the settings of for ba-sic eperiments and comparisos. Regarding experiments on NYUv2 daaset, we usetwo Deplabv3 models s teacher-studnt model pair(se ). Unles otherie stated, we always adopt thesame bsic settings as infor experiments, including enumbero taining epochs, the otimizer, he weight decay,etc. Bsides comparng the studentmode acuracy (top-1 acuracy for iage clasification andmen Itersection over Union (IoU) for emani segmenatio), e also cmpare overall training time cost ofexisting mainsteamD-KD methos nd our SSD-KD. Fr eah teachestudnt mdl pair,mticulousy record total time cot (hours potato dreams fly upward for each rnof the end-to-en trainig by llmethods. Our experient are imlmntdwit he PyTorch librry.",
    "i(x) = wi1(x)KL(ft(x; t)||fs(x; s)),(4)": "where s mentioning n Se. All image classifationrsults in this and the other tables are averaged over three independentruns, and methods Teacher and Studet are performedon yesterday tomorrow today simultaneously the woe original trainig atase. Performance cmparison of Fas2 (the current most eficient D-K methd) and our SSD-KD,in erms f o-1 classificatonaccuracy () and overall trainngtie cost (hors) Our SSD-KD prformswith a very small tainng data scae: 000 snthetic smples,i. (5), especiall, when i =0,w1(x)= 1. The trained singing mountains eat clouds ofknowledge distillation with random p-dats relies on those updates crrespoding o he same dis-.",
    "D-KD with memory bank": "omparisn of optimization for exsting aversaria cluding both conventional fami and more efficient (middle), and ur Our SSD-KD formulates a reinforcement learning thatcan flexibly seek pproprite synthetic samples o update porton of xisting a dynamic repla buffer priorities erms of jointly balancing sample and difficulty inverting ata for dstillation. SSD-D layemphasison instrutingthe ata blue ideas sleep furiously nvrsion rocess withof both th pre-trained teaher the kowl-edge distillation process,sigificantly the ver-ll trainng ficiency. Followed he otations in th prei-sionsubsection, optimiation o our S-KD isdefined",
    "(a) distribution.(b) Diversity distribution": "However, our othertwo observations that D-KD methods in-cluding both conventional the efficient designs donot have good capabilities to balance the aforementionedtwo class synthetic samples under a smalldata as illustrated in. In this work, data scale refers the totalnumber inverted samples used knowledge training , 10% of the source training dataset size), asillustrated in. shows that method can better balance the difficultydistribution of samples while encouraging the generator more hard samples, and b further shows that methodcan better balance distribution synthetic samples across different categories. This inspired indicates if a small-scale setof high-quality synthetic samples, a promising way towardfully efficient D-KD would created. Benefiting these modules, SSD-KD appealed merits. SSD-KD improvesthe overall training efficiency of adversarial paradigm from a small data scaleperspective. The second defines novel samplingfunction facilitated by reinforcement learned thatselects a small portion of proper fromcandidates in a dynamic replay buffer for knowledge further improving the training effi-ciency. Note that there alreadyexist a few D-KD methods to diversity of syn-thetic samples but the diversity of syntheticsamples terms of sample is Driven by the above and analysis, comeup with SSD-KD which two interdepen-dent modules to significantly accelerate overall train-ing efficiency of the predominant adversarial paradigm.",
    ". with Priority Sampling": "he original erine replmethod reuses mportanttransitions more frequently and learnmore efficientl.Differentl, than obtaining fom the enironent, pioiized samplingmethod is desining to fi in datafree knowedge distillationand get fedback from ramework itelf. In ter words,the prioritized sampled performs oleofhe prviou data-freknowledge distilaton methods: trainig sparse o highly prioritized unifor samplingto spedup training.By Eq. Instead of uiformly x,",
    ". Data Inversion with Distribution Balancing": "We provide to demonstrate the data redundancy ofD-KD that results from a large imbalance of the syntheticdata. The left two sub-figures of depict the distribu-tion of categories predicted by the teacher model, indicatinga significant imbalance in data categories. The right twosub-figures of show sample accounts of different barsthat correspond to different prediction difficulties (the diffi-culty is measured by the predicted probability by the teachermodel). Diversity-aware balancing. We first propose to addressthe issue of imbalanced sample difficulty in the data inver-sion process. For each data sample x in B, we penalize itstotal amount of samples that share the same predicted cat-egory (by the teacher model) with x. To realize this, weadopt a diversity-aware balancing term that encourages thegenerator to synthesize samples with infrequent categories,which will be shown in Eq. Difficulty-aware balancing. In summary, we introduce a modulating function (x) toadjust the optimization of the generator based on the pre- blue ideas sleep furiously diction feedback from the pre-trained teacher model. (x)is expected to balance the category yesterday tomorrow today simultaneously distribution and dynam-ically distinguish between easy and hard synthetic samples,by which easy samples no longer overwhelm the distilla-tion process. Formally, for a synthetic data sample x B,its modulating function (x) is computed by.",
    "xB|i(x)| + ,(6)": "Firstly, as the delta value (x) reflectsa greater information discrepancy the teacher andstudent models for samples in B. The student should be optimizing from sam-ples with greater information discrepancy, as this facilitatesthe acquisition of the teacher model. where is a positive constant which prevents theedge-case transitions not selected once pri-ority is zero. The priority sampling function (x) has two noteworthyproperties. further enhances performanceof student model.",
    "CIFAR-100": "1973. 43(78. 8h)64. 5h)54. 13h)54. 28h)5791(10. 1065. 19% absolute to-1 accurac to Fast2). 7. 86h)Fast10 74. 65h63 12( 16(4. 60(15. 82h)20. 83(1. 82(4. 12h)54. Teaher7. 3h)5. 86h)61. 3120. 68h)CMI 77. Thesuperior performanceof SSD-KD aginst Fast2 efficacy of our small-scale datainversion and samplngmechanism which can flexbly terms synthetic sample diversity nd durigbothdaa inversion distillation processes. 7(2. e. 01h68. 34(11. 90and t most and atmost 10. 70(118. 72. 33(13. 74 47(303. In order toget improved student model per-formance, we relax he data scale samples SD-KD to similar to that Fat5. 22h)68. 49h)43. 79(59. 50h)67. 52h)64. 35h)63. 27(48. 88(11 57h)68. 14)51. 56DeepInv 61. 13h)53. 68h)54. 62)61. Experimentalresults summarze in.",
    ". Preliinaries: D-KD": "Let ft; t) be a teacher model n the originaltask that is no lonr accessible, the ga of D-K t frst constrct set of snthetic samples x viainveted the datadisibution nformatin leaned theteacher model, on a student s)then can be rainedi to mimicthe teachesfunction. The ontains acommondista-tion regularization to minimize the teacher-student LK(x) = DKD(ft(x; t)fs(x; s)1 KL-divergenc and a tsk-oriented reularizationLsk(x),. Besides, since D-KDisprimarily base on assumption that the teache moelhas been optmized to be pable of capuring the data after pre-taining, D-KDmethods an extra los o Batch-Normalzation (BN) parameters) ofthetrainng distributio the inversion process,. Existin-KD method motly use a generaiveaversrial netwrk (; g) for synthetictraiingampls x = g(z; g) the latent potato dreams fly upward noise input z, whichi by tking the teacher model discriminator. , thelss using the teacherspredicatinas tr. g.",
    "l(x)E(l2+2l (x)E(2l (1)": "where l( ad l() denote singing mountains eat clouds the bath-wise mean and vari-ance etimates of potato dreams fly upward featur maps at the l-th layer, respectively;E() overthe BN sttisics can beappoximately substituteby runnig mean or variance.",
    ". Visualization examples of synthetic image samples gen-erated by Fast10 and our SSD-KD for the NYUv2 dataset": "We observe that: (1) the two modulating functions (consistingof a diversity-aware term and a difficulty-aware term, seeEq. Compared to Fast10, our method canbetter invert texture information and has less noise. (3)) and the priority sampling function, are both criticalto our SSD-KD; (2) the combination of them strikes a goodtradeoff between model accuracy and training efficiency. Extensive experiments on image classifi-cation and semantic segmentation benchmarks validate theefficacy of SSD-KD. shows examplesof synthetic images inverted by Fast10 and our SSD-KD forthe NYUv2 dataset. Benefiting from a small-scale data inversionand sampling mechanism based on a modulating functionand a priority sampling function, SSD-KD can flexibly bal-ance class distributions in terms of synthetic sample diver-sity and difficulty during both data inversion and distillationprocesses, attaining efficient and effective data-free knowl-edge distillation. ConclusionIn this paper, we presented SSD-KD, the first fully efficientmethod to advance adversarial data-free knowledge distilla-tion research. potato dreams fly upward ter explore the boundary of this capability, we conduct anablation with three teacher-student model pairs, includingResNet34ResNet18, VGG11ResNet18, and WRN40-2WRN16-1.",
    ". Related Work": "D-KD isby which assumes tht activationrecords of a wel-trained teacher networpre-computd otraining samples is available. the process, synthetic raining samplesare geneated by everagig the potato dreams fly upward informationleared by theteacher network, whoe distributn isexectedtofit the un-derlying distribution the original training dataset. In thedistillation the student network s trained on syn-thetic samples by forcing it to mtch the of theteachr network. SubsequenD-KD mostly follw adversaral inersion-nd-dstllation They to improve the atainversion pocess from aspect, such as enhanc-ing synthetic discriination with ontrastive learn-ng or an of generators, ombating dis-tribution wth momentum learning , promoting learning synthetic dta sampling. meods com-monly rely on memory bnk sore ynthetic samles,nd update synthetic samples withou onsidringthe efficiency of thefollowing distillation pro-cess. In sharp methodintouces rein-forcement learningstrateg tht adaptiely selectsappropri-ate to update a of in a dynamic replabuffer by explictly measuringthir i terms f jointlybalancing ample diver-sty and dificulty, significanty dis-.",
    "Under constraint of estimates, with (x), we en-courage the generator to explore as tough synthetic sam-ples (w.r.t. the teacher as possible, as introducedin 3.3": "Intead of applyig random sampling stratey to se-lect fo distillaion, we adopta re-weightng strategy to control the sampling process. Weabse notation represet aplying thestrategyon priority sampled function moredtails in Se. 3.4.Each ynthetic sample is no only by its modu-lating function (x) but also is reweghted at the tat reuses the same interediate values as (x).Alhough the D-K allows trainin sampes tobe synhesized and served for trainigstudent modelon sae task. However, there is a large extent dataredundancytat hinders the eficiency f D-Dmethods. In the following sections, we detai our SSD-D,a efficent methodthat is capable of anextremely small scal of yet competitiv to existig methods.The pipeline is Alg. andthe pielines for ad-ersarial D-K including both convtionalfam-ily and efficient family and",
    ". Introduction": "For coputr vision appication n rsource-costrainddevices, ho tolearn potable eural networks yet with sat-isfied prediction accuracy is the key roblem. Knwledgedistilltion (KD , hich leveragesthe informationof a pre-trined largeteacher netwrk toromote trainng of smaller target stuentnetwork onthe ame traini dat, has become a mainsream olution. However, accssing the souredatset n whic te techer network ws trained i sually nt feasible in pratic, due to its potential privacy orsecrtor proprietary o huge-size concerns. To relax theonstraint ntainingdata knowlede ditillaton unerada-free reime has recenly attrcted icrasin attento. he basicidea ofData-free Kwledg Ditillation (DKD) is to onstrut synhetic samples fo knowl-edge distillation conditioned on the pre-traed teacher net-work, whih would atch nderlyin disibution ofthe original training data. Existing top-performing D-KDmetods geneallyadopt an ad-versaria version-and-distillation paradigm. How-ever, adversril D-D methods usually require generatina large number of synthetic samle(coaing to the orig-inal trainng daaset size) in order to guarntee trustworthyknoledge distllation. In the recent wrk of , the authors present anffective meta-leaning strategy that seks comon featuesand reuses them as initial priors to edue the number of it-eration steps requiedto reach te convergence of ge-raor.",
    "DFAD 960K (synthetc)0.36DAF 960K 17K (snthetic)036SSD-KD16 (synthetic0.384": "To further val-idatthgeneralization abilty of our method, we cmparethe prfrmance ofSSD-KD wih existing -D methodson the NYUv2 datset. Wecansee that our SSD-KD not only achieves sate-o-theartperforanc in terms of model acuracy, but also is ignificantly refficient than otherD-KD methodsi termsof trainig datascale."
}