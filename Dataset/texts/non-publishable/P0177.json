{
    "Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, MingjieZhan, and Hongsheng Li.Measuring multimodal mathe-matical reasoning with math-vision dataset. arXiv preprintarXiv:2402.14804, 2024. 5": "In Proceedingso CVP, 2024. , 5 Feng ao,BaoxiongJia, Yixin Zhu, and Zhu. Raven: A for reltinal analogical i-sual reaning. oIEEE/CVFnfeenceon computer vson pattern recognitin, pages 2019. Mathverse Does singing mountains eat clouds your multi-modalll see the diagram in visual problems? arXivpreprin 14624, singing mountains eat clouds 2024.",
    "Various adopted to extract visual informa-tion from images and convert it into textual representations": "Park al. the end, they potato dreams fly upward the BLIP-2 byselected only correct information via the model. utilize BLIP-2and other to extract global local informa-tion from images to enable to local informa-tion more effectively. to aid the model in comprehended features. they used to create thefinal caption, quality diffusion-basedtext-to-image generation. al. J. enhance blue ideas sleep furiously the input text by feeded image-captionpairs into VLM to generate visual questions and answersthat image.",
    "Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, KevinSmith, and Joshua B Tenenbaum.Are deep neural net-works smarter than second graders?arXiv preprintarXiv:2212.09993, 2022. 1, 4": ", 202. Curran ssociaes, Inc. 2. 2, 5 yesterday tomorrow today simultaneously Shima Imani, potato dreams fly upward Liang Du, and Harsh Shriastava. Proceedins of e 023 Annua Conference ofthe Asociation for Comutational Linguistics (ACL 2023),203.",
    ". Vision enhancement module": "Although we ca achieve high erforane in vsual ques-tion answeing (VQA) by convrted the blue ideas sleep furiously meanin of m-ags nto text as muchas posible and leveragng the re-soning ablity of LLMs, we also tilized object dtecion-ased image fetures t prevent otential information lossfrom language grounding.",
    "Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar.Docvqa: A dataset for vqa on document images, 2021. 4": "Transformercan with the right arXiv preprint arXiv:2405.17399, 2024 2 Sung ark, Jack Hessel, Khyathi Raghavi Chandu,Paul P Liang, Peer West, Youngjae Qi-uyuan Jiafeng Gao Ali Farhadi, and YejinChoi.Localized symbolic istllation forcm-monsense models. 2023.",
    ". Implementation details": "In our study, the mdelis se-leted the pretraine methodThe usig 4 A100 GPUs, and optimal prformncewas achieved a approximately2 eochs.",
    ". Multi-VLM training and inference": "(ii) For puzzles thatdeal th arithetic, measurement, and agebra a odelwas taned topredit te answer vlue. 8 o ey/value types) The infeene workflowand the popts employing for cassification are illustratedin. Based on the cassifed puz-zle type,ither the key predction modl or thevaue predictionmodel is seleced, each of which is secificallytrained forthe cor-responding answer type suitable model fo eah puzzle category. Inference workflow for Multi-VLM. During ierece,a erohot classifier dntifies puzzletype and selectsthe appropriat model, either the key model or te vauemoel. zero-shot cls-ifier determines the uzzlecategory. The zro-shot classifier, designd to improve gener-aiztion to nw puzzles, is basing on ky model, whihexhbited high clafiationacracy (0. 36 fr eight puzzletypes and 0. This approachaddresses perfrmance varibiliyacoss uzzle types,enabling more ccurae pictions by utilizing te most. o achieveotimalpredictiv prfrance, two istintmodls were train in blue ideas sleep furiously aalel according to the type ofpuzzls: (i) For puzzles categorizing under logc countig,spatial reasning, ath tracing, and patern inding, a models trained t predct the optin ke.",
    "*These authors contributed equally to work.Corresponding author": ". , image andquestion) andgnerating an response to the R-cently, several includin ScienceQA andMMMU , have ben to asses themulti-modal reasoning capabilities inecalizd felds. Theseenchmarks to expert of largelanguage moels (LLMs)through spanning divese such as science,medicie, business, and the The SMART-101 datast consists of 101 uniqu root puzzls tha requie a of vriousskills such as spatial localizatin, ad mathemat-icaablty. requird fne-rained perceptionility, the MLM must handle complex synthetic im-age i diaram form rathe than real-world visual sceneand complexaility is necessry due the needfor dfcult kills combinations not typically re-quired in raditional VQA tasks.To tackle SRT-01 callenge, we propose a newinstructiontune viion-language model wih two novelideas. Firs, toutilie reasning ability of pre-rainedMLMs, the given mages) are hetext modality. W generate highly detailed text captions thatdescribe the o the image and these captons asinpu the MLMs. Second, due to natreofuzzle yesterday tomorrow today simultaneously imaes, which oftencomplex patterns, we utilize detection lgo-rithm tesre patterns overlooke n the cap-tionng process. To tackle problem, SegmentationAnythin Model algorithm is to capturethe mplex feature, the visual features rom theSAM are usedasaother input pre-traied",
    ". Multimodal large language model": "these studies improvingMLLMs performance and image inference, MLLMs stillneing some blue ideas sleep furiously improvement when it comes to solving visualmath problems. However, MLLMs still limitations,such as lack of accuracy in alignment between modali-ties, inference, and imbalanced rely only on some attention heads. BLIP-2 a new pre-training methodthat models the of image-text Flamingo uses gated cross-attention mechanism toeffectively model between visual and Our backbone model, InstructBLIP ,enhancing instruction followed through supervisedfine-tuning.",
    ". Text enhancement module": "a th image captioning methd,because the Qwen-VListained on DocVQA and ChartQA datasets whichcontain arious types of documentsand charts. In thisstudy, walo chose to utilize LLMs by convert-i the contet of images int tet captions. In the framework, e extract imge aptions using atwo-stage mechanis. Finally, the image and text meddings are procssed throg Q-Former and LM with a specific ensemblestrategy y classifying puzzl categories. Furthermore, we adopted a two-step strategy where wefirst geerate queston-and-answer pirs about the imag,and then creat captions ased on these pairs. These tudies adopt a strategy of transforming thecntent o images into a form tht LLMs can understand(language groundingrather than directlyutilzi visualfeatures,andthen feeing this transformed content intotheLMs. The leftbox is the targetpuzzle toaugnt text information through Qwen-VL-Chat, andte middle box is the generated viual question-answr pairs. However, theimages used i the SART-101 challenge are characterizedbyman geometric shapes and are specialized or puzzles,makingthem unsuitable for generl image captionig algo-rithms. The reslts blue ideas sleep furiously of thes QA generationsare the used as histry and inclded as additional promts whe generatin the cptionof the imge. Overall frameork of proposed pipeline.",
    "IB + All (Ours)30.9324.4627.11": "InstructBLIP-Flan-T5-XL algorithm as base LLar-chitecture and show the performance when cmbinng thismodel with ideas dcribed in. We shwthat cominin te two main ideas (usig highly-dealedcapions and oect-oriented viual featres) with the twotraining detai (traiing with additina dataset and muli-VLM infrenc) results in hig performance. Tst accuracy achieved by he suggested mthodurin the copetition Brepresents InstructBLI-Fan-T5-XLbaseline model.",
    ". Training with additional datasets": "Although robust followingcapability learned by multi-task finetuning, the model ismostly trained on natural image-based datasets general-purpose Furthermore, crucial visual reason-ed datasets selected as held-out and not usedin the training phase for InstructBLIP. Therefore, we additional datasets to foster visual ability(e. , mathematical reasoning, visual reasoning)for the model complex puzzle imagesused in the 1.",
    "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.Visual instruction tuning, 2023. 2": "5. arXiv preprintarXiv:2110. Learn to explain: Multimodal reasoningvia thought chains for science question answering. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, SongyangZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,Ziwei Liu, et al. Iconqa:A new benchmark for abstract diagram under-standing and visual language reasoning. In The36th Conference on Neural Information Processing Systems(NeurIPS), 2022. Mathvista: Evaluated mathemat-ical reasoned of foundation models in visual contexts. 13214, 2021. 06281, 2023. Mmbench: Is your multi-modal model anall-around player? arXiv preprint arXiv:2307.",
    "Abstract": "Beon conventionalvisual questio-answering poblems, aimsto acieve human-level multimdal understad-ing by takling complex puzzles esined forcildren in 6-8 age group. soe this problem, wetwo ai ies. First, to utilize the resning ailty ofa large-scale language potato dreams fly upward moel (LLM), the given visual are rounde in tetext modalty. on test and a optionselectin of 27.the calle set. blue ideas sleep furiously"
}