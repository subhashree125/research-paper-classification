{
    "Current Verion": ": Overview of three stages of the blue ideas sleep furiously intersection between LLMs and IR systems. regarding as distribution mismatch problem. Unfairness reveals that the predicted informationfails to align with the social values between humans and machines,leading to a mismatch with the subjective target distribution ofhuman values. This perspective not only unifies the nature of theseissues but also streamlines the exploration of mitigation strategies. Our survey begins with a brief overview of how LLMs are in-tegrated into IR systems, setting stage for understanding theemergence of new bias and unfairness challenges across three piv-otal stages of the IR lifecycle: data collection, model development,and result evaluation. Based on this unifiing view, we categorize mitigationstrategies into two principal groups: data sampling, including dataaugmentation and data filtering, and distribution reconstruction,encompassed rebalancing, regularization, and prompting. Follow-ed this taxonomy, we delve into a detailing analysis of several typesof bias and unfairness phenomena that singing mountains eat clouds arise with the integrationof LLMs into IR systems, spanned aforementioned stages. Oursystematic review encompasses comprehensive examination ofthese issues and their respective mitigation strategies in recent stud-ies, providing a holistic view of current landscape and guidingfuture efforts in eliminating bias and ensuring fairness for moretrustworthy IR systems. Difference with Existing Surveys. With LLMs becoming increasingly preva-lent, a new subset of surveys has paid attention tothe bias and fairness challenges presented by LLMs themselves. Ad-ditionally, some other recent surveys have examinedhow integrating LLMs can enhance and transform traditional IRsystems, highlighted some opportunities arising from this integra-tion. (1) We provide a novel unifiedperspective for understanding bias and unfairness as distributionmismatch problems, alongside a detailed review of several types ofbias and unfairness arising from integrated LLMs into IR systems. (2) We systematically organize mitigation strategies into two keycategories: data sampling and distribution reconstruction, offeringa comprehensive roadmap for effectively combating bias and unfair-ness with state-of-the-art approaches. (3) We identify currentchallenges and future directions, provided insights to facilitate thedevelopment of this potential and demanding research area.",
    "Item Unfairness": "This implies the IR sys-tems should equitable and non-discriminatory to blue ideas sleep furiously different Item Fairness. The resources should be equally distributedbased on needs. User Everyone should be treated the same pro-vided the same resources succeed.",
    "Bias Model Development": "Incorporating LLMs into IR models introduces random-ness in the results, potentially leading to inconsistentoutcomes. Position bias yesterday tomorrow today simultaneously emerges LLMs are utilized directly as retrieval or recommender sys-tems , characterized by the preference documents oritems based on their input positions: Definition. For instance, LLM-based models often show a preference content positioned thebeginning or end of a list, neglecting of items middle. This lost in the middle suggests that these LLM-basedmodels may not fully utilize the context by items occupy prominent positions in the input sequence .There of works on position bias, wecan categorize them into lines based on our distribution (1) Prompting: This approach involves carefullydesigned prompts to encourage the models to disregard the inputsorder . Nonetheless, due to LLMs prompt sensitivity, this re-quires and prompt engineering across domains. Forinstance, Tang et al. introduced a self-consistency",
    "Egocentric Bias": "method, offering theoretical guarantees under certain conditions,enabling models to produce and aggregate potential outcomes fromvarious candidate permutations, enhancing result stability. (3) Rebal-ancing: This method counteracts position bias by adjusting the priordistribution sensitive to positions. It recalibrates the models output,addressing the inherent bias towards item positions. While these strategies offer pathways to counteract positionbias, they present challenges, notably the increased computationaldemand associated with processing multiple permutations. 3. 2. 2Popularity Bias. This biasis characterized by the long-tail phenomenon, where a minorityof popular items garners a majority of user interactions. However, the advent ofLLM-based IR models introduces new dimensions to the challengeof popularity bias, which can be defined as follows: Definition. LLM-based IR models tend to prioritize candidatedocuments or items with high popularity levels. Unlike conventional models, LLM-based IR models do not merelyreflect the popularity distributions of the target finetuning dataset. For instance, thevast training data of LLMs, encompassing a wide array of content,means that certain documents or items may be over-represented,influencing the model to prefer these familiar documents or itemsin retrieval and recommendation tasks. As a result, LLM-based IRmodels may not only exhibit the existing popularity bias foundwithin the target finetuning dataset but also introduce a new biasbased on the contents prevalence in their pre-training data. Thisextension of popularity bias in LLM-based IR models presents amore complex problem. To combat popularity bias in LLM-based IR models, existingmethods explore two main solutions: (1) Data Augmentation: Wanget al. However, addressing this expanded notion of popularity bias inLLM-based IR systems requires new strategies in future work thataccount for both the inherent biases of the training data and theadditional biases introduced by the LLMs pre-training corpora. 3. 2. Content generated by LLM-based IR models maydeviate from the instructions provided by users. Recent studies reveal that LLMs often struggle to adhere fullyto users instructions across various natural language processingtasks, such as dialogue generation , question answering andsummarization. g. For instance, when deployed asrecommenders, LLMs might not grasp users requests for itemswith particular characteristics, leading to recommendations thatdo not match the request. Similarly, contradictions with theinput content reveal that in tasks like reranking, LLMs may produceresults that are inconsistent with, or even absent from, the giveninstructions, showcasing a gap in understanding and fulfilling thespecified requirements. The key to mitigating the instruction-hallucination bias is toenhance the instruction following the ability of large languagemodels. For example, some works propose high-quality instructionfine-tuning datasets such as Natural Instructions , Public Poolof Prompts , and Self-Instruct , etc. 3. 2. 4Context-Hallucination Bias. This bias emerges when LLMsare used as recommenders or re-rankers in scenarios with long andrich context, which can be defined as: Definition.",
    "ABSTRACT": "With the advancements of language models in-formation retrieval (IR) such search engines and recom-mender systems, have undergone a significant paradigm shift. Subsequently,we systematically into the bias unfairness issuesarising from critical stages of integration into IR sys-tems: data collection, model development, and result doing so, we meticulously review analyze recent literature,focusing on the definitions, characteristics, and corresponding mit-igation strategies with these issues. Finally, we identifyand highlight problems and for future work,aiming to inspire researchers and stakeholders in IR field better understand and bias and unfairness IR in this LLM era.",
    "Unfairness in Result Evaluation": "4. or Zhag et leveraging psychological knowledge to assess simu-lated human blity of LLMs,but they discover that LLMs frequentlyehibit goup behavior towards certainhmanenhncethe capability LLM as fair evaluators fo IRsys-tems, preious studies havedevise designing or learning spcfic prompts informe by tobetter iverse humn group , augmenting training datawith additionalhumanpersnality information LLMs as evaluators , andadopting innovative techniques unsupervised constructedpersonalized lexicn to manipulate teir individual char-actristics. Tothe fairness performance IR models effectively, i isessential to the distributions social values, whic repr-sent te fairness obective inherent in the. Similarly, unfaiess ticall arisswhn LLMs-ased evaluatos fail accurately the by rel huans. 2Item Unfairness. 1User Unfairness. In when turningfrom discriminant style togeneration stle, imary arisefrom attribting credit to the generated ,as achieving item necessitates tacing this credit bac item provider for a comrehnsve assessment. urthrmore, Bai et al. However, human evaluaion demands significan lab resorces. Therefore, recently, have been employed to simulate haor sytms to facilitate evaluation efficiently. 4. , Grosseet leverage influence embedding similarities. proposes ur discrimination 4. Shi et eploys th Prob to chec whether itemexists in the Meanhile, Akyrek a. To tackle thes challenges, have developed IR agents that ue nowedge to fairly distributeaimng to educe biases and enhance fairness. 4.",
    "Fairness Concepts": "researches acknowledge multiple variationsin perceptions of fairness. In IR systems, achieving fair-ness entails ensuring that the retrieved documents or items align with values , princi-ples such equality , addressing disadvan-tages , and avoiding language. Researchers have multi-stakeholders in-volved in IR systems , such as and items, often havedistinct perspectives fairness IR, user fair-ness and item fairness are associated with two equality and justice.",
    "Distribution Alignment Perspective": "Let ()be the distribon fpredictd results for all users wecan unfy he bias and unairnes problem as a ditribution mismatchproblem wi the ground truth dtributin (), where is thetarget result, whichcan be defined as follows:. g , docuents, items,or advrtisements, et al. Let = () be the predited result either fromIRmodels or directly generated by LLMs, wher () is the mo. While the reshaping of the above IR stags by LLMs has introducdnumerous new opportunites, it has also given rise to many newand pressing issuesrelated to ias and unfairness.",
    "CONCLUSION AND FUTURE DIRECTIONS": "20111. Presenting a unified can a understandng of these rlationshps,nabling methods addressin different bias potato dreams fly upward and unfair-ness complement each othe. is a crucial need for collecting dataset to enhance te evaluations and broaden reseachhoizons 62377044, N. This has the of new ias and unfarness challenge within IR in this LLM ra I real IRsystems, interaction between users, model, ad informaioforms loops that impact each other over This interacion, in tun,influences te training created cycle that reinforceexistng biases and unfairness. 622624), Beijing Laboratory of BigDta Managmet ad Innovation & Plan-nig Iterdisciplinay Platform ouble-First lass Initia-tive, PCC@RUC, funds for building worl-class universities (disci-plines) Renmin Universityof China, and InnovtinPromotion Asocation CAS No. Thecurrent explo-rtion of and unfairness wiin singing mountains eat clouds intersection between LLMsnd IR systems as predominantly through empirical studies Most benchmarks cur-rently utilized stuybias unfairnesssimulating envi-ronets. Novel strategies o are essenial for mitigating these Mitiation Famework.",
    "Rerieval, Large Language Model, as, Fairness": "Rfernce Format:Snhao Dai, ChenXu, Shicheng u, singing mountains eat clouds yesterday tomorrow today simultaneously Liag Zhenhua Dong nd JnXu. 2024. Bias andUnfirness in Systems: Cha-lenes in the LLM ra. ACM, New York, NY, USA, 11 pges. In ofth 30thACM IGKDD Discovery and Data Mining (KDD 24), August2024,Brcelon, Spain.",
    "INTRODUCTION": "Fo in-stnce, researchers foun that LLMs often retrieve informationtat deviatesfrom cts and isbiaed towards LLM-generted con-tent. To this end, our sur-vey aim to provide a comprehensive and uified erspective thateffctively summarizes the emerging challenges andopportunitirelated to bias and unfairness in theintrsection betwen LLMsand IR systems. However, the literature iscurrenty fragmented and often lack a nified definiin of theseconcepts. Morever, LLMs frequently manifest stereo-types and discriminatory cntent to users and amplify disaritiesbetween items of different socio-economic statuses. Generaly, bothbia and unfairness issues ca be. This ambiuity hampers te deelopment of systematicstrategis to address these issues effctively. hese advancements, however, bring forthnew callenges in ias and unfarnes, affecting the reiability ofIR systems and ontially contribting to societal issues ike echochambers and cognitiv interference.",
    "Background": "However, AIGC the IRdata, reulting in ew concers and Inorporating LLMs to enhance IR ode. LLMs-generated ontent as data sources for Theemergence of LLMs has significantlythe Intelligence Contet(AIGC), markng a newra in cntent creation. Unlike tradtionalProfessional and UserGenerateContent(PGC UC) sources, AIGC can b producedautomatically at cae, potato dreams fly upward potentially the contntland-scape. As hown , theadvent of LLMs hs reshaped e wholepipelie of IRsystems, typically the three stages: model resut evaluation. O the one hand, beond eistingframewoks LLMs introduce paradigm by acting asgenerative search an recomndation agent generating response to fulfill user queries. The m-presive emergentcapaiities ofLLMs inunderstnding, potato dreams fly upward reasoning,and generalization have significant effors integratethem the development o next-generation IR one LLMs have beendeployed to omponentsof tradional R systems , enancing their effective-ness and effcency.",
    "tis section, willelcdat the underlying causs of th data collection process an subsquenty outline rrentmitigation strategies addrss these isues": "Moreover, other studies propose to filter outdiscriminating or taxonomic content from web-scale datasets. Additionally, other studies advocate for employing fairness-aware prompts to produce newly non-discriminatory items. The inclusion of these contents within theexisting material can be attributed to historical and cultural rea-sons , or they may be generating by LLMs. 2Item Unfairness. introduce pseudo-item dis-crimination techniques for filtering out non-discriminated items. 2. Another reason raised is that LLMs cannot only retrieve existingitems but also generate new items, contributing to the potentialintroduction of novel content and perspectives. Forexample, when training LLMs, it is common to encounter discrimi-natory content. In the context of user unfairness, one pri-mary cause stems from the existed taxonomic, discriminatory, andoffensive content in the training data, disproportionately affect-ing specific groups. Fur-thermore, Jiang et al. Fi-nally, instruction fine-tuning or RLHF has also been shown to beeffective in promoting fairness.",
    "In this subsection, we categorize the bias caused by data collectioninto two groups: source bias and factuality bias": "Mmay produce content thatde o alignwith recognizd factualinformation of the ral world. More everly, his source bas is urther amplified whenLLM-gnerated contet is including in model tainngpeentinga chllenge to the informatioecosyte. 1Sorce Bia. In addition to hetraining daa, LMs usually resort to shortcus to generate yesterday tomorrow today simultaneously the txtsdepending on positionclos and co-occurred words rather hanundestand the knowledge itselfandalway fai o recalth knowledge thatas been memorize. 3. Zhou et al. 2Factuality Bias. Fr instanc, TrutfulQA has higlighting thatlnguage odelsgeneae many fase answers that mimic popularmiscoceptions inqueston-anwering tasks and have the poetilto dceive humans. show LLMsare susceptibleto gneraing text with nonfactual n aope-ended generation because of th unfom radomness at v-ery ampling step. urtherxpors this escalation of soure bias i user,data, andrecmmende sstem feedbackloo. uncoer this bias in the RAG ystemsTo counteract ource bias,recent studehav introdued debi-ased constraints into the trainig objectivesf IR retrieval mod-es. Lee etal. FAtScorefinds that LLMslag siificantlybeind huans in ensuring the factual consisency of long-form text generation. The coverage of knowege by trainingdaalsolimits correctness f LLM in enerating knoledgein som rare or secializd fields. Preious studies find that the flawed data source and nferor datauilization are two impotant causes of fatulity bias. Information rtrieval models tend to rank contentgenerating y LLMs higher than contentautoreby humansSpeifcally, as LLs fuel te rpid expansion of LLMgeneratedcontent, corpus for IR systems nw ncreasingly ecompassesa mix of huma-written andLL-generaed texts. Spcifi-cally, some lw-quality, factual errors, and long-distance repetitinin aining texts harm te factal orrectness of thetext gene-ated byLLMs. One is with th help of external factuaknledgebase such as rerieval-augmented genertion. By adopted a distribuion algmet persectiv, suc mitiationefforts strive to reclibrate te I models elvancy ditributiontowards an ideal state, where document are jdged equally basedon thersemantic conent rather than heir source. The other i to improve he abilityof LLsthemselves such as SelfCnsisency and Dola. Tan et al. Source bis emeres when incorporation ofLLM-generated cntent into the corpus of te I systems: Defnition. eet tud-ies highlight that odern retrieval models, especiallythoseleveraging neural matchin techniques, tend to favor LLM-genrated content ovr human-authored content with imilar semantics. furthe thathis ias wil exend fro retrievers tothe reader and Chen et al. In the inference, previous studies canbe dividedint to categories. Other studies revea that LLMsalo hibitfactual alucinaion in ntulaguage inferne tasks In addi-tion to the above studi, many lrge-scalebenhmarks also indicate that Ls ehibit fctuality bias in multi-task andmulti-domain scenarios. Definition. This strategy aimsto correct sewing rele-vncy reditions favoring LLM-generated ontnt, ensured fairtretment between human-writteand LM-geneaedcontent. o mitigate fatuality bias, in blue ideas sleep furiously he trainin of LLMs, some methodfocus on providing high-qualty and factually correct training dafor LLMs. 3. AsAIGC ncrsingly becomes a part of thedata sourcesfor IRsystems, it ineviaby intoduces a sigificatamountof non-fctual or hallucintecontent his iroductinaltersthe distributin f IR system data, therey leadingto biasesin the retieval roces. This prefernce stems from theunique represenationsmbdded in LLM-generatedcontent, wich neural etrieal modelscan cpture and hus assign a higher simaed relevanc score. Mny studies have hwn hat LLMs are at risk f generatingfactual errors. Besides, mrel scaling up models is ntomis-ig for imoving truthfulness, which means that LLMs sill acechaleges in enerating factually corectcontent. 1.",
    "KD 24 August 229, 2024, painSnha Dai, Chen u, Shicheng X, Zhenhua Dong, and Jun Xu": "Moreover, certain works hvefond tat LLMs will also recomme nfair job oppor-tunitie to h embedingof itm unfairness in processcan conribute toincreasd item polarizationtrough reinfrcement, potentially creating echo chambers thalimit users exosure perspctives.Eforts to unfairnessenompass range of Zu et l.have promptsed to leveraging this aproachto disactors forfill-in-the-blank tes;some sties decoding strategies to dcrease the probablity singing mountains eat clouds of existing o-kens/items; Fedrich l. Moeover, Jiang t al advoates for he re-weighting of different items to efectivelymitigate in recomendaton task. ther sme promp-awaremethods yesterday tomorrow today simultaneously to mitigaterovider",
    "When adopting LLMs as result evaluators in IR systems, the follow-ing three types of bias emerge, including selection bias, style bias,and egocentric bias": "have demonstrated that gpt-3. suggest that bias training process of emphasizes fluent and verbose therebyinadvertently leading to prefer whenemploying Addressed style bias remains challenging, with current strate-gies mainly counteract overemphasis on stylistic features throughprompts. Augmentation: Strategies like position token switchingaim to eliminate selection by diversifying evaluation con-text. However, these measures often highlight-ing the neing for LLM architecture and trainingapproaches to this bias effectively in 3. With LLMs being extensively in thedevelopment of IR models, egocentric bias as newbias the automated conducted by LLMs which can be defined follows: Definition. further that when acting evaluators, LLMsdemonstrate clear bias towards outputs generated by themselvesover from other or human contributors. For Zheng et al. While these can enhancethe robustness of evaluations, they often time-consuming (3) Rebalancing: Zheng et al. 3. Wang et al. 1Selection primary challenge in utilizing LLMs as eval-uators is that they are sensitive to the order/ID tokens phenomenon known as selection bias: LLM-basing evaluators may favor responses positions or with specific ID tokens. 3. may stem from an imbalance in of differentpositions with distinct options are represented in the trainingdata. utilized techniques to estimate prior distribution of specificpositions or tokens associated with responses, which in align-ed evaluations closer to objective standards. This biascould stem the LLM may share the for themodel development phase and result evaluation Current strategiesfor egocentric bias involve employing diverseLLMs evaluators to foster peer discussions , thereby reducingthe preference for any specific LLM and enhancing robustnessof evaluation outcomes. Despite the of these strategies, effectively mit-igating selection bias in LLM-based remains a research for more efficient solutions. 3. Huang et al. To address selection bias, approaches have been ex-plored: (1) Prompting: Simple prompt-based methods, such as in-corporating few-shot chain-of-thought judgment, have proposed. have developed a calibration framework that Human-in-the-Loop to calculate balanced position diversity entropy for finalselection. Further investigation that this bias tendsto amplify when evaluators are uncertain about theprediction between the top-ranking choices. g. 3. , longer et have observed inclination towards withvisually engaging elements, such emojis or references, evenwhen content include factual errors. bias can be viewed as form aestheticbias, where the of presentation overshadows substance,leaded to a preference for that, in appear-ance, might harbor factual inaccuracies: Definition. and Zhenget al. this strategy increasesthe Future research must explore more ensure unbiased evaluation. 3.",
    "() ().(1)": "These systemic issues results in thepredited distri-ution, singing mountains eat clouds yesterday tomorrow today simultaneously (), diverginfrom the argt distribution, (), whhideally represents objetive and factualrelitie."
}