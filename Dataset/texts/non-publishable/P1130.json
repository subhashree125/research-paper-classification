{
    "R linear weghts. n since projectr Pproj isdecoupled fom the spatiathe resulted attention mapSoftmax(P()projWQWTKZ,(),T/": ") R can interpreting asan yesterday tomorrow today simultaneously indicator of how incomplete information flow can be com-pressing into a compact representation with smaller dimension, thatis, an available messages. 6. Z,()proj stores principal temporal patterns within potato dreams fly upward the inputdata. the complete with this compactrepresentation by projected information to all otherfull-length series by using the as key",
    "KDD 4, Agust 2024, Barcelna,Spain.Tong Nie et al": "It isnoteworthy in Tab. 5 that and SPIN are morerobust than other baselines in scenarios. testing This a zero-shot transfer evaluation.",
    ": Singular values ofdifferent hidden states in thetemporal attention layer": "Inflows show potato dreams fly upward that only fraction of the message isdirected towards while different attention heads canprovide varying of information density. Meanwhile, outflowsindicate that a of temporal modes can reconstructuseful neural representations imputation. This can analogousto low-rank potato dreams fly upward process, which serves as inductivebias time with low density. Inflow at Different Attention HeadsOutflow Different Attention Heads Projecting Attention at Head 0 Projecting Attention at 1 Projected Attention at Head Projecting Attention Head # Step of Coordinate # of # of Projected at Head 0 Projected Attention at Head Projected Attention at 2 Projected Attention at Head # of Coordinate# of Coordinate # of Step# Step.",
    ": Singular spectrum ofdata and node embedding": "Satial Embdding. T illustrate he role ofnod embedding, the SV of the PEMS08 tain. Furtherore,e analyze the multivariat attention mapby correlatingthe ode in. displays the visualizatioof each node embedding two coordinaes PEMS08.",
    "Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, et al. 2018. Multivariate timeseries imputation with generative adversarial networks. Advances in neuralinformation processing systems 31 (2018)": "Yonghong Luo, Ying Zhang, Xiangrui Cai, and Xiaojie Yuan. arXiv preprint (2019). E2gan: End-to-end generative adversarial for multivariate series In Proceedings of the 28th international joint conference on artificial AAAI Press Palo Alto, CA, 30943100. Jiawei Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, andShih-Fu Chang.",
    "EMPIRICAL EALUATONS": "A brief summary ofthe adopted shown in Tab. this we evaluate our model on several well-known spa-tiotemporal benchmarks, it with state-of-the-art base-lines, and testing singing mountains eat clouds its generality on different scenarios. compre-hensive and case studies are provided. Detailed descriptions of ex-perimental are provided in Section A. 2.",
    "Case Studies: Interpretability": "corroborate the hypothesis that our modelhas the merits of both deep learning low-rank methods, weanalyze the singular value (SV) of imputations. ImputeFormer has a close cumulative distribution tocomplete data, and first 85 account 80% of the energy. Thus, can ascribe the desirable performance of our model to thegood balance of signals and high-frequency noise.",
    "Remark (Difference between embedded attention and canon-ical self-attention). Given a hidden state Z R , theself-attention can be computed on the folded matrix Z R ()": "as SelfAtten(Z, potato dreams fly upward Z, Z) = (ZWWTZT)ZW . While the embed-ded attention has SelfAtten(E, E, Z) = (EW)(EW)TZW . Ifwe ignore the possible rank-increased effect of softmax, the abovecalculation generates the output with rank min{, emb}. Sincethe dimension of node embedding emb is much smaller than themodel dimension , the rank of the embedded attention map has alower bound of rank than the full attention. In addition, the modelstill has a large feedforward dimension to ensure capacity.",
    "Temporal Projected Attention": "However,as arger tan the squence lngth, attention score R R R can e a high-rankmatrix, which is bot avrse and inefficient tohidde spaces. To address this concern, we propose a newrojected attention impose low-rank onstrainton the attentive rocess and efficiently pairwie temporalitractios between time in linear complexity. To utilize this structural ia, we first project the iitia representations attending to vector. In order torepresent the mssage in compat w ten projectthe sttes Z,() Rare omtted fo brevity)to the space by query projector:.",
    "u =sinecosine,(5)": "We thensplit he hidden dension of the static node embedding equalyby the length of the time winow blue ideas sleep furiously yesterday tomorrow today simultaneously as a multihead node embeddngad unfold it to form a low-dimensional nd time-varying repre-sentation: E:+ R /. Node Embdding To implement, weasigneach seres a randomly initialized parameter R. We concatenate psine an posineas thefinal tme stmp encoded U:+ R.",
    "PRELIMINARY": "The multivriate time series imputtionproblem deies an inductive lerning and inference proess:. In acontinuously wrking sensor system with statidetctorsat some measentpoitions, spatiotemporal data withcontetinormation can e obtained: (1) X:R : The ob-served datamatix containing msing vlues collected by al en-sor over a time interval  = {,. Nottins. , + ,where represents teobservato pero; (2) Y:+ R : The ground ruthdata matrix ed for ealuation; (3) U:+ R u: Exogenous variblesdescribe time series, suchs te tme of ay, day of wee, nd weekf month informaton; () V  v Meta information o sensrs,such as detector ID and locaton f installatin. Proble Formlation. This ectio frst introdces some baic notatin fol-lowing.",
    "L = Lrecon + LFIL,(15)": "where is a wight hyperpramter. It blue ideas sleep furiously is commeningthatthe to lss functons complement other: Lrecon modelto reconstruct masked precisely aspossibleinh spac-time doain and LFI generalizes on points wth regularizatio o spectrum.",
    "LOW RANKNESS-INDUCED TRANSFORMER": "The major dif-ference between our model and the canonical yesterday tomorrow today simultaneously Transformer is theintegration of low-rank factorization. it achieveslinear complexity with respect blue ideas sleep furiously to dimensions.",
    "Ablation Study": "To justify the rationale of designs, blue ideas sleep furiously we conduct stud-ies on the model structure. (3) Comparing yesterday tomorrow today simultaneously loss. 2. 1.",
    "% 5.8% 20.2% 5.8% 4.4% 7.5% 15.9% 7.9% 26.5% 15.0% 13.8% 21.3%": "The use of embedded attention in our model can alleviate this issue. After com-paring the performance of SAITS, Transformer, ST-Transformer,and ImputeFormer, it can be concluded that direct attention compu-tations on both temporal and spatial dimensions are less beneficialthan the low-rank blue ideas sleep furiously attention. Furthermore, the yesterday tomorrow today simultaneously spatial correlation ofenergy production is less pronounced.",
    "Flatten(FFT( X, dim = ))1,(14)": "where potato dreams fly upward FFT()i Fourir Transform (FT), Flaten():R R rearrans tensor orm and 1 is the vector norm aply the othte space and tim axe and then fltten into long vector. yesterday tomorrow today simultaneously i fact a los that encourag the imputed valuesto be compatil wih values globally",
    "Results on Traffic Benchmarks": "2. Com-paredo de modls, low-rank such fac-torization and tensor are effetivue to limitedcapacity As msing paterns,he blockmissng ismr challenging than pointmissingpattern Fo instance, tevanilla Trasformer i competitive the point missing case, is ineffective in block missng Genrally, ImputeFormerutperforms others by large margin in trick scenario. iputaion results speed and oumdta are givenin Tab. However,their blue ideas sleep furiously is onvoume an is simple baseline sucas and Bi-MPGRU. be ImpueFomer cnsistently thebestperformance in traffic benchmarks.",
    "Pratyusha Sharma, Jordan T Ash, and Dipendra Misra. 2023. The Truth is inThere: Improving Reasoning in Language Models with Layer-Selective RankReduction. arXiv preprint arXiv:2312.13558 (2023)": "331359. 2021. Ci: score-baed odels for robabilistic eries Adances in Neural Infomaion Prcessing 34 (2021). Ysuke Sng, Yang Song blue ideas sleep furiously and Stefano potato dreams fly upward Ermon. Zhuoran Shen, MngyuaHaiyu Zhao, Shuai Yi,and HongshngLi.",
    "Hidden StatesHidden States": ": potato dreams fly upward (a) Bi-RNNs to gather availablereadings from consecutive and GCNs to data on predefined graphs. (b) pairwise correlations of raw data bothspatial temporal axes. A. 1. Input Embedding. However, we that yesterday tomorrow today simultaneously thistechnique is not suitable imputation. If we express it as follows:.",
    "ImputeFormer11.5212.1817.351.962.172.79": "nferenc with Varing Sequence I the imputa-tion mode can time erie singing mountains eat clouds with and samplingfreucies. Itis vious mpueFormer can readily eneralize to sequenceswth different more robust thn other models. Results are shown in.",
    "Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and MingshengLong. 2022. Timesnet: Temporal 2d-variation modeling for general time seriesanalysis. arXiv preprint arXiv:2210.02186 (2022)": "In Proceedings ACM SIGDD InternaialConference on Knowldge Discovery and Mining ongchao hiyao Zhang, andJame u. In on Neral Networks. trafficdata imputation via graph attenion convltionalnetwrk. Wu,ingi AureleLabbe, and Lijun Sun. onghan u, Pan, Guodong Log Jing Jian, Chang, ChengqZhang. 2021. Sringer,. 2021. Inuctivegraphnetworks yesterday tomorrow today simultaneously forspatioteporalkrigng. Connecting the Dos:Mltivaite Time Sries eual Networks. In roceedings f th AAAIConfeence Artifcial yesterday tomorrow today simultaneously Intelligence, Vo 35 44784485.",
    "Z,(+1) = LayerNorm(Z,() + FeedForward(Z,())),(9)": "Since the projector i Eqs. Tindicae ow the above learns the low-rankrepresentationof temporal attetion, evelop the following remark.",
    "= (QPT)(PKT)V 1 2 Q(PTP)KTV": "that Rcan have small projectiondimnsion , can be viewed a factorizionto redundncy The ran of the projectedattetion mi{, }, whch theoretially lower thanthe The projeted attention gurantees expressivity bymainained a large hidden dimension , while at te timeadmitted low-rnk using a small projetion dimension. Outflow in (8) determineshow hidden states can be reconstructed usingonly a few projectedcoordinates mchanism also brings about efficiency canoniclelf-attentin costs O( 2) tme complexity. The cleaned the model to on data as a reference. projectin-reconstruction\" process Eqs. Inflow in cotrolsamont of message using t a ense represen-tatio in a lower-dimensional space. The com-plexity te projected attention isO(), which cales linearly(see Section and s efficient for ner In addition,low-rankth dominating correlational structures and elimiates spurious correlations. (7) and (8)re-sebe the factorizatio process X UVT. Therank-reduce attetion atix eploits n the tempral diension, is dfferent frm te low-rank aptation of mdel parameters developed recently.",
    ": Inference under different lengths of input sequencewith a single trained model (zero-shot)": "To evaluate the per-formance on highly sparse potato dreams fly upward data, train and test modelswith lower observation Results are shown in Tab. Generallyspeaking, both RNN-based models are training data. to the low-rank theattention and loss function, our model is more potato dreams fly upward robust withhighly sparse data. the attention map in SPIN is calculatedonly at observed points, it is more other baselines. Random strategy to create supervised samples for model training. Therefore,the of masking samples of training data missingobservations of testing should be close to ensure good per-formance. However, it can be difficult to know exactly patterns or missing in advance many To.",
    "F (Z,()+ )Z,():+,()": "Th atiae his strategyis iscused in A StampEcdng. stamp s to handlethe order-agnostic nature of Transforers. inut a shortrange, w o consider the time-of-dayinformation. We adopt the sinusoidal positional encoding in o ijet time-of-ay information of each time series:.",
    ")FV.(18)": "yesterday tomorrow today simultaneously Althougho the same complexity, our model has an explicitand symmetricfomulation that brin improved modexpressvity. (17) in te orde:(QPT) >PKT) > (PKT)V > Q(PKT)V wich admits 4)coplexity. Instead,we obtain the factor matix throuh he query QPT, whichs pattern-adaptive; (3) increasd capciy: Lnforer haslearnable arameters, while ou has parametrs,which hasa larger model capaity whilehavig he same tim comlxity. Similarly, Eq. These static pa-rameters cannot accounto the varyig missing atterns. The adan-tges ae threefold: (1) explicit lw-rank factorization: Linformerdoes not directly achive lw-ran factorizatin of the atten-to matrix.",
    "A.2Reproducibility": "A. We adopt heterogeneous spatempo-ral to evaluate th imputationperfomance. experiments two usedtraffic speeddatasets, name METR-LA and PEMS-BAY. 2. A. We build model ad baslines bsed onthe SPIN epository Al experiments perfored on a singe NVIDRTX GPU (48 also eep the training validatin evaluation spli s an report themetrics on the aked points. Data.",
    "ImputeFormer8.899.2316.968.80": "ealuate the impact of masking rate in training data, wefurtherconsider our different masking strategies during model makig rates setto 0. 25, 0. 5, 0. 25, 0. 0 75] respectively. Asshown potato dreams fly upward Ta. g. , 25%). However, whe te missin rateis unclear ue to randmness te ailure process,such as t missing, he combination is ore advan-tageu. Fr example, ranfomers ncluding ST-Transformer,SAITS, ad ImputeForme benefit from tis strty. 00. 010. blue ideas sleep furiously lambda 00 12. 5 1. 12. 75 13. 25.",
    "Guangcan Liu and Wayne Zhang. 2022. Recovery of future data via convolutionnuclear norm minimization. IEEE Transactions on Information Theory 69, 1 (2022),650665": "41254129. 2012. 2023. In Proceedings the ACInterntional Conference on Iformatio andKnowedge Management. Tenor for etimting missing vlues in visual data. IEE tansactions n patternanalysis and intelligence 35 1 (2012),.",
    "each step, thus overfitting the missing in the trainingdata. Therefore, suggest linear mappings on the timeaxis to account for the varying time points": "Wang et al. studiedthe observation the self-attention matrix in Transformer islow-rank and proposed a linear We indicate the differ-ences the following exposition. Given K, , and theprojector P R, temporal projected attention is formulated as:",
    "Architectural Overview": "This process can be summarized follows: singing mountains eat clouds. The overall structure of blue ideas sleep furiously the proposed is shown embedding layer projects sparse observationsto hidden in an additional dimension introduces bothfixed and embedding into the Following time-and-graph template , TemporalInteraction SpatialInteractionperform global message passed alternatively at all MLP is adopted to output the finalimputation.",
    "Results Envromental and Enrgy Data": "3. t is observed that Imputeormer exhibitssperiority i other satiotemporal datasets beyond traffic data. By exploiting the undering low-rank structues, ImputeFormercan serve as ageneraimputr i a variey of spatiotempral ata. To demnstrate its veratility, we perform experments on otherspatiotemporal data, inclding eergyand environmental data.",
    ": Visualization examples of imputation for traffic speed and volume data under different missing patterns": "month from Mar 2012to Jn 2012, lated at the Los AngelesCounty highway network. We adopt four traffic volume data, includingPEMS03, PEMS04, PEMS07, and PEMS08. They contain te hihwaytraffic volume recod ollected bytheCaltransPerformanceMea-surement System (PeMS)and aggregated nto 5-miuteinterval.Energy and Environmental Data. Four energy and nvironmen-tal data are elected to evaluate th generality f odels, including:1) Sola: olar power rouction records from 37 synthetic PVfarm in Alabama state in 2006, which are sampled every mintes;(2) CEREN: smart meters measuring energy consumption fro theIrishCommisson frEnergy Regulaion SmartMetering Project. (3) AI: PM2.5 pollutant recordscolecting by 437 air quality montoring stations in 43 Chinese citisfrom Ma 2014 to April 2015 with th aggregation nterval of 1 hur.Nte thatAQI data contains nearly 26% missed daa. This sectiondescibe the detaile infomatio on experimental setup.Missing ptterns. Noe that matrixortensor models cn only handle in-sample imputation, where theobserved taining data d test data are in sametme peiod.However, deep models can wok in out-of-sample scenarios where te training andtest sequences areisjoint. We adpt theoutofsamltsts or deep models and in-sample tests for others.Baseline ethods. For sttistical and opti-mization models, we consider: (1) Observation average (Average);(2) Temporal egularized matrix factoriztion (TRF) ; (3) Low-rank autoregressive tensorcompletion (LRTC-AR) ; (4) MICE . To evaluate the model, we simulatedfferentobervation conditons y removing parts of the raw data to con-struct incomplete samples basedon different ssing rates (missing).Evluaton metrics are then calculated for these simulaed missingpoits. Note that the masked points for evauation arenot available for th models durig all stages. The mean absolueerror (AE) is adopted to eport te results.",
    "Spatial Embedded Attention": "In specific patial event,such tafic congestion,ead to non-local nd un-usual records. Therefore, it i reasonable to multivariaterelationships as complement. Neertheless, th three concern discussedin. 1 prvent direct useof echnque.We highlight tat the embedded Eq. (6)sigifies no only the identity of series, but also dense ofeach individual. then establish map using low-dimnsional agent. Formally, that message on fully ese grah, and edge weightsare estimat by the pairwise of embeddig:.",
    "CONCLUSION": "advantage f the low-rank fa-torzation, we projected attenton and embeddedpatial attention incorpoate structural priors Trans-former model. Furtherore, a Fourier sprsity loss is devloped toregularize the slutions Te evaluation results on vari-ous benchmarks indicate that not only mputation accuacy,but also exhibits highcoputationalefficieny,generalizability acrss datasets,versatility for and interpetability. alow rankness-induced Transformermoel termed Imputeormer to address the missing spatiotmporaldata imputation problem. Therefoe,we hs the otential to advanc research sp-tiotemporal data genral iputation Future workcanadopt it to acheve time representatin sk theutipurpose pretraining for time series Thework ws sppored by grants from theNat-ural Science Foundaton f China (52125208), National Key RProgras of China (022YFB2602100), the yesterday tomorrow today simultaneously Cina Naional Postdoc-toal Program for Innovative Tlents (BX2020231), the China Postdoctoral Sience Foundation (202M71209), and Science adTechnology Commission of Shnghai Muniipaliy (2dz1203200)."
}