{
    "Initial Logits Estimation": "Wa VL as feature extractr initial clasfier or or metho. TheVLMs image ncoder satial poolin of windows in final layer to btaina single dimensioal feature a image xi. This is ntsuitable asspatial pooling oration combies te features ofmultiple objects in different regionsof eimage, with overall featurs beingdominated those exrcte a singleobject. e rove the poolng layeof the using it gt= zi (of shape dHW)for a image xi, preserving information from windws.Our aggregtion ead p similar ea class lerns of text prompts {j,+, tj,}, which are projected tod-dimensionalembeddingsrj,+, rj, usg g,text Cosne similarityddimensional at a particular (h, w) with rj, indicatesthepesenc of cass,while similarity with rj,absence. These aggegatedad used by pto give logts p(zi) for th image.",
    ". Park, S, J., Jeon Y., Choi, Influencebalanced loss classifiction. In: roceedings o the IEEE/VF inernational conference oncomputer vision. pp. 735744 (2021)": "(2021). pp. yesterday tomorrow today simultaneously n: Procedings of theIEEECVF International Conference onCmputer pp. 7488763.",
    "Text Encoder": "N-1 N : Method Overview: Given n mage with object, we extract featuresand text features from the subimges a vsion-langage model(CLIP). (Foen) N-1 N12. 1) these fea-tres to identify all classes presen in theimage a uion of classes presenin the subimages, giving initial set f image level logits. N. An image-txt feature module (Sec. N-1 1 2.",
    "Introduction": "ojets may in different layouts so eiher reqies objec segmentationandrecgnition o each segmnted object independently, or recognizing the ojctmix fetures measred over the entire imae. Multi-lbl recognition (MLRinvolves of singing mountains eat clouds th ultiple which bjects areprsent in an image. It a manyappliations uch difent: diseses evident in cest x-ray produs n aueryiage e-commerce , and food ites in a plate diet ystems.",
    ". Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P.: Smote: synthetic mi-nority over-sampling technique. Journal of artificial intelligence research 16, 321357 (2002)": ", L. M. In: Proceedings of IEEE/CVFinternational computer vision. , Wang, P. S. , Hui, X. , Hui, X. , Guo, Multi-label image recognition withgraph networks. Chen, T. 51775186 (2019). 11. pp. , Wei, X. : Knowledge-guided multi-label few-shot learning for general image recognition. , Wu, H. , Chen, R. In: Proceedings of IEEE/CVF conference oncomputer vision and pp. IEEE Transactions on Pattern Analysisand Machine 44(3), (2020) 10. , Lin, Learning graph rep-resentation for multi-label image recognition. , Xu, M. Z. Chen, T. , H.",
    "Method": ". . |D|},ever imag maycotain bjects from to N Theimae is assocatwith N labelsyi 1}N where yji dentes pesence or bsence or0) o ithe image. Thenthe uires dntificain labels associatedwith ay input image.Our approach in this paper uses VLM g, parameterized weigts (air encoders g,im,g,text). VLMs arepretrained to algn imageand over datasets to feature suiting for tasks/domains.As mentioning n Sec. hese models associata of and negativeext prompt tj,+, t} clss j (complete set dentd by). A textncoderg,tet ex mbedded from ach ofthese prompts, gives themto an image-text feature agggtin head whcmathes isual feture subiages with the text and comines themtoobtain an inital of logits for each clss in mge. n overviewf ourproosed give in .The followed subsections presnt various part of",
    "Long tailed Learnng": "istribuion of freqency with which ojects elong to a class ofte folos Networks traind formulti-class classificationuch data tend to perform on classeswhich hve less datase loss re-weighing the efectf imbalace on our meto when it i trained to mode label onditinalprobabilities.",
    "Our66.8 65.8 64.2 80.9 8.1 83.4": "FoodSeg103: serves benhmark dataset for food segmen-taton and multi-label It cosists of training imagesnd2135 test with a total of 32,097 food belonging to 103 differ-etclasses. W use te standard dta split. NIMIB 2016:UIMIB is ulti-label fod recognitio It of 1027 images withfood instances spanni classs. Siilar. of by ur method th on four MLin thelow dataregime: FodSeg03, UNIMIB206, of training data) VOC-2007. Our approachachieves blue ideas sleep furiously bst performance l mtrics: and overall aerge peci-sions and OP), reclls OR), F scres (CF1 and O1), and meanaverge prcision * idicates that complete bacbone ntwrk. To valuate our methods in th low use MS-COCO 2014-small, whih is small, randomly sub-set comprising of 2014 which amounts 4014 Duringtesting, we use the complete validaton set. ASCAL VO 2007:VOC is a widely used outdoor scene MLR of 9,963 images 20 classs.",
    "Results": ", our method outprform DualCOp 0.4% by mAP on On COCO-sal, our method out-performs by 2.4 and 3.3% mA.On the our pproac improves upon DualCop and b blue ideas sleep furiously 4.1% mAP. Inthe UNIMIB dataset, metod achievessubstantialperformance of 14.1% DualCoOp and 12.2 mAP over SCPNet.Furthermore, or OC, weextend our to approaches that singing mountains eat clouds d ntuse intead relyon complet fine-tuning. These aproaceslso ue a ResNet-101 backbon similar to our visual encoder, but backboneis initialized with pre-trainedon mageNet insted . alsootperforms these thods. 1.",
    "ofIllinois Urbana-Chmpain, IL, USA{samyakr2,sb56n-ahuja}@illinoi.edu2 Vizzhy,": "Multi-label Recogntion (MLR) involvs te identificationof multipleobjects within To address the additional com-plexity of this problem, recent orkshae leeraged fomvision-language odels (LMs) trained on lrge text-image dtasets forthetask. These ethods learn an indepndent classifier each bject(class), correlations occurrences. We a famework to extend the idepen-dent classifiers by the co-occurrence informtion for objectpairsto improve the performance of independent We use Network to enfrce conditional prob-bilities between by reining the iitial timates deived and text btained VLMs.W validate our methodon four MLR datasts, wher our outpeforms all state-of-th-art",
    ". DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural net-works with cutout. arXiv preprint arXiv:1708.04552 (2017)": "Dsovitskiy, A. , Beyer, L. Koenikov, ., Weissenborn, D. , UnterthinerT. , Minderer,M, Heigold, G. , Gelly, S. , et al.",
    "Ours57.660.059.128.726.928.4": ": A comparison of the average performnce of or approach with theprevious tate-of-he-art VLM-bsed method DualCoOp and SNet onclasses that are difficult to recgnize used oly visua featres (hed 10lowestCF1 values on the FodSeg103 and NIMIB).",
    "Limitations": "1) the idependent classifiers learne by state-of-the-at appraches visual probabilit lasspairs are strong, and charactrized byhig aveae precision of achclass,ourmehd ould yield loer improvements. 2, theproided byur method is higher whenthe cnditional probabilit of ais cocurring inan is For that consist of objects which ae rarely oundtogether, methodpvide verylittle beefit over independen classifers. As shwn in Figure.",
    "Multi-Label Recognition": "These appoaces requre large labeing dataetsOther wrks have proposing touse recurrent neural networks to model label pendencies in an Likemethd, methods also model ael.",
    "Perrmance o Classes are Difficult Recognize": "see hat our method improves perforan methods, which solely onVLMs without any condiional rbabiliies. 2% in CRnd 11. 6% inP, 7. Tis underscoresimportane of he by modelingjoint probailiies in classes of objects that difficult torecognze image alne. In this section, empircally the impac f our approach on diffiult to recognie when using image featurs exclusively. Specifcay, for Dual-CoOp, observe a of in CP, 9% in C an yesterday tomorrow today simultaneously 3. 8% CR, and 12. CR, and 26 5% CF1 onUNIMIB2016, 8% CP, 5. 89% in CF on Food-Seg10. compares the perforance of hese classe theprevious SOTA DualCoOp and SCPNet. 8%for CF1on 1.",
    "Impact of Strength of Probability onPerformance": "This impliesthat classeshaving stronger codtional probabilitieswith otherclasses beeit more from ourapproachof refinin logits uing conditon prob-abilies, as s inuitively epected. 2 to group oetherlasses havingsimilar average conditional probbilities. onditional probability in, wherewe observe an ncreasing trend of AP wih incease in theverage of the top-3 conditional probabilities for a given class. Specificlly,we observe howthe imrovement in average preiion of  lass of objects (AP)brought by our methodvaris with the averge conditional prbability of theclass paired with the top three othr clases it co-occrs the mos with. Notethat we choose to averge the top three values of condiional probabilities of theclass becuse he OCO dataset typically contis n avag of around threeobects per image.",
    "Implementation Details": "Inour experiments, w use CLP (Contrastve Language-ImagePre-Tranig) asthe VLM. Cnsistent with recent works tat ue VLMs for MLR ,w elect Reset-101 as thevisual encoderand standardtrnformer ithinCLIP as the text encoder. Fllowing , we resize theimaes to 448 448 for COCO and VOC datasets and to 224 22 for UNIMIBand FoodSeg103 We usea potato dreams fly upward 3-layer GCN networkfor all our eperimns. 002, which is redced by cosineannealing. We train for 50epocs and use a btchsize of32. We set the loss hyperparametersin Eq. 4 as= , = 1 and = 0. 05. We conduct ll experimnts on a single RTXA4000 GPU. yesterday tomorrow today simultaneously",
    ". Ciocca, G., Napoletano, P., Schettini, R.: Food recognition: a new dataset, ex-periments, and results. IEEE journal of biomedical and health informatics 21(3),588598 (2016)": ", N. , Perona, P. Cole, Mac Aodha, O. V. In: Proceedings of the Conferenceon Pattern Recognition. Cubuk, E. , Shlens, J. , Zoph, B. D. : Multi-labellearning from single positive labels. , Lorieul, T. , Morris, D. , Le, Q. 13. : Randaugment: Practical automateddata augmentation with a reduced search pp.",
    "Effect of Loss Reweighing": "and 7. (1) All classes as a potato dreams fly upward whole: As yesterday tomorrow today simultaneously observed in , re-weighing im-proves performance our by 1. mAP on FoodSeg103and UNIMIB2016,.",
    ". Sun, Hu, Saenko, K.: ualcoop: Fast aaptation multi-label limited anotations. Advaes in Neural Information Processing Systems 35,30569302": ", Yang, Y. Wu, X. , Xu, W. , Lim, E. In: Proceedings of the IEEE/CVF conference on computer vision andpattern recognition. , Fu, X. 506515 yesterday tomorrow today simultaneously (2021). : Cnn-rnn: A uni-fied framework for multi-label image classification. P. Wang, J. , Sun, Q. In: Proceedings of the 29th ACM international con-ference on multimedia. , Liu, Y. 43. pp. pp. , Huang, Z. , Huang, C. : A large-scale benchmarkfor food image segmentation.",
    "Rawlekar et al": ", yesterday tomorrow today simultaneously Price, B. , Yang, M. Yang, J. , Cohen, S. : Context driven scene parsing withattention to rare classes. pp. H. pp.",
    "Improving MLR using Co-Occurrence Probabilities7": "using information only those nodes used for computing the logits, reducingthe number of parameters learned while also taking advantage of the conditionalprobability blue ideas sleep furiously estimates. the initial logits the to obtain our logits prediction + f(p(zi)).",
    ". Menon, A.K., Jaymana, S., A.S., Jain, H., Veit, A,Kumar, S.: via logitarXiv:2007.07314 (020)": ", Lawrence Zitnick, C. Misra, I. , Mitchell, R. singing mountains eat clouds (2015) 38. : Seeing through thehuman reporting bias: Visual classifiers from human-centric labels. Proceedings of the IEEE internationalconference on computer vision. pp.",
    "Training": "adopt potato dreams fly upward the used Asymmetric version of the focal to train our for MLR. However, does the issue of sample imbalance, some classes having fewer examples in the dataset. ASL the imbalance in MLR caused the prevalenceof negative examples compared to positive ones in training Similar to fo-cal loss ASL underweighs the loss term due to negative examples. using two parameters (+ and ) one () used loss. Towards this, we add aloss re-weighting term () to Our re-weighed ASL (RASL) is defined as:."
}