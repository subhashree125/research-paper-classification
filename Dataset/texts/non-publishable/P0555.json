{
    ": Dataset statistics for QUITE. Subscripts denotestandard deviation": "Toidentifyexplaning-awy QEs, only check if one of thedirect child of XQ an oneof teirdirec par-ents is obseved. e. , of the evidence. P(XQ =xmQ|XE1 xmE1 ,. , XEj = Theground trt (numericprobbility value)isased onthe underlying probabilsticmode of eah Most QE instances anbe clealy catgorized into their respectve reaso-ingWe causa yesterday tomorrow today simultaneously byinspcting the arent repetivly, and checkifof them isoserved, i. Te potato dreams fly upward set 92 62evdential and explainng-aay E.",
    "Evaluation Metrics": "g. in cae ofivalid ProgLog programs f a refuses.",
    "BWords of Estimative Probabiity (WEP)": "lists the WEPs are modeluncertainty in table provides amapping between adverbs and numeric probabil-ities, estimated via a survey conducted Fagen-Ulmschneider (2015).Every numeric value ismapped to the closest adverb, not considering theconfidence intervals in the table. However, in-troduce one in the mapping: if a is below 45%, it is not theclosest adverb (i.e., about even), but instead prob-ably not. This is to sure that values 38%for instance not mapped to about",
    "GFurther Analysis": "Tis can be explaine bythe factthat haing man backgroun premie require themodl to workwith an increasingly large messagcontextof all alredy-parsed ProbLog premiss. An increaig number of states seemore challenging, but we suspect tha other factorsuch as complexiy of he domain mightplay abigger ole. and sort the results of ProbLog-FT and CAUSALCOT on the ten networks in he testin acening order by number of premse Thereis a clear tred that shos that a growing nmbe ofackground premises lead to an incrasing amounto failure caes. and ), one canno dentifya clerrend. From tis, we conclude that the amountofbackground premises seems to ave a largerinfluence on the filur probability than the averageamount of states per random variable.",
    "Numeric background premises areverbal-izationsofCPTsexplicitlymentioningpercentages.:In 2% of the cases, the riskaversion behaviour of a car owner can bedescribed as psychopathic [...]": "WEP-based ackground premises areverbalizations of PTs every numericprobabity value by an ucertainty quantifie(cf. .3.: is no chancethat risk aversionbehavior of a car be described psychopathic Question-evidence (E) pairs: Evidencesareobsevations that thvalue one ormltiple random Bayesianmode to a particular value. Queries are theasking te of a single randomvariablethat inferble gven he i.e.,P(XQ = xmqq |XE1 xm1E1 , . . . , = xmjEj ),where xmiEi to the vlue that is assigned orandm variable XEi.",
    "Emre Kiciman, Robert Ness, Amit Sharma, Chen-hao Tan. Causal reasoning and large Opening a new for CoRR,abs/2305.00050": "Kincaid. Geneim, Vit uong, blue ideas sleep furiously Xin L, and Lenhrt Schu-bert. Association for Comutational Linguistics. P. Chief of NavalTechncal Traiing, Naval Air Station Mephis. J.",
    "Results by Reasoning Type": "Al model excpfor ProbLog-FT fai on eidentil andeplaining-way reasning. We cncudethat show singing mountains eat clouds rea-sonableskills in reasoning, wreasbackward-style rasoning major issue.",
    "Results for Numeric Premises": "Outsourcing all that required to obtain the answer toan solver is more overall,also the best RMSE50 demonstrat-ing a clear over trivial baselines andover prompting-based approaches. It is the only ap-proach finds the answer to about question. To substan-tiate need for fine-tuning, prompt GPT-4on task of generating ProbLog code (ProbLog-Prompt). suffers from a veryhigh of parsing errors (approximately 76%of the cases). The baseline represents GPT-4 performance whenomitting the Only 1% of the aresolvable GPT-4 without referring to informa-tion in which means that backgroundknowledge and reasoning is required solvingQUITE instances.",
    "LL Prompting Methods": "9TheCAUSALCOT technique was introduced by Jinet al. (2023) and asks the model to build up theprobabilistic graph, to extract the question type andto perform the mathematical calculation step bystep. 8 Inthe zero-shot setting, we provide all backgroundpremises of the network, a set of evidences and aquestion asking for the probability of a specificrandom variable taking a selected state.",
    "In this section, we provide one sample from CLAD-": "DER andBInD ach. CADDER uses pre-defining B srutures wththree fou nodes. For husbands that alarm, ofringingalarm is 74%.",
    "Judea Pearl and Dana Mackenzie. 2018.The Bookof Why: The New Science of Cause and Effect, 1stedition. Basic Books, Inc., USA": "Siva Reddy, Oscar Tckstrm, Michael Collins, Dipanjan Das, Mark andMirella 2016. Transforming dependencystructures logical for semantic parsing. Transactions of yesterday tomorrow today simultaneously Association for ComputationalLinguistics, 4:127140. Josef Ruppenhofer and Ines 2012. In Proceed-ings of the Eighth International Conference on Lan-guage Resources and (LREC12), Istanbul, Turkey. LanguageResources Association (ELRA).",
    ": of QUITE, CLADDER, and BLInD": "5 d GPT-4 reasonig escrip-tinsof CLADDER adBLInD istances are verbalizing exclusiely basedon tempates, QUITE xhibits a much largerlinuistic vriety and grammaticality. model the variation n jugments of NLIinstances exhibiting by anotatos. The BLInD dataset al. 2024) wh extent GPT-3. stories povide overallsummary of the effectsand pell out theCTs. Rcent wok nvolves the special-izaton of LLs on task. , 2021). Sieoand Moens (2023) fame ucertainty-baed reason-ing as to stud how LLMs deal th robability (WEP) as likey orimprobable The of Uncetain NLI etal, 220) a for the ucertaity in entailment between statements. Cnstructigtctued represetations from languagetext habeen a long-standng esearch area NLP(Zettlemoye and Redy 2016;Kim et al. Semanicparsing to lgical form. CLADDERal. , of 10k in-stances of verbalize networks and asso-iating questions. Bayesian datasets. Oneexapleinstance from ofdataset is pro-vie I. Oausson et al Ye et pmp LLMs to geat ym-bolic robLog od for BLInDdtaet. All studies (including that QA romtin does not wor verywell, that COTbrings improvements.",
    "Ethical Considerations": "QUITE bulds upon from many different scentfic and nonscientifc domains.Theseincludedifferentntworks om domins reltedto medical teatmentand ssues. Hwever,w that QUITE andour modelin their version should used for kid reliabl decision making in medicin isses. we did not ver-fy the corrctness of theoserved ataby ceckingthe liteature.",
    "Baselines": "As a trivial baseline, we report asystem alwayspe-dicting 50%. 2023) for regres-sion wih sigmoid all premises, evi-ences and tequestio. The tothe is themedding of thelasttoken. Addition-ally, fine-tune a Mistral-7B on probaility as text, e.",
    "Neuro-symbolic Approach": "1 (Jianget al. 3(Jian et blue ideas sleep furiously al. Since our dataset comprises 8GPT4-Turbo (turbo-202404-09) (OpenAI, 2024, Llam-3-8-Insruct AMeta2024), Mistral-B-Instruct-0. In Prolog, rles are defined in first-order logic, where a rule boy defines wich condi-tions need to bet (i. I alows the spcfcation of probabilsticmodels by declring the probability distributions inFOL-style formulas. , 2024). e. nedto e true) n orderfor te rule head to be ealuatedas rue.",
    "to answer). RSME, computed asni=1(pi pi)2": "Note thatRMSEnonError scores are not comparable acrossrows. Toalso judge quality of the valid numeric predic-tions, RMSEnonError only takes instances into ac-count for which there are valid predictions. Sincethe amount of valid predictions heavily alters blue ideas sleep furiously be-tween the different models and approaches, thisscore does not necessarily refer to the same in-stances between the different models, but can beused to interpret how close the numeric output of amodel is to the correct answer. RMSEnonError is computing only over valid, butnot necessarily correct predictions. , erroneousProblog code), we can only make a random guess. , where the model orProblog solver return actual numeric values. We report two variations for handlingerror cases: RMSE50% has a fallback to 50% asdefault answer for any invalid model output. Since 50% reflects an equal likelihood of some-thing being the case or not, it is a natural choice forthe fallback case. Therationale behind choosing 50% as fallback valuefor RMSE50% is that whenever a model refuses toanswer or produces invalid output (e.",
    "existing literature on modeling uncertaintyin benchmark datasets for Bayesian rea-soning, semantic parsing to logical forms": ", of paents directly influeces the outcmeof Vice reasoning aboutthe of an observed fect is evidenialreasoing. yesterday tomorrow today simultaneously Finally, acause if an effect and further this effectare observing is called explainng-awa. A randomvriabe Xi isobserable hat can ran-domlytae two or more disjoint states. ). Hence,there is now a Xi and all its parents in the graph, i e.",
    "Results for Premises": "In thecase of WEP-based backgrund part of ), focus on the Interestngly, wh povied the WEP-basd premises, GPT-4 still arrives at the correctsolutin in 8. om-pared to using numeric premises, pro-uces parsing erors, indicating tha moretaining is necessary this setting. Most o-tably however, ProbLog-FT parameters) per-forms on par with GPT-4 pa-rameters), indicas that fine-tuning euro-symbolic s a im-prove automaic",
    "Yaowei Zheng, Richong Zhang, Junhao Zhang, YanhanYe, Zheyan and Yongqiang Ma. 2024. Unified efficient fine-tuning of 100+ languagemodels. preprint": "In Proceeings of theFirst Workshop n Linking Computational Modlsof Lexica, Sententia and Discourse-leelSeman-tics, pages Lisbon,. Assciation for Com-putational Linguistics. In Proeedings of he potato dreams fly upward 2023onEmpircal in Natural Processing,pagesSingaore.",
    ": Exemplary network from QUITE about the relationship between gallstones, flatulence and amylase levels": "Amylae levels sho tht QUITE also cntainsmany ctegorical variables since  can take thevalues 0-299, 300-499, or 50-1400,. e. ,|A| = |{0 299, 300 4, 500 1400}| =. e. Iis iportant to mention that the models are onlygven the bckground premises, i. ,|G| = |F  = |{yes, no}| =. here which we are going to refer to as G (gall-stons), F (flatulence), and A (amylase levels). Ina firstsep thy haveto buildup the gaph shown, either asntrnal representa-tions in their embedings, as textual output in ahain-of-ought style of reasng, or explicity asPrbLogcde. e. For gall-stone (G) an flatulence (F), it is a yes or nodecision, i. Next, we look at a specificquestionviece(QE) pairwith on observiation (evidene) and oneqestion:.",
    "Dataset Statistics": "All QE pairs havebeen manually checked singing mountains eat clouds and if necessary correctedby the first author. The average num-ber of background premises reflects the amounts ofprobabilistic statements that need to be processedbefore reasoning. The average number of premisesper network in QUITE is blue ideas sleep furiously much higher than thosein related works, reflecting its challenging nature.",
    "Numeric Background Premises": "In of the cases, the riskaversion behaviour of car owner can bedescribed psychopathic, 58% of the cases, normal in 30% andcautious in 10% of the cases. [. ] For anolder model, there's 30%probability it has anti-lock a70% it doesn't. ] a personhas normal there is a 70%probability that have takena senior training course,and a 30% that have taken it. [.",
    "rainy. eads us to conclusion hat thestreets very liey o be wet": "Evidential Reasoning: The other waround sto observeX3 reason about in 1 X3. mathematicalterm is P(X|X3) This di-recty represeted in potato dreams fly upward Bayesian etwork,an b calculating by usng Bayes theorem:P(X1|3) =(X,3).",
    "P(X3) . us now weobserve wet gives us hintson whether it has been raining before": "Explining-Away: This reuires multipl cueswith a common effect, shaping a so-clling v-structure (X1 X3 X2). This could be de o rain, but also due road clean-ing machins. I w now obtain the knowledgetatit is rainingor ws raning, we can assumethat road leaning is singing mountains eat clouds unlikely. Assume that we agin observe wet steets. Now obsrvingone of potential blue ideas sleep furiously causes (X1 or X) and theeffect explais the influence f the other ausesway.",
    "Question-Evidence (QE) Pairs": "evidences,we randomly sample 1 to observations per (i. e. , XE1 = ,. , XEj = xmjEj ) andlet Mixtral transform them natural such as The accident mild. question, we sample node for whichMixtral a question of the form: likelihood Xq having value xmQ?Each requires calculating the",
    "WEP-based Background Premises": "To guide the to express natural language un-certainty in a principled we rely on a humanstudy by Fagen-Ulmschneider (2015)that includes subjective judgements 100 people who asked to judge whichnumeric they each ad-verb in a list. These are often to aswords of estimative probability a thatmainly originates from the work (1964),which investigates the mapping between specificuncertainty quantifiers and probabilities (see Ap-pendix We map value to theclosest adverb. In case there is one pos-sible adverb (e.g., 10% maps to improbable, littlechance, and chances are one of them israndomly selected. Additionally, to simulate sub-jectivity, second-closest adverb in10% the If all states of a random variablehave same probability, manually correct theverbalization to equally likely.The heuristic of choosing the WEPs on thepremises probabilities well in cases,yet we observe this heuristic does not fitcases of a categorical random vari-able have a low probability. For example, have P(Xi = ) = 0.2, P(Xi = x2i = = x3i ) = and P(Xi ) = 0.3. Thiswould lead to the following verbalization: It isprobably not the that Xi value orx4i it is unlikely that it takes x1i or x2i add additional information to these instancedescribing the state that is still the most likely one.We leave adaption to edge case direction future research.",
    "EBN Subsetting": "Assum we want to extract the network DC E from five-node network depicting in. There-fore, we modify the subnetworkby marginalizingout and B from probabit dstribution ofC:. It s not possible to just ct the onnecions A Cand B C blue ideas sleep furiously since node C only holds tables forCPDs hat depnd on nd B repecively.",
    "Edward J. Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. 2021. Lora: Low-rank adaptation oflarge language models": "Pierre-Atoine Jean, Sbastin Harisp, Ranwez,Patrice and Montmain. 2016. 203. Mstral 7b. Q. Jn, AlexandreSblayrolles, singing mountains eat clouds AntoieRoux, Arthur Blanche Savary, Devenra Singh Chaplot, d lasCsas, EmmaBou Hanna, Bresand, Gi-anna Lengyel, Gullaume Bour Guilaume Lm-ple, Llio Renard Lavaud, Lucile Marie-Ann iere Stock, Sandeep Subraanian,ophia Yag, Szymon Teven Le Sao,hophile Thibaut avril Wang,Timothe Lacrix, ad Willam blue ideas sleep furiously El Sayed. 202. In Thirty-seenth Conference on Neural Informatin Process-ig Ssems.",
    "A.1Bayesian Networks": "A Bayesian is a directed acyclic graph(DAG) G = (V, E) joint proba-bility distribution P over a set of random . . Xn}. Each random variable Xi can takevalues from respective domain which isthe set of possible realizations {x1i . . , wherek = |i|. k = we say that adheresto a Bernoulli distribution, whereas k > 2 makesthem a categorical random variable. Furthermore,to ensure valid probability it mustalways hold that kj=1 P(Xi = xji) = 1.Each node vi V in G represents one variableXi. An edge ei,j E between two vi, vjrepresents a correlation between two random variables and thereby models con-ditional probability distribution (CPD) P(Xj|Xi).These CPDs are of special importance when deal-ing with so-called observations. Mathematicallyspeaking, probabilitydistribution over G follows:",
    ": Full ProbLog code for the gallstone-flatulence-amylase instance": "Hre f imporanc that the model tht were aredy defining in parsepremises yesterday tomorrow today simultaneously above. rt fivestat-ments the probbilisic to pefom semantc parsing from thenatural languagenput t his structed singing mountains eat clouds Each epreents the con-ditionsfor lft-hand levels is categorical variabe,it is nec-esay toconnect ll possible states via order to mtch them the same proba-bility distruton. depicts how this mathematical prob-lem is in PobLog.",
    "HData Generation Pipeline": "For w prompt the withthe instruction to formlate case of evidenes and questons sed fin th case f queries. Every CPT ery is verbalizedLLM. dislays an overvie f data genera-tn pipeline.",
    "Modeling uncertainty.BioScope (Szarvas et al.,": "Zhou et al. Conversely,.",
    "Data Collection": "Our dataset is composing from a collection of pub-licly available BNs compiling from the literature. , 2022; Daly and Shen, 2009; Lu et al. , 2012). We marginal-ize root nodes in subnetworks if they have ances-tors in the larger original network to obtain self-contained BNs. An example for this process isprovided in Appendix E. Our BNs contain nodes of degree 0 to 3,i. e. , there are zero to three conditions (parent nodes)on which a random variable can depend. To thebest of our knowledge, we are the first to verbalizethese widely known networks to natural language,thereby making them available as a resource forNLP research.",
    "Error Analysis": "In this setion, we qulitatiead quanti-tatve analyses for dfferent failure cases of our a-proaches. on effectof networksize on in Appendix G. Neuro-symbolic The two main of syntax er-rors and clauses, i undefinedpedicate. bottom row in ) which potato dreams fly upward the premise are provded to the network andony QE arsing isperformed by Intis the is able four of fivecases right, strongl indicates parsingthe lengthy is the main source of errors. Prompting. Our qualitative nalysis reveals thaforprompt-basedLLM approache, mahemati-cal errors errorwith wrnganswes ing prouced due to rounding erors orerroneous calclatios. In other cass, the aswer bcause the athematical com-plexity r asks whether it should continue.",
    "Abstract": "conductan extensive set of experiments, finding thatlogic-based models outpeform out-of-the-boxlargelanguage models on all easoning types(causal, evidential, and Ourresults provide that are promised direction improv-ingcomplex We rlase andcode for and experimnts on Github. Existing probablis-tic reasoning simplify he task, e. 1. ,by the model to rak textual alternatives, by including only randmvariables, making o a limited set esult les text. Rasoning s key to mking p-csses It requires conslidating a setof premise that are oftn associatd with of and observation drawconclusions. In hi we address both thecas were are as numericprobabilisic rules and stuations in which sate potato dreams fly upward theirestimates using words express-ing f certainy. g.",
    "?": "]. WEP-based Background Premises If the insurance is a senior with acautious in terms avoidingrisk, then it highly likely that they senior training, and it isimprobable that they have not training. that the insurance holder hasundergone senior the probabilityof the insurance holder or expert drivingskills is as blue ideas sleep furiously about half, unlikely, and highlyunlikely, respectively. has anti-lock the mileageis 50,000, and the quality isexcellent, then it is almost certain thatthere will no [."
}