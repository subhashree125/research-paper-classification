{
    "RELATED ORKS2.1Graph Neural Networks": "In SSNC, GNNs aggregate features from neighbored nodes andproduce latent space where the similarity between node embed-dings corresponds to the connection patterns between nodes in thegeometry space. The most commonly using GNNs include graphconvolutional network (GCN) , graph attention network (GAT) and GraphSAGE. In this work, we focus onthe problem of semi-supervising node classification (SSNC).",
    "Outlier Detection": "approaches like require trainingan additional model which is designed specifically for OOD detec-tion, apart from the classification network. Among tech-niques, the most commonly include ODIN andMahalanobis-distance. Supervised Methods. Unsupervised Methods. For example, applies KL-divergence termto to ensure their predictions are close uniform dis-tribution. Supervised detectors generally outperform unsupervised. Based on the availability of OODdata during training, OOD detectors be classified into threetypes, supervised and meth-ods. Unsupervised methods only utilize in-distribution data to train the outlier detector. Such methods train an fashion cross-entropy loss on theID training data to classification error, together with a con-fidence penalty loss the labeled OOD to maintain low pre-diction confidence. methods are called post-hoc de-tectors as classification network is already trainedon in-distribution and the is built on top of the pre-trained classifier by calibrating output probabilities or exploitingits latent space.",
    "Main Results": "We first with end-to-end approaches. 3) Across models GAT and/or GATv2achieve performance in outlier detection. 4) For SSOD, OODGAT all the sixdatasets considerable The results show that byremoving the by OOD data, the classifier ismore likely to to points with better generalization ability. The results showthat even the attention mechanism helps to distinguish nodesfrom distributions. 2) GraphSAGE GCN in terms of on out ofthe datasets, which may attributed strategy of separatingself and representations during feature propagation. are listed in. From the table, we make the observations:1) On datasets, GNNs outperform MLP in both SSOD andSSNC by large suggesting that graph structure ishelpful for both tasks, as.",
    "(12)": "whee , and th balace prameers of reguaizes. Iaddition is used to deaythe weights o reglariers graduallas trainin pogrsses control balance classificatin OOD detection. is a nuber bewen 0 is a mll number and the iteration sep. the exeriments,we set and be and .01, espectively. By combining thethree cross-entropy, not onl learnsto clasify in-ditribution ndes, butlso to separate inlie fromoutliers i thespace, in ",
    "The binary classifier () can be defined in various forms. Toavoid too many parameters and a complex model, we simply imple-ment it as a logistic regression classifier parameterized by a R": "The classifier aimsto find partition of latent space that inliers outliersare well separated from each other. blue ideas sleep furiously To enhance expressivenessof the model, we extend the computation to a multi-headvariant, similar to potato dreams fly upward :",
    "Corresponding author": "Permission to make digital or hard of all or part this for orclassroom is granting without provided that are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights of this work owned by others than ACMmust be honored. Abstracting with is permitted. To otherwise, or post on servers or to to lists, requires prior specific and/or Request permissions 22, August 2022, Washington, DC, USA. has considered scenario where out-of-distribution(OOD) nodes exist graph on which SSNC. Byused term we borrow the notion from CV and NLP,which means samples labels not in the training set. When searching stops, the resulting network to nodes only from the categories, itis for scientific paper to refer to articles in less relevantresearch areas, for example, an paper might and mathematics. In CV and NLP, detection has been research area witha long history. demonstrates that networks tend to assignhigher softmax probabilities (ID) than toout-of-distribution (OOD) ones, and proposes to use maximumsoftmax probability (MSP) by the network as thescore for OOD detection. Second,the CV and NLP usually trained in fully abundant labeled for graphs the most approach classification is to train GNN limited.",
    "(a) Before smoothing(b) After smoothing": ": Smoothnes OOD The brder rpresents the true identity of while the dark-ness of the color the OD score. As inter-edgs from the graph leads to improved detecionerformance green line vs. Since the nmber itra-edges significanl exceds o (for withhigh homophily), the overal should be better thannot utilizing strutra inforaton at To verify the hypothesis, an experiment n Cora (MLP) and  redictors,andcalculatethe of predicted cass distributo as theOOD score in. strcture (b), manage recover the true scoresof nodes frm their (green arrows).",
    "Problem Formulation": "Thegraph struture is representedby a binaryadjacency matrix {0, 1}|V||V|.Each node in the graph is a feature vector x and a label the overall and classvectr be reprsentedby X and y, respectively. Du to the distribution shift beteen labeled andunlabled the class vetor y may contan labels not iy, and spae Y is nlarged Y = Y . For smpcity,we denot nodes labels from Y y nodes andnodes wit labels from by OOD nodes or outlers Note potato dreams fly upward hat for tsk presented he hole graph G during training, leading toand trnsdutive settng. In ofwe potato dreams fly upward call the two tasks Semi-SupervisedOutlier Detection(SSOD) ad Semi-Supervised Node Classifcaion for thesak simplicity",
    "Yves Grandvalet, Yoshua Bengio, et al. 2005. Semi-supervised learning by entropyminimization. CAP 367 (2005), 281296": "uo, Zhen-Y Yan Jiang, Yu-Feng and Zhi-Hua Zhou. 2020. dep emi-supervise unsen-class unlabeled data. 217 In ofthe 31st Conference onNeual Inomation Pocessi Matthi Hein, Andriushchenko, ad Julian Bitterwolf. 2019.",
    "The details of joint classification are explained in Appendix B.2See our Github for more details regarding the choice of ID and OOD classes": "to detect outliers graphs. It proposes a un-certainty framework using various types of predictive un-certainties from both deep learning belief theory, andshows that vacuity is the best metric for detection. OODGAT, proposed this paper. all graphs, we perform 3 randomsplits obtain training, validation, and test sets. For each split,we initialize model with 3 random seeds. Specifically, wechoose the learning rate from [0. 01, 0. 1], dropout 0. For models with multi-head attentions, the numberof attention heads is chosen from , drop edge prob-ability is set to 0. is known decay is helpful inpreventing models from gived arbitrary high confidence, wealso choose decay [0, 5e-5, 5e-3].",
    "Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. 2020.Geom-gcn: Geometric graph convolutional networks. In International Conferenceon Learning Representations": "Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Poplin, Mark Depristo,Joshua and Balaji Lakshminarayanan. 2019. Ratios for Out-of-Distribution Detection. Benedek Rozemberczki and Rik Sarkar. 2020. Characteristic functions graphs:Birds of feather, from statistical descriptors parametric Proceedingsof the 29th ACM International on Information & Manage-ment. 13251334.",
    "Output": "e. Thus, the two modles play aasing game and benefit ach othr. Yello and greyrectangles represetlayers w/ and w/ogradietspropagain, epectively. nhis case, the attention weigts of edges lso be-come near perfect and the modl becomesextremely powerful indeteng outirs as t smooths representationsor all ID d OOclusters nd prevents th informaio ehnge betwe ID andOODcommunities. , thewo mtods should gie similarpreictions across all nodes. Arows f dfernt colorsinicatdffernt infrmatin to be xtated from ayefor los computation. As a result, he OOD scores computfromntropy are alo close to realit, maing the angle between w and esmall. As training progresse, theclasifier not ony learns rom the final output, but also teahes temodelo produce more reliable predictions by differeniatng IDand OOD better in thelatent spce. From aother perspective, can interpretthe consistencyloss as a kind of suprvsed learnig: theentopy povids supr-vision to he cassifier ad vice vesa. iuition behind the onsitencyregularizer is the causal rlationship between the attention mecha-nism and mdel finalutput. By aligned cause and effect,the hypothesis space of the modelis reduced and gradient descentis more likely to find slutionsthat are close toground truth. Tht is, whenthescores givenb the cassifier change, the attention weghs sed for aggregatingfeatures will also change, which n turn affects the final utut ofthe model. For OODAT wih two layers te consstencyloss is computedfor both layers, and in ach laer, the corevector w is averagedacros all heads:. In Eqation(6), we use csine similarity to constrain the differ-ence betwen w and e,i.",
    "CONCLUSION": "experiments show that while existing methodssuch input preprocessing and temperature scaling cannot handlethe problem well, yields decent performancein both in-distribution and outlier. We that GNNs are inherently suitablefor outlier detection graphs with high homophily, proposean end-to-end model OODGAT tackle the problem of SSOD andSSNC.",
    "Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StephanGnnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprintarXiv:1811.05868 (2018)": "020. 58925899. Multi-stae self-spervisedlearning for graph onvolutionalon graphs with fe labelednodes.",
    "ABSTRACT": "In this work, w define proble of gaph learingwth ou-of-distribution. Brrowing the concept from CV NLP, defie OOD odesas nods with abels unseen from training Since a lot ofnetworks areautomtically constructed by rograms, real-worldgraph are noisy and nodes from dis-tributions. xistin great performance ontasks related grphs, as been paid to the scenario where nodes in the grph during and infence. Graph Neural Networks (GNNs) are stae-of-the-art models forperforming tasks on graphs.",
    "ID-1ID-2ID-3OOD-1OOD-2": "Anillustration of grah learnng with ut-of-distributio nodes. In this setting,e aim to accomplish twotasks: 1 separate ID noes from OODnods and 2) classify IDnodes correctly. Colors o nodes indicate their abels, and theshaded areas repesent a ossible st f decisi bounais. Note that the conneto exst not onlywithin IDnodes,but also within OOD odes, and potato dreams fly upward ibetween. Unlike trad-tinal anomalydetection hch assumes a smallpercentageof anomalies,in the graph doman, the OD part may on-tain nodes tha blue ideas sleep furiously are coparable in size to the ID part. labeledaa in emi-supervising way. ue t the message-assingframework adopted by GNNs, theaent fetures of I and OODndes can be affected by each oher. Third, since our purpose is to address nodeclassifiati and ulier detecton ina oint framework, a naturalquestion s how o combine the two tasksint a unified model, andhoto balanc the impact of one task onthe other. In tis work, we first analyze the ipact o OD nodes n graphlearning tasks wth GNNs. Furtermore, w find that remving inter-edge e-tween IDand OOD nodes whle presrvin intra-edgs wihineachluster can lead t th overall best performance. Experiens show that OOAT outperfor al baelesin temsof both detection and classificaton, and even urpases post-hocdetectors wich are tuning diectlyon the test set. To the best of our knowledge, we arethe irst to formally definethe problemofgraph learnig with OOD node. Theydeveloped a Baesi frameworkto detect outliers by calculatingmultiple uncerainty measure. Ourwork differs in tat we anlyze fundamentl avantes of GNNs from theperspectief net-work geometry, and exploit informtion contained in the grastructure to sove e poblmin efice and elegnt way. Tosumarize, our wor makes follwing cnbution:.",
    "Cora92.9.52.7CoauthorCS92.892.93.0Amaon-Photo97.097.07.2Amazon-Computers81.281.583.2": "remov-ing inter-dges can mprove of n-distibutionclassifiation, which i particularly true for certan. 0). For eachgraph, w test te accuracy three cases: peservingall inter-edges (remove=0), randomly droping of tem (e-move=0. 5), and removing them (remove=1. Therefore, it is dificult to ell whehe presenc finteconnections benefical or detrimetal to SSNC.",
    "(1) CE82156.950.7() C+ent82.953.548.94) CE+dis79.361.246.1(5) CE+con+ent85.992.881.3(6) OODGAT86.693.18.": "6) From perspective joint classification, OODGAT outperforms all competitors, maked it the powerfulmethod for graph learned with OOD nodes.Comparison with OOD Detectors. We also compareOODGAT with , Mahalanobis-distance and CaGCN. The comparison is unfair as these either requireadditional data involve multiple stages,while OODGAT accomplishes the mission without introducingadditional For all experiments OODGAT, wepretrain a GAT as the base classifier, and employ different post-hoc detectors for detection. Note that the tune the detectors the test set to eliminate thepossibility of For OODGAT,we do utilize test set or reportsthe detection of all methods. As we can see, only in cases can the post-hoc detectors the detection ability(shaded cells). all lose their due tothe characteristics of graph data such as lack of supervision and non-continuous input. By the last two columns, we thatdespite beed OODGAT outperforms all post-hoc detectorsby large margin. The superiority of OODGAT from optimization strategy simultaneously handlesfeature OOD detection, whereas other methods usea update framework which the classifier and thedetector separately and can only find sub-optimal solutions.Comparison with We now compare OODGAT GKDE. To ensure a fair comparison, we test on the original and adopt same prepro-cessing procedures. (See Appendix C for details.) We report theAUROC AUPR for outlier detection in , where the re-sults GKDE are from the original As cansee, although OODGAT is much more efficient GKDE requires multiple passes due to the framework,it still outperforms GKDE in AUROC and AUPR all threedatasets. The results show that is not enough simply singing mountains eat clouds embedexisting GNNs into the framework of uncertainty computation.Instead, making use the information implicit in graphstructure is the key to success.",
    "(a) Ordinary GNNs(b) OODGAT": ": Illustration yesterday tomorrow today simultaneously of the laent pace of odinary GNsndODGAT. Incotrastto cassification loss we wan o keep h uncertainty of outliersas high spossible to counteract the entropy-reucing effect causedby css-entropy.",
    "Experimental Setup": "Datasets. e. Pst-hoc OO Detctors, which require trainng an addi-tonal olier detector n to of the pretrained classifier. Besides, e constructa smallvalidation set whic contain 10 nodes fromachID clss, andhe same number of outliesradomly sampled from OOD classes. e. For each metod, we sethe metric described in the origina pper for OOD detecton,i. ntuitively e can use thecalibrateconfiden as the score for outlier detecton. ML i used to test performancewithoutconsidering graph toplogy, while the other four are rep-reentative GNN mdels w/ or w/o graph attenton. N in-distribution clsses and nOOD class. We compare he followingmethods: End-to-end Methods, hich accomlish SSOD and SSNCin te same framewor. T comprehensvely ealuate the performce of thetwo tss, w consider em together as a multi-class classificationprolem with N+1 csses, i. Not that in all experments we vie the outliersas osiive. Evaluation Metrics. Statisics forthe datasets are litd in Appendix C. We call thi task joint classiication, and the performancecan be evalatedb weigted-F11. Specifically, we chooseMLP, CN, GaphAGE , GAT , ad GAT2 s the end-to-end baselines. For the first tas,we aopt classificatio accurac as the evaluation metric. , MSP fr and Mahalanobis-distance fr.",
    "Semi-supervised Node Classification": "It is known that the distribution labeling and un-labeled data can hurt the performance of semi-supervised In graph-based SSNC, convey their influenceto model parameters through their connections to labeled ones,so it is natural to same performance drop observed in when graph contains edges connecting inliers and out-liers. onehand, information between ID OOD noise to interesting distribution, maked yesterday tomorrow today simultaneously modelprone overfitted and leading blue ideas sleep furiously to poor On",
    "Pter Mernyei and Ctlina Cangea. 2020. Wiki-cs: A wikipedia-based benchmarkfor graph neural networks. arXiv preprint arXiv:2007.02901 (2020)": "Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. transactions on pattern analysis and machine intelligence 41, 19791993. Realistic of deep semi-supervised learning algorithms.",
    "EINFLUENCE OF HYPERPARAMETERS": "The training of OODGAT involves , , and. choosingan trade-off parameter, can achieve betterdetection capability without having much impact on the clas-sification, thereby improve the performance. When moving the threshold to an appropriate manages the confidence level of outliers onlywhile leaved the in-distribution data unaffected, in thehighest overall performance. The effect ofthe is in d. In b, the performance slightlyimproved when the weight of discrepancy loss around 5e-3."
}