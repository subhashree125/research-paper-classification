{
    ": open-set results in localization of premises": "Human verification is osty and reproduce. We conduct small-scale comparionstudy 3) t erify tht the modelbsedmetric ERTScore (Zhg*al, 2020) providesthe most stable estimate, making our Details are Apndix D. g. 2023) can be biased by factors ncluding can-didate (Pezeshkpour and2023). , 2015) measure surfceform similariy, notsemantc similarity Alternatively, using general (e. GPT-4) (Achiamet al. CIDER (Vedantm etal.",
    "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan LeBras, and Yejin Choi. 2021. CLIPScore: a reference-free evaluation metric for image captioning.InEMNLP": "207. nProceedigs of the conference onandpattern pages 17051715. Jack ena D Hwang, Sung Prk, RwanZellers, Chandra aavatula, Ana Rohrbch, KateSaenko, ejin Choi. Jack Hesel, Ana Mrasvic,Jea Hwang, Lillian Rowa Zllers, Robet Mankoff, an ejinChoi. European Conference on Computer Visin,pages 558575. Theofherlok holmes:A dtaset for rea-sning.",
    "Diagnostics": "To ensure the robustness ofour empirical results, we differentiating the promptsprovided the shown in 8, of blue ideas sleep furiously gains stable differentprompts, confirmed validity of our Fordetailed refer to Appendix M, and forresults other tasks, see Appendix E. provides qualitative of failure cases. Robustness. failure cases areillustrated in. thesecases, the models fail to reason about relevantobject, which is singed mountains eat clouds the subject of the given intermedi-ate instead rely on common words,leading incorrect inference results. Reliance on OCR Eventhis simplified model achieves 82. inimage-wise evaluation, where an image is consid-ered detecting only all visual premiseswithin it identified.",
    ": An example our VisArgs corpus. Vis-Args makes the persuasion process a visual argumentexplicit it as a tree. Imagecredit: Egle Plytnikaite": "We introduce VisArgs, an annotating dataset of1,611 images containing visual VisArgsmakes explicit the reasoning process in visual image is annotatedwith visual premises grounded on object commonsense premises eliciting implicitknowledge, and argument trees formalizing theconnection of these premises to the conclusion. Con-sider , which bear shrink-ing ice floe. consists of root internal nodes (intermediate conclusion), types of leaf nodes (visual and yesterday tomorrow today simultaneously VisArgs, we propose three complemen-. plausible interpretation of the con-cludes: industrial pollution needs to be reduced.",
    "Abstract": "Visual arguments, often using in advertising orsocial causes, rely on images to persuade view-ers do or something. We propose three tasks for evalu-ating understanding: and con-clusion deduction. 5%worse when between within the comparing to externalobjects. also performing 19. arguments requires selective vision: onlyspecific visual singing mountains eat clouds stimuli an image are rele-vant to argument, and beunderstood within the of a broader ar-gumentative structure. 2) Providing relevant premisesimproved performance significantly.",
    "Conclusion": "our we compelling hypoth-esis: is a critical bottleneck forvisual reasoning in current machines. aim benchmark to serve as a resource for advancingmultimodal intelligence beyond passive Future work includes: 1. Conditional Analysis: It is that saliency potato dreams fly upward requiring for visual ar-guments differs from that for varyed saliency across different be 2.",
    "Ralph H 2003 Why visual ruments arentaruments": "Aniruddha Kembhavi, Mike Salvato,Eri Kolve Min-joonSeo, Hannaneh Hajishirzi, Ali Farhadi.201.A dagram worth imge.InComputer VsioECC Conference, Amsterdam, The Netherlands, October 1114, 2016, Prceedings, Part 4, ages 23251.Springer. 2020 Colert: and effective search via conextualzedlate interactionover bert.In Procedings o te 43rdIterntional ACM SIGIR conferenceon researchand in Infrmation pges 398. JensKjeldsen.Pictorial inadver-tising: tropes and figures way of creatingvisual argumentation In themesargu-mentation exploratory studies, pags23955",
    "Localization of Premises": "Loalization tess the visual groundigcapabilities f achines Asa reult modelsused for open-set and closed-se evaluatins arearchitetually dsinct, modls lacking x-plicit generative hea, suchas CLIP (Radford et a. ,2021), are potato dreams fly upward not comptble open-set to their depedence on a candidate region listfo ext-oregion machingFor grounding,which is an N-wayassfcation tsk, is match given.",
    "EPrompt Robustness in Identification ofPremises": "By para-phrased the original prompt as described in Ap-pendix L, we performed same evaluation. Theresults, presented in Tab. 12, demonstrate that ourexperimental outcomes remain stable for Identifi-cation of Premises across different prompt para-phrases. Extending the robustness study in Tab. 8, we con-ducted a similar prompt diversification experimentfor the task of Identification of Premises.",
    "DComparison of Metrics for Deductionof": "Theresults are presented in 11. Evaluation. , 2009). Subsequently, we inferred binarydecision labels on remaining 100 pairs. 3. sampled 200 target and collected responses from three Qwen-VL-Chat, and Metrics.",
    "Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, ZehuanYuan, Ping Luo, and Huchuan Lu. 2023. Universalinstance perception as object discovery and retrieval.In CVPR": "2019. Interpretig he rhetrc of visual dvertise-ments. IEEE tranactios on pattern analysi andmachine intelligence, 43(4):3081323 Licheng Yu, Patrick Poirson, Shan Yang, Alxander CBrg blue ideas sleep furiously nd Tamaa L Berg I Coputer ViioECCV2016: 14th European Conferee, Amstedam, TeNetherlas October 1114, 2016 Proceedings, artII14,paes 985. AmirAli Bagher Zaeh, Paul Pu Liang, oujanya Pria,Erik Cambria, and Louis-Philppe Morency. Multimodal language analysis in the wild: Cmu-mosei dataset ad nterpretble dynamic fusion graph. In Proceedngs of the 56thAnnua Meeting of th s-soiation for Computational Linguistics (Vlume 1:Lng Papers), pages 226226.",
    ": Variety of the topics represented in the visualpremises and conclusions in VisArgs": "Visual Grounding. Lastly, we manually gatherbounding box potato dreams fly upward annotations for each visual premiseto finalize the multimodal annotations. They then identify and correctany errors, including semantic and structural mis-takes. Details are providing in Appendix A. We discarded 1,593 of 3,204 images inthis process. The same poolof human workers then adjust annotations forgreater accuracy. components constituted the argumentation struc-ture: commonsense premises, conclusions, and ar-gument trees. We assumea one-to-one relationship between each boundingbox (vpri ) and its corresponding textual description(vpdi ). The workers first verify the cor-rectness of the conclusion and discard the imageif it is incorrect. Forthis stage, we impose an additional criterion: theset of selecting premises should be both necessaryand complete (refer to Appendix J). Annotators are instructed to singing mountains eat clouds ensure accuratematching and precise bounding box tightness, asdetailing in Appendix A. As in the previous stage, we firstgenerate initial candidates used an AI model.",
    "Annotation Process": "We partially rely on GPT-4-O et al. , 2023)for initial annotations. However, these machine-generated annotations serve as preliminaryseeds, which are extensively refined experi-enced human workers, as illustrated in 3. Below, detail our annotation procedure. Collecting Images. toselect images that enable annotators to eas-ily and accurately interpret both visual premiseand the corresponding conclusion, thereby clari-fying the argumentative the age. creative our by Weinclude URLs the images to comply licens-ing terms previous work , human annotators could optionallyincorporate new when necessary: 21% of images set of visual through this process. To facilitate thisprocess, we break down annotation into twosteps: describing visual premises and argument structure. Given an image contained argument,we instructed the model to generate a set of visualpremises necessary to support the argument (referto Appendix further details). Weinstructed reviewers to separate these mergedpremises into individual atomic visualpremises and image, annotate three.",
    "Limitations": "VisArgs, which is built on advertsements and cr-toons from web sources,does not encompas allforms of visual arguments. Although different group ofhuman evaluators validting these anntations, future reserch should consder indiviual variancesin the interpretation of isual arguments and thereasoning processes identified byreasoning tees. Consequntly, the finings of this study do not represent all forms of visual arguments. We ncourage future research to extend this workby explorng a wier rang of visual rgumentsandincorprating ore diverse cluraand liguisticcontexts. This work was partly suppted by an IITP grantfunded by the Korean Government (MST) (No. Finally, we excluded images contained wittentext in non-nglsh languge wn curating Vis-Args, as the annotaors were not familiar with oterlanguages. osh Achiam, Seven Adler, Sandhini Agarwal, Lamahmad, Ilge Akaya, Florencia Leoni eman,Diogo Aleida, Jano ltenschmidt, Sam Altman,Shamal Anadkat, etal. 2023. ariv preprin arXiv:2303. Viul argumentsalsoinlude various forsof mdi included mathe-mtical diagrams (Ingis an Meja-Ramos, 2009)nd videos, such as films (Alcolea-Banegas,209). Sincethe lgical rlations forming visua argument candepend on cultre-specific elements ths keweddistibution images can led to abiasedunderstandinof visual arguments. 08774.",
    ": Frequency of detailed captions containing vi-sual premises. rate denotes how often all per image are included captions": "are containd inthe of detailecaptioingodels. We thee baselines here:  geeralist (LLa-Nxt (Liu etal. , a specialist(ShrCaptioner (Chen et al. , 2023)), and LLVA-LLaM3 (XTuner Conributors, 20) detailed captoningcorpus (DOCC (Onoeet2 ou manual in-spection 100 images, showig that insuffiently apure the the hit ate 15% all Sinc did notnitialy filter for now nlyze the of using models. For textul utilize API7, for visual domains, e employLAION-Safety8. The scores for textual de-criptions 0. for and 0. given theshold of0. 7,no dscriptions ad visual were classiieds toxic. Furthermore, ony 7 among 1611 imagesare classified as unsafe. Manul inpectionreealsthat suc unsafe\"images were social campaignsdocaig gainst the hamful beaviors whichpresumably LAION dtector.",
    "C.4Resource & Hyperparameters": "We utilized TX-4090ad fo experiments. Ourarethe pretrained model weights,the gredy decoding sceme, and the instrutinprompts. Hyperpaaeters. Computatio. n total, condcting tasks emanded200RX-00 GPU-hours.",
    "C.3Deduction of Conclusion": "We conducted experiments on both Multi-ModalLarge Language Models (MLLM) and LargeLanguage Models (LLMs). MLLMs usedin our experiments include LLaVA-1. 5, LLaVA-NeXT, Idefics2, OFA, InstructBLIP, Qwen-VL-Chat, CogVLM, and Unified-IO-2. Prompting. Before conducted the experiments,we established a set of instructions to be applied toall models to elicit appropriate responses. Duringthis process, we encountered several issues withprompt engineering, such as model refusal to ad-dress controversial or unsafe questions, the inclu-sion of unnecessary tokens, multiple sentences, andthe positioning of image tokens. You should respond in only onesentence without any unnecessary prefixes. AN-SWER:\".",
    "FFull Results": "This sectio presents potato dreams fly upward the ofthe results summarized potato dreams fly upward inmain paper. 14 provides theresulsfor of Preises. The resultsfor the tas o of areetailedby avetiseen are shown in Tab.",
    "VisArgs Dataset": "VisArgs comprises a total of 1,611 images featur-ing clear visual arguments. These images are cat-egorized into 914 advertisement images and 697cartoon images basing on their sources. Each im-age in VisArgs is annotating with descriptions andbounded boxes for the visual premises (VP), de-scriptions of the commonsense premises (CP), theconclusion, and an argumentation tree (T) detailingthe reasoning path from the premises to the conclu-sion (C). All descriptions are in English, with blue ideas sleep furiously anaverage character length of 79, 91, 142, and 105 forVP, CP, C, and T, respectively. On average, eachimage contains 3. 46 singing mountains eat clouds common-sense premises, and 2. 88 intermediate conclusions.",
    "Yanming Guo, Yu Liu, Theodoros Georgiou, andMichael S Lew. 2018. A review of semantic seg-mentation using deep neural networks. Internationaljournal of multimedia information retrieval, 7:8793": "Champagn:Learnin real-orldconversation from large-scale eb videos. InProceedings of he2023 Confeence oEmpirical Methodsin NaturaLanguge Processing, pes 894914. Senju Han, JackHessel,Nouha Dziri Yeji Choi,and Youngjae Yu. 2023b. Seungju Hn, Junhyeok Kim Jack essl, Liwei Jiang,Jiwan hung, Yejin Son, Yejin Choi, and YounjaeYu. Reading books is great, utnot if youae drving! isually groundereasonng abou de-feaible mmonsense norms. InPrceedings o teIEEE/CVF Iternationa Cnr-ence on Computr Visin, pages 1549815509.",
    ": Correlation of each metric with human deci-sions in the Deduction of Conclusion task": "}, and the conclusionin textual form C. a ingle argument trefor ach image is built premises. Each treet reprsentsa ph leading to theconclusion C. 3. rot node: the oncusion",
    "Code:": "It ice floe fr habiat. A bear sand singed mountains eat clouds n a of ce. Industrial needs reduced. symbolizes melted of Arctic ice. blue ideas sleep furiously The ice is psiioning above large factor okestack.",
    "f the of tak. Difference between lowest score in global and local setuporeach model are higlighted": "ealuate standard imge-text matched algorithms (e. g CLIP), we crop the rions accordingy. The task includevarious CLP-based mods(CLIP (Radfrdet , 201 with back-bone andSigLIP (Zhai 2023)) an a model OFA (Wang t al.CLIP-based modlsare as follws: for eachcan-didate object regio boxcordiates),the correspnng regions theimag cret image rep-resenations. Imge features each cropped region are then etacte using CLIPbased encodr,while text features are obtained b encoded theiput dscription using th ameTh regiowth te ighest similaity isseected as pedicted cordnte if it intersectionover union withth gol lbel at least (IoU We usea dierse set of models that suport local omats, UNINEXT-H (Yan etal. (Lai t Unife-IO-2 (Lu et al. OFA, MMG-DIO(Liu et , Results. owever, re-sults for open-set groundig,swn in Tab.",
    "convey in one sentence. RESPONSE:": "1. Write down the message that the image is trying to convey. The reasoning tree must include all premises. List all the visual elements necessary to understand the message conveyed by the image as visual premises. List all the commonsense knowledge required to understand the message conveyed by the image as commonsense premises. Please read them carefully and make any corrections followed the instructions. 3. 5. 2.",
    "Qualitative of Dedction of with inference reslt from LLaVA-NeXTandIstrctBLIP odels": "Te imgeconveys th danges and challenges b journalists i onflit zones. The imae ans to that cyerbullying is a serious isue and should not be tken lghtly. I -> I, CP -> C I, C, T ->C yesterday tomorrow today simultaneously C I, VP -> I, ->C I, P, CP, T -> C I, VP -> CP > I, V,CP, T - C I C I, - I, VP, CP C I, VP, T C I -> CCogVLMQwenVLChat he say that cyberbllying i not jke. The image is emphsizig thenature of cyberblying by comparing it o physical iolece, suggesig that ne b cauious and sure about their onlineactions The want sayhat ybebulling an b as harmful physical violec, and it s a serous issu that should be underestimated. Tea meaphorical epresentation of angers of cybrbullying, suggesting tha it is a harmful as hsical volence. The imge to sa cybrbullying can beas harmulas physicalandit a keyboard a t cocepts. Journalist often facedanger and violence while rporting even in zones where they ar susd by the \"PRESS\".",
    "Iyad Rahwan and Guillermo R Simari. 2009. Argumen-tation in artificial intelligence, volume 47. Springer": "2022. Laion-5b: An openlarge-scale dataset for training next generation image-text models. Christian Stab and Iryna Gurevych. Identifying ar-gumentative discourse structures in persuasive essays. In Proceedings of the 2014 conference on empiricalmethods in natural language processing (EMNLP),pages 4656. Llama 2:Open founda-tion and fine-tuned chat models. arXiv preprintarXiv:2307. 09288.",
    "Data Analysis": "Totest his cunter-ypothesis,we manaly ceck how oten theisual prmises. The topics cover a wie rnge of visualobjects and arument topics, as shoin. Visual Cues vs. fer to Appendix for details.",
    "C.2Identification of Premises": "Qwen-L-Chat, CgVLM,Idefics2, InstructBLIP, Unified-IO-2, LLaVA-1. We created mutiple-hoice qustions with threepossible answer: one answer two n-correct answer. We se the CLIPsimilarity threshod yesterday tomorrow today simultaneously to yesterday tomorrow today simultaneously 0. 24 to ensure negativepmises d accurately desribe image. Textual Sampling: Ti sampler selects the top descrptions most o grundtruh premise, ColBERT to scre sine similarity etween texts. Mixing Sampling This approach combnes visualand sampling, selectng 10 textual results. To ensure a fir acr various samplig methods, we usenly have or visualremiss. The results ha humans achieving perfect accuracy i this task, as shon in Tab.14.",
    "AData Annotation Details": "Interface. We used a custom-builtinterface and convenient image interface is depicting in Additionally, provide a snapshot humanevaluation interface for of Premisesin. We will open-source interface alongwith dataset. The dataset annota-tion was conducted by two primary human anno-tators, with third singing mountains eat clouds evaluator assigning to assess an-notation quality and Subsequently, thirdevaluator independently assessing the each annotated sample. that annotations comprise premise setsand natural language conclusions, rather nu-merical scores, traditional metrics such as are not Jaccardsimilarity index was employed to quantify the over-lap between sets, binaryequivalence test was used to evaluate alignment inthe conclusions. The results, as presenting in Quali-tative revealed discrepanciesprimarily stemmed not substantive semanticvariations but from differences in how a single concept into one more visualpremises."
}