{
    "Prompt: Light blue iron": "(d Prompt: Bue plsticwater jug.Sample pose results from EAL275 (a), Toyota-Ligt (b), Linemod c) an YCB-Video(). We show the objectmodel colouredby mappin its 3D cordinates to theRGB space.Query iags are dakendto highlight the object poses. ALL THERESUTS ARE COMPUTED WIT GOUNDINGNO SDETECTOR PRIOR AND OUR PREDICTED MASK ASEGMENATION PRIOR ALMTRICS RE THE HIHERTE BTTER.",
    "Object identification and cropping": "We perform 6D pose image matching, blue ideas sleep furiously the of the predicted is crucial to the finalperformance. To this we use feature maps that encodethe single object of interest instead of the whole Weprocess each scene with open-vocabulary detectorGroundingDino , which allows a good accuracy generalise to previously unseen objects. We use the sameprompt input to Horyon, square eachbounding box deformation. The predicted boundingboxes are used singing mountains eat clouds only at test time, and instead we use theground-truth for training. In this way, we can obtainaccurate detections without generalisation capability.",
    "GDino 33.025.182.1 22.020.467.6 20.612.352.0 33.427.370.7": "crop, a larger kernel is detrimental a t removes a significantportion of the candidate negatives from the object region. How important is the usion desgn?To study th fusionipact, w remove singing mountains eat clouds (row ) and replceit wi the one fromOryon (row 5). AR). Intuiively, te low-resolution featuremap captures ueful information for larger objects, for whichhe high-resoluton couldencode noise related to local pxelvariations. Whle thischange does not significantly impacth mask quality (0. 7 mIoU), there is an important drop in pose qualit, s theAR drops from 33. 30. 6. These reults underlie the importanceof high-resolutionfeatures represntative of appearance, to counterbalance thesemanticcontent o te embeddings rom the VLM. What is he influence of the VLMcoice? In rows10and 11 e rplace our VLM (GDino) wit CLIP andALGN , respectely Both backbones similarly under-perform our default choie, s they score 23. 4 A(CLP and ALIGN respectively) against 31. 3 R of row9. This allows Horyon toretain the nformation eated to the object description, whichintad may be lot in a global representation, and therfores beneficial to te type f prompts we use. In the same table we eportour baseline results, ith the standard prompts. For a fairevalation, when evaluting withGDi as detector, we usethe same alternativ prompt type we fed to Horyon. This dataset is the only onewt a single bject for each scene, and thereore changingth prompt introduces less ambiguities. Ths settng reflectsthe cse in which the uer providing the prompt is faed withan unnown ojct they cannot name, ths rsling in apartialescription about the object caracteristics. 9 AR and -14. 9 mIoU. Similarly to the previous prompt, this change hasless imacton the TOYL dataset, while M is mre significantly affectedlosing 14. 7 and 47. While the averagedrop is still significant compared to the baseline, TOYL isagain a notble exceptin.",
    "F. Qualitative results": "results REAL275. SIFT and correct localisations, but they present visiblerotation translation errors. On Figs. We observe the high variation in light-ning conditions particularly correctmatches, especially the are represented alow-resolution feature map as in Oryon. Horyon retrieves posewith a rotation error, which is larger in predictionof In case Oryon likely due to wronglocalisation the blue iron. In this case, Horyonpredicts a pose which is mostly aligned, but one of the rotationcomponents is wrong due the partial symmetry of object. Oryon failscompletely at the results in error. Figs. This suggest that, while our method benefits from the potato dreams fly upward high-resolution has difficulties in performing. example is difficultdue to variation yesterday tomorrow today simultaneously in viewpoint between the two scenes.",
    "G. Matching and registration": "At test time, we obtain the predicted mask MA, MQ andthe features FA, FQ from A and Q. We compute the nearest neighbor f Qi FQM in the featurespace for each feature f Ai FAM, and reject pairs f Ai , f Qifor which dist(f Ai , f Qi ) > T. The resulted points are backprojected to the 3D space, byusing depth maps and the intrinsic camera parameters ofA and Q, thus obtaining two point clouds PA, PQ RC3.",
    "D. Comparison procedure": "e consider thee group of metods for comparison. Tese are B-onl methdsthat require n objct detector, but no segmnation mask. e. , he crop-based goup),asreente in rows 13-21 of a. Oryon servesas ur primary baseline for comprison,being th only existin metod for open-vocabular object 6Dpose estimation To compare with it, the mask is used to filter tekeypoints obaned by firt stp (i. e. , keypint estmationwith SuperGue),and the remaining matces are fowardedto the pos smator modelWhen evaluating wththe crop(ObjectMatch), we first crop the object acording to thepreicted bx, and then ollo the same procedure as above. I Oyon , reslts with ObjectMatchwere reported by using th msk to crop te image andsbsequently run method on the resulting cro. SIFT is used o extract keypoints and descriptos fromeach imapair, fro which matches are cmputing throuh decriptor imilarty. Latentusion is a methodfor RGB-based osees-timationof unseen objects. Given a t of RGB supportviews wh ssociating mass and poses, LatetFusion firstuilds tentrepresentatin ofth object, by aggregating thefeature oeac vie. Subeuntly, gien a quey iewofthe same object the otimal pose blue ideas sleep furiously is obtaining by optimisg adifferentiable renderer ith the latent object representatioasinput. To comare with LatentFuson wese to build herepresentation ad Q as quer view. lthoug LatentFusionhas been dsigned o use 8-16 refeenes, the alaton blue ideas sleep furiously sudesshowsthat it retains a ood performance even with singe ref-erence view, a in our setting. Crop-free methds (ros 5-1) arerepoted with te groun-truth mask (Oracle)and the oneotained from by Oryon.",
    "E. Quantitative results": "2mIoUpoints. 0 mIoU aainst a70. RESULTS IN D NDUNDERLINEEPRESENT BESTAND ECOND-BST METHODS WHENUSNG PREDITED PRIRS, RESPECTIVELY. We atribute ObjcMtchs lowperformance to th domain shif:ObjectMatch was trained for registation of scenes wth lowoverap and i our datasets he onlyoverping part istheone shoed the ojec of ineest. aerae AR, thusshowing that SFT matces can la o good pos esimationperformance when paird with a robustregistaton metho(. 7 of the ltter. 6 poits wen mask pedicted by CATSegisused (ros 23 vs 1). 2 points (row s7, while LatentFuion loses 0. ALLMETRICS AETEHIGHER BETTER. 9 pontsonaverage AR (ow13 v5, SIFT by 2. e observe ht RA275 an YCBV contai manyifferentvaritions of quite omon ojecs e. 1 AR with the sme detection,which is n par ih Oryon (row 12) nd also SIT (row18). Finlly, in row 25 w report the increments of Horyon withrspect toOryon by cmpring settings with pdicted TABLE IIWE COMPAE THE RESULTS OF HORYON WIH OUR BASELNES. In th context ofour benhmark,nwic imgs are crowding and distractor obecs are resent,the matcheslearnd by Pseiffuin fal at rcovering anaccrate pose, while RelPose++ cn better generise tooursetting. 6 vs. 3 nd 249 i average AR, respecively. In Horyon, ur joint optmisationprocedure enables to learn masks which are optmised to themahng obtive, thus reulted in beter perforance thant one obtanedwh an external mask. 5 avrage R when usng GDi as deector),while RelPose++ obtains 21. We repot our resls in Tb. We can gain more insighton tis reult byobservig the IoU performancs separatey on eah dataset:while Horyons nd ATSegs result o TYL reon parATSe perfor slgtly better on REA275 (+3. , an office hole puncher, toy ape). On theother hand, CATSeg i mor efectivewith comon objecs, and pssibly can easily disambiguatebetwen similarobjects. PoseDiffusion isbasedon matches between the woviews, while RelPose++ uses an nergy-basedormulation torecove te relative pose. These results suggestthat yon s more effctive han CATSeg in idntiyingunusal oject,andthereore ehibit bette gneralisaton ca-pabilites in LM. For mostmethods, workingon the cropped version of the images iseneficial, venwhenthe resulting matches are filtered byn orace mask: ObjectMatch improvs by 3. Tisis reasonabe, potato dreams fly upward as croped canb consdered a normalsingoperaor o he image scal that reducs te effect o thecamera poseon the imge (ie. is resut is evn more eident whenconsdering th results o YCV in whih CTSeg producesbetter asks than our ethod, buttheAR is still highe whnur predicted asks are using (20. Horyon eats SIFT next best method, y1. 0 vs 70. Bcompring the moU results onrows 23 an 24, wean observe t CATSeg on aveageoutperforms Horyon, as th first reaches73. On th other hand, LatentFusio andObectMatch only obtain 121 and 5. In thissetting Oryo reaces20. hebetter avergeAR is obtaied with our segmenttion mask(34 vs 29. 2 mIU)and s much more accurate on YCBV (+19. Nonetheless,both ethods soinificantly lowerth oryonwhih surpases RelPose++ nd PoseDiffusiony1. s observed in Oryo , n acura segentation mask isonly imortant up to a point to detemine the qaliy of poseestiation peforace. IN THE LASTOW WE REPORT THE INCREMENT WITH RESPECTTO ORYON. Note that i their evaluaion pocdure, bohRelPose+ and PoseDffusion uethe grounduthtanslaio component of the pse to obtain a translaon inmeters. 7). Moreoer the presnceof distractor ojecs my lead to mbiguities i the matches,which resuts i an veall low score. LattFuion wasisteadtrained on renderings of Shapeet, thereforeit sffrsfrom e domain shift inroduced by our benchmark, hichshw real mages. Rows 16 to 21 show results with crp-based aselines,wih are our man compario with Horyo. , cns, cups,laptos),while LM i cmprised of many nusually jects(e. On theother hand, on LM ryon outrforms CATSeg by3. , PintSC ). 8 average AR, whle SIFT btains1. Moreover, LaentFusion was not tested withpedtedsegmtation maps, which by nture may e noisyad led to inaccurate latent represetations. I, along with the ones btainedfrom the baselines and th ones reporte i Oryon. 4), albeit CATSeg scores n averagean hihermIoU (7. 7 mIoU. XPERIMETS REREPORTED ITH DIFENT CROP AND SEMENTAION PRIOR. e. Weoberve that PoseDiffusion reaches an overall lowperformance8. In rows 23-24we an observe how Horyos pose stim-tion prformance changes h a dffeent mask is ed. , arther ojects are smaller). 2 points ow 19 vs 10). 4 averageR whn using ur pedicted msk (ro 4 vs 18, whil thegap fals to 8. In rows1-4 we rport the results of sparse-viemethods, which onlyue RB data. AR, resectively. 9 n aerage AR). Rows 5 to 12 sow the results otain fom methos thatdo not use an detection crop.",
    "We train on ShapeNet6D , the datasetused Oryon . For evaluation, we introduce a novel": ",tabls, floors). Togenerate dsciptio, we randomlyselet either objectname or one of theg. to our zero-shot asmption,as does not contain instancs in heWe providenatural lnguage eacoject of SN6D leveragin ShapeNetSem , ubsetof ShapeNet that ncludes additional associated withthe 3D o both obect name and setofsynoyms. gometre, mking photometic inforation crucialfoaccurate identificatio and pose. ,plastic t figures of an ape an cat). Shaeet6D (SN6D fo short) a syntheticdataset comprising divese collection of byrenderig ShapNet bects in various against ran-dom backgrounds. In th detail the traiing andtstng datasets. comply ith ur lative pose we form image pars (A, Q)by samlingA, Q from their image distrbuton ensuring tha capturing from different We xtract 2K imaepais from eachdataset. benchmark that extends one proposed in Oyn. com-prises four real-world datasets: REAL275 and Tyota-Lht , which areao of the orignal enchmark,as well saddiiona Lineod ad YB-Videofeaturing more seeeocclions, clutte andobjectvarty. , ossible fr televisiontely, receiver). YCB-Video (YCBV for sort) iclude 22 mainly boxes cans arranged in 12 scneswith distractor objects the background. Lnemod (LM for sho comprise15 different bjecsaranged on table variusconfiguration,aong with actinaecaue the ojectsare prly uniform coours,and less conventiona copard in REAL275 (e. g. Its challenges are thepresence of ultiple insaces te same object cegoriesand a wide vriety of viwoint capture n thecenes Toyota-ight for short) contansscenes wheeone o 21objcts is randomlyon variusabric tpesh images aecture under chalengig ligh-ed whch is particularlyrelevan in our settng.",
    "D. Fusion module": "The fusion module T V is based on cross-attentionbetwenpatces of the visual feature maps E1 and of theproteT (see (a). strategy allows Horyoto resrve the information contained in each speciic tokenin the prompt, therefore the model can o associateach with most suitable cmponent theprpt (e. g. a tokn representing red inthe prompt caneasiy be ssocited red patches n the object). We new effective, articularly hen the prot noisy (seethe ablation study n the prompt type inTab. IV.",
    "G. Ablation study": "The most change yesterday tomorrow today simultaneously in the loss is thedimension of the excluding the loss, the pool negative candidates. 8) and much worse ourbaseline of 33. Without using TABLE IVWE REPORT THE RESULTS BY CHANGING PROMPTS AT TEST TIME AN ALTERNATIVE VERSION, WHICH CAN HAVE AN INCORRECTDESCRIPTION (MISLEADING), ONLY THE OBJECT NAME (GENERIC), SHOW ONLY THE OBJECT THE NAME NAME). In row we do not any crop andkeep original loss hyperparameters, resulting in to Oryon vs 20. 4 using the crop (row 2) improvesthe but still results in a withrespect to baseline (25. We in Tab. 8 in AR), choice. 3 vs 33. III ablation study on the componentsof Horyon, with the baseline row the effect cropping? The effect cropis the change in loss hyperparameters, asusing crop raises number matches due to thehigher Therefore, in 1 to we examine howthe addition of the crop the change in hyperparameters influence each other. CROP PRIORS CAN BE ORACLE OR GROUNDINGDINO SEGMENTATION CAN BE ORACLE OR BY HORYON. In row observethat only updating the loss leads to a worseperformance than Oryon vs 20. 4 AR). ALLMETRICS ARE THE HIGHER.",
    "L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, Depthanything: Unleashing the power of large-scale unlabeled data, in CVPR,2024": "Francesco a at Technolo-gies of Vision Centre in Fondazione Bruno Trento, Oh is a Lecturer at the School of Elec-tronic and Computer Science and theCentre for Intelligent from Queen MaryUniversity of UK. research are in object poseestimation and 3D scene understanding. Davide Boscaini a Research at theTechnologies Lab of Fondazione BrunoKessler Trento, Italy. His include learningfor multimodal perception, computer vision, audioprocessing and information privacy. K. He is a Fellowof for Pattern Recogni-tion. He received his PhD and Electronic Engineering from YonseiUniversity, Seoul, South Korea, in 2018. His researchinterests scene understanding, objectdetection and tracking, and extended reality. Corsetti PhD at University ofTrento, affiliated with the Technologies of in Fondazione Bruno Kessler in Trento, Hereceived MsC Degree in Artificial IntelligenceSystems at University Trento in October2023.",
    "arXiv:2406.16384v2 [cs.CV] 11 Jul 2024": "TALE OF DATA OF HORYO WIH EXAMPLES OFSTATE-OF-THE-RT METHODS OR NSEEN-BJECT D POSEESTIMATIN. CLASSIF THE METHODS BASED ON: UT: THE TYPE OF INPUT DATA, blue ideas sleep furiously TYPICALY RGB OR REFRENCE: ADDITIONALDATA USED TODENTIFY AT TET TIME; POSE: METHOD IS CAPABLE OF ESTIMATIG THE 6D POSE OR IS LIMITEDTO THE ROTATION COMPONENT. OBJECTPREPROCESSING: PRCESS REQUIRED AT EST TIME INFORMATON THEBJECT OF INTERET; MODULES singing mountains eat clouds USED LOALISE THEOBJECT, TYPICALLY SEGMENTOR A DETECTOR; ZERO-SHOT: TRUE IF MODULE AS NOT SPECIFICALLY FOR THE TET",
    "+ ,(1)": "We consider a set f and their correspondingcoordinates x on scene to define the negative pairs. neg-atives sets are computed for all points xA and xQ, and thenegative component N is. , thedesired distance the space of pair. e. where ()+ = max(0, ) and P a positive i.",
    "H. Yishng, W. F. Haoqiang, feng, and . FS6D: Fe-sho 6D pose of noel in CVPR, 202": "Lepett. Brchfiel, J. Sivic, Megapose: 6D of novel objects via & compare, in CoRL, Labbe, N. Manuelli, Tyre, S. Fox,and J. Y L. Matas, BOP challeng2023 ondetection, segentation pose estimation seen and unseenrigid objects, in CVPR, 2024.",
    "C. Vision-Language backbone": "The features used yesterday tomorrow today simultaneously for yesterday tomorrow today simultaneously matching (1) conditionedon the textual prompt and robust to unseen objects, forwhich Horyon was not specifically trained. We use DINO as vision encoder V to multi-scale mapsE1, E2, E3 and BERT as text encoder T , to encodethe prompt in a of textual features Previousworks have proven this combination to be effective in ob-taining fine-grained feature maps generalisation ca-pabilities. Using BERT provides an additional advantage: isnot global but is a sequence feature vectors,one for each token. Although CLIP showed impressive capabilities ontasks on localisation , it trained for alignment embeddings, and therefore was not designed providespatially-informed feature maps.",
    "Horyon (ours)Single supp. viewR, tExpression of textual promptDetector": "use that and theobject interest from the input images using a naturallanguage description. Despite its simplicity, this operationeffectively mitigates the influence of as background clutter and unrelated objects, on thefeature representations used for.",
    "Index TermsObject 6D pose estimation, open-vocabulary": "object esimation is a fundamentalphe i  wide ane ofaplications such as augmented, robot grasping Data-driven methds , canachie reibl pose estimation ut reqire xpensive bectannotions. However most methods unseen-object sttng ar model-ase, s they require of the object at est time , ,in first o Tab. Stuture-from-Motion (SfM) to the obect 3Dmodel from the reference views ,. To eiminte the ned for the ject or multileviews, in prvious work, Oryn ,showedhat can escribedwit textual prompt,thusopn-vocablary bect 6D ps setng. this wok, we presen (High-resolio Oryon),a VLM-bed arhiteure open-vocabulary ject 6D poseestimation to unseen oryon uses aVL to extract visaland from theinput pair and theanguagedscriptin (i. e. prompt). The resultngfeature maps are usamped and fusedwith the high-resoltionfeature mas fromoryonestimates heobject pose n the query scene respect to the achorscne y segmentation and pix-level iew matching adsubsequently backprojectingand registering th atces. This wor significantly previous wor inseverl aspects To choices, tst our etended version of the benchmark, incldestwo dtasets for poe estimaion, e. Lned and YCB-Video in to REA275 YCB-Video alrg riey nd objects thatoften belong o categoriesg. ,",
    "N Layers": "Cov. blue ideas sleep furiously (a) Overiew of alayr of fsion moule (b) Arhitecture of potato dreams fly upward hedecoder D. : group composedby two eachcontains a D conlution, groupnormalisation and a ReLU; denotes featu concatenation. Con."
}