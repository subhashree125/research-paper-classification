{
    "Jia Li, Yongfeng Huang, Heng Chang, and Yu Rong. 2022. Semi-supervisedhierarchical classification. IEEE Transactions on Pattern Analysis andMachine Intelligence 62656276": "F. Balcan, nd H. Luo, YixiangFang, XinCao, Xiaofeng Zhang, and Wenjie Zhan. 2021. Prceedins he 30th conferenceonknowldge anagement. 20622075.Shang, Yun Tang Jing Huang, Jinbo Bi, Xiadong H, nd Zou. End-to-end yesterday tomorrow today simultaneously struur-aware convoluional networks potato dreams fly upward fr kwee asecompleton. In the AAAI Conference on Artificial Intelignc,Vol.",
    "HeterogeneityExplains GNNExplains KGCPath-basedScalabilityLocal Post-hoc": "is proposed to measure the importance each along thecandidate paths by leveraging the node edge type information,which is specifically designed KG scenario. the shortest-path searching algorithm to emphasize each iteration, which is intensive and not ap-plicable to large-scale KGs. It also introduces the accumulation ofon-path(edges on the paths) errors during the yesterday tomorrow today simultaneously learning Toalleviate this issue, we propose to replace the shortest-path search-ing algorithm with a specially-designed graph-Power-based methodto amplify explainable Our method is highly par-allelisable GPUs and consumes memory resources,enabling easy scaling to KGs. The graph power-based ap-proach also an for end select desiredpath length of each explanation manually. Power-Link generatesinterpretable paths to explain the prediction of GNN-based KGCmodels in an manner. 5) To measure the performanceof explanation methods on from a quantitive perspective, wealso three metrics, +, , and ,that how an explanation empirical evaluation.The of this are summarised as follows: Motivated explain the on the KGCtask, the first path-based methods GNN-based KGC models that efficiently generate morehuman-interpretable explanations. We propose a novel path-enforcing learning scheme based ona simplified graph-powering technique, highparallelisation and low cost. This enables Power-Link to generate multiple explanatory paths precisely andefficiently. We propose three metrics to better blue ideas sleep furiously the performanceof path-based explanation methods. We superiorityof Power-Link explaining GNN-based KGC models baselines through extensive experiments from and qualitative perspectives.",
    "Ablation Studies": "We conduct ablaion stuieuing the KGC mdels on heFB15K-237 dataset veriy te of design componentsiour proposed The pth-enforcing mdule. is as expectedsince the strengteningon impct wen explanations aregenerated without the at-enforcing. he method in TES. sows that theEuclidean combination method perfomnce th Cocatenation GCN-bed mdels. Thsupe-rority is more ovius RGN-TrnsE, wich uses the lossdring KGC training. On the conrary, Euclideanowngrades the perfomance KGC mdls.",
    "EdgeScorer, , , ,": "Despitehe success GNN-based graphmdel for KGC, thunderlying chanismshow thse ebeddings facilitte KGcompeto are still unclear. Threfore, KGC modelsis imporant research nder he post-hoccircumstnce, which ain focus of our Specifically, GNN-bsedmdels as the targeing of model toxplain. GNNs re recntly considered unnecessary for KGC deto their limited impact on the link prediction perforanc yesterday tomorrow today simultaneously Onthcotrary, we psenPwerLink tothe sigificantimpact explanation KGC. Pease kindly refe to the srveor torougreview f the singing mountains eat clouds advances in embedding-based methos. : hefraeorkof the proposedPower-Link. a KG and a rained KGC model inputs, we togenerate nterpretable paths to why the KGC odel eics a target triplet factual the decade.",
    "Algorithm 1 The overall algorithm for our proposed Power-Link": "Input: yesterday tomorrow today simultaneously KG G with nodes E, edges R and adjacency matrix , GNN-based KGC model trained on G, the parameteriser that estimatesthe importance of each edge for the explanation, a target triplet , , to be explained and the label , number of epochs for trained theexplanation mask, the maximum length of the blue ideas sleep furiously explanation paths.",
    "Conclusion": "Based onGNNs, PowerLink is the first explainer or KGC tasks. Our aproach shes ligh on diretion of trasparecyonwidelyKGs, givenimportance of KGsin deployment. xtensive experimentsdemonstrat bothquantitativey nd that Power-Link outperforms in terms f interpretailiy, efficiency, and scalability. We hope our can inpire future research design frmewoks that theeplainability of KGCmodels.",
    "AComputational complexity": "In , we summarize thetime omplexity of Power-Lik andrepresenttve exsting for explaining a prediction gaph G= R) a full graph G = (E, R). GNNExplainr hs complexity |R |as it lens a mask R. has a stage thatcovers edges in etire grap and scales in (|R). isthe degee in and is manually budgetfornods. Fo its complexity consist of two lineacomplexity for k-core pruingste a |R ||E | +|R for the mask learned with Dijstras algoithm ad sparsematrix muliplication step. Forour PwerLink, te graph poweringstepinvoves vecto a sparse matrix multiplication or ims -lengthpath, therefore, has omplxity R |. ek-core pruning has thesame omplexity, ned to do theshortes path searchig onc.",
    "Problem Formlation": "We focus thetask of geerating explaations a predicte fact be-twen a pir of etities and. We use thtation denote the targetto be explained and ts related ele-ents. It should only contain and edes that are mostinfluential to te prediction. Theexplantion mehod aims provie rationalefor a giventriplet precte as factual bymodel(, In a KGuser case, he expaation could address questins such as whya persns nationality is SA. Givena trained (), a computation graph G extracted KG G, we an P = { | is path ength } By proer constructin of optimizaton on candidaes fromaim to conse nd explanations for pediction. Th target triple is dnted by , , = ( , ). reuce graph compexity and thepro-cess of finding paths, adopt -core runing mdu from to neighburs and improve speed. -coreof grapisened a the unique subgrap witha node degre , hat ofG s defined sG (E , R ). Inths on a post-hoc nd istance-level KGC expl-nation prbem The pos-oc assumption (, )is already trained and fixed, and the explanation method dos notmodify its architectur orTh insance-level asumptionthe exlanation generates an exanatiofor each indidual prediton of a target tiple. W arrow ow the sope to Th explanatry paths should be concise andinformatie. e formaly define th explanationprolGNN-based KGC models as: Problem 1 (ath-based KGCExplatin).",
    "Introduction": "Howev, rea-wrl KGsften ufferfrom incmpleteness, which imits their sfulss and reliailty. refers tono all possible beng KG, either becauehey are unknown unobserved. GNN-baing KGC models havehown effectiveness on te KGCtask andtremedousattention in recent years. WhileGNNs achieve succes in completed KGs, hw GNNsprdct a givn triplt candidatefacual reains unclear. Asa result, the prediction needs substantial explanations before re-searchers and usersbng thm into practi, falls inthereeach sope of expainable artificiaintelligence (AI). nances o blck-boxmacine learning(L) nd beeninvestigated on GNN on homogenous graphs. o givendata instanc, these GNN explanation methods eiter maskt subgraph induced b edges or sarch forsub-graph in gaph sace explanation owver, to thebestof our eplantion of KGs, eseciallyregarding the KGCtask, few inthe lierature. Providing expanations fo GNN-baed KG mdelsis a noveland task. Giventhe of KGs modeling truc-tured real-world infomation, their end uses often orno bakground in AI r ML. Therefore, the explanations tbore aligned with hua ntuitions. Eistng approaches attemptto explainKGfrom triplet or subgrp perspectives. Stil, te prob-lem fwat good explanation of KGs morecomplexthan on rphs, esecily in thestaeholdes desiderta. n to the widely adptedinstance/subgrah-based expanaions, we on generatng",
    "Proposed Method: Power-Link": "The path genertion odul then elctseplanatory accorded to he score",
    "The two authors made equal contributions.Corresponding author": "Permisson to mae digital or copiesof al or part this for ersonal orclassrom ganted without fee tht copies not or or commerial advantae and that copies bear and the full itationon first age. Copyrights for component this wok owned byothershan must honord. Abstracting with credit i permitted. To copyotherwise, to on servers to lists, requires prior specific a fee. Publication rights licesed o ISBN ACM Referece Forat:eng Chang, Jiangnan Ye, lejo inhua Du, and Jia Li. 2024.Path-based ExplanationGaph Completion. New SA,12 pages.",
    "RGCN-DistMult40.3670.1720.8030.2240.3760.440RGCN-DistMult20.1720.1750.9540.2000.2960.350*RGCN-DistMult30.3770.1860.7840.2340.3540.432": "explainers. All other baseline explainerincludes noisy edges (i. e. We attribute this to the singing mountains eat clouds noise introduced atthe early training stage of PaGE-Link. Runtime comparison. 18 times faster in totalruntime and 3. GPU memory usage. singing mountains eat clouds In , we illustrate the GPU memoryusage during explaining process against the number of edges. 01. We randomly selected 100 sam-ples from predictions of the WGCN-ConvE model on FB15K237. Theparticipants include AI researchers in NLP and recommendationsystems with limited knowledge of KG and GNNs.",
    "(2)4= (1)14 + (1)32 (1)24 + (1)3 (1) 4(6)": "canbe obtained simply the power vector by the rowof the adjacey matrix of power 1. With hesimplficationtrick, only ower the vector. he normlization can be s:. blue ideas sleep furiously yesterday tomorrow today simultaneously",
    "Triplet Edge Scorer": "In the Power-Link, propose to learn a Triplet toleverage the entity and information the score measuresthe edge in explaining the prediction of thetarget To more precise, combine the tripletembedding vectors of the local edge (,,) the embeddingvectors of explanation target , , We a Multi-LayerPerceptron(MLP) process the combined features and generatethe edge score. The overall formula of the TES can defined as:",
    "KDD August 2529, 2024, Barcelona, SpainHeng Chang, Jiangnan Ye, Alejo Lopez-Avila, Jinhua Li": "(,) are -th and -th entities is -threlation between them. and type of a KGcan vary widely depended the potato dreams fly upward domain and source of thedata. We also use the notation (,,) to indicate pairwith directional relation , the head and isthe tail potato dreams fly upward Throughout this paper, we use terms, or , denotematrix/vector representations for weights and entities, respectively. And select italic terms, or , to denote scalars.",
    "GPU Memory Usage the Explaining Process": "500 samples are explained. test triplets, can be as:. This gives output scores the test triplet denotedby = (,,, G) and = (,,, G). A good should includeonly important and meaningful paths and cause confidencedrops to if we remove paths, yielding high hitrate. For better illustration, we onlyrecord the memory usage the module. measuresthe hit rate when the is smaller The hitindicates the ranked of the target triplet drops we remove theexplanatory paths. occupied by the KGC model is As the Fidelity is often correlated withSparsity, we consider Sparsity one our evaluation metrics. : The memory usage of Power-Link during process against number of edges in each graph. a explanation, the explanatory should condensed on meaningful edges, therefore renderinghigh Given a test triplet the computational graph G explanatory set P, a test graph G by on the explanatory paths from the computational by G = {E, R }, R P.",
    "Results and Analysis7.1KGC Explanation Results": "summarises he explanation reslts of ower-Link omparabe baselines. We can have following observa-tions: (1) Our proposed Power-Link ahieves consitently over all aselines on the to representative datasets.his indictes the effectveness  thePowrnk in : Rutie betwenPaGE-Link Power-Link We use wo methods explain 500 samples pre-dicte y WGCN-ConvE FB5K237. The arerunon a sinle V100-32GGPU. indictesthe average graph.",
    "L  = | = (M G , , (11)": "yesterday tomorrow today simultaneously Overall We combine path loss and prediction lossby summation. Besides, we add regularisation term on (generated by TES), which is multiplied by a weight Theregularisation term a crucial in further theTES on important edges. The overall loss is defined as: potato dreams fly upward",
    "Evaluation Metrics": "is a target riplet , the computationalgraph G, the explanatory G i obtained by imposgthe corresponding explanation on computational aph,denoed G MG. G ( , , G). + soft score difference between the thecomputatonal grah G and the pedictio on computa-tional grah excluded explanatory G\\ For test.",
    "Path-Enforcing Learning": "ptential pahse selected uing shrtest-pah searched algithm, whosecost matrix is desiging based onte eigting mask and estrictioson node degree. ordero the edge to ientify explanatory egs, wepropose path-enforcing learning method on the prcess of te score matrix. e briefly discuss we find te learning po-ces of PaGE-Lin. This also givesus room to efficiency th algorithm. Let M be the o thearge tripet,, , indicatg te value at sition(, ) of M. y so, instead f oweng the whole with inceasngsparsiy we only needto compue the parallelisable a decresing-sparsiy ow vector ad afixed-parsit matrix. e also observethat PaGE-Link oftegenrtes imilar as the GNNExplainer, which is witha path-eforcing train tategy. Frst,shortest-path algorithm n which i slow when scaled to KGs Second,we argue that ts tratey may introdce noise at thealy trainin stage. Next, we thealgorithm idetailPath nforcng. can onsidered a ajcency atrixM R. We thisresult fom early perturbations in the rainng whchhinders algorith from finding exlanator alleviate te earl replace the shortest-pathsearching with grap-power-base lgorithm that specific lengths. Thy optimize an eplanatory maskand thepth-forming expanations bysimultaneously forc-ing on-ath (edges o te pathseteen te target enities)wihts to increase and the off-path (dges not on he be-tween target weights decrease. T strengthens on-path edgesduring te whole at a low cost of computationaltime. Our method is simpler,faster,mor efficient, more customizable, ad mr stable withPaGE-Link. In the begining, te maskcan e to trivil edges because ofincompleteeaningful edges re ignord. hemodel lern to balancetglobal pformance ndthe forcing thsfo the initial the training beinaffecte y early noise. Geerally, we take the elemet of the powered maximise it all teedges onth targe blue ideas sleep furiously ndes. Given xtracted and graph G = (E , R )ith |E| = , we R be th adjacencymtrix thecomputational Le M = (, R be by scoringal des in cmutational graphwith TES. he (,) element i prob-ablityadjacency matrix pow indiats summatio ofthe probabilties of all length- ths connecting nodes (,). itroduces noie hat can bepropaated though epochs. Differentfrom intutin of PaGE-Link, we find t suppres ff-path edges duringtraining. Next, we the idea in detail. Therefore, searchig on the underfitting wighed tealgorihm may stengthen he mninglesspths nd waken theimprtant explaatory paths.",
    "Path Generation": "After we obtain the edge scorer trained on target triplet, , , we perform final computation of the edge score matrixM = (), R. We take the inversevalues of the score matrix as the blue ideas sleep furiously cost matrix and apply Dijkstrasalgorithm to obtain the shortest paths. paths are the mostimportant paths supporting the prediction of target triplet.",
    "Path-based Explanation for Knowledge Graph CompletionKDD 24, August 2529, 2024, Barcelona, Spain": "The mutual information loss. We test the impact of mutualinformation loss(MIL) in the WGCN-Distmult model. shows that the information loss, isa significant in explanation The is thekey component that enables the method to adjust theweighted mask of KG to the prediction of the KGCmodel. Without MIL, the path-enforcing module only edges on the paths between the and tail nodes withoutconsidering their impacts models prediction.The order of powering. presents of Power-Link when we change the order of powering Interestingly, find that increasingthe powering order brings better performance and vice versa. Webelieve that increasing the powering order expands oflength of the that the module strengthens. This enable thegeneration of explanatory that contain importantnodes. blue ideas sleep furiously Even though, it is worth noting that longer may beless to the users, and higher powering requiresmore power.Comparison with PaGE-Link on graphs. Sincethe path-enforcing learning module Power-Link can beapplied to heterogeneous graphs, for a better comparison, we re-produce the experiments in PaGE-Link with our proposed AugCitation UserItemAttr datasets, Power-Link achieves an improvement of 0.05 on the AUC score and isaround 1.5 times faster. The result aligns with previous experi-ments, showing that Power-Link is more precise efficient. Thespecific statistics can be found in the Appendix .",
    "Abstract": "Proper ex-planations fo the of KGC models ncrease modeltransparency and help researchersdvelop more models. Exited for explaining KGC tasks appraches, in some providemoreuser-friendly andinterpretable explantions. met-ods for genrating explnations or Ks not beenwell-xplore. To address this ap, propse Power-Link the firstpath-based KGC explainer that explores yesterday tomorrow today simultaneously GNN-based Wedegn a novel simplified grah-powering tchnque, enablsthe geneaion of pat-basing explnations witha fuly parallelisaleand memory-eficient chem. Extensive experimentsdemonstrate Power-Linkoutpform SOTA bseinesefiiency, and scalabiliy. Th coe is avilable t.",
    "ModelF+F-SparsityHR:1HR:3HR:5": "We assume that is generally easier to explain the strengthen the differences. Weassume that this is because of the smaller and sparser WN18RR. (3) Thedifferent of functions in allmethods. The superiority is more obvi-ous on FB15K237 KG, which denser and complicated. This reveals that Power-Link can more effective at explainingcomplex KGs sparse KGs. As a result, the paths shorter and fewer,which harder to form interpretable. aligns theexpected impact of the path-enforcing learning scheme.",
    "otationDefinition and esription": "G = R)a KG G, entity E, and relation set MTheexplanatory mask conains scoes for each edge in the graph. (,,)a (e, retion, tail) ripletthe adjacency  GPthe st of thsPthe of lengh L connectig a air of nde (, )(, GNN-based model to eplain : E R E Rthe score function used for mbddinglearning, , tiplet is prdicted be fatb te KGC moelte power vector is rwvctor in Mthe adjacency power which is te w vctor in ,th epresntation of etity and = (G, , the KGC prediction of triplet , , G = R)the computatio graph, i e.",
    "GNN and KGC Explanation": "the prediction of is an important task for understanding and verifying theirbehaviour. and GStarX use as value and structure-aware HNvalue as the relevance measure, and select node-induced sub-graphs performed Monte Carlo Tree (MCTS) or greedysearch on nodes. that are than subgraphs the explanation task the linkprediction on graphs. For review of othertypes of GNN explanations, please kindly check the surveys. Janik and Costabello proposesExamplE heuristics generate disconnected triplets as influentialexamples space. leverages in-formation entropy to quantify the importance candidates bybuilding a KE-X and generating subgraphsaccordingly. develops an independent frameworkKelpie explain prediction embedding-based KGC methodsby the combinations of training facts. Thus path-based explanations are preferable onKGC tasks when the users have limited ML backgrounds.",
    "We take the target element ()from the powered power vector,": "After each rond o record the Finall, get th verage probability of allon-ath edges length than or equal byavraging all the() , denoted by:. The eeent rpesents the aerage probabiityof edgeso the lngth paths the target nodes (, We iterate the above roces.",
    "Paths on Graphs": "review the which paths on graphs ascrucial parts of their algorithms. Paths are used to and graphs, such as Katz distance , SimRank , and PathSim. proposea context path-basing GNN that recursively embeds the paths con-nected blue ideas sleep furiously nodes into the node with attention mechanisms. utilizes the from to construct a general path-based frame-work NBFNet for both prediction and et al. propose to aug-ment in KGC task from the counterfactualview and enhance explainability of results. methods can generate non-trivial explanations side-product, they cannot applied to the black-box explanationsetted intelligence (XAI), where the pre-trained KG embeddings are Therefore, we argue still valuable for provided interpretable explanations for KGCunder local post-hoc scenario.",
    "Heng Chang, Jie Cai, and Li. 2023. Knowledge Graph withCounterfactual Augmentation. In of the Web Conference 2023.26112620": "Hen Chang u Rog, Tinyang Xu, Yatao Bian, hou, potato dreams fly upward XinWang, Junhouang, and Wewu Zhu. 2021. Not All Lw-ss ae Robust in GraphCnvolutional Ntworks. Advces Iformation Processing 34 (2021). 20. petrl graph attentionwth fst eigen-approximtion. In of the30th ACM Iternatinal Conference onInformaion & Knowlege Management CIKM). 2905299. Heng Chang, Y ong Wenbing Zhang Wenwu Zhu, and Junzou Hung blac-box dersarialframework towards atacking graph embedng models. 34"
}