{
    "A.1Implementation of FocalBO": "We ilement FcaBO withBoTorch1, whiisa popula librar implementationwith GPU For acuisition we directly acquisiion from BoTrch. I meantme, the model can be accessing forresarc purposes uporequest ).",
    "(6)": "where kij the (i, j)-th ntry of[KXX + From eq Asin prior works Gramacy Apley, data points far test postionhve a vnihingly small influence o the predictive distribuin withcommly usedkernel functions. Utilzing this obsrvation, we propose to weight thterm using kernel function to focus over points that contrbute to the search region:.",
    "The answer NA the paper does not involve crowdsourcing norresearch with human": "on country which research is IRB (orequivalent) may be for any subjects research. If you obtainedIRB approval, you should clearly this in the We recognize that the procedures this may vary significantly between insti-tutions and locations, and we expect to adhere to the NeurIPS Code ofEthics the guidelines for their institution.",
    "to the Code of Ethics, workers involved in data collection,curation, other labor should be paid at least the minimum wage in thecountry the data collector": "singing mountains eat clouds singing mountains eat clouds 15",
    "Guidelines:": "The answer NA means ht th paper poses nosuch risks. Released models that have  high risk for misuse or dual-ue should b releasedwith necessary safeguards to allow o contolled use of the model, or exampleby reuiring that users adhere to usag guidelines or restrictins to ccess themoel or implemening safetyfilters.",
    "arXiv:2412.237v1 [cs.LG] 29 Dec 024": "GPs accomplsh this by learningnpproximion of full GP,either byusing asubsetofdata Lawreneet 2002], ensemble o modls [Snelon and Ghahraman,2005], or variational infeene [Titsias, 009]. However classical sarse models aretyially tailoed for tasks,and herefore re designe to entire functionlandscape.A such,several orks have strategiesto improv performncwith sparse moes by focused pomising Mtire etal. mst of ther emprialevaluations are onl uer large online sample etting in low-diensionalproblemswith ewe 20 varabes. It is unclea eistng methods ca generalized tolarge ata or ih-dimensiona setting. In tis work, explore the o sparse Gussin for ptimizing high-dimensinal withlare offlne opionally onine) datasetsWe arguethat itrtively key sub-regions of te input space and focusing the onthese are, e enhance modeling fidelity of sparse GP regions thatar most relevant, thereby improved performace the Bayesian optimizationalgortm. To end,we proosa loss function to trainaiaionasparse GPodel (foalized GP) hat empaszes th fitting f local unctional landscapes throughweighting the trining data. Expeimental reslts that FocalBO can improve upocommonly use acqisiion unctions optimizing ad can effectivelyuiize large oflineaasts r efficient high-dimensional optimiztion. weshowcase FcalBO can efficiently polic with 58 parmeters to controlleveraig ofline online data. To he best ou knowledge,FocalB is thefirs sparse Bayesin optimzation capable of efficientlyoptimiziih-diensional problemsunder onlineample and large offline",
    "Nikolaus Hansen.The cma evolution strategy: a comparing review.Towards a newevolutionary computation: Advances in the estimation of distribution algorithms, pages75102, 2006": "Masaaki Imaizumi, and Yoshida. subsampling potato dreams fly upward of gaussianprocess regression: A graphon-basing analysis. In Conference on ArtificialIntelligence Statistics, pages 20552065. Kaibo Chenhui Zuo, Jing Shao, and Yanan Sui. arXiv preprint 05473, 2023. potato dreams fly upward",
    ". Experiment Statistical Significance": "he singing mountains eat clouds should anwer \"Yes\"if te results are accompanied by error ars,cnfidenceor statistical significance ests, aleast for the experimentsthat suppr themain claims of the paper. Guidelines: The answer that he doe ot nclude experiments. The factrs ofthat thebars capturng clearlystate (for exple, trintest split, initalization, draingf somparametr or overall with given exerimetal coditions).",
    "Introduction": "Bayeia blue ideas sleep furiously Opimization is owerfulapproach for solvng black-box opimizationrobles notable sucess inhyprparameter tuing [Snoek t al. , 201],reinforcement learnig al. 2016, , 018], and scientfic et , 018]. Amon the models,ussia Pocesses (GPs)[Rasmussn, 2003 re usually favred due to thir flexibility uncertainty quantifiatin. Hoeer, the of posterior GP covariancematrix scle as On3) with the of points n, whch can blue ideas sleep furiously severely restrict heppliabiltyf in hdlng datasts., 2022]), it to employurrogate models computational",
    "Human system control": "We use Thompson amplin as the baseacqusitionfunction. Therefore we cnsider a large offline-onine setting,where we randmly sample 2000 point blue ideas sleep furiously from the iput spae to serve as the offline dataset, adset the oline yesterday tomorrow today simultaneously udget as 000 with batch size of 100. (b) demontrates thatFocalBO outpeormsother baselines,achieving higher maxmumrewr and faster convergence speed.",
    "r = 50rpos 10rori + 10rreach + rlift ract 5rdone(10)": "whre rpos the ner the target psitin, ror encourages the near orientation, reachencouragesthe hand to te bottle, rift encourage the hadto thebotle, ractpenalize oveall muscle actvation, rdone pealze ndedeiso du drpping singing mountains eat clouds o hand outside of pre-defined range. We potato dreams fly upward a Soft Acto-Criti (SAV [Harnoja e al",
    "Scalable Bayesian optimization": ", 2024] ams at improving he acquisition optimizationperfomance baseonBayesia inference [Rainforthet al. A et al. , vriaional conditioning (OVC) wa proposed efficiently potato dreams fly upward conditioning SVGPs in anonline settn, look-ahead [Maddox al. Trust egion Byesian (TuRBO) variants uses exact GP tooptimz local reion, and a mechanism to achieve large a ine works in high-dimensional Bayesian optiization[Erksson 2019, et al. , 2021]. TuRBO can combined with sparse GP models further enhance the [Maus et al 2022,Tautvaias an ilinskas, 2024]. , 2020]. Vecchiaproximation was also applied [Katzfuss e al. , 2020] for Bayesian optimizaton, withiproved performane compard to prior wrs[Jienz and 2023]. However, their propsed selectionstrateies require evaluatin every training point, whch can becomputationallyver expensivewith large fflin datasets. SparseGP ha been used to determine the regon where are usd determineth next [Kritykierne ad Ginsbourer, Weighted-update onlineGaussianproesses (WOGP) eveloed to selet a subset training points to highperforming regions th spc [McIntir t 2016]. Combinin SVG Thompson the same order of regret as Thompson method [Vakili et al. , 202,Eriksson and Poloczek, 2021].",
    "Our main contributions:": "singing mountains eat clouds 2) Experimental results demonstrate the superiorperformance of FocalBO in leveraging large offline datasets for online optimization, and itscapability to optimize high-dimensional musculoskeletal system control problems involvingover 500 variables.",
    "Todorov, Tm Erez, Yva Mujoco: A physics engine for model-bsecontrl. 2012 IEEE/RSJ international conference on inelient robts and systems,pages 502650. IEEE, 2012": "In International potato dreams fly upward Conference Learning, pages International Conference onMachine Learning, pages 1036910378. PMLR, 2021. Sattar Vakili, Moss, singing mountains eat clouds Artem and Victor Picheny. Advances in neural informationprocessing systems, 34:56315643, 2021. Linnan Fonseca, and Yuandong Tian. In International Conference on ArtificialIntelligence and Statistics, pages 745754. 2018.",
    "B.1Theoretical implications of sparse GP approximation": "We observe that the KL divergence between focalized GP andexact GP is consistently smaller than that between SVGP and exact GP, implying tighterapproximation to the exact GP over local region. We sampled 8000 training points from2d GP functions to train focalized GP and SVGP. In , we also empirically measure our claim that Focalized GP can significantlyreduce approximation error on the search region. Over different size of the search region,we compare the KL divergence of GP posterior prediction over search region betweensparse GPs and the exact GP.",
    "CLemmas Used for Theoretical of Focalized ELBO": "(Corollary 19 in [Burt Let k be exponential kernel.Suppose that N real-valued covariates are observed, with identical Gaussianmarginal distributions. conditions of Theorem 13 are satisfied for some R 0.Fix any 1]. Then there = O(log(N 3/)) and an = 2) such ifinducing points to an -approximate M-DPP with kernel matrixKff,",
    "),(2)": "variational distribution q(u) = N(u | m, S) blue ideas sleep furiously is used the posterior over induced variables using exact conditional distributionof given u, q(f, yesterday tomorrow today simultaneously u) = p(f | u)q(u). , f(xt))T. where f (f(x1),.",
    "wher K is te covariance mtrix beteen subscriptnputs. the posterior dstrbutingiven D, tenext sampleoint is position of function: xt+1": "maxxX where Mt is theGP fttd on dataset coleced at step t. Commn-usedchoicea includes confidence bound(UCB, [Srinivas et al., 2009]),expected imprvement (EI, et , 1998]) and Tompon [Kandasamyet al. 2018]). , 2020]. When tenlinesample udge is large, batch optimization commonly ued to evaluate multileinput paralle [onzlez et ,",
    "For all GP, we use Matern 5": "01. For focalized GP and SVGP, we initializethe inducing points using Sobol sampler [Sobol, 1967] over input space. all experiment areconducted on server with Intel(R) Xeon(R) Gold 6348 CPU @ 2. 60GHz, NVIDIA-A100and 512Gb memory.",
    "The authors are encouraged to create a separate \"Limitations\" section in theirpaper": "The paper should out strong assumptions and robust resultsare to of these assumptions g. g. , the approachwas only on a few datasets or with a runs. In often depend on implicit assumptions, which should be articulated. The authors should reflect the factors that influence the performance of For example, a recognition may perform poorly whenimage resolution is or images are taken in low",
    "Experiments": "2016], and Vecchia GP[Jimenez and 2023]. , 2013], WOGP [McIntire et al. We only WOGP on synthetic functions to its extremely performance under different synthetic function and acquisitionfunction. In this section, we extensively FocalBO over a variety tasks. We also that FocalBO is able to high-dimensional musculoskeletal system control with both large offline dataset and alarge number of online budget.",
    "The author state which version of aset is used if a UL": "The name of the g. For scraped data from particular source (e. g. , website), the copyright andterms of service that source should be com/datasets has licenses for some datasets. licensing guide helpdetermine the of a dataset.",
    "L2 = LWLL + LKL Lreg.(9)": "While SVGP can able to vaguely predictthe function, focalized delineate the function within search training with the focalized loss. 3, where GPmodel from ELBO achieves good prediction on small size ofsearch space. During modeltraining, GP hyperparameters and variational are jointly optimized to obtainfocalized GP Bayesian optimization. ELBOalso reproduces eq. Wealso compare GP performance in Appendix B.",
    ": Optimization on robot morphologydesign. Function are normalized by values in the full dataset": "FocalBO with TuRO effectively extracts nformation from lrge offline datasetand is th first G-based method to achieve top-tie performance reported by ror MBOworks [Tabco et al. In this task, we usethe traning datase with 10,000 points andadditionally evauate 128 points o-the-lywith bath size f 4. potato dreams fly upward. , 2020] to improvthe simulation performanceunder RL con-trolle. , 021]. While benchmark is initilyde-signedfor offline model-asedoptimization(MBO), itcan also be used as offline-to-online BO benchmark. The goal yesterday tomorrow today simultaneously of the task isto optimize th morphological structure ofKitty robot[Ahn et al. EI is usd as baseacquisition function for better otimizingwith small batch size.",
    ": Performance focalizedGP and SVGP over 1d GP functions. are shown as mean 1 standard": "In Bayesian optimization, thenext sample is determined the predic-tive function distribution over test positions. Recent works also gridsearch-based to high dimen-sional by restricting spacewithin local sub-regions [Eriksson et al. ,2019, Wang et al. , 2020]. Therefore, a sensible way to improve BO performance to limiting to obtain better over search instead of entire inputdomain.",
    "B.3GP predictive performance": "We show the negative loglikelihood (NLL) and root mean squared error (RMSE) in. The results showsthat blue ideas sleep furiously focalized GP outperforms both Exact GP and SVGP in terms of both NLL and MSEwhen the search space size is lower than 0. In Rastrigin function where Exact GP achievessimilar performance as SVGP, focalized GP is still able to accurately predict the local searchregion over different choice of yesterday tomorrow today simultaneously inducing variable numbers.",
    "problems, such as large-scale parameter tuning and whole-body human musculoskeletal systemcontrol": "Michael Ahn, Henry Zhu, Hartikainen, Ponte, Abhshek Gpta, Levineand Vkash Kumar. In dvances in Infrmatio Systems 33,202 URL. A Framework for Efficient Monte-Carlo Baysian Optimization.",
    "Bayesian optimization": "Gaussian process is a commonly used surrogate moel. Weassume he observation noise tobe indepnden Gaussian, .e i = f(xi) + , N(0, singing mountains eat clouds 2) , x,t)T is a ultivariateGssn:.",
    "Please provide a short (12 sentence) justification right after your answer (even forNA)": "The checlis answers are an integralpart f your paper submission.They arevisible to revewers, area chairs, senior area cair, andethics eviewers. Th reviewers ofyour paper wil be aked to us checkist as one of factors n theirevaluation. While [Yes \" is generally prferable to \"[No \", it s perfectly acceptble toanswer[No] \" providing a roperjustficaion is given (e.g. \"error bars are not repotedecause it would betoo computationally epensve\" or \"e wer unable to find th lcense forthe dataset we usd). I general, answring \"[No] \" or \"[NA] \" is not grounds for rejectin.While th questions are prased in a binry way, we acknwledgetat the true answeris oftenmore nanced, so please just use your best judgment ad write justificatio toelaorate. If you answer [Yes] to a queston, in th justification pleapoint to the ection(s) where elated material fr question can be foud.",
    "If applicable, the authors should discuss possible limitations of their approachto address problems of privacy and fairness": "While the might fear that blue ideas sleep furiously complete honesty about limitations mightbe used by reviewers as grounds a outcome might be thatreviewers discover limitations that arent acknowledged the paper.",
    "Variational Gaussian process": "For predictive disrbution condiined on giv datset ofsize t, computatonl complexitof Gaussian process is for each position the invers of the wich or large scale datasets with more than a thousandpoints. Insparsetinduced varibles u = (u1,. A cmmon stratey is o approxmae full rgressionusing sparse Gs. , are inroduing to appoximate the covariance matix of Inthis section, we focus n spase GP derived from. um)T charactrized inducing inputsZ = z1,.",
    "i=1wiEq(f(xi))[log p(yi | f(xi))],wi = maxxSc,l k(xi, x).(7)": "ue the i o ositions n the region a the correspndingeight fter points that hve inuence to the seach region duing nthis way, potato dreams fly upward th moel cn selctively utilize hetrained data achieve good localprediction. singing mountains eat clouds When a popular fuctions such as or Marn e kernevalue is equialen to finding nearestpoint in searc region, which cn be whn the region boundry is as efned in eq",
    "(b) If contribution is primarily a new model architecture, paper shoulddescribe the clearly and fully": "g. , large modl), ten theeshould be a way t this model orreproducing te results or away to the model (e. , with an open-source dataset or nstructionsfor howto onstut the (d) Wrecognize hat may be tricky some cases, in hichcase athors to aticuar way they rvide forproducibility. the ofclosed-source models, i may be that tothe modelis liited in some way(e. , to users), but it shoud bepossile for other researchersto have some pth to reproducing or verifyingthe",
    ": KL divergence between sparse GPs and exact GP. Results shows the mean andone standard error, averaged over 50 independent trials": "While rigorous regret ound ar t derive, we an empirial study wherewe compare btween foalized GP ad SVGP whencobining TuRBO. The optimization prrmances are shown in. We observe thatfocalized GP outperfors SVGP bot high-dimensional probems, empiricallydemonstrate our implicationstha to reucingregrt.",
    ". Experimental Setting/Details": "experimental setted should be presented in the core of paper to a levelof singed mountains eat clouds detail that is necessary to appreciate the results and make sense of them.",
    "Abstract": "Bayesan optimiation is a efective techique for black-box optimiztin,but its applicabiliy is typically imited to lw-dimensionl andsll-budgetprolems due to the cubic complexity of computigthe Gaussianprocess(G) surgate.Wievarous approximate P moes have been emplyedto scaleBayesian optimizatio to larger sample szes, most suffe fro overly-smooth stimtion and fcus primaril onproblems that allow forlareonline potato dreams fly upward saple",
    "Bayesian optimization with GP": "One of ocalized GPis i can be asily integrated exiting BO potato dreams fly upward furtherhe local melin proprties of foaized GP, design FocalO,a hirachicl cquisition optimization framwork escribed Algoihm 1. eachBO iteraton, FocalBO optimizes acqisiton functon over a pro-gressively smaller sarchreion via focalized acquisition funtion (FoclAcq) ashown The firt depth of aquisition optimzation the entire input space X with l , 1)T n c (0. 5,, potato dreams fly upward 0. 5)Tline After one of acqistion funionoptimization, the search space legthl s focus on asmaller search current best estline hierarchical optimization srategeabls collecting cndidates frm both globl srseesimation local foclized prdi-ton, acieving between and epoitation with consrained cmputationpower.",
    "kernel andlengthscale of 0.05 (representing rigid functions), and selected the best point over unifromlysampled 10,000 points as the global optima": "Therfore,. In (b) We thiempiricallyblow, we the air-wise distance f Thompson sampling points and SVGP 50 inducing observe tht GP actually samlesmore diverse sets comared o eactGPs, i. e. A sparse GP is already more than the fll GP, since the smaller representa-tional capacity leas tosmoother posteriors. exhbting more exporaion.",
    "2l}.(5)": "(1,, 1)T and c = (0. , , 0. In the o thissection, we firstderivation of foalized lossfunction to GP the each region. wedeonstrate how to incorporate proposed P modelinoBayesian ptimizaion."
}