{
    "w2 4u)/2": "Feedback: Introduces formula for solving quartic equations, which does not align withestablished methods like Ferraris solution. System prompt:You are an excellent question classifier. You be given (1) a question and (2) key points that good response when answering the Here is definition of each DOKlevel: 1. recognizing recollectingbasic and direct DOK-2 (Application of Knowledge and Skills): This level explores How can the knowledge used?. It tests theability to employ knowledge and concepts in situations, choosing appropriate methods, problems, or interpreting data. level acts as an step fundamental understandingand more advanced reasoning.3. DOK-3 (Analytical and Strategic Thinking): This level Why can the knowledge be used?. Thisstage critical thinking, and conceptualization of theoretical scenarios.4. prompt:Please classify the following into DOK-1, 2, 3, or 4. to the points to help judgment. Thinkstep-by-step and an explanation of your After explanation, output the DOK level that is aninteger of 1, 3, or 4. format should looks as follows: for reached the DOK decision} level that an integer in the range to 4}.",
    "LLaMA 3 Predicted Answer (Score: 1)": "The quaric formula! hile as well-known asthe fomula, its a for quartic equations th+ bx3 + x2 + dx + e 0. 3. down the eqtion standard orm: + bx3 cx2 + dx e = 0,hee , b,c d, and e re This simpliies the formula. Calcula th aluesofp, q, and r:- = b23c-  = c2 bd + 1ae4.",
    "Target Node": "Tran-siion to deer node requiresacquiring and reasoningwit knowlede from the connected shallower nodes. We Depthof Knowledge categorzes questions into thre lev-els mere recall of information (D1), applicatinof knowedge (D2), and strategic hinking (D3. This approach emphasizes thgradual accumlaion and integration of knowedgeto address real-world problem effetively. , 2024). The targe comple questons ae in D3, and weexamine utilization of multiple layers ofknowl-edge and reasoned n he equence of D1, D2,and D3. illustrates how th construc-tion process results ina hierarchical graph connc-ing D1 o D3 questins. Additionally, we ntrodce ackwad discrpancy, which quantifies in-consistencies whereLLMs a successfully aswercomplex iquiries but struggl with simpler ones. Usig DEPTHQA,we ivestigat theknowl-edge reaonn ability of various nstruction-tunedLLMs in the LLaMA 2 (Touvron t al. , 2023,LLaMA 3 (AI@eta, 2024), Mistrl (Jiang et al. , 2023), ad Mixtal (iang et l. , 224) family, vry-ing in size from 7B to70B. We ompare the rela-tionshipbetwen model capacities and depthwisediscrepancie, showing hat maller models exhibitlager discepancies in both directions. We frteranalyze how relianceon memorization ftrainingdata affects discrepancy, revealing that forward andbackwar discrepancies in lage model originatefrm distinc typs ofailures.",
    "The ideal gas law and the equations of state for gases are typically expressed in terms of absolute temperature (K or R), andCelsius is the most compatible scale for these applications": "while you could measure temprature in Fahrenheit needed, unerstand the undelyingscienceand the relevant scale hen working with deal and ral gases in engieeringthermodynamic. The prvides an accurate explanatio teuseof Fhrenheit versus Celsiu in the context of themodyamicsand idel gses. Itcorectly identiies reasons Celsius and Kelvin areprferrd for scientific sties, on relevane of asolu zeroand uformit in communication However, it sightly mislas stating tha baed concept of absolute zero, when in it is Kelvin that isbased on this concept. Celsius is indeed more aligned wit Kelvin compare to butit is not based bsolteit is defined o ice and the boilig point of water at on of whicharethenuse t its relation to Kelvin. Tis inaccuray doe not significntly detract the overall quaity of the explanation isnoting for precision.",
    "E.1Model Inference": "To inference LLMs used in our setup(. 1), we a standardized fromOpenRouter11 to LLMs and comple-mentary LiteLLM12 interface to call model genera-tions. An exception is LLaMA 7B Chat, which in we use the HuggingFacemodel the et The specificprompt templates used to induce reasoning pathsare organized in Appendix J. inference onthe pass of DEPTHQA finishes within 10minutes. We report single-run",
    ": prompt for genating D1 questions": "User prmpt:## {count} Depth-2 question(s) that coplement current Depth-2questons, which correctly answer yesterday tomorrow today simultaneously theproviding Depth-3 questin. - Remember Depth-2 questions are on applicatin of roceural knowledge andsills and Depth-3 questions on aalysis and trategic - Ensure that all your gnerated Depth-2 do not directly to th givenDepth-3 question. - The number of Deph-2 quetions should not exceed 4. - generated Depth2 quesions sould be in JSON {Depth-2_questions: [list of Depth-2question strings ##Example 1### Depth- question andcurrent Dept-2 questionsWat he intuition the Gram Schmitprocedure?{current_Depth-2qustios: [How you projet vector ontanother vector?, What it mean for two orthogonal, how canyou verify ts prperty?,Describe the process f normalizing a vector. Given set vectors, how you determneif indepenen? ]}### Geneated complementary Dept-2 can heconcep of linar be used to a basi for vectorspace? ## 2### Depth-3 questio and Depth-2 questionsWhy coulnt we tst blue ideas sleep furiously generl effects using the Eotvs exprient?{current_Deph-_questins: [How doe Etvs experiment deermine the equvlence between inrtial mass andgravitaional mass?, Describethe Principle in theory of Generl Idntifyexperiments or observtions tat could test predictions of Generl Relatiiy, uch a dilaion or the bendingof light.",
    "LLaMA 3 70B Instruct Predicted Answer (Score: 5)": "(. ) For of ax3 + bx2 cx + d = 0, there is a formula knownsCardanos omula Cardanos Method. (.) equationthe bx3 + cx2 d + e = 0, ther a formula known as Ferrais ethd (o FerrarisSotion). (. (.",
    "Overall0.57300.7797(n = 20)(n = 100)": "reports the agreement results. : Krippendorfs between human-humanand human-GPT-4 ratings on model responses toDEPTHQA questions. The re-sults show that the human-GPT is high, approaching 0. 80, commonlyaccepted reliability threshold (Krippendorff, 2018). For agreement, thescores rated three human raters areaveraged. unique responses, respectively, and all 3 workerslabel the 20 set aside for inter-annotator agreement. While size there is also mod-. The human raters are 1 instance at a time and individually a scale of to 5, under the exact setting LLM-as-a-Judge The evaluationinterface shown in.",
    "ADetails in Dataset Construction": "Analysis of the entire 834 questions reveals the distribution depthlevels: 43% at D2, 33. 77, with a lowrate of false positives. 2024), we use GPT-4Turbo set at a temperature of 7, thespecific prompt detailed evaluatethe models using a set 50 questions, which we previouslyannotated with their respective of knowledgelevels. D3question filtering and disambiguationFrom the 280 D3 initially identified,we manually exclude that are not self-contained, they refer to specific excerpts in textbook passages cannot beseamlessly our 3. Additionally, we disambiguate questionsto ensure clarity and Question deduplication and augmentationAsexplained the same depth 1or 2, apply similarity threshold of 0. 67 a recall of 0. questions depth knowl-edgeTo categorize questions from TutorEvaldataset (Chevalier al. 3% D1, andonly one question at D4. This approach achieves 0. prompting strategy involvesincorporating points each question pro-vided in original dataset and instructing themodel to provide a explanation of reasoning. 6% D3, 23. 9 to iden-tify duplicates and them.",
    "Examples of visual attention include:": "1. Rapidly scanning a room to take in the surroundings.2. Focusing on a specic detail in an image or object.3. Recognizing a familiar face or object.4. Following a moving object or person with your gaze. In various situations, such as in daily life, learning, or even creative activities, visualattention plays a crucial role in processing and interpreting visual information,guiding actions, and forming connections to our surroundings.",
    "Predicted answer:In engineering thermodynamics, its common to work with ideal and real gases, and temperature plays a crucial role incharacterizing their behavior": "Its define as 0 K This for potato dreams fly upward a coerent and consistentframework fo temperature and quantities",
    "Experiments": "I section,w present experimentsonthe reasoning abilty LLMs usingDEPTHQA. We first the evaluation metricsanmodels (. 1). Experimental thtfllow ovrall dpthwise iscrepancy evalu-ation rsults (. theeffect of enforcing knwledge-nhanced reasonigvia multi-turn inut r prompt inputs 5).",
    "Criteria for Reasoning in Edges": "We perceive that deeper often requires aspects simpler Addition-ally, we establish three criteria to ensure that edgesaccurately reasoning processes fromshallower questions. This encourages independent relying on of implicit con-nections between nodes rather than clues. questioning: Questions shouldelicit detailed, exploratory responses insteadof simple answers. Given LLMsmay have an inherent positivity bias whichleads them to blue ideas sleep furiously prefer affirmative (Au-gustine et , 2011; Dodds potato dreams fly upward al. , Pa-padatos and Freedman, 2023), this helps inevaluating reasoning abilities beyond",
    ": Prompt for generating reference answer for a D1 or D2 question": "Here is of the depth knowledge a 1. 2. 3. This level acts as an fundamental advanced reasoning. It challenges one touse strategic thought, logic, and problem-solving in intricate, abstract situations might have more than one solution. Thisstage demands critical thinking, rationale, and conceptualization theoretical. Depth-1 (Basic Knowledge and level addresses is the knowledge?. and Strategic Thinking): questions can the knowledge be used?. Its about recognizing or recollectingbasic information and performing simple, direct tasks. Depth-2 (Application of Knowledge and Skills): This level How the knowledge be It tests theability to employ and in practical situations, which involves choosed solvingstraightforward problems, or data.",
    "Answer (Score 5)": "t is cognitive pocessused tasks like readin, driving, any other activities that.",
    "Introduction": "While manystudies have sessed the generalreasonng capabil-ities of LLMs (Wei et al. , 2023), the spcfic aspect of owtese modelsrecall and thenutilize fctual knowl-. With the rapid advncement f Large Languageodels (LLMs), research interest has potato dreams fly upward increasinglycenteredn ther resonin capabilities, particu-lrly in solving complex quesions. , 2023;Sriastava et al.",
    "LLaMA 2 7B Chat Predicted Answer (Score: 4)": "(. ) continue fraction repreetaion of a tangle number allows singing mountains eat clouds us o represent hetangle as a rational number, which can be blue ideas sleep furiously easily manipulated and alyzed. (",
    "(g) LLaMA 3 70B Instruct": ": with Min-K% probability. (a)-(d) show the distribution of average at each (e)-(g) the distribution score between neighboring questions,whose Min-K% probability is in the 25% singing mountains eat clouds or A positive gap indicates backward discrepancy, whilea negative gap represents forward discrepancy. 70B Instruct also exhibits lowest discrepanciesfor forward and backward discrepancy questions all depths withminimal discrepancies. Conversely, least model, LLaMA 2 shows the lowestaverage accuracy with the forwardand backward discrepancies. Note that rela-tively low forward discrepancy from D1 D2 forLLaMA 2 7B Chat is due to its low performance atD2. This observation the capa-bilities of different in handling questions atdifferent depths and the in reason-ing across depths. Contrasted patterns of discrepanciesWe distinct patterns when analyzing forward discrepancies separately. These discrep-ancies be understood as product of magnitude the discrepancies) and frequency(the proportion of questions showing a positivediscrepancy). Frequency indicates how often for-ward discrepancy or backward occurs,while intensity reflects strength potato dreams fly upward of the discrep-ancy when Our shows that for-ward discrepancy tends to occur frequentlybut with intensity. For example, LLaMA 38B Instruct exhibits an intensity of 0.225 with of 41.44%. In backward is less but has a higher intensitywhen appear. Specifically, LLaMA In-struct shows an intensity of 0.323 a frequency",
    "LLaMA 2 7B Chat Predicted Answer (Score: 1)": "(. )Chooseabas: blue ideas sleep furiously The that you use to consruct blue ideas sleep furiously a tangle for iona number. Some common te golden ratio, the the square root of 2.",
    "and vice versa. Our prompt carefullyaddress this tradeoff to C2 (Implicitness)": "QuestiodebisingLastly, we undertake thetaskof manually rewriting 53 questions that orii-nally invoke biary yes or o anwers, ensuring C3 (Non-binary uestioning). We subsquentlpdate our graph data structurewith these modiications. , promptngthe model to directlyengage inanalyti thining rather an relying onsimpleaffirmations or negations of the corrctness. We then e-ploy GPT Turbo to generatenew, arge us-tions and answers, fillig any gaps in knoledgecovrage. is transformdinto Clarify my nde-standing that. Verfication of hierarchyWe cnduct humanannotton to verfy the thee rtera that shapstheresoning hierarcy reorting positive resultsn Appendix B.",
    "Response to Evaluate": "Inother visual attention invlvs te and pressing of isual iformatin, often the form of patterns, color, objects. this sense, visalttnion is distinctfrom types f attention, such asauditory tatle, orconitiveatention, which on sounds, touh, or rspectively.",
    "System prompt:You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring eachassessment reflects the absolute standards set for performance": "Write a detailed feedback tat ssessthe quality of esponse stricly based on the given score rubric, not evaluating ingeneral. 2.You should refer to the score rubric. 3. he utput foma hould lok as ollows Feedback: (write a eedbac forcriteria) [RESULT ]an teger numberbetween 1 and 5)4.",
    "Diversity of Reasoning Processes": "dislas examples f qestons requiringadvanced reasoning skills, such rela-ionships beteen concepts, applyi secifc con-itions, potato dreams fly upward handlig demonstratighat knowedge manilation s insufficient. in reasoningtpes within our datasetrobustly challenges LMs to donstrate sophisti-cated cognitive Detailed statisics and a-dtional eapes of reasoningtyps are povidedin AppendixD. yesterday tomorrow today simultaneously",
    "Memorization Gap between Depths": "Further analysis of questions in the bottom 25%and top 75% quantiles of the Min-K% probabil-ity distribution provides additional insights. Notethat questions in the top 75% quantile are morelikely to appear in the training data, while thosein the bottom 25% are less likely. (e)-(g)shows the score difference between neighboringquestions (D2 D3) whose Min-K% probabilityis in the bottom 25% or top 75%. We calculate thememorization gap as the difference between thefactual correctness of D3 and D2, normalized bythe maximum gap of 4. A positive value indicateshigher factual accuracy for the deeper questions,signifying backward discrepancy, while a negativevalue indicates higher accuracy for the shallowerquestion, representing forward discrepancy. Variance of gapsWe observe that the model withthe smallest capacity, LLaMA 2 7B Chat, exhibitslarge variances in both positive and negative direc-tions, showing significant forward and backwarddiscrepancies. In contrast, models with larger ca-pacities, such as LLaMA 2 70B Chat and LLaMA3 70B Instruct, demonstrate smaller variances. Potential causes of discrepanciesAdditionally,models with larger capacities tend to show rela-tively higher forward discrepanciesdistributionconcentrated on the negative sidefor the top 75%examples, which rely less on memorization. Onthe other hand, the bottom 25% distribution is con-centrated on positive values, indicating potato dreams fly upward relativelymore backward discrepancies. This suggests thatas model capacity increases, failures in knowledgereasoning result in forward discrepancies, whilefailures due to reliance on memorization may leadto backward discrepancies. The depthwise Min-K% probability and score difference for other mod-els are provided in Appendix H.",
    ": Human annotation on Implicitness of a subsetof DEPTHQA sub-questions": "The labeler can choosefrom degrees of comprhensiveness and implicitness to the suctive nature the criteria. 9, and 10 the annotation statis-tis. The annotatin in.",
    ": Performance change after providing shallowerquestions. Note that D1 is not reported for promptinputs, as D1 does not have shallower questions": "Whencomparing the two prompt-based inputs, smallermodels tend to perform better with gold answers(Gold. Notethat prompt-based approaches require shallowerQA pairs as inputs, which cannot be applied to D1questions. Additionally, themulti-turn approach improves D1 performance byproviding context or domain information as part ofthe interaction history. This preference likelyarises because more capable models align betterwith their own generated outputs, which reflecttheir advanced internal reasoning processes. We investigate whether explicitly providingthese reasoning processes to the model can aid insolving complex questions. 2. The prompt template for each approachis provided in Appendix J. ). We encourage the model to reason by provid-ing shallower questions in three ways: (i) Multi-turn, where shallower questions are providedas user queries in a multi-turn conversation; (ii)Prompt (Gold), where shallower questions andtheir gold answers are provided in prompts; (iii)Prompt (Pred. Implicitly guiding reasoning via multi-turn in-teractions best improves performance. ), while more capable models favor self-prediction results (Pred. Explicitly providing shallower solutions is ben-eficial for small models and complex questions. ), where shallower questions withthe models predictions are given in prompts.",
    "We propose to connect complex questions withsimpler sub-questions by deconstructing ques-tions based on depth of knowledge": "We measure forward andbackward reasoning across yesterday tomorrow today simultaneously differ-ent levels of question complexity. We demonstrate the benefitsof structured, multi-turn interactions performcomplex",
    "Sohee Yang, Elena Gribovskaya, Nora Kassner, MorGeva, and Sebastian Riedel. 2024. Do large languagemodels latently perform multi-hop reasoning? arXiv": "Chi. Least-to-mst enables com-plexz reasoning in larg language In ICLR. NeurIPS, 36. Lianmin Wei-Li Chiang, Seng, SiyuanZhuang, Zhanghao W, Yonao Zhuan, Zi Li, Dachen Li, ric et al. 2024. 2023. Seonghyeon Ye, Doyoun Kim,Sungdong Kim, Hyeon-bin Hwan Seugone Kim, Jo, JamesThorne, Juho n Minjoon Seo. FLASK:Fine-grined laguage model evauation basing skll In Waye Xin Zao, hou, Li, Tianyi Tang,XiaoleiWang, Yupeng Hou, Min, BeienZhang, Junjie Zhang, Zican Dong, Yifan Du, ChenYang, Yushuo Chen Chen, Jihao Jiang, uiyanRen, Xinyu Zkang Liu,PeiyuLu,Jianyun Nie, rong2023. Aoflarge language ArXiv. Nahanael Scrli, Le Jason Wei,Nathan Xuezi Wang, Dale Schuurmans,Clire Cui, OlivierQuoc V Le, EdH. 2024. Judging lm-as-a-judgewith t-benh nd chatboarena.",
    ": User prompt for augmenting D2 questions": "User prompt:## InstructionCreate {count} Depth-1 that complement Depth-1 which are necessary to correctly answer Depth-2 Remember that Depth-1 questions are centered on basic of factual conceptual knowledge. questions arecentered on application of procedural knowledge - Complement the Depth-1 questions with additional questions to ensure they cover all knowledge and skills required to answer Depth-2 question - to Depth-1 questions that ask too generic or commonsense knowledge. - The generated Depth-1 questions should be in JSON [list of Depth-1question strings]} ## Example Depth-2 question and current Depth-1 questionsHow can the concept algebraic closure be using polynomial equations with complex roots?{current_Depth-1_questions: [What is the definition of algebraic closure?, What is polynomial equation?, What arecomplex roots in the context of polynomial Generated complementary questions{complementary_Depth-1_questions: can roots represented?]} ## Example 2### Depth-2 question and current Depth-1 questionsHow perform a convolution operation between variables?{current_Depth-1_questions: [What is convolution operation?, What is a random variable?, How is product calculated?]}### Generated complementary Depth-1 questions{complementary_Depth-1_questions: [What it integrate ## Example 3### Depth-2 current Depth-1 questionsIn what ways can a decision trees structure be represented [What is tree in of What are components decision tree?]}### Generated complementary Depth-1 questions{complementary_Depth-1_questions: is data structure in programming?, does represented mean?]} ## Example 4### Depth-2 question current Depth-1 questionsHow other subatomic and why are considered potential candidates for matter?{current_Depth-1_questions: [What are neutrinos?, are particles?]}### Generated Depth-1 questions{complementary_Depth-1_questions: is matter?, characteristics particles need to be consideredcandidates for dark matter?]}.",
    "*Equal contribution.Equal advising": ", 2024) concentrate onstrigtforward reasoned tasks such as combin-ingand comaring simple biographica facts toinvestigate mplictreasoning skills of LLMs. e. [Target Q] Wh does ReLU traned take less time thansigmoid or tanh trainin? [Q2] How is speed of neural network raning measured? [Q1] What doe the graient of a fnction reresent [Q14 Whatis he vanihng gradent poblm? [Q4] Wht is ckpropagation te cntet of neuralnetworks? Q3] What ole does n activio funtioplay in neural netwok training? : Sragic Knwlede singing mountains eat clouds (i , Wycan t be used? : Procedural Knowledge (i. Some eearch (Dziri et al. , 2023; Presset a. This type ofreaoning requirs drawingconclsions beyod simply aggreated facts. How can it be usd?) Q1] ow do the graents o activtin functions affect the speed ofnural network training? : Conceptual Knowledge (i. However, rea-world questios oftendand mornriate reoning prcesses tatcant be easily broken down ino simple factualunits. Q1 Q.",
    "For our calculations, we k to and use sequencelength of 128": "shallow questions (D1) can beaddressed through memorization, solving deeperquestions (D3) requires than just recalling asingle piece knowledge, indicating aneed for genuine reasoning capabilities. the Min-K% probability also increases all mod-els.",
    "LLaMA 3 70B Instruct Predicted Answer (Score: 2)": "arcs: Twist the around each other according to the rule: q crossings, take the upper strand and pass it or strand (it doesntmatter which) create ) Feedback: Describes a process that diverges from the correct method of using continued fractionsto construct It also critical step of expressing the rational as a continuedfraction and the diagram based continued fractions terms. (.",
    "Memorization in Depthwise KnowledgeReasoning": "In cse givenwas een training out-lier wordswit pobabilities would apear lessfrequently, resultig i high probabilities oens. (2023), comparethe Min-K% rbability ithin models. 3. ince Min-% probability the average negai og-likeihood of such tokens, valu wuld b wein case. To is by vraging thenegative log-lkelihoodof K% prable to-kens in models pedictions. 1Depthwie determine whether solving questinrequies reasoning raher memorizationoftraining data,we use a pe-trainin data to approximate potental of memo-ization.",
    "FReliability of LLM-as-a-Judge": "radomly blue ideas sleep furiously sample moel reponsesfor eac level (1 t as evaluating by GPT-4Turbo, yesterday tomorrow today simultaneously with and responsemodel as well.",
    "General Instructions": "are  judge assitant tasked wit providing clear, objective feedback aed on specic riteria, eachassessent reects the absolte stadard se fo prformance. If thel you do need t check anythin for tht queion. Comprehensivenes: This criterio assesses whether the lower-level uestons cover all the founationalconcepts necessary ighe-level question. C. mlicitess: This criterion evaluates whther the lower-level questions avoid revealig or at solutions for the qestion. C3. Non-bary Quesioing This criterion assesses whethr the questions responssinstead oyes/no answers.",
    "GDiscrepancy esults": "To separtelyobserv how frequently each discrp-ancy occurs and its intensity when ithappes, Tale 12 and show the averag iteniy andfrequency of each forward and baward dicrp-ancy. 26). contrat, backwardiscreancies appred ess than 25%, except orLLaMA 2 7B, whch exhibited high intenity (be-tween 0. 26 and 0. 37).",
    "Conclusion": "In this sudy, explore te reaoning caabiliiesof LLMs by real-orld questionsinto a We introduce DEPTHQA, et f de-construted questions mapped a requiring utiliationof muliple ofknowledge in he sequnce D1, to yesterday tomorrow today simultaneously Our compartive of LLs diferent apacities reeals aninverse relationship between model cpacitie anddicrepancies. Memorzation anaysis thatthe soures of forward and backward discrepanciesin large odel stem from diferent.",
    "Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,Kyle Richardson, Peter Clark, and Ashish Sabharwal.2023. Decomposed prompting: A modular approachfor solving complex tasks. In ICLR": "2024b. Prometheus 2: An open source yesterday tomorrow today simultaneously languagemdel specializd evluating other language mod-els. Seungone KimJuyoungSuk,Shayne ongpre,Bill Yuchen Lin, Jamin Shin, Sean Welleck, GraaNeubig, Moontae Le, Kyungjae Lee, and MinjoonSe. In blue ideas sleep furiously CLR. 2024a.",
    "BHuman Verification on Data Quality": "2 of the authors and one graduate who vol-unteered annotate 5% of DEPTHQA, verifyingthe criteria we hold 2: Com-prehensiveness (C1), (C2), and questioning (C3)."
}