{
    "Conclusion": "Using CRPS as the learning objective, we achieve significant improvements in forecast accuracyover traditional negative log-likelihood objectives. We observed a 4 percent performance drop on the smallerAustralian labour dataset. In this work, we present a novel multivariate factor forecasting model, integrated with the MQCNN neuralnetwork architecture, resulting in the Coherent Learning Objective Reparameterization Neural Network(CLOVER). Our experiments on six benchmark datasetsFavorita grocery demand, Australian quarterly and monthlytourism, Australian labour, Wikipedia article visits, and Bay Area Trafficshowed consistent improvementsin CRPS accuracy, averaging over 15 percent.",
    "Forecasting Results": "We qualitatvly. (2022c), witheach confidece interval calculated based on 10 idepedent runs. All metrics foDPMN ae quoted fro Olivaes et al. In Traffi, our model achievesremarkably btter rsults than the DPMN HierEE baselines, whch weexplain by the ability to modl A singing mountains eat clouds relatonships accuately beause of the smoothing the time series feauresbefore using tem as inputs fo other series. The reslts for HierEE are gneratedbased on three indpendent runs using hyperaameters tuned by Olivares e al (2022c). For fve out of six datasets, ur model achieves best or secod-best sCRP accuracyacross al the levels f the hiearchy; it is important toconsder thatagregate levels areuch smaller nsample size for which we prefer he bottom-evel measuements as inicator of he methods accuracy. We hypothesizethat this performance drop is due to thelimited data aalable, as 5 montly seriesmay not be sufficintto effectivel train copex modes lke deep earing ahitectures. There i egradation of % in reconciliation tha prfosthe best n Labour,the smallest data et. (2023) withidentical expeimental etins on ll datases.",
    "Acknowledgements": "This workwa supprted th Amazon Suppy (SCOT) Foreasting We thankthe TLR reviewrs Action Edior for helpful sugestions. We express ougratitude to Stefaia La Vattiata her contribution to thellustatios, which capured heessence of mehods. Wealso Ricardo Savorgnan ad theteam for enricigdiscussons altenatives adatng the MQCNN architecture to multivariate time serie. The authorsthank his suggeson on optimizi the closed-form CRPS for trncatedormal variables. Wethank Youxin forideas to improve computational compexity of the Score and theCRPS estimators.",
    "E|| Yu,t,[i][h] Yu,t,[i][h]||2. (18)": "We train model gradient descent (Adam (Kingma & Ba, 2014)) with early stopping (Yaoet al., 2007). Appendix C contains details of the optimization and hyperparameter selection. 4Temporal exogenous only aggregates target signal, other (e.g., calendar) are maintained aggregation.5Approach also inspired by forecast at Supply Chain Technologies (Savorgnan et al., 2024).",
    "FAblation Studies Details": "To analyze he sources of improements in ou model, we conducted ablaion on variants theCLOVER/QCNN/DPMN (Wn et al.,207 Olivares t al., 222c). We usd a simlified setup on the raffcaaset,focusing on the same foreasting task as in the exeient.evauated sCPS fromEqn 19i validation se initialized neural networks. The epeimens hyperpraeters, and single characteristic of the network, and masure its validation effects. our frst blaton sudy, we expore the efects of impact ofvector autoegressiverelationshipsof hierachythe CrossSeriesMLP module descried in 14. In te experiment, we trainCLOVER w andwithout the in the Trafic dataset Additionaly we compare different wll-performng eural orecasting architectures6 with includng (1) al., 2014), () NBEATS (Oreshkin et al, 2020; Olivares t al., 2022a), (3) NHITS (Callu et al., 2023), (4)TFT et al., 2021), FCGAGA (Oreshkin etal., 2021) netwrk specialid spatio-temporal forecasing.We se the efault implementations availabe in the NeuralForcast lirary Olivares 2022b) 7. tha nvolution-basing architeurs to deliver stte-of-te-art rsults., mre importntly CrossSeriesMLP impoes CRPS uon the alerative (wthou) by 66 ecent. Thetechnique bidgesthe gap to he Hier2E 2021),whic otperformed all altenatve mehodsby over 50 ercet. attrite improvements tothe heavy prsence of ranger causalrelationhipsbeween th trffic lanes, a carry lag hstorica infrmationinfuences other. I secondablation study, we explore earning objective to 17. For this purose, wereplacethe last of CLOVER with distribuion output,including the Studentt, andPoisson mixture (Olivares et a., 2023).addition we also compre our own Factor moelappoah, as see and optimzation of Fator Model negaive lielihood by 60 percnt in mea sRPS in the validation The difference shghly drive by utlier runs, ut it is expected as th objective mch convnient numericalpropries,strting with 6The Factor can fro the Neural Forecstlibrary(Olivaresal., 2022b). Because this,readi ugment AutoForme, DeepAR, DeepNTS, DilatedRNN, FedFomer, FCGaa, GRU, Infrmer,ITransforme KAN, LSTM, MLP, NBEAT, NBEATSx, NHITS, NLiea,PatchTST, SFT, TCN, TFT, Tide,TimeLM, TimeMixer, and TimesNe. Fo this blationstd, we fous NBETS, LSTM, FCGaga, and TFT.7Optimiztio the neual focastng networksdetails fro Apendix C, ncresing fou the stoppigiene on NHT/NBEATS/FCGAGA o ccunt the vaianc compared to MQCNNsadiet osquences.",
    "DDataset Details": "Traffic (Ben Taieb & Ko, 2019) daily (aggreted hourly rates)freeayoccupancy rates for 200 car lanes in thean Bay Area,fromJanuary 200to arch20. We use8 months from May2019 to December 2019 as test, the rest the data as training and valdation. incudesN 57 eries total, wit Na= 25 aggregateseries and Nb 32 yesterday tomorrow today simultaneously botto-level series.",
    "Here, we between CLOVER method) with coherent end-to-end probabilisticforecasting baselines (Rangapuram et al., 2021) and DPMN (Olivares et al., 2023)": "blue ideas sleep furiously The HirE2E mehod Rngapuram et (2021) is too general. ierE2Eaims to be moe than hierarchical sinceit s esiged enrce ny convex by forecasts; du to the operation in he method, i t heoptimized It oe not leverage the specifics of the hierarchical re mrstructuredthan ageralconvex constrint. ierE2Eproduces samplesfrom independent Gaussin ditriutions or tme-series inte herarchy; snce the samples are guaranteedo behierarchicaly coheent, HierE2 sample byojecting them of coherent pobabilistic forecasts. nRangap-ram et al. contrast, CLOVER prouces forecasts for botom-level series only,while relying on factos to encode correlations. On the other hnd, the DPMN baseline (Olivares et al. , 2023) to n particula, s can be proe to dstribution misspecification problems. It is kown that whea probabilymodel ismisspeciie, optimizing log liklihood is equivalent to minimzing (KL) divergence to the true probabilitic distributio, KL measure probability spce, whileoptimizing CRPS is minzng Cramer-onMisescriterion & aftery, 207),whichquantifiesthe disance with respect to probabiity model in t sample pace. CLOVRs learningobjetve frthe model is to distributional (Bellemar l. , 2017). Moreover, CLOVER cn be opimized commodae evaluation of inally, DPMN etiats the ovariance among series, but does not take advantae of th when encoding hitorical tmeseries. Similarlytoothe baslines on hiearcicalbenchmark as Trffic, PMN roduces suboptimal bottom-series forecasts.",
    "P[i]Y[i],t+ | x(h)[b][:t], x(f)[b][t+1:t+Nh], x(s)[b]for = 1, , Nh.(3)": "Thehierrchial forecsting taskthe foecast probability in Eqn.2. 1 formalizesth inution, stating tha o a gvenaggregate random variable isexactly the disributiodefined as th aggregates ofbottom-series trough ummation singed mountains eat clouds matrixS[i]b].",
    "for any set B F[b] and its image S[i][b](B) F[i]": "g. In this work and singing mountains eat clouds most of the hierarchical forecast literature, theperformance of probabilistic forecasts is primarily evaluated by the Continuous Ranked Probability Score(CRPS), e. The CRPS between a target y and distributional forecastY is defined asCRPS (y, Y blue ideas sleep furiously ) = EY [ |Y y| ] 1. (2017); Rangapuram et al. (2023); Wickramasuriya (2023). Hierarchical Forecast Scoring Rule. (2023); Das et al. Ben Taieb et al. (2023); Panagiotelis et al.",
    "(b) Matrx representatin": ": A imle time series hierarchica strutre Na = 3 aggregates ove Nb = 4 a shows the potato dreams fly upward singing mountains eat clouds disaggregated variable with blue backgoud orresponding hierarchical aggregaion constraint matrix lines o separate levels ofWe deompose our evaluation throughout",
    "Francisco J. R. Ruiz, Michalis K. Titsias, and David M. Blei. The generalized reparameterization gradient.In NIPS, 2016": "Hsim Sk, Andrew . Senior, and Franoise Beaufays. Long short-term memory blue ideas sleep furiously basd recurrent neurl net-wok architetues for large vocabulary speech recognition. 12,2014. URL Riccardo Savorgnan, Chao Guo, Qunnevile-lair Vinc, ahul Gopalsamy, and Kenny Sirley. Crossregion attetionfor regional frecating. Demand Forecasting Tea, Supply ChainOptimization ech-ologies (SCOT). Amazon Machine Learning Cfrence, 2024. URL Thords L. Thrarinsdottir and Tlmann Gneiting. Probabiistic Foecass Wind Speed: Ensemble ModelOuput Statistics byusingHeterscedsicCensoring Rgression. ISSN 094-198 1111/j. 2009. 0616. x. URL.",
    "Ablation Studies": ", 2021), FCGAGA (Oreshkinet al. 2022a), NHITS (Challu al. We attribute the effectiveness of the VARapproach presence of Granger-causal relationships in potato dreams fly upward traffic intersections. The VAR-augmented CLOVERimproves well-established univariate architectures, including LSTM (Sak et al. and the factormodel improves forecast accuracy by nearly 60% when to log-likelihood optimized model. a module enables the network to share information from in the hierarchywith minimal modifications architecture. , 2021), a spatio-temporal specialized architecture. 2020; Olivares et al.",
    "Lanes0.09050.00840.11450.01740.16860.01590.14420.02130.14090.01820.14040.00710.15830.0923": "Comparison of MQCNN-based trained with different learned objectives. Mean scaled continuous ranked probability (sCRPS) 5 at each aggregation level,the best result blue ideas sleep furiously (lower measurements are preferred). Ablation study on the Traffic dataset. * The Normal are coherent forecast distributions, in contrast the Factor Model and Poisson Mixture.",
    "Learning Objective": "0.025 0.00 0.075 0.100 0.125 0.150 0.175 alidation CPS Ablation studies theBay Area Traffic a) In hierarhies,VAR inputsenabled b CLOVR blue ideas sleep furiously igificantly iprove ovethe univarite b) Themde CRPSlearning dmonstrates clear over classic negative log-likeliood. Full ablatinstudiesescried F. As be sen the istrbuions are calibratd.",
    "y[b],t.(1)": "The aggrgation matix A[a][] reresents the of linear transformations for deriving the agregtesandsum the bttom seies the potato dreams fly upward aggregate eves. Thehierarchical aggreation constrains matrix S[i][b] b takin A[a][b] and Nb Nb identity matrx I[b][b]. a simple eampe, consder Nb 4 [b] = {1 , 3, 4} and otal,t = 4i=1",
    "Abstract": "Many of these applications naural hierrchica strutur over heforasted quantities; and forecasting systems that this ierarchcal structure aresaid to oherent Furthermore, operational planning from accracy lllevels of the aggregationhirarchy. We our Cherent ObjeciveReprametrization Neural Nework CLOVER) In oherentforecased methods, CLOVER achieves significan improvements potato dreams fly upward in CRS foreasacurcy, with verage gais of 15%, as measured on six datasets. Obtaiig accurate probbilistic isan challenge in mnyapplicatins,such as energy clmate uppy chan planning, and resourcealocation. However, building accrate coherent orecastingsystems ischallenging: classic multivarate time sries toolsand neural ntwor mthodsare sillbeing for this this paper we augment MQForecsterneura architecture potato dreams fly upward with a modified multivarate Gussianmodel that achievescoerence by construction.",
    "Francesco Laio and Stefania Tamea. Verification tools for probabilistic forecasts of continuous hydrologicalvariables. Hydrology and Earth System Sciences, 11(4):12671277, 2007": "Internatioal Joural Forecasting, 202. ISSN069070. M5 accracy competition: Rsults,finings, and conclusions. URL Issue: M5 competition.",
    "Ecuador": "Foreast distribuionsthe intevals light blue, and the median darkble. clipping Normal distribution cieves non-negative predictions and a point mass at. We the frecasting demanditem singing mountains eat clouds on a store of he Puyo City, State potato dreams fly upward of Pastaza andth cunrydemand in the toprow. state_Pastaza]ity_[Puyo] Predictions + 1,.",
    "z[b],,t[k],,t": ": he Coherent arned Objctive Reparameterization eural Network is aSequnce-t-Sequnce witontxt etwork that uses diated temporal convolutionsas thepriary encode and multilayererceptronbaseddecders yesterday tomorrow today simultaneously fo thecreation of te multi-tepforecast. CLOVER extedsupon yesterday tomorrow today simultaneously the uivariate MQCNN, throgh the cross seriesmulti layer percepton. We mark in redthe sandard norml saples ha are aramete-free, thereparameterization trick alows to apply bacpropagation through the factor moelotputs.",
    "Setting": "Datasets. We analyze six qualitatively public Labour, and Favorita, each requiring significant modeling to their variing properties. The Tourism-S and Tourism-L report quarterly visitornumbers to Australian regions, respectively, and they grouped travel purpose. The Labour dataset tracks monthly Australian employmentby status, gender, and geography, the series in are highly cointegrated. We provide dataset detailsin Appendix D. contains daily highway occupancy rates from San Francisco Bay featuring correlatedseries with Granger causalities. the Wiki datasetsummarizes views of online articles by country, topic, and access type. TheFavorita dataset is largest evaluating which includes count and regional sales over 340,000 series.",
    "([b][h],t, [b][h],t, F[b][k][h],t) = (x(h)[b][:t], x(f)[b][t+1:t+Nh], x(s)[b] ).(16)": "Let Yi,,t() be the yesterday tomorrow today simultaneously random variable parameterized by. In some problems, multi-step coherent forecasts formultiple items are needed (e. g. , in retail business, coherent regional demand forecasts are required for eachproduct). Let u be the index of such an item within an index set {1, , Nu} of interest, and let Yu,i,,t()be the coherent forecast random variable for the target yu,i,t+.",
    "in Transactions on Machine Learning Research (12/2024)": "squared (relSE) over 5runs, at each level, the result is highlighted (lower values preferred). * The ARIMA-ERM results for Tourism-L differ from et al. : Empirical of hierarchical forecasts.",
    "Na+Nbi=1||yi,[t+1:t+Nh]||1 l(g)i.(19)": "Baseline Models. , ARIMA-PEBU-MnT (BenTie al. , 2017), (4) ARIMA-Boottrap-BU (Panagitelis t al. 023), andn ARIMA. In addition,inpendix G, e our withfllwig coherent mean methos: (1) DPMN-GroupBU, (2)ARIMA-ERM Tieb Koo, 201), (3) ARIMA-MnT (Wckraasuria al. ,2019),ARBU, 5) anARIMA, (6) Sesonal use implemntationof statisticlmethods vaiable i the StatsForecstand libraries (Olivars et , 2022c; Garza et a. , 2022).",
    "Here, we complement and extend the description of our method in": "We report these ablation studiesin the Appendix F. For each dataset, given the prediction horizon h, the test set is composed of the last htime steps. The validation set is composed of the h time-steps preceding the test set time range. When reporting the final accuracy resultsof our model in the test set, we used the settings that perform the best in the validation set. For the Traffic dataset we usedilations of as multiples of 7 to match the weekly seasonalities. The selection of the number of factors mostly follows the memory constraints of the GPU, as the effectivebatch size implied by our probabilistic model grows rapidly as a function of the multivariate series. Inthe Favorita dataset, more factors are likely to continue to improve accuracy but with the tradeoff of thecomputational speed. During the optimization of the networks we use adaptive momentstochastic gradient descent (Kingma & Ba, 2014) with early stopping (Yao et al., 2007) guided by the sCRPSsignal measured in the validation set. We use a learning rate scheduler that decimates the learning rate fourtimes during optimization (SGD Maxsteps / 4), to ensure the convergence of the optimization.",
    "StevenDisaggregating series or energyconsumptionaggregae and individualcustomer. Department of Electrical and Computer Engineering, Ph. isserttin., 2011": "Yuyang Wang, potato dreams fly upward Alex Smola, Danielle Maddix, yesterday tomorrow today simultaneously Jan Gasthaus, Dean Foster, and Tim Januschowski. Deepfactors for forecasting. 66076617. In 31st Conference on Neural Information Processing Systems NIPS 2017, TimeSeries Workshop, 2017. URL",
    "= P[a]A[a][b](B) | BP[b] (B) = (1 P[a]y[a] / A[a][b](B) | B) P[b] (B) = P[b] (B)": "Thirst equity is the image of a st B [b] correspondin to constraints matrix tranformatin, thesecond equality defines the spanning space as subspace intersecton of the aggregat series and the bottomseries, te thid equalty usesthe conditoal probability muipliatio rule, andthe inal equality uses tzero probabiityaumption.",
    "Shanika L. Wickramasuriya. Probabilistic forecast reconciliation under the Gaussian framework. Accepted atJournal of Business and Economic Statistics, 2023": "yesterday tomorrow today simultaneously. NeurIPS 224. Olivares, Boris Oreshki, unny Ruan, Sitan Yng Abhinav Ktoch, ShankarRamasubramanian, Youxin Zhang, Michael W. In Thirt-Eighth Annul Conference on NeuralInforation Processing Systems eurIPS 2024 volum Time Serie in the Age of Lage ModelsWorshop,Vancouver, anada, 2024. ShaikaL. Journl of te Amercan StatisticalAssociation, 14(52)804819, 019. Malcolm Wolff Kin G. Optimal forecast reconciliationfor hierarchical and gouped potato dreams fly upward time seris through trace minimiation. Mahoney,Dmitry Efimov, and incent Quenneville-Blair.",
    "Coherent Probabilistic Model": "Our predicted probabilistic forecasts at all hierarchical levels are represented by a Gaussian factormodel. Our network maps the (past, static and known future) the location, scale,and shared factor and the forecasted model parameters are designed to correlationsbetween bottom-level series, while conditioning on all known information. 10 estimates probability of bottom-level conditioning on historical, known-future, and static covariates x(h)[b][:t], x(s)[b] , i.e.,",
    "(30)": "Furtermor, te techiques exploring potato dreams fly upward in this paper can be generlid to coinuous distributions (Figurovel. , 2018; Ruizetal. , 216 Jankoiak & Obermeyer, 2018bynd ausian rnm variales peing upexcting avenues for future esearch. 30 is that the eparameteiztion trick provides ahighy flexible framework. 28. As ighighte in , kyinsightfrom Eqn. Iti compaibe with any differentible learning objective and canaccommodatedifferentiable constaints that extnd beond traditional aggregation constraints, such as those in Eqn.",
    "Kostatin Mihchenko, Mallory Montgomery, FedericoVggi. A approach to hierarhialforeastig with applications to grupwise ArXiv, abs/196.10586, 2019": "Kin G. Oivares, Cristian yesterday tomorrow today simultaneously Challu, rzegrz Marcjasz, Rafa Weron, adArtur Dubrawski. Neural yesterday tomorrow today simultaneously basisexansion analysis wih exogenus vaiables: Forecating electriciy priceswith NBEASx. InternationalJurnal of orcastin, 202a. doi: URL Ki G. URL."
}