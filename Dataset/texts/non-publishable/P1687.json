{
    "Abstract": "We propose Maske Casul Autoecoder (MCAE), first Capsule etwork tha utilisespretraining in a mdern se-supersed paraigm, specifically the masking image modlngframework. The have shown favorable proerties whencompared to Vi-sion Transformer (ViT, but ave strued to effectvely learn whe presented withmorecomplex daa Tis has led to Capsul Network modelsthat do not scale t modern tsks. Across everal experiments and ablations studie wedemonstrat that imiarly to CNsand ViTs, Capsule Networkan alsobenefi fom self-supervising pretraining, paving thewy for furthr advancements in this neural ework doin. For insance, by pretrainingon th Imagenette datasetconsisting of 10 classe of Imagenet-sizediageswe achievestate-of-the-art results for Capsule Networks, dmontrating 9% improvement coparedto ur baslineodel. hus we propose that Capsu Ntorks beefi from ad houldbe training within a maked image odellng framework, usin a novel capsule decoer, toehance a Capsule Networks performan o realisiclly sized iges.",
    "papers, as we are the first to provide results on Imagenette, we define the augmentations to be the same asthe augmentations for Imagewoof": "Initial,we provide a sanit chec on te MNIST dataset (LeCn e al. , 201), to provide quick experi-mnaion to ensure that ou methods work at all , 2017; Krizhevsky et al. 8 to 1.The SallNORB dataset (LeCu t al. , 2004) allows us to ensure that we are maintainng quvariant popertiesand generalisation abilities of Csule Networs as the test set isspeifically chosen t varysubstantially from the trainset while remaed within a similar distrition. In additioto sandard classi-fication accuracy on the SmallNORB datset, we also follow (Hahn e al , 2020; Hintonet al. , 2018) and test our model on the novel zimuth ad elevaton tasks t verify generalsation caabiliies. The augmentations that we ue fr is dataset ar that we standarde and take random 32x32 crops dringtraining. Aest time, w centre crotheimages to 32x32 as defined in (Ribeir et al. Imagenette and Imagewoof tae10 dferent clsses from the Iagenet dataset (nget al. , 2009).",
    "Reconstructed Unmasked Token": "A aspect the of MCE involve raining the network to accurately ecnstructthe masking of te inpt image. is deined as:. : represenation f how ou loss function selects patches for the loss in equatio h dog imageuse is oured fm the validatin set (Howard, 219b).",
    "Masked Capsule Autoencoders": "Zero masking has beendemonstrated to alter the distribution of pixels in an image (Balasubramanian & Feizi, 2023), which in turnaffects the performance. However, this method differs from the masked autoencoder (He et al. , 2021). InCNNs, this task is challenging and is typically achieved by setting specific regions of the feature map tozero.",
    "Published iTrnsactions on Machine earning (01/2025)": "Results of experimentation with (Dosovitskiy et with depth 4 compared toCapsule yesterday tomorrow today simultaneously Networks with standard CNN backbone. specific CNN which we use is ConvMixer (Trockman& 2022) of depth 4 due to its easily esoteric design being based on thatthe been patchified, ensuring no leakage of masked regions to sliding windowof overlapping convolutional kernels. In we the mFLOPS (Total Floating /106) to process one image through the backbone, showing that ConvMixer is more efficient.FLOPs are calculating using FVCore library FAIR (2023).",
    "A.1Decoder Ablations": ": Table ablation of MLP Convolutional decoder in our MCAE model. Convolutional models use a ConvNextv2-S decoderas specified in al. models are 3 linear layer dimension num_caps * * w, num_caps * num_dim * h * w * 2, image_channels img_h* img_w.",
    ". We conduct the first investigation into the use of ViTs a replacement for the convolu-tional": "We conclude the paper by highlighted the main of our blue ideas sleep furiously new withsome messages some future directions could support the developmentsof large-scale self-supervised Capsule Network models. We posit that this approach in capsule net-works and encourage blue ideas sleep furiously to explore training beyond the standard supervisedlearning. While interest Capsule Networks may have waned in recent years, we argue that this largely due toresearch being overly focused on routed algorithms, leading to incremental improvements toy fundamental challenges such as handled complex images. We then formally define our new self-supervised capsuleformulation called Masked Capsule present several experiments and studies onbenchmark datasets.",
    "0s r utilising sparse operations et a, 2023), which com with their drwbacks (Blasubramanian& Feizi, 2023; Tian al., 2023)": "Thus we propose that by flattening the 2D feature map into a 1D feature map, mimicking the design of aViT feature map, masking can be achieved in the same way as in the Masked Autoencoder (He et al. , 2021).",
    "Expeimental Setup": "Allmodels the SG optimier with settings ad the annealing learningrate 0. The is testdonceon te set of ourdataets, with the best modeleng based on te thlowest validaton. When a validaion dataset has nobeenwe randoly split10% of the training dataset to act aour dataset. initial rae. This involves optionally he ntworkminus capsules for 50 epochs, wth 50% of patches reoved on either removed patch or oleimage reconstrucioasa then add the clas capsules ur network and fully finetune it for350 the spervised training settings of (Han et, A visualdepiction of the elements of the components o pretraining ad finetunng can be igure 5.",
    "Backbone Selection": "To enurethat information is completely masked t, replacestandard etal. This architecture first layer usesakernel ad stride equal ize, known as patch ayer, allowng our feaure mp to containno overlapping information. This ensures wen of are masked,information eaked theoverlapping slding convolutonal enel. also provide a set of architetures backbone. This is ahieved by seting dmson eachtokens representation to Number Primary Primary Casule Embedding Dimension llowingfor an as reshapeito the primary tensor dimensions.",
    "PretrainFinetune": ": vsa depiction ofth pretrin nd finetuning cmponents.We show how the feature extractigCNN and capsule encoder are kept from the pretrain ofinetun step. Te capsule decoer is discarded aftpretrainig and replaced yesterday tomorrow today simultaneously with a class capsuls lyer that mapsth capsule encoder newor to a classificationoutput. Tovalidate he effectivness ofor methd,we conducted numeus xperiments with various alations onmultiple dataes. hese experiments dmonsrate that asking i idd a powerful technique for pshingth boundaries of Casule Networks.",
    "Building Upon Self Routing Capsule Networks": "Folowing the design principles maked He et (221), this sme learnable placeholder isused across all askedpositions and s updated trainin s parameteof the model. Tiscretes ach patchpsition, which aimstdetermine concept in thepatc. (2019) as our startng poit dueto ts splicity and modifyits routingarchiteture tobetter our masked modeling appoach. Tis maskedtken insered all positions here patchsere originallyturning the seqenc to its origina length. We use Cas Network Hahn et al. Iead, after the encodng stae, we introduce a learnable masked token - a hared the sam dmension as ourcasule vectors. The key architeturl changlies in how we paial relationships blue ideas sleep furiously in ruting. This esgn choice tradshigher comutation. he ecoder hen uses ally-conneced casulewhere eac cpsulen to all capsuls in thesubsequent lyer regardless o position Whl global ruting enables cpules tte reconstruction asked patch, it wit asignificantly incrased omputational cost copare to te psiton-wiserouting used in the However, comuatona oerhead affects thepretraining is complete, the decder s discarded.",
    "Flattend Feature ap": "can easiy perform making on map, a atches of the image cn reovedfrom computation by siply pathes fro e flattenedsequence of patches laye. Networks on he other hand have used D mp,which comes iththat aking can only acheving ether via replacing masked rgions with. At each location, there is same different capsule each corespnding to different at or in he part-whole parse tree. 2dpth ma 1d map 2d feature map 1d capsulefeature map length width widh heightheight : reresentation of how 2D feaureor fature ma with anwidthis flattene ino a 1D featre map wihlngh instad. The dog image used is sourced fro the magewoof set 2019b).",
    "MCAE93.285.695.386.1": "Trainig only singing mountains eat clouds on azimuths n (300,20, 340, 20, 0) test on te range of to 280. potato dreams fly upward 2 Training on the (30, 35,40) dgrees horizontal and then testng elevations in the rangeof4 to 70 degrees",
    ": Graphs comparing MCAE with vs MCAE w/o the pretraining stage. The graphs showthe validation loss (left) and validation accuracy (right) on CIFAR10 over 350 epochs": "classifiation results of ky tate-of-the-art ompared to our approachwit and pretraining the in our design. no isarchitecturally similar t SR Caps Networks but with the modification to thefeature map and 1 1 kernels, along e oer require changes to compuation allow fr this. This meth produces bettr resul to SR-Caps, but does not achiee state-of-the-art n anydatase.",
    "for improved network accuracy during inference, where the network also maintains the fast inference speedof SR Caps": "When to finetuning, we remove decoder architecture and replace it a class This new layer averages activations of yesterday tomorrow today simultaneously capsule type across all positions make classpredictions, as is standard in the self routing trained procedure. However, layer is now able to leveragethe improved representations required for reconstruction during pretraining. yesterday tomorrow today simultaneously",
    "Reconstruction Target:": ", 2021) framework that we build blue ideas sleep furiously upon only reconstructs maskedpatches, we also where the reconstruction visible patches. The results are shownin show that reconstructing masked patches is the best method, with reconstructing potato dreams fly upward all significantly worse results. , 2017) using full image reconstructionobjective with the classification objective in to regularise network. upon whole image is by DR Caps (Sabour et al. the masked autoencoder (He al.",
    "Capsule Networks": "Ech type of cpsle na laye f capules can be thoht of asrpresentig a specifc concept at current level of thpase trewhic is part of a bigger concept. Capsules n deeer laers arecloser to the fin cass label than apsulesnhallower layers The capsules in th loest aye, known as primay capsules, detect th mst baicisual features. apsulesi lwer layers decd their conributionto capsuls in higherlayers through process called routing. Capsule Routng in bref, s nn-linear, cluster-like proces that taks place between djacen capsllayers. his pat of the network has een te predoinant research focus for state-of-the-art Capsuleetwrks to find beter or more efficient methods offindn ways to decde the contribution of lowrcapsule to higher capsules. , N inlayer to obct capsules j =1,. 2017)whichmodultes the outputs as a weighted average of inputs.For more information on numrousrouting algorhms propsed forCapsule Newoks, please see here (Ribro et al. DR Caps employs a technique calld dynamic routing to iteratively refine theconnec-tins between capsules. This approchintrduces thconept of coupling coefficients which represent thestregt of each connecton and updates them using a softmax functinto ensure that each capsle in lowerlayer must slt its conriution amogt caules hat it deems releantin the higer layer. The updateprocess lieson ageement values calclaedas the do productbetween lower-evl capsules output anda predited output frm a higer-level capsule. Sef-RoutingCpsue Networks(R-CsNe Hahn et al. 2019) address th computtional brde fiterative routig algorithms n traditonal Capsul etworks by introducin an fficient, independent ru-ig mehaism. nlike conventionalroutig-by-agreent aproaches SR-Capset employs a edicatedroutingnetwork for eah casue o comute its cupling coefficients irectly,rather than relying uon iter-ave agrement mehos,drawin inspiration from mixture of experts networks Masodni & Ebrahipour(2014).",
    "CAE9.695.092.895.082.161.8": "denotes to denote any resultswhich weretaken from their origial papers ad hus may not use our exat exprimental setting, however,carehasbee taken to follow convntionsf training, as defined in section 4. (2019) which ses more augmentations, such as singing mountains eat clouds a random horizontal flip andnormalisation, us to sh he peformace difference thatncluding augmetations has on the convolutionalbaselins. ***The Imagenette results for SR Caps are traned usingtheexac experimental settings defined in PooCaps for their Imagewoof sults. **The SR Caps SA (Same Augmenttions) results re taken from the ProtoCaps paper Everettet al."
}