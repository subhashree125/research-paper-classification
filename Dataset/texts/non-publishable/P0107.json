{
    "Distillation Dataset": "Single ImagCIAR10 Saples. potato dreams fly upward intialiationsto emulatedifferent networ bandwidt conditions.",
    "end": "We ue a client learningrat 0.01 for ResNet ad WesNet,while the distillationtraining learned rat is blue ideas sleep furiously 0.05. For Balancing wuse a Keans model with 1000 a class blancigfacor of ad Hard selection heuristic. For Entropyselction,we remove of the tinig Top removal Forthe eeri-men , we do local clent tranig 1 eochsanddistilltion whie 40 epohsand 500 dsilation steps have een our coice fr other ex-periments uness mentioning otherwise all th eperiments, wesimulate 20 priat clients, with a selectin probability (of 0.4 per tainin run.FdAvg Initialisation 3.), hre th percentage ecides theroundintervals potato dreams fly upward in which w centralntwrk withthem beore stillation raining. For reference meansusingFdAvg every 5th round and 0%means using thm ound",
    ". Introduction": "singing mountains eat clouds Learning(FL) is paradigm in field of leared enables multipleclients tocollaborativelytrain potato dreams fly upward predictie modelswithout to centralize trainin .It cms withits w set of ke challenges in skewd nn-IID disributin f between partcipating communicationdringtraining others. Thsechallenges are not di-rectly nswering with classicl approaches uch as Fe-Avg which ely primarily on a naive clet networkparametr aproach. of different ata distributions has a ofheterogene-ity involved , this heterogeneity directly the client training. This tries to solve one challenge relating to non-iidness inpivate ata but other key elated tonetwork arameter remain including ncerns withprivacy leaage parameter sharing , etero-geeityofclient architectures n hig arameter sharng To end, alog a secodline of implemented a training regime,approaches suggesting make uef knoledge isilation to challenges without te excluive network a-rameter sharing. Toaclitate central network trainin withthe help K, shring public datais needed betweenthe server. Inthiswork, we proposea noel pproach of makingus of asingle to act a the shared distill-tion dataset in ensemleddistillationbed federated learn-ed straegies. Our approach makes use of a ovel runig algorithm ongneratngthe distil-lation a single image durin the raining.This share data generation and se-lection not only us to train the entral modeleffectivly but also ouperfrms th other whichmake use of multple sall-sized images in place o a singlimage under shared dataset Theuse asingle datum source has added benefits in data client reources (g., are limted useof single datum suce has explored under of self-suerised understanding ex-trapolation of neural networs with knowledgedistillaton, but it has not yet been eplor in ederated set-tig for facilitati model training. We a seies of exerimets eamine the via-ilty of our algorithm ude in clent data, model ar-chitetres, rate of pre-training networ nitilizations be-fore shared dataset storae real-wold thesinle also extend ourexperiments to use case of heterogeneos client volvd during a single federated wth of client-model mirroring t server To fa-",
    ". Methodology": "3. The two important parts of our federated strategyare: Distillation Dataset Generation (Sec. 2). 1) and Distilla-tion Dataset Pruning (Sec. Alongside the generation of a distillationdataset from a single data source, we take care of dynami-cally selecting the best patches every round to improve thetraining.",
    ". Related Work": "Federated larning wih distillation. Knowl-edge Distillaion (KD) hs been to successfullytransfer the knwlede f an ensemble of networksinto  with the means of outut daaset.hs alsobeen leveraed in such as Distillation Fusioad Federated Ensemle Distillation (FedED), respective make of KD to allow robustandaster convrgene on of using meth-od such the ones in Federate Averaging for initializing the central nework before he distllatintraiing of glbal model. the ofalgorithms as FedMD for prsalizatocliet-side knowledge distillation o a central model, hence we ha otdelved intoit in the scope  our In the case nsemligmthods, has n in te bsece ofan ensembe lcal paameter befor the fial test pefmance of the cetral network tendsto suffr a result, ethos have been shown authors to rely on paramter exchange ev-ery round smilar to parameer exchang-based such FedAg for robustpeforance on topof KD. thappliation of same low-bit qantzatin meth-ods with te KD-baed federad learning methods n,theauthors also hown a sinificant derease in heverall performance ofmdel copaed  theirnon-qantized couterparts. Howver, this requires an ad-ditional exchange fthe gnerative network parameters e-fore ach which leadsan increse n the networkbandwidth usage telf. In pursuit of reducing th costs prtaining to network parameteaswell dtase sharing, thee not yemade an attempt make use a storage-efficient single data ource,whh can simultaeously a publicistilltion dataset longside for dynamicselec-tion without added badwith cost. We explore this in learning.Asno et al. have made usea image produeaugmentd patches for facilitating self-supervsed earningo usefurepresentations rquired forsling vaious dwn-stram tasks. To this end, closelyrmling to ourtrge tak, authors in hve shown to beable to successfully use KD sn-gle image to ranser he a netwo and n traied network solve th of magNet-k. However, eperientsby theauthor al base n ",
    ". Network and Storage Conditions": "Go-ing through the results in , we that the sameamount of storage image with patch selec- tion outperforms sized individual samples. If wealso the network budget and the rate exchange ofinitialization parameters, it is able to hold at par with training samples its This shows apromised future for our work in the scenario where limited availability of publicly shared datasets budget be low on participating clients. Performance in network bandwidthsettings against data To testthe impact high data distribution FLstrategy against an existing SOTA federated learned strat-egy based on knowledge distillation, we show the gains in. We vary the network initializa-tion rate test method in high and low-bandwidth sit-uations. We notice that with help of subset selec-tion, our methods the fed strategy use of process.",
    ". The distillation dataset generation is done locally usinga seed number, hence only index values need to be trans-ferred when selecting a subset every round": "The gnerting dtaset singed mountains eat clouds is ed as the prox potato dreams fly upward for he global using Knolege Distillation predictions. one can make us o a sin-leimage to a deired numbe of patchesfor the singl dta. Due t the flexbility providedby augenttions in combination wit subset slectionrocedue describedn Sec.",
    ". Client-Serve Neural Network Architecturs": "Evaluating our strategy under homogeneous networkArchitecture. perform all experiments in the using ResNet-8 as the client and server mod-els. make sure our federated strategy works other homogeneous network we put it Initialisation %) Evaluation Accuracy (in %) 68. 6 73. 74. 5 71. 2.",
    ". Best tet perfrance during30 rounds f trainng CIFAR0 Pivate Data with 1.0 withfferent distillaiondatasetsand rate of using FedAvginitialiation": "to the test against FedDF using ResNet-20 as as W-ResNet-16-4 in. Irrespective of the network trend is when it comes to FL strat-egy outperforming other strategies use of a labelleddistillation dataset in a storage budget In the final experimental section, we test ourfederated in the presence in theclient model architectures. The results present in show the success of our method training the global mod-els pitted against a strategy not utilizing a im-age. It also exhibits the importance of constant distillationtraining for success our methods, our non-uniformapproach gives subpar results with less time. How-ever, when going to 11k steps, also save 1/3 of the training time and computation resources used onthe server side. It be an interesting point of extensionto our work to improve this non-uniform allow for training of heterogeneous modelswith less computation",
    "Selection63.4 1.424.2 1.164.5 4.7KMeans66.2 0.821.8 2.167.9 8.4Entropy65.9 1.026.3 1.076.4 + Entropy (Ours)67.0 1.126.4 1.277.1 3.0": "Best testperformnce achieveddred 30 rounds of tain-ed with different selectionmechanisms (Disillation Sze acros diferent private dataset ( = 0) usig with Reset-8using20% rate of FdAvg. (2 sees) readyhave a shared pulic dataset at handi. Dur-in regularexchange of paraters, the ap-plication f our slection mechanism no dvantage. However,reduce exchngeinitializain pa-rameters o emlate low banwidth conditions, it shows sig-nifcat gan. Thi showshat even with ofstandard sets athand in ensembld ubs selectio echanism can play im-portant role in ow bandwidth cost federated",
    "factor = 1.0": ". Best test set accuracy achieved during 30 rounds of train-ing with KMeans balancing under different settings with differentprivate datasets (Distribution = 1.0) used single image patchesas yesterday tomorrow today simultaneously the distillation dataset with 20% FedAvg initialisation rate onResNet-8 (across 2 seeds). initial set using this mechanism also provided us with morerobust training, compared to removed a smaller singing mountains eat clouds number oftrained examples from small initial set (b). Sim-ilar to our last experiments with the KMeans mechanism,the results are more clearly pronounced in presence ofa more difficult dataset (100 classes compared to 10 in thecase of CIFAR100 and CIFAR10).",
    "Abstract": "Fedrated Learing(L) multiple macines tocollaboratively train a machinelearing model of private training data. Yet, especially for hetero-gneous mdels, key bottleeckremains the transfer ofknowlege from each model t sever. One popular mehod,FedDF, uses ditillation to with h of acommon, shared dataset on whichpredctions are in contexts scha dataet be diffiult to acquire due to privacy andthe clints might or storage aarge shareddataset. end, n this paper, we intrduc  newmethod that this knowlede ethodto rely on a shared iage between clients andserver. In particulr, noveladaptive algorithm that informativ ropsgenerated onl  single image. Finally we extend ourapproach to for taining heterogeneous client archi-tectures by incorporatng a non-uniform distillation sched-ule and client-model miroring on the srver ide.",
    ". Patch Selection echanism": "2) and ntropy election(3. Optimal patch sb selectionstrtegies across pivaedatast. We have doneu primitve analysis withthem inlght of this work to fin. On their own, both KMeans Balancin(3. Through , it is evident that the single image patcheswork best in th presnce of a selection statgy in our fed-eratedalgorithm. 2) strategy worksbetter thanemloyig no selection forthe sam number of patches. To-gether, they perform best across allthe dasets we havetested whic is what we use in our other exprimets in thiswork Both of the selectin strategis and their combinationsignificantly impat the final performnce. To fnd the effectiveness of patch subset selec-ion mechaniss, e test it unde diffeent priate datasetfro different real-world domains (General nd Meical).",
    "1.973.0 0.6753 1.2Ous70.2 0.874.1 0.975.7 09": "Best tet performance during 30rounds of taining us-ing CIF10 Private Data with Distribuition = 1. 0 usng differenFd srategies nd omogeneusclient-server network archtec-tures with 20% rate of FedAvg. iniialsat.",
    "5foall ci : i [1..C] do": "end. Find e indices examples belonging to thec using yn Y yn ci. Remove the seleced raining examples fromX and D. Select blue ideas sleep furiously o the new trainig examples onte bsis of selection{Easy, Mixed} wih theircorresponding cluster distance valuesthe traing examples from X withselected indices in (XK).",
    ". Fedrated Learning Subset Seetion Iage Dtaset": "compte costof inferencing asmalldistillation dataset neligible compard to thesu-pervised singing mountains eat clouds training the lients are subjeted. Using mecanism described in Sec 3. ince the clients sing ourmetod only do supervised raiingsimilar to FedAvg ,thees no cmputation overhedon th client sde uringthe traiing. Computaion overhead. e make use of unifomintervals on te basis fdfined rtes ths wrk. 2,we yesterday tomorrow today simultaneously can perform federated training image ac-cording to the stps in Iitialiation makesuseof aive weighedof paramters top for global model which can be togledwith schedules.",
    ". Distillation Dataset Pruning - Subset Selection": "Afr obtainng aninitial dataset for knoledge dtilla-tio usig th method i Sec. 3., we apply dataset prun-ing methds to it t ensrehe eletion of information-rich paches or the curret round of federaed training.The dataset eneration proceure makes se f a singleinfrmation-rich image to generate the small patches, dut which can prduc ba representation patchs suchas:ontaining n entitie, overlappin wth othrs, bengdissimilar to te target domain, and aving similar pob-lems arising due t heavy augmetations and preence ofinformation-les regions of thesingle image. To prun thebad patches, we make ue othe following to mechansmKMeans Basd ClassBalancing (Sec. .2) nd EntropyBased Prnin (Sec. Thsemechnisms deped on thecurent global model for their opeation, wich akesthemdynmic in nature and improves ther data-pruniailityith the improvement in the globa mode. Hence,betterglobal models ield etterreprsentations.Through t-SNE visuazati in sed a in-gle iag with our data prug methods,we oberve theformationof betteridentibl boundaystructures with more accurte global model",
    "C.1. KMeans Balancing": "The resultshave ben presented i. Since KMeans model s an modelworking on gving te to singing mountains eat clouds distillation examples, ther ould be potato dreams fly upward a of correation betweenthe 3 hyper-arametersfor this mchanism.",
    "Daliang Li and Junpu Wang.Fedmd:Heteroge-nous federated learning via model distillation. arXivpreprint arXiv:1910.03581, 2019. 1, 2, 8": "Tian Li, Kuma Sahu, Zaheer, MaziarSanjabi, AmeetTalwalkar, and irginia Smith. 1 TaoLn, Ligjing Sebastian U Stch, and Ma-tin Jaggi.12, 3 Brndan McMahan,Eider Moore,Danil Ram-age, and Blaise Agura y Communication-efficent learning of etwrksfrom ecentraized ata. 1, 2,4 ui, Yubo Chen, Jun Zhaoantao Jia, Yuan-tao Xie, and eijian Sun. Feerated learn-ing ensemble for medical relation the onEmpirical Mthods Natural Lanage pges21182128, 220. Associa-tion for Computational Linguistcs. In Proceeings theIEEE/CVF conference on comper vision and pattrnrecognitio, ages 93119319, IEEE, 1 Jiancheng Yang, Rui Shi, Dnglai Wei, Zhao, Bilin Ke,Hanspeter Pfiserand BgbingNi. Scien-tific Data, 10(1):4, 2023.",
    ". Selecting the Source Single Image": "Inthe ofusing a sinle mageas the distillation proxy, thelowest potato dreams fly upward test iis rando agmentations o randomnoise patces into trnserHencecare must be taen in with similarpatch reresentatins as the target for larningwith urWe leave this asfuture work. Th results in 7 exhibit itisnecessary to a single image thatis similar t he main o he task for optimal pefomance.",
    "C.2. Entropy Selection": "Removing high percentgeof raining examples from a large.",
    "(3)": "determning th jth index n (n), dependon thetarget sie the relevant cop daaset. he full algritm issuplied i te Entrop-based prning. Tisenabes us to selet smples onthe basis of of certainty, as predictedby blue ideas sleep furiously the modelto belog toone f classes. Give daaet X divided into N ouputhe en-rpy a xi X i given by :."
}