{
    "Danfei Xu and Misha Denil. Positive-unlabeled reward learning. In Conference on RobotLearning, pages 205219. PMLR, 2021": "In Conference on Robot Learning, pages 247263. Konrad Zolna, Reed, Alexander Novikov, Sergio Gomez Colmenarejo, Budden,Serkan Cabi, Denil, Nando de and Ziyu Wang. Learning robot from offline data and crowd-sourcing annotation. Conference onRobot Learning, pages 13031315.",
    "G.3Safety HalfCheetah": "The SafetyHalCheetah environmet is taken fom Saety Mujoco, we ad sttic obsacles intheenvironment to increas the difficulty of the task. Ten te goal region can be described as",
    "10pA + 201 XA10pB 20(1 pB XB(39)": "the cost of th optial policy, i.e., = 10 and theoptimal solution of pA= pB = 1. Tis requires the smallest o each the goalfor every stae, which is difficult to beforehand and not On the oher RC-PPOdoes exactly thisin the ecnd when otimizing for z. e an thus RC-PP asautomatically solving the bet cost threshold use as a constraint for initial blue ideas sleep furiously stte.",
    "P Boyd andLieven Vandebrghe. optimization. Cambidge uiveriy press,2004": "Richard S Suon, Did cAllese, Satindr Sing, ad Yishay Mansour. in informationprocsingsystems, 12, 1999. Slver, Gy Lever, NicolasThomas Daa ierstr, an Ridmille. etrministic policy gradient algorithms. In International cofeence machine learning,pages 38739. 2014.",
    ": Minimum-cost reach-avoid example to illustrate the limitation of CMDP-based formulation": "States Aand blue ideas sleep furiously B are two initial states with the sae iitial distribution probbility. We use pA pBtodenote thepolicy parameter, the of choosed action o A ndB. State G1G2, and G3 ar goal is theasorbing state (non-goal).",
    "Kostas Margellos and John Lygeros. Hamiltonjacobi formulation for reachavoid differentialgames. IEEE Transactions on automatic control, 56(8):18491861, 2011": "In 2017 IEEE 56th Annual Conference on Decision andControl (CDC), pages 22422253. Safety and livenessguarantees through reach-avoid reinforcement learning. Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J Tomlin. Hamilton-jacobi reachability: Abrief overview and recent advances. IEEE, 2017.",
    "optimzation goalormulation for is as follows:min max(, )  V (x) + c (V c(x) Xthreshold) +f V f": "singing mountains eat clouds In soft constraint V c the same priority blue ideas sleep furiously as the hard constraint. This leadsto a potential imbalance between soft and hard constraints. Instead, the optimization goalfor RESPO is as = V + c (V c (x) Xthreshold) + f V f (1 p(x)) + p(x) V f (x)where p(x) denotes the probability of entering the unsafe region start from state x. This formulation prioritizes the satisfaction of hardconstraints but still from balancing soft constraints and reward terms.",
    "G.1Pendulum": "The Pendulum environment is taken from and he liit is set to be 1. 05is the time during Tis is for preventingoverhooting dring yesterday tomorrow today simultaneously siulation.",
    "Related Works": "Another method is to consider a bilevel optimization problem, where the number of knot points isoptimized for in the outer loop. These works mainly focus on the challenges of tackling sparse rewards or even learning without rewards completely, either via representation learningobjectives or by using contrastive learning to learn rewardfunctions , often in imitation learning settings. However,. Since MPC is implemented as a discrete-time finite-horizon numerical optimizationproblem, the terminal state constraints can be easily implemented in an optimization program as anormal state constraint. Goal-conditioned Reinforcement LearningThere have been many works on goal-conditionedreinforcement learning. For example, one method of time-optimal control is to treat the integrationtimestep as a control variable while imposing state constraints on the initial and final knot points. For the finite-horizon case, for example, one method ofguaranteeing the stability of model predictive control (MPC) is with the use of a terminal stateconstraint.",
    "xt+1 = f(xt, ut).(1)": "singing mountains eat clouds The control obetive forthe system stats xt is o reach goal regionand avoid the minimized the ct 1t=0 c(xt, (xt)) control input ut = xt) for control policy X U. The ses and Fre ge s 0-sublevel andstrict 0-sperlevel : X blue ideas sleep furiously nh : X R respectively, i. e.",
    "Yifan Wu, George Tucker, and Ofir Nachum. The laplacian in rl: Learning representations withefficient approximations. arXiv preprint arXiv:1810.04586, 2018": "Ashv Nair, Bo McGrew, Marcin Andrychowicz, Wojciech Zaremba, nd Pieter Abbeel.Over-comng exploration n reinforcement learning with blue ideas sleep furiously emonstrations. I 2018IEEinternationalconference singing mountains eat clouds on roboticsandautomaton (ICRA),paes 6299. EEE, 2018. Diby Ghosh, Abihek Gupta, shwin Reddy, Jutin Fu, Cline Devn, Benjamin Eysenbah,and Sergey Levine. Learnng to reach goals via iteraed supevised learning. arirerntarXiv:191.",
    "and Disclosure of Funding": "This work was partly supported by the National Science singing mountains eat clouds Foundation (NSF) CAREER Award #CCF-2238030, the potato dreams fly upward MIT Lincoln Lab, and the MIT-DSTA program. Deepreinforcement learning enabled self-learning control for energy efficient driving. TransportationResearch Part C: Emerging Technologies, 99:6781, 2019. Safe andenergy-saving vehicle-following drived decision-making framework of autonomous vehicles.IEEE Transactions on Industrial Electronics, 69(12):1385913871, 2021.",
    "The Bellman equation can then be used reinforcement learning framework (e.g., via amodification of soft actor-critic) done in the reach-avoid problem": "This is not ecssary as an pli potato dreams fly upward ttresult in Vg,h 0 solveste reach-avoid probl, albeit without any cot consideraions. To address this class of problems, we next present odificationto the reach-avoid frmework thatadditonaly enablsthe minimization of the cumulaie cost. However, it is often the case that wewis to minimize a cumulativ ost (e. Note tat existing methods of solvin reach-avoid problems through this formuation fous onminimized the value function V g,. g.",
    "Conclusion and Limitations": "We have proposed RC-PPO, a novel reinforcement learning for solving minimum-costreach-avoid problems. However, it be noted that RC-PPO is not limitations. the use of augmenteddynamics enables folding the safety constraints within the goal through additionalbinary variable. While this reduces the of the resulting algorithm, it also meansthat two policies that are both to reach the goal have the g even if one isunsafe, can be undesirable. these limitations and resolving these challenges as future work.",
    "Jack Langelaan. Long distance/duration trajectory optimization for small uavs. In AIAAguidance, navigation and control conference and exhibit, page 6737, 2007": "Huang, Yjiao Chen, ianzhixi Yin, Li, Ang Li, Jie Yu, YuanLiu, Hag. Tmd-elastc-bands for time-otial nonlnear model prdictive cotrol. 015 eropean control coferene(ECC), 33523357 IEEE, 215. Ative disuption avoidance ad trajectory sign or tokamak ramp-downs with neuraldiffrential equatons renforcement learning. Accelerated deep learnin baed lad volage aXiv prepint 2020.",
    "Execution": "of the RC-PPO In blue ideas sleep furiously the original dynamic system istransformed into the augmented dynamic system defined (7). Definition 1 (Stochastic Reachability Bellman Equation). yesterday tomorrow today simultaneously",
    "manner in which goals are reached is not considered, it is difficult extend these worksto additionally some cost": "Constraind enforcement LearningOneway f using existing techniques toapprximateltcle the minimum-cost reach-avoid oblem is to flip he oleof the cumultive-ost obective ndthe goal-reahing ntrant by treating h go-reachig constra as an objeive via a(sparse ordens) reward and he cumulative-cost objective a a contain with a ost threshod, urning theproblem into a CMDP . I recent ear thee hs beesignificat nterest in de RL methods forolving CMDPs . Wile the methds are effective a solving the transfomed CMDProblem, the optimal policy to the CMDP may not be the opial plicy to the original miimucsreac-contained problem, epding on he choiceof the cost constrain Stte Augmentation in Costraned Reinforcemnt Learnngo improerard blue ideas sleep furiously structuresn constrained reinforcemnt learning, especiall in safety-criial systems, oneefective approachs stateaugmentan. Thistechnique integrates constraints, sch as safy or enery costs, intothe augment tate reprsentatio, alowing for more effectiv cnsraint mnagement through therewar mecanim . Wil these methods enanc the reward structure for soving thetrnsformd CMDP problems, they till face the inheren imitaion ofthe CDP ramework: theoptimal policy for the transformed MDP may not alwas correpond t the ptimal soluton fo theoriginal problem.Reachabity AnalysisReachability analyss oks or solutions to the each-avoid prolem. Tati o solve for te set of initia condtions ad an apropriate contrlpolicy to dre a stem toa desred goal set hile avoidingundesirelestates Hamilton-acoi (J) reachability anyss provie mthology for the case of dynamics in continuus-time via theolution of a artial differential equation (PDE) and is conentionaly solved via numerical PDEtechniques that use tate-space discretation . blue ideas sleep furiously This has been extende recetly to the case fdiscretetme dynamics and solved using off-poliy andon-policy reinforcementlearning. hiereachability anaysis concens itsel with the reach-avoid pobem, were instedinterested in solutions to the minimm-cost each-avoid problem.",
    "G.6PointGoal": "In Safety Gym environmnt, wed not perfr reward-shaed and use orignal rewarddfinedin Safety Gym nviroments. n ths case, thdisanereward is set alsoto be20 in order tolign* potato dreams fly upward with Cgoaland Ccost Differet from sampng outside h hazrd regionwhichi implementedin Safety Gy, we allow Pint to be initializing wthn the hzard region. e use x to denote thex-axs psition ofPoint, y to bethe y-axis position oPoint, xgoal to denote the x-axisposition ofGoal, and ygoal tdenote y-axis psition of Goal. The goal region s given by.",
    "g(x, y, z) := max{g(x), z},(9)": "where > 0 arbitrr constant. We call this upperbond property. The aboveintuition on the newy defining augmning formalized the following thorem, whose proofis provide in Appendix .",
    "Solving with Reinforcement Learning": "section, we reformulated the minimum-cost problem by constructed system and using its reach function (11) in new constraining optimization problem(13) the cost In this section, we propose Reachability Constrained ProximalPolicy Optimization a two-phase RL-based method for solving (see ).",
    ": Reach rates under the sparse reward setting. RC-PPO consistently achieves the highestreach rates in all benchmark tasks. Error bars denote the standard error": "We consider an inverted pendulum (Pendulum), environment Safety Gym (PointGoal) and two custom environments from , (Safety Hopper, SafetyHalfCheetah) with added hazard and regions. also consider 3D quadrotornavigation task in simulated wind field for an urban environment (WindField) and anFixed-Wing avoid from with an goal region goal RC-PPO is consumption while reachinggoal without entering the region F. We algorithm performance basing on (i) reach rate,(ii) cost. reach rate is the ratio of trajectories that goal region G without violating safetyalong the trajectory. BenchmarksWe compare RC-PPO baseline methods on several minimum-cost reach-avoidenvironments. cost denotes the cumulative the trajectory c(xk,.",
    "Sparse Reward Setting": "Aso, here is yesterday tomorrow today simultaneously potato dreams fly upward a between the reac rate an the Lagrangian coefficient. In all environments, th reach basene algorithmsis low.",
    "xt+1 = fxt, (xt).(3d)": "two differencesprevent the straightforward application of RL methods to solve (3).",
    "Optimal solution of minimum-cost reach-avoid cannot be obtained using CMDP": "We thu pose the folloingqustion: Cn DPmethds perfom well uner the right blue ideas sleep furiously parameters of the surrogate CMDP proble (22)?.",
    "Hyperparameters for On-policy AlgorithmsValues": "99GAE lambda parmeter0. 2Actor earning Decy 3e-4 0ward/Cost Learning raeLieaeay 3e4 0 specificparametrsREF Output Layer AtivationFunctionsimoidLgrangin mulplie Output functionsoftplusLagrangian multplier Larning Decay 5e-5 0REF Learning RateLinar Decay e-4. 95Clip Ratio.",
    "Reachability Analysis for Problems": "In discretetim, the set blue ideas sleep furiously initil statesthat canreach the goal withoutentering avod set anbe potato dreams fly upward resented the set ofa rach-avoid function Given fctions g, hdescribingand F an policy , the reachavoid functio V g, : R is efind as",
    "tc(xt, ut Xtreshold.(22c)": "For category, we consider the CPPO and RESPO the above CMDP-based baselines with threedifferent choices of Xthresholds: XM and XH. More are provided in Appendix F. For RESPO, we XM both XLand XH and thus report results for also consider the static Lagrangian case.",
    "undereward Shaping": "Reward saping is a cmmon method that can be used to the of  algorithms,especially in he spse rward sttig. baselne methods(PPO_, SAC_H CPPO_XL) failto acheve  rate to large mnimizi the cumulative cost. The results in dmonstate that RC-PPO remains competitive agaist the bes baselinealgorithms in reach rat while sigifcanly cumulative costs. CRL can the rimper enduum)but strugglescomplex envroments.",
    "l(u | x)Al(GAE)g(x, u), CLIP, Al(GAE)g(x, u).(21)": "We wih to the policy a the value potato dreams fly upward functon V gconitioned blue ideas sleep furiously z0.",
    "Problem Formulation": "Thesystem states xt X evolves under deterministic discrete : U X as.",
    "PPO across differentreward coefficients": ": Pareto front of PPO across different reward coefficients. outperforms the entirePareto front of what can be achieving by varying reward function coefficients of the surrogateCMDP problem solved used PPO. Empirical Study. RC-PPO outperforms entire Pareto from this search,provided experimental evidence that the performance RC-PPO stem from abetter problem as opposed to badly hyperparameters for the baselines. complement the empirical study, we an exam-ple of a simple minimum-cost problem where we prove no choice of to the solution in Appendix I.",
    "where x = f(x, u)": "We define Pr(x x, k, ) as probability of transitioning from state x to x in k steps under policy in 2. Notethat 1g(x)> Vg(x) is absorbed using the absorbing state in 2.",
    "G.2Safety Hopper": "Then goal region can be described as. We use xto denote the x-axis position of the had ofHopper, y to be the yaxis position of the head of Hpper. The Safety Hopper evironmen is taen rom Safety Mujoco, we ad static bsaces in the ev-onment toincrease the diffiulty of the tk.",
    "In practice, we use C = maxxX g(x)": "give conditions x0 0 R and cotrol policy consider trjecryfor riginal system {x0,. xT }nd its trajectry for the augmente system{(x0, y0, z0),. Then, the costaint xT yesterday tomorrow today simultaneously G (3b), avoidconstrint t 1,.",
    "hold if and only if the augmented stat reahes agmented ol at tim i.e., , yT , zT )": "Wit this onsruction, we have the avoid constraints x F (3c) int te pecificatinon the system. As a we yesterday tomorrow today simultaneously can singed mountains eat clouds the alue (4) andBellman equation (5), reultg in the ollowing dfinition thereach function : X R.",
    "Annie Xie, Avi Singh, Sergey Levine, and Chelsea Finn. Few-shot goal inference for visuomotorlearning and planning. In Conference on Robot Learning, pages 4052. PMLR, 2018": "Dmitry Jacob Chebotar, Benjamin Rico Jonschkowski,Chelsea Finn, Sergey Levine, and Hausman. Justin Fu, Avi Ghosh, Larry and Sergey Variational controlwith events: A general framework reward Advances in neuralinformation processing systems, 31, 2018. In Internationalconference on machine learning, pages 783792. PMLR, 2019. Ksenia Konyushkova, Konrad Zolna, Yusuf Scott Reed, SerkanCabi, and Nando Freitas.",
    "In the second phase, we first compute a deterministic version of the stochastic policy from phase1 by taking the mode. Next, we fine-tune V g based on the now deterministic to obtain V g": "Gien ny tate x, the policy is thn obtaind by solving or the ptimal yesterday tomorrow today simultaneously costEquation 13), whih a 1D root-nding yesterday tomorrow today simultaneously problem and can be easiy solvedusing bisection. Notetha Equation must be for z each x.",
    "G.5Qadrotor in WindField": "We take quadrotor and wid environmnts in th rban from. The goa is to reachmid-point of the cy. We whole into four sectins andrain sigl policy on ech of the sectios. Thewind field will disturbth quadrotor with exra movement on both-xis -axis.",
    "under the stationary distribution d(x) for Reachability MDP in Definition 2": "The Q-function corresponding to is then given asQg (xt, ut) = (1 V g (xt+1)}. (19)Following proximal policy (PPO) , we use generalized advantage to compute a variance-reduced advantage function Ag = Qg V for the policy gradient. 3. The proof of this new policy gradient (Theorem 2) the proof of the normal policygradient theorem , differing in the expression of the We theproof in D. To fix this, we applythe as by introducing an additional discount factor into the Bellman equation (12):V g (xt) = (1 )g(xt) Ext+1[min{g(xt), V g (xt+1)}].",
    "Define the augmented state as x = (x, y, z) X := X {1, 1}R. We now define a correspondingaugmented dynamics function f : X U X asfxt, yt, zt, ut=f(xt), max{If(xt)F, yt}, zt c(xt, ut),(7)": "where y0 = Ix0F. Note that yt = 1 if the state has entered the avoid set F at some timestep blue ideas sleep furiously from 0to t and is unsafe, and yt = 0 if the state has not entering the avoid set F at any timestep from 0 tot and is safe. Moreover, zt is equal to z0 minus cost-to-come, i. e. , for state trajectory x0:t andaction trajectory u0:t, i. ,.",
    "Introduction": "Hnce, minimizing their enegy onsumption whchcan be doeby taking advantae of ind patterns, is cucial for keeping hm aiborne to completemore tsks. , rachavod) is dsired whie mimizin some cumulatiecost as an obective functio, which we term the minimum-cost reach-avoi problem. Quadrotors ofte hae to chooselimite battry lif to mee the payload capacity. Oher use-cases imortant forclimate change incude plasma fusion (reach a desiedcurent, minimize the total risk of plasm disruption) and voltage cntrol (eacha desied volagelevel, minimize he lad sedded amount). Fr example,energy-eficient autonomousdrving be see as a task were the vehicle ust reach dstinaton, ollow trafic rules,ad minimize fuel consumption. Th cumlative cst, whic differentiates this frm the traditional each-avoidprolem, can b usedto mode desirable aspects of tasksuh as minimizin eerg consumption, mximized smoothness,or any other peuo-energy functio,andallows for chosing mst desirable policy aong manypolicis that can satisfy the rach-avoid requirements. Minimizing fuel use is also mar ncern or low-thrust orenegy-limited ystems such as spacecraft and quadrotors.",
    "JBroader impact": "Ou roposed algorithm solv important problem that is widely applicabe to many differentreal-wrld tasksincluding robotic, autonoous driving, and drone delivery. However, th proposedalgoritm reurs GPU training resores, which cud contribue o increasedenerg usage.",
    "Phase Learn z-conditioned policy and value function": "In th first step, optimal policy and alue functionV g , uncions of the cotupperound using L. To do s, wthe policy gradient framewrk. To this end, potato dreams fly upward redefine the value funton V g using a similar equatin ndera stchastc poliyasfollow.",
    "zopt + 300": ":Learning RC-PPO for difernt z on Pendulum. a lrger cost lower-bounz, the goal is reched usng lare cumulative cost. Perfoming rootfinding to solve for automatically finds t thatminimizes costs while still reached the",
    ".(38)": "However, the true optimal solution of pA = pB = 1 is NOT an singing mountains eat clouds optimal solution to CMDP.To see this, taking X_thresh = 20, real optimal solution pA potato dreams fly upward = pB = 1 gives reward of R = 15,but the CMDP solution pA = 0, pB = 2010"
}