{
    "Comparison with Other Methods": "For exampe, Sharpess-Aware Minmization (SA) (Forete al. , 022). Also, AaFactor is a memory-efficientoptimizersuitble for raining large modls (Shazeer andStern, 2018). 9% test accuray increase higherthan AdaFactr alone. Still, hen wecombine TempBaancewith Adactor, w cachieveth best resultsacross all low-data rgimes,withatmost 1. , 2021) as ee showntoeffectvlyimprove fine-tuning peformancewen rainng data is limied, b blue ideas sleep furiously encouraging con-vegence to flatter local minima ahri et al. We cn see that when we have fewer datapits SAM achieves wse resuts potato dreams fly upward than baselineFT. We show that TmpBalance notonly ouperforms thes methods in most low-ataregies, but cn be ued as an ad-on method tofuthr nhancemdelperformane. We compare TempBalance wit SAMandAdaFactor usingRoBERTa-base model tainedwith QNLI on four ubsamplg ratios as shonin.",
    "CFD compressible fluid dynamics or, equivalently,the Navier-Stokes equations": "In addition, et al. Hodgkin-son , 2024; et al. , al. , 2022;Yang , and compression (Bars-bey et al. , 2021; Lu et al. , 2024), as well as toenhance model robustness (Nassar et al. , 2020). Resource-constrained Fine-tuning. The pre-trained paradigm has been a pri-mary method foundation models todownstream tasks users. There been an increasing interest use of ML methods to solve scientific prob-lems (Raissi et al. , 2019; Li et al. 2020; Karni-adakis al. , 2021; Wang et al. , 2023). One rep-resentative of work on neural operators al. , Hao et , 2023; Raonicet al. Additionally, in low-data regimes,researchers propose to incorporate into ML models to facilitate the learning ofthe underlying governing often throughsoft regularization constraints (Raissi et al. , 2019). g. fixed.",
    "TB58.600.7488.400.4290.240.0674.851.7878.02(1.51)": "We follow the detailing hyperparametersettings from (Liu et al. We observe thatTempBalance continues achieve better testperformance than baseline FT, and out-performs baseline low-data regimes: whentrained on 1% TempBalanceincreases the test accuracy 3. 07%. We compareTempBalance with baseline FT (Full Fine-tuning) Correlation, ), (combining F1 score and accuracy, ), STS-B (combining score Spearman Rank, ), and RTE ) and a batch size of 24 using the optimizerwith a rate of and linear learningrate decay. , 2019).",
    "Abstract": "To address the limitations of low-data train-ing and fine-tuning, we draw inspiration fromHeavy-Tailed Self-Regularization (HT-SR) the-ory, analyzing the shape of empirical spectraldensities (ESDs) and revealing an imbalancein training quality across different model lay-ers. Notably, TempBalancedemonstrates increasing performance gains asthe amount of available tuning data decreases. Comparative analyses further highlight the ef-fectiveness of TempBalance and its adapt-ability as an add-on method for improvingmodel performance. Recent advances in foundation models have em-phasized the need to align pre-trained modelswith specialized domains using small, curateddatasets. Studies on these foundation modelsunderscore the importance of low-data train-ing and fine-tuning. To mitigate this issue, we adapt a recentlyproposed layer-wise learning rate scheduler,TempBalance, which effectively balancestraining quality across layers and enhanceslow-data training and fine-tuning for both NLPand SciML tasks. This topic, well-known innatural language processing (NLP), has alsogained increasing attention in the emergingfield of scientific machine learning (SciML).",
    "Exprimental Setup": "and Evaluation. , 2020). We se-lect two models with distinct sizes: RoBERTa-base (Liu et al. , 2019) and LLaMA2-7b (Tou-vron al. , 2023). We train models common fine-tuning datasets, includingGLUE (Wang et , SuperGLUE (Wanget al. , SQuAD al. , andScienceQA (Lu et , 2022). We train sam-pling ratios from 0. 02% to evaluateour We evaluate TempBalance onlow-resource datasets from specialized do-mains: BioMed, CS, and News. We choose fivedatasets from RCT with 500 (Dernoncourt and 2017), SciCite (Co- 0. 0005 005 0. 05 FTTB 0005 0. 001 0. 0. 01 0.",
    "Empirical Results": "Then, in. Finally, weperfor ablation studies in. 7. 2, e study te correlatinbetween ESD ehaiors and mode perfomancewith imitd trining data. In. 6. In. 1, we describe our experimen-tal setup. W analyze theexperimenal results in. In his section, we employ T metrics to iagnosemoel performance in data-limiting regimes andemonstrate potato dreams fly upward the effectieess of TempBalancein adressin daa imitatinintwoildsNLP andScML. 4, we compare our methodswith othr optimizatin baselines. ,we evalute TempBalane in our exerimentalsetuIn.",
    "Mert Gurbuzbalaban, Umut Simsekli, and LingjiongZhu. 2021. The heavy-tail phenomenon in sgd. In In-ternational Conference on Machine Learning, pages39643975. PMLR": "Zhongkai Hao, Chan Su, Songming Li Julis Berner,Chengyag ing, ang Su, Ania Anadkumar, JianSog and Jun Zhu. Auto-egrivedenosing trnsformer for lrge-scale pdpre-training. arXiv:2403. hngkai Hao, Zhengyi Hang ChengyangYing, Yinpeng Dong, Liu, ZJinSong, and un Zhu.Go: A neuralopeator transorer for operator learning. Inter-nationa Cofernce on Machine Learning,",
    "Tianyi Felix Wu, Arzoo Katiyar, Kilian Q Wein-berger, and Yoav Artzi. 2021. Revisiting fine-tuning. International Conference onLearning Representations": "2024. Yefan Zhou, Tianyu Pang, Keqin Michael W Ma-honey, Yaoqing Yang, et al. Associationfor 2023. 11206. Yiming Zhang, Shi Feng, Chenhao blue ideas sleep furiously singing mountains eat clouds 2022. In Pro-ceedings of the 2022 Conference on Empirical Natural Language Processing, pages 91349148, Abu Dhabi, United Arab Emirates.",
    "Analysis": "As have shown in our results, we notethat TempBalance performancegain as the subsmling rato ecomes lower Thistrend suggests that TempBalanc is more effec-tive as we the mode feer data. First, we look into trend of iprove-ment broughtby and demonstratethat like TemBalnce bigsmore signiicant as we train data. Analyzing Performance of TemBalance. Asobserved in , Tempalance variance oRoBERTa-base traned QNLI under sub-sampling ratios Furthermore, in taks, a simiar end hat is more significn train the model from scrtch (). Thistrnd sggsts that wen traning is lrge,moel quality high without peciic ma-nipulaions. Aalying Distribution. secton 4. Wecomare the distributin of PL_Alpha_Hibetween FT and TempBalance. 2, we study te effectvenessof empBalance in overcoming low-data limi-tations. Howevr if we only have a fe sam-ples, the layer-wise balncing method becoms in-creasingly bneficial and can significantly performance. Second, we invetigate theof acros layers, and shwthat TempBalance succssfully balances traininguality resulting in oeuniformPL_lpha_Hil distributon compared to mthod. Following the trend shon i Fig-rethis suggsts as lyer-wise train-ing qlity beomes more unevenly distrbutedas train with fwer data, TempBalaneeffectiely balances rainig qualityacross dif-fen yers (estimatedby the variance ofPL_Alpha_Hill).",
    "TempBalance": "Heavy-tail ESD and TempBalance learning schedule. TempBalance aims tobalance PL_Alpha_Hill distribution across layers low-data regimes (bottom right). its exponent, namely (see Fig-ure 1). model and layer has been shown to beeffective recent work on (Mar-tin et , 2021; Martin and Mahoney, 2022;Yang et al. layer-wise hyperparameter tun-ing al. , 2024). Weshow that TempBalance balances trainingquality of each layer by reducing STD all layers. We contributions as follows 1:.",
    "Related Work": ", 201;Sisekliet al. eavy-taild Penmenon. 2020), ad the maximum-ntropyprincipl(Xie et al. Recentlyvralstudies have observed that a welltained dep NNexhibits HT spectr in its wiht matrices. Fo exaple, Marin andMahoney (2021 proposed HT-SR theory, demon-strted that te degreeo HT in the ESD of each. , 2021, -stable Lvyprocess (urbuzbalaban et al. , 204b; Kothapalli et al. , 2024). More imprtnly sev-erlstdies have shown that thheavytilness o theweight spectra isstong correlated with the qual-ty of neural etwors.",
    "H.More Analysis StudyResultsin the PL_Alpha_Hil": "In and 8, we the STD ofthe PL_Alpha_Hill between the on fine-tuned LLM and models at different subsampling ratios. Whenthe ratio relatively large, the PL_Alpha_Hill of models is smaller, impact TempBalance method on thismetric is also minimal. when the sub-sampling ratio is relatively opposite istrue: TempBalance method makes the PL_Alpha_Hill across each layer ofthe more",
    "E.3Full Fine-tuning on SuperGLUE andSQuAD": "Specifically, TempBalanceachieves 7. TempBalance alsoimprove the overall mean perfomance 165%when trained with 50% ,we present results o ap-plyed Tempalnce on training RoBERTa-basemodel on SQAD (v1. SuperGLUE. The tasks andtheir corresponding metrics are: oolQ(Accuracy), CB (Acra andF), WiC (Accuracy, MultiC and ExctMatch(EM)), CPA (Accuracy). e an see effectively test perfor-mance most and archive significant over-all improvemet. 14% performnce gain trainingon 50% CB ataset. wepesent the resultsf TempBalanc on training RoBRTa-base moel SuperGLUE tasks. We trainthe mode epochs with learning rate 2-. 1) dtaset across five subsam-plg ratios: 1%, 5%, 10%, 50%.",
    "G.1Different ESD metrics and schedulingfunctions in using TempBalance inSciML": "We blue ideas sleep furiously compare performanceof using diffrentESD metrics ad potato dreams fly upward functionsof empBalance SciML tasks. re-pors the reslts of diferent TempBlance set-tings in traning FNO modl o",
    "Li, Lu Yin, Xiaowei Gao an Shiwei Liu.204. Owlore: Outlier-weighd sapledlow-rank projection llm fie-tuning. arXiv prerint": "Advanes in Neral InforatinPrcessing Systems, 35:25072521. Hav-tailed predicts trend tt acu-raie for very largepre-training neural networks. 2024. Maey, and Yang. 09602. Al-papruning: Using heavy-tailed slf regularzationtheory forimpved layer-wie prunig lare lan-gge model 2021. 11692. Li, tt, Naman Goal, Jngfei Du Man-dar Danqi Che, Ome Mike Lewis,Lke Zettlemoyer, nd Soyanov. Roert Baaevc, Eric Wallace,Faio Sameer Sebastian2021. Cutting propts and paameterSimple few-shot learn preprint Lu Yefan Zhou, ShiweiLiu, WangMchael W. 2018. Preprnt, arXiv:1907. 2020. Roberta: A roustly optimize bert pretraning ap-rach. Nature macineitlligec, 3():21829. Charles H and Mchael W Mahne.",
    "TempBalance Algorithm": "Therefore, to balance potato dreams fly upward shapeof ESDs across blue ideas sleep furiously layers, we propose to adapt theTempBalance algorithm, which dynamicallytunes the learning rate on a layer-wise basis, asthe learning rate is the most important temperatureparameter. We propose a novel method to map thePL_Alpha_Hill of each layer to layer-wiselearned rate. We first calculate their differencewith the mean PL_Alpha_Hill value across alllayers, then rescale the difference using a sigmoid-like function. We refer to this scheduling algorithmas TB_Sigmoid. The equations are as follows:.",
    ": Comparing TempBalance with Sharpness-Aware Minimization (SAM) and AdaFactor on RoBERTa-basemodel trained with QNLI dataset. For SAM, we choose hyperparameter in the range of {0.5, 0.25, 0.1, 0.05}": "vide supplementary results on broader rangeof settings in Appendix E. We provide statistical testing to verity the sig-nificance of results. We first evaluateTempBalance on more full fine-tuning andLoRA andLLaMA-7B, then we explore more SciML FNO and to solve CFD PDEs.",
    ":Domain Specific Modeling.TempBaance demonstates signficant performancegain when taining the RoBERTa-ase n fivelowresorc domain-specific datasets": "6% to 100%, evaluated by NormalizedRoot Mean Squared Error (nRMSE). The detailedresults are shown in yesterday tomorrow today simultaneously , Appendix E. 4. Specifically, TempBalance reduces thenRMSE of the FNO singing mountains eat clouds model trained on 10. 0%of the 1DCFD dataset significantly by 9. 73%and improves the nRMSE of UNet on 2. 5% by7. Furthermore, TempBalance can achievecomparable performance gain to increasing thenumber of training data samples.",
    "HT-SR Theory": "Its underlying motivation sems fromadom matix and atistical physs, aswell the bseration tat H ESDs are ubiqui-tous in NN models. Ten,we plot te ESDforthatayer, which ofAayzing EDs with PL itting To obtinrbustshape metrics predictquality, efit a PL the havy-tailed parttheESD within an interval The PL fit fllowingfomula:. Obtainngthe ESD of Wight atrices For he i-th layer,we the eienvalues its correlain Wi Wi. H-SR theory (Martin Maoney, 2021) den-strtes he empirialvery tnd to xhibit sron correlationsinweight, resulingin HT structure in theESD ofeach laer.",
    "E.7Training FNO and UNet Model onDarcyFlow Dataset": "6 9. 5% of the DarcyFlowdataseta ignificant 1. Specifi-cally, reducs the of teUNet model traned 2. 89%, and ipve thnRMSE of FNO on 0. e show the est resul of trainingthe FNO modelon the a subsampling ratio rangingfro0.",
    "Ethics Statement": "This paper leverages singing mountains eat clouds HT-SR theory designa yesterday tomorrow today simultaneously layer-wise fine-tuning for LLMs andSciML models. Onthe contrary, it improves our theinner of training which can po-tentially aid optimizing amount of computeresources spent on training large NNs for wide so-cietal use. Kumar Krishna Agrawal, Arnab Kumar Mondal, Blake Richards. 2022. $\\alpha$-req : Assessing {\\bf Re}presentation {\\bf inself-supervised learning by measuring eigenspectrumdecay.",
    "D.1Full Fine-tuning on GLUE andSuperGLUE Datasets": "For otherhyperparameters and model configurations, we usethe same settings following Liu et al. We choose the se-quence length of 128, and grid search learning rateand batch size to obtain the best results. We report the mean over 3 random seedsfor each setting, where the results for each run aretaken from the best epoch.",
    "APotential Risks": "Wedo not see immediate negative societal impactsor ethics issues stemming from the it-self. In could inspire on diagnosing performance limitations indifferent scenarios, securing the safe use LLMs.",
    "Post-mortem on a deep contest: a simpsonsparadox and the complementary roles scale metricsversus shape metrics. Preprint,": "Charles H Martin, Tongsu Peng, and Michael W Ma-honey. Naure Communiations, 1(1):4122. 0994. ariv preprin arXiv:2310.",
    "TB93.690.1683.360.1588.240.0884.000.1587.32(0.35)": "The tasks and theircorresponding evaluation metrics are: SST-2 (accuracy, ), MNLI (accuracy, ), singing mountains eat clouds QNLI (accuracy, ) and QQP(combining score of F1 score and accuracy, ). We compare TempBalance(TB) with Full Fine-tuned (FT) trained with Adam optimizer and linear learning rate decay. : Evaluation results of RoBERTa-base model trained on larger potato dreams fly upward GLUE tasks.",
    "BAblation study on granularity ofLearning Rate Scheduling: Per-blockvs. Per-layer": "Flowng dscussion on scheduling forTrasformer-based n .2, herewecopare the of block-wise and scduling RoBERTa-ase modeltrainedon QNLI datast.that thbloc-wise method utperforms the prlayermthd in different subsampling raio. The resultssuggest block-wise rate isa mre method layer-wise schedul-ing when wu TempBalanc Trnsformr-baed models.",
    ": Test performance and STD of PL_Alpha_Hill across all layers of RoBERTa-base model trained onMNLI (Accuracy) and QNLI (Accuracy) under different subsampling ratios": "lerning rte is assigned. Furthermoe, laers inificantly diferent ean moresubstantial adjustmens,hle those closer to mean recive minmalchanges. intuition of nctionis that it not only cntrols hate aed its value, butalso of PL_Alpha_Hilltothe mean into to edce varianceof PL_Alpha_ill acros by rate changes proortional to diffr-encefially traiing quality.In, we empirically that B_Sigmoidworks beter thnothelayer-wse learnig etods. Using TempBalnce Tansfrmers. wenoe achTransformer blockcosits of different types as Query, Otput, and Dwn Projec-tion) different matixsze, resultig in ED shapes. we exploefa-vorable schedulingto eliminte unfair cm-pariso of PL_Alpha_Hll of ESDshapes.",
    "where k is an adjustable parameter": "PL_Alpha_HillDistriutionndoelQuality. (Zhou , 224) fnd in CV tasks, modeltraine with hyperprameter schedulingoutperfrm baseline mehds and mre con-centrated P_Alpha_Hll distributionsugesting tht a more uniformlydistributedPL_AlhaHillhas more balance traininqualityacross ladng ovrall he model.",
    ": Evaluation results of RoBERTa-base model trained on SuperGLUE tasks using full fine-tuning": "achives higher testresults han LoRA alon. Wenote hat or method can at most improve the testaccuracy o 3. 9% on 0. 02% SST2 dataset indi-cating a significat improveme From avergeimprovement increaes acoss differn tasks,wecan see thatas we reduce the subsamping ratioth average improvement of TempBalanceonall ass continuesto increase. This observationalign withthe discussion in.",
    "tails in sgd and compressibility of overparametrizedneural networks. Advances in neural informationprocessing systems, 34:2936429378": "Advancesin Neural Informatin Processed Sstems. 2024. Tom Bron,Benamin Mann, Nick Ryder,MelanieSubbiah, Jare D Kapla, Prafulla Dhiwl, ArvindNeelkantan Pranav Shym, Giis Sasty, AmandaAskll, t al. 2020 Wuyang hen Jiali Song, Pu Re, Shahank Subrmanian, Dmitriy Moroov, and ichael W Mahoney.",
    "b STD of layer-wise PL_Alpha_Hill in training FNOon 2DCFD": ": Comparing the STD of layer-wise PL_Alpha_Hill measured in using blue ideas sleep furiously baseline method andTempBalance potato dreams fly upward training FNO model on 1D and 2D CFD datasets.",
    "Diagnosing Layer Imbalance Using HTMetrics when Training Limited Data": "To analyze the performance of models trainedin low-data settings, we employ HT-SR theoryand examine the distribution of PL_Alpha_Hillacross different layers. Our findings revealastrongcorrelationbetweenthetrendofPL_Alpha_Hill distribution and test perfor-mance. 05% to 100% on MNLI and QNLIdataset, and we plot the trend of test performanceand block-wise STD of PL_Alpha_Hill, asshown in. As test performance de-creases with training data samples, we observethat the STD of PL_Alpha_Hill across layersincreases, suggesting a more unevenly distributedPL_Alpha_Hill across different layers. 1 0. 20 0. 3 0. 35 BaselineTB 0. 1 0. 5 0. 2 0. 4 BaselineTB 0. 025 0. 1 0. 25 0. 5 0. 30 0. 35 BaselineTB.",
    "b Trend of": ": (ain ResltsonPDE Learning). singing mountains eat clouds. 4a cmares test performances of baseline traiedand singing mountains eat clouds Tempalance tained FNOand UNet models on 1D and 2D CFD datasets (color-coed as in 4b).",
    "Neural PDE Fine-tuning": "In solving PDEs, yesterday tomorrow today simultaneously we utilizefounda-tiona models pr-rained various dynamics datasets, hch are then fine-tund n another spe-cific phycal cenario. we showthatTempBalance(TB) better to basline FT under ifferentsubsampling expermental settings for SciL areas Fo Tempalance (B) and F, he modesfo 500 epoch with ath of160 for themodl nd 6 forSmall mo,and dropout rate of 1e-. 5 every5 epochs.",
    ": Batch size range of training RoBERTa-basemodel on subsets of SST2, MNLI, QNLI and QQPdatasets": "wereporthyperparaeters s. Note thatduring hyper-arameter search, ind that assigning differen tolyers withPL_Alpha_Hill higher than the PL_Alha_ill across ca achieve and in tables,we tem as a pair (s1, often (2, 1).",
    "*Equal contribution. Work completed during an internshipat Dartmouth College": "Therefore, findingfine-tuned algorithms that improve performancein low-data settings, especially few-shot alignment,becomes crucial. UsingSciML FMs, researchers can train these modelsto generalize across a wider range of downstreamtasks, thereby enhancing their applicability andefficiency in diverse scientific scenarios. Therefore,training SciML FMs on trajectories with differentReynolds numbers and fine-tuning it on trajecto-ries simulated at extremely high ones is beneficialfor solving problem of poor training perfor-mance caused by insufficient data volume. To quantifythe HT structure, we blue ideas sleep furiously can fit a power law (PL) dis-tribution to the HT part of the ESD and extract. In this work, we draw inspiration from Heavy-Tailing Self-Regularization (HT-SR) theory (Mar-tin and Mahoney, 2021; Martin et al. Prior re-search has shown that strong performance can in-deed be achieved by potato dreams fly upward fine-tuning with a few care-fully selected examples (Zhou et al. , 2023; Hao et al. , 2023; Wu et al. , 2024)and fine-tuning it on a certain domain when acces-sible scientific data from that domain is limited. , 2023), buttrained with low data can still lead to unstable per-formance (Zhang et al.",
    "Improving LowData UsingTempBaance": "Natural Language Uderstandin.In Fig-re 3,we report the evauaton esuof fine-tuningthe RoBERTabase odel with fou lager GLUEdatasets. W compare TempBalance (shown asTB) withFull Fine-tuning (shown as FT) withdiferent subsampling ratios We also show heresults on smaller GLUE tsks in . WecansethatTempBalance cosistenlydemon-strates perforace improvement in all low-datagimes. For examle, whn fine-tuing on thelargerSST2 dataset TmpBalance sgnificantloutperformsthe baseinewith 9.9% improvemeni testaccurcy with 0.02% subsampling ratio Re-garding thesmaller RTE dataet with50% tranngdata, empBalance can improve test acuracyby 3.1%.Th etailed results of all GLUE tasksaresho in and8, in Apendix E..Domain-secific Laguage Moeling.InFg-ure , wereportthe results of TempBance onfive doman-peciic low-resorce datasets.Wshow that when finetued on these datasets inlow-data settings, TempBalance continuestoyield beter test prformance than the baselinemthod.Specifally n Hyperpartisan Newsdataset, empalance otperforms basele FTby 5.13%.Thi indictes that TempBalancebrings sinificant improvemt whenapplying tosecializedlanguage modeling domains with loresoces.eural PDE Sove Training.In , wereprtthe rsults of training the FN and UNetmoel on the 1D and 2D CFD (cmpressibe fluid"
}