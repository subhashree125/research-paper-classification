{
    "Video Annotator: A framework for efficiently building video classifiers using vision-language models and active learningKDD 24, August 25-29, 2024, Barcelona, Spain": "video vision transformer. ArenAlsaid and D Lee. The DataScope: mixed-initiative archi-teture for data Anura Arnab, Mostfa hghani, blue ideas sleep furiously Che Lui, ndCordela Schmid. Flamingo: visualmodel forew-hot learing. Jea-Batiste Alayrac, Donahe,Luc, ntone Iain yesterday tomorrow today simultaneously Barr, YanaHaso, Karel Lenc,Athur Mensch,Millcan, Malcolm Renold, et al. IEEE,. 2022. 2022. Proceedingsof theIEEECVF inernationalcnferenc oncompter 6866846 emntic search by auto-matic annttin using TensorFlow. iNeural Iformation 352371623736.",
    "RELATED WORK2.1Visionnguage Models (VMs)": "Specifically,VLMs such as good zero-shot naturallanguage understanding performance across a wide variety of vi-sual Similar to SeeSaw, we use VLMs to enable search. In contrast to our approach on video the purpose of unseen videos vs. interactivesearch sessions, and uses active learning of query alignment.",
    "EXPERIMENTS": "For every label, 20% singing mountains eat clouds of labeled data,obtained through uniform sampling, is allocated for testing, whilethe remainder serves as the training set. evaluationmetric employed is mean Average Precision (). yesterday tomorrow today simultaneously This section details our experiments to study the sample efficiencyof VA comparing to baselines.",
    "INTRODUCTION": "techniques for training machine classifiersare intensive. This labelingprocess tends to time-consuming and inefficient, often haltingafter a few annotation cycles. These annotators perform the labeling task without a deepunderstanding of the models intended deployment or usage, oftenmaking consistent of borderline hard examples, in subjective tasks, a This often necessitatesmultiple review rounds domain experts, to unexpectedcosts This lengthy can also result in model drift, takes longer to fix edge cases and deploy new models, potentiallyhurting usefulness and stakeholder trust in these technologies. We that more direct involvement of experts,using a human-in-the-loop can many of these prac-tical challenges described above. We a framework,Video (VA), for annotating, managing, iterating classification datasets. Our interactive framework employsactive learning techniques, to guide users to focus their efforts onprogressively harder examples, enhancing the sample effi-ciency and keeping low. Equipped with knowledgeand context, they can rapidly make informed decisions on during the annotation process. This design simplifies modeldeployment, scale to a number with-out increasing complexity. building the data annotation process, facilitating uservalidation the model before therefore and sense of ownership. VA supports acontinuous annotation process, users to rapidly deploymodels, their quality in and swiftly fix anyedge cases by annotating a few more examples deploying a newmodel This empowers users to.",
    "Castellano. [n. d.]. Breakthrough/pyscenedetect: Python and opencv-based scene detection program and library": "Vison-audio-language omni-perception petraininmodl and dataset. 2023. Boris Chen, Air Zai, Reecca ucer, and YuchenXi. 08345 2023). Shan Cen, He, Guo, XinxinZh,Weinng Wang, Jinhui Tang,and Jing Lu. 21152125.",
    "ln /, where is the total number of steps executed so far, and is thenumber of times arm has been selected before the currentstep. We tested various and found = 102 toperform the best": "5. 2Experimental setup. The design of VA enables users haltthe labeling process once certain minimum conditions are",
    "METHODOLOGY": "Fig 3 depicts an overview VA. e gathera divere set ofvideo the em-beddings for which extracted usig , and for efficientretrieval (e ) other metadata abot clp.",
    "Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Zamir, Fa-had Khan, and Mubarak 2022. Transformers in vision: survey.ACM computing surveys (CSUR) 54, 10s (2022), 141": "Joosug Kim, Gwon, Jin-Tak Pak, Hakil and Yoo-Sng Kim. Junnan Li, Dongxu Li, Xiong, nd Steven H.Blip Bootstrappinglanguage-iage pre-training for unified blue ideas sleep furiously viion-anguageandgeneration. International Coference on Machine Leaning.",
    "Step Text-to-video search to bootstrap the ano-tatio process": "as shown in. We the ondensed ovis ,whichof closet belogin t 3.oviesacross wide range o genres, decades, and countries Then we extracted shot-level embeddingsand ued those to remoe nar-duplicae sht.",
    "Ira Cohen and Moises Goldszmidt. 2004. Properties and benefits of calibratedclassifiers. In European conference on principles of data mining and knowledgediscovery. Springer, 125136": "Maureen Daum Enhao Zhang, Dong He, Stepen Mussmann, Brandon Haynes,Ranjay Krishna, and Madalena Balazinska. arXiv preprint arXiv:2303. 0408(023). 2023. Leae The World Behind. Boris Chen et al. 223. Netflix Techlog 2023).Vi Iyengar et. 222. NetflxTchBlog (2022).",
    "Experiment 2: annotation candidate sourceselection": "A demonstrated th typically yields supeior qualityand data diversity cores.Neerthelss, anecdoa feedback suggested that our users, whieappeciating the VA workfow and user optiml action to takeflloin modelretraining sep.However we that this cold be preservedwhile also assisting users by intrmittently recommending  batchof examples or ime step , the occur:.",
    "Chris Smith. 2021. Operation Varsity Blues: The College Admissions Scandal.Netflix (2021)": "Lubomir Stanchev, Egbert, and Ruttenberg. 2020 IEEE on Semantic Computing (ICSC). IEEE, 2023. of large visionmodels and visual prompt engineering. (2023), Hongwei Xue, Yuchong Bei Liu, Jianlong Fu, Song, Houqiang Li,and Jiebo Luo. 2022. Clip-vip: Adapting image-text model to video-language representation alignment. arXiv preprint arXiv:2209.06430 (2022). Vinvl: visual representations invision-language Proceedings of IEEE/CVF conference on computervision and 55795588.",
    "DD24, Agst 25-29, 2024, SpainAmir Ziai and Anees Vatakavi": "However both larger ( 100) nd smaller( 0) bath sizesed to a ecline in performanc. Interestingly, ven a bsic oud robinapproach can improve performane over h baseline. At = 0, we independentlyevaluate thefirst atch foreac source nd compute. 1. e. , VA2). 12) compared to VA withoutan agorithm forandidatesource selctio as the baseline. summarizes e distributionfor each algorithm. At = 1, the algorith choosesVA, rompting us to concatnate the first three btcheswitha new VA batch f size (i. : We first divide the ata for each source int batchesof size , illustrated here as VA, ZS (zero shot), and (ra-dom. At = 2, the algorithmselects , which leads to appending 2. 4points (excluding the reed oracl), resultng in a cumlative me-dian improvment of 8. 3Results. 2. 40 steps nd ahieved the hes perormance. We find that the median gain is positiveor allalgorithms. 5. Uponreplicatingthese experents with a larger batch siz of = 50, singing mountains eat clouds we obsrvevery similar outcomes. For each lael,we calculate the gain similar to. 3 points over th most competitive baselinemethod CC (se. CBacheves the highest performance of 3.",
    "ABSTRACT": "VA achieves a median 8. com/netflix/videoannotator. High-quality and consistent annotations are fundamental to the suc-cessful development of robust machine annotation methods are resource-intensive and of-ten leading to a reliance on third-party annotators are notthe domain experts. 3 point improvement Average Precisionrelative to the most competitive baseline of We release a with labels 56video understanding tasks annotated by three professional videoeditors using also release code to replicate our experimentsat github. These can ariseunpredictably during the process, requiring a variablenumber of iterations and rounds of to unforeseenexpenses and time to guarantee quality. We leverage the zero-shot of models combined with active techniques, demon-strate that VA enables the high-quality models. We posit that direct involvement domain experts, usinga human-in-the-loop system, can resolve many of these practicalchallenges. We propose novel framework we call Annotator(VA) for annotating, managing, and on video Our approach offers a new paradigm an model process, enhancing the efficiency,usability, and of video Uniquely, VA allowsfor a continuous annotation seamlessly integrating datacollection and model training. Hard samples, which are usually the model to difficult to accu-rately and consistently without business context.",
    "A the annotator through labelng process displaingmodel qulit andata diversity scores": "e the sum all he counts, and noralize (i. e. We mapte positive and annotations idepenentlyonto theseclusters, and compu reslting capped it to maximumvalue. The scores fom 1. model for qualiycoud exhibit bias to diverse train-ing of visual cncepts is neces-sary th robust modls. Wetherfre forulated heuristic we call te diverity score to guie goalo accumue annotaions a varietyof visualelements. An example is depced in. We picked BA because is notsenitive class imbalance and is an metric o uderstad. We reportthe percentile of mean/median in tobe more consevativein ur rporting. To measure his, we use -foldcross-validation report h 25th percentile acrss the Bal-ancd Accuracy (BA) scores."
}