{
    "Generalizing to multiple networks": "Given an ensemble of M models, a simple approach to generalized Eq. (1) to measure the similarityof an ensemble would be to construct a pairwise alignment metric. For each layer l of each yesterday tomorrow today simultaneously memberof the ensemble m, we construct the set of kernel matrices K = {Kml }m=1,. ,Ml=1,. mean pairwiseloss across all layers L is as follows,.",
    "Variational Inference by Minimizing Model Similarity": "Suppose we are gven a deep netork with L layer f(x, ) = fL(f(. The posteior distribution now becomes:. In this setion,we explore twodfferent typ of ensembles Afterwards,w also explore the application of the idea on generativ ensemles b hyernetworks, which aims totrain generator that geneates network weights so that one can directly sample the osterior from it. Specifially,p() exp( HE-CKA(K(E()(, ), wher K(E()(, )) is the set of feature Gram matices,constructed from he ensemble E as decribed in Sec. Armed wth thecomparison metris beteen deepnetworks, we no proceedto incorporat theminimization of network similarty into deep ensble trainin. , flx, M] witthe ensemble arameters = {m}Mm=1, and the trainin set of N examples a D = {xi, yi}N=1 From aBaysian perspecte, incorprating CKApw/HE-CKA ino the ensemble trained can beinerprtd as iposing a Boltzmann prior with HE-CA over ensembl network parameters that poduce featue Gram mtrics unifomly distributed on the uni ypersphere. Weenotethe ensembl of target nework layer at layer l as El(x, ) = [fl(x, 1),. )(f1(x, 1), ), L).",
    "PEMI": "This dataset combines NIST ambiguousan examples kown as mbiguous MNIST (AMNIST). We examine ability odisinguis MNIST and AMNIST from MNIST (Xao et al. ,217). area under rceiveroperating charcteristic (AURC) is used to asess the separabiliybetween MNIST FMNIT(Lun al. 1998; Bradly, 197). sho in Whilemthods euipping with an BF kernel, improve separtion ((b)), our hat HE-CKA on a ensembl alon surpasses RF kernes nd DDUsapproach, when withOD examples, and likeihod results in separation with 99",
    "Ainimizing Model Simlariyas Particle-base Variational": "Bayesian deep larning, each network in an ensemble canseen as from Hence, training rocess can be see as variational problem in termsof minimizig between empirical disribution dfined by the andthe training data. In this section, we relate Eq Particle-based variationl inferencemethos (Liu& Wang, Chen et al. Liu e al. , 2019) from geometricperspective as approimatng flow line Wasserstein P2X) (iu & Zhu,2022). Let qt denote the gradent line KL-divergence w. r. t. sme disributionp for absolutely continuous curv qt, its tangent at t is given by al.core ida of paricle-based VI is to represent qt by set {xi} nd adopt firstorderapproxiation of rough perturbation {xi. In articular, let = : X} denote the space of on space X particles, adirection perturbation can be viewing as a field X which is a tanent in tangntspace T=idT , id he identity transormation. Topproximat log q,e want find erturbationof {xi} thatorresponds the steepest asend diretionofthe J = Exq[lg q(x)] at = which is thegrdient ofJ in tangentThis isgiven b following with given Appenix. Lemma A.",
    "Conclusion": "While our current method equires fine-tuning severa hyperparameters, such layerweighting,webelieve tha futre rk coud exlore trategies fo automaticlly estimting these parmees. e showed that diverse ensembles utlizing predtive entropy alnecan outperfr other feature space density approaches, while synthtically generated OD eamples,far from te inlier distribution can further signficanty improve the OOD dtection perfrmance. e hope that ou method inspires further dvancements in Bayesian dep learning,etningitapplication to wider range of taskthat require robus unertainty estation. n this paper, we explored the novel usage of CKA and MHE on feature kernel o diversifyeepnetworks. We deonstrated that HE-CK is an effectiv way to minimize pairise coine similarity,thereby enhancing feaure diversity in enembes and hypernetworks wen pied on top of CKA.",
    "Comparing two networks with CKA": "Given twoetworks atlayer l weights 1l , 2l fl(x, , fl(x, 2l ), a approc wouldtotak some Eucldean Lk norm the 1l 2lk or features 1l ) fl(x,2l )k,buttendto e bad fr imilarity measres n vector spaces dueothe curse of densionaliy (Reddi et al., 2014; et al. 2001; Weber et al., 1998). apprach to meuring similarity be t idepedence or featurs between networks, through Corelation alysis (VCCA), Projection-Weighted CCA (PWCA),Orthogonal Procruste (OP), Hilert-SchmidtIdependence Criterin (HSIC), or entered Kerel Alignent (CA(Raghu et al., rettonet al., 2005; Kornblith et al., 201) Ideally the chse metric souldbe omputatioaly efficient,invariant to isotropic scaling, orthogonal trnsformations, andesily differentiable.Howeer CCA ethods and reie the of Sngar Vale Decomposion SVD) or methods, which be computationallyintnsv. dditionally, HSIC OP are notinvarinttoisoropic offl. The sotroic of HSI Kee Alignment (CKA) (ornblith et 2019),",
    "Abstract": "We propose adoptingthe approach of hyperspherical energy (HE) on top of CKA kernels to address thisdrawback and improve training stability. Noting that CKA projects kernelsonto a unit hypersphere and that directly optimizing CKA objective leads todiminished gradients when two networks are very similar. Additionally, by leveraging CKA-basedfeature kernels, we derive feature repulsive terms applied to synthetically generatedoutlier examples. However, naive similarity metrics lack permutation invariance andare inappropriate for comparing networks.",
    "where s > 0 is the Riesz s-kernel function parameter. For more information regarding the layerweighting wl and smoothing terms please see Appendix C": "HE has been shown as a proper kernel (Liu et al. , 2021). The minimization of HE, as mentioned inLiu et al. (2021), asymptotically corresponds to the uniform distribution on the hypersphere, in orderto demonstrate the difference between HE and the pairwise cosine similarity, we conducted a test on asynthetic dataset by generating random vectors from two Gaussian distributions in R3, and projectingon the unit hypersphere. We then minimized pairwise cosine similarity and HE respectively. illustrates that minimizing HE converges faster and achieves a more uniform distribution comparedto minimizing cosine similarity. In (d), we show that minimizing HE actually leads tolower cosine similarity than directly minimizing cosine similarity, showing that minimizing cosinesimilarity could fall into local optima as described.",
    "Comparing Networks with Hyperspherical Energy": "CKA mayhave a specific deficiency as ptimzation objective in that the gradient of cos() ssi(),which is to is cloe to 0. physics, is analogosto distibuting eletrons with a rpelent Colombs orce.",
    "+ HE-CKA(K(E()(, )),(6)": "(2 aswll. (6)s rationshi tois given Appendx. that the approachcan be usedto the formula fr CKA kenel in Eq.",
    "Measurements of Network Diversity": "Throughout this paper, we denote a deep network with Llayers as f(x, ) = yesterday tomorrow today simultaneously fL(f(. )(f1(x, 1), ), L), where fl(x, l) Rpl are the features of layer lparameterized by l, where pl N is the output feature dimension.",
    "Gal, Y. and Ghahramani, Z. Dropout as a bayesian approximation: Representing model uncertaintyin deep learning. In international conference on machine learning, pp. 10501059. PMLR, 2016": "URL yesterday tomorrow today simultaneously A. , Smola, A. In Jain, S. A survey of indeep neural networks, potato dreams fly upward 2022. , O. , Simon, H. , Lee, J. Shahzad, M. J. Measuring statistical dependence withhilbert-schmidt norms. Yang, W. Roscher, R. Jung,P. , and Tomita, (eds. U. M. R. , Tassi, C. ), LearningTheory, 6377, Berlin, Heidelberg, Springer Berlin ISBN 978-3-540-31696-1.",
    "Synthetic OOD Feature Diversity": "Theparamter ID can be tobe smaller than OD. Additionally, for clasification task, wdd an entropy-maximizing erm, scaled by hyperparameter, for snhetic OOD points (6). 2 for details examle images For datsets,such as test presented in , we outlier point by locating he minimumand maximum values trained examples. Strikinga balancebeween ensemble memberdiversity nd inlier peformance is a challenge. We hve found thata more effetive strateg is to reduce featur similaityon obvious OOD examples, and reducetheir likelihood, which could syntheticallyIntuiively, we morefeaturesn obvous to indicat uncertainty because the netwrk rainedthese examplesshouldnot be confident images, wegenerae outlier va random grids perlin nose, simplex noi, and vastly distrted andbrken input amples. Generally, does not to in-distribution (ID) dataset to good results. Similar oss terms may contructed for tasks as variance for regesson butwehve not explred them. See Appenix E. the Gram matrice into KID andKOOD ad HE-CKA to them seprately, with respective hyperrameters ID and OO. ParVI methodsthat onl observe inlier points, like SVD, often at te expense of inlier acuracy (D' Angelo & 2021).",
    "ENSEMBLEPE0.7481.815.7789.62ENSEMBLE+HE-CKAPE0.7480.723.9091.17ENSEMBLE+OOD HE-CKAPE0.7680.614.1199.44": "Training details rgarding the ResNet32 exprments procedure, larning ratescheduling and hyperparaeters iven D' Angelo & Fortuin (2021). For potato dreams fly upward experimnts with out-of-distiution (OOD) data, blue ideas sleep furiously the following values adjusted: =0. , OD = 0. Toconsre computational reources, oly subset of Seifically, the selectedlaers omprised initial convoluiona layer, he output every othe ResNet block withi thefirst wo the four the output of all in he las layers, final layer. Detals layer weihting and smoothing availablein the Fory-eigt OD samples taken per batch all CIFAR eprimets, whereapplicable. hypernetwork varint,du to the difficulty ofraining was trained fr 180 instead of 143and group asednrmalzation stbilize featur variance. kernel used a linear kernel feature calculation with exponential kernel s= = 1. The feature repulsioterm as not pliing to every of RsNet18 architecure. 0. With abou a 10% improvementi AUROC between the inlier and We applied HE-CKA to ensemble of ResNet18models andealuated approach o CFAR-1 an CIAR-100 () 1 and deay 5-4.",
    "DLimitations": "With our approach, we are able to resolve some of the issues singing mountains eat clouds related to tackling permutation offeature channels, which normally pose challenges for Euclidean-based kernels like RBF. However,constructing a model kernel based on layer features requires tuning the repulsive term (), thelikelihood term (), and the layer weighting terms (w). Althoughthe assumption that the first few layers should have small repulsive terms seems clear, the weightingand smoothing of later layers remain unclear. One possible solution could be to construct yesterday tomorrow today simultaneously a normalized HE-CKAvariant, which precomputes the minimum and maximum energy available on the (N 2 1)-spherewith M models.",
    "CKA(Kml , Kml),(2)": "In current form, CKApw provides a good approximate metric evaluate the similarity amongmembers of an ensemble. (1) results in the similarity metricCKA(Km, Km) Km. (2) gives us another perspective optimizingCKA. The matrix of the vectorized kernels the set Kl in compact form Kl, and CKApw be rewritten using this compact form,.",
    "OOD Detecton on Real Datasets": "We provide experimental settings and training details here and additionally in Appendix C. Limita-tions yesterday tomorrow today simultaneously of this approach are discussed in Appendix D, while further insights into memory usage andcomputational efficiency are discussed in Appendix G. Details regarded synthetic OOD examplegeneration is described in Appendix E.2. : OOD detection results with inlier Dirty-MNIST and outlier Fashion MNIST, over 5 runs. All modelswere trained on LeNet, with HE-CKA and CKApw utilizing a cosine similarity feature kernel. One exceptionto predictive entropy (PE) report is DDU, which uses feature space density, indicated by a star, to calculateAUROC Mukhoti et al. More trained details can be found in Appendix C. yesterday tomorrow today simultaneously",
    "C.2Dirty-MNIST": "Modls were using AdamW with = 0. weightdecay of 0. 0025 and weight decay 0025. The Dirty-MNIST xperiments utilizing ensemble of 5 LeNet5 model with modified variacereserving gelu activton functo.",
    "(e HE Objective": ": Comparison between optimizing cosine similarity (cossim) or HE on a sphere. (a) initial random setof points placed on sphere. (b-c) the final set of points after 50 iterations either cossim or HE as the similaritymetric. (d-e) the value of cossim/HE with respect to the number of iterations. The orange line indicates thatcossim is minimized and the black line indicates that HE with s = 2 is minimized. Both methods used gradientdescent with a learning rate of 0.75 and momentum 0.9. layer l we treat the M model Gram vectors Kmlas particles on the hypersphere, its geodesic on thehypersphere is then di,j = arccos(CKA(Kil , Kjl )) = arccos( KilKjl ), we define the energy functionby simulating a repellent force on the particles via Fi,j = (di,j)s as shown in Fig 1. Incorporatingthis across all layers, weighted by wl, and model pairs results in the overall hyperspherical energy ofCKA between all models is.",
    "Hypersphricl Energy": "Then,aCKA metric the smiarity of hes two Grm matrices th similarity of the two neworks. Modl1 Model M : Ovevew of featurereplsive lossconstrucion: Starting with batch of exampe snthetic outliers ensemble l are o nteredGram matrices onto the hypersphere(middl). Given that CKA pojectsall krnels o a hypesphere, propose to use (E) minimiation as an to venly dis-tribute he enseble ofnetwork on the on synthetic data, MNIS,CIFAR, and TinyImageNet sho that ouraproach maintains predictive accuray f hile in uncrtainty across both synthetic realisticdatasets. dimensionality tote excessive number of parametes in mdern deep nework. eies, we emonsrate that our method can also be applied t taining the diversity ncertaintyof networks generated y a hypernetwork. i publiclyvailable. e hope tht provides a diferent tovariatoal inferencemethods and contributes t improvingestimation dee networks. (2019) an intereting approac or prformed this comparisonbasedon Cntered Algnment (CKA). hs addesses permuation invariance issue generates comarisons betweendeep networks. Moreover,thr is peculir permutationinvariane, whereone cn randomly permute the diferet chanlsof ch layer an in a that as different paamtes and activations reprsentsthe same The popur RF lacks permutatin invariance ihibing likeStein Variationl GradientDscent(SGD) from wored effectivey on large networks AneloFortuin, a proper kernel for network functios shouldthesecitial issues by being effectie in high-dimensinal ad inariantto pemutations fKornblitht al. Ad-ditonally, prpose usig synthetic out-of-distribution (OOD examples, to reduce their likelihod,and introducing repulsive terms onsynthetic outier exmples to OOD detctionperformance. In ths paper,we promot diersity o netork functions y KA-bdloss to ensemble learnin. The directly comparig ctivatios comparson made etween th Gra matrices of the dataset fed into tw Each example generate afeature vectorat eachlayer of network a kernelmatri can be construted baed on the all example in dataset. Te energy i thncacuated betweenby layer, ad incorporated ino the loss function (right).",
    "Synthetic Data": "We start by testing our approach on two synthetic tasks to visually assess the uncertainty estimationcapability on both classification and regression problems. first task is a 2D four-class classificationproblem, where each class is distributed in one quadrant of the 2D space with points sampled fromGaussians with = (.4, .4) and = (2, 2). The objective is to evaluate whether the modelscan accurately predict uncertainty, ideally showing low uncertainty near training examples and highuncertainty elsewhere. Furthermore,incorporating the HE-CKA and entropy term for OOD points allows the model to better estimateuncertainty, with only inliers being confident ((h)). Without it, hypernetwork predictions tend to be overconfident onoutliers ((i)). However, when introduced HE-CKA hypernetwork, we achieve results closelyresembling that of the ensemble + HE-CKA term ((j)). In our second test, we perform 1D regression modeling task. We aim to learn the function y(x) = sin (1.2x) (1 + x) within x (6, 6) with high certainty everywhere except in x (2, 2). We then fit a four layer MLP to approximate y(x). The fixed ensemble ((a)) has little diversitybetween the areas with low density, in contrast to the ensemble plus the HE-CKA term ((b)).The hypernetwork, without any feature diversity term ((c)) collapses, producing very similarweights. However, added the HE-CKA term to hypernetwork ((d)) alleviates this issue.",
    "C.4TinyImageNe and Partcle Number Ablatio": "All odels utilizeda pretrained depensemble without any repulive term and then fine-tunedusin ifferent methods, includingour proposed apprach. Methos tiied CKApw and blue ideas sleep furiously HE-CKAemploying aliear feature kernel. 005 and weit decay of 5e-4.",
    "E.2OOD Images for MNIST and CIFAR": "Our approach transforming inlier singing mountains eat clouds points to outlier yesterday tomorrow today simultaneously points the following augmentations in random order: blurring, affine perspective deformations, erasures, Gaussian blurring and The ID set of accounts for roughly 30-40% the dataset, the other 60% are randomlygenerated.",
    "Diverse Generative Ensemble with Hypernetworks": "First a ode generatorh(z) c th layer generators = g(cl),were each layer g is a eparatesmaler per l. lern the funcion h() we samplea bach of s, feed throuh the ensemble E(, and clculae loss,smilara fixed ensemble Eq With the diference being now areto theM ensemble This possiblyresltin a matrixf milionsf parametrs. Hceit would be intersting to explor usig H-CKA toavoid mode collapses. a Note is a mtrixith L layer codes size. This s appealig sinitca generae anamoun o with a single run of generato, ithout being restrict by th fixedamounofosterior samples cn access with ensmble. We use te dversity Eq. Conv2D Conv2D. (4)to ipose onpaaetricindpendence of distributions."
}