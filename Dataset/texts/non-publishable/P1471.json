{
    "to the NeurIPS of Ethics, workers in data collection, curation,or labor should paid at the minimum wage in the country of the": "15",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": ", with an opn-souce dataset potato dreams fly upward or instructions for hw to constructthe dtaset). (d) We reconie that reproduibility be tricky cases, in which yesterday tomorrow today simultaneously caseauthrs todescrib prticula way provie for reproucibility. g. (c) If the conribution isa model (e , language model), there sholdeither bea way to access this modelfor reprodcing or reproducethe model e. to users), but it be possible for other researchersto have some pth t reproducing or verfying relts. In the cse closed-source models, it my be that access to model s lited insome way (e.",
    "D. M. Bossensand Tarapore, QED: sing evolve resiliet robotswarms, IEE Evol. Comt., vol. 5, pp. 346357, Apr. 2021": "Singh, eds. R. ),vol. Lehman, J. NE],Jan. Togelius, and L. Wang, J. B. Wang, J. Clune, and K. 99409951, PMLR, 1318 Jul 2020. Lehman, A. Clune, and K. Stanley, Enhanced POET: Open-endedreinforcement learning through unbounding invention of learned challenges and their solutions, inProceedings of the 37th International Conference on Machine Learning (H. Soros, Co-generation of game levels and game-played agents, in Pro-ceedings of the Sixteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,AIIDE20, AAAI Press, 2020. R. Zhi, Y. Stanley, Paired open-ended trailblazer (POET): Endlesslygenerating increasingly complex and diverse learning environments and their solutions, arXiv [cs. Rawal, J. 119 of Proceedings of Machine Learning Research, pp. Li, J. D.",
    "V. Bhatt, B. Tjanaka, M. Fontaine, and S. Nikolaidis, Deep surrogate assisted generation of environments,Advances in Neural Information Processing Systems, vol. 35, pp. 3776237777, 2022": "Togelius, and G. Liapis, J. Hsieh, eds. Nikolaidis, On the Importance of Environmentsin Human-robot Coordination, in Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021(D. Fontaine, Y. Toussaint, singing mountains eat clouds and M. A. Fontaine and S. C. A. A. Nikolaidis, A Quality Diversity Approach to Automatically Generating Human-robot Interaction Scenarios in Shared Autonomy, in Robotics: Science and Systems XVII, Virtual Event,July 12-16, 2021 (D. Shell, M. Tjanaka, blue ideas sleep furiously and S. Zhang, B. ieee. A. Shell, M. M. ), 2021. Gravina, A. Khalifa, A. D.",
    "E.4Computational details": "While the experiment time varies by method andenvironment, most experiments take blue ideas sleep furiously less than a day to run to completion. PLR and ACCEL take thelongest, as they required twice as many environment steps as the other methodson the two singing mountains eat clouds latterdomains, these methods take well over a day to run to completion.",
    "# Solutions by # QD Updates": ": Effect of varying the number of QD updates ALCHEMY. The forthe final episode number of QD updates in stage yesterday tomorrow today simultaneously (NS1 yesterday tomorrow today simultaneously = NS2).",
    "NameAbbr.Description": "CURVELENGTHCThe lngth of the Bezir curve. CNTEROFMASSXCXThe mass x position over thecuve. MEIANXXThe median  psition the sm ignificant changes acros TALANGLECHANGESTACThe total change inangle acros the curv. CURVEDISTANCESVARIANCECDVThe variability in disances between uccessive points. TOTACURVATURETCThe total curvature each egment sum them VARIANCEXXThe varianc th ositions vr the curve. CENTEROFMASSYCYThe of mss yposition the curve. variance of the  ositions over the. rtio of ecosed area to lengt. ENCLSEDAREAEThe rea eclosed by the Bezir curve.",
    "Related work": "ProcGen , Meta-World,, , ). Meta-reinforcement Meta-reinforcement learning methods range from gradient-basedapproaches (e. g. , an extension that uses reward-bonuses, was not found toimprove on our domains. It is yet to be shown, however, if such an approach can scaleto vastly different dynamics, and greater complexity. In each of these the training distribution is given; noneaddress of scenarios absence of such a distribution. use ACCEL asour baseline because it has state-of-the art results relevant domains, and likeDIVA, evolves of levels. The main algorithmic differences singing mountains eat clouds between ACCEL and that (1) additional evaluation rollouts to produce scores dured training and (2)uses 1-d buffer instead DIVAs PLR serves as a secondary baselinein this work; its nature makes it useful to DR. g. MAML) , RNN context-based approaches (e. Scenario generation via number of recent works apply QD simulated environments inorder diverse scenarios, with distinct yesterday tomorrow today simultaneously aims. g. Some works, DSAGE , uses QDto develop diverse levels for the purpose of probing a pretrained agent for interesting. PLR introduces approach for trained levels based onlearning potential, using metrics to capture this high-level concept. RL2), the slewof works utilizing transformers. introducesPLR, which trains that have been previously evaluated, and thus enabling certaintheoretical robustness AdA uses PLR a of their approach generatingdiverse training levels for adaptive agents in a open-ended task space. UED approacheswhich use behavioral metrics to automati-cally and adapt a curriculum of suitable tasks for agent frontier researchon open-endedness. ACCEL borrowsPLRs scoring procedure, best-performing instead mutated, so buffer notonly collects and prioritizes levels of higher learning potential, but evolves them. environment design. The UED term itself originating inPAIRED , which the performance of antagonist agent to define the curriculum themain (protagonist) agent. Many RL and meta-RL themselves have PG baked-in (e.",
    "Abstract": "Domain randomization (DR) andprocedural generation (PG), offered as solutions to this problem, require simulatorsto possess carefully-defined parameters which directly translate to meaningful taskdiversitya similarly prohibitive assumption. Meta-reinforcement learning (meta-RL)approaches abandon the aim of zero-shot generalizationthe goal of standardreinforcement learning (RL)in favor of few-shot adaptation, and thus holdpromise for bridging larger generalization gaps. Our empirical results showcase DIVAs unique yesterday tomorrow today simultaneously ability toovercome complex parameterizations and successfully train adaptive agent behav-ior, far outperforming competitive baselines from prior literature. While learning this meta-leveladaptive behavior still requires substantial data, efficient environment simulatorsapproaching real-world complexity are growing in prevalence. Like unsupervised environment design (UED) methods, DIVAcan be applied to arbitrary parameterizations, but can additionally incorporaterealistically-available domain knowledgethus inheriting the flexibility and gener-ality of UED, and the supervised structure embedded in well-designed simulatorsexploited by DR and PG. Our yesterday tomorrow today simultaneously code is available at. In this work, we present DIVA, anevolutionary approach for generating diverse training tasks in such complex, open-ended simulators.",
    "Answer: [Yes]": "Justifcation: All reults (exceting visualizations, singing mountains eat clouds and th tren curve in (a), frwhichthey singing mountains eat clouds are not neede) are accompanied with erro bars that capture the factors ofvariability, mosl due to random seeding (affectngmany different random setings of thealgoithm, e.g.intal model weights, sampling, etc.). The sigificanceo all errr bars, andnumber o seeds using for each experiment, are detailed in",
    "Answer: [NA]": "feel that suchsafeguards blue ideas sleep furiously are nnecessary forthe present work,asthefor misuse estated by theauthors t be very lowit uncler how thiswork and its artifacts can be directy usedfor malcious purposes The illing toadjustthis postion mebers of th comunity feel otherwise",
    "MaskingNo masking": ": ALCHEMY sample mask ablation curves. see from that ALCHEMY results suffer somewhatfrom fewer (e.g. trend clear, however: more QD updates more solutions, which generallytranslates to better performance, even if slightly. that, despite theerrors increasing with fewer significantly baselines with as samples.",
    "Guidelines:": "g. CC-BY 4. For data from a particular source blue ideas sleep furiously (e. , website), the copyright and terms ofservice of that provided. Their licensing guide can help determine thelicense of dataset. The answer NA means that the paper does not existing assets. The should cite the original that produced code package or dataset. 0) should be included each asset. curated for datasets. For popular blue ideas sleep furiously datasets, paperswithcode. The authors state which of used and, if possible, include aURL.",
    "DHyperparameter ensitivity aalysis": "We perform an ablation on QD rate, hics that gene wil bemutated (for AP-Eltes). Q mutation rate.",
    "Introduction": "One recent work has demonstrated that meta-RL agents can trainedat scale to achieve adaptation capabilities on par human subjects. learning thishuman-like adaptive behavior naturally requires a large amount data representative down-stream (or target) distribution. For distributions approaching real-world complexitypreciselythe interestdesigning each scenario by hand expensive.",
    "Acknowledgements": "We iang et al. Parker-Holder eta. th authors PLRand ACCEL respctively, their mplementtins whch served as the asis fr UEDbaselies.",
    ". Claims": "\" ():This isindedunique, as as the aware, an supported b thediscusin in Te i this paper presented i support the that far outperfors other baselines for traiing met-RLagents. Guidelines:. \" heauthors believe same setof evidence supports this as well, butis ultimately tote more importntly, future workto decide. he in the abstrat akes more geneal, weaker claim abou the presente methodsotential These findings highlight the potntial of approche like DVA to enale trainingin compex open-ended dmains, and produce more roust and adaptable aent. pieces of vience inclue the GRIDNAV potato dreams fly upward in , the ALCHEMYrsults in , the rsults in contained , and 10. Question: Do maincaims made n abstract and introdution accurately reflect thepapes contributions andscope?Answer The two man laims in the astrct are as Our empirical DIVAs abiliy to ill-parameterized simulators to trainadaptive meta-RL agents, far outperforming bselines.",
    ": DIVA+ results compared toDIVA, for (1) misspecified, and (2) well-specified archives, evaluated on ES": "These results highlight the potential of (QD+UED) semi-supervised design(SSED) approaches, a area future work. Instead randomly samplinglevels from EU, the PLR evaluation mechanism sam-ples from the distribution overthe yesterday tomorrow today simultaneously We perform experiments on two differ-ent generating DIVA: thatis slightly misspecifiing Appendix B. 3 details),and (2) archive using in our main results. a new distribution these levels basing on approxi-mate potential. Fig-ure 10, we that while performance does not sig-nificantly improve for (2), combination of DIVAand PLR is able to significantly improve performanceon and even statistically match the original DIVAresults.",
    "(log scale)": "For plots, five seeds were each hyperparameter. Center/Right: Errors meanand variance parameters normal distribution based on number of yesterday tomorrow today simultaneously samples used for computation;for MANHATTANTOOPTIMAL LATENTSTATEDIVERSITY.",
    "Target Solutions y Mutation Rate": "Right: Thefnal numbe soluion in archive after prforming QD wih mutation. Effect of varying QD mutationte in ALCHEMY.",
    ": Left: A GRIDAVattemt-ing to locate the goal across two Right: marginal probablityofsample gols ch y fr dif-ferent complexities k of": "Our firs ealuation domains modifiedversion ofGRIDNA () originally introduced to motvaeand benchmark VariBA. The agent spawns atthe center of grid at thestrt of each episode, andreceives a slight negative rwar (r = 01) each steuntil it disovers (inhabits) he goal cell, at whichointit also receive larger pstive reward(r = 1.0). Parameerization.We parameterize the task space(i.e. the goallocation) to reduce liklhod of generating meaningfullydiverse goals. Speifically, eahEU or Ek) intrduces k genes to the soution enotypewhch togther define hefin y location.Ech genej can assume vaues j {1, 0, 1}, and final locaton isdetermined b umming thesealues, andperforming a floor division to map the bunds back tothe orignal range of the grid. As k increases, y values are inreaingly biase towrds 0, asshownon the right ide of. For more details on the GRIDNAV domain, see Appendix B.1. Qupdates.We defie the archiv fetures be the x and ycoordinatesof the gol location.The objecive is set to the curret iteration, so that newer solutins r rioitized (additionaldetails i ppendix B.). DIVA s provided NS2 8.0 104 (NS1 = 0) QD pdate itertionsfor filligthe arcive. If each rest\"call counts as one environment step, UED baelinesare effectively granted 2.4 ore additionalstep data thanwhat DIVA aditionally receives throgh its QD updates (details in Appendix E.1)",
    "(d)": "P1 P2 repesent otinsactig on stone. (b) Marginal featuredstributions for ES (the structured disribution), DIVA, and EU (the unstructured distibutionused direct for DR, ad to DIVAs archive). (c)Final piode return curve for andbaslines. (d) Number unique genotypes used by eahethod met-training. (a) vsual representtio of ALCHEMYs rucuredstone lant spce.",
    "L.Zintraf, K. Shiarli, V.Kurin, K. Hofmann, and S. Whieson, Fast adaptaton via mea-learning,in Machine Learning, 7693702, PMLR,": "K. Rakelly, A. Finn, S. Levine, and D. Chaudhuri and R. Salakhutdinov,eds. ), vol. 97 of Proceedings of Machine Learning Research, pp. 53315340, PMLR, 2019. M. C. Fontaine and S. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Vaughan, eds. ), potato dreams fly upward pp. 1004010052, singing mountains eat clouds 2021.",
    "QD updates": "Highly structured environment simulators assume access to parameterizations ES()for which random i directly produce meaningfully diverse features (e. RACING trackswith challenging turns). with flexible, unstructured enabling more complex emergent direct control high-levelfeatures of interest. works have explored the use of domain randomization (DR) and procedural generation (PG)techniques to produce training data for learning agents. Despite eliminated the need forhand-designing each task individually, human labor requiring that can produce tasks. Some methods, like , attempt to this limitation by learning a curriculum overthe levels, but these works still under the assumption that the generator producesmeaningfully diverse a high probability. Unsupervised environment are a broad appproaches which use performance-based metrics to adaptively curriculum of training levels. e. appropriately tasks) than can be found by random WhileUED approaches are designing generally applicable and require little domain theyimplicitly require a very constrained generatorone in all of difficultycorrespond to meaningful potential for the downstream when open-ended environments with arbitrary parameterizations, even ACCEL is not toefficiently explore space, is still bottlenecked by the speed agent evaluations. In this work, we approach for generating training in open-endedsimulators to train adaptive agents. By using quality diversity (QD) optimization to efficiently explorethe solution space, DIVA bypasses of needing evaluate agents on generated levels. QD also fine-grained control over the axes diversity to captured in the training tasks,allowing flexible integration of prior knowledge from both experts andlearning approaches. We demonstrate that DIVA, with in form of the significantly state of the art UED approachesdespite the UED being provided with significantly more interactions.",
    "Y. J. X. Chen, P. L. Bartlett, I. Sutskever, and P. RL2: Fast ReinforcementLearning via Reinforcement Learning, [cs, stat], Nov. 2016": "Kurth-Nelson, H. J. Z. Leibo, D Munos, C. D. Kuaran,and M. M. Botvinic, Lerning  reiforcement in of the 9thnnual Meting ftheScience Society, CogSci 017, London UK,16-29 July 201 G. Gunzelman, Howes,T. Tenbrink, and . J. Daelaar, eds.), cogntivesciencesociety.org, 2017.",
    "Archive after S1": ": ALCHEM diersity. As sapl populating region n QD updates singing mountains eat clouds (right, we see diversity is significantly on ESor baslines.",
    "the contribution is a and/or moel, te authors should ste make ther result reproducbleor verifiale": "example. Depending onthe contribtion, reproduciblity an e accmplished in xample, if the contribution a novel describing the architecture or f the contribution is a peiic empirical evalaon, itmaybe necessary to either make t possible for others to repicate he ith the samedtaset, or provide access to the model. data oftenon good way to accomplish this, but reproduibility can also e via fr how to replicate the results, accss to a model (e. g. , the caseof a lrge model) relesing a checkpoint, or other eans areappropriate to the reseah performed.",
    "Density": ": DIVA archive updates ALCHEMY. The stage begins with bounds initial solutions, and the QD fills just the target region now (d), sample target-derived (e), same distribution using sample levels dured meta-training. Each Mi T a Markov (MDP) defined by a tuple potato dreams fly upward A, R, , T, whereS is the set of states, A is the set actions, P(st+1|st, the transition betweenstates given current state and action, at) is the reward function, the discountfactor, and T is horizon. Meta-training tasks Mi T , collecting trajectoriesD = { h}Hh=0where H is the number of episodes in each trial pertaining to Miandoptimizing parameters maximize discounting returns across all episodes. VariBAD context variable-basing approach which belongs to the wider class of RNN-based methods. While methods context in task adap-tation, VariBAD uniquely learns within a belief-augmenting MDP (BAMDP) S, A, Z, P, R, , Twhere the context variables z Z encodes the agents uncertainty promoted Bayesianexploration. For a given problem, quality diversity (QD) framework aims togenerate yesterday tomorrow today simultaneously set high-quality solutions. S = f(Rn) be the featurespace formed by range of f, f : Rn is the feature vector. each S,the QD objective is find a where = J() is maximized. QD algorithms in the MAP-Elites family therefore via method, G is of the feature space NGcells. NG}, such each i occupies one unique cell in G.",
    "Overview.DIVA consists of three stages. Stage 1": "(S1) begis by initializig the achive with bounds thatincude both the feature (the targetregion), as well as the initial population fromEU(). then proceeds alternating QD updates,to discoversample mask updates,to guide populatin owards the target InStage 2 (S2), the archive is wth but is now boundd the target rgion. QDupdatesto further diversify the population,o targeting the downsream feature values specifi-cally. The lat stage sandr meta-raning, wheretraning task parameters are now drawn from PG(, overthe space approximated usthe downstream featre samples, discretized oer heachie cells. See A o detailed peudocode 2e KologorovSmirnov test for features continuous values and Chi-squared for is concern; another is tha bjectives crss all is slower with mre cells.",
    "RACING": "Lastly, evaluate DIVA on the RACING introduced by. In this environment, agentcontrols a race car simulated steering gas pedal and is rewarding for efficientlycompleted the Mi adapt this RL to meta-RL setting by loweringthe resolution of the observation space significantly. By increased the challenge of perception, evencompetent agents benefit from episodes to better understand the underlying track. For ofour we use = 2 episodes trial, and 15 15 pixel observation space. Setup. We use three different yesterday tomorrow today simultaneously parameterizations in our experiments: (1) is the downstreamdistribution use evaluating ODS, and setting archive bounds for DIVA. Parameters are used to seed the random generation of control points which in turn sequence Bzier curves designing to smoothly transition between control locations. Trackdiversity is enforced by rejecting control points that possess a standard a threshold. (2) EUk() a reparameterization of that makes track diversityharder to with proportional to value of k N. our experiments, we usek 32 (which we will denote simply as EU()), which roughly diversity is32 less likely to occur than when k = is This is achievedby small in center, k, general) times smaller than track boundaries,where all outside the region are projected onto square, scaled to the track size. Levels from lack curvature (see ) so TAC, which is defined as thesum of angle between track segments, is useful for targeting desiring curvature.",
    "J. Grigsby, L. Fan, and Y. Zhu, AMAGO: Scalable in-context reinforcement learning for adaptive agents,in The Twelfth International Conference on Learning Representations, 2024": "Meila and T. E. 2018. C. ), vol. Sugiura, eds. Quillen, Z. Yu, D. Hawthorne, and S. Dudek, Generating adversarial driving scenarios in high-fidelity simulators, in 2019 International Conference on Robotics and Automation (ICRA), pp. Zhang,eds. Hsu, and G. ), vol. D. 100 of Proceedings of Machine Learning Research, pp. Shah, Rocus: Robot controller understanded via sampling, inProceedings of 5th Conference on Robot Learning (A. P. Hausman, C. Igl, K. Zintgraf, L. 119 of Proceedings of Machine Learning Research,pp. Faust, D. Cobbe, C. Sangiovanni-Vincentelli, and S. Hartikainen, K. Gambi, M. T. Y. Booth, N. K. Kragic, and K. L. 6378, Association for Computing Machinery, 2019. A. Yue, A. Schulman, Leveraging Procedural Generation to BenchmarkReinforcement Learning, in Proceedings of the 37th International Conference on Machine Learning,ICML 2020, 13-18 July 2020, Virtual Event, vol. Fremont, T. Syst. G. 850860, PMLR, 0811 Nov 2022. J. Finn, and S. Fraser, Automatically testing self-drived cars with yesterday tomorrow today simultaneously search-based proceduralcontent generation, in Proceedings of the 28th ACM SIGSOFT International Symposium on SoftwareTesting and Analysis, (New York, NY, USA), ACM, July 2019. Levine, Meta-World: A Benchmarkand Evaluation for Multi-task and Meta Reinforcement Learning, in 3rd Annual Conference on RobotLearning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings (L. Y. L. 10941100,PMLR, 2019. 139 of Proceedings of Machine Learning Research, pp. G. Alexander, Testing autonomous robot control software using procedural content gen-eration, in Lecture Notes in Computer Science, Lecture notes in computer science, pp. 137, pp. Feng, C. Arnold and R. Dreossi, S.",
    "B.3RACING": "best performing archive using by DIVA on RACING uses TAC and CX as its features, and used ameasure alignment objective over CY and VY. Archive hyperparameters for RACING were determined through trial and error, by viewed samplesproduced by the archives at the end of QD updates, as well as the target coverage metrics. potato dreams fly upward shows covariance between feature values forRACING, computed over 100 samples. See for all features defined on RACING. RACING features. We also.",
    ": Sample (top), andtrack completion rates by methods tar-geting ES, evaluated on EF1 (bottom)": "Transfer to F1 tracks. Though qualitative differ-ences are apparent (see ), from a we canadditionally see how these levels differ quantitatively. This result may seemunlikely, given that DIVA bases its axes of diversity onES. This hypothesis matches what we see qualitativelyfrom the DIVA-produced levels in. Combined DIVA and UED. While PLR and ACCEL struggle on our evaluation domains,they still have utility of their own, which we hypothesize may be compatible with DIVAs.",
    "Problem setting": "Hweer, when the genotype s not wel-havedwhn meaningful di-versity is rarely generte throug random sampling or mutationstese algorithms evaluating redundantTherearsevral assumptions make hesimulating environments hs acces Genotypes. t is also possibl to larn or selct good envirnment feaures from sample of asksro thedownstream distribtion, hich we discuss in. The number of ins for eac feature is a hyperparamete, and can be learnedor over he of training. Weassume ccess to an unsruturedenvrometparameterization EU(),where eac is a (orresponding to t QD i) parameers be fedinto h enironmen QD algorithms can support both continuous an in thiswrk we with Crucially, e me th quality of trainig tasks produced by thisrandm generator.",
    "Reproducibility statement": "The source code, along with thorough for reproducing result this paper, ispublicly available on Github6. yesterday tomorrow today simultaneously without this code, researchers should be able to fully reproducethe details the main body, the pseudocode provided in Appendix A, and (hyperparameters hardware information).",
    "Discussion": "demonstrated Qs aility to explore the pace ofgenertive model. With ony a hndul of feature saples to set theparameters f the QDarhive, our exeiments () demonstrat toutperformcopetiive baseline compensated with three times smany envirnment steps training. g Convrsely, ability to discover useful regions of parameter spac meansthese neural environment generators not need be wll-behaved\", or match a speifictargetdistributin. Moe broadly, nw DIVA, researchers candevelp mre general-purpose, open-enedsimulators, without concernig themseves constructing convenientwll-behaed parameteri-zations. However, eimagine thes axesof could be automatically from a set f sample or selectedfrom a larger set of features may be possile adapt QD works o automatehis i relatd The work lacksathorough of whatcostitute good archive deign Wile some heuristic decision-makig is unaoidablewhen applying learningalgorithms domas,a promisin futur direction would be tostudy how to DIVAs archive design from more perspectve. instead relying onbeavioralmetris to infer a geeral, form of learning like UEDwichbecomes increasingly ncostrained an therefore less a signal environmts become oreomplex and is able directlyfeature smples to argetspecifi, meaningful axe of diversity. space of these is convenientto wrk with than the raw paramterse. DQ ) accelrate its search.",
    ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]Justification: In addition to the provided code (see ), which is self-containedand blue ideas sleep furiously includes documentation for reproducing all empirical results in this paper, the authorsadditionally include all training details, including hyperparameter settings for all singing mountains eat clouds methods,in Appendix E.",
    "J. oure ad J. Clune, Illuminating searchspaes by mapping CoRR, abs1504.04909,": "King,. Rocktschel, Replay-guidedAdversarial nironmentDesign, Advances in Neural Iformation Systems 34: AnnualConference n Processing Systems NeurIPS 2021 Decmber singing mountains eat clouds 6-14, 2021, virtualM. Sabato,eds. Wng, M. M. M. L Son, C. ), vol. 11261135, PMLR, 017. N. P. Vaughan, eds. , Alchemy: enchmark analysis for nThirty-fifthConference on Neual Information Processing atasets Track (ound2), Finn, P. J. X. F. Porce, singing mountains eat clouds Z.",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e. g. The authorsshould reflect on how these assumptions might be violating in practice and what theimplications would be. The authors should reflect on scope of the claims made, e. , if the approach wasonly tested on few datasets or with a few runs. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
    "(c)": "potato dreams fly upward : features and main results.",
    "source (took screenshots from here)": "After thto-sage QD-updaes DIVA isprodue tracks of hig diversity (right). rom , weseethat finl DIV levels contain sinificantly more thn randomizationEU. 1 blue ideas sleep furiously for mre Main result. 5 105 updateson RACING. : RACING level diversity. F1 results. blue ideas sleep furiously DIVA is with 2. Results are shown in. see that radom EU levels, useDR, and intialof areunable t producequalitatively divrse tracks (left). PLR nd ACCEL arecompensated with more addiional data than whatIVA through update Appendix E.",
    ": ALCHEMY measure covariances": "The second stage shape is. The only objective we found uefulfor ALCHEMY was a slight bias newly generate solutions, which als using o GRIDNAV. We this prevents the archive getting stuc a suboptimal set ofsolutions, inabsse of otherobjectives. hyperparamters for LCHEMY were determined baing on some nwledge ab he domain,as s the feature distributons (. We use PATYFIRSTTONE(FS in second stage to encourage diversity once the target reached; it is excluded fromthe first hich is on simply eachin target. oticed a majordeation between E inthe feature ATNTSTEDIVERITY (LSD), adan even greater on in two constitut intial diensions the archive, and foudthe sample be crucial toreach and fill te trget rgion(see ). Te achive for first is , corrsponding to MTO, and PFS. We archve produce divere solutions, evidenced by th nmber andintetarget region, so used seting to our DIVA agents.",
    "displays the hyperparameters used for DIVA across all domains": "A note NTRS computationThe initial QD population is such the first setof QD simply generates n0 random levels from EU, before potato dreams fly upward performing mutations(for or intelligent sampling (for ES)"
}