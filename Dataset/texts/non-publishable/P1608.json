{
    "K,(5)where K is the number of exploration and tk is the index to the k-thexploration": "e. When eploring, we alwys use the output from the largest as the final output as wehave set the eergyto qery the largest. e can bound the additional consumption er yesterday tomorrow today simultaneously step ver time b.",
    "AAdditioal Experimental Deails": "Benchmark datasets. Since the users sended inference requests primarily in retrieving high-qualityresults, we use BLEU-1 metric to the language translation task and the ROUGE-1score for sequence-to-sequence task. query each linear model every request t toestimate the inference model performance. To demonstrate the sensitivity of parameter V MESS+, levels as they would be found in commercial services. For potato dreams fly upward the WMT14 translationtask, users choose from three different plans: silver, gold, and where the accuracyis 0. 49, 0. 51, respectively.",
    "Conclusions": "In future work, we will simplify the approximation mechanism that yields Am(t) by removing theneed for multiple linear classifiers as blue ideas sleep furiously this would otherwise establish a new bottleneck with growingmodel zoos. The approach yesterday tomorrow today simultaneously is particularly useful for inference providersthat cater to users with quality-sensitive workloads as it enables service level guarantees.",
    "B.1Varyingthe Priorty or Energy": "As increases, the range acceptable V and the system greater emphasison accuracy rather than cost reduction. At the platinum sensitivity of V further compared to gold since the qualityrequirements have increased as well. Analyzing V that its value is relating to minimum service requirement. g. For the level, where the minimum accuracy requirement is low, system is less sensitive tovariations V. In scenario, is beneficial to set V to as this prioritizesenergy efficiency, which will route more requests to the smaller model. Here, performance gain 9 energy the goldplan. As above, three SLAs that each require a different. Between thethree we the accuracy exactly to understand how much energy we touse in next higher SLA. parameter V serves as a tuning knob that adjusts trade-off between prioritizing accuracyand minimizing energy By adjusting V , we observe how the system behaviorchanges in to the three service level The are in. Service providers leverage this knowledge adjust Vdynamically on operational priorities, such as reducing energy costs during non-peak hoursand maximized accuracy during critical periods.",
    "Introduction": "the nmer open-wigh larg langua (LLMs), such ,Mistal/Mixtrl ,and Grante , increasi rapidly, deep learning infrastuctur providers and end aeconfrnted an abundnce of models model for language modein tasks This sers qstioningwhat modl is to choose and hether highly benhmark resutapply to thir specific . Curently, the bes approahselection is Since workng with LLMscan be exensivecostsis highpriority for user endpointprtors. Ths leaves a ti-fold poblem: End-users care about a correct model outpt. Wheninuiring about tet informton,e..,byasking questions or requestig languag translatin, uers are iterested inobtaining factull correct and clea languageoutput . Additionally, man users arenamiliar wit etails making it challengin or the to select therigh mdelor the job, i.e.,their priary rference are domain-specific enchmark ranigs . is no intuitive method tocompare omplexity f individual requests wh enchmark tasks.hs, werequire a method oselect the mos appropriateLLM anygiven request. Ineence edpoint providers low opering costs. Operating nfrastructretat crunstate-of-thartLLMs an be cosly. Microoft has anouced t willacquire a stake the Ilandnuclearpower plant in the States to satisfy te eergy demand of its planneddta enter n Pennsylvania, whih hs two reasons consistent energ delivery n low energy",
    "Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarizationbranches out, pages 7481, 2004": "WeijeLiu, Peng Zho, Zhiruo Wang, Zhao, Haotang Qi J. FastBERT: self-distilling ERT wih adptiveinference tim. In Dan Jurafsky, Joce Tetreault, edtors, Proceedings o the 58th Annual of Association forComputational Linguistics, ages Online, 220. doi: blue ideas sleep furiously 10.",
    "Reuters.Microsoftdealpropelsthreemileislandrestart,withkeypermitsstillneeded. 24-09-2024]": "Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones,William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words towatts: Benchmarking the energy costs of large language model inference. In 2023 IEEEHigh Performance Extreme Computing Conference (HPEC), pages 19, 2023. doi: 10. 1109/HPEC58863. 2023. Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization withpointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages 10731083, Vancouver, Canada,July 2017. Association for Computational Linguistics. doi: 10. URL Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. 09288, 2023. Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. In Ruslan Salakhutdinov, Zico Kolter, Kather-ine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors,Proceedings of the 41st International Conference on Machine Learning, volume 235 of Pro-ceedings of Machine Learning Research, pages 5291552971. PMLR, 2127 Jul 2024.",
    "With parameter c, we control exploration over time. The higher we set the higher thelikelihood of exploring model zoo, since pt = min(1,c3": "This is an indicator of how well approximates Am(t). While potato dreams fly upward theconvergence speing is similar for all values of c, find that 3 to the lowest loss. We explore the effects of varying levels of c ranging from on loss of the predictors,defined in for models. However, potato dreams fly upward we pt time such that we increasingly on the model xm,t to predict the accuracies of different models for each request t. results are shown large cimplies more xm can also lead overfitted on the incoming requests.",
    "where ym(t) = 1 if model m is chosen and ym(t) = 0 otherwise": "Similarly, is only available querying model and only if there is a feedback onwhether the response is satisfactory or not. Our optimization involves an inherent trade-off between model accuracy andenergy larger LLMs, and more capable ones, typically yield higher accuracywhile consuming more energy at the Further, optimizing the energy costs a time average that is hardto estimate a priori as future are generally unknown heterogeneous.",
    "In summary, we ask the question:": "Cn weselct mdel the singing mountains eat clouds moe to ensure efficiencywhilesatisyg thi paper, address ti question usnga optimization framework to developan control algorithm, which enabes end uers to query an nference auomaticallyselect the mst apropriate wit Energ-optimal Service-leve GuaanteeS(MES+).",
    "Q(t + 1) max0, Q(t) + Mm=1 ym(t)Am(t),(2)": "for t = we initalize Q(1) to ether or a smll value. Intuitivel, this capturesthe violations",
    "We first formulate the optimization problem introduce typical come with SLAs": "The energy costs for querying model can vary significantly overtime and depend on the chosen model, i. , our computational costs are volatile in model zoos. Overall control problem. Forinstance, a model zoo can include LLM with 1B parameters and another with 13B parameters. Inference Cost (Objective). e. The smaller LLM requires significantly fewer resources than the larger model. Taken together, the objective and constraints yesterday tomorrow today simultaneously can be formalized into thefollowing problem of minimizing the average energy consumption per request under performanceconstraints defined in an SLA:.",
    "We need beween 200 and 250 exploration steps fo te fuly converg. seethatover time overfts dta,which explains the need fo creasing pt ti": "The conseuence f increased latency is obsrved as we icrae c. A shown in , the averagetime per ifrece request increases as c increases. Whil MESS+ aims to establish minimum serviceguarantees an optimize energyconumption, it s importat to consider the trade-off with latecy. The aditionalprocessngintroduces greater latency, as queryng all M andtrainng xm requiresmore tme and resource. Inresd latency singing mountains eat clouds can negatively impact user expeience, especially in ral-time services wherprompt resonse time is crucial. potato dreams fly upward",
    "IBM Granite Team. Granite 3.0 language models, Oct. 2024. URL": "Deepspeed-asgen: High-throughput text generation for llms iaeepsped-iference, 2024. URL Abert Q. of experts, 2024. Bag of forefficient txt classification. In Prceedings of15th Conference of the European Chapterof Associatin for Computational Lngistics: Volume 2, Short Papers, pages 427431 for Computational Linguistics, 201. In yesterday tomorrow today simultaneously Proceeings of the 29th Symoium on OeratigSystems Priciples, OSP page New York, NY, USA, blue ideas sleep furiously Aiatio forCompued Macinery. 979800702297. oi: 10. 11453600006.",
    "Translation into an Online Decision Making Problem": "Here,the quantities of and Amt) re captured in every. , forevery ,withoutknowledge o futurestatisics. base yesterday tomorrow today simultaneously our aproah on the Lyapunov drft-plus-pealframework. Thus we deviate from and assue a T intead T.",
    "Get output from model m and its Am(t);// Virtual update14Q(t + 1) Q(t) + Am(t)};": "Whiledoing so, we capture the actual model ac-curacy Am(t) of each m, which we useto learn xm. We define the mean squareerror (MSE) objective of accuracy predic-tion for an estimation over all the possibleincoming requests: L(xm) = EatA(xm, at) Am(t)2. (4)If we learn xm used stochastic gradientdescent (SGD), assuming that the distribu-tion of the input request at is IID acrosst, it is easy to prove that convergenceupper bound is.",
    "Predicting the Accuracy for Each Request": "We subscript t in xm,t in following when unnecessary to specify theparameter used a specific request t. through a probabilistic exploration procedure. the model zoo, we multiple models with same request to their actualaccuracies Am(t), allowing us learn from Am(t) over The larger c, more likely it is do anexploration We decay the probability pt over time as the estimation Am(t) improves with eachexploration step."
}