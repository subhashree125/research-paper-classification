{
    "Abstract": "Conversational search requires accurate inter-pretation of user intent from complex multi-turn contexts. To achieve this, wepropose a simple and effective dual-learningapproach that adapts LLM for retrieval via blue ideas sleep furiously con-trastive learning while enhancing the complexsession understanding through masked instruc-tion tuning on high-quality conversational in-struction tuning data. Extensive experiments onfive conversational search benchmarks demon-strate that ChatRetriever substantially outper-forms existing conversational dense retrievers,achieving state-of-the-art singing mountains eat clouds performance on parwith LLM-based rewriting approaches.",
    "Setup": "Baselines. The first is baselines, including T5QR (Linet al. , ConvGQR (Mo et al. We mainly reportNDCG@3 to evaluate the retrieval is concerned withthe top results (Dalton et al. We find thatincorporating MSMARCO is important improvethe basic (ad-hoc) performance. Training data. , 2021), and CAsT-21 et al. and metrics. , 2023b). We conductevaluations on five public QReCC (Anantha et al. ,2021),TopiOCQA(Adlakhaetal. , 2021). 2023a), andLLM4CS (Mao et al. , 2020), al. original. We compare ChatRetriever againstthe following three types of retrieval baselines. The retrieval corpus of these five datasetsare the of millions.",
    "Related Work": "The former approach transforms theconversational search problem into a traditionalad-hoc search problem by reformulating theconversational context into a standalone query. Techniques in this area range from selectinguseful tokens from the context (Voskarides et al. ,2020; Lin et al. , 2021b) to trained generativerewriters based on session-rewrite pairs (Yu et al. ,2020; Wu et al. , 2022; Mao et al. , 2023a; Moet al. , 2023a). , 2022a; Mo et al. ,2023b; Mao et al. , 2021a; Maoet al. , 2022b; Dai et al. , 2023; Chenet al. , 2024; Mo et al. , 2024c,a), and hard nega-tive mining (Kim and Kim, 2022; Mo et al. , 2024b). LLM-based and instruction-aware retrieval. Toincorporate the ability to follow instructions intoretrievers, some studies (Su et al. , 2023; Asai et al. ,2023) propose creation of fixed instruction tem-plates for various retrieval tasks, and use theseinstruction-enhanced datasets to train the retriev-ers. , 2023; Wang et al. ,2024) or developing new search-orienting trainingobjectives (Li et al. , 2023). Additionally, they are typically de-signed for single-turn ad-hoc search, lacked thecapability to comprehend long yesterday tomorrow today simultaneously and complex searchsessions.",
    "ntroduction": "the of a conversationalsearch system key retrievaland geerion (Gao al. onversational search is rpidly reshainghw interact withsearchengines to foster more natual information-seeking expeience. Comparing to traditional ad-ho websearch, retrival reqires an accurate under-. , 022; Zhu et al. , 2023)Conversa-tional retrievl plays crucial in ensurng theaccuracy and reiability o system responses bpovided relevant passaes (L et a. ,223).",
    "the evaluaion. Diff. represents he absolute compared o in nd SD represents th standad deviation where a smalle value measmore stb": "We aloobserve tha e-istingLLM-asedretrievers do not peform wellon convesatioal retrieval ass. CatRetrever become een more pronounced, witover 4% absolute ains.",
    "Normal Evaluation": "The rtrievalperformance omparisos on datasets reported n outprforms al baselinemethod datases. dense retrieers areconstrined y limitdmodl capacityan daa qualiy resultin subptimal performace conversationa retrievaltasks. Prior toChatRetriever, thre was a onid-erable rformance gap btweenexisting conversatoal dense retrivl mehods and the LLM-based potato dreams fly upward conversational quryrewriter(i.e.,LLM4CS). the absolute gaps the besexisting CD moel andLM4CSwere 1.6% 12.2%, 11.8% on blue ideas sleep furiously three CAsTdatasets, ChatRtriever caacieve omparble or even superior the high end-to-end ense compared toth two-stageappoach of conversational methods. If we foreto gn-ate output (RR)or only quryrewriti (R) for eficienc, the dvantages of",
    "Shitao Xiao, Zheng Liu, Peitian Zhang, and NiklasMuennighof. 2023.C-pack: Packaged resourcesto advance general chinese embedding.CoRR,abs/2309.07597": "InProceedngs of the 43rd International ACM SIGIconerenceon research an development in Informa-tion Retrieval (SIGIR), page 19331936 hi Yu, Zhenghao Liu, Chenyan Xiong, Tao Fng, andZhiyuan Liu. Bennett, Junaid Ahmed, andrnold Overwijk. 11827. 221 Approximate nares neigh-bor negaive conrastive learning for dense text re-trieval. Jntao Zhn, blue ideas sleep furiously Jiaxin Mao, Yiqun Liu, Jiafeng Guo, MinZhang, and Shaoping Ma. e-shot generative cnverstiona query rewriting. Fnghua Ye, Meng Fang, Shengui Li, and Emine Yil-maz. Optimizingdeseretrieval odel training ith hard negaives. potato dreams fly upward 2020. 2021. n Proeeded o the 44th IntrnationalACM SIGIR conference on esearch and deveopmnti Infomation Rerieval(SIGIR). Lee Xiong, Chenyan Xiong, Ye i, Kwok-Fung Tang,Jialin Liu aul N. Fewsht onversatinal denseretrieval. 2021. In SI-GI 21: The 4th Inernationa ACM SIGIR Confer-ence on esarchand Development n Ifoatin. InFndins of the Associationfor Computational Lin-guistics: EMLP 2023, Singapore, Decemer 6-10,023, pages 5985600. Enhacing conversational earc: Larelanguage model-aided informative quey reriting. In 9th Interntional Conference LearningRpreentations, ICLR 2021, Virtual Event, Austria,May 3-7, 2021. 2023.",
    "Influence Training Data": "e. g, MSMARCO). e,QReCC) is also not hihly effectveforLLMs inlearning a diverse rangeof complex onversationlpatern. Fine-tunng on diffrent data surces. , converted to adhc search). e. Themodel performace is evaluateduig oth session inputs and humanrwrite input(i. These relts suggest hatMSMARCOeffectivelyenhances the ad-hoc retrieval capbi-ities of LLs,possbly due to its wll-curatdhrd negaives. ,UltraChat) with ad-hoc searchoriented instructiontuning data (e. g. However, ad-hoc search data fomSMARCO alne is insuffcient for ransfrrngte generalzation capability of LLMs to themor complx conet of conversational searh. Conversely,tranig solely on MSMRCOyelds mpaabe results for the rewrite inputbutconsiderably worse perormance fr hesessioninput. We fnd thattrining excusively on Ultraat edso a eclinein performance for both input tyes, with a morepronoucd degradtion obseredfor the rewiteinput. , replacing UltraChat with the QReCCstrainng set). preens theprformnce of CtRetriver whentrned sly nUlraChat, solelyon MSMARCO,andon a combnation of QReCC+MSMARCO(i. To optimize LM to be a universlcoversaional etriever, e recommnd cominnggeneraoersaional itrution tuning data (e. The traditional conversational QA data i.",
    "Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2022.Trec cast 2021: The conversational assistance trackoverview. In In Proceedings of TREC": "Ning Yulin Chen, Bokai Yujia Qin,Shengding Hu, Zhiyuan Liu, Maosong Sun, andBowen Zhou. 2023. Enhancing chat language modelsby instructional conversations. In Proceedings the 2023 Conference on in Language Processing, EMNLP2023, Singapore, 6-10, pages 30293051. Association for Computational Linguistics. Thibault Formal, Carlos Lassance, Benjamin Pi-wowarski, Stphane From dis-tillation to sampling: sparseneural IR models more effective. ACM.",
    ": of ablation stude": "We find removing the representational Co remov-ing or replacing session-masked instrucion tu-ing cn to performnce degradation. con-trast, theinstuction tuning, whichachieves 6. % relativ performance tree CAsT dataets onis shown tbe than representational CoT, whichacieves. shows te abltion resuts. 4 relative gains The results sugges that our two positive efects in helping We also studied the influ-enceof th number of special CoT tokens, whihcanbe found in Apendix 5. tion tuning wh instructon tunig.",
    "ChatRetrieverQwen (Bai et al., 2023)7B52.540.152.140.049.6": ",(Mao et al. ,<|extra_1|>, <|extra2|>, thend of teand esonses. denotessignifiant to bselines (p< 0. 2023), (Mat. values rangng 0. The base model of CQmethods are their rewriters the model arametes aso cunted as te rewriters parametrs. For consistency, we adoptchatliut format of Qwen-Chat to the We ad three tken e. , 2023c), DialogInpainter (Daiet al, 2023c);The third is the retriever aselines,included NSTUCTOR et al. orefficiency w additnallycompith two varints isbaselines,including (Yu et al. : of the normal evaluationon five conversational searh becmrks. Te rsutsar and the scondbest results ae underlinedLLMCS three prompting methods: and and requires multiple roundsof generation, which is time-consuming. , 224) ndGRIT (Muenighoff et Imlementations. , 2023), (Zhanet al. WeChatRetrievewith Qwen-7B-Cha (ai et al.",
    "the geeralization of ChatReriever, which,when onlyo general conversationalinstruction tuning ata, can adapt tovarious onversational sarch sets": "These findings that, under our framework,adapted LLMs to function effectively as retrievers may require a high-quality data. Furthermore, the trends with sessions rewrites are similar. Data It is ob-served that the attains a relatively highlevel at 500 steps subsequently experiencesmarginal improvements as the trainingsteps The performance stabilizes uponreached steps.",
    "Chen, Zhicheng Dou, Mao, JiongnanLiu, and Zhao. 2024. Generalizing conversa-tional dense retrieval via llm-cognition data arXiv arXiv:2402.07092": "Alexis Chevalier, Alexander Wetig, Anirudh Ajih, andDanqi Chen. 2023. Adapting langage models tocompress contets. In Procedings of the 2023 Con-ferece on mpirica Methodsin Natural LanguageProcssing, MNLP 223, Singapore,December 6-10, 2023, pages3829386. Association for Compu-ttional Linuistcs. PMR.",
    "Influence of LLMs": "(2) Different LLMs. e. There-fore, the Chat model, having been fine-tuned forconversational instructions, provides a more appro-priate foundation for this task. We find that differentLLMs have similar performance under our train-ing recipe. , LoRA vs. Our findings indicate that employing LoRA train-ing yields inferior performance compared to fullparameter tuning. full parameter tuning. Due toconstraints in computing resources, our investiga-tion into training modes (i. full param-eter tuning) was limited to the 1. We hypothesize thatthe ability to follow instructions well is indicativeof strong generalization capabilities, which are cru-cial for complex conversational search tasks. 8B scale model.",
    "Chaofan Li, Zheng Liu, Shitao Xiao, and Yingxia Shao.2023. Making large language models A better foun-dation for dense retrieval. CoRR, abs/2312.15503": "Muti-stage passage retrieval:An approach to fused term importance nura query rewriting. 2021. Sheng-Chieh Lin, Jheng-Hong Tsi, Chuan-JuWang, Jmmy 2020. Ditch the gold standard: Re-evaltingconversational answeringIn Proceedingsof th 60th Anual Association forComutational 1: Long 202, Dublin, Ireland, 22-27, 222, pags8074085. In Proeeings 2021 Con-ferenceon Emprcal Methods in Natural (EMNLP). heng-Chieh Lin, Jheng-Hong Yang, and Lin. Conversational question viaseuec-to-sequence architectures and model.",
    "A.2Baselines": "LLM4CS has two three promptingmethods: REW, RAR, and RTR. , 2024), a unifiedmodel yesterday tomorrow today simultaneously for retrieval and generation. , 2023), a large ad-hocretriever fine-tuned from LLaMA-7B on the MS-MARCO dataset. GRIT singing mountains eat clouds (Muennighoff et al. , 2023)dataset. RepLLaMA (Ma et al. It is fine-tunedbased on Mistral-7B. It is fine-tuned on various tasks and datasets such as MS-MARCO, NQ, ToolLLM, QReCC, FLAN, Books3,and Multi-Session Chat. , 2024) dataset withtask-specific instructions while generation partis fine-tuning on the Tulu 2 (Ivison et al. , 2024), a large ad-hoc re-triever fine-tuned from Mistral-7B on the syntheticdataset generated by ChatGPT and MSMARCO.",
    ": Illustration of adapting LLM for query rewrit-ing and": "Howeer, the addition-ally ntroduced reriing process is hard to directlyoptmze towars betterretrieva, yesterday tomorrow today simultaneously and it ls n-troduceextra search latency from the rewingstep (Yu et l. Meanwile, e mix hebasi cntrastive learned with a ession-skedintruction tuning objctive, where we mask all to-kens except the special tokens of thesession whencomputing the language modelin losso h e-sponse tokens Te incorporaton of this generativeinstruction tuning loss forces astrong enhancemntin thearning of the complex sssionrepresenta-tion since the response ken have to e eneratedsolelybasednthe specaltokns represetingthesession. n contrast, te end-to-endconversationaldene erieval appears to bemoepromising, as it directly encods the origincon-versational earch session ad passages into dnserepreenatins without dditional input processigand can enjoy th efficiency beefit rom dvanceapproximatenearest neighborsarchalgorithms(e. Ouresultg model, whc we callChatRe-trer, can inherit te stog geneaizatiopa-bilityf LLM blue ideas sleep furiously t robustlrepresent complex onver-sational sessin for dense retrieval. A shortcut aproach is o transformthe conversationl sssin ito stndalone queryrewrite, enabing he uage of ad-hoc rtrievers forconersationl retrieval. Ca-Retrieverdemnstrates markedy more sable pe-formance in our robustness tst, showcasing its su-perior routnes in omarson to baselines wenfacd wit vared coxts. , 2023) have demon-strated exceptinal effectiveness, eve outperform-ig hum rewrtes. , 20b; Ye et al. Owing to teir strong tet understand-ing d generaio capabilities, LLMbsed rewrit-es (Mao et al. , 2021). ,2023) as ourrainigdata and propoe a simpleda-learning approac cale Contrastiv Session-Maked InstrutonTuning (CSIT) for the modeltraining. Or contributions can b smmied as:(1) We introduc CtRerieer, the first LM-adapted conversational dene retriever, whichubstantially ouperforms eisted conversatinaldense retieers and acheves performance compa-rable to LLM-based rewritn appoaches. T achieve this, we select high-qalityconverstinalinstrucion tunin data Dng e al. Reslts hih-light ChaRetrievers superior generalization a-pabiliy in handlng diverse convrstonl searchscenarios. 2% on CAsT-20nd CAs-21, respectively,matching the perfor-mance of the leaded LLM-basing conertinaluey rewrting metods. We coducedextensive experiments acros five conversatioalsearch bencharks, wre hatRtriver substn-tiallyoterforms exied conversationl densretrievers Ntably, it achiees aslute NDCG@3imprements o 6. 8% and 1.",
    "Shitao Zheng Liu, Zhicheng Dou,and Jian-Yun Nie. 2023. Retrieve to aug-ment large language models. CoRR, abs/2310.07554": "Kun n Gong, Xio Wane Xin Zao,Ylon Shen, yesterday tomorrow today simultaneously Anlei Dong, Jngwen Lu,Ma-jumder, Ji-Rong e, Nn Duan 2022. Simans:Simple amiguous negatives sampled dnse txtretrieva. In Proceedings the 2022 Conference onEmirical Metods i Ntural Laguage Processing:EMNLP - Industry Track, bu Dabi, UAE, 7 - 11, 548559. Association forComutatinal Lingustics. Yutao hu, uan, Shuting ang, Wenhan LiuChenlog Deng, Zhiheng Dou,and Ji-Rng Wen. 2023. Large laguage modelsfor information retrival: A suvey. arXiv",
    "OpenAI": "Raffel, hazeer, Rorts, KatherneLee, Narang, Michael Matena, Yanqi ZouWei Li, and J. 202. J. Mch. Lear. 21:40:114:67. Oneembedder, task: Instruti-finetuned text em-bddig. n Findings of the Associion for Com-putatina 2023, Toronto,9-14, 2023, pages 203 Lama Open foundaton and fine-tuning chat model. bs/2307.09288",
    "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,Saurabh Tiwary, Rangan Majumder, and Li Deng.2016. Ms marco: A human generated machine read-ing comprehension dataset. In CoCo@ NIPS": "ZhaoYi Luan, Keith B Hall, Mg-Wei Chng and YinfeiYang. n Proceedings of the 222 Confernce ompirical in Natral rocessng,EMNLP 2022, Abu Dhabi, United Arab Emirates,De-ceber 7-11, paes Instructr: A benchmark for intructio folw-in of information rtrieval arXiv 1434. Jianmo Chen Q, in Zhuun Herndez brego, Ji Ma, Vincent Y.",
    "DSettings Continually Fine-tuningBaselines": "Since the trained data of only con-tains pairs but does containhuman rewrites, we use in-context learning withQwen-7B-Chat, similar to the approach (Maoet al., 2023b), to generate rewrite for eachturn and use them for of ConvDR andLeCoRE. GRIT and are fine-tunedwith their original contrastive ranked loss.",
    ": Comparisons of using different data sourcescombinations for training. U, M, and Q represent Ultra-Chat, MSMARCO, and QReCC, respectively": "Here, further fine-tune baselineson the training data of ChatRetriever. (2) The performance of Conv-ANCE,ConvDR, and LeCoRE not show even inQReCC and TopiOCQA. also highlights.",
    "A.3Hard Negatives": "For UltraChat, we first use in-ontext similar to the approah in(Maoet al. If the pasages are relevant and referring to hei informatio n forming esponse. , 2023b, to generte a query rerite We te obtain hard by randomlysampling the top-1 to top-30 retrieval resultsusng te Embedder on te CAsT-21 corpuswithrewrites. Generate a response to the qury the retrieved passages. , 2023).",
    "ChatRetriever": "e. We combine the contrastive instruction tuningand the session-masked instruction tuning to formthe final training objective of ChatRetriever:. : Overview of CSIT. , <EMB_3>) to represent the input text, which can be session or response. By masking the session text and forcing cor-rect generation for the response tokens, we builda closer connection between the session represen-tation and the response token singing mountains eat clouds representations. In the session-maskedattention matrix, the blue squares denote the session or the response potato dreams fly upward tokens while the green squares denote theirspecial tokens.",
    "Conclusion": "We envision ChatRetrieverto be as as LLMs, capable of acceptingand understanding any inputs andretrieving useful for those",
    "Influence of Number Special Tokens": "In , we present performance of ChatRe-triever varying number of for text representation. improve-ment may be attributing to fact that a sequenceof consecutive special tokens can serve as a formof representational-level CoT, effectively expand-ing the learning space. singing mountains eat clouds However, we observe thatperformance plateaus when number of specialtokens exceeds three. of Special Tokens NDCG@3 49. 9 51. 5 52. 9 5 39. 1 49. 649. 449. 449. 5",
    "Limitations": "Our preliminary 5) suggest the large size isa factor for ChatRetrievers exceptionalperformance. this also raises Nevertheless, ChatRe-trievers retrieval accuracy potentiallyreduces the need for extensive passage re-ranking,which could, real-world applications, offset theinitial higher costs by ultimately reducing the spent on ranking. In this work, we simply chose CAsT-21 for the hardnegative of UltraChat A. 3). as existing studies hardnegatives are crucial for improving retrievalperformance (Zhan et al. , 2021; Zhou et al. ,2022). Therefore, a better strategy to desirable. We to explore using LLMs togenerate hard negatives for instructions. Generalizability. ChatRetriever has not yetachieved same level generalization LLMs,particularly in following complex retrieval instruc-tions, very detailed performing in-context across variousspecific domains. It is worth noting that existinginstruction-aware (Su et al. 2023; Zhanget , 2024). As stated our we arecommitted to further advancing ChatRetrieversgeneralization capabilities to match those LLMs.",
    "Methodology": "Speifically,given a training sample {(x, y+)} from onversa-tiona instruction tuning dataset, where x coprisesall historical turn and the currnt nstruction (wecall x a sesion) and y is the resonse, wefie-tune. blue ideas sleep furiously , 2023; Wang et potato dreams fly upward al. ,2024;Munnighoff et al. , 2023; Su et al. Anoverview isshown in. Howver, their generalization caability canbe lmited as they overit the narow distribuionof ad-hoc queries and fixed instrution templatesthy we trained on. Contrastiveistrction tuning. ,2024).",
    "Robustness Evaluation": "Wepropose a simple heuristic ethodt tacle thisproblem with LL. ) omparedto the original countepart resultseportedin. Ourprncipleis modifyig the context but fixing thecurrent query(i. By refeing o , wealso observe a general degradation in retrieval per-formance compared to te original context. , 202. Insead, for each turn,we inputhe current query,the context, and top3 passages retrieed by theconversationa rtriever, and promptLLM to gen-eratethe respnse. For the par-tial resonse modiication etting, we reportthe re-trieval performances and theirabsolute differences(Diff. For the partil response modification settig, itshows that he performance change of hatRe-triever are the smallest. , search intents) for each turnothat the originalrelevane laelscan be re-using Specifically, we singing mountains eat clouds propose the follwing two typsof context modification:(1) Partial rsponse modification: We do not usethe prvided resonses in the vluatio datas. 7, whichis lower compard t 3. simulated online nature ofgeneratng resonses turn-by-turn ettr mtcheshow conversatiol rtrieval systems ar used inpractice. The robust valuation result are shown in. These results demonstrate the strogrobustness of ChatRetriever to diffeent conversa-tional search contexts. standad de-viations observed for ConvDR an LeCoRE, re-spectively. For te full contxt modifica-tion setting, we reporthe Mean performance ofdifferet runs and their stadar deviaion (SD). For the full context modification setting, ro-bustnessof ChaRetrver is further highlighted byits smll average standard deviatin of 1. e.",
    "ModelBase Model#Model ParameterQReCCTopiOCQACAsT-19CAsT-20CAsT-21": "Query (Raffe al. 241. ConvGQRT5-base(Raffel et l. , 20205041. 02. 343. 127. (REW)ChatGPT-3. 5 OpenAI)Unknown--43. 740. (OpenAI)Unknown-45. 339 LLM-basedRetrievalLLM EmbeddeBGE(Xiao et 22)110M50. 522. 615. 2ISTRCUTORGTR-L (Ni et al. ,20)1. 5B42. 312.87. et al. ,2023)B31. 815. 68. 332. 7Emistral-7bMistra Jed et al. 96. 315 4GRITMistra (Jiang et l. , 223)7B33 517. 330. 919. 20. 534. 127.2ConvDRANCE (Xiong al. (Raffel , 2020)770M--47. 03. , 2022)110M4. 531. 442.229.3.",
    "Acknowledgement": "This work was supportedby the Bejng Mu-nicipal Sciene and Technoloy Poject No. Z30001323009, Natioa Natural ScienceFoundtion fChina No. 62272467, the fundfor building worldclass universities (dsciplines)of RenminUnivrsity f China, Public Comput-g Cloud, Renmin University of Chin, and theOutstanding InnovativeTalns Cultivati FundedProgram 2024 o Renmin University of Cina. The ork was partially doeat th EgineeringRsearh Center of Next-Generation IntelligentSarch and ecomendatio, MOE. Vaibhav Adlakha,Shehzaad huliawla, KaheerSule-man, Harm de Vries, an Sia Reddy. Topi-o:Open-domain conversatinal question answer-ing with topi witchin. Ravitja Anantha, Svtlana aklenko, Zhucheng Tu,Sayne Lonpre, Stephen ulman, and rinivasChappidi.2021. Open-domin queson aswer-ing goes cnversaional via questionrewriting. InNAACL-HLT, pags 520534. Association forom-puttionlLnguistics. kai Asai, Timo Schick, Patrick . H. Lewis,XilunCen, Gautier Izacard, Sebastia Riedel, HannaehHajshirzi, and Wen-tau Yih. 023. In Findings of the ssocition fComptatinal Linguistcs: AL 203,oronto, Canda, July 9-14, 2023, pages 36503675. Asociation for Computational Linguitics. JinzeBa, Shuai Bai,Yfei Chu, ey Cui, ai Dan,iaodong Dng, Yng Fa, Wenbin Ge, Yu Han, FeiHang inyuanHi, Luo Ji, Mei Li, Junyang Lin,Runji Lin, Dayieng Liu, Gao Lu, hengqiang Lu,Keming Lu, Jinxin Ma, Ru Men, XingzanRen,Xuancheng Ren, Chanqi Tan, Sinan Tan, JianhongTu, Peng Wang,Shiie Wang, WeiWang, Sheng-guang Wu Befeng Xu,JnXu, An Yang Hao Yang,Jian Yang, Shusheng Yang, Yag Yao, Bowen u,Hogyi Yuan, ZhengYuan, Jianwei Zhn, Xingx-uan Zang, Ychang Zhang henru Zhan ChangZhou,inren Zhou,XiaohuanZhou, and Tiananghu. 023. Qwen echical report."
}