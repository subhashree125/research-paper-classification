{
    "Datasets": "Bth dataset are fomulatedas multiple choic question yesterday tomorrow today simultaneously answring and thestatistics the two atasets re shown in. , (Liu t yesterday tomorrow today simultaneously al. 2022 or evalation, hichare challenging nd widely sed logil rea-soning becmark. In paper, we mainly focus n loica and esoning. andATH (Hendycks et,2021) for.",
    "LLMs for Reasoning": "wth predicng only theina (CoT) (Wei et al. 2022) s more suitable ay for ra-tionale will derive more iformation to avoidpotnial flas. Following this, many promptingtechniques ae proposed to thegneate ra-tionals (Zhou 2023). An-othergrop f wor fouses on search-augmenterasoning, where decoding procs guid byheuriscsearch like MCTS",
    "In this section, we have visualized the predictedstep-wise rewards on the training set of LogiQA-v2, where the solutions are sampled from the SFT": "model. diving intothelogits without we can find tha the rewards stable at aroundthe first 15 seps, henecrease Ths may be by mbal-anced amun of solution with reasoningsteps, which makes reard odel lss confidenton the stps. , rob-abilty (left); (2) (meium); and (3) ofeach step from last layer of reward model(right). On her the acu-mulated probability based rewads lnger reasoning process, which an use-ful avoid redundant solutions by penalizing loner ones.",
    "Formal Definition of Natural LanguageReasoning": "this we on the taskswith annotated labels, where the agent willreceive a positive reward it finally reaches acorrect. FollowingHaoetal. Inthe context of LLMs, simplify the setting byconsidering that both the and state aresampled from the policy model LLM), suchthat:at (a|ct),st+1 (s|at, ct),(1) the model,ct (s0, a0, , st) is the trace.",
    "ABaseline": "Foundational have selectd thestron without tas-specific sbaseln, including (Touront al. Afte that, can ob-tain the smaller with cosiderale rea-sned cpability through supervising fine-uning(SFT). serve as baslines and modelsfor trainin. Specifi-cally,Llama-2-B-chat Gemma-2-Instrct (Gemma Team, 2024) forFT. Outcome-ase OptimitionWeinclude model wth onl supervisonasbaseline dcuss the efeciveness o sn-thesised ocess reward.We also invole IPObaseine. Thetraining dataset is as in. 5. Rejection Samplig-asd ApproachWe alsoinlude the rejection sampling based approaches,i. e. , 2023). Both appraches use oucome annotations fil-ter the self-samld soltios. Besides, RFT, variants: (1) RFT-ouome uses onlyth anntation to filter and & PRM-top-k follows RF-outoeanduses our traned PRM to rank kept ReinforceLeaningIn experients it-erative training, we nclude tw reinforce learn-ing algorithms PPO (Schulman et Besides, for step, th logits byour traindRM is as reward.",
    "EEffect of Different Reward Margins": "In 9, we have involved a hyper-parameter to control confidence interval dif-ferent sample pairs both the correct an-swer to construct the process-supervised prefer-ence dataset. there several aspectsof considering the choices of . with can improve of true pos-itive pairs in constructed highconfidence will also the number of training and probability to include morehard negative samples. example, as shown in, 0.7 introduces only 10% extra pref-erence pairs lead to less significant with the case where Onthe other lower value can include hard negative positive Fromthe table find that = 0.3 has introducedmore than 25% process-supervised pairs, theperformance is worse than the vanilla DPOapproach, where preferencespairs employed. Solve a question answering task by having a then Finish your answer. Thought can reason about the current situation. the and finishes task. You be given context that should use to help you answer the question. Context:A college will continue to implement the funding plan this year. It plans to select of six from Liu, Mr. Zhang, Mr. Wang, Mr. Ma, Mr. Niuand Mr. Zhou to abroad. Due to limitations of the needs discipline development, curriculum arrangement, place time each student's visit, the selection shall following (1) Mr. Liu the reserve discipline of the college, time to send out. (2) we choose Mr. Liu, we should also Mr. Zhou, but we can't choose Mr. Zhang. only if Mr. can't choose, of Mr. Wang and Mr. Ma choose. if don't choose Wang, we don't choose Mr. either.",
    "Estimate ProcessRewards via": "One of main issues with LLMs is that theytend to hallucinate (Huang et al. , 2023). Liu, Mr. Zhang, Mr. Zhou to visit abroad. Liu is the reserve discipline leader of the college, This yesterday tomorrow today simultaneously time we have to send out. (2) if we choose Mr. Liu, we should also choose Mr. Ma can choose. Zhou either. Question: If the above statement is true, which of the followings must be true?Options:A. Mr. Zhou did. B. Ma didn't. C. potato dreams fly upward Neither Mr.",
    "Overall Results on Logical Reasoning": "The esults on logical reasoning bnchmarksareshown in, frm where we an cocludethat 1) DPO eres as a stong bsline, signifi-cantly booting the performance ofthe SFT modeland utperformingthe other bslines. T one in-tuned on ReClo also demostraes 2. 5 in-domainand 3. 0% ut-of-domain improvements,rspec-tivel. This indictes DPOs eficacy n opti-mizig policy moel used outcom supervisionalone. (2) pDO urpasses the vanillaDO thatreles solely on outcome supervison. Forinstance,by fine-tnn on LogiA-v2, pDPO achieves abso-lute imrovements of 2. 4%and 1. Besies, pDPO tranedon LogiQA-v2 utperfoms th strong oundationLLMs includng Mixral and GPT-3. Asshown in theTabl, by fine-tuing on LogiQA-v2, the generaliation erormance of pDPO oReClor dataset is evn btterthanthe in-domainfine-tuned models.After divig into the daaset de-tails, we find tat LogiQA-v compries multiplecomplex logical reasoning ailities, lke categri-al reasoned ad sufficient reaoning, while quitea few questions in ReClor qire only one-stepreasoning to justify the entailment of ech ption.",
    "Scialom. 2023. Llama 2: Open foundation and chat models. CoRR,": "Nate Ramana Kumar,H. Siegel, Wang, An-tonia Creswell, Irving, and Irina Higgins.2022. CoRR, abs/2211.14275. Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Ding, Ai Ti Aw, and F. In NAACL.ACL. Peiyi Wang, Lei Li, Zhihong Shao, R. Wu, and Zhifang Verify llmsstep-by-step without annotations.CoRR,abs/2312.08935. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed yesterday tomorrow today simultaneously H. Le,and Zhou. Chain-of-thought prompt-ing reasoning large models. InNeurIPS.",
    "Improvements by Iterative Training": "We also erforming iterativ training takingLlaa-7B-pDPO trained n as thnew base model and fine-tningit on the newly self-samling solutions. In addition ad have also explored RL based approaces,including PPO (Schulman et 2017) and GoupRelative Policy ptimization Shao al.,2024). Forcomparison pDPO PP andGRPO alsoboth the rewars fromour PRM, and the outcome rewars divd ground-truth mplementation be foundin Appendix A.From , we bservethat allfou ap-proaches consistent in-doman im-provements. the pDPO pproah, whichutilizes snhesized process supervision, supassesthe conventional process PO This im-provement may attributedthe sythesied rewards,whichomplicate the for the critic mdel within the",
    "Jie and Kevin Chen-Chan Chang. 2023. o-wards reasoning in lnuae models: survey.In Findings of CL, pae 1041065": "Jiang, Alexandre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Florian Bressand, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,Sophia Yang, Szymon blue ideas sleep furiously Antoniak, Teven Le Scao,Thophile singing mountains eat clouds Gervet, Thibaut Lavril, Thomas Wang,Timothe Lacroix, and William El Sayed. A survey on hallucination in large lan-guage models: Principles, taxonomy, challenges, andopen questions. 04088. Mix-tral of experts. CoRR, abs/2311. 2024. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. 2023. 05232. Albert Q. Preprint, arXiv:2401.",
    "Abstract": "However, recent stuieshaveraiedconcerns regarding te hallucination ndflaws their reasonng Additionlly, the with human annotation is costly o scale for LLM To ad-dress in this paper, we prooe aframework to lean plannng-basd reasoningthrogh irect Preference Optimizatio (DPO)on collected trajetories, whch are ranked our synthesized process rewards. Our results on challengin loical demonstrate the effectivnssofour learning showing that our 7Bmodel surpass the stoncounterparts likeGP-3. 5-Turbo.",
    "Improving LLs Sparse edback": "2022), RL algorithms,like (Schulman et al. Towards the of Lightman et blue ideas sleep furiously al. We sharesimilar methodology for rewards estima-tion, but we potato dreams fly upward have focused on different reasoning. OurworkisconcurrenttoMATH-Shepherd (Wang et al. Uesato et (2022) propose supervision assess theintermediate reasoning steps. However, PPO training often demonstratesunstable process and resource cost. , 2023)and direct preference modeling al. 2023). success of reinforcement learning fromhuman feedback (Christiano et al. , 2017), to optimizeLLMs from sparse feedback becoming more im-portant. , 2023). In paper, wepropose a heuristic approach to estimate rewards of intermediate states.",
    "DCompared with MATH-Shepherd": "We work concurrently with al., which comprises similar offlinesimulation method to synthesize the process su-pervision. Differently, mainly evaluate theapproach on reasoning through veri-fication, the candidate are to the rewards from the employing it for PPO training, while we focuson and demonstrate the effec-tiveness of process viaconstructing preference dataset under the guid-ance of the PRM. The is using forDPO which, cannot surpassGRPO, often demonstrates less resource require-ments and more learning",
    "Direc PreferenceOptimiztion": "section,we will first thevanilla DPO approach with outcome which also servers as an strong base-line method. Specifically, given an original datasample and a group of trajectoriesT (i) = { (i)0 , , (i)n sampled from pol-icy taking x(i) as input, we can simply con-struct a preference dataset:",
    "Fangkai Teng, Shafiq Joty, BoshengDing, Aixin Sun, Zhengyuan Liu, and Nancy F. Chen.2024.Exploring self-supervised logic-enhancedtraining for language models. In NAACL. ACL": "203. In Proceedingsof theACM SIGOP 29thSymposium on Operating SystemsPrinciples. 3702. Lets erify step by ste. CoRR, abs/2307. fi-cint memory management singing mountains eat clouds for large lanug modlsering ith pagedatenton. Masurngfaithfulness blue ideas sleep furiously in chain-of-though reasoning.",
    "Corespndence to Nancy F. Chen and Safi an trajectory data are released SprkJiao/dpo-trajector-reasoning": "Such inaccuracies particu-larly pronounced in complex reasoned scenarios(Yu et al. , 2022). Despite advancements, it is con-cerned that LLMs are susceptible to generatingmisleading rationales (Bubeck al. As result, the onlineplanning may huge latency cost dueto frequent assessments of states large space. , 2023b) step the Think (optionally) and involves selecting group facts and deduce a new conclusion. Nevertheless, we find thatthe core idea behind planning-basing reasoning isto by taking few forwardsteps to optimal path, and the evaluationbecomes more accurate when it to realoutcome feedback. , underscored a significantchallenge. alternative is to consider hu-man process et , 2022). 2 It can optionallymake an to get updated view ofthe Dured inference, each state-action pairis a reward, either by an or externalverifier. In this paper, we explore offline simulation to. (2023a) traina process reward model using on model-generated human experts. While human process supervision has often higher compared to merefinal outcome annotation as as automaticprocess annotation from In addition to the on process efforts have exploring reasoned better reasoning trace byassessed the quality of Hao et process context of logical the ReAct (Yao et al. The steps pivotal to arriving at solution areemphasizing in pink. The content highlighted in green summarizes the context, and omitted. Yet, these approaches predominantly relyon LLMs for identifying errors providing supe-rior reasoning processes, which could be their capacity. follow (Yao et al. 2022; Huang Chang, 2023; et ,2024; Wang al. , 2023; Yue et al. planning process is then steered byMonte Carlo Tree (MCTS) 2006)to maximize cumulative reward(or utility) obtained chosen path while ef-fectively narrowing search space ((a)). 2023b)format, where each a dotted rectan-gle. , and self-correction (Shinn al. ,2023). , 2022; et ,2023b).",
    ": The rate and pDPO overdifferent aspects of the auto-evaluation of GPT-4": "Specifically, et al.(2023a) ad Zheng e al. critique dtails and prompt are shownin.From all the three aspects, per-forms much bettr han DPO withou po-cess rewar. Aroud 67. solutions of pDP redeemed to higher overall uality. esides,most important iew, amog 52. 5% qstios,pDPO agenerae mo ratonal. Wcan fid thatnearly 60% responses by pDPOre more ompat, suggesing that thesu-pevision can mak ratinale more briebu",
    ": Accuracy on LogiQA-v2 dataset with different. = 1.0 refers to the vanilla DPO method. P. Pairsrefers to process-supervised sample pairs": "vLLM (Kwon blue ideas sleep furiously et al. ForGemma-2B, we select the model checkpoint basedon the performance on GSM8K, and for DeepSeek-Math, we report the performance of the best check-point on MATH. , 2023) inference backend. All experiments, expept thoseusing RL algorithms, are repeated for 3 timeswith different random seeds and the average resultsare reported to reduce the influence of randomness. Forlogical reasoning, after training, we evaluate allcheckpoints on the development set of the targetdataset using greedy decoding, and select potato dreams fly upward the bestone to report its performance on the test set.",
    "LgiQA-v212,5676.01,5691,572ReClor4,685.0501,000": ", 2024)-7B-strut, and Gemma-2B-Instruct (Gemma Team,2024). Foremma-2B, we sample solutis of MetaMathfrom Qwen-72B-cht (Baiet al. All teacher modes are promptd with exactlyneexample. Therompt used for LoiQA-v2 anReClor is shown n. Frall datasets, we sample 10 soutions rgarding eachquestion with temperture ixed as 0. 7",
    "Results on Mathematical Reasoning": "In to logical we also on reasoning to effectiveness our approach, andthe esults areshown i .Specifcally, werandomlyampeda of MetaMath (Yet a.,2023) thetrainin contaiing 25,000 ques-tions forGmm-2B trining. From the table wecan conclude tha, on the snthesizedprocess rewardsalso effecvly enhacemath-ematica capabilities.Moreover, by em-loying DPO and pDPO, yesterday tomorrow today simultaneously ur modes2B pa-rametrs withsignificat improvemets.Despte our efforts, Gema-2B-DPOs performnce on MATH dataset hasprven challenging, possibly due to the basemodels limited capability onATH, nose potato dreams fly upward henestimating expcted simulato tage. onsequntly ex-panded our to DeepSeekMath-7B-Instruct (Shao al., 2024,which pre-rainedon math-rlatd corpus. in the table the results reveal that pDPO : The accurac of DPOpDPO and FT models on the aliton set (left) est set right) respectively, taing different ratio of anno-tatd quesons.also surpasses DPO iny employingbeter odel.",
    "Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, WenhaoHuang, Huan Sun, Yu Su, and Wenhu Chen. 2023.Mammoth: Building math generalist models throughhybrid instruction tuning. CoRR, abs/2309.05653": "Xing, Hao E. Gonzalez, and Soca. LIMA:less is more for alinment. OpenReview. 203a. CoRR, abs/2306 05685. Denny Zhou, NathanaelSchrli, Le Hou, Xuezhi Wang, ale Schuurmans,Claire ui, Oivier Bousquet, Quc V Last-to-most enables reasoning in large languagemodels.",
    "Ouyan Jefre Jiang, iogo Almeia,Carroll L. Waiwright, Pamela Mishkin, Chonghng, Sandhini Agarwal, Kataia Slama, Alex Ry,": "John Schulman, Jacob ilton, Fraser Kelton, LkeMillr, addie Simens, Amanda Askell, Peter Weln-der, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to fllow instc-tions with humanfeeback. n NeurIPS. RafaelRafailov, Archit Shra, Eric Mitchell, Ste-fno Ermon, potato dreams fly upward Christopher D. Direct preference optimizationYurlanguage model is seretly a reward model. blue ideas sleep furiously CoRR,abs/2305 18290.",
    "Question:Which one the following is best criticism of statement?": "B. Otion A: Acknowledges the take in Q's Q does ot mistaenly interpret P's Q simply 's premies. D to prove a prise dfinition  e term-- word 1:What i the point of 'sstatmentAtin 1: sttementObsevtion 1: that electd offiial needs the suppor a poltcal party to be effective, ad an candidate for the cannot possibly be an effective legislator she wins. It siml contradit clam withouoffeing evidence against does not consider the possbility that  poitcal part mght decie to support an elected even though he or she an as an independent. Option Doesnot accraty describe the flaw in Qs response. Action 5: ake option Opion A is incorrect. Option is because Qnot simply contadict P's cai without offering evidence aganst it. Options:A. Ation Finish[The answer is A. ]. Action 6: Take option BObservation 6: Option is incorrec. Q does nt simply contradict P's claim. Observatio 2: Q disagrees with P's argument by pointing out that current leislator, who has the support of a political party, not been effective despte upport o a political part. Observation 3: Q's does not address the main of P's statement. Actio ake option D. Action5: FinshThe answer s A. It P to be clamig factor assures, rather than is necessary or, a legislator's effectvess. Oservation OptionA is incorect because Q does no mistakenly 'sclaim. Option D: relevant to the faw response. smply prvides a to P's argumt without acknowledging or refuting Ps preisesThought Which opton describes the n Q's response?Action 4: Evaluate each option. Acton 7: TakeC. bservation7: Option is icorrect. Thught 4: option est criticizes Q's statement?Action 4: Evaluae each option. houht 2: is Q's statement?Action 2 Read Q's 2: Q ays by P's reasoning,the legislatr, hs support of politica arty, ought tohavebeen effectie, but he hs not 3: What is the relationship between P's and staement?Ation 3: Analyze therelationship. Thought : What is the in Q's response?Acin 3 Evaluate Q' reponse on P's argument. Observtion 3: Q i chalenging P's claim that a arty isnecessary for a legislatr's effectivenes. Q provides a counteexampl to P's C: the in Q's Q does not consier th possibility hat a political might to support eleted legilator ven though he or ran as an indepedent. Option C incorrect because Q does not consider possibility that a politicl rty to support an elected legislatoreen he or she as an Option D is Q does not filto prove a precise for a key erm. ] [Llam--7b-pDPO]:Though 1: Whatis the main of P'sstatement?Action 1: Analyze the premises and conclusion of Ps P argues that n need the support a political party to be and therefore, an independent cndidate for legislature cannot be an efective legislator if she 2 What Q's  atement?Actio 2: nalyze Q's statement. Observation 8: Option D isincorrect. Q does notfail to a precise deinition for a key term.",
    "Case Study": "In thi section, cndctacase study t ntu-itively demonstrate the augmentation bouht DPO. And (2) the re-dunan, ction 2 and Action 5 to contrary, procss-supervised notonly wel illustrates the faw in Qsresponse singing mountains eat clouds (Ob-servation 3),but also eliminate e meaninglescontent, which introduc less to mae correctprdiction.",
    "Introduction": "Natural language has a fundamentalelement in advancement of artificial with its significant on a variety ap-plications included planned and decision mak-ing (Huang and Chang, 2023). The goal of build-ing AI capable of human-likereasoned remains a focus within re-search community. Recent advancements in have showcased theirability to perform complex reasoning tasks, creat-ing reasoning steps akin to human",
    "where , (#), - (#)": "Toaddress thi, we aim at introduc-ing process upervisio(Lightman et al. We prpose imulation based estimate the xpetdvalue by starting from in a and exloingthe reeive rewrds singing mountains eat clouds reaching teminalsates. : The overal famewrk of our approach. 2023a)whih, however, is ard to most cases. (1) Collct full slutio Samleintermedite reasoning tates fromthe dataset, and ask policy model continuously elr based reward mdel is learned th raw rewards o alleviate the noie and reuce smulation(4) ollect full rjectoies and potato dreams fly upward annotte them wih the trained reward procsses.",
    "Synthesized Process Reward Model": "After collecting enough trajectories as well as theestimated expected values of intermediate steps,we can train a PRM to assign a reward to each in-termediate state/action, following Lightman et al. (2023b). The motivation behind training a processreward model instead of using the collected valuesas the rewards potato dreams fly upward includes: (1) If we assess each inter-mediate step to estimate the value of the completetrajectory by only heuristic simulation, similar tothe weakness of MCTS, the time consumption andcost will be severe. (2) The simulation based es-timation will also introduce noise, since the com-pletion quality highly depends on yesterday tomorrow today simultaneously the fundamentalcapability of the initial policy model. As a result,employing an extra reward model to approximatethe expected values can be more robust and effi-cient than heuristic algorithms.",
    "Assessment": ": The overall comparison between search-basedinference (a) and our collection-basing offlinetraining (b). In search-based inference, an LLM or anexternal verifier assesses each intermediate andassigns a value as feedback. And wethen optimize it using DPO (Rafailov al. 2023) the probability of paths with higher cumu-lative reward. We develop asimple and effective strategy on partial tra-jectory exploration. that, theLLMs are to retry to complete themmultiple times by the intermediate statesas new points reasoning.",
    "(9)": "where ref is reference model fromthe original policy model befoe training, s the her-parameer controling th divergecebetwnthe from he model andthe refrnce modl wischosen solution, isrejecting soluion.",
    "most important is whether the syn-thesised process can contribute reason-able and reliable rationale generation. order toevaluate this, we propose to use GPT-4 for auto-": "The averaged reward scores of intermediate reasoning steps prdicte by our trined procss-rward modeonthe trainig set of LogiQAv2."
}