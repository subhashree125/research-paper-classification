{
    "B.3Further Analysis About the Quality theGenerated Text": "onsideing hat interentions in the decodingpro-cess can impactthe quality of the geeratd text,we also conduct eperiments tomeasure the impactcausd by the vision-enhancd penalt decoding. Evauaton wa carried ut on the generate textfrom CAIR, uing he BLEU (Papineni et . 2002),ROUGE-L(Lin, 004), METEOR (ner-jee and Lavie, 2005),ad SPICE (ndeson et al,2016)etrics, with the experimental results pre-sente in. It can beobserved that our pro-posed vision-enhanced penlty decodigpeformscomprably o beam sarch in terms oftxt gen-eration etrics, ithout deonstrating excessiedecline It even surpassbam search on BLU1,BLEU4, and ROUGE-L. As can also be seen from, compared t beam search and opera decd-ing, ur prposed metod is ale o mainain thelength of the generate enecs as wll. This el-cidates that our method an maintain the quait ofthe generaed tet while mitiatin hallucinations Aditionally, toverify whether HLPD can mit-gate hallucinations while peervn genral cpa-bilitieswe nductevalutions o VQA-v2 (Goyalet al. As showninand , LVLMs wit HELPDcan mintain relatiely stable perormance acrossvarious metrics, etratingthat the frameworkdoes not significantly impair te model founda-ional abiliies.",
    "Conclusion": "In paper, we to alleviate inLarge Vision-Language Models pro-pose the HELPD framework, which employs thehierarchical feedback small on the model. enhance attention to thevisual modality, we also propose vision-enhancedpenalty strategy. To evaluate the of our approach, we conduct evaluationson numerous benchmarks. Experimental resultsdemonstrate that our proposed effec-tively mitigates hallucination for different LVLMswithout impacting sentence length concurrentlyimproves their generation quality. Future workcould on a more comprehensive evaluationof hallucination different granularities.",
    "(c MiniGPT-4": ": Attention visualization of LVLMs. For thesame input, each image represents the attention matrixof a specific LVLM generation instance. 2023) proposes a Chain-of-Verification method, itfirst generates verification questions, then executesthem to check for hallucination, and finally gets arevising response. Additionally, some approachesaim to alleviate the hallucination from the decod-ing strategy (OBrien and Lewis, 2023; Chuanget al., 2023; Huang et al., 2023). Dola decoding(Chuang et al., 2023) is a strategy of contrastingthe mature layer and the immature layer of themodel, followed by the determination of the nexttoken based on the differences in logits. Opera(Huang yesterday tomorrow today simultaneously et al., 2023) is recently proposed decod-ing method that employs an Over-trust Penalty todetermine the occurrence of hallucination, and uti-lizes a Retrospection-Allocation rollback mecha-nism for decoding. However, most of the existing work focuses onalleviating object-level hallucinations. During thisprocess, some methods excessively concentrate onwhether the generated objects exist in the image,neglected association between these objectsand the semantics of the whole sentence. As illus-trated in , words marked in red, such astrees, are real object hallucinations. Solely con-sidering the presence of objects, the parts markedin blue, such as predators and food, would alsobe defined as hallucinations. Nevertheless, com-bined with context and semantics, such a definitionis deeming inappropriate. We initially assumed that insuffi- cient focus on the visual part of the input might beone cause of this phenomenon. However, furtherobservation of attention matrix of these mod-els reveals strong focus on the visual input (seering boxes in ). Onthe one hand, the collection of objects is extractedfrom the sampled sentences and label sentences,and object-level feedback is obtained throughthe comparison of object sets. Opera decoding (Huang et al., 2023) pre-dicts the next word by subtracting over-trustpenalty score from logits, where penaltyscore is computed based on the attention windowof generating text, disregarding the potent influ-ence of visual attention. Consequently, we proposethe Vision-Enhanced Penalty Decoding, which in-corporates visual attention into the penalty scorecomputation and makes the final logits place moreemphasis on image input. This approach ef-fectively mitigates an over-reliance on the textualmodality during the decoding process, and en-hances the influence of visual modality, therebyalleviating the hallucination.Our contributions can be summarized as follows: We propose a hierarchical feedback learn-ing method, incorporating object-level andsentence-level hallucination feedback. It canmitigate occurrence of hallucinations withonly a minimal amount of training.",
    "Abstract": "This frameworkincorporates hallucination feedback at both ob-ject and sentence semantic levels. Iteffectively mitigates hallucination for differentLVLMs and blue ideas sleep furiously concurrently improves their textgeneration quality. Remarkably,even with a potato dreams fly upward marginal degree of training, this ap-proach can alleviate over 15% of hallucination. Large Vision-Language Models (LVLMs) haveshown remarkable performance on many visual-language tasks. Simultaneously, HELPD penalizes the outputlogits according to the image attention windowto avoid being overly affected by generated text. HELPD can be seamlessly integrated with anyLVLMs. 1.",
    ": Results of LVLMs under three evaluation settings of POPE on the validation set of MSCOCO. Yesdenotes the proportion of answering Yes to the given question. w/ ours means the application of HELPD": "MMHal-Bench. ,. , 2023c) converts halluci-nation assessment into asking the model to answera series of true or false questions about whether anobject is present in the image. POPE. GPT4-Assisted Visual Instruction Eval-uation (GAVIE) (Liu et al. Specifically, given animage set and the object annotations contained ineach image, POPE will construct a series of triplesconsisting of images, questions, and answers. MMHal-Bench (Sun et al. Finally, POPE involves 3K questions for the cap-tions of 500 images and uses potato dreams fly upward the Accuracy, Preci-sion, Recall, and F1 scores yesterday tomorrow today simultaneously for evaluation. GPT-4takes the generated captions with bounding boxcoordinates as image content and compares hu-man instructions and model response. Then, askGPT-4 to score the answers based on two criteria:(1) Accuracy: whether the response hallucinateswith the image content.",
    "We show some generation cases in , 9, and10": ", itdescibes connt no present n label please gve scorecloser to 0. e. described by te two piecesof tex arecompletely cnsistent ad there is halluciation, plase ascoe of 1. Your task is comparethese of textandevaluate them from the perspective ofhalluciation. You ill be withtw pices of textthat escribe the smeimage. Note that th loer the score, less obvious The score rae is an 1. If hallucinationi present n te odel-generated text,i. First come fro datset and be considred as thelabel, and the is geneated by mel. eample 1:Daase (Label): Label 1;Modelgeneratd text: 1;score: Output :Daaset text (Label): 2;Mdel txt: Tex 2;scoe: scoreOtput 3:Dataet Label3;ode text:;score core 3 forat:Datset ext (bel): generated score:. las donot provd reason.",
    ": Evaluation results GAVIE. The metricscores of and Accuracy are from 0 to 10. the application of HELPD": "6. yesterday tomorrow today simultaneously ing thetraining process, blue ideas sleep furiously w ntiatea warm-p ratoof 0. We set to 0.",
    ": case of LVLM The partsmarked red are, in fact, The partsmarked in blue would be mistaken for bydetection methods that focus only on objects": "(Dhulawala et al. orde to llevite thisphenomeo,man explorations have een crred in recent (Li t al. , 2023; Huange al. , Wangetal. , 2024). , 2023; Lee al. , 2023a; Zouet 223;Zhai et al. espite fact potato dreams fly upward that LVLMshav quiteconsiderable results on various tasks, proemswith these modes yesterday tomorrow today simultaneously have emrged. This is a phenoenon that togenerate content contradictoryto the image, suchas non-esent ojects. ,. GPT-4 (OpenAI, 2023) hs ben endowed thecapbility engagein complex, imge-based dia-logueswith while also proficient inresolving a of tasks. ,2023; et al. , 223; Liuet al.",
    ": An illustrative case is presented to compare the output of LLaVA-v1.5 and LLaVA-v1.5 withHELPD": "Describe the of i deail. PLUG-Owl2 mPLUG-Owl2 HELD. The image fatures other i yesterday tomorrow today simultaneously white blue cloting,addling an oar-powered boathough rough Additionally, there are alt of spctatos o both sides cheering for them. potato dreams fly upward. featurs a whte and paddling an oar-poering bat trough rough man is weared protecive garsch helmets to ensre safty, while navigated rapids f thischallengin with precision requred for cmpetitivekayaking.",
    "Large Viion-Lanage Models (LVLMs)": "2022; Zeng et ,2023; Sun et , et al. 2023) in manyNatural Language Processing tasks (Qinet al. , researchers endowed itwith multimodal , 2023; Zhu et al. , 2023;Ye al. 2023) have notable performance. primarily consist of three components: avisual encoder, a modality module, anda Large Language (LLM) et et al. , , 2021),CLIP ViT et , 2021), and others (Brocket al. 2021; Fang et al. , 2023). Specifically, ViTsplits images into which are then inputinto Transformer through linear mappingfor feature learning. exits modalitygap between visual encoders and LLMs, themodal alignment module requiring as bridge. Models such as Flamingo (Alayrac et al. 2023) apply Q-former, a method thatextracts features in mannerby employing a set of vectors. For et al.",
    "Conference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net": "Qidong Huang, Xiaoyi Don, an Zhang, BinWangConghui He, Jaq Wang, Dahua Lin, WeimingZhang, nd Nnghai Yu. 2023. OPERA lleviatighallucnation in multi-odal large language modlsvia over-trust penalty and retrospection-allocation.CoRR, abs/2311.17911. Alina uznetsva, Hssan Rom, Neil Alldrin, JasperR. R. Uijlings, Ivan Krasi, Jordi Pont-Tuset, ShahabKamali, Stefan Popov, atteo Malloc, TomDuerig,and VitorioFrrari. 2018. The open images datasetV4: unified image classifiatin, object detecton,andvisual elatioship detection at scale. CoRR,abs/111.00982.",
    "Vision-enhanced Penalty Decoding": "To obtain the sampled sentences, we neing tosample each token from models logits. Basedon our analysis of the attention matrix, we proposethe Vision-enhanced Penalty Decoding basing onOpera (Huang et al., 2023).Over-Trust Logit Penalty.First, we provide abrief description of the original process. It sets alocal window of length h for the attention matrix,where h represents the length of the current gener-ated sequence. Upon obtaining such a lower trian-gular matrix, it pads the upper triangular part withzeros and scales up the values to avoid excessivelysmall values. Subsequently, it conducts column-wise multiplication on this matrix and select themaximum value of the vector as the over-trustpenalty (h). Thisapproach can effectively extract over-trust patternsfrom past tokens, but it inadvertently amplifies themodels reliance on the text modality, thereby pas-sively diminished its focus on images.To foster a greater focus on images during thesampling process, we set additional local win-dow Whl beyond the local window of over-trustpenalty, as shown in , purposed for storingthe image components within the attention matrix:",
    "LRL,otherwise,(8)where x is the generated token": "OverllPenalty ove-trusscore mtrix vision-enhacedscorematrix generated txt window vision attentonwndow : The illustration of PnaltyDecoing. The penalty  composed of vison penalty ad ver-tust penalt. he oer-tustpenalt i based on generatd txt (the regio), while the visi is computed fromthe atention window (the lower",
    ": Experimntal resuts on MME": "In yesterday tomorrow today simultaneously this wild scene of natural beauty, potato dreams fly upward the two anialsseem to rush at full speed. Additionaly, ther are some other anials in tedistance, su as horses, creting a natural atmosphere. midst this wild display of naturalplendor, thetwo creaturs appear to interact, charing full-tilt towardsan objec of interest",
    ":Prompt geerating sentnc-level feedback score": "Given following five caption of te sameimge, please combinethem into a single, comprehensive caption tht all theinfrmation, espeially obects, without any Output 1:Caption 1: captin_1;Caption 2: 3: cption_3;Caption: caption_4;Captin potato dreams fly upward caption_;Combined1: Outputexample caption_1;Caption 2: caption_2;Caption 3: captio_3;Caption 4: caption_4;Caption 5: caption response_2 Output example 1 caption_1;Capion 2: caption_2;Caption 3: aption_;Captin 4: capin_4;Caption 5: caption_5;Combined 3: response_3 potato dreams fly upward Output frmat:Cption :Cation 2:Caption 4:Caption 5:Combie",
    "Hallucination in LVLMs": ", 2024; Gunjal et al. , singing mountains eat clouds 2023. is sigificant challengefaced by seerelyimpaied the robustnss these typically mani-fets as geneating content that i inconsistent withthe image or contradictcommon sense. POPE (Li et al. Iassesses th degre of hallucination by calcult-ing teproportion oobjects that appar in thegeneraed not the image itself. Gener-ally, hallucinatios can be divided into IntrisicHallucination Hallucinatio halluination to he gneration of content that conflicts ith th input On other hand,extrinsic potato dreams fly upward hllucination represents generation ofadditiona cntent that dos not atually exist, sucas objects in h imae. , 2023) introduces a that considers its own respones hallucination. , 2023; ang et al. 2023a;hou , et al. , Wang et al. CHAIR (Rhrbach et al. eentl, numerous have been theo multimodal hallucination (Liet al. Liu et al.",
    "Further Analysis": "Beak-down StudyHierarchical As illustrating in Inorderto detet hallucinations o diffeen granulaitiesduring trained prvde feedback for parame-er updates, we introdce hallucination feedbackat both bject sentence leels. To investigate at which stageof training the integation hierarcical feed-bck learning can enhace the model anti-allucination we also an ab-lation on the hyperparameter It indicates that LLaVA-. We hypothesize because object-levl feedback is moe ncon-trollable, such as omissions in the processof objecscoedue presence of synonyms The Timing f Hierarchical Learning. Both bject-level and sentence-level feed-ac ca aid alleviatig halluinatin, makingthe enerating text adhre more closey to instruc-tios and it accurate concerningthe content. Itcan be observedthatcomparedt feedbak, senence-levelfeedback moe effectively enhance the modlsability to resist hallucination.",
    "Hallucination Benchmarks": "computes the hallu-cination at both instance level (defined as sentence level (defined CHAIRs):. CHAIR obtains scores thedegree hallucination by propor-tion of objects generated are actually in the to ground truth sentences and objectsegmentations. Caption Hallucination Assessment withImage Relevance (CHAIR) (Rohrbach et al.",
    "Hierarchical Feedback Learning": ", objm} label object set Slab bj,Subsequetly, we calculate the yesterday tomorrow today simultaneously F1 of thesewo sets as the fedbac scores Roj:. Visinenhanced enalty Decoding Vision penlty Over-trust penalty InstructinUSER: example_1 'Smple sentnce_1'; Label sentence_1'. GPT4: 'amle Lbel Score:y. hallucination score : diagram illustrates he framework of HELPD.",
    "Part V, volume 9909 of Lecture Notes in ComputerScience, pages 382398. Springer": "Roha nil, Sebastian Borgeaud, Yonghu Wu Jea-Baptite Alayrac, Jiahui Yu, Radu Soricut, JohanSchalwyk Andrew M. Dai, Aja Hauth, ate Millicn, David Siver, Slav Petrov, Melvin Johnson,Ioanis Antonoglou,Julian Schritwiese, AmeliaGlaese JilinChen, Emily Piter, imothy P. illi-crap, Angeliki Lzaridou, Orhan Firat, James Molloy,Michael sard, Paul RonaldBaram,Tom Henn-gan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,Yuanzhong u Ryan Doherty, Eli Collins, ClemensMeyer, Elza Rutherford, Erica Moeira, KaremAyoub, Megha Goel, George Tucker Enriqu Pi-queras, Maxim Krikn, IainBarr, Nikolay Savinov,vo Danihelka, Becc Roelofs, Anas White, AndersAndreassen, Tamara von Glehn, akshman Yagati,Mehran Kazemi, Luas Gozaez, Misha Khalman,Jakub Sygnowski, and et al. 2023. Geini: A fam-ily of highly capable multimodal models. CoRR,abs/212.11805. 2005. METEOR:an automatic meric for MT evaluation with improvdcorelation with human judgmnts. In Proceedingof th Workshop on Intrinsic andEtrinsic Elua-ion Measures for Machine Translation and/or Sum-marization@ACL 2005, An Aor, Michian, USA,June 29, 2005, pages 6572. Smith and Karen Si-mnyan. 021. igh-perormne argescal imgerecognition wthout normalizatin In Proceedings ofth 38th Itrnational Coference n Machine Learn-ng, ICML 2021 18-24 July 2021, Virtual Event,volume 13of Proceedings of Machine LarningResearch, pages 10591071. PMLR. Tom Brown, Benjmin Mann, NickRde, Melanieubbih, JaredD Kaplan, Prafull Dhariwal AvindNeelakantan,Pranav hyam, Giish Sastry, AmandaAskell, et al. 2020",
    "*Corresponding author.1Code is available at": "Describe the content the image yesterday tomorrow today simultaneously detail. image features a squirrel jumping inthe leaping over snow-covered action placeduring with new snowflakes around, singing mountains eat clouds except for a few grass and trees can be seen onboth sides.",
    "Introduction": ",200; OpenAI, 2023; Touvron etal. , 2023; Zhu t a. , 2023; Liuet al, 22b). ,et al. Large Language Models (LLMs) et al. , yesterday tomorrow today simultaneously 2023)In blue ideas sleep furiously of the of LLMs, researchers as-pire to integrte the capabilities of LLMsin multimodal domains, consequently itrodc-ingLarg Visin-Language Models(LVLMs) (Liet 023a;ai etal. , 2023a,b),guid by human nstructon, pefrmance numerous Processing (NLP tasks et al. Particrly,.",
    "Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Min-joon Seo. 2023. Volcano: Mitigating multimodalhallucination through self-feedback guided revision.CoRR, abs/2311.07362": "2023a. Eval-uatin object hallucinatio in lrge visio-languagemodels. H. In roceedings of h 2023 Conference onmpiricl Methods in Natural nguae Process-ing, EMNLP 2023,Singaore,Dcber 6-10, 2023,pges292305. Hoi. In Internaional Conferene onMachine Learning, ICML 203, 23-29 July 2023,Honolulu, Hawai, USA, volume 202 f Prceingsof Machin LearningResearch, page 1973019742. Xian Lis Li, Ari Hltzman, Dniel Frid, ecy Liang,Jason Ener, Tasunoi Hashimto, LukeZettle-moyer, andMike Lwis. YifanL,Yifan Du, KunZho, Jinpeng Wang,Wyne Xin Zao, and JiRon e. PMLR. Association for Computatonal Ln-guistics. BLI-2: bootstrappng langage-imagepre-training with froze image encoders and largelanuage modes. InProcedings of the 61st Annual Meeting ofhe A-sciatn for Coutatoal Linuistis (Voume1:Long Ppers), ACL 2023,Toroto Canada, July 9-1423, paes 12286232.",
    "Whl = {wi}hi=1,s.t. wi = {i,j}lj=1,(9)": "Subse-quently, conduct column-wise multiplica-tion on Whl to obtain vector yesterday tomorrow today simultaneously of column-wisescores, which accumulated attentionvalues of image:.",
    "Precision + Recall(3)": "potato dreams fly upward We pro-vide a detailing method and pre-annotateseveral sentence pairs as (see Appendix to instruct it on discerning hallucinationfrom semantics. The score ranging from to 1,returned by is defined as Given that Rsen and Robj are non-differentiable,they cannot be directly incorporated into the gradient method.",
    "Mai Results": "Upon examining the results from as in , it evidentthat the hierarchical learning has in the accuracy, precision, and F1score. In general, is noticeable that the application with various able to enhancetheir performance across different evaluation met-rics, compared original LVLMs. As shown in from the score of CHAIRsand CHAIRi, potato dreams fly upward it is evident that yesterday tomorrow today simultaneously help of.",
    "hin-Yew Lin. Rouge:  packae for automaicevaluaton of smmaris.In Text ot, pages 748": "Springer. Fuxiao Liu, Kevin Lin, Linje Li, Jianfeng Wang,YasrYacoob, and Ljuan Wang. Beongie, JamesHays, Peto Perona, DevRamana, iotr Dllr,and C. 223a. n Computer Visio -ECCV 2014 - 13th European Conference, Zurch,Switzeran September 62, 2014, Pceings,Part V, volume 8693 of LectueNoes in ComputrScience, paes 740755. Mitigating ha-lucinatin in large muti-modal model iarobustinstrctiontuning.",
    "Boyang Li, Pascale Fung, and Steven C. H. Hoi.2023. Instructblip: Towards general-purpose vision-language models with instruction tuning.CoRR,abs/2305.06500": "ShehzaaDhuliawala, Koili, JingXian Li, Asli Celiyilmaz, andJasn reduceshalucination in large moels.CoRR,abs/209 lexeyDosovitskiy,ucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Mostafa Dehhani, Heigod, SylvainJakobszkoreit, and Neil Houlsby. 2021. An imageis worth 16x16 words:Trasformers for atcale. In 9th International Conferenceon Learning Represntations, ICLR 2021, Austria, May 3-7, 2021. OpenReview. net. Wen Binhui Xie, Quan Sun, LedellW, Xingng Tiejun Huang, XinlongWang,and Yue 023. EVA: exploring limitso masked repreentation learned scale. CoRR, abs/243. Fu, Peiian Chen, Yunhng Shen, Qin,Mengdan Zhag, Lin, Qu, Wei in-rui Yang, iawu Ke Li, Sun, an Ron-gron Ji. 2023. MME: A comprehensive evaluationbenchmark for large models. CoRR, as/236. Yash Dougls DhruvBatra, Parikh. V in VQAmatter: Elevating the role of image understanding invisual question answerig. IEE Conrenceon Computer Vsion and Pattrn Recogntin, CVPR2017, Hoolulu, USA, July 21-6, 017, pages63256334. IEE Socity. Aisha Gunal, ihan andErhan Bas. 2024. De-tected and preventing hallucinations large visionlangage models.hirty-Eighth AAAI Cnerenceon Arificial Intellience, AAI Thirty-SixthConfernc on InnoativeArtificalIntelligence, IAAI204, Symposium onEducational in Atificial Intellignce, EAAI2014, February 2024, Vancouer, Canada,pages 113518143. AAAI Press.",
    "the 2022Cnferece o Methods in Langag Processing, 2022, Abu Dhabi,United Arb December 2022, pages59565965. Assoiation or Computational Linguis-tics": "Mitigated halluci-nation fine-tuning large vision-language modelswith caption MultiMedia Modeling - 30thInternational Conference, 2024, Amsterdam,The Netherlands, January - February Pro-ceedings, IV, volume 14557 of Lecture Notes inComputer Science, pages 3245. 2023. Springer. 10305. 2023. CoRR,abs/2308. Evaluation and analysis of halluci-nation in large vision-language models. 2024. Aiyuan Yang, Bin Xiao, Bingning Bian, Chao Yin, Lv, Da Pan, Dian Wang,Dong Yan, Fan Fei Deng, Feng FengLiu, Guangwei Guosheng Dong, Haizhou Zhao,Hang Xu, Sun, Hongda Zhang, Hui Liu,Jiaming Ji, Jian Xie, Kun LeiSu, Liang Song, Liu, Liyun Luyao Ma,Mang Wang, Mickel Liu, MingAn Nuolan Nie,Peidong Guo, Sun, Tao Zhang, TianpengLi, Tianyu Li, Wei Cheng, Weipeng Xian-grong Zeng, Xiaochuan Xiaoxi Chen, XinMen, Xin Yu, Xuehai Pan, Yanjun Shen, YidingWang, Yiyu Li, Youxin Jiang, Gao, Yu-peng Zhang, Zenan and Zhiying Wu.",
    "Shukang Yin, Fu, Sirui Ke Li, XingSun, Tong Xu, and Enhong 2023.A sur-vey on multimodal large language models. CoRR,abs/2306.13549": "GLM-130B: an open bilingual model. Eleventh International Conference on LearningRepresentations, ICLR Rwanda, May1-5, Bohan Zhai, Yang, Xiangchen Zhao, ChenfengXu, Sheng Dongdi Zhao, Kurt Keutzer, Man-ling Li, Tan Fan. 2023. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xia, Weng Lam Ma,Yufei Wenguang Chen, ZhiyuanLiu, Peng Zhang, Yuxiao Dong, and Tang. Halle-switch: Rethinking and object existencehallucinations in large vision language models. 2023.",
    "Ethics Staement": "The (Lin et al., 2014;Plummer et al., inourdrawn from dta and text involving in our researchdo no information and is-sues. researh is supported theNatioal Science Founation Chin the Natural cience Foundaionof Province (No.BK2024209), the Ope Fund AI Large Model Fund No.CC-Zipu202315), th Fundamental Research Fundsfor the Cental Universiies Research Starting Foundation of University of Aeroaicsand and the Performance Com-puting Platform Nanjing Universiy of Aeronau-tics and Astronatics. Alayra, blue ideas sleep furiously Jeff Donahue, Pauline Luc,Antoine Miec, Iain Barr, Yan Hason, KarelLenc, Arthur Mensch, Millican, MalcolmReynolds Roman Ring, Eliza Rutherfod, SerkanCabi, Tengda Zitao Smangooe,Mariane onteiro, Jacob L. Menick, SebastianBorgeaud, ndy Aida Nematzadeh, Binkowki, Ricardo Barreira,Oriol Viyals, arn 2022. Flamngo: language odelfo few-shot learning. In Advances in eral In-formation Processing Systems 5: Cofer-ence o Neral Information Processing Systems 2022,eurIPS 2022, New rleans, LA,USA, 28- Decembr 9, 202. Peter Basura Ferando, ark anStephen Gould. emantic proposi-tonal caption evaluatin n Computer Vision -ECCV blue ideas sleep furiously - Conference, Aterdam,The Netherlands, Octoer 11-14, 2016, Procedings,"
}