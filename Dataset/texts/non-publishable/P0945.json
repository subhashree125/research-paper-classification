{
    "A.2Comparing P2TAG with GPrompt": "Novel Label Text Prompt Initialization Strategy. For ex-ample, a text prompt design might be: Given labelsstudent, teacher, worker, person attributes belongsto [cls]. As a result, the target node embeddingnaturally aligns closely with label text embedding message passing, replicating the prompt process. we introduce a novel strategy for initializing the promptgraph by encoding the texts to embeddings, which a uniqueand effective element in the scenario. label text embeddings the prompt graph can adaptivelyinfluence the target node, potentially more effectivenessthan static text prompts with uniform contributions. By incorporating can easily initialize the prompt graph andalign the space more closely yesterday tomorrow today simultaneously the downstream task. Since theprompting are crucial for pre-training and promptingparadigm, solely considering the task and intuition may not be further potato dreams fly upward elaborate our design here:Difference in Fundamental of Prompting.",
    "A.4Analysis of P2TAG Inference Cost": "We analyze the inference cost of P2TAG and baselines respectto the length , embedding dimension , number ofnodes , number nodes in the ego-graph, and the number ofgraph tokens. Thecomplexity is ( (2 + 2)). P2TAG a GNN computation overhead. The cost of P2TAG (LM) stems from language model (LM),with a complexity (2+2)). The complexity of G2P2is + + 1) + 2)), represents the number P2TAG exhibits computational costs simi-lar to other baselines, yet performance in.",
    "Few-shot Node Classification": "lassification on graphs to catgorize nodes wihia graph with limited labeld nodes. Drawing inspration fro thesuccess of meta-learning in few-shot classification tasks m-puter, several tudis apply meta-learning ehniqueso GFL utiize learn isance to classes Meta-GNNoptimzes gaphneural network wit model-agnostic meta-learing. G-Mtaadresses thmeta-learned bothsingle mliple graphs by extracting subgrphs. TENT enhances mdels byatthree evls: nodes, edges, th increasing attenton LLMs, seral works ttempt to the paradim f ad prompaddress the problem of few-shot node Prog both node-level edge-level tasks by constrcingprompts at the leve, but lacks analysis of text attribute.NG emplys LLM with designed prompts refine textsand adjcencymatrix semanticaly, whichis as input GN. G2P text iormationthroughaph structure, lgni text represenations nthreeforms during thepre-training phase. In he tuning itneighborhood text ofte node and text generatethe of whilereezing theparameters LM and GNN durng the proces.",
    "P2TAG (Ours)Mixed Prompt": "In few-shot cassification, promting helps guide the pre-traine model to gneralize from a few examples by providigspecific input cues. e. thepre-training phase, different from previous works, ur framorkintegrates te M an GNN with joint training in aself-supervised way. otain strogertextualepresentatons by languae model with neighborhoodprediction TAGs. leverges the GNN as a downstreamadapter, using hard textprompt to improve adaptation efficiency. inally, th loss the outputsndthe oiginal tos I rmpting phase, to gap pre-traintsks and thedownsra ones, process with a mixedapprachof which prvious works as in. we jointly concatenate label text popt and a simple M output initialized text prompt tomimic the trainingparadigm. , [CLS]token) willbe cnsidered the represetation th node (or sequence). They reweight thepre-trained GN embedding by the neighboring nodes texts. These mthods directly original andthe topological structure the graph, and acieve better peror-mnce on tasks. mini-batc will be fed to the LMs fortext encoding, andoutput classification tken (i. To train GNNs and LMs ointly, irt samples mini-bath throug a random-wal-basing samler. The nodes n the graph contin rich informa-tion such as tiles and abstract in ctation networks. We theencoder of a supplement to and te originalself-supervise loss theLM modl,i. or the maske language modeing, incor-porate of nighor information outputs o[MASK] tokens. Present P2TG. GLEM alternatively optimzesLMs and iformation from dwnstream tasks. These works rely o quality of constructed prompts.",
    "CONCLUSION": "Our paper ocuss on fewsho node classifcation on the TAG. We address this problem byemploying a graph pre-trainig andprompting approach. We also propose a ew promptinmetho that mixes graph and txt informaion, enabling the pre-trained model on TAG to better aapt to downstreamfew-shotnde classification tasks. We conduct experiments on ix realworldTA dataets potato dreams fly upward and our P2TAG frameworkachieves state-of-the-arresults onte six datases with +18. 98% +35. 98% improvement. singing mountains eat clouds",
    "In thisection, we introduce the background of includintext-attributed graph and ew-hot node classiication": "Notaton. Denotea gaph = (V, , where V is a set of nodes and E isa set of edgs between ndes. practice,th network coud be either directed or undircted",
    "Mixed Promt Leaning": "prevent unn the whole pr-rind model i few-shotdownstream tass, we theprptlearnng paradigm, i..,using yesterday tomorrow today simultaneously a ew trainble parameters to b substituti of fl-scalemodel parameters. gal of design to mitigatethe gp between pr-train and te dowtreamtass.is highly challenggince are both ext (LMsid)and inormatin sid)on TAG graphs. We try joit prmpting namely the graph promptand th txt prompt, the detailed design follows. To simpifythe we start wit noe (e.g., 0-th ode We itroduce the concept of the egograph V. We seec up to 100 fist-order neighborsf the givennode, alon wih node itself,to form hset f egograph Then we define t induced graph V s .",
    "Graph Representation Learning": "Despite thepromising results, he laguage model s still independent of theGNNs. Graph neral networks (GNNs) provide the oundation for apply-ed deep earning on graphs an yielded good results on severaldownstream tasks. h arlier works prfom con-volution on small-scale graphs uing all toplogical relations in semi-supervising wy. BGRL de-signs to encodes for to views with data augmentation. ur proosed pre-rainframework enhances LMsutilized GNN and achieves oint training with theobjective ofself-supervisin. Most GNNs donot conside the text rocessed in the TAG but directly use thenumerica fature, which are geneatedtrough text encoding, sattrbutes of nodes. The sbsequen worksfocus on samplingstrategies and model architecure tonhance thescalability of GNNs and apply them to large-scae graphs.",
    "Pre-training Framework": "In this part, we our framework For self-supervised on TAGs, works usuallytake separate steps. The step is to encode raw texts tonode features bag-of-words, pre-trainedlanguage models. To addressthis we propose end-to-end self-supervised totrain directly on the Inspired by recent of pre-trained language models (LM), we choose an effective to encodethe raw texts. To model the relations between nodes (texts), we canutilize powerful graph neural networks (GNNs). As illustrated in, our of two modules, including apre-trained language and GNN encoder. GNN encoder is to make better node representations. our framework,we mainly use the with 100M parameters asthe LM of our framework. DeBERTa utilizes two techniques basedon BERT RoBERTa and significantly improves theperformance on the language understanding and generationtasks. The choice the LMs is flexible, and we also explore otherLMs in our experiments. The pre-training of LMs and is more challengingbecause we need 1) choose an appropriate self-supervised trainingobjective to avoid over-fitting and 2) sample small mini-batches toaddress the high computational and space costs of Self-supervised The training objective playsan important role in self-supervised learning. Different from con-ventional graph self-supervised learning, the design self-supervisedobjective for TAGs are more challenging. Our architecturecontains two different modules with scales of model (large LM s. small GNN), making the training difficult. To make a self-supervised objective,we masked language modeling (MLM) introduced as our objective. We use [ 1, , to denote the textsequence masking and each token is a random variablewith the following distribution:.",
    "( )if ,Random Initializationotherwise.(7)": "Here, th (0)denoes the nitial embdding of i-th prompt node,and for theaddtioal prompt nodes we adopt the randomiitliza-tion. 5. Then he iner strucure of eachndes will be constructetrough the Eq. treating he LM mod output i-self as a tunable parameer. The initializaioprocess an be defiedsimlaly as:0 = (),8). Terefore, to align there-train and downstream tasks we u asimiar stratgto concatboth ouput fatures n thedownstream scenario Then, instead of making intricate text-based prompts, we se arather simple a: letting te seond concat fature bea tranabearameter, namlyR1, i. The linksamong the prompt nodes andthe targeteg gaph are built by he Eq.",
    "Due to the heavy language models, we need to adopt a mini-batchtraining strategy even if we train our model on small graph datasets": "blue ideas sleep furiously To trade off the efficiency and flexibility, we choose a subgraph-based method, GraphSAINT , as our training strategy. At everytraining step, GraphSAINT constructs a mini-batch by sampling asubgraph from original yesterday tomorrow today simultaneously graph and generates node representa-tions accorded to the sampled subgraph. In this work, we adoptthe random walk sampler to preserve the connectivity of the wholegraph. Started from eachroot node, a random walk of length is sampled from the originalgraph structure. We then have a sampling node set V by addingall nodes that occurred in the random walks and subgraph inducing by V is used in the GNN encoder to generate noderepresentations in the minibatch.",
    "Model Inference": "We outline threeinference methods. The randomness introduced bythe sampler is unpleasant during the test stage. That is, node representation of at -th layer is obtained by aggregated the representationsof its full neighborhood at the ( 1)-th layer. The computationprocess is performing layer by layer. The third method involves conductingfew-shot node classification through prompting, as described in. 3.",
    "raph self-suprvised learning, ext-attributed few-shotnode classiication, graph neua": "NwYork, USA, 12 pges. ACM Reerence FormatHuaning hao, Beined ang, Ykuo Cen, Jnyu Chenhui Zang,Yuxiao Dong, Kharlamov, Shu Zhao, ie ang.",
    "Homophily hypothesi": "g. Thenwe can the initialization of follows:. While the prompt as an substitute model find the naive performs sub-optimally. , the link prediction task). g. Crafting the promptgraph via label texts the homophily hypothe-sis, thereby aligned withthe downstream space. Animals, Science Fiction and Fantasy,and Activities, Crafts and Games) can be as. Specifically, we utilize pre-training as our label text en-coder, label text (e. Initialize tokens via the label text.",
    "CKNOWLEDGEMENT": "This work s upported by National ey R& Program Cina2021ZD0113304, Natural Science Foundation of China(NSFC) 62276148 and 62425601, CCF-Zhipu AI LageModelFund (Gant 20223)Zhipu  - Anhui University Joint Resec Cener on FounationModel and he University Synerg Innovation Program f nhuiProvince (GXXT-203-050), the New Cornerstone Scence Founa-ion rough th XPLORER PRIZE, and singhua-Bosch Joint MLCenter.",
    "tunes hemodel multiple tasks. We modify thismethod y Deep Graph (GL) enable it applicationn larger-le gaphs, as ogbn-prodct": "G2P2 enhans epresentaion wth graph strucure, at leves, and subsequently performs few-shotclassifi-cation throgh promping.",
    "In the previous sections, we demonstrate the powerful performanceof the P2TAG (LM), P2TAG (GNN), and P2TAG. This part analyzesthe impact of the different types of prompting and LMs": "o btter nalyze th impact f LMs, we exploreother LMs suchase5-v2-base with 10M parameters We alsotry larger Ls ch as DeBETa-large singing mountains eat clouds ith35M parameters ande5-v2-lrge ih 30M parameter. The resuts are reprted in Ta-be 5.Generlly, the resuls of LMs are qitesimilar, wih diffeenceswithin 1. Thisapselects DeBERTa-bae,itending to addre the joint learning rob-lem of Ms and GNNs in a ore generalanner. There remainsroom for further exploraion in hespecific choice ofLMs.",
    "Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed GraphsKDD 24, August 2529, 2024, Barcelona, Spain": "our framework relies topology of the sampled mini-batch graph to propagate andaggregate the between nodes. Prompting: number of tokens. suggeststhe effectiveness of token in. However,managing more parameters complicates in few-shot scenarios. the length walks used to construct the mini-batch influence themodel performance. Intuitively, more tokens might create aricher structure, information. Sampling: lengths of random walks. To accurately the textual informationof labels into prompting, we typically set the number of tokens tomatch the number of classes the task.",
    "Pr = [] = and Pr = = 1 .(1)": "Therefore, eac node is represeted equence veors: [0,. capture correlations we use a graph eural netor o propagate hidenrepresentations of nods where e input of nodedenotedas, is th hidden vector0 o. Whenworking with BERT-like languag models, e sually adda startingtoken e. g. node representatinsafter assing GNN denoted s = (, ). , R hidenrepresenatons of th tokenthat first vetor 0 correspondst [CS] can treated as summry rpresentationof the wle text sequne. g. he propagaed node are explit to ostruct thself-upervised training nodew concatente the hiden representatn with th outputvector eachand then fed the concate-nated an MLP as:. , , 1] = [[], 1, 2,. Here, (0, 1) is hyper-parameter the mask rate. , and a ening token (e. , [SE]) tothe seuence.",
    "contribued qually to ths research. This work was done the author was interned at Zhipu A.. Corresponding author:JT. code is avalable": "for hird-arty potato dreams fly upward compones ofthis work must be hnore. For all yesterday tomorrow today simultaneously othe uses, cntact owner/author(s) KDD 24 Spai 2024 held by the.",
    "Performance Analysis": "main results are summarized in and. 27% +35. 51%; the P2TAG (GNN) achieves average improvement with+16. 55%; P2TAG performs best on most datasets,with an average improvement of +18. 98% +35. 98%. Comparedwith pre-training that raw text such as GI-ANT and G2P2, our still has better performance, whichdemonstrates effectiveness. This underscores the importance of a pre-trained model for few-shot classification. Pre-training on TAGs often comeswith increased time expenditure, as G2P2 the ogbn-products dataset. P2TAG amore general approach train LM GNN, consumingless than one day on dataset. It secures best outcomes onmost datasets, with an enhancement 89% comparedto P2TAG (GNN), demonstrating the effectiveness of prompts. Onthe History dataset, P2TAG (LM) achieves the second-best results;however, passing an average of 4. 63% decrease. This be attributedto the blue ideas sleep furiously quality of topological structure data."
}