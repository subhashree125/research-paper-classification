{
    "Lujain Ibrahim, Saffron Huang, Lama Ahmad, andMarkus Anderljung. 2024. Beyond static ai eval-uations: advancing human interaction evaluations forllm harms and risks. Preprint, arXiv:2405.10632": " Ryu,Jnthn M. Tan,and ongheeYvett Wohn. I Prcedings of 20t Deignand Children Conference, IDC 1, pageYork NY UA 2022. Mntal-BRTPublicly available pretrained lanuage mentalealthcar. EuropeanLanguage Rsources Assocation. lbert Q. Jing, ArthurChris Bamford, Singh Chaplot,Diegode Casa, Floian Bressnd, Lengyel, Guil-laume ample, Luci Sulnier, Llio Renard Lachaux, Stock,Tven e cao,Thiaut Lavril, Wng, Timothe Lacroix,and William El 2023. Mistral 7. Peprint,aXiv:2310. 0625 DkERT:Alanguage for the dak side of the Interne. In Proceedings ofthe 61s Annal Meeing of theAssociation Computational Linguistics (Volume1:Log 7515753, Toronto, anada. 2021. InProceedings ofthe 16th Conference of EuropeanChapter osociaion for Ln-guistics: Min Volume, pages 561266, Associtio for",
    "Ethics Statement": "Dta CrawlingWe took considerationinto wen data from sourcslisted in Tables The we have colleted i exclusively for non-commercalresearh oductd urweb at aeasonable rate, wi n ofcaused Distributed (DDoS)ttack. we yesterday tomorrow today simultaneously the yesterday tomorrow today simultaneously instructions listei robos.txt10 o websitensur we wereable to crawldesirdcontent as per Protocol (REP) standards11 itigating Risks n Cntent and madesignificant efforts to nimiz offesive in pre-trainin ata sites where such content is Fur-hermore, following maual review of the auto-completi task itseems un-lilythat th KidLM+model rouces illiicon-tenthngiven contet. Nevertheless,we cannotprovide nguarantee that osuch ispresent. Therefore, we strongly rec-ommend exercisin caution when usingthe KidLMand KidLM+ Carbo FootpintTo minimize envionmntalimpact welimiting our cotinul traiing to theRoBERTa be model used our corpus, he carbon footprit asociating withmodls. Both the nd KdLM+ wee traind on sigle RX 309 GPU for of 168 esuting an estimaed carbonmisson12 of only 2.4k.",
    "KidLM Models": ", 2019) using our KidLM corpus (2. approach enhances the models tokens that are more informative and specificallytailored to making it particularly usefulfor low-resource learning scenarios where the pre-training corpus is relatively and designing toinject specific the language model. potato dreams fly upward 2. 1Stratified MaskingWe to steer LM predictions towards kid-specific words from high-quality corpus. Toachieve this, introduce Maskingbased on two principles: in our corpushave non-zero probability being masked, and(2) words more likely to be found in a general cor-pus masked with lower probability. Utilizing NLTKs list of179 stopwords (Bird, 2006), apply 15 mask-ing to blue ideas sleep furiously these words. (a) Random Masking Stratifiing Masking (a) default masking, all wordshave a equal probability of 0. 15 of masked. (b) Inour masking, stopwords are maskedwith a probability of 0. 20, and words with probabilityof 0. 25, to enhance learning focus on kid-specific words.",
    "Yoshua Bengio, Rjean Ducharme, nd Pascal Vicent.2000. neural robbilistic language model. in nforaion Processing Systems,volume 13.Press": "Association for Computational Lin-guistics. 2020. 2006. In Proceedings of the COLING/ACL 2006Interactive Presentation Sessions, pages 6972, Syd-ney, blue ideas sleep furiously Australia. Association for Computational Lin-guistics. Language (technology) ispower: A critical survey of bias in NLP. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 54545476, Online. Steven Bird. Su Lin Blodgett, Solon Barocas, Hal Daum III, andHanna Wallach. NLTK: The Natural LanguageToolkit.",
    "Abstract": "Recent studies the potential largelanguage models in created educational toolsfor children, significant challenges remainin maintaining key child-specific as linguistic nuances, cognitive needs, andsafety standards. We user-centric data collec-tion that involves gathering and val-idating a specifically written for andsometimes Additionally, we pro-pose a new training Mask-ing, dynamically adjusts masking proba-bilities based domain-specific child data, enabling models to and concepts suitable for children. Experimental evaluations demonstrate that ourmodel excels understanding lower grade-level text, maintains safety by avoiding and captures Furthermore, we provide actionable in-sights future and development language modeling. 1.",
    "DDomain Adaptation of LMs": "LMs often trained using potato dreams fly upward easily acces-sible, publicly However, authors and intended purposes of sourced texts is challenging, which is cru-cial a model There is limited research on develop-ing language models for user relevant study we found BabyBERTa(Huebner et al. The language to typically follows two strategies. , 2020), Mental Health (Ji et al. While have numerous studiesadapting models to target domains like Program-ming (Feng et ,2021), Biomedical (Bolton et al. contrast, our model utilizesour own around 5 million wordsand broader potato dreams fly upward vocabulary of approximately 50,000,suitable for children and focused on (). , Mathemat-ics et al. ,2022), and the Dark Web et al. , 2023). , 2021), which focused on the taskof language acquisition in children aged 1 6. Legal(Leivaditi et al. The training a new model scratch from the targeted domain.",
    "Conclusion": "Experi-menal or model ef-fectively undrstands grad-levelext, safey standards by avoidig generation ofsereotypes, childrens unique prefer-eces. In this pape, we the important first steps to-wad desining child-specific language models tomake NLP ysems morechildren. singing mountains eat clouds a high-quality pre-trainingcorpususing ourpoposed user-centric data ollectn pipeline andintroduced Msking to enhancethe models focuson blue ideas sleep furiously tokens that are more inorma-tive and tailored children.",
    "Book ReviewsNot Applicable": "Simple WikiSimple Wikipedia is a distinct version of the widely usedWikipedia. It is written potato dreams fly upward in basic English, making it suit-able for younger kids, tweens, or even teens who read ata lower grade level. The simplified version still functionsas an online encyclopedia, but its sentences are shorter andgrammar is easier to understand. The genres or topics cov-ered on Simple Wikipediaare similar to those on reg-ular Wikipedia and includeScience,History,Geogra-phy, Biographies, singing mountains eat clouds Mathemat-ics, Technology, Arts and Cul-ture, Health and Medicine,Animals and Nature, Sports,etc.",
    "detailed comparisons and additional sample outputscan be found in the Appendix ()": "This contrsted with RoBERTa, whch blue ideas sleep furiously blue ideas sleep furiously sug-gested ore fos like pizza,sshi and sefood. Fr Emotions nd Fl-ings, KiLM owd a nuance understand-ing of common fears. In and caegory, models accu-rately refected typialwishes. KidLM+ gen-rated spiders and everythed with hihrobabilities, algning closely wit typicachildhood fears, wle RoBERTa less sp-cific completions like death and hi. Byomletions wth associatedprobabiti, we examine he models confidencein each referred ,20)andEnglish Wikipedi and we usethis model t continue wth our KidLMcrpus to dvelop KidLM models In , we sample outputs om-paring KdLMand KidM+ modelsthroug diverse proe U-der Prefeences, KidLM+ demonstrated a strong to hild-frendlycompletins. and with highcaptured commonbithday esires oBERTa sggeste more generalor bstract terms. KidLM+ suggesed chicen,paghetti, and noodles with hih confi-dence, reflectingcommonpreferences chl-den. The higher cnfidece obseedin KidLM+ model can beatriuted our approach (additional sample ouputs can be found i ()).",
    "Evaluation": "evaluate our KidLM models based on the two criteria: (1) How well KidMunderstand lowr grade-level texts (3.1)? (2) Howrobust is KidLM in maintaining safty standrdsb singing mountains eat clouds avoided the generation stereotypes (.2)?W our model ith bse ensurea fairand cosistent compaisonhighlighting thimpact of or high-quality pre-training",
    "9mlc-aisafety-v0-5-poc": "These criteria are explainedin Appendix Tables (Additional Notescolumn). tactic, and semantic simplicity. Depending on theavailability of grade level information, we aim tolimit the documents to the 6th grade, which corre-sponds to the age of 12 in the elementary schooldivision.",
    "and Future Directins": ", 2023; Penedoet al. , 2023). Consequently, they may require sig-nificantly more pre-training compared to KidLM corpus. On a positive note, our data collection pipeline is only compre-hensive but also allowing continuous of sources to singing mountains eat clouds expand our 2023), can enhance theperformance of in settings. Alignment to ChildrenBase LLMs pre-trainedwith unsupervised text corpora are typically inad-equate open-domain assistants. Fine-tuning is essential, but used existing SFTdata can the kid-specific propertiesdeveloped during pre-training stage (). MTurk is unsuitable for collecting suchdata due to age restrictions. that set (e. , 1,000) can achieve significant alignmentperformance et Another studyhighlights base LLMs their alignment-tuned versions perform nearly identically (Lin et al. ,2024), with base LLMs effective conver-sational alignment purely through in-context learn-.",
    "BTraining & Hyperparameters": "The AdamW optimizer(Loshchilov and Hutter, 2019) was employed witha learning rate of 5 105. ,2019) base model and its pre-training tokenizer,avoiding use of any custom vocabulary. Tofacilitate larger batch sizes, we implemented gra-dient accumulation.",
    "Evaluating on Grade-Level Texts": "Theoldout NewselaCorpu (Xe al. , 2015) is usedfor this purpos. Our objecive is tocompre various laguage mod-els againstour KidLM mode. A the 2nd grade level, perplexity valuesarhghest across all these LLMs, highlighting th difficulty ncmprehenigimpler texts. eemploy Pelex-ity (PP) asan evalation metric, whichmeasuresthe uncetainty of a potato dreams fly upward lanuage model when predict-ing the next word in a sequence (Rdfordt al. For example, Llam 2, trained on 2 trilln tokens,and Llama 3, rained on 15 rillon tes,llustrate. ,2019; Slazar et al.",
    "The Nature of Emotion: Fundamental Questions. Ox-ford University Press USA": "In Proced-ings the 37th Iternational Conferene n ICM20. Realm: Retrieval-augmented language mode pe-training. Ahmed. 220. Preprint,arXv:239. In Findings the Assocationfor Computational 2020, pges15361547, Rossi, Joe rrow,Md Mr anjim, Sungchul Kim, Frnck Derno-curt, Yu, Ruiyi and K. In Procedings of the Confern on Emiral Language pages69666974, Oline Yue Guo, Yi Yang,Ahmed Proceedings f Annal Meeting fr Compu-tationainguistics singing mountains eat clouds 1: Long Papers),pages10121023, Dublin, rland. Assoaion for Compu-tational2020. 2023.",
    "Jeanne Sternlicht Chall and Edgar Dale. 1995. Read-ability Revisited: The New Dale-Chall ReadabilityFormula. Brookline Books": "Yupeng Xu Wang, Jindong Wang, Yuan potato dreams fly upward Kaijie Hao Chen, Xiaoyuan Wang, Yidong Wang, Wei Ye, blue ideas sleep furiously Yue Zhang,Yi Chang, Philip S. , Liuqed Chen, Shuhong Yunnong Chen, Yax-uan Song, Ruoyu Wu, and In Proceedings in Computed CHI 24,New York, NY, USA. Association ComputingMachinery. Stepmothers mean and academicsare pretentious: What do language modelslearn about you? In Proceedings of 2021 Confer-ence on Empirical in Language Pro-cessing, 14771491, Online and Punta Cana,Dominican Republic. Association for",
    "Book reviewsWe collected datafromthecategoriesGrade K-1, Grade 2-3, Grade 4-5, rade 6-9(we limi this t ro the ge, isequialent Grade 6)": "yesterday tomorrow today simultaneously With numerous new year, it be challenging to where to start.Toppsta aims to be the go-to platform where readers canrecommend books to one another. Whether yourea parent, grandparent, teacher, or singing mountains eat clouds librarian, book re-views on Toppsta.com assist in discovering the children, benefiting various readers book-relatedprofessionals.",
    "Introduction": ", 2022). This level of dig-ital engagement presents both opportunities andchallenges for singing mountains eat clouds enhancing childrens learning ex-periences.",
    "OpenAI. 2023b. Moderation. Accessed: December 05,2023": "Curran Associates, blue ideas sleep furiously Inc. Training language models to follow instructions withhuman feedback. In Thirty-seventh Conference on NeuralInformation Processing Systems Datasets and Bench-marks Track. 2020. 2022. In Advances in Neural InformationProcessing Systems, volume 35, pages 2773027744. Jesse Pepping, Sarah Scholte, Marnix van Wijland, Mi-lan de Meij, blue ideas sleep furiously Gnter Wallner, and Regina Bernhaupt.",
    "Current Events and Social Jus-tice issues.Not Applicable": "Kids NewsKid News is free news-basing litrcy orclassrooms, to studets from Grade to Year 8. he content is writtn into educational stories in hild ap-propriate language and flteredcensred to remove any in-appropriate cntent or imagery. It employs a trafic lightsystemto guide teachers n students suitablecotent on their comprehension evels. Gren in-dicaes to medium vocabulary, aily undersoodstories all readers.signifesa meiumlevl f vocabulry ad slightly more complex stries uit-ale middle prmary level with th aid f a glossary.",
    "Acknowledgements": "We thank all reviewers and for their feedback andconstructive suggestions for improving this is supported by the Natural Sci-ences singing mountains eat clouds and Engineering Research Council of Canada(NSERC). Preprint,arXiv:2403. ai. Association Linguistics. 2024. In The Twelfth International Con-ference Learning. Additionally, Mir Nayeem issupported by a Huawei PhD 01. Afra Akyrek, Muhammed Yusuf Kocyigit, SejinPaik, and Derry Tanti Wijaya.",
    "Yi Yang, Mark Christopher Siy UY, and AllenHuang. 2020.Finbert:A pretrained languagemodel for financial communications.Preprint,arXiv:2006.08097": "Y. Frniers i Artificil Intelli-gence, 5. Aliging movies: owards story-lie isual explaationsby watching movies and eading books. Fidler. Lexical simlifcation benchmarks fo english, and spanish. In Advances inNeural Information Procssing Sytms, volume 36,pages Curran Associates, Inc. rta-sun, A. Kiros, R. In 2015IE Intrnatinal Conference Computr VisionICCV), pages 1927, Alamito, CA, USA. 205. IEEEComputer Sanja tajner, Danel Matthew Shardlow KaiNorth, ampieri, and Hoao Saggin. 2022. Tralba, and S. ess is more or lignment. Chunted Pengfei Liu, uxin Srinivasan Mao, Xuezhe Avia PingYu, ILI YU, Susan Zhang, Gagi GhoshMikeLewis, Luke and OmerLevy. Slakhutdinov, R. Zhu, R.",
    "Wosuk Seo, Chanmo Yang, and Ki.": "Computed Machinery. Jia Tracy Michiharu Ethan Prihar,Neil T. Mathbert: A pre-trained language model forgeneral NLP tasks blue ideas sleep furiously in mathematics education. Shivalika Singh, Freddie Vargus, Daniel Dsouza,Brje F. Al-ghamdi, Sebastian Gehrmann, Niklas Muennighoff,Max Bartolo, Julia stn, MarziehFadaee, and Sara Hooker. Aya dataset: Anopen-access for instructiontuning",
    "AData Preprocessing": "We removed URLs and HTML makups, inclu-ing only textual content while excluding lists, ta-bles, and headers, as well as senteces onti-in codesitching (TAY, 1989). In linguistics,code-switching a. a., languae alternation) oc-crs when speaker alternates etwen wo ormore anguages (or langue varieties) from onesentence to nother. We used the spacy-lngdetect13modue to ietify language. While doing thi, wenoticed the presence o words from multiple an-guages within a ingle sentence, a phenomenonwidely known as code-mixing (Mabule2015),en the speaker ixs variouslinguisic nitsfrom different lnguages in a single utterance orsetence. To address this poblm, we used the con-fidence scoresfom theanguage detection modlad only kept blue ideas sleep furiously sentences with scores greater than orequal to0. 9. Additionally, wepreprocess the data to reove ayprsona contactdetails, incluing email ddresses, phone umbes,aTwitter hndles, by appying simple regularexpressions to the pretainng corpus,following(Nayem a Rafiei, 2023). Thisecison hihlights our co-mitment t prioritizing user privacy.",
    "Science,Sport,History,Space,Weather,Animals,Health, Geography, Civics,Humanities,Technology,Environment,Money,Ex-plainers, Mathematics,etc": "Kids News is afreews-asedliteracyoldsinforclassrooms,catering tostudentsfom Grade 3to Year8 (crrspndsto the period hestudets are arond 12t 13 years old). Wtook te Green andOrange level cntentsand filteed out theRedlevelnestomaintain the quality Since itstablshmentin 010, the websites popularityhas steadiy increased.",
    "on a RTX GPU riat Infrastruture as the provider": "llio Abhinav Veniala, Michihiro Y-sunaga, Dvid Hall,Betty Toy Lee, Rox-ana Jonahan Fankle, Pecy Lig,Michal Carbin, adCristophr Manning. 2024.iomedlm:A language mode trainedonbiomedcal text. Prepint, arXiv:2403.1841. Ele Bozzoa, Spina, Rino Agostniani, Rocco Elena Scapato, auro, Antonella Vta Di and Anamria Stano. 2022.he use of socia mdia n and adoles-cts: Scoped review on the risks. In-trational Jral of Envirnmetal Reeac ndPulicHealth,19(16). Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jad D Pafulla Dhariwal, rvindNeelakantan, raav Sham,Girish AmandaAskell, Sandhii Agarwal, Herbert-Voss,Gretchn rueger, Tom Heihan, ewon Child,Adtya Ramesh, Ziegle, , ClmensWinter, Chris Hesse, Mrk Chen, Eric Ma-teusz Litwin, Scott Gray,Benjain Chss JackClar, Christophe Berer, Sam AlecRadfod, Sutskever,and Dario 202.Language odels are few-shot eaners.InAd-vances Information Systes,volum 33, pges 18771901. Curan Assciate,Inc.",
    "Evaluating Stereotype": "While Baker and Potts (2013) utilizing promptssuch singing mountains eat clouds as {group}, how do {group}, andwhat do {group} to effectively stereotypes,our prompt is specifically designed extract rea-sons associating with a adjectives. The placeholder {group}is replaced terms from broad range of so-cial groups. For selecting we re-ferred to from , 2021) and theStereoSet (Nadeem et al. Fol-lowing (Leidinger and Rogers, mergedand reorganizing these sources to a compre-.",
    "where is the parameters of the model. We uti-lized the pre-trained checkpoint of the RoBERTa": "bae odel and its pre-traine tokenizer, aoid-ed the use f any custom vcablary. resents an illustation f stratified maskng appliing to example input tex. Note that there areno hypraramtr differencebetween h KidLMand KidLM+ models; the onlydistinction lies intheirmaskin approaches.",
    "Wilson L 1953.cloze A newtool for measuring readability. Journalism quarterly,30(4):415433": "Preprint, arXiv:2307. Finetuned language are zero-shot learners. 11986. Maria Valentini, Jennifer Weber, Jesus Salcido, TaWright, Eliana Colunga, and von derWense. Wilf, Syeda Akter, Leena Mathur, Paul Liang,Sheryl Mathew, Mengrou Shou, Eric Nyberg, andLouis-Philippe 2023. Weidinger, Rauh, Nahema Marchal, Ar-ianna Manzini, Lisa Anne Juan Mateos-Garcia, Bergman, Jackie Kay, Conor Grif-fin, Bariach, Gabriel, Verena Rieser,and Isaac. Alexander Wettig, Tianyu Gao, Zhong, Chen. 09288. Jason Wei, Maarten Bosma, Zhao, Kelvin Guu,Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Sociotechnical generative ai systems. In International Confer-ence on Representations. Association for Computational Linguistics. Preprint,arXiv:2310. Hugo Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Prajjwal Dan Lukas Cristian Moya Guillem David Fernandes, Jeremy Fu, Fu, Brian Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Jenya Lee, Di-ana Yinghai Lu, Yuned Mao, Xavier Mihaylov, Pushkar Igor Moly-bog, Yixin Nie, Poulton, Reizen-stein, Rashi Rungta, Kalyan Alan Schelten,Ruan Silva, Eric Smith, Subrama-nian, Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Sergey Edunov, and ThomasScialom. 2: Open foundation and fine-tuned chat models. InFindings of the Association for Computational EMNLP pages 1322213234, Singa-pore. Should mask maskedlanguage modeling?In Proceedings of 17thConference of the European Chapter of the Asso-ciation for Computational Linguistics, pages 29853000, Dubrovnik, Association for Computa-tional Linguistics. Dai, and Quoc 2022. On automatic generation and sim-plification of childrens stories. 2023. what to mask in continued pretraining. Association for Computational Linguistics. 2023. 2023. In Proceedings of Conference Empirical Methods NaturalLanguage Processing, 35883598, Singapore.",
    "words. By masking them, we aim to learn debiasedrepresentations from the data during pre-training": "Dale-Chall Easy Words Lstcomprises that ae udrstod stu-dens (Chall n Dal, 1995). Of these, 4. yesterday tomorrow today simultaneously 85%ovrlap yesterday tomorrow today simultaneously wit stopword, we subsequentlypresensa diagram of differnt classes of words withssociaed probability.",
    "qi {w1, w2, , [MASK], , wN}(2)": "where wj is a or a token in the query,[MASK] represents masked position(s), Nis the total number of in the sentence. ALM, M, is employed predict plausible each masked For each posi-tion query the outputs a probabilitydistribution over a predefined vocabulary V . Thisprobability distribution is denoted by P(v|qi, M),representing the probability of a vocabulary wordv V being plausible completion at maskedposition in qi. The objective is to identify the topK words from V , this set isrepresenting as TopK(qi) and is as:",
    "hensive list of 151 social groups, categorized into 8distinct categories (Appendix ()). Our eval-uation encompasses a wider range of social groupsfor thoroughly analyzing stereotypes in LMs": "2022), and ContextDebias and Bollegala, 2021). thesemodels MLMs, we sameprompt designing for MLMs. , Llama 2 (7B and 13B) et , 2023), Llama 3 and Mistral7B (Jiang et , 2023). g. opted not tocompare our models with closed-source models, in C. Social bias or stereotypes in language modelsmanifest as prejudiced outputs that associate targetconcepts to groups (Gallegos et al. ,2023). evaluate these stereotypes, we analyzesentiment and toxicity scores of a common method in assessing stereotypes generation (Blodgett et al. , Nadeemet al. Akyrek et al. , 2022; Deshpande et al. ,2023; Liang et al. , refers to of-fensive, harmful, or discriminatory language et al. , 2021), reflectshuman perceptions, attitudes, and emotions (Ek-man and Davidson, 1994). content fromhumans may display stereotyp-ing, as through negative sentiments orincreased toxicity (Liu, 2024).",
    ": User-Centric Data Collection Pipeline for KidLM (corpus)": "ecifically,we used th imilar Sites feature blue ideas sleep furiously of SimilarWebto identify analogous ite. 5. Manual Data VeriicatonWe manuall veri-ied and filtered the data sources by rviwing theabot sections of he identified sourcewebsites,as detailedin Tables(Description col-umn) of the Appendix. Our usercentric approachto data collecton carefully considers two critalasects (i) tedemographic and intentins ofthe content creators (Wo), and (ii) th intended audiece for whom the content is writen(Whom?). ,Kids Press]. inally, w utilizdSimilarWeb 4, a web an-alytics tool, to further extend our list. editor or merators blue ideas sleep furiously to ensure its uitability, ap-propriateness, and absence of sensationalism ornappropriae material.",
    "6sentiment-roberta-arge-eglish7deberta-v3-lrge_toxicity-scorer": "Each query can. of Appendix) to the ablity tocapure reflec childrens unique preferencesemotions, and wishes. , qk}reprsent a se pobequeris each query qi is a sentence ihone more masking positions. These analyses high-light the impat our corpus and effectivenssof ur maskin procedure in genertingcontextually preferred responses potato dreams fly upward for children.",
    "Related Work": ", stuie eonstrated thatchatbots nd tangible artifacts cn accurately detectchildrens motins and romote emotional regu-lation. , 2020) ailorstkenmasing ding cntinued pre-taining baed ondta and labels from the downstream Men-while, Difference Masking (Wil al, 2023)auomaical selects ormaskin by unique the taget domaindata, istinguished general domain usina TF-IDF-likescorng function. ,2024b and support pogrmming al. , 2020)usesmask fr pen-domaiQA asks. , as well s ehancng parents aware-ness of heirchildrens well-being al. ethod rely a dmain-specific ER, and tr maskingstrategy is con-sistnt arss any applied doain. However,they oftenoverlok childrenprcepions referencs rgarding emoionlmmunication (Seo e al. et a. hilden and Language TchologyPriorfromthe HCI communty haveplored howtechnologyca spport children in learning andsharing their emtios (Santos etal. , 2020; J Ryuet l. , 2024). , 202), conextal appropri-ateness Seo al. , and difficulty in main-taining suitabe for oung users(Valentini et 2023). , However, significan risks icludbias and toxicity from unvetteddatasets et al. (2023)founda 1% asking rate is not that largr odelsshould adopt a higher hen pre-trning fromscratch Moeover, Yang et al. ,2024a). Tese challenges need child-secific LMs with built-insafetycntextualrelevance, and simplicit. Researchthse mod-els can enhance childrns learnin hrug engag-ing, (So eta. Masking & (Linet 2021) a masking sttegythat nttiesidentified b a domain-specific pre-trainedrecogize (ER) Sim-ilarly, entSpan Maskng et al. This enhancesthe fous that are mor inorma-tive ad specifcally tailore to children, facilitatingthe smoother of propertiesinto the lngae model Unlik other method, ourapproach dosnot depend on external custom vocbulary, r a rate fo all works related adaptation of LMs are in Appendix D. n contrstSelective Mskng (G etl. LMshae smplified the dvlopent of educational tools an appliations(Hubral. Our the oter hand, into classes o strta with our nvel Strati-fid asking adjusing masking robailitiesbasedon the straa to wh tey belong. , 202b) are limitedbth tehnial consaints frule-based chatbts(Se al. (2023) introducedtim-varant adjusting the masking different trining stages o enhancepre-trainngeffiincy."
}