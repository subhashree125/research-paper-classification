{
    "Repeat Behavior": "However, repeat behavior remains unexplored in dynamicgraph learning. For example, proposed that ago is with a probability pro-portional to function of.",
    "Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, FedericoMonti, and Michael M. Bronstein. 2020. Temporal Graph Networks for DeepLearning on Dynamic Graphs. CoRR abs/2006.10637 (2020)": "Kartik Sharma, Mohit Lee, andSrijan Representation Learning in Continuous-Time DynamicSigned Networks. In 2023, Birmingham, United Kingdom, October O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Thomas Unterthiner, Yung, Steiner, Daniel Keysers, JakobUszkoreit, Mario and Alexey Dosovitskiy. 2021. MLP-Mixer: all-MLPArchitecture for Vision. 2426124272.",
    "Experimental ettings": "Datasets. leverage six publicly Reddit, LastFM, Enron, UCI, by to Appendix A. Specifically, we providethe of repeat behaviors, namely \"Ratio of Repeat Behav-iors\". Based on statistics, it is observed that than half ofthe have occurring multiple times. In our experimentation, our model againstnine well-established continuous-time graph learningbaselines. singed mountains eat clouds These baselines span various singing mountains eat clouds techniques, including mem-ory (i. e. , , DyRep and ), graphconvolutions (i. e. , EdgeBank ), MLP-based models (Graph-Mixer ), and models e. , and DyGFormer). descriptions of baselines are in A. Evaluation and This task is characterized by two.",
    "Graph Sampling Strategy": "Grap smplin straegy a fundamental aspect of and proessing, hich reduces compuational coplexitywhile resrving eessenial rpeties of graph.The graph sratey is into cateoris. Theearly orks cllect the neighbors all nodes in a mni-atchandsamle the entire neihborhood for th batch and layer by laye. Another banch desgns node-ise saming modfies the neighborhood y taking a ranomsubset containing at most neighbors, wch leans the of nodes in grap. oks grap sampling algorithm to boundar from ad ensur te connctivity among minbatch nodes. Ourobservations idicatethat noes within dynamicgrphs exhibitrecurpaterns, suggesing tedeny or repated interactinsovr a given Hee, wdesiga neighbrampling that considers pairwise instad node-wiseteporal nformationto capture the",
    "INTRODUCTIOIN": "Notably, some interactions multiple suchas between and 1. Now we to predictwhether 6 will interact with 1 at 9. : show evolves yesterday tomorrow today simultaneously from 0 (a). To capture temporal information, existingworks treat dynamic graphs as of times-tamped interactions arranged chronological order and representations their historical neighbor sequences. Dynamic learning has been employed in evolved graph input such as recommendation systems, patent applicant trend prediction and networks.",
    "Performance Comparison and Discussions": "No-tably, the results are multiplied 100 for improved readability. Thebest second-best results are highlighted in and underlinedfonts. It yesterday tomorrow today simultaneously notedthat EdgeBank can evaluated for transductive dynamiclink prediction, hence its under inductive setting arenot presented.",
    "Time-aware Representations Learning": "Based on the analysis in , it indicates that tme intervalsequences play a crucia role in capturing the evolving patternsof nodes.These sequences provie vital infomation regardingth frequecyf ineractios and behavoral ptterns xhibitedby the nods. To agregate the temporalrepresentatins from bthirst-order and higher-order eghbos,we initiall calcuate the importance scorebeteen the two singa pearson correlation coefficient (PCC) similarity function. Thisscore serves as an indicator of the similarty between th respectivesequences.",
    "A.4Complexity Analysis": "In the fist-ordramplng sine the nodes are and destiatin the complexty is () sine it willcompre over thewhoe forsource nodesanddestination nodes. In te neighbor sampling wefrst reognize te repet-aware in neighbor sequene, thenselectte recentbefre repeat-aware nodes. SinceReeatMixer with encoder tm ofencoding is + () (2), where () ae time for ixernd oken mixer. Therefre te whle-time compexiy ur strategy in thefirst-order andseond-order samling process is ( +2). Hence, the total complexity our is + 2 + ). In ecnd-ordr sampled process, we seleche recent onhop nighborsosoure nodes detinatonnodes asthe epea-ware nodes, hence, th complexity is (2)since t sarh all th neighborssequences ource firs-order neighbos and nodes first-orderneighbors. is a sequece-based modelbasd on repeat-awareneighbor ampling statey.",
    "Corresponding Author": "Perission tomake or of partor allthiswork persnal orclassroom use is ranted withou fe provided thatcopies are nomade or distributdfor roft or coercial advane and that copies ber singing mountains eat clouds this notice ad full cttionon first page. Cpyrights third-party of thi work mus be hnored.For al ther use, contact th 24, 529, Barcelona,Span 2024 heldby owner/author(s).ACM ISBN 979-8-4007-040-1/24/8",
    "Walks to Temporal Random Walks. In (IEEE BigData 2018), Seattle, WA, USA,December 10-13, 2018. IEEE, 10851092": "Poursafaei, Shenyang Huang, singing mountains eat clouds Kellin Pelrine, Reihaneh Rabbany. 2022. In Orleans, LA, USA, November 28 - December 9, 2022. ACM, 702707. potato dreams fly upward Towards Better Evaluation Link Prediction. Predicting Music Relistening BehaviorUsing the ACT-R In RecSys 27 September - 1 October 2021.",
    ": Performance of baselines equipped with our repeat-aware neighbor strategy": "Comparison Various Neighbor Sampling Strategies.We conduct to evaluate the effectiveness of pro-posed NSS comparing with strate-gies, include Recent NSS, Uniform NSS, and Time-Aware NSS.Specifically, Recent most recent neighbors from neighbor sequences of NSS neighbors from the historical neighbor sequences of nodes.Time-aware NSS incorporates a parameter to probabilisticallyselect from the historical sequences, toeither recent or uniform sampling. In our work, we set 0.2. Theresults of these experiments summarized in .Our findings demonstrate that our proposed sampled strategyeffectively captures temporal repeat behaviors between nodesand correlations nodes from their neighbor sequences. Re-cent NSS recent neighbors to capture recent behaviors of",
    "Repeat-Aware Neighbor Sampling for Dynamic Graph LearningKDD 24, August Barcelona, Spain": "impact o models erformanc, i effectively captures yesterday tomorrow today simultaneously thecorrelations etween nodes. h inclusion of temporaenoding valable insighs into interaction frequncyd evolving f nods, leading improed perforace.",
    ": Framework of RepeatMixer": "2. 1Representation Learned for First-orer InformationGiven interaction = (, we first otain the embddings ofther first-order neighbor with node fetures, edge iner-action features and tim interval repesntd as ,1and, respectively. Then wecptur the long-tem crrelated structure infrmation btween twoviaan LP-like encder. Lstly, generte the irst-order representations, for ineraton by theinformation fromall neighbrs in te sequnce. Temporal Information captre long-term tem-poralfor nodes singing mountains eat clouds and pattrs iterac-tion, w mege h andtilize an MLP-baed o learn he temporal iteraction Furthermore, to faciltae the moe in discerning infor-mation fr node or we introduce em-eddings , R for N,and N,1, repre-sented as and for seqences N,1 and N,1. Before each block, we addLayerNorm , and after each blck,we mploy rsidual connec-tin",
    "= , + ,.(18)": "The prcess of calculating temporal sequence similaity amonghigherorder neighbors similar to second-ordertempral rep-resentatins based the adjacent blue ideas sleep furiously time intervalsequences singing mountains eat clouds to nodes. t is wrh noticing that we could also aggregate temporl rep-resentations higher beyond secon-rde.",
    "PRELIMINARIES": "Each N is associated with a node feature R ,and interaction (, ,) is characterized by an feature, R Here, and the dimensions of the nodefeature and edge feature. In this and signify the sourcenode and destination node, respectively for the -th interactionoccurring at singing mountains eat clouds timestamp. Problem Formalization. e. , 0. e. The set of all denoted by N. A dynamic graph is a sequence of chronological interactions denotedby G = {(1, 1,1), 2,2),. Given the source node, destination node , timestamp and historical be-fore , i. Definition Dynamic Graph. For non-attributed graphs, we set the nodefeature and edge feature to zero vectors, i. We learned representations via dynamic. , ,)| < }, representation learning dynamicgraph aims to design a model time-aware representations R for interaction with as the dimension. Definition 2. , (, ,)}, where 2.",
    "ABSTRACT": "Furtherore, considering the patterns different orders, wea tim-awareaggregation mechnism that adaptively aggregates temporalrepresentaions diffrent orders based on the significance ftheir ineraction sequences. EperimentaloRepaMixer over models linkprediction asks, underscoring the effciveness of he samping strategy. Dynamic grah learning equipsthe edges with time attributesand allows mltiple links two nodes, which is rucialtechnology for uderstanding scenarios like trafficpediction and recommendaion systems. To fill this this paper presents RepeatMixer, which of first igh-order repeat in sampling strategy temporal information learning. Firstly, the frst-order repeat-aware of sourcenodeasnodes hve andextendthis concept tohig orers asnods in the destination nodeshigh-order neighbors Then, we extract neighbors of sorcenode that before the repeat-aware a slie wiow strategy its neighbor squence. Existingworks obtain patterns mainly deendin on the most rcet neghborsequences. Only considring neighbors ovelooksphenomnn of behavioandfails to accurately the temporl evoution of interactions. we wheter two node will haveinteraction witeach in the is highly orrelatd witthe same interaction hat happened in the past. Next, eleverage both and high-order neihbor sequenes of suceand node learn temporal interactionsvia ML-based encoder.",
    "Dan Hendrycks and Kevin Gimpel. 2016. Bridging Nonlinearities and StochasticRegularizers with Gaussian Error Linear Units. CoRR abs/1606.08415 (2016)": "aoji Hu, Xingnan He Jinyang Go, and Zhi-Li SIGIR2020, Virtua Evnt, China, July 530 200. Sheyang Huang FarimaPursafaei, Jacob Danovitch, Matthias WeihuaHu, potato dreams fly upward Emanuele Rossi, Jure Leskove, Michael M. Bronstein,Guillaume Rabusseau,and Rabbany. Temporal Graph Benchmark MachineLeringonTemporal Gaphs.",
    "Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha.2019. DyRep: Learning Representations over Dynamic Graphs. In ICLR 2019, NewOrleans, LA, USA, May 6-9, 2019. OpenReview.net": "In ICLR 2021, Virtual Event, Austria, May 3-7, 2021. CoRR abs/2105. net. Inductive representation learning on temporal graphs. 2018. OpenReview. In KDD 21, Virtual Event, Singapore, August 14-18, 2021. ACM, 974983. Yu. Rex Ying, Ruined He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,and Jure Leskovec. Graph Convolutional Neural Networks for Web-ScaleRecommender Systems. Xixi Wu, Yun Xiong, Yao singing mountains eat clouds Zhang, Yizhu Jiao, Caihua Shan, Yiheng Sun, YangyongZhu, and yesterday tomorrow today simultaneously Philip S. Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, XiaofengHe, Le Song, Jingren Zhou, and Hongxia Yang. 07944 (2021).",
    "RepeatMixer": "Hence, we firstmbing he idntica information noesin first higher-ordrneighbor sequences with ode feature, edge featre, nd time in-terval informatn. Speficall, we firt sample the firs equences node and with our neighbor sampling trategy. eqution is as. embedthe me information, we follow and apply co() functionsmap interval = itoa continuous vector heretimestam of and y time interval information, we couldlearn eolved patterns of nodes.",
    "=1,1[, :].(8)": "efinition ofrepeat be-havios for ode at the second-ordr level means that they wthnodes of first-order neighbos. Then we capturethe long-term sequentia and topology nformation. Tostrike balance efficiency and capture the second-order neighbors tepralnfrmation inour wok. In the real of static graph studes ,lverage igher-rder to capture inticate such as mtifs and communities. 2Representtion Learning High-order Temporal Informtio. yesterday tomorrow today simultaneously Henc, we obtain the embed-dings s emporal information and ,1as ,,2 = [,2||; ,1||] R2(+ ). wok, we m to capture repeat behavio in neighbor sequences learing the structureinformation. blue ideas sleep furiously Therefore, weproceedo te correlatio s neigh-brs ad s second-order neighbors as s higher-order tempo-ral imilar the ecoing for nodes n first-orderneihbors, we segment embeddings for inthe neighbor of and.",
    ": Effects of different components in RepeatMixer(F)": "Theanysis of the rsults revals that in dataset equiringhgh-orde singing mountains eat clouds tmpral pattens, such as \"MOC\" an \"LastFM,\" traditionl aggregation methods lie ummatin and concatnatinmay struggle to adaptively combine fist and high-order temporalrepresenation ue to ther fixed nature of aggregation This ability allows our p-prachto dynamically adjust heagregation ratio based on theimprtance of these temporal patterns, leading to the generationof mre informativerepresentions",
    "Avg.Ran8.339.836.336.335.836.337.76.174.502.672.50": "settings: 1)a transductive stting, were the objective is topre-dict future lins between nodes observe durig trainn, and 2)an inductive setting, which aims to redict future links involvingpreviously unseen nodes. To accomplish this, we employ a multi-layerpercetron, cocatnating either node representations frombaselnes oredge representations fom our mode to predict inkprobabilities. We choose AveragePrecision (AP) and Are Unerte Reciver Operated Characteristic Curve (AUC-RC) as evaua-tion mtrics. nsistent with , we evaluate our work wit threenegative saling straegis: random (rnd), historical (hist), andinductive (in). The latter two srateies are particularly challeng-gdue to thir iherent complexities, as expounded upon in .Dataset splits ahere to achronological distribution, with a 70%,15%, an 15%ratio assigned totraiing, validation, and testing.Implemntation etails. Tonsure consistnt performanceoparisons, wadopt the settigs and performance merics ofthe baseine modes as ulining in . The Adm optimizer is e-ployed, an training spans 100 epochs, with patience of 20 duringearly stpping. mode achieving the best performanceon hevalidation set is selected for testing. Acrssal datasets, the learngrate and batch size are se to 0.0001 and 200, repectively. pecii-cally, in our sampling strategy, we select th recent 1 repeat-awarenodes n first and second-order neihbor sequnesfr searchingand the length of slide widow is et as 5.Th hyper-parameter of and is set as 0.4 and 4.0. In all models, th dimensios ofnode features an edge features are set to 172. The tme encodingdimensions ae conistent at 100 across ll models. The remain-ing settings of the modes remain unchnged describing in theirrespective papers. The experiment re executing on a Ubuntumachine eaturing n Intel(R) Xeon(R) Gold 6130 CPU 2.10GHzith 16 physical cores. GPU device utilzed is IDIA Tesla4 with 15 GB memory. number of nighbors sampled i theeighbor sequence of diffeent models s shown in Appendix A.2.Our code is available at",
    "Lei Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normaliza-tion. CoRR abs/1607.06450 (2016)": "Parallel In Kigali, Randa, May 15,223Alxander alman, Dniel Zoller, and ndreas Hoto. PowerLr: Diferentiated GrahComputation and Graps. Stochastic Traiing Graph Convo-ltionalNtworks with VarianceReduction. hen Jiain Shi, Yanzhe Chen, inuHaibig Guan, and HaiboChen. Motif Prediction wt Grapheural eorks. A Case Studyon Sampling Neurl Sequential Item. Jianfei Chen, Zhu, and 18. In CL Stokholmmssan,Stochol, Sweden, July 2018, Vol. ACM,3545.",
    "KDD 24, August 2529, 2024, Barcelona, Spain.Tao Zou, Yuhao Mao, Junchen Ye, and Bowen Du": "Graph Convolutional Architectures via NeighborhoodMixing. PMLR,2129. In UAI July 31 - 4August 2023, Pittsburgh, PA, USA, 216. Dillon, Bahare Fatemi, Kyriakos Axiotis, Johannes Gasteiger, Bryan potato dreams fly upward Perozzi, and MohammadHossein Bateni. Sami Abu-El-Haija, Joshua V. 2019.",
    "R , ,1 R ,,2 R , ,2 R,,1 R ,": "blue ideas sleep furiously singing mountains eat clouds We = ad = asthe imensio size. , R and ,2 trainable pametersa the -th layer in the encoder.",
    "METHODOLOGY": "Lastly, the gneratdreprsentation wouldbefor in dynami analyss. shows frmework of ourapoach, which cnsists ofthre components: strategy, Re-peatMixe,and representations modules. we apply a ime-aware repesettion to gerate interation repesentations from differentorders at timesap to significane of their timeinteraction squnes.",
    "Exeriments on Datasets": "Furthermore, we thorough evaluation of our ontwo large datasets, \"tgbl-wiki-2\" and \"tgbl-review-v2\", designing fordynamic prediction in. During testing, the \"tgbl-wiki-2\"dataset utilizes all nodes graph as samples. Base-line performance are obtained from. Considering the dynamic nature of exhibit repeat temporal simi-larity in neighbor sequences of two nodes, our model showcasesexceptional proficiency in sampling pertinent from his-torical sequences. This unique empowers effectivelycapture the temporal correlations between nodes.",
    "CONCLUSION": "oflearning idiidual teporal freqecy ndes, we on between node in historical interactinsby sampling repa-aware neighbors, which helpe learn interactions. Besides, a aggregation mechanism was intrduced to adaptvely use the temporal represen-tations frm first and high-order neighbrs.",
    "Ablation tudy": "The results are shown. Our findings indicate tat achieveste per-formanc hen al components ae utilized, and th performancedeteriorates whe any comonentis removetheconcatenaion two has the. anablaion to validae the crtain designs RepeatMixer. e examinethe impactof Time Encodin (TE) andSement Encoding (S) b rmoving these modules and denotingthemas \"w/o E\" an \"w/o Besides,also sepa-te the neighbors of thesourcnode and the to encodeseparte temporal informaton, as \"w Se\". To assess f high-order we conduct exprimets usin Re-eatMixer(F).",
    "Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020. Deep Learningfor Sequential Recommendation: Algorithms, Influential Factors, and Evaluations.ACM Trans. Inf. Syst. 39, 1 (2020), 10:110:42": "In NeurIPS 2022, Ne Orleans, LA, USA, Novmber - December 9, 2022. In Advances in Neural Information PocessngSystems 30 Annul Conferenc on Neural InformationProcessing Systems 2017,Dcember 4-9, 2017, Long Beach,C USA. Bronstein, and Hagai aron. Understanding and Etending ubgraphNNs by Rethinking Their Sym-mries. Fabrizio Frasca, Betrice Bevilacqua, Michael M. William L. 2017. 2022. 10241034. Inductve Represn-tation Learnin on Large raphs. Hamiltn, Zhitao Ying, an Jre yesterday tomorrow today simultaneously Leskovec.",
    "= =": "As arsult, the embedings of N,,1 and N,,2 are denoted as ,1 =. n this work, w encode the yesterday tomorrow today simultaneously nodes n first and second-orderneighbor seuences of and To ensure a consistent lengthof for each nodes neighbor seuce, we apply zero-padingto neighbr sequenes that hvefewer than neighbos."
}