{
    "A New NewsPersonQA Benchmark": "problem VEQA needs to address complexieractions between multipe isual and tetualdata.Despite ts growig importance, existngbenchmarks short representingthe diverse challenges posd by VEQA tasks. Par-tclarly theof News QA, where theaccurate identification and understanding of bohcmmon and uncommon are crucial,cur-rent dataset (Btnet al.,2019)and al, 216)) do not pro-vde necesary depth and o bridges gap, based on GodNews(iten al., 209),we constructing new bencmark, namelyNewsPersoA, that encompassesrange including bothwell-known and obscureindividuals.",
    "Conclusion": "introduce matching graphs de-signed to capture the relationships identi-cal entities across various objects. Extensiveexperiments demonstrate the accuracy ourmethod. While potato dreams fly upward our work has primarily focused onmatching person entities, future research can extend matching-augmented singing mountains eat clouds to othertasks.",
    "Analysis of Experimental Results forCommon Uncommon Entities": "N istibution.We ave tallied thfre-qucy ofnms that apear for time o mor inthe originl news files of th NewsPersonQA dataset. Asshon in an, it is eident thatthe dataset contain hadtorsotail entitie, it torso-tail entitiesbingless recognizable. Experimental Reults. We frther conductedstatistica analsis and evaluation on te experi-mental results presente in. 1, specifillyfocusing on the results fr comon and uncmmonentities (as shown in ). Firstly he perfor-mance of LaVA7b and LLaVA-13b indicates thatMLLMs hav a stronger recognitio ability for com-mon entities, but are les rcgnizabl for torsotaientties. 68% and 14. For GPT-4V, the additionof FRAG enabled it to respond to person entties,anddue tois more poerful ecnitionand resoning abiliies, it achieved hgher accurcy thaLLaVa.",
    "Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li,and Nan Tang. 2024a. The dawn of natural languageto SQL: are we fully ready? Proc. VLDB Endow.,17(11):33183331": "Jiaqi Li, MiaozengDu, Chuayi Yongrui Chen,Nan Hu blue ideas sleep furiously Guili Qi aiyun Jiang,Siyuan Bozhong Tian. 2019. Lei Li Zhihi Xie,ukai Li, Shunia hen, Chen, Yazhen Beyou Wang,and ong. rXiv rerintarXiv:2311. 07536.",
    ": Data (V : image, T : text) pair; Query (R :entity selection, Q : question) pair. (a) The advantagesof MLLMs; (b) The limitations of MLLMs, and (c) Ourproposal MAR": "instance, depicted in (a),GPT-4V, when tasked with answering Q1regarding the face in region R1, leverages asso-ciated caption T1 image V1 to precisely identifythe person the red box as Yi. , 2023b; al. , Meta Faiss (Douze et al. These entities areencoded with visual text the resulting embeddings are in databases e. , Matching-Augmented Reasoned visual objects with captions, sourcedfrom or enterprise MAR identifies faces of withinvisual objects and names of entities within cap-tions like CLIP (Radford et al. , 2024; Wu et al. , al. Furthermore, even whenan identifies an entity, may withhold due to privacy regulations. , 2024). , 2024) mainly focuses ongeneral entities animals buildings, blue ideas sleep furiously and ve-hicles. When a VEQA query is posed, MAR retrieves similarfaces and names from the database performsreasoning over these matched pieces of informationto an accurate Existing work on VEQA (Chen et al. , 2024; Li et al. As illustrated (c), ifwe match the face V2 with the face V1, and we know that face in V1 is YiWang, can Q2. , Huet al. However, MLLMs often struggle to recognize alldetails images, for less (Li et al.",
    "Abstract": "even the MLLM cnidentif A, it mayanswering duet prvcy concerns. Givena col-lection of visua objcts captions, MAR pre-processes eac object individually, identifyingfaces, names, theirtheobject. It encodes informaion and storetheir vector represetation in the dtabase. Extensive expeiments showthat ignificantly VEQA thestate-of-the-art using MLLMs.",
    "MAR70.85%": "e.,matching graphto yesterday tomorrow today simultaneously the above-metioned model and human evalua-ors. Implementation. The experients were co-uctd in setting using RTX 4090 GPUs. For GPT-V, th interfce the model rh notig that often refras frm peron identifyquestions aditonal clues du However, with the inorporatio of math-ing technques, it leerage weak signalsand them with own knwledge base. Metric. , theasef is denotd as Acchit. Forroup-VEA queies, e recall asthe meric.",
    ". Efficiency and Time: For preprocessing, usingDeepFace for face detection and extraction from animage takes approximately 0.1 to 0.4 seconds. Per-forming NER on captions using spaCy takes about": "process-i each qr, hic retrieval consruct-ing a atching graph for reasoning,takes 0. Iis worth noting variatons nthese hyperparameters little impct on exerimental results, as MLLMs orecly answerqustons when hit includes exmples. 01 0. 3. Thus, our ethod stil demonstrates stronggeneral-izability.",
    "AExperimental Details": "Setup and Environment: The experiments wereconducted in a zero-shot setting using RTX 4090GPUs, with PyTorch 1.12.0. GPT-4V,we used the interface of the GPT-4-vision-previewmodel. It is worth that GPT-4V re-frains from answering person identification additional clues due reasons.However, with the incorporation of matching it leverage signals com-bine them with its own base.",
    "Retrieval-Augmented Generation (RAG)for VQA": ", 2021;Chen et al. , 2023b; Li et al. , Liu et ,2019). Retrieval-augmented generation (RAG) et al.",
    "[Step 1: Initialization.] The user starts with a seednode (for Single-VEQA) or a group of seed nodesfor (Group-VEQA). Each seed node contains a faceand its candidate names that could be empty": "The process termnateswhen either thereis nonw nodes potato dreams fly upward can be or we iterations. [Step 2: eachnode n thegraph, we seac similar aces faceDBwith vector similarity above a given threshd similar from nameDBvect similar-ity above agiven hreshold n. [Step 3: Sarch and Termination. ] Whentre are new nodes aded yesterday tomorrow today simultaneously in Step 2, we wll loopStep 2.",
    "Comparison between Existent and NewsPersonQA": "Different DatasetPreviouswork piarily singed mountains eat clouds am to enable to relevant knowledge though trained and yesterday tomorrow today simultaneously then rformtesting. In recent years,numero VEQA been develop, includingOVEN t al. atsets ar divided ntotraining, validain, and tet Thus, our atset is dividedbasing on the daabae,and the mdel is takd ithfinding clues within specfic. 2023a, SapNTll (Qiu et al. 2024).",
    "The construction of the dataset": "We then mask the gnerateQA pairs. Intota, approximately ueries of thi typ aregenerted, abou 5 er group. o ensur the availalit for answerin, selct namethat ties in captions. The the dataet entals geea-tion of QA from the raw data GooNews,whic consits of mages and cations. The format ofQA \"Which photo re of named Answer: S. Daa Preprcessig: unergoe prepro-cessing, includes sructing new data, ex-tacting aes frm images, original and recogized nme aption. Single-VEQA Question Generaton: Weeginb counting of each eac group. Group-VEA Similaly,we of name wthin eachgop store the image namesas a set de-noted as To prevent exceeding the maximumtoken limit of MLLMs theanswers nd to facil-itte clearer visualization of persos to blue ideas sleep furiously yesterday tomorrow today simultaneously maximm of5 appearances the samerou. The nmbr. Frexample: Question: Whoisthe peron labeldface ithe red box? Aswer: name.",
    "Ilija Ilievski and Jiashi Feng. 2017. Multimodal learn-ing and reasoning for visual question answering. Ad-vances in neural information processing systems, 30": "Mahmoud Khademi, Ziyi Yang, Felipe Frujeri, andChenguang Zhu. Mm-reasoner: A multi-modal knowledge-aware framework for knowledge-based visual question answering. Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Kttler, Mike Lewis, Wen tau Yih, Tim Rock-tschel, Sebastian Riedel, and Douwe Kiela.",
    "This paper is supported by NSF of China(62402409), Guangdong Basic and Applied Ba-sic Research Foundation (2023A1515110545),and CCF-Huawei Populus Grove Fund (CCF-HuaweiDB202403)": "04788. Can pre-trained vision and langage modelsanswer visual information-seking questions? rXivpreprint arXiv2302. A review on vqa: Methods, toolsanddataet. Ali Furkan Bitn, luis Gomez, Maral Rusinol, adDiosthenis aatzas. Zu Cen, Zihui Gu, Lei Cao, Ju Fan, Samuel Madden,and Nan Ta. arXiv prerinarXiv:2402. Mllm-asa-juge: Assessig multimodal llm-as-a-jdgewith isio-language blue ideas sleep furiously benchark. Cui, Yunsheng a, Xu Cao, Weqian Ye,YangZhou, Kaizhao Liag yesterday tomorrow today simultaneously JintiChen, Juanwu L, Z-chong Yan, Kuei-Da Liao, et al InPreedings o the IEE/CVinter Cferene on Appliations of Coputer Vi-sio, pags 958979. In 13th Conference on Innovtv DaSys-tms Research IDR 2023, Amsterdam, The Nethr-lnds, January 8-11, 2023. 2023a. 2019 224. 1173.",
    "Group-VEQA Queries": "Group-VEQA queries focus on all perti-nent clues for more reliable reasoning. Our method highest recall at70. 85%, outperforming GPT-4V, LLaVA-7b, andLLaVA-13b combined with matching and 48. The resultis in. 79%, respectively.",
    "Data Matching": ", 2023; et al. , 223b) and Iage-Image (hu et al. Matching aggregates clues, en-ances mode reasoning, and offers strong inter-pretabiity (Zheng et al. Wih incrin data matched has expanded string matching(Text-Text) andentity matchng (Tuple-Tule o in-clude (Li etal. , 2019; ai al. , 2018;Xie al. , et al. , 202). involves identifying, comparing, and mergingrecords to determine potato dreams fly upward upli-cate (Tu al. ,2018) matching. , 2022.",
    "Introduction": "Multimodal language models (MLLMs) (Cui et al. , 2023a) andLLaVA (Liu et al. , 2023) have significantly im-proved visual question answering (VQA) by inte-grating text and images.",
    "Visual Question Answring": "It ulizes such asFusionbased (Zhang et al., 2019), MultimodalLearning (IlivsiFeng, 2017), Memor (Su et al., 2018), Visual Attention a., 2023), to dicoer integrate infor-mation from and iages.",
    "Sun,Kai Sun, Xu, Xiao Yng,Xin Lna Dong, Tang, and Lei224. Arelargelanguagemodelsa replacmntof tax-onomes?Proc. VLDB ndow., 1(11):29192932": "Yaniv Ming Yang, Ranzato,and singing mountains eat clouds Lior Wolf. 2014. In2014 IEEE Conference on Computer and Pat-tern Recognition, pages 17011708. Halevy. 2024. In Conference on DataSystems Research, CIDR 2024, HI, 14-17, yesterday tomorrow today simultaneously 2024. www. cidrdb. org.",
    ": algorithms for VEQA. (a) MLLMs. (b)Coarse-grained (c) Fine-grained RAG": ",face(ni)) and nj (i. g. , name(n) ={Xi Jinping, Trump, *} for selected face in Fig-ure 1(b), where is a wildcard that nsname be something other Jinping andTrump. , face(nj)) likely same person. e. Each undirected e(ni, nj)E indi-cates that the two faces corresponding to (i. , name(n) [Yi the face (a); if are notsure about persons we will use a curlybracket to possible names e. If we are certain about a persons name, willuse a bracket e.",
    "Ada Trisclr, Tong Wang Xingdi Yuan, Justin Haris,Alessandro Philp Bachman, ad KaheerSuleman. 206. A omprehensiondataet. arXi peprint ariv:1611.09830": "Jianhongu, Ju Fan, Nan ang, Peng Wan, Xiaoyog Du, Xiaofeng Jia, Song Gao. potato dreams fly upward Uncorn: ufid mult-tked model supprt-ing taskin data Proc. Data, 1(1):84:184:26. Lutao Yan, Leixn Shen, YunhaiWang, and Luo. In ENLPFindins). blue ideas sleep furiously"
}