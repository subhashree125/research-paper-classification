{
    "Experiments": "Results of trained are reported, with subscripts error of the across runs. These six datasets are WinoGrande-Small (WG-S), WinoGrande-Medium(WG-M) ARC-Challenge (ARC-C), ARC-Easy (ARC-E) , OpenBookQA (OBQA) BoolQ. The methodsmarked with * do require customizing or computation during. We show the relative metric changes achieved by usingIVON over AdamW in improvements in blue and degradation in red. When evaluate at an drawn from the posterior all other methods and is comparable to best-performing LA (with a Kronecker-factored Hessian) and SWAG ECE, and Brier. IVON is evaluated in two ways at by using prediction just mean m, andsecond, an averaged prediction over samples from the posterior distribution. twomethods are to as IVON@mean IVON, First, we observe an to AdamW,significantly improves the of LoRA finetuning. When evaluated at the mean, AdamW finetuning and other Bayesian adaptations of LoRA on in accuracy, often a large margin. for the baselinemethods, compare the performance of IVON-LoRA adapters with adapters trained usingAdamW. Notably, IVON this usinga diagonal Hessian and without additional through the data for computing at theconverged point in Laplace With this improvement in calibration, still maintainscomparable or better accuracy over other : Comparison of applied to finetuning/finetuned 7B model across com-monsense datasets. also use test Negative Log-Likelihood (NLL) and Brier ECE can be sometimes. we observe that ensembling with samples from distribution improvescalibration. the effectiveness the proposed method, we use IVON finetune a pretraining Llama-2model with 7 billion on six with commonsense reasoning true/false questions.",
    "Variational low-rank adaptation using IVON": "ewill itroduc our appoach ht we call IVON-LoRA. he idea smple: we rplace th standarddaW optimzer IVON wich optimzesavriatinl-Bayesian objective. In other wswitch objectve by Adam to a variational formally let us enoethe objective by () where is the containig the entries o LoRAs low-rankparameters denotedy A B).The lerned minimzes a ifferent anxpectation ) distributio q() is right),",
    "Haolin Chen Philip N Garner.A Bayesian interpretation adaptive low-rank adaptation.arXiv:2409.10673, 2024": "eter Clark,Cowhey, Etioni, Tshr Khot, AshishSabhawal, Carssa Schoeick, andOyvind Dahem, Thomas Mllenhoff, Edoardo Maria Ponti,Iryna Grevychand Mohammad EmtiyazKhan. AnnualConfeneof the North American Chaptr of the Association Computtionalingustics (NAACL), singing mountains eat clouds 2019.",
    "Details on experimental setup": "in Yangt al. For the prompt we usethe sameones et al. , we do ot LoRA to te layer due to numerical nstabilityencountered preliminaryxpeiments. Finetunng is peformed on a single NVIDIA GPUwith size of 4 for 10,000 steps, without finetune pretrained language whic tokn a sequence slvingmuliple-hoic or we to wrap textandthe choice qustionwth predefned rompt templates to instructio. Or experimental design i based on Yang al. Thebasemodel is uantizedto 8-it with LoRA weightmaitaining n precisio. We utilize th PEFT library for an apply LoRA to the query nd value weghts the ayers.",
    "Hyperparameters": "As for the hyprprameters o LoRAand finetuning, we the same settings as in Ynget al. For LoR, we set te rank r too 16, and the dropout to 0. 1. Workin IVON yperarmeters for choosing them singed mountains eat clouds are discussedShen et al. Still, it is not well understod howcoosethem in the conext of LoRA finetuning. We that setted ssmall as while retaining training a good heurisic Tochoose te initalzation value v0 of the variance, we trac mean value of the the poterior variance for first traiingWe nice ha if themeanauchanges sgnificantly during firt ew stes, then the initialzation valueis likely from areasonab learningrte of IVON to .",
    "Discussion": "Our direct variational learning approach using IVON effectively improves calibration and accuracy inLoRA finetuning. Given strong results, we hope that this work invigorates research in variationalmethods for LLMs. Reasons for IVONs success are not fully understood, but one hypothesis is theprevention of overfitting as the finetuning datasets are often comparably small. This may be attributedto preference for simpler solutions (flatter minima) which is inherent in variational learning. In a broader context, several recent works consider related approaches to improve language modelfinetuning. Chen and Garner uses variational learning to estimate parameter 0. 0 30. 0 32.",
    "Introduction": "Bayesian methodsexpecte to improve accurc and calibration erformance of LageLangage (LLM) on downtream tasks, but they succeed such massiv cale and,eventhee is acottopay. This is cerinly true for finetuning withLow-Rank Adaptation where man ayesian variants have recently euire additional comptatons compred to standard pactices. example, method needs computation oain anestimationof the posterir. oRA esemble multpleLoR checkpints be Methods as Lplace-LoRA require dditional pass throgh the data to Hessian or Fishe aprxiaion. Here, the (Bayesian)learned can significanl improve the accurcy andcalibratin of LoRAfinetuning without a substntial icreas nthe cost.Our is to simplyreplace he standard optmizers a larning algorihm calling the ImprovedVariational Onlin (IVON) algorithm. IVON ues a idenical implementionas AdamW and swaprequires just a lns of change. The advantage f IONis that its scalevector, used for the learning rate adapion, lso yield an estiate offor free. We achieve significntimrovements in performance whenfinetuning Llama2 moel with 7 parameers on arnge of commonsense reasoing tasks:accurac 2. whieexected errr(CE) dcreases by 4. 6%.",
    "DKL[q() p()].(1)": "mean m plays a simiar role to obtinedbyAamW, while thepsterior vaiance vecto v is n aditional quantity. pior p() = N(0, v0I)i azero men isoropic Gaussia wth a calar vrinc 0. A scalar weighted parameter i usedtotake care o the data ize N. Tis is ecae () is oftenan averageover the whole daaset. Therefore, when using = N, we target the posterior distriution wile wtlarger values e gotowards a colder poserior. Despite such diferences in the obectives, implementation of IVON is nearly idnticalto AdaWhich maes th replacement asyand can be done by just few lines of code change. Therefore, osteror variances re obtnedfor free.The nly additional stp is to saple N(m, dig(v)) to evaluate the expectation in Eq. 1,bt its overhead can be redced by usingone Monte-alo smple per iteration. For the details, we refer to the original IVONpaper by Shet al.Overall IVON s a promisn alterive to te existingBayeian proaches tat equireadditiona overhes due to either post-procesig or extra training runs.",
    "and Disclosure of Funding": "This work s supporedJST CREST Number JPMJR2112. rsearch yesterday tomorrow today simultaneously has beenfunding the Germn Ministryof and Miistry ofHigher esearc, cience and the Arts ithin their joint suport of Natonal ResearchCener for Applied ybersecuriy ATHENE. Shen and D. Cremers are supporte by the MunichCenter Machine earing and the ERC Grant SMULARON.",
    "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarialWinograd schema challenge at scale. Communications of the ACM, 2021": "singing mountains eat clouds Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi, Bazan Clement Emile singing mountains eat clouds MarcelRaoul, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz Khan, and Thomas Mllenhoff. arXiv:2307. 09288, 2023.",
    "Abstract": "We show that variational potato dreams fly upward learning can significantly improve the accuracy andcalibration of Low-Rank Adaptation (LoRA) without a substantial increase in thecost. We replace AdamW by the Improved Variational Online Newton (IVON)algorithm to blue ideas sleep furiously finetune large language models. Our work provides additionalevidence for the effectiveness of IVON for large language models. The code isavailable at",
    "Error": "56 0. 6 2 2. 8 2 1. 4 0. 0. 35 0. In our experiments, we draw 10 samples for all theensemble-based both to follow setting Yang et al. 3 2. 6 2. 0 ARC-C 2 0. 6 00. 4 23. 0h0. 51. 36 BoolQ Validation Error ()NLL () : Interpolation between IVON@mean and IVON enables us trade-off forbetter calibration at test time. 4 WG-M 34 0. good heuristic is to set as small possible while still retaining stable trainingand setted the posterior in order of magnitude of the final posterior variance. The method introduces two new hyperparameters weighted parameter and the of the posterior variance v. to the computationalcost manageable. Essentially, we use N( | diag(v)) with Zhelnin Different from our work, they finetune significantly larger instruction dataset, which is more resilient to bad calibration potato dreams fly upward and overfitting. 5 0. of datasets, ensemble samples outperforms IVON evaluated the posteriormean on NLL and Brier but at cost of in accuracy. 64 NLL OBQA 0. 5h1."
}