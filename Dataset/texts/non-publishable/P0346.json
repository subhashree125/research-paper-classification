{
    "Farhad Niknam, Qazvini, and Latifi. Holo-graphic optical field recovery using a untraineddeep decoder network. Scientific reports, 11(1):10903, 2021.7": "Giacoo Oliveri, Marco Salucc, and An-drea ssa. Untraindneual netork priors or inerse imagin. 1 Adnan Fah Fari Bous-said, Bennamun,and Junaid Qdir. Compresiv sensing pplied to nvers prob-lems for imgn: Theoy, applications curent treds, challenges. IEEE Antennas Propagation Mga-zine, 2017.",
    ". Moment-Aggregation Training": "If the loss L singing mountains eat clouds satisfies the sim-ilar properties with F(x; ), singing mountains eat clouds the loss has a high probabil-ity of converged to a similar optimal with F(x; ). It isnoteworthy that the neural network G can only optimize xthrough optimizing w since i can be viewed as an inde-pendent variable with w. Now, we define the new loss andname it aggregation loss,",
    "2y A(x; i)22,(4)": "e th0R() as prior isincluded in neuralnetwork. However, thissolionextremely which is not friendly forapplications ith contraints, whe the umbe o candidates isTo addressthis issue, we present blue ideas sleep furiously or framework prvide the theo-retical insights rm convex singing mountains eat clouds optimizaton.",
    "Tese authors contributed eqally this paper.Corresponding Author": "Left: The losses by parameters and one of precie param-eter. Their abels are unnown (i. the precise o not prcse)dring training. oss a different moments by MA. The loss is moment-wise conve/smooth, and the verall traningcan achieve heminima reconstuction ung",
    ". Comparison of the upper bound and our proposed method (MA) for Left: convergence and Right: runtime each iteration.i.e.forward-propagation and backward-propagation": "This problem is also an ill-posed IIP problem, and hereP (, d = z) can be considered as the forward model withuncertain parameters due to the low-quality equipment oran inaccurate precision optical rail. 520mand 5000m to generate holograms, respectively. We choose samples fromthe Gland segmentation dataset (GlaS) and the Multi-Organ Nucleus Segmentation (MoNuSeg) dataset togenerate the simulated holograms. iii) Uniform Aggregation, and iv) Alternating Opti-mization. Evaluation Metrics. PSNR, and SSIM. Results. For example, there are 0. 307 and 0. 167 gaps in PNSRfor Glas and MoNuSeg, respectively. This may be because the reproduced measurement (i. e. y the prediction after the forward process) in this task is stilllike an image, which can be partially fitted by the neural net-work. However, the detailed texture represents depth infor-.",
    "Hengyong Yu and Ge Wang. Compressed sensing based in-terior tomography. Physics in medicine & biology, 54(9):2791, 2009. 1": "Twin-image-free holog-raphy: a compresive sesng approach. 1, 6, 7 Jun-YanZh, aesung Park, Phillip Isola, and Alexei AEfros. In rocedings fhe IEEEinternational conference on computer vsion, pag 22232232, 2017.",
    ". Relted Work": "are works prvide the convergence and rror fo with. Hence,DIP becomes naturachoice tserve asthe priori IIs n Eq. Theseworks inolve a initializd en-eraive model and solve th inverse proble trining thenetwork prameters. It is oberved thatin order to ensure derivation isractable, hes oftenemploy assumptions,such as Lipshitzcontinues, rage of neural the netrk has linar layers and activationfunctions. an is employed bynumerous works. Recnt dep-learingmethods, such as superviselearning andunspervsedlearnin , deonstrate outstandingability to solveseveraimage tasks. to solve IIPs rely on prior domanknol-edge; hoever, thee are sensitiveto the hyperprameters , in E. onvergnce uaantee. Besides, their theoreticalguar-anteeis often deigning or C problems reieso which is difficult to toboader scenarios. We admit simplifypocess o he reconstruction, teirtheoretical reults offr enough insights for the ommunityto develop furtr Agin, their works ofte assumethe forward model parameer i known, and in this work,we buil theoretical analyis in cenarios asedon Howeer, they the erro of theforward odl is white and relatively smallto the precise paramer. Due to this tool, authors show inverse proes can b slved byusi the from pre-taied geneative modls, whichis nown as learning neworkwih priorthat s learned by triningdata recently, the com-munity has observed that even without training daaset, the randomly initializing covolutional neralntworks (CNNs) hold prir for image signals. Afterward, authos i inves-tigat the convergece rate projecting gradient generative neworkwhil authors studyan gorithm base on Langevin with learnedetwork Likewise, with netork n the convergence for under-parameerized authors prove for theover-parameterized etworks. neuralnetwork For eample, authors provea covergence for cntin-ue generative network. As hesework ofen knowingthe ecise near-precis of model,our is orthognal but to them and aimst recorsignals under a set of parmters. Left: The sigal is successlly recstructed under forwardmodelwith the precise while failing a wrong parameter. Cotrastinly, we consder rcon-truction with iscreteset of parameter candiates, andthe dstance different measuremnts from. This prio as dep (DI, statsat CNNs able captre o lw-level image statistics before training on a specific imagedataset.",
    "tural Similarity Index Measure (SSIM)": "around 19 dB PSNR X-ray imagerecnstruction. interstng that opimzatio shwsvious uperiorityove blind reconstruction, and it can reconstrut the signalometimes, e. The rsults ar shown in. Our is caused bythe compuation of th forward processfor each caniate parameter. 07 SSIM reduction. This laging is rasonable, ecaue theuer bond usg theknownprciseparameter is eay o coverge, whie undera unertain set candiates the error landscape for opt-ization smore complicate. The first singing mountains eat clouds observation using andom Parametr toreconsruct signal blindlis not feasibe, onlyachieves around 1dB PSNR,meaning almost nothing isrecostuted. Resuls. show run-time fo eah by ui a set of parame-ters onyone recise parameter. Soe sampes of reconstructed signals fo MNITand atasetsare shown in (Lef and ight), r-spectvey. lso deontrate the convergence raten (Lft), which our method canconverge othe of error with a lagging. Both them illtratethat recontrute signalsbyur metho very siilar peformance withte upper bound. For an our ethod has 04-0. Althou th overead exists,our method is still mh fater han traing differet neu-al netwoks diferen or xape,if we only that cn train themodel, in theourmeth requires 5 seond.",
    "Mathieu Rosenbaum and B Tsybakov. Sparse re-covery under uncertainty. 2010": "2018 IEEE international on acoustics,spee andsignal proessing ICASSP), pages 46094613. Cheikh, D. Gland segmentation in colon histologyimages: The glas challenge contest. Sanche, A. 2,3 Viraj Hede. 2 Pluim, H. Med Image Anal, 2017. Wang B. Pfeiffer, Urshler,. hm, O. M. pags 26291. Matieu osenbau Alexade B Tsbakov. N. Qi, P. J. IEEE, 2018. Rajpoot.",
    "Stop Gradient for i,": "Here H(i; F(; , F(x; is the function al-culate th weightfor cndateo the surrgatequalitiest each moment (i. i blue ideas sleep furiously wil t H should i) nci=1 i =1 ii)limxx0 H(; F(x; 1), , F(x;nc)).",
    "Mingqin Chen, Peikang Lin, Yuhui Quan, Tongyao Pang,and Hui Ji.Unsupervised phase retrieval using deep ap-proximate mmse estimation. IEEE Transactions on SignalProcessing, 70:22392252, 2022. 2": "Chen, Kornblith, Mohammad Ge-offrey A framework visual In conference learning, pages 15971607. 2 Xiwen Chen, Hao Abolfazl Razi, Michael Christopher Mann. Op-tics Express, 2023. Li Feng, Leon Axel, Chandarana, Kai Block,Daniel K Sodickson, and Ricardo Otazo. Xd-grasp: golden-angle radial mri with reconstruction of extra motion-state di-mensions using compressed sensing. 1 Yosef Gandelsman, Assaf Shocher, and Michal double-dip: image decomposition via coupleddeep-image-priors. 2 Kaimed He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for recognition. In of the IEEE conference on vision and pages 770778, 2016. singing mountains eat clouds In International Machine Learning, pages 41494158. 3 Hegde. Algorithmic aspects of inverse problemsusing models. In 2018 56th Annual Allerton Communication, and Computing (Aller-ton), 166172. 2018. Jaeger, Sema Candemir, Sameer Y`-Xiang JWang, Pu-Xuan Lu, and George Thoma. Two chest x-ray datasets computer-aiding of pulmonary dis-eases. imaging in and surgery, 4(6):475, 2014.",
    "arXiv:2405.02944v1 [cs.CV] 5 May 2024": "typical workflow of IIPs. the measure-ment is to reconstruct original via machine learning(ML) deep learning (DL) algorithms. n), which means they multiple interchangeably solutions.The core idea to solve these prior information about the original signal(e.g., prior distribution, smoothness, into thereconstruction algorithm. This enhances the reconstructionquality by the space and steering the toward the most probable and reality-compliant so-lution . Mathematically, an IIP is typically in avariational formulation:",
    ". in Phase Retrieval": "Setup. also yesterday tomorrow today simultaneously show the feasibility of our in phaseretrieval. Suppose = 0) denotes complex-valuedobject wave at location d = 0.",
    "iederik P Kingma andJimy Ba. A method fstochastic preprint rXi:1412.6980,2014.": "R. Verma, S. S. A. Dataset and a Technique Segmentation for Computational Pathology. IEEETrans Med Imaging, 7 Lan, Juze Zhang, Changchun Yang, and Fei Gao. Compressed sensing photoacoustic computed tomogra-phy basing on untraining neural network with a shape prior. Biomedical Optics 12(12):78357848,"
}