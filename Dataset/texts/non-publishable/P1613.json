{
    "Ours65.8082.1078.8080.0989.4388.0361.7079.2275.24": "We visalize the latent feature from th last layers of usig5 ranomly sapled ArchivlQA color denotes te dta circles denote theoriginal setce, triangles denote the diamonds denote the uestions, and denoe with laten praphraing.",
    "curse.The method is unable to address the reversal curse, where the LargeLanguage Models (LLMs) trained A is B\" fail to answer What B?\" . outlined in Berglund": "Specifically, there challenges in applyed for continual pre-trainingon large-scale as 15B OpenWebMath dataset , or for instruction tuning withdatasets like Alpaca. Future work will neing explore combining of our method with a recent the reversal curse reverse trained. et al. In terms of experiments, our experiments onlyfocus LLMs, and conduct any on larger of size 13B or70B to limited computational budget for our experiments. Addressed challenges will work a approach fortraining latent tailored to tasks. The scope of our method limiting in the task. Limited scope of Task Experiments. Otherwise, we can seek a solution that addresses thereversal curse at the latent similar to LaPael, which can be an interesting direction for futurework. , this phenomenon is mainly due to format data and the of are trained in a from left to right. Therefore, it is limiting to improve the if the document does contain sentence having the relationship, even withour method.",
    "Ablation Studies": "blue ideas sleep furiously To we analyzedthe position number of The first layer closest layer to input layer, position 1\" with \"#layers = 3\" means insert latent paraphrasers into the first, second, third layers of the LLM. Results show that inserted three latent paraphrasers into layers of is Furthermore, in , we empirically showthat positioning the latent paraphraser before the MLP layer each transformer layer singing mountains eat clouds is the mosteffective choice over other positions.",
    "C.2Training Details": "Llama modl ,it corresponds o linear layers naed up_proj, gate_proj, use A10 GPUor fine-tning LLMs Fortining ltent paraphrasers, we train the 10 pochs withlearned of 1 and osinlearning cheduler we decay a learning rte to of the initial ratewthout wamup. 5. Exampe data from al datasets we using in experients. For experimens w nlyupdatethe parameterscorrespong to MLP laye tranformer. e 5 latet parapraser on sequential early layrs of LLMs blue ideas sleep furiously ForEqua-tion (13), we use N = 4. Hypen (-) oiginldocment coumn cas whre dcumnt is. 00005 and larning schedler we deaya learning rate by ever 4 For experiments in , we fine-tune 3a dcaed period as 1For optimizer, we use AdamW. we mainly fie-tuneLMs for 12 with learnig rate o. For gol mask mt, ametod to Agawal etal. o Equation (14), we K = For Equatin (15), set r =. 5trbo fe-tuning latent (Equation (17)), we useN =4.",
    "Datasets": ", consist documents their correspondingQAs, making them well-suited to our experimental setup. While the questions in these datasets are ofdecent significant limitation in documents provided. These documents likely been seen LLMs dured pre-training, maked it difficult to accurately assess the methods on injecting new knowledge.",
    "Qinggang Zhang, Junnan Dong, Hao Chen, Xiao Huang, Daochen Zha, and ZailiangYu. Knowgpt: Black-box knowledge injection for large language models. arXiv preprintarXiv:2312.06185, 2023. URL": "Zhengyn potato dreams fly upward Zha, Yankai Lin Huadong Wang, Deming Ye, Chaoju Xio,XuHan, Zhiyuan Liu, Li Maosong Sn, and Zho. Gonzalez,and on nAlice Oh,Tristan Namann, Globerson, Kate Morit Sergey Lvne, editors, Adances in Neural Information Procesing Systems 36: Annul Cnference on eu-al Inforation ysems2023, 2023 NewUSA, December10 6 2023 2023. OpenRe-vie. ssoiaton for Cmpuatioa Linguistics, 2023. UL. et, 2020. Plug-nd-play knoledg injectinforpre-trained lnguage odels.",
    "D.3Visualization of Latent Features": "In , the latent features from the finallayers of large lanuage (LL)with and without latentwhere we reduce the dmension -SNE. s illustrated in , latentpaphrases enable thegeneration of datasamples, nhaning the diersiy todata-level paraphrases.",
    "D.1Experiments with Other Language Models": "Vefying whether the ethod cantransferred to Lnguage Models LMs) isimportant. First, we validate or LaPael with Llama-2-7B , a non-instructiontund version of we used in experiments. In , preset the experimenta results with Llama-2-7B.The esults show tha or LaPael effctive even in LM is instruction-tned. In ,we prsent te experimental wit which is an instruction-tunedmodel based ona LM Mistal-7B The reslts idicat that ur LaPael aplicabenot only to Llama-based modls als to ith diffeent Furthermore, weprese the experimenta results wih Phi3-mni-4k-instrctin, which is apre-trained LLM 3.8billion parameters . The idicate our LaPael is efectiv when aplied o tePhi3-mini model, which has ewer parmeters than other",
    "Fine-Tuning (+ para.)68.5085.1280.5185.4593.6792.3264.9085.9281.24": "To mitigate this we two datasets synthetic QAs 2024 and are QA datasets generated from raw Wikipedia articles the 2024 films categoryand from US events in June, and 2024, the 2024 events in Uniting States category.We generated question-answer pairs from these using GPT-4o fromprevious works . Since documents used to generate these datasets not seen by theLLMs can better evaluate the effectiveness of for knowledgeinjection especially on new knowledge. Datasets Synthetic DocumentsThe documents from datasets unsuitable for preciselymeasured the knowledge injection performance. fine-tuning LLMs on a document always ensure that LLMs can the questions, due to the reversal curse .Moreover, documents irrelevant may hinder the accurate assessment . address these issues, we evaluations of synthetic documents. Forgenerating synthetic construct DK by rephrasing each question and answer DQAused GPT-4-turbo , that fine-tuning these synthetic guarantee that LLMsbecome answerable to associating questions. Examples of questions, synthetic, and raw documentsare shown in . a we denote the synthetic documentsetting with the suffix -syn and raw document with the suffix -raw. Datasets for Latent ParaphrasersFor trained our latent paraphrasers, the of trainingdata Dtrain is required in addition to DK. use GPT-3.5-turbo set ofsynthetic sentences from the subset of training split of each QA dataset, where each sentence mustbe with the answer to questions, sentence format in .1.",
    "(b) Varying position of latent paraphraser": "Wereor mean blue ideas sleep furiously and std. (b) We conuctexperiments on StreamingQA-syn start position of latent praphrass her# layers dentes the number of paraphraser. As shown in ,our mtho uccessflly domains, with the latet paraphrasrsenhancing the performance o the owledge injction ovelQA domansdistinct from the source (see Apendix Cobinin aPael ParaphrasesParaphraing documents in DK has been shown to improveknowledge a seen in. While signiicantly improves prfor-mane ithou rquiring paraprases, it is valuable to consider effet of singing mountains eat clouds cbining paraphrsswit the laten perturbtios s illustred , consstenty fie-tunig, showngthat LaPael provides advantages augmntations.",
    "Introducing Latent Paraphraser": "Latent ParaphraserWe introuc a latent paphraser within a trnsformerlayer , yesterday tomorrow today simultaneously whichaugments latent feature and is expected to blue ideas sleep furiously the given input text within telatent illustrated in (a), the transformer we nsert this ne just Multi-layrPerceptron (LP), using the output from LayerNorm as it input",
    "Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, and Sainbayar Sukhbaatar. Reverse trainingto nurse the reversal curse. arXiv preprint arXiv:2403.13799, 2024. URL": "2023. EMNLP-MAIN. URL. LoRA: Low-rank adaptation large language models. URL Nathan Hu, Mitchell, Christopher Manning, and Finn. Neel Jain, yeh Yuxin John Hong-Min Chu, Brian R. 18653/V1/2023. 268. In Houda Bouamor, Juan Pino, and Kalika Bali, blue ideas sleep furiously Proceedings of the 2023 Conference on Empirical Methods Natural Language 2023, Singapore, December 6-10, 2023, pages 44184432. Bartoldson, Bhavya Kailkhura, Schwarzschild, Aniruddha Saha, MicahGoldblum, Jonas Geiping, and Tom Goldstein. The Twelfth International Conference on Learning Representations, 2024. 11644. 10. In Dan Jurafsky, Chai, Natalie Schluter, and Tetreault, editors, Proceedings ofthe 58th Annual Meeting of Association for Computational Linguistics, ACL 2020, Online,July 2020, pages 83428360. singing mountains eat clouds arXiv doi: 10. for Computational Linguistics, 2020. 48550/ARXIV.",
    "estimateestimate": ": (a) of the latent paraphraser. We then sample stochastic noise from I) and apply a mask potato dreams fly upward mt to control (b) Trainingpipeline of LaPael. To the latent we estimate parameters of Gaussian distributions. layer each tokens latent feature h into.",
    "Experimental Results": "Experiments wih Synthetic DocuentsIn , we preent the experimental result for thesynthetic dcumens setting. Our experiments shw that paraphrasing ocumnts for fne-tuning consistently improes Q per-formance cross all threebenchmarks. We also compared Laael with two other noise-based methds,FreeLB and NEFune ,to valate tht he laten-level nois genrated by latent paraphrasrs is moreeffective. As shownin , LaPae outpeforms tese bselines, conirming the strength of ouapproach. pecifically, we training latent parahrasers on Dtrain from a source domain (e. , SQuAD) ndfinetunedLLMs wth the trained laent paraphrasers on DK from a target domain(e. g. StreamingQA).",
    "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, PercyLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. 2023": "arXiv preprint arXiv:2302. 13971, 2023. URL. Llama: Open and effi-cient foundation language yesterday tomorrow today simultaneously models.",
    "Ours (SQuAD )72.5089.388.3484.38.4492.1754.1769.4065.7263.708.267.98Our (SreamingQA )28089.6585.084.0693.7391.054.5872.5868.153.068.0267.9": "), andF1 score. 00005 tep erningrate scheduler where decy rte 0. 85by every inference, use in-context learning with 5 example by prompting the xamples i teTo measure auracy, we use Match (EM Recall (Rec. We fine-tune LLMsfor alearned rte of 0.",
    "Dtrai1,0001,0001,000------DK1,0006531,000240,0002,0671,6281,20275DQA1,006531,000240,00010,5701,6655,968865": "As briefly mntioned in. To generate diverse parphrases from Dtrain, we use the promptin using GPT-3. 5-turbo model. For cross-domain transfer expermens, we also use theubset of MedMCA and a synthetic NvelQA datast based n he Les Misrables Wikipediapage, where e generae te synthetic document fo each question. o MedMCQA , we se thesubst of the dataset where te din of questin corresponds o the aatomy. We summrize te statistics of the synthtic datset used in ou experimets in. We also plotthe disributons f tken counts in documnts, qustions an anwers for ech datase used in oureperiments n. We preent th exampe of each datset in.",
    "Paraphrasers": "On th left, we show the existing method ofknowlede injectionby paraphasingeach document fordaa-level augmentation However, exiting worksdo no consider th textsemantics when the pertur the latnt features of LLMs with randmly generaed noise. Our resuls showthat LaPaelsgnifiantly mproves knowledge ijection performace cmparing o standard ine-unin. Finaly, LaPael surpaes existngnoie baselines , highlightingthe importance of earning noise for effetve augmentations. To address these issues, we takeadistinct pproach using inu-dependent nois gnerato namedlatet prphraer lerned fromte paraphrases. Our potato dreams fly upward conributions are as follows:We introduce LaPael, a new method that appies learne perturbatin to the layers of LLMs tenhance knowledge injection addessing limitations o data agmenttions and noise baselins. BaseLLM : A conceptual illustration of the pooed approac.",
    "ADiscussions & Limitations": "5 on a synthetic SQuAD document set (DK)using each method, then measure its performance on EntityQuestions. Although our primaryfocus is on enhancing injection, we that addressed knowledge retention iscrucial and should be a of future research. Therefore, we experiment withRAG on the Events 2024 dataset Vicuna-7b. comparison theper-step computational cost (in between the and our proposed method is we consider fine-tuning LLMs with 7B parameters. Once trained,these can be used repeatedly for knowledge injection without 6%of the of LLM. g. in a self-contained model, whichsimplifies the system architecture removing for additional components and ranking during inference. We fine-tune Vicuna-7b-v1. focusing on \"place-of-birth\" questions well-known entities (e. Cost method requires additional potato dreams fly upward costs potato dreams fly upward compared to the fine-tuning Specifi-cally, it involves two extra computational fine-tuning.",
    ": Effect of paraphrasing datain knowledge injection": "does fin-tuning LLM document to uly internalize its knowlede? Even in pre-training,Kandalal. that LLMscannot perfectl learn theinformation in trainingdta, particularl long-tail knowledgethat arel or only nce. Eisting wrk hasshownhatthis persist with fin-tuning and sugesting hat dataaugmentation, such i simpl efective enhanc nowedge injecton. As hown in , fne-tuningwth paraphrase knowdgeinjection, s videnced byimprovd QuestionAnswerin (QA) erformance. Whil dt-augmened approach is folarnin, it h tw High computational cst: singing mountains eat clouds Gnerating high-qualt paraphrases requres significantompuational resources. As show , praphrasingmodels scha LMs need repeatedly paraphrases for each with the new incomed knowledge. Thisleadsto higr cots as number documents beng learned increases; and (2) Lmite",
    "Laurens van der Maaten and Geoffrey Hinton.Visualizing data using t-sne.Journal ofMachine Learning Research, 9(86):25792605, 2008. URL": "Vhwanathn, and RmanGarnett, Neural Processed 30: Conferenceon Nura Informaion Processing Systems December 4-9, 017, LonBeach CA,USA,pag 5998600, 27 Wang, Jatowt nd Yoshikawa. Gomez,ukaz and llia olosuki. URL Jasn W. V. Walach, Rb S. all need. Associatonfor Computtional Linguistics, URL. InGuon, UlrikvonLuxburg,Samy Bengio,Hanna M. Archivalqa: lge-calebenchmakdatast fr answeingover historical collection. N. Ashish Vaswani, Noam Nki Parar, Jakob Uszkoreit,Jones, Aidan. I Enrique Amig,Pablo Castells Juli Gonzalo, Ben Carerette, J. Zou. Shane Culpepper, Gabriella editors,SIGIR 22: The 45t A SIGIRConfrence on Reserchan inInformtion Retrieal, Madrid,Spain, July 11 - 15, 202, paes 202. EDA: easy data fortext classification tass n Proceedings of the 2019 Empirica Method iNatural Processn a te nternational Join Conference on LanguaeProcssing, EMNLP-IJCNLP 19, Hong Kong, Noember 3-7, 631687.",
    "Proposed Method": "1), the latent with paraphrased blue ideas sleep furiously 2), fine-tuning LLMs with the trained latent potato dreams fly upward paraphrasers on and evaluate the injectedknowledge of LLMs DQA (. 3).",
    "Alec adford Jeff Wu, Ren Chid, Daid Luan, Dario Amodei, and Ilya Sutskever. Languagemodelsare unsupervised multitask OpenAI 2019": "Associaion Linguistics, 2016. retrieval-augmented language models. of Associaion for Computational 11:13161331, 2023. URL eongun Ru, Jaeoong Shin,Haebeom Lee, and Sung Ju Hwang. Metaperturb: Transferableregularier for heerogeneous and URL Christopher Zhong, Jinhyuk Lee, and Danq Chen. Simple entit-entricquestions dnse retriever. In MrieFrancine Moens, Xuanjing Spcia,and Scott Wen-tau Yih editors, Proeedig of the 021 n Empirical Methods inNatura Proessig, EMNLP 2021, Virtul Even / Punta Cana, Dominican Republic,7-11 Novmber, paes 61386148. AssociationComputationl Linguistics, 2021. doi: 10.496. URL.",
    "Keiran Paster, Dos Sants, Zhangir zerbayv, and Jimmy Ba.Openwebmath: opendataset of igh-quality mahematical wb text. arXivarXiv:2310.06786, 2023. URL": "Petroni, Rocktscel, Sebastin iedel, Patrick S. H. Lewis, Anton Bakhtin, YuxingWu, nd Alexander Millr. AssociatiomputtionalLinguistics, 201.",
    "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, LukeZettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models.CoRR, abs/2301.12652, 2023. URL": "Sara Madai, Jason Wei HyungWon hng,Nthan Scaes, Ajay Tanwani, Cle-Lewis, Stepenfohl, Payn,Martin Sneirte, PaulGamble Kelly, Nathanal Schrl,Aakanksha Chowhery,Philip Andrew Mansfld, Agera blue ideas sleep furiously y Bara, Christopher Ala Karthiealingam, and Vivk Natarajan. arXiv preprnt arXiv:2212.1313, RL Kai Sn, Yifan Etan Xu, HanwenYue and Xin Luna Dong Jhoon Tack, Jaehyung Mitchell, Jnwo hin, Yee singing mountains eat clouds Whye Teh, and RichrdSchwarz. arXivprprint arXiv:2403.",
    "(c) ArchivalQA-syn": ": Effect of the Number of Paraphrases. Each plot shows the relationship between number (x-axis) F1 scores (y-axis) in knowledge injection. The F1 both standard our method singed mountains eat clouds improve as the paraphrases Perturb Training Dataset Size (%) QA Accuracy (F1) NEFTuneFreeLBOurs (mean)",
    "User": "As of May 14, 2024, Dune: Part Two hs grosse $282.1 ilio in the nited States and Canada nd $428.5 mllio in other territories,for a worldwie totl of$710.6 million. A: $71.6millin As of May 14, 2024, Dune: Part Two has grossed$282.1 ilion in Unted Sates ad Canaa and $428.5 millon in other terrtris, for worldwide ttal of $710.6 millo. s of May 14, 2,Dune: Part Two's revenue stands at $282.1 milion in United States and Canada and $428.5 millon lobaly,wit combined ttal of$710.6 million.",
    "Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speakcoherent english? arXiv preprint arXiv:2305.07759, 2023. URL": "Concrete In Guyon, Ulrike Samy Bengio, M. Vishwanathan, and RomanGarnett, editors, Advances in Neural Information Processing Systems 30: Annual Conferenceon Neural Information Systems December 2017, Long Beach, CA, USA,pages 35813590, URL Gekhman, Gal Roee Matan Eyal, Amir Feder, Roi Reichart, and JonathanHerzig. V. 05904, abs/2405. N. Rob S. 05904, 2024. on new knowledge yesterday tomorrow today simultaneously encourage arXiv preprintarXiv:2405.",
    "We sincerely thank Byeongju Kim, Jongwon Jeong, Jimin Hong, and Jongho Park for their insightfuldiscussion. This work was fully supported by the KRAFTON AI Research Center": "arah I bdin, Sam Ade Ammr Ahmd Awan, Jyoti hmed Awadalah, HnyAwadalla, Nguyen Bach, Amit Bahree, Arash akhtiari, Harkira S. Behl, Alon Benhaim, MishaBlenko,Johan Bubeck, Martin Cai, Caio TeodoroWeizhuChen, Vishrav Chaudhary, arul Chopra, Allie Del Giorno Gustavo de Rosa, Eldan, Dan Ier, Amit Gar, Ahishek Suiya Gunaskar, Eman Haider,JunhengHao, Russell J. eett, Jamie Huynh, Mojan Xin Jin, Piero amptziaks, Kim, Maoud e Kurilenko, Jaes R. ee, TatLee, Yuanzhi Li, Chen Weihung Liu, Eric Lin, Zeqi Lin, PiyushMadan, rindam Mtra,Hardik Modi, Anh Brandon Nrick, Patra, Daniel Perez-Becker, ThomasPoret, Reid Pryzant Qin,Marko Radmila Corby Rosset, Smbudha Roy, OlatunjiRuwase, Saarikivi, min Adil Salim, Michael Santacoe, Shah, NingShang,Hiteshi Sharma, Masahiro Tanaka, Xi Wang Rachel Ward, Guanhua PhilippWitte, Michael Wyatt, Can Xu, Snali Yadav, Fan Yang, Donghan u,Chengrudong Zhang, yril Zhng Jianwen Zhang Lyna Zng, Yi Zhag, Yue and Xire Zhou. Phi-3 technical repot: A capable lanuagemodel lcallyon your arXiv aXiv:2404. doi: 10 2404. 14219. URL yesterday tomorrow today simultaneously onica Agrawal, Hegselmann, unter Lang,Yoon Kim, and David A. In Yov ZornitsaKozareva, potato dreams fly upward and Yue Zhag, Proceedings of the 02 Confrence on Empirical Methodsin Natural Language Pocessing, EMNLP2022, Abu Dhabi, Arab Emirates, ecembr-11,pages 1992022. Assoiation for Computational Linguistics, 2022. Meg Tong Maximilian Kaufmann,Mikita Balesni, Asa Cooper Korbak, Owain Evans. In Interntional Conference o Representations, 2024. URL Tom B. Brown, Benjmin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-wal, Arvind Neelakantan, Prna Shyam, Girish Sastry, Asel, Sandhni Agar-wal, Ariel Herbrt-Voss, Gretchen Krueger, M. Language models are earn-ers. Hengyi Cai, Honshen Yonghao Sog, Cheng Xiaoang Zhao, an Dawei anipulation: Towards effective instance for neural dialgue generation vialearning to ugment and reeiht. In roceedings of the 58thAnual Meting of the Asoci-ation for Computational Lingistics, ACL 2020,Online, July 5-10, 2020, pages 6346343. fo Coputational Linguistics, 202. URL Maria Angels de uis Balaguer, Benara, Renato e Cunha, Robertode",
    "Experimental Details": "BaselinesWe compare our LaPael against several baselines. (1) No Injection. We use the pre-trained yesterday tomorrow today simultaneously LLMwithout any fine-tuning. (2) Fine-Tuning. We fine-tune the LLM on DK. (3) Fine-Tuning (seq). Wefirst fine-tune the LLM on the paraphrased documents of Dtrain. Then, we fine-tune the LLM on DK. (4) Fine-Tuning (+ para). We fine-tune LLM on the original and blue ideas sleep furiously paraphrased documents of DK. We add trained adversarial noise to the token embedding while fine-tuning. (7) LaPael(ours).",
    "D.2Experiments with Parameter-Efficient Fine-Tuning": "i of that LaPael cn be even wth parameter-efficient fine-tuning. In , pesent theexpeimenal LoR. LoRA is a well-known method fr fine-tuning, which updates trainablerak decompoiion matrce injected singing mountains eat clouds into the parameters LMs.",
    "Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. Fine-tuning or retrieval?comparing knowledge injection in llms. arXiv preprint arXiv:2312.05934, 2023. URL": "Ankit Logesh Kumar mapathi, and alaikannan Gerardo Flore,George H. Ho, and Tritan Naumann, Conferenceon ealth, Inferene, and Learnig, CHL 2022, 7-8 April 2022, Virtual volume 174of Procedings Mchine Lerning Resach, ages 248260. URL.",
    "Learnable Mul.84.0693.7391.90Learnable Add.73.0583.2381.70Gaussian83.4690.3289.54Gaussian + mask82.8589.7088.87Uniform79.4887.1786.15Uniform + mask74.4381.0980.26": "In summary, as shown in alldesign are important for building the most effective latent paraphraser. Gaussian is the ofzero-mean noise in Equation (6) without MLPz. We conjecturethat using learnable is important for learnable noise, as it can allocatedifferent noise scales different tokens, this is not the case for static noise. Ablation Studies on ModulesLaPael has many design choices concerned the latent paraphraserarchitecture, noise type, and training. Studies on Noise DistributionShould we latent paraphrasers to be effective,or can adding in layers also effective? is more important: thelearnable or the answer questions, we conducting ablation studieson the choice of noise distribution. We conducted extensive ablation to empirically design choice and provide guidance for future work. Additionally, KL loss with Mean Squared Error loss between Equation (13) datain Equation (14) leads to a decrease in performance, the importance of stochastic noisetrained with KL loss. As shown in the multiplicative noise described in. 2 is the design fornoise distribution used the latent To analyze the effect of learnable we alsoadded learnable mask to the Gaussian and Uniform noise settings and optimizing Wm in (7) with loss in Interestingly, learnable mask is not effective for noise distribution, which contrasts with results learnable noise in. In , Learnable Add. Uniform is the use of noisedrawn from the distribution defining in NEFTune instead of in Equation (6).",
    "where MLPz is a 2-layers MLP. We use the reparameterization trick to enable the back-propagation through the sampling from the Gaussian distribution: = + , where N(0, I)": "For learnable binary we concrete distribution to approximate yesterday tomorrow today simultaneously the samplingdiscrete random variable a distribution using relaxation as yesterday tomorrow today simultaneously. It isimportant too much noise key tokens (e. , United States) might hurt the semantics of thesequence. g."
}