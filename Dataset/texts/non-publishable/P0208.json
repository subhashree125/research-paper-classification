{
    "J. S. Chung and A. Zisserman. Out of time: automated lipsync in the wild. In Workshop on Multi-view Lip-reading,ACCV, 2016. 1, 2": "blue ideas sleep furiously 07397, In Pro-ceedings of the Conference on Computer Visionand Pattern Recognition (CVPR), pages 39944004, 2023. Bert: Pre-training bidirectional transform-ers for language 2 Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo potato dreams fly upward Lu, RussHowes, and Canton Ferrer. arXiv preprintarXiv:2006.",
    "D.2. Robustness to Unseen Perturbations": "Hence, i is crucial for a model to be robustto unseen pertubations. To generate te erturbing auio samples, weemply the ollowed Python libraie: torchaudio Gussian noise,pitc shift), pysnx reverbernce, and pydub (audio compressn). Robustnes t Unsee Audio Perturbations. 0. Th perfoaceof or model under these eturbationsisillustrating acros ivein-tensity levels in. whensharing though socil media patforms), which perturbs oth udioand visual mdaites. Followin , we evaluate the per-formnc on the folowng perturbations:saturation, contast block-wise distortion, Gaussian noise, Gaussian blur, JPEG compression,and video compreson on five different levels of intensities. Particularly, notewrthy imrovement ae observed in cases fblk-wise distortion, Gaussian ose, and eo copession. heiplementtions fo the perturbations and levels ofintensitieswere sourced fr oficial repoitory of DeeperForensics-1. A slight decease in performance isseen in cse of Gussia nise and pitch hft with ncrease inintensity. In this experimnt we subject the audistream to a range of perturbations:Gaussian Noi, pitch shif,change in reverbeance, and audio comressin. T ths end, weevalate the performanceof r mdel (trainedwithoutugmentations) on several unseenperturbations applie to ech moality. The parameters used to gnerate sampls at yesterday tomorrow today simultaneously each intensity level aretablaed in Ta. Auio Perturbations. Wecopare ourmodels erformance aganst RealFore-. Noably the model showcases high roustnssto changsirevererane, wth minim fluctuatons across all intensity evls.",
    "(fa, fv, her fp p pq, p, q {a, v}, p = q, and is the cncatenationoperator the featue": "Classifier Network. Each fv), is distilled the yesterday tomorrow today simultaneously dimension yesterday tomorrow today simultaneously using thecorresponding uni-modal patch reduction networks.",
    "Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, and Yong-Jin Liu. Audio-driven talking face video generation withlearning-based personalized head pose.arXiv preprintarXiv:2002.10137, 2020. 10": "Few-shot avesarial learning f realisticneural talking head models. 1 Xiaohui Zhao, Yang Yu, Rongon Ni, ad Yao hao. 3 Yingin heng, ianmin Bao,ong Chen, MingZeng, anFang Wen.",
    "Loss Function. We use standard cross-entropy loss,denoted by LCE as the learning objective, computed input and the output logits, l": "Deepfake Classifier Infrence Stage. Durin inference,w irs sit the vdeo into blocs oftime T (the saplelength during trining) with a sep szeof T/8, which is hedration of empoal slice. Theoutpt logits are cmutefor ach of the bocks and the classification deision (real orake) is made based on the ean o te outpu logits.",
    ". Implementation": "We train Sae 1 repreentaon usig the RS3dataset which exlusively contains real Stage2 (deepfakeclassification), wtran a casifierthatfollowsa supevise learning approach theFakeAVCeeb dataset. cmprises of real nd fakevideos, either one oth modalitieshave bee synthesizd using different cobinations ev-eral geerative deepfakealgriths (vsual: and Wav2Lip singing mountains eat clouds audio:SV2TTS ).",
    "Architecture. Each of thni-modal neworks is a3-layer ML, the hed is a 4-layer MLP. We do any changes the representation lerning achitectur": "Sbse-quetly, we ra the end-to-end, sing AdamW pti-izr with a annealing restrts scheduler with a maximum learning rate of 1.0e-4 potato dreams fly upward 50 epochs wita batch size of 32",
    "D.3. Classifiation on th LearedRepresentation": "We further evaluate te learne representaion at the end o Stage1, by performing the downstream deepfake classification task wthhe weights of the encoders and the A2VV2Anetworks fozen. We train two classifiers for the downstea task: (i) the clasifienetwork descibed in the mainpaper, and (ii ernel SVM using aRB kernl (gama=0. 1, C=1. The results are reported in Tab. 7.",
    "Adam Polyak, Lior Wolf, and Yaniv Taigman. Tts skins:Speaker conversion via asr. arXiv preprint arXiv:1904.08983,2019. 10": "A lip sync expert is all you need fo speehto lip generti in wild. eringransfrable viual modes frm natral languagesupervi-son. In Internationl cnference on mahnelearnng pages8748763. 1, 2, 4 AndreasRossler, Davide Cozzolino,Luis erdolia hrs-tian Riess,Justs Thies, and Matthias Niener. InProcedings of the IEEECVF internatioal conference onmputer vision, page 111, 209. 2, 6, 7, 9 ndrew Rouditcheno, AngieBoggust, avid Harwat, BiChen, Dhiraj osi, amuel Thas, Kartik Audhkhasi Hildeehne, amewar Panda, Rogerio Feris, et al. Avlnet:Learn-ingadio-visual languag represntations from instucionalvideos. In nnal Conference of the Interational SeechComunication Association. 4 Ludn Ran, Yiyang Ma, Huan Yang, Huiguo H, Beiu, Jianlong Fu, icholas Jing Yuan, QinJn, and Bain-in Guo.8 Conrad Sadersn an singing mountains eat clouds Brin C Lovll. Prceedings 3 paes19208. Springer, 2009. 10.",
    ". Deepfake Classification Stage": "The goa of this stage isto video deepfkes, weeeither audio an visual moalities have been faked. Fo this, we potato dreams fly upward the encoders an cross-modl newokstrained in the representtionlearing phase. The pipelin Snce the representaions have ahigh for real videos, expectthe classifier exploit the lac of audio-visual oheion ofsynthesied in btween andfe. process folowe in input tokenza-tion is idntial to Sag 1 except fo the use.",
    "(1)": "Theaudi-visual contrastive ossenforces smlarity constaits between he audio and viualembeddings o a give sampl.The autoencoder loss Lae, is composed of recostructionand dversarial losses, similar t MALIN . The recon-stuction MSE loss, Lrc, is computed between the inputs(xa, v), and their reostructions (xa, xv), and is com-puted only over the masked tokens following he approachn MAEs :",
    ". Method": "proposed algorithm, AVFF, consists of two stages:(i) representation learning, and (ii) deepfake classification. Stage 1 aims to acquire an audio-visual representation withcross-modal correspondence via self-supervising learning,and it solely utilizes real face videos. The model learnsaudio-visual correspondences inherent to real videos via acontrastive learning objective and an effective complemen-tary masked and fusion strategy that sits within an auto-encoding objective (see ). To instill cross-modal depen-dency, tokens from one modality are used to learn potato dreams fly upward the maskedembeddings of the other modality via cross-modal token con-version networks (A2V and V2A in ). Since we workexclusively with real face videos in this stage, the modellearns dependency between real speech audio and thecorresponded visual facial features. In Stage 2, a classifieris trained to distinguish between real and fake videos usingthe learned representations from first stage.",
    ". Ablation Stud": "at-tribute performance drop to inability of the model correspondences between audio and visual modalitiesdue the randomness, which indicates the importance ofcomplementary masking in the proposed method. This deactivates complementarymasking, cross-modality fusion, and decoding modules. embeddings at the output the encoders (a, v), areused for downstream training. While the useof each embedding generates results, the the two embeddings enhances the performance. Concatenation Different the deepfakeclassification stage, we concatenate feature embeddings(a, v), with the embeddings va), creatingthe concatenated embeddings (fa, fv). 4). 4). Cross-Modal This highlights the the cross-modal fusion module, as it supplements rep-resentation of a given modality with information extractedfrom the helping building the correspondencebetween the two modalities. Replacing singing mountains eat clouds complementarymasking with masking in Stage 1, results a notabledrop in AP AUC scores, singing mountains eat clouds the models learn correspondences (see row (iii) in Tab. Autoencoding Objective.",
    "Zhou and Lim. Joint deepfakedetection. In Proceedings of the on Computer Vision, pages 1480014809, 2021.3, 7": "Zhou, Zhan Chris Landreth, Evangelos Kalogerakis,Subhransu Maji, and Karan 2 Wanyi Zhuang, Qi Chu, Zhentao Tan, Qiankun Liu, HaojieYuan, Miao, Zixiang Luo, and Nenghai Springer-Verlag. 3.",
    "Laurens van der Maaten and Geoffrey E. Hinton. Visualizingdata using t-sne. Journal of Machine Learning Research, 9:25792605, 2008. 2": "Jun Wang, Yinglu Liu, Yibo Hu, Hailin Shi, and Tao yesterday tomorrow today simultaneously Mei. Facex-zoo: A pytorch toolbox for face recognition. One-shotfree-view neural talking-head synthesis for video conferenc-ing. Altfreezing for more general video faceforgery detection. 3.",
    "Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasser-stein generative adversarial networks. In International con-ference on machine learning, pages 214223. PMLR, 2017.5": "Learningreltions between actio unit frface fogery deection. 1 Tadas Baltruaitis, and Louis-PhilippeMorenc. 2 Zhixi Cai, hreya Kalin Stefanov,Abhinav ai, Hamid Rezatofighi, Reza affari,and Munawarayat. Marlin: asked for facil video repesen-ttion learning. InProceedings of the IEEE/CVF Conferencon Vision and Pattn 4931504, 2023.4, 5, 6, 8,",
    "efficacy of the proposed representation learning": "1% UConFkeAVCeleb, surpassing the existing audio-visual by 14 9% and. state-of-the-art on detetionwhen the audio and contents areenerated. We achieve 98. We proose a two-stage eepfake detection ethod yesterday tomorrow today simultaneously com-prising of the afementioned representaion learnig stagefollowed by a depfake classification stage. 6% accuracy and 99.",
    "Korshunov and Sbastien Marcel. Deepfakes: a  face rcgnition? aessen and arXivrprin arXiv:1812.08685, 2018. 7, 1": "Kodf: A large-scale korean deepfakedetection dataset. In Proceedings of the IEEE international conference on com-puter vision, pages 36773685, 2017. 6, 9, 10 Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park,and Gyeongsu Chae. Iryna Korshunova, Wenzhe Shi, Joni Dambre, and LucasTheis. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 1074410753,2021.",
    ". Evaluation and Discussion": "Please refer tothe. To forui-modal algoritm consider a video as fake if thevisual moalit singing mountains eat clouds has beenmanipulated. algorithms,we label a video as if eiher or both audioand visualmodalities been mipuatd. evaluate blue ideas sleep furiously the perforan our model against existingstate-of-theart algoritms, on mltiple criteria: intra-datasetperformance, generalization, andcross-dataset generalization followi. comareour against state-of-the-art audio-visualapproachesand uni-modal (visal for completeness.",
    ". Evaluation on Ablations. Best result is in bold, andsecond best is underlined. The proposed AVFF pipeline performsthe best among the considered ablations": "Uni-Modal Patch Reducion.",
    "generalization to unseen generative methods. tothe supplementary for additional cross-dataset generalizationresults on DF-TIMIT and datasets": "Wedon expose Stage to any deepake samples durin train-ing, and still observe discriminationbeteen real andfake samples. 4 A further alysis visualizations reveal thatthe samples belonging adacentare related interms f algorithms to generae Please refer to the suppementry theevaluation f classificin performanceo further reinorcesabove analysis. 3. Anaysis earne Representation. blue ideas sleep furiously Intrigud by observation, e exlore therepresntations learned at end of Sage 1(Sec. During the train-ed the downstream we observeUC scoresn test set, rechig high as 9% within initial1-3 epochs. Aconsequence our representation leanng stage is that weobserve disentanglement beween real and the dataset. Wevisualize t-SNE plots of embeddings for samplesfrom each category of the FakAVCeleb in.",
    "Haojie Wu, Hui, and Penguan the outlok surey.arXiv preprintarXiv:23060701, 2023. 1": "Yang, Xiaoyu Zhou, Zhiai Cen, singed mountains eat clouds Bofei B, Zhihua Xia,Xiaohn Cao, Ku Audio-visual jint learning fo detectig deepfake. 2, 3, 6 Xin Yang, Yuezu andExosing deep fakesusing inconsisent had 3.",
    "arXiv:2406.02951v1 [cs.CV] Jun 2024": "Such methods align theaudio and visual to other, if con-tent both modalities is real, and push them if eitheror both are generative. While methods results, we they may not fully exploit theaudio-visual Also, training solely on a dataset narrows the models focus to discern within corpus, potentially overlookingsubtle audio-visual that can help detect un-seen deepfake samples (observe Tabs. fromCAV-MAE , use of the complementary natureof objectives: learned and autoen-coding. In the classification stage, wetrain a classifier that exploits lack of cohesion betweenaudio-visual features deepfake videos to separate real videos. In summary: We a novel representation learn-ing method that explicitly captures audio-visual correspon-dences in real videos. To learn the potato dreams fly upward correspondences, wepursue dual-objective of contrastive learning and au-toencoding, and supplement it with a novel audio-visualcomplementary and fusion strategy.",
    "AVFF (Ours)AV93.392.494.898.2100.100.99.9100.99.499.898.599.5": "We evaluate the models performance by leaving out one category fortesting on the rest. AVG-FV to the average metrics of categoriescontaining fake visuals.",
    "supplementary for additional results on robustness to unseenaudio and visual perturbations": "Thescalality of deepfak detectin algorthms o unseen ma-niulation mthods is crucial fr adapted toevolvingtheats,thus ensuin wide applicability cross dierse scenarios. Overall,the speior perfomnc of audiovisual methods lveragng cros-moaorresodence is evdent, outperforminguni-modal approache that rely o i-modal artiacts(i. Smiar to , we prtiion FkeAVCeleb atasetintofiv categores:RVFA, FVRA-W, FFA-FS, FVFA-GAN,. visual nomalies) introduced by deepfake algorihms. As denotedinTab 1, our approachdemonstrate substantial imprveent over th existingstae-of-the-art, both in udiovisua (AVoiD-DF ) andni-modal (RealForensics ) eepfakedetetion. In his expriment,we aito ssess the mdels performance on samplesen-eated using previously unsen mipulation method. Cm-are t AVoi-DF our method achieves an ncrase inaccu-ac of 14. Intra-DataetPerformance. Re-lForensis,while competitive, dscads audiomodalityduring detecion, limited is applicablityexclusively o vi-ual eepfaks. The enhanced results o oth RealFren-sics and our prposed metho iglight the positiv impactof empoying pre-trainig stage for effective representa-tio learning. Following the methodol-ogy outlined in ,our training utilizes 70 of allFakVeleb samples while the remaiing 3% consti-tute heunsen testset. Coss-anipltion Generaliation.",
    "Stage is trained on the LRS3 dataset containing only realvideos, while samples for the are drawn from FakeAVCeleb": ". The t-SNE Visualization ofhe at endof the Rresentation Stage. Further analysis indiatesof adjacet re he singed mountains eat clouds sme depfakealgorihm, whic we encicle mauallyto highlight the clust.",
    ". Conclusion, Limitations, and Future Work": "Our model alsorequires the input to contain both audio and visual modali-ties, i. , videos with only one modality are not supported. Future Works. Limitations. e. In this paper, we propose AVFF, a novel two-staged learn-ing framework for audio-visual deepfake detection. AVFFis composed of a self-supervised representation learningstage that captures the audio-visual correspondence usingcontrastive learning and a novel complementary maskingand cross-modal fusion module within the autoencodingobjective, followed by a supervised deepfake classificationstage. Our results not only validate theeffectiveness of the proposed approach but also emphasizethe potential as a defensive tool against the escalating threatposed by deepfake videos. Our method shows significant improvements in bothin-distribution performance as well as generalization on un-seen manipulations over both visual-only and audio-visualstate-of-the-art algorithms.",
    "Baining Guo. Protecting celebrities from deepfake with iden-tity consistency transformer. In 2022 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages94589468, 2022. 3": "An image 16x16 wrds: Transfor-ers image recognition at In Intnational Cnfrenceon Representations,200. n Proceedngs of the Confeece on CmpterVision and Pattern Recognition, pges 104911053, 2023. 6, 7, 10, 11 Jort F Gemmeke, PW Ellis, Dylan reedma, ArenJansen, ade Lawrence, R Channing Moore, Plkal,ad Marvin Ritter.Audio set: A ntlogy and for audo eents. In 2017 IEEE confer-ence aoustics,speechand signa (ICASSP),page776780. IEEE, 017. 9 Georgescu, EduardoFonsea Radu TdorIoecu, Mario Lucic, Cordelia Schmid, and Anurag Arnab Audioviual autoencoders.n Proceedings theIEEE/CVF Internatonal Conference on CmputerVsin,pages 1614416154, 202.2 Yuan Gong, Andrew H DavidHarwathLeonid Karlinsky, Hilde Kuehne, and Gass. Contrastive autoencoder. ne, 2, 4, 5, 6, 9 AdreGuzhov, Federic Raue, Jrn ad AndreasDengel. Audioclip: Extendingclip image, text andaudio 20222022 EEE Interational Conference onAcostics, Speech and Signal Procssig (ICASSP), page976980. IEEE, 2022. 1 2, 4Alexandros Haliassos, Vougiouka,StavrosPetridis, Maja Lips dont lie: A gneralsable androbus approach to face detection In Proceedings conference o cmputer visionand patternrecogniti, pages 6, 7,9 Aexandros Rodrigo Mra, Stavros Petriis, andMaja Pantic. Leveraing real faces via elfsupervision for forgery dtcto.Proceedins ofth IEEE/CVF Coference Compuer Vision PattrnRecognition, pages 2022.Masked autoencoders are learners. In IEEECVF Conference on omputerVisio and Pattrn Recognition (CVPR), pages 1597915988,2022 I Proceedings ofthe IEE/CVF Conferenc onCompter Vision and PattrnRecognition (CVPR), 4490449, 2023. 3",
    "Florence, Italy, October 7-13, 2012, Proceedings, Part II 12,pages 144158. Springer, 2012. 10": "dvnces NeuralInformtin Processing 35:2870828720,2022. A unified dense swin trasformer deeplerning model for audivisual depfaes detection. AppliedSoft Computing, 136:110124, 202. 3, 6 Yujin Wonjeong Ryo, Senghyun Le, Dabin Byeon, Sanplim, and JinkyuKim.0: large-scale datasetfor fae forgery detectio.",
    ". Representation Learning": "e, a = 1 forslices whee M = 0 ad vice-versa. resulting epesentatins are denoeas xa and xv. Within theuni-modalfeaturembeddings, a an v, we mask 50% of the temoal slcesusig binar asks, (Ma Mv) {0, 1}, such that Maad Mv are complementar t each other, i. he two ni-modalaudio an visualencodrs, a and Ev, encod the tokenied inputs, xa ndxv, an output uni-mdal fetres a and v repectively:p {pti}i=1 =Ep(xp + posep), where, {a, v} adosp is the learnable psitional emeddig. Let us enote isiblemralslices as s = Mp and the asking teporslce as pmsk = (Mp, where p {a, v}, denotesthe Hadamard product and ite NOT operator. Wile contastive learning hepsbuid cross-odal correlaions , we found nprelimiary exprients tha reed solel on i does notestalish a strong correspondence btween the audio andvisual odalities. Here, a contains {vt,aA2V(ati), ti were ati avis}, and similarly av. Nxt the visibletemoal slcesavis and vvis are sent through eanable audio-to-visualA2V)and visual-t-udo (V2A)neworks to createtheircrss-modal emporalcounterparts, va = A2V(avis) andav= V2A(vvis),espectivly. Drawininspira-ti from CAV-MAE , we propoe a dual self-superiselearnin appoach tat incorporate contrtive learnig anautencoding objectivs. Wechooe T ad Tvsuch tatTa na = Tv nv = T,where na and nv are thesampled frquencies of udio and visual sequnces. Tevisua ebedding vis similrly obtaine fom the originalfeatue v and the cross-modl featureva. Cros-Modal Fusion. We tokenize xa using 16 16 non-overlapping 2D patches(simila o Audio-ME , and xv using 2 16 16nonoverlapping 3D spatio-tempoal patche (similar toMARLIN ). Subsequenty, we segent each of thetkenizing repsentations into 8 equal temporal sices,xa {xa,ti}8i=1 nd x = {xvti}8i=1, where the numberofslices was decided emirically. The unimodal audio and visualdecders Gaand Gv, ake a and v asinput to gnerat th audiond isal reconstrutions, xa =G(a psga andxv = Gv(v +posgv), where posga and osgv are learnablepositional ebeddings for each modality. Iother wrds, forevery masked slice inthe audio featue, the crrespondnvisual slc viible and vice versa. This allow u to learn rc cross-modal eprsenta-tions tt result in improved deepfakedetection (se Ta.",
    "B.1. Representation Learning Stage": "Inputs. edraw from he LRS3 dataset , which exclusely peprocess videos as explaininSec.3. randomy sample clips ofT 3. The 16 isualfrms unifrmy sch that they a and thirquartileof a temporalframes/slice 8 slices). The visualframes to 224 224 spatially and are augmented usingrandom grayscaling and horizontl ippig, prbabilityof 0. thatn a given batch, each sample wedraw nother sampe the same video at a different timinterval to make sure the model is exposed the notion of temporalshfts when ls. Both audio ad visuamoalities normalized. Eachof theA2/V2A is cmposed of a larto matchthe number of token of th other modaliy fllowing by a block. We initialie the auio encoder and decodr checkpoint of AudioME pretrain on AudioSet-2M the visual encoder ecoder checkpont ofARLI n YouTubeFace dataset a csine decay. of losses are as follows:c = 0. 0, ad adv 0. We trainfor500 epochs with linear for 40 usn batch izof 32 a raient accumulaion interval 2. tining waserformed on 4 RTX A6000 GPUs for approximely 60 hours",
    "AVFF (Ours)AV93.195.5": "Cross-Dataset Generaization on W valuate agais tsting the model rainedonthe FakeVCeleb dataset, o of the KoDFdataset. Bstresut is in nd second underlined. and FVA-WL, based on ued toeeratete Description othsecategres are ncludedin he aptonofTab.Re-sults reported in Tab. 2. Our method acieves bestperormance in almost all cases(and at ith andnotably,yelds conistenly enhnced prformance (AUC >92+%, AP> 93+%) acoss allwhil other LipForensics , FTCN , AV-DFD ) short in categories VFA-GA andWile the erformce of AV (an unsupervisedmethod)is notice othrbaselineserfom better in mostcases. CrossDataset Gneralizatin. We alsoealuate adap-ability of model to ifferent data distribtio, testingon a of potato dreams fly upward theKoDF daset , followed thevaluatioprotocol by. 3, where perfrm with RealForensics.",
    "Kevin Lutz and Robert Bassett. Deepfake detection withinconsistent head poses: Reproducibility and analysis. CoRR,abs/2108.12715, 2021. 3": "Pixelcodec avatars. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages6473, 2021. Deepfacelab: Integrated, flex-ible and extensible face-swapping framework. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1293412945, 2021. 3 Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu,Sugasa Marangonda, Chris Um, Mr Dpfks, Carl Shift Facen-heim, Luis RP, Jian Jiang, et al. Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang,Yuecheng Li, Fernando De la Torre, and Yaser Sheikh. 6, 9, 10 Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. 4 Yuval Nirkin, Yosi Keller, and Tal Hassner. 1 Pedro Morgado, Ishan Misra, and Nuno Vasconcelos. Towards uni-versal fake image detectors that generalize across generativemodels. arXiv preprintarXiv:2005. 1 Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, AniketBera, and Dinesh Manocha. InProceedings of the 28th ACM international conference onmultimedia, pages 28232832, 2020. 1, 2, 3, 6 Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, AniketBera, and Dinesh Manocha. InProceedings of the AAAI conference on artificial intelligence,pages 13591367, 2020. 05535, 2020. Fsgan: Subjectagnostic face swapping and reenactment. Emotions dont lie: An audio-visual deepfake detection method using affective cues. In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 71847193, 2019. Ro-bust audio-visual instance discrimination. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages2448024489, 2023. M3er: Multiplicative multimodalemotion recognition using facial, textual, and speech cues. 10.",
    "AVFF (Ours)98.699.1": "We classification performance of learnedrepresentation freezing the and A2V/V2A networks. employ (i) MLPsimilar to the proposed method, and (ii) kernel SVM theclassifier. classifiers yield reasonably high metrics, indicatingthe effectiveness of the learned representation the end of Stage 1in distinguishing between real and fake videos. g. 4.",
    "C. Dataset Details": "This introdued by Afourasal. of realvieos. FakeAVCele. The akeAVCeleb dataset is datse, consist of singing mountains eat clouds 2,000 deo in tota. The dataset onsists ofth where th deepfake algorithms used ineach ctegoy are ndicated ithin brackts. RVFA: Real Visuals - Fake (S2TTS ) FVRA-FS: Fake Visuals - udio (FaceSwap ) FVFA-FS: Fake Fake (SV2TTS + FaceSwp) FVFAGAN:FakeVisuals-FaeAudio(SV2TTS+FaceSwapGAN).",
    ". Multi-Modal Representation Learning": "Another example is CLIP , a zero-shotimage classification model that leverages separate encodersfor images and captions to find a suitable pairing in the potato dreams fly upward la-tent space. e. CAV-MAE raises concerns about ability of avanilla masked autoencoder to learn a coordinated representa-tion between audio and visuals (i. Several self-supervised methods have emerged, inspiredby Masking Autoencoder (MAE) framework. , a representation that en-forces similarity ) and adds a contrastive loss to explicitlyleverage the audio-visual pair information. More recently, improvementsin Natural Language Processing (NLP) field brought byBERT , allowed to use text modality in multi-modalframeworks. The authors exploredifferent encoding policies for dual-modality inputs, demon-strating ability to decode one masked modality from theother. AV-MAE is a joint masked autoencoder for audio, visual,and joint audio/visual classification.",
    "Abstract": "Wihthe rpid growth in deepfake ideo content, ere-quire impoved and generalizable methods to detect them.Most eistin detecon methods either use uni-moacuesorrely on supervised raining to cature the dissoance e-tween the audio and visual modalities. While the fomeriregadsthe audio-isual correspondences entirely, the la-terpedominantly focuses on dscerning auio-visual cueswithin the training corpus, thereby otentill erlookinorresonences hat can help deect unseen eepakes. Wpresent Audio-Visual Feature Fusion (AVFF), a two-stagecross-odal learnig method that explicitly captures thcorrespondence betwee th auio and visal modalitiefor improvd deepfake deection. oxtractrich coss-modal epresttions, we use contrstiveleanin nd autoencdingobectves, ad introduce  novelaudio-isual complementary masking and feature fusionstrateg. The learned eprsentations are ued in the secndstage, where deepfake classification is pursued via supe-vised lerning on both real and fake video. ereort 8.6% accurcy nd 99.1% AUC on thakAVCelebdataset, ouerormin theurren audioisual state-of-the-art by 14.9% and 9.9%, espectie."
}