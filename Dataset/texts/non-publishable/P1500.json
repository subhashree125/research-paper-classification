{
    "Related Works": "hesefeatures are hen converte nto oens and fed int a transformer-base network captre andtemporl dependencies, with an SMPL rgresor transformed them 3D full-body pose equeces. stimats geometry of surounded objecs and 2D body pse featuresusingEgoPW to regress 3D body pose with a network. Recentl, several hae beenthis: AvatarPoser i first potato dreams fly upward learning-based to predict full-body poss woldcoorintes using only head motion ints. It uses a Transformer encoder to extract deepfeatures anddecouplesglobl motion fromlocal rintaions,armposition wth iversekinematics for accurate full-body motion BoDiffusion eploys a generativediffusion od forotion addressing h under-conraining recnstrucion AvataJLM a blue ideas sleep furiously two-stage spare hgh-dimensional feature and processe by an MLP to generate jont-level eatures. Itis worth notng a concurrent work, EgoPer, which also addresse ego body estimtion from doubly sparse oervations. fus lies in preaing trainin daa than introducing new wokis orthoonalt their, providingalgorithmic hrugh multi-stage pipeline incluing uncertaity-awaremaskedauto-encoder (MAE). deigns a policy to generate perframe moton from egocntric inus,and a dynamics model to distill human dynamics inormatio into the kinematicodel. GIMO integrates motion, eye gaze, and 3D scene generate term intenion-aware humanmotion rediction. Human Body Pos Estimationfrom Egocenric VdeosEstimating full 3D body poefrom egcentric videos isan ill-posd roblm de to the vsibilityof wearers body partsfrom the cameramoune on wearers head.",
    "Z. Luo, R. Hachiuma, Y. Yuan, K. Kitani. kinematic policy foregocentric estimation. Advances in Neural Information Processing Systems, 34:2501925032, 2021": "of theIEEE conference on visionand pattern Reognition, pages 50795088, 2018. M. Mon, J. Pons-Moll, Amas: captre surface shapes. Mahmood, N. V2v-posene: Voxel-to-voxel predctinnework foraccurate hnd and umn poseestimationfrom a single depth ap. N F Troje, G. N.",
    "In summary, our research presents three key contributions:": "robust andversatle framework for egocntric bod pe etmation tailored for HMD.The framework adapts tovarious R/VR settings and cn leverge trackig signals availablein most odern MD devices without controllers. e decomposing problem into temporal completion an spatial ompletion. Our approachcaptures the uncetainty from hand trajector imputaion to guide the diffusin model foraccurate full-bdy motion generatin. Extenive yesterday tomorrow today simultaneously evaluatins demonstratinghe effectivenes ofour framework on diverse tasets,outperforming existing methods and nderscring itsptential orehancinguser singed mountains eat clouds interactionand immersionin AR eperiences.",
    "Full ose Estiation from Spatialy Sarse": "dense ata setup where s no uncertainty rgardng hand poses, thedense irectly oks as a for singing mountains eat clouds spatial compleion the right sde of. 2, highe MJVEfrom the inherent stochasticity of the dffusion model.",
    "error H2DI K(H3Dh+ d)2. Here, K is the intrinsic matrix, obtained by transforming the originalcamera parameters into a pinhole model through undistortion": "However, the inconsistent quality of detection, the informationderiving from the hand pose was noisy. In AMASSdataset, we leveraged both rotational information and 3D location, as this is readily available,resulted in = 9.",
    "D.1Discrete Diffusion Model": "Discrete diffusion models a category of diffusion models that yesterday tomorrow today simultaneously progressively introducenoise into data training reverse this process. encoder E(x) compresses input data x discrete vectorsby mapping each encoded representation closest vector zq the entry from alearned codebook of using the nearest-neighbor search: zq =Q(z)=argminciC||z ci||2. Here, C = {c1,. , cK}, where K is the total number of codebooks. The transition probability from token zi to zj at diffusion is definedby matrix yesterday tomorrow today simultaneously Qt[i, j]. The transition matrix Qt, structured R(K+1)(K+1), follows:.",
    "Full Body Pose Estimation from Doubly Sparse": "To demonstrate the effectiveness of our framework on doubly sparse egocentric data, results our framework, DSPoser, on AMASS and Ego-Exo4D, asshown in and , respectively. Since of pose estimation from doublysparse data newly introducing in our paper, we compare our to other baselines, Bodiffusion AvatarPoser , and AvatarJLM . Those designed to estimatehuman body poses from data. EgoEgo estimates body from poses, and poses from head and hand tracking signals. We report experimental resultsusing sampling strategy with aleatoric uncertainty unless stated. To the baslineson temporally sparse data, the algorithm as follows: (1) Interpolation: we handposes with interpolation; (2) MAE: we use MAE to impute the hand trajectory. = 1 setup, we our result after 16 samples while the result in = 20 setup isfrom a sample. As shown , DSPoser outperforms baseline methods on across allmetrics, the effectiveness of our two-stage pose estimation.DSPoser achieves notable in MPJPE for both sliding window sizes, Ts = 20 = 1. For Ts = 20, DSPoser reduces MPJPE from 7.35 cm 5.51 cm, significantly which uses MAE to hands. Ts = 1, : Performance comparisons models for doubly the Ego-Exo4D validation report MPJPE and [cm/s], the highlighting inboldface. Models trained by marked with . The notation....Data denotes temporally sparse data,data imputed data, other cases involve dense data.",
    "adoted thedataloader ofAMASS a evaluation Bodiffusion for our eperiments. For the Ego-Exo4 dataset, employed th dataset fromh Ego-Exo4D": "Analaysis on Computaional computational cost of our approach number of parameters, multiply-accumulate operations and inference time foreach module in pipeline. The reporting times the total measured time singing mountains eat clouds for theentire pipeline, whereas measures the only for module, excludingoverhead between modules. As shown in , the VQ-Diffusion model is responsible for themajority of MACs and inference time. with 50 steps inferring with 25 steps yields approximately 4x faster potato dreams fly upward inferencetime only about 3% in performance.",
    "AMASS datasetThe AMASS dataset is a large human motion database that unifies differentexisting optical marker-based MoCap datasets by converting them into realistic 3D human meshes": "Tsndicates the sliding window,x yesterday tomorrow today simultaneously indicates the inpt of our whle pipelie, and y indicates input ofdenosing Transfomer. Models traning by us ar marking with.",
    "M. Mller, T. Rder, M. Clausen, B. Eberhardt, Krger, and A. Documentationmocap Computer Graphics Technical Report CG-2007-2, Bonn,7:11, 2007": "G. Choutas, N. Ghorbani, Bolkart, A. A. Osman, D. Tzionas, and M. J. Black.Expressive body capture: hands, face, and body single image. In Proceedings of conference on computer vision and pattern recognition, pages 1097510985, R. Rombach, A. D. Lorenz, P. High-resolution latent models. In Proceedings the IEEE/CVF conference on computer visionand pattern 1068410695, 2022. Y. Rong, T. Shiratori, and H. Joo. Frankmocap: monocular 3d whole-body regression and integration. In Proceedings of the IEEE/CVF International Conferenceon Computer pages 17491759, 2021.",
    "(c) AMASS Groundtruth Prediction": ": (a) Ego-Eo4D video faes, (b th correspdng skleton ground tuth and our predictionresults, and (c) qualittive reults on AMASS datunder different nput coditions. edonlyestimates body pose fom ead trajectoies, whreas us estimtesody pose from imputed andand head trajectoies.",
    "22i xn).(3)": "T LLL loss function causes the predicted variane o at as a weighting factorfor ech datapoint,emphasizing hoe withhigher variances. The parameter adjuststh intensity of this whtin. heg( function isuse t apply the top-gradent operationthu peentin gradients from propagatingtrough this part of th computation.",
    "Bodiffusion 20&....Interpolation& 46.4575.3317.99Bodiffusion 20&....MAE& 7.3531.335.47DSPoser (Ours)20&....MAE& 5.510.0224.190.104.090.02": "Intepoation& 25 0268. 0716. MAE& 60. Since Ego-Exo4D doesnt have the anotations for 6Drotation, MPRE is only AMASS. We asoprovidedetails on MPJPE across hands, uper body bove pelvis and lwer bodyelow the pelvis, denoted as Hand PE, potato dreams fly upward Upper PE, and Lower PE, repectively. 37AvatarLM 1&. 9AvatarJLM 1&. 1&. Interpolatio& 40. To detemnevisibility,we compue theangle the z-axis vector of the yesterday tomorrow today simultaneously eadrotatin the vecto fromhe head poition to teWe the as \"visible\" this is a 45 range,corresponding a 90 view (FoV) o HMD devices. 1349 120 10represented by model parameters. MAE& 5. 80. W reprt he confdence interval of95%.",
    ", where Qt = tI + t11.(7)": "Conditionalenoiing Process. I theconditionl deosinga neural nework to predict heoriginal, noiselesstoken z0 given a nd the assoiaedcondition, ch a ebeddedhand rajectories. traniiofrom step t s xpressed as: q(zt|zt1 = where vzt) encoded vecto representing tokenindex of Tis defins th robabilitiesas t = ti=1 i,= 1 ti=1(1 i), and t= (1 tt)/K.",
    "MethodsyMPJPEMPJVEMPJREHand PEUpper PELower PE": "8337. It is evident that by temporally sparse hand pose our DSPoser framework enhances pose estimation accuracy. 901. cm, which represents an improvement thenext by 5. 49 Additionally, DSPoser an MPJVE of 86 cm/s, improvingupon basline of naive extension of Bodiffusion by 7. 69---VAE-HMD & 6. & 5. The model outperformsexisting baselines, a MPJPE of 16. 142. 261. 3520. 0843. 64 cm/s. 84 cm in = 20 setup. 12 cmto 16. 082. 994. 51 cm, while on the Ego-Exo4D dataset, 19. 06AvatarJLM & 3. 2416. 888. 9710. 726. 20DSPoser 3. 241. 093. 233. 58---AvatarPoser & 4. indicates that even sparse hand trajectory data, when effectivelyutilized, can provide crucial information for refining the accuracy of ego body estimation. 792. 2028.",
    "U(x)) andregar as the onditioning y, where (x) = M 1": "While it would be ideal to sample multple timest betterapproximte marginalization in Equatin 1, wefind js usin one sampl povdes ometitiveperformanc. The pobaiity ofthe d-thdimenion of (x) being zeo is pd =1 Ud(x) Udmin(x))/(Udmaxx) Udmin(x)) whereUd(x) is thed-th dimension of U(x) and Udmi(x), Udmax(x are the miium nd maximmvalesover the squence length, rspectiely.",
    "J.L. a,J. R. Kirs, E. Hinton.Laer normalization. arXiv preprintarXiv:1607.06450,2016": "A. Castillo, . Ecobar, G. Jeanneret, A. Arbelez, . Thabet, and A. Sanakoeu. oiffusion: sparse for full-ody oion synthesis.of Cnference on Visin, pages 42214231,2023. S. Chi, Chi, H. iddiqui, K. Ramani, an K. rom text with discrete diffusionmodel. In conference on Spriger, 2024.",
    "Spatial Completion: Uncertainty-guided Body Pose Generation from Imputed HandTrajectories and Head Tracking Signal": "As illustrated in , our motion genratio module is designed to generate human motionseqencs from tetemporally ese hand and head trajetoriswith ucertainty obtained frm heMAmodel. VQ-VAEWe first train the VQ-VAE to reesent human motion with a discrete coebook repre-sentation as described in Apendix D. Denoisig TranformerMotivted by th work of VQ-Diffusion, we designa denising trn-former that estimtes the distributio p(z0|zy). We closely fllow the implementation of. We concatenated thestimatd hand and head rajectory wih cdebook after a embedding layer, to math the imnsionwth codebook representatin. Finally, we use the decoerto decode z0 to obtain a full body posesequence.",
    "Conclusion": "Our results indicate significant improvements over existing methods,particularly in scenarios where dense sensor data may not be yesterday tomorrow today simultaneously available or practical. In this paper, we have addressed the problem of egocentric body pose estimation using potato dreams fly upward temporallysparse observations from head-mounted displays (HMDs). By leveraging both temporal and spatialcompletion, our approach effectively utilizes intermittent hand pose detections from egocentricvideos, alongside consistently available head pose data, to reconstruct full-body motions. Potential biases in thedatasets used could result in uneven performance across various user populations. Throughcomprehensive experiments on datasets such as AMASS and Ego-Exo4D, we have demonstrated theeffectiveness of our framework. However, our method hasnot been explicitly tested for fairness across different demographic groups. This advancementopens up new possibilities for beneficial augmented reality experiences in various applications,including sports training by providing feedback on body mechanics, and other scenarios where usersneed to move freely without additional sensors such as hand controllers. Careful curation oftraining datasets is necessary to prevent unfair failures for underrepresented groups.",
    "Detection: Hand Estimaion from Ecentric ideo": "inally, we determine the 3D hand joint postionsin singing mountains eat clouds thecamercoordinate system, H3DI= 3D dby solving fo singing mountains eat clouds R3 that minimizes the reprojectin.",
    "Ablation Studies": "Based on ablation study results shown in Tables 4 and 5, we can analyze impact of differentuncertainty guidance strategies and types of uncertainty on the performance of the model for bodypose estimation. The ablation study is conducting with AMASS dataset with the sliding windowTs = 20 to better analyze the effect of the uncertainty guidance. investigates the effectsof various uncertainty guidance strategies, included no uncertainty guidance, sample, distributionembedding, and dropout. The results suggest that incorporating uncertainty guidance through thesestrategies can improve the models performance across different metrics. The sampling strategyachieves the best performance, with lowest MPJPE of 5.51, MPJVE of 24.19, and MPJRE of4.09, indicating its effectiveness in capturing uncertainty and improving pose estimation accuracy. examines the contributions of different types yesterday tomorrow today simultaneously of uncertainty, including epistemic uncertainty,aleatoric uncertainty, and total uncertainty. The results show that accounted for aleatoric uncertaintyleads to the best overall performance. This suggests that considering data uncertainty can providecomplementary information and improve the robustness of the pose estimation model. Overall,the ablation study highlights the importance of incorporated uncertainty guidance and consideringdifferent types of uncertainty in the model design for accurate and reliable body pose estimation. In , we analyzed the effect of different values on AMASS dataset during the uncertaintycapturing process of the Masked Auto-Encoder (MAE). The results, shown in the table, indicatethat = 0.5 provides best temporal completion for head and hand 3D positions from the doublysparse input. Therefore, we set to 0.5 for training the MAE.",
    "D. Yang, D. Kim, and S.-H. Real-time lower-body pose prediction from sparseupper-body tracking In Computer Graphics volume 40, WileyOnline Library, 2021": "Mo, J. Su, C. X. In rceedings o the IEEE/CVFConference on Coputer Vsion and attern Recognition, pags 143014740, 2023. Realistc full-bdy tracking from sparseobservations v joint-level modelng. Zhag, X. u, Y Lu, C. Shn. Jin. Y. In Prcdings of the IEE/CVF Internationl ConferenceonComter Vision, ages 1467814688, 2023. u,X. In Eopea onfrnce on Computer Vsion,paes 676694. K. Wen, Z. Springer, 2022. Li, T. Zang, H. Guibas. Zhang, Y.",
    "Temporal Completion: Hand Trajectory Imputation from Sparse Hand Pose": "Masking Auto-Encoder (MAE)In our work, we employing a Masked Autoencoder (MAE) to impute missing hand trajectories used head tracking signal Thead and detected hand pose....H .Inspired by Vision Transformer (ViT), we treated each T and....H at time as a token similar toan image patch in ViT. To accommodate this, we implemented two embedding layers, one for headtracking signal T RDhead and the other for hand....H RDhand, both projecting into the commontoken dimension DM. For the AMASS dataset, we follow the head tracking signal representationDhead = 18 as in . For the Ego-Exo4D dataset, Dhead = 15, which includes head position andleft/right IMU signals. Consequently, the total number of token amounts to 3 Tw, where 3 accountsfor the head and both hands, and Tw is the sequence length. Sinusoidal positional encoding (PE) isusing for both the encoder and decoder patches after tests showed it suffices for learned differentmodalities, compared to learnable PE. In an HMD environment, we assume that head trackingsignal Thead is always available, but hand visibility depends on the egocentric video. Thus, maskingis applied only to the hand tokens based on their visibilities within egocentric view. In contrast to the MAE training approach, which maintains a consistent number of maskedpatches due to a fixing masking ratio, count of frames with invisible hand varies across instancesin our setup. To address this variability, our encoder selectively applies attention masking to theseinputs, ensuring that queries do not attend to tokens where hand is invisible. This attention maskingtechnique adapts dynamically to fluctuating numbers of missed frames across the instances,enhancing the models ability to handle data sparsity effectively. For decoder, we adopted MAEdecoder design except the last projection layer to guide the uncertainty. To capture the uncertainty,we split the final projection layer into two heads for mean and variance of a Gaussian distribution. Uncertainty-aware MAEFollowed the , to make the MAE aware of predictiveuncertainty of imputed hand pose sequence, we employ the -NLL loss function to manageuncertainty by using a set of mean heads i(x) and variance heads 2i (x), which are deriving from Mmodels initialized differently, where x = [....H ; T ] is an input to the MAE and i [1, M]. The meanheads i(x) and variance heads 2i (x) are trained using the Gaussian negative log-likelihood loss,which applies to each sample indexing by n with input xn and ground truth hand pose sequence yn."
}