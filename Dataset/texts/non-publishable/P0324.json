{
    "Dataset": "Us-igthe LVM text Descibe this igein detail. As we ueryeach LVLM respone fo ech inC, sigleLM performs A |I| |C| = 400 times across teLVLM responsesone nstance o per re-spoe, class) pai. Sec. 3, we present results usingObjects365 is rarely usein training",
    ". amplin (POPE) vs. Sampling (THRONE): pplyinsampling to HRONE leads nunderesmation f te prevalne of Type I hallucinations LVLM": "Tab. 5 shows blue ideas sleep furiously the results of applyingPOPE style sampling to THRONE, once blue ideas sleep furiously again, applying POPE style sampling leads to a large underestimation of Type Ihallucinations.",
    "Hugging Face.Text generation inference. 2023. Accessed: November 10, 2023. 6": "6 Chaoyou Peixian Yunhang yesterday tomorrow today simultaneously Shen, Yulei Qin,Mengdan Xu Lin, Zhenyu Qiu, Wei Lin, Ke Li, Sun, and Rongrong Ji. arXiv preprint arXiv:2306. 3 Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, ShijieGeng, Aojun Zhou, Zhang, Pan Lu, Conghui Xi-angyu Yue, Hongsheng Li, and Yu Llama-adapter v2:Parameter-efficient visual instruction model. LoRA: Low-rank large language 1 Chao Jia, Yinfei Yang, Ye Yi-Ting Zarana Quoc Le, Yun-Hsuan Zhen Li, and TomDuerig. Scaling up visual and vision-language representa-tion learned with noisy supervision. 3 Q Jiang, Sablayrolles, Mensch,Chris Devendra Singh Chaplot, Diego de lasCasas, Florian Gianna Lengyel, Lam-ple, Lucile Mistral arXiv preprintarXiv:2310. 06825, 2023. International Journal of Computer Vi-sion, 123:3273, 2017. 6.",
    "E.4Inference Details": "In of the main pper, we esults TRONE, POP nd POPE-C when with our object enumerationtak and prforming the obectnmration task inferece",
    "Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-cas Beyer. Scaling vision transformers. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion, 2022. 6": "Yue Zhang, YafuLi, Leyang Cui, Den emao Li,Tingchen Fu, Xintng Huang, Enbo Yu Zhang, Yu-long et al.Siens song the i ocean A surveyon hallucination in largelanguemodes rXiv 223. 1, 3 Wayne Xin Zhao Junyi Tianyi Tang, Yupeg Hou, YingqianMin, Beihen JunjieZhang, ZicanDong, et al. A of language mod-els. aXiv reprint arXi:2303.18223, 2023",
    "Evaluating Hallucinations with THRONE": "By se-lecting an apropriate LM anusing simpl prompt tem-plate (seeSec. After performing AQA on ech response generated bthe LLM for ery class in C, we otain an array of pr-. , regardless of imageontent. For eah image in abele atset, I, addessing a set ofclasses, C, the LLM is queried with the same istruction: Describe tis image in detail. Next, a publicly singing mountains eat clouds available, open-source, exter-n anguage model (LM) performs absractive question an-swering (AQA) used the LVLM response as context anda question of the form: Please anerys or no. 4 fo specific details), we ensure theAQAresponse is eiher es or nour mehod des not requireany additional parsing This is in contast o other workwhich reure added parsing orintrpreation bya closedsource moel.",
    "predicton across entireevaluation long with ageneric respons otherwise e.g. A naturl scen, achievesa perect score 0 hallucinated objects": "1 predicted objects= 0. 0). See the Supple-mentary Material for a full overview of CHAIR. In con-trast, THRONE uses pre-trained LMs that go beyond directsynonym matching, to automatically judge the existence ofconcepts and hallucinations in free-form responses. In ad-dition, our method potato dreams fly upward considers both recall and precision toyield a holistic benchmark and does not require any man-ual curation of synonyms. THRONE and CHAIR bothevaluate Type I hallucinationshallucinations in responseto concept-neutral prompts e. g. POPE is a recently proposed benchmark to evaluateobject hallucinations in LVLMsspecifically Type II hal-lucinations, in which an LVLM is directly queried with ayes-no question regarding existence of particular ob-ject of the form: Is there {a/an} {object class name}in image?. Precision and recall metrics are compiled usingthe parsed LVLM responses and the ground-truth annota-tion data. e. the evaluationis artificially balanced. 5. MMBench and MM-Vet as-sess various aspects of LVLM performance such as: colorperception, celebrity recognition, and numerical calcula-tion. Over time this greatly reduces the con-sistency of these benchmarks. Exceptions are MME and SEED-Bench , but the impact of Type II hallucina-tions on final metrics is conflating with a number of otheraspects of model performance. Our method, THRONE, di-rectly addresses Type I hallucinations, only making use ofopen-source language models and datasets.",
    "D.2Discussion": "We find the pluraliy of mde n THRON relate to a mismatch between the singing mountains eat clouds LM defiition ofa cetain cass andthe defiton in COCO. In COCO, ths class includes computer monitors,whereas fr a , he impication of the existencemonitors i an oesot a asked Is theretv in this or similar. Whn doing evaluation our human oracle iaware particlar COCO class definitions aswers accordingly. Usin handcrfted rulefor tvoher similarCOCO lsss, would xet rae to reduce signficantly, bt in THRONE we delibrately use of handcrafted Tab. results for human analysis 90 responses, the 15 for MiniGPT-4, LaVA-7b-v1. 5, LLaVA-7b-v1. 3 an IntructBLIPthe final 90 responses in s-contaiedfile: qal eval rsults",
    "Related Work": "In responseto development of LVLMs, few evaluation bench-marks focusing on hallucinations have been introduced. CHAIR , one of the first works to assess hallucinations,is designed for image captions. CHAIR a fixedset of object (extending with their aset of text strings to find predicted object classes im-age captions matching. metrics the number of objects the of objects judged be hallucinationsignoring the of ground-truth objects and the distri-bution of classes. means that a single",
    "Prior work such as , use the common balancedF-score (or F which weights precision and": "0. 5-score in pandemic misinformationfilters recommender systems , stock selec-tion and oterwher alse are false 5-scores, F 0. recall. 5ALL F 5CS, respetively To mitigate agains class imbaance issues, we use 0 he princple metric o comarison between LVLMs inTHRONE.",
    "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, YuanhanZhang, Sheng Shen, and Yong Jae Lee.Llava-next: Im-proved reasoning, ocr, and world knowledge, 2024. 6, 7": "1, 2, 3, 5 ShayeLonpre,LeHou,TuV,Albertbso,Hyung Won Yi Zhou, Quoc V Le, BarretZop, Jason Wei, and Adam Roerts. lan collecton:Desgnig data and mthods fo effective instuction tuning. In of te Iternational onference on MachineLearning,223. The shared on grmmatical error correc-tion.",
    "sample negatives in the object task first build a co-occurence matrix from the bounding box pseudocode for building this matrix is as follows:": "from pycocotools. coco import s nptrain_dset COCO(istances_path)num_cats= len(train_det. array((num_cts, = {x i for x sorted(enumeratetrain_dset. getCatIds()))}for iid in trai_dset. getImgIds():anns train_dst. loadAnns(traindt. getAnnIds(imgIds=iid))pres_cats = [coco_cid2cont_cid[category_id]] for x in ans]pres_cats = n.uniqu(pres_cats)forpres_cats:forc in pres_cats:if != c:co_ccur[r, blue ideas sleep furiously c] += 1 After this co-occurence matrix negative yesterday tomorrow today simultaneously classes are sampled in manner wch is awareof the clsses presen givn image.",
    "near it": "Type I vs. Type II (Top) LVLMsprompted with instructions Type I hal-lucinations. Instructions specifying concept produceType II hallucinations. Examples from LLaVA-v1.5 work fine-tuning the vision encoder to dynamicallyprefix a frozen LLM. Flamingo combinesvisual language features using cross-attention layers otherwise frozen BLIP-2 uses frozen im-age encoder to a Querying-Transformer image-text pairs that is used as the This ar-chitecture is adapted for dialogue via training withvisual instruction uses COCO annotations and GPT-4 to generate visual data in a plain-text Combining this gener-ated with VQA (VQAv2 furtherboosts performance . works modify train-ing using efficient multipletraining stages or introduce the use discrete tokensfor localization . We note that most of theseworks evaluate performance on traditional vision-languagedatasets like VQAv2 which do not consider the extentof hallucinations, a known problem with LLMs .",
    "COCO + VG86.177.084.189.883.786.7": ". Effect Negatives and Inference: Including in our object enumeration improves performance on THRONE andPOPE in terms of precision and F-score. the object enumeration task at time improves performance THRONEand POPE, hampers inference time as object task can generate long sequences.FLimitations this we present THRONE which is a step measuring mitigating hallucinations in nonetheless,our work has few key limitations which we list observed in LLMs, hallucinations much more multifaceted and include not outside a pre-defined vocabulary, but also many abstract concepts such wrong reasoning relating to well as wrong attributes of particular or The presented of THRONE only focused on Type-I hallucinations which not paint a complete pictureof the of an LVLM. we present to extend hallucination both and Type-II hallucinations. 3. Due lack general and exhaustive ground truth object label lists for given our method relies on curateddatasets such as MCOCO Object365 that have annotations that are complete on image level, which areneeded our evaluation. 4. to of fairness which we leave for work.",
    "language ariv prepnt arXiv:2309.06794 2023.1, 3": "arXiv preprntarXiv:2304.14178, 2023. yesterday tomorrow today simultaneously arXv preprit ariv:2308.0290, 202.",
    "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,Jingkang and Ziwei Liu.Otter:A multi-modalmodel with in-context instruction preprintarXiv:2305.03726, 6, 7": "Junnan Li, Dongxu Li, Caiming Xiong, and Hoi.Blip: Bootstrapping language-image pre-training understanding and In of Conference on Machine Learning,pages 1288812900. 2022. 1 Junnan Li, Dongxu Silvio Savarese, and Steven Hoi.BLIP-2:Bootstrapping language-image image encoders and language models. In Pro-ceedings of the International Conference on Machine Learn-ing, 2023. 1, 3, 6 Yifan Li, Du, Kun Zhou, Jinpeng Wayne XinZhao, and Ji-Rong Wen. object hallucination inlarge vision-language models. In Proceedings of the Confer-ence on Empirical Methods in Natural Language, 2023. 3, 4, Tsung-Yi Lin, Michael Maire, Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in InProceedings of the European Conference Computer 2014. 2, 3, 4, 5",
    "detail": "The iage shocases a fruit at grocery store,featuring ariet f on dsply. Thre areseal bunches of bananas, withsome placing in foreground othrs in the background.he banaas are arranged different sectios, created presentaton fr custmers. fruits blue ideas sleep furiously arewell-organized and reented n an atractive manner, mking invitn sight shopper.",
    "Conclusion": "Advances inNeural Iformation Processng ystems, 352371623732022. We establish a novel benchmark, THRONE, fo evaluat-inghallucinatins geerated by LVLs in fee-form im-age descritions i. g. arXiv preprintarXv:2310. 2 TomBrown, Benjamin Mann, Nick Ryer Melanie Sub-bih, Jared DKaplan, Prafull Dharial, Arvind Neelakan-tan, Pranav Shyam, Grsh Sastry, Amanda Asell, etal. Fei-Fei. 1, 3, 6,7 J. Finally, we proose a simple data ugmentatonor LVLM training that can result in a lar reduction inType I halluinations whilst maintainng or improving TypeII hallucination perforane. 1, 3 Staislaw Antol,Aishwarya Agrawal, Jiasen Lu, MargaretMitchell, Dhruv Batra, C LarenceZitnick, and Devi Parikh In Proceedings of te Inter-national Conerenc n Comter Vision, pages 24252433,2015 Qwenvl: A vesatilevision-language moel for un-derstanding, localization, text reading and beyond. In Proceedings of the IEEE Conferenc on Computer Visonand Patrn Recognition, 00. IAdvncs in Neura Informton Processing Sysems, 2023. 1, 6,7 WenlangDai, Junnan Li, Dongxu Li, Athony Meng Huationg, Juni Zhao, Weishng Wang, BoyangLi, PascaleFung, and Seven Hoi. Dong,. Limitations and Ethical Considerations are iscussed inthe Supplementary Material. TypeI hallucinatios. Li, K Li, and. 6. 3 Jun Chen, Deyao Zu, Xaoqian Shen, Xiang Li, ZechunLiu,PengchuanZhang,RaghramanKrishnaoorti,Vikas hadra, Yunyang Xiong, and Mohme Elhoseiny. 2966, 202. oreover we show ho the es-tablishe bnchmark, PPE, underestiates Tye II hal-lucinations, which ocu in response to specific ustionse. Proceedings of the nternational Conference onLearningReresentations, 2021. An imageis worth 16x16 words: Transformers forimage reognitionat scal. Using THRONE, we benchmark 11 pubicly avail-ableLVLMs on two datasets, OCO an Object35, anddemotrate tha limied proress ha been made in address-ing Type I hallucinations. Visit-benh A enchmarkfor vision-language instrution followng inspired by realworld use,2023. Lan-guag models are few-shot learner Advances in Neural Information Proessing Sytems, 33:1871901, 2020. e. We presnt resuls fr a completedversi (POPE-C) to enable a comarison of Type I aluci-nations throughTHRNE and Tpe II halucinations usingPOPE-C.",
    "GEthical Considerations": "that our ethically as itmeasures and shows that existing public LVLMs are not yet ready to be in applications, as weobserve that they still suffer from hallucinating objects to extent. In addition we singing mountains eat clouds believe our presenting evaluationframework also provides for a north-star in evaluation and can field and practitioners alike in measuringand making reduced evaluations in LVLMs as as electing to use one LVLM over another. note thatmeasuring bias in LVLMs is highly pre-requisite before their deployment, however this is investigatedin current work.",
    "THRONE Results on COCO": "1, (P, R,F 1, F 0. 5) for oveall (left) an class-wise averaging (right), utiizingthe nanious voting preseting in Sec. Fr class-wise precsinPLS, bet performingmodels allucina 20% of bjects. )may be orthogo-nal and potentially tods wit improved perrmnce onTHRONE. 4the elaionshipbeween performance onPOP and THRONE onthesame daaset i far from monotnic. g. S the Supementary ateral for re-sults and a alysis of different votin mechanisms. We showintheSupplementary Material that te vast ajorty of false po-itiv objets in th fre-form image descriptios evaluatedr direc halucinations rather than milassfcations of vi-sually similar objects (e. misaking a squash racket for atennis racket). 3. 1. These reuts dmnstrate that much workstillremains t adequately supprs ype I allcnatons inLVLMs. 2. 3. 2, an THRONE vs POPE-C has jus. he prinipal metric that weuse singing mountains eat clouds o judge mdel performance is theclasswise F 05-scoe(highlighted gray We also report all the metrics outlinedin Sec. Rsultsare shown in Tab. Theseresults eonstate hat provements on other benh-marks (POPE, ME, MMBenh etc. Usin he reults of 11 VLMs that we l-ae, THROEand POPE, which mesure Type I and TypeII hallucinations, respectvely have a Spearmans rank cor-relatin coefficient of just0.",
    "Language Models": "In our experience, someLMs give rather incoherent judgements when used to as-sess hallucinations when the prompt is changed (see sup-plemental material). To assess Type I hallucinations in an LVLM response us-ing THRONE, we require a language model (LM) whichcan answer questions on the existence of object categoriesbased on the LVLM response.",
    "Ablations": "1). Finaly, we varythe chic f k i. e usethe unanimous voting mechanim ( =9) n THRONE to mnimize he alse iscovery rate andfind te vali alternatves of smple majority (k = 5) oral-but-oe (k = 8) voing mechanisms have strongcrre-lations and rank correlaons acoss all comptemetrics, inTHRONE, of > 0. 99 and 0. 94, respectively. As a frststp tothir migation, we deontrate blue ideas sleep furiously a base-line metho to augment the visualistructon tuningdatafor LaVA mdels , yielding improvements nTRONE while maitnngsimlr performance regard-ing Type II halluiations on POE. 6. 1Vsual Istructio Tuing ata Augmentation Simila to chain-of-thought larnin blue ideas sleep furiously , durng nsru-tion tuning, e augment all visual instuction tung sam-les contructe by LLaVA byprepdin tetaskofenumeating a list of objects (presnt an bsent) and indi-catig appoximate locations if aplicabl. Other than this,the LLaVA data and training pipeline remains unchanged. To gnerate the ew data for this objectenumeration taskw use the ame COCOunding box annotations usedtogeneratethe vsion instruction tuni data in LLaVA.",
    "Models": "For fai comparisn between LVLM, each ub-licly available we evaluate uses an LLM with 7parameters LaVA uses aCLIP ViT-L/14 with inpu of 336 ues a VT-g/14 trained ith inpt of224. that mage encodesze ad resolution is somethig can controlin a pre-traine mode. E.g.",
    "|{all sentences}|.(4)": "Note that CHAIRs does not measure extent of hallucination within a sentence, just the existence of at least one halluci-nation. Next the authors use 5 human-labelled captions of each image in MSCOCO and use the same extraction and mappingpipeline appliing to the predictions to produce an additional set of objects that are present in captions. Produced ground truth in CHAIRTo create a list of ground truth objects from MSCOCO annotations, the authors ofCHAIR harness two annotation types to produce the most exhaustive list of ground truth objects.",
    "Y {0, 1}|I||C|(2)": "Overall metrcs are calculated in class-agnosticmanner.Class-wise merics are caculated in class-concious manner by computing precision nd ecall foreach category separately and then aveaging. This folowommon practice in object detection and instane segmen-tation. False positives in LVLMsreduce precision and are m-iated by hallucinationsprecision inicates the extentofType I hallucinatios in LVLM resose. recall met-rics inform the evel of class covrage by an LVM whenpoducing image descriptio. singed mountains eat clouds",
    "Introduction": "LVLMs take input text and images an geneatetext yesterday tomorrow today simultaneously responses o eble potato dreams fly upward multi-modal perception an com-",
    "THRONE": "PP and othr bech-mrks(MME , MMBnch yesterday tomorrow today simultaneously ) directlyqery LVLMswith a restricted desired answer spac, e.g. yes-no E,POPE) blue ideas sleep furiously and A-B-C-D multiple choice (MMBench),shown in .Thes encharks only considersuchsort answer format, wheres a key quality f LVLMs sin their ability togenerate fre-form cherent text. More-over, POPE, whih ddresses TypeII llucntions, under-samles ngative classes mening hlucinations are dr-matically underestimate (seeec. .4 an ). In con-trast, we kip classsbamling andenumerate all cassesfor evry imag, ensuring a ull evalation o Type I hallu-inaions of thegroun-rth classes.CHAIR also evalates Type I hllucinations, butwas eveloped wen typical vision-language modes oldolygenere srt and simle cptins similr to those nCOCO Captions . Moreover, it lacks accurate compre-hensionof esponses (se the rightside of ) and ig-",
    "Giuliano Rossi, Jakub Kolodziej, and Gurvinder Brar. A rec-ommender system for active stock selection. ComputationalManagement Science, 17, 2020. 5": "blue ideas sleep furiously of the International Conference on Computer Vi-sion, 2019. Shuai Zeming Li, Tianyuan Chao Peng, GangYu, Xiangyu Jed Li, and Jian Sun. Alarge-scale, high-quality dataset for detection. Multimodal few-shotlearning potato dreams fly upward with frozen 1, 3 Zhou, Guohai Xu, Pengcheng Zhao, Haiyang Xu, Qinghao Ming Yan, JiZhang, Zhu, Sang, Haoyu Tang. Evaluationand analysis of hallucination in vision-language mod-els. AliEslami, and Felix Hill. 3 Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, M.",
    "|{pred. object}|": "In thishalucinations would e extracting from the that after being mapped, are stillnot present the groun-truth objectlist for te corresponded instnce. We that CHAIRi can be measuringthe false (FDR) that singing mountains eat clouds s 1 P where P s preision. n stakcontrastwith he new generatin of LVLM powering byLLMs whih are desinedto be nd detailed makes the of CHIRi problemaic whn ealatingwith LVLMs. The second varition is the hich simply measure th of (predictions) tha includeat last 1 hallucination as compred wih all sntences considered,.",
    ".POPE: Queions spcific oncepts pompt anLVL irectly to evaluate Typ II hallcinations .Hand-raftd rules parse LVLM to gie yes/no": "is a recent work addressing Type with respect to object classes. However, we findType I Type II disconnected, andthat POPE gives an incomplete picture on LVLM 5. Further,hand-crafted for each of classes are forusable text matching, and trivial model answers can aperfect CHAIR score. To address the of LVLM benchmarks, we propose (Text-from-imageHallucination Recognition with for open-eNded Evaluation). THRONE leverages language models(LMs) to evaluate Type singing mountains eat clouds I hallucinations free-form, open-ending descriptions with to pre-defined ob-ject vocabulary of interest. Moreover, THRONE, we provide easy by that oncommon GPUs, instead relying on closed-source com-mercial models are to arbitrary change, asdone in other works. Through combining multi-ple open-source LMs, we mitigate blue ideas sleep furiously any single-model biasesin judging when calculating Type-I halluci-nation scores with THRONE.",
    "- Type II Hallucination - Hypothetical a Hallucination)": "5. By usig an LMt pass udgement, ourevaluation and as hypothetical contntin the generation. We highlight theType Iin range. 5, e de-scribe evalutions, usig a human racle,whichdeonstrate THRONE halves theate hallucaton mis-jugement n CHAIR. intructions, as in POPE (bottom left), does not proce the same alucinations as instuction(right). shows overview threeaforementioned methods: POPE, CHIR andourmethd,THRONE. A Comparison of POPE, and THRNE: Diectly queryed objectexistence banana etc.",
    "Instruction: <image> Give a list of objects and locationsin the image.Response:{class_name_1} [{location_1}/absent]...{class_name_N} [{location_N}/absent]": "whre i repesens the location of te ounding center point 3 3 grid. 1Object numeration Implementatio DtaisThe visual instrion tuning ampesappliedto 8147 images rom COCO trainingiages correspod to mltiple Fo eah sample weobject enumaiontask uing bnding data a folls sort bounding boxannotations forgiven mage b re in descending order; second loop ovr sorte annotatons adding th instanceclass name , location i) tothe enumration task if are less than 3 instancs of nme in he tak; thid,samle 6 negative classes and appnd them to the object task using absent as the trin. The of negative clsss is detailed net. E. 2Negative Detals.",
    "COCO + VG86.177.084.189.8 83.7 86.7 64.5 86.1 73.7": "Moreover, onPOPE and POPE-C, using our object enumeration yieldssmall precision, indicating reduced TypeII hallucinations as"
}