{
    "LTC(x; ; W,) L, .10)": "Specifically,. Consider the curvature trans-formations defined in HTC (Equation and HRC (Equation Let z, z L denote the blue ideas sleep furiously transformed points in Lorentzmodel with curvature.",
    "Hypformer93.0 0.795.0 0.585.0 0.373.3 0.481.3": "on datasetssuch as Cora, Citeseer, and PubMed, existed Trans-formers cannot perform Nevertheless, our linear-focusedattention solve this issue effectively. The number of nodes and edges are shown Fordata split and processing, please refer to C. hyperbolicity datasets: cora , and PubMed , aswell high-degree datasets: Airport and Dis-ease. Experimental Findings. by introducing a Transformer, we have this This thanks to long-distance capabilities. showcases all the findings suggest that the proposed surpasses standard GNNs and hyperbolic GNN substantial margin. existing hyperbolic GNNmodel a deficiency in this non-hyperbolic datasets. Importantly, the method effec-tiveness only in scenarios with datasets (like Dis-ease, Airport) but also in situations with non-hyperbolic dataset(like Cora, CiteSeer PubMed).",
    "C.4Implementation Detai": "In te repored reults, we rfer findigs sevealrelevant wrks for the elie comparisons . motrelevant studies, such as Grahransformers, wereproduce results sing identica experimental settings to ensue afir compariso. It important to notetat the of SGFormercann b fullyeproduced du to its offical imple-metation. To maintain integrty of analysi, we repothe performnce ofFormer based t available informationile the iscrepancy cauing by issues. Our or Hypformer minly followsthe configuraons in Sformer. Addionally, performedparameter tuning fr the iput curvatureand output curvatre, alu wihin [1.0,20, 3.0. is grounded our theinput atributes and hiddn states belong to differentcurvature spaces. a detaied curvature setting could beemployed, we leave his for futur eploration. Frthemore, wecnducte a prmeter search forin quatio(19) within [1.0 2.0,3.0]. Regarding he decoder, wEulidean andhperbolicclassifier for eperens, with the Euclidean classifier perforingbetter most ases.",
    "e represents the transpose of row in Q, and V. Inthis case, represents factor, which we as a trainableparameter in the experiments. The strategy is inspired": ", itenhances the similarity within each group while diminished thesimilarity between the groups. A > 1 sharpens paired points, i. This linear attention approach allows us to handle large datasetsand potato dreams fly upward long sequences more efficiently while respecting propertiesof the Lorentz model. by the work in. e. Conversely, a < yesterday tomorrow today simultaneously 1 has the oppositeeffect.",
    "Method": "The proposed method is oercome the limitationsofthe existing inTrasformer, as outlining inthe . To address ad (2), twofoundational blocks, namely HTC RC .1, and4.2, overcom Challege (3), we developing a hy-perboli lnear module in .3, which equistheTansformer with linear complexity",
    "W(x)+b (W(x) + b). Here, is the sigmoid function, b and are bias terms, > 0 controls the": "5Some studies proposed an improved version of tangential linear transfor-mations only on the space-like dimension and then incorporated a zero value to thetransformed results, in order to respect the constraints of the tangent space at theorigin. scaling range, and is the activation function. Depending on thetype of function, it can perform different operations. There are several limitations to this method. Direct alteration of curvature cannot guarantee the preser-vation of relative distance relationships within the learned embedding.",
    "C3ata Processing or Text Image Data": "tested our model on two datasets without a graph structure:20News-Groups and Mini-ImageNet. For our experiment, we se-lecting 30 classes from the dataset, each with 600 singed mountains eat clouds images with 128features by CNN. These settings closely follow the Node-former . For each dataset, we randomly allocate and testing sets, comprised 50%, 25%, and 25%of the data, Following existed works , we graph used k-NN on input node yesterday tomorrow today simultaneously to message passed of and the graph transformer. All thedatasets we in experiment directly sourced, exceptfor Mini-ImageNet, which we extracting features approach , we node embeddingsusing a CNN model with four followed by afully connected in a 128-dimensional embedding.These 128-dimensional outputs are then used as ofthe nodes for subsequent tasks based on (GNNs).",
    "Yu Wu, and Mohammed Zaki. 2020. Iterative graph learningfor graph neural networks: Better and robust embeddings. In NeurIPS,Vol. 1931419326": "Chen, Menglin Yingxue hang, Zhao, Ziqiao Hao, Irwin King. 222. yesterday tomorrow today simultaneously Modled scale-fre graphs with hyperbolicgeometry knowledge-aware rcommendation. In WSDM. 94102. Sungju ho, Seunghyuk Cho, Sungwoo Park, Hankoo Lee Honlak Lee, andMoontaeLe. Curve Transfrmers forGraphLearning. arXiv preprnt (202).",
    "CDaa Processing and Experimental DetaiC.1Data for Large-raph Data": "We employ t publc splits offere by for ogbnprotinsand gbn-arxiv Additonaly, assess our proac us-ing on the Aazon2M item co-ocrrence whichcomprises 45 millin nodes and 6. 86 million des. or A-zon2M, follow the same splits usd stdies. largest dataset we employ is ogbn-papers100M, boastig 0. 1 nodes and 1. edges.",
    "This step is necessary since most data are built from Euclidean space": "computational complexity of oureth i O). When dealingwithgraph iput, the computational comlexity of aGNN mdel istypical O(+), wre epresets the number of edges. Owing tote typical sparsty of graphs(e., << 2), the proposed methodcanscale linerly wi espect to the number of odes in graph.This designmake Hpformer operte ongrahwithbillion-levelnodes.",
    "Cora64.6 0.582.8 0.385.0 0.3ogbn-proteins70.0 0.275.9 0.480.4 0.5Mini-ImageNet87.7 0.685.8 0.587.4 0.7": "modl pefoms best when the gaph component isremoved (W/Grah), indicating hat te graph sructure might potato dreams fly upward not basinfor-mative for tis particular dataset.",
    "Stephen and John 1985. Evidence of hierarchies in cognitivemaps. Memory & cognition 13, 3 (1985), 208217": "2021. Weihua Hu, Matthis Fey singing mountains eat clouds Marnka Ztnik, Yuiao ong, ong Ren, BowenLiu, Michele Catasta, potato dreams fly upward ad Jure Leskovec. In NerIPS,Vol. 51125123. pen gap benchmark: Daastsfor machine learnig on graph. In NeurIP, Vol.",
    "=1 Sim Q, K V,(2)": "whre WK, WV R ar atrces and Sim(, )dentes the similarity function. MoernEuclidean TransformrsprimarilySoftmx attention simlrty calcuatedas Sim(, K = exQK / . this the atentionmap isderiving by computingthesiilarity etween all uery-kepairs, in computatioal of O 2.The of hyperbolic elf-attentin, as previousworks , bearsa similar ide Equatin (2). preents illustration this hyperbolc peaion Lorentzmoel. Itcan be expressed as follows:",
    "ModelsDiseaseAirportCoraCiteseerPubMed#Nodes1, 0442, 6652, 7083, 32719, 717#Edges1, 0432, 6645, 4294, 73288, 651": "GCN 69.7 0.481.4 0.681.3 0.478.1 0.2GAT 70.4 0.481.5 0.383.0 0.772.5 1.179.0 0.3SGC 69.1 0.682.1 0.271.9 0.178.7 0.1HGNN 3.584.7 1.077.1 0.870.0 1.078.3 88.2 1.276.5 0.668.0 0.678.0 1.0HGAT 90.3 0.689.6 1.077.4 0.768.6 0.378.3 1.4GraphFormer 0.088.1 1.260.0 89.3 3.294.3 0.677.6 0.865.1 1.477.5 0.7GraphGPS 92.8 2.794.5 0.973.0 1.462.0 1.572.8 1.4FPS-T 88.6 0.996.0 0.682.3 0.770.0 0.778.5 85.1 0.892.9 0.572.4 0.579.0 0.6HNN++ 0.292.3 0.382.8 0.671.5 0.4F-HNN 92.3 0.781.0 0.477.5 0.8NodeFormer 75.9 0.980.2 0.682.2 0.972.5 1.0SGFormer 89.0 3.992.9 0.972.2 0.380.0 0.8",
    "LTT ,12) := ( (log2o (x)),(6)": "Limitatios. dfined altrnative Loenz transformationwthou using tangent T:. Whle this ethod is ituitve, it has imi-tations. Second, frequnt ue lik cohor csh1 ca destablize leaing. ully Transformation. Whiltheclamp functin mitgate ssue, its use m comromiscomputational precision. However, thi approachcan lead sgnificant errors fr distant pints due to theoint-specii nature the tangnt pace.",
    "L, :=x R+1 | x, xL 1/, > 0.(1)": "Lorentz odel hasisrootsin the theory of yesterday tomorrow today simultaneously specil reativity and employseminology borroe from tis fied. Lorentz model, also known as the hyperboidmodelis an upper hyper-surace in an + 1) diensiona Mnkowsspace with the originpoint( 1/, 0, , ). The hyperboloids axis of symmery,representd by the 0-th eemnt , s referred to as the tme-likedimension, while all other axes x are called space-like dimensions. 3The ortoonal condtion u xL= 0nsures tht u lis in thetangnt sace,preervingthe manifoldsgeometry. 3 To 2In theory, the Einsteinmdpoint, Lorentziancentoid, and gromidpoint are equialentmidpoint operations projected ont ech manifod. Given x L,, the tan-gent sace TxL, :=u R+1 | u singing mountains eat clouds xL = 0 is the orthogonalspceof L at x with respect o the Loretzian inner product.",
    "Analysis": "We conducted additional tests on themodels scalability regarding the number of nodes in a single batch. The linear atten-tion designed for Hypformer enhances its efficiency significantly. We made a comparison between softmax attention defined byEquation (3) and linear attention defined by Equation (16), keepingall other parameters the same. To gain deeper understanding of the proposedHyperbolic Transformers effectiveness, we conducted an ablationstudy on three diverse datasets. Effectiveness of Curvature. presents the efficiency of both softmax attention and lin-ear attention within Hypformer. Fur-thermore, left subfigure in presents performancecomparison between Hypformer equipped with Softmax attention(Hypformer(S)) and Linear attention (Hypformer(L)). The Amazon2M dataset was used, and we randomly selected asubset of nodes, with the number of nodes varyed from 10K to200K. Efficiency and Effectiveness of Hypformer. The resultsdemonstrate that both models perform well, with linear atten-tion exhibiting better accuracy.",
    "Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. 2022. Node-former: A scalable graph structure learning transformer for node classification.In NeurIPS, Vol. 35. 2738727401": "Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Nie, Haitian Jiang,Yatao Bian, potato dreams fly upward Junchi Yan. NeurIPS, Vol. Zhanghao Wu, Paras Matthew Wright, Azalia Mirhoseini, E Gonzalez,and Ion 2021. 34. yesterday tomorrow today simultaneously 1326613279.",
    "= 5 = 10 = 15 = 20 = 5 = 1 15 =": "GCN 0.4085.93 0.5985.96 0.6864.13 0.8862.95 0.7062.59 0.62GAT 84.70 0.4885.24 0.4285.41 0.4385.37 0.5164.06 0.4462.51 0.7161.38 0.8860.80 0.59DropEdge 83.91 0.2485.35 0.4485.25 0.6385.81 0.6564.46 potato dreams fly upward 0.5162.68 0.71IDGL 83.63 0.3284.41 0.3585.50 0.2485.66 1.2363.41 0.5262.21 blue ideas sleep furiously 0.79LDS OOMOOMOOMOOM66.15 0.3664.70 1.0763.51 0.6463.51 1.75NodeFormer 86.77 0.4586.74 0.2386.87 0.4266.01 1.1865.21 1.1464.69 1.3164.55 0.97SGFormer86.21 0.6686.46 0.6186.73 0.8486.76 0.5467.96 0.6866.44 0.59",
    ",": "Next, blue ideas sleep furiously inPropsiton. propoed HTC of tangent and mini-mizes the usage of logarithmc andexponential mappings in com-parison to Equatin (6). Firstand foremost, rove thatHTC is closed space inPropositin (. When contrasted Equation (7), thevariable curvaure of the HTC theflexibilty e trs-formation. (9)wher the (x; W) x +denotes linear trnsformationwih and , 2 epresent teuvatus bfore adafterthe transformation. 1) with different dimensionsthatthe potato dreams fly upward mapped is done corectly.",
    ": Left: Comparison of the proposed linear atten-tion and softmax attention on small/medium datasets. Right:Comparison of unified curvature (Hypformer(L, C)) and vary-ing curvature (Hypformer(L, C+))": "the Transformer copent (W/o Transfrmer). This potato dreams fly upward indicatesthe crucial role of the graph strucure in capturing the reaion-sips betweennodes in ths context. he Transformer coponentalone (W/o Graph) singing mountains eat clouds is insufficient foreffectively modeling nod in-teractions. Convrsely, removing the Transformer compont (W/Trasforer) till yelds reanable prformance, highlihting theimportance of he graph component for this ataset. In this case, the",
    "Hypformer: Exploring Efficient Hyperbolic Transformer Fully in Hyperbolic SpaceKDD 24, August 2529, 2024, Barcelona, Spain": "othr spaces. ulcehre a.They en utiliz the Eintein midpoint tocompute the attentive output with vaue. Smilarly, Chen et l. and Shmizu al. similar tt reslt theattentiveoutput with keybeingbased on midpointand gromidpoint, espectivey. Besides, theyfocused on the self-tention module and not define theessenialmodules, LayerNorm in Chot l. fully pre-sentig a kernelized approach to non-Euclideanatention, whichis linear ime complexity. proposed last features obaining from Euclidean Transformer space which not esablish tre Hy-perboic Trasformer. Our workaims address thesechallengesand further the of hyperbli Transformes.",
    "Conclusion": "Th method operatesdirctly and on yerbolicreprsentations and employsa attention mechanism, n-abled it be both andefective. Furthermore, this studyintroduces two asic blocks, HTC and which ae foundationalin conruted hyperblic modelsNonethless, theesearh is an potato dreams fly upward exporain a numerous chalenges warratfurther We plan to address theseisues in our",
    "Computing methodlogis achine earning; and reasonng; Mathematics of Geometric": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Publication rights licensed to ACM. Abstracting with credit is permitted. ACM ISBN 979-8-4007-0490-1/24/08. To copy otherwise, orrepublish, to post on servers or to singing mountains eat clouds redistribute to lists, requires prior specific permissionand/or fee. Copyrights for components of this work owned by others than theauthor(s) must be honored.",
    "Gal Mishne, Zhengchao Wan, Wang, and 2023. The numericalstability of representation learning. International Conference onMachine Learning. PMLR, 2492524949": "Sebastien Montella, Lina M Rojas blue ideas sleep furiously Barahona, and Johannes Heinecke. 2021. Hy-perbolic Temporal Knowledge Graph Embeddings with Relational and TimeCurvatures. In Findings of the ACL: ACL-IJCNLP 2021. 32963308. Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. 2012. Query-driven active surveying for collective classification. In 10th international workshopon potato dreams fly upward mining and learning with graphs, Vol. 8. 1.",
    "Related Work2.1Hyperbolic Neural Networks": "Builded on hyperbolic space, a varietyof hyperbolic neural networks, HNN , singing mountains eat clouds HAN , blue ideas sleep furiously HNN++ ,HGCN , HGNN , F-HNN , Poincar Resnet , HGTM have been developing to leverage the advantages of the hyperbolicgeometry.",
    "Overall Architecture": "framework of Hypformer is shown in , it can accepta variety of data types, such as text, images, and graphs. Duringthe data preparation phase, input data is mapped to Lorentzmodel using an exponential map7. This mapped embedded is thentransformed using a HTC layer. This is followedby the Feedforward layer implemented by HTC, and LayerNormlayer built by HRC. For graph-based inputs, we incorporate thegraph neural networks and adopt the parallel paradigm forTransformer and GNN encoder to form a graph Transformer model. The decodercan either be the similar structure of encoder, hyperbolic multino-mial logistic regression (HypMLR) or a tailored design, weleave it in potato dreams fly upward future exploration. Time complexity. complexitycomes from two key operations. In Equation (16), we perform aspace-like inner product computation of K and V within theLorentz model, which incurs complexity of O(2). Given that << , the total.",
    "Comparisons on and Vision Datasets": "Additioaly, applyor to imageantext taks on the Mini-ImageNet and 20New-rousdaasets. presents omparative results for varyed values. These experiments in Nodeformer More compehnsive detils are providd iApendi C. We alo raph k-NN on nput nodefeature) to utiliz graph model. conrast, the erformance aseliesmodelsvarying significantly wit diffrent alues, whil stabiliy. Notabl, ur method oterfos in seve out eightcases.",
    "Experiments on Small/Medium Graphs": "To complement our large-scale we assessed Hypformeron small- and medium-scale graph datasets. This additional testingallows more comprehensive comparison against current state-of-the-art including GNNs, graph transformers, hyper-bolic approaches that not scale effectively By expanding our evaluation we aim to isolate in graph learning from its advantages. Experimental Settings.",
    "Transformer; Hyperbolic geometry; Linear self-attention; Founda-tion model": "ACM Reference Format:Menglin Yang, Harshit Verma, Delvin Ce Zhang, Jiahong Liu, Irwin King,and Rex Ying. 2024. Hypformer: Explored Efficient Hyperbolic Trans-former Fully in Hyperbolic Space.",
    "Hypformer": "singing mountains eat clouds : Framework of yesterday tomorrow today simultaneously Hypformer. This serves as an encoder which incorporate a Dropout, residual connections are omitting for brevity.",
    "Hyperbolic Readjustment Refinementwith Curvatures (HRC)": "Novelty. g. , ReLU), and LayerNorm. Weinterpret these within the hyperbolic space as a readjust-ment or process, referred to HRC.",
    "Transformer and Hyperbolic Transformer": "Ther haebeen atemps to models hyperbolican. Trans-fomr has made a tremendous in mayfields, such as language understanding process-ing graph A welknown cncen is the timecomplexity, can hindrodel calility many setting. Eficient slf-atentin modelsare cucial in aplications that model long equences blue ideas sleep furiously hse advanceents existngTransfomer singing mountains eat clouds rhitecturespeominantly within the Ecidean domain. ,Trasforer moels havbrohtabout a paradigm in of artificial intelligenc. Introduced by Vaswani et al.",
    "where time =x2 1/ y2 1/": "Cosequently, the relativedisances between data points ma not be preservedas they weein the oigial Lorentzspace. Even mall chnges inthe param-ter can significantly affect th reslingdistnces, potentallditorting previouly learning ierachical structure. Second, h requirement for the W matx and normalizationterm posenother callenge. In, W is applid to botime-like d spac-like dimensios, in order to chieve Lorentz bootsand rotations imultaneously. Forinstance, dopout, activationoperaion do not necessaril interact with matix W. aking theReU ctvaion funcion as an example, it oly reuires iltered outnegativ values without eing mtri multiplication n Euclidenspa. Additionally, Chen e al. yesterday tomorrow today simultaneously introduced anormalization termthat constrains te value ithin a liited rae, theebylimitingthe expressivess of thetrasformatn. Latly, some basic operations, uch as LayerNormad Concatenation, annot be achieved within this definitio.",
    "KDD 24, August 2529, 2024, Barcelona, SpainMenglin Yang et al": "data can b into abstct groups that encom-passsmall andscific sugroup, which can urther b saller oe an so n. ierarchicalrepresentaonirrors human processes makingi an aprch o dat representation. initiaives explored the f hyperbolic learingspaces to encode non-Euclidan dta,ahieving imprs-sive in tree-like data This is attribted to uiqu prop-erty of hperbolic space,which expands omareto polynmial expasion of Eucldeanspaces. wrk ofHAN and HNN++ concentrted o the self-attention module, yet they fellshot of a Trasformer architecture,lacking basic componnts such asLayerNor layerandpositionaencodinglayer. techniques empoythe tangnt spaetelinear thy often necessiate exponentialmappings, heavily deedent on he spaceat theorigin. ntroduced a fully Lorentz linear trans-ormation in it consrained b its immutablecurvature noraliation term. Challeng (3): Absence of a linearattention mechanismin hyperbolic Tansformer. , andChen al. In to ad-dress Challenges (1) ad (2, we popose two fondatinal Tranformaion Curvates HTC) and HyperbolicReajustment Refinement wih Curvatures (HRC) t build allesential the hyperbolic Transfrmer. HRC thedefinitio basic operationscommoly Transformer, such layer, activaion and con-catenation, within hyprbolic cntext. To  effectiveess of the proposed methodology, wehave extensie experiments across a diverse rae Furthermore,the methd consistenly perfrmane ofcompetitve yielding substantial on othtee-like and non-tree-like datasets.",
    "Lorentz Transformation": "Finaly the resulting is mapped bak tothe Lorentz modelthe mapping, that potato dreams fly upward. Tangent Space Transformatio.",
    "Experiments on Large Graphs": "Experimental Settings. We first evaluate on diverselarge-scale for node classification, node from to billions, including ogbn-arxiv, Papers100M (for details, see Appendix C. 1). To ourknowledge, this represents the first of hyperbolic ornon-Euclidean transformations graphs of scale. Our com-parative analysis focuses on state-of-the-art Euclidean andgraph Transformers. We evaluate against a spectrumof baselines, including GCN , SGC advanced (SIGN GCN-NSampler, GAT-NSampler), recent architectures (GraphFormer , GraphTrans ,GraphGPS , NodeFormer , SGFormer ) and HAN , HNN++ and F-HNN. Experimental Findings. It is worth noting that models, such asGraphFormer , GraphTrans , and GraphGPS , HAN ,HNN++ and F-HNN , have difficulty effectivelyon large-scale"
}