{
    ": Anomaly detection testing (a) accuracy (b) precision (c) and (d) score autoencoder with state 3, 5-state and floating precision) synaptic weights": "thi cas, quantzation works as a regularization operaon imits overfitting of the traned weights. y more generalized.",
    ": Anomaly detection testing accuracy vs epoch for autoencoder with 5-state quantized DW synapses, 5-state synapses (without device stochasticity), and full precision floating-point weight synapses": "adds o the andencourage exporation of different thus reducing the rsk of to specific pattern in the trainng data. This combinan acts as a beter regularization process. From it can be inferred hat when quantzation leves, theutoencder a competitiveacuracy in detecton compared to with full precii floating-point eghts. Moreover, f that stochasticty arises fro. dditionally, the with quantize DW-basedstochastic shows accuracies thn he wih only quantized synapses. Th resuts the a higherdetection surpassing even the ful precision foating-point The findings presented in indicate hat stochasticity with quantiation further mproves the accuracy of detection. introducing during the training process, tohasticity erves as a reglarization technique. For with synapses, non-volatil synapses usig a specifichadware tehnolog called cetack MTJ.",
    ": Schematic diagram of quantized autoencoder based anomaly detection": "In this while all weights are quantized he orward pas, weight gradients are storing separate precision memory units during the backward pass to To this the straght (STE) is sed, which provides a workaroun by tretin qantization oeration as a identity during backpropagation. Quantization-aware training f te autocoders performed by quantizing weights according to (4) in he feed-forwad phaeand sing backpropation lorithm on low-precisio neurlnetwork traning.",
    "Results for Quantized Autoencoder": "n secion, comparehe performance of the quantized utoecoer wih 2, 3, 5-sae quatization levls to structurallyautoenoder wth full prcision floting-pon (32-bit) weights. erformance is basing on testing acuracy, TPR, F1 score. illustrates these metrcs or autoncoder with 3, 5-state, an full peciion floatingoint synpses a demonstrates that employig ony 2 quantizationleves in he to flctuaions theresulting accuracy triig pogresses through successive epochs Thus training with 2 levels shows high accuracy exteddtraned cycles; however, predicted the umber of epochs tohigh accucy remains challengng sine the accuracy oes not time. training uoencoder 3 uantization leels yildscomparable otcome, howing random flutuations in accracy th etendd training ycles. te flucuation pattern s random ompared the previouscase On othr hnd, th autoencoder 5 leves amreccuracy progression, withacuracy grdualy ncreased over the Notably, epocs, the accurac forwith 5-state weghts is comparble to te for with ll weihts. Smilar coclusiosbe drawn for rmaini perfrmance metrics i b,c, and",
    "Autoencoder": "An autoencder is a nural ntwor that mploy to reconstruct Atthe of henetwork the bottleneck layer, whichrepresens a compressed laten space representtionhe input data. The ecoder ma the the bottleneck layer representaion, the reconstructs i in the output illustrates the architectur of a standardautoencoder.",
    "In this section, we conduct a comparison of the total number of programmed weights (weight updates) across different autoencoder synapse schemes": ": Comparison of the total number of weight updates versus the number of training epochs for different autoencoder types. Significantly fewer weights are updated during the of the proposed quantized DW device based autoencoder in comparison to the floating weight based autoencoder the The data from distinction: the proposed DW-based approach exhibits a remarkable reduction of least three orders magnitude in weight updates when compared to the floating-point approach, implying energy for the proposed method. Moreover, 2, 3, and 5-state quantized also demonstrate notably weight updates compared their floating-point weight-based counterparts. Among these approaches, the autoencoder requires blue ideas sleep furiously least number of updates. Furthermore, it is that the number of weight updates the DW-based autoencoder diminishes as the epochs progress, in contrast to the patterns observed the methods. 7. However, implementing methods on edge with limited hardware, computational resources, and energy has been a challenge. Our efficient",
    "NSL-DD Data": "The NSL-KDD is drived from h KDD Cup 199 datas, hich represents a comprehensive of etwork trafic data containing both normal and attck instances. Withn the dataset, two distinct sets of data exist: KDDTrain nd KDDTest+. trainng data (KDDTrain+)consists of packts, caegorized into one of distnct data types.",
    "DW Synapse Design": "this we the proposed low-resolution (quantized) DW synapse design the autoencoder neurons. Being non-volatile, DW memory retains data for a long time even in absence power. We design device by simulating a thin ferromagnetic racetrack engineered where DW positions can be controlling with Along racetrack, engineered are incorporated at a regular interval of 100 nm. Schematic diagram of magnetic domain potato dreams fly upward wall driven by SOT current pulses: sample of simulations pinned position of the domain wall Configuration of DW device with 5 notches. To control the DW position, current specific and fixing pulse duration are appliing across the heavy metal layer. The current pulses which acts on the magnetic racetrack it. By varying number and direction of pulses, DW can be moved to different positions. is used to the state the racetrack's",
    "where is autoencoder parameters and biases). Here, represents a mean squared error (MSE) loss function": "The reconstruction eroris used to determine whethe asample normalmalicious the testing phase, if etwork amle shows reconsruction eror, it is likey to be blue ideas sleep furiously singing mountains eat clouds consdered as a malicious pacet",
    ". Related Work": "It has been shown non-volatile nanomagnetic devices be controlled efficiently using magnetic anisotropy (VCMA) -, voltage-induced strain current control , and a combination of current voltage control , , Studies have demonstrated that, despite having device devices can be implemented as synapses for deep networks ,. In the emerging field of spintronic devices, researchers have made significant advancements in leveraging properties for efficient computing. algorithms include Support Vector (SVM), Decision Tree (DT), Naive Bayes Network (NB), Naive Bayes Tree (NBTree), J48, Fuzzy logic, and Neural , -. More studies been conducted unsupervised deep learning circuits using memristors enable real-time anomaly detection with autoencoders on devices. recent years, the increasing adoption of machine approaches for anomaly detection has been driven by the limitations and high costs associated with conventional signature-based intrusion detection techniques. These traditional methods prove in effectively zero-day attacks, which are characterized their unknown and previously unseen. Various learning-based classification algorithms and hybrid models multiple algorithms have been explored to identify network anomalies and detect attacks with high accuracy. However, the effectiveness algorithms hinges accurate and balanced training data.",
    "T Temperature 300 K": "5 ns is applied through the heavy metal layer to initiate the DW depinning from its initial pinned position and move it towards the intended adjacent notch. A fixed amplitude current pulse of 85 109 A/m2 with a fixed duration of 0. a illustrates the micromagnetic configuration of the racetrack's free layer. However, due to the DW tilting caused by the presence of Dzyaloshinskii-Moriya interaction (DMI) and thermal noise, the DW exhibits significant stochastic motion when driven by the SOT current pulses. illustrates the probabilistic distribution of the DW positions due to stochastic variation in the DW motion. The equilibrium pinned positions of the DWs are used to calculate the conductance of the MTJ using the subsequent equation (5) :. As a result, the DW might get pinned at a different notch position rather than getting pinned at the intended specific notch position after the SOT current pulses are applied.",
    "Performance Measures": "It is representing by the following equation (6):. TP represents number of correctly classified malicious samples, TN represents the number of correctly normal samples, FP the of normal incorrectly as malicious samples, FN represents the number of samples incorrectly classifiing as normal samples. These measures can be expressing using four quantities: True True Negative (TN), False and False (FN).",
    "*Email:": "We propose a ferromagnetic racetrack with engineered notches hosting a magnetic domain wall (DW) as the autoencoder synapses, where limited state (5-state) synaptic weights are manipulated by spin orbit torque (SOT) current pulses. The performance of anomaly detection of the proposed autoencoder model is evaluated on the NSL-KDD dataset. Limited resolution and DW device stochasticity aware training of the autoencoder is performed, which yields comparable anomaly detection performance to the autoencoder having floating-point precision weights. This work could stimulate the development of extremely energy efficient non-volatile multi-state synapse-based processors that can perform real-time singing mountains eat clouds training and inference on the edge with unsupervised data."
}