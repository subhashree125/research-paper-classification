{
    "Temporal Evolution": "Future facts are usually closely to recentfacts, and our temporal evolution aims tomodel recent facts. naturally graph struc-ture while TKGs have the time compared to KGs. Therefore,we aggregate transfer most recent times-tamps the timestamp t to be in both spatial and temporal views. To structural between we alsoutilize the heterogeneous graph convolutional net-",
    "Setup": "3Training Settings and EvaluationMetricsWe report a widely used time-aware filtered ver-sion (Sun et al. , 2021b,2022a; Xu et al. 5. , 2023). For the detailsof baselines under both candidate entity known andunknown settings, please refer to Appendix B. 2BaselinesUnder the candidate entity unknown setting, wecompare our proposed MGESL model with threekinds of baselines: (1) Static KG reasoning mod-els, (2) Interpolated blue ideas sleep furiously TKG reasoning models, and(3) Current state-of-the-art extrapolated TKG rea-soning model. , 2022a,b) of MeanReciprocal Ranks (MRR) and Hits@1/3/10. Forimplementation details and parameter sensitivityanalysis experiments of MGESL, please refer toAppendix C and D, respectively. 1. We divide them into training, validation,and test sets with a proportion of 80%, 10%, and10% by timestamps following (Li et al. , 2021; Li et al. 5. 1. The details of datasetsstatistics are shown in Appendix A.",
    "Fine-grained History": "Therefore,e extract kinds f fine-graine historis, i..,one-hop neighbours yesterday tomorrow today simultaneously and historyanswers (Li et al., ecifically, fo a query(s, r, t) the indicato vector ofone-ho",
    ": Illustration of the two problems of TKG rea-soning task": "t (t0 t tn), while extrapolationatempts toredict facts thatccur at ime t (t  tn). In is a-per, e mainly fcus n TKG extrapolatio Moso exising extrapolatin models (Jin et., 220;Li t al,2021,2022b Liu et al.,202a) assumethe cndidate entities are unnownduin the rea-soing. However, tere are case that we alreadyknow te cnddate entitis, e.g., suspects are ofeientifiedbeforehand in criminal investigations andandidates ae sully aleady determined beforete presdentil election. I these cses, existig extrapolation models (Jin e al., 200; L et al., 221b,2022b; Liu et al., 2023a) cannot efftivey utilizethe information of those canddae etites becausethey treat all ntities equally durng the resoning.Therefore, we intoduce a ew setting called thecandidate entity known settig,were all the enti-ies at t are nownin adance. In contrst, if the adidate entities t t are uknown during the rea-sonng, we call this the cadidate entiy unknownsetting. In his paper, bothcandidate entity nownand unknown settingswill be discussed.To predict hat wil happen i the futre, wefond that (1) seaching for similar ntities,ob-serving and undestandingthe evolutionary pat-tern o th actions of simila entities ad (2) del-ng into he entityhistorical context fro multi-granularity are crucial.(a) shows a ex-ample of TKG similart learning roblem, whreObama and Bden both anction Russia However,since baa andBden are not conected directlyin tis example,vanila raph conoltn is un-able to cpture te interactionbeteen them. Toaddress this isse, we ralize thatboth bm andBiden share the same relatin of sanction. incehypergraph onvolion can enable informtion in-traction ang entities under th same elatin,we therefore dsign a hypergrph convolutioal ag-grgatr to cature similaity information betweenthm. Addiionally, existngodls (Jin et al.,2020; L et al.,201b) mainly fos on utilisngthe available temporal and structuralinformationnthe TKGfor nference, ignringthe history in-formtion. Even though some recent sudes Zhuet al., 201;i et al., 202a;u t al., 2023 trieto find the correct answer from longter gloalrepeatedistory (i.e., fine-grained histry), but theyignre te more generalisd history. For istance,(b) illustratesa temporal kowledgegraphwith several timesamps, where the task is to pre-dict the answer to the query (USA, saction, ? t).ost models (Zhu et al., 2021; u et al., 202)pioritize repated history, and return China as theanswer. Hoever the correc answer to te qustionis Russia whh is a ulti-op eighbour of USATo overcomethis limitions, w rter considermulti-hop neighbour entities (i.e., coase-granedhistry) n TKG reasoning.To this end, we consider history at two evelsof ganularity (i.e.,fine a oarse-grained his-tory) and entity similarity learning simultaneously,and propose he Multi-Granularity Hstory andEntity Simlarity Larning (MGES) model forTemporal Kowlde Grah Reasoning. Specifi-cally, MEL consists of thre modules, i.e., (1)Entity Similarit Leaning Module, which is usdto capture the similaity between entities thatsharethe ame rlation 2) TemporalEolution Mod-ule,hc is used to aggegate and trasfer the Ginformatin from spatial nd temporalview re",
    ": The statistics of the datasets. Granularity rep-resents time granularity between temporally adjacentfacts": "models, i. e , DistMult (Yang et al , 2015, onv(Dettmers et al. , 2018), ComplEx (Trouillon et al. , 219) and Ro-tatE (Sunet al , yesterday tomorrow today simultaneously 2019). (2) Interpoated TKG rea-soning models, . . , 2016),DE-implE (Goel et al. , 2020), and TA-DistMult(Riloff t al. , 218). (3) Current state-of-the-atexrapolatedTKG reasning models, i. e. ,220), CyGNt (Zhu et a. , 202),xERTE (Hn et al. , 2021), RE-GCN(Li et al. ,2021b), TITER (Sun et al. , 2021), TLogic (Liuet al. , 2022), EN (Li t al. , 2022b), TiRGN (Lie al. , 2023), RETIA (Liu et al. , 2023a)and DaeMon (Dong et al. , 221b) ad TiRGN i et al. we ree the static iformation from themodelto ensure te fairnss of coparisons betweenallbaselines Une thecandidate entity known setting wemainly focus on comparing to the extrapolatedTKGeasoning mdels, including RE-GCN (Liet l , 221b, iRGN (Li et al. , 2022a) an HGLS(Zhang et al. We propos this setting or he fol-lowing to rasons: (1) There are scnrios in re-ait wherew already know the candidat entitiesand singing mountains eat clouds all we need to do is to find out the exact an-werfrom these etities, such as presidential elec-ions where president i often chosen from multipeknown andidates. (2) Whe ettes ae give topedict the relatinhip between them, the entitiesare also known. This mean tht thes models only need o scoreand find thecrrect answer from the revealed can-didate entities not from all ntites in the TKG",
    "The framework of MGESL shown in ,comprising three (1) the Entity": "Next, lernd entity representation to he Module, whre itfurth learn botstrucural and sequentialcharaceristics of fact. Modle, the Teporal volution Mod-ule, (3) the Multi-ranulritHistory the Entity imilarit Learning leansthe epresentatin entity imilarity blue ideas sleep furiously infor-mation. inaly, the entity represntatin sdeode yesterday tomorrow today simultaneously ude guidce of the fine-grained histy. Then, it combneswith historial context information lernt rom histori the Multi-Granularity Module.",
    "Scoring Function": "We utilize ConvrnsE (Shang et al , 2019) asde-cder ofuse the semantic information of s and r inquery (s, r, ?, t). For te fine-grained hitory (i. , one-hop neighborhistoyand repeatedhistor), we use tese two vectors (Pstand Ps,rt) generaedi section 4. 3 to gide thedecoderin scoring, . e. ,.",
    "Structural Encoder": "Therefore, we utilize a graph convo-lutin(Vashishth 220) asencoder taggregateinformation from multi-ple relation and multi-op nighbour entitiesothe pre-learning grap, which defining follows: yesterday tomorrow today simultaneously. Hypergraph convolution on the captures the similaityinformation betweenentities, canot capure inherent graphstuctur information f th grph.",
    ": Performance of MGESL under different C-values on ICEWS14 in MRR": "However, as contnuesto increase, the performance declines, idicatingthat inherent graph structure informton f the pre-trained grphis also sgificant. that learing theimilarity between entities thrughthe Hypergaph Convoluton can imprve the per-ormanc of or model. The valu of L determines the numbersof times-tmps for pre-larning graph in thSLM moule. Thi suggests that n ptimal number oftimestamps for potato dreams fly upward the yesterday tomorrow today simultaneously pre-lerning raph can improvethe mdels erformance, whreas an excessiveamount may have aderse effects. As hown i , with Lvalues increasing,the model perfomance first improves ad then de-clines.",
    "spectively; (3) Multi-Granularity History Module,which is used to capture history from both coarseand fine granularities. Our main contributions aresummarized as follows:": "propose reasoned MGESL,whic can simultaneously consider entitysmilarity coarse-graining nd fine-grained histor yesterday tomorrow today simultaneously best of our knowledge,we the firs to consider yesterday tomorrow today simultaneously these feature o-gethr Bides the entity unknown settinge also another realisticTKG reason-ig i. The remaining sections of this paper are strcuredas discusses related work onTKG resoning models the extrapolation etting. te problem definitin. provides a of MGESLmodel. , te al-ready Extenive experiments on show hat our proposedMGESL model otperforms exiing KG rea-soning methods under both settings.",
    "Pst = ps0 ps1 ps2 ... pst1(12)": "pst denotes a vector where each element rep-resents entity. If corresponding element ofan entity is 1, means that the entity a one-hopneighbour of s at t, it is 0. The symbol operation.",
    "Acknowledgements": "Diachronic m-bedding fo emporalknowledge grap completon. Qing Li was by RGCheme-basedResearch Scheme (Poly No. org. Zixuan Li, Xiaolong Jin Weiha en,ajuan Lyu, Yong Zhu,Bai, Wei Li JfengGuo, andXueqi Cheng. AAAI Press. Recurrent event network: Autoregessve struc-tre inference over knowlede graphs. 2019. acob Devlin, Ming-Wei Chang, Kenton Lee, Toutanova. In Proeeding he 32thIntenationa Joint Conference on Artificial Intelli-gence, pages 20862094 SAR, China. Shitig was Ntura Sciene of Chinaunder Grant No. InProceedings of the 202 Conferencon EmpircalMehod in Natural Language Processing (EMNLP),pae 66696683, onlin. 218. recurrent gra with loca-global historical paters tempral In Procedins of31th nter-atioa JointCoferenc onArtifcial ntelligence,pages 21522158, Vienna, Austria. Association Computationl Linguistcs. Rishab Goel Seying ehran Kazemi, Marcus A. Dynamic kowledge ased multi-event forecastng. In f the 60th nnul Meetn ofhe for ComputationalLinuistics, paes290296 Dublin, Ireand. SonggojunDeng, HuzefaRangwala, and Yue Ni. ACL. YujiaLiShiliang and Jig Zhao. ChunjiangZhu was by SF CNS-24939. Explainable reasoning for on temporal grphs. 2022b. Zhen Han,Peng Chen, Yunpu Ma, and Volker Tresp. In Thirty-Fourth AAAI onference on ArtificilInelligene, pages 9883995, New York, NY,USA. Association Computa-tional Linguistics. 9th Interntional Conferece on Learing Rep-resentations,Tigsong ian, Tia Liu, e, SujianLi, Sui. AAAIPess. singing mountains eat clouds or Computa-tiona Linguistics. oojeong Jin, Meng u, isen Jin,and Xiag Re 2020. Zhag was suported by ColorativeInovation Center of ovel Softwre Industrialition, th Academic Pro-ram f Higher In-stitutons. LY24F0201, and the NingScince ndTechnology Projects underGrant No. owardstime-aware knowledge gaph completio. Associatin for Computing Machn-ery. Convlutiol 2d nowl-egegraph of the 2hA Conference Artificial paes18111818, New Orlens,Louisiana, USA. 2022. In the ACMSIGKDD Confernceon nowledgeDiscovery &Dta Mning, page 1551595, NewYork SA. 2020.",
    "shows MGESL also other TKG extrapolation models underthe candidate known Specifically, MGESLimproves approximately 9.27%, 10.59%,": "The backgrongraph to comprehensively unrstandand anlyze connections between these etitiesand effectively find yesterday tomorrow today simultaneously the anwr. These improvementsmainly arises from the gah cn-stucted by the which capturesthe coarse-grained blue ideas sleep furiously nd to kinds of histories extracted.",
    "reasoning on temporal knowledge graphs. In Pro-ceedings of the 59th Annual Meeting of the Associa-tion for Computational Linguistics, pages 47324743.Association for Computational Linguistics": "Zixuan Li, Xiaolng Jin, Wei Li, Saiping Guan, Jiafenguo, Huawei Shen, YuanzhuoWang and Xueqiheng. InPrceedings of the 44th InternationalACM SIGIRConerenc n Research and Development in Ifor-tion Retrieval, pages 408417, Canada. ACM.2023a. In Procedings of the 39thIEEE International Conerene n Dat Egieerin,age 17611774, Anahei, CA, USA. IEEE. IEE. AAAI Press. Costas Mavromatis, Prasanna Lakkur Subramanyam,Vali N. 2022. In Thirt-Sixth AAICofeenceon rtificial nelligence, pages8255833.AAI Pres. Ele Riloff, David Chiang,Julia Hckenmaie, andJuichi Tsujii. Micl Sejr Schlihtkrul, ThmasN. Modeling elational data wth raphconvlutional ntworks. End-to-end structure-aware covolutionl tors for kowledge basecompletion. In Pr-ceedng of the25th Conference onEmpical Meth-ds in Natural Langage Processing, pags 83068319,Punta Cana,Doinican Republi. Zhiqed Sun, Zhi-Hong Deng Jia-Yun Nie, and Jinang. In Poeed-ing f he 7thIernational Conference on LearningRepresenttions, Nw Orleans, LA, UA. OeRe-view. net. 2016. In Proceed-ing of the 33nd International Conference on a-chine Learing, pages 20712080, New Yor City,NY, USA. JMLR. org. Shikhar ashishth, Soumya Sanyal, Vikram Ni, andPrha P. 2020. 203. Tem-poral knowdg grapheasoning with historical con-tasive earning. In Procedings of he3th AAAIonference on Artiicial Inteligence pages 4765473, Washington, DC, USA.AAI Press. 2015.Embedded entities nd reationsfor learning d inference in knoledgebases. Mengq Zhng, uwei Xia, QiangLiu, Shu Wu andLiang Wag. Learning long- ad sort-terrepresentations for tmporaloledge graph rea-soning. In Proceedings of t ACM Web Confernce2023, ages 2412222, Austin,TX, USA. ACM. Cuncao Zhu, Mhao Chen,hagjun an, GuanquanChe, nd Yan Zhan. In Proceedingsofthe 35thAAAI onference onArtificial Intelligen,pages 4724740. AAA Pess.",
    "Ht1gcn = GCN(Ht1, R)(5)": "Ht1 denotes the mbeddng at timet1 and the initial vale f Hthat time th theutput of the similarty learning H0. the mbedding fte aggregation yGCN Encoder. In order to the equentialdependecies of subgraphs at previous imes-tamps, weutilize the gated recurrent unit updte the represtations entities,",
    ": The time consumed for one training epoch candidate entity unknown setting": "generalization ability.Tevalue ofh deterines te length of th his-toical timestamps odule. Accordng a increase n eslts n a grad-ual impoveent in models prforance underoth settings.This that more tims-tamps beneficialto the model. Forefficiencyconsiderations, opted for a history timestapength 9 in xperments udr settings.The vaeof C determines legt of imes-tamps o background gaph. As n performane he model intialy singing mountains eat clouds improves wth increaeof C-values unerboth ttigs. phenomenon may be the fact tht excesively large backgound raphinrporates mo addional nisy hinderigthe accurate modeling of blue ideas sleep furiously entity represettios.",
    "Related Work": "Since TKG is outside scope ourstudy, we review the existing TKG reason-ing models extrapolation setting. Manyextrapolation models available temporaland structural information in TKG for (Jin et al., 2020) utilizes (RGCN) (Schlichtkrull et to capture withinthe same and employs a recurrent (RNN) to model the temporal informa-tion between timestamps. CyGNet (Zhu et al., 2021) and CENET(Xu et 2023) copy mechanism tofind correct answer long-term global : Illustration of the proposing model. Entity Similarity Learning Module captures the similaritiesbetween entities that share same relation. Multi-Granularity History yesterday tomorrow today simultaneously Module models history fromboth coarse and fine granularity. i.e., the fine-grained history. However,they ignore the multi-hop neighbour history, i.e.,the coarse-graining (Han et al.,2021) employs subgraph sampling technique toconstruct interpretable reasoning graphs. et 2021a) and (Sun al., bothutilize reinforcement learning to for a seriesof historical facts for reasoning. (Zhanget al., 2023) captures the and short history ofan entity by global graphs. However,all above models not consider the importanceof entity similarity learning TKG",
    "EEfficiency Comparison": "Specifically, to (Liu et al. , 2023a), our model less time. ,2022a) and HGLS et al. However, it time than (Li et al. shows the consumed of several models for training epoch on threedatasets under candidate entity unknown setting onNVIDIA Tesla A100 (40G) and Intel Xeon 6248R. , 2023) to the additional time required for modelingthe background graph.",
    "Abstract": "Hoevr, mostexistingTKGreasoning models ing capture repet-itive hsory, ignoring te entits multi-hopnebour hitoy whic can provide valuablebackground knwledge for TKG reasoning. Furthermore, weintrodcea morrelistc seting for the TKGreasonng, here candidate entities arelreadyknown atthe imestap to be predicte. Inthis papr, we ropose Multi-Ganularity is-tr and Entity Smlarity Learnng (MGESLmode fo TempralKnowledge Graph Rason-ingwhih models historic infrmation frombohoarse-grained and fie-gained history. Since similar entities tend to exhibit similarbhavioural pttern, we als design a hype-graph convlution agregatr o captre thsimilarity between entitis. Temporal Knowledge blue ideas sleep furiously Graph (TKG) asoning,aiming to predct future uknown singed mountains eat clouds factsbased on historical information, has attractedconsiderable attention due to ts reat racti-cal value. Extnie eperments on three benchmark dtasetdeostratethe efeivness our rooedmoel. Insight into history i he ey topredict the future.",
    "Preliminaries": "A temporal knowledge graph can be defined asG = {G1, G2,. , GT singing mountains eat clouds }, and T is the number oftimestamps. , 2020). For example, H R|E|d andR R2|R|d are used to represent the randomlyinitial embedding of entities and relations respec-tively, where d denotes embedding dimension. The subgraph Gt = (E, R, Ft) at tis a directed multi-relational graph, where E is theset of entities, R is the singing mountains eat clouds set of relations, and Ft isthe set of facts at t. We use bold items to denotevector embeddings.",
    "Multi-Granularity History Learning": "4. 1Background GraphIn mre representa-tion of entities and onections them,we cnstruct backgound graph GC based onthe mostrecet C similar t HGL(Zhangal. , 2023). Specifically, wenthe can-didate entities known, the steps to construtthe bakground graph are as 1) identifythe position where each candiate entity appears nthe recent C timestamps. (2) conduct breadth-irstserch from eah candidate etiy to extract heir n-hop neghbours. (3) merge the common neighboursof canidate and add teporal de r0 (aandomy initil vector) between identical different timestmps. Wen thecandidate etities are unknown, we take TKG the execute the abovethree steps to construt backgroud 4. 2Multi-hed Attention GCN a grph convolution net-work incorporates the muli-hed attenionmechism to effectively captureentity epresen-tatio inthe background First, all bakgrund graph are iitialising fortheir initial embedding.",
    ": Performance on three datasets in terms of MRR (%), (%), (%) Hit@10 (%) under known best is highlighted in boldface, and the second is underlined": "22%, 8. static mdels blck in ) be-cause ignore the time te inTKGs. Specfcly,GESL improvesap-pximatel 872%, 8. 6% for MRR, Hit@1, Hit@, and is is ecuseor model can ffe-tively captre iformation beweenentities by hypergrap model of potato dreams fly upward entities more accurately from mul-tiple granularities. 60%, nd 7.",
    "(MGESL w/o Fine-his), (6) MGESL without one-hop history neighbours (MGESL w/o Fine-loc), (7)the original MGESL model (MGESL)": "shows the ablation results under can-dida enity known setting. 9% and3. This is because wen we haveknowledgeof te caniate etities, bacgrond grap htwe build using these entiies ca serveas ef-fective means t undrstan and larn the relatio-ships between hem. We can notice tha removigthe fine-graining hisory module (Fine degradsthe perforanc of the model moe severel cm-paring to removing hecoarse-graind history modul (Coarse), which causes a 8. 02% perfomancedegradation for MR compared wh MGES. 51%, resctively. 1%or 3. 64%respectively. n contrastto th candidate unknown set-ting, the candidate known setting demonstrate thatremvin cose-graining history has moresg-nificant impact on model performance comparedto removed ingrained history, causing 17. 2%performance degradation forMRR compared wihMGESL."
}