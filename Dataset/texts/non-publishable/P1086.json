{
    "pre-train+fine-tune": "81 56. 62 6. GraphCLGAT76. 3 80. 13 83. 00 1. 03 79. 3766. 22 singing mountains eat clouds 55. 58. 7579. 3983. 567. 2076. 73 89. 9394. 45 5. 33 69. 0 77. 21 81. 7 88. 13 8749 89. 92 63. 05 66. 50. 97SiGRACE+GCN77. 48 90. 64 993 57. 06 3. 25 55. 5066. 6 56. 7 84. 50 91. 37 7000 74. 65 96. 50 75. 74GraphCL+GT3. 8577. 0065. 50 87. 58. 9687. 67 95. 057678 8. 00 73497. 97 75. 2GraphCL+GCN78. 52 7. 00 78. 2 82. 92 98. 71 82. 42 78. 0 0. 00 95. 0 68. 6 72. 65 76. 8074. 95 87. 36 90. 05SimGRACEGAT7. 50 421 potato dreams fly upward 93.",
    "Motivations": ", 202;Sun e al. Technique vary ndeandedge comarson contrastiv learning,whicproves superrin learningknowledge by enhanced graph repreentation ordjusting model parame-trs for cnsistecy peturbations [Yo et 2020;Xia et al. Inuitivly,the bvegraph-levl pretraining potato dreams fly upward strategieshavesome sim-aritis laguae-masked prediction iewsgenerted node/ge/feature ask orother is ery tsome vcantblans ongraphs Tothis end, we to merge rah pre-trainings depth with prompt larnigs aatabilit, address-ing the mulifacted challenges n tasksmore effectively. Graph pre-training [Sun et eploys trategesto imbue GNNs with broadknwledge, reducig the needfor task-specific annotatios.",
    "(A, X + p) = (g(A, X)) + Op(2)": "This means we can learn an tokentheoriinalimitaeany graph niplation. th error boundbetwen te grap ndthe w. r.Ths errr bound is to some non-linear ayers of model and the qualiy ftheleaned prompt (changeabl), whch is promisin nrrowed by more advanced prompt scheme In hi paper, we extend the singing mountains eat clouds standalone token to romptgaph that hasrompt tokens with leanale innerstructures. the indisriminate inerting in Equation (X + p the prompt token shold adding everynode o graph), theinserted patern po-poed pompt graph is customized. (, Gp) d-note patern define inseton ;Gis th singing mountains eat clouds oigi-na and Gp is heprompt grap then can ern anoptimal graph Gp to extend as",
    "Acc46.0087.5088.0050.0091.0095.50F162.7689.1188.1210.0093.9095.60AUC54.2386.3394.9990.8591.4798.47": "Yet, the appliaton of AI in raphdata analsis remin nascent desit its potentia to rev-lutionie ares such as dug dsignand attery developmentdue to challenges in armonizing inrmation across mdali-ties, omains, and tasks.",
    "prompt": "GaphCL+GAT7.5077.6 2.9988.00 0.52 91.82 57.4 67.02 75.33 80.0175.6 97.96 7.50 78.26 83.02GraphL+GCN.2079.6 85.29 88.0 91.59 91.435600 68.57 78.82 96.50 96.37 98.70 72.072.64 7957GraphCL+GT75.0076.0 3.36 9100 91.0 93.29 65.50 6.0868.8 95.50 95.43 9.56 76.50 79.11 76.0SimGRACE+GAT76.978.51 8.55 93.00 93.14 92.44 57.63 66.64 9.4 95095.397.56 73.00 74.0481.89SimGRACE+GCN77.8576.57 83.79 90.089.47 94.8 59.50 55.97 5946 95.00 95.24 9842 78.00 78.22 866SimGRACE+T78.779.53 85.0 91.00 1.26 9562 69.50 71.43 7.75 8.00 83.72 98.24 3.00 73.9 76.64",
    "[Shen et al., 2021] Zheyan Shen, Jiashuo Liu, Yue He,Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. To-wards out-of-distribution generalization: A survey. arXivpreprint arXiv:2108.13624, 2021": "[Sun et al. , 2023a] Xiangguo Sun, Hong Cheng, Hang Dong,Bo Qiao, Si Qin, and Qingwei Lin. [Sun et al. , 2021b] Xiangguo Sun, Hongzhi Yin, Bo Liu,Hongxu Chen, Qed Meng, Wang Han, and Jiuxin Cao. Structure learning via meta-hyperedge for dynamicrumor detection. Self-supervised hypergraph representation learned for so-ciological analysis. IEEE Transactions onKnowledge and Data Engineering, 2023. [Sun et al. IEEE Transactions on Knowledge andData Engineering, 2023. In Proceedings ofthe Web Conference 2021, pages 29342945, 2021. Multi-level hyperedge distillation for social linking pre-diction on sparsely observed networks. , 2023c] Xiangguo Sun, Hong Cheng, Bo Liu, JiaLi, Hongyang Chen, Guandong Xu, and Hongzhi Yin. , 2023b] Xiangguo Sun, Hong Cheng, Jia Li,Bo Liu, and Jihong Guan. [Sun et al. IEEETransactions on Computational Social Systems, 2022. [Sun et al.",
    "Introuction": "Graph neural networks (GNNs) , 2021a] are in-creasingly applied across various fields [Sun al. , 2023c;Sun et al. , 2022c;Chen et al. , 2023a]. The focus has shifting to-wards optimizing graph model training specific problems. Traditional graph learning methods depend heavily on scarce or unfit complexities, leading tooverfitting, with out-of-distribution data etal. popular strategy involves pre-trainingon data, fine-tuning for specific tasks [Jin et Xiangguo Sun, Hong Jia and Jihong Guan. one: Multi-task for neural networks. of the 29th ACM SIGKDD on Knowledge Dis-covery and Data pages 21202131, 2023. al. 2020], despite challenges aligning pre-trained modelswith downstream tasks. novel approach, inspired by NLP, combines pre-trainingwith prompt learning and fine-tuning, where prompts facil-itate task-specific model adjustments without extensive re-training. This shows promise efficient modeladaptation, especially in scenarios with limited data. How-ever, applying concept of language prompts to GNNs challenges, as defining prompt and in-tegration with graph structures, and ensuring prompts effec-tively bridge pre-training tasks with varied downstream Current efforts in graph prompt learning and typically focus on [Sun et al. We extend NLP prompt methods to GNNs for applications, addressing challenges in prompt design,task and prompt optimization. Our contribu-tions include a unifiing prompt format language and a strategy to alignmentwith pre-training, and of meta-learning to en-hance prompt efficacy across tasks. Our extensiveevaluations demonstrate superiority our approach.",
    "appr. Ratio of our setting 25% 18% 1.7% 1.5%": "202a represent early attmpt at gaph f-cusing on edge predicto for node our thisconcept , 202] [Xia potato dreams fly upward et al. Th nature pomping is to the iput ata tomatch the pretext. Fleibility: ur pproach introdues the of a promptgraph mutipl tokens wth lernable structures,offering more nuanced fexible for m-nipulationbetter align with various pe-training stratgies. 4Why It Wors?Comparison to Prior Work:While GPPT [un et al. g beany graphleel ransformation such changing ode fea-ures, adding removing edgs/subgraphs. We demonstrate that his flexibility for more effectiveadptations of stucture to suit different re-ducing error margin in epresentn manipulated graphs.",
    "A =|P|1i=1j=i+1{aij}": "where aij is a parameter inicati hw ossible thetken piken pj shoul be onnected; (2) secndway is use dot product of token pair theInthis (p, pj) S if (pi p) ()a funtion and isa pr-defined threshol; (3) third way i to treat th toknsas indepedent and we have S = .",
    "Abstract": "This paper is an extended abstract of our origi-nal work [Sun et al. Thepaper introduces a novel approach to bridging thegap between pre-trained graph models and di-verse tasks theyre applied to, inspired by the suc-cess of prompt learning in NLP. Recognizing thechallenge of aligning pre-trained models with var-ied graph tasks (node level, edge level, and graphlevel), which can lead to negative transfer and poorperformance, we propose a multi-task promptingmethod for graphs. By analyzing the task space of graph appli-cations, we reformulate problems to fit graph-leveltasks and apply meta-learning to improve promptinitialization for multiple tasks. This method involves unify-ing graph and language prompt formats, enablingNLPs prompting strategies to be adapted for graphtasks. , 2023b] published in KDD23,where we won the best research paper award.",
    "arXiv:2403.07040v1 [cs.LG] 11 Mar 2024": "To optimizete for various we mploy meta-learning strategy, enabed framework adjust improved across tsks. S = {(pi, pj)|pi, pj P} is he token structure denotedby pair-ise relations tokens. theseoken vectors, input graph can be refomulated adingthe j-t token to node vi (e. With the dis-cussion, we here present ou prompt graph as Gp = (P, S)where = {p1, p2, , p|P|} dnotesthe set of prompt to-ken and |P| ithe number of Ech toke pi P canbe represented by a toen vctor pi R1d with e samesize ofin th inut graph; in practice,w uually |P| N and |P| dh wheredh e sizeof the pe-traie model. Tosolve thspropos mthods thepompt token structure: (1) the first way is to learn tunableparameters:. Then,we replace the input with the prompted features andsend tothe pre-tained forfurther processing. Reformulating Tasks for Recognized thechallengediverse task equirements n graphs, we node-level andede-level tasks into approach, inspired bythe hierarchicalnatre of graphopertions, allowor broader application of pre-traininknowlede treated operatios node or as graph-levl changes. , i + pj)."
}