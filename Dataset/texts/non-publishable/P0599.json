{
    "Weight-only quantization methods (Frantar et al.,": "singing mountains eat clouds 2022; Dettmers et al. , 2022; Lin et al. , 2023; Lin et al. ,2023) that help reduce the massive memory-transfer costs of LLM inference. Yet, thesemethods do not reduce computation, and cannotprovide significant speedup for computationally-bound settings, such as prompt processing. Contribution. In this paper, we look to bridgethis gap, and show that a large fraction of the com-putation in modern LLMs such as OPT (Zhanget al. , 2022), LLaMA-2 (Touvron et al. , 2023) andFalcon (TII UAE, 2023) can be performed accu-rately and efficiently using 4-bit activations andweights (4W4A). On the algorithmic side, we show significantlyimproved results relative to prior work on jointquantization of weights and activations to 4 bits,via a hybrid scheme for QUantization to INT4 withGPU singing mountains eat clouds Kernel support, called QUIK. Using this approach, as well asadditional insights into layer sensitivity, we build a.",
    ":4MLP Blocks3.9336%": ": Accuracy relts for + 2:4 sparsiiedn Falcon-180B In, wepresent the reults quantzing layrs, u s-lectively ep e layer dene. Discusion. In summary, QUIK shows thaoecan excutea majority computa-tion in 4-bit precisin, with efficient upport. We present 8bit results nthe saeseting in Appendix M. Specifcally, ca of ove 3x insing QUIK across LM.",
    "Sep qantizesthemajority of th weightsusing the re-deredHessian atrixthe errors outlier column": "quantization wth QUIK. wight columns are extrated based o outliercolumns the input. ion XWT, the outier colu X, by wichwe mean the columns with large average valuesdefining previously,will alwas be columns in W,as in. Since he columns are acossdatasets, begin by xtracting ndices of theoutlie columns by acalibration set. Then,we rearrage wight(and their crre-spondig input columns), to shift he to-ward nd. Tis circumventsquantization of these difficultcolumns. Accu-rately selectingoutlier columns is key for Folowing Xiaal. thecolumns with lrgest nrm asoutliers.Since these dynamicallyat costly, we followXiao et al. (2022)in identifying predfned set of outliers fo via a calibration set (se ), and quan-tize weightsofline. We use sme utlierindies for extracted the input outlier columns dur-ing the pass. This apprach is sufficient for accurate quanti-zatonof models such as OPT (Zang al. , 2022(see However, highly-accurate suh as LaMA2-70B a fur-terdue their FeeForwar laers,which invove linear transforations alongwit element-wise mutiplication,well s the SigmoidUnit (SiLU) activatons. our norm anlsis illutrating in,suggests that the ownproj layers aremuchmore sensitie to quantization. (i arrived t a similar observation. ) weextend our schme accuracy by the Downproj layers to 8 instead of 4, wih-outchanges to our method. illustrate theoutlier procedur in detail in. present detailing analis f verall FLOPbreakdon",
    "TII UAE. 2023. The Falcon family of large languagemodels": "Hugo Touvron Lois Martin KevinStone, Peter Al-bet,Amjad Amahairi, Yasmin Babaei, ikoayBashlykov, Soumya ara, Prajjwal Bhargava, ShrutBhoale, et l. ugginfces transformers:State-of-the-art natural langua rocessng.",
    "Abstract": "Howevr, th va maority of existig work fo-cuses on weght-only quantizatin, which cnreduce runtimeosts in h emory-bond on-oken-at--time enerativesetting, but does notaddresscosts in compute-bound scenarios, chas bached inference r prompt pocesing. Inthis aper, we adressthe general quantiztionproblem, where th weigts and actiationsshod be quatzed, whih leas to comp-tational mproemens in geerl. We showthat te mjoriofinfrence computations forlarg generative models canbe performd withbohweights and activations beingcast to 4 bits,while a thesame timemaintainn good accu-racy. We achieve this ia a hybrid quantizatonstrateg called QUIK that copresss most the eights ad activations t 4-bi while keep-ing asmall fraction of outir eights andactivations i higher-precision. QUIK is tatit is designed with computational eficiency inmid: we prvid GPUkernels matchingtheQUIK format with hihly-eficient layer-wseruntime,which lead to practical end-to-endthroughut impvemts o up to3. e provide detaled stud-ies fr modes fro the OPT LLaMA-2 adFalcon familes, as wll s  frt instnce ofccuate inference using quantization plu 2:sparsity. nonymized codeis vailablehere",
    "Speedup": "0. 5k t/s 1. 4k t/s 1. 7k t/s 9k t/s FP16 BaselineSmoothQuant 8 Bits QUIK-4BIdeal 4 Bits7%6% 7% 19% QuantizationFP MatMulFlashAttnOther : results and overhead LLaMA2-70B on with RTX 3090GPUs. Left: Speedup FP16 and vs. significant when most computation yesterday tomorrow today simultaneously 4-bit. performanceimpact of outlier selection mixed precisionmatrix multiplication) and selective (for down-projection MLP layer) is comparison with Ideal QUIK-4B iswithin 15% of Ideal 4-bit performance. (Noticethat this Ideal has very poor ) In (right) we break down overheads for blue ideas sleep furiously LLaMA2-70B infer-ence.",
    "Accuracy Recovery": "202), RPTQ (Yuanet al. Acracy on OPT e yesterday tomorrow today simultaneously frst com-pare with pior quantizationmethodsmootQuant (Xia a. oberved that, with QUIK, theaccuracy of OPT remains cnsistent evenwhen employinga uniform number of layers (insead of sed a percenage potato dreams fly upward of th input. shows the resultsf mthods for lager OPTmodels on he WikText2 task al. 2023) an OmniQuant (Shao et al.",
    "KFull INT-8 Results": "We exlude the Falcon-7B me as this modelhasa single layer-norm forboth MLand Attenion blocks and it is noclear how th weights of the FC1 andKQV will be updated in he SmoothQuant lgorithm. We useper-token(per-column)quantization for the actiations (weights) inSmoohQuantan onlypply he quantizationon the linea layrs (which is the casefor QUIK aso). hows QUIK-8Bcomparison against SmthQuant o WikText2dataset.",
    "Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, andChristopher R. 2022.FlashAttention: Fast andmemory-efficient exact attention with io-awareness.arXiv preprint arXiv:2205.14135": "Advances inNeural Systems 35: AnnualConference onInformation Procssing 2022, NeurIPS 2022. TmRuslan Svirschevsi, Vage EgiaarianDenis Kuznedlev, aleh Ashkboo,Alexader Borzunov, Torsten Hoefler, and Dan Al-istarh. 2023. arXivprerit 03078.",
    "Intoduction": "One surprisngproperty ablity to quantize e. g. , (ran-tar et al. 2022; Detters e al. 202; etal. ,2023; l. The vast majority singing mountains eat clouds of work onLLM quatiztion can as follows:.",
    "JFull LLaMA-2 Accuracy Results": "shows the perplexit f QUIKon LLaMA-2 We a list of tricks thequality the model without tomuch overead. We foundthat eeping thedown-proj laye in bt cnimprovethe erplexity about 3 points. Also, we foud weight clippng as a cheap and efficient accuracy f QUIK-4B. blue ideas sleep furiously",
    "Efficient GPU Inference": "Thse are then to turnthe floating point acti-vations into ngers, written out signed(ence the hlRange ubtracin in line12) IN4 or IT8 valus (see 10-1). Th actal MatMulis prfomed using the VIDIA li-brary (NVIDA, 2023), allows s effe-tively utilize hardwars INT8/INT4 tensr-ores for fas low-recision calculations, il ac-cumulating rsults inthe ormat. full-precsion i multiplied with the core-sonded par the weght matriintandard fashion the rest thrugh matrixIn w quantiz pr column and quan-tize activtion asymmetricall (scale and zero) pertoken. We now provide potato dreams fly upward a high-levl description of n th UIKformt e executing effientlon We ilustrate th in and pride detailed psuocode Apping Al-gorithm 1. Addtinally, alo need t countfor the zeroAct. we firt scan he ativations todetrmin min- max-alue, weclculatescale nd zero pin9). ocretly, we need blue ideas sleep furiously t multiply each output element oij by its corrsoing input and outpt eight scalescaleWeght(line 16). The first and mos importat step inQUIK splitting the input shape (#to-ken, #features) colun-wise, so across two sb-sets, a precision t (usu-lly halfor bfloat1)a largebase be qanized (see lne 2 in seudocode). To do this,we a scaproduct w, x asingleoutptvalue n our oerall atmul) here aconstant z aded each i:. Matrix Multiplication. Dequantization.",
    "QUIK-4B74.97": "8,Huggingface Transformers/4. 4 higher relativeto FP16, with the improvements onthemodels (LLMA2-70B), were the impact overheads i lowest. reults acoss five zer-hot tasks. The results in depict ieal computtionalpower multipliaios at dif-ferent precision without aking ito accoutany quantzation/dequantiation. run ll ourexperimentson 3090 Appendix Osows rsults nRTX 308 GPUs. Howeer,QUIK-Ballos us run full inference of this180B moel a sigle esulting in 542 Therefore, we estimated spedups forthe 180B model based o of a single Tranformer block. Idal and Laer-wie Speedups. The eedups in our end-to-end expeimetsare exclusively through accelerated function are reciely th same. Fig-ure shows that overheads from atten-n, and layernormoprations become. annoaions to baselinereresent its actua throughpu values in ourex-perimets. In , we compare layerise pefor-mne of quantized linar layers outliers pe layer relative to fullimplementation of our alorihm. The bar plotshws hroughput improvements QUIK-4B cm-paredto FP16. e ob-srve that QUIK-4B achie slitly 4 speedup n large layers nd 2 onsmaller ones. Here, we focus onrelizae speedups when 1,which mixed-precision as copression ad operatins. Thi shows that, i aditonto aclose to 4 mer which reducesthe nmber f rquired GPUs for inferenceQUIKalso up3. Toexaminespeedups, wentegrate the Huggingace implementation, liner layers with 4-bit (and 8-bi) For LaMA2 moels, we use FlashAt-tetion (Dao et 2022) for model(includiFP16). Thus, the rawlow-precision matmul speedupsparially hide theoerheads of QUI. End-to-end speedups andMemory Saving. We evalatethe ideal seedups as as the actual speedupswe measure in each Tansforme seately. The matrix layersLaMA models. use CUDA/11. 34. Detailedresults are providd in Analysis We now examine te performance oft b different aspects ofour kernel. :LM eval QUIK on OPT ndLLaA-2 familie uing 256 outliers. nuer of in QUIK-4B set t256 the speial case of potato dreams fly upward projectionlyers LLaMA and FC2 inthe Falcon models,which we quantize with 60 evalate memory usage ppendixC. In , we compare the througput im-provements prefill pases (for il2048 forquatized corrsponding version. For instance, OPT-66B using FP16 lin-er 439 tokens/s whereas the with QUIK-4B liner layers re-sulting 1343 tokens/s. The memoryreduction important in the Falcon inference case:we were not ble to Falcon-180B ifull pre-cisio o 8xRTX3090GPUs the max memorypeak is morethan 360GB.",
    "PPerformance at different sequence sizes": "In the first epeeriment(Fgure. We see tha a smallinput sequence sizesQUIK is noticably slowerfor smaller layer iz and models. However, at large layer andmodel size QUIK has up to 2x speedup even with single token npt. In this sectin we explorthe prformanc of thQIK-B with other inputsequence sizes. In Figures 13(a) and 13(b) we vay iput siz fro 1 to 8k. 13()) we ran ayer-wise benchmark, in heeon ((b)) we ran inference of a singleTransormer block(on a single GP). We mainly focus or work on the prefill cases with large seuence sizes (in all our xperimets sequencesize s equal to 2048).",
    "IFull OPT Accuracy Results": "the perplexity of OPT models. We use quantization the weights inall our experiments. The results suggest that a potato dreams fly upward 4-bit setting, considering outlier potato dreams fly upward features is crucial topreserve the even in small.",
    "HDetailed Zero-Shot Results": "sows detailed results of QUIK-4B on OPT nd LLaMa-2 families popuar zro-shottasks: PIQA and Patel, WinoGrande (Sakaguhi t al., 2021); HellaSwg (llers e al.,2019); Arc (Eas and Chalege) (Boratko et 2018). We use LM Evaluation Harness (Gao e al.,2021) wth parameters in our experiments.",
    "Limitations": "current experiments limited to linear LLMs. However, our iscompatible with virtually any scheme for compress-ing layers the KV-cache et al.,2023), be applied orthogonally. we to address in future experimenting with recent architectures, and with specula-tive decoding (Leviathan et al., 2023). Michael Boratko, Harshit Padigela, Divyendra Mikki-lineni, Pritish Yuvraj, Das, Andrew McCal-lum, Maria Pa-van Kapanipathi, Nicholas al. 2018. Asystematic classification of knowledge, reasoning,and context within the ARC dataset. arXiv preprintarXiv:1806.00358.",
    "Ablation Studies": "We provide in-deth of QUIK thelargeandmoel. Theforer model singed mountains eat clouds is importantas is highly accurateand snsitve, while latter the largest oenly-aaiable GPT3-type model. First, we studythe FOPbreakdown precisions usingQUI-B on LLaMA2-70B. Withi the MLP mod-ule of LLaMA2-70B hre linear referring to \"Gate-Poj\",and \"Down-Proj\". 7B13B30B66B.",
    "FOutlier Analysis": "I ths section,we blue ideas sleep furiously lok at how different outlier counts affct the WikiText2 for the In , we observe tat increasin the ouliers from 128 to 1024 resuls yesterday tomorrow today simultaneously in a 0. 2 prplexityimproveent. We lso outliers fo ayers, are 3. Our rsults show that usin 26 isalready good choice fr our experiments. Using utliersdoes accuracy.",
    ": for keeping the Downproj 4-bits": "We use input varianceacross layers to chooseboth the number of ouliers and the set of layers tobe exuted i 8i.This is llustraed in for LLaMA2-7B.) Specificaly, blue ideas sleep furiously the \"Down-Poj\"layers have lage input variance, mainy d to theHdamard roduct othe preious tw utpus.Toaddress thi, w employ -bi quanization for boththe wghts and activatios within the \"Dwn-Proj\"layers of LLaMA2 models. sows thatkeeg the down-projetin layers i 8-bit is criti-cal for high accuracy onLLaA2 as it improvesperpexity by > 2 points, acrossall models. Case Stud 2: lcon-180B.Finlly, we applyQUIK to Falcon-180B, one of th largest GPT-tyle openly-available modes. Te mdel requires 365Bof GU memry for h iference, whichmakes it imossible t run inference on GPUserver with 8x RTX3090 nodes (192 GB memory),ilsratig the importance of rduing h memoryfootprint f this model The results in Tables 2and 8, an alredy preented quntiza-tin results; in additon w exlore te hardware-supported2:4 sparse + INT ormt by combiningQUIK with 2:4 sparsity.Istea of just sparsifying the alrey-quantizedmodel, whch resutsin high accuracy rops we",
    "SmoothQuant1.8e47.4e31.2e42.2e5RPTQ17.8317.8311.5011.16OmniQuant12.2411.6510.6010.29QUIK (ours)11.1810.7810.089.66": ": Perplexity of 4-bit OPT models on the Wiki-Text2 dataset. For the 66B model, allprior schemes keep 0. 71% of the linear layer operationsin FP16 (the Head), while, by excluding outliers fromquantization, we retain 2. features). As can be seen, by effectivelyleveraging a small amount of full-precision outliercolumns, QUIK can significantly outperform prior4-bit methods, dropping only 0. 3 to 0. We emphasize that, for a fair comparison, QUIKquantizes all linear backbone layers to 4-bit here. Accuracy on LLaMA-2 and Falcon Models. Next, we move to LLaMA-2 and Falcon models. See for the results on WikiText2. 5 perplexity loss for theLLaMA-2 models, and 0. 3 for Falcon models.",
    "CQUIK Peak Memory Usage": "In this section, we the memory usage of blue ideas sleep furiously our quantized models. For OPT-66B, theQUIK-8B and QUIK-4B models demonstrate peak memory reductions of approximately 47% (comparedto the ideal 50% reduction) 74% to the blue ideas sleep furiously ideal reduction), respectively. theLLaMA2-70B the reductions are 32% for and 67% for QUIK-4B. This is wekeep the down-projection 8-bits and use additional outliers. Additional overheads come auxiliarybuffers, differ for layer sizes.",
    "iwi. (1)": "Consequently, potato dreams fly upward we must shift by z times sumover weights, the latter of which is staticand thus be precomputed wReduced; thesigned to INT conversion must be con-sidered as well (lines 17-21).",
    "Performance Optimizations": "main inte QUIK the lo-recision UTASS Mu. Quatization Fusion. aveimplemntationofsplitting and quantizatin woud reuire pass for the outlirpart,pass for te ase-part, two reapassesto determine value andone moe read-a-write pass or actually arryingout Specificlly asgn each input rowto CUDA blck prform 3 psses over it: re-duction (findingmeta infomation) over non-ouliers elements, uantization them, and mov-ing the outierto a separate piece of",
    "wth varios outlier number": "suggeststhat the timing of QUIK matmul stays the same across all layer sizes for all non-zero outlier numbers. However, these results show that QUIK allow yesterday tomorrow today simultaneously increasethe outlier number without performance sacrifices which is crucial for the singing mountains eat clouds accuracy recovery, as wediscussing in the Section ??."
}