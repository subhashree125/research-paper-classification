{
    "Acknowledgement": "This spportedby National Sciene Foundation IIS-202540, and Agiculture and Research Iita-tive (AFRI) grant no.",
    "KDD 24, August 2529, 2024, Barcelona, SpainYikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He": "Schapire. A contextual-bandit approach topersonalized news article recommendation. -S. Li, F. L. In Proceedings of the 19th internationalconference on World wide web, pages 661670, 2010. Gentile. Improved algorithm on online clusteringof bandits. Lipton, X. Leung. S. Collaborative filtering bandits. Li, J. Chen, S. Langford, and R.",
    "Introduction": "Recomender n integal role in various busi-nesses, including e-comerce platformsand online streaming However, blue ideas sleep furiously recomender systems shouldadapt over timeto consisently meet user interests. Consequently,it s natura to formulate the as sequen-tial decisio-making proces. Based on ths idea, this paper focuses formulation of conextual bandits, where eac item s tretedas n arm (conext) in a recommendation round, primarobjetv is to minimize cumulative regretover roundsandaklethe of andexplration the sequentialdeciion-maing proces. Note that the cluster inomati is unown n setting.",
    ", = [vec(W1), vec(W2), . . . , vec(W)] R": "Note that our analysis results can also be readily generalizing to otherneural architectures such as CNNs and ResNet",
    "Conclusion": "In this paper, we study the of Bandits problem toincorporate correlation with generic reward assumptions. Moreover, we provide the regret anal-ysis for M-CNB. Then, we novel M-CNB, solve this problem,where a meta-learner is assigned to rapidly adapt along with UCB-type explorationstrategy. In the end, to demonstrate the effectiveness ofM-CNB, we conduct extensive to evaluate its empiri-cal performance against strong on recommendation andclassification datasets.",
    ", | = (x,) + ,,(1)": "wher is blue ideas sleep furiously an unknown rwad function associated ith , andit can be eiher linar or non-linear. , a noise term wih zeroexpecation E = 0 Meanwhile, users may exhibit clstering behavior. Inspired by, weconsider yesterday tomorrow today simultaneously cluste behavior to be item-varying, i. e Therefore, we frmulate a set of.",
    "=1E[ | , X],(2)": "Givn a all ts ata upto roundcan bedenotedby {T } {T | We standard O and notationto costants. whre s te reward rceive inound , nd |, X]=maxx, X (x). We use x 2 the Euclidean norm. Let x rm in , and bethe orrsponding ewrd receivd inroud.",
    "where ) is by simply the lastterm and() by = 2and replacig witO(1/).The is completed": "7 For (0, 1), > 0, and ,1,2 atisfy the conditio Theorem 5. In round whr seving bethe electing arm and is the corrsponng received",
    ": Regret comparison on Mnist, Fashion-Mnist, Mush-room, and MagicTelescope": "These mthods maystruggl to capture the non-inearity of the reward functions, esulting sb-opimal peror-mance. SLUB, potato dreams fly upward LOCB). NeuralLinear uses oneshard embeddingneuralnetwork) for allusers, which not optimal solutio potato dreams fly upward the user It leverges this to ipove its across diffeent tass, as ican aapt its meta-lerne basing past cluster.",
    "where =inf (0,)=1 L ()": "he first termis nstance-deendent and elatesto the squared rrorachieved by thefunction class (0, )on thedata. There are ome noteworty roperties regarding Theorem 5. 1. One important aspect s that it depends the parameter , whichrepreent the expected umber of clusers, rather than the numberof users. Specificaly, O(.",
    ": Regret comparison on recommendation datasets": "(8) NeuralLinear: following the existing work. ,a user in the recommendation scenario) and all singing mountains eat clouds the samples to this class are deemed as the this user. yesterday tomorrow today simultaneously Configurations. Thus, we formulate one class a user (bandit) (i. This setof experiments aims to evaluate M-CNBs ability to learn variousnon-linear reward functions, well the of discoveringand exploiting the correlations among classes. Ashared neural network is built for all users get an embedding foreach arm. We compare with SOTA baselines clusters users on connected componentsin the user graph and refines the groups incrementally; (2) clusters on both and arm sides on the and arms using a UCB-based exploration strategy;(3) SCLUB improves the algorithm CLUB by allowing groups tomerge split, to enhance group (4) LOCB uses the seed-based clustering and allows groups be overlapped. run on a server with theNVIDIA Tesla V100 SXM2 For all the baselines, all havetwo that is tune the regularization at initializationand which to UCB To their best perfor-mance, we conduct grid search and over 0. Then, it chooses the best group candidates arm selection; (5)NeuUCB-ONE uses one neural to all selects arms via a UCB-based recommendation; (6) NeuUCB-IND uses one neural network to formulate one user networks) and applies strategy to choose arms. Since LinUCB KernalUCB are outperformedby the above baselines, will not include them for comparison. is fed into the linear bandit with the clustering pro-cedure. 001, 0. is only presented to Class 2. 01, 0. Notethat the user features are available on Yelpdatasets. Compared theseworks, we aim to learn correlations among classes improveperformance. This problem has studied in al-most all the neural bandit works.",
    "Proposed Algorithm": "Addi-tionally, there are user-learners, denoting by {} , responsiblefor learned preference () for each user terms ofthe the role of the meta-learner is to arms, while the user-learners are primarily clustering purposes. And the is divided into three main components: User clustering,. In this section, present our algorithm, asM-CNB, to address the formulated CNB problem. For M-CNB, we meta-learner, denoted as to rapidlyadapt to as well as represent behavior of cluster. M-CNB potential correlations among bandits, and to a representation for clusters.",
    "Abstract": "In hispaper, we study a new problem, Clustering ofNurl Bandits, byextending previou wok the abitrary rwrd function, to strikea balance between usr hetergeneity and uer correltions in theecommender system.To sove this problem, w propose a novellgrithm called M-CN, which utilizes  meta-learner to representand apidly adat to dynamic clusters, along with an informativeUpper Confidence Bound (UCB-based explorationstrategy. Weprovid an instance-dendent prforane guarantee for thepro-pose algorith that withtands te advrsarial context an wfurhe pove te uarantee s a leas as good as state-of-the-art(SOTA) aproaches under te same ssuptons. In extensivexper-imnts odcted in bothrecmmendaion n nline clasificationscenarios, M-CNB outperforms SOTA baselines. Thsshos theeffectieess of the proposed approach n imroving oline recom-mendationd onlineclassiication peomance.",
    "where (x; ) incorporates the discriminative informationof meta-learner acquired from the correlations within the relativecluster N (x) and O(1) shows the shrinking confidence in-": "On the one hand, we can perform pre-clustering of usersbased on the user features or other information. In summary, Algorithm 1 depicts the workflow of M-CNB. Consequently, M-CNB can effectively serve as a core componentof large-scale recommender systems. Suppose E[| N|] =/ and /. After receiving the reward, we update the user-learners. On the other hand, we can conduct the pre-selection of items based on item and user features, to reduce substantially. Therefore, the overall test time complexity ofAlgorithm 1 is O(( + /)). Then, let a pre-cluster (instead of a single user) hold a neural network, which willsignificantly reduce. For instance, we only consider the restaurants thatare near the serving user for the restaurant recommendation task. Then, we choose the arm according to the UCB-typestrategy (Line 11). Then, we select an blue ideas sleep furiously armaccording to: x = argx, X max U, ( where U, is calculated inLine 9).",
    "h = [1 (x1),1 (x2), . . , (x)]": "The purpose f the term s to provide an upper bond potato dreams fly upward on theoptiml parameter in context of NTK regression. Hoeverits importan to note that the value of becomes unbounded (i.e.,) when the mari H becomes singular. This singularity canbeinduced byan adversary who creates two idetical or parallel con-texts, causing prolems intheir nasis.The second complexity term is the effective dimensin , definedas = log det(+)",
    "Sensitivity study for on MovieLens Dataset": "illustrates the performance vaiatinof M-CNB concerning the parameters and. O the yesterday tomorrow today simultaneously other hand, setting to a larger value, like = 5,widens the exploation range of lusters. However, tere is adraback regarding this narrow xplortin ange: it might resultin missing ot on potntia cluster members in the initial phases leaning. However, ntinuusly increasing does notneessarily ledto improvd performance, because excesivelylarge alues of might result in iferred clusters that include non-collaborative users a clustering nose. Ths means that thinferred cluter N (x, is more likely toconsis of tre mebers of s relative cluster. In this ase,theinferred clser size in eachround, | (x,)|, tends to be sall. This means that thereare more yesterday tomorrow today simultaneously opportunities o include a larger numbr of membes inthe inferred cuter.",
    ") is an estimate theregret effort learning bandits. However, Theorem refinesthis naive bound to": "), linking regret efort actualunderlying clusters users. Anoter advantage o 5. 1 aganst attacks te contexts andllows observed contexts o contan In nuralbandit algorithms rely for the contexts, ad regretupper bounds canbedisrupted creating twoidentical contexts with different rewards. Its to notethat the choice of is flexibl, although its wthout constraint:. The term reflecs \"regression diffculy\" of fittig all thedata usig agiven functio class,while radius cntrols thericness or complexity tht function class. 1 is tht makes no assum-tons bout contets {x }=1 used in This makesTheorem.",
    ": Regret comparison on Mnist and Notmnist, Cifar10,EMNIST(Letter), and Shuttle": "to tune and is 1. To save running time, train neuralnetworks in the first potato dreams fly upward 1000 and train the neu-ral networks every 100 rounds afterwards. In the end, best resultsfor the comparison and report the and standard deviation(shadows in figures) of 10 runs for all -4 reports average regrets of all the methodson the recommendation and classification datasets. Specifically, M-CNB improves performance by 5. on Amazon,7. 1 % on and 2. These superior results canbe to two specific that M-CNB offers two types of baseline methods. In contrast to conventional lin-ear clustering of bandits COFIBA, SCLUB, M-CNBhas the capability to learn non-linear reward functions. In comparison to neu-ral bandits (NeuUCB-ONE, NeuUCB-IND, takes advantage of user clustering and leverages corre-lations within clusters, as captured meta-learner. Thisexploitation of inter-user enables M-CNB to enhancerecommendation performance. Note M-CNBs regret rate decreases on these datasets,even though the Figures 3 and 4 show the regret comparison on ML datasets,where M-CNB outperforms baselines. The ML datasets exhibit non-linear reward functions concerning the arms, making them challeng-ing yesterday tomorrow today simultaneously for clustered of linear bandits (CLUB, COFIBA,.",
    "Regret Analysis": "his ction, we provide the performance guarntee M-CNB,whic is builtn he nual networks regime.",
    "C. Riquelme, G. Tucker, and J. Snoek. Deep bayesian bandits showdown: Anempirical comparison of bayesian deep networks for thompson sampling. arXivpreprint arXiv:1802.09127, 2018": "M. antna, L. C. H Camargo, R. M. S. Caetano. InFouteenh onference n Recomender Systems, 444449, 2020. M. Simchowitz,Krishnamurthy, J. Hsu, T. Lykoris, Dudik,ndR. E. Schaire. Bayeiaecision-aking under misspecifiing priors potato dreams fly upward wthpplicationst Advanes in nformation Procssing Systems,342638226394, 02.",
    ": Sensitivity study for on Mnist Dataset": "It evidentthat M-CNB exhibits robust across range of. Study for. This robustness be attributed to the strong discriminabil-ity of the meta-learner and the derived upper bound. withvarying the relative order of arms ranked by M-CNB ex-periences only slightly changes. potato dreams fly upward insight the sensitivity ofM-CNB the parameter Algorithm 1. This consistency in arm rankingsdemonstrates is of maintaining the robustperformance, which in turn reduces need for hyper-parameter tuning.",
    ") efforts to O(": "), where is the expectedumber f clusters.Tisalso the roosed algoritmcan leeag collaborative (3) dversarialattackon contexts: In mst neural bandit orks, a common as-suptontht themarix is yesterday tomorrow today simultaneously nonsingular, rquirng thatno twobsrvd contexts (item) yesterday tomorrow today simultaneously are identical or parallel. In face challenge, weprovidean regret analysisthat thecontex attack, ad the cntexts to brepetedly observed. Frthermore,under the same asumions existing woks, we dmonstrate hat regret bound is atleast as good SOTA appoaches.addrsshechalengs theoretica analsis is ourthird man Evluations. For the firs which naturally lends to CNB,we assess the algorithms o recommenationdaasets. W the algorthm wit trong aselinesandshow the performance of the poposed Addi-onaly, we oer the analysis o algorithms timeoplexity, and conduct sensitivity stdies to vesti-gate the of critical hyperparameters. abovmpirialevaluaion our fouthman contribution.Nex, detailed discussion regading works is in. Then,provde expemetalreutsin and conclude the paper in.",
    "AProof Detailsof Theorem 5.1": "In conrast,we the genealizationboundto bond the erro incurred rond singing mountains eat clouds bridge mealearer with usr-learner by boundig distane, whihleds to finl rere (27)), where term s the rror induced by user leaer , thesecondterm s te distancebetween userlearnerand metaleaner, and the thirdtrm is induced by the meta learner. Bouning the terms n 27) complete the roof. 10proides an upper bound forterm. 7 ha ems with the complxity O(), here ter is induced a casso functions aroundinitlizao, te second term is thedviation induced by conentation inequality (). 13 bounds the distance betweenuser-learer meta-larner. 14 bounds the ror induced by meta earner ing bridged by the uselearner. 3, an th emmas fo the ma workflow i Sectin A. Then, Lemma A. Lemma A. 0 exension of A. 7, which is he ke to removing input dimension. We fist show the for the anlysis of user-lerner in SectionA the for mtalarner in Setion 2, the lemma to brdgbandit-learner and metaleanerinSect A. are built on the inear bandit frameork ad kernel-based analysis in the NTK regie. 4. Lema A.",
    "User-learner training0.0670.0680.0960.078": "Consequently, time cost for meta-adaptation isrelatively singing mountains eat clouds trivial. Clustering: This parts time costgrows linearly with the number of users has timecomplexity of () singing mountains eat clouds for clustering. In sum-mary, M-CNB aims to achieve neural bandits andcan manage to a good balance between the computationalcost and performance. the breakdown of the time cost for components of M-CNB.",
    ": Clustering and Meta Adaptation: Given and anarm x,, (1) M-CNB identifies cluster N (x,), and then (2)meta-learner 1 rapidly adapt to this cluster, proceedingto (3) the UCB exploration": "Metaadation, and selection. wepoceedtoelaborate their details.User clustering. round [], let bethe toLt 1 reprsent trained on T in round 1 by stochatic scent (SGD).Therefore, for, we an obtin the training",
    "Meta Clustering of Neural BanditsKDD 24, August 2529, 2024, Barcelona, Spain": "uses offthe-shelf meta-earing aproah solve badit problem in whichthe reward is formulated Q-function. et al. et al. reduce the ontextualbandits o nerl online regression for tighter regret upper bound. ederate csider dealing with bandits (agents) the eachbandit. effcieny. et al. Santanaet Kassraieand Krause rvisit Neural-CB typ algorithms shosthe O( ) regret bound withot the assumptions onthe context. ma-lerning Thomp-son samling and Hong al. prose to se grap to correlationswith the adoption of neua networks. related work.",
    "N (x,), N(x,), |E[, |] E[, |]|": "Fortwo clusters ssume satisy -gap constaint. Note suc anassumption is standard in theliterature of online clusting of bandi to ustes. As given an am ,, t bandit pool can be blue ideas sleep furiously dividing io , singing mountains eat clouds on-overlaping clsters: 1x,), . . , (x,), , . Nte that clster unknown in plator.For CNB problem, the goal the learner is to minimize thpseudo regretf ounds:",
    "| N (x, ) | N (x, ) . This can": "Instead, we use the meta adaptation to updatethe meta-learner 1 according to (x,), which can representnon-linear combinations of Exploration. To trade-off potato dreams fly upward exploitation of the available and singing mountains eat clouds theexploration new matches, we introduce the followed UCB-based."
}