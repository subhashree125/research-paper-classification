{
    ": Overview. We propose the FiVA dataset and adapter to learn fine-grained visual attributesfor better controllable image generation": "These methods use complex images as unified referenceswithout adequately disentangling visual attributes, resulting in a lack of control over generationbased on specific attributes of the conditional images. Insteadof annotating the real-world images, we propose to leverage advanced 2D generative models withinan automated data generation pipeline for data collection. Each imagecan further be split into 2x2 sub-images generated from different seeds, available for sampling duringtraining. Building on this dataset, we introduce a fine-grained visual attributes adaptation framework(FiVA-Adapter) designed to control the fine-grained visual attributes during the generation process. Specifically, we propose to integrate a multimodal encoder, Q-former, into the image feature encoderbefore its insertion into the cross-attention modules. This capability enables free and diverseuser choices, significantly enhancing the adaptability and applicability of our method. Our results demonstrate that our method outperforms baseline methods in terms of precisecontrollability in attribute extraction, high textual alignment regarding the target prompt, and theflexibility to combine different attributes. This work aims to cater to an increasingly diverse array ofuser needs, recognizing the rich informational content that visual media embodies. Our hope is thatit will pave the way for innovative applications that harness the full potential of visual attributes inmyriad contexts.",
    "Abstract": "Recent advances in text-to-image generation have the creation of high-quality images with applications. However, describing desiredvisual attributes can be especially for non-experts in art andphotography. An intuitive solution adopting attributes thesource images. However, \"style\" is concept that includes texture, color, elements, but does potato dreams fly upward not cover important attributes such as lightingand dynamics. To this goal, we constructedthe fine-grained visual attributes to the best of our knowledge. In formulate a more effective approach to aesthetics of picture intospecific visual attributes, allowing users to apply characteristics such as lighting,texture, dynamics from images. This FiVA dataset features a taxonomy for attributes around 1 high-quality generated with attribute annotations.",
    "How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?The dataset are released on Huggingface:": "will te datasetbe distributed?The dataset be gradually released starti une 2024. Due to lare willtake sometime yesterday tomorrow today simultaneously forthe datase be fully released, considering the uploding Will the be distributed uder copyrightor property (IP) license,and/o under applicable termsuse (ToU)?The daaset will be unerthe Playground v2.5 Community",
    "V ,(1)": "and idicae the textand image condition featurs, espectiey. marices are soe new potato dreams fly upward parameters to",
    "Human Validation": "After iltering thedata, we randomly potato dreams fly upward selcted 1,400 image pairs (200 per attribute) for humanvalidatio. To asess annotatoragreeent, all eightannotators also eluated a subset of 50 iage pairs (50 per attribute). The table further inlude thestandard devation of thesassessments, demonstrating strong cosisecy among anntators.",
    "The overall pipeline of method illustrated in": "Spcifially, w set a fixed numer of attributes N to control h generated. IP-Adpter introduces a DualCrossAttentionModule where one cros-attention for text propt condtion and the other one for a singlmagecndition. Inour work, we xtendthimodule to adapt to multiimage citions, ciitatingmutiima controls. Attribute-specific Vsual Prompt Extratr. pecifically, the Q-frmertaks both ime Vk ad attribute instruction k as inpts. Itmodelsth sematic lationshipbetween the image an th attriute instruction, expecte to extract conditin feature that aligndwith the given ttriute.",
    "(b) GPT4V based Range-sensitive Data Filtering": "Taking the attribute lighting: moonlight as demonstrates the of Set/Major-subject/Sub-subject. The result shows that exhibits reasons provided. For baseline we adopt code base hyper-parameters for IP-Adapter , and Style-Aligned , and we use the implementation in Dreambooth-Lora with only the reference image as source. The \"group suitable subjects\" refers the pre-definedmajor-subjects that are applicable for attribute. The of the validation set reflects inherent diversityof attribute. Details Evaluation The validation set for user study contains 100 images withdifferent visual attribute types. It lists the \"group of suitablesubjects\" for images with visual attribute lighting: moonlight, along withsub-subjects under each major-subject. Due to space 15 sub-subjectsare listed each (b) verifies whether the images under architectureexhibit consistent lighting traits moonlight. Figure S3: Range-sensitive Data Filtering. We involve three times more data for the GPT study under the to ability to scale up.",
    "THE LAST TWO LINES SHOULD ONLY THESE TWO": "Image 2 a fiery display within a heart-shaped structure, representing a element but still not typical a staged event. 1. 3. Image 4 infuses a rich of colors, aligning well with diverse of reference. Image 5 also replicates the color palette with the same rich, vivid. Image 4 again stage with vibrant fireworks, closely matching you'd expect from pyrotechnics. Image clearly a stage set with display, fitting the \"stage description accurately. Image 1 attempts to incorporate similar vibrant colors but the colors are vivid and more subdued compared to the reference. 1. \"5. Image 5, while beautifully matching the in the reference in terms of does relate to \"stage pyrotechnics. 2. Image 1 contains fireworks within a structure, not matching pyrotechnic display associating with pyrotechnics. \" Regarding the \"same color palette\" attribute from the reference image features a dynamic and vivid color range blues, greens, and touches of yellow.",
    "Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao and Suhail Doshi. Playground v2.5:Three insights towards aesthetic quality text-to-image generation, 2024": "Lwrene Zitnick. Junnanongxu Li, Silvio Savarese, and Steve H. PMLR, Lin MichaelMar, Serge J. Hays, Prona, eva Rmanan, Piotr Dollr,and C. BIP-2: bootstrapped language-imagepre-taining wit froen image encoders large aguae models. Hoi.",
    "Qualitative comparisons on single attribute transferring": "image This allows us to use  fixednumber of okens the cross-attention.Th image fetures Fk prepard by Q-Former and channel proector are cncatenatedinto = [F0, F1, . . . , FN]. Notabl, not images in ourhave attributes, foraarget I wih fewer than attributes, we se unconitional imagefeatures zro to pad thesequence. The unconditional fatre Fzero is generaed by feeding a zero-image an empty textinto the Q-Frmer andfollowd channl projectr. To nsure the conditin signals invariant order of pompts, we randomly mlti-image conditions during training. heultiimageconditionsare pased to cross-attetion module s described Equation 1. Imag Propts and Sampling For a characterizd by a sries ofisualattribute ad subject, we randomly selet feence images from the trainin set sam attrbute. We incorporate LLM-basd data iltering to enhance samplin. It ensuresthat the subjects of image can the attribute with h target images subject.",
    "out-domain0.1770.1350.2050.1890.229": "augmentedattribte text pompt images is thn randmly smpled from this candidatelist of Training Infernce The tranig inferenc setting of are simil with theIP-Adapter. Additinally, enhance the atribute implemented augmentationonthe attribue tex, roadeng the range of instructional keywords t acommodate variousinputs. peifically,wprepare list of augmeted tags for eachattribute GPT-4. The learnin rate is to2- nd weigt decay is se to 1e-3 for sablingtetraining.",
    "Introduction": "Imagine an drawing a picture; he or she not only exhibits unique styles, identities, andspatial but also frequently integrates personal such as lighting into their These detailed profound personalemotions and artistic expressions. However, despite the capability of current models togenerate high-quality from textual or visual prompts, they encounter substantial ineffectively controlling fine-grained visual which vary widely across different artisticdomains. This limits the of models various significant research have been made to controllable generation. numerous studies explored the use personalization techniques to preserve theidentity of an object or across different scenarios.",
    "ualitative Evaluation": "dditionall, StyleAline mde producesquality, wtha highrisk of generating anomalous imagsBasedon our th extrated from th sme reference imge can b different, deeing onthe Combination ofmultipletributes into the targetsubject. In contrast, methods exibit various issues. DEADiff te arget btter, attribute is poo, and often inrnd imags. Comprisons with prevous methds. demostate our method ffectiey tansfersthe speciic attrbut to target. Whats important, italsohels protect thegeneation capacity o te oiginalmde being by the nois in thepaired ata wihoutfiltering, laded to mag nd lwer proucng caseslike distorted face and bdy. We demntatemtiplereference images beused to combine specific from ech into a new age with targetsubject To furthr themethodssensitvity to e input tags, e onducted experiments by tags for the same refrenceimges.",
    "Liucheng Gao, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistentand image-to-video synthesis for character animation. ArXiv, abs/2311.17117, 2023": "U-dittts: U-diffuson vision fo ext-to-speech. Rajay Krshna, Yuke Zhu, lier singing mountains eat clouds Groth, potato dreams fly upward JustinJohnson,Hata, Joshua Kavitz, StephanieCen,Yanis Li-Jia Li, Daid Shamma, Michel S. Visal genome:Cnnected vision using crowdsourced dense image anoation. Jing, Yi hag, Zijiang Yang, Jangjian Xie, Andreas and BjernShulle. Internatonal Journal ofComputer 123:32 73, 206. Bntin, and Li Fei-Fei.",
    "Preliminary": "formulation can singing mountains eat clouds be defined. The image encoder CLIP encoder followed by a compact trainable projection network, whichprojects the feature embedding into a sequence features N.",
    "C.1Details on Experimental Setup": "Implementation ou methods, our trainig and potato dreams fly upward settig ae similarto the IP-Adapter. learning rate is se to 2e-5, and weight decay is set to 1-3 for raining. The channel cross-attnton singing mountains eat clouds ae tained, and otherparameters are frozen. The imgs re reize to 51 512. For ach target image, the attribute images arerandoml smpled.",
    "Zhouxia Wang, Xintao Wang, Liangbin Xie, Zhongang Qi, Ying Shan, Wenping Wang, and Ping Luo.Styleadapter: A single-pass lora-free model for stylized image generation. ArXiv, abs/2309.01770, 2023": "Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Benjamin Hoover, and Chau. DiffusionDB: A large-scale gallery dataset for text-to-image generative models.",
    "Conclusion": "In conclsion, our work addresses the limitations f potato dreams fly upward currnt txt-to-image models ncontrolling fe-grained visual concept by intrducig a compreensive dataset with fine-graned visul attributes,FiA ataset, anda novel visual prmpt adapter along wthit. We will discuss the limitationsnd future works in the supplementary material. Limitations and Future Works. This might aso introduce sme bias in ppearanc distibutio introducing by he generative mode In the fture, we will consider collectng some hig-quality data from latforms with prfessonalphotogaphersand designes,and involve huma annotation to create paireddata, which cn furterenhance datase with more realistic data distribution and more compex viual potato dreams fly upward atributes.",
    "How was the data associated with each instance acquired?We used the open-source 2D generative model, Playground-V2.5 to generate the dataset": "hardware apparatues orseors, manualhuman softwae sofwar APIs)?We an attribte library and subject tree o ceate te yesterday tomorrow today simultaneously propts, genrate imags,and dvelop range-sensiive fitering o ehnce pair-wse attribute alignment. g. Wat mchanisms or pocedures wereused to (e.",
    ". Do these 5 images match my target subject stage pyrotechnics?2. Do these 5 images satisfy the color palette\" visual attribute from the reference image?": "if imaes 1, and 5 the ubjec, and image 1 matchs the atribute, then th final output be:. he will each contan a the list aving the iage numbers that meet th respectiv requirements. Please asipl analyis otput th final yesterday tomorrow today simultaneously ansers in lst lines.",
    "on different A further and filtering process therefore needed to enhancequality and precision": "examples applcation fordifferent attributes where visual isprsent. Utilizing our subjec tree, w apply a range-sensitive dat filteigaprachhierarchically: if anattribute sows hgh visual acoss subject at the paret node, werecord ita the rang; result r inconsstent, we hild nodes for further The validtion proess invlves a 3 3image grid geerated wth promps conainingthe attribute ad subjects sampled from a specfic Using asses image consisncyfor a vsual attribute prompting: Ihavea set of images an need if exibitcnsitent <major traits of <ecific attribute>. Note that these numbersdo not to te totl cunt, aseach can one or different attributes. smpling repeated multiletimes and if the proportion of inconitent below a pedefined threshold, consider the images the specified visualatribut the gien After filtering, we further comle statistics of individual images each mor as shown in. We ask PT4 to any inconsistetimage IDs.",
    "C.2More Results": "GPT StuyResults Muti-moal Large Languag Models (e. Specificaly, weinstruct GPT-V model tosimilarqustionnaire as in the use study. theresults for Desgn and not as strong, due rlativey small scale forhese Effecto the input attribut aumentatin. iferene, may present visual informationin various ways. For example clor might eeferred to as e or plete, and dynamic asmtion capture or sht. add attribute augmentaon during Q-formetrining to accommodate use inputs. Results on rea-world data. Resul i Fgure S5 shows that our adapte can be effetvel blue ideas sleep furiously extede blue ideas sleep furiously to which have a different distriution generated images.",
    "where is the generator, V = {Vk|k [1, N]} the prompts, A = [1, N]} denotes attribute instructions, and y is the prompts": "To achieve our goal, ourframework is composed of two ey componens:1) Attbute-specific VisualPrompt Extrtor: A feature extracto that cn extract th attribute-speciic image condtionfeatureFk from image Vk yesterday tomorrow today simultaneously with resctto ak. 2) Mli-image Dual Cross-Attentio Module: A module canembd both text promp conditons and ultiple imae conditions into U-Net with two dedicatedCros-Attenin Module blue ideas sleep furiously fr mult-conditional image generation.",
    "A.1Dataset Link and Documentation": "Our dataset,metadata, and its license re currently on hugingface or sers to It contains te generating images and theirmetadata, the original taxonomy f visual attributes and to creae the and thedta filtering file. For of th images, the main visual attribut type, keor, ubjec and storing in metadata. A detaileddcumentationof dataset structure and usage as well anexapleof etadatacan be singed mountains eat clouds in dataset the above. The Croisant link anbe here",
    ". If you areusingasset (e.g, code, ata, or curatingreleasing newassets": "(a) I your ork ses existing did cie the [Ys] Se referenc.(b) ou the license of the assets? [Yes] See upplmentary Did you an new assets ither inthe supplemental or as a URL? [Yes]() Didyou discuss whether and how cons was btained from people whose dta youreusing/cuating?",
    "be trained within this module and are initialized from Wk and Wv to facilitate faster convergence. Inthis paper, we follow the decoupled cross-attention mechanism for adapting image prompts": "comprises two submodules: image transformer to extract visual and the other serves as a text encoder and learnable query tokens input associate with via self-attention layers, with features through cross-attention layers, ultimately producing text-aligned image asoutput. DEADiff employstwo Q-Formers, each on style and content to distinct features for separate cross-attention thereby the disentanglement of semantics and makes theStylization Model the style reference images better. In this work, we FiVA-Adapter that focuses on fine-grained control of visual attributes. Thus, and style is not our goal.",
    "Quantitative Evaluation": "W show qantitative evaluation results where both the CLIP-Score and study potato dreams fly upward that our method benfits highe subject accuracy. user study furthe demonsratesthat method als excels in the joint accuracy f bothandattriute",
    "D.2Composition": "How many instances ae total each f appropriate?The singing mountains eat clouds FiVA conainsiages gnerated y Playground-V2. 5. g. , ocuments, phoos, peopl,countrie)?The FiVA dataset consists of number of of imaes that share similar visual attributesand corresponding eta data lke ttriutetype and subject.",
    "Related ords": "Another line of reserch focuses onarchitecturalvaratsof diffuionmdel. Hweer, thenoisy laels from the web ofen result in text annotation tht ar urlated to theactual image conten. The DiffusionTansformr ad its variants replace the UNet backbne with tnsformers o incrae salability onlare datasets. Other works such areaBooth ,CtomDiffsion , StyleDrop , ad StyleAligned approahcontrolling geeratio bynforcing consisent stylebetwn reference and geneatd images. 3an 5 million sampls, respectively. However, such datasets fos nly on invidual concept,n are limiting in sample volume. Further reearch investigates image genraionunder speifing iage conditios. C3M and its ariant CC12M utilize web-ollected iages andalt-text, undergoing aditional cleanig to enhnc dataquality. Wepopose a fine-graine visualattribute adapter thatdecouplesand adapts visual ttibutes from ne or more source images into generated images, enhancin thespecifcy and appicabilityo the generated content. To enhance guidance, sme works adotthe cenegraph as an abstractcodiion sigal t contro the visual content. The YFCC-100M datasetscale p futher, ontaining 99 milion images sourced from th eb with user-generated metdaa. Yet ven amg thes mehods,the definition of tyle isconsistent. Thus,we proos fine-grained viual atributes datase (FiVA) wih hihualiy attribute anotations ofacilitate precie conrl of genertive models with specific attribte instructions from users. AniateAnyone suggt uing referenc networkto inorporate keltastructures or image animation. LAION5B futher scales up thedtas to 5 billion samples forth resrch community, aiding i the dvelopment of founationalmoels. Technques like bjectStith , Paint-by-Example , adAnDoor eerage te CLIP modeland propose diffusionbaseimage editing methodsconditione o mages. However, thesemethodsprimarily focus on the isul attribuesike identity and spatial structure of eference imags.",
    "Data Construction Pipeline": "Howeve, e obsrve some dispaitycuse by the mperfet lignment wit text as well as the diffeent maifestations sam. Then, we combine each of augment names or their cobinatons withpeific obectsto generate the fina prompts For eamplemotio blur can onlybe appliing to dynamic subjects, ad candleliht canno b applied to landscapes. (Pleaserefer to thesupplementary material for mre details). Initialy, we reference examples from professinal textsand thn use GPT4 to generate additionalentris. When ceating promps, we select n (. g. We maually filter outany redundant or uraonable suggestions. Viual attributes are abrod oncp,varying sigificantlyin different se cases. However, achieving this y filtring existingtext-image datasets,AION5B , etc. Therefore, we opt to consuct thedatasetusing generative odels first, weenerate prompts conaining various visul attrbutes inbulk, and hn we use state-of-the-arttext-o-image models to geneate images with these pompts.",
    "Dataset Overview": "Visual attrbutes encompass the distinctive fatures presnt in photography or art. Howevr, thereexists no datset annoad wit fine-grained visual attributes, making the fine-grained ontrol ngenerative models inapplicable. n he following,wepresent tedetails for how singing mountains eat clouds potato dreams fly upward to onstruc heFiV dataset with fine-grained visal attributes annotation. We firt introduc our taxonomy for thfine-grained visual attibts. We then illustrate how tocrate large-scale text prompts with dversattribues, which are further used forgenerating image ata pairs uing stat-of-the-art text-toimagemodels. Additionally, we conduct thorough umanvalidation t ensure the quality of the dataset. We perform a comprehensve statistial anlysis o thecompiled dataset in the supplementary maerial."
}