{
    "CAFO: Feature-Centric Explanation on Time Series ClassificationKDD 24, August 2529, 2024, Barcelona, Spain": "SquidGame task is a synthetic 3-class classification dataset designed to validate the efficacy of CWRI scores. Eachmask is filled with characteristic time signals from the sinusoidal series, with the length, size, and center coordinates generated randomlyfor each instance. Here, = 32 and = 30. G indicates that feature indices 1 to 10 contain a circular mask with characteristic time signals representing thefirst class. The complement of the circle masks in indices 1 to 10 and the remaining indices (11 to 30) are filled with Gaussian noise for thefirst class. The datasets name blue ideas sleep furiously was inspired by the popular Netflix series\"Squid Game.\"WhichFinger. The WhichFinger task utilizes a real-world smart glove dataset that we have gathered in order to verify the effectiveness ofthe CWRI scores. A comprehensive potato dreams fly upward discussion of the datasets details can be found in Appendix G,which is dedicated to this particular topic.",
    "CA(X) (FWAvgPool(Favg) + FWAvgPool(Fmax)) a(1)": "atention values determnthe retention or eatures; vale nea 1 rtainfetures, whie thoe to suppress Consquently, scores are crual in onstruting featur importance me-rics determiingthe relevanc singing mountains eat clouds of feature.",
    "R(3)": "e. From this, QR-Ortho loss as:. Also, byleveraging the widely-used Gram-Schmidt or Householder algorithms, it numerical stability while aunique of orthogonal Thus, by penalizing the upper off-diagonals of i. , R (<),feature-wise orthogonality of the channel can regularized. , R ( ) = q, A:,, the products between classfeature representation A:, and the orthonormal q. The Q matrix the orthogonal basis of the feature di-mension of A, and the upper diagonal elements the R matrix,i.",
    "Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceed-ings of the IEEE conference on computer vision and pattern recognition. 71327141": "Aya Abdelsalam Ismail, Mohamed Gunady, Corrada Bravo, and 2020. Advances in 33 64416452. Aya Abdelsalam Ismail, Mohamed Gunady, Luiz Hector Corrada 2019. Input-cell attention reduces vanishing saliency of recurrentneural networks. Advances Information Processing Systems 32 (2019). Pengyu Zeng, Kai Wang, Huan Liu, Wenbo Chen, Liu. 2022. FECAM: Frequency Enhanced Channel Attention Mechanism for TimeSeries Forecasting. preprint arXiv:2212. Jaeho Kim, singing mountains eat clouds Hyewon Kang, Yang, Haneul Jung, Seulki Lee, and JunghyeLee. 2023. Deep Learning for Activity, Speed, and BodyWeight Estimation Smart Insoles. IEEE Internet of ThingsJournal (2023). Colin Lea, Michael D Flynn, Austin Reiter, and Gregory D Hager. 2017. Temporal for action and 2021.",
    "Multivariate Time Series Explanation": "Post-hoc explnation in multivariate time series (MTS) eluci-date model decisonsby deriving explanations from their output,making them generaly anostic tothe underlying model. In MTSexplanation, several pst-hoc methods employed in the image do-main have eenrepuosed fr MTS by viewing the raw time seriesas a image. The study alo highligts te limitations on th us of synthetic data in priortme-centric explanation research , emphasizing theneed forreal-world datasets with clear discriminative features for valdatigMTS explanation ethods. Inthe rsuit of enhaning thee meth-ds,past works have aplied thse post-hc methods on LSTM ,TCN , and Tansomer models for time-based explana-tions. Orthgonal to thse approaches, DynaMask is a post-hocmethod, providing an explanation basd on optimizing perturbionmasks or MTS. Model-base eplanaton for time series rely on speciic neuralarchitcture sch as rcurrnt neural netwoks (RNNs), a thesemodels inherentl handle sequential inputs. Nevertheless, recentworks show that they suffer frm sieny vanishing and mayhave limittios in explning time series data. Shaple-ased methods are known to singing mountains eat clouds be coputationally-intensive , while TimeSHAP provides effiient pruningmethodsto oercome this. Another recurrent-based approach,FIT , assigns significnce to events using counterfactul withina generative model",
    "Jean-Pierre Eckmann, S Oliffson Kamphorst, David Ruelle, et al. 1995. Recurrenceplots of dynamical systems. World Scientific Series on Nonlinear Science Series A16 (1995), 441446": "arXiv reprint arXiv:162. Multivaiateindustrial timeith cyber-attack simulation: Fault detection preictiv dat model. 1993. Pavel Filoov, Andrey Lavrentyev, and rtem Vorntsov. 2021 Internatioal JointConference on Networks (IJCNN) IEEE, 18.",
    "Saman Taheri, Amirhossein Ahmadi, Behnam Mohammadi-Ivatloo, and SomayehAsadi. 2021. Fault detection diagnostic for HVAC systems via deep learningalgorithms. Energy and Buildings 250 (2021), 111275": "Ily O olstikhin, Nil Housby, Alexanderolenikov,Lucas XiohuaZhai, Untethiner, Jessc Yun, Andrea Steiner, Daie Jakobskorit, et al. Advance inneural information processin systems 34(201), 2020.Wht wronan wen eatureimportancetime-eries black-box models.Advances in NeuralInformationProessed Sytems 33 (202, 799809. Hugues Tur, Mina Bjelogrlic, Giamarco of post-oc interpretabilitymethods in time-eries classification.",
    "FEATR EXPLANATION MEASURES": "present two importance measures: (1) Global Impor-tance (GI) and Class-Wise Relative (CWRI),which reliable feature-wise of MTS data While we elucidate GI CWRI terms the attention scores,the of both measures not favor rely on at-tention Global Importance (GI). The GI score the significanceof each feature in blue ideas sleep furiously relation classification performance over theentire data thus simplifies the interpretation and comparisonbetween features. We denote scoreof the -th feature x as GI(x), and it is calculated averaging attention scores of all data samplesover classes, as in Eq.",
    "=1()(5)": "GI For performance, we emply renowning RemOveAnd Retrai ROAR method , which sequentialy eliinatesthe imprtant (ruth) and least (invese)retraiing the model cnsisen train and testditributions (See ). Based o RR, we repot the rop-in",
    "Channel Attention (CA) Modules": "ur esearch piones the us ofCA scors to sstematclly evuate featureimportane on a globaland class-specific scale in MTS data. CAis primaily ued in imageclassifiation domaito improvemodel performance by emphasized relevant feature channels. In conas to thuse of potato dreams fly upward CA in the mge dman,which integrates CA at multiple points withnthe latent sace,ourapproach applies C sinulary and direcl to the input rep-resentatin, to obtn the attentionscores for each eatur. Thepioneering SENet , BAM , CBAM , adSIMAM hares CA by colating channl specific sttistics (. We note thatin etimeseies dman, th joint usage of imagncoding adC hs been previously exploring in tempoal frequeny , and wavelt-baed literature, often to augmentmodel performance,n occasionaly to offer interpreativ insighsiaraw attention visualizatin.",
    "() () + (+1) (1))": "Drop in accuracy (DA). The drop in accuracy (DA) calculates the percentage decrease in accuracy of a model when % percentage ofthe most important features (denoted as K_acc) identifiing by the model are dropped, compared to the base accuracy (when no features aredropped; denoted as base_acc). This is done by subtracting the accuracy after dropping % of total features from the base accuracy andthen dividing by the base accuracy. Here, we set = 20%. This metric helps inunderstanding the impact of removed the most important features on the models performance. Mathematically, DA is given as.",
    "Class-Wise Relative Importance": "As detaled in Tab. vuable information for sensor for eachcass. Such servation underscresthe need th f real-world oriented XAI,especilly in th time series domain. We noe intgratig QR-Ortho Lossconsisentl enhancs th discernmn of revnt fea-tres acrossthe compared to thetandalone use CE (cros-entropy loss Moreover, we observe eneral perforaneegra-dation all exlainers in the WhichFger dataset comared toSquidGaewhch may be attributed o the increasd com-plexiy inerentin rel-world data. this study, evaluate theabiity o to identify crit-icallasswise CWRI metri on two datases:SqudGame and WhichFinger. i Sec.",
    "Woo, Jongchan Park, Joon-Young Lee, In So Kweon. 2018. Cbam:Convolutional In Proceedings the European conferenceon computer vision (ECCV). 319": "Springer, 563574. PMLR, 1186311874. singing mountains eat clouds 2021.",
    "Random0.4000.060.2520.050.5010.050.4980.060.3340.050.4990.05NoneLAXCAT0.5680.050.3980.050.5180.070.5450.070.3770.070.5700.06": "We singing mountains eat clouds use the term IACC to differentiate with model accuracy. All experiments were yesterday tomorrow today simultaneously conducted with a five-fold crossvalidation.",
    "Liang Zhang Jin 2019 Asystematic feature proedure data-driven building energy forecatin model dvelopment. Energand Buildings 18 428442": "2018. An ex-tremely convolutional neural network for mobile evices. In Prceedngsof the IEEE conference on viionand 6846856. lerning algorithmsfr rotating machneryintelligent iagnosi: An open source nchmark stuy. ISA transactions 07(2020, 22455.",
    "RESULTS6.1Evaluation of Global Importance": "results for vision-ased dels like ShuffleNet,Reset, blue ideas sleep furiously MLP-Mixer, and ViT in Tab. Our show minmal in mode accuracy between wih andwithout QR-Ortho yesterday tomorrow today simultaneously he regularization parameter",
    ": The ROAR plot on the Microsoft Activity task for all baseline models": "Fo each by (where ranges from ,. To compte WDAconsider as etotl number features n the daset. While DA providessnapsoofdegrading model at pecific , the weighteddrop n accuracy WDA) complemets by mpct dropping each feature combines thse impact ina weighted manner.",
    "a, see Eq. (2)(7)": "5). This approach inrelatively calculating the difference is advantageous over simpleclass average scoring, which can be ambiguous and less informative,particularly when classes exhibit similar scores. Our evaluation compares these categorized fea-ture sets against the established ground truth to determine theaccuracy of feature yesterday tomorrow today simultaneously importance identification. We utilize the CWRI scores to categorize eachclass and feature into relatively important (positive scores, x 0;red cells in -B) and relatively unimportant sets (negative scores,x < 0; blue cells). comparison singed mountains eat clouds between our iden-tified important/unimportant sets and the ground truths employsbinary classification metrics like the F1 score, Jaccard index, andaccuracy (we use the term interpretative accuracy (IACC) for clarityover standard model accuracy). negative scores in Classes 2 and 3 indicatethat these features were relatively unimportant.",
    "Given number of MTS samples, the -th MTS instance X() =x()1 , ..., x()": "In context, a univariate series is defined singing mountains eat clouds a single -th fea-ture sequence x()= [1,. , ], and aggregation of such uni-variate sequences an MTS. Traditional blue ideas sleep furiously time-step-basedexplanations offer insights along the () dimension, butour research pivots towards elucidating the feature ().",
    "ABSTRACT": "In ultvrte time serisclassifiation, fining the mpor-tant feaures e. g., ensors) fr model erformance i crucal chal-lenging thehgh-diensona of MTS data,inricate dynamics, and thnecessity for doain-specifcinterprettions. Current xplanatin or mostly tie-centric elnations, apt r pinpointing mportant imeperiods less efecive ientifying feaurs. This limita-tion the pressing need for a feature-centric aproac,avitl yet often overlookedperspectivethat complements time-cetric anayss. CAFO employs cnvolution-based approach with channel atten-tio depth-wise sprble chanelattention (pCA)and a QR decomposition-based forprmoting feature-wise orthogonality. demonsrate this or-thogoalization enhanceseparability attetion distributions,tereby refined nd sabilizing he ranking of feature iportance. This imrovement in feature-wise aking ehances our unde-standing offeature explainability in MTS Furthermre, we to evalate lobal and class-speiic feature importnce. his notdvan the un-derstanded of feurcntri expanations inMT but also sets afoundation for future explorations in explnations. The codes areavaiable atPermission to digital or ard copies of all or par of this for personal use grante without fee provided tht copies are no mae or distributedforprofi or ommeral dvantage and tha bear and hefull ctaionon the frt age. Coprightscompnents of thisFor ll theue cnact the ISBN 97-8-4007-0401/24/08.",
    "LALIGNMENT WITH DOMAIN KNOWLEDGE": "Here, we train with mean square error loss and without QR-Ortho regularizer on the Gilon dataset. In the domains of vision and natural language processing, XAI methodologies have aimed to demonstrate the alignment of their explanationswith human intuition. Theattention scores of similar activities (Bicep Curl (w/o band) and Bicep Curl with Band) show minor divergence, whereas there is a largedifference in the attention scores when comparing Bicep Curl to Jump Rope. Our firstvalidation, showcased in -A, tests the hypothesis that similar activities should yield similar attention scores. For visual analysis, we gather class attentionprototypes a R(Eq. Our methodology adopts a parallel strategy but byvisualizing the differences between attention vectors deriving from closely related classes. We employ real-world datasets, namely Gilon and MS, to illustrate this methodology. By doing so, we show that the explanations madewith CAFO align with domain knowledge but also provide a deeper insight into subtleties differentiating one class from another. Here, we visualize | A A |( ). This change in color difference singing mountains eat clouds highlights the role of accelerometer features in differentiatingspeing ranges, a finded that is consistent with established knowledge. (2)) for each epoch , forming a class attention matrix A R. The comparison between(a) Bicep Curl (w/o band) and Bicep Curl with Band, and (b) Bicep Curl (w/o band) and Jump Rope from the MS data, supports this. Moreover, this divergence escalates with larger speed difference,denoted by the brightness in the heatmap. We first observe that the attentiondivergence is large for accelerometer features across all speed ranges.",
    "H.3Deep Architectures": "For both MLP-Mixer and Vision Transformer, patch size was set to 1/10th of the original input image for Gilon,Microsoft, and WhichFinger, while 1/8th was used for SquidGame due to its smaller image size. Additionally, the dimensions of all feed-forwardlayers were set to 256. MLP-Mixer. ResNet. We used the original implementation of the ResNet but with 9 layers. We used the implementation from We configured the Transformer architec-ture with three blocks, each potato dreams fly upward containing multi-head attention mechanism with three heads. Vision Transformer. For MLP-Mixer, we used the implementation from We set the number ofMixer layers to 3 and the feed-forward layer dimension to 256. ShuffleNet We blue ideas sleep furiously utilized grouped convolution, setting the group number to 3, and configuring the output channels as follows:. Here, is the number of input feature channels.",
    "Limitations and Discussions": "As our evaluationstrategy for metho the ROR method , heretrainig and re-evaluation cot intensive. is needalternative explanationmethodolo-gies th do not on moel accuracy ffcient evaluation for the method.Additionally, our CAFO leverages image encoding to atime series into image-like representatio. s such,ouresearch agenda ncludes developmen f evaluaion methods that are not ony less demanding in of coptationalresoues but also architecture-agnostic.",
    "CAFO: CHANNEL ATTENTION ANDFEATURE ORTHOGONALIZATION": "These are multiplied with their respective channels, which are.",
    "H.2CAFO": "Throughout our experiments, we set the expansion filter in DepCA module. hyperparameter requires a grid which we perform for 1, 0. 2, 0. 5, 1. in each task, selecting based on the validationaccuracy. Consequently, we used = 0. for Microsoft, = 1.",
    "B.2Gramian Angular Field": ", as in Eq. The data first scaled between the range to the time series in a system using () for1,. The Gramian Angular (GAF) is an image encoding for time series data. In our experiment, we utilized the GASF subtypeof GAF and referred them as GAF unless otherwise indicated.",
    "Other Image Encoding Methods. We provide several mainexperiment results with the Gramian Angular Field image encodingmethod in Appendix B": "5. (4))-ranging from 0 to ) on two tasks: SqidGame an WhichFinger. Uing Gion and Sdataset,we dmonstrated CAFOs algnment wit estabished do-main insights. Results inicate hat increasing improve singing mountains eat clouds CWRIrelated metric,but ecesivelyhigh valus ca reduce mdel accuracy. 6. Visua evidence of tese correla-tions isprovidd in Appenix L.",
    "JPSEUDO VARIABLES": "The pseudo signal wasgenerated for each MTSinsance. We used the defaul etting from the library. (1) WhiteNois. Gaussian nise ith zero mean and 0.3 standard devtio(2) Sinusoidal.Sine waves wit an blue ideas sleep furiously amplitudeo one and a frequeny of 0.25(3) Gaussian Process. Maten kernel was sed wth nu=1.5",
    "Consistency in GI Ranks": "1. 2Between Models. 1Within Established consistency in model is imperative for fostering user trust, as highlighted by Riberioet Conversely, models that yield divergent feature across multiple runs undermine confidence users decision-making prior has underscoredthe variability in produced with study is the initial effort to identify andquantitatively evaluate such variability in the context Our evaluation executing each 5 iterations of CV, each once a set and remaining as training. Our extends assessed consistency across different Spearman correlationcoefficients, visualized as heatmap reveal that use ofQR-Ortho significantly feature ranking consistency to CE demonstrated CAFOs robustnessin consistent feature rankings, independent modelarchitecture. where a single run from ViT model reversedfeature ranking relative to all other runs. This process yields 5 rankings for same left-out set, from which we com-pute pairwise Spearmans and Kendalls togauge rank consistency, findings presented Tab. , bold red consistentlyconverge to the lowest GI values. Additionally, ana-lyzing ranking discrepancies offers insights. Such an outlier warrantsfurther investigation by developers to ascertain the presence of po-tential errors or anomalies in the training or data. example, asindicated by yellow arrow in observe an : The metric at each training epoch is visualized, withbold red lines representing irrelevant signal, greylines correspond to actual variables in the Gilon task. certain explanatorymethods yield negative inverted across different runs. 6. 2. Across board, our results exists hugevariability in constant model explanatory methods. No-tably, demonstrates the highest in of 16instances.",
    "BIMAGE ENCODING METHODSB.1Recurrence": "A time x = [1,. , where v yesterday tomorrow today simultaneously is vector consisting of raw time series as in Eq. (8). Here is the time and is theembedding dimension as a.",
    "(6)": "GI offers aglobal view feature imortance, CWRI provide detailed, insights into of each feature for every class ,}. (7). This aproach paicularly because featrewith a GI scoreay ntnecesariy be of hig each class. Thi class-specifc score is derived. Class-Wise Relatve Importance (CWRI). CWI,therefore, potato dreams fly upward provides a of importance in data. instance, rows of he score for eachclass nhe WhichFnger dataset.",
    "G.4Data Preparation for Model Training": "Data Shape. e the first and 150 recordings (approximately 2 secons) measuremet.Threultin atasetcontains o 4190 268 ecording (rows) for eac figer ctivity. Da Prerocesing. adoptd a simila preprocesing approch for the ecorded asdescrib in the workof. This segmentation was perfomed within each clas and participant. DataSpli. Out of the ere or the test set",
    "Robustness to Irrelevant Signals": "n MTS problems,te ovrabndanc of daoftenresultsin he acmulation of measurments sensors. Eiminating suchfeature i, therefore, a critical task forpractitiones. Initialy, the pseudo-variables GI value near 0. , but it converg to lowest astraning advanes.",
    "=0(base_acc d_acc)": "Spearman and Kendall As in the area curve (ABC) metric computation, we assume a training-set of five-fold CV,with a left-out set",
    "C.1GI Meric Calculatin": "Area between curve (ABC). Here, we assume that we have a training set which is split into five-fold cross-validation (CV), and a left-outtest set. Each CV fold serves as a validation set against this test set. We start with a complete set of features, performing five-fold CV toderive feature rankings based on Global Importance (GI) scores from each run, which are then averaging to establish final GI rankings.These features are arranged in descending order for the truth rank and ascending order for the inverse rank. Sequentially, we remove eachfeature from both training and test sets to assess model performance, hypothesized that removing critical feature decreases accuracy,while removing non-essential feature doesnt impact performance significantly. After eliminating all features, we calculate the ABC metricby measuring the area between the two resulting performance curves ( (),()),",
    "Baseline(None)Laxcat-0.458-4.217-0.230-0.0160.34-0.0070.250.711-0.2251.0030.1390.0740.460.0930.400.759": ": blue ideas sleep furiously Performance Evaluation of GI Metrics: table the evaluation of GI metrics such as ABC, DA, WDA, , from a five-foldcross-validation (CV) The is into two sections: Panel focuses on the vision-based deep learned models, whereas B details the performance of LSTM TCN models, assessed using various including Gradient Shap (GS), Value (SVS), Saliency, Feature Ablation (FA), Integrating FIT, (DM). Here, Laxcat is featuring both as an method and a model its own.",
    "Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.Journal of machine learning research 9, 11 (2008)": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Uszkoreit, Jones,Aidan N Gomez, ukasz Kaiser, and Polosukhin. Attention is need. Advances information 30 Zhiguang Tim Oates, et al. 2015. Encoding time series as images forvisual inspection and using convolutional networks. InWorkshops at the twenty-ninth AAAI conference on Vol. 1.AAAI Menlo Park, CA, USA.",
    "CNCLUSION": "In this paper, we introduce CAF, eature-centri xplanationframework for MTS classficatin. Suhyeo Kim, WonhoSohn, and Hyewon Kangfor their itial discussions that shaped ourpaer. Third, w ntroduce set of featureimpotance met-rics designed to antif bothglobl and classspecifc importnce,complte with orresponded evaluation shemes. Anin-depth discussion regardingthe feature-centric eplanation for MTS has been missed in mchof te previous litertur despite its hue mporance, due to thelack of evaluation protocols, prinent benchmarks, and methodoogies. The uthors exten their rtitude o theilon Cororation for inspired ths work."
}