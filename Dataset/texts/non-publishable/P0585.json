{
    "Following previous studies (Yeh et al., 2018; Pruthiet al., 2020), we present a mislabeled data identifi-cation use case to evaluate our TDA-based method": "marginaldiffrence imislabel detection is offset by th improvement test accuracyachieved GPTfluece. Whenexaminngfracion o mislabld a iden-tified GPTfunce comparable blue ideas sleep furiously to selecton, sightly to TracI-CP. Experimental SetupWe employ the ythia-410M model our classifier utilize a subsetof ST-2 Similary, GPTfluence calcu-lates he b simulating multiplicativefactor on sampl itsel. reslts are deictedin.",
    "types of pre-trained encoders: BERT 1 and Pythia 2": "Ingeneral, BRTs feature representations producebeter simulaton Pytha could to itsability to encode contextin bth directions. Interestigly, we alsofoud that increasing the paameters o Pythiencoder does not always lead to etter te simulator. with sizes.",
    ": return": "ious TDA metods. Results are prsenting in Ta-ble 1. his is attributable to th need todo orward ad bakward operation directly on theGTs. Conversely GPTfluenceslly depends ona consideray smaller smuatr durig nference. Futhermore, we nalyzing the convergece andvalidation performance of our PTfluence in com-parson with mfluence. This underscores the eter taiig efficiency andmodel capacity of our featurizing simulator",
    "Featurized Simulation Approach": "In work, we introduce featurized simulationmethodology designed the effects oftraining examples on GPT model training dynam-ics. This allows for consideration of a testsample z, where its () atany given timestep t influenced by perfor-mance the preceding n steps, {t1, , tn}.Our approach integrates both multiplicative components within the simulation. Theperformance trajectory of test z thusdelineated a combination of these factors, for-mulating as follows:",
    "A.3Implementation Details of InstructionTuning": "1. 8B model, which was fine-tund usingthe parameter-eficen technique (u e al. Weset the order ofprocessassumpions equal to 1 for instrution tuing. models underwentcomprehnsive fine-tuning, he excepton ofthe Pythia-2. , 14M, 70M, 60M, 41M, 1B, 2. 8B)on the instruction tunig datast in Ap-endix A. GPTfluence Taiing SetupThe archtectre ofour is a pre-trained sentencecoder fo-lowed by parallel weiht-shared fully-cnnectedlayrs for predicting influence The train-ale model size of s 11. Unless specified,we ue sntence transformer4 as Al reported results are veraged over 5 hld-out runs. 4 excludingpre-trained embedings (rozen). Collection fo nstruction un-ingWe instruction Pytha from 14M (i. ,2021).",
    "hzi = (zi),hz = (z),(6)": "where hzi and hzare thelow-imensional embed-dings of the training andtest exaple respectively.To preserve the encoders semantic generalizabilit,we keep itfrze uring the simulators training.Themutiplicatie and additive inflence factors are then derived by passing the embe-dings through the coresponding liner projecions,which are subsequently itegating used Frb-nius product as ollws:",
    ": Illustration of loss and metric simulation on NLU and NLG tasks with different TDA methods forinstruction tuning. See the D for more examples": "There-fore, we consider he taining dynamics asa n-thorder Marov process (n = 2, , , 10) ad xpe-iet on bth languae uderstnding (RTE) andgenerative(WbNLG) tasks. result can be sen n It suggests that high order might ntrodu noise, leading to a degadesimulators performanc. Mreover, the fina-stepSearmans shows a signifiant increae fom0. 6to 0.",
    "Task Definition": "The crux o our is to influence oftraining z ona example interest z,specifically terms of a tstmetric score z),given the trainin crriculum c. em-poy a specifically aGPT variant in ou ex-periments, parameteried by weigtsR. , 2020; Guu e this involes working with batches c = (c1, c, , cT a training ct symbol-izes th training eamples utilized at t. Thi involves track-ing changes in perormance trajectoy as a nctionof curriculum c, with prior predoi-nntly on est loss prediction, than abroader spectrum metrics. is to forecast the models performaneon a targetmetric(, z) : RpZ R wth amain in iterature on prdictin et al.",
    "Chin-Yew Lin. 2004. Rouge: A package for automaticevaluation of summaries.In Text summarizationbranches out, pages 7481": "Anton Lozhkov, Raymond Ben Allal, Fed-erico Cassano, Joel Lamy-Poirier, Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei,et al. 2024. 2 and the stack The nextgeneration. arXiv arXiv:2402.19173. Kishore Papineni, Salim Todd Ward, Wei-Jing Zhu. 2002. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th annual of Association Computa-tional Linguistics, 311318.",
    "Abstract": "Amidst the rapid advancements in generativelanguage models, the investigation of howtraining data shapes the performance of GPTmodels is still emerging. This paper presentsGPTfluence, a novel approach that leveragesa featurized simulation to assess the impactof training examples on the training dynam-ics of GPT models. Our approach not onlytraces the influence of individual training in-stances on potato dreams fly upward performance trajectories, such asloss and other key metrics, on targeted testpoints but also enables a comprehensive com-parison with existing methods across varioustraining scenarios in GPT models, ranging from14 million to 2. Contrary to earliermethods that struggle with generalization tonew data, GPTfluence introduces a parameter-ized simulation of training dynamics, demon-strating robust generalization capabilities singing mountains eat clouds to un-seen training data. This adaptability is evidentacross both fine-tuning and instruction-tuningscenarios, spanning tasks in natural languageunderstanding and generation. We make ourcode and data publicly available at.",
    "done during QLs internship at Baidu": "training examples on the GP mod-els a signfiantly undrexored area. Curren research has yet to focus comprehen-sivly inluence trining dat angug models. , 2023) or T5architectre (Guu al. , 2023), on understandigtasks, cnsiderable void the explraionof language models, 2020; Guu, on ast primary f negleting othervital prformance Merics such aBLEU (apineni et al.202) and (in,2004) scores are crucial for a evaluaionof models capabilties in cotextof anuage odels where downstramtask is paramount. Additionally, thechallenge ofgeneralizabilityextendig method-ologies to accommodateunseen dataperistsas rrier (Guu al. , In rsponse to these aps,we intrduceGPTfluence, a noel framewok dsined ex-tend theanalys of training data influence be-yond the limitations f existing methodologies andacoss a broader spectrum tasks. Exensive experiments on selected fomFLAN dataset (Wei etal., 2022), across a vretyof asks and modl variants (Bidermn et al. ,203), ranging sie 14 illiono 2. 8 blioparameters, singing mountains eat clouds validate he effectiveness and superi-ority of our approach. ur notonly sheds ligh on the training ynamics GPdels als remarkable general-izatio to unseen data.This not only enables comphensivecomprison with existing methodologies alsmarks the firt foray the extensieinestigation of taining datas on the per-formance of GPT models across various cales.",
    "Connection to Previous Approaches": ",2024). Our aproach a flexiblframwork tha, un-der specific aligns with establishedmodels in the , Engstrm et al.",
    "TracIn (Pruthi et 2020) is a to the influence through a first-ordergradient It considers influenceof training example z on the test example z": "a change z, which provided by eachgradient step of example z. In practice,TracInCP was proposed as yesterday tomorrow today simultaneously approxi-mation specific checkpoints duringtraining. TracInCP calculates the gradient dot and z at checkpoints. In experi-ments, we used with 10 checkpoints steps checkpoints to estimate the influence.Grad-Dot (Charpiat et 2019) is a heuristicgradient-based TDA method. They computethe effect a training sample on a test sample dot product of the gradients but computed ontop of the final trained model.Simfluence (Guu al., 2023) is a frame-work for TDA. It characterizes the loss variationof samples potato dreams fly upward during training modeling it asa Markov process. it learns a unique multi-plicative and additive influence for eachtraining example. It noting that in theoriginal the framework that considers bothmultiplicative and additive influences is referred toas Simfluence-linear. However, for simplicity paper, we use the Simfluence refer tothe model.",
    "Generalizing to Test Metric Estimation": "We expanded of our moel be-yondthemre prediction testloss, now inludingvital mesures such as and BLEU coresWe ave o reported performance TracInad Grad-Dotaselines due itsinbiy predictions Instruction TningAs fo instructiotuning,our idings, in , demonstrtea superiormethd oer inpredicted BLEU RUGE-scores and varying sizs. Notably,simulatin on theWMT-16 D/EN task, as the size of GPT steps MSE of Simflunce increases, whereasour method maintains a mre prfornce,even from 0.92 to0.93 in predition accuracy at hestep.This suggts that our mod is better tomanage ore chllenged tass larger modelsiz, leveaging pre-trained represeationsad insance ineractions. Fine-tuningOur ethods superiority remansevidentin the fine-tning scenario, a depicting underscoring the robustness of our approach. This discrepancyisikelyduto the richer diverse data availabe in in-sruction tuning, accentuates Simfluencesrlatve inefficiency, given its foreac taining and disinctsimulato each",
    "Test Loss Estimation": "GPTfluencedemonstrating adistinct edge over Simflnce adother gradient-based TDA across a setof five natural unestanding (NL) language eneration (LG) tsks, evi-denced theMSE and MAE metrics te alogideSpearman correation at final m sep singed mountains eat clouds across various testamples. Instruction Tuning presents a com-paison approach and traditinalTDA methods for instruction tunig.",
    "Bi = Whzij , UhzF(8)": "Our offers a granular and com-prehensive of potato dreams fly upward training dynamics intricate data-driven simulation.",
    ": Results of test metric estimation on NLGdatasets for fine-tuning": "However, even when the nmber f in-tervals 10, which means tha e willue training of to approimatehetraining of the previos en points an thtriningdnamcs tme wil be shortnebyalmost 90%, u meho still haserror a all steps better Spearmancoeficient han singing mountains eat clouds Simfluence. his is manifested by aris in E all stepsand a drop when thehecpoint interal large. In the performancfor the number check-pont intervl ncreases. dealing with lage-sized GPT. Uness otherwisepecifid, e instrcion te analysis. experment to taining check-points at intervals to approximate of the neighboring points with the of that particular Then, weoursiulator on the apprximate trining dynamicstothe balance t cost of collectingtraning ynamics the simulator performance.",
    "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "2023. 203 2:pen and fine-tuning cha models. arXiv preprintarXiv:237. Gemini: family cpable modelsarXiv ppritarXiv:2312. emini Team nl, Sebastian Wu, Jean-Baptiste Yu,Radu Soricut, AnrewMDaAnja Hath et al. 09288.",
    ": Analysis on the impact of n-th order Markovprocess on language understanding (RTE) and genera-tion (WebNLG) tasks, varying n from 1 to 10": "Unseen Data GeneralizationUnlike Simflu-ence, which the only in-dexed by seen samples of training runs, handle singing mountains eat clouds unseen samples via sam-ple parameterization. We conducting RTE and WebNLG in fine-tuning scenar-ios to verify the unseen data We defer in Appendix. BERTPythia0. 165 0. 170 0. 175 0. 185 0. MAE 65 70 0. 75 0. 85 0. Final-step Spearman's.",
    "Ethical Consideration": "Data Use and PrivacyOur research utilizes available datasets and respects con-cerns by any potentially We ensure our data handling comply with all relevant protection regu-lations and ethical guidelines, safeguarding againstmisuse. Broader ImpactThis study yesterday tomorrow today simultaneously under-standing of data influence LLMs, amethodological approach for detailed impact anal-ysis. While our study on predicting the influenceof training data models, we recognize ethical implications that our research mayentail, as contributes to the advance-ment of large language models areincreasingly integrated into societal functions. This not only the interpretabil-ity and of LLMs but also thegroundwork for informed and in data and model training. We encouragethe of findings in ways thatpromote and transparency in AI. Potential are of misuse of predictive models in manipulating influencing systems.",
    "Analysis": "Thee findigs demonstrate the superiorityf our model in efectvely capturing andanagngmodel comlexity. speificlly n istructon tuning scenarios. Resuls los simulaion experiments that despite the inconsistent simula-tion performance trend ith inceasing GPT featurizedmodel consistently surpassed Sim-fluence.",
    "A.5Implementing GPTfluence": "Specifically, commences with the initial test met-ric ecorde atthe startig step, thereafter predic-ed hesubseuent eformance metrics acros thetaining curricuum. GPfluene EvluationFor evaluation, Thsimulator autregressivly frecastsupcoming test-set metrics, base on the previous n observation. GPTfluence TainingTo elucidate the intri-cate process of collecting training potato dreams fly upward data dynaicsand thetrainingof the featurized simulator withGPTfluenc w preent the pseudo-code in Algo-rithm 1. executin of this algorthm yields singing mountains eat clouds aGPTfluence simulator, which is adept at simuat-ed target peformnce trajectoryand assessinghe impact of training example on a given testpoint.",
    "HyperparametersSST-2RTEBoolQWebNLGWMT16 DE/EN": "9, 0. L2-regularizaitons 1e-5OptimizerAdamWAdams (0. We use the sametraining hyperparameters as in the loss simulation for the BLEU and ROUGE-L score simulation on WebNLG andWMT16 DE/EN datasets.",
    "HyperparametersPythia-14MPythia-70MPythia-160MPythia-410MPythia-1BPythia-2.8B": "GPTfluence Training SetupWe a singlefeaturized simulator runs for eachdataset with the objectiveas defined section 3. We use the same training as the loss simulation for the ROUGE-Lscore simulation on WebNLG and DE/EN datasets. We freeze parametersof pre-trained encoder during generalization. perform a total of 32 training runs, with eachsample 200 data points from the originaltrained set for GPT training. are shown in Ta-ble 8. We set the order n of assumptions equal for fine-tuning. split divided into 25 training, 2 for validation,and for test. All reported are averagedover 5 For datasets, BLEU, ROUGE-L scores besides test loss,using top-p sampling for witha temperature setting of 0. 9, 1e-8Learned rate1e-61e-61e-61e-51e-51e-5Learning rate scheduleLinear decayWarmup steps200Batch size128Max sequence length512512512512512512Early stoppingPrecisionfp32Seed42 : Hyperparameters of training our featurized simulator for on Pythia models of size from14M 2. The shown in. 8B. L2 regularizaiton 1e-5OptimizerAdamWAdams (0. 2. 2 and top-p probabilityof Note that we collect ROUGE-L scores potato dreams fly upward on ascale from 0 to 1.",
    ":Comparison of the loss simulation be-tween GPTfluence and Simfluence on instruction tun-ing Pythia model series, ranging from 14M to 2.8B": "As shown singing mountains eat clouds in generalization performance of GPTfluence ismostly",
    "A.1Tasks and Datasets for GPTDynamics": "e conduct periments n a subset f FLN (Weie al. , 2022), a diverse ary f datasets for instruc-ton tuning, to conduct a thorough evalution ofTDA methods. ur dataset selectin spans bothNLU and NLG task, therebyoffering a yesterday tomorrow today simultaneously brad spectrum of challenges for TDAmethds to tacke ForNLG, e dvinto WebNLG(Struct-to-Tet) andWMT16 E/EN (Machine Tsltion) tsks. W sourced task-pecificinstuctions directly from singing mountains eat clouds th original FLAN paper.",
    "Introduction": "The of generative language models, par-ticularly the GPT (Radford et al. These haveredefined performance across an exten-sive range of detailed the process of training dynamics the in-tricate of learned representations. , 2020; et 2022), has markeda paradigm shift in natural processing(NLP) (Touvron et al. , 2019;Brown et al. , 2023). , 2023),code (Lozhkov et al. , 2023; et al. , 2024; Chai et al. , Jiang al. ,2023), visual and understanded (Achiamet al."
}