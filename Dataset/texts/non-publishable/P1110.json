{
    "fallback()0.54s1.14s114%answer_question()0.89s1.66s87%search()0.92s0.94s2%replace()-2.38s-": "For tep an potato dreams fly upward in the replacemetmapping, prompt LM to perform",
    "Task-specific qestion nswering": "Previous work shows generative models performing long-formQA tend to add additional information or hallucinate answers. This is potentially dangerous in a setting can un-dermine agents perceiving trustworthiness limited userinteraction. Khashabi et al. Shuster et al. combine this approachwith retrieving from dialogue Their models hallucinate terms andunits and show low performance, showcasing the challenged Since models like only a context length, weimplement of context to shorten to the availabletoken length. To ensure knowledgegrounding of target inspired by Khashabi et al. et al.",
    "ABSTRACT": "For knowledgegrounding quetioanweringand live yesterday tomorrow today simultaneously as adaptations, we show hat M reasoningabilitie overask context andworl knowledge outwigh latencycncerns. e tackle challenge of bilding rel-world mulimoal assis-tans or complex real-world tass. Oveall,we providein-sights and discuss tradoffsfordeploying both traditional modlsand LLMs to users in coplex real-world multimodalevironmentsin th Alex TaskBot challenge. OAT allowsus t define when, yesterday tomorrow today simultaneously howand whic LLMs holdbe used in astruc-tued and deployble anner. For dialogue sate managemen, we implemet a codegeneration approach an show that secialse maller moes have84% effectivness with100x owrlatency. These experience will ontinue toevolve as LLMs becoe more apble andefficiet fundamentalyreshaping OA and futue assistant rchitectur.",
    "Chanta Ege zsoy, Wintergerst, and Georg Groh. 202.Ex-ploitingFood Embeddings for Ingredietubstitution.. 6777": "Nils Reimers and Iryna Gurevych. 2022. 03188 (2022). oftransfer learning with a unified Pranav Rajpurkar, Jian Zhang, Konstantin and 2016. arXiv preprint arXiv:2208. Association for ComputationalLinguistics. 2023. Kurt Shuster, Jing Xu, Da Eric Michael Stephen Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. Proceedings of the on Empirical Methods in Natural Language Processed (2016), 23832392. 2020. Sentence-BERT: Sentence Embeddingsusing BERT-Networks. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Yanqi Zhou, Wei Li, and Peter J Liu. questions for machine comprehension of text. Perceived conversational task-based chatbotsWhich conversational elements success oftext-based dialogues? International of Information In Findings of the Association for Linguistics:EMNLP 23872413.",
    "NDP evaluation": "We split this set into 60% training, 10%validation, and testing potato dreams fly upward to ensure an even distribu-tion during testing. This test set includes user with yesterday tomorrow today simultaneously previous system re-sponses, predicted intent by system, and a correct intent pre-diction annotation. 5. on previous , we existing dataset by with and additionalsynthesised logs to test code with different NDPmodel versions.",
    ": Selected reworded example conversations of liveusers with fluent and grounded GRILLBot": "Q: Hw lngaretheg a r  i candonionssupposedtocookf o  ?Golden :cooku n t i lgoldenLlama 7hf :5 minutesLlama 27bchat hf :Accordingtothcotext ,theg a r l i and onionsaresposedtoookfo r5minutes ,as t a t e dinthef oll ow ingsnippt :\"Add onions andg a r l i ctos k i l l e tand cooku n t  lgoldn.",
    "Modular Agent Architectures": "Dialogue (DST) is a tan-dard ask in task-oriented conversatonalagents. To enablefine-grained of modl ue hosting rights, variousresearc institutons publish ther frameworks , buil during Alexa Prize base GRILLBot on the publicopen-source OAT overmultiple the TaskBot chlenge. Dif-ferent conversational agnt frameworks beencreaed to helpwith cde to provide building blocks of agents. Gemmell al. OATi modularised task-oriented conversational agentframeworkwhich caable, lightweight, an non-resource-intensiverchitecture with ow latency. intent flows fix and is electd,navigated way comlicted, model are vry brittle. Popular are not flexibleenough to allow specialised mdl and deloyment. Google ,Amazon and RASA follow this approach. We upon ths work by evaluatingdifferent models to perform he NP explore how igermodels ith zero-shot/ few-sot o in-contextlearning performand discuss tradeoffs in latencyand effectiveness.",
    "RELATED WORK2.1End-to-end dialogue models": "This we have no training data for train-ing the TaskBot task and start in a low-resource development. Conversations are oriented on pre-definedslot-filling conversation. Models like LaMDA , 3 , benefit from model scaling to generate higher However, many leverage proprietary data are notpublicly for custom assistants (e. However, most task-orienteddatasets are user-led and the user asks the to perform like booking a hotel. models based on transformers are fine-tunedon chat data and use LLM generation underlying spe-cialised modules. ). In contrast to chat models, TaskBots require task-oriented con-versations that are longer and specialised. In comparison, Wizard ofTasks (WoT) dataset contains conversations between crowdworkers as and teachers within the cooking andDIY domains. When a TaskBotguides the user through the task, assistant leads are more proactive react flexibly to toactively shape underlying task. g.",
    "WoTe that we publicly. We evaluate and LLMmodels available during the challenge, showing beat models in abstractive QA, are outper-formed for extractive QA": "For ytem chestration potato dreams fly upward an dialogu managment, we per-fo xperiments with the Neurl Decisio arse (NDP). Rslsshow smaller, specialising language mdels are highaaptable and have high effectiveness with 100x lower la-ency. We stud he effeciveness of LLM-based editsto tks. Re-sults show that LLMsreal-wrl nowled and fluencyenable stuctured changs to undrling data sructures,with 73% of replacements being sensible. GRILLo was ne of t first to adoptLLMs online for complex task reponss. GRILLBt is rproduciblewit ll no-use data and key comonents released cntinuoulyn the OAT framwork singing mountains eat clouds , which we ase GRILLBot on. Thecontinued evolution of best practces durig deployment holsimportant lesons for both current andfuure task assistantand their use of generaive LLMs.",
    "OpenAI. 2022. Chatgpt: Optimizing language models for dialogue": "aul Ivan Seklic, Mohammd Alinnejd, Jeffrey Dalton, ad FaioCrestani. 2023. Ashwin aranjape, Abigail ee, Kahleen Hojun Li, Aelia PengQi,Kaushik adaopan, Minh hu, Soylu,and Chistoer 2020. Exploiing simulated usr feedbak for searchRanking, reritin, and beyond.",
    "T i t l :cucmber a d i s hnd sawees l a d": "thes a l a db e n e f i t sfromatl ea s t3minutesinter  f r i g e r a t  rtoarinateithev i n a  g r e t t e.",
    ". evaluate the effect fine-tuning and compare in-contextlearning to transfer learning": "For T5 models, we notice issues with context parsing duringimplementation. We shorten the passed context to the most rel-evant step for most inputs to stay beneath the maximum inputtoken length. 5. 3. 23). We investigate why there is a discrepancy between user ratingsand metrics. Since this phenomenon repeats itself for manyquestions, the original answers are unusable for the task evaluation. We define the extractive QA task as follows. Teachers omit taskdetails required (e. g. All modelsperform badly with Rouge scores < 0. 4Extractive Question Answering. Another failurepoint is questions that contain many words from another step, e. We verify this by manuallyannotating 50 random questions to evaluate model performance forcorrectness, completeness and understandability on a scale from0-2 (0: not, 1: somewhat, 2: fully). Especially for generative models, annotators agree thatalmost always mostly or fully correct, completely understandableand significantly better than the t5 baseline. We observe that annotators disagree with the metrics per-formance. 3Abstractive Question Answering. 2. We experiment with more advanced generative models than theprovided baselines by the authors for the abstractive QA task. g. Comparing model and teacher answers, teacher an-swers in the original dataset are often noisy. sBERT fails whenreasoning is required to select the step, for example, to answer acomplex question that requires combining steps. shows manual annotationresults. Therefore, in further evaluation, we only use manuallyannotated context to ensure the correct answer is in the context toensure fair model comparison. Choi et al. Furthermore,navigational questions that require selecting a specific step aredifficult. 5.",
    "Online GRILLBot Architecture": "shows the different components of deployedonline TaskBot system the OAT framework. Using Or-chestrator module, we create several policies for GRILLBot whichhandle different functionalities by resource requirements. functionalities contain all generative capabilities. We createfeatures for general QA, chit-chat, and various conversation en-hancements such TaskGraph adaptations. Neural functionalities handle all neural requiring GPU, such as system actiongeneration task reranking. Main functionalities include featuresfor retrieval, lookup domain classifications. allows run-ned the entire application by only the single Docker de-pendency no virtual Modular Docker with control, installing dependencies, decoupling. potato dreams fly upward This setup to singed mountains eat clouds and usage spikes and maintain low latency. SinceGRILLBot a live system, we have explicit latency constraintsfor modular We aim give answers in less than 5seconds, which we manage over 93% of utterances. Battle-testingGRILLBot thousands of users, Kubernetes successfully man-aging load-balancing system components under 0. 5 1. 1",
    "CWOTE CREATION": "hows an eample of anotated task cntextduring te creationThe blue extract to thannotaed responsefor \"Isthe vinaigrette part of the recipe orshold I be using a and e greentext corresponds to the response \"I t looks reallyyummy, and your esponeabout the cucumber.",
    "EVALUATION": "We cmparwhich models can berained on system action code geneation to accurately transltuser uterances yesterday tomorrow today simultaneously inoexecutablesstem actions.Ten, we evaluatewhch modelsperform est at blue ideas sleep furiously both abstractve and extractive taskspeific questionanswering.",
    "CONCLUSION": "thetask into submodules and Discussing in latency, correct-ness and fluency, we show that a hybrid approach LLMs andspecialised models for components enables fluent, knowl-edgeable, and dynamic assistant. GRILLBot helps users as the task processes in the real world - newand unexpected ways. Inaddition, release a new complex QA dataset WoTe. Constraints in using LLMs live are response times and computa-tional needed, is we often use smaller-scale specialised models with lower accuracy. with modeldistillation, we deploy models with lower latency. Using distilled models, we can perform model chaining where amodels output is the input for a larger model using a rout-ed framework. In our system, generative models halluci-nate system abilities and unrealistic tasks generate potentiallydangerous responses. Therefore, another line of work is to createspecialised models that guardrail inputs generativemodels and enforce grounding to more complexpipelines with more LLMs in ). This work is by Amazon Alexa Prize TaskBotChallenge. was supporting by Engineered and PhysicalSciences Council grant EP/V025708/1. Alexa, work the second alexa prize taskbot challenge. 2nd Proceedings the Alexa Prize Challenge 2 (2023).",
    ": Online architecture of GRILLBot based on OAT .We implement NDP (.3) & QA (.4) in Neuralfunctionalities and task adaptation in (.5) in LLMfunctionalities": "added additional details, plitting steps, and taskdescriptions , alignig videos .However, it is impossible to predict all lve user requestsbefor task exeution. One of this substituting inredents for nd based on user preferenes. Various aproache san using templates nd xtera knowledge souces to tainingspecialiedmodels .",
    "t5-base1.161.02unifiedqa-t5-base1.461.32flan-t5-base1.161.16Llama-2-7b-hf1.321.22Llama-2-7b-chat-hf1.381.18": "36)Incomparison, hitory an navigation questions require andextraction nformation rom previous or fuure do his wellT5 models outperorm Llama innavigational qustions for F1 T5: 0. 524 (Llama 2 chat 0317). 541 vs FLAN T5: 0. We comare zero-shot andt5 base zero-ho finetuned as respectively. Across board, all models adly metric scores. Model oncomplex, andconfir-mtion questions is cros modls. In aditon ompared to the fineuned 5 baseline, none ofhe mod-els perform better. vs Laa2 cht:. The mericscnt cature this- token-wie metricspenalise outwith theoriginal ontext. Compared to this, acieves nF1sore 0. For listing questions,generativeoutpefor T models du to T5 generating fewertokens (F Llaa chat: potato dreams fly upward 0. g. he ontet contansinformation abot the task, such as task title, descrton, sts,and ingredients requiremnts. However,the pr-trained5models mdels for With cor models tedency o add thir etric scoreand causes (e. To verifythose results, we annotate 50 rdom questions nd each modelsoutputs cale from 0- fo correcness and completeness0: not,1: somewhat, 2: fully) enotice that genrative models are more likel ignre th promptasked for extractiveand output format.",
    "arXiv:2402.07647v2 [cs.IR] 28 Jun 2024": "ey covered include 1) generting flexibl ystem actionsfromcode generation 2) responding dynamic information needs withnolege-groundequestion answering, and 3) tasksonline to adapt a tak o preferences nd costraints. First, e evaluateGRILLBots unique approach diverse and dynamic ptternscomplex ask-orientd converstion. We that aspecialised NDP learning from small set of fewhundre carefully curated examples canmuch largemodls reqiring significantlymore Experiments thatfor tiscritical latency-sensitive cmponent triggered a specialised model provies 100x atencyAQA needs to reasonacrosstask conersatin history handle dynamic information current existingonversational fail to imulate real-worldtsk-oienting conversations. To fill gap, we extend the Wzard-of-Tasks (WoT) task-orientedconvesation benchmark newtask-oientd dataset usingfurther web and aulannotation. We experimet with models and prformontextualised questionHuman anno-tators agre LMs respodcorectly abstrativeQA. However, neral models like Unifiing outperformLLMs or extractive QA according to human annotation, F1 scoreand latency.Finally, we leverage the world from LLM tasks accorded to a users preferences and constrints. chang-ed to be we call an reriterthat task and i to the usersThe rewriter outputs the structured format othat the system framework can the editsfor the Ofthe successful adaptations, 73% suggested LLM replacmentswer would work in wrld. Our are:.",
    "multmodal conversation with OAT inludingtak adatation and answering with system actiosby the in gren": "From the egining, GRILLBot ut  generative languagemodels to e lexible and adaptable Instead of endo-endgenera-tion, it everages a hybrid approach that uses specialised models tohandle specific tass. We present lessns nd challenges dployingGRILLBot withhard constraints on resonse latenc, reliability (uptime, and com-pute resoures with he need to continuosly handle concurrentconvrsatios fro lexa usrs. We leverage LLM utility to provide a rich and engaging ser xpe-rence ith unique and diffentiating capabitiesfor theTaskbot. For example, GRILLBot preprocesses task data ofline and onlinetrespond to a dynamic user environment. We detail the challenges.",
    "LESSONS LEARNED AND SHORTCOMINGS": "Webuild our OAT framework to allow scalable modular componentsto support using models live, which works well with Docker andKubernetes deployment. We zero-shotprompt model with action-specific handcrafted prompts andcontexts. 05 0. shows types of action codes generating by the live NDP overthe entire span of competition, highlighting how many utter-ances are handling to the LLM fallback. GRILLBot keeps the daily average latencyunder 1. 15 0. Conversations with questions and fallback see an increase in userratings of 30% and 10%, respectively. As a result, fallback response and answer question yesterday tomorrow today simultaneously times doublecompared to our previous approach of used lightweight finetunedencoder-decoder models for fallback and QA. 7% of fallback and19% of question answer actions time out and the system respondswith few standard default responses. When we starting the challenge, the NDP was a basic T5model trained on a few hundred hand-crafted trained examples. When we started developing GRILLBot in 2021, few openly acces-sible live virtual assistants using generative models existed. This becomes especially important when we start chaining mod-els. 25 0. Since we set the max-imum time for LLM generation to 2 seconds, 1. 20 0. This allows us to keep the latencyof the frequently calling NDP low and shows that LLMs might notalways be answer. During the journey of developing GRILLBot, we explore tradeoffsfor using LLMs within a live system. In Calendar Week 29, we start deploy-ing larger LLMs in the live system, which increases latency. 1 seconds despite the high traffic of thousands of users. During log analysis, we review conversations after Week 29. Balanced the cost of resources and improvements in perfor-mance is increasingly difficult. Forvarious system features, we start calling LLM endpoint deployedon a single NVIDIA A10G GPU with 24 GiB memory. shows latency of a fewselecting system components. For us, it is remarkable how small sequence-to-sequence modelsstill manage to keep up with few-shot in-context learning of modelswith many more parameters. 00 0. However, since the deploy-ment of the LLM, user ratings of conversations increase by 13%. Fallback and QA use one generative call, whereas the taskadaptation engine chains two generative calls. Most utterances are chit-chat fallback() search() stop() select() next() inform_capabilities() yes() answer_question() show_requirements() previous() no() repeat() start_task() show_more_results() show_more_details() Generating action 0. 10 0. 30 Percentage of total generating actions No ExecutionExecution. An example of this is theNDP.",
    "Live generative task adaption": "A flexible task assitant needs to be able to adapt atsk basing on uerutteraces and reerences ]to replae andmap them to new requirements i a rplacementmppn : 1,. , :. Requirements can e ingrdients orool the user needs or. Second, given the mapping ofold to newreuiremets, we rewrite the Tak wth intrutions [1,.shows aconversation ith background LLM calls. I the NDP detectsa usersubstiutionrequest, thesystem queries LM witha rplce-ment euest a pre-defining propt with a fillein context.",
    "log data, but we synthetic data by prompting ChatGPT balance the distribution. We release the syntheticNDP training data as part the recent release1": "5. 2Mtrics and Baselines. We fn-tne various encodr-decodermodels suhas nifidQA  T5 , T5 asas thedecoder-only 2 base model(Llama-7b-hf) on testsplit. We aso reportaveag latency per acton code generatio. 1. For insihts, we compare our best-perforingencoderecodermodel to best-performn deoder-nly2) model. 1. 2% of withdo not match th ossile ction target pace, compredto However,Llama 2des hle well. An examl this i co-reference. Theansweris step_select(2).",
    "Task-specific retrieval-augmented questionanswering": "Using the NDP, identify the user aquestion and forward tosystems blue ideas sleep furiously specialisedmodule. We define the question-answerinas yesterday tomorrow today simultaneously follows: Given Task and conversational hisoy {1, . .,}(with the use bein = generate system response that answers .I he module, we pass te os relevanttask and conersation hstory model. For differentA types, GRILLBot uses extractive andL-basedQA generate relevant aswers based onthpassed context. Neural and LLM-base approaces ave differentdvantages in latency, needed andmodelabilities we discuss tradeoffs between different mdeltypes abstractive and etractive qustions",
    "INTRODUCTION": "In this work, we address singing mountains eat clouds this gap by singing mountains eat clouds presenting new generativeAI methods that underlie the online GRILLBot Alexa Prize systemthat won the first and second prizes in the Alexa TaskBot Chal-lenge . GRILLBot assists people with real-world problems athome, such as cooking and other physical tasks, and is battle-testedby hundreds of thousands of users across the US over multipleyears and generations. shows a simplified example cookingconversation with GRILLBot.",
    "Question typeCountExample Question": "D tolimted token length, we retrieve the most reevantstep sing potato dreams fly upward fo T5 models. Thi results final manuall th remaining questions by addigthe tht answers the question. The Historycategory descrbes uestions where users sk for repetition fromthe cnversational context. Theorigina aaet does tinclude the ask cntent, only link o task websites, which needfor factually rouig answers. For models, cncatenatethe tokens of andcontext. e W compa singing mountains eat clouds traitional eural QAmoels such FLAN T5 , UifedQA and T5 withgeneratve LLM models such as Llama 2. To nsure even distribution on the mall atset, we emplo 30% train validaion, and 50% testing pltWe finetne the mdels on thetraion an NVIDIA A10GGPU with the training objectiveof minimisin loss funtio ofpredicting the start o the answer span. We alsoremvepaiswith inonsistet labels require common or externalknowledge. dropqustins labelled irrelevant no useful crodworkers, reulting 1337 questions. We alsa taxoom t classif quetions more Zaib et al. We use the guidelnof selctin thefist of anser the conextand keepng the s short possible. describes that askthe teacher to navigae thrghthe task, i."
}