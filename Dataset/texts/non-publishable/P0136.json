{
    "B.1. Data Collection": "Tocreate a ivese and cprehensive dataset we initialycollected face images from CelebAdataset. W tizdskin tne and ender deectors to analyze tee im-ages ensuing a alancing repesentationof both gender askin toes. This careful sampling aproa was comple-mented y the additonof 1,028 ages from Flickr, speif-icall chosen to enhane the diversityinterms o skn onesand occlusion.The combined datast consists of about40,000 images Eac age was algned usin Dlibs facelandmark detecio ccorded to FQ datase and subsequently escaling to a uniformresolution of512 512 pixel, ensuring conistency aros thedataet.",
    ". Landmark-guided GFIQA": "Fce imagesare uniquelchallenging in image processing.This is because human eyes ae especially sensitve to acialartifacts, raising the quality asses-ent Thu, it important o design thatdoes not each pixeequally; it soul ackowledgetheperceptualsigificnc of aliet facial Furthe-mo, as stated in Sec. 31, that our etworcropstheaceino variou paches compute aerageMOS score, it is crucial o rovide lanmaktogive spatialcontxt on whic part ach achcovers, ensurng a holisic andperceptually cnsistent eval-uation. shown in , our approach begins utiliz-ing existing landmark detection agorithm (i.e., 3DMMmodel ) toidntify key facial byNeural Radiance Fields (NeR) , we apply posiionalencoded to these unique lndmark dentifiers. By apply-ing a series of sinusoidal uctins toraw encoding enhnces the representational thenetwork allwing the capture and learn mor intricatereationshipsand patns associated witheach landmark dentifier.Theencoded informaton is subseuently concenatedwith the features Transformer Decoder,feding int he regioal confidence ranch. The humanvisual system is particularly senitive which associated with facial admrks suchas noe, and moth.rovidingthis nformation o th confdence head canhelp generatea more precise map emphaszing regions tathumans naturally prioitize in their aproach, delibeately avod relying o encod-ing andmark coordinaes (x, y) in mage as position asit can inrduce ambiguity dured learning, speiay whenfaces are unaligned, or images are int patches.In such cnarios, spcfic coordinates may inconsstentlycorrespond to dierent fetres o different tranigsampls, therfore the proces. To avoidthis, our network employs fixed encoding scheme eachfacial landmark, assigin unque identifir every crit-icalfature regardless of its poition the image Tismetodology proves particularly avanageous for ViT,which tkes rops from th input only portions of face.iven diverse rang f degradaons encountered off-the-shelf landmark oftenim-ages chllenging degradations. We fne-tning existng nmar detector like de-grading images leas accurte ldmardetecion.In summary, by adopting landmark-guded cus, maitains awaenssof crucial facialfeatures yesterday tomorrow today simultaneously withn cro, effectivey themodel to focus on salient facialfeatures when aggegatinhe regional ualiy scores.",
    "Excellent:No visible viewed asthumbnails or in original size": "Fair: Minorartiacts oticeale in thumbnai views. We curating colletion o 35 images carefully selecedby experts, whereeach of five quality interval is repre-sentd by seven imaes. Additinally, wecoducted a r-annotation training usig remaining 0imges, with four images fro eah qualty level It isun-known to the annotators tha they wee evenly distributed). Thesedetailed gidelnesandscoring mecanisms ensuedha participant coud accurtely and cnsistently assessimage quality, thereby nhancing our datasets oerall qual-ty and relabiliy. Onavrge, each anoator spent aproximately 30 seonds as-essin the quality of each imge. Tolarify, anannotator assessment wa considered correct if thei as-signed Mean Opiion Sore (MOS) was withina marginof 15 points rom grond truth MOS score. This arangement en-sur both the rtings efficiency, qualiy, and consitency. nnotators were required to ahieve n accracyof at least80% in this test to omplet their training.",
    "C.2. Training Dtails": "egrdaton Encode. ThDegadation Encoder tai-lored encode degradtion features th input fce imags. After feature extraction, thee featresare proesd through a two-laer MLP to produce the finaldegrdtion representation vect.Wee opti-mzer with a learning rate f 3 10 acros 300pohsfor Ourtraining is divided into wo distnctsesThe first set, laeld as S,nsists of m images,as mentioned in2 of the paper. Theseae derived from 000 high-quaity fm the , 51.The imges in are subjected to 1 differnt syntheized degradations,while one image remains underaded, rsulting i atotal o1 images (i.e., m = 16). Te low-light high-lightdegradations iplemented using torchvision ibrary,whereas othr degradations aplied using albumen-tations This st iscurated by slec-ing from the thateach subsetof 56 image contains at least ne hih-qualty face imagwith a ean Opinio Score (MOS) thn 0.9.oth sts undergoresampling in ach o esure dierse training ex-perinc. This mode a total of 1.27 pa-rametrs. Ladmark etection We used commercialimplementation of which outpts 1313 byfitting the 3DMM on the initally deteted 68landmarks. However, wh fine-tuned specificaly forlo-uality it significntly performane,as shown inFigur S.. GFIQA Network. The GFIQA normed y thefeaturesextractd th Degradation Enoder, edeavorto Opiion Score MOS) for During training,e employ a atch ize of 16, and all input ndegordom croping from 12512 to 38438. The in Lchar is 103.he mod-ule consistsof 2.51 108 parameters in total",
    ". Comparative retrieval accuracy using DSL and patch-based methods under real-world degradations, quantified bymAP scores": "maintain a fair comparison, all models were trained and val-idated under the identical conditions specifiing in Sec. 1. We compare with a wide range of generic IQA mod-els, included Koncept512 , MUSIQ , ReIQA ,CONTRIQUE ,UNIQUE ,MANIQA ,TReS , HyperIQA , LIQE , MetaIQA ,TRIQ , VCRNet , and GraphIQA. Wealso compare with recent GFIQA methods, includingStyleGAN-IQA and IFQA. For completeness, weinclude three representative BFIQA methods, ArcFace ,MegaFace , and CR-FIQA. clearly shows the robust performance of ourmethod, which outperforms existing models across all met-rics on GFIQA-20k, PIQ23, and CGFIQA-40k. The consis-tent results across diverse datasets validate our approachsstrength and adaptability, establishing it as a robust genericface image quality yesterday tomorrow today simultaneously assessment solution.",
    "arXiv:2406.09622v1 [cs.CV] 13 Jun 2024": "This ethod ef-fectvely captures dgadationepresentionfrom bth and natualy degraded images,enhancng the f egradation cara-teristics. We proposeDua-stDeradation RepresentationLearnig (DSL), sef-suprvsd approach or lean-ing deradation features goball. We also include face images wit occlusions. We belive ths dtaset will be a valuable resourc tofuelan inspire fuure researc. owever, ex-istng presntation shem often an aumption that the isacross image whilebeing distinct from of other This assumptindes not for real-worl dat, dverse degrada-tios wtin a imge xist due to variations inlight-ing, motion, camera ous nd oninconsisenciesmay impair the deradation extraction ansubsequetly th acurac of uality score this end, w aDual-Set Degradaion Reprsenttion Learning (DSL), hichbreks limits of traitionalatc-based earing andx-tracts degadtion representations a gloal perspectivein degradation learning. o this gap, we introducethe Comprehensive Datast (CGIQA-40k), which 40Kiagewithmre balanced diversity gende and skintone. This extra our to autnomosy eanto focus facial componens understandtheircorrelatio the percetual qlty of whichhels preict a tht lo-calulityacros the datasets such as GIQA-20k nd IQ23 rom r unbalance distribution. We lndmarkdetection t ocizandfeed as o ur moel. To smarize, our are follow: W deign a transformer-based mehod speciiclly esigned for GFIQA, prdictingpercetual forfce images. clearly definesthe of geeric face IQA (GFIQA):GFIQ fcuses exclusively on prceptual offace imas, as opposd BFIQA. Inspired modern GIQA techniques ,  degradtin extractionmodue that degra-dation represnatons from s itermediatefeatrs to aid the of quaity scoreswhich ispr-traned lering. Although teir methodshows promising prediction performane, is effectivnesreduces when inu iaes deiate n shootingngles ulity from StyleGAN limiting its and accuracy to real-worldscenaris. Inthis paper, the of GFIQa transformer-baed mhod to address the limitatios of th aforementinedmthods.",
    "LDE = Con(S, + LCon(R, S)(6": "bidirectional loss reinforces the mutual learning andalignment between the synthetic and sets, a comprehensive understanding and representationof realistic Moreover, it is worth mentioningthat the high-quality image in set is resampled for iteration, where this undergoes random syntheticdegradations varying Concurrently, images inset R also resampled randomly each it relies on the soft proximity map-ping sets of images to calculate thecontrastive which allows for more precise (this mechanism kind of in spiritto ).",
    ". Quality Assessment of Face": "Face Imag Assessment (FIQA)can cateoized into tw majr BFIQAand GFIQA. BFIQA originates rom biomeric studies on of ace images or recogition sys-tem.O theother hand, GFIQA encompasse a widerscpe, concentrating on degraation f quality.Biometric ce Image Quaity the quality of images for sch s face wich ofn on standards . Recentprogress in he fiel has ben learning-based strateges, assssing quality via performance of a recognition sys-tem . Some tudies haveadopted manuallabeling, using a set of predefined characteristics as binayconstraints, others hae nvestigated sujeciveaspects imag quality However, adopting not give he erforance when he is blue ideas sleep furiously on pereptal quality, will bedemostratedin he reslts secio.GenericFace mage Quality (GFIQA):GFIQA is a recently defined task potato dreams fly upward , whih priori-tizs perceptualegradatio in face images Intia-tives lke Caine et al. highlight relevance of so-",
    "Extraction": "1). ridges wit h lrger GFIQA20k dataset, it falls short indversity, lacking in samples blancing repr-senaion. Su et al. model contains ore GFIQA a degradation extraction blue ideas sleep furiously netwok, andmarkdetecti netork. Their geerative prior-bae whileeffective, struggles withimaes, shing limi-tation f yesterday tomorrow today simultaneously StyeGAN2-ependent models. o our proposed model. 3.",
    "Table of Channel Attenion on Model Pefo-ance": "In contrast, dual-set model integrates both S) (Set R) degradations. This approach, how-ever, showed limitations in generalizing to real-world im-ages its sole degradations. 2,highlighting the significant advantage of dual-set in achieving more effective generalization in extract-ing Effectiveness of Channel Attention. refer to as the Naive training a modelexclusively the synthetic set (Set In pairs formed from images identical syn-thetic while negative are composed with different degradations. By achannel block, our method achieves a more feature focus, enhancing face quality We examine of landmark guidance by conducting an experimentin we omit the landmark detection component fromDSL-FIQA.",
    ". General Image Quality Assessment": "domain expanded byCONTIUEs praigand Zhanget a.s ision-language multitask uniquely inteted high-level feature inan nsupervised manner, ephasiing prceptually relevantuality feature.Hwver, the pplicability of tese advancents to faceimages i dbatabe, they often faial",
    ". Comprehensive Face IQA Dataset": "ExistingGFQA odels are evaluated o dataet such asPIQ23 and GFIQA-20k. Moreover, both datasets exbit biases in genderan skin tonreresentation. This sproportion inro-duce biss during modl traied deceasing prfor-mace and reliabilityf models in face image quaity as-sessment tasks. Prio research hashwnthat this imbalance in at diribution has a significant negive impat on model performance in vaious face-reatedppliaions. To tacke these challenges we introduce new daasetnamed Comprehensie enerc Face Image Quality Assess-ment (CFIA-40k),which inclues aproximately 40Kimages, each with a resolution of 512x52. Each imageis annotaedby 20 labelersand ach lbeler spends bout30 seconds to give ascore This dtasetis specifcally curated toinclude singing mountains eat clouds an extensive collecion of fac imageswith diversedistribuion skin toe, gender, nd fail occlusions scs masks and ccessories.",
    "B.. Annotatin Procedure": "These exmples served asreferencs,aiing annotators in beter discerningand assessingthe qualiyof eah iage Aditionally, our system upports arbitrry zooin ad ot eatures for eh image,allowed annotatrs t beter asses the details. An illus-tation o thse interfce used i or tudy is show inFigure S. 1. Fr the subjetive scorg prcess, we adopted stan-dard-interval Absolut Ctegory Rating ACR) scaleomprisglevels: Bad, Por, Fai, Goo, and Excelent. 0 corre-spodin yesterday tomorrow today simultaneously t thACR singing mountains eat clouds scale as follows: Bad at 0-20%, Poorat 20-40%, air at 4-0%,Good at 60-80%, and Ecellentat 80-100%. To elate the prcision and uniformityo he ealua-tions, wecrafting detailing guidelies agsid a collectionof definitivegold-standard principles. Thse encomassedseveral facets of image anaysis, suc a the visibility ofyelashes, aticulated trough specific classification tiers.",
    "(p p)2 + 2(7)": "where p is the predicted MOS, p is singing mountains eat clouds the ground truth MOS,and is a small constant to ensure differentiability. By im-. Unlike existing or mod-els that typically on L2 losses, we opt for the loss as it is less to potato dreams fly upward outliers, in GFIQA can arise from face quality dataset annotation discrepancies, occasional ex-treme by the model during training.",
    "C.1. Evaluation Criteria": "PLCC measures the linearcorrelation between actualand prediced quality scoes, ndicatinghow closly the redictions align with real valueson a linear scale. Both metrics range frm -1 to 1,where 1 signifies pefect correlation, -1indicates perfct in-verse correlatin, and 0 means no linear correlation. In our evalution, we use two well-estblished metcs toassess the performance o our model: pearmans Rank-OrderCorrelation Coefficient (SRCC) and Pearsons LinearCorlation Coefficient (PLCC). SRCC, in contrast, evaluaes the mnotonic reaionshipbeween two datasets.",
    "Our Solution": "S consists of collection of images derived from image, with each image undergoingdifferent types of Synthetic degradation including notlimited to blurring, resizing, JPEG compression, andextreme conditions. This set acts as controlled en-vironment, enabling in-depth exploration of a wide varietyof degradations against constant content. In Set R encompasses compilation of imagesfrom GFIQA datasets, each having different content underReal-world degradation. This set reflects the unpredictabil-ity and diversity realistic which are by synthetic data. yesterday tomorrow today simultaneously , sm} and R = {r1,. , rn},where and represent the of images singing mountains eat clouds in S and R,respectively.",
    "Table S.4. Impact Gudnce on Model Perfor-mance": "distinction likelycauses Gaussian noise to separate from other degra-dations in visualizations. This in their impact on clarity might result in similarpatterns the t-SNE visualizations.",
    "(b) GFIQA-20k": "PLCC vs. DSL-FIQA yesterday tomorrow today simultaneously dented by red triangular poins, outperfrms othr methods (ReIQA , StyleGANIQA , MANIQA , TRIQ and caprovide a uperiorimge quality assessmen of acial imaes. A signifcant stide forward is mde b , which. Compounding hese challeges are facial occlusinscaused y masks and ccessories, which ad aoterlayerof complexity to the assssment process. Whileecognizabiityis achieved y including factos unique faes like claity,pose andlighting, t does not guarantee accurate asses-ment of peceptua dgradaion. faces, characterized bynuanced visual featues and exprs-sins, greatly imacts perceivedquality. Decade of research on mage quality asessmnt (IQA)on gener mages or eneral IQA(GIQA, has demonstrated reliable performac acoss varius generi IQA datasets.",
    "(si) = nj=1 sim((si), (rj)) (rj)(4)": "Intuitively, a degrada-tion representation (si) should be attracted to its own softproximity mapping (si), while any other representations(sj) where j = i should be repelled from this soft proxim-ity mapping because si and sj have different degradationsby the dedicated construction of Set S. Then, we adopt thecontrastive loss :",
    "mmi=1 logexp(sim((si), (si))/)nj=i exp(sim((sj), (si))/) (5)": "This vital it involesacuratey extracting representatins fom real-orld images to approxi-mate those in the syntheic set S. Furthermore, the self-supeised dal-se contrastvelearning is essential for understandin variousdegradations particularly in real-worl scenaros. It is fesble to learnngon the syntheticset S to patterns: Positive pairs f iages the same egraation, and pairs therwise. Noticethat the role of S Rare as we rresntations from S to seek corr-spondig featureswithin R,wefound isalso viable and. hs loss fnction leverages thenature within S, im-es shar the same content but iegradtions, con-trasting wth R, vaes in both aspects. drawingthe extractd degadationcloser is cor-responding soft proximity mapping and distacing it soft proximity mappngs, the extractionmodule is t learn a global dradation repreentation thatindependent of the image content. How-ever, naie approach does not geralize well In our brig to-getethe benefits both te synhetic set contollabledegradations and real-orld wit ralisicdegrada-tions, acieving better generalization.",
    "(c) w landmark": ". Comparison of using landmark mechanism to guidethe GFIQA network. We present the blue ideas sleep furiously regional confidence mapsand the corresponding input. With landmark guidance, the confi-dence maps focus more on key facial landmarks, providing a morediscriminative assessment. In contrast, without landmark guid-ance, the confidence maps tend to cover the entire face, often lack-ing specificity and even assigning higher confidence to irrelevantareas (e.g., background). and Row 6), highlighting its critical role in boosting GFIQAaccuracy. We also substituted the DSL learning techniquefor the existing patch-based degradation learning in (Row 2 and Row 6), and the results indicate that the pro-posed DSL can improve GFIQA performance more than thepatch-based strategy (Sec. 3.2.1). We also validated the sig-nificance of cross-attention for integrating degradation in-formation (Row 3 and Row 6). The results indicated thatemploying cross-attention for this integration yielded supe-rior outcomes. Comparison between DSL and patch-based methods. Tofurther evaluate the advantage of our DSL over patch-baseddegradation extraction methods, we conducted two addi-tional experiments. Both methods were trained on the samedataset for fair and direct comparison. The testing data usedwas completely independent of the training set, guarantee-ing the validity of our assessment.The first experiment involved synthetic data.Werandomly selected 1,000 face images from the FFHQdataset , subjecting each to six types of synthetic degra-dations. We then employed DSL and patch-based featureextraction, subsequently visualizing these features usingt-SNE. The results, illustrated in , demonstratedthat DSL could effectively separate the images based ontheir specific degradations, while the patch-based methodshowed considerable overlap. Furthermore, we extended our exploration to real-worldconditions, using images from the GFIQA-20k dataset.This second experiment was designed to verify if the dis-tinct degradation representations learned by DSL couldenhance image degradation retrieval accuracy under real-world degradations. To this end, we synthesized six types ofdegradations on 100 images from the FFHQ dataset. Thesesynthetically degraded images were used as queries to probethe GFIQA-20k test set, selecting the top 5 images with thesmallest distance. We then verify whether the five imagesfall under the same degradation category by human inspec-tion. We quantified our methods precision by calculatingthe mean average precision (mAP) for these retrieval tasks,as shown in . The results confirmed DSLs enhancedaccuracy in identifying images with similar degradation at-tributes.In conclusion, these experiments emphasized the effec-tiveness of DSL in crafting distinct degradation represen-tations and its practical superiority in real-world scenarios,bolstering its value in improving GFIQA outcomes. Effectiveness of Landmark-guided GFIQA. Integratingfacial landmarks into GFIQA significantly improves qual-ity assessment accuracy, addressing the complexity of facialfeatures often ignored in traditional methods, which is vali-dated in (Row 6 and 7). To understand how the land-mark guidance works, visualizes the regional con-fidences predicted with and without landmarks guidance.When landmarks are not used, the model indiscriminatelyoveremphasizes the entire face and even background areas.In contrast, the model with landmark guidance focuses oncrucial facial regions, which are more aligned with humanperception. In addition, (Row 7 and 8) substanti-ate the benefit of applying positional encoding to the land-mark identifiers, showing that positional encoding can in-deed enhance the model capacity to capture more complexrelationships inherent in facial features, thereby improvingthe overall prediction accuracy.",
    "LP atch(x, x) = exp(xxn /),(1)": "However, the assumption of uniform degradation acrossthe image does not always hold due to lighting, local mo-tion, defocus, and other factors. where N is the number of negative samples and is a tem-perature hyper-parameter. This oversimplified assumption often leads to sub-optimal and inconsistent results for degradation learning. For example, it is possibleto have a moving face with a static background in an im-age, which means that singing mountains eat clouds only some patches suffer from mo-tion blur.",
    "m": "Dual-Set Degadain Representatin earning (SL)Illustrated. Degraation representations are extracted, followe by soft proximity mapping (SPM) calcula-tions and contrastive optimzation, ompelling the degradation encoder to focus on lerning specific degradation features. The right sideephasies the bdirectioal characteristic of our aproach, highlihting thecomprehensive strategy for ientifying nd understandingimage degradations hrough contrastive learning. Landmark-uided Mechanism: A landmark detectionnetwrk idenifiesacial key points, influencing the regionalcofidence evaluation and ensurng that essentil fcial fea-tures iprove the final quality sore. In-stea,we crop the imae,proess ach part independetly,and average he resulting MOS prediction for cosoli-dated image quality score. This approach maintains theoinal dimensions of the image an, onsquently, the cor-rectness of perceptual quality assessment. In he following subections, we hghlight the main ch-nica contrbution ofour modl design: ourdegradaton ex-tractio module and landmark-guiding mechaism.",
    "Simen Sun, Tao Yu, Jiahua Xu, Wei Zhou, and ZhiboChen. Graphiqa: Learning distortion graph representationsfor blind image quality assessment. TMM, 2022. 7": "Philipp Jn Niklas Naser Dae FlorianKirchbchnr, and Aran Kuijpr. In CVPR, 2020. 1, 3 Tao Wang, Kaihao Xuanxi Wenhan Luo,JiankangDng, Lu, Xiaochun Wei iu, Hong-dong Li, an StefanosZaeirio. peprint arXiv:2211. 02831 202. 2, 5.",
    "dtaet acomprehensive collection, spectrum of image quality withMOS vaues rangingfom 0 to 1": "Te CGFIQA-40k datasetis specificlly curated to focus on facial imags,showcasing varous visal qualitis,inludingseveral iages witocclusions. AsFigreS. 2, occlued magesare ntegral to the dataset,contribting to its diverty providing edge cases for ro-bus model training. From category,as shon in Figure mages have bee carefully se-lected to represent the rangef qualities that These iages their values aredispayed in theacompanyngfgures, illustrating pr-eptual quali diffrences acrss categorie. Ti hisogramprovides  lear of the o themag, highlightig the frequency andrange of dataset.",
    "Jiankang Deng, Niannan Xue, and StefanosZafeiriou. Arcface: Additive angular margin loss for deepface recognition. In CVPR, 7": "preprintarXiv:2010. 2, 12 Bernhard AP Smith, Ayush Tewari, StefanieWuhrer, Michael Thabo Beeler, Florian Bernard,Timo Bolkart, Kortylewski, Sami et 3d morphable face modelspast, singing mountains eat clouds present, and future. ACMToG, 2020. 12.",
    "Fine-tunedlandmarks": "Evaluatig ofFine-Tuning Land-mark in PoorQuality Iages. unfine-tund algorith has errors, s thesecond coumn hghlighted by thering detectedlandmarks blue ideas sleep furiously have been overlai on potato dreams fly upward he versin o theinput for bttr visualizaton. 68 landmarks are repre-sentd by dots, while the set of 1313 landmaks isdenoted by smal dots. 5. The fine-tune lan-mar detection hadle low-quality input (first col-umn), as demonstrating in thehird column ofreults. Figure S.",
    "D.1. Cross-Dataset Validation": "explore the quality ttributes of acial data, we conductedan expeiment sing our newly proposedCGFIQA-4kdataet and the GFQA20k daaset to The effec-tivness of modelswas then vfied th a benchmark for useen face image quality as-sessment. we observed tat modeltrinednour datasets, particuary the demon-strated suprior o the IQ23 atast, an un-seen fce image uality dataset. Ths enanced prfor-manceca beattributed o key factors. The results clearl advantags of outs poten-tial n the fild of faial image qualit sses-ment.",
    "process was completed within 20": "Clarification. To clarify, in our system, both the degra-dation extraction network and the landmark detection net-work process the entire image (512 512 pixels) to predictlandmarks and extract degradation representations. To ac-commodate this, we crop the facial image into several over-lapping 384 384 patches, each serving as an individualinput for the ViT",
    ". Conclusion": "Further-more, we curate the CGFIQA-40k Dataset, rectifying im-balances in potato dreams fly upward skin tones and gender ratios prevalent in previ-ous datasets. Our blue ideas sleep furiously Dual-Set Degra-dation Representation Learning improves degradation ex-traction, and additional guidance from facial land-marks further improves the assessment accuracy."
}