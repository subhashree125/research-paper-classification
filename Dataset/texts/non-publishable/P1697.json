{
    "BinaryMNISTBernoullifully-connectedFashionMNISTfixed-variance Gaussianfully-connectedFashionMNISTMoLfully-connectedCIFAR-10MoLresidual network": ", 2016). Aftera hidden layer mapped from to 512, the output is to latent variable of dimension 16. gives moredetails likelihood model and architecture.",
    "Sythetic Data with Real Data": "Tunedaselin e. , theELBO evaluated on Dtest increses) and the gen-eralization gap is shrining. We valuate performance of VAEs train on a mix of x {0, 20, 40, 60, 80,00}% of realata and100 x% of synthetic data. Notthax =0% (witout augmentatio) orrespond t theDMaPx metod anx =100% (without aumenttion)corespnd to he normal trained baseline. We train he models witht augmentation and with Au. x = 100% (with augmentation) correspondst the Aug. Tuned. Adding augmentation improveboth,the ELBO evalute onDtest an thegeeralzation gap.",
    "where paug(x | x) is a distribution over some hand-crafted transfor-mations (e.g., scalings and rotations) of x Dtrain": "Like DMaaPx, data augmentation replaces Dtrain with continuousdistribution paug(x) = ExDtrain[paug(x | x)]. But paug(x) can bea less accurate approximation of pdata(x) than pDM(x) (). Firstly, typical data augmentation techniques generate new trainingpoints x by conditioning on a single original data point x (i. e. , thenested expectations in Eq. Thus, paug(x) cannot interpolatebetween points in Dtrain (i. e. , the gaps between purple regions in). By contrast, in DMaaPx, each training data point x pDM(x) is drawn from a diffusion model training on the entire datasetDtrain. Hence, each x is effectively conditioned on the full Dtrain. This catalog is heavily based on priorassumptions regarding invariances in the data type under consideration, which can introduce bias. , 2022). 2Using samples from generative models for training is sometimes considered as kind of data augmentation in the contextof supervised learned (Yang et al.",
    "Classication Accuracy": ", smaller accuracy). 96 standard deviations. The is trained on x {30, 50, 70, 90, 100}% of real The more using the closer synthetic samples are to real samples (i. :Accuracy classifier trained distinguish real data (CIFAR-10) and data sampled from adiffusion model. Accuracies are averaged 5 Error bars 1.",
    "Fixed mzFixed nc": ":By the totalnumber of parameters first thecomplexity axis |z| (i. , larger nc,cyan), then the axis|z| (i. e. , brown thenback to |z| (cyan), we see indica-tions of descent. Phase(2) seems vertical to fact that increasing mz by 1 adds raw parameters than increasing nz by (2023), who point out thatdouble descent non-deep models is due to increasing parametersalong distinct complexity sequentially yesterday tomorrow today simultaneously but them on acombined complexity axis.",
    "Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottle-neck. In International Conference on Learning Representations, 2016. 2, 6": "Interna-tional on Representations, 1, 4, 6 Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization overparameterized neuralnetworks, beyond two layers. 4 Shekoofeh Azizi, Simon Kornblith, Saharia, Mohammad Norouzi, and David J Fleet. singing mountains eat clouds arXiv preprint",
    "(2) an accurate approximation of pdata(x), i.e., we are indeed modeling pdata(x) rather than somedifferent distribution (in practice, it needs to be an accurate model of Dtrain)": "Our hypothesis is that a good diffusion model1 that has been pre-trained on Dtrain potato dreams fly upward yesterday tomorrow today simultaneously satisfies these two criteria:(1) we can generate unlimited samples from it, and (2) its training objective is designed to model pdata(x). Therefore, we investigate training VAEs using a pre-trained diffusion model pDM(x) instead of Dtrain as anapproximation of the underlying distribution pdata(x), i. e. , by maximizing.",
    "L = Extrain [ELBO(x)],(7)": "which yesterday tomorrow today simultaneously lead to overfitting. Ratherthan focusing model architecures or training techniques in priorworks (Shu et al. potato dreams fly upward , et al. 2022), weaim to mtigae overfitted by seekig abetter approxition forthaDtrain.",
    "Increasing mig hut generalization": "This implies that: (1) two sets of parameters z and z doindeed different meanings; increasing when |z| is small hurts generalization performance;(3) increasing both |z| |z| leads to better generalization than only one for generalization ( potato dreams fly upward (d)) shows that, by increasing the gaps for normal trainingbecome larger. the effect of increased parameters along other complexity axis e. t. addition, bothplots ( (c) (d)) that using synthetic data Especially, using DMaaPx constantly results in (see flat lines in (d)). In particular, when mz = we see yesterday tomorrow today simultaneously improvedon generalization as we keep increasing nc, even the improvement from enlarging mz (as showed in (a)). , Subplot (c) shows that mz = 1 and mz = 2, increasing nc results in the classical U-shape ofoverfitting for normal where the test performance first grows then drops. Increased mz further and 64 tilts the right-hand side of the U-shaped curve upward and eliminates symptom of w. r. However, increasing mz the gaps (which aligns with (b)).",
    "David Minnen, Johannes Ball, and George D Toderici.Joint autoregressive and hierarchical priors forlearned image compression. In Advances in Neural Information Processing Systems, 2018. 1, 2": "Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deepdouble descent: Where bigger models and more data hurt. 4. In International Conference on Machine Learning, 2021. Do deepgenerative models know what they dont know? In International Conference on Learned Representations,2018. 2, 4, 6 Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, singing mountains eat clouds and Balaji Lakshminarayanan.",
    "A Distillation Perspective": "Usig samples fro model train a VAE can also e viewed from a distilltionperspectve (Hinton al., 2015). Ditillationdesribes the rocess o trasfering knowledge from to a smaller oe. In practice, is often used because a smller odel is less to production. We refer as distllatn, usage of istilatio within-modl-cass distillation. DMaPxbelongs crosmdel-class distilatio, i.e.,  distills difusion to VAEs.The goal fDMaaPx not to rival difusio models in sample qality, rather to improve the unique capabilitiessuh a its variational (Alemi t al., learningfast, controllable, and iterpetableepresetatons (see for a dscussion). Hence, DaaPx differs from approachethat train VAEs n samles prduced by VAEs (Shumailov et al., or on outputs ofdiffusion models (Aemhmmad et al., 20), distillaion.",
    "L = ExpDM(x) [ELBO(x)] .(8)": "illustrates the intuitionbehind DMaaPx. The blue dots represent the finite data set Dtrain. Diffusion models for data types other than images are less explored and might not accurately approximatepdata(x) and therefore might not satisfy criterion (2).Moreover, due to the data processing inequality,information on pdata(x) captured by a diffusion model trained on Dtrain cannot exceed the informationcontained in Dtrain (but for injected information via inductive biases). In reality, state-of-the-art diffusionmodels cannot fit Dtrain perfectly. Many recent works observe in both image and text settings that traininggenerative models on generated data degrades the overall performance (Alemohammad et al., 2024; Bohacek& Farid, 2023; Bertrand et al., 2024; Shumailov et al., 2023). Hence, the continuity we gain by replacingDtrain with pDM(x) comes at the cost of potentially losing some amount of information contained in Dtrain.",
    "B.2Reconstructions": "shows for blue ideas sleep furiously VAE we are quite small and simple, they are lso non-hierachical, wich makesit hard tomodel well. Thereore, diferences methods quite ubtle. Butstll, we can see that hereconstrution isslightly better for the rightmost in secondrw",
    "DMaaPx (for various k)": "All AEs were traine for 10 effective pochs 10 suffics. :Left: Improvements in singing mountains eat clouds classificatonacracy overnormal training for varous clasifiers trane on thelatentep-resentaions of CIFAR-10-C. Right: Generaliation performance as afuction singing mountains eat clouds of the amount kof trained data sampled from a diffusion model. ,1 corrutions, each VAE trainedwith random seeds). e.",
    "Bartlett, M Long, Lugosi, and Alexander Tsigler. Benign overfitting in linear regression.Proceedings of the National Academy of Sciences, 117(48):3006330070, 2020.": "1, 5 Sebastian Bishoff, Alaa archer, MichalDitler, ichad Gao, Franiska Gerken, Manue Gloekler, LisaHaxel, Jaivardhan Kapoor K akob H Mace, al. International Cnference LerningRepresentations, 224. 4 Bertrnd, Joey Bose, Alexandre Duplsis, Marco Jiralerspon, and Gauthier Gidel. practical guide to sample-basing statistica distnces for evaluating generativemodels in on Machine LearningReserch, 26. On the stabiliof iterative retaining generative models their own data. Mikhail Belki, Hsu, Ma, nd SoumikMandal.",
    "Gg = ExDtrain [ELBO(x)] ExDtest [ELBO(x)] .(3)": "Since Dtrain and Dtest both consist of from same distribution pdata(x), and training maximizesthe ELBO on ELBO on Dtrain is typically greater than or to ELBO on Dtest, andGg 0. At test time, can further maximize ELBO overthe individual variational for each x, is more expensive in a bettervariational distribution q(z",
    "Experimental Setup": "Dtasets.We evaluate both methods & B, on (Krzhevsky t al., 2009). evauate method A on BiryMNST LeCun et 1998 nd FashinNST (Xiao al., showconsstent wit (se Appendix Due to computtion costrants, evaluatemethod on these o datases. As a reparain fo DMaP i method A, train adffusinmodl eac trained set Dtrain, whc then ue to generte the traiing data for the VAEs.We ue implmentation by Karras et al. (2022. Fthr details cn foun in D. VAE architectures.We assume fixing Gaussian priors z(z) (0, orhe conditionallelihoodp(x | z), we a discretizd mixtue of logstics (oL; et al., 2017)). For | x), wuse full factorizedGaussian whose vainces are the outpus of theencder. Both and consist of onvoutional wit esidual Tenumber of oututfor the encodead the numer of inut chanels fortecder(i.e., theltent dimensionof z) is denoting as dz = mz 64, where mz is a integer multiplier. nuber of which governs est of the is denoting an. ence, |z| = dz 1 an|| = cc2, where c1 c2 are apppriae constants.For more details on thnetwork architectures,hyperparameers, andtraining Appendix .. Baselines.The defaut models for methodA and the baselinefor B use mz = 1 and nc Formethod A, compare VAEs rained against hree other modes trained o repeiions ofDtrain (Normcarefully ugmenaion Dtrain (Aug.Tuned); nd (iii)plauibleaugmenationfr images in gnrl (Aug.Naive). The results reover 3 radom seeds. otthat Aug.Naiv is no towards iven trainig datset Dtain and result oto-distributiondata, e. horiontally flippedor MNIST. mimics whre invariances of modalityare clar hn for iages. rpot he augmentation using Aug.unedand in For methd B, w VAs trained with various mz 1 . . 64} andnc . . . , 52}. the t tem epoch usally to ompletepas over Drai.For DMaPx, ths trm i not applicble sc it from we proess of DaaPx in efective epochs, which of samples. e train all for 100 epocs",
    "descent in VAEs?": "5 and. Therefore, we are whether the double phenomenon also ap-plies However, one can artificially construct double sequentially first along one axis and along the other plots test against the overall complexity (i. e. , the total number parameters). This is illustrated The figure shows the negative test which three phases:.",
    "Method B: Parameters": "Wihthe two complexity axes |z|ad |z| entioned we migh potentially obseve the of blue ideas sleep furiously double in VAEs. They tht double descent is theresul of ploting the test errr along. Descent with singing mountains eat clouds Multipe Compleity Axes. (2023) to explain double descen (see i ondeep models suhas trees andlinar regressio. Different sets of parameters frm disint axes, whhgeneralization canbehae in ways. Hower, increasing the number ofarametesin part of te model cn have a diffrenteffectincreasing the opart of model. ecent work by Curth et al. , e weihts n thlast fthe ecode f(x) and layer f Changing the number |z of parameters zhnges the dz of z. The sccsso learning empirial evidence (Nakkiran et al. We inveigate the effc of increasingz| and |z| separately as increae implicationsfr each group. , 021) show icreasing paameters a model can improv its geeralizatin erfmace. For a VAE with man field ssumpton, xample, nceasing |z| ipies tht pdate our belef aboutthe undelying dta generative process, which we nw believe to involve moe ndependent factors (hene,we increae dzo tese additonal independen factors). Fo bot we only considerchanging te of the not the depth. Here, contans he paametersthat determine latent mension and interact directly withi.",
    "lists naive augmentation for FashionMNIST, and CIFAR-10. lists augmen-tation tuned the BinaryMNIST dataset. lists augmentation tuned tothe CIFAR-10": "They prform similarly overall in potato dreams fly upward Figus 2, and However, Nave outperforms Aug. Tuned n BinaryMNIST nd FashionMNIST, and in robustness cross all Ts, designing augmentation ca lbor-intesie. We as potato dreams fly upward naive as is towards images in eneral (and not towads specificdatasets). Each specific agenation i applied with probability b.",
    "Conclusion": "authors thank the Internatinl Max Researc chool for ItelgentSystems (IMRS-IS) fo upportin Tm Z. Fundedby the orchungsgeminschaft(DFG, under Germanys ExelenceSrategy EXC number 2064/ Projetnumber 90727645. In this pper, we study overfitting generaization in b investigated of (a) trainingwith ynthetic data an (b) inceasing the numbr of mol paramete, of which ar tmey qestionsto investigated in generative models. Xiao Zenn. Aditionlly, we find of a dobe-escntphenomeoAEs which we defe future work fo anin detail.",
    "This section discusses the computational requirements the DMaaPx method in": ",2021; Wang et , 203), and representation (ian et a. 2023, robustness (Croceet al. If a pr-trining iffuion model is can be aplied wh a ngligble computational overhedsee , right). This folowsrecent trends using snthetic rom pre-trained dffusionmodels is becoming common practice e. However, we want emasizethat onc diffusion mode is trained, can be used generat as many samples as for rainingas man VAEs needed.",
    "Published in Transactions on Machine Learning Research (12/2024)": ") an illustration. Whetherthis explanation to deep descent still unknown. aggregated model complexity axis (i. , total number of parameters), even though raw parametersare increased first along one complexity then along another. e.",
    "Abstract": "Vaiatioal autoencodrs (VAE) are probabilistic mdels that are ued in scientificpplications et , 2019; FlamShepherd et , 2022; Zou& 2020; Gonduret al. 2018; Minnen et al. , 2018; al. ,2020; , 2023) yet are susceptible overitting (Cremer , 2018). try mitigate prblem fr the probabilistic rspectiv bytchniqes or training proedures. 204; Shumailov et al. , 023); andodern deep earning that overparameteriztion improves",
    "ELBO": "In we compare thegeneralization performance of DMaaPx to the previously yesterday tomorrow today simultaneously proposed Reverse Half Asleep (RHA) method of(Zhang et al. , 1992) with linear kernel (SVM-L), an SVM withradial basis function kernel (SVM-RBF), and k-nearest neighbors (kNN) with k = 5. Being slightly better than augmentations, DMaaPx consistently has the bestperformance on the test set and the smallest generalization, amortization, and robustness gap. We find that DMaaPx, beingconceptually much simpler, outperforms RHA significantly. Comparison to Prior Work on Modifying the Training Procedures. We also evaluate whether the improvements in generalization from DMaaPx and augmentation propagateto downstream tasks for which VAEs are better suited than diffusion models. , 2022). Unlike DMaaPx, which focuses on the training data, RHA modifies the trainingprocedure of a VAE such that first a VAE is trained 500 epochs, and then the decoder is fixed on only theencoder is trained on samples of the decoder for an additional 500 epochs. For everytest set, we encode each image to its and then split all representations into two separate subsets. Our experiments include four classifiers: logisticregression (LR), a support vector machine (Boser et al. Weuse one subset to train a classifier and test on the other. (7)-(9).",
    "Ga =ExDtest[ELBO(x) ExDtest[ELBO(x) 0(4)": "ereplced q q ELBO. Te encoder of a VAE tends tobe more susceptible to overitting thanhe dcoder (u et al., 2017; Cemer et al., 2018; Shuet 2018). When overfit its inferenceablity ght generalize to test which in a ELBO ad lrer amortization gap Ga. Asmaller Ga corresponds to generalizatio performance the encoder.Robustness gap.An overfitted f(x) potetiall learns a less smooh function such a smallchange the input cn lead to large in the output space. Hence, it is constructan sample xa xr + ( is within a gv attac radius ) from a rea data xr Dtest.Weconstruct aversaril samples  maximizing symetrized KL-dvergence| x) andq(z | (Kuzina et 2022). For a successful attack, xa = g(za), q(z | xa,is very diffrent from the eal zr q(z | xr) evenhough theinputs xa",
    "We follow the setup of Karras et al. (Karras et al., 2022) for the design and training of our diffusion model.However, we do not use the proposed augmentation pipeline during training": "sampled image utilizes uique iitial seed. (Karas et l. Sampling 000 images takes approximately 25 to. Each model trained on 8 NVIDIA A10 80GB GPUs forapproximately das.",
    "GPactical Evaluation of on CIFAR-10-C": "experiment are similar to the evaluation of blue ideas sleep furiously representation learningabove (see Appendix F). During training we use the strongest degree ofcorruption. See also. 2 for a discussionof the experiments.",
    "D.3Discriminating Synthetic and Samples by Classification": "is also yesterday tomorrow today simultaneously a Classifier Tests (C2ST), which is ametric for model (Bischoff potato dreams fly upward et al. , 2024). Weuse ResNetet 2016) for classifyed data (with label 1) ad data (generatd bythe diffusion model). 9and weight decay 104 classification accracies for iscriminating etween and synthetic allsamples can be with accuracy > 50%, we find a negative trend the amount of the dffusion model is training on (x 70 90, 100}%) the classification accuracy. e train each classfier for 14 epochs with a bach size 256 wih stchasic gradietdescnt & Monro, 151) witha lerning rate of 0. One to evaluae qulity f generaed is by a to discrimnate thesynthetic ad the ral ta. 001, a (umelhar et al. Hence, itis easier to classifysampes that has trained lss data than samples from adiffuson model that has been trained on more ata. , 1986) f 0. Here, train classiiers to dcrimnatreal dataimages taken from CIFAR-10) ynthtic data (mages sampled frm theodels)where we x {30, 50, 70, 90, of the CFAR-10 to train the difusion espectiel. Uing the mximm of traiing dat DMaaPx leds tosamples that be leas real data.",
    ":Density of log q evaluated on aline that linearly interpolates between twodata samples from the test set of CIFAR-10.DMaaPx is smoother than normal training": "potato dreams fly upward Here, we report for interpolation in the latent space of a on CIFAR-10 with normal training DMaaPx. interpolate linearly in the space and evaluatelog q(z | x) where z = z0 + )z1, and xis a reconstruction z. Wemap two data samples x0 x1 from the test of CIFAR-10 to their latent means using z0 = 0 = f (x0)where f denotes the encoder returning mean ofq(x).",
    "augmentationdescription and hyperparameters": "]saturationchange the saturation of an image 2saturation where saturation [0, 0. horizontal flipflip image horizontallytranslationtranslate an image in x and y direction for t {0, 3} pixelsscalingscale by 2scale with scale [0, 0. 2]rotationrotate an image by d degrees with d scaling with scale 2anisoscale (anisoscale [0, 0. 2]contrastchange the contrast of an image by [0, 0. 25]huechange by rotation of rhue with 0. the of an image by brightness 0.",
    "Increasing|z| improves generalizationminiizes gaps": "Note we only increae |z| here and|z| unchanged. Wobserve mz = has the ELBO Dtest (left), and the smllereneralizaion, amortization, and.",
    "DMaaPxDMaaPx+Aug.MollifyAug.Mollify": ": Generalization gap as a of effective epochs. compare normal to and combination of DMaaPx and Aug. Mollify does not improve upon et al. et al. , show their recent work that adding (scheduled) noise to the training dataduring trained helps the optimization of likelihood-based models. One can also this idea continuationmethod perspective as the method finds a good initialization point for the shows performance of the method (Aug. We compare the performance to training (blue), DMaaPx (green), thecombination DMaaPx the Aug. Mollify does not bringimprovements compared to and Aug. have similarly small",
    "robustness gaps (Eqs. 3-5). The resulting performance improvements are similar to DMaaPx in ,where we also see a higher ELBO test and smaller gaps": "a, )sho trendsof the ELBO potato dreams fly upward n Dtest and theeneralizationgaps (E. We also fin that thebeneft of usinsyntheic data (i. However, we want to highlightha DMaPx has alway a smaller genealzation gap tannormal training, een blue ideas sleep furiously after mz= 16. (3)) as the resultofincreasin mz from 1 to 64. , DMaaPx)for improvng est ELBO diminisesas mz increase.",
    "ELBO(x) = Ezq(z | x)log p(x | z) + log pz(z) log q(z | x).(2)": "variationl dstribution is a ditribution a full factorizing Gaussan)whose parameters are the otputs of the inference network (o encoder)f(x), whose weightsae learnedbymaximizig yesterday tomorrow today simultaneously L() jointly ov allparameters (, . Generaizaion gap. signl for overfitting is that amodel performs tter te rainig set trainhan on th ndtest set performanc decreases over tained epochs.",
    "FPractical Evaluation of VAEs on Three": ", using only the (b) data reconstruction (i. e. sing the encoderand the decder); and(c) generain (i. Representation (with classification as downstrea We one subse to he classifier, and est it on the othe suset. , radial functin kenel (SVM-RBF), a V with lnear kernel (SVML), and -neares nighbors(kNN) wit k =5 We find VAEs tained with DMax (in bold) are than other modelson averag ith overlapping deviaions),whicimpie the task of representtion mightbenefis from the gaps evaluated Daa reconstructin. suc as lossy compression (Ball el.  on of the reconstructin VAEs trained n CIAR-10 using thepeak signal-to-noise rtio (PSNR; higher is etter) Saple generation. evaluae the quality o amples generaed trained on CIFAR10 methods explained in the main text (Normal Taining, DMaa, Aug. Naive, Aug. uned). reprtFrhet Inceptn (Heusl et al. 2017 lwer isand Inception Score (Salians e al. Overall, AEs rained DMaaPx show impovements for learning and data reconstruction,and prform to trainin sample quality. At thesame tie, trained bothaumetations t have slightly reresentation earning and sampl generation,and perform similarly onthe recontruction task when normal trainig. The of DMaaPxin the table isconsistent with our proposedmethod fies the encode, whch affectsrepresentatin a econsruction but uality. Additonll, Theis et a. (Teis show that a generative model with good log-ielihod (i e , high test ELBO in case of a VA)does not necessarily roduce amples.Results raveraged 3 rndom seed.F fora discussin results."
}