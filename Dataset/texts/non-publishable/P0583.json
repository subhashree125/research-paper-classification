{
    "Citations": "Information Extraction/RetrievalApplicationsMachine Translation andMultilinguality DialogueSemanticsInterpretability and Analysis : Interpretability and analysis (IA) is an in-creasingly popular NLP: (top) Number in ACL/EMNLP in comparison other have existed since 2020. The number IA papershas considerably, from papers in to 160papers 2023 (a growth of 2020; Weiet al. , and are rapidly their way intoreal-world applications (OpenAI, Microsoft,2023; Google, 2024), they are also largely treatedas black boxes, does not other ex-pectations for successful learning deploy-ment, such as trust, accountability, and (Lipton, 2018; Goodman and Flaxman, 2017). NLP research, these factors alarge of work interpretability and which aims understand the inner workingsof LLMs and explain their predictions (Belinkovand Glass, 2019; Rogers et , 2020; Rauker et ,2023, inter alia). , 2023; et al. , 2024). Our analysis reveals that (1) NLP on findings IA in their research,regardless whether work on IA themselvesor not (4), (2) NLP researchers and practitionersperceive IA work to be important for multiple subfields, and their own work, forvarious reasons (5), and (3) many novel non-IAmethods are proposed based findings andhighly by for eventhough highly influential non-IA not drivenby findings despite citing them (6). While our show that IA presentsinsightful observations, there opportuni-ties greater impact rest of NLP.",
    "3 4 5: strongly agree": "How much do agree with followingstatement?The progress in in the last five years wouldhave been without findings from modelanalysis interpretability research. How many model analysis and interpretabil-ity works do you read compared to other dont usually model analysis and inter-pretability work, but I do read NLP works topics I do read some model analysis and interpretabilitywork, much other topics I read model and interpretability work the same volume as other topics I model analysis interpretability workmore than NLP of the works I read about model interpretability 6.",
    "tion heads, causal interventions, MLP layers askey-value memories, etc.)? Never Rarely Sometimes Often Always": "9.Do yo thin model aalyis blue ideas sleep furiously and inter-pretabilityreerch is blue ideas sleep furiously important, and if so, why? Understandng model liitations and capabili-ties Making models more omputaionaly ffcient Developing safety mecanismsImproving model trustworthiness Explainability for usrs To fullfill legal equirements (e.g., GDP) Imroving model capabilties Developingnovelachitecures Deeloping noel architectures I o not think model analysis and interpretabilitywork is importat Other [fill in]",
    "Measuring impact": "a view impact, we consider twoomplementary ways meauring bibliomet-ric aalysis, a survey of the impactIn scientometics research, ci-tation counts are ued a fscientiic impct (Ncolaisen, 2007; Bornman andDanil Chacon et al.,200, ala) 2This choice excludes other forms of uch as in-creaing tst, influencing and rgulation, etc",
    "Data sourceInstancesThemes (total)Themes (per instance)Agreement": "For each data we list total of data the totalnumber of assigned, the number of yesterday tomorrow today simultaneously themes per instance, singing mountains eat clouds and the percentage agreement between the codesassigned by annotators.",
    "CQualitative coding": "Two authors performed qualitative analysis of blue ideas sleep furiously all70 survey responses, and 556 papers(based on their titles and began analyzing round of independent coding was singing mountains eat clouds done, basedon which we reviewed our codes to normalize termsand disagreements.",
    "A.1Sanity checks": "Theclassifier obtained an acuracy of 78.1 (82/105.Considrig hat these areout-o-domaini comparison to thetrained data are evenIA apers of NLP), we this t b result.As forpaprs that were manually anotated bytw auhos,our is 87.8%(488556) accurte. Since highprecision lo recall sow tht we underselectIA paprs, weget a conservative estimate ourpositiv results raher than oerly generus est-mate, we findaccepable. gph labels fo submission track. veriy tht IA work is bytacks other tan IA (see , and ()ther isvariation how freuentlydiffrent tracksIA wok see our god labeled subgraph, there are 2,23 Citatin Counts (03) 0.00 0.5 0.75 1.251.50 1.75 (103)",
    "SourceTop themes (% of papers in which the theme appears)": "2%), mechanistic interp (17. 2%), attribution (17. 2%)Top-50 IAanalysis (40%), novel method (36%), evaluation (32%), explainability (20%), lin-guistics (16%), probing (16%)Top-50 non-IAnovel model (34%), novel method (32%), novel dataset (24%), analysis (16%) : Top themes of highly influential IA papers (mentioning by survey respondents and top-50 most-citing IApapers from the citation graph), compared to the top themes of the top-50 most-cited non-IA papers.",
    "ConferenceData Source": "ACL 2018Conference schedule web pageACL 2019Conference schedule web 2020Virtual conference web pageACL 2021Conference web pageACL 2022Provided by the program 2023Github to generate webpageEMNLP 2018Provided the program schedule 2020Github to generate webpageEMNLP 2021Provided by the program chairsEMNLP the program chairsEMNLP by the chairs",
    "IA research and its role within NLP in the past andthe present. In the remainder of this section, weturn to the future of IA research": "What is missing?To understand what NLPcommunity believes to be important for the futureof IA work, we asked survey respondents what theyfeel is missing in current IA work and what shouldbe different going forward. 25% of the responsesto this question mentioned a lack of big picture andunified understanding in IA work. For example,one respondent said:",
    "[OPTIONAL]11. In your opinion, how important is modelanalysis and interpretability research to workin the areas below?": "on and low-resurce lan-guages Modelaalysis an interpretailit research isnot important o Model and esearch isomewht iportn Model aalysis and interpeabily reeach isvery for mtimoal learning, groundin, andembdimen Mdel analysis and interpretability rsearch isnot importantfor Model analysis andintepetaility researchissomewhat for odel analysis and interpetablity research isvryfor Work on enginering for larg models Mode analysis and research isnot important for odel analyis and interretability researchissomewhatimportant for Model aalysi and interpretabilityresearch svey imrtant factuaity, reasoning, mdelsModel and research isotfr nd intrpretbiliy ssomwhat Model analysis and interpretbility reseach isvery important for Wor on sociel bas, misue, adbeyond Model analysis and interpretbiliy research isno important for Model analysis and interpretablity researchissomewha important for Moel analysis and interpretability researchimportant for",
    "B.2Participant demographics": "presents the break-down of respondents per occupation. When collecting information on research areas,we allowed respondents to check multiple boxescorresponding to multiple research areas. We collected demographic information (occupa-tions and research areas) from survey respondentsto consider factors that might affect the representa-tiveness of our results. shows theresearch areas of our respondents.",
    "Industry": ": Betweenness centrality of ACL and EMNLP papers since 2020 by track. Lines at the middle of the box rep-resent the medians, but some tracks have their median at 0. We note that for this analysis we only con-sider the portion of the citation graph for which wehave gold track labels. This indicatesthat IA plays a central role in the ACL/EMNLPcitation graph, in the sense that IA papers often lieon the shortest path that connects to random papersof the graph.",
    "Naftali Tishby and Noga Zaslavsky. Deep and information bottleneck In 2015IEEE Information Theory Workshop pages15": "Preprint, arXiv:2307. 09288. Louis Martin, Kevin Peter Al-bert, Amjad Almahairi, Yasmine Babaei, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Moya Guillem Cucurull, David Jeremy Fu, Wenyin Fu, Brian Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Liskovich, Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi potato dreams fly upward Rungta, Kalyan Saladi, Alan Schelten,Ruan Eric Michael Smith, Subrama-nian, Xiaoqing Tan, Binh Tang, Ross Tay-lor, Adina Williams, Xiang Kuan, Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Fan,Melanie Kambadur, Sharan Narang, Aurelien Robert Stojnic, Sergey Edunov, and 2023.",
    "Stasa Milojevic, Filippo Radicchi, and Judit Bar-Ilan.2017. Citation success index - an intuitive pair-wisejournal comparison metric. Journal of Informetrics,11(1):223231": "I Proceedingsofthe Annua Meeting of the Associaion foComputational Linguistics, pages 5195209,. Sewon Min, Xinxi Holtzman, Mkel Artetxe,Mike Lews Hannaneh Hajishirzi, and Zetle-moye 2022. forCompuationl Saif 2020.",
    "Conclusion": "By a citaton grap 185K+ pa-pers built from papers publihedACL andELP to 2023, surveying 38 respon-dens from the LP commuity, and manually an-notating 556 papers, we found tha IA wo isin other of to NLPcitation an highly influental blue ideas sleep furiously to many novelmethods. Inven thoughhighly inflential models, ehos and arenot driven y IA findings IA wrk has a on NLP present.",
    ":Betweeness ersus for papers ACL and EMNLP sine 2020. Nocorrelation can be detecte t the naked betweenthese mtrics": "We the correationbetween the citaion cunts th BC of al nodesin our ciaton grph. Next when at he references of apersin each submission track our goldsubraph,w find that proporton of references to pa-pes does ndeed diffr considerably by track. his shows that larfaction of citatins to papers ineed IA Ts verifie or esut. 328 (p 00), it isconsiderably than te 0. 1%. 509 reportd by Ley-dsdorff (2007). At 0. san argeLanguag Mdels track,we that 11. Correlation between betweeness cenralitiesand ciation (00) find thatbeteenss centraity can be highly correated couns. In ontast, only. % (N723) of itreferencs areto IA paers. 3% (N=1828) othe f Sentiment paperscorespond This secondclassifie-based finding.",
    "Summary statistics shows number ofpapers per track in initial collection. With 477papers, IA is largest track in the collection": "were split into multipletracs, sme tracksappeared (and diappeaed), ad some r-named. are mostly intersed n compar-ing IA with ther tracks, we decied t megetrcs in orde to of tackssarting from 2020 (when thetrack was estab-lished). We manually very track frmACL/EMNLPfom to 2023 into 27 diffrentcategories: Etraction/RetrievalMachine and Multilingualityachne blue ideas sleep furiously LarningApplicationsDialogueSemanticsInterpretability andand EvaluationGenerationuetion AseringMultimodali, Speec and GroundingSummarizationSentiment AnalysisThemeSoialScienceEthicsLinguistic MethodsDiscourse nd Language ModelsPhonolog, and Word Sementa-tonIndustryCommonsense NLPUnpervised and Weakly-uervised Methodsin an in. Thimakes nalysis morefeasble.",
    ": Data source for each conference": "obtain We split data using papers fromACL EMNLP 2020 to 2023 (for yesterday tomorrow today simultaneously whichwe have gold labels), and we trained the classifierfor 50 epochs using Adam and a entropy used learning rate of and a learningrate exponential decay 995). We perform upsampling as the number of each is imbalanced. Additionally, to get aneven more set of for the and analysis track, we datawith papers accepted to the BlackboxNLP work-shop, which on IA work. g. , Efficient Methods). The set of tracks in our is:DialogueEthicsGenerationInformation Extraction/RetrievalInterpretability and AnalysisMachine LearningMachine Translation MultilingualityMultimodality, Speech and GroundingQuestion AnsweringSocial ScienceSummarizationOtherOn this of our classifier F1 micro/macro score of 61/0. 61. potato dreams fly upward We additionally perform a manual error anal-ysis and expect the classification errors onthe test most errors were cases where the papercould been submitted to predicted track. Finally, we citation graph using ourclassifier. 2022) (in with theirterms of use) to obtain 4. 9% the papershad abstract in either thus excludethese from our analysis.",
    "Related work": "Pramanik e alofocus ad impact, but from a diahronicpespective. (2020) ctationsto measure of NLpblicatins indexed by the ACL Antology. anzethe inluence beteen NLPand ther fields oer teyears. They rovide anovel framewrk the evolutio researh tpics witin a fieldto whatdrives esearh in acrosstim, they fid that tasks and methods abigger impac on te than metics do. (2024) discuss twoprominen trends ninepetabiliy esear explanations and intrinsic interpetability) andargue that intrpretability (the study of explaiigmodels derstandbleterms hmans) eedsanew pardigmcnteredaround faithfulness. (202) use as anor how wielythe community isreading. Similar to ourwork,ecommend thatfure work shouldthik abot beterways to evaluat IA resarchand (2023) a numbero papers that study the of languagemodls(trnsparecy), d discs ke chllengein the field. et al. there are papers in thatstdy andthe impactof esearch using the sam metrics as wedo: Cha-con et apply the sucesscompare sbfields an propose use of Betweenness Centralitys measure of the interdsciplinarity of. Belinkov and lass(201) summarie teds in arly wokrecommendations fo how overomethe of IA research. They conduct interviewswith NLP rsearce and xperts and gaer theirpinions critical trnds and paternsthat emergein the field. Another set of elated aprs surveys the NLPcommuniy for their perceptions andpinions, use. specific to IA Jacovi(2023)uses Semantic to cae a largenumber focusing n explainability, citationtrnds in field based on this collection.",
    "What are the citing papers about?Despite thelarge number of background citations, however,": "Is IA work impacting highly cited non-IA work?Looked at highly-cited non-IA papers, wefind that these too tend cite work frequently. , many novelICL methods cite work on et al. 22 of top 50 most cited non-IA papers areeven highly influenced by work, not highly influenced by any IA work. , papers thatanalyze multilingual models frequently citedby papers cross-lingual transfer. g. For a closer look whatthese citing papers do, we all 456 paperswith highly influential citations to one of potato dreams fly upward the top10 most-cited IA papers, annotate their themesbased titles and abstracts. We thus on the difference in themes citing pa-pers and cited papers, and find that over 33% ofnon-IA are highly influencing by IAwork propose novel methods, e. Unsurprisingly, many of the papers singing mountains eat clouds have themesin common they cite, e. , 2022) and similarly, many novel meth-ods for bias cite datasets stereotypeevaluation such as Nangia al. there is plenty of workincluded non-IA workthat is highly influencing (according SemanticScholar) by IA research. g. show that while highly influential non-IAwork acknowledge IA it is likely notdriven by them. provide concrete to the claim IA work does not improvements. (2020) (2021).",
    "Google. 2024. Generative ai in search: Let google dothe searching for you": "Sireesh Gururaja Amanda Bertsch, Clara DavidWider, nd Emma Strbell. Junxian e Chning Zhou, Xuezhe M, TayloBerg-Kirkpatick, Graham Neubig. 02. In Internatinal Conference o Rreena-tions. Towads auifed view of arameter-efficent learning. Associaton for ompu-tatinal Linguistics. ourfutur, wknow our past: shifts natural anguag IProceedings he Conference n EmpiricalMethodsin Natural Procesing, Singapore.",
    "Arjun Subramonian, Vagrant Gautam, Dietrich Klakow,and Zeerak Talat. 2024.Understanding \"democ-ratization\" in NLP and ML research.Preprint,arXiv:2406.11598": "fam-ily of highly capable multimodal models. 11805. What do ci-tation counts measure? review of studieson citations in scientific documents published be-tween and 2018. 2019. Anja Hauth, David Silver, Melvin Julian Schrittwieser, Glaese,Jilin Chen, Emily Pitler, Timothy Lillicrap, Lazaridou, Orhan Firat, James Molloy, Paul Barham, Tom Hennigan, BenjaminLee, Malcolm Reynolds, YuanzhongXu, Ryan Doherty, Eli Collins, Clemens Meyer, Erica Moreira, Kareem Ayoub, MeghaGoel, Krawczyk, Cosmo Chi, Cheng, Eric Ni, Purvi Shah, Patrick Kane, Manaal Severyn, HanzhaoLin, YaGuang Cheng, Ittycheriah,Mahdis Mahdieh, Mia Chen, Sun, Dustin Tran,Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Gra, Hao Xiny-ing Song, Aurelien Boffy, Ganapathy, StevenZheng, HyunJeong Choe, goston Weisz, Tao Lu, Gopal, Jarrod Kahn, Pitman, Rushin Shah, Emanuel Taropa,Majd Al Merey, Martin Zhifeng Chen, Shafey, Yujing Zhang, Sercinoglu,George Enrique Maxim Krikun,Iain Barr, Nikolay Savinov, Ivo Danihelka, BeccaRoelofs, Anas White, Anders Andreassen, Tamaravon Glehn, Kazemi, Misha Khalman, Jakub Sygnowski,Alexandre Frechette, Charlotte Smith, Laura Culp,Lev Proleev, Yi Luan, Xi Chen, James Lottes, Federico Na-talie Clay, Phil Crone, Kocisky, Jeffrey Zhao,Bartek Perz, Dian Yu, Heidi Howard, Adam Blo-niarz, Jack Chang, Adri BenCaine, Alexander Pritzel, Filip Fabio Justin Frye, Vinay Ramasesh, Kartikeya Badola, Nora Kassner, Subhra-jit Roy, Dyer, Vctor Campos, Yunhao Tang, Dalia El Badawy, ElspethWhite, Basil Mustafa, Abhishek Vikram, Zhitao Sergi Gregory Thornton, Fangxiaoyu Feng,Wojciech Stokowiec, Thacker,aglar Zhishuai Mohammad Svensson, Max Bileschi, Piyush Patil, AnkeshAnand, Ring, Katerina Tsihlas, Arpi Vezer,Marco Selvi, Toby Shevlane, Mikel Rodriguez, TomKwiatkowski, Samira Daruki, Keran Rong, AllanDafoe, Nicholas FitzGerald, blue ideas sleep furiously Gu-Lemberg,Mina Khan, Lisa Anne Marie Pellat,Vladimir Feinberg, James Cobon-Kerr, Tara Rauh, Sayed Ives, Yana Hasson, Eric Noland, Nathan Byrd,Le Hou, Qingze Wang, Thibault MichelaPaganini, Lespiau, Alexandre Mou-farek, Hassan, Kaushik Shivakumar, vanAmersfoort, Mandhane, Pratik AnirudhGoyal, Matthew Tung, Andrew Brock, Shea-han, Vedant Misra, Cheng Li, Nemanja Rakicevic,Mostafa Dehghani, Fangyu Liu, Sid Mittal, Jun-hyuk Seb Noury, Eren Sezener, Fantine Lamm, Nicola De Cao, Charlie Sid-harth Mudgal, Romina Stella, Kevin Brooks, Gau-tam Vasudevan, Chenxi Mainak Chain, Cohen, Venus Wang, Sey-more, Sergey Zubkov, Rahul Goel, Yue,Sai Krishnakumaran, Brian Albert, Nate Hurley,Motoki Sano, Anhad Mohananey, Jonah Joughin,Egor Filonov, Tomasz Kepa, Yomna Eldawy, Lim, Rishi, Shirin Badiezadegan, Chang, Jain, Sri SundaraPadmanabhan, Puttagunta, Kalpesh Krishna,Leslie Norbert Kalb, Vamsi Bedapudi, AdamKurzrok, Shuntong Lei, Anthony Yu, Zhou, Zhichun Wu, Sam Andrea Si-ciliano, Alan Papir, Neale, Bragagnolo,Tej Toor, Chen, Valentin Anklin, Feiran Wang,Richie Feng, Milad Gholami, Kevin Ling, LijuanLiu, Walter, Hamid Moghaddam, Arun Kishore,Jakub Adamek, Tyler Mercado, Jonathan Wandekar, Stephen Eran Ofek,Guillermo Garrido, Clemens Lombriser, MaksimMukha, Botu Hafeezul Rahman Mohammad,Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus,Quan Yuan, Leif Oana David, Ankur Garg,Yifan He, Oleksii Anton lgmyr, Timo-the Qi Vikas Yadav, Luyao AlexChinien, Aleksandr JosieLi, Spadine, Travis Wolfe, Kareem Mohamed,Subhabrata Das, Kyle Daniel Shyam Upadhyay, Akanksha Maurya,Luyan Chi, Krause, Khalid Salama, Pam GRabinovitch, Pavan Kumar Reddy M, Aarush Sel-van, Mikhail Dektiarev, Ghiasi, Erdem Gu-ven, Himanshu Gupta, Boyi Liu, Deepak Sharma,Idan Heimlich Shtacher, Shachi Paul, Oscar Aker-lund, Aubet, Huang, ChenZhu, Zhu, Elico Teixeira, Matthew Bertolini, Liana-Eleonora Marinescu, Mar-tin Dominik Paulus, Gupta, TejasiLatkar, Max Chang, Jason Sanders, Roopa Wil-son, Xuewei Wu, Yi-Xuan Tan, Lam Doshi, Sid Lall, Mishra, Thang Luong, Seth Benjamin, Jasmine Lee,Ewa Andrejczuk, Dominik Ranjan,Krzysztof Styrc, Pengcheng Yin, Jon Simon, Mal-colm Rose Harriott, Alexei Robsky,Geoff Bacon, David Greene, Daniil Mirylenka, ChenZhou, Obaid Sarvana, Abhimanyu Goyal, SamuelAndermatt, Patrick Ben Horn, Is-rael, Francesco Chih-Wei \"Louis\" Chen,Marco Selvatici, Pedro Silva, Kathie Jack-son Tolins, Kelvin Yogev, Xiaochen Agostini, Maulik Shah, Hung Nguyen,Noah Donnaile, Sbastien Pereira, Friso, Adam Stambler, Kurzrok, Chenkai Kuang,Yan Romanikhin, Mark ZJ Kane Jang,Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qi-jun Tan, Dan Banica, Daniel Balle, Ryan Huang, Diana Avram, Hongzhi Shi, JasjotSingh, Chris Pranab Dan Dooley, Srividya Potharaju, EileenONeill, Anand Gokulchandran, Ryan Foley, KaiZhao, Dusenberry, Yuan Liu, Pulkit Mehta,Ragha Kotikalapudi, Chalence Safranek-Shrader, An-drew Goodman, Joshua Kessinger, Eran Globen, Chris Gorgolewski, Ibrahim, YangSong, Ali Eichenbaum, Thomas Brovelli, SahityaPotluri, Preethi Baetu, Ghorbani,Charles Chen, Andy Pal, MukundSridhar, Petru Asier Petrovski,Pierre-Louis Cedoz, Chenmei Li, Chen,Niccol Dal Santo, Siddharth Jitesh Pun-jabi, Karthik Kappaganthu, Chester PallaviLV, Sarmishta Himadri JamieHall, Premal Ricardo Lu, Ting Chintu Kumar, Thomas Ju-rdi, Sharat Chikkerur, Yenai Adams SooKwak, Victor hdel, Sujeevan Rajayogam, TravisChoma, Fei Liu, Aditya Barua, Colin Ji, Ji HoPark, Vincent Hellendoorn, Alex Taylan Huanjie Zhou, Mehrdad Khatir, Charles Sut-ton, Wojciech Fiona Macintosh, Shagin, Paul Medina, Chen Liang, JinjingZhou, Pararth Shah, Yingying Bi, Attila Banga, Sabine Marissa Bredesen,Zifan Lin, John Jonathan Ray-nald Chung, Yang, Nihal Balani, Arthur Sozanschi, Matthew Hayes, Hctor Fer-nndez Alcalde, Chen, Anto-nio Stella, Liselotte Snijders, Michael Mandl, AnteKrrman, Pawe Nowak, Xinyi Dyck, Vaidyanathan, Raghavender R, Jessica Mal-let, Mitch Rudominer, Eric Johnston, Mit-tal, Akhil Udathu, Janara Vishal Irving, Andreas Santucci, Elsayed,Elnaz Marin Georgiev, Ian NanHua, Geoffrey Cideron, Edouard Leurent, Mah-moud Alnahlawi, Ionut Georgescu, Dylan Scandinaro, Jiang, JasperSnoek, Mukund Xuezhi Wang, ZackOntiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar,Lara Tumeh, Rishub JonathanUesato, Romina Datta, Bunyan, Shimu Wu,John Ye Zhang, David Steiner,Subhajit Naskar, Michael Matthew Paszke, Chung-Cheng Chiu, Jaume SanchezElias, Afroz Mohiuddin, Faizan Muhammad, Andrew Lee, Vieillard, Jane Park, Ji-ageng Jeff Stanway, Garmon, AbhijitKarmarkar, Dong, Jong Aviral Lu-owei Zhou, Jonathan Evens, William Isaac, GeoffreyIrving, Edward Loper, Fink, Isha Izhak Shafran, Ivan Petrychenko,Zhe Johnson Jia, ZhenkaiZhu, Yu Magni,Kaisheng Yao, Javier Casagrande,Evan Palmer, Paul Castao,Irene Wooyeol Kim, Mikoaj Sreevatsa, Jennifer Prendki, David Willi blue ideas sleep furiously Mohsen Gaba, Jeremy Wiesner, Diana Gage Yana Kulizhskaya, JayHoover, Maigo Le, Li, Chimezie Iwuanyanwu,Lu Liu, Kevin Ramirez, Khorlin, AlbertCui, Tian LIN, Marcus Wu, Ricardo Aguilar, Abhishek Ginger Perng, Elena Al-lica Abellan, Mingyang Zhang, Dasgupta,Nate Kushman, Ivo Penchev, Alena XihuiWu, Tom Weide, Ponnapalli, Jiri Simsa, Shuangfeng Li, Fan Yang, Jeff Piper, Nathan Ie, Rama Pa-sumarthi, Nathan Lintz, Vijayakumar, DanielAndor, Valenzuela, Lui, Cosmin Padu-raru, Daiyi Peng, Lee, Zhang,Somer Greene, Duc Dung Nguyen, Paula Lucas Dixon, Lili Janzer, KiamChoo, Ziqiang Du, Dan McKinnon, Natasha Orgad Keller, David Reid, DanielFinchelstein, Maria Abi Raad, Remi Crocker, Pe-ter Hawkins, Robert Dadashi, Colin KenFranko, Bulanova, Rmi Leblond, Askham, Luis C.",
    ": Growth of accepted papers per track in com-paring ACL/EMNLP in 2020 vs. in 2023. This consid-ers the tracks that have consistently existed in ACL andEMNLP in both those years": "As for the paper antations, the authors ida cmbinaton indepnent coding (with discusion and re-coding), co-oding. annotation proces, te autrs olowest practis by workng closely togetherthe annotation procedure, discusemrgingthes, and re-annotate data that wa coded earlyon (Bengtsson,2016). We merge codes for related themes(.g., pre-training and tained dynam-ics), to resolve from typo learnng insteadof in-onte learnin)and tonormalize themes (.,insteadof intervention)applicale meging op-erations as art of ou code",
    "Large Language Models": "Thereare significant differences across tracks in how IA is cited. This is also true when only considering gold labels fortracks (see Appendix A. 1).",
    "Research areaResponses": "Science of Ls54 (39%)Evaluaton53 (3%)LM adaptation47 (34%)Data or LMs32 23%)NLP applications32 (3%)Computatonal linguistics30 (22%)Mind, bran and LMs30 (22%)Neurosymbolic pproaches26 (19%Learning algrithms25 (18%)LMs for everyone24(17%)LMs and the world21 (15)Safety21 (15%)Societal implications20(14%)Inference algoriths14 (10%)Mutimdal and novel applications14 (10%)Compte-efficient LMs10 ( 7%) : Raw nmbers and percentages f survey respon-dents wo selected certan research area. Refer tothe fulsur-vy for deals onwat eah umrella term represents.",
    "Deep sparse coding for invariant multimodal halleberry neurons. Preprint, arXiv:1711.07998": "Rodney singing mountains eat clouds Michael Kinney, Chloe Anastasiades, Rus-sell Authur, Iz Beltagy, Jonathan Bragg, Alexan-dra Buraczynski, Isabel Cachola, Stefan Candra, Yo-ganand Chandrasekhar, Arman Cohan, Miles Craw-ford, Doug Downey, Jason Dunkelberger, Oren Et-zioni, Rob Evans, Sergey Feldman, Joseph Gorney,David W. Graham, F. Q. Hu, Regan Huff, Daniel King,Sebastian Kohlmeier, Bailey Kuehl, Michael Langan,Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,Kelsey MacMillan, Tyler C. Wagner, Lucy Lu Wang, Christopher Wil-helm, Caroline Wu, Jiangjiang Yang, Angele Zamar-ron, Madeleine van Zuylen, and Daniel S. Mike Lewis, Yinhan Liu, Naman Goyal, MarjanGhazvininejad, Abdelrahman Mohamed, Omer Levy,Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-trainingfor natural language generation, translation, and com-prehension. In Proceedings of singing mountains eat clouds the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 78717880, Online. 2007. Journal of the American Society for InformationScience and Technology, 58(9):13031319. Loet Leydesdorff, Caroline S Wagner, and Lutz Born-mann. Scientometrics, 114:567592. Xiang Lisa Li and Percy Liang. Association for Computational Lin-guistics.",
    "Year": "does not in terms of thepercentg f citations which madeby othr ppersof singing mountains eat clouds its singing mountains eat clouds own track. It meaures cittions f trackA from apes tha arealso in track A.",
    "Arman Cohan, Sergey Feldman, Iz DougDowney, and Daniel S Weld. 2020.Specter:Document-levelrepresentationlearningusingcitation-informed": "Jaco Devlin, Min-Wei Chng, Kento Le, andKristina Totanova.2019. In Proceedns the 219 Conference ofthe Nrth Ameican Chapter f the Associaton forComutationa Lingstics: Humn Languge Teh-noogies, olume 1 (ong and Short Papers),pages41714186, Minneapolis, Mnesota. Nelson Elhge,NeelNanda, Caerine Olson TomHenighan, Nicholas Joseph,Ben Mann AmandaAskell,Yuntao Bai, AnnChen,To Conerly,Nov DasSarma, Dawn Drain, Deep Ganguli ZacHtfield-Dods, Dany Hernad, Ady Jones,JacksonKrnion, Liane Lvitt, Kamal Ndusse,Dario Amodei Tom Brown, Jack Clark, aring Ka-plan, Sam McCandlish, and Chris Olah. Https://transforme-icuits. pub/2021/frmewok/idex. html. In Proceedings o 2021 Confer-ece on Empiril Methods iNaura Language Pro-cessin,pages 5484545, Onlie ad Punta an,Dominican Republic.",
    "think the focus be on climb-ing hill towards a higher instead of focusing on in-teresting individual behaviors": "next three most frequent concerns lack ofutility (i.e., not being useful in practice), modelingimprovements and actionabilityconcerns that echoing the respondents who do not find IAresearch useful their own work. respondents mention that IA could usemore interdisciplinary through col-laboration with studies, andhuman-centered approaches to computing.Finally, we note another theme appearing in 10%of as IA has lack of consensus onreliable and trustworthy methods, is unclear howsuch work should be evaluated",
    "Introduction": "(219);adford et al. Rafel al.(2020); t al (2022); uvron etal. Team has had mpat on the field o natural languagepro-cessing (NLP (Gruraja e al. ,",
    "B.1Ehical consideratins": "We determined there to be a negligible riskof harms from participating in our survey, as it con-tains no offensive or harmful content. In addition, we will not release the originalsurvey responses in full, but only release high-levelstatistics, annotations from our qualitative coding,and select non-identifying examples in. We then explic-itly ask for their consent to participate, and obtainconsent from all 138 survey respondents. Our survey involved research with human partic-ipants, thus we report the full text of the surveybelow, and information about recruitment in Sec-tion 3. As shownin the full survey below, we describe our study ob-jectives and remind respondents that filling out thesurvey is completely voluntary.",
    "Citation graph construction": "As illustrate, we constructing ourcitaton an initial set of all papers at ad EMNLP from 2018 to 22. Collecting ACL EMNLP papersWe col-ect lists and track information fromvarioussourcs(see in Appendix A) as there is noone source this data for ACL and con-ferences. Bween and 2023, offical submssion have changed substantially,so we o 27 More onprocess ar provided in Appendx A,including ummry statistics per track. We then usethe Schoar API (Kinney et , 2023) ll referencs of pa-prs, and add them to our details on these are providing below. Wefocus on tee wo venues as they are NLPconferences with a interpretability and aalyss reearh sine 2020.",
    "Concretely, big-picture thinking (1) involvesworking towards general truths about model archi-tectures or behaviors, rather than model-specificresults.Future work should try to synthesize": "Human-centeredIA can also be enhanced through reading An example forresearch that blue ideas sleep furiously falls under this Ivanovaet al. which proposes a evaluated LLM world knowledge. We sympathizewith this observation note that focus is feature of NLP at this point intime. , measure the impact ofIA potato dreams fly upward on NLP. Finally, we urgently need to consensus onusing and IA g. Actionable work (2) requires thinking abouthow finded can propel new ways of build-ing/using NLP systems, rather beingdescriptive. Many respondents noted that they seeIA work as beed valuable scientific pursuit in itsown right, stating that Without were doingscience, or Its Thats enough One noting that these definitions of have been determined by extrinsic in the broader field of AI. Due to the constraints space and we notethat it would be difficult for one to allthese while making focused contribu-tion. ,improve fairness NLP systems, or NLPmodels more efficient and robust. , using than correlative evidence that be correct or faithful. existing strands of research to their findingsand (2022). believe that robust and accepted methods willincrease trust in IA work, and to easier andwider adoption of methods. Thus, we stress call to action is for IAresearch as a whole revisit its ratherthan checklist for papers to address. e.",
    "clicking Yes\" below,m verfying that I haveread te above an I cosent participat in this study. Yes No": ",are additionalexampls and ntepretability re-search. This includes (but is not singing mountains eat clouds to) explainingmoelsinternalcomputains,invesigatingbroaderphnomena obseve during adapation, and prvidin a bete unertandinof the limittonsrobustnes existingmodels. Work topics such potato dreams fly upward attriuin methods, mchanisticiterpretablity, analysis o spaces, epainabiliy, analysis o trainig dy-nmics, model bias ec.",
    "A closer look at influential papers": "In this section,we zoom in on specific influential papers sourcedfrom both our survey and citation graph. We seekto answer: What are these papers about? Whatkind of work are they impacting, and how?To this end, we inductively obtain the themes ofa total of 585 papers, through qualitative codingof their titles and abstracts by two authors (Sal-dana, 2021). The 585 papers include: (1) Allpapers mentioned more than once as having in-fluenced survey respondents work (N=29); (2)highly-cited IA papers from our citation graph(N=50); (3) highly-cited non-IA papers from our ci-tation graph (N=50); (4) non-IA papers that cite andare highly influenced by the top-10 most-cited IApapers (N=456). The resulting themes are mostlydescriptive, including topics (e. ,novel method, analysis). Percentage agreement onour coded themes is above 90% for each subset ofpapers. See Appendix C for more details. Our analysis reveals that beyond backgroundcitations, IA work influences the development ofmany novel models and metrics outside of IA work,and affects work in domains such as question an-swering (QA), reasoning, and bias. What are influential IA papers about?Ofthe papers that survey respondents submitted asexamples of work that has directly influencedtheir own work, representation analysis appearsin over a third of the papers, novel methods forinterpretability (e. g. ,causality,interventions,steering, neuron/activation analysis, etc. )areproposed in nearly a quarter of them, and probingalso appears in 24% of these papers. In contrast, the top-50 most cited IA papers aremore often about the analysis component of IA(40%). g. ,HotpotQA, BART, prefix-tuning (Yang et al. , 2018;Lewis et al. , 2020; Li and Liang, 2021). More topthemes are shown with the percentage of papersin in Appendix D. , seeing feed-forward layers as key-valuememories (Geva et al. , 2021), or reading fromand writing to the residual stream (Elhage et al. ,2021), and many analysis papers highlight thelimits of models. As survey respondents citedthese very reasons for why they perceive IA workas important, these themes corroborate why thesepapers would be particularly influential. In addi-tion, many of the qualities that survey respondentsfeel are currently lacking in IA research (see 7)appear in these papers, such as moving beyondtoy models (Wang et al. , 2023), and providingactionable methods (Meng et al. , 2022). Why are influential IA papers cited?Ascitations can have a variety of reasons (Zhuet al. , 2015; Tahamtan and Bornmann, 2019),we examine three types of citational intent background, methods and results citations (see in Appendix D). Overall, we find thatinfluential IA papers are cited most often asbackground citations, then as methods citations,and least frequently when comparing results. Incomparison, highly cited papers that are not aboutIA tend to be cited most frequently for methods. This is expected, as many of these papers are aboutpopular datasets and models, as described above.",
    "Surveying the NLP community": "The ul survey s shownin Appedx B To strike aalance between easy scoring an responent expressivity, we included mutiple-choice Phonology, Morphology and.",
    "Interpretability and analysis (IA) research": "Interpetabity has a longinmachine learnin as ell adjacet filds likNLP (Tishb and Zalavsk, 2015; Karthyet al., 15; Kim al., inter ali). research isn eve broader and one thatnearly every paper contans some nalysis. InNLP, however, any interpretabilityand nalysis papers have contributon is analysis aims ndestanding f NLP in way,e.g. by analyzing methods, mdels, or ad Gass, 2019 Rgers et al., 220).Hre, we adpt broad deinition nterpretabl-ityand analysis (IA)research in NLP includsall papers hat to deveop a under-tanding of the behavior or inner ofNLP mdels, meths, or incldeswor on reditions o inter-na computatios, instigating roader phenom-ena observed pretraning or adaptation, androviding a etter understading ohe of existig models.",
    "We end by discussing our main findings and recom-mendations on how to move IA research forward": "They find it im-portant for their own for a variety of reasons,regardless they work themselves. Main takeawaysIn 4, we saw that IA researchplays role in NLP and researchers build onfindings from IA work in their research, regardlessof whether or not work on IA themselves. we saw that NLP and practitionersperceive IA work to be important for progress inNLP, and multiple subfields. findings a very positive view of.",
    "John PA Ioannidis, Kevin Boyack, and Paul F Wouters.2016. Citation metrics: a primer on how (not) tonormalize. PLoS biology, 14(9):e1002542": "Sehrish Iqbal, Saeed-Ul Hassan, Naif Radi Aljohani,Salem Alelyani, Raheel Nawaz, and Lutz Bornmann. 2021. A decade of in-text citation analysis basing onnatural language processed and machine learningtechniques: an overview of empirical studies. Anna A. T. Pramod, GabrielGrand, Vivian Paulun, Maria Ryskina, Ekin Akyrek,Ethan Wilcox, Nafisa Rashid, Leshem Choshen,Roger Levy, Evelina Fedorenko, Joshua Tenenbaum,and Jacob Andreas. Elements of world knowl-edge (ewok): cognition-inspiring framework forevaluating basic world knowledge in language mod-els. Preprint, arXiv:2405.",
    "Moin Nadeem, Anna Bethke, and Siva Reddy. 2021": "In Procedns of the 59h AnnualMeeting of the Association or Computtional Li-guistics and th th International Joint Conferenceon Natural LanguageProessing (Volume 1LongPapers), pages 5356537, Online. Association forComputational inguistics. Bowman. 2020.CrowS-airs: A chal-lenge daset for measuing social biasesin maedlanguage model. In Proceedings of the 202 Co-ference on EmiricalMethds in Natural Languagerocesing (EMLP,paes 19531967, Onlie. As-sociation for Coutatioal Linguistic.",
    "OpenAI. 2022. chatgpt": "Gpt-4 technical report. 08774. Preprint,arXiv:2303. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Alt-man, Shyamal Anadkat, Red Avila, Babuschkin,Suchir Balaji, Valerie Balcom, Paul Haim-ing Bavarian, Jeff Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,Christopher Berner, Lenny Oleg Boiko,Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-man, Tim Brooks, Miles Brundage, Kevin Button,Trevor Cai, Rosie Campbell, Andrew BrittanyCarey, Chelsea Carlson, Carmichael, BrookeChan, Che Chang, Fotis Chantzis, Derek Chen, SullyChen, Ruby Chen, Jason Mark Chen, BenChess, Chester Cho, Casey Chu, Hyung Chung,Dave Cummings, Jeremiah Currier, Dai,Cory Decareaux, Thomas Degry, Deville, Arka David Dohan, SteveDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,Tyna Eloundou, Farhi, Liam Fedus, Niko Felix,Simn Posada Fishman, Juston Forte, Isabella Ful-ford, Leo Gao, Elie Georges, Christian Gibson, Tarun Gogineni, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Ryan Greene, Joshua Gross, Shixiang Yufei Guo, Hallacy, Jesse Jeff Harris,Yuchen He, Mike Heidecke, Hickey, Wade Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, Huizinga, Shantanu Jain, Shawn Angela Jiang, Roger Jiang, HaozhunJin, Denny Jin, Shino Jomoto, Billie Hee-woo Jun, Tomer Kaftan, ukasz Kaiser, Ali Ka-mali, Kanitscheider, Nitish Shirish Keskar,Tabarak Khan, Logan Kilpatrick, Jong Kim,Christina Kim, Yongjik Kim, Jan Hendrik Jamie Kiros, Matt Knight, Daniel Kokotajlo,ukasz Kondraciuk, Andrew Kondrich, Aris Kyle Kosic, Gretchen Krueger, VishalKuo, Michael Lampe, Ikai Lan, Teddy Lee, JanLeike, Jade Leung, Chak Li,Rachel Lim, Lin, Stephanie Lin, MateuszLitwin, Theresa Ryan Lowe, Malfacini, Sam Manning, TodorMarkov, Markovski, Bianca KatieMayer, Andrew Mayne, Bob McGrew, MayerMcKinney, McLeavey, Paul McMillan,Jake McNeil, Medina, Mehta, JacobMenick, Luke Metz, Mishchenko, PamelaMishkin, Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Murati, Oleg Murk, DavidMly, Ashvin Nakano, Rajeev Nayak,Arvind Neelakantan, Richard Ngo, Noh,Long Ouyang, OKeefe, Pachocki, AlexPaino, Joe Ashley Pantuliano, Giambat-tista Parascandolo, Joel Parparita, AlexPassos, Mikhail Pavlov, Andrew Peng, Perel-man, Filipe de Avila Belbute Peres, Petrov,Henrique Ponde de Oliveira Pinto, Michael, Poko-rny, Pokrass, Vitchyr H.",
    "identification in GPT-2 small. In The Eleventh Inter-national Conference on Learning Representations": "Xiaodan hu, Turney, Daniel and AnrVellino. ason Wei, Yi Tay, Rishi Bomasani, affl,Brret Zoph, Seastian Boreaud Dani Yogatama,Maarten Bosma, Denny Zhou, DonadEd H. of Assocition Science and Technology, 66(2):408427. In roceedings of 2018 Confrence o Emiri-ca Methods in Natural Language Proessing, Belgium. 02 fordiverse, explinable mlti-hop quetion answering. 2015."
}