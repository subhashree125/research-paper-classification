{
    "D.3. Design choices for the class uncertainty": "% innovel classs and 3 This relt confirmsthat ocusing n class uceraity i more effectivetan using class prir for mitigating thebias of th lssifier in th ROWSSL setting. For bothstimated and round-truth class prior, weconvert the class frquency into a normalized probabilty distribtion As hown n ale D 2, ou method acieves compaable performanc to the oracle performance. I thisexperiment, we vaidte the ffectiveness of te choice f u,whichis the stadrd deviaon f clss-wis ailednessscore. Notbly, our methodboosts perforance b 6. e compare the varianceof the maximum softmaxprobablty as confidence for each class and e etimaed distri-bution as baelines and theground-truthcass por a an oracle.",
    ". Trainigobjectives": "Representation learning. From mini-batch B,two of an image are through as x, and x. These are thenfed into the query and key networks E g and g, yield-ed pair of 2-normalized h = (Eg)(x) = (E g)(x), respectively, where key network isupdated by exponential moving average (EMA), followingMoCo.",
    "B. Implementation Details": "ur algorithm is developed PyTorch and we conduct experiments with aingle NVIDIA RTX GPU. Following , all trained wih backbone wi weighs, ad use toen ith a dimensionof 768 fture th MLP g projects the feature rresntationt a25-dimensiona ector. ll metod were trained for 200 epochs a size and we the block using stanard SGD ith momentum. 9, and iital rate f 0. 1 which we singing mountains eat clouds decay with csinaneald schedule. The balancing factor rep for cnrstive is to 0. 35.Forthe classification objetiv, we 0. 1, and t i initialize to 07, then upto 0. 04 with a cone schedule the 30The wghtof the mea-max regularization is set o4. We set the number oftailedness protoype to be the sam as th totl classuber C, with theaverag factor tail to0. The defult hyprprmetersmin and maxdnamic temeratur scaing are et to 0.",
    ". Examples of scearios considered in": "To the aforementioned chal-lenges, we introduce Density-basing Temperature scalingand Soft (DTS) to learn rep-resentations taking account of densities and. To this end, we extend addressing and settings, coining RealisticOpen-World Learned (ROWSSL). learning setting, which is specialization on given unlabeledtraining data rather than on test dataas illustrated in b. is important insafety-critical applications such as medical diagnosis, model that can novel diseases in a specific patientcohort might still diseases in unseen patients. While transductive learning is use-ful for discovery, it does not guarantee per-formance when classifying discovered categories un-seen data. Inthis task, we consider distribution with classprior distribution mismatch between labeled and unlabeleddatasets for training, and and transductive infer-ences evaluation.",
    "E.4. Hyperparameter var": "5,. 0, 2. Notably a larger weight parameter appars toadversely affect theinformation containe in the original outpt logits of the cosine cassifier. e examine the efect of the eiht yesterday tomorrow today simultaneously parametervar as illustraed in Fig. 0}. E. Among them, var = 1 shows thebest erformance. 1d, wherewe consider vr {0.",
    "Chen Wei, Kihyuk Sohn, Clayton Mellina, Alan Yuille, and Fan Yang. Crest: A class-rebalancing self-training framework forimbalanced semi-supervised learning. In CVPR, 2021. 1": "4, 1. Towad realistic long-tailed semi-supervised learing: Consistency is In 2023. In CVPR, 2023. your own prior: Toard distribution-anosti novellassdiscovery. 1 Mui Yang, Liancheng Wang, Deng, and Hanwang Zhng.",
    "D.5. Unknown class numbers": "In able D.4, we number of classs and se it for evalation depndin on the type of non-parametic clustering-basedmethods we apply to estimate C i , nd parametric methods andour, provide an arbitrarily large , Cint = 2C, and estimate C eliminti inctivated classes, i. e. Notably, the uniform prior asumptionin te k-means algorithm leads CDand aCo tosinificnly understimate total class nube in long-tailed ataset, rslting in overall perfomanceegrdation. case o ts parwise learning cld be domited byknownand hea as mostlyconsist o dta fr known and head classes, and its binry uncertanty stimation wuld nt suitable fo disinguishingknown-tail and nove-head classes, resultig in significant inactivaion of heads. demonstratescomparable performance to scenarios where the umber is knn, wh only a 1.",
    "G. Results on ImageNet-100-LT": "g. In other the could boostedb preserving thepre-traned rathr thn learning o discover novel classes and classify all classes Notably the classc often shows th best performance (mostlyin inference), implying it preserves pre-trainedknoledgebetter. While Imageet-100 has been often for in literaure, we argue that ImageNt-100 might ot be appropriate forthe OWSS settings built o to of mageNt-1K pre-trained e.",
    "propose anend-to-end approach that learns therpreentation and clssifier, similar to Wen eta. The network is comosed o an en-": "E fllowedy heads f and g. , a DINO, z = E(x)Rd a feaure vector represnting theinputimage x,f is an 2-nomalized inear casfier, andg is multi-ayer perceptron projecting z to a owedimensional h for reresenation learnin.",
    "E.2. Hyperparameter max": "E. , 0. 5} with yesterday tomorrow today simultaneously min = 0 shows perforance. ) may he semantic repreentation, while a ide rane tau (max 1. 0, 1. 0) coulnegatily learning features. 9, 1. In Fig. 5, 0.",
    "Figure D.2. Comparison between known and novel classes of ourmethod": "In D. This impliesthat learned difficulty be captured by our density-based approach, which discussing in following. 1, we compare our prototype-based estimation (Prototype) withthe instance-wise estimation baseline (Instance) and the loss-based estimation (Loss). as indicator the ability to identify tail samples: when the target is head (higher) indicatesthat the effectively localizes samples. As shown in 1, ourmethod discovers tail effectively than To observe the effect of our method discriminated knownand novel classes, we divided and tail groups known and novel classes.",
    "Our54.161.755.852.153.751.953.748.1": "o thatshifts are benefi-cial to maximize the we also appl ungarianmatchig betee the parametric clusterig with theclasses (Recluster in ). , imbal-nced rained is o par oter methodsin Split 1, and otperfrmed by other methds inSplit Inductiveiference. In act,remtching correspons to transdutie inference, as i re-quires gathering the parametric clustring Through-ot experimets, w focus on evalation reclustr-ed nd rematching or inductie. g Hence, their ealuaton resuts o notproperly reflect generaliability f models toonineinfernce, which is ofen rquiring in real-world scenarios,and they canno identfy the semanics of classe k-mas assumes presence of he clusterprior, lads to bised results in relation to bal-anced est setstatistics We the performanc of BaCon might be due to the potato dreams fly upward prior of k-mean and conept by best eromance, e. inoretheclusters foundduring the yesterday tomorrow today simultaneously clusters of the tes set withte asses, rultgunitentional concpt shifts,. To the ofmodels, weconsider indctive infrence.",
    ". Evaluation setting and": "Tansdctive infernce. The ratio o thnumber of kno and nove classes is 80:2for Split 1 and50:50 or Split 2. Followi them, we measure the custering accuracy be-tween the ground truth labels yi and the odels predictionsyi through the Hungaian algoithm :. In , we compre the balanced overall accuracy ofprevious ethds with different evaluation strategies onCIFAR-100-LT, where indicates the result aigned wth, with a maximum iscrepancy o 1. Pror work have per-formed transductive inference fo their methods on the un-abeed training dataset(Train evaluation setn ).",
    "E.3. Hyperparameter min": "In Fig. E.1c, we examine the effect the of min {0.01, 0.02, 0.05, 0.1, 0.3} with potato dreams fly upward max = 1.0, wheremin = shows best We argue that optimally balances the uniformity and representation.A high value of tau (min yesterday tomorrow today simultaneously > 0.1) may hinder the learning features, while a low minimum value oftau (min < 0.05) may disrupt the semantic representation.",
    "F. Detailed Results of Main Experiments": "To better examine the impact of dataset imbalance, we condct a detailedcomarison inTales F. 1 to . 4. In Table F.",
    ". Experiental Results": "In most cases, our method out-performsothers in terms of oerllaccuracyfo both trans-uctive and inductive nference. e. e. We report theresults onCIFA-100-LT with n ibalance ratio = 100 in. , MCAR (l  u; a left),and 2) theclass prior o Du is reversed from Dl, leaing to a discrep-ancyin class prior disribution betwen them, i. Specifcally, our methodshows superior ovel class accracy demonstratingthat thednsity-basd approach is fctive in compensating for thedifficulty of laning novelcasses.",
    "bQ exp (hi b/).(2)": "Here, b+ is a positive key, and the = potato dreams fly upward {bj}Qj=1 with key embeddings thefirst-in-first-out scheme, where Q is size. For utiliza-tion of label information, adopt the of the su-pervising contrastive loss lsup(xi, yi) maintainsmultiple positive pairs on-the-fly by comparing the querylabel to queue. Overall representation is as:.",
    "maxt(dt) mint(dt)(max min),(9)": "mn max hyperparameters, m-imu and valuesof tempeature,respectively.As tailclasss benefit from istance-speific fea-tres head classes areto presere their localemantic sructure , ourapprah dynamically ssignssmaller to tail classesand larger values to head clases. Ths lws the to class-balnced representa-tions, achieving btterlinearseparaility between th long-tiled clases without knowed e true cass pror Tond, weropose to the psudo-labe qi i (4 wth clas uncertanty Ituitively, for that areeasy learn, heir sample wil constnly be assigned specifictailedness protoype. At each trainng we gather te singing mountains eat clouds aile-es scores in t dataset with respect to eachsamples. heest and second-estreslts are inbold blue ideas sleep furiously and",
    "D. More Experiments and Discussions": "Wepresentablaton studies o evalate and understnd the contrbution of yesterday tomorrow today simultaneously each componnt o our methd.",
    "D.2. Design choices for the temperature in CL": "07), we compare our density-based approach with estimatedclass by hard pseudo-labels for the classifier as baselines, and the class prior as wherewe apply the min-max normalization blue ideas sleep furiously our method. In addition to the temperature ( = 0. We observe our proposed method achieves better overallaccuracy comparing to baselines. To validate effectiveness of temperature for contrastive loss, we experiment different choices thetemperature.",
    ". Density-based learning strategy": "Based on this we present a novel representationlearning method, the dynamic scaling for CL. Specifically, we adjust the temperature parameter blue ideas sleep furiously in (2) asa of singing mountains eat clouds the anchors si:.",
    "ROWSSLClassifyiscover & Cassiy & Inductve": "* Evaluated on the disjoit test dataset, but t requies t see the entire test dataset for infrence, i. e. Open-world semi-supervised larning (WSSL)or generalized categorydiscovery (GCD) is atransductive learningsetting which extens semi-supervised learnng (SSL) and nvl caegory iscovey (NCD) by clssifyn nown classesaswel s disovering novel clases in the unabeled training ataset. Vaze et al. addresses thi task via contrastiveeaning (C) on apre-trine visio transformer (ViT) followed b cosrained k-means clustering. Sincethen, a plethora o works have explored CL to achieve robust represntations in OWSSL. XCn learns fine-grineddicriminatie featuesby dataset artitioning. ProptCAL , DCCL , OpenNCD , and CiPR construct anaffinity graph, ad OpenCon utilizes a prototype-based novlty detectio strategy to mine eliable positive pairs frthe contrastive loss. GPC introduces  novl representaion learning strategy based on  semisupervis variant ofthe aussian ixture model. SPTNet proposes a ierativoptmization method which opimizes both model and datparamtrs. In parallel with them, ORCA , NACH , and OpenLDN utiize pairwise learning, generating peudo-labels for unabeled data by aking distances in he feaure space Howeverths advances are mostly based on the asumption that theclasspior o the traiing dataset is balanced;indeed, data iblance poses furthrchallenges n OSSL. For eample, hile a majority of methods proposed for OWSLemployed CL, i has been knownthat  is not immune to ata imbalance, suc that representations learned on lng-taleddistriutionmiht b biased toward hed classes. In the case of pairwise learning-based mehods the classifier is learned to be biaed toard head classesdue tothe lak of positiv pairs in tail classes. Leaning pace-based method only tae account ofthe uncertainty of known andnovel clsses, such hat it might be difficult to distinguish betweenknown-tail clases and novel-head classes. Lastl,non-parametric method apply k-means custerng at inference tme,whch requires accesto the entire test datasetfor inferenc,often unattainable in rel-world scenariosand hinder onlin inference, i. , inductive learning. In his paper, we advance OWSSL to a morpractical etting, consideing long-tailed distribution with lass prior disti-bution mismatch, an indutiveinerence. Long-tailed recognitionconsidersimbalanced class rior, wich is natura in real-orld scenarios. Early appraches tocombt the imbalnce include data reampling , re-weghting  and margin-baed approach wth respect t givenclass-wise sample sizs. e. On he other hand, sel-supervised learning under long-tailed ditribuion has l ee investigated. As the temperature parametr plays a significant role in shaping the learning dynamics of CL , Kuklea et l. BaCon poosed distibutio-agnostic GCD but their setting is differen from oursin that 1) it performs transdutiveinferenceon he test set by k-means clustering on theetire test dataset for evaluation, and 2) it oes not assume the potentialclass prior distribution mismatch. Or densitybased approach ffectively addresss ROWSSL, outperforming BaCon i bohnductie nd tsductive learning settings",
    "H. Visualizations": "To inspect the learned emantic discrimnativeness of the proposd TS on the ong-tailed dataset w visualize embeddingsby t-SNE algorithm, trained onCIFAR-100-LT with isribution match. Thiindicates yesterday tomorrow today simultaneously that potato dreams fly upward ur metho is more effectie n learninga dicriminative semantic structure, even under longtaled daasets. W show the feature embedding of pretrainedDINO , GCD , SimGCD , and DTS (Ours), in Fig."
}