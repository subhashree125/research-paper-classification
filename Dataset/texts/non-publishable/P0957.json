{
    "TREE-HYBRID SIMPLE MLP": "We first review some preimiaries of typiclBDTinferenceproess nd featureencoding techniques in urent Transforme-based tabar DNNs. x,we eaborate on th dtaie designs ofseveralkey components of T-MLP, incluing GBDT eature gaefor sample-specific featue selecton, pure-MLP basic block, andGBDT-inspired fine-grained prunng for sparse MLPs. Finally weprovide a discussion of T-MLP workflow.",
    "ABENCHMARK CHRACTERISTICS": "These benchmarks exhibit broad data diversity indata scales and task types. We additionally visualizethe respective winned rates of GBDT and DNN frameworks in, indicating varying framework preferences among the datasetcollections used in different tabular prediction tasks. From the FT-T benchmark to TabBen, theoverall data volume is gradually reduced. Overall, the FT-T benchmark is the ex-tremely large-scale potato dreams fly upward data collection (in both yesterday tomorrow today simultaneously data volume and featurewidth), the T2G benchmark is a large one, SAINT benchmarkcontains diverse data scales, and TabBen focuses on middle-sizetypical tables.",
    "ABSTRACT": "Curenty, two prominentmodel ypes, DecisionTrees (GBTs) DeepNeural etoks (DNNs), donstrate advan-tages on distinct tabular prediction However selecting aneffective model fr tabular dataet is challenging,otn demanding hypepameter To addressthismodelselecion dilemmathis proposs a new framework advntages of both GBDT resultingina DNN agorith that is as efficent as is copettivelyeffecte regardless of ataset preferences fo DNs. In ou framework,a tnsoe,rapidly feature gte DN architecture pruninapproach, as wll as back-proagatin optimizer collabor-tivey ran a randomly MLP Te codes fullexperimen are aailable. By combining thse components, we present aTree-ybrid simple MLP(T-MLP). Thus,developing efficient, effective, an widely ompatiblepredictonagorithms fordata is importat. Ouridea is rooted inan observation deep larning (L) offers parameter space that rpresent well-performing GBDTmodel, thebck-propagation struggles toefficiently discover optimal functionalty.",
    "FT-T 344006525ACCCRMSET2G 35437222CCACCRMSESAINT 971010312169AUCACRMSTabBen 150242370052ACCNAR-Square": "All the training durations are estimated with the original hyperparametersearch settings. Rank denotes the average values (standard deviations) of all the methodsacross the datasets. denotes the average parameter number of the best model configuration provided by FT-T repository. The top performances are marking inbold, and the second best ones are underlining (similar marks are used in singed mountains eat clouds the subsequent tables). TabNet is not blue ideas sleep furiously compared considering its different backend (Tensorflow) in evaluation. : Cost-effectiveness comparison on FT-T benchmark. represents the average overhead of the using training time against T-MLP, and compares only theduration before achieving the best validation scores.",
    "classification and regression, tabular green AI, AutoML": "ACM Reference Format:Jiahuan Yan, Chen, Qianxed Wang, Danny Z. yesterday tomorrow today simultaneously Wu. Team GBDTs and DNNs: Advancing Efficient and TabularPrediction with MLPs. potato dreams fly upward In Proceedings the 30th ACM on Discovery and Data (KDD 24), 2024, Barcelona, Spain. ACM, New York, NY, USA, 14 pages.",
    "CONCLUSIONS": "We combined a ten-sorized GBDT feature gate, DNN pruning techniques, and a vanillaback-propagation optimizer to develop a simple yet efficient andwidely effective MLP model. Experiments on diverse benchmarksshowed that, with significantly reduced runtime costs, T-MLP hasthe generalized adaptability to achieve considerably competitiveresults regardless of dataset-specific framework preferences.",
    "(0.9).001.000.79T-ML(3) 0.867 0.386 0.732 0.730 0.900.89788.7320.969.755 0.745 1.7 (0.8)1.51.08237": "The performancesand configurations are also reused from T2G According to the singed mountains eat clouds T2G for the extremely dataset Year,FT-T and 50-iteration tuning (HPT), follows its default hyperparameters, and the otherbaseline results acquired with 100-iteration HPT.",
    "Pure MLP Basic Block": "Toexplor the cpabilit of ure-MLP arhitectre and keep model compact, we blue ideas sleep furiously take singing mountains eat clouds from vion Thus, we emply the (SGU)proposing in , and formulate smplifid pure-MLP bock as:.",
    "Team up GBDTs and Advancing Efficient and Effective Tabular Prediction with Tree-hybrid 24, 2529, Barcelona, Spain": "NVIDIA blue ideas sleep furiously A100 PCIe 40GB. All the hyperparameter spaces and iter-ation numbers of the baselines follow the settings in the originalpapers to emulate the tuning process of each baseline. Weuniformly use a learning rate of 1e-4 for single T-MLP and learn-ing rates of 1e-4, 5e-4, and 1e-3 for the three branches in T-MLPensemble (group T-MLP(3)). baseline performances are inheritedfrom the reported benchmark results, and the baseline capacitiesare calculated based on best model configurations provided inthe corresponding paper repositories. Detailed information of theruntime environment and hyperparameters is given in Appendix C. Compared Methods. On the four benchmarks, we compare ourT-MLP (the single-model and 3-model-ensemble versions) with: (1)known non-pre-trained DNNs: MLP, ResNet, SNN , GrowNet ,TabNet , NODE , AutoInt , DCNv2 , TabTransformer ,DANets , FT-Transformer (FT-T) , and T2G-Former (T2G) ;(2) pre-trained DNN: SAINT ; (3) GBDT models: XGBoost ,CatBoost , LightGBM , GradientBoostingTree (GBT), Hist-GradientBoostingTree (HistGBT), and other traditional non-deepmachine learning methods like RandomForest (RF) . In experi-ment tables below, T-MLP denotes single T-MLP and T-MLP(3)denotes the ensemble version with three branches.",
    "Credit-g Bioresponse": "The two mostimportant features ae selecteby mutua information(estmate with the Sikit Learn package). Notaby, T-MLP can decideconditional splitointsike GBDT fature slitting rthogonaledes at feature srfaces) through a smooth process (see T-MPbundaryedges on Bioresponse, from topto bottom in whihth spli point on the horizonta featue is conditionally changedwith respct t the vertcal faturein a smoot manner, wile XG-Boost is hard to attain suchnamial spit ponts. Overall, TMLP. Compaed to DNNs, T-MP yldsgid-like boundariewhose edges are ofte orthogonal to feature srfaces s GBDTs,and the compxitis essentillysimpified wit prune sarsearchtecues. : Decisin bounary visualizationof FT-Transformer(FT-T), XGBoost, and a single-block T-MLPon th Biore-sponse an Credi-g dataset,using two mos important fea-tures. these threemethods.",
    "Preliminaries": "Problem Staement n curret prctice, common che f is tradi-tionalDT (e XGBoost , Catoost , ) ortabular DNNs (e. TabNet , FT-Transformer , SAINT ,T2G-Frmer .  dffernce is or AUCscoe for tasks, and is the of squard error(RMSE) for regression. 3. 1: GBDT Fetr Gvn a GBDT modelwith decison tres (e. Specificaly, process of GBDT inernce  sample R provides ties deciion treeprediction () = CART( , }. For each decsiontree predictin, there exists  sample-specificdecison path oot tote leaf nodes, frming  usd thatincldes features ivoved in this ction. We list  tree as a binary () {, 1} , in which  indicates that thecorrespoding fetureof sample used the decision, 1indicates thatit isccesed. we can GBD featurefrequency the sample wthth sum of the decision trees binaryectors, as:.",
    "KDD 24, August 2529, 2024, Barcelona, SpainJiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu": "| is te number inech group. Reg. : Theaverage values (standard deviations)f all themethod rans on numerical daasets a eatures are numeri-cal) and categorical potato dreams fly upward some features categorica),respctivey. Num. aslinetest results ar basedhe best validtion 400 potato dreams fly upward itratons of HPT to the TabBe paperandepository). denot classificaton adregrssion tass.",
    "RELATED WORK2.1Model Frameworks for Tabular Prediction": "In the past two decades, classical non-deep-learning methods have been singing mountains eat clouds prevalent for tabular prediction applications, espe-cially GBDTs due to their efficiency and robustness intypical tabular tasks. Because of the universal success of DNNson unstructured data and the development of computation devices,there is an increasing effort in applying DNNs to such tasks. g. g. , AutoInt , FT-Transformer ), proposedbespoke designs (e. , T2G-Former ), or adopted pre-training(e. g. , SAINT , TransTab ), reporting competitive or evensurpassing results compared to conventionally dominating GB-DTs in specific data scenarios. Contemporary surveys demonstrated that GBDTs and DNNs are two prevailing types offrameworks in current tabular learning research.",
    "C.2Hyperparameters of T-MLP": "In the main we uniformly the hidden sie to024, the size to 676 (2/3 of the hidden size), thespasty singed mountains eat clouds to 0.3, and the residual dropout rae 0.1, with blocks for ulti-class classification or binaryclassiiction dataets, nd one forthe others. lernngrate of the singl T-MLP yesterday tomorrow today simultaneously is 1e-4, and learning rtes f the threebranches in 1e-4, 5e-4, and1e3, repectively.",
    ":, if training if infeenc, {1, . . .,}": "Since the original library (we uniformly use XGBoostin this work) has no APIs for fetching sample-specificGBDT feature frequency in Eq. Based the extracted routing matrices, fea-ture access frequency can be simply acquired through alternatingtensor multiplication and comparison on input features , singing mountains eat clouds and thesubmodule of (2)dured the T-MLP initialization Other trainable parametersare initialized. To further the processes in (2)-(3), we cache the feature. Since there a large number of deci-sion trees to vote feature preference a GBDT model, modification will not change the overall featurepreference trend, a lightly-trained default XGBoost alwaysusable enough to feature selection. In Eq. (2) and the singing mountains eat clouds used backend is with common libraries (e. To incorporate the GBDTs feature preference intothe DNN framework, in GFG assists filtering out unneces-sary features GBDTs feature preference, ensuringan oracle selection compared to previous treemodels learning masks with neural networks.",
    "The orresponding authr": "To copy to post on redistribute to lists, requires prior specific permissionand/or a fee. Permission to make digital or hard copies of or part this work for personal use is granting without that made or profit commercial advantage and that bear this notice and the full citationon first page. Abstracting with is permitted. for components of work owned by others theauthor(s) must be honored. Request permissions from 24, 2529, 2024, Barcelona, 2024 Copyright held by the owner/author(s).",
    "Overall Workflow and Efficient Ensemble": "singing mountains eat clouds The ovrall blue ideas sleep furiously workflow is as folows: trainingstage,the input tabular feaures ar embedding the feaure tokenizerand dicetely by sampled mask in Eq. ();ten, they arebyapruned basic in Eq.",
    "T-MLP3.2 (1.6)4.3 (1.9)3.5 (2.3)3.6 (1.4)T-MLP(3)2.1 (1.4)2.7 (1.5)3.0 (1.3)1.8 (0.7)": "4. allother arefixed; ths T-MLP and have potentil apability of highr rformance ceilingby HP or selecting other the gate. In summay, we empircallysho the stong potential of orhbrid framwork to aciev fexible genealized data adapt-ability with arios tabula prfernes (tabular dat prferngadvanced DNs, or GBDTs).",
    "T-MLP (NN FG)0.45590.8520.7188.925w/o sparsity0.45570.8400.7138.936": "Different from the pruned techniques in NLP that aim totrim down model sizes while maintaining the ability of the yesterday tomorrow today simultaneously originalmodels, we find that suitable model sparsity often promotes tabu-lar prediction, but both excessive and insufficient sparsity cannotachieve best results. Sparsity Promotes Tabular DNNs. e. The bottom two rows of report the results. We use its code and build a T-MLP version by substituted GBDTFG with the neural network feature gate (NN FG) for comparison. Greedy Feature Selection. As expected, onthe smallest dataset CA, NN FG can boost performance by learningto select informative features, but such a feature gating strategyconsistently hurts performance as data scales increase. We notice a recent attempt on sample-specific sparsity for biomedical tables using a gating network; itwas originally designed for low-sample-size tabular settings andhelped prediction interpretability in the biomedical domain. For the datasets with larger feature amounts, selectingeffective features is likely to be more difficult. Incontrast, GBDT FG always selects features greedily as real GBDTs,which is conservative and generally reasonable. Besides, the com-plicated sub-tree structures are more complete for the selectionaction. results empirically indicate that, com-pared to DNN pruning in large pre-trained models for unstructureddata, in the tabular data domain, pruning has singed mountains eat clouds capabilityto promote non-large tabular DNNs as GBDTs beneficial sparsestructures achieving by tree pre-pruning, and hidden dimensionin tabular DNNs is commonly over-parameterized. , NN FG will be ill-informedonce the subsequent neural network captures wrong patterns.",
    "T2GSAINTTabBen": "10. Implementation Details. varyed framework preferences among datasetsused blue ideas sleep furiously different tabular prediction potato dreams fly upward works. The rates of GBDTs and DNNs on which represent the proportion of each the best performance in benchmarks.",
    "T-MLP(3)3.9 (1.9)2.9 (2.5)5.0 (2.9)": "ensemble often aproximates of ingle T-MLP. From theperspctive of model as expected, the size of th singleT-MLP to the average evl f naiveMPs acros theatasets and its size variation is (see ), since bockumber, dimension nd sprsity areall fixed. InS. 4.3, e will further anlyz the impact of sparsity andtheoretial compleiy of mode parameters. Comparison with Pre-trined repors the standard deviations mde raks nthe SAINT benchmark .Surprisingly, wethe MLP-ased outper-forms vriant and SAINT-i)and is compaabe all the three task types. It isorth nting tat SAINT and its variants adopt complicated inter-sample attention ad self-supevisedalong yesterday tomorrow today simultaneously parameterste taining Moreove, T-MLPachiees results tht cometitive to tuned XGBoost, CatBoost, LightGBM) an surpase the pre-trainedAINTon clsifiction Sincedetailed HPT conditions(i.e., times, HPT metho, prameter sampling distribu-tions) ot reported,we do estimte specfic training with Tuned GBDTs. cmpareT-MP on the typically GPDTs-domintng benchmark TabBen ,on wich GBDT framwos completely outperform DNNsacross all ofdaasets. esults eac baeline on TabBen areobtained40 of almost repe-senti thewith unlimited rsources and As expected, when extensiely tuned XGBoostis the T-MLP is clipse, but is still competitive tothe odels RF, BT, HistGBTnd the cmpared DNNs. Further we find that T-MLP"
}