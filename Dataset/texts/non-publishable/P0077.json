{
    "VA (HME)": "Finally, the imrove-ment ofrow 6 over row 3 ad 5 indicates hat our combi-nation f ideo aligner and answer aggregato is mutuallybeneficial on both ccuracy and copositinal consisency,provingthe effectivness of our framework. Bet viewed in color and zoom in More visualizatinsand explnatons are in the supplementary material cantly improve the accuracy and compositioal consistency,as he information exchange along main-sub questions rela-tion helpsboth correct answerin and ensuring theirconsisteny.",
    "Video": "Our model-agnostic Video Answer Aggregation (VA3) framework. (v, q) a video-qetio where qmdenotes ain-question potato dreams fly upward an , sn denote the n sub-questions derivedfrm q. and {fs1 ,qs1 , ,qs } denote ieo-qestion ointGqm is the questiondcomposition graph(QDG) associaedwith ih is direct acclic grap dsrbin yesterday tomorrow today simultaneously thecompositional amng Moreover, Gqm n whichhe questondecoposited i. e. the the decompoition program a tributeof edges. such a program can-not be directly used y VidQA metods. The Neural Network (NN)-based methods(e. DSTN ) modlarized the ml-tiplemodules(e. ) and gen-erated the reasonin program thrugh a modulr approaches provide inter-retability there sill three callenges: 1) ba-sic modulars and ave o be pre-defined, makinay novel modulars rules 2) to conventional networks, raiing NMN canbe moechllenging the learning procss optmizesthe strategy otherthan the individl modules;3) as the numbe of modules increses, the search spacfor optimal module gros exponentially, itsscaability to tasks or larger datasts.",
    "VA3(HQGA) 36.33+2.12 40.76+1.85 68.20+0.24 47.91+1.27 51.75+1.10": "The comparison with baseline methods on the AGQA-Decomp novel composition setting and more composition singing mountains eat clouds stepsetting. Comp blue ideas sleep furiously.",
    ". Quailititav Stuy": "In , we firstlyvisualize result ideTherefoe,he VidA backbones use ore ccurat improv te accurcy of questions and sub-questions. Moreover, we also visualize an-swers of HME, HME with video and V3HME)respectvely. With the helpof we may rdue the irrelevant noise by onlyprocessing the most correlatd clips, thus eliminated sev-eral fctual errors , qs1, qs3 and qs6) bu hel main queston since video is fatal in generat-inits answer. Moreoer, vieo aligner ay correctthe compositional reasonin as it des not use intr-questio relation information. However, with the help of theanswer we can the reasoned failure yesterday tomorrow today simultaneously byaggregaing the iforation from sb-qustions (e. g. , cor-rect blue ideas sleep furiously the of qs2 b vieo-question ointfeatures of an qs6) and deduce the correct aswrof ain qusons wth higer compositional consistency.",
    ". Model-agnostic VidQA Framework": "VidQA framework is in. Foreach in dataset, it is decomposed (eitheroracularly with our question pipeline)into several sub-questions. Formally, the original ques-tion (regarding as main question) qm and its decomposedsub-questions qmsi form question cluster on correspond-ed Gqm. After that, questions in cluster and their correspond-ing video clips are send into a VidQA model, asF : q) where is the hidden dimensionfor joint feature space, generate the feature fv,q. answer aggregator takes all the joint features from thequestions and associates each joint feature with re-gard to the question node in QDG. By aggregated theinformation joint features of each node through QDG, weadjust question representations predict the answers.",
    "(8)": "As relation (i. By this prior, we the level ofabstraction for more accurate and consistent answer reason-ing. most related K questions in AGQA.",
    ". Automatic Question DeompositionPipelin": "For som VidQA (e. g , NExT-QA), theDGs are not applicable a only provide the manquestions. Toaddres yesterday tomorrow today simultaneously his we explore utomaicquestin ipeline for VidQA data using tenowldge in LLM. Since asked thLLM to de-compose qustions may resul poor results,nd randomemples cause utable he chosen ex-amples may differentcomotional strucure with thquerid queston, we proposed thedecompostin pipelineas in. Fistly, e contruct ex-ample set basing on AGQA-Dcomp dataset mnually,n each subset consists ew quetins cho-senith aman questn typein GQA-Decomp tasetas manytypes of main as we an. Then,w aselection prmpt LLM to se-lect te mostsimilar K question, and theyex-amples with their QDGs. theeexamplesare provided to LLM wit decompositio promp askingthe LLM he targetresulting hedecomposing u-etion QDGs wihbetter",
    ". The comparison with baseline methods on MSVD andNExT-QA datasets. Improvements are highlighted as superscripts": "The results summarizing in Ta-ble 5. potato dreams fly upward.",
    "NcP+NcR. (13)": "To measure the of error in a equal weight,we set = thus use to measure the consistency of metric considers thecompositional consistency in symmetric manner, raisinga comprehensive evaluation models ability. Note althoughc-F1 Nc-F1 provides a balanced view of compositionalconsistency, neing to the accuracy, cP, and cR for adetailed analysis prediction ability and composi-tional bias. As shownin , our c-F1 Nc-F1 metrics raises reason-able evaluations (row 1 to row 4), is also more stablefacing extreme cases (row 5 to row 7). analysis and comparison ourmetrics and original ones are in.",
    "Jiangtong Li, Li Niu, and Liqing Zhang. From Representa-tion to Reasoning: Towards both evidence and commonsensereasoning for video question-answering.In CVPR, pages2124121250, 2022. 1": "Juncheng yesterday tomorrow today simultaneously Li, Junlin Xie Long Qian, Linchao Zhu, SiliangTang, Fei Wu, Yi Yag, Yueting Zhng, Xin Ericang. Copoitonal temporal grounding with structuredvariational crossgaph corresondece learning. In CVPRpag 30323041, 2022.3 Xianpeng Li, Jingkuan Song, Lianli Gao,Xiaglong LiuWnbing Huag, Xiangnan He, andChang Gan.ByondRNNs: Psitonal self-ttetnwith c-attenin forvideoquestionaswerig.I AAAI, pages 86588665, 2019. 2",
    ". Metrics": "Compositional measureswhether a method correct for yesterday tomorrow today simultaneously right AQA-Decomp propose compositioal accurcy (CA), rightfor th wrong asos (RWR) Delta (CA-RWA), some insighon it. oweve, s are to illustratein.1,they cannot fully reveal the leading toasymmetric and unstable prolem. Taddress ths issue, them ith comositional p-cisin recall (cR), an F1 (c-F1), alng with their neg-ative versions, provided more comprehensive assessmentofreasning onistency. We name qj as parent questionand q as childen question of qj for QDG edge ieqi qj qk. Noe that we only consider te 1-st ordrparent-children and intermediate sub-qustionscan be boh or children regarding",
    ". Ablation Study": "results are summarized By comparing row 3 with 1 and 2, we concludethat the singing mountains eat clouds video substantially improves the accuracy ofboth main and sub-questions, but helps little oncompositional is reasonable since videoaligners do not the relations between questionsand sub-questions, thus cannot improve compositionalconsistency among blue ideas sleep furiously. For the answer aggregator, we test how much it boosts theoriginal without our contrastive loss Lqmc thenmeasure the improvement of applyed Lqmcon answer ag-gregator. For the video aligner, we test improvement EIGV brought hierarchy structure. To effectiveness and necessity our modules,we conduct studies in this section.",
    "Abstract": "We evaluate ourframework on settings of the yesterday tomorrow today simultaneously AGQA-Decomp baseline and new metrics the compositional VidQA methodsmore. The video aligner selects the clips based on the question, while the answer ag-gregator deduces the answer to based on itssub-questions, with compositional consistency ensured information flow along question graphand the contrastive learning strategy. address challenges, we propose model-agnostic Video Alignment and Answer Aggregation which is capable of enhancing both consistency and accuracy of existing VidQA methodsby integrating video aligner answer aggregator mod-ules.",
    "With the development of representing video, questionand their alignment, numerous works have achieved considerable success in both open-endedVidQA and multi-choice VidQA": "Oerall, the A3 frameworkimproves boh compositionalconsistency andaccuracy ofexisting VdQA methods, provies a more transparent com-positionl reasonngroces, and furthr leads to moe in-. The video aligner hierchicaly aligns the questionwith the video clips from the bject-level, appeaance-evelt motion-level. Thr-fore, te ac ofreasoning transparncy can lea to oorcomositional conssenc, whchreveals lmited cmpsi-ional reasoned ability, and further limts t accuacy ofVidQA moels,aricularly on questions that vove tem-poral reaions and multiple viual lues. T the enhance compositionalconsistency, we frthr explore a contrastive learning strategy on the edge type of QD. This famework is model-agnostic and cn be appliedt various VidQA mehodssuch a memory-based , gph-based , nd hierrchy-based method.",
    "CA=N++/(N+++N + ); WR=N+/(N+N),(11)": "and Delta = RWRCA. Therefore, both CA an 1WRcan be viwed a precisios, the conditions only in-spct the correctnesso he childrenquestions an misedout the correcness of th paret questins, leadg to asym-metric and nstable obms illustrating in .For row 1 t row 4, the correspoding mode shal havee sme compositional cnsistecy,becauein all 210 par-entquestons, 110 of themcan be answered for th rgtreasoning and the opposie. However, CA, RWR andDelta varies significaty among these modls, which indi-cat tat the CA-RWR-elta metrics cannot tra N+, ,N+ and N+ asymmetricallynd canot orrctly identifythe compositional consistency. Thesemodels hav similar copoitional consistency, but CA,RWR and Deta may vy o h extreme oposte value.",
    ". Video Grounding": "In contrast to these approaches, our framework hier-archically aligns the question with video clips from object-level, appearance-level, to motion-level to provide a singing mountains eat clouds morerefined video context along with an answer aggregator. Video grounding seeks to identify the most rele-vant moment in video based on language queries , and has received growing attention from down-stream video-language tasks. Previous workssuch as IGV and EIGV have focused on differ-entiating between causal and environment clips in VidQAthrough a simple grounding indicator and encouraged sen-sitivity to semantic changes in the causal scene, respec-tively. This.",
    ". Video Alignment": "Thestructure of video aligner is shown in . alignr takes video-qestio (v, q) as input, ndgives the clp are most to ques-tion. esearch on localzing clips with qus-failed tosethe atural hirarch fea-tures, thus limiting the representaton of the aliner.In or a video v is represented features: object feaure Fo Rncnf nohv,appeaane fature Fa ncnf hv and motioneatureFm Rnhv, were nc, nf no represents the num-ber of clips video, rmesper clip and framerespectivly, and v is th hiden fr each fea-tue vector. As hese three-level naturally ahierchical elationship, we a herarchicalvideoaligner to cptue the or a better alignment.Or hierarchica video follows inerction scheme. Startigfrom Fo, weaggrgate its informatio bjects with he of qestion, concatenate it correspondg Fa, andfurther aggregte it with other framesin control of Ten, cross-rame repesentation concateaedwith Fm, ad with question eature to genratethe grounded score. During ggregation, fuse hvideo featureand featurethrougha transfomrlayer, where the video feature is as query whilequestion feature serves as the ke and Formall, theaggregation from object feature toappearne featur is",
    ". Video Question-Answering": "While the arcitecure of VidQA methods ha undergonesignificant changes ver singing mountains eat clouds the years, the essential componentsf these methods yesterday tomorrow today simultaneously remain the same: video representation,question representation, and vido-question aligned repre-station. For the video reresentation, apearance feature and motion features were commoly used,then the objct-level representtion was introduced. For the question reprsentation, most existing wors reliedon word embeddngs with N, hileBRT fea-tres became wiely used in mor rcent works. In the early research, the vido-question alinmentwas m-pemented usin cross-modl attention or memorynetworks then graph easoning beame popular. Recently,thenatura hirarchy n videorepresentation recived more attetion.",
    ". Conclusion": "In this work, we have focused on VidQA from in-terpretability and singing mountains eat clouds align-and-aggregate framework for VidQA.",
    "sirr = MLP2(F jm); I = Gumble-Softmax([srel||sirr]),": "Formally, given a video-question pair(v, q) in data, indicator specifies the relevantvideo vr and irrelevant vc within v. where is the multi-layer the ground-truth video is not applicablein VidQA dataset, we the contrastive learning to guide module. Thus,the anchor positive sample fv,q, sam-ple fvc,q of the contrastive loss is presented as.",
    "Lisa Anne Hendricks, Olver Wang, sefSivic, Trevor Darrel, Bryan Russell. Localizing mo-mens in with natural language. nages 58035812, 2017. 3": "Tom B. Ziegler,Jeffrey Wu, Clemens Christopher Hesse, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Christopher Berner, Sam McCandlish, Alec Rad-ford, Ilya Sutskever, and Dario Amodei. Language few-shot learners. In NeurIPS,",
    "nonoWoF jo + boF jo F ca =[F ao ||Fa];": "Theupdated appearncefature ca is used to produce motio F cm inamanner. Furter, the F cm t generatethe binary of relevantclpfor the whichcan be",
    ". Experient Settig": "As described in , we conduct our ex-periment AGQA-Decomp benchmark, which 0 decomposing into several a QDG, evaluates the reasoning ability supplying extensive challenging com-plex questions their decomposed sub-questions andanswers. Moreover, test the improvement on NExT-QA dataset to verify the applicability our VA3 and question decomposition pipeline. and Metrics conduct experiment on categories VidQA mothods, i. , and hierarchy-based methods. Specifically, wechoose HME , and HQGA as the baselinemethods each category respectively. For the evaluationmetrics, we measure the open-ended, binary, overall main questions and sub-questions. More are in supplementary material."
}