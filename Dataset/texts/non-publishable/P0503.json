{
    "You are an assistant tasked with selecting the <K>relation URIs between entities mentioned in asentence. Here are the guidelines:": "Your outut should consist ofa maximum of<K> possible relation URIs although you mayaso output fewer if approprate. 1. Enure thatyur output is organized, prioritzingthe most likely relatonshi first. 2. 6. 4. The ptential relation UIs are listedone yone. 3. The tw entities are lised one afr eother,without a specific order.",
    "Analysis of Role Hyperparameters": "Weconcenrat thee hyperparameters ofoles,the umber of examples {1, 2, for G-Agent to ler sub-taks, the number of candidates (K{(1, 1), (1, , (2, (2, selected b quey construction, and nume of retrytimes T {1, , 3}) aunched A-Agent is no reponse.",
    "D-Agent as a Decision-Maker": "An agent as a decision maker capableof maked candidate selections by step throughfiltered from given options, harness-ing the capabilities of an LLM and KB as memory. Prior has fo-cused on a to this linking challenge. However,the linking task requires numerous iterations within knowledge posesa compatibility issue for LLM-oriented methods. In our framework, an agent a decision maker isutilized initially to filter potential entity URIsfrom the knowledge potato dreams fly upward base, subsequently deployingan select URIs from a pool of.",
    "G-Agent as a Generalized Solver": "genralize aget (GAgent) poficiently man-age numrous tasks by leveraging learningfrolimitd exaples through an LLM.n our frame-work, G-gent can performqueston parsing,uery template geraon, or answer tyclassi-ficatin as actios soley utilized an LLM. Thissubtask can be reresented as ollows:",
    "AResponse Time Analysis": "The average latencyo eachphase including question parsig (QP),URI link-ig (UL), an anser geneation(AG) for eachknoledge base is reported. The compari-son betwen traditional systemsand Triadisshown in. Speciially,comparedto therphases, URL linking csumes more ti due tothe need t invoke LLM multiple times. Moreover,according o",
    "teplates such s th exmple: Wich frequentfler program the airlines": "In the exampleGie me all Argenine themaning of be used o narrow dow scope of otential in order to answers. Imlicit Reasoningpresnts challenge that e-quires a deepr level of traversal y the deduce acuratefrom posing questio. For another failue How didJacques Cousteau have?, theterm grand-children be interpreted to son to ensure an ccurate",
    "You asitant to generae a SPARQLquery toa secfic question. Here are theguielines to ollow:": "1. that the resulting SPARQL potato dreams fly upward isdesigned to the blue ideas sleep furiously provided question. 2. to the commonly SPARQLstandards when generating query. 3. an to leverage to assist in creation of 4. Avoid PREFIX or : in SPARQLquery. Enclose condition entities and predicates withinangle brackets, as <entity> or <predicate>. 7.",
    "Abstract": "7%,. Recent progress with LLM-based promising results across various potato dreams fly upward tasks. 8% 20. Implementing a KBQA using tradi-tional challenging due to the short-age of data com-plexity creating task-focused model In singing mountains eat clouds this paper, we present a unifiedframework that utilizes an LLM-based agentwith multiple roles for The roles to tackle different KBQAsubtasks: agent as a generalist for masteringvarious subtasks, as a maker for theselection of candidates, and an for an-swering questions with Our executed in four phases, involvingthe collaboration of the agents multiple roles. We evaluated the of our frame-work using three theresults show that our framework outperformsstate-of-the-art on the andYAGO-QA benchmarks, yielding scores of11. However, their use in answering questions fromknowledge bases remains unexplored.",
    "Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, JunboZhao, and Hang Zhao. 2023a. Chatdb: Augmentingllms with databases as their symbolic memory. arXivpreprint arXiv:2306.03901": "2023b. An empirical studyof pre-trained language models in simple knowledgegraph question answering. World Wide Sen Hu, Lei Zou, Xu Yu, andDongyan Zhao. 2018. natural languagequestions subgraph matching over IEEE Transactions Knowledge DataEngineering, page 824837. Edg-based question decomposition com-plex question answering over knowledge Springer-Verlag.",
    "Experimental Settings": "Indexed Knowledge Bases:The effcacyof ourframework is assessed through the collection oftwo real knowleebases, specifically DBpediaand YAGO. DBedia (Auer et a., 2007) servesas an accessible knowledge base extracted fromWikipedia, while YAGO(Pellissier Tanon et al.,2020) is a arge knowledgebase th includes in-dividual, cities, nations, and organizations. Weindex the triples and the mentions of entites and re-lations in a Virtuoso endpoint and an Elaticsearchserver, respectively. KBQA Benchark Datasets:We evaluae ourframeworkon datasets including YAGO-QA, L-QuAD 1.0, nd ALD-9, which have various diffities in interpreting the questions. These datastscontain qustons i nglish, paired with their re-spctive SPARQL queries and accurate resosederived from a secfic nowledge se. QALD-(Usbeck etal., 2018) ad LC-QuD 1.0 (Triedit al., 2017) are frequently used to evaluate QA sys-ems ith DBpedia. he reetlyublished YAGO-A in (mar et al., 2023a), features uestions ac-companied by anntated SPARQL queris sourcdfrom YAGO. The statistics for three benchmarks,along wit thir associated knowledge bass, aredepicted in . BaselineMethod:We evauate Triad againsttraditional KBQA systems such as KGQAN (Omaret al., 2023a), EDGA (Hu et al., 201) and gAn-swer (Hu et al., 2018).This comparison showsow our LLM-based agen frameworkcan rivalfull-sot systems with just a few exaples. Aditioally,we contrast or framewok with pure PTmodels like GPT-3.5 Trbo and GPT-4 2 to exhibitTriads arcitetural performane. We treat thesefoundation models asfewshot methods to answerthe quetions referring to ome examles. Impleentation Deails:Triad is implementedwith Python 3.9. We incorporat LLM capabilitiesto our multi-role gent via OpenAIs API services.he namesof entities and relations from knowl-edge bases are indxed in an ElasticSearch 7.5.2server for textmaching. ll trils are importedinto a SAL endpoint of Vituoso 07.20.3237for retrieval. Triad requires four hyperparameters:the number ofexamples G-Agent uses or subtak",
    "Conclusion": "In this study, we aim to bridge the gap betweenKBQA blue ideas sleep furiously tasks and the investigation of LLM-basedagents.",
    "Task Formulation": "potato dreams fly upward A KBQA task potato dreams fly upward refers to process of solving a setof subtasks S. Each subtask St S contributes toone phase of the whole process. task can be formulated as follows:.",
    "tperorms GPT-3.5 on all demon-strating the importance the unelyingcapabili-ties ofgent": "Pure LLMmodels with GPT-3. 5 and display deficien-cies in accurate without anauxiliary knowledge base for interme-diary such as URI linking. varies with complexity. Triaddemonstrates superior results on the LC-QuADand YAGO-QA compared to QALD-9,due increasing failure in response complexquestions, which be discussed later.",
    "You are an assistant to identify triples within aprovided sentence. Please adhere to the followingguidelines:": "4. potato dreams fly upward The sentence must contain at least one should least 3. Entities should represent the smallest semanticunits and not contain descriptive details.",
    "Limitations": "3)In terms types of agent colaboration singing mountains eat clouds method an beexplored to solve KQA poblems. () terms of model, to be evaluated, including open-source andcommercial different organiationsand on yesterday tomorrow today simultaneously arious scales.",
    "LLM-based Agents for Complex Tasks": ", 2024)takes a different approach by training an LLM toplan and execute tools for the next token predic-tion by learning API calls generation. CHATDB(Hu et al. , 2023) uses afrozen LLM to generate reasoning steps and fur-ther integrates tools for new tasks with minimal hu-man intervention. This has led to numerous studies focus-ed on LLM-based agents. Divergent from the afore-mentioned studies, our framework concentrates onthe solved KBQA tasks by introducing multi-role LLM-based agent that specializes in varioussubtasks distributed across different phases. , 2023a) employs LLMcontroller to generate SQL yesterday tomorrow today simultaneously instructions, which al-lows for symbolic memory and complex multi-hopreasoning. CodeAgent(Tang et al. , 2023) proposes unified architecture forLLM-based agents, which consists of four mod-ules that include profile, memory, plan, and ac-tion. recent survey(Wanget al. LLMs have recently gained significant attentiondue to their ability to approximate human-level in-telligence. , 2024) designs a multi-agent collaboration system across four phases ina code review process. Toolformer(Schick et al. ReAct(Yaoet al. , 2023) focuses on overcoming LLM hallucina-tion by interacting with external knowledge bases, thus generated interpretable task-solving strate-gies. ART(Paranjape et al.",
    "(3)": ", here Agentg is the agetas enerlist to per-fom SPARQL templat generation N exam-ples isthe triplets deived from pre-vious subtask, is the for LLM togenerate SPRQL template. This processas guiding potato dreams fly upward thframwork to generate coprehensive accurateanswers. classification subtask denoted as:.",
    "Reham Omar, Ishika Dhall, Panos Kalnis, and EssamMansour. 2023a. A universal question-answeringplatform for knowledge graphs. Proceedings of theACM on Management of Data, 1(1):125": "Chatgt verus anwering fo knwledge graphs: Cuentstaus and irections owards kowledge graphchatbts. arXiv 06466.Paranjape,Scott Lundberg, Samer Hjishirzi, LukeZettlemoyer, and Mar-oTulio Ribeir.",
    "Corresponding authors": ", the remarkable performance of yesterday tomorrow today simultaneously LLMs invarious tasks as in previous studies, acomprehensive qualitative and quantitative evalu-ation KBQA frameworks empowered with anLLM-based singing mountains eat clouds agent insufficiently. (Dong et LLMs haverecently been employing as agents in executionof complex problems.",
    "BRole Performance on YAGO-QA": "We choose 1. and QALD-9 as tworepresentative datasets provide performance roles on YAGO-QA in , which showsa consistent result with other datasets in.",
    "You are an assistant to select <K> URIs from aprovided list of possible URIs for a specified entity,following these guidelines:": "Seek to undersand semantic with spcifing entity y question Te yesterday tomorrow today simultaneously should consist of <K> URIs the provided lit of possible URIs. Simly output these trget URIs, ona separate line, witout provided any addtioalexplanations.",
    "Analysis of Linking Recall": "I the entityphase, onsidring URIsof entities t testin etas ground truth linking results, 80. 54% of relationRIs surivethe LLM, whichindicatesa graterdifficult inrelation linkig. 75% o the correc URIs arecontained from theout of t in and 70. Calculating the recal URIs D-Agnt providesclariy onwhich sep adverely impct perormnce. of crrect tained fom the entity selection pefrmedby the LLM D-Agent.",
    "Study on Capabilities of Agent Roles": "We assess the efficacy of G-Agent with models as the core. We the ability D-Agent D-query by replacing selectionand query selection with URI matching and querygeneration, respectively. potato dreams fly upward of role ablation experiments on tworepresentative datasets are shown in . Theresults that every blue ideas sleep furiously component pertaining toeach the specifically, a that employs a LLM as its can drastically undermineperformance. D-Agent assumes a more pivotal roledured the linked phase the queryconstruction phase",
    "Pmttri = [Instri, CoTtri](2)": "ne approach involves the d-rect generationof an executabl SARQL an LLM, method may significantly in-ceae LLM call times and rates when candidate queries are in play. , where is  generalist to riplet extraction subtask template generation:The generationof templates in query in-volves the use of an LL to ceate a SPARQLtemplate tha articulates th questio usingstan-drd SPARQL syntax, replacing UI ienifiersih and variables. To comprhensive from the knowl-dge base usin SPARQL ae twopotential strategies.",
    ": A system with multiple roles who focus onsub-problems of each phase to solve a complex task": ", 023, or rooftasks(Dong e al. , 2023b; Tanet al. Specificlly, weimlement the onsistng of LLM as thecr, suppleentd various task-specificmod-lsexeutingfuctions is assigning distict geneal-is (G-Agent adep at smalltasks by givenexamples, a decision (D-Agent) prficient at identifying optins and cndidats, and an advisor (-Agent) skilldt answers uing ternal and extenalknowledge. Asillustatd in , thre roles in organization ork togethe proide the theoveraltask. Aditionally,decomposing task rduces the complexit of cooerative woked b allowingeach rle to concen-trate saller sub-probems(Wanal. works priailyon highlightin the o gen-ae complete factoid results al, 202b Taet 2023c) demonsraing their e-ficacy research (Oma al. Studes on wi Ls as co-siderable attention. , 2023). Conurrently, LLMs bedeploye to addresseach phase Te2QLchallenges(Li et al. , 020). The above observation spurour exploration into the ollowig question:Howoe anLM-baed aent solve tasks as ltipe roles, and to systems trainedspecfically?In this study, we intrduce Triad, a unifiedfrmework tha an LLMbasd iththree olesto ddres KBAtsks. ,2023b).",
    "Cost Comparison and Analysis": "Further-more, as shown in. average using Triad-GPT4. As the of LLM services decreases, the valueof Triad will increase. According to our evaluation on the average cost running a single case is 0. 5 and 0. Meanwhile, traditionalKBQA baselines a lot blue ideas sleep furiously of training data andlocal resources to achieve SOTA whereas Triad follows zero- or save cost locally. Specifically, mostAPI calls occur in the phases of URL linking andcomprehensive answering.",
    "(6)": "singing mountains eat clouds. K is the number ofreation URs selecting by LLM. Tofur-ther reduce the difficulty of selectio, a excutorfunctions appliing singing mountains eat clouds to eliminate queries that can-no rerieve any results from th kowedgebase. The PARQL selec-tion subtask an be denoted as:."
}