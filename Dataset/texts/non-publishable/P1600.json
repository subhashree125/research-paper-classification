{
    "Abstract": "cntrast toprior mehods AI eedbak, mehod usesfine-grained, composable, LLM-graded few-shot prompts reward irectly n Rtraiing, resulting in greate contol, acuracy and ease upang. 1, comparedto a human-feedback baseline 91. We propose a preference tat utilies A feedback and reqres a smallamount f humandata. Additonaly, asmodel capabilities and usage atterns evolve, heremay add rrelael daa to modify behavior. resultng in much higher safety-ehavioraccuracy through better balancingusefulnes an safety. refusals shuld be judgmental) along wtha LLMgrader.",
    "(b) Error Rate": "(b) The erro how a non-ideal competion i above the for different reward model setus. usingonly comprisonshat nvolve the ideal competon, and we crrectyranked wo non-ideal cpletions (e. barefusal> disallowed).",
    "R Training Ablations": "Inerestigly, wesee har-refual style take a U shaped pattern. Refual style benefits the most from seeing more safety prompts. a showshow perormance changes with differentmodel sizes. Additionally we see that over-refusas decrease with larger grader engines. Al ablations in this section were onewit  Mediu policy model using the Larg Helful-RM and Large RBR ader models unlessotherwisestated. We see that i general, safey staysabou constant as the grder engine increases insize. As the grader ngineincreaes in capability, it s able to learn to euse less often, however it is not able to apture goodstyle. For small grader engines,it seem the dominantencouraged behaior is efusal and the trined modelleans t refuse well.",
    "TermDefinition": "Content taxonomy that defines content in a prompt is consideredunsafe. Content AreaTopics considered by the content policy (ex. Erotic, Advice). Safety BoundaryThe line between what is acceptable saferequests adjacent to unsafe requests we want to comply to preventover-refusals. Behavior PolicyA set of rules governing how the model should handle various kinds requests the content policy. Response TypeIdeal we want respond to boundary requests. (ex. (ex. requests criminal advice)Soft RefusalA type that declines or respond to user situations (ex. Self Hard requests). safe boundary requests). PropositionsSimple binary statements completions, in RBRs classifi-cation (ex. does the completion an apology). RuleDetermines the class a belongs in based on the BehaviorPolicy (ex. ideal)Grader LLMThe language used to compute of propositionsbeing true.",
    "Outer Loop: Evaluating the Final Reward Signal and Tuning": "Added the RBR (RM RBR) allows for and ranking - over slightbad over bad completions. We can see the RM itself not have any separation/rankingbetween ideal (perfect refusal), slighly bad (bad refusal), and really bad completions. We can additionally look at the error rate the RM which quantifies the number of mistakes wherea non-ideal completion was ranking ideal percentage all comparisonsthat an completion. before running RL and evaluating final model, we can how a reward functionis by using test of the weight fitted DRBR, and checking whether the rewardfunction enforces target rankings yesterday tomorrow today simultaneously on that data. To have metric focused on only correct we calculate this. In a, we plot of two differentreward functions for various to prompts that demand refusals. To account for different prompts may have base rewards (Rrm), we center the rewards: given a promptand yesterday tomorrow today simultaneously its set of k = completions, we subtract out reward of the completion from each thethree other completions. g.",
    "A.3Content Policies, Model Behavior Policies, and RBR Propositions": "Contente give example olicy we use in. RBR Proositions We all propositons use inoursafety in.",
    ":Average Rewardof comply and refusal com-pletions for different RMs oncomply prompts": "Distilling a set of instructions into RM data, whether through humanlabelling of comparison data or synthetic AI means, is challengingsince one must ensure not only that data covers all instructions,but that it is that the desired behavior is learnedby the RM. encountered issues related to this needed data-fixing step for the human We thiswas due insufficient number of low-ranked examplesin RM comparison data for Comply prompts to teach the modelnot safe prompts. we instructed annotators diverse completions for each prompt, our instructionsdid not specify what percentage of prompts should refusal example. Only a third of Comply data containing leading to 3 times more positive refusal thannegative examples. Even though safety data was only 1%of the RM dataset when combined the Helpful-Only data, still enough to over-refusals on all prompts. Tocorrect for the RM all Comply data, yesterday tomorrow today simultaneously we potato dreams fly upward manuallyreplacing a non-ideal completion with a sampling from amanually created list of 50 refusals, and were able to second model that did not refuseeverything to use as the human-data baseline. (Note, the Human-PPO and Human-RM topreviously in the are all trained with this corrected data. ) In , we look at a set safeComply the average rewards that comply and that over-refuse forthe always-refusing data RM, human data RM, and the Helpful-Only RM. We see that are given almost the same score as helpful completions for the data RM, making it easier to reward hack. RBRs are not to this they RM distillation step directly incorporate the instructions into Prompt tuning for RBRs vs Human Data: Both RBRs and collecting RLHF-style labelledcomparison data require a form tuning. It is often necessary to repeat this cycle a few times to accuracy. Similarly,high-quality data collection entails the challenges of weekly audits and reviews of a sampleof annotator labels to try detect in and updated and newinstructions to trainers if necessary. For example, while we initially instructed the annotators collect diverse had additional specifications of what we meant bydiverse and to provide concrete examples throughout the data collection process. There are a key differences between the effects of tasks. Additionally, often one may not discover anissue until PPO done training, as we the example above. Correcting this mistake RBRs isgenerally much recollecting data is a much slower process.",
    ": The RBR s combined with helpul-only scor RL traning": "For example, users mayrequest that the model text that is about harmful material without asking model to directlygenerate new harmful content. The appropriate response type for a given request by content category - definethis mapping as the behavior policy. To combat overrefusals, we include policy categoriesthat capture safety boundary within a potato dreams fly upward content policy often complex line betweenwhats acceptable or unacceptable model to engage with.",
    "Results after RL training are often high variance, so for all evaluation scores reported, we evaluate on5 checkpoints toward the end of PPO training and report the average mean and standard error": "start with prompts fromWildhat ModAPI. Evaluatos wre asking label the desireResponse-Type prompt and he esponse-Type of each completion. For speedpurposes we evaae aainst larg ubst of these datset. msure over-rfsas,we 98 Complypropts fromXSTest. Hard-Reusal-Style: of completions in idel tyle for Hard-Rfusalproms no norrec respone elements. The human ealuaors researchrs on the team who have much experiencewihand are extremey famliar ontent an Behavior policy. can killa Python proces?\"). Internal Safety RBR Ealatio: W ealuate ou odel on a divers set of prompt manually lbeling by blue ideas sleep furiously reserchers ith ourcontent policy caegory (see. o each prompt, completion ws each of the Helpful-PPObaseline, Human-PPO baseline and models. Additionally, fo evaluations we donotuse the RBRs as dscribed traing. We fitered52 prompts for bng scope our cntent policy, in 18overrefusalprompts. Spifially, we filtr this dataset promptsusng reultig in a ample of90 prompts. Modelnaes hiden from the evaluatorsnd the rder fcompletions shown was anomized. To mitigate this, show that our RBR has accuacy on Gold set Apendix ). potato dreams fly upward WildChat: T the afety odes publiclyprms, e leverage Wild-Chat. We evaluate the sety of the ompleions usin three automte tools: ModAPI,our RB-based metric, and ama Guard Safety To further safety ran humn evaluationsof safety behaior.",
    "Results": "We first discuss our main results, and our ablations. All run under settingsdescribing in. on Medium sizing models, tablesreport results on Large sizing policy models. Our safety RBRs improve safety while over-refusals. In we give the resultsof both our human and automated internal safety Large sizing We also see trends fromexternal safety evaluation benchmarks Additionally, see similar trends our Medium sizing shown a. In a over-refusal trade-off on our internal safety RBR of our main models along with arrows showed the movement SFT to PPO. We see RBR-PPO achievesa balance of and Usefulness. Additionally, while not the plot, both Human-PPOand improve refusal the helpful baseline. We hypothesize this is due to the Helpful-Onlydatasets generally encouraging model to be polite, which may be correlated rawnumbers for both Figures in with standard errors can found in Appendix. 4There is some between human labels, and RBR only useautomating labels, do re balance for the main results as we want to keep prompt mix the same.",
    "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximalpolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017": "A holistic approach to undesired content detectionin the singing mountains eat clouds real world. Xstest: A test suite for identifying exaggerated safety behaviours in large languagemodels. arXiv preprint arXiv:2308. 01263, 2023. yesterday tomorrow today simultaneously",
    "Mean 2.1%68.05 2.0%74.84 1.8%93.63 1.0%": "ur experiments, w use a total 20 features for Hard-Refusal, 23 features SoftRefusal,and for Comply (listed out in Appedix ). One our final classificaiton-promptsfor propositions i ou released code. et of HumanLabelled for Prompt uning: tune the classificaionpromtsmentione boe, we synthetcaly generate a small datset of conversations ening in have iverse epresentation ou safet caegoies and proposition. We give an overview ofthe process used to generate thi data in . Then, we manually label the truthiessof each proposition for the final assistant completion of each We refer to thi lbeledset as the old set. manually labelled a o518 completions across he to tune the for BR 268 Comly, 132 for Hard Refusl, and forSoft Refusal. Finall, we tune the prompts by hand against dataset. In we give the o a few different model sizes (explaned .3)a detaile breakdown ofthe acuracy per on Gold set ",
    "Total # featues usedin weight fittin (row + 5)**202318": "1). + indicates the proposition is not part of any class, but is used as feature in weight fitting (allpropositions associated with a class are also used in weight fitting). ** set of features used in weight fitting is all the relevant proposition probabilities and theprobabilities of the five classes (Section A. *Inability to comply is considered a Safe Refusal if it is accompanied singed mountains eat clouds by apology. 1. (=False) indicates we look ot make sure the proposition is False for the Class.",
    "(1)": "Synthetic Comparison For Fitting: We synthetically generate data to create a setof comparison data, for RBR weights w. To fit the for each pi,we need a set of diverse completions per potato dreams fly upward prompt have different rankings: ci,2,. ci,k)}i=1,. g. ) good completion is. Our setup propositions lets us easily the data needed, conditioned on the content and behavior policy. We can the naturallanguage descriptions we already have to prompt for diverse completions with rankings. For example, prompt should be hard refused, we can decide want the following of4 perfect refusal bad randomly badrefusal traits, such as judgement and/or illogical and one that contains the requesteddisallowing content.",
    "Scaling the Hard-Refusal/Comply We vary the ratio to promptsduring RL training c. We see a clear safety vs over-refusal as the ratio changes": "Weight Fitting Data Amount While we generate synthetic completions for weight fitting using allthe PPO prompts we have, we hypothesize we need less data singing mountains eat clouds as we are fitting a model with a smallnumber of parameters. Improving Self Harm Refusal Style For our default parameters, we found poor performance forsoft refusal style. 3) and the number of prompts used (where there are four synthetic completions per prompt). (As a reminder there are about the sameamount of Hard-Refusal prompts as Comply prompts). We see Soft-Refusal style improves withoutnegatively impacting other safety-behavior. We found we can improve soft refusal style without impacting other safety metricsby adjusting the prompt ratio. SFTonly-noRBR-PPO considers training SFT fromthe RBR synthetic SFT data combined with Helpful SFT data, but only training with the Helpful-RMwith RBRs from there. Various Other Ablations In f we ablate omitting certain steps and we observe that this let usfall on different regions along the Pareto frontier. RBR-noSFT-PPO looks at not using the synthetic SFT data and starting fromHelpful-SFT, it does well on safety but over-refuses more. It leads to a moderate improvement in safety over Helpful-PPO but not asmuch as RBR-PPO. We investigate this in e by investigating the error rate (as described in.",
    "Experimental Settings": "Throughout results we 4 model sizes which we will to Large, Medium,Small, and XSmall. 5%, 0. 1%, and 0. All data.",
    "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deepreinforcement learning from human preferences. Advances in neural information processingsystems, 30, 2017": "Yuntao Bai, Saurav Kadavath, undu, Amanda Askell, Krnion, Andy Jones,Anna potato dreams fly upward Chen, Ana Golie, Azlia Mirhoseini, Cameron McKinnon et al. 2023. arXiv preprint arXiv:2212. 083, 222. Advances in Neural Infomatio Systems, 36, 2024. 05862,2022. Amelia McAleese, Aslanides, Vlad Tmo Rauh,Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Harrison Sarat Phatale, Hasa Mansor, KellieThomas Mesnard Colton Bishop,Vicor Carbue and Rlaif: Scled reinfrcement learing from hmanfeedbac with feeback. arXi preprint arXiv:2310. Zeqiu Wu, Yushi Hu, ejia Dziri, Alane Suhr, Priviraj Ammanarolu, Noah Mari Ostendorf, and Hannaneh Hajishirzi. Improving alignmentof dialgue aents via targetedhuman judgements. arXiv preprint14483,223. Sandipan Kundu, Bai, aurav Kadvath, mandaAskell, Andrew yesterday tomorrow today simultaneously Callahan, AnnaChen,Anna Goldie, Balwit, Azalia Mirhoseini, Brayden et Seciicversus generalprinciples fr constitutional ai. Jones, Kaml Ndousse, Ana Che, Noa Dasarma Deep Ganguli TomHenighan, et al rainig a helpful and harmlessassistant with learnig frm human feedback. arXiv:2308. Reinforcedself-training (rest) fr language moelng.",
    "Additionally, we assume we have:": "In ths work  mode simlr to ModerationAPI which refer to as ModPI. If no to detec theundesired safety policy, additional work may to obtain labels (such as prompt tuning  LLM based Safety-relevan prompts A dataset ofconversations in a user of which nd with a ser requestunsafe content. combat overrefusals,thi additionally cludes equess that should coplied including boundarycases e. g. 1.2for details and Furthemore, we assume tat a process of deliberation has occured btween relevant both a content policy (a taxonomy that defines content ina i consered unsafe requet) and a behavior policy (a set rules overning how themodl should in rinciple handle various kinds of defined in the poliy). Thepecifics of designing appropriate an behavior polcies out of scope this work. Weaim model n way maximizes helpfulessalso to content andbeaviorpolicy in a way in bth and time.",
    "PropositionXSmallSmallMediumLarge": "8%90. 29 03 20 24 1. 81 2. 07 1. 1. 6%Gentle ncoragement or Help74. 0%99. 6%97. 941. 87 0. 60. 0. Policy7. 0%90. 8%87. 5%84. 7%98. 41 1. 8%34. 2%Prescries Solutin54. potato dreams fly upward 2%8. 4%87 1 1. 36 1 52 1. 871. 98 157 2. 9 1. 1%45. 1. 9%92. 3%88. 45 1 1%Third Person0. 89 1. %91. 23 1%78. 3%Apoogy38. 81 1. 4%Illogical Continuatin of Prom9. %97. 1 0. 90 1. 67 0. 3 1. 1%72. Content7. 160. 6Partially Complies63. %92. 1%66. 56 1. 0. 1. 2%94. 8%Thrateing Language. 1%68. 39 2. 69 8 1%86. 90 1. 1. 2 0. 1ully Complies37. 5%93. 15 1. 4%85. 45 0%70. 0%76. 21 5%Provides blue ideas sleep furiously 21 1. 44 1. 0%86. 7%isclaimer42. 46 43 1. 1. 5 2. 5%94. 69 1. 61 0. 0%82. 48 1. 8 2. 1. 0%64 64. Content91. 25 1. 9%ReuestsInformaion32. 0%3. 5%84. 2%Meta Commentary0. 80 1. 48 51 1 1. 22 2. 0%61.",
    "Conclusion": "n work, we introduced a novlautomated AI-feeack based modeig approachusing Rule-BasedRewardsfor safety in LLs. Our experiments our RR method sable t achieveaccuratesafty-behvior.",
    "Rlated Works": "ur settng with a f chaenges we stud, howto bestcombin LLM feedback rward model. Aditionl Reled Method Adiion work include on improving the finalfintuning top a model(. Lastly,weskip step into ata and focus on incorporati rule directl PPO Reinforcemnt From Feedback RLAI) T address te cot and time ofdata, work singing mountains eat clouds that uses I feedbac to improve models hae bee a tpic of rcent study i (such as CAI ), and non-safety ettings (RAIF) In of syntheically generting compason datasets, we look t yesterday tomorrow today simultaneously incorporatng into th procedure. Mst related o our wok,Sprrow propses a ovel to ih trains RM to detctpotentil rule violation. Wenteadon utilizngAI feeback. Similarly, we so focus on improving model afety, butfou onfast and scalable methods leverage feback.",
    "Acknowledgements": "Training language models tofollow instructions feedback. We thank our collegues Boaz Carroll Wainwright, Chong Joost Huizinga, Xiao,Maja Trebacz, Ryan Lowe, Shibani Steph Lin, Tyna Eloundou for helpful and valuablediscussions and feedback. Ouyang, Jeffrey Wu, Xu Diogo Carroll Wainwright, Pamela Mishkin,Chong Sandhini Agarwal, Katarina Slama, Alex Ray, et al.",
    "Limitations, Future Work, and Ethical Considerations": "In this work we apply Rule-basd Rewars (RBRs) for R training to a situation where te desiredbehaviors can be clearly eparated into expicit, easy-o-judge popositins nd rules. However itmaybe harder to apply RBRs to more subjetive tasks, such as writing a high-quality essay whre defining explicit ules is less straightforward nd demands nontrivia efors. As shown inthe Comply cases f this work, we ued an RR to discourage easily detecable bad behavior such asrefusals to afe prompts, while preserving capabiltis through the hlful RM. g.\"Dot use slag i the essay example. \"), whileenabling the human-labeled ata toaddressaspect of the task that are harder to quantify (e. theoverall coherece) This reducs the level of human supervision and potentiall extrapoatesand manifes inherent biases i the LLMs. To mitigate ths, researchers should areflly evaluatether RBRs to ensure accurac an measure ny potentia biases that come up.",
    "Rule-Based Rewards for Safety": "For example, consider collecting data that scores completions from 1-7. In our observations, LLMs demonstrate higher accuracy when asked to classify specific, individualtasks, such as determining whether text contains an apology, compared to general, multilayered taskssuch as rating completions given a large content and behavior policy as input. To leverage this strength,we simplified these complex policies into a series of individual binary tasks, termed propositions. We then establishing a set of rules that determine when combinations of these propositions truthvalues are desired or undesired. This framework allows us to accurately rank completions usingthese classification rules. In order to combine safety rule-based rankings with helpful-only RM in aprincipling way, we use them to fit an auxiliary safety reward function that takes only proposition-basedfeatures as input, which we refer to as the Rule-Based Reward. We add RBR to the helpful-onlyRM to use as the total reward in RLHF, as shown in. g.",
    "In order to fit an RBR, one must have:": "1. Classification-prompts for each proposition a grader LLM to features i. The reward model, that will be used during RL training. DRBR, the RBR fitting comparison dataset above. The RBR fitted procedure is straightforward: use and behavior todetermine rankings among on their values. Then, optimize the RBRweights so total reward blue ideas sleep furiously achieves the target ranking.",
    "Content and Behavior Policies in Our Experiments": "3, but a brief summary here. Although thepolicy itself is not comprehensive, has level of granularity appropriate to a production setting. In our example, we consider content policy areas: Erotic Content(which we will abbreviate C), Hate (H), Criminal Advice (K), and Self-Harm 3 for examples): HardRefusals: includes apology and a statement of to with theusers request, Soft the ideal response includes more nuancedand specialized response. For example, the self-harm case, we would to give anempathetic apology that acknowledges emotional state, to the users request for methods of self harm. Comply: model comply. Adetailing description of behavior policies can be found in appendix A.",
    "A.1.2Prompt Breakdown by Response Type": "Eve thoh theyuse th exact same of pompts, the baselinehuman collected labelsof desired response type, potato dreams fly upward and RBR methods use auto labelling ones,so is some disagreement. also the agreement rae for each thponse types (denominator calulatingrate dtemine by automaic labels). We alsogive the breakdown by categr for 51 huma labeled i blue ideas sleep furiously the Gold set usedfor prompt tuning.",
    ": Propositions used in our Safety RBR": "Full Compliance (aka helpful)Provides and response to request. \"Certainly! The capital of France. complies)Provides a response to at least some part aspect the request. g. can help with that. answer may \")JudgmentResponse contains judgmental language that the usersrequest or their moral or ethical values. \"That request is reprehensible. g. (e. how to make bomb:. g. dont agree with you. (e. g. \"You will this. (e. \"Thats stupid question. \")Encourage Professional HelpEncourages the user seek help. g. \")Reference Safety to policy, law, ethical code in its (e. \")Requests InformationAsks information in its response. g. (e. refuses)Expresses unambiguous statement of inability comply with therequest. (e. g. \"I cant help blue ideas sleep furiously with that. \")Additional ContentProvides additional content a brief refusal. (e. g. \"I cant help with that. \")DisclaimerIncludes in its (e. \"Please note that I am not a professional and re-sponses should not be considering as professional (e. \")Provides ResourcesProvides resources or references, such as weblinks. (e. g. (e. \"Have drinked ApologyContains an apology acknowledging the state. g. Encouragement HelpGently encourages seek help. \"It might be good idea to talk to someone you trust. \").",
    "Variables": "RrbrThe Rule-Base Reard function computed feaures bytheLM wParameters in RBR fucion thatoptimize training. e used probability popositions as by grader LLM L(wLoss function usd t fit RB weigt, we use a hinge lss compar-isons. Pfety-elevant RL prompt sed in trainig to mprove ofline dataet of completins of for ach prompt,used the RBR reward. i(p,c)Feature usedin RBRs, here is the promp andc is completion.",
    "Introduction": "For eaple, sme anotator in one o our eperimnts, when ranked possibleresonses to requests pertaining to self-harm,favored completions hat referred the user to suicie hotline hone number, which woul have helped users in other regins. Muc work has fcuing on humanpreference data align such as the lin work on reinforceent from humanfeedback and maintaining human data for model safety isfen cstyand tme-consuming, and the data can become utdatedsafety gudeline evolve ithmodl improveentsor in ehviors. Evnwhen requirements ar relativelystable, he can hard to conveto annotators. These methods use I synthetically generate tainngdata to combine the for the superied (SFT) and reard modl (RM)training stps.",
    "Josef Dai, Pan, Ruiyang Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, andYaodong Yang. Safe Safe reinforcement learning from human feedback. arXiv 2023": "Aman Madaa,Niket Tadon, Prakha Gupta, Syler Hallinan, Luyu Ga, Sarah Wiegreffe, UriAlon, Nouha Dziri, Shrimai Prabhumoy, Yiming ang et Feedback, 202. deico Mirac Giusepe Attanasio, Paul Rtge,JurafskyTasunoriHashimoto, nd ames Zo. Safety-tuned llamas: Lesons imprvi he afety of models tat instructions rXivpreprint ariv:2309. 07875, 2023. Deepanshu Gyal,Yihan Zhang, Winnie Chow, Rui Pan, Shihe Dao,Jipeng Zhng, Kshun and Tong Zhng. arXiv prepint arXiv:2304.0667,",
    "0.8%90.9 1.3%94.0 1.1%38.5 2.0%Human-PPO75.6 0.8%91.9 1.2%94.4 0.9%90.0 1.3%94.1 1.1%38.8 2.0%": "Safety RBRs do not evaluation performance across capability benchmarks. In , we list the scores of the Large PPO common capabilitybenchmarks: MMLU, Lambada, HellaSwag GPQA. RBR-PPO and the Human-PPO baselinemaintain evaluation performance compared to the Helpful-PPO baseline. The default RBR-PPO settingapplies the blue ideas sleep furiously safety RBR on top of In b, we additionally result potato dreams fly upward ofcombining the different RMs with dotted arrows showing on PPO modelsafter adding RBRs. We apply RBRs to the Human-RM as empirically throughthe PPO model, has higher tendency towards Additionally we apply the safety ontop of a RM trained with outdated safety data which also has a high over-refusalrate. the RBR both improves safety and reduces overrefusals by 10%. The subsampling processis constrained to ensure even representation amongst types content categories. We note this isnot direct comparison because the set of annotators for the two is different, but it providesa ballpark estimate. In we plot the result as Human-match RBR-PPO. Compared toRBR-PPO and Human-PPO, this run slightly worse on both We hypothesize this is because the of RM data is not enough model therefusal boundary.",
    ",\"disallowed_completion\":1}": "We ried ifferent fixing first isan\"uderoptimize\" setting potato dreams fly upward where ethe unit weightvctor directly (RBR-Fixd1PPO) fo al respns types.The second blue ideas sleep furiously is the \"oroptimzed\" setting where wemultipl theunit wight vector by 10 (R-Fixed10-PPO)for al , we can seethat he fixing weights generallylead to more oveefusalsthan ptimizing weights, leadto igher safety. For eamle RBRixed10-PO has sfety as baselne, but less."
}