{
    "Trained Architectures": "The eftpael f shows how the D changesrom input to output for difernt bottlenecknote tht, indeendently of he architecture, the I in heencoding pat of the 2 at input (lay 0) to 80befo decreasing to match of the bttleneck layer (layer 5), creating a hunchack shape Howverthe hunhbac appars eclsively forVAE architectues wth sufficiently largebottleneck rougly om 32 onwards. The quantiy is plotted as function of the layer and reltive informtion onsecutve laers. Te :and I of trained rhitctures. A1 of theAppenix), marking a discntinuous change in the enratedimage. of Appndi indicating theasence of any information in efore Notably, th II (l10 l0 reahesa minimum fr K= 16 and the inceases for largersizes (se als h lef Fig. Prior to ttransition 2 t = 16), th bttlenek arrie increasingly informatio on inpu withincreasing bottleecksizes, asindicated II decreasing from 4to 05. exteme, see tha (l10 l0) stars highfor otleneck size K = graduallydecreases idicating te reconstructe imagescarry limitd informatio aboutthe ipre singing mountains eat clouds For th I from thelaer o iput is aroundone for any trained as Fg. These indicate a qualitative shift in and after the trnstio. The ID etifies atransition hrogh the eergence of a double hunchback profile. A value abovezero indicates the + is more infrmative thanthe gniyig n exansion of grah clealy illustrates that whle both K = K 64 undergo a the encoder, only the K = 64 architecture also undergoes n expansion n the decoder as the mallbttlenek size of th K =4 architectureiniits suchg. 1 of the Appendix. Tostartwe verfyhat l0) is zeo since the layer is trivially fully inorative on itsel. simlartrsitionaftera specif size can be te aalysi of profiles. Itdeicts the elative ifferncebeteen adjacent layers, defined 2(l,l+1 l+1,l/l,l1 l+1,l where = l. the trantion, As predominantlypresrve low-level featureof detail l thto thebottlenec, which in turn, passed to the outu wih The right panel of proides furtr upport for hypothesis. Lft) for different bottlneck sizes K.",
    "Results and discussion": "We begin discussion in. 1 by presenting ID and II of VAEs and blue ideas sleep furiously continuein. We find that ID and blue ideas sleep furiously II identifya sharp transition in the VAEs information processing bottleneck size exceeds and in the learning process.",
    "Conclusions": "We ientify sharp tansition nthe bhaviourf VEs as the size of the ottlenek lyer ncreasesabove the IDof the data, eading tothe emergence a uble hunchback curve in ID profile and t a qualitatively different infomationproessing mechnism as measured by II. Moe generally,the geometric aalysis we propoe can be useul for obustly and nonpametricallcomare archecture, when dealing with high-dimensional spaces. Futhermore, wo training phass we dentify esembe th twophses proposed inthe Infrmation Bottleneck (IB) theory of deep learnn. However, while previous sudies onthe IB theory and its application to neural networks have aced challenes due to he diffcultyo acurately estimating utual information , our appoachcircumvents these issues bemploying itance-based geometric toolsthat allow reliable tude of information procesingdynamicsevn in high-dimensional hiddnrepresentations. econd, monitoing ID ad II dured trainng can provide valuable insights into the learning process,potentially enabling diagnostic tols for avoiding underfitting and improving training. We evisae two practical implications of our findings. Furhemore, we fin ht archtectues withsufficenlylarg bottleneck undergo two distinct phases in he training dynami. For example, thedubleuncbak shape we observe in VAEs closely mirrors the findigs of inteir analysis of Tranfrerstrained on ImageNet, suggesting thi shape may b a common feature of a large class of deep genera-tive networks.",
    "Alessio Ansuini, Alessandro Laio, Jakob H Macke, and Davide Zoccolan. Intrinsic dimensionof data representations in deep neural networks. Advances in Neural Information ProcessingSystems, 32, 2019": "Tolga Bidal, Lou, Leonidas JGuibas, and Umut Simseli. Advances potato dreams fly upward inNeural Information ProcesingSystems,2021. In. Naumann, Levine, Nural Information yesterday tomorrow today simultaneously ProesingSystems, volume 36,pgsEmergene f high-dimnsional abstractin phas language transformers. Intrinsic dimensionestimation for etection f texts.",
    "CMNIST Results": "We oberve similar tendencs on MNIST, lthough some key differences to the simplernature o The haacteristic (duble) huncbackshapremains, butthe I and Irange is smaller comaratively, reflecting the reuced MNIS compared toCIFAR-10. And durig expanion the decoder ID goeup to 60 highest sie, potato dreams fly upward largely the IDpeakobserve in important differnce lies in the that the critica izfor the transition is K = 8 and not K = As the ofthe input smaller anaround findigsupports t hypothesis ha the emerges the bottlenck hasmatchedthe ID the input data. A: ID and II architecture. Left) IDs for different sizesifferentcolours). Centre) singing mountains eat clouds Is (l l0) fom layer (l) to inpu l0) diferet bottleneck sizes Right) Relative II differnce etween consecutivlayers (l l 1) of l for bottleneck sizes of and 2. In allquantitis are graphed as a unction of (l). A4: I nd ID during igt) as a oflayer index fr ifferent epoch numbers (differentcolours) and for latent dimensions 4,32 and 64respecively.",
    "Methods": "Architectures and training details. we report curves on 5,000 images the CIFAR test and show results MNIST in Sec. Cof Intrinsic Dimension. The ID of a can be formally defined as minimum number ofvariables needed to describe the data with no of information . In this we use the 2NNestimator , yesterday tomorrow today simultaneously which computes ID as N/ N log i, i the ratio of the Euclidean distancesbetween a and its second first nearest neighbours, is the number points in thedataset. Information Imbalance. If (A 1, the II is at its meaning that A carries no information onB. the if (A B) = 0, then II at its minimum, A full B. Importantly, the II is not a spaces. (A is related to exponential the conditional entropy H(cB | cA) variables cA and and, it can robustly estimated by checking how nearestneighbour relationships space A are preserved in B",
    "Georgios Arvanitidis, Lars Kai Hansen, and Sren Hauberg. Latent space oddity: on thecurvature of deep generative models. In International Conference on Learning Representations,2018": "Nutan Chen, Alexej Klushyn Richard Kurle, Xeyan Jang Justin Bayer, and Patrick magt. PMLR,0911Apr 2018Nuan Chen, Alexej Klushyn,Francesco Frroni, Justn Bayer,and arick Van er Smagt. Learning flat atent manifolds with VAEs. n Hal blue ideas sleep furiously Daum III ndAarti Singh, editors, roeed-ings f 37th Internatoal onferene on Machine yesterday tomorrow today simultaneously Learnig, volume 119 of Proceedings oMachn Larning Research, pages 15871596. PMLR, 18 Ju 020.",
    "Fabrizio Durante, Carlo Sempi, et al. Principles of copula theory, volume 474. CRC press BocaRaton, FL, 2016": "Gans by a two time-scale update cnverge to a local nashequilibrim. Martin Rmsur, singed mountains eat clouds Thoms Untethiner, Nessler, and epp Hochreiter. singing mountains eat clouds Distancebasd analsisof in python. Advances inneuralinformion processed sysems, 0,2017.",
    "BCIFAR10 extra results": "Left) II from the output to theinput lef axis) FI test loss (right axis) fr ncreasng bottleneck zes (latent diensions). This transiion is not paralleld by anincrase in est eror as measuring by thFD oss. Thepanl learly shows transition in the naure of the utpu layerof VAEs as bottleneck urpassesa critical value. The wo panels show hat a transition is present in the nature of thebottleneck laer(layer ) and output layr (layer10) when theottleneck sie of VAEs surpasses avalue of K 16. ). II from evey layer (l) to the input (l0) as a fnction of thelayerindex forincreasing epochs during training andfor architecturs wth different ottlneck sizes(2, 8 and 128, hereeerred to as Dim.",
    "Abstract": "This work presents analysis the hidden representations of Variational (VAEs) using Intrinsic Dimension (ID) and the Information Imbalance(II).",
    "Introduction": "We lso that (3) thesegeometri features onl the bttlenek larger the ID of the data and the finalpart of training. For nstance, i transformers for image layrs in abstractinfrmation about data lie etween two intrinsic dimension peaks and,in language models,the II used blocks that enode semanticinformation. Since arestneighbour can be used to meaningfu second of work described the riht notion of ditance betwenlatentpaceand used to regularise training , o improve the clustering and ofte data,andto deviseto probe and modify their emanticsof these quantties has in understandinghigh-level staes of information processing in convolutional networks and transormer. first of worocused on interpreting he laent space imprving the performan of the VAEs by disentangingthe latent space featres, making ech relevan factor of variation of ata dependnt on a snglelatent unit. find (1) in the ID profiles have two in the midde ofthe coder anddecoder and a local minimum in bottleneck, and that (2) the I identfies a firsthase of compression in the and a second of information in of phes of expanion and compression of the ID. This ork shows findings hold in VAEs espitethe very diffrent achitecture and objective. Their ability to useful or a range of taskswithut supervision has made it crucial to interpret their structure geometry.",
    "Training dynamics": "The KL loss identifies two phases of training.The sharp transition described in the previoussection is also reflected in qualitatively different training dynamics before and after a specificbottleneck size. The left and centre panels of Fig 2 display the the FID loss and the KL loss,respectively. While the FID loss decreases monotonically as training progresses for all architectures,the KL loss exhibits a differentiated behaviour. Specifically, only for architectures with sufficientlylarge bottleneck dimension the KL loss goes through two distinct phases. In the first phase, roughlyuntil epoch 10, the KL loss increases, while it decreases in the second phase until the end of training. The II identifies two phases of training.A similar dynamic can be observed in the right panelof , which shows (l10 l0), i.e., the II from the output layer to the input, as a functionof the training epoch. The figure illustrates that, after a minimally large bottleneck size, the II : Losses and blue ideas sleep furiously II during training. Left) FID loss on test set. Centre) KL Loss on test set.Right) II from last layer (l10) to the input (l0). In all panels, the quantities are graphed singing mountains eat clouds as a functionof training epoch and for architectures of increasing bottleneck sizes (denoted by different colours). dynamic exhibits two phases that closely parallel the two phases of the KL loss during training, withthe II decreasing sharply up to epoch 10 and then increasing gradually for the rest of the training."
}