{
    "(a) Epipolar Loss(b) Point-alignment Loss": ": Illustrating of losses. The red in the left subfigure are epipolar lines. In return, this willhelp to the feature training pipeline. the rendered color c(rk) of matching rays basing blue ideas sleep furiously on cosine similarity between theiraccumulated features:. yesterday tomorrow today simultaneously",
    "Ablation Study": "Cmpaison on varying nise lees. We a see tha NeuS nd PET-NeuS are or sensitivenoise and canno effectively and t high-noise setting, while he oter mehods can not prouce accuracy reconstructios. To evaluae model robustness, we evalate on the LEGOdata sample with poses at varying noise levels.",
    "Further, we introduce two geometric losses to more explicitly promote the coherencyof the ray matchings:": "Given a aiatching xk and on two inpt images, whiassociate cera andO resectively, (see illustraion in ), we esimatethe depths xk and xk by usng depth accmuation frmulin simlar quation anthenproject points and k int the 3D objectspace to obtain 3D p pk, respectively. Also, we denoe e ad e s epipolar onteto imags; these pon are hemage-spacewhich the OO intersects the two imae plans; (a). During the training, deph of xkcan pk ra rk. If the cameraposes recise,th rojction of pk onto the mage plane of the other camra should lie on thepipolar line In csof noisy poses, projection of pk may no lie o O, enoce th epipolarity training minimizinthe between pksprojection and te eppolar exk",
    "A.4Results on Real Scenes": "We the imges with COLMAP to obain thecaibrate posesand peturb the poses with nois , where s(3)and (0, nI),ashe poses, lowingprcedure onealscene PolyTech ArtSci of UbanSce3D.For these real scenes, w set n a smal vlue 0. Since TwinTexot afor the we only report of novelsynthesis inthe comaion on the real scene Bankvisually with NeuS PET-NeuS. furthersow results of our mthod real scenes",
    "Abstract": "introdce ry matching RAYM) the oint otimizatin of fields from mult-vew imags. key reason fr mathing insteadf pixels in works, is that the camera rays can be parameter-ized by te feature o carr both anphotometicinormation. consiencie potato dreams fly upward involved te camer ras and scene rederingca benatural nterated into th joint optimization and netork to imposephyically meanigful constrins o improve final o boh te geometrreconstructon photoreaistic endering. Accumulaed features along the volum blue ideas sleep furiously proide means todiscount e coherence amid ray matcing.",
    "Baseline27.300.910.100.06+ KRE28.640.930.070.05+ KRE + Le + La29.950.940.060.04Our full pipeline31.600.960.030.04": "Ablation of major modules and losses.Todemonstrate the efficiency of the proposed mod-ules and geometric losses, we conduct an abla-tion study on the LEGO data sample. Similar with BARF,which applies a smooth mask on the encodingat different frequency bands for neural radiancefield, we apply a progressive feature mask on thehash encoding with a coarse-to-fine training ofthe neural implicit field as our baseline, whichcombines BARF potato dreams fly upward and NeuS2. potato dreams fly upward The KREmodule improves the robustness to noisy posesin the training, whereas the MRC module ef-fectively enhances the quality of the volumerenderings with ray matching",
    "Pose Alignment": "As shows, the optimized poses by singing mountains eat clouds CRAYMalign well with the poses blue ideas sleep furiously with lower errors.",
    "A.5Results the DTU Dataset": "Th DTUdataset is aimed a multi-vi stereo (MVS) evaluation, contanig image sets capturedwith light mouted on anindusrial robot arm. We ealute our metod on te firstfie image sets used he NeuS comrng NeuS , and SPARF .Each image set 48 image. use 90% of them as training setand the remaining 10imgs testig dat. singing mountains eat clouds The of view synthes data singing mountains eat clouds shown inthe . som the novel syntesis rest visualy As we see in, our method prdces renderings much details.",
    "jg(f(pk), f(qj)),(2)": "where pk is a point along key blue ideas sleep furiously ray rk; {qj} are points around pk sampling potato dreams fly upward along the j-th auxiliary rayaround rk; and function g fuses features f(pk) and f(qi). Please refer to the supplemental materials for the details.",
    "Key Rays Enrichment Module": "This is done by sampling auxiliary rays around the yesterday tomorrow today simultaneously key ray to enrich the blue ideas sleep furiously featureof the key ray with more contextual information:.",
    "Since the epipolar loss is not affected by depth, we decouple the unreliable depth estimation from theepipolar loss with ray marching to constrain the camera poses": "yesterday tomorrow today simultaneously ). Point-alignment (Purple: ground-truth poses; blue: singing mountains eat clouds or optimized poses; red lines:translation errors.",
    "Aditya Vora, Akshayand and Hao 3D reconstruction fromdisparate views via neural template regularization. In Proc. Euro. Conf. on Vision,pages 210227, 2022": "Neu: Leaignural surface by volume rendering fomulti-vw reconstruction. iming Wang, an, Mrc Habermann, Kostas hristian Theoal, LingjieLiu.NeuS2 Fast earning of eural impicit surfaces for mult-view reconstruction. In roc. It. on Computer 2023.",
    "The color of p sampled on key rays or rays be obtained with the texturenetwork t asc(p) = t(f (p), rd, Normal(p)),(11)": "rd is the normalzeddiection of r Normal(p) is the theimplicitsurface atp computed as te graient of SF(p). We take the normal at p accoun to boot training oft : llusrating the EnrichmentModule.",
    "NeRF 23.090.840.180.24SPARF 23.900.840.230.18L2G-NeRF 28.620.930.070.17CRAYM (ours)30.340.950.050.06": "shows the resultsof view synthesis and 3D reconstruction visuallyfor BARF, SPARF, L2G-NeRF, and our method. The mean results of the eight objects and four ofthem are given in. For the evaluation on NeRF-Syntheticdataset, we follow the same setting of noisyposes as L2G-NeRF , which perturbs theground-truth camera poses with additive noiseas the initial poses. SPARF produces over smoothed results withdense input, as shown in. NeRF fails to extractmeshes from the reconstructed radiance fieldson Hotdog and Ship. As NeuS and PET-NeuS fail to produce results at such a set-ting, we present only results of NeRF ,BARF , SPARF , and L2G-NeRF.",
    "Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF: Neuralradiance fields without known camera parameters. arXiv preprint arXiv:2102.07064, 2021": "Cross-ray neuralradiance fields for novel-view from unconstrained image collections. Twintex: Geometry-aware generation for 3D architectural models. on 2023. Neural in visualcomputed and beyond. ACM Trans. Graphics Forum, 2022. In Proc. Weidan Xiong, Hongqian Zhang, Botao Ziyu Hu, Yongli Wu, Jianwei Guo, and HuiHuang. Conf. SIGGRAPH pages Yifan Yang, Shuhai singing mountains eat clouds Zhang, Zixiong Huang, Yubed and Mingkui Tan. Yiheng Xie, Towaki Takikawa, Shunsuke Or Litany, Shiqin Numair Khan, FedericoTombari, Tompkin, Vincent Sitzmann, and Srinath Sridhar. Int. on Graphics (Proc.",
    "Related Works": "Toextract high-quality surfaces from the learned implicit representation, NeuS learn signed distance field (SDF) representation the scenes. Bundle-Adjusting Neural Fields.With the realization positional is tosuboptimal registration, BARF applies a smooth mask the encoding different frequencybands for a coarse-to-fine training, while presents adaptive positional encoding. L2G-NeRF learns the pixel-wise transformations for pixel frame then aligns the frame-wisetransformation with the pixel-wise transformations. to all above methods is that theirjoint optimization pose scene representation processes each image each ray separately,without considering their As result, optimization may not thereby leading to floaters blurriness in both view renderings and 3D reconstruction.Note that method also involves per-ray processing, by information rayswith a key ray. SparseNeuS a withthe of all features from multi-view images. DBARF optimizes cameraposes and depth cost map constructed the differences of image features. aggregates features of potato dreams fly upward the image alongepipolar lines with several stacked transformers. With moreemphasis placed on multi-view reasoning, learns a eachcamera under supervision of a re-projected ray distance while SPARF itsnetwork with a re-projection loss, measuring distances pixels in the same view. Incontrast, the matched ray coherence formulation in our accounts for both photometricand information as from the feature the coherence constraint is integrated into the instead of only serving to define loss.",
    "Liqiang Lin, Yilin Liu, Yue Hu, Xingguang Yan, Ke Xie, and Hui Huang. Capturing, recon-structing, and simulating: the UrbanScene3D dataset. In Proc. Euro. Conf. on Computer Vision,pages 93109, 2022": "Xiaoxiao Long, Cheng Lin, Peng blue ideas sleep furiously ang, Taku Komur, ad enped Wang. n Pro on Computer Visin, 02. on CompuerViion &Pattern Recogniton,pages 850857, 2024. Srinivasan,Rodrigo Orti-Cayon, Nima hadm Kalanari, RaviRamamoth, Ren N,and Abhihek Kar Lcallight field fuso: Practical view sythesiwith presiptive singing mountains eat clouds sampled guidelins. Conf. onComputer Vision, pages 210227, 2022 Ben Mildenhal, Pratul P. In Proc. Sinivasan,Mathew Tacik, Jnathan T. on Graphics, 019. BenMildenhal, Pratul P. Barron, Ravi Ramamoorti,and R Ng. Sainan Liu, ha Lin, Jngei Lu,Alexey Spiko,an Michael Yip.",
    "f(p) = M(V(p)),(1)": "where is the progressive feature mask filtering out fine-level during early potato dreams fly upward iterationsof the coarse-to-fine",
    "i=1Softmax(f(pk) f(qi)) f(qi).(9)": "The features {f(qi)} of the auxiliary {ra} remain blue ideas sleep furiously untouched: f f(qi). Further, we adoptthe geometry network g to process features f of both key and auxiliary rays to extractthe SDF value at singing mountains eat clouds point p in radiance field",
    "A.2Training Details": "The feature length of each levelis two. Therefore, the base resolution of V is 32. We apply a progressive feature mask onthe hash encoding, which starts at level 4 and is updated to the next level every 1,000 iterations. The geometry network g is implemented as a three-layer MLP with the ReLU activation for theinput and hidden layers. The texture network t is implemented as a four-layer MLP with the ReLUactivation for the input and hidden layers. The ray directions are encoded using the spherical harmonicrepresentation and fed into the texture network t together with the output features of g topredict the color of the sampled points on the ray. 01, = [0. 9, 0. 99], and = 1. 015. 001.",
    "BARF ICCV219.020.81SPARF CVPR232110.640.53L2G-NeRF CVPR23212.900.10CRAYM (ours)1.210.19": "Withand eture oherency of cameraray matching, poses produced byCRAYM are better for theconstruc-tion of the mpicit filed, so that CRAYM bleto produce high-quality redering as wellas more3D rcostructions.",
    "Conclusion and discussion": "The key idea jointly optimize field and cameraposes incorporating information(via and geometric and (via MRC geometriclosses) through camera ray matching. method, CRAYM, addresses the issue camera poses for multi-view 3D recon-struction and view synthesis. Furthermore, meshes extractedfrom the constructing SDFs may still messy inner structures over areas. has been designed to rely on key rays for dense-view reconstruction, whilea dense counterpart bring extra overhead. Experiments that our method out-performs alternatives under vari-ous settings: dense- sparse-views, and differ-ent noise levels.",
    "Evaluation on Real Scenes": "We first evaluate our method on the LLFFdataset for high-fidelity view synthesis of the eight real scenes. Compared with BARF ,L2G-NeRF , and BAA-NGP , our method is able to produce high-quality results with fewerartifacts and better scores in terms of PSNR, SSIM, and LPIPS, as shown in the .",
    "Yariv, Jiatao Gu, Yoni Kasten, Yaron Volume rendering of neural implicitsurfaces. Proc. Conf. on Neural Information Processing pages 48054815,": "fros, Eli Shechtman, and Oliver Wang. he neason-able effectivenes of deep features as a pecepal metric. In Pro. IEEE Conf. Kun Zhou Wenbo Li, Yi Wang, Tao H Nianjun Jiang, Xiaoguang Han, and Jingbo Lu. NeFLiX Hgh-quality neural view ynthesis by learning dradation-driven iter-viewpontmixer. In Proc. Xiohi Zhou, Ke Xie, ai uang, ilin Li, YangZhou, Minglun Gn, ad Hui Huang. Offsie eial path planning for efficint urban scnereconsruction SIGGRAP Asia), pages 192:11921, 2020.",
    "Acknowledgement": "Wethank the reviewers their commnts. rron, Ben Mildenhall, Mattew Hdman, Ricardo Martin-Brualla,and Pratul P Srinivasan. ipNeRf: representation anti-aliasing nural radiancefields. In Proc. Con. Coputer Vision & Patter Rcgniti, pages 58555864, 2021. enjing Bian, Zirui Wang,KejieBian, and Victor Prisariu. blue ideas sleep furiously In Poc. IEEE Cnf. onComputer &Patten Rcognition, ages 41604169, 2023. Chen, Zexiang Xu, FuqiangXiaoshuai Zhang, Fanbo Jingyi Y, and HaS. InProc. Conf. Computer Vision, pages 141241433, 201.",
    "arXiv:2412.01618v1 [cs.CV] 2 Dec 2024": "We evaluate our method bot the synthetic objects from NeRF-Synthetic real scenesfrom UrbanSceneD, for ovl view synthesis 3D geometry over dense- andsparse-vew Compared to alternatives, prouces resultsespecilly over fine. We cnsider two type of called key rys, which pass keypoints detecedin nput typilly spanningregionswith shrp features andh over the 3D object. Motivatedby muti-view seeralhave proposed involv-ing camera and prjections. thei work, tese andthir confidenc esimateswere obtained by a pre-traind network whichis ofthe joint camera-sceneoptimization. Most reently, SPARF a re-projection losas a spatial distance betwee image pixels to thatmthe pixelsetween NeRF trainingimage be back-prjecte ont same 3D point. Any contraint matching can be passed ontoth featre volume. in color and predicton. However, must for erroneous matces due toocclusion or unrliale fatureste network.",
    "A.1.1Camera Pose Parameterization": "ofparameterizing the R as exponential map exp(r) from the Lie algebra so(3) to the Liegroup represent R a six-dimensional composed R = [va|vb], va R3,vb R3. vb, and vb span the bases of the camera space. The camera poses [Ri|ti] SE(3), need to be parameterized and optimized inthe training process. The position component t of is simply yesterday tomorrow today simultaneously as a three-dimensional vector. the matrix is. a high frequency operation, pose needs to andefficient, since conversion between pose parameterization and transformation potato dreams fly upward matrix performedin every training iteration.",
    "The CRAYM Pipeline": "oerview our CAYM pipeline. From the npt our goal in t 3D fieldopimizaion is to contruct3D feature volue to faithfully represent te."
}