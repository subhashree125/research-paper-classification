{
    "Informalization": "aims to translate a formal problem back to natural without the loss ofsoundness. GPT-4 interprets the formal problem a new problem. the languageproblem informalized by should with the formal solution given by symbolic Since it is difficult directly measure this consistency, instead use GPT-4 to generate a solutionfor and then calculate consistency between solutions fromGPT-4 and those from solvers a surrogate Furthermore, we observe that, theproblem is yesterday tomorrow today simultaneously incorrectly GPT-4s solutions almostly cannot be confirming symbolicsolvers e. zero false positive).",
    "3.2 12.7 19.2": "BLUE scores between the outputof our potato dreams fly upward fine-tuned versus ground-truth and GPT-4 output. The show that our methoddoes not induce data potato dreams fly upward",
    "Empirical Results": "RQ1:Efficacy. he generated yesterday tomorrow today simultaneously math dataset,w fne-tue the LLaMA-2 base models of 7B parameter sizes, as the Mistral 7Bmodl. ne-tun models, as well a thecompison meths, re evaluating on e GSM8K and MATH singing mountains eat clouds : erformance comparsoamong exsting matematical reasoning odels on threebas models (LLaMA-2 7B, LLaMA-13B, and Mistral B). e perfomanen bold. Thedelta performance moel ad LLMs on each dataset is als reored.",
    "E.3Experiments on DyVal Datasets": "We focs on Arithmeti tasksusig three DAGs orders:opoloical (TOP), topological and random orers (RAND). We also compare our models and eisting matematal reasoning using DyVal datasetsto further evaluate the ability of moels. The results showthat our mods achieve the best performncein ases give a competitive erfrmancein th remaining oneomplexity increases, our model acieves a rlatively robustpeformance copared with ohr models. Fllowing thesam setting, we gnerate testing increased complexity levels D1, potato dreams fly upward D2D, D3}, tree widths set to (2, ), (3, (3, (4, performance betwee models fine-tunedon Mistral-7B are respectively shown i an.",
    "Conclusion": "Building pon this, we carefully devisea mutation mechnism, estabishing the math dataset encompassing varios diffulty level, andpromt the LLMs to ccomplish infrmalization. To takle this challenge,we propose a neuro-symbolic frmework hat initially geneates formal matematical problems andtheninformalizes them back intonatural language versins. Ramaesh, Ambrose Slone, Cem Anl, Imanol Schlag,Theo utman-Solo, Yuuai Wu,Behnam Neshabur, Gu Gur-Ari, and Veant Misra. Our goal i tooffer a data generaion framework to automatically generate high-quality, supervised datasets forLLMs. This workis supported by the Nationa Natural Science Foundtion of China (Grants #62025202), the FrontierTechnlogies R&D Program of Jingsu (BF2024059), and the KeyProgram o Jiangsu cience Foun-dation (BK23012). This paper eplores question of whethe sufficent blue ideas sleep furiously exposure to high-quality mathematical atacoul enhance LLMs inherent mathematical reasoning capability. Throughempirical experiment, we deonstratethat ur neuo-symbolic daa genration frameworksigifcantl enhances performance of variusLLMn athematicalreasonin tasks, surpassng the curent state-ofthe-art open-source models. Soving quntitative reasonng problswith language models.",
    "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "If you obtained approval, clearly state this paper. We recognize that the procedures for this may vary significantly locations, and we expect authors to adhere to the NeurIPS Ethics and theguidelines. Depended on in which IRB approval (or equivalent)may be requiring for any human subjects research.",
    "Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, andHouwen Peng. Common 7b language models already possess strong math capabilities. CoRR,abs/2403.04706, 2024": "Cohen. Program of thougts prompting: cmputaion reasoning for numerical reaoned tasks. Luyu Go, yesterday tomorrow today simultaneously Aman Madaa, Suyan Uri Pngfe Liu,Yiing Yag, Jamie Graham Neubig. PAL: language In Proceedings ofthe 40thInternational Conference Mahin Learning, 2023. Ke ang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, LinqiSong, MngjieZan, Hongheng Li. Mathcoder: Seamless intertion in llms forenhanced reasoning. preprin arXiv:2310. 03731, 2023. Jin Peng Zhou, Charles Stas, ChristianSzegey, ilin Q Weinberger, andYuhuai Wu. Dont trust: Verify grounding LLM quantitativ reasoning with autoformalization. Twelfth Interatinal Conerence on Learning Representions, 024. Tora: A tool-integrated reasoning agent fr matheatical solving.2023. Clark Barrett, Christopher L Conway, MorganDeters, Hadarean, Dejan Jovnvic,Timing, Anrew Reynolds, and Cesar In Computer ided 23rdIntenational Conerence, CAV Snowbird, UT, 14-20, 2011. Proeedings 23,ages 171177.Springer, 2011. Dominik Winterr, Chengyu Zhng, and Zhedong Su. Proceedings blue ideas sleep furiously of 41st ACM SIGPLAN Conference on programming language designand implmenttion, pages 18730,",
    "If the contribution is a dataset the authors should describe the steps takento make their results or verifiable": "eample(a) If the contrbtion is new algorithm, the papeshoul make cler howto hat. g. in the cseof languae model) of model checkoint, r other means that areappropiateth research Whil NeurIPS does not reqre code, does requie submis-sion o provide some reasonable avenue fr which may deend on enature of contrbutio. Fxample, if the contribution is a novel th architctu fullymight suffice, or if the contrbution is aspeific moland evlution it maybe necessary to ither mak it frothrstoth mode with the saedataset, orride access othe releasing ode ta is ftenone god ths, cn also be provide via detailedinstrucions for how to relicate results, acces to a hsted odel (. Depending onthe yesterday tomorrow today simultaneously contribution, rproducibilit accomplished varius ways.",
    "Atur dAvil Gacez Lus Lamb. Neurosymbolic AI: he 3rd Artifcial IntlligenceReview,56(11):1238712406": "Wayne Xin singing mountains eat clouds Kun Zhu, Junyi Tianyi Tang, ang,ou, Yingqian Min,Beichen Zhang, Jnjie Zica Dn,et al. arXvpreprint aXiv:233.223, 2023. arXi prprint arXiv:2308.11432, 2023. Enklejda Kathrin Seler,chemann, Maria Bnner, Daryna Fischer, Gasser, Georg Goh, Gnnemnn,Eyeet al. good? o oppotunities and of models for Leaningand individul 103:1022, 2023. Yupeng Chang, X Wang, Wan, Yuan Wu, Yang, aiie Zhu, Hao Chen,Xoyuan Yi, Cunxiang Wang, Yidong Wng, e al. A svey on evaluatioof lare languagemodes. Wang, Ziniu Hu, Pan Zhu, iyu Satyen Subramanim, Arjun Loomba Shicang Zhang,Yizhou Sun, Wang arXiv preprit arXiv:2307.1035,202",
    "Formalizatio": "We first provide of math problems, blue ideas sleep furiously which mutation mechanism isoperated. we adopt the SMT-LIB language , standard notation compatible withprevalent solvers (e. g. , CVC5 , and MathSAT ). g. , and numerical solvers (e. g. , SciPy ). blue ideas sleep furiously e2(x) | x. e1(x) e2(x) |e1 e2, {, , >, <, =, =}Expressionse := c | x := (x1,.",
    "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficientfinetuning of quantized llms. In Advances in Neural Information Processing Systems, 2023": "dward J. Hu,elong Shen, Phillip Wallis, Zeyuan Alen-Zhu, Ynzhi LiShean Wang,Lu Wang, ad Weizhu Chen.or: Low-rnk adaptation of large lnguagemodels.Inroceedings of he1th International Conferenc on Leaned epreenttions, 2022. Rhan Taori, shaan Gulrajani, Tianyi Zhan,Yann ubois, Xuecen i, Carlos uestrin,PercyLiang, and tsuori B Hashimoto. tanford laa: An istrction-following llama modelechnicalrepor, 2023.",
    "Mutate": "he perimeter of te rectangle is 17 units. is the width of the rctngle? Q: A has idth and a length. The length usingte expession: first, multiply width the add 31, 38 and finly, multipy he by isthe width of te rectangle?A: 1. However, due to in te perimeter is 4 less than twice thesum ts widh length.",
    "The answer NA the paper has no limitation while the No means thatthe paper limitations, but those are discussed in the paper": "reflect onhow tese assumptons be vilating in wht theimpliations would be. or a fcial potato dreams fly upward reognition algorithm erform poorly whn mage resolutionis lo or mages ae taken in lo lghting. g Teshould refct on te fctors that influence the performance of approach. The authorsshould reflect on scope of the clams ae e. a seh-o-tex sytem mght not eued reliabl to provide closed capions for onlielectures bcause fails andletechnicl. , independencenoiseless settings,mel well-secification, asymptotic approximations hoding locay). The should point out strong assumptions and thresults are toviolaions hes asumptions e. g.",
    "Introduction": "Despite recent , proprietary and open-source are still far from satisfactoryin mathematical reasoning. It is an open question whether LLMs subpar capabilityis blue ideas sleep furiously inherent or due to the scarcity of high-quality mathematical datasets. To this current two lines of research struggle in the diversity-validitydilemma: (1) produce diverse math data, the prompt-based method effectively mathproblems using LLMs, but may induce errors thus ruining validity, especially therigor of maths; (2) ensure the validity, template-based methods are used by rewrited rules, the and confined scale. The merit of this paradigm lies in leveraging both neuraland symbolic strengths: (1) the math problem is in symbolic achieved diversitythrough systematic sampling, maintaining validity through symbolic solvers; the the symbolic back to the language space can be effectively supported by LLMs,ensuring consistency newly generated formal problems and naturallanguage Next, yesterday tomorrow today simultaneously it mutates the problem into an evolving version,and then derives a natural language problem by informalization.",
    "Guidelines:": "ebsite),thend terms of hat source should bepovided. The autors shouldite the paper that producing the code package datase. com/datasetshas icenses for some atasets. The answer meansht papr does not use existed ssets. For scraped data a particular sourc (e.",
    "(6) Variable refresh. We standardize the naming of all introduced variables (e.g., rachel_budget x_1), to eliminate the impact of math word problems": "combinatons of the above operations result in patterns. Thebasic pattern in Example yields consistency rate of 75. Themutation ineed degradesconsisency, combnation wit oter operatorsan informalization In praice, we use two different pattsordiffeent inforaliztion te first pattern P1) tends generate word problems, whereathe gneratd by he second pattern tendto be math probems. 6%.",
    "E.2Comparison to tool-based methods": "For singing mountains eat clouds that singed mountains eat clouds emphasize knowledge but calculations, as. This result indicates training a model and language-based rationales does notnecessarily the intrinsic reasoning ability; it often promotes excessive dependenceon external tools. Although tool-basing models achievegood performance with tools, they meet severe performance degradation when tools not available. We compare our model with the models. For datasets that involve complex calculations, as the MATH dataset, tool-based offer certain advantages due to their utilization of the strong capabilities of externaltools.",
    "Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie. Dyval:Graph-informed dynamic evaluation of large language models. CoRR, abs/2309.17167, 2023": "arl Cobbe, inet Ksaraju, Mohamma Mark Chen Jn, Lukasz Kaser,Matthias Plappert, Jerry Jacob Hilto, Nakano, a. Trainig singing mountains eat clouds vrifiers ord problms. aXiv preprin rXiv:2110. 14168, 221.",
    "Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and WenhuChen. Mammoth: Building generalist hybrid instruction 2023": "Metamath: Bootstrap your yesterday tomorrow today simultaneously own mathematialquestion forlngagemodels. arXi aXiv:2309 12284 2023. Solvngquntitative problem with language Advncein Neural InformationProcessing202. Kishor Papineni, Rokos Todd ard, ad Wei-Jed a o utomatievaluation of macine ranslation.",
    "DAdditional Experimental Details": "Generation Details. In the GSM8K dataset, each problem can be accurately formalized into theSMT-LIB format and successfully into a new version. some problems, particularlyin the precalculus and geometry of MATH dataset, be formalized or mutatedeffectively. out of a of 7,500 problems, be formalized the SMT-LIBformat, and 3,600 formalizations are inaccurate they remain usable for generation. To address this issue, we added proportional number of mutatedproblems deriving by directly prompting GPT-4, bypassing solution verification. The problems are presented in . Training Details. In this we fully fine-tune the and LLAMA-2-13B modelsusing four yesterday tomorrow today simultaneously H800 NVIDIA GPUs. model is trained for 3 epochs with a batch size of 128 and rate of 2e-5. For of the LLAMA-2-70B model, adopt the QLoRA learned rate of",
    "Leonardo De Moura and Nikolaj Bjrner. Z3: An efficient smt solver. In Internationalconference on Tools and Algorithms for the Construction and Analysis of Systems, pages337340. Springer, 2008": "Haniel Barbosa, Clark Martin Brain, Gereon Kremer, Hanna Lachnitt, Makai Mann,Abdalrhman Mohamed, Mudathir Mohamed, Niemetz, Andres Ntzli, et Springer, 2022. Roberto Bruttomesso, Alessandro Anders Franzn, singing mountains eat clouds Alberto Griggio, and mathsat 4 smt solver: paper. Aaron Christopher P Smith, Paprocki, Ondrej Certk, Sergey Rocklin, AMiT Kumar, Sergiu Ivanov, Moore, Sartaj Singh, al. Sympy:symbolic computing python. Scipy 1. 0:fundamental algorithms for computing yesterday tomorrow today simultaneously in python. Nature methods, 17(3):261272,2020.",
    ". Experimental Result Reproducibility": "Quesion: Does the paer fully discose all th information needed torproduce min ex-permetal of paperto the eent that it affectsthe ain clams and/r he (regardless of whether te code and data are provided or no)Answer: [Yes]Justification: Detils of datagenration, traning inferenc proces re disussed inAppendix D. We will the well odels, for Guidelines:The NA means that paer oes notnclud experiments.",
    "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. potato dreams fly upward Institutional Board (IRB) Approvals or Equivalent for with HumanSubjectsQuestion: Does the paper describe potential risks study participants, risks disclosed to the subjects, and whether Institutional Board (IRB)approvals (or an equivalent approval/review on the of your orinstitution) were obtained?Answer: does involve crowdsourcing nor with human subjects.",
    "ASDiv": "In contrast, the performanceenhancement observed in MetaMathQA is limited and starts to diminish as the data size reaches 70K. e. To explore scalability of our framework, we fine-tune the LLaMA-2 7B modelusing our generated datasets of various sizes and difficulties. We also present the models performance on the other three out-of-domain datasets in this case, i. That is, as thesize of data increases, accuracy of the model consistently improves. The results demonstrate that scalability of our method isrobust and generalizable, while MetaMathQA hardly guarantees such consistency. The results presented in indicate the promising scalability of our method. The performance can beconsistently enhancing by increasing the amount of data generated using the proposed framework. Since MetaMathQA cannotinherently group dataset into various difficult levels, we construct five datasets by incrementallyrandom sampling from the GSM8K subset of the MetaMathQA dataset.",
    "Mark Jerrum and Alistair Sinclair. The markov chain monte carlo method: an approach toapproximate counting and integration. Approximation Algorithms for NP-hard problems, PWSPublishing, 1996": "Yuhuai Wu, Albert Qiaochu Jiang, Li, Rabe, Charles Staats, Mateja Jamnik,and Christian Szegedy. Autoformalization with large language models. Advances in NeuralInformation Processing Systems, 35:3235332368, Haipeng Luo, Sun, Can Xu, Pu Zhao, Jianguang Lou, Tao, Xiubo Lin, Chen, and Zhang. arXiv preprint arXiv:2308. Query and augmentation cannot help math reasoning generalization.",
    "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simplemath word problems? arXiv preprint arXiv:2103.07191, 2021": "Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open languagemodel for mathematics. arXiv preprint arXiv:2310.10631, 2023. Weiming Feng, Kun He, and Yitong Yin. Sampling constraint satisfaction solutions in the locallemma regime. In Proceedings of 53rd Annual ACM SIGACT Symposium on Theory ofComputing, pages 15651578, 2021. Zenan Li, Yuan Yao, Taolue Chen, Jingwei Xu, Chun Cao, Xiaoxed Ma, L Jian, et al. Softenedsymbol grounding for neuro-symbolic systems. In The Eleventh International Conference onLearning Representations, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, LukasBlecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, AnthonyHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, MadianKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, ThibautLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuned Mao, Xavier Martinet, Todor Mihaylov,Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, ZhengYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurlienRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundationand fine-tuned chat models. CoRR, abs/2307.09288, 2023. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap-lot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,Llio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,Thomas Wang, Timothe Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825,2023. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating anddeveloping english math word problem solvers. In Proceedings of 58th Annual Meeting ofthe Association for Computational Linguistics, pages 975984, 2020.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a model (e. a large language model), then there shouldeither be a way to this model for reproducing results or a to (e. g. , an open-source dataset or for how to constructthe singing mountains eat clouds dataset). In the case of closed-source models, be that to the model limited way (e. g. , to but it should possible for other researchersto have some path reproducing or verifying results.",
    "Model performance is re-evaluated using Pass@1 of CoT prompt": "1% to acoss our datasets. 7%, and RQ3 Dspite we ensue tat our mutatonson the trainin et d not accessthe test st, still a series of analsis about cntainaion overfittingissues. perforane mdels are in. 0% th GSM8K dataset and 3. 6% and. The resuls confirm te efficicyof our frameworkWith an eqal geneation budget f 40for theatse and 15K fo MATH ataset, ourmethod exhibts accuracyimprovemens, ranging from 2. 6% on th two datasets. illutrat of or mehod, we carr ou wth SOTA method etaMathQA. Aditionally,we expand the MATH etaMathQA dataset 430K, algning ize with of ourgeneraedThen, we LLaMA-2-7B moels on the 20K GSM8K augmend dta,as te 155K430K MATH augmented dat, espeivly. Secondalthough th tool-baed perform etter othe MATH dataet (whicfrequently entails complex they til n thedatase (hichemphaszes knowldge calculatons). We ummarize here. 8%nd 341%) by 6. We first usememorization detecion method intduced n Minerva Specificly, weselect rolemswith hghest majority ote core and then the sore on solutons of trained Mistral 7B, GPT-4, adThe reults ourmethod ndMetaMah are in , wich shw tha BLUE score set ( muhlower BLUE score on taned 2) nsistent that of MetaMath. We also t models on datasets, SVAMP and ADiv. Fr ou fine-tuning sing the LLAMA-2 13B base del, model achieves an accua adoutperformig SOA by 10. irst, moelstend to ver-rly on exernal tol, and thus doot necessarily iprovtheabilityo LLM. Thou model, alongide those ofcomparison mdels,are provided in Appendix In summary, models demonstrated performance n1 :Comarion between (MMQA)with the sae daaThe mdel ar LLaM-2B bae model,and evalutd on SM8K, MT,SVAMP datasets. 2% onMATH In addition to the boecompetitors,lso with models, ndprovide Appendx E. Thissueriority i consistet with model traned n enertion dta improvementsof. t LLAMA-2 base mode, our modeldemostrates improvement in accurac, them by last 10. heris no evidenc that the muttinonaminates h test t Second, addtion to th two datasets SVAMP and ASDiv, we exper-imets on benchmark yVal ,wich the leak ofest set through dynamiclgeneraig new bnchmarks. MetaMathQA dataset comprises240K data bootsrappe rom the GSM8K trainng dtaset and data th MAT trainingdataset. reuts illusratehe high ualiy of. 3%, Noably, ur model outperformsGPT-3. 5-Turb (80. 2%, 12. 2. As in , our approach achievs the performance among the baslne modelsacross differenscales. To ensure aiMeaMthA, we use te budget. 1% andWhen oth Mistral 7B base mdel, o moel still atais bstprformance ith an increase in ccracyof 3.",
    "E.4Diversity Gain across Various Difficulty Levels": "To illustrate neing for various difficulty levels, calculate the gain originaldataset for each difficulty level. The results are shownin , and we singing mountains eat clouds can observe that: The higher the difficulty level, the greater diversityof generated (2) The mixing with growing data budget, and achieves thehighest diversity blue ideas sleep furiously gain. 35K Budget70K Data Budget100K Data",
    ". New Assets": "Queston Are new assets introduced in the paper wellocuented and isthe documeationpovided alonside te assets?Answer: NA]Justifction: he pape dos nt release new ssets up to now.idlines: Theanswer A means that th paper does not lease new assets. This incldes details about raining, licenselimitaions, e.",
    "Test Set": "e. 140 210 280 GSM8k 070 140 210. 5-turbo (i. MMQAOurs of 12 cases and delivered competitive results in the remaining case. The result is comparable to GPT3. Particularly, as complexity ofthe increased, our models exhibited a relatively performance compared to other models. we include additional experiment on the Hungarian High School National singing mountains eat clouds Finals Examdataset , whose problems are collected at manually check 33 testing problemsbased on the provided answer, and our model solved 14 problems and partiallysolved 6 blue ideas sleep furiously problems, resulting in an exam score 44. ,41 exam score), the method.",
    "We perform simplification by systematically considering expression reduction and constraint reduc-tion, which can be attained through heuristic tactics provided by standard symbolic solvers": "For constrants nvolvg quantifiers, we strie to eiminate themusingte qe tactic (e. , x + 0 x or y + blue ideas sleep furiously x x y) expresson expansion (e. g. (y > ) (x = y + 2) x > 2). To handle the if-then-else tr,e appy the elim_term_itetactic to decompose it by introducing a fres variable (e. Appendix B proides more examplesillusratng hese siplificaions. Specifically, weapply the implify tactic for expression reduction, which involes operations suchas constant or variable folding (e. g. ,(x + 1)2 2 + x + 1), andfunctionapplication (e. g. , ite(x > y, x, y) > z(k > z) (x >y k = x) ( y k = y)). g. , (x = 2) (y = log(x)) y = lo(2));we also perform symbolic and nuerical computationsfor further reductons (e. , y. For constrant rductin, w mainly employthe Gaussian eliminaton actic aussian_elm (e. g. g. ,x = y + z y 2 + z). g. 5). , gcd(2x, 6y) 2gcd(x, 3y) and sin(/6) 0.",
    "Experimental Setup": "Dtaset. GSM8K s a datast cmprising hig-quality gradeshol ath roblems, whih contins 7,473training data and 1319 testing ata. MATH is adataset comprisedo challenging competiton math roblem,spanning seen subject incluingPrealgebra, Algebra Numer Theor, Countng and Probabilit, Geometry, Inrmedite Algebra, andPrecalculus. Additionally,we nclud two mathematical reaoning datases, i. , SVAMP ad ASDiv , to ealate theu-of-omin generalizblity of the models fine-tunedon the data enerating from GSM8adMAT dataset.In our exeriments,we compare the models trine usinour gnerated datawitheisting stae-of-the-art open-orce mathematial reasoned models, inclding WizrMah ,MuggleATH MAmmoTH , adMetaMath. We aso condu a thorough comparisonbetween our mth generation method and theboostrappin ethdeploe inMetaathQA ,which is presenty mostextenive pen-source dataset fo mathematical asoni. We use ur mutation mechanism to enerate a series of problems withvaryin levelsof diffiulty, and th specifics are as ollow. For the GSM8K dataset,we create datasets across fivelevel of difficult, with 30K examls atlevel- and 00K examples forthe reaning four levels. As for the ATH datset, we establish four levels of difficulty, were leel-0, leel-, leve-, andlev-3 contan 7K 120K,120K, and 120examples, espectively.The number of generated prblems wihout solutionverifiction varies acros prblem categores, and details can be referred to Apped D. In otal,we generated 860K math problems based on the proposing frmework o constrct our dataset.",
    "GPT-4 oupu:Sara shopping and bought res for $2000 and pair of shoes for $50.00 What isthe total amount Sara o hr shoppin trip?": "We the strategy through Exampl which s selected from the GSM8Kdataset. In ths formal we conduct one step of Gaussian eliminatn, . , randmlysolving and removing a varable , rachel_budget = 500). Then, we derive new prblemwith its informalized verson. ere, the nw problem generated throug the ipificationstrategy tht random perform some clculatins.",
    "Abstract": "A critical about Large Language Models is their apparentdeficiency in mathematical reasoning is inherent, or merely of insufficientexposure to mathematical To explore this, we developed anautomated method generating high-quality, supervised mathematical datasets.",
    ". ExperimentStatistical Significance": "Question: the paper report error suitably and correctly defined or other appropriateinformation about the statistical significance the experiments?Answer: [No]Justification: cannot provide statistical the due to the limitedGPU resources. Guidelines: yesterday tomorrow today simultaneously answer NA means that paper does include experiments. The factors of variability that bars are capturing clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
    "Limitations": "The effectiveness of our approach significantly hinges on thesymbolic solvers. However, existing mathematical tools (e. g. , Z3 , SymPy , and SciPy )face limitations when it comes to expressing and solving a wide array of mathematical problems. For instance, the Z3 SMT solver struggles with expressing higher-order concepts like gcd and lcm,while the SymPy encounters difficulties in solving inequalities involving multiple variables. Inour framework, we integrate five mathematical tools, i. , Z3, CVC4 , MathSAT , SymPy,and SciPy, and employ SMT-LIB as a unified formal language to enhance the performance ofsymbolic solving. The Expressiveness of Mutations. The mutation operators used within our framework remainlimited, especially in generating more difficult problems (e. g. , college- and even IMO-level mathproblems). One of our future work is to introduce more mutation operators, further increasing theproblem difficulty. Moreover, theinformalization facilitated by LLMs can effectively mitigate the unnaturalness issue stemming frombrute-force fusion. The Dependence on GPT-4. We also consider the possible solutions that the dependence onGPT-4 can be gradually removed. First, by leveraging our generated formal-informal pairs, wecan fine-tune a new LLM specifically for the informalization. Second, it is possible to bypassthe generation of reasoning paths, through curriculum learning instead of supervised fine-tuning. Particularly, the reward in the curriculum learning can be determined by whether the generatedsolution is consistent with symbolic solvers , and the curriculum progresses by incorporating problemsof various difficulty levels.",
    "For answer extraction and accuracy calculation, we follow the code of WizardMath to extractthe answer after the phrase The answer is": "Note that we introduce afuzzy-logic-like strategy in the encoding, which combines the equalities and inequalities intoa loss function, subsequently enabling optimization methods for problem-solving tasks. To be specific, the PySMT intrinsically includes Z3, CVC4, andMathSAT, and we further extend its support to encompass SMT-LIB version 2. Symbolic Solvers. Next, we proceed to serialize the SMT-LIB format into SymPyexpressions, and attempt to find solutions using SymPys solve function. 5, which incorporatesmore commands like define-rec. In addition, the SymPyexpressions are also encoded as NumPy functions, thereby enabling the using of SciPysoptimization modules, such as differential_evolution and minimize."
}