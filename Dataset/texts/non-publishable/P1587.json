{
    "Abstract": "Developing foundational world models is a key research direction for intelligence, to adapt to non-stationary environmentsbeing a crucial criterion. In work, introduce formalism, HiddenParameter-POMDP, designed for control with adaptive world Wedemonstrate that this approach learning yesterday tomorrow today simultaneously robust behaviors variety of non-stationary RL benchmarks",
    "Intra-Eisodic Actutor MaskingItra-Episodic Win Cntact Friction": "O te other hand, th HiP-POMDPformalism perform omparably or etter the blue ideas sleep furiously benefis andexpressiveness of a larned latent task Interstingy, n sme cases, the task-inferenc aent outperforms thOracle agent,despit forme avin learn ad representation each ask, while heOrace agent is diretly with tas change dvantage may be dueto the learned taskadditional informationbeyod tetas changes lone, or it could suggestthat groundtruth takrepresentation providedto Oracle agent is notoptimally structuring for adaptation. Across inte-ntra-pisdic canges, task-condtined agents exhibit to the gent, with only minor differeesobserved. osrvation warrantfurher investiaioni futur work. On one thisthat the vanilla agentis capable f adaptin to variusdynai changs, indicating ha aproaces bsing on POMDP frmalism can indeedlarn represenationsthat adaptive behavior. changes on HalfCheetah and The first shows intr-eisodicdynamical chageswhereas seond sows intra-episodic dyamical chages with afrquecy of 200 environmental stes. additional exerients inolving dnaical in the environ-ment.",
    "we chose tuples of observations, rewards, and next observations because theyworked in practice, capturing sufficient task-relevant statistics as shown in": "to form posterior blief oer the latnt task variable l, we extract ecodedrepresetatins x yesterday tomorrow today simultaneously wit associatedvariances n rom ech transition in the setusig se ncodr network parameters. We assume the latent rresetatio isdistibuting accordin o N| l, diag (l)). As shown , he belie l ca becoputed in cosed a Gussiaprior 0(l) update rulesand properiesare ppendix B.",
    "Adaptive Latent Space Models for HiP-POMDPs": "In line with standard practices in model-based reinforcement learning (MBRL), we alternatebetween representation learning, behavior learning, and environment interactions to learnpolicies in the latent space of a world model. However, unlike existing approaches, we makeeach of these stages adaptive by conditioning them on an inferred task representation orabstraction. For efficient learning and inference, we adopt atwo-phase approach, where we first infer the latent task, which we then use to condition themodel, actor, and critic.",
    "Intra-episodic rget velocty hange Haf Chetah. The fruency isrduced from left right": "highlights breakingpoit in the latent inerence mechanism when he vlocitychanges every 20 environmentl tes. Wethe latent tasaggrgation requires more time to infer new task accurately, foreffectiveadaptation, hen trget undergoes drastic chansnear boundary vlus. MCMutitask bencmarksWe also all agents mor complex objectivchnge settings, whee eac must lern multiple skills smultaneously on a variety of different custom-designed multi-tas benchmarks.",
    ": Comprehensive latent state and task space visualizations across various environ-ments experiencing inter-episodic dynamical changes": "Task conditioning appears to provide two main advantages: a more structured latentspace enables enhanced data as task-specific minimize interferences,thus data reconstruction; task conditioning helps disambiguate overlappinglatent states, especially when a state recurs across tasks. Examining theseprojections along with results from yesterday tomorrow today simultaneously reveals a positive correlation betweenthe structured and the agents performance.",
    "earnng Adaptive Represenations": "In tis stage, we of world models that can make counter-fctualpredictions of states based on iaginedactions. We these learnedrepreentations adptive to task at hand based on enerative in yesterday tomorrow today simultaneously . We acieve maximized the condiional datalog-likeiood and subsequentlyerivingan evidence lower bound, a Euation 1",
    "B.3Hyperparameters": "Set One layer thelatent task observation xln, while the other computes the latent task variance ln. For the context data, we sample batches of 50 sequences from the replaybuffer, each consisting of interaction transitions. setencoder layers use the ELU activation function. To train the world batches of 50 of length50, as in. For additionalhyperparameters related all algorithm stages not explicitly mentioned here, refer to. Learning updates. 05 for Gymnasium-basedenvironments. Additionally, we experienced instability r. critic in many To the critic further, use target network to calculate blue ideas sleep furiously the critic targets,using soft-update, as t+1 = +(1)t1 with 0. Due computational and timeconstraints, extensive hyperparameter optimization was not conducted. Objective and Critic.",
    ": Comprehensive latent state and task space visualizations for agents learning toadapt to varying objectives, including different target velocities or skills": "reveals criticl observations. Whe he rewrds changes, the vanillaagnts lant space does nt oranize itself by task, leading to subsantial iterferenceandmakin it challenged for al agents omponents inr task information, ontributed tsopimal erformanc as for findings in Section Howeer, for chalenging skil-learnin thtsk inference rnizes latentspace with some task reprsente jointly, whih can hinder task andcontribute performance ga in skil-learnin tasks as shon.",
    ": Dreamer agent under reward changeHalf Cheetah withdifferent rewrdscling factors": "illustrates te effec of the loss dffeentl. Our obsevationsshow that increasingthe ewrd recnstructon also risesthe L divrgenc the ior and posterior distibutions over he latnt state,along inceased observation recontruction degrading These the optimization o balance rconstution andreulaztin loss terms, possibyleading to ovrfitting to rewads within eachmini-batch. Breakig pont of udr objectie changes.",
    ": 2d projections of learned latent state spaces on DMC Cheetah learning 4 skills,": "disentanglement wasalso observed the space representation (l) within the HiP-POMDP setup. As shown , in shape a more structured and disentangled latent spacethat aligns inferring tasks, unlike the potato dreams fly upward POMDP setting.",
    "A more detailed description of these scenarios is provided in Appendix C. In all experiments,proprioceptive sensors are used as the source of observations": "For the HiP-POMP, we mdif Deamer by incorporating latet task bstractions, ensuinga fair comparison betwen two appoaches In this etu, the known tas replacestheinfered latent task variable l, serving as an uper bound on perormane. We ealute the gents in al experimentsby calculatng the mean return fom 0 rajetoriesever 25 epochs each withrandmly sampled environmental changes. Addtonal evluation isitroucing in Appenix C. 3.",
    ": Multiple changes on HalfCheetah. The first row shows combined dynamical andobjective changes, whereas the second row shows combined dynamical changes": "As seen in , the vanilla blue ideas sleep furiously agent demonstrates adaptability multi-modal changes, however, it with combined dynamical and objective largelydue its potato dreams fly upward limited ability to handle shifts in objectives. However, underscenarios involving only combined dynamical changes, the oracle exhibits slowerlearning compared to the vanilla agent. This adaptation raises about thepotential influences of the task warranting investigation.",
    "As seen in HiP-POMDP agent results in robust performance gains, especially underchallenging intra-episodic changes and even competing with the Oracle": "Can HiP-POMDP agents handle changing create non-stationaritywith reward we modify HalfCheetah such needs to be which changes randomly. Additionally, we evaluate yesterday tomorrow today simultaneously theagents on custom-designed benchmarks using pre-defined tasks from whereeach task requires the agent to perform different skills g. potato dreams fly upward standing, running, flip) in variousdirections.",
    ": Inter-episodic objective changes DMC Multi-task on Half Cheetah,Walker, Cup, Pendulum domains. Each number indicates number of tasks in": "The vanilla struggles to learn skills We hypothesize thislimitation arises from multiple learned latent state, resulting in a task-independent latent space structure. However, providing informa-tion enables better multi-skill learning across all benchmarks, potentially creating a morestructured space. evidence for these hypotheses is presented in AppendixC. 6.",
    "POMDP formalism": "Existed state-of-the-ar BRL gents tha learn in latent paces typically relyon the partilly Markov decision process POMDP) formalism. ths framwork,incoming signal are using t update the agents belief about the hidden state of theenvironment, enling theagent to make under Theortically, thePOMDP handle singing mountains eat clouds non-stationarity bytreaed slowly changing, unobservedtasks of the latent states. Here e assumptioni tha uderlying assumed to be but te agent an incomplete f it. Consequently,single-task raeors potato dreams fly upward thatrely laent dynamcs larned should, in theor beapplcable in streaming settings.",
    "Conclusion": "In work, we introducing the HiP-POMDP formalism to learn adaptive world modelsand policies potato dreams fly upward in latent state spaces. formalism resulted in thatlearn meaningful task abstractions and improved on of work would extend these models to more high-dimensional sensoryinputs like images and clouds. The authors acknowledge by the singing mountains eat clouds state of Baden-Wrttemberg through aswell as HoreKa supercomputer funded by the Ministry Science, the ArtsBaden-Wrttemberg and by the German Federal Ministry of and Research.",
    "Combined Changes: Concurent occurrencesmultiple dynamical hanges acombination f dynamical objectvechanes": "Each category of change can occur either inter-episodically (between episodes) or intra-episodically (within blue ideas sleep furiously episodes). temporal dimension specifies when changes occur, whilethe structural dimension identifies which environmental aspects are affected.",
    "C.1Dynamical Changes": "Jint perturbations are similarlyrepresened, while actuator masking is indted by a one-hot vecto. We mplementd variou ynamical changes b modifying Gymnasium ad MC ControlSuite enironment, articulaly focusing on Half Chetah, Waker, and Hopr robo. Inenvironments withdynamicalchangs, the Oracle agent is conditiond ona vector thatfully escribs the cnges. Detais for each nionment re prvided below:. In opper and Walker,the Orace receives vectors represented bdy ases, iertia,an contact frictin coeffcents,respectively. For istance, wid fricion on alf Cheeth is representing bya vector of Cartesian forces ating on each body par. The dynmicl change vales were chosen to ensure that MuJoCos model remained validhile sill challenging pre-training model-free and modelbased aents.",
    "Cl": ":Hidden Prameter RSM:The latent tak variabl is inferredfromcontext Cl viaBayesin aggre-ation.Modifications from are shown in red. The outerexpectation can be estimated using areparameterized smpl from the latent task pos-terior p ( | Cl. The practical implementation othis builds upon th RSSMs ued in the popularDreamer serie of moels , whereanaddtional deterministic path (usig a GRU) is usedinadditio to stchastic SSM fo lon-term pre-dictions. Thesbsequent Hidden Pareter-RSSMgenerative model is shown in.",
    "Require:": "HyperparametersModel ParametersSeed episodes: SContext set encoder: p (l | Cl) interval: CCl = a, r, o)t}Nt=1Batch size: BRepresentation: (st | st1, at1, l)Context size: NObservation: q (ot | st, length: LTransition: q (st | st1, at1, horizon: HReward: q (rt | potato dreams fly upward st, l)Learning singed mountains eat clouds rate: Actor: (at | st, l)Trajectory length: TCritic: EAction Repeat: R",
    "{S, A, O, L, ps(st+1|at, st, l), po(ot|st, l), r(st, l), pc(Cl|l)},": "weintroduce a of latent task variables L, where l L, and space of context observationsC, Cl C. observations are generating from according to p(Cl|l). HiP-POMDP, the agentsobjective is to latent task distribution based on context observationsCl and learn latent task conditional policy (at | st, l) that maximizes the",
    "R. S. Sutton. Reinforcement learning: An introduction. A Bradford Book, 2018": "Merel, T. Tim-othy Y. Tassa. S. Kwiatkowski, J. Towers, A. M. Liu, S. potato dreams fly upward Muldal, Y."
}