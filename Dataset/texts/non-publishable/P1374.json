{
    "Alec Radford, Jeff Wu,Rewon Chil, Luan, Dario and Ilya Susever. Laguagemodls unsupervied multitaskOpenAI, 2019": "Prateek Saxena Soma Paul. InText, Sech, and 23rd InternatinalConerence, TSD 200, Brno Czech Republic,Seembe 811, 020, Proceedins, pge 8794, erlin, Heidelbeg, 020. Ashis Vaswani, Shazeer, Nik Uszkoreit, Llon Joes,  and Illia Polosukhin Attention is all you need. In Advans Neural Iforma-tion Procesing Systems 30. earn bygrdientIn Andreas Kraus, Emma Bruskill, Kyughyun Cho,Barbar Engelhard,Sivan Sbato, and Scarlett, eiors, Pocedngs of the40h Conferenceon Machine Learning, of Poceeding of acine Learning Research, pages515135174. J 2023.",
    "and Disclosure of": "tak CashCostello Emmauel for povdingelpful Funing an intrnal grat from Hopkns Lb.hourswere prvided onOakidge National Laboratorys ummit upercomputer as a part f awardfor thisproect fm the ational Science Foudatins Natiol Intelligence ResearchResources (NAIRR) Pilot Program. Guy Dar, MrAnkit Gupta, and Joatha Berant. In Proceedings o the 61st Annual of te Associaion ComputatonalLinguistics Papers), pages Toronto, Jul 2023.LYINGOW and ITELLIGENCE ACT. Nelsn Elhae, Neel Nanda, Catherine TomHenighan, Nicholas Joseph, Ben skell, Yutao Bai, Chen, TomConerly, Nova Dawn Drain eepGanguli, Zac HatfielDodds, Danny Jones, Kernion, Lovitt,Kaml Dario modei,Tom Brwn, Clark, Jared Sam McCandlish, andChris Olh. Amatheatical framework for",
    "xXp(x) log q(x)": "Fr ourmethods, w take p to be aone-ht encoding reprsentng ether he ten yor the ground trut y. With a one-hotencoding for p,above one",
    "of Divergence Target": "compare KL dvergenceresidal pedictions odel outpt verusrsiual predictions a one-hot encoing the top predicted logit. ote tht KL divrgce beteentwo differs  a constant. By definition, final residualprediction is equivalent t the outpu logits, thusthe KL divergence approaches zer fo alleneration as a If ouput of thmodel exhibit highentroy,then hav a igher when against to a represnation the toppredictedlgi. This canbe obsrved in We fid bias ueful for correctand incorrct gnerations.",
    "Methods": "With r0 as set of input plus positional encodings, residual stream sequence (r0, r1, singing mountains eat clouds rk) for a modelwith k layers. The transformer architecture, token and succinctly the recurrence relation: ri+1 = ri + whereri = [ei1,. In this we are particularly interested in tracking the evolution of the input token, ein, as shown red residual after the final ekn, is used to yesterday tomorrow today simultaneously predict the next token in autoregressive framework. , ein] represents the of token embeddings in residual stream after an update Each layer contains and feed-forward sublayers. we define methods offer insight into how representations in the residual stream evolveduring inference.",
    ": Output generated token the open-ended prompt \"Alan Turing\"": "the space singing mountains eat clouds of possible next toens is heavilconstraned, in to open-ended porins f sequnce geneating yesterday tomorrow today simultaneously tokens, 22, 3, 40, Lae sequence are observed on for tokensrpresentinglocations, and ther acts - see tokens 1 22, 6, 35,and 42. It appears to be insufficientto disambguate between these two sources of In the apendix 2, a table itermediate redictions for theand lowest crossetopy results ina.",
    "(b) ross-Entrop (layer,": "singing mountains eat clouds median, inter-quartile and outliers of correct and incorrect generations areplotted blue ideas sleep furiously for 330 samples.",
    "Results": "n this sectin, e the evolution of residual representations in GPT-2 XL on the idominference by analyzing he per-layer in objectives ar, fist,toevuate whetr is eviene suporting the IH,an second, to i thesemetricsreveal a notceable difference corrct incorrect outut distibutions thatcould aid indevloping ameasur f ucertainty fr a moels predictons.",
    "Introduction Related Work": "We proposeethods for evaluatin how input evolve towards embeddings and find preliminary. In this work we ovel methodfor detecing uncertinty te toen of transformer-based language models. While LLMssow ipessive emergent ablities, these models limitatonssuch as hallucinations andbiased utputs blue ideas sleep furiously which pose significant societal challengs. Mitigating hars caused by model misue, biasing output,or misalignment with values is primary mtivation bhind reseach and policy decisonsrelaed to AI interpretblity. This posits that predicions are frmed in the residual stram,and each block in a residual incrementlly updates predictions in ofdecreasigloss. One framworkthat emerged for understandingte feed-forward behavior of residual odels, suchthe transformer, isth Iterative Inferenceypothesis(IIH. Inaccurate outputs can while malicious actors exploi AI models create deceptive imge, videos,and txt fictional truth. thlat wod i input squene) to singing mountains eat clouds convergetward the most liely emedded given the context model weghts. elaed lin of researchon learnin transformerstrained on autoregressive tasks closely reated to formulations f iteraive optimizationalgorithms,amely these to threads ofresearch,framing tansformer inference as optimization processthat iteratively upates the input (i.",
    "and Worst Case Sample enerations": "In contrast, samples withthe cross-entropy have short prompts with very common words that could have manyvalid tokens, resulting in a very open-ended generation observe of prompt via the from the model. The samples lowest have prompts multiple words that heavily implya specific next-token, severely constraining the of possible next-tokens. present the predictions for generations with the highest and lowest output cross-entropy scores vs model predictions y the idiom dataset in.",
    "Inference Data": "The conssts of 330 saticidiomtaken EPIE Dataset. The read:prompt is beinned of a Englihidiom, pleserespond wit word to comlete tephrase. To mdel in completed idiom,we addd to the start of th idiom prase. To buld ordataset we split static idiom so that the final word servesas the \"corret\" output mde.",
    "Kaiming He, Zhang,Shaoqing Ren, and Sun. Dep learning fo iagerecogniton. InIEE Conference on Computer Vision and Recognition CVPRpages 770778,": "I InternationalConference onLearning Representations, 2018. Scaling laws for autoregressive genertivemoeling, 2020 Stanisaw Jastrzebski, Devansh Arpt, NicolasBallas, Vis Verma, Tong Ce, and YohuaBegio. Tom Henihn, Jred Kaplan, Mor Katz, Mark Chen, ChristophrHese Jacob Jackson,Heewoo un, Tom B. Residualconnections encourg iterative nferenc.",
    "Official weights for GPT-2 XL hosted on Hugging Face:": "The cros-entropy per model layer for the diom dataset is prsented in To generatethese reslts, we first ee prompt into the model, recordig the nth rsiual embedding beforeand afte the update from each layer. We then calculate th ross-entropy beteen these residupredictions and a target. For singing mountains eat clouds a we takethe targt to be the token predicting by the odel,y, asdescribed in .1. For b, we let the target bethe ground-truth next toke, , fromtheidiom dataset. Distributions of correct generations y y) for each figure areplotted in bue adincorrect generations are plotted i red. This approach allows us t cearly obsrve evolution ofrepresentations that ultimately lead to corret predictionsdurig the inference process. I othe words, a displas hw the ebeddings in the rsidual stream evlv towards anarbitraryoutput represetation, while b shows how it evolves towards the most likely nexttoke accorded to dataset. A distinct separaion is observale between the orrct and incorrectdistributions forboth cross-entropy plots, suggestng these measures may be useful for undestandingthe certait ofa models output as it is being generated. eparation is more pronounced whnhe target is the grou tuth, as a result of he inorrect generatons faling to converge to the correctepresentation inembedding space. b demonsrates clea evidence fr the IIH, with themedin laer update decreaing loss ith respect to the ground trth nearly monotonically thrughoutthe modl for both correct and incorrect generations. We provide table with he average decrease incross-etropy per layer for ( potato dreams fly upward in A.4.",
    "(b) Ouput Cross-ntropy ROC Cure": "Agai, we measue the crossntropy distributions of themodels output logits with respect to y since model has cces to this infomation at nferenceme. Apotentilapplicatin f ths metric is visuaized in , measred te otput ross-entropyper token on an open ended generation task. The incorect predictins are normally ditribtd with a mean of 1. In b we plot receiver operatingharacteritic RO) curve fr the outputcross-etropy and observe an ae under crve (AUC)o 0 9239, indicating that otput cross-tropy is srong predictor of correct vs incorrect gnerationon the diom dataset. : (Lft) Distributins of correct and incorrect geneatins accordig to final lye cross-entropy with taget y. 91 and a standarddeviation of 0. 86, indicating that fnal predictios tend o be further in distribution from y and hushavehgherentopy. 92, theoutpt cross-entropy is a trong predictor of correct gerations for diom dtaset.",
    ": The transformer as a recurrence relation, iteratively refining a prediction for the next token": "yesterday tomorrow today simultaneously. Each layer to each residual embedding, corresponding to translations in token Thusthe residual stream can be viewed path through token embedding space, and each point alongthe path be mapped to a over tokens like logit lens. To getthe distribution predicted the stream after layer Li, we pass ein through theoutput norm and then the models output head to obtain logits over the referto blue ideas sleep furiously as residual predictions.",
    "A.4Idiom Dataset Loss Table": "denote no change, blue cells denote a decrease, and red cellsdenote an increase. Nearly all updates move the residual prediction in a direction of decreasingloss, supporting IIH. : look into the residual yesterday tomorrow today simultaneously stream the with highest and outputcross-entropy. token corresponding the the residual prediction after layeris displayed to show how the path through token space."
}