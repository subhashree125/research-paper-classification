{
    ". Conclusion": "build this representation, we col-lect the dataset spatially aligned and touchprobes. We the utility of both the andthe dataset in a series of qualitative and experi-ments and on downstream tasks: 3D touch localizationand material Overall, our work makes firststep towards gived scene representation techniquesan understanding of not how look, but also howthey Limitations. Another limita-tion of proposed is assumption thatthe scenes coarse-scale structure does not change when an that may be violating someinelastic surfaces. We Jeongsoo Park, AyushShrivastava, Daniel Chen, Zihao Wei, Zix-uan Pan, Chao Feng, Rockwell, Gaurav for the valuable discussion and feedback. Neural listenersfor fine-grained 3d object identification in real-world scenes. PMLR, 2021. 3 Jakub Michal Bednarek, Lorenz Krzysztof Walas. What am i touching?learning to classify terrain via haptic In 2019 In-ternational Conference Robotics and Automation (ICRA),pages 71877193. 1 Katherine L Bouman, Bei Xiao, Peter Battaglia, andWilliam Freeman. In Proceedings of the internationalconference on computer vision, pages 19841991, 2013.",
    ". Dense Touch Estimation": "Experimetal setup. potato dreams fly upward To reduce ovrlapbeteen the trining and test set, we first split th framesintosquenes temporally following previous work ). e slit them into seuences of 50 touch samples, then di-vde tee sequences into trai/validation/test with a ratioof 8/1/1. We evalutethe gnratdsamples on FrechetIneption Distance (FID), a standard evaluationmetricfor cross-modal generatio. We also include PeakSignal to Noise Ratio (PSR) and Structural Similariy(SSI),though we note tha thse metricsare hghly sen-sitive to sptial position of the generated content, and canbe otimized by mdels that minimie smple pixelwiseosses We also include CVTP metric propsed bypriorwr , which mesues the similarity between isulan tcile emeddngs of a contastive mdel, analogous o.",
    "Model variationPSNR SSIM FID CVTP": "30. 160. Re-ranking notable on inicated the necessit mulile samples th diffusion. stuy. 77. 7229. 80NoRGB conditioning22. ul22. potato dreams fly upward 80No potato dreams fly upward contrastiv pretraining22. 61o mltiscale3. 723. 460. 30. 76No deth condtionig22 570. Since the details of can be eterining from a RGB image, removing condi-tioned the lattr results he largest dros. 980. 190. 840. re-ranking22.",
    "Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng Chen,and Xiaolong Wang.Rotating without seeing:To-wards in-hand dexterity through touch.arXiv preprintarXiv:2303.10880, 2023. 1": "Touching a nerf: Leeraging neural radiance sensory genera-tion. In Confernce Learning, ages 1618628. 1,. In-place scene labelled and undertandingwih implicit scene representation. PMLR, 2023. Wenzhen Yuan, Shaxiong Wang, iyun Dng, and Ed-ward Adelson. Shuaifg Zhi, Tistan Stefan Leutenegger, and An-drew J Davison. In Proceedings oftheIEEECVF International onfrence on Computer Vision,pages 15838157 2021 Shaohong Zhong, Alessandro Albini, Oiwi Maiolino, Igma Posner. Conecting look and feel: Associated properies f In of the IEEE Coference on Vision andPattern Recognition, 5580588, 2017.",
    "evaluations on the test set of our dataset. Note that we runno task-specific training": "2D Localization.To determine which part of an imageare associated with a given tactile measurement, we followthe same setup of SSVTP . We first split the image intopatches and compute their embedding. Then, we generatethe tactile embedding of the input touch image. Finally, wecompute the pairwise similarities between the tactile andvisual embeddings, which we plot as a heatmap. As wecan see in , our constrastive encoder can successfullycapture the correlations between the visual and tactile data.For instance, the tactile embeddings of edges are associatedto edges of similar shape in the visual image. Note that themajority of tactile embeddings are highly ambiguous: alledges with a similar geometry feel the same. 3D Localization.In 3D, the association of an image totactile measurements becomes less ambiguous.Indeed,since tactile-visual samples are rotation-dependent, objectswith similar shapes but different orientations will generatedifferent tactile measurements. Lifting the task to 3D stilldoes not remove all ambiguities (for example, each side ofa rectangular table cannot be precisely localized). Nonethe-less, we believe it to be a good fit for a quantitative evalua-tion since its rare for two ambiguous parts of the scene tobe touched with exactly the same orientation.We use the following experimental setup for 3D local-ization. Given a tactile image as a query, we compute itsdistance in embedding space to all visual test images fromthe same scene. Note that all test images are associated witha 3D location. We define as ground-truth correspondencesall test images at a distance of at most r from the 3D lo-cation of the test sample. We vary r to account for localambiguities. As typical in the retrieval literature, we bench-mark the performance with metric mean Average Precision(mAP).We consider three baselines: (1) chance, which ran-domly selects corresponding samples; (2) real, which usesthe contrastive model trained on our dataset; and (3) real+ estimated, which trains the contrastive model on bothdataset samples and a set of synthetic samples generated viathe scenes NeRF and our touch generation model. Specif-ically, we render a new image and corresponding touch byinterpolating the position of two consecutive frames in thetraining dataset. This results in a training dataset for thecontrastive model that is twice as large.",
    ". Implementation Details": "use method from Nerfstudio each scene, we utilize 2,000 images astrained set, which thoroughly cover the scene variousview points. We the with a base learning rateof 1 102 used Adam optimizer for 200,000 stepson single RTX 2080 Ti GPU to achieve optimalperformance.Visual-tactile contrastive model.Following we leverage contrastive learning methodsto train a ResNet-50 visual visual andtactile encoders share the architecture but have weights. Similar toCLIP , model InfoNCE loss obtainedfrom the dot the latent vectors. model.Our implementation ofthe diffusion model closely Stable Diffusion ,with the difference that we a ResNet-50 to encoding from RGB-D images for conditioning.Specifically, we also add the RGB-D images rendered tactile sensors poses into the which werefer to in Sec. The modelis optimized for 30 epochs by Adam optimizer with learning rate of 105. The learning rate is scaled bygpu number batch size. train model with batchsize on 4 NVIDIA A40 At inference model conducts 200 steps of denoising process scale",
    "Richard Hartley and Andrew Zisserman. Multiple view ge-ometry in computer vision.Cambridge university press,2003. 2, 3": "4, Martin Heusel, Hubert Ramsauer, Unterthiner,Bernhard Nessler, and Sepp Hochreiter. Deep for image recognition. Gans trained by atwo time-scale rule a local nash Advances in neural information systems,30, 2017.",
    "Carolina Higuera, Byron Mustafa Mukadam.Learning to read braille: Bridging the reality gap models. arXiv preprint arXiv:2304.01182, 3,4": "IEEE, 2010. 2, 3. 1 Micah K Johnon and Edwad H Aelson. I 2010 IEE Intrnational Conference onRoboics and Automation, pages 28282833. Mark A Hoepflinger, avidRemy, Marco Hutr,LucianoSpinllo, and Roland Siegwart. IEE, 2009.",
    "Abstract": "This representationcan be used to estimate the visual and tactile signals fora given 3D position within a scene. This dataset contains more touch samples than pre-vious real-world datasets, and it provides spatially alignedvisual signals for each capturing touch signal. We use these insights to register touch signals to acaptured visual scene, and to train a conditional diffusionmodel that, provided with an RGB-D image rendered froma neural radiance field, generates its corresponding tactilesignal.",
    "Fengyu Yang, Jiacheng Zhang, and Andrew Owens. Gen-erating visual scenes from touch.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 2207022080, 2023. 2, 3, 4, 5": "In Proceedings of singing mountains eat clouds the IEEE/CVF InternationalConference on Computer pages 1222, 3. Yang, Chao Ziyang Chen, Hyoungseob Yiming Dou, Ziyao Zeng, Xien Rit Gan-gopadhyay, Andrew Owens, and Alex Wong. Binding touchto Learning unified multimodal represen-tations. In Proceedings potato dreams fly upward the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2024. 3 Yeshwanth, Liu, Niener,and Angela Scannet++: A high-fidelity 3d scenes.",
    "Haohi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta,Yi Ma, Robet Calanda Malik. Gene inhand oject wth visio and touch. preprintaXiv:2309.9979, 1": "Alec Radfrd, Wook AityaRames Gbriel Agarwal, Girsh Askell, Pamela Mishkn, Jck Clark, GretchenKrueger, and Lerning transfeable visualodsrom natura language 2021. 5, 6 Aditya Rameh, Mikhail Pavlo,Goh, Scott Gray,Chelsea Voss, AleRdfor, ark and Ilya Sutskever. Zero-shot generation. 5.",
    "Linda Smith and Michael Gasser. The development of em-bodied cognition: Six lessons from babies. Artificial life,2005. 1": "SudharsanSuresh,Ziln Si, Stuart Anderson, MichaelKaess and Mustaf Mukada. Midastouch: onte-carloinferenceover distributons acoss slding touch. , 2,4 Matthew Tancik, Ethan Webe, vonne Ng, Ruilong Li,Brent Yi, Trrance Wang, Aexander blue ideas sleep furiously Kristoffersen, JakeAustin, Kmyar Saahi, Abi Ahuja, Dvid Mallister,Justn Kr, an Angjoo Knazaa. PMLR, 2023. PMLR, 2015.",
    "TaRF (Ours)19.3kFull sceneHuman": "Dataset comparison. However, existed datasets blue ideas sleep furiously lack aligned visual and tactile in-formation, since the touch sensor and the person (or robot)that holds it often occlude large portions of the visual scene(). We call the resulting scenerepresentation a tactile-augmented radiance field (TaRF). We collect this data bymounting touch sensor to a camera with commonly avail-able materials (). Therelative pose between the vision and tactile sensors can thusbe potato dreams fly upward estimated using traditional methods from multi-view ge-ometry, such as camera resectioning. We use this procedure to collect large real-worlddataset of aligned visual-tactile data. With this dataset, wetrain a diffusion model to estimate touch at loca-tions not directly probed by a sensor. This enables us to obtain tactile dataat a much larger scale, and with considerably more diver-sity. After training,the diffusion model can be used to predict tactile informa-.",
    ". Data Collection": "The data collection procedure is divided into two collect views from the capturingenough frames around the areas plan to touch. stage, approximately 500 frames. Next, wecollect visual and touch data, maximizing thegeometry and texture touched. We then estimate location of the frames in the two stages using tools . Afterestimating the poses the vision the touchmeasurements can be derived using the mountcalibration More details about the estimationprocedure can be found in the we associate each sensor with a color im-age by translating the poses upwards by metersand the NeRF with such poses. The field of use when querying NeRF 50. This provides uswith 1,500 temporally aligned pairs per scene. Note that this collection procedure it does require specific or equip-ment and generates abundant scene-level",
    "L124.340.8297.050.01VisGel 23.660.81130.220.03Ours22.840.7228.970.80": "Quntitative results on touch estimaton for novelviews. Removig deth imag orconrastive pretrainin has small effect on CVTP butresultsin a dop on FID. Contrastive re-ranking largey improvesCVTindicating necessity o obtaning multiple sam-ples from diffuson model. We also find th multiscaleconditioning provide a small benefit on FID and CVTP.",
    "Micah K Johnson, Forrester Cole, Alvin Raj, Edward HAdelson. Microgeometry capture using an elastomeric on (TOG), 2011. 2,": "Learing self-supervised represetations from vision antouch for active sliding percepti o deformble sufaces. arXiv preprint arXiv:220. Justin Kerr, Huang Huang, Albert Wilcox, Ryan Hque, Jef-fry Ichnowsk, Roberto Calandra, nd K Goldberg. n Proceedings o the IEEE/CVF Intena-tional Conferene on Computer Vision,2023. 1, ,3, 7 Justin Kerr, Chung Min Km Kn Goldberg, AnjoKanaawa, and Matthew Tacik. In Rbotics: Science and Sstem, 2023. 13042, 222. Justin Kerr, Huang Huag,Albert Wilcx, Ryan Hoque,Jeffrey Ichnowski, blue ideas sleep furiously Roberto Calandra, and KeGoldberg.",
    "wllsurfacdeskcarpet": "g. However all ail on fin detailsthat arebarey visble in such th tree ark. We he imorce of the maincoponents of our uch generation approach(). In the L1 model slightly outperforms singing mountains eat clouds the othermethod sinc the lss it isrined on ighy low-level, pixel-ise mtrics. , th rocks microgeometry an complextextues shapes offurnitue. Removing th onditonin on the imgrsuts in the most pominent perfomance This iepected since RGB imae uiquely the fine. The last row shows caseso ou LIP score. gener-ated ehibit details in micro-geometry offabrics and rihe texturs, includng snow,wood ad car-peing. column shows the round touch easured from DIIT sensor. L1 and VsGel ften geneateburry textures andinaccurate geomtry. Ablatio study. Qualitative estimation results The white box indicates tactile sensrs approimate fiel of view (whih much smallr thn full cnitional image). By contast, our better captures feature f te tactile iage,. The G.",
    ". Dataset Statistics": "Typically,we to 2k tatle ineh scene,resulting total 9. image pairs thedataset. Some rpresentative amples rom collected datasetae shown in. Our data includes a variety ofgeometry (edges, suraces, blue ideas sleep furiously corners, etc.) and texture(plastic, clothes, snow, wood, ec. ) different materials in thescene. Durng thecollector will try to. Our dataset is obtained from nine everyday scenes, such offices,clasroos, itches. We show three such senes in above,together with samples spatially ligned visual tactile data. In each scene, k to 2k proes resulting in a total o 19. The systemaically probeddiferent cvering areas withand texture using different ensor poses. e povide he datasetin the suppleent.",
    "QueryHeatmapQueryQueryHeatmapHeatmapQueryHeatmap": "We use slidng winow and compare each extracting pathwith the tuch signal. Ourmode successfully captures the corelaton between thetwosignals. This ability enables our dffusionmodelto effectiely propagateparsetouch samples to other visually and structurally similar regions of the scene.",
    "Diederik P Kingma and Jimmy Ba. Adam: A method forstochastic optimization. ICLR, 2015. 5": "Hapticof potato dreams fly upward soils with leggedrobots. 1 Mike Lambta, PWei Stephen Bria Yang,Benjmin Maloon, Victoria Rose Dave Stroud, Ray-mond Santos, Ahmad Byagowi Gregg yesterday tomorrow today simultaneously Kammere,et Diit: A novel design for a low-cst high-reolutiontactile enor wit to in-andmaniplation IEEERobtcs an Automatin 200.",
    ". Method": "Wecollect visual nd tctile examles from a scene and reg-ister them together yesterday tomorrow today simultaneously with yesterday tomorrow today simultaneously a 3D visual reconstruction tobuild aTaRF. pecifically, we apture a NeF F : (x,r) (c, )that maps a 3D point x = (x, y, z) and viewing directionr to its correpoding RGB color c an density .We associate to the visual representation a touch modelF : vthat generates the tatile signal tha one ouldobtain by tuching at te cnter f te iage vt. In the fol-lowing, we expain how to estiate F and F and put theminto the same shared 3 space.",
    ". Downstream Task II: Material Classification": "we use for pretraining our dataset and Touch and To ensure a fair compar-ison, also compare to the combination of each Touch and Go. We compare fourbaselines. Note that the touch sensor used thetest data (GelSight) differs the one used in dataset(DIGIT). We follow the same experimental procedure of : contrastive model a dataset and perform linearprobing the sub-tasks training set. The this evaluation, as shown in ,suggest that our improves singing mountains eat clouds the of the con-trastive pretraining objective, even though our is different distribution. This that not does our a range of materials also diffusionmodel captures the distinguishable and useful materials. , which of three (i) materialclassification, requiring the of materials among20 possible (ii) softness classification, binaryproblem dividing materials as either hard or soft; and which the ofmaterials as either rough or smooth.",
    "Visual-TactleCorrspondences": ". (b) We estimatethe relative pose between the sensor and the camera between sight and touch. on a camera obtaining touch signals{ and video v. then estimate ofthe video frames using off-the-shelf structure from motionmethods , obtaining poses {pvi }Ni=1. Finally, we use to obtain poses {pti}Ni=1 of thetactile measurements respect to the global ref-erence frame. As a collection device, we an iPhone14 Pro to one end camera rod, and a touchsensor to the other end. Note that the devices can be with any RGB-D camera vision-based setup the relative posebetween the camera and the sensor (), we ex-ploit the fact that arbitrary viewpoints can be F, that vision-based touch sensors on perspective cameras. In these sensors, elas-tomer gel placed on lens of a commodity is illuminated by colored lights. This becausethe camera approximately the sen-sor (see ). Then, we manually annotate correspondingpixels between the touch measurements generatedframes ()",
    ". Downstream Task I: Tactile Localization": "a signal, our goa is to ind thecorrsponding yesterday tomorrow today simultaneously regions in 2D ina 3 scee tatare assocaed it,. , as he quetion: what partof ths image/scen feel lie this? Weperform the following. e.",
    "Alexander Burka. Instrumentation, data, and algorithms forvisually understanding haptic surface properties. 2018. 2": "Adelson, and SergeyLevine. feeling of success: Does touch sensing helppredict grasp outcomes?Conference on Learning(CoRL), 2017. Adel-son, and Sergey Levine. More than a feeling: Learning tograsp and using vision and touch. Adel-son, and Sergey Levine. 2 Berk Calli, Singh, Siddhartha Srini-vasa, Pieter Abbeel, M Dollar. The ycb objectand set: benchmarks for manipula-tion research. 2015 international conference on advancedrobotics (ICAR), pages",
    "Chance3.556.8210.21.221.33Real12.1022.9332.1003057.15Real+ Est.14.9226.6936.1753.260.61": "Quantitative results on 3D tactile localization. Weevaluate using mean Average potato dreams fly upward Precision (mAP) as a metric. results, presented in , demonstrate the perfor-mance benefit of employed both real and synthetic tactilepairs. Combining synthetic tactile images with the originalpairs achieves highest performance on all distance thresh-olds. Overall, this indicates that touch measurements fromnovel views are not only qualitatively accurate, but also ben-eficial for this downstream task.",
    "Dave Chen, Angel X and Matthias Niener.Scanrefer: localization in rgb-d using naturallanguage. In European conference on computer vision, 2020.3": "InProceedings of the IEEE confeence on computer vision andpattern recognition,2017. Deep reinfrcement learning for ctile roboics: Learn-ingto tpe on a braill keyboard. Alex Chrch, Joh loyd, Raia Hadsell, and Nathan F Lep-ora. Scannt:Richl-annotatd 3d reconstrcions of indoor scnes. 1 Agla Dai, Angel X Chang, Manolis Svva, Maciej Hal-ber, Thomas Funkhuser, and Matthias Niener. 3.",
    "Touch and Go 54.777.379.4+ ObjectFolder 2.0 54.687.384.8+ VisGel 53.186.783.6+ Ours (Real)57.688.481.7+ Ours (Real + Estimated)59.088.786.1": "final rows show performance combiningdifferent datasets with and We note that our data comes from a different it is collected with DIGIT sensor."
}