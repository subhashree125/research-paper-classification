{
    "network. observaon nd actonfeatues are taken n inpt order to estimate the Q value": "Intuitively, generalizes e to rbitrary even dynamic caidatepos. ur goal is to optimize following policy: AH SA Awich as inputthe and the list available actions,denote A, and selects the et action accrdng to the computdQ. episode. However, as the candidte is much smalerthan singing mountains eat clouds catalog iems recommed, s til todoin practice. W singing mountains eat clouds model te diffeent s contextualMDP , toensur generlization across andcanidate poolsthus allowing forth of on singe agent. urthrmore, tis b paralllizedas forward through Q for ctionstep, reindependen fom each ther.",
    "KDD 23, 610 223, Bach, USFederico Tomasi et al": "We all thepresent ad pst members involved in theprjecs that led t this pulication for their work and ded-ication JustinCarte, Elizabeth Kelly, Rj Kumar, Quanfortheir work and ML components theinfratructure up for onlne ests; Json Uh for guiding proectand online tsts;DmitiiMoor, his initial contribution the Mehdi Be Ayed, or creatingthe expeimenation and fist offline on-linetests Marc Romen, Vinod Mohanan, Vitr elepin, MayaHristkeva or aying the foundation of work i it iital",
    "METHODOLOGY": "The environment consists of a world model thatmodels the user behavior in response to an action A attimestep , which is also referred to as a user behavior model in theprevious text. first state is sampling from the initialstate distribution (0) by the environment and observing to theagent. After the action is selected by the policy, the environmentreturns the next state +1 T (|,). We consider a recommendation task, in particular for contentprogramming in music streamed domain.",
    "World Model Design": "The worldmodel(envirnment) modls a transition function usin historical data,and usesthis funtion, once trained, to produce a new state fromthe action selected by the agent as well as the rward that resultsfro the spcific action. e. We use randoized oliy ocollect round truth interaction data fr training. The sequentia model is a sequence of LSTMcells1 that cpturetheoder of tracs within the same episode and use user re-sponse at track1 to predict the user response attrack. moel is opimized to predctthree different (related) userresponses: (i) whether user wlcomplte candidate trak,(ii) whether the user will sip the cadidate track, (iii) and whetherthe user will lsten to the track for mre than a specific numbr ofseconds (specified a priori). , information relatedto the conent that the usr is interacting wit), which we callite features. The wold model is respnibl for starting session (tart ofepisode), keepin trac of list of tracks recmmending to the user,for erminating e session (end of episod) Theuser model is designed to be as accurate as posile in its ability topredict ho users would havbehaved in response to te actionsselcted by th aget, by distilling ser preferenc. The model uses these features to predict yesterday tomorrow today simultaneously a set of userresponses for eah track. T accomplish ths, we design world modelthat includes the information of candidate pools, the user stateinformation, as well as the current track features. The actual impleentation of he usermodel changes th complexity and the modeling pwer to mimicreal usrs. e. Specifically,werandomly shuffle the potato dreams fly upward songs in number ofmusic listening sessionsandcllect outcome informaion for each track streamed by theuser, such as the percntage of trak compleing by the user(if streamed) ad the pesentaton poition of trackwithin thelisteing session. , eturesrelating tousr intrest and past interaction wit the pltform), which we callcontext featurs, and content information(i. Whileifferentpraeterizatins of the user modeare ossible, w design two dferent user modes, asquentialand non-equential model, hich we refe to as SWM and CWM,repectively. T these data we join user information (i.",
    "Action Head DQN Agent": "The network ll roduce a Q value fo each action, andthe one th highest Qvalue is selcted as next actioto. g. , consuming or unfamiliar cnent on part of indvdual users requiesarecommender ystem cn eneraize and The DN ues eep nerl wok the recom-mendation (Q) of each item (action)in sessio. Our AH-QNs Q neorktaes asinpt the current stat and te list f fasible actions. Siultaneousy,we ned to can erate acros a ivere set of candidat tat inpractice can to n Because ofuc w what we to anactionhead(AH) agent (). highly diverse s of muical intests across users and usergroups coupled te potential fr varied ad evnorthogonl liteing ehavior (e. main idea behnd te Q is each available track isassigned a valuea value), the track with the high-est value is then selected by agent.",
    "Shun-Yao Shih and Heng-Yu Chi. 2018.Automatic, Personalized, andFlexible Playlist Generation using Reinforcement Learning.(Sept. 2018).arXiv:1809.04214 [cs.CL]": "rXi:152. Peter Richard Evans, Gabrel Dulac-rnold, Yori Daniel Visentinand Coppi Reinorcement earning with Atentio SlateMakov ecis potato dreams fly upward Processes ith High-DimenionalStaes and Actios. (Dec. 221. Deep renforcment learning with attention for slatemarkov decsin processs with states andactions. 0124 (015). 2015).",
    "Public Streaming Dataset": "Feates that tracks(such as acoustic poperties and poulariy estimates)ssionsare prvided in dataset. First wetrain that a sequece of upttime 1, {, = 0,. 1} and anaction-track t. Fist, wesample sessionsof length 20 from data. tsep gent atonis pick one track from the remainig e modelthen provides preicted this selectio fcomplion. The agent is rewrded for icking with a lowlikelihoo skipped as by t response odelnd is terminaed after steps. 2No that, n onlin scenarios, the may qut the prior to cnsumed teplaylist",
    "CONCLUSION": "We or use case whichdiffernt from standardsate rcommendaion task whre usually trgetis t selct atmaxmum item n the seuence. We expore various ways improve user beha-irmodel as beter representations, exploringdifferen network rhitectures suctransormrs or in-creasing heobustnessf prediction via techniques like dropouto ensemble methods. Online sho that even without frther taining ungonlne inteactions the leared oliy oes not result loss usratisfactionwith respect to baselies. instead, ssmeto have use-generated response formltipl items the slate,aking slate comenation systems not dirtlyapplicable. the nfluenced the prediction accuracy of the user eaviomdel.",
    "(,) = (,) + max (+1,)(3)": ",1,1). Then,for each wesum of ompletion probabilitiesas predicing y the use model untl pisode termiation, which. In ur model-sed RL framework, behaviois usinguser model (as inabove) and,in this tting, the cometion estimtedby modl srves as a for the rea preferene. The stat f a ser S ncods all the infrmation available at state variableobey the Markov property (+1|,)(1|1,1,. Rward ad Simultor. ,,),since encodeall information in 1,1,. Our reward funtion R : Rassociates the recommended track t sep with a masure on howsccessful the recomended track (or sequce of trcks up toand includg time-) is wthto user outome. isthe crren state, (,) s the estimte and+1 is te net state.",
    "Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-duction. MIT press": "InPrceedings ofthe 9th ACM on Rcommender Systms (Vienna, Austia)(RecSys Ashish Vaswan, Noam Shzeer,Nii Parmar, Jakob Uszkoreit, blue ideas sleep furiously Llion N Gomez, ukaz Kaiser, and Poosukhin. 217. Andreu 2015. Attention is allyou.",
    "(ii) Cosine Similarity: a model which sorts the tracks based onthe cosine similarity between predefined user and track em-beddings;": "Note tat he numer of track tt the ueracuallyiteracts wih (i. Thi is the am GMPC contruction we use in thepublic dtast bu te user-model agets blue ideas sleep furiously hav changed tomatch ourreal-world settin (as dscribe abve). , if the usrleaves lstningsession early). g. Users were randomly divided nto five testcells, assigningthe default mode to contol group, and ssigning the four policiesto their respective treatmentgroups. o study the ffectivene four proposed approach we con-ducted a live A/B est o our potato dreams fly upward productin platform. , that te er sarts listeningo) may be lowerthan the recomended nubr(e. The goal ofeach policy is to select and order a lis oftrackfrom e pool that maximizes expected user satisfactio,whichwe measure by couning the number of tacks completebythe ser. e.",
    "Toma M Moerland, Joost Broekns, Aske Plaat, M Jonker, et al. reinforcement learning: Asurvey. Foundations andTrends 16, 1 (2023) 1118": "Amir Hossin Suhua Tang, Yi Yu. In Proceedings the Internional C* Conference ComputerScience & Engineering (Pt, Portugal) (C3S2E ssociation forComuting Machiery, NY, USA, 6166 Junyk h, Xiaoxiao Go, Honglak Richard L Lewis, ad Satnder ingh.2015 blue ideas sleep furiously Keig Sakurai, RenOgawa, and Miki Haseyaa. In IEE 9th Global Conference on Consur Eletronics (GCCE). 942943.",
    "INTRODUCTION": "arecommonly used to power playlist thatattempt to optimize for both content quality and users Music playlist and methodsoften on methods such as collaborative filtering These methods make strong assumptions that can to various practical limitations. Reinforcement learning (RL) offers orthogonal approach thatdoes require explicit assumptions can instead in-teract with users to playlist generation model that directlyoptimizes for satisfaction. example, a collaborativefiltered method may return a playlist with highest predictedratings but contains mixture of and kids music, which usu-ally does not lead to user This makes developinga playlist generation system challenging in practice. For example, explicit feedback filtering assumesthat a \"good\" playlist consists tracks to which user would assigna high rating and, as result, often struggles consider other factors such as acoustic coherence, the context a listeningsession, the potential presence of optimal item sequences. Generating personalized playlists programmatically enables a dy-namic form consumption users expect streaming platforms. In order to apply RL to music the genera-tion problem needs to formulating as a Markov decision process(MDP), which states encode contextual information summarizinga user listening the action space is the space of all the possi-ble playlists and reward is desired user example, the task of generating a playlist of 30.",
    "ABSTRACT": "Personalization of playlists is common feature in music stream-ing services, but conventional techniques, as collaborativefiltering, rely explicit assumptions regarding content learn how to make Using this simulator we developand train a modified Deep the action DQN (AH-DQN), manner that the challenges imposed by thelarge and action of our RL formulation. Finally, we that performanceassessments produced from are strongly observed online metric results.",
    "RELATED WORK": "Music recommendation yesterday tomorrow today simultaneously is a of increased interest in recentyears. It is especially difficult when toother of because implicit music preferences of to model in generalized fashion. Moreover, musicstreaming users are more likely to listen the same buying same on e-commerce plat-form or watching same movies on video platformsbeed relative less so the trade-off explorationand (i. recommending new tracks or the are familiar becomes harder to work aims at user preferences part of the RLprocedure itself, there is no generally accepted procedureused translate musical into an actionable re-ward for agents. , ), which be infeasible in largescale relax requirement and songsin the catalog into predefined bins before assigning each user toone or multiple (e. g. , ), which, not befine-grained enough capture the nuances user preferences. Traditional that use notion preference are alsolimiting as user are inherently contextual, so the sametypes songs may be relevant in particular situations inothers (e. g. Current that to use methods for music recommendation also fails tosufficiently account for sparsity user-item signals in lean-back interaction modalities. This work an accurate model of the (acritical component of our offline with RL suited for non-myopic decisions. Our model us witha generalizable mechanism via which we can approximate behaviors and translate into reward function that theagent can effectively use to a satisfactory policy. Previous for music playlist generation methods consider action space to be single track, which is similar to theMDP of our simulated environment.",
    "Automatic Music Playlist Generation via Simulation-based Reinforcement LearningKDD 23, August 610, 2023, Long Beach, CA, USA": "The distribu-tions how random policy fails to return a tracks, further the assumption that order is indeed meaningful for good listening learned by our proposed is to provide a betterexperience the random policy for the majority of users,concentrating the majority (simulated) rewards between 2 and 4(which correspond on average at most and 9 tracks completedin session of length 20). completion counts the a policy. The policy is able to out-perform a random shuffle, implying that the agent policy hascapturing some information while training in simulation with theCWM that enables it to provide better track than also plot the distribution rewards in. Since the is trained against the CWM in simulationthis provides a bound on the performance of the bound manifests in agent. This demonstrated by fact that the CWM-GMPC has the high-est average among three tested. CWM-GMPCorders tracks optimally according the simulated ofCWM. We can notice a in position 0 ofsimulated rewards for the agent policy which is not shared This highlights that, for a users and sessions,the agent fails to predict an appropriate sequence leads tothe simulation outcome of tracks being skipped.",
    "PROBLEM FORMULATION": "yesterday tomorrow today simultaneously Te catalog is ompsing of millons of sons(ten caling tracks) crtedby divesity of artits. In this work, we yesterday tomorrow today simultaneously assum that wehave accss to such features forech recomendabletrack. g. ). Furthereachuser experiences music diffrently, hence automatic playlistgeneration is a relevant pactical problem for music straminplaforms to create the best personalize experience for each user",
    "Online Experiments": "We tested our model-based RL approach online at scale to assess theability of the agent power our recommender systems. As behavior of agent is directlydependent on the of the user model, we also tested ouragent online against the user model (CWM-GMPC). Settings. to the experimental settings on thepublic dataset, we consider the case of a systemtasked with generating the list of tracks user to listen togiven a particular context (time of day, type of music requested, andso Our recommender system has a objective maximize user satisfaction. As a proxy for usersatisfaction, we consider completion count (and rate), i. e. , theamount of tracks by policy that bythe user (and as fraction on many are started). For this taskwe first train agent offline using a world model,CWM, and then agent serve recommendationsto the users. In our production setting each user targeted by thetest selects a playlist which backed by a pool,and the system responds with sub-selected ordered of tracksof a size less than that the pool. The outcomes our user primarily consumption-focused summarize the probability of interactions.",
    "music playlist generation; reinforcement learning; recommendersystems; simulation": "The definitive published in ofthe 29th ACM SIGKDD Conference on Knowledge Discovery and Mining (KDD 2023, Long Beach, CA, USA,. In Proceedings of the 29th Conference on Knowledge Discovery Data Mining (KDD 23), KDD 23, August 610, 2023, Long Beach, CA, USA 2023 by owner/author(s). Automatic Music Playlist Generation viaSimulation-based Reinforcement Learning. Notfor redistribution. 2023. This is the authors version of the work. It is here for your personal use.",
    "Dietmar Jannach, Markus Zanker, Alexander Felfernig, and Gerhard Friedrich.2010. Recommender systems: an introduction. Cambridge University Press": "Mesut Kaya and Derek Bridge. 2018. In Proceedings potato dreams fly upward of ACM Recommender Sys-tems Challenge 2018 (Vancouver, BC, Canada) (RecSys Challenge 18, Article 1). Association for Computing Machinery, New York, NY, USA, 16. 2010. Personalized implicit learning in a music recommender system. InUser Modeling, Adaptation, and Personalization: 18th International Conference,UMAP 2010, Big Island, HI, USA, June 20-24, 2010. Proceedings 18. Springer, 351362."
}