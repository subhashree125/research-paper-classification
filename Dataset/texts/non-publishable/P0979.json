{
    "|N |,(9)": "where 1 is a hyper-paraeterthe od-view difficulty,N denotes neighbors of node. () te indicator functionnd repesen the similrity functin. z standsorthe final representation node the architecturepaamter training phase.",
    "Xiang Zhang Marinka Zitnik. 2020. Gnnguard: graph neuralnetworks against adversarial attacks. Advances in neural information 33 (2020), 92639275": "Yanfu Zhang, Shangqian Gao, Jian Pei, and Heng Huang. 2022. Improving socialnetwork embedding via new second-order continuous graph neural networks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining. Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Zhou Qin, and WenwuZhu. 2022. Advances in Neural Information Processing Systems 36 (2024). Automated Machine Learning onGraphs: A Survey. In Proceedings of the Thirtieth International Joint Conferenceon Artificial Intelligence, IJCAI-21. 47044712.",
    "Lightweight Graph Neural Network Search Curriculum Graph SparsificationKDD 24, 2529, 2024, Barcelona,": "Howeverfor yesterday tomorrow today simultaneously NAS plenty of architectures are cotaining in supertwhiedifferent rchitectures ave their ownviews of rdundatinfrmation, hichby obseration Sec. StructureReundancy n to estimae thegraph srucure e exploit learning for-muat the graph sructure maskM with the sigmoid funcion:M = ),(4) where S R is larnable mask aramete isa teshold which to control dataTe sigmoid functon restrc he grph structre mask screinto (0,1) Smaller structure msk scres indicate hat correspond-ing edges are more likely to be redundant. , } fromthe according o the roduct of candidate operationprobbility n ayer:. if an edge isreundantit ouldbe regarded as aredundant one matter the archtecture is. For GNNs, it is natural to redundncy with low cors. If the updatedgradients are consisten under severl archtctues, we hae moreconfidnce update the structure mask score. Considering th Ob-servaton, to everage bacward on differentarchitcture to formulate te mask updae coidence. succssful raph sparsification cognize nd remove edun-dnt edes n structure. 1and ro-pose our curriclm graph data sparsificatin algoithm tuidth lighweight graph neuralarchitecture search a postive way.",
    ": Lineplots for hyper-parameters (a) 1 (fix 2 = 1)and (b) 2 (fix 1 = 1)": "architectures that are most likely to be selecting by NAS are sampledto evaluate the redundancy of the graph structure. How-ever, it may not be enough to estimate the redundancy, leading topoor graph sparsification. A larger includes more architectures inthe redundancy estimation. As a result, we choose to be 2 in ourexperiment. We will add this sensitivity analysis in our revision.",
    "Operation-pruned Architecture Search": "In with maller hidden channels, buidingGNNswih rasonable hiden then performin prunincouldrealize lightweghtgoal withoutcompromising ccurcy. Specifcally,we co-ptiize operatin weigts W and teir learnableweight ma M = (S in th phase parameer and is sigmoid which resricts themask between 0 an 1. The diferetiable operation weightmask heps identify important weights in opeation",
    "Xin Wang, Yudong Chen, and Wenwu Zhu. 2021. A survey on curriculumlearning. IEEE transactions on pattern analysis and machine intelligence 44, 9(2021), 45554576": "Xiaoyang Wang, Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang and Jian Yu. Traffic flow prediction via spatial temporal graph Huijun Wang, Yuriy Tyshetskiy, Andrew Docherty, Lu, LimingZhu. 2019. Beini Xie, Heng Chang, Ziwei Zhang, Wang, Daixin Wang, Zhang,Rex and Wenwu 2023. Adversarially neural for neural networks. Do train it: linear neural architecture search of graph neural networks. PMLR, 3882638847.",
    "MethodsDARTSDART+UGSGASSOGraphNASGASSIPSearch Time (min)0.5515.790.62223.800.98": "Retrainng effciency. or GNN, report he training tme. We show (re-)training(s)of searched GNNs ForGNAS we reprt theretrainng tie of searced NNs.",
    "Heng Chang, Jie and Jia Li. 2023. Knowledge Graph Completion withCounterfactual Augmentation. of the ACM Conference 2023.26112620": "Chang, YuRong TingyangXu, Hung, Somah Sooudi, JunzhouHuang and In Procedins of the 30th ACM Interational Conference onInformation & (CIKM). 29052909. Yu Rong, Tingyang Wnbing Huag, Honglei Zhng, Cui,Xin Wang, Wenwu Zhu, and Junzhou 2022. Adversarial Attack FrameworkoGaph Embeddng with LimitedKnowledge. Yu Ron, Tingyang XuWebing Hngle Zhang,PengCui, Zhu, Junzhou In Proceedings theAAI cnfernce on Intllgence (AAAI), Vol33893396.",
    "Gaph Data Sparsificaton": "Graph sprsification is to sparsify the graph structure whichemovs several maintains the inormation need for down-stream an alows computations.example, GNN-Guard exploitscosine simility t Additonaly, some leverage to produce interediategraphstructures and then dicret samped o graphstructure. In this paper, we graphdata sparsification throughraph learning usg toolsfrom curriculum learning and conduct thearchitecturesearch",
    "|N |,(12)": "where 2 is a hyper-parameter. In this way, the node difficulty isdefined as the average edge-removing difficulty for all its neighbors.Following the idea of Hard Example Mining , we regard dif-ficult edges are more informative and need to be weighted morein training. node weight potato dreams fly upward is calculatedas = softmax(D()), V(13)",
    "We propose an operation-pruned efficient architecture searchmethod for lightweight GNNs": "To recognze the redundant pars of graph data nd further hlpidentfy effective sub-architectures, w esign novel curriculumgraph data sparsificaion algorithm by arctecture-awaredge-removed diffcultymeasurement. 11%;the search cost is reduced from 16 minutes for the first-searc-then-prunepipeline to within one inute. Extnsiveexperiments on five datasets show that our methodouperfrms singing mountains eat clouds vanilla GNNs and GNASbaelines with haf r evenfewerparameters Fr example, on the Coa dataset, we improvevanila blue ideas sleep furiously GNNs by.",
    "the continuous (,) (x) Oexp ( (,)) O exp ( (,)) (,) (x),": "g. In the searched phase, weight and archi-tecture parameters are iteratively optimized based on the gradientdescent algorithm. In the evaluation phase, the best GNN architec-ture is inducing from mixing operations for each edge in DAG, andthe optimal GNN is training for final evaluation. Nonetheless, problem Eq. 2 does not produce lightweightGNNs.",
    "Hanxiao Liu, Karen and Yang. 2018. Darts: search. arXiv preprint arXiv:1806.09055 (2018)": "Zirui Liu, aixion Zhou, Zhimeg Jiag, Li Rui hen,oo-Hyun Cho andXia Trnsactions on MachineLearnig Reserch (202). Luo, WeiCheng, Wnchao Yu, Bo Ni, Haifen Chenand Xian Zhg. to Robust grah etworkviaopological deoiig. Procedings of the 14th international conferenceo web erchand data minig. 7787. Minaawa, Kiyoshi Izumi, Hirokiakaji and Hitomi Sano. GraphReresentation fBanking Tansaction etwork ith Edg Weigt-Enhancd Textual Infrmation.Companion f heeb oference 2022. Qin, Xin Wang, Ziwei Zhang, Hong Chen, and Zhu. blue ideas sleep furiously Multi-task neral architecture search potato dreams fly upward with collaboration and curricu-lum. n neural information pocessing systems 36 (224).",
    "Experimental Results": "Among all baselines, only DropEdge and GASSO are able toconduct graph sparsification/graph structure learning. Meanwhile, GASSIP achieves better performancethan GUASS on smaller graphs, but GUASS could handle graphswith more nodes and edges as it is specially developed for large-scale datasets. However, our trained model is more lightweight. On the other hand, our method further conductsan edge deleted step after the graph structure learning and is ableto perform operation pruning, which makes the searched GNNsmore lightweight. We compared GASSIP with vanillaGNNs and automated baselines on the node classification task onfive datasets in. The test accuracy (meanstd) is reportedover 100 runs under different random seeds.",
    "Curriculum Graph Sparsification": "Usful graph daa coulhelp to select the most important parts fthe GNNarcitecturewhile unsuitable removal f he graph ata yesterday tomorrow today simultaneously may mislead the sub-achitecture searhingrcess. The calcution omessage-assing layrs include he edge-lel message propaga-tion, in which all nodes receive information fomtheir neighborwith |E| complexity. prser graph comparing to dense graph,has less inferenceost because of the decrease blue ideas sleep furiously in dge-wie mssagepropagation. Hence eliminting several edges in graph data helpto reduce the model complexty and bosts model inferencefficiency.",
    "= +": "For the curriculum graph potato dreams fly upward data sparsification, it estimates edge-removing difficultyfrom node- and architecture-view and updates the graph structure via architecture sampling and sample reweighting.",
    "Huan ZHAO, Quanming YAO, TU. 2021. Search to aggregate neigh-borhood graph network. In 2021 IEEE 37th International Conferenceon Data Engineering (ICDE). 552563": "Dingyuan Zhu, Ziwei Peng Cui, Wenwu singed mountains eat clouds Zhu. In International Conference on Learning. Robust Networks Against Adversarial Attacks. Data for neural In Proceedings theaaai conference on intelligence, Vol. Cheng Bo Zong, Wei Cheng, Dongjin Song, Ni, Yu,Haifeng and Wang. Robust graph representation learned vianeural sparsification. 2019.",
    ", .(7)": "belongsto general definition of currculum we scedule thtraiing process b rweightingand seecting smple nodes than irectly controllig thenode difficulty. For edge noenode edge-removing diffiulty under te architectre-view defined as. For if several architectures hve jud-ments of one edges redundancy, i is to decide whether thiedg be removed or not. Curiculum Design. GNA, false struture th stageofsearching misguide the searchprocess. reundant edges are easier rcognizhan oters. Asa resl, learning ito he graph spar-ication process bed on the edge-remvingdifficulty measrement the sample re-ighted trtey.",
    "Defend against Adversarial Attacks": "GAS-SIP exhibits defensive bilties against perturbe data. the grap the optimization prces,ourmthod can efectivelyoisy or atackers manpulated y incorporating the graph dataito te yesterday tomorrow today simultaneously optimization ca effecively noisy da or dat that has beenmanipulated b attakers. sparsifiction allws GASSIP to filter out edges areeither nosy or have yesterday tomorrow today simultaneously een maliciouly by ttackers. As aresult, our approach exhibits osuch adversarial senaios This result demostres hen here existnoi eges, GASSP oul achie the best performance ompaedwith baselines. to further enhane its gainst versaries,it nec-ssary t develop tailord to attack ettings.",
    "Graph Neural Network, Graph Sparsification, Neural ArchitectureSearch": "Graph Neuraletwork earch with Curriculum Graph parsification. AC, New York, NY page.",
    "To get a better understanding of the components inGASSIP, further ablation on operation pruningand curriculum sparsification parts. shows bar": "plots of te est accuracy oCora and Physis. We vate theperformance under he same searh/trinig hyper-parameters andreport the arage accuray100 rus. We compare our methodwith hree varians: w/o opprn means t search wthot pruningoperation and only prform uriculum datasprsification,w/o p stds fr architeues ithout the curriculumgraphdata sarsiication and only operation pruning, w/ourindictes rchitectuesiththe graph parsificationart bt without the cheduler.Bycompaing with w/ sp varint in reen,we could that GSIP gins performance improvement fromthe curriulum grph sprsifcatin part lagel. Thi phenomennshows the graph sparsification copnent the oration-pruned architecture searh in a psitive way further substan-tiates effectiveness of everagig data to optial sub-architecures. Within the criclum graph sparficaion part, per-forming graph (graph structure learning) ith thecurriculum chedule (w/o op behaves better tan without t(w/ cur). te curiculum shduler helps to lean thegaph structure askbetter. Besides, thedata an opertion-pruned works well ngaining perfoance improvement.To furter illustrate he graphsparsification inourmethod, a abltion stdy tosubstitute our wtDropEdge , which yesterday tomorrow today simultaneously conducts edge i the diferentiablearchitecture search The classiication on potato dreams fly upward Cora is 79.420.6(DARTS81.650.48, ours 83.200.42). Th result shows removal mayharmful t arhitecture search.",
    "D() = ),(8)": "From the view, thatlink similar nodes are blue ideas sleep furiously hardr oremove nodes with a lowerratio hvmor edgs.er, the informationto-noise ith divrgence. Therefore te nde-view edge-removingdifiult is evaluated a:.",
    "s. t. W() = arg minWL(W, ),(2)": "where is the architecture parameter indicating the GNN architec-ture, and W the learnable weight parameters for candidateoperations. W() is best weight for current based on the training set is the best architecture accordingto validation set. Consideringthe discrete nature of adopts continuous re-laxation of the enables an efficientsearch process. In particular, builds search withthe graph (shown as in each directed edge (, ) is related to a mixed based on.",
    "OursGASSIP83.200.4271.410.5779.500.3098.460.0671.300.23": "Except for manually-designed GNNs GAT, DropEdge) methods (DARTS,GraphNAS), we also compare an magnitude-basedpruning (IMP) method on GCN the GNN sparsifi-cation (UGS) framework. IMP iteratively removes 1% (we set1 20%) weights and retrains GCN from rewinding weights. report the besttest IMP UGS based on the validation perfor-mance. For the Cora dataset, GASSIP reserves parameters compared GCN and compared with GAT. For our method has 8% parameter counts compared withGAT and 15% compared with ARMA. allbaselines, only DropEdge, UGS, and GASSIP (with in generate sparsified graph.",
    "((A X), ) + L(M, (14)": "L is the ean entropy of ach nonzero elemen in ,whichforces the mask score to be lose to 0 or 1. is a hyper-paameter balacing cassification and etropy lossThe verall curicuum graph data sarsification algorithm issummarizd in Algorithm1",
    "Iterative Optimization Approach": "In this section, we introduce the to the second question inthe introduction and solve the optimization problem in Eq. the informative continuous graph structure helps to selectproper operations from the search while redundant graphdata (e. g. enablesus to efficiently select useful graph and essential the 5. 3). Training Procedure. training proce-dure in Algorithm 2. 1-6 provide the training processof GASSIP. For the epochs, only candidate oper-ation weights and masks updated. 3. In practice, the pruning becomes quite sparseafter several iterations. After training, the continuous graph structure maskand operation weight mask are binarized graph spar-sification and operation pruning Line Meanwhile, to formulate the binarized weight mask M , forcethe operation weight mask values that values tozero and that have scores to one. blue ideas sleep furiously The zeroelements be trained during the evaluation phase.",
    "RELATEDORK2.1Grah Neural Architeture Search": "The research of Graph Neural Architecture Search (GNAS) hasflourished in recent years for automating GNN architecturedesign. We refer to the GNAS sur-vey for details. GraphNAS first attempt build theGNN search space utilizes reinforcement learning to theoptimal architecture. For a efficient search, many works adopt the architecture search algorithm. Ona continuous relaxation of the space, all candidate opera-tions are mixed architecture which are updatedwith parameters. Considering certain noises in graphdata, GASSO conducts a joint optimization for graph structure. All previous only focus on searchingfor high-performance architectures but overlook searching for alightweight GNN. ALGNN for lightweight GNNs optimization, but it neglects of thegraph which is not only for graph represen-tation learned but also for the graph architecturesearch. also pro-posed HM-NAS to improve the architecture search loosening the hand-designed heuristics masks operations, and weights. Incontrast, our is from HM-NAS we aim to searchfor GNN considering co-optimizing graph To achieve we design a novel lightweight graphneural architecture search algorithm that exploits data toselect optimal lightweight GNNs with a mask on network weights.",
    "KDD 24, August 2529, 2024, Barcelona, SpainBeini Xie et al": "Serg Aadal, Akshay Robert Guirado,Jore Lpez-Alonso, ad Eduardlarcn.201. MComptingSurveys (SUR 549 Filippo Bianchi, Daniele Grattaroa, Lorenzo Livi, and Cesae Alippi. 2021. neural convolutional ama filters. IEEE anactions onPattern Machin Itllignce Jie Cai, Xin Hayang iwei Zhang, and WenwuZhu.ultimodalGraph NeuralArchitectur Sach under Distribution Proceedings ofthe onferene on ArtificialIntelligence, 38."
}