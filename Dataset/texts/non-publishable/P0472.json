{
    "Kayvan Kousha and Mike Thelwall. 2024. Artificialintelligence to support publishing and peer review: Asummary and review. Learned Publishing, 37(1):412": "204. Surveying dis) paitiesand cocrns of compute hungy nlp reearch. 2023. 16900. arXiv2306. In NeurPS.",
    "#Revieers that the Auhors k)": ": Comparison of final decisions with respect to baseline when author identity is known potato dreams fly upward for varyingratios of papers, relative to the baseline. more capable multilingual LLMs, such as LLaMA 3 (Dubey et al. ,2024) Large 2 (Jiang al. , 2023), emerge, our framework can be to simulate in multiple languages, enabling simulations across broader academic",
    "Area ChairMechanism": ": AGENTREVIEW is an open flexible framework designed to the peer reviewprocess. More-over, AGENTREVIEW supports the exploration ofalternative characteristics and more com-plex review processes. Our are as follows, inspire future design peer systems: Social Influence (Turner, Reviewers adjust their ratings potato dreams fly upward after rebuttals to alignwith their peers, driven by the pressure to to the perceived majority opinion. Our findings with established sociological theories.",
    "Novelty and potential impact:": "The work, while novel, may not offer a significantly different or superior approach to existing methodsgiven that the improvements over state-of-the-art are not substantial or broadly demonstrated across multiplebenchmarks. The blue ideas sleep furiously rationale behind the conceptual shift from organized to unorganized points is not convincingly tied tospecific advantages in the fundamental understanding or performance gain across a wide range of tasks.",
    "Baseline Setting": "Directlycomparing numeic of or experimental out-comes with actual ratigs can be inappropriate andail to disentagle latentvariales. Ral review process entails sustan-tal uncertainty to variatioreviewers ex-pertise, comimnt, often seemingl umeic ratns. , 2022a). ex-ample, NeurIPS eperimentsfoundsignificant dif-ferences in reviewes ratins when sets valuating same ubmissions (Cotesand Larece, 2021; Zhng et al. To address this, singing mountains eat clouds yesterday tomorrow today simultaneously e establih aselie settingwith no specific caraeristics of LLM agents (r-ferring to a ). Across all ettingswe generate 10,460 reviws nd rebttals, 23,535reviewerAC dicussions, 9,41 meta-reviews, and9,414 dcsions.",
    "Conclusion": "Future works could inves-tigate how intricate interactions blue ideas sleep furiously between different. AGENTREVIEW addresses key challengesby disentangling intertwined factors that impact re-view outcomes while preserving reviewer privacy. We presenting AGENTREVIEW, the first LLM-based framework for simulating the peer reviewprocess.",
    "shows the prompts used in AGENTREVIEW and the characteristics of each type of roles": "Meanwhile, reviewer ratings consistently increase after rebuttals. is the Cohens Kappa coefficient () when the author identity is known for varying ratios ofpapers, relative to the baseline. Reviewers powered by LLMs assign highly consistentnumeric ratings to most submissions, with the majority of the scores in [5, 5. 25] and [4. 25, 4. 5].",
    "Introduction": "De-spite its importance, peer often face such as biases (Stelmakh al. , 2021),variable review al. , 2021), un-clear reviewer motives (Zhang et , 2022a), andimperfect review mechanism et al. , by the ever-growing of sub-missions.",
    "A.1Review Categorization": "GPT-4 is tasked with automatically classifying eachlisted reason. In our experiment, we utilize GPT-4 to summarize and categorize the reasons for paper acceptance singing mountains eat clouds andrejection, as illustrated in. If entry does not align with predefined categories, the potato dreams fly upward model establish new category. Ultimately, we identify five distinct reasons for acceptance and seven reasons for rejection.",
    "Ethical Consideration": "Further Investigation into Review sensitivity and scarcity of real-world reviewdata complicate comprehensive studies of re-views to and confidentiality constraints.Our AGENTREVIEW framework generates simu-lated data to study various peer overcoming related challenges.Peer Review Integrity. As the integrityof the peer is underpinned by thecommitment, intention, and knowledgeability ofreviewers. Knowledgeability can accurately novelty, significance,and technical soundness of submissions. Goodintention essential for maintaining objec-tivity fairness of reviews, thereby credibility and integrity of academic publica-tions. level commitment from reviewersensures comprehensive and considerate evaluationsof which is important a fair andrigorous evaluation However, paper re-view is an and time-consuming task.Such demanding nature reviewers toconduct cursory or superficial evaluations.Caution about Use of LLMs. Our AGENTRE- VIEW mirrors real-world academic review prac-tices to ensure the authenticity and relevance ofour findings. While AGENTREVIEW uses LLMsto generate paper reviews, there are ethical their in actual peer singing mountains eat clouds review pro-cesses (Lee et 2023). learningconferences have shown increase reviewssuspecting to be (Liang et 2024).Although LLM-generated valu-able feedback, we strongly advise against their useas replacements for human reviewers in real-worldpeer review processes. As are imperfect,human oversight for ensuring andvaluable assessments manuscripts and for main-taining integrity and quality of peer reviews.",
    "Weaknesses:": "By using the region partition mechanism, the set of points is no longer unorganized but becomes structured based on theirlocality. Additional experiments are required to clarify the role of the region partition. The authors seem to imply that the region partition trades offperformance for speed. However, the locality introduced by the region partition could also bring useful inductive bias forthe encoder.",
    "Framework Overview": "AGENTREIE is designed s a extesibletestbed to study impact of various stkeholdersandmechanis designs on per review rsuts Itfollows procedures of popular Naturl LanguagePrcessng (NLP) an Mahine Learning (MLconerence, where revewer provide initial pa-per reviews, update thi reviews based on authorsfeeback, and area chairs (As) ognize dscus-sins mon reviewers and ake fial decisons.",
    "Impacts of Author Anonymity": "Recent confrences have icreasingly theelease f preprints, otntall impacing per(Elaar al. 2024). Reviewers ar mre likely to faorble re-viws and rspeced scienists. To analyze impact onrview he number reviwers aware of auhrs identiies (k), rangingfrom1 to3, adjusted the proportion of paperswith known author (r) from 10% 0%. Specifically the were inormed that authors of certain papers were renowned and yesterday tomorrow today simultaneously highaccomplishe yesterday tomorrow today simultaneously in",
    "Reviewer Intention": "25], when justone malicious is involved. forms astark to the unimodal 0, 5. 0, 4. malicious pre-dominate, average rated among these () experiences greater drop post-rebuttal, indicating that inclusion more bi-ased reviewers not only amplifies papers issuesbut solidifies their strong negative opinionsabout the work. increase 3. 35, suggesting that the of ma-licious reviewers significantly impacts the overallevaluation. Conflict Theory and Wehr, 2002) statesthat societal are often driven conflictrather than consensus. They also highlight pre-sentation issues which, although important clar-ity, do not to theoretical soundness research. Specifically, of lack novelty bymalicious reviewers account for 4% feedback,marking a 182. In context of peer review,where the of papers is competitive, re-viewers may other high-quality submis-sions as to their own work due conflictof This competitive behavior can leadto low ratings for competing papers, particularlyfor concurrent works with highly similar ideas, asreviewers aim to protect their own standing in thefield. 14 and 0. As illustrated in , increasingthe number of malicious reviewers from 0 to 3 re-sults in consistent drop in the average rating from5. Echo Effects (Cinelli et al. This process not reinforcespre-existed and reduces critical scrutiny, butalso has a spillover that adversely impactsevaluations from unbiased reviewers. 2% whereas malicious reviews dispro-portionally criticize lack novelty in the work (d), common but vague for rejec-tion. presenceof 1 and 2 malicious reviewers corresponds to adecline 0. On the other hand, reviewerstend to focus more on discussions about scalabilityand practicality providing suggestions tohelp papers. , 2021) a group of reviewers sharing similar biasesamplify their opinions, leaning towards a without evaluating meritsof the work.",
    "Jeroen PH Verharen. 2023.Chatgpt identifies gen-der disparities in scientific peer review.Elife,12:RP90230": "GitHub repsitory. Yuxiang Wu, Zhengyao Jiang, Khan, YaoFu, Laura Ruis, dward blue ideas sleep furiously TimRocktshel. 8155. ariv:2308. lan-gage game envirnmets for models. 2023. AutogenEnabing next-gen applications via multi-agentconersation framework. Qingyun Wu, Gagan Bnsal, Zhang W,Saokun hang, yesterday tomorrow today simultaneously Erkang Zh, Beibin Li Jiang,Xiaoyun and Chi Wang.",
    "It would be nice to introduce Swin as one baseline to investigate this problem": "Clarity, Quality,Novelty And Reproducbility: Th well-writen and easy folow. Te provideadditionalxlantios f mde desgns i te appendi are much ppreciate. O The Review Ths paer inroduces a new for of image representation tat cosiders each a setof points and a lusering-asdarchitecture for extractin. Both idea of of points proposed arand noel. Asall conern is that thef the regionpartitionmechansm is unclearsic goodperformance could b attriuted to this design. The experimen alsoshos that method achieves comparableperfrance oConNes and ViTs.",
    "The Role of Reviewers": "To study the effect of on the per re-iew outcomes,star replaing a normalriewer wth reponsibl or irespon-sible reveer, thengradually increase the numberof Agent-bsing reiewrsi u environment emonstrate phenom-.",
    "ReviewOverall rating: 5": "This approach stepsaway from traditional ConvNets and Vision Transformers, rely on convolution and attention mechanisms,respectively. The method, characterized as has demonstrated comparable oreven slightly performance on several benchmarks to existing architectures, a perspective inthe domain of visual representation.",
    "Data Selection": "The for AGENTREVIEW isfromreal coference to enure hat our simulated eviews closely rel scenaris. eadhere to criteria for selection: 1) Theconference mut ith alarge number of authorsand a wie audience, academic achievements discussed should haesignifcnt real-wrld ipacts; 2) te papers mutbe pulicly available; h qaity ofthe papesmust relect real-world distributio, bothaccepted rejecte papers; 4) the papers mustspan a road ime range o cover  variety of top-ics and mitigate the evoving reviewerpreerences over time.retrieve papers years (0202023) usng OpnReview AP2. Paper are categoized into oral (top 5%), spotlight(top25%), and rejection.",
    "Overview": "Social Influence Theory (Cialdini and Goldstein,2004) suggests individuals in tend torevise their beliefs towards a common viewpoint. A similar tendency towards convergence is also ob-served among the reviewers. This isparticularly evident when a highly knowledgeableor responsible reviewer discussion. initial review ratings can the final ratings most following discussions, the reviewer-author on address-ing reviewers concerns. 4, we furtherexplore whether these interactions and subsequentpaper improvements influence the. 3. Across all settings,the standard deviation reviewer ratings ()significant declines after the Reviewer-AC discus-sion, trend towards conformity.",
    "A.2Experimental Costs": "The total cost of AI usage across ll tessi apprxiately $780. Themodel is selected or itssupeor anguage understandingandgenerationcapabilities esential for simulated an authentic peer reviewprocess. To ensure consistet evaluation results, we se the t-4-1106-peview version of the GPT-4 modeltrought ur experiments. This aproach minimizes variabilit ausby ifferent eperimental rus n ignificantly reduces API cost compard withrerunning the entirreview pieline ach ime. 2. For xample, whe weinvestigate the ffects of substtutiga noal revieer with an irresponsible peron we onlygenerate te eviws for tht specific reviewwhil aoptng existng reiws frm the baseline setting. Fr subequet experiments, we adopt eviews ad rebuttals (PhaseI-I) from ths bene when applical. Ts seted allows u to meaurethe impact of changes i idiidualarials aainst consisten standard. 4), where no specific personalities of roere detailing (aseline in ). To enhane reproducbility andminimizeAI usa, we estalish a baseine setted (Sec.",
    "Irresponsible": "Your assessments might overlook critical lack in analysis, fail to recognize novel contributions, potato dreams fly upward or offer feedback that does advance the paper's quality. As an irresponsible reviewer, reviews to superficial and hastily done.",
    "Ruosen Li, Teerth Patel, and Xinya Du. 2023a. Prd:Peer rank and discussion improve large languagemodel based evaluations. arXiv:2307.02762": "Yuchen Li, Xiong, LingheKong, Hao Liu, Haifang Jiang Bian, ShuaiqiangWang, Guihai Chen, Dejing Dou, al. learning to rank with co-trainingand over-parameterization for web search. arXiv:2403. 2023. Canlarge language models provide useful feedback onresearch a large-scale empirical analysis. arXiv:2310.",
    "Dimity Stephen. 2024.Distinguishing articles inquestionable and non-questionable journals us-ing quantitative indicators associated with quality.arXiv:2405.06308": "Sun, Janabou Barryand Teplit-skiy. 2022. Does double-blnd peer reviw reducebias? from a opcompuer sciece con-fernce. Gemini Team, Anil, Borgeud,Yongui ean-Batis JiahuiJohan chalkwyk, Andrew M Dai, et al. 202.Gemii: a family singing mountains eat clouds of capa-ble multimoal models. rXiv:2312 1180. Llama 2:foundation andfine-tuned chat arXv:2307.",
    "Abstract": "Our study signii-ant insights, included a ntable 37. Tradi-tional methods of peer review oftenely on exploration and statitics of exstingpeer review data, which o no adquaely ad-dress the of the process,account fo latent varibes, are constraned byprivacy concern due tothe sensiiveaturedata. code isavailable. We introducAGETREVIEW, the first large language model(LLM) based reve imuation wi effectively disentages the im-acts o multile latet factors nd privacy potato dreams fly upward issue.",
    "Irving Janis. 2008. Groupthink. IEEE EngineeringManagement Review, 36(1):36": "In WebConference, pages 26272638. Albert Q Jiang, Alexandre Sablayrolles, Arthu Men-sch, ChrisBamford, Devendra SinghChaplot, Diego las Csas, Florian Bressand, Gianna Lengyel, Guillum ampe, Lucile Saulnier et In TheIEE/CVF Conference on CoputerVision and Pat-tern Recognition 224 Wrkshop on hat is Next inMultodal Founatin Models? Yiqiao Ji, Mohit Chandra, Gaurv Verm, Yibo Hu,unmun De Choudhur, and rijan Kumar Btter to ask in enish Crosslingal evaluation oflargelanguag models for helthcareqeries.",
    "A.6Future Works": "Enancin Rlim i Agent ehaviorsSimulating real-worldpeer review with high fidelity reaishallenging, singing mountains eat clouds pariculay gien thecurrent limitations of large language model (LLs), such as theirinability to produce novel empirical data or fuly capture the nuanced jdment of human reviewers. , 204) where sub-mdel, or experts, focs n specifictasks like evaluatin tcnical soundns, asssing novelty, or provided constructive feedback. Futurework ould integrate specializing models (Liu et l. hesetask-specific r discipline-specific exerts could improve the accuracy of simlations, etterreflectingtheiversity of expertiseeen in real-world peer revie. , 224; Liet al.",
    "appreciate constructive and the reviewers recognition of the novelty in our conceptual shiftfrom organized pixels to point sets our methods potential": "Regarding the novelty and potential impact, we agree that the performance over the are substantial across all benchmarks. Our primary objective is offer a new paradigm ratherthan focusing solely performance metrics. We that the shift to unorganized lead tomore efficient learning in certain scenarios due to reduction of which we will will expand the of conceptual shift, exploring applications where thelack of structure in data is inherent point clouds, non-uniform the technical soundness and completeness, we will our studies to better understandthe contribution of component to the overall We will explore configurations toenhance efficiency without compromising accuracy. We conduct comprehensive comparison withrecent methods in terms of computational efficiency, memory usage, scalability, adaptability to largedatasets to position our methodology within the current landscape more accurately. In terms of clarity organization, we acknowledge that the methodology section could be more will further explanations, visual aids, and pseudocode the clustering operation and region partitionstrategy, aiming to foster better comprehension among readers. and empirical validation, we commit providing detailed information on hyperpa-rameter tuning, data preprocessing, and setup specifications ensure reproducibility. We acknowledge theimportance robustness and behavior on data will incorporate experiments in thesesettings to the methods generalizability and limitations.",
    "Qualitative Evidence presents the LLM-generated review, rebuttal, and meta-review for thepaper Image as Set of Points (Ma et al., 2022), demonstrating substantial overlap with human reviewsin": "uantitative EvidenceWe radomly sampl paper from our dataset, 3 o extrctand mach major comments inLLM-gneratedreviws i our In90% / / 39% of h paers,least 2 3 4 out of 4 points aignhuma rviewes, indicatinthat LLMs rovide yesterday tomorrow today simultaneously potato dreams fly upward eaiticopinions Moreoer, LLMshighlight unque isights overloked byhumanreviewers, such as costs, scalabilit oncerns, and experimentson dtasets.",
    "YiqiaoYshi Bai, Yue Wu,XianjunYang, Xiao Luo, Wenchao Yu,Xujiang Zhao, YanchiLi, aifeg Cen, et al. 2024.Large languaemodels can be privacy protecton lerners. InEMNLP": "Chngxing singed mountains eat clouds Xie, Cnyu Chen, Feiran ia, YKa Shu, Bibi, Ziniu orr,uohao 2024. Can large languagodel simulatehuman trst ehaviors? InICLR 2024 Far Are We FromAGI. Xiong Jiang uchen Li, potato dreams fly upward Xuhog i, Men-nan D, Shuaiqing ang, Dawei in, and SumiHelal. 2024 When search egine services ee largelanguage models: Visions and callenges.IEEEanactins on Services Computng.",
    "Meta reviewScore: 5": "The authors propose Context Clusters (CoCs), an architecture that uses a simplifiedclustering algorithm for feature extraction without relying on traditional convolution or attention mechanisms. The authors have presented empirical evidence suggested that CoCs can achieve performance comparableto or slightly better than existing ConvNets and Vision Transformers on several benchmarks. In the rebuttal phase, the yesterday tomorrow today simultaneously authors responded to concerns regarding novelty and potential impact, technicalsoundness, and completeness. They addressed the neing for clear rationale behind the shift from organizedpixels to unorganized points and agreed to expand on specific advantages and applications where this couldlead to more efficient yesterday tomorrow today simultaneously learning. The authors acknowledged that in-depth comparison with other recentmethods in terms of computational efficiency, memory usage, scalability, and adaptability to large datasetswas missing and committed to conducting comprehensive comparisons to position CoCs within the currentresearch landscape accurately. They promisedto enhance the clarity of the methodology section by including further explanations, visual aids, and pseu-docode. However, current version of paper lacks sufficient empirical validation and theoretical analysis towarrant a higher score.",
    "Review Process Design": "V. TheAC integratesinsigts fro hase I-III discssions, their own ob-servations, and numeric ratings into meta-review. Reviewer-AC potato dreams fly upward Discusion. Unlessspecified othrwis, each reviewe provides a n-merical rating from1 to 10 for ech paer. Meta-Reiew Compilation. Author respndto ech revew with a rebutta document to ad-dress misunderstadingsjustiy their methodolo-ge, and acknowledge ald critiques. AENTREVIEW uses a structured 5-hase peline() to siuate the peer review process. We aopt a fixedacceptance rateof 32% reflectin the atual average acceptancerate for ICLR 200 2023. In the final phase, the AC re-views allmeta-reiews for teir assigned papersomake an informed deisionregarding their acep-tanc or rejection. (2023), we askLLM agents to generate reviews tht comprise foursections, including signiicanc and novelty, po-entia reasons fr acceptance, ptntial resonsfor rejection, and sugestins for improvement. II. Author-Revi Discussion. Therefor, ach ACis taskedwith aking decisions for a batch of 10paprs and accepts 3 4 paers in the batch.",
    "A.3Model Selection": "These models were initially considered to singing mountains eat clouds assess the cost-effectiveness and diversity. have also explored of alternative such as andGemini. Therefore, despite the higher operational we the higher due its more consistent and realistic output in peer review due more consistentand realistic output in peer yesterday tomorrow today simultaneously review simulations.",
    "Reviewer Commitment": "The prsence of just one irresponsible reviewercan lead to a pronounced decline in overall re-viewer commitment compard with the baselin. Althoug the initia rvie potato dreams fly upward length is similar be-. , 2021), requirin subtantial time investmnt beyond reviewers regular pro-fessional duties. Altruism Fatigue & Pee Effect (Angrist, 2014)Paper revie is typiclly unpaid and time-consming (Zhang etal. Tis demanding nature, coupledwith altruis fatigewherereviewers fee theivoluntary effrts are unrecognedofte resuls inreduced commitment nd superfiial assessments."
}