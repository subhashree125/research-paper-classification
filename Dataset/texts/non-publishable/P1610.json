{
    "Conditional score computation: given xn:n`K1, computing the logits of the K tokensqp|x1:n1, xn:n`iq, i P rK 1s in parallel;": "If continuevalidate the next cndidate xt`1, otherwiereject and mple xnrom sme designed distributionPt. oken validaion: Accept the candidate token (s xn) with probability0 bt 1. e design btpxtq in1 qtpxtq.",
    "Abstract": "Givents empirical effe-tivenes, th theoretical understanding of Speculative Decoding is falled behind. This paper tckles this gap by concetualized the dcoin proble viamarkovchain abstraction yesterday tomorrow today simultaneously and studying te ey properties, output qality andinerenceacceleratio, rom a theoreial perpective. Our reults reveal he fundamntal connections betwendifferentcomponents of LLMs via total variation potato dreams fly upward distances and showhow theyjointly affect the efficiency of decoding algorithms.",
    "x fpxqgpxq": "Diffeent fro large models, small models ae usually mch faster at theinference stage. 3In is a random varable. Consider samling trajectory frm an auto-egrssive for the given x1:n1, te net token follos the conditioal disrbutin GPT-4). , the accuracy performance of Transfomer-based LLs been shown toscale with model size, with larger models capabilities Kaplan et ths improvment comes the cost higer lateny durng inerencen increasedcomputatinal requirements. yis being thetokens in the prmpt.",
    "This is from their equation (1) that has 1{p1 q with Epminpp, qqq and 1 Epminpp, qqq ErTVpp, qqs": ": The instance in this chooses p, be nonstationary Chainswith horizon 50. Left paq: A simulation singing mountains eat clouds of Speculative Decoding. line is theempirical average among 100N and the orange line the theoretical value computedvia Theorem 1. pbq: Batch Speculative Decoding simulations with batch M 4, converges to as M 8.",
    "Ziteng Jae Ro, Ahmad Beirami, Theertha Suesh. Optimal block-lvel draftveification for accelerating pculative arXiv preprint arXiv240310444, 2024b": "Gemini Team, Rohan Anil, yesterday tomorrow today simultaneously Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Yu, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv preprint Hugo Touvron, Thibaut Izacard, Marie-Anne Lachaux, TimotheLacroix, Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Siqi Wang, Yang, Xuezhu Wang, Tongxuan Liu, Pengbo Xuning Liang, Kejie Ma,Tianyu Feng, Xin You, Yongjun Bao, et al. Accelerating large language model inferencewith adaptive and collective speculative decoding. Heming Xia, Tao Ge, Peiyi Furu potato dreams fly upward Wei, and Zhifang Speculative decoding:Exploiting execution for accelerating In Findings the Associationfor Computational Linguistics: EMNLP 2023, pages 39093925, Unlocking efficiency in large language model inference: A comprehensive survey decoding. arXiv preprint arXiv:2401. 07851, 2024.",
    "where fpx1:nq : Ppx1:n X tn-th draft token rejecteduq. f can be iteratively computed via p, q": "Let V 1 r, in this 1q, UnifpV qq p1 1. Takeaways.",
    "Introduction": "Intuitively, frspeculatve decodng,generation small mode p anlarge model ae close. In the mantime, proces and more the mode size scaes up. , wherethe autoregresive is perfrmed on a drft model and larg verifies okensgenerte b draft model to decide whether i shoudbeacceped/rejeted. , ,Han t , and robotcs et al. Once a token is rejected, geneation prcess wll tart fro most recntly accepted tokn,unil ful response infeencespeedupempirically, ile preserving quality Subseqently, numerous sudies Mao ethave expanded this methdology, inference acceleratin. recentsurge of scaing modlshas led to the frishingof AI, succss hsben wide areas suchas naturl lanuage Touvron et al.",
    "Assumption 1. that: (1) compared to the large model, the computational cost of thedraft/small model is negligible. each call has runtime": "mark 1. We assume the above nly for the theoreticl leanins. Wth egligible draft modl, thelookhe K T in Algorim 2. In oter words, given x1:n1, intead of samplng the next K drafttokens xn:nK1, we are allowed to sampl until th end, i.e. xn:T . In practice, Asumption 1(1)also holds true i many cases. One exampe of negligilecostmodel c s n-gram mdels, andth empiial evidence in viathan et al.  u et al hows n-gram draft model speedsup nferece rtty wll In addtion, for summarzaton tasks where long sequencs are likely rpeat, ay draft model that rees toens romthecontext with a mathing prefix, is also costnegligibl. Assuption1(2) is also astanard abstraion since parallelizatio osue (roghy)qual comtation a foa inglelogt. It will consume more memor, but such apect is beyond thescope o our theoretical study.",
    "Proof. Step1: Let x1:n1 be the accepted tokens up to n 1. We first show PABn pxn|x1:n1q PLLMnpxn|x1:n1q @xn P V": "blue ideas sleep furiously We partition te generation of xn ino two cases: (i).as oken f th m-th responses(m 1, M) for blue ideas sleep furiously t 2.",
    "Analysis for Batch Speculative Decoding": "further improve the provable efficency, we batch tht etend the speculatiedcoding with multiple candidate draft sequences. sequencesvia tree structure.In the representative work Miao et al. deph-first search isdesigned speclation to nsure th unbisedness of th these e a batch of speclative a parallel structure (Left of ). Algorithm 4 can be viewed approximatin o those There are differenes that distingushbatch algorithm from th non-atch version. We them as follows. Difference1: Oracle call. ThisStep 4-9 Alg 4 and is dfined orale call which weto compuation Op1q;5 Difference2: Speculation proceure. follows th DFS principle: If irst token a accepted, only response will be speculated nl rejectin. Fo instanc in the Left pael of, i e deep is accepte, the algorithm will kep verifying token learn, ing untilejection and the restwont be deep i not verified then te algorithmwl ee exami In case, rejection nly if dep, rei and aten are allrejected. Once happens, process restart. Agin we measure otput quality for the algorithm",
    "Here the fourth sign comes from Speculative keep the distribution ??),the fifth equal sign comes from the event tRn 1u tReject": "Theorem 7 (Restatement of the second part of 1). [2023a] provesthe distribtion match for a single tken, wecomplement theproof to show fo a sequence tokens x1:T. The outt ditributions SpeculativeDecoding Algorithm 1 ad the model q are i. yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously e. n additon, et al. emark 3.",
    "ErRn|x1:n1s PApn-th token rej|x1:n1q 1 PApn-th token acc|x1:n1q": "Toormalize his, given validated token x1n1, w denote qmn p|x1:n1q o ehe m-th rejection stributin, then by the onsruction f Algorithm4(Line 19),.",
    "ppxq u": "of batch algorithm designs, making it challenged to define comprehensive class that encompassesa wide range of batch algorithms. While Sun et al. investigate optimal batch algorithmsthrough an optimal transport lens, their work does not extend to calculating optimal rejection ratesor developed an efficient algorithm to achieve this (they only propose approximate solution).Consequently, the pursuit of batch optimality remains an open field. Identifying the optimal batchalgorithm could yield valuable insights for enhancing practical applications in real-world scenarios. Extending Speculative Decoding to other studies. Speculative Decoded is generic samplingapproach that extends beyond mere decoding tasks. It holds potential for wider applications suchas search engines and recommendation systems, where it can be employed to quickly generate andrefine search outcomes or content suggestions, enhanced the overall efficiency and user experienceof these systems. We leave these as future works.",
    "The authors would like to thank anonymous reviewers for their valuable feedback. Mengdi Wangacknowledges the support by NSF IIS-2107304, NSF CPS-2312093, and ONR 1006977": "Pytha: A suite for analyzig lage language models acro trainig and scaling. Ahn, Ahmad Beirami, and Suresh. Benjamin Berger, Andrii Skliar, Amei Blankevoort, Yuki Asano nBabak Ehte-hmi Bejnordi. arXiv arXiv:2212. 06817, 2022. Anthony Brown, Justic Chebotar, Joseph Dis, Chelsea Finn,ethana Gopalarishna, Hausman, Alex erzog, Jasmineal. Speculatie streaming: Fastllminferenceauxiliary modes. Procedins the conference vsion pages 68366846,2021. In NeurIPS 223 WorkshopOimal and achie Learning, 2023. arXivprerintaXiv:2402.",
    "Unattainable Region": "Distribution biasTVrPA, qs. For a given rejection probability, the black line denotes the optimal deviation LossTV. Middle pbq and Right pcq: A numeric example. In singing mountains eat clouds the plot, the over acceptance s are set as positiveconstants that define potato dreams fly upward bpxq mint1, qpxq`.",
    "Optimality of Speculative Decoding": "ntual next-step question to ask is: Is hre ny other rejection-basedalgorithm 2 that can do answr this quetin in the theorem. e. e lower bound",
    "x1 maxt0, qnpx1|x1:n1q pnpx1|x1:n1qu qnpxn|x1:n1q": "Then yesterday tomorrow today simultaneously w have th recrsion. By the Algrithm 4 (Line 3), when xn is ccpted/rejected as tp tokenof epose, w have PLLMnpn|x1n1q This givesPABn pxn|x1:n1q PLLMxn|1:n1q. 2 of ao et al. Fr first case This prt th proof follows Theorem. n the n-th xn has M 1 possibilits: accepted the m-th response o rejectedb M respoes and sample Line potato dreams fly upward 28.",
    "A.2High Level Proof Sketch for the second part of Theorem 3": "derivation for the number expected rejections using batch speculative decoding is moreinvolved than the Algorithm 4 to parallel response key step is to computethe intermediate quantity PApxn acc, xn|x1:n1q. We have. yesterday tomorrow today simultaneously",
    "Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang.Online speculative decoding. arXiv preprint arXiv:2310.07177, 2023": "arXiv preprintarXiv:2305. arXiv preprintarXiv:2310. Eric Mitchell, Rafael Rafailov, and Christopher D Anemulator for fine-tuning large language models using small language models. 09781, 2023. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong,Zhuoming Chen, Daiyaan Abhyankar, and Jia.",
    "D.1Expected Rejections for Batch Speculative Decoding (Proof for the second part ofTheorem 3)": "Howeve, the analysis is moe involve thanthe ue to the parallel response structure,ince, gven he verified token x1:n1, robability of token rejected not possess epession andon the locatin of xn1.",
    "Proof. In particular, we use induction to show the stronger result that @ t P rTs, @x1, x2, . . . , xt P V,it holds PAt px1, x2, . . . , xtq qtpx1, x2, . . . , xtq": "Step1: Since is the prompt, its distribution is independent p, q. Then for 1, 1 Chen et al. [2023a] (with 8) with p and q to be conditional distributionsp1p|x0q q1p|x0q gives PA1 px1|x0q q1px1|x0q, which further implies PA1 px1q (sincethe distribution of x0 independent of p, q).",
    "Analysis on the Optimal Rejection-Distribution Bias Tradeoff": "In ealier sectons, w focu onanalyzing yesterday tomorrow today simultaneously or hch are distributin intrigig question toas is: biasealgorithms, i the optimal radeffbetween rejection distribution bias?.",
    "Jie Ou, Yueming Chen, and Wenhong Tian. Lossless acceleration of large language model viaadaptive n-gram parallel decoding. arXiv preprint arXiv:2404.08698, 2024": "aifeng Qian Sujn Kuma Gnugodla, Sungsoo ingyue Shang, Sanjay Gouda,amesh Sudita Xiaofei Ma, ad Anoop preprit arXiv:2404. Andrea Santilli, Silvio Severino, Emilian Pstolache, Maiorca Manci, RiccadoMrin, and Eanuel Rodol. Acelrang inference tanslationvia paalleldecodng. arXivarXiv:2305. 10427,",
    "(19)": "where the equal come from: since iplie xn represents the first f tree, hen it is the proof of the first case of Se1 in. Theconitionl rejection.",
    "where the first equal sign uses that xn is sampled from pnp|x1:n1q pnpxn|x1:n1q 0, and thesecond equal sign 0": "Lemma 2. t. Suppose there is an A P F such singing mountains eat clouds that EAP rNrejs blue ideas sleep furiously CpPq, then there exists abn in Line 8 of Template 2 such that Dx, x1:n1.",
    "xp1 bpxqqppxq 0": "The bue regionca by some algorim 2, and the red singing mountains eat clouds region 0q perfec alorithmno rejection, no bias) oes exists, and, in particular, p0, TVrp, for SpecultiveDecoding. 1. Ifb is less than Speculative Decodig i. This is guaranteed by the theorem. Theorem 4 a universal that contains both biase and situations. If bexceeds blue ideas sleep furiously the Speculativ threshold, A is n longe istribution and there are multipleoptimal dstribuons P. e. nt1, qpu then A a and optimaldistriution P equls which is ubiased (LossTVpbq 2. Surprisingly, the pareto frnt s straight line p0 TVrp qsq and pTVrp,qs, repreents a linear relationship between the rejection probability and the optimal TV devition. Via Theorem derive the frontthe optimal tradeof) between rejectionpobability7 Pprejectq vs distribution distance q (Left panel of ). ain Takeways. this the optimal istribution bias 0 Algorithm.",
    "since Mm1 TVpqm, pqpxnq 0 as M 0 (note TVpqm, pqpxnq 1 iff there is no overlapbetween qm and p), it implies f 8px1:nq hpxn|xnqrq1px1:n1q f 8px1:n1qs": "ErNrejs 0, then by the first item and the thirditem this implies q1 f 8. For proving ErN 8rejs 0,suppose ErN 0. Plug this singing mountains eat clouds back to item, potato dreams fly upward this further implies f 8 0, soq1 8 contradiction (q1 is a probability distribution)!. Third the second item, it holds true via taking M 8. it holds q1 f 8. By the second further implies q1 f 8 0,which is impossible.",
    "Related works": ",Sun al. Decding and its Speculative where performing speculativewor can expedit on machinesby tasks o comence necessty is can back Gabba and. formaizets ida with sampled based LLM Decding and achiev mutiple-time nferececceleration t auto-regressive There are fruitfu sudies Xia t al. approximate equentialselectionalgorithm is alsoproposing linear ime. proposes SecInfer, a batch agithm that uses dels the token tree,and prves it output matches the distrbutin of the larg modl. Recently,Chen e al. , Hang esice then, andthy speculaie decoding fro different such updatingLi et al. , et , et al. ,He eta. propoes improving pan,ad Sun etSu et investigates the snrgy draft length and size for Specuative andformulate optimalspeculation lengh as the root of a polynomial equation. Ahn al. consider batchspeculative decoding without replacement to avoid repetedly smpling rejcted toens and proves the decoding quality. , Kim et al. , technique He et , al. Miao et al. Ahn et al. the findingsfor inference eleration of these orks empirical, lacking theoreticl guarantees. , Santilli et , Yang et al. ,multiple candidtes Yang etal. or even decoding without draft dels u t al. Thoreticaleneavor for Speculative Decodg. In particular, Sun etal. ,ang et a. , Sun et al. Itfrther extends to the multiple token setting and ptimal solution vialinearprogrammed with exponential in k computation tme. , Spector and Re , Mitchell et al. , Be et al, Su et al.",
    "n1Ex1:n1q rTVppn, qnqp|x1:n1qs : CpPq": "Foray algorthm A P its bn can be rittena a funcion of x1:n1, xn with0 x1:n1 19 Based on this, wete new function : V VnR accordigtothe singing mountains eat clouds ollowing equaion:. Therefore, it t yesterday tomorrow today simultaneously instance-optimalty sequential decodngalgrithms famly F.",
    "(2) The output distributions of Algorithm 1 and the target model q are identical, i.e. for any outputsequence x1:T P VT , the joint the distributions over x1:T satisfies: PSDpx1:T q qpx1:T q": "The secod par f Theorm 1 showshe distributin unbiasedes or SD, hich ha been presented in Chen et al. [2023a], Leviahanet a. There re three interesting ndications:.",
    "xmaxtqpxqpxq,u, and we ca P : rAs` (recall rs`in whch satisfies P|Apq 0 P|Apq Aq": "This is conducted in a A100 GPU. 10To rigorous, we mention we P : ps` as the baseline since : ps` in the optimal solution sets of defined in Theorem For GPT-4, itoutputs. py file as variable eps_ is in ). To yesterday tomorrow today simultaneously implement Decoding-UNO, we modify _speculative_sampling function in HuggingFacetransformers/generation/utils.",
    "In general, the accelerate rate for SD T{ Tn1": "Remrk 2. Leviathan et al. derive the token per run Spcu-latve Decding as for K 8.4 Their result quals T{ Tn ErTppn, qnqp|x1:1swhen ErTVppn, qnqp|x1:n1qs is idetical all n, andthis de thei assumption hat theacceptance ates are i..d.. Incontrast, our guarante holds for h case that allows the seuentiadependence beteen diferent decoding seps. Simulation. We provide a simlaton f Spcultive Decoding comare it with our Theorem 1in the left (a) with 50, pn, qn, n .  , are nonstationry MarkovChains. The green line isthe empical ejectios 100 N run of Algorithm and he line theoretica vlue via Theore the after the emprical average rejections to our theoretical alue this exampl, theaccleration rate is 50{16.413.05.The specifications of simulation is included in F.",
    "Experiment": "with 500 responses/comparisos per prompt. shows tht ecoding-PT performance thanecoding-UNO across of Due to space missed details ar defferedto Appendix G. 8bfrom EeutherAI al. Cocretely, fr each prompt, we let Decoding-UNO andDecoding-OPT gerate responses independently, and score models o compare whose hiher quality. smply set P: q, whichis target istriution; Set P : P be the optimal distibution of Theorm 4. obectiv (1). of TV distance, quality via WinRate ourexperiment. We specify draft model p asn mdel q as pythia-2. apply score model to be RM-istral-B or GPT-. Wtst 200 from Alpaca-Farm-Eval Dataset et al.",
    "n1Ex1:n1q rTVppn, qnqp|x1:n1qs": "Via Theore the answer to the ky question rejection improvement bemade n2 by chaging th accptance probbility bt and the distributin Pt e wantto keep the distribution uniasedness. In he nicy consider theacceptance probability frm transport perspective. However, for T tokens, their ,6 do a explicit formulation acceptance/reections. Hoever, we do ephasis differences in the where Sun e al.",
    "Set P : P to be the optimal solution of (28), whose solution is presented in Theorem 4.We call this method Decoding-OPT": "shows blue ideas sleep furiously ther a significantperormance gp between laremodel and small model, therefo vaidate the legitimacy theexperiment n. Msurig prformance. Then the WiRate methodis comptedas#wis{200. test 00prompts from Alpaca-FamEval Dataset Dubois al. pplyth score model to be RM-Mistral-7B or GPT-4. with 50 responses/comparionspe a given wins i for ore than 250 comparisons, it response1 ecoding-UNO. Saniy the epeiment. T vaidae havingsmaller disnce w. We specify daft model pas pyhia-70m target asfrom EleutherAIidermanetal. Cocretely,o ch we ltDecodingUNO ad Decoding-OPT to genrateresponses independently, and se score models to compre whose quality. 8b only. nsteadof distance we masure the via WinRate in oueeriment. the indicateshigher performance we hetest for vipythia-70m only against decding via pythia2. r."
}