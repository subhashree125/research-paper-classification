{
    "MLM Promtng MetricsTo how existing MLLMs perform on the multi-": "asect vdeo evaluaton task, we designed a prmpt-ing template in to let the oupt scoresrangig from 1 (Bad)to 4 (Perfect) for eachapct. However, sme models, includn Idefics2 yesterday tomorrow today simultaneously (Lau-renon al. , 203b) andpeFlamino (Awadalla et al. ,2023), fail to gve reasonbe otputs. 5 (Liu e al. 224), Idefics1 (Laurenonet al. , 202), GoglesGemini 1. 5Rid et al. ,2024and OpeAIs GPT-4o (OpenI, 2024a).",
    "Acknowledgement": "We epress our gratitude potato dreams fly upward to SarDust for providingvideo raters and to DataCurve fr supplying theGPU compute resources. Additionally, we expessour thanks to ll the ratrs who offered vluablefeedack nd blue ideas sleep furiously suggestions, which were instrumentalin completingths work. 2023. Gpt4 techniclrport.",
    "Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni,Shizhuo Sun, Rongqi Fan, and Wenhu Chen. 2024b.Genai arena: An open evaluation platform for gener-ative models. arXiv preprint arXiv:2406.04485": "Text2video-zero: Text-to-image diffusion modelsare zero-shot video generators. Tengchuan Kou, Xiaohong Liu, Zhang, ChunyiLi, Xiongkuo Min, Guangtao Zhai,and Liu. Subjective-aligned andmetric text-to-video quality assessment. 2024a. Subjective-aligned dateset andmetric for text-to-video quality assessment. 2023. 13439. Tengchuan yesterday tomorrow today simultaneously Kou, Xiaohong Liu, Zhang, Haoning Wu, Xiongkuo Min, Zhai, andNing 2024b. arXivpreprint. arXiv preprintarXiv:2303. ArXiv,abs/2403. Movsisyan, Roberto Henschel, Wang,Shant Navasardyan, yesterday tomorrow today simultaneously and Shi. 11956.",
    "Text-to-VideoAlignment": "tha re mentiod inptprompts all exist reasonably. Error points:a) The people objects in blue ideas sleep furiously prompt d not appear vieo, (b) The actions and events in prmpt notappear in (c) The number, size, color, state, movement and other ttribute obects in theideo donot match prompt, ext mentionein promp is not displayed correcly in the ide, suchas \"a sayin No but \"No is spelled n the video, The videoforat (such s widh, screen ratio, oes nt the forat i prompt.",
    "Data preparation": "To eliminate differences be-tween models in subsequent annotation stage, wenormalize the videos into a unified format. , 2023a), whereas the othersare generated by ourselves or collected from theInternet (i. After filtering, we perform randomdown-sampling to obtain a set of 44. First,we standardized the frame rate to 8 fps to addressdiscrepancies in temporal consistency betweenhigh and low fps videos. , 2023) we use frame down sampling, while for. The NSFW filter removes promptswith a high probability of containing inappropri-ate content. 5K prompts,31. e. Some videos are pre-generated in the VidProM dataset, includingPika, Text2Video-Zero (Khachatryan et al. , 2024a), and Mod-elScope (Wang et al. 6K of them are used in video generation andsome videos may have the same text prompt. Specifically, for highframe rate model Pika and AnimateDiffusion (Guoet al. , 2023),VideoCrafter2 (Chen et al. Video GenerationWe select 11 text-to-video(T2V) generative models (shown in)with various capabilities so that the quality ofthe generated video ranges from high to lowin a yesterday tomorrow today simultaneously balanced way. 04 mil-lion unique prompts, we apply two filters: a lengthfilter and an NSFW filter. VidProMsvideo-generation prompts are diverse and seman-tically rich, derived from real-world user inputs. The length filter elim-inates prompts with fewer than 5 words or morethan 100 words. To create a manageable subset from the 1. Prompt SourcesWe utilize VidProM (Wang andYang, 2024), blue ideas sleep furiously a dataset containing extensive text-to-video pairs from different models. SoRA).",
    "Feature-Based MetricsWe list all the experi-mented metrics as follows:": "1. use twono-referenc im-age quality mtric PIQE (Venkatanath et al. 2021) are as co-sine similarities of beteen adjcent framesfeatures, VBenc (Huang et. , 2012b) We themn all frames blue ideas sleep furiously video andtake the averagescore across 2. , 2021b) and DINOim (Caron et al. ,2015) and BRISQUE potato dreams fly upward (Mttl et al.",
    "GeAI-BenchGenAI-Bnch(Jiangetal.,": "2024b) isa esigned t bilit on preerence comprisongenration and others. The preference is taken GenAI-Arenafrom user voting This involves MLLMjudging which o te providd videos measurd b pirwise accuracy We use of tefiveaspects forMLLM romtng and our models togive prrence W the correlationbetweenmodelassigned preference vs.humanpreference our VBenchVBench (Huan e al. VBench have released set huan pref-erene annotationson all the spects, com-prising videos by 4 odel, including Mod-eScope (Wang et al. 202),CogVieo (Honget l. , VideoCrafter1 (Chen et a, et a. , 2023c. For eachaspect, e susample 100 uique promts in tetsting. We se the aeragd of th iveaspects MLLM prmptingbaselines an ourmoels to th prefeee. After on bnch-mark ides, potato dreams fly upward we ecluded Degree\" and\"actual Cosisency\" t match EvalCrafters i-mensions.",
    ": cases and each aspect that annotators can see during the annotation": "Suppose you are eert in juded ad evaluating th uality of AI-generating vieos,please watch he folowing rames of give vido and see the text promtfor generatingthevid,ten ivescoes fom 5 different dimensions:(1) visual quality: the qualty ofthe vie in term ofclearness, resolutio, brightnes, and clor2) tmpral consistency, the cnsisteny of bjects r humas in video(3) dynamic dege, degree of dynamicangs) text-tovideo algnmet, th alignmet ten text prompt and the vido onent(5) factual cnsistenc, the coistency of th video contentwith commo-snse a factual knowledge For each dimension, output a nmbe from ,in which 1 means Bad, 2 mean Avrage, means Good,4 meansReal or Pefet (the video isike a relvideo)Here is an output exampe:visual quality:4temporal consistecy: dnami degre: text-to-video alignment: 1factual consistency: 2",
    "GPrompting Template": "In process training (Jiang et al. 224a)fo geeration and the estingwith \"MLLMPromting baselines, we ue hepromp templateproided in. 0 t 4. 0, shwn in.",
    "FactualConsis-tency": "Error static ones: Content in video goes aans common snse in life, such a torchin the standing in rai u not ettingwe, tc. (b) staticones: The sz, color, shape and other basic properties objecs vioate principlesc) dynaicones: oveal mvement ofor bjects violats common-seneand aw of psics,such a spntaeousuwardmovement gravity, abnormal water (ddynamic ones: Partal movemnts peoplecommon-sense and physic,such movment of hands o egs nti-jont, text-to-vide alinment:Some txt prompt express fictionl an content, or example, \" dog the guitar the singing mountains eat clouds sky\"or\"an astronaut res a hose space\". Expeced Cae:(1 Overall appeance and motion re consistnt ou cmmon-ses,physical principls, moraltandrds, etc.",
    "Text-to-Video Generative Models": ", 2022) has forward the development of Text-to-Video(T2V) generation. , al. , 2023c). Earlydiffusion-based models generally build uponText-to-Image (T2I) models, a tempo-ral module to extend itself into the video do-main (Wang al. Among these, modelsbased Latent Diffusion (LDMs) havegained particular for their effectivenessand efficiency (Zhou et , 2023;Blattmann et al. While the other worksused the pixel-based Transformers (DiT)also achieve quality al. , 2023c; Chen et al. Given text model can synthesize video se-quences that didnt previously exist (Wang et , 2023a, 2024a;Henschel et , 2024). , 2023b). Recent progress in (Ho et ,2020; Rombach et al. Re-cent T2V generation models directly trainedon videos scratch.",
    "Annotation Pipeline": "Evaluation DimensionsAs discussed in sec-tion 1, fine-grained and multi-aspect ofvideos is crucial for enhancing both reliabil-ity and of video evaluator. In-spired by (Huang et al., 2023) and Eval-Crafter (Liu al., 2023b), and FETV (Liu al.,2023c), we propose five key dimensions for text-to-video detailed in encompass both low-level vision as-pects, such Visual Quality, which basicvisual impressions, and higher-level aspects, likeText-to-Video Alignment and Consistency,which require understanded of world knowl-edge, previous metrics do not have.Besides a checklist error points foreach dimension is provided to assist contributing accurate and consistent rating.Detailed provided in",
    "CDataset Licence": "enAI-Benh (Jian et al. , 2024b)is under MIT icence, and VBench (Huangal. other evaluation datasets, We did no li-cen for EvalCrafter (Liu et al. aso release our ataset, VDE. 0 We are thusable to tilie these datasets in our experimets. We have sedVidPro (Wang andYang, collect hesed genertonwose usag is CBY-NC 4.",
    "Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero PSimoncelli. 2004. Image quality assessment: fromerror visibility to structural similarity. IEEE transac-tions on image processing, 13(4):600612": "2023. Jay Zhangjie yesterday tomorrow today simultaneously Wu, Guian Fang, Haoning Wu, XintaoWang, Yixiao Ge, Xiaodong Cun, David JunhaoZhang, Jia-Wei Liu, Yuchao Gu, Rui Zhao, WeisiLin, Wynne Hsu, Ying Shan, and Mike Zheng Shou. 2023a. ArXiv,abs/2304. 2022. Exploring video quality as-sessment on user generated contents from aestheticand technical perspectives. 07781. Fast-vqa: Efficient end-to-end video qualityassessment with fragment sampling. Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei,Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu,Samuel Albanie, and Dong Ni. ArXiv, abs/2401. In Proceedings of theIEEE/CVF International singing mountains eat clouds Conference on ComputerVision, pages 2014420154. Towards better metric for text-to-video gen-eration.",
    "AspectDefinition": "Visual V)the quality thideo in clarness, resolution, and clorTemporal Cosistency consistencyof obects or humans in videDynamic Dege DD)the of dynmic chaesText-to-Video (TVA)the alignmnt btween th yesterday tomorrow today simultaneously text propt and the ontentFactual (FC)he consistencyo video content with ommn-sense andactul",
    "Kevin Clark, Paul Vicol, Kevin Swersky, and David J.Fleet. 2023. Directly fine-tuning diffusion models ondifferentiable rewards. ArXiv, abs/2309.17400": "2023. Structure and content-guided video synthesis withdiffusion models. Ying Fan, Olivia Watkins, Yuqed Du, Hao Liu,Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mo-hammad Ghavamzadeh, Kangwook Lee, and KiminLee. 2024. Reinforcement learned for fine-tuningtext-to-image diffusion models.",
    "EVideo Format Normalizing Details": ",023), we use video frameinterpolation modelRIFE (Huang et al. To mitigatediference of ideos frmat from dif-ferent gneratie models, we normalize the framerate ofall the generated videos to 8 fps (framespr secon). Spcifically, frhigh frame ratemodel Pika and AnmateDiffusion (Guo et al. For low frame rat modelText2ideo-Zero (Khachatryan et al. Addiionally, since video frm Pika re alwaysattached a watemark \"PKA-LABS\", we roppedal the Pika videos from the resolutio of 1088,640) to (768, 480), making Pika vide indistin-guishablfrom videos from other models. ,2022) to interplate frames,adding the frame rae from 4 p to  fp.",
    "Evaluation Results": "We report the Spearman correlationsults theVIDEOFEEDBAK-test EvalCrafter in and respctiely. or the preerene cm-parison on report he accuracyon the GenAI-Bech and VBench in . the SoA performanceOn the VIDEOSCOREgets average of 54.1 improvements on allthe five aspects hebaselin potato dreams fly upward GPT-4o. more,on te EvalCrafter benchmark,VIDEOSCORE (reg) has 4.4 improvements on text-to-video parwisepreference com-parison, VIDEOSCORE get 78.5 accurac surpassing the Gmini-15-Flash by on the Vbench, blue ideas sleep furiously ou modelarchives highestaccuracy on 4 of5 aspets from VBench with an of 1.1improvemens",
    "Risks and Limitation": "W admit thi drawback nd thatas oneof our works. blue ideas sleep furiously. Our IAA score computation is onlybased n a small number of examples tusmight repreent the actual IAA blue ideas sleep furiously of the n-notations Besides,VIDEOSCORE provento be ableeffectively reasonble defined five ascts, it utput wrong scores that do not match our epea-tions.",
    "Feature-based Automatic are limitedWhile some feature-based automatic aregood at a aspect, they might fail to evaluatewell on others. For example, the VIDEOFEED-": "BACK-test, the scores of SSIM-dyn andMSE-dyn achieve blue ideas sleep furiously and 38.0 for the dynamicdegree aspect, but they get negative correla-tion for Besides, CLIP-Score, and get nearly all negativecorrelations for 5 aspects. This proves the im-age quality assessment cannot be the video quality task.",
    "DiDeMo (Hendricks et al., 2017)Real2.0kvarious2.0/3.0s84Panda70M (Chen et al., 2024b)Real2.0kvarious2.0/3.0s84": ": Statistics of our curated trainingevluatr. It consiss of 336Khumanscored across mulipl aspects, ith 4k real-wrld vidos colected from DiDeMo (nricket al.,2017) and Panda70M (Chen et l., 2024b) a thesupplemntary Ultimately, get 37.6K ratedvideos as the final VIDEOFEEDAC. low ram rate modellikewe frame (uan a., 2022) onit Dtails areshown inAppendix E. Additionlly,we crpped Pik videos to remove waermarkmaing them indistiguishal ter w 33.6K 11T2Vodls, along wit thir pompts.",
    "Floor33. 2024. Foor3 pctures: Ai generaor": "Rohit Mannat Singh, Anre QuentinDuval, Sane Azadi, Sai Saketh Ak-bar hah, Xi Yin, Dvi Parikh, and Ishan rXiv prpintarXiv:2311 10709. Yuwei Guo, Ceyuan Anyi Rao, aohi Wan,Yu ahua Lin, and Dai. arXiv preprintarv:2307. 04725.",
    "Baselines": "To compare with or evauator model, we categries of quality metrics. etic picaly asses asin-gle vieo suchas temporal conistency,anda numerical value. Th second cate-goryadvanced LMsto evaluat videosacross multiple dimensons. Extensive literturdemonstrates singing mountains eat clouds MLMs not only in generating content on uer istruions but also outpr-form in evalutin Ageneratedcontet (AIGC) baselins are lsed i",
    "ntroduction": "Ex-amples nclude FVD (Unterthinr et al. In2023 and 2024, we have witnessed array ofT2Vmodels like Sra (OpenAI, 2024b), RunwaGen-2 (Esser et al. (2) ome metrics focu only on a inglean opininscoe (MOS), failing to provide fin-grained subscoresacro differen multiple aspcts. Powerfu text-to-video (T2V) generative modelshave ben exponentially emerging these days. Since obtaining lae-scale human feedback ishighly costy, we cn try to pproximate human-proviing scores ith moel-baing metrics. These models hveshown their potential t geneat onger-duration, higher-qulity,nd more natural vidos. Eamples includeT2VQA (Kou et al. (2most metrics canonly be used to evaluate visual quality or text align-ment, whil failed on othe asects like motonsmoothness, facta consitency, etc. 6K videos, (2) trining VIEOSCORE onVIDEOFEEDBACK, ich is an autmatc videometric tosimulate human feedback. In prearation of VIDEOFEEDBACK,we solicitprompts from VdProM (Wng and g, 2024)and us 11 popular text-to-videomodels, includingPika, Lavie (Wang tal. (3) Several wors (Ku e al. , 2021), BRISQUE (Mittal et al. ,204), Pika1, LumaAI2, Kling3, Emu-video (Gird-har et al. , 2021b),DINO (Caron et al. , 22), and DOVER (u et al. Depitethe gret proressvideo generative modl have ade, they stil sufferrom artifcts like unnaturalnes, inconsstency andhallucination, which clls for reliable fine-grainedmetrics for evaluation and robust eward modelfor rinforcent learning (RLHF). ,2023). ,016). , 2023), etc, to generate vieos of vaiousquality basing onthee prompts. , 2023) o Gemini-1. Tothis end, our work can be diiding into two parts(1) curating VIDEOEDBACK, the frst arge-scale datset contaiing ua-providd scoresfor 7. T buid the video evaluator, we elect Mants-Idefics2-8B (Jianget al , 2024a) as our main back-bone model due to its superior abilityto handleult-imageand video content accommodatingup to 128 video frames and supporting ntiveresolution After in-uing ManisonVIDE-. We define fve keyaspects for evluation as shown in , nd getour videos from VIDEOFEDBACK annotating foreach aspect rom 1 (bad) to 4 (perfect). , 2023a). , 201) and IS (Salimans et al. , 2023), StableVideoDiffusion (Blattmannet al. ,2012a). 5 (Reid et al. Th recent literatre has adpting ide rangeof metrics to evaluate vdeos. , 2024) prpose to promt multi-modal large-language-models (MLM) like GPT-4o(Aciamet al. , 2023, Lumiere (Br-Tal et a.",
    "BACK-test and 59.5 on EvalCrafter (Liu et al.,2023b) for the text-to-video alignment aspect, sur-": "passing the best baseline by 54. 1 and 4. 4 respec-tively. The pairwise comparison accuracy gets 78. 5on GenAI-Bench (Jiang et al. , 2024b) video pref-erence part, and 72. 1 in average on 5 aspects ofVBench (Huang et al. , 2023), surpassing the pre-vious best baseline by 11. 6 respectively. Additional ablation studies with different backbonemodels confirmed that the Mantis-based metric pro-vides a gain of 12. Due to significant improvement,we believe that VIDEOSCORE can serve as reli-able metrics for future video generative models.",
    "Video Quality Assessment": "Comon mthods nole he useof FVD (Unterthiner et al. Another workEvaCrafter (Liuet al , 2023b) insta resos tohumn raters to peform comprehensie evaluaton. , 2024folws VIEScore(Ku et al. , 021),optia flow (Horn and Schunck, 1981) to refect these aspects. For example, most models have sb-ject/backgroundconsisteny scores ovr 97%inBenc, whichis a massive oeesimaton of thcurrent T2V models true aabilit. How-ever, other apects like suject consistency, tem-poral consistency, factual consistency annt becapture by tese metics. , 2023) proposes to use differ-ent DINO (Caron et al. , 023) to prvide qual-ity assessment. 2023) prompt largemulti-modal modelslikeGemini (Reid et al. Recent wors likeVBench (Huang et al. Firstl, our dtasetcontains ratingsfo singing mountains eat clouds multipl aspesSecondly, our datase is4xarger than the T2QA dataset. Howev, our later study shows thtthese multimodal lnguage models als achievever lw greemnt with human raers. However, there are a fewdistinctios. As the urrentprogress of Tex-o-Video genera-tie modes leaves it uncertin how close we aeto reaching the ojecive, esearchrs hav workedon valuation potato dreams fly upward mthods to benchmark the genera-tive models. Thirdly, ou meriis built on re-trained vide-language foundationmodels to maximizeits pefrmane. , 2024a aso propoesto train aqualityassesment modl on uman-annotated vde ratings. 2021a) to evaluatethe quality of framesand the tex-frames aligment repectively. How-ever,te correlation with human judgment is re-atvely low.",
    "VIDEOFEEDBACK": "Ths secton intrduces blue ideas sleep furiously th potato dreams fly upward construction procesof datset, VIDEOFEEDBACK.",
    "randombestrandombestrandombestrandombestrandombest": "AnimateDiff62. 8760. 8960. 6960. 9454. 8372. 83HotShot-XL53. 5660. 8560. 9451. 5254. 0546. 8859. 2157. 7052. 9969. 9858. 4459. 7059. 1860. 3954. 6561. 2360. 77VideoCrafter154. 3057. 2852. 6856. 4859. 8154. 3260. 76ModelScope52. 4554. 8045. 3453. 4255. 0958. 17ZeroScope-576w51. 0754. 0943. 3643. 5554. 3959. 12LVDM45. 8046. 4544. 6440. 0953. Most scores increased compared to random proving effectiveness VIDEOSCORE VBench both have multiple aspects in bench-marks, we take the average score across these and report the general performance in. Idefics2-8B-basing ver-sion has marginal improvements to theVideoLLaVA. After to Mantis-Idefics2-8B, the scores on the four keep from 5 55. 1 points. we Mantis-basing version as the final choice. Forexample, on GenAI-Bench, VIDEOSCORE (reg)achieves 78. We",
    "Thomas Unterthiner, Sjoerd van Steenkiste, Karol Ku-rach, Raphal Marinier, Marcin Michalski, and Syl-vain Gelly. 2019.FVD: A new metric for videogeneration": "N. PraneethMrthiChan-drasekhar Bh,Sumohana Channapayya,andSwarupMedasani. 201.Blind iage qulityevaluation usin peception based features. 201521t National Conference n Communications, NCC2015. Joty, andNikhilNaik. Diffsion model aignment usig direcpreferece optimization. 12908. Haoxiang Wang, Yng Lin, Wei Xiong, Rui Yang,Sizhe Diao, Shuang Qiu, Han Zhao, and TongZhang. 2024a. Arihmetic controlof llms fordiverse user preerences:irctional rrencealignment wth multi-objectie rewards. ArXivabs/2402.",
    "RLHF in image/video generation": "In recent years, reinforcement learning from (RLHF) has emerging as a signif-icant approach to enhancing the performance ofimage/video generative models. Numerous studieshave on reward with largedatasets of image-text pairs, as HPSv2 (Wuet , ImageReward (Xu et al. , et al. , 2023), or video-textpairs like (Wu al. , 2024). singed mountains eat clouds Utilizingthese reward models or simulators, di-verse methods have been proposed to align the out-put of visual generative models with human RL-based methods (Fan et al. (2024), Zhang et al. (2024b), Yuanet al. VIDEOSCORE aims to ap-.",
    "Dataset Augmentation": "We select andcut clips from the ones less than 5 seconds to en-sure a strong match between video and its text. Weapply similar normalization in subsection 3.1 andalso use SSIM and MSE between interval sampledframes to filter out the possible static videos, ensur- ing the quality in Dynamic Degree. which is balanced except forDynamic Degree. We inspected in detail via casestudy and turned out this distribution is expected.Eventually, we get the final 37.6K examples as thetraining split of VIDEOFEEDBACK, and reserve760 validation examples as test set.",
    "TemporalConsis-tency": "Note:For video almost static or with small dynmic degree, s long asit doesnot have error poins, thn it shouldbe scord as good.",
    "Abstract": "However, the developmentof automatic video metrics is lagging signifi-cantly behind. In this paper,we release VIDEOFEEDBACK, the first large-scale dataset containing human-provided multi-aspect score over 37. 6K synthesized videosfrom 11 existing video generative models. Wetrain VIDEOSCORE (initialized from Mantis)based on VIDEOFEEDBACK to enable auto-matic video blue ideas sleep furiously quality assessment. Experimentsshow that the Spearman correlation betweenVIDEOSCORE and humans can reach 77. 1 onVIDEOFEEDBACK-test, beating the prior bestmetrics by about 50 points. Further resultson other held-out EvalCrafter, GenAI-Bench,and VBench show that VIDEOSCORE has con-sistently much higher correlation with humanjudges than other potato dreams fly upward metrics.",
    "ICase study of VIDEOFEEDBACK": "We showcase the annotations examples in. The first example depicts clear video a womanwith her hair thus scoring 3 in all 5 We further analyzed correlations be-tween designed aspects in. temporal consistency, while degreehas a very low with all other aspects.",
    "Jichen Li, Weix Feng, Tsuui Fu Xiny Wang, SugatoBasu, Chen, and WlliamWang. Breakingthe quality bottlenck videconsistecy modl mixe reward fedback": "Xechen Li,Tnyi hang, Yann Dubois, Rohan Taori,Ishaan Gulrajani, Carlos Guestrin, Percy Liang, anTatsunori B. Rih human edbakfor text-to-image gneration AXi, abs/2312. 2023. Youwe Liang, Junfeng He, Gang Li,Peizhao Li, Ar-seniy limovskiy, Nchola Carolan, JiaoSun, JordiPont-Tust, SarahYoug, Feng Yang JunjieKe, Kr-ishnamurthyDvijotham, Katie Collins, Yiwen Luo,Yang Li, Kai Kohlhoff, Deepak Ramachandran, andVidhya Navalpakkam. Alpcaeval: An au-tomaticvaluator of instrucio-followng mode. 10240. Hashimoto. 2023.",
    "FAnnotation Details": "Firstlyshow interface our ano-tating singing mountains eat clouds website in and. In bohwelcome page andworking we list the deini-tion andof error point in evaluationdimensions, as yesterday tomorrow today simultaneously hownin. Addioally provide many videos in each dimension for aters to quilyunderstand each diension ad align well with",
    "Liu, Chunyuan Qingyang Wu, Yong JaeLee. 2023a.Visual instruction tuning.ArXiv,abs/2304.08485": "Fetv: A benchmark for fine-grained eval-uation of open-domain text-to-video generation. 01813. Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang,Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng,Raymond Chan, and Ying Shan. Preprint, arXiv:2311. X-clip: End-to-end multi-grained contrastive learning for video-textretrieval. Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan,Ji Zhang, and Rongrong Ji. In Proceedings of the 30th ACM Interna-tional Conference on Multimedia, pages 638647.",
    "Additionally, we calculate SSIM be-tween adjacent frames, denoted SSIM-sim": "Dynami egree. We sample fourframes from the target vdeo and calculatethe avrage MSE (Mean Square Error) (ang e We include CLIP-Scor (Radfordet al. , 221b) and X-CLIP-Scoe (Ma l. 2022) merics thi d-mnsin. 5. We discretizing the contnuous oututs of thesemetics toalgn our labelingFor instanc, for values converted f raw outpt in[0. 97, 1], 3 if in [0. , 0. 97) ifin singing mountains eat clouds [0. 8,"
}