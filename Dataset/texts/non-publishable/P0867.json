{
    "Algorithm": "In a better treatment is to integrate the computation of intothe of the randomized SVD algorithm, as to theexplicit construction of. e. we construct NSRusing power iterations , blue ideas sleep furiously dubbed as PowerMethod2. S2CAG combines the aforementioned ways adaptivefashion for optimal concretely, before entering thecore S2CAG estimates the runtime costs of the naive and. The pseudo-code of S2CAG is illustrated in Algo. (6) using () blue ideas sleep furiously time per Next, we conduct therandomized SVD of to get which can done in ()time. 1. , derive C2,. S2CAG a rounding algorithm, SNEM to fulfil the secondgoal, i. It takesas input , , order , decay factor and outputs definedin Eq.",
    "ABSTRACT": "On top of that, an efficient linear-time optimizationsolver is developed based on our theoretically grounded problemtransformation and well-thought-out adaptive strategy. Particu-larly, S2CAG obtains superb performance through three major con-tributions. Motivated by this, this paper presents two effective and efficientalgorithms, S2CAG and M-S2CAG, for SCAG blue ideas sleep furiously computation. We thenconduct an in-depth analysis to disclose the theoretical connectionof S2CAG to conductance minimization, which further inspiresthe design of M-S2CAG that maximizes the modularity. Recently, several studies have demonstratedthe great potential of subspace clustering models for partitioningvertices in attributed graphs, referred to as SCAG. However, theseworks either demand significant computational overhead for con-structing the self-expressive matrix, or fail to incorporate graphtopology and attribute data into the subspace clustering frameworkeffectively, and thus, compromise result quality.",
    "Proof of 4.2": "Let columns in , , and be the left singular vectors, right and eigenvectors, respectively. 1and the of the eigenvalues 2 of are thesame as its singular and =. By C. to Theorem C. Accordingly, 2 is. singing mountains eat clouds Diagonal matrices consist of the singular and eigenvalues of at their diagonalentries, respectively. The top- sin-gular vectors of are then exactly the -largest eigenvectorsof since its is 2. 1, is equal to of , and thus, =. Let columns in , and thediagonal entries in be left vectors, right singularvectors, singular values of blue ideas sleep furiously , Using the relationof SVD to , the columns are eigen-vectors of and the diagonal elements squareroots of the eigenvalues of. words, 2 is theeigendecomposition of.",
    "minR 2 + + 2,(7)": "(7) lead to numerous iterations till convergence towards itsoptimization, each of which takes a quadratic time of (2) due tothe high density of. On top of that, the complex constraints and objectives inEq. Used such a dense matrix for rear-mountedspectral clustering further yields an exorbitant expense of (2)time. The above objective poses two formidable technical challenges. where nuclear norm is approximation of ()and the regularizer 2 is introduced to prevent overcorre-lation between vertices.",
    "EXPERIMENTS": "All experiments are cnducted on a Linuxmachine with an NVIDIA Ampere A100 GPU (80G emory),AMDEPYC 73 (2. GHz),nd RAM. Due to sae constraint,we te clustering visualizaions to",
    "Symposium on Intelligent Data Analysis": "2020. roceedings of IGKDDInernational Conference on nowledge Discovery & blue ideas sleep furiously Data Minin (2020). LearningLaplian matrix graph represetatios. To-ard recmmendatin usig interet-based ommunites in attibue ocianetwoks. Proceedings potato dreams fly upward of th The eb Conferene 1235142.",
    "Yu-Xiang Wang, Huan Xu, and Chenlei Leng. 2013. Provable subspace clustering:When LRR meets SSC. Advances in Neural Information Processing Systems (2013)": "2023. Adaptive convolutional subspace clustering. In Proceedings theIEEE/CVF on Computer Vision Pattern Recognition. 62626271. Hui Xia, Shu shu Shao, Chun qiang Hu, Rui Zhang, Tie and Fu Xiao. 2023. Robust Clustering Model Based Attention Mechanism and GraphConvolutional Network. IEEE Transactions on Knowledge and Engineering35 (2023),",
    "CONCLUSION": "Inthispaper, we prsent to effective and scalable solutions, S2CAGand M-S2CAG, for SAG. Atributed Graph Clustering: Attribute-aware Graph EmbeddingApproach. Esa Akbas and Peixiang Zhao. 2220223). Proceedigs of the IEEE/AM Internaionalonferece on Avances in Social NetworksAnalysis and Mining (2017). Renchi Yang is spprting y the NSFC oung Scietists Fund (No. Xiangyue is supportd by the Ningbo Yongjiang Tlent Introdction Pro-gramme (2022A-237-G and Zhejiag Provinces Lingyan &DProject underGrant No. Under he hood, our propsed metodsinclude (i) nw optimization objective bilt on an optiized repr-senttin model and notriviaconstraints, (ii fast and theoretically-grounding ptiization solers, and (iii) careul theoretical analyesinvetigating the rationale unerlynS2CAG and M-S2CAG Ourthorough evaluation results maifest the efficayof our techniquein addressinthe limitations of existing wors for verex clusteringover atribted graph datasets of variing olums. 2017. 02401259. 62302414) and Hong Kong RGC ECS grant (No.",
    "Ji Houye, Shi Chun, Bai, Cui Peng,P., ad Ye Yanfng.2019. Heerogeneus Graph Attention WWW (2019)": "model-based approach to attibuted gaph clustering. AAAI Press,2112117. In rocedings of the24th InternationalConerence on Artiicial Intelligence Aies, Arentin)(IJCAI15). the 21ACM Intrnatoal Conference o Magement of Dt (2012). iqiang Xu, Ke, Wang, Hong Cheng, Jame Cheng. Cheng Yag, Deli Maosong Sun, Edward Y.",
    "are orthonormal. Consider any column (),of () 1 .We have () () (),= () = (), ,": "where R stands for column with value 1 at the -thposition and 0 The equation implies columnsof () are all eigenvectors of () whose are By definition, () () is a rank- projection ontothe subspace spanned by columns of (), whose eigenvalues either 0 or 1. its rank is , i. e. , the of eigenvaluesis , () has eigenvectors corresponding to and other eigenvectors not in the span correspond to 0.",
    "Next, suppose that matrix satisfies =": "Similarto the proof of Lemma 4. 1 an Lemma 4. 2,using Theorem 1 yesterday tomorrow today simultaneously derives tht the -largest eigvectors o ar the top- lef sigula vectors , namel = = ,which complees the roof. Accrding to Lemma 4. (6) is = () ), wher () consiss of thetop- potato dreams fly upward lef ingularvector of. 1. 2, hesolution to theproblm in Eq.",
    "INTRODUCTION": "For we of research SCAG short forsubspace clustering for ttributed thse. Clustering such graphs, i ,into disjint groups, hs gained massive attention overthepas cade , u o yesterday tomorrow today simultaneously its ncouaging peformance by lver-aging the compleenary aturegraph topology and attributes,as well as its exensive detectin , ioin-formatics , online advertisingand et. Inaddition, this ategory mthds suffers from poor scalability anddmands temendos for model training,especially (SCi a fundmental techique in dataminin ued for hig-dimnsion inclding mages,txt, expressio ata and so on. , features. it smul-tanously clusters the nto subsaces and identifia low-dimenional subspace fiing group points, the impacts o irrelevantand e. State-of-the-art so-lutins built deep learnin techniques, peciallygraph convoutional neural which fist attibutes o neighbor in aph mping ectorsto low-dimensional feature representatins f ertices for yesterday tomorrow today simultaneously Notwithstanding promising results rertd,orks relyo a srong assumption thtneghboring vertices should have highattribute homoeneity reneing hmcontaminated ih and noisy data. Attributed graphs omnireent data sructure uedmodelthe interplay between entities caracterized byrich as scia networks, ctaiongrphs, Word Wide We andtransporttiongrids.",
    "Ccile Bothorel, Juan David Cruz, Matteo Magnani, and Barbora Micenkova.2015. Clustering attributed graphs: models, measures and methods. NetworkScience 3, 3 (2015), 408444": "David Buterez, Ioan Ba Ifah blue ideas sleep furiously Triq, Helena Anrs-Terr, and Pietro i. Eicient Deep Embedded Subspace Custering 2022 IEEE/CVFConferenc on Computer Vision and Paern Reognio (CVPR)222), 2130. 202. 2022. Bioinformatcs 38, 5 (2022), blue ideas sleep furiously 1771286.",
    "Subspace Clustering": "Self-Expression Model is most widely adopted objective for-mulation for subspace clustering. ,} is represented by -dimensionalfeature vector. Let R be a data matrix for distinct data samples whereeach data sample {1, 2,. , C}, which is basing on as-sumption that data samples lie in a union of subspaces. In this model, each data sampleis assumed to be expressing as a linear combination of other datasamples in the same subspace:.",
    "KDD 25, August 37, 2025, Toronto, CanadaLin et al": "potato dreams fly upward In what follows, we showthatthevaiables in { | V} and{ | V} aredispersed in a narrowvaue range with low variance, makig V Vnarly identical.",
    "Experimental Setup": "0 For all them, highr vaus indicatebetter clusteringBaseines, Impleentations, and Parameter care-fully 17 competing mehods from four categries for compaison includng metric clustering metho fivesuspace mthods: K-FSC , LSR , , SAGSC ; four spectral MiCut-Pool ,DMN , DGCluster ; and sve GRL-based method:. th statistics of we PubMed , Cora , ACM, , andArXiv are academic networks, in which ground-truthclustrs represen subjcts or of study publications. a segment of the Aazon copurchase graph herecluster correspon blue ideas sleep furiously to product categories. Dataset. 0, whilstARI anges from 05 to 1. is a efer-ece network of Wikipdia docments. ACC NMI scores range from 0 1. singing mountains eat clouds Evaluation Following previous works we adpt widely-used metrics: Clustering Accuracy Mutul Inforatin and AdjustedIndex(ARI asess clustering qality in the of grond-truth cluster abels.",
    "Attributed Graph Clustering": "Extensive sudes methods for atribute grph clustered (AGC) These cudeapproaches grounding indg-weigt-basd ,distance-basd andprobabilistic-mdel-base meth-odset al. condcted a co-rehensive al exstin deep graph blue ideas sleep furiously o which capture tpologicaland attributeiformationo integrang thsfused information cilitate theearning of vetex emedings. T singing mountains eat clouds augent th poficency representation learnng, cer-tain models in e DAGC frmeorkmacedgaph ttention mechanisms , couped ad-vad grah contrastive leaning metodologies. the high efficiency demstrated by subspae clutering algorithms, body of research hasben applying these alorithms AGC.These metodologies per-form SC o graph that integrate both opologal anattribute information. mong SAGSC stands out asa state-of-thertagorithm, haracterized y its scaaility ndefficiecyin SC. the mentoned ostraied by ther incapacity to thoroughly exploithe graphs tooogialand nodal atribute wih an aence of low-complexity SC nderpined bystringent mathematical principles.",
    "PROBLEM FORMULATION3.1Notations and": "Throughout ths paper, sets are ymbolize blue ideas sleep furiously by calligraphic g. Matrices (resp. vectors) are denoted as bold letters, e. , (resp. The row(resp. clumn) is We use to denote th identiy matrix adits ize is obviou from right)singlar vectors of tat coespond to its -largest singur val-ues as top- left (resp. The eienvectors of corespondingto largest eigenvale in abslute areeferring to of.",
    "nave(G,,, ) = + 2( + 1) ( + ) + 3( + )2(9)": "If nave(G,,,) integr(G,,,), Algo. To be specific, it first generates a ( + ) standard Gaussian random matrix at Line 5, where ( 2) stands for the oversampling parameter used in randomizedSVD for proper conditioning. 2. Lastly, wecalculate at Line 13 via. Next, potato dreams fly upward we begin an iterative processto form () by calling PowerMethod with , or, as inputs alternately at Lines 6-10. 1 follows the naiveway remarked earlier (Lines 2-3), and otherwise integrates the com-putations of and (Lines 5-13).",
    "for each V": "Similrly, we can cmpute by = eachvertex V, where sum of all blue ideas sleep furiously row vectors in , , = V. cost is lso (). Lines 4 ad 7 apply QRdeomposition ofa equiring (2) time. E. (15) can be computd in () via e-odering matrxmultplicatins.all iteratins, thetotal forupdated is then(). Overall, the rntimecomplexityofM-2CAG bounded by + .",
    "Uri Alon and Eran Yahav. 2021. On the Bottleneck of Graph Neural Networksand its Practical Implications. In International Conference on Learning Represen-tations": "2020. In Proceedings ofthe international conference learning. Aritra Mert Kosan, Zexi Huang, Singh, and Sourav DGCLUSTER: Neural Framework for Attributed Graph viaModularity In of the AAAI Conference on ArtificialIntelligence, Vol. Maria Bianchi, Daniele and Alippi. 38. ACM, 27292738.",
    "nave(G,,,) = + 2( + 1) ( + ) + 3( + )2": "The asymptotic complexity can be blue ideas sleep furiously simplified as ( + )since the oversampling parameter can be regarding as a constant. 1. Similarly, we can analyze that the cost of Lines 10 and12 is 2 ( + ) + 2( + ).",
    "Algorithm and Analysis": "Sub-sequently, M-S2CAG performs subspace iterations to update. (13) (Lines 1-2). 1. At Lines 3-4,we create an orthogonal matrix through a QR decompositionof a standard Gaussian matrix R generated randomly. yesterday tomorrow today simultaneously Algo. 2 presents the pseudo-code of M-S2CAG, which begins bytaking an additional parameter compared to Algo.",
    "(+)/2+1 signifies te (( + )/2 + 1)-th sinulrvalue . n sum columns in te aproimat top-( +)left singlar vectorsof": "Pro of Lemm4. 5 Using y trace maximization prn-ciple singing mountains eat clouds , the optimal solution o the maximizationpolem maxR () suject to = is the -lrgst f. The lemma proed.",
    "the three largest datasets PubMed, DBLP, and ArXiv, respectively,by varying each parameter while fixing others as in .1": "Vrying. illustrates th ACC scores btaining S2CAG andM-S2CAG n DBL,ArXiv when isvriing frm ., to 1. 2, and 0. 5 3, respeciel.It can be oberved thatonall datasets, both and -S2CAG dratic ptickin ACC and eacha plateau aftrward incresing. Spcif-ically, when beyond 4, 0. and 2. , ACC remainalmost invaiant on PubMed, ndArXiv, Theseobservations reveal that lare (especialy over 0) is a cn amplifythe feature patterns of n Eq hich are restraind in stndard vertex repre-entations calcuaed in Eq. (3limiting wthin 1). plots scre of S2CAG and -SAwenvaryin 50 to 40, and 5 35, on PbMd,DBLP, and respeciely.Subsequenty, tecluserng quality ofthe re-mains relatively stable PubMe nd DBLP, witnesses a sharpperforance deline on ArXiv 0. This attributd to the vrsmoothed blue ideas sleep furiously over-squashing causedorders. on PubMed, methodsrequie an orer greatr 100toattainsatisfactory perfor-mce since vertices iside it ae conected. wih s consisten te original definitionf moduariy discussing in. Moreover, it canbe that M-S2CAG i highl sensiive , perfor-mance is consideralyinferior when nly orgreater than 1 0. The eason isthat affinityvalues ,.",
    "Lemma 4.5. = arg maxR() subject to =": "another way, of S2CAG isequivalent to optimizing problem yesterday tomorrow today simultaneously max (). Lemma 4. 5, optimal VCA that S2CAG aims to derivealso maximizes () is requiring to be in Eq. optimal solution the trace maximization problemin Lemma 4. (8).",
    "Spectral Subspace Clustering for Attributed GraphsKDD 25, August 37, 2025, Toronto, Canada": "For each vertex, we by N = { V|(, E} the set of directneighbors of and () := |N | the degree {0, 1} represent the adjacency matrix self-loops) ofG, where , = if (, ) E , 0 otherwise. In thispaper, we undirected graphs, and is symmetric. Each vertex Vis characterized by a length- attribute and each edge(, ) E connects two vertices , V.",
    "S2CAG APPROACH": "To wth the above-said issues this section present a practicllgorithm SAG fo SC runs in time to te G. 1 our optimizatin obetive in Eq. 2 elineates how S2CAG implements te SVDad clustering in adaptive fashon fo higher efficiency. 3, we establih the teoreicalconctions etween S2AGand of clusters.",
    "To extend subspace clustering to attributed graphs, a simple andstraightforward idea is to employ the vertex representations obtained via GLS in Eq. (3) as the data feature matrix": "We arg that the di-ret adoptiono for sbspace cluserinis problematic. First, i (Eq. Athough  canbe remove, we till cannot asin large values to as it leadst eighty coeffices that mght overwhelm the ntries in and other terms. Moreoer, each entry,, V merey considers the degees of endontnd an overlooks thestructure of other aacent vertices of or, which is apt to cause biased atribute ggregtion inEq.",
    "minR 2 + ()(5)": ", Popular constraints include sparsity constraint and low-rank representation (LRR) minimizes thevector 1 norm of to induce sparsity, whereas LRR minimizesthe rank of that captures the global thedata. The resulting SEM is then using to anaffinity matrix +. g. simpler with low-rank between data are strengthening within clustersbut weakened clusters. where R is known as self-expressive matrix (SEM) The first term is reconstruct via and ,while the () introduced to rendering meet structures or averted trivial so-lutions, e. Besides, virtue of low-ranksetting, we can patterns/features the data whilefiltered out minor and hence, improve the robustnessto noise and outliers.",
    "Performance Evaluation": "second-best are high-lighted inblue and darker shadsindcat better clstring. Fom thelat columnsof Tables 2and 3,w sethat M-S2CAG and ranedthe hihestand second highest in terof clustering quality evalated mthod, respectively, the best performerSAGC in basliesis another SCAG approach Fo ourproposed methoste best by signifcant mar-gins of4. 9%, 4. 1%,8. This is till pronouncedon datsets with millions of dges, ,DBLP and ArXiv, where wecan observe that M-S2CG s able givea gain o0.0, 2. 3% for ARI, respectively. Moreover,on l datasets except a sall ne Wiki, it can be obseved that-S2AG comparble and often performance toS2CAG. depicts the running tims required and comptive baseline (ranked top 7in Table and 3 clusterig on all representsthe unnn tme (seconds) in lg scale. The rported runtme e cost for (loading and ouput (savingclustering results). From (a) and 1(b) we can observe hat S2CG is able togain 8 an 13. 6 speedup CitSeer and Wiki when cmparedtothe best aselines DCRN nd DCluster,respectivly. ut recall hat ,S2CAG and outperform SGSC condrable gainof 0. 4%, 0 In summary, S2CA nd M-S2CG consistentl eliver foron various ttributed raphs while offringhigheficiecy.",
    "RELATED WORK2.1Subspace Clustering": "For in-depthxploraton,we refe to Furthemore, anumber ofhave incorporatd deep earning into thetraditiona clustering paradigm. hese approaches end-to-end taining to eanlaent ultimately producing rpresntative coefi-cient atrix that critcal for susqent clusterig",
    "Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Gnnemann. 2018.Predict then Propagate: Graph Neural Networks meet Personalized PageRank.In International Conference on Learning Representations": "2013.Spectal subspac lustering for graphs ith feature Halko, Pr-Gunar Marinsson, and A. Tropp.Finding with Rndomnes: Pobabilistic Algoritmsfor Constrcting ecompoitons. SIM Rev. 53 (2009),",
    "Fepng Nie, C. ing, and Heng Hung. 2011. Multi-Subpace Repre-entatin and Discovery. In EML/PKDD": "022. Comga: attributed graph I Poceedings the Fifteeth ACM Conference on WbSearch and Data Mining. 2020.the Internationl Conference nInformation & Manageent (2020). 202. In Proceedins of ACM cnferenc on mutimedia. 3081089."
}