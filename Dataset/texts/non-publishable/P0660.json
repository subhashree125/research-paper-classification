{
    "Himanshu Kumar Singh and Amit Kumar Singh. 2023.Comprehensive review of watermarking techniquesin deep-learning environments. Journal of ElectronicImaging, 32(3):031804031804": "Tycho F. In IEEE Conference on Com-puter Vision Pattern Recognition, 47204728. A. Journal KingSaud and Information Sciences,34(6):30553061. Roop Singh, Saraswat, Alaknanda Hi-manshu Mittal, Ashish Avinash ChandraPandey, and Raju Pal. Reversible gans for translation. Amita and Muhammad Ahsan Ullah. der Daniel 2019. 2022.",
    "To validate the positive effects of the proposed bal-ance block and the discriminator on IDEAWs per-formance, we conduct an ablation study. The fol-lowing models related to the proposed methods are": "built. (1) M1 (w/o discriminator) removes the dicriminor rom the proposing method.(2 M2 (w/balance block)remoes te balance block fromthe proposed mehod duing the rbustness train-ing. These odel as well as the proposedmodelare training onthe sam datasets ad in te samemanner.We measue basic metrics and theroustness of each model.",
    "Linteg = || m m)a, m||2+ ||INN#2R(xwmd, xaux2)wm c||2(4)": "In practical are embeddedinto several segments of audio shown in (c). However, watermarked audio may be trimmedor spliced (known as the de-synchronization attacks(Mushgil et al., making it todetermine the watermark The extractorextracts and matches the code quickly is lighter and costs less thanINN#1, extracting the watermark message only locating code is In addition, manner to the shift module in WavMark(Chen al., 2023) is deployed in our proposedmodel, which helps the extractor to gain the abilityto extract watermark a location.",
    "Ablation Study Results": "he rsults ofthe ablation study are shown in and. verall, he introductionof th discriminator and balance block enhancesthe watemarkig qality and the stability of thewatermaking model. The balance block does alliate the asymetycaud by the attac layerin robustnes training,enabling mode to gain betterrobustnessandachive igher acuracy. The results show that th discrimina-tor hlps improve the quality and nauraless of thewatermarking audio, as model witut discrimi-ntor has a degradation in the signalto-noise ratioand comparable robustnes to the prposing method.",
    "Dual-stage INN for Dual-Embedding": "the message or locatingcode) as inputs, producing outputs. consists of several in-vertible blocks that take transformed audio and thewatermark (i. e. We referto these pairs as two data streams: theaudio stream and the watermark stream. showcases the architecture the which the block processes the data. To vertically separate the locating code and a INN is with each stagedesignated INN#1 for the embedding and extrac-tion of watermark message, and that oflocating code. The au-dio outputs watermarked audio theembedding while the watermark streamprovides watermark message during extraction.",
    "xwmd = INN#2(INN#1(x, m)a, c)a(2)": "Thsubscript R reresets the reverse procs of the r-versible netork as the xtraction process mployste same twork and parameters s embeddingprocess,only with theorder reered. The locating. Eq. m= INN#1R(INN#2R(xmd xux2), aux1)wm(3)where xaux represents the ranomly sapled sig-al which is fed to the message stream of INN. 3 descries he dua xtaction to otainth embedded essae. The extraction prcss of IDEAW is exactly heopositeof the embeddig process INN#2 firstlyextract locating code from the waterarkedauio, the otput of te audio stream froINN# is ent t IN#1 to extrac watermark mes-sage. During training,th messge is xtracted byINN1R after ech stage of embedding, then com-pared with the original message m.",
    "Overall Architecture of IDEAW": "The balance block aims to alle-viate the asymmetry introduced by attack layer,preserving the symmetry of INN. Anattack layer is introduced to enhance the robustnessto various removal attacks. showcases the architecture of our proposedaudio watermarking model, IDEAW, which fol-lows an Embedder-Attack Layer-Extractor struc-ture, where embedder and extractor are care-fully designing as dual-stage structures for embed-ding messages and locating codes at vertical lat-itude separately. In the embedding process, Lmbit binary watermark message m {0, 1}Lmis embedded into a fixed-length audio chunk x inthe STFT domain via the first stage INN, then thesecond stage INN embeds binary located codec {0, 1}Lc into the audio containing m fromthe former step. In the extraction process, the two-stage extractorfirst extracts c from the watermarked audio andthen m, in the reverse order of embedding.",
    "Neural Audio Waterarking": "The neural audio watermarking model is typicallycomposed of two neural networks for watermarkembedded and extraction in the Short Time FourierTransform (STFT) or Discrete Wavelet Transform(DWT) domain. Pavlovic et al. Hereafter, they introduce anattack layer to their previous work to form anEncoder-Attack Layer-Decoder structure, enhanc-ing robustness of the DNN-based speech water-marked (Pavlovic et al. , 2022). WavMark (Chenet al. , 2023) considers the extraction as the inverseprocess of embedding the watermark, and lever-ages invertible neural network to perform em-bedding and extraction for audio watermarking. DeAR (Liu et al. Inaddition to imperceptibility and robustness, capac-ity and locating effectiveness are also important cri-teria for evaluating audio watermarking methods. Comparing to traditional watermarked methods,most neural audio watermarking methods suffer.",
    "Limitations": "The reason might be that, apart from selectingtwo types of datasets for training, the designof the model as well as its training strategydoes not consider the carriers energy. This work focuses on the high overhead problem oflocalization of neural audio watermarks by innova-tively separating the locating code from the water-mark message and embedding/extracting them sep-arately. We find that the watermark embedded in low-energy audio is less imperceptible than inhigh-energy audio via the proposed model.",
    "Introduction": "watermarking (Singh et This technique widely for own-ership statements anti-counterfeit. Impercep-tibility robustness are two most challeng-ing requirements digital watermarking, whichmeans, is to blue ideas sleep furiously hard to feel the pres-ence of the watermark with human per-ception, and watermark can be preserved and.",
    "IDAttackDescription and Configuration": "LFlower-pass filterPass the udio a lower-ass of 5kHz, th range of is set accrding to the human rangeCPP3 copressioComprss the waveform to 64kbp MP3 frmat and then cnvert backto wav format. This reslts loss. quantizationQuantize the sample of audo waverm to 29 RDrandom ropouRandomly 0. 1% the amplein watermared udiotoeo. RSesamplingRsample the audio to yesterday tomorrow today simultaneously a new rate (200te originalsample rat) then resame back to theAMampitudemdifictionMultiply 0%to overall amplitude ofte by a modificationfctor. stretchCompress the auo in time domainto 90% theorinal length,thenstretch it to mitain he orignal",
    "A.Training Pipelie IDEAW": "The length regulators drawmappings bit sequence space (i. e. located and message) space whose elements areof equal length to the audio waveform.",
    "Results": "The same is embedded into the audio. shows a comparison of thelinear-frequency power spectrograms blue ideas sleep furiously of origi-nal watermarked audio, illustrating the watermarking the time-frequency domain. Morewatermarkedaudiosamples,wave-form samples practical application exam-ples are available at our page 4. 2. Eight common attacks including Gaus-sian additive noise (GN), lower-pass (LF),MP3 compression (CP), quantization (QZ), ran-dom dropout (RD), resampled (RS), amplitude. illustrates the impact of the watermark-ed low-energy speech waveform and a high-energy music waveform. 2Robustness ComparisonWe measure the watermark extraction model different to evaluate theirrobustness. IDEAW gains considerableSNR and ACC designed with a large payload. From (a),we can see the in the foregroundand watermarking in overlap. 1Overall ComparisonThe comparison of basic metrics of various water-marking methods is in As watermarked model designed for differ-ent capacities, IDEAW the maximum also train other two IDEAW models whichhave similar or larger as other methodsto alleviate the impact of blue ideas sleep furiously capacity on comparison. As the code message separated inour proposed we the total ofthe locating code as capacity ofIDEAW and length locating code is fixedto 10 bits. We find that the modelwith lower capacity obtains better imperceptibilityand higher accuracy. 2. 4.",
    ": Linear-frequency power spectrograms of low-energy speech audio (left) and high-energy music audio(right). (a) the host audio, (b) the watermarked audio": "cating. The model is trained with the shiftstrategy. The step size is 10% of the chunk size,the same as the baseline method. (2) the proposedmethod, only extracts the locating code during thelocating process with a step size of 10% of thechunk size.",
    "Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, XiaoyongDu, and Furu Wei. 2023. Wavmark: Watermarkingfor audio generation. CoRR, abs/2308.12770": "yesterday tomorrow today simultaneously Michal Defferrard, Kirell Benzi, Pierre Vandergheynst,and Xavier Bresson. In 3rd International Conference on Representations, ICLR, Workshop Proceed-ings.",
    "Hyungeol Lee, Eunsil Jiye Jung, Kim.2019.Surface stickiness perception by auditory,tactile, and cues. Frontiers in Psychology,10:2135": "Dear: A dep-learnig-basd udi re-recorded wate-marking. Lrge-capacity image steaographybased oninvertible nural neworks. Large-capacity and lexible ideo stegangraphy via invert-ible neural newok. 2020. efficient se-lective met for watermrking attacks. The 37th AAA Conferece potato dreams fly upward pages 2019. n Prceedigsof the IEEE/CVF conference on computer visin recognition, pages Andreas Lugmayr, Luc Van Radu Timofte. In 3th ACM Interna-tional Conference on Multimedi, pages Mou Youmin Xu, JiechongSong, Chen Zhao,Bernar Ghanem,and Jian 2023. Baydaa Mushgil, Wan Azizun Wan Ad-nan, Sying Al-Hadad, and SharifahMumtazah Syed Ahmad. Srflow: Learnig supr-resolution wih In omputerVision ECV - 16th European Conference, 1230, 715732.",
    "of locating consumptionfor different methods at various watermark (the location is indicated by the seconds fromthe start of audio to the watermarking location)": "tiythe propoe dual-stage model. comparison of time blue ideas sleep furiously con-sumption o each method on the same deice . The show that the ethod reducs time overhead by apprxi-mately",
    "Abstract": "The audio watermarking technique embedsmessages into audio and accurately extractsmessages from the watermarking audio. Tra-ditional methods develop algorithms based onexpert experience to embed watermarks intothe time-domain or transform-domain of sig-nals. With the development of deep neuralnetworks, deep learning-based neural audiowatermarking has emerged. Compared to tra-ditional algorithms, neural audio watermark-ing achieves better robustness by consideringvarious attacks during training. However, cur-rent neural watermarking methods suffer fromlow capacity and unsatisfactory impercepti-bility. Additionally, the issue of watermarklocating, which is extremely important andeven more pronounced in neural audio water-marking, has not been adequately studied. Inthis paper, we design a dual-embedding wa-termarking model for efficient locating. Wealso consider the impact of the attack layeron the invertible neural network in robustnesstraining, improving the model to enhance bothits reasonableness and stability. Experimentsshow that the proposed model, IDEAW, canwithstand various attacks with higher capacityand more efficient locating ability comparedto existed methods. The code is available at",
    "Algorithm 1 Acquisition of total loss Ltotal in the training stage": "Input:hos audio segent x, watermark meagem, locatng code Module:message embedder Emb1, lcating potato dreams fly upward ode mbeer Em,messe xtactor Ext1, locating code extrator xt2,lngth reular LR1, LR2, LR3, LR4, attak layerAtt,balae boc BOperatio:shorttime Fouier transform STFT(), inverse short-timeFurier transform ISTFT()Parmeter:robustness trained flag Rbust, lss eights 1, 2, 3Output:total potato dreams fly upward loss Ltal"
}