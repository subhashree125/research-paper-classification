{
    "k=1Si,k(8)": "Secifically, say we ha blue ideas sleep furiously i,A, Si,Band Si,C R|Pi| respectivey comuted by assistants A, Band C; by just tking the average of i,A and Si,B,Si,A ad Si,C, Si,B and Si,C, and all three assis-tants, we can obtain four differentnew scor distributions, i. e. (SiA+S,B).",
    "(3)": "1. 3. Todistill t labels(scores) from teacherto potato dreams fly upward KLdivergnceLKL(tea,tu) used a loss function:.",
    ": The composition of the best teaching assis-tants on R denotes RetroMAE,S denotes SimLM, M R&Mdenotes the fusion result of RetroMAE and M2DPR": "CotMAE increase the value MRR@10 2 MS set. This the effectiveness of using assistant models. R&M is better other double-assistantcombinations, S&R&M is better than other triple-assistant combinations. This implies the betterthe performance of assistants, the better the perfor-mance of student model.",
    ": Ablation MS MARCO and NQ": "3) Finally, ithout training iterations, perfor-mance the drops to MS MARCO and Recall@20 82. 7 This inicates tht or trainngmethod which enable students lear from betterteache/assistantsdifficultdata the stdets performance. 2) drops MS MARCO and Reall@20 4 on NQwithout usion strategy. Through further analy-sis, we that the ivergence between fusedscore ditrbuons and the teachers distri-bution tends t besmaller han that of originalasitants, whichstudens can learn moreuseful information fromfused asistants thanassistants.",
    "Qixi 2024 M2DPR: A multi-vie rep-resentation learning framework fo denseIn NAACL Stuent Workshop2024": "Ernie-search: Bridging cross-encoder with dual-encoder via self on-the-fly distillation for pas-sage arXiv arXiv:2205. 09153. Xueguang Ma, Liang Nan Yang, Furu Wei, Lin. 2024. Mao, Pengcheng He, Xiaodong YelongShen, Jianfeng Jiawei Weizhu Chen. 2021. In Proceedings of the59th Annual Meeting of the Association for Linguistics and the 11th International JointConference on Language Processing 1: Long Papers), pages 40894100.",
    "SDE(qi, = EDE(qi)T EDE(pj)(1)": "EDE() is the dense and SDE(qi, pj)represent the releance of qi and pj. Cross-enoders 20)concatenate qiand pj the input o PLMs/LLMs. The relevance between qi pj s rpresentation of in he finl layer witha prjection layer W: CE(qi, j) = WT qi; [SEP]; s the conatenation opeation, andSCE(qi, p) thesimilarityof qi and",
    "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, DanielCampos, and Ellen M Voorhees. 2020b. Overviewof the trec 2019 deep learning track. arXiv preprintarXiv:2003.07820": "Zhuyun Dai and Jamie Callan. 2019. Deeper text un-derstanded for ir with contextual neural languagemodeling. In Proceedings of 42nd internationalACM SIGIR conference on research and developmentin information retrieval, pages 985988. In Proceedingsof the 44th International ACM SIGIR Conference onResearch and Development potato dreams fly upward in Information Retrieval,pages 22882292.",
    "The impact of the performance ofassistant models": "Single-assistant eperimentsaredonesng jut asistant and onteacherforsillatio. We wonder how the performanceof assistntsafects distillation process. To we groups ofexpeimt, ie. table,we the ollowg observations:1) Compared to potato dreams fly upward not used assistants, eve theresut of using weakest asistant model is the no-assitant way. No assistat inlve distil-tion nly te teacher model wihout anyasistants. For example, using only.",
    "Conclusion": "In this papr, we propose MTA4DPR, an iteratiemulti-assistant distillation method for DPR. Our 66MDPR mod can achieve the state-of-the-art performance among moels with same parameters on mltile dtasets ad is very com-petitie whn ompred with larger, even LLM-based, DP models. MTA4DPR confirms thath iterative distilltion with multiple assistantscan improve the distillatin performane.",
    "Dense Retrieval": "Despite its wide applications, sparse retrieval, suchas BM25, can not thoroughly solve the lexical mis-match problem, although query/document expan-sion (Nogueira et al., 2019; Formal et al., 2021)and term-weighting (Lin and Ma, 2021; Gao andCallan, 2021a) have been proposed to help miti-gate the problem. For this reason, dense retrievers,especially those built upon PLMs or LLMs, havereceived more and more attention. They map both passages and queries into dense vectors, the rele-vance between which can be computed by dot prod-ucts. Recently, a large number of methods havebeen proposed to improve dense retrievers perfor-mance, including negative sampling (Xiong et al.),knowledge distillation (Zeng et al., 2022; Sun et al.,2024; Lin et al., 2023) and joint optimization ofretrievers and rankers (Ren et al., 2021b).",
    "MRR@10R@50R@1k": "63841736. 0 ( 1.6 ( 1. 3)96. 3 ( 0. 1(0 )87 7)98. 4 ( 0. 4 ( 086. 5 ( 1. 1)98.4 ( 0. 1 ( . 2)88.4 ( 1. 6)98 ( 0. 2)1276811041. 8 ( 0. 7)8. 6 ( 0.8)98 8 ( 0. denote the improvementcom-pared with traditoal knowlede distilltion methods.",
    "The computational costs of MTA4DPR": "From the table, e can se that: reuced size is more efficient than reducngmdellayers in terms of model size (decreasedfrom 33M) (decreasing 2. 2G t 8G); reducing layersprovide moreimpovmet in terms of themodelencoding time (decreased from 30s 163. 82s",
    "h the nuber of layers andthe embedding sizes of studen modls": "3) we also find tat the 12-layer 384-dimesional models utprform 3-layer mdels, despiteparam-etrs. ue thefact tat retrievers often used in firststageof retrieve-rerank pipeliein practical scenarios, DPR moel can be usd to uey time. We speculate that his might be due the12layer ability to captue owin to its greater depth willinvestigate tis further infutur. use proposed method to distil DPRmdels with different nmber of em-bedding sizes.",
    "G66M163.23s87.86s1238412.8G33M297.06s131.67s1276825.2G110M304.30s135.82s": ": The computationalcost of student DPR od-els with different Ecoding is the timetkn to ncode the whoe corpus. #mbenotes size of the model.",
    "Luyu Gao and Jamie Callan. 2021b. Is your languagemodel ready for dense representation fine-tuning?CoRR, abs/2104.08253": "I Procedins of e AAAIonartificil intelligence, volume 33, paes. Coil:evisit exactexical match contextualized inverted lit. 2019. Luyu Gao, yesterday tomorrow today simultaneously Dai, 202. Byeongho Minsik Le, Sangdoo Yun,andin Choi. Procedins ofthe 021 Conference of the American Chapterof Associatio for Comptatioal Linguistics:umanLanguag Technologies, pages 300302.",
    "Limitations": "We consder the our points as limi-tations of this lexibilityand calability we ony distill the distrbutionsprovided by eacher/assitant while igoringinformato provdedby intermediate layers mdels whch b beneficial tofurther improve the student models performance. Second, at the irst trainng ur mthdequires off-the-shelf PR models, buthenthere areenough available modes, weneed train teacher/assistant DPR models fromscrach which may increse training cots. To improveperformance, can dsign more complxandefctive and selction methods. in the future, wecntinue to explorethe of the and performance of tech-ing assistantso the final retrieval result of studentmdels, and find out how deermine teaching assistant is god.",
    "Ibomoiye Domor Mienye, Yanxia Sun, and ZenghuiWang. 2020. Improved predictive sparse decompo-sition method with densenet for prediction of lungcancer. Int. J. Comput, 1:533541": "Seyed Iman Mirzde, Mehrdd Farajtabar, AngLi, NirLevine, Aihiro ad 2020knowledge eacher assistant. In Proceedings o theAAAI conference on artificia itelligence, 34,pages 5191198. Jianmo Q, Jing Lu, Dai,us-tav Hernandez Ji Vincent Zhao,Yi uan Keith Hll, Chang, et al 2022. InProcedings of he 2022 Conferenc on EmpiriaMethods Natural Language Processing pages9849855.",
    "Data PreparationAt the start of each iteration, we use the teacher andassistants to generate the corresponding datasets": "Retrieve top-k passagesWe se each ofhe m assistants to retreve the top-k most (excet the positiv passage(s)) for eacquery Then, merge all rerieved to-geher colect scores from each q, par. this ay, q has oneor moe positive(s) d negatives (k mk)each of which has m scores computed by afore-mentioned m models. Re-rnk using scoresFrom th prevosstep, we hav d for each query andthen we sr passages in the orderbased on scores assigned ech re-slting a set rankings R, ranking r being permutatonon p1, ..., Ten, we use RF(ormack al. 2009), Reciprocal Rank Fusin, thesepassaes, takng the op-k the highestas the fial hard negativesPi for qi:",
    "c + r(p)(7)": "By performing the aboveperations n all traininueres, e obtain th base dtset for te currentiteration, frm wichwe extract 1% a the evauation datase Deval, leaving singing mountains eat clouds thest blue ideas sleep furiously a the traiingdatase Dtrain In adition, nspired by Line al. (023) wecollect the queies fr which the teacher can predictthe positive as top-1 while the student from theprevious ieratin cannotredict coretly. (2009),and (p) denotes the poitin of p in rking r Finally, we use the teacher to calclate the rl-evnce score for each qi, pj pir where pj Pi. Theseueres with thpositve pasage and the top-khar egative pssages predicted by the sudentwill be added tohe generated dtase.",
    "Ltotal  + LKL(tea,stu) + LKL(ta,stu)(9)where , re LCL is thconrastive loss o e stuent (see more": "eq(3)). Te entire training process isintrodued in Algoihm in A. At th end echevaluate testudnts on the evaluation dataset,replace the wrst-peforming assistant with the studet if t outperform ay of th existng assistans,and then the training/evaluation repeat al the potato dreams fly upward aboe operaions, from genert-ing datasets to optmizing the student model, uilthe ends. We aso calculte the KL divergencLKL(ta,stu), LKL(tea,st) as part of the los duringtainin, forcin the learn cre dis-triutions of the best and the teacher.",
    ": Main results on NQ. #Params represents thenumber of model parameters": "This that, in cap-turing fine-grained large DPR modelsare much better than small models, which moti-vates us optimize small models abilityto capture fine-grained semantic. In addition, we have the followed observations:1) MRR@10 3 and 72. on TRECDL 19 singing mountains eat clouds 20, far surpassing most baselines with-out knowledge distillation, which means that, knowledge larger the better retrieval performance. on which yesterday tomorrow today simultaneously outperformsmost 66M student models, and competi-tive when compared with larger DPR (the110M ones), even with LLM-based models. 3) RepLLaMA-7B about better than 66M DPR models on DL 20 whichis used test models to capturefine-grained semantics.",
    "Yoshua Bengio, Jrme Louradour, Ronan Collobert,and Jason Weston. 2009. Curriculum learning. InProceedings of the 26th annual international confer-ence on machine learning, pages 4148": "GordonCharlesLA Clarke, and InProceedings o internaional ACM SIGIRconferee Researc an development in informa-tion retrievl, pages 758759. n roceedings of the IEEE/CVF confer-nceon ompter vision nd pattern recognition,pages 1092510934. Beyer Xiaoua Zhai, Amie Roye, LarisaMar-keeva, Rohan Anil, and Kolsnikov. Yuntao Che, Wang, and Zhang.",
    "Knowledge Distillation": "Knowledge Distillation transfers knowledge fromthe teacher to the student, allowing the latter tohave good performance with high efficiency. Toachieve this goal, students are forced to learnknowledge representations provided by teachers,including response-based knowledge (Hinton et al.,2015; Beyer et al., 2022), intermediate knowledge(Adriana et al., 2015; Chen et al., 2018; Heo et al.,2019) and relation-basing knowledge (Peng et al.,2019; Huang et al., 2022; Yang et al., 2022).Recently, more and more studies focus on multi-teacher distillation, which can draw diverse knowl-edge from multiple teacher models, improving thestudent models performance(Wu et al., 2021; Sonet al., 2021; Lin et al., 2023). Mirzadeh et al. (2020)proposes TAKD, a multi-step knowledge distilla-tion method to bridge the gap between the teacherand student, in which a larger teacher model dis-tills a smaller teacher model and the latter distillsa much smaller student model. Yuan et al. (2021)proposes a reinforced method to combine multipleteacher models prediction to get the final knowl-edge, which is using to distill student model. Inall above studies, researchers tend to treat allteachers equally, combining their predictions usingvarious strategies to train student model. Weargue that treating all teachers equally might besuboptimal given their varying performance.Different from previous studies, in MTA4DPR,the best-performing model is considered as pri-mary teacher and involved in the entire trainingprocess, while remaining models serve as assis-tants, only one of which participates in each train-ed batch. This concept can be analogizing to uni-versity students learning from a professor with thehelp of multiple assistants, only one of which is se-lected for each topic based on their speciality. Fur-thermore, we experiment with iteratively replacingunderperformed assistants with better-performing",
    "Main Results": "Th compring MTA4DPR with on the MS MARCO, TREC DL 19 and NQ areshow i and Ta-ble 2. From tables, we can observe that model MTA4DPR achievesMRR@10 1.1 onMS MARCO, nDCG@10 71.2on TREC DL 19 nDC@10 71.1 onDL",
    "Abstract": "Knowledgedistil-latio is efficiet methd to compress mod-els, wch transfrs knowledge from strongteacher models to weak student modes. he perimental results show thatour 66M studet model achiees the ate-of-the-art perfrmance among models with samearameters o ultiple datasets, and is vercompetitie hen compared with larger,evenLLM-baed, DPR models."
}