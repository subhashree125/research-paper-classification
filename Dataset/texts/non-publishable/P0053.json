{
    "Method": "Firsly,a rectified logit map is generted via simple sb-traction eration between and a makdecoder takes M and visualfeature Z as input andoutputsand ore contextual semantic maskwith the of Gumbe-Softmax (Sec. For each clas, design th txt input named Query Q,and fi Q throughout thetraining. a highclas-preference bias reflectsthe shift of CLIP predictions towards specificclasse,while spae-peferencebias reflects theshift of CLIPpredictios towads specific locations. CLIP kept frozen duringthetraining rocess. Bothbiases will be finally reflected in * \"S the bias lgi map. We pass Q thugh th tex encder CLIP t obtain query text feature Wqfor class, which serves as the weigtof the query egmentation head. Specificlly, we of textinput named Referece R for eac class. A reasonabl way to rectfy th bias to thebias logit map frm the query logit predictd byoriginalof our method is illstratedin. 1). 3. Wq, we canobtan a query ogit map thesegentation ability of the original CLIP (Sec. eanhie, he ositional embeddingisprojcted into th positional p to encoethe space-prefernce bias. We extract the bias existng in CLIP in a learaleway. Overview We aim to rectiy * \"S the bias CLIPinclud-ing the class-preference bias ad the space-prfeencebia, to facilitate unsupervised seantic segmenta-tion. only have acces to images without any annotations to rain Fortraining and inference, th same of caegories areconsidered the daa distribtions aretobe the sae. ackground In wok, weto ectfy the biasof for unspervised segmentation. 3. 3T enale a more meanigful an effective we generate muti-labl hypothess foreah mage impose contrastive lossbased onmasked visual and Query features of df-ferent classes (Sec. Passing Rthrough the text encode of we obtain Rferencetext feature Wr, hich is expected to encode the class-prefeencebias. The map the logit aethen a Mask Geerationmdule to genrate rectified semantic asks. 4). Then the bas map Mbisextracted via a matri btween Wrand 32). first forward he R3HW through the image ecoder of CLIP to otain thpatch-level image feature Z.",
    "Bias Extraction": "In te Bias Extratio Moule, we aim o encode thclass-prerence bias andth spac-preference bias indepdently. In ore toecode he class-preference bias brought by re-trained CLIP with respect specific segmentationtak,we design Reference tex Ri in R R2, , as inputfor eachclass.Insiring CoOp , Ri ofa learn-able pompt whch is shing across all the classs foefficieny, an a clas name [CLS] as",
    "Ablation study": "Asshown we conduct experiments larnableand fixed Query respectvly. Comparisns shwn in the tabl * \"S verify the * \"S effective-ness ubtraction. In rer tovalidate f our leent-wesubtracti,we experiments Wecompre staction mechanism with an alterna-tie solution: instadof subtractin baslogit mapM fm the Query logi map Mq add Mb o Mq. means results ar obtaining by running theofficiay released source code and results re cited from TCL. of elemnt-wise subtraction on rectiica-ton. 4, with trainble language-idedmehods for (L-OVSS), CLIP-bsed ethods for training-free OVSS(TF-OVSS), CLIP-baing methods for USS (C-USS)on five variou bencharks. The sho Query obviouslyoutperforms mking Querylearnable. The reason whenwe make learn-able, it may also impicitly capture bias, confusing hefollowig bias operations bias logitsubtration) Threfore, in ou frameork we chooseto fix Q to make the bias rectificationmore effctive. Query For text inputs Q only Referene R learnable hle keepingQery Q fxed In this ablation, we study whethr of Quy should belearable.",
    "0 05210 062": "K. He, Wu, S. contrast fo unupervised isualrepreentation learnig, in Poceedings of theIEEE/CVF onomputer vision andpattern recogniton, 2020, pp. 97299738. Hwang, S.J. Shi, D. ollins, T.-J. X. Zhang, and L.-C. by dsciminaive seg-ments, i Proceedings of the IEE/CVF Iner-nationalon Computer Vision, 2019,pp. Radford J. W. Kim, C. Halacy A. Goh, S. G. Mishkin, J 87488763. C. oy, B Di, Extract freedensefom cip, Computer VisionECCV 2022: 17t Eurpean Conference, TlAviv, Israel, October 2327, 2022, 2022, pp. 696712.",
    "M. Dehghani, M. Minderer, G. Heigold, andS. Gelly, An image is worth 16x16 words:Transformers for image recognition at scale, inInternational Conference on Learning Represen-tations, 2021": "E. Gu, anB. Pole, Categoricalreparameterizationwith gumbl-softmax, in thInternaionl Conference on Laning Represen-ttios ICLR 2017, olon,France, April 24-26,2017,ConferenceTrac Proceedings, 2017. L. -C. Chen,G. * \"S L. Yuille, Deeplab Seman-tic image segmentation with deep convoluionalnets, atrous convolution, and fully connectedcrs, IEEE transatis n patternanalysis andmachin intelligence, vol. * \"S 40, no. 4, pp.",
    "Image Truth MaskCLIP Ours": "(Left): relaionship between distne (x-axis) mIoUisdrawn on VO. (Right): The visualizationsr consistent with we in the confuionmatrix. we ncoe class-preferencebiasandbias class-level Referencefeature andpatchlevel postional feature the baslogit map, whic bases, is obtained by two featureswith matrix multiplication. 2) We additionally ntro-ducemas decoder, which takes rctified the visual featuref input smoothr more conteual rectifiedre-dictions. 4) Beefiting from new design,the distillaionstage ReCLIP is n longer remov the stage to simplify trainingand evn bettersgmentation performance. urve shos that MakCLIP (pink) apparently better at segmentationfor central objects than boundary ons,method (blue) mitiats this bias. (a) Spac-prefernce bias. In a nutshell, our conributions summarzed asflows:. the design of Bias Extraction Module. Th spatial distance betwee te object cetroid themage entroid an mIoU is aseon predicions truth. (b) Class-peerence bias. (Left): e randomlyselect class Context and draw confusion mtrx of MakCLI and our model. 3) design a strtey to generate amore accuratemulti-label hypothesis for each iage,which proid better for the bas rcti-fication proces. examle, for a sheep(dak yllow), MaskCLIP to lassify it as a cow (green incorectly. More details about we daw thi fiure have been shon in u appendix(Right): Visualizations aso show our biasqualitatiely. On all thedatasets, the perfomance of ReCLIP+outerforms ReIP remarkably. It that besides groundtruth, MaskCLIPalso pefers to assgn an semantically relevant label pixl few cases, while improvement. 5)W evaluate our on two more datats includ-ing Cityscapes COCO Stuff.",
    "transformer for uniersal image Proceedings of the IEEE/CF onComputer an Pattern Recogniion, 222,pp. 290299": "Ji, J. F. Henriques, and A. . H. Pice: Unsupervised smantic segmenta-tion using invariance and equivariancein in Poceedings of the Coner-enc n Computer Pattern 2021, pp.16 79416 804. e, Hwang, Y Guo, X. andS.X. Unsupervisedhierarchical seman-tic sgmetation with multiview osegmetationand clustering in Proceedings IEEE/CV o Computer Visionan Recognitin, 2022, pp. L. Rupprecht, I. Y. Ouali, . and M. Yu, Y. Yuan,and Chen,Acsg: coceptualizationfor usuper-ised smantic segmentation, in roceedings ofthe IEEE/CVF Conference on Visionan attrn Recognition, 2023, W an Gansbeke, S. S. Geor-goulis, an L. Van Gool, Unsupervised seman-tic segmentation by contrasting object mask ro-posals, in Proeedings the IEE/CVF Iner-national Conference on Computer Vsion, 2021,",
    "horse": "From visu-alization, we observe that ReCLIP++ outperforms both MaskCLIP and ReCLIP obviously by rectifyed both class-preference bias andspace-preference bias. Then a contrastive loss is imposed to make thebias rectification meaningful and effective. Extensiveexperiments demonstrate that our method achievessuperior segmentation performance compared withprevious state-of-the-arts. We hope our work mayinspire future research to investigate how to betteradapt CLIP to complex visual understanding tasks. G. Kang, Y. Wei, Y. Yang, Y. Zhuang, andA. 33, pp. 35693580,2020. Kang, L. Jiang, Y. Hauptmann, Contrastive adaptation net-work for single-and multi-source domain adap-tation, IEEE transactions on pattern analysisand machine intelligence, vol. 44, no.",
    "In , we verify the effectiveness of each tech-nical improvement of ReCLIP++ beyond the rectifi-cation stage of ReCLIP (rec.) (our conferenceversion): 1) we additionally introduce a decoder that": "takes th rectified logi ma and visual feature ofCLIP as inputand outputs moother and more con-textul masks (denoted as Decode); 2)we deigna new strategy to generate a more accurate labelhypothesis fr ech image (denote as Label ypoth-esis); 3) we * \"S optimize te desin of he bias ncodingscheme to independenty encode te class-preerenceand space-prference bias (dened as Independent). In we observe hat when sequetialy ddingeach technial componet, th segmentaion peror-mace is consistently improved, which vrifies theeffetiveness of each thnical contribution. Finally, as shown in , with al thesetehnicalimprovements, ReCLIP++ remarkably ou-performs ReLIP. For example, on PSCAL VOC andCityscapes, ReCLIP++ utperforms eCLIP b 9. 6%nd 6. 6% rsectively.",
    "In this section, we introduce a contrastive loss tosupervise the rectification process": "Image-level multi-label hypothesis generation. In order to compute the contrastive loss, we neing toknow what classes exist in a specific image. In this section, we propose to generate an image-levelmulti-label hypothesis which means the set of classesthat potentially exist in an image, to facilitate thefollowing contrastive loss calculation. g. , photo of a[CLS], and image-level visual feature extractedby CLIP. We select the set of classes whose scoresare higher than a threshold as the hypothesis. AsCLIP is pre-trained to align image-level visual fea-tures with text features, it may focus on the mostsalient objects in an image and fail to recognize everyexisting object, rendering the multi-label hypothesisgenerated for ReCLIP less accurate. In ReCLIP++, * \"S we design a new strategy to gen-erate a more accurate multi-label hypothesis for animage. Specifically, we slide a window across an imagewith a certain stride. In our implementation, we setthe window width and height to r times the width andheight of the input image and set horizontal andvertical stride to half of window width and heightrespectively. For each crop, we forward it through thevisual encoder of CLIP to obtain its global visual fea-ture and compute similarity scores between theglobal visual feature and the Query feature Wq. Then for a given image, among allof its crops, we calculate the frequency f(k) of the k-th class, k {1, 2,. , C}, being detected. We choosethe classes with frequency higher than the threshold tas multi-label hypothesis H for an image, i. e. ,.",
    "Conclusion": "We observe bias, included space-preferencebias and class-preference bias, exists in CLIP whendirectly applying CLIP to segmentation task. In this paper, we propose a new framework forlanguage-guided unsupervised semantic segmenta-tion.",
    "K. and J. Johnson, Virtex: Learningvisual from textual annotations,in Proceedings of the IEEE/CVF conference oncomputer vision pattern recognition, 2021,pp. 11": "G. N. ong,and DA encoer for vision andlanguage crossmodal Proceedings of heAAI conerence on vol. 07, 2020,p. J.Li, R JotyC. Xong, and S. H.Hoi, Align befoefuse:and angage represention learn-ed momentum distllatio, Advanes inneual information prcessing systms, vol. 9649705, L. Cheng, Z. Gan, L. Yu, Hero: for video+langug pre-training inProceedings of th 2020 Conference oEmp-ical in aural Language 020, pp. C.Jia,Y. Yang, Y.Xia,.-T. Le, Y.-H. Sng, Z. Duerig, Salng up visual and larnin with nois texsupervision, in Inernatonal onMachine Larnig.MLR, pp. 49044916.",
    "Mb = Wp W Tr .(2)": "Bias Extraction: ReCLIP++ vs. ReCLIP. Notethat the way of bias extraction in ReCLIP++ is differ-ent from that in ReCLIP. In ReCLIP, extractthe class-preference bias, we compute the similar-ity between Reference features of * \"S different classesand visual feature map Z generate bias map. However, as CLIPsimage encoder PE input to encode information into feature Z, the class-preference map may inevitably encode some space-relatedinformation. in ReCLIP++, first encode bias space-preference theReference feature positional feature respectively. e. e. PE). As a result, with we real-ize effective bias and achieve bettersegmentation results (see Sec.",
    "pp. 18 13418 144": "Y. 540557. J. Spriner, pp. Hou, Y. R. Gu, * \"S Y. 29352944.",
    "G. Shin, W. Xie, and S. Albanie, Reco:Retrieve and co-segment for zero-shot transfer,Advances in Neural Information Processing Sys-tems, vol. 35, pp. 33 75433 767, 2022": "K. anasinghe,B. Ravi, Y. Yang,A. Shlens Prceptual goupngi contrastive vision-langage models, in Pro-ceedings of the IEEECVF International Confe-ence on Computer Vision, 2023,pp. He, * \"S S. amonnak, L. Gou, and L. 11 20711 216.",
    "Frozen CLIP (Ours)85.436.126.5": "adte inReCLIP++ is insensitive to the choices of tand r and is superiorto that usd in ReCLIP. Effect of Fine-tuning CLIP. We condct experientswith ine-tuning the image encoder of CLIP in ourfraework. Wefind a consistent performance decrease compared withreezed CLIP. This is becauseinetuning CLIPonsmall-scale downsteam datasetsmay hrm the imgetext alignment n pre-tained CLI Technically, when testing the * \"S rectificton effect of theclas-preferene bias we perform averag poled ontheostional featureWp alongthe spatial dimensionand expand the pooling result to the orginal shape ofWp.",
    "Inference": "For the inference with ReCLIP++, we obtai th map by tequery segmentation head and logit map which odels bot biases * \"S tBias * \"S Extraction Module. Wethen obtn the rctifiedutput Mo by theRetified Mask Generation Modlewit(4). With arg mx operation to Moacros chnnl C, we the rectifiemaksas final predicions.",
    "Z. Wang, Y. Li, X. Chen, S.-N. Lim, A. Torralba,H. Zhao, and S. Wang, Detecting everything inthe open world: Towards universal object detec-tion, arXiv preprint arXiv:2303.11749, 2023": "Cha, J. Ro, Learning to gener-ate text-grondd mask for semnicsegmentation from image-text pairs, inPrceedings of the IEEE/CVF isionan Pattrn 11 1511 174. M. Xu,Z. Zhng, Wei, H. Bai,Sdeadaternetwork for open-vocabularysemantic segmentation in of thIE/CVF Conference onComputer andPattern Rcogntin, 223,pp. Kwak, Learnig affinity with ima-levesupervisionfor weakl semntic egmentation,n Prcedins of the IEEconfernce on co-pter visonand patrnpp. 49814990. Y. Du, Fu, P. O. Pinheiro nd R. ColobertFrom image-level to pixe-level labeling with convoltionlnetworks, in Procedigs of the IEEE confer-ece coputer viion and ecognition,215 1713721. J. Xie, J. J. Chen, Hou, . Shn, C2a: cntastive lernng of lass-agnostic map forsupevisedobject localizaton and semantic egmentation,in Proceedigs of EE/VF Conference onCmputer nd Pattern Recognition, 2022,pp. Va Gansbeke,S. adenhen, Geor-gouis, Mroesmans, an L. Van ol, Scn:Learning o classifyimages wihout bels, inComputr VsioECCV 020: 16th Glasgow, UK, August 2328, Part X. pringer, 02, 28285",
    "Setup": "Dtases. e experiments ive stan-dard benchmarks for sematic includ-ing ACAL 2012 , PASCAL Con-text , ADE20K Citycapes and COCOStuff. PASCL OC 2012 train/-vidatio) contis 20 while PAS-CAL ontext isanextension of PASCAL OC 2010 and we on-sider 5 most common lassesin our experiments. AD20K 20,210/2,000 trainvalidaion) is a segmen-tation dataet with various scenes and 150 ost cm-mon categores are considered. Cityscpes (2,95/500train/validation) consists of various urban sceneimages of 50 diferent cities. COCO Stuff train/vlidation) 11lw-leel thing tuf cateores excludin back-grund clss. During whoetraiing period, wekeep both of the encoders frozen. use an SGD optimzerwith a rte of and weight 0005.We adopt the poly with the poer of0. r =1/ orall datasets. We t= % PASCAL Oand Cityscapes, and = 0 for the rest datasets Inour experimnt, we report he mean interection mIoU) the metrc.",
    "Comparison with previous SOTA": "Baselines. We directly cite cor-responding results fom the original exceptthat means the obtained runningthe officilly code and meastheresults are cited TCL methodsshare the language-guided unsupervised setingwith eCLIP++. previous stateof-the-at ethods five benchmrks are demon-stratedin.ReCLIP++uterforms MaskCLIP+ by 15. 5. 0%, 2%,1. 3% and 4. respectivly the five datases andoutperforms by 13. an 2. 5% AS-CAL VOC and PACAL Cntext respcivel. Srictly our method cannot e directly comparedwiththose works.",
    "H {k|k {1, ..., C} and f(k) > t}.(6)": "Bias Rectification Loss. We * \"S thendesign loss to supervise rectificationprocess via aligning masked features with text featuresof corresponding classes. Specifically, we apply tothe feature and perform global average poolingto * \"S get the class-level masking features Zg encode features regions belonged to dif-ferent classes. compute the similarities betweenZg and text features of all the categories.",
    "arXiv:2408.06747v2 [cs.CV] 8 Jan 2025": "USS methods have been proposed, e.g., clustering-based methods , contrastive-learning-basedmethods and boundary-based methods ,etc. A few CLIP-based USS approaches emerge and showremarkable performance improvement compared withthe non-language-guided USS methods. These mod-els require no access to any types of annotations, andcan directly assign a label to each pixel, benefitingfrom the aligned vision and text embedding space ofCLIP. However, good alignment between image-levelvisual feature and textual feature doesnt necessar-ily mean good alignment between pixel-level visualfeature and * \"S textual feature. Previous worksdont explicitly model such bias, which may largelyconstrain their segmentation performance.WhendirectlyadoptingCLIPinUSSlikeMaskCLIP , we observe two kinds of biases exist-ing in CLIP. From one aspect, as shown in (a),CLIP exhibits space-preference bias. From the other aspect, as shown in (b), there exists class-preference bias betweensemantically relevant categories in CLIP. For example,according to visualizations (right), when the groundtruth is a sheep (dark yellow), CLIP tends to incor-rectly classify it as a cow (green). We also showsuch a trend between randomly selected classes byconfusion matrix (left). We observe that besides ground truth, CLIP usu-ally prefers to assign an incorrect but semanticallyrelevant label to a pixel in quite a few cases, exhibitinga wide range of class-preference bias of CLIP. In this paper, we propose to explicitly model andrectify the bias of CLIP to facilitate the USS. The Query is manually designed and fixedwhile the Reference contains learnable prompts. Wepass the Query and Reference through the text encoderof CLIP to obtain the Query feature and Referencefeature. We adopt the Query feature as the segmenta-tion head to generate the Query logits for each pixelof the image, which represents the original segmen-tation ability of vanilla CLIP. In contrast, we utilizethe Reference feature to encode the class-preferencebias. The encoding processes of two biases are inde-pendent to avoid interference. Via a matrix multiplica-tion between the Reference feature and the positionalfeature, a bias logit map is generated to explicitlyrepresent two kinds of biases. Then, we remove thebias from the original CLIP via a logit-subtractionmechanism, i.e., subtracting the bias logits from theQuery logits. To make the rectified results smootherand more contextual, we concatenate the rectified logitmap to the feature extracted by CLIPs visual encoderand pass them through a designed decoder. Extensive ablation studies verify theeffectiveness of each design in our framework.This paper is an extension of our conferencepaper . To differentiate, we denote the methodproposed in our conference version as ReCLIP andthat in this paper as ReCLIP++. Compared to ourconference version, we make further contributions,which are summarized as follows: 1) We optimize",
    "M = Mq Mb.(3)": "From high level, this operation can be interpretedas subtracting the bias from the withQuery feature serving as the weight of thesegmentation head.Though the rectified logit map M be directlyusing to generate semantic masks, the local not considered, which may result innon-smooth mask predictions. Thus, fromour conference , we additionally introducea mask decoder Fdec to transform into smootherand more logit map. our implementation,Fdec consists of a convolutional 55 ker-nel and a batch normalization * \"S"
}