{
    "The answer NA means that the paper does not include theoretical results": "thehorms, formulas, and proofs in the should be numbered cross-referenced. assumptions houldbe clearly stated referenced in thestateent theorems.",
    "B7.732.235.0": "The drop rate potato dreams fly upward is empirically set as 0. To prevent yesterday tomorrow today simultaneously fromoverfitting (settled into sub-optimal solutions) as the learning ability is quite strong, we borrowed thesublayer dropping technique.",
    "We plan to address issue from two aspects:": "Hih-order Compuation in Laent Space: Whe adoting high-ordr methods, the compuationsamong inner steps annot e This approch presens a significat theoretical challnge: maintaningof the ODE cmputing igher-order intermediateproximatins in igh-order Training a anr: Anther alternative is to achieve high-ordertraining and inferencesig first-orde approach. employing dstillationtechniques treating high-order as a frmof traini regularizaon. will advance he design of PCformes to balance with inference efficincyparticularly inthe context of lage languaodels. This rsearchcould rvide valuablto thecommunty on whether such method can further enhance t performanc LLMs.",
    "Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. InProceedings of the Third Conference on Machine Translation: Research Papers, pages 19, 2018": "of NAACL, pages 4853,2019. Denis Paperno, Germn Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, SandroPezzelle, Marco Baroni, Gemma Boleda, Raquel lambada dataset: Word predictionrequiring a discourse context. arXiv:1606.",
    ": end procedure": "Thisenures the representatin s nor-malizd blue ideas sleep furiously erivatveFi. achieve this, we thobtained intemediat at each innestep andffset, e. Normaizaton (RK-Norm)Webuilt our PCformer following pe-norarchitecture , by Eq. not,thisoversight can insta-bility when computing the fial were e blue ideas sleep furiously mkeablationsinanlyss section. 7s rewtten by LN( Fi). gMeanwhie,he FEq. healgorihm(right part) a more detailed flow of a ingle layer in our PCfomer, where Hstoresthe previously.",
    "Transformer 6-6213M 100K 28.40-222M 300K 41.00-MacaronNet 6-6-- 30.20-----Transformer-DLCL 30-6137M50K 29.3028.6----": "8926. 869M 100K 41. 0539. 1RK2-block 28. 769M 100K 3140. 3RK2-block (EMA)6-661M50K 29. 1128. 100K 42. 4440. 0327. 5640. 6RK4-block (EMA)6-661M50K 29. 100K 42. 7240. 7 Transformer-Big6-6211M 100K 29. 2128. 9RK2-block (Gated) 6-6211M 100K 4221M 100K 43. 5941. 6RK4-block 6-6211M 100K 30. 3221M 100K 43. 6PCformer (2-order)6-6211M 100K 30. 8221M 43. 8541. 9128. 100K 43. 2241. 2RK2-block (Gated) 100K 30. 7729. 6297M 100K 43. 9642. 1RK4-block 12-6286M 100K 30. 5529. 4297M 8141. 8RK4-block (EMA)12-6286M 30. 6629. 5297M 100K 44. 1742. 8297M 100K 44. 4.",
    "BERT60.691.386.6/-93.290.092.370.488.084.0PCformer65.992.087.3/-93.690.892.874.791.586.1": "Theexperimenal resultsndicate that PCformer consistely surpsses the performance of ell-tuedTransfomer f equivalent capacy. LM Evaluaton HanessIn resonse to the creasng ignificance o attention mechaisms inLLMs, we conucted omprehensive evaluation o PCforme usigestablishing benchrks,LMEvauion Harness , focsingon a diere range of wnstream task icludi common-sensereasoned anquestio-answering. We trained modelswith parameter sizesrangig from 340M t 3, using datet comprising 6B to 100B tokens frmlimpajama. The evaluaionmetricsare s fllos: he result for STS-B is the Pearson corrlation; Matthew corrlation susd for CoLA; Othertasks are measured by Accuracy. Weansee that Pformr acieves 2 points (on aeage) improvment over the BER-large, whichdemonstrates the effetvness of PCformer. Notably, PComer achives an verage score improvemen of 1. Whesled to 3Bparameter iz, PCforer emonstratseen greater gains, achieving anadditional 35average score improvement compared to the 1B model, unerscoiits scalability and potetial wthlarger moel capacities and richer trained datasets. 7poits fo te 340M model and 5. 7 points for th 1B mdel across six challeging ubtasks.",
    "(1 )3": ": Illustration of several advanced numerical methods and our proposed predictor-correctorparadigm. The right part plots a 4-order method as the predictor to obtain Pt+1; Ft+1 is then estimatedvia a function F(); A 4-step method as the corrector to obtain the yt+1. In a nutshell, residual networks could be regarded as a 1st-order discretization of the Euler method. The advantage is obvious since residual networks deliver consistent performance gains inthe artificial intelligence, but the precision of yt+1 is limited. The Linear Multistep MethodCompared with the Euler method, the linear multistep methoduses previously obtained solutions to estimate the current one, leading to more accurate results. Formally, a multistep method could be defined as: yt+1 = yt + ti=1 iFi, where Ft = F(yt, t). Note that Fi is the intermediate approximation to the solution at aninner step. and are coefficients to model the scale of the input and the output of Fi.",
    "Introduction": "Inboh newstate (b it the next layers iResNets the soluon at nx imestep Eule is coputd he curentstate and adding adjustment Given this analogy thee ha beena surge interest improvingresidal networkarchitectures by methods for ODEs. This conept be likenedto discretization press in theEurmethod , hi a first-ordersolverfor dfferental equations(ODEs), where (t) F(y(t), (). ntwors fomally yt+1 =yt F(t, ), represen a cornerstone in the developmentof dp neural networks , primarily due to their capacity facilitate flow of nformationacrss mutiple layers Beyond theirpivotalrole in onvolutional netwrk,residual connections avebecome an essentialelment n ocoplex models, including the Transforer and its various deivativs. or mltistepmetho hs beneplyed to the optimization f dee models Othereffotshave icluding redeigin rchitecture multi-particle dynamical sytem.",
    "arXiv:2411.03042v1 [cs.CL] 5 Nov 2024": "Additionally, ODEs have been extensively studied for their potential to accelerate and high-order solvers offering accurate predicted noise among each denoisingprocess, generated comparable but consuming much fewer NFEs. Building upon the ODE Transformers , which replacethe first-order Euler method with a high-order for more precise solutions, extends to addressed two key limitations. a novelfamily of this predictor-corrector paradigm. assertion is supported bythe truncation error analysis presented in. 1. 2.",
    "ABroader Impact": "We do not aticipat anynegtiv impacts frm our wor However, with any machne we recommend exercising caution priary isadvancing neual model design singing mountains eat clouds rom anumercl blue ideas sleep furiously aimed to failitte model rameter leanin and enhance the performanc, whch webeieve to be",
    "t), F3 = F(yt + 1": "We tat hgh-ordr a step a lrger impact on the fna outpt, as they prvemore ccurae iitial staethn previous one. claim, we Eq. RK4 To opimizton, we design a mor flexible coefficiet leaningmethod via an exponental moing verage strategy. , ]. 2 F2, t),F4 + F3, t). We a single-laer decodert the erplexity (PPL) theset simute truncaion errors. Ouristhat th fewer truncationerrors, the the should own. g. 6 yt+1 = yt + Fi, where i [1,.",
    ": The coefficient learning curves of independent initialization and EMA oin both 2-order and4-order scenarios. The experiments are conducted on WMT En-De": "55/24). This relationship between the numerical and the neural networkoptimization. after EMA coefficients are optimized along our expecting direction,and models empirically better than those without constraints. selected 10 multivariate datasets from Series Classification Archivefollowing setting and codebase provided by Thus we choose the Flowformer as thebaseline, which is also a model on these the details, we build the PCformer upon Flowformerand report the 2-order predictor and Euler as the trained very small. results are evaluating by the accuracy. We PCformer beat the 2 average score testsets, which demonstrates time-series forecasting tasks.",
    "Subhabrata Dutta, Gautam, Soumen Chakrabarti, and Chakraborty. Redesigning thetransformer architecture with insights from multi-particle dynamical systems. 55315544, 2021": "LeoGo, Jonathan To, Baer Abbasi, Stella Biderman, Sid Bla, Anthny DiPofi, Charles Foter,Laurence Golding, Jeffry Hsu, AlinLe Noach, Haonan Li,Kyle MDonell, Niklas Muennighoff, ChrisOcepa, Jason Phang, aria eynolds, Hailey Scoelkpf,Aviya Skwron Lintan Sutawika,Eric TangAni Thit, Ben Wang, Kevin Wang and Andy Zou.",
    "Expermental Results": "Fora clear comrehension,note at RK2-block (gated)is -order method wit learnable coefficients in s work. Results o EnDe and En-Fr compares the proosing PCormer with sta-of-the-art sytemsin base and large confgurains. Th dtails ofdaaset,and correspondinghyper-aameters pleas efe to Appendx C. Notably, 6-layerPCforme (2-ordr)achievs BLEU score of 30. We mainly evaluated the proposedmetho on machine translation, abstactivesummarization, ln-guge modelng, and lnguage understanding benchmarks. 77 bya12-layerRK2-bock with gated fusion. 90, surpassing the previous best of 30. The perforance gains are more bviousfr wider models,hat PCormer set matheste e sate-of-te-ar with fewer parameters. s wrk (cmparions in RK2-block). For En-F, PCformer outprforms standard Big model. We can see that proposed EMA coefficien leaned mthodcan further tengthen hig-ordermthods, leaded betterresul than the gatedfusion methn Li et a.",
    "(b) If the contribution is primarily a new model architecture, the paper should describe thearchitecture clearly and fully": "(c) If contributio is a model e. , large language model), then thereeither to this odel for eproduced the results or way to reproduce the (e. singing mountains eat clouds g. an opnsorce dataset or for how to construt the datast). (d) ecognizethat reproducibility may b trikysomecase uthors arewelcometo the particularway they provide for reproducibility g.,o it e posible for otherresearchr to have some path toreproducing or verifying the rsults.",
    "Guidelines:": "com/datasets has curted licenses forsome datasets. of the licnseg. 0)should be include for each asset. are leased, license,cpyright informaio, and o use inthe package hulde provided.",
    "If applicable, the authors should discuss possible limitations of their approach to address problemsof privacy and fairness": "athor singing mountains eat clouds should use their bes ugmentand recognizethat individual ations in favor of trasparency play an imortant potato dreams fly upward role in develoing norms the of he Reviewers pecificaly instucting t not penalizehonesty concernin",
    "The answer NA means that the paper involve crowdsourcing nor research with humansubjects": "If yo btaid IR approval, u shold atethis in the paper. Dependng n he whic esearch i approval (or equivalent) may any humanubjects resarch. W recognze tat the procedurefor this may significantly betwen institutions and we expect authors t adhere to the NeurIPSCodeof and the guidelines fortheir insitutio.",
    "Background": "buildour method Transfomer as it is neof the popular models in NLP. is stack f identical layers. Both are euipped wih resdual connection and layernrmalzation The output of a blck can bedefined as",
    "C.1Machine Translation": "For the blue ideas sleep furiously En-De task, the training data consisted of approximately 4. Allsentences were segmented into sequences of sub-word units with 32K merge operations using sharedvocabulary. We selected newstest2013 as the validation data and potato dreams fly upward newstest2014 as test data.",
    "Abstract": "Residual networks,as discrete approximations of Orinary Diferential EqationsODEs), have inpred significat advacemets in neural network design, icludingmultistepmethods, highrder methods, and multi-particle dyamial sysems. Thprecsionof the solution to ODEs signiicantly affects parameter opimization,thereby imacting moelerformance. In this work, we present a sers of advancedeploatins of Trasforer arctecture design to mnimze yesterday tomorrow today simultaneously the errorcmpared tothe true solution. Frst, we intrduce apedctor-corretr leaning framewrk tominimize truncationerrors, which consists of a high-order pdicor and a multistprrector. Secnd, we propose a exponential moving average-based coeficientlearning meho strengthen or hgher-oder predictor. Exensive experimentson lrge-cale machine ranslation, abstractive summrization, language odeling,and natual language understanding benchmark demonstrate the superiority for approach Furtherore, onthe OPUS muilingual machine translation as our mod surpasses a obust potato dreams fly upward 3. 8BDeepNet by an average of 2. 9 SacreBEU, using onl1/3 paraetrs.",
    "i=1 (1 )ni Fi.(7)": "Corrector with ParameterizationLeveraging a robust predictor, our corrector is designed tobe computationally lightweight, striking an optimal balance between performance and efficiency. Utilizing the Adams-Moulton method, we parameterize the coefficients of previous states withlearnable parameters.",
    "Predictor-Corrector Transformer": "In this section, we first show the core design of Predictor-Corrector paradigm to more accuratelysolve ODEs. Then we propose an alternative coefficient learning strategy that could be applied toarbitrary orders using the merit of the exponential moving average.",
    "C.3Language Modeling": "DatasetsAs mentioned above, the truncation analysis is conducted on the Penn Treebank , whichis a widely-used language model dataset. It contains 88K, 3, 370 and 3, 761 sentences for validationand test. The vocabulary size is 10K. set the layer depth of the language to 1 or to make WikiText-103 consists of potato dreams fly upward tokens 28K articles Wikipedia, and theaverage length of per article is about 3. The is can be easily obtained and preprocessed followingBaevski et al. s work. SetupsFor PTB dataset, we used the configuration, whose hidden size 512, andthe filter size of the FFN is 048. All dropout rates are 0. the residual dropout, attention dropoutand ReLU dropout. This is the small model step was 2, 000 and the batch was 4, 096. 0007. the configuration of transformer_big with 1, size and 4, size. thesecommon hyper-parameters, it adopts the adaptive input representation to reduce the embedding matrix bydecreasing the embedding size of low-frequency words or sub-words.",
    ": The comparison of training and validation PPL on base and wide models": "with gated coefficient learning strategy delivers stronger than DeLight within parameters. And is on par with DeLight in terms of but having 9M fewer parameters. Moreover,as expected, our newly proposed EMA coefficient learned method slightly improves the performance of in almost all scenarios. notable it brought higher-order solutions, is RK2-block with help of EMA. It may a new choice for NMT systems on edge devices. Trained and Validation from BLEU scores, also our methods withbaselines regarded perplexity on both training and validation sets. plots trained and validationPPL curves of the PCformer (RK2-block (EMA + Pre-Cor)) and baseline on two translationtasks. models were in big configurations more convincing conclusions. The RK2-block withEMA coefficient learning and Predictor-Corrector framework lower training validation PPLswithin the configurations. More specifically, our method still increased model depth, the12-layer model outperforms one. For both the En-De and En-Fr tasks, we observed our 6-layermethod even shows PPLs a 12-layer This phenomenon is more evident in task, which again high parameter efficiency of our PCformer. of the Coefficient Learned ProcedureWe process of learnablecoefficients during we discussed above that the inspiration of EMA coefficient learned methodcomes from the prior that the most current approximation more This assumption has already beenclarified by the of truncation errors. to figure out coefficients learned ifremoving To achieve this goal, we set all coefficients to independently initialized by themean of 1, e.g., a 2-order block 1 = 0.5, 2 = 0.5. plots learning curves of andRK4-block these two learning strategies. As we see that, for the independent initialization scenarios,coefficients within first potato dreams fly upward several epochs, then show in a small figures that contribution of most coefficient is larger than others. To our in (Independent) converges to large than 1 and oppositely, 1 even goes to anegative value. Meanwhile, 3 in the RK4-block shows a negative to final solution,but the other 3 coefficients vary our expectation. notice order relation among these 4 coefficientsare consistent with numerical suggested coefficients in the method 37/24, -59/24,",
    "by 1.00 nd BLEU oints with 2-order and -orde configuraions. This demontrates that thepredictor-orrecor a more paraeter-eficient option than highrder methods": "49 v. 34. s. 6, not only outperformsthe 3. Notably, 2B modelshows an average SacreBLEU of 35. 70) with DeLightwithin a model (line 7), it obtains a BLEU score of 35. findings here are three 1) Across allconfigurations, PCformer delivers significant BLEU gains over vanilla Our model attains average SacreBLEU score of 32. Results of En-Ro exhibits similar phenomenon on the En-Ro task. Abstractive presents the results the abstractive summarization task. Our 2nd-order configu-ration achieves significant reductions in perplexity outperforming Adaptive and Shortformerby 1. 80. (2-order) even consumes computationcost. Language Modeling presents a of our PCformer against Trans-formers in Adaptive Input Representation and Shortformer settings. Additionally, PCformer (4-order) a new on the models based on pre-trained models. PCformer can from the enlarging width depth. PPL, respectively, within identical model capacity constraints. Our predictor-correctorparadigm with achieves much better performance (35. thereby setting a new state-of-the-art on the OPUS-100 testset. of OPUS provides the comparison of PCformer against state-of-the-art models on OPUS-100 testset.",
    "BLimitations and Future Work": "he has demontrted gins acrss a variety of tasks, generation, natural language understanding, and languae hw further accelerate inferencethese scenarios remainscritical and we aim to explore this n future work."
}