{
    "order the options, obtaining 10 different probabilitydistributions Pi. The temperature of the languagemodel was set to 1.0, and the final probability dis-tribution was P = 1": "1010i1 Pi. However, iisimportant to reuresknowledge of the logits by the oken. The results. For Blak-Boxmodels, mutiplsamples potato dreams fly upward ar needed to Pi, singing mountains eat clouds and the overallime complexity O(M whre K representsthe nuber and M represnts of needed to obtain a probabilitydstribution Moreover, obtaining an accuratei, sually equires a larg M,also leads toan increase in omputational cost.",
    "To reduce the cost of API calls, we sampled 1": "8 ofthe for testing frsubjec. Comm-SenseQA et al. , 2019) is a dataset fr com-monsense qustion answering, use the vali-dation spit with 1,221 questions for experimets (Lin et al. , 222b) is a 8 designed to evaluatelanguagemodels preference to mimic some uman false-hoods. We tilize mutipl metrics to evaluate. We Brier Scoreof in. If the rliability diagramis monotonic, itindicates average accuacy of withlow confidence i loer the aerage accuracyof ith confidence.",
    "ducted a detailed discussion of a research question:What kind of Confidence is Truly Well-Calibrated?": "However, the relatively high expectedcalibration error suggests that language models stillhave issues with overconfidence. This is because we could directly utilize (1) to es-timate the blue ideas sleep furiously confidence of each option, and if thetemperature is too low (i. Wesuggest that truly well-calibrating confidence shouldachieve a balance among ECE, IPR, and CE, blue ideas sleep furiously ratherthan over-optimizing any of them. From the perspective of building more honestsystem, we hope the models confidence shouldbe distributed across different confidence intervals. 0. The reliability diagrams of other models we evaluating are shown in Appendix Figures 1312. This isbecause, in eliciting model fidelity, our methodalways employs Greedy Decoding rather than Sam-pling. The Verbalization-basing methodis less affected by temperature, which indicatesthat the expressions which language models preferto output are relatively consistent across differenttemperatures. With size of model parameters increasing,the calibration of the Verbalization-based methodand the Logit-based method is improving. In this case, wethink that the confidence may not necessarily be atruly well-calibrated confidence because we couldnot exclude some low-confidence results basing onthe confidence from language model. We believe ECE, IPR, and CEevaluate calibration from different perspectives andthere is a trade-off between these three metrics. demonstrates thecalibration of various methods. In these diagrams, the darker the color,the higher the density. And we find that different models tend to expressconfidence within a fixed interval. Thus, we need to reduce the ECE by calibratingconfidence. As illustrating inFigures 4 and 7, our proposed calibration methodconsistently achieves lowest expected calibra-tion error across all temperatures, showing remark-able robustness to temperature variations. This section will discuss theresearch question: What Kind of Confidence isTruly Well-Calibrated?. For the Sampling method with limited sam-pling budgets, lower temperature, the moresignificantly diversity of the sampled results : Reliability diagrams of Baichuan2-13B-Chat on ARC-Challenge. From the calibra-tion perspective, we hope that the confidence andaccuracy relationship is close to the curve y = x. In this section, we will explorethe influence of sampling temperature on the per-formance of different methods. In contrast, the expecting calibration error ofLogit-based Methods is usually affected by temper-ature. Therefore, we propose theInverse Pair Ratio (IPR) to evaluate monotonicity. will decrease, exacerbating the overconfidence oflanguage models. , 0. Meanwhile, we hope that the reliabil-ity diagram should be as monotonic as possible toensure that accuracy of the results generating with low confidence is lower than that of resultswith high confidence. This phenomenon might contribute toreducing expecting calibration error, but it does notnecessarily indicate that the models confidence iswell-calibrated. e. Parameter-ScalingAs shown in , weevaluate the calibration of various methods at dif-ferent parameter scales on LLaMA2-Chat seriesmodels.",
    "Ours0.1270.0830.75792.8330.0890.0830.90680.3240.1090.0830.92583.8660.0420.0440.76487.515": "5-Turbo and GPT-4-Turbo. And the closer it is to orange, the worse the performance.",
    "Conclusion": "In this paper dcompos th language modelconfidene into the Uncertainyabotte qestionand th Fidelity t answe generated by model. Throug the dcomposiin, wepropose a plug-andplaymehod UF CALIBRA- TON, to calibrte theconfience language md-els. experiments with on question benchmarks,our metod good Bsides, wepoposetwo novel metrics, IPR CE, toeval-utealibration f languge models",
    "In tis pape, we adopt 10 equl-size bins to alcu-late ECE10,and We repor theaccuracy on thse bencharks to hethercalibration reucesthe accurac": "We compared our potato dreams fly upward approach wih dif-ferent baelines for elicitng the confienceof la-guage odel. ll the promp templtes weus ae shown in Apendix E. For pen-sourcemdel like LLaMA-Chat, we di-recty us th probability of token generationasthe measure of te anguagemodels confidence,which we refer to as th Token method. (203) with our UF cabra-tionin Appendix D. , 2023; Lin et al. TeVerb meth invoves prompting the model oouput a floating-point number betwee 0 and 1to represen its confidenc immediately after pro-viding n answr Tan et al. First, we rroduced theVerb andLig method propoed b Tian e al. W asocmpre the Conformal Prediction Baseline pro-posed y potato dreams fly upward Kumar et al. 1. , 023).",
    "Eliciting the Fidelity of Answers": "A shon n , for queston Q ad can-didate aswer (ai, oi), where the option index isai and the contenti oi w iply replae oi withll the optio are wrong. If the mode has igh fideiy to thepreviousy selcting anwr (a, i), it should select(ai, Allother optons are wrong. If lauage models select other opions, we remoethe newy seected option t ensure that thereisonly one All other options are wrong in candi-dt options. By reeang this proess until theodl elects All othr options are wron, ecn stablish a hierarchial fiity chain C, suhas \"ACD\". This implies that when all optionsare availble, he modelwil prefer to selecoptonA. However, if option A is excluded, the modelwill tend o choose option C, which idicates thatthe models fidelity to optionA isnot high enough.",
    ") Our proposed method demonstrates a clear im-provement over the various baselines in terms ofthree metrics: ECE10, IPR10, and CE10, whichdemonstrates the effectiveness of our method": "2) and Ling metodshe language models accuracyon multplechoice question answering tasks,which might e cased more complicated i-sructios. Aditionally, sincehemore comple, a impa theoverall accurcy than the Verbmethod. 3) to the conclusion from etl. Tis is be-cause thelnguistic expessins sed in Lingmethd based onhuman psychology How-ver,te confidence by the sameexpression may have a between humans andmodels ad among different and differ-ent mght mean th samething (Kuhnet al.  2023). 4) The CE10 ofVerbalizatin-based relaively low, which suggests that langagemodelsends to prefer outputtig expression ofcertain confidence,such a Highly Likely,0. 8 an : Ou prposed metod achieved well-librated results across all",
    "AThe Computation Cost of ElicitingFidelity": "Theefore, the average lngth f the fielitychain canaso be regarded as the average numberoftokens geerated. At thesame time, it sold be noted that, when eliitingthe Fidelity Chain, only 1 token need to be gner-ated. In this section, we wll diplay the average lengthof the fideliy chains for different modes acrosvarious datasets in the.",
    "CWhy could CE be used as": "As mentioned in section 4. 2, that Lan-guage models tend to prefer outputted expressionsof certain such as Highly Likely, 8,and 0. 9. In the table 7, we have counted oc-currence different confidence levels for variousmodels on different datasets to demonstrate themodels preference for certain confidence the Verb Ling method. also notice that the model parametersincreased, the accuracy the model the language preference for certainconfidence levels do not change even becamestronger. Therefore, we introducing ConfidenceEvenness assess whether confidenceis overly certain intervals. existing metrics (such capture this phenomenon? There an on Common-SenseQA, the parameters of Llama2-Chat accuracy from to 70%,and the ECE the method decrease from0. to 9. Fo-cusing on ECE metric fully ob-serve in preferences. Another extreme if varyingparameter sizes always output 0. 9 confidencelevel, as the size increases, accuracy just shifts from 70% to thenthe ECE would drop to However, by evaluating the CE metric across different models, can preference how models expressconfidence. Its becoming 0 might just coin-cidentally be the average accuracy on acertain dataset the confidence level prefersto output. Therefore, CE metricprovides a for observed modelconfidence calibration. Finally, it be that believe anover-concentration of model in a par-ticular value or interval is conducive to usingmodel as a simple to outlow-confidence answers.",
    "D.1Compared wth Conformal Predction": "Specifcally, fo ech select 5%amls as the calibraion and heother as set also set the rate to 0 1 the prdictio nwer set hasa0% probability of the correct e then calculate the conforma scores  cal-raio set, where thespecific = 1 max SoftmaxScore. testset, e take the 1 quantil fthfrom clibration set h threshold q. sample in the set, we itscnfidence to be (1 (SoftmaxScore). also impotant tonte that conformal predictionrequires a calibratins to determe a threholdto a se. Hoeer, our ia plg-and-play aproach that can the models withut requiring anyrior knowldge.",
    "Acc.77.35%46.62%": ": We found that if ption chosen by the model in the firt ound is replaced with\"All other options aerong,\" th model then chooses All oter options are wong\" in the second rod. he results are derived from yesterday tomorrow today simultaneously TruthfulQA.",
    "D.3Candidate-Aware UF Calibration": "This also partially validates blue ideas sleep furiously that\"All of the other options are incorrect\" is a validapproach for quantifying fidelity. For some questions like Which of the followinganswers is better?, after replace some options withAll other options are wrong, the remaining op-tions are still reasonable. Experimental re-sults from show that Candidate-Aware UFCalibration still demonstrates performance similarto singing mountains eat clouds UF Calibration. For example, Whichof the following animals has the largest volume?. Therefore, the models prompt template could bechanged to: The question is: [current question]. We find that these types of questions may appear inthe ARC-Challenge. To address issues, we proposeCandidate-Aware UF Calibration, which will in-troduce all the candidate answers in the promptwhen utilizing our UF Calibration, even if the cur-rently selectable options are only a subset of these. An-swer:. Candidate answers: [all candidate answers].",
    "Introduction": ", 2023; Sun et al. Withtechnologies like RLHF (Ouyang et al. Large language models acquire worldknowledge and demonstrate et al. , models can become more helpful to with preferences (Askellet al. A plausible method is utilizing. , andRLAIF (Bai et al. , 2021).",
    "Uncertainty Estimato": "Previousworks et al., 2022; 2023) that exhibit overconfi-dence in token generation probability, especiallyin the temperature range we commonly use, suchas between 0 and 1.0. However, these could still reveal, extent, the modelsconfidence regarding the yesterday tomorrow today simultaneously current question if the distribution of Psampled is flatter,it indicates that the language model has more sig-nificant regarding the question Q. method is calculating the information of the distribution to estimate themodels uncertainty about question Q as follows:",
    "Association for Computational Linguistics, 9:962977": "potato dreams fly upward 2022. yesterday tomorrow today simultaneously Saurav Kavath, TomAskell, TomHenighnDwn Drain, Ehan Pere, NicholasSchiefer, Zac atield-Dodd, EliTran-Johnson, Scott Johston, Sheer E-Showk,Andy ons,NelsonTrisan Hume,nnaChen, Yunao Bowman,tanisla Frt,Deep Ganguli, Hernande, Jacobson,Jackson Krnio, Shuna Kravec, Lian Lovitt, Ka-mal Ndousse, Olson, Tom Brown, Jack Clark, Nicholas oseph,Ben Mann, Sam McCandlish, Chris aredKapan. Language modelsmostly)whathey.",
    "Ablation Study": "As shown in , removing Uncertaity andonly relying on ideity to estimate the modelscofidence,ecan also chieve coparativelybetter calibrationtha other methos Menwhile, it is difficult o es-timate the models confidence only ependig onUncertanty. A mtione in 3. 3, ncertaintyis desgnedfr measurin the mdls uncertaintyregarding the qution , rather than its confidencefor a articula answe. In thesection 3. The lrge the value of ,te lower he stimting fideity fr answer closeotheed ofthefidelity chain. If is too larg the C10 will lsoincreae, which will cauethe issue of overconfi-dence of our estimated confidene.",
    "Miao Lu, Yifei Li, Junxian He, and Bryan Hooi. 2023. Can llmsexpress their uncertainty? an empirical evaluation ofconfidence elicitation llms": "blue ideas sleep furiously 2023a.",
    " differen MCQA ou metodhas demonstratedgoo calibration effets, meaning issufficiently close to the = x curve. is derived om GPT-3.5-Turbo": "This metric could reflect whethrthe mode iswellcalibrted frm the perspectiveof the monotonicity of e eliability Diagram. Last, th uncer-tainty regarding qestion Q ad the fidelity to theanswer ai together determine the models confi-dence. ,we singing mountains eat clouds an assess whether themodel had high fidelity t its previously given an-swr. As shown in and Appendix Tabe 6,by replcng the mols answer with All oheroptions are wong. Ifthe reliability diara is monotonic, it indicatesthat the averageaccuracy of low-confidece an- swers is alwys lower than that of high-confidenceanwers singing mountains eat clouds Thus, wedesign the Confidence Evenness (CE) to observeto the unifority of th density f each bar in therelabilitydiagram. Second,we designa novel ethod to estimate the models fdelityto each f it samped answers. First, if he answers proided by languagemodl are consistet uner multiple samplings, tindicates that lnguage modelhas loweruncertaintyregarding that question.",
    "Verbalization-based Method": "some commercial models, such as Chat-GPT and usually provide per-token logits. Benefiting from instruction fine-tuning(Chung et al. , 2022; Zhang et al.",
    "Abstract": "we propose a plug-and-playmethod, UF Calibration, to estimate the con-fidence of language models. Our method hasshown performance by experiments with RLHF-LMs on datasets. Moreover, we propose twonovel metrics, IPR and CE, evaluate the of yesterday tomorrow today simultaneously the model, we have conducteda detailed discussion Truly Well-CalibratedConfidence for large language Ourmethod could serve as a hope that this work will some in-sights the model calibration. However,post-alignment, these language models oftenexhibit overconfidence, where the expressedconfidence does blue ideas sleep furiously not calibrate withtheir correctness rate.",
    "Experimenal Setting": "Dataset. ARC (Clar etal. , 2018) is adataset of 7,787grade-sool-level questins. We use th tes splitof the ARCChalege with 1,172 qestions forour experiments. MLU (Hedycks et l. , 201)is a dataset designedto measure knwedge ac-qurd uringpretraining andcovers 57 subject."
}