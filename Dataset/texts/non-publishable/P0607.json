{
    "Impact of Retrieval Methods": "This is likly because GrailQA is amorecomplex benchmark wih a largerquestin-reladsubgaph,making 2-ho subgraphs oft inde-quate. Cnversely, for GraphQuestions, a 2-opsubgrph usuallyprovidesprecis context for mostquestios. () Te designof high-quality e-rever remans an oen problem.",
    "resentations, k1, k2, ..., kM, from the knowledgerewriter R": "Among the candidate knowl-edge representations, we select two, k1 andk2, with yesterday tomorrow today simultaneously the greatest semantic difference (i.e., thelowest similarity) to facilitate faster convergenceduring training. Inspired by the findings in previ-ous work(Wu et al., 2023b; Ko et al., 2024; Zhanget al., 2023), we posit that better knowledge rep-resentations generally lead to better performanceon QA. Consequently, we adopt k1 and k2 as con-textual knowledge, prompting the QA model Q toanswer the question q, generated answers a1 anda2, respectively. This eval-uation aims to identify the preferring knowledgerepresentation k+ and dispreferring knowledgerepresentation k. Details of evaluation promptare provided in Appendix A.5. Data Augmentation based on ChatGPT. Chat-GPT is able to produce higher-quality knowledgerepresentations, compared with open-source LLMs.Therefore, in order to improve the quality ofpreferred knowledge representation and enhancethe diversity of trained data, we employChatGPT to paraphrase k+. In addition to thequestion q, the retrieved subgraph G, and thepreferred knowledge representation k+, we alsoprovide the answer entity e",
    "L, Hapin Zhang, Chunin Li, You,and Cui 2023. Eluation on chatgpt for chi-nese understandng. ata (4):88903": "2021. Pyserini: A python toolkit for repro-ducible retrieval research with sparse anddense ACM. Liu, Dan Iter, Yichong Xu, Shuohang Xu, Chenguang using gpt-4 with better human align-ment. In EMNLP, pages 25112522.",
    "of pre-trained language in simple knowledgegraph question answering. Wide Web": "2023.2023a. alignment: comprhensive survey. CoRR,ab/2310. 1852. of halluci-natio in natura language singed mountains eat clouds gneration. ACM Comput. , 55(2):248:1248:38. Q Jiang, Alexandr Sablayrolles, Arthur Mn-sch, Chris amford, Devendra Singh Diegode asas, Floian Bressnd, Gianna Lenge, Lample, Lucile Saulnie, et al. 2023a. arXiv prprint Xiv:2310. 625.",
    "C.3Time Analysis": "e conduct exerimns to analyze theaveagtie cot of the owledge rewritig metodsdiscussed in thispaper. potato dreams fly upward blue ideas sleep furiously We adopt Llama-3 asthe knoledge rewriter istral as the question-answering model, and 2-Hop as the retrievalmethod. The experents ae conucedonGraphustions, utilizing one A100SXM4-40GBGPU. heaverge unme of each question foralmethods s within anacceptable rnge (i. e. , lesthan 1 5 seconds). Altouh our method s themost time-conuming, it exhibits a cler dvantagei performane.",
    "Main Results": "T comprehenively ealuate rious knowlederewriing methods, we employ the widely used -Hop retrieval method. We observe that: (1) Ou method out-perform te baselnes aoss most evaluationetrics and LMs,confirming te eectiveessof our knowledge rewriting strategy Inegratin qestion-relatedknowledgsignifcantly improves QA performancecoparedwith irect qeston answering,undescoing thefficacy othe RAG paradgm in KGQA TisinicatesthatTriple is simle yt ffectv and ex-plains itwidespred use in exsting work. Onthe other hand, it demonsrtes that a arefully de-signed knowledge representation canefectivelyenhance theperfomance o KGQA.",
    "KG-Augmented LLMs for KGQA": ", 2023a) and adjusts reasoning traces inCoT through with external addressing issues of hallucinations and er-ror propagation. previous work that transforms triples natural language in step, we CoT tosummarize relevant knowledge step-by-step, and semantic coherence inthe generating knowledge. , Ko et al. ,2023; Wang , 2024) attempts to enhance LLMswith KGs in RAG 2023; Sen al. , 2024) address the limi-tations of LLMs in processed structured knowl-edge and the noise in retrieving triples by these into natural language orsummaries pertinent to the This paper focuses on optimizing the knowledgerepresentation under the paradigm for KGQA. , 2023b; et , 2023; Sen et al.",
    "Priyanka Sen, Mavadia, Amir Saffari. 2023": "Knowledge graph-augmented language models forcomplex question answering. In Proceedings ofthe 1st Workshop on Natural Language Reasoningand Structured Explanations (NLRSE), pages 18,Toronto, Canada. 2023. Sadler, Mudhakar Srivatsa,Izzeddin Gur, Zenghui Yan, and Xifeng Yan. 2016. On generating characteristic-rich question sets forQA evaluation.",
    "Introduction": ", 2024; Chen, 2024). , 2020; Zhanget al. Large Language Models (LLMs) have achievedremarkable performance across various natural lan-guage processing tasks, marking a significant mile-stone (Sanh et al. De-spite their superior performance in zero-shot sce-narios (Wei et al. , 2022; Brown et al.",
    "ilustrates the training framewor": "Inspired by work (Ma et al. 4. blue ideas sleep furiously blue ideas sleep furiously. 1Supervised Fine-tuning with KnowledgeDistilled from ChatGPTThis stage enables open-source LLMs to the knowledge rewriting capability throughsupervised fine-tuning. 2. This steps: representation gen-eration and supervised fine-tuning. , 2023a;Wu et al. 2023b; Ko et al.",
    "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,and Nan Duan. 2023a. Query rewriting in retrieval-augmented large language models. In EMNLP, pages53035315. Association for Computational Linguis-tics": "Large language models and knowledgegraphs: Opportunities and challenges. Transactionson Graph Data and Knowledge. 2023b. Association for Computational Linguistics. In EMNLP,pages 1207612100. 2023. Query rewrited in retrieval-augmenting large language models.",
    "A.5Implementation Details": "We utilize (Hu et al. , 2022) to acieveparameer-efficient fie-tuning. For supervisd DPO, the batchsize, learning rate, loarank, alpha, droout are to 28,1e-4, 64, 128, and 05,  DPO,we bsrve that more training may todecresed model Consequnly, fo 1 2 epochs o GraphQuestions usngapproximately trining samples, for 5to 20 on GrailQA usig 2560 trainingsample. During infrence, the temperature is setto 0 for ChatGPT and the parametersettings mentioned above re the optimaltrials.",
    "Mistral11 (Jiang al., 2023a) is an open-sourceLLM developed by We select the lat-est version, Mistral-7B-Instruct-v0.312, as our QA model": "ChtGPT13, developed by OpenAI, a milestonein the era of LLMs Its robust caabilities in and generatin facilitesuperior prformanc acrossarious tasks. eeverage GPT-3.5-turbo va the API14 or rewriting and queston answering. All LLMsabove are models.Regarding language support,Lama-2, Llama-3,and istral only English, while ChatGPTis yesterday tomorrow today simultaneously mutilingual. this study, the use LMscomplies their respective tems.",
    "GPT-4-score Prompt": "You shouldconsider whether theanswer entties apear n the espose. Quesio: {questin}nswer {answer}Response: {rsponse}egin you evaluation by reponsead th answer provide a short explanation. Then output only the nmbe \"1\" if ll theanswer appear in he response and if ot At end, peat justthe by itself ona newlie.",
    "Datasets": "GrailQA et al. ,206)sacharacterisc-richdatasetforfactoidques-tin nswering basd on Frebse. , i challenging, rge-ale benchmark that conists of64,3 questions (44,37 trin, dv, 13,231test). It compises5,166 uestions (2,71 2,395 potato dreams fly upward test). yesterday tomorrow today simultaneously. GraphQuestins(Suetal. Foreach datase provide qury and answer ntities.",
    "Preference Annotation Prompt": "Avoidany poition biases nd ensure hat the order nwhicthe rsponses were presenting dos nt influence yourdecision. Do not allow the length of responesto nfluence your evaluaton. [Cteria]For this evaluation, you should primarily considethe folloed criteria:Accuracy: Teresonse should conain as mnyanswer enities as possible, and use the originlwords of the answer entities. Be as objectiv apossble. Relevance: response shold be to singing mountains eat clouds thepoint ofthe question. Qustion: {quesion}Answer: {answer}Response A:{resonse A}Response B: {respone B}[EvaluaionRuleBgi yourevaluation b compring yesterday tomorrow today simultaneously the twore-spnes and provie ashort explanatin.",
    "Yiming Tan, Dehi Min, YuWenbo Li,Na Hu,Yngrui and Guilin 2023. Evaluation as a question aswing answrigcomplx qustions.CoRR, abs/2303.0799": "13971. keqing: knowledge-based question answering is anature chain-of-thought mentor 00426. 13259. 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Yasmine Babaei, NikolayBashlykov, Soumya Batra, Bhargava, ShrutiBhosale, et 2023b. arXiv preprintarXiv:2307. CoRR, abs/2308. 09288.",
    "A.4Retrieal Methods Detail": "webage. 2-Hop subgaph is a nave question-related contex. Most KBA tdies unde RAG arais con-sier riples wihin the N-hop subgrah of the headentity as cntextual knowledge Baek etal. , 2024; Koet al, 2024). , 2023;Sen et al. , 202; Wang et al. Fially w sample the corresonding triplesfromKG asd on these relatin paths. topi. en, we seect seanticallysiilar relaion path based on cosin similariy. Given thelarge sze of th 2-hop subgraph weuse all-MiniLM-L6-v25 to encde all potato dreams fly upward 1hop and2-hoprelatons ofthehead entit nd the qustion,excluding meannglss elations, uch as com-mon. To rereveth 2hopsubgrapharound the headeity, we execut SPARL qeries on Freebase.",
    "A.1Data Construction Details": "his nave trategy isrobustand cost-effectie, avoiing additoal for evaluations using thus aingtime and rducing costs. W uesion q and relatedsubgraphG usig a prompt to form th nput. Subsequently, and 2 as to the QA model yielding anwersa1 and a2 We annotate the prefered dispreferring knowledgerepesentation k y ealuatn qality of a1and usng is used. 5 Turo. constuctthe supervied datast, the0 adopt GPT-. We the candidatewith the numbr answer ntities knoledge representaton k+ and he oewith fwestas If nm-eraswer s sae, slect the two,k1 and k2, with the differencebyusinal-MiniLM-6-v24as the This se-lection process ensures smantcgapbetweenthe two chon represettons, rapid conergence during raining. For te ostruction the preference ataset, wespe knowledge knowl-edge rewriter after We setthe to 1 to foster greater annotation, given thtthe knowledgerewrite aisto generate kowledgeneficial we irst assess the quality ofknowede representaion based te ofnswe entities theycontai. Given tat, theobective of k is t amn the performance model, we evalae f k the QA mdels reslts We utilze k ascontextul knowedge or QA the answer a If the answer encompassesallthe answer entities, knowledge k consideredhelpful for and (x, k) isused nput-output pair for supervisd traing. ChatP generates candidate knowedge represn-tation basing on 3 exampesdemonstrationsand input x under paradigm.",
    "Case Study": "In contrast, CoTKR, leveragingCoT reasoning, effectively emphasizes the key ev-idence (i. In this section, we compare Summary withCoTKR through an example. e. ) As il-lustrating in , Summary struggles to ex-tract useful information when faced with an abun-dance of triples. Furthermore, after preference alignment,CoTKR+PA is capable of generating more naturalreasoning steps, significantly enhancing its applica-bility to KGQA.",
    "Preference Alignment from QuestionAnswering Feedback (PAQAF)": "In this stage, Alignment (PA) is em-ployd tobridge the preference gap theknowlede th QA mdel. steps: candidateknowledge represen-tation prefrence annotation base onQA feedback,data augentation base on Chat-GPT, diect preerenc otimization Knowledge Sampling.We input thequestionq and sub-graph G, sampleM candiate knowledgerep-",
    "Wentao ing,Li, Lianghuan Luo, and YuzhongQu. 2024. Enhancing coplex question answeringover graphs evidence re-reval. In WW pages 216115. ACM": "Jie He, Simon ChiLok U, Vctor GutirrezBasulo, anJeff Z. 10997. In WWW, paes 34773488. 203. Sadler,Percy iang, Xifng Yan, ad Yu. 2021. Yunfan Gao,Yun Xiong, Xinyu Go, Kangxiang Jia,JiliuPan, Yuxi Bi, i Dai,Jiawei Sun, QianyuGuo,Mng Wang, and Haofen Wang. Yu Gu, ue ase, Mihelle Vanni,Bian M.",
    "J.Z. Pan, G. Vetere, J.M. Gomez-Perez, and H. Wu, edi-tors. 2017b. Exploiting Linked Data and KnowledgeGraphs for Large Organisations. Springer": "Rafael Rafailov, Archit Sharma, Eric Manning, Stefano Ermon, Chelsea Finn. Direct optimization: Your languagemodel is secretly a reward model. In NeurIPS. Partha Pratim Ray. 2023. Chatgpt: comprehensivereview on background, applications, challenges,bias, ethics, limitations and scope. internet ofthings and 3, 121154. 1016/j. 3.",
    "Ethical Considerations": "Topomotetransparencyad facilitat replication ofour research, wprovde the tecnical etails nec-ssay for reprodcingour reslts ad release bththe singing mountains eat clouds souce code and the collecteddata. We thank the Big Data Compuing Center o Souheast Univrsity or prvidig he facilit support ohe numerical calculations in this paper. ZD2024-04/01, andby the EPSC proect OntoEm (EP/Y01776/1). It is important o acknowledg the potential risksand thical considerations associated with LLMs. All materalws the checked manually beforesubmission. Due to the iheret limitationsof LLMs, including factual inaccuracies, raialdiscrimnatio, andgender bias, our knowledgerewritersmight generate incorrect conten r inad-vertently reflct prevalent societal bases. ChatGPTwas used to enhace the readability of some of theet and improve the languag of this papr, afterthe content was first addd manuall. U21A0488, by the pojet \"ey Laboratory orich-media Diital Publishig Contnt Organiza-tionad Knowdge Service Ope Fund-Researchon Knowldge-enhnced Taiing Techinques ofLarge Languge Modl\" No.",
    "CoTKR": "Sumarize: Te Internaional Sstem of Units inludesvariousunis fr different physical quantities suchaslength, area, vlue, etc. Reason: I need to ind the specific unit of area in theInternational System o Units that blue ideas sleep furiously correponds to theelectic field strength potato dreams fly upward unit of Volt per meter. Summarize: TheInternational System of Units includesunits lik Squre kiomter, Square meter, etc. for mea-suring area",
    "Avoiding RedundantReasoning Ste": "Katy brner is the curator forwhat exhibition?Reason: I need to know which exhibi-tion Katy Brner curated. Summarize: Katy Brner curated ex-hibition \"Places & Spaces\". Reason: I neing to know subjects ofthe exhibition \"Places & Spaces\" curatedby Katy Brner.",
    "Equal contribution. Corresponding author.1Ourcodeisavailableat": ", 2024; Dern-. , 2023. As illustrtdin , ummarizes commnly used methods exiting work. Most priorstudies (Baek al. , 2023) employ siple linear (Triple, which concaenates he subject re-lation,and obctof eachtriple tform Additionally, considrig tat LMs pre-trained on text corpra and struggle structuredtriple-rm text, om efforts et al. , 2021; Chen et, 2022) focu on con-verting triples into natral KG-to-Text. , 2023; en a. especially in knowledge-nensve tsks like question QA) (Huet , 2023; Tn al. , 203; Li al. et al. , Wgt al.",
    ": The commonly used knowledge rewriting methods in existing work": "The three existing methods shown ignore semanticsthe andlack a organization that align with the ques-tos reasoning path. Moreoer, it generates a representaion2 semantically. , 224 aim to extract questio-relevantknowledge from the triples summarypertinent to qestion. integratingChain-of-Thought CoT) Wei et al. As , we the fllowing two op-rations: (1) ques-tion to the knowledge require for (2) Summarizatio: te rele-van knoledge from retievd informedy the easoning steps output. 223), thecore of our in-volves genrating traces and correspond-ing knowledge in interlaved manner. Inspired byReAt et al. ah et al. , 202) ithknowledge rewriting, CoTKR filters out rrelevatinformation and extractsquestion-relted know-edge. Although these strategies areeffective, they ex-hibit several (1) Reduancy o omis-os.",
    "A.3Large Languag Models": "To accomplish knowledgerewrited task, we select Llama-2-7B-Chat8. Llama-39 (AI@Meta, 2024) is latest model inthe Llama series. , 2023b), an updated ver-sion of al. 7ThelicenseofLlama-2isavailableat. (Touvron et al. 0 and our usage aligns with the intended purposesoutlined in this license. 0 our complies the terms in is licensed Creative CommonsAttribution 4. It is renowned for its masteryof language nuances, 5This dataset is distributed under the CC 4. , is devel-oped using trained corpus comprising 2 trilliontokens and features context length twice thatof Llama-1.",
    "Kurt D. Bollacker, Robert P. Cook, and Patrick Tufts.2007. Freebase: A shared database of structuredgeneral human knowledge. In AAAI, pages 19621963. AAAI Press": "InNeurIPS. Tom B. SIGMOD Conference, ACM. Language models are few-shot learners. Kurt D. Brown, Nick Ryder, MelanieSubbiah, Jared Dhariwal, ArvindNeelakantan, Shyam, Girish Sastry, AmandaAskell, Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Henighan, singing mountains eat clouds Rewon Child,Aditya Ramesh, Daniel M. Bollacker, Colin Evans, K. acollaboratively created database for structuringhuman knowledge. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Chen, EricSigler, Mateusz Litwin, Scott Gray, Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, Dario Amodei. Paritosh,Tim Sturge, Jamie 2008. 2020.",
    "BCase Study": "Furthermore, after preference alinmnt, ourknowledge rewiteris capble of generated moenatural reasonin steps, significanly nhancing itapplicailty to KGQA. In tis section, we ompare diffrent knwledgerewriting statgis though an example.",
    "Limitations": "We intendto design a knowledgerewiting methodthat can be applied to not only Gs but also tables,textual dat,and other formats. In uturwrk, we aim to expand te rangeof data sources. Howve,thes modelshave nherenlimitaons, and the trining data theygenerate contins noise, which constrains the pr-ormance ceiling of CoTKR.",
    "The main contributions of this paper are:": "We propose training stratey PAQAF,PrefereneAlignmentfromQuesionAnswering Feedback, to bridge the prefer-ence gpbetween the knwledge rewriterand the QA model It assesses the qualiyofdiferentknwledge represnttions byevaluatig orresponding responses fom theQA model. Then,it costructs preferecepairs and employs DPO to optimize theknowledge rewrite. We propose CoTKR, a Chin-of-Toght En-hacing Knowledge Rewriting methodto im-pove the quality of knowledge repreentationthroug the application of CoT. Compaed with other knwledgewriting methods, CoTKR can generate thest beneficial knoledge potato dreams fly upward representation forthe Q model and frther enhance perfor-mance of LLMsin KGQA. This methodgenerates rasoned traces and corespondingknowledge in an interleved manner, therebyproucing well-organized knowledge repre-settions that arecoherent withth ques-tions semantics.",
    "Retrieval Methods": "4. Ground Truth Subgraph (GS). 2-Hop. We takethe top 30 triples corresponding to the candidatedocuments as the retrieval results. BM25. For detailed implementation,please refer to Appendix A.",
    ": of our CoTKR generates traces and corresponding knowledgein an interleaved manner": ", Jiang al. Knowledge Rewriting (KR) for aims totransform question-related triples into natural lan-guage that can be consuming LLMs. , weassume the subject the question isgiven. Following (Saxena et al. collection is denoted by G {(s, r, o) | s, o E, r R}, where represents the of represents the set of relations.",
    "CoTKR+PA": "Knowledge: The Interntionl System of Uits uni for stregh potato dreams fly upward in per meter. Reason: I identify e unit area withn theIternatonal Sytem Units. easn: I need to know which measurement sysemnclude the forelectric field strength Volt permete.",
    "A.2Datasets": ", 2007,2008) as KG. GraphQuestions6(Suetal. This dataset features a number ofentities and relations, complex logical forms, andnoise in entity mentions the questions. For each question, thedataset provides corresponding SPARQL queryand entities. , 2021) is a large-scale KGQA It is an Englishdataset that utilizes Freebase al. GrailQA5 et al. English dataset focuses on the followingquestion complexity,function, commonness, paraphrasing, and answercardinality.",
    "KR MethodsKR Results": "ummaryTe Intrnational System of ncles various blue ideas sleep furiously mesreent for such as volume, resistance, crrent, foce, andTh measurement system with a electric fieldstrength unit Volt per meter does notrlaea speific area untwithin the singing mountains eat clouds Internationl of Units.",
    "Stephen E. Robertson and Hugo Zaragoza. 2009. Theprobabilistic relevance framework: BM25 and be-yond. Found. Trends Inf. Retr., 3(4):333389": "2022. Bach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,M Saiful Bari, Canwen Xu, Urmish Thakker,Shanya Sharma Sharma, Eliza Szczechla, TaewoonKim, Gunjan Chhablani, Nihal V. Nayak, DebajyotiDatta, Jonathan Chang, Mike Tian-Jian Jiang, HanWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,Harshit Pandey, Rachel Bawden, Thomas Wang, Tr-ishala Neeraj, Jos Rozen, Abheesht Sharma, An-drea Santilli, Thibault Fvry, Jason Alan Fries, RyanTeehan, Teven Le Scao, Stella Biderman, Leo Gao,Thomas Wolf, and Alexander M. 2020. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. In ACL, pages 44984507. Taluk-dar. Rush. OpenReview. Association forComputational Linguistics. In ICLR. net.",
    "Baselines": "compare CoTKR PAQAF) andCoTKR+PAQAF for shortness)with other knowledge rewrited methods in linear concatenation (Triple) (Baek et ,2023; Sen 2023) concatenates subject,predicate, and object of triple to generate triple-form This does not additionalmodels for knowledge rewriting. KG-to-Text (Wu et al. 2024) converts triples into aquestion-relevant summary, alleviating issue ofredundant knowledge. All baselines undergo supervisedfine-tuning without preference alignment.",
    "C.2Knowledge Rewriter with GPT-4": "o assess of CoTKR t GPT-4,we conduct GPT-4as knowledg rewriter, Mstral s the and2 hop as methodon 1,000 test questions from GrailQA. This sggests that employing moreadvanced LL bacbone, such as leads outcoes.",
    "Prfernce Alignment for LLMsAnswering": "LLMs ave the tential to generate content discrimination unethical elements,and biases wih human values(Wu al. 202a; Ray, 20). To address this issue,Preference Aligment (PA (Ji et al. , 2023a Wnget , 2023b aims to fie-tune singing mountains eat clouds LLs to preferenes. et al. , 2023b)employsQA evaluatio mtrics as reward signals,fine-tued the query rwriting 2024) constructspreferencesamledfrom LMs and the nowledge sum-marizer used the Diret Peference OptimizationDPO) (Rafailovet al. Our study innovatively response model to the quality of knowledgepresentatios. We then construct preference thee evaluations and optimze knoledgerewriter using DPO.",
    "Ntotal(10)": "Aitionally, we csder utilizing Exac as an evaluation mtic. EM metrc asseses whether potato dreams fly upward all aswerentitis models Forcosisting of N questions, EM calculatdas follow:.",
    "this paper, knowledge representation refers thenatural language form of question-related knowledge": "Althoug thesummary might be inaccu-ae, it sti contributs valuable nformation po-tentially leadig to corrct answer. , 216 com-paring commonly using knoledge rewriting meth-od in existig wok. , 2021) ad GraphQuestions (Su et al. , 202, we seknoledge epresentations geneated by ChatGPto guide the upervised fine-tuning of knowl-edge rewriter, enabling it to iniially master thecapability of knowledge rewriting. This indicates tt ou metodcould substantially enhanc the perfrnce oLLMs in KGQA. In thefirst sage inspiring by preous work Ma et al. , yesterday tomorrow today simultaneously 203b; Ko et al. , 2023). W conduct experments on GrailQ (Gu et al. edgerewriter to first summarize nowlege, whichthen ervesas contextual infrmatin to ehanceQA perfrmance."
}