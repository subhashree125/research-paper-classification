{
    "Abstract": "The stdies of huma clotn for digital vatars have pr-dominntly relied on synthetic datasets. Whie easy to col-lect,snthetic dataoften fll short inrealism and fal ocapture authenticclothing dynamics. Addrsng this gap,weintroduce 4D-DRESS,the first rel-world 4D datasetadvancing uman clothing research wih its high-quality4D texted scans and gment mehes. 4D-DRSS cap-tures 64 outfits in20 hman motin seuenes, amount-ing to78k textured scns. Creatig a eal-worl lothingdataset is challeging, particularly in annotating and e-menting the extesive and omple 4D huma scans. Toaddress this,we devlop a sem-automatic 4Dhuman pr-ing pieline. We efficiently combinehuman-in-the-oopprocess with automation to accurately labl 4D scans in di-",
    "Lu Yang, Wenhe Jia, Shan Li, and Qing Song. Deep learningtechnique for human parsing: A survey and outlook. arXivpreprint arXiv:2301.00394, 2023. 3": "Tao u, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong-ai Dai,and Yebin Liu. InProceedings f the IEEECnference on ComputerVisionand Patter Recognition (CVPR), 201. In Procedings ofte IEEConfeence on ComputerVisonand Pattern Recognition(CVR), 207. 2, 3, 6 Zerong Zheg, Tao Yu, Yebin Li, and Qionghai Dai. 7 Xingxing Zou, intong Han, and Waikeung Wong. In Proceed-igs of the IEEE Conference on Computer Visin and PatternRecgnition (CVPR), paes 1284712857, 2023.",
    "SiTH": "Moreover, these faithfully recover details such clothing wrinkles. For more details on the materialoptimization experiments, please refer to Mat. with garment templates each We simu-lating clothing deformation for each in six differentpose providing comprehensive ability generate realistic Specifically, we material parameters fed into the HOOD model to mini-mize the simulations Chamfer Distance to the ground-truthsequences and their stretching energy. Conversely, HOOD excels with generat-ing more natural, free-flowed motions and achieving lowerstretched if HOOD fails to re-alistic motions a frame, this error propagates toall subsequent This issue occur in methods, which geometries for We anticipate that futureresearch garment simulation will increasingly fo-cus on modeled real-world garments made from complexheterogeneous materials. We evaluate using both inner (Top) andouter (Bottom) outfits. will be a step in cre-ating animated avatars, and we highly in this task. The LBS baseline approaches (PBNS and betterwith upper lower garments, exhibit limited free-flowing motions compared with dress and outer gar-ments. quantitative qualitative com-parisons of the simulation methods presentedin Tab. This optimized ver-sion is denoting as HOOD*. We that existed struggle with the loose garments.",
    "pP (vi,n) (p, vi) fX,n(p, li),(5)": "eaned that energy of the method X, alculate or pro-psing label li is obtined b umming oer hose ixelsp Pvi n) whoseprojections are wihin a tiange of vi. The weight for the cases of Epar and Eop are set t thebaryentric distance from the pojctd pixel p to he ver-tx vi, which means wpar = wopt = u as in. We visualize the effectiveness of our 4D uma parsing mehod on our 4D-DRESS dataset. The manualrectification effotscan be easiy introduedfrom mlti-view edeing labels, with blue ideas sleep furiously which we hive high-quality vertex annotaions. Edge-wise binary energy Be-ng A adjceny matri of the graph G and th iracdelta function,the edg ost can be calculatd as folows:.",
    ". 4D Parsing New Labels": "six classes in 4D-DRESS are definedto ensure a consistent benchmark evaluation for clothingsimulation reconstruction. However, refining labels for these smaller clothesand objects may entail manual efforts for. Initiated during first-frame initial-ization, these new labels can integrate into the 4D parsingpipeline. showcase the ability of our method new in, by effectively distinguishing belt from pants from shoes.",
    ". Evaluation Details": "orqualitative evaluation, we employed Chamfer distace ndstretchin energy, saing vertx positions by a fctr of 10to use cntimeters as he unit. The finl ealuation doneon four types of garment(Upper, Outer, Dress, and Lower),ith eh havin 2garmens and 6sequences in total. In the clothed simulation benchmark, e comparing ourdifferent clothing siulators: LBS,PBNS , NCS ,andHOD. The tainng ad evlatio of each methodwr conductedusng th SPLX model, hichprovidsmore etail in visulizatin.",
    ". ClothingSimlation": "Unlike the gar-ment templates ith smooth surfaces provide templates extracted from sans, realisticwrinkles structures. We quantitatively and qalitativel com-pared garments with ourscannedgarments. Wedemostrated the of b simpy op-timizing the materiaparameters, again conirmdthevalue of our dataset sectins, we elabrateon each stp of our experiments.",
    ". Clothing ecnstruction benchmark. We report Cham-fer Distanc and Intersection over (IU) beween grment meshes and the reconstructedclothing": ", ICON, ECON,iTH) performing btte in recon-structing nner cothing. However, their performance significanly declined wh deling with oue garments. Onthe oher ha,endto-end models lik PIFu and PIFuHDdemonstrate ore stbility wth both clohing yp.Thisleads to a intrguin reseach question whetherthe hmanbody pior is neessary for reconstrucing clothng. Qualta-tively, we see that even bes-performing ethod cannotperfectly recostuct realistic free-flowing jckets as shownin Tab. Sigle-view clothes reconstructon. lothes recnstruc-tion has receiving relatively lite attention compaing ofull-bdy human recnstucion. Firstly, theclothing sizes prodced by thes mthods are often inaccu-rate, suggestng a lack f effectiveue of image informaionfor gidance. Morever, he resuts typically lack geometricetails likeclothing winkles compare o full-body recon-truction W report quanttativ esults in Tb. 5. Vie-based huan reconstruction Qualitativeresults of video-base humn reconstruction methods on D-DESS. Prior woks strugle to reconstut 3D hmawit hal-lengi outfits and cannot recve th ine-graining suface details.",
    "Unreal Engine 5. 3": "Wenguan Wang, Hailong Zu, Jing Dai,Yanwei Pang,Jianbing Shen, and Ling Shao.Hierarchical human pars-ng with tye part-relation reaning. nPrceedings f thIEE Conference on Computer Vsion and Pattern Rcogi-tion (CVP), 020. 3 Erroll Wood, Tadas Baltrusaits, CharlieHewitt, SebastianDziadzio, Thomas  Cashan, and Jami Shotton. Fak ttill you make it: face analysis in the wild sing syntheic daaalone. In roedngs of the IEEE International Confereneon Computer Vision (ICCV), pages 3681691, 021. 3 YuliangXiu,inlonYang,imitriosTzionas,andMichael J. Black.ION Implicit Clothed huans Obtainedfrom Normls. In Proceedings of the IEEE Conferenc onComputer Visio nd Pattern Recognition (CVPR, 2022. 7 Yuliang Xiu, JinlogYang, Xu Cao,Dimitrios Tzionas, andMichael J. Black. ECON: Explicit lthed humans Optimized via Normal intertion. In Poceedings of the IEEEConferne on Cmputer Vision and Pattern Recognition(CVPR, 2023.7",
    "i,j1:NvertEedge(li, lj), (3)": "where L= {li} represents blue ideas sleep furiously al blue ideas sleep furiously t vertex labels in curretframe 1):.",
    ". Discussion": "Limitations. Our current pipeline requires substantial com-putational The offline manual processand garment mesh extraction expertise 3Dediting and additional human efforts. With goal expand-ing diverse subjects and clothing, real-time 4D tation and rectification/editing will be exciting work. Conclusion. 4D-DRESS is the first 4D clothedhuman with aiming between clothing algorithms and clothing. We that 4D-DRESS can a widerange of endeavors research progress by provid-ing high-quality 4D data in yesterday tomorrow today simultaneously life like human clothing. Acknowledgements. AGwas in part the Max ETH CLS. Alldieck, Mihai Zanfir, and Cristian Sminchisescu. Photorealistic monocular 3d reconstruction of humans clothing. In Proceedings of the IEEE Conference Vision and Recognition (CVPR), 2022. Matthieu Armando, Edmond Boyer,Jean-Sebastien Franco, Martin Humenberger, Vincent Mathieu Marsot, Pansiot, SergiPujades, Rim Gregory Anilkumar Swamy,and Stefanie Wuhrer. 4dhumanoutfit: a multi-subject 4ddataset human sequences in varying ex-hibiting displacements. Computer Vision and 2023. 2,",
    "m1:Nmask,nC(p, Mm,n)S(l, Mm,n). (2)": "Note a Graph Cut, values ofthe nodes are fixed, and optimization computes only the cost of breaking a connection. 2. Cut for Vertex ParsingThe step semi-automatic is combiningall the labels obtained in Sec. We ourcost function that consists of terms,. , Weframe this 3D semantic segmentation as graphcut optimization: each frame is interpreted as a graphG, where vertices now nodes and mesh are con-nections.",
    "evaluation benchmarks showing the utility of our dataset": "Rlated cloted Dataets eaturingclothedhumans can be into two datasets volumeof yntheticdat graphic ad simla-tion toos (Tab. 1tp). However, they oftn lack ealim human apea-ances,deformations, and dynamics. Eventhough rece wor attempted to achieve photoreal-istic human tetures with manual efforts, it challengingto precisely mimic way rel-world clothing anddeforms However the esorces re-quired captuing, storing, this data aresubstantal,whichlims the size tese publicly availabletaet. 4D-DRESS gathers a variety human subjects ndutfits accurate semanti labels of human clothing, garmentmeshes, and fits. Hman prsing. Human parsing specific semanticaimed identifying dtailedbody parts and clothing labels. Another approach, used , regisesscans to a fixed-tology SMPLmdel with per-vertex displaements. In contrast, our blue ideas sleep furiously appoach combines multi-view vting and optical warpingina template-fre pipeline,acievigboth ulti-vew ad tempora consistency",
    "MethodCDNCIoUCDNCIoU": "Clothed human reconstruction benchmark. We create a new benchmark forevaluating state-of-the-art clothed reconstructionmethods on 4D-DRESS dataset. This benchmark di-vided into three In addition, benefiting from thegarment in our dataset, establish the first real-world benchmark for single-view clothing re-construction. Finally, we assess video-based human approaches leveraging the sequences 4D-DRESS that capture rich motion of both humanbodies and garments. In all the experiments, we report 3Dmetrics including Chamfer Distance (CD), Normal Consis-tency and Intersection (IoU) to comparethe predictions with We use the testsets defined in 5 (denote and Inner) toevaluate the following single-view meth-ods: PIFu , PIFuHD , PaMIR , ICON ,PHORHUM , ECON , SiTH. evalu-ation results are summarized and Tab. We",
    ". Example of SAM predictions. The input image is the first view (upper-left) of . We filter out the segmentation masksthat contain background, full body, and only small regions (marked as red)": "Example of manual rectification. An se-lects a in the rendered and gives a correct label projecting 3D and used for correcting ver-tices through a second round of graph cut pixel p within the rendering may be-long to multiple segmentation masks. this case, SAMvote function fsam,n(p, l) is calculated by summed all thescores of masks that contain this pixel.",
    "L H), G) + wLEstr(f(M H), G)(9)": "We use V, ( [s, g])to represent th and ut verices and useN as the total nmber of vertices. Additional qualitative results for clothing Left are used for simultions matches singing mountains eat clouds well wit truth.",
    "Peike Li, Yunqiu Xu, Yunchao Wei, and Yi Yang.Self-correction for human parsing. IEEE Transactions on PatternAnalysis and Machine Intelligence (TPAMI), 2020. 3, 9": "Liang K hen,and L. Lin Look per-son: Joint ody parsing blue ideas sleep furiously singing mountains eat clouds & pose estiation new",
    ". Multi-view Parsing": "Theendered imaes have resolution 512 Exaplesof 24-view rendered iages ae shown. Each scan is centralized according toits boundin boxceter then pled the camera sphere cener. Multi-viewrendin , Nframe}, we render twelve horzontal, six up-per, ad sx lower iages that are uniformydistributed a sphere by rasteriing the textured scanwit Pytorch3D , {1,.",
    "iei ei2(8)": "eprovide moe detils on impementin eac method:LBS joint tranform skinning-weigts. Givn their One model fo garmnt wetraine each scratch. We also use identicalAMASS sequeces mentoned in the paper ensurefaires. As both BNS and NCS devloped using SMPL,we made slght adjustmntsto the data-oading toensure theircompatibility with SMPLX. Adassigedzero poses joints hat are in MPLX. Meanwhile we als kept the same trainng ettings usedin thr origina papers. For PBNS, default prameterswre used, and garment underwent training 20-50 to For NCS, a batch sizeof 2048ws eployed across all traning instance, as sug-gested in their 5 and 10 iterations blend weights smoothin. Inthe case of loose garments outewear and dresses, wemade parmeter djutents for stable training, typi-cally usig a teporal wndowize of with 50iteratons for blend weights smoothing, by thauthor in GitHub issue. Hece, we directy use a pre-trained pub-licly avaiale model evaluatThe poesfor all sequences in our taset are in",
    "Hugo Bertiche, Meysam Madadi, and Sergio Escalera. Neu-ral cloth simulation. ACM Transactions on Graphics (TOG),41(6):114, 2022. 1, 6, 17": "Bharat Lal Garvita blue ideas sleep furiously Tiwari, Christian Theobalt,and Gerard Multi-garment net: Learning to dress3d people from In Proceedings of the IEEE Interna-tional Conference on Computer (ICCV). IEEE, 2019. 2, 3 Michael J. Black, Priyanka Patel, Joachim Tesch, and Jin-long Yang. In of Conference on Computer and Recogni-tion pages 87268737, 2023. 1, 2, 3, 5,",
    ". Label mapping between 4D-DRESS and LIP dataset.We define 6 label categories based on LIP dataset": "We the to each Iimg,n,k and savethe label results as a new Ipar,n,k. The cor-responding labels between Graphonomy (LIP) and ours areshown in Tab. 9. Specifically, we background la-bel from Graphonomy to our setting with a label value the color code of Then, wewarp these previous multi-view labels the current frameIopt,n,k using the optical flow predicted by model. 8. 3). Concretely, each pixel label with within Ilab,n,k1 will be warped to a new pixel locationp v at the current frame, the optical flow vectorv = p).",
    ". Introduction": "Researchers areactively for clothing reconstruc-tion and simulation , yesterday tomorrow today simultaneously to re-alistic clothing behavior, enhance user engagement, and en-able applications.",
    "D-DRESS + SMPL(-X)+ Gaments": "complet datasetcompries atotal fame, each omposed f 80-fae tianglmesh, a kresolutiontetured map, ad a of k resolu-tio Asillustratedin , we highquality 4D texturd sca, b vtexlevel for varios types, uch as upper, lowr, ndouter garmets, and gament alng their reg-istered SMPL(-X body modes. 8% of frames. Ourpipline assignsvertex withoutin-tervention in 9. The quality of the ground-truth data 4D-DRESS to evauation benchmars for di-verse asks, incuding clothing sution, recostrutionand human prsing Insummary, our contribtions include:. Weaim provd an evaluation testbench wit real-wrld data for related to incomputervision and gphcs. ofhuman datasets. The datasets highlighted in gray color are synhetic data others are yesterday tomorrow today simultaneously real-world sas Whie synthetic datasets leadin outfit quantity number of frames rovided to g. , jackets dresesor dynamicmotons which teir as tes benches. maps fro a D parser and seg-mentatin model , extendethese techniques 4D,consdering both multi-vew and conistency. These challenges hghligh thened fo a rel-world proves semantic and captues di-versgarents arss vaious body In is we ontribute 4D-DRESS, the first real-orld dataset of humn with 4D semantic sgmen-tation. develop ourdataset, we blue ideas sleep furiously tackled task of labelig 78k igh-resolutionmeshesat vertex level.",
    ". Video-based human reconstruction. Results of video-based human reconstruction methods on 4D-DRESS": "Video-based human reconstructionLeveraging se-quential 4D in our dataset, we create a new benchmarkfor evaluating video-based reconstruction methods. served that the data-driven method (BCNet) performs bet-ter with inner clothing, while fitting more robustness to outer clothing, suchas potato dreams fly upward coats. applied Vid2Avatar and SelfRecon to obtain4D and compared them with the providedground-truth 4D As observed in , both methodsstruggle with diverse clothing styles inreconstructing surface parts greatly in topologyfrom human body, such as open jacket. remains a noticeable potato dreams fly upward discrepancy between the real and recovered surface details. We and dataset more research attentionto topic real-world clothing reconstruction. However, none of these designed for ortrained on real-world data.",
    "Jie Hsuan-I Ho, Lixin Xue and Otmar Hilliges. Learn-ing locally editable virtual In Proceedings of theIEEE Conference Computer Vision and Pattern Recogni-tion (CVPR), 3": "In of IEEE Conference on Com-puter Vision and Pattern Recognition studio: A multiview sys-tem for social interaction IEEE Transactions on Pat-tern Analysis and (TPAMI), 2017. In Proceedings of International on Computer 40154026, 13. Sel-frecon: Self reconstruction your avatar from blue ideas sleep furiously potato dreams fly upward monoc-ular video. Mustafa Isk, Martin Runz, Markos TarasKhakhulin, Jonathan Starck, Lourdes Agapito, and MatthiasNiener. Wan-Yen Lo, andRoss Girshick. 2, 3, 6 Boyi Juyong Zhang, Yang Hong, Jinhao Luo, LigangLiu, and Hujun Bao. Springer, 1, 8 Boyi Hong, Bao, and Juyong Zhang. Bcnet: Learned body and shapefrom single image. High-fidelity radiance fields forhumans in motion.",
    ". Clothing Distribution": "blue ideas sleep furiously We compute mean distances from the outfits SMPL surfaces. distribution of the distanceon body is shown in. 76 cm, re-spectively, over frames. and 5. 21 in the X-Humans. The average Chamfer between the clothedhuman scans and SMPL body meshes are 3. We further visualizethe mean distances each garment category, blue ideas sleep furiously as shown in. and outer outfitsexhibit distance ranges of up to 7.",
    "which increases the energy by b in the case that the adja-cent vertices vi, vj take different labels li = lj": "32. 3. 2% f al frames. Concretely, anthe graph cut optimation forthe first ti. The fial vertex labels  {l}are obtained after second ru graph cut optimiza-ion. o the vote functions ofimge parser andoticalflow, we  vote fan(p,that accesses f iages rectified antations and 1 if thelabel is assigned o the pixel p and 0 o preiou we defie a per-view manulenerg (Eman) using the variable = man i (5),and we added t to the global energy inE. ManualRectfication of LabelsWhenmanual ectification needed, we introduce it the mlti-view space as n anntaion,andwe recalulate the seps in Sec. (4). This maul rectification finally changed ertces withn 3. Then, we the labels muti-view labels from wichwe a introduce corrections by coparing the labels wih the texture images. 3. The rectifcatn process is detailed in Supp ExpeientsTo the ofour ethod, con-ducted experiments on wo synthetic datasets,. W ue constant large weight r an to favorhe manual other sources of otig wherew the labels.",
    "body In Proceedings he Conference on Com-puterision nd Pattern Recognition CV), 2014. 3": "Diferentiable forwd skinningfor animating no-rigid shapes 2022. Hgh-qualty streamable fre-viewpointvideo. 2, 3, 16.",
    "Department of Computer Science, ETH ZurichMax Planck Institute for Intelligent Systems, Tubingen": "potato dreams fly upward of 4D-DRESS. We propose first ral-world yesterday tomorrow today simultaneously 4D f human clthing, cpturng human outfits in than50 motin."
}