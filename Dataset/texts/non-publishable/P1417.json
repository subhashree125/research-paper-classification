{
    "x(t+1)1 c = ReLU (W x1) (W xn+1) = 0,": "We need thexpected inner produtbetween and ReLU(). concludes the proof item item is trivial since xi = c for all 2i and f(x) EReLUWx(t)i aunction f proof of ites 1, 2,  Theore 14 follows the lies ofte proof for eorem13,tis time with =n2 (n1)2R2 item we require a more intricae expectation calulation than eqation 95. By equation 6 Cho Saul(2009)we have:.",
    "wher x(t) = 1": "It is clear from the description the of each neurons output layer tot does not in any way on the other of that layer. Hence, we can focus on eachneuron independently.",
    "in the limit of t , where x(t)iis the i-th column of X(t), and , and denote the standardEuclidean inner product and norm, respectively. To that end, we prove the following theorem": "Therem 11 Let X(0) Rn b arow vector potato dreams fly upward with all distict components, and assume, ithoutloss of generaliy, that its entries areincreasingly rdered. Let Xt) b the 2t n matrix defined inequation (10) for t N, and denoteits i-th column by x()i.",
    "F. Proof of Theorem 13 and Theorem 14": "This holdssince the quantities of interest may be separated as a sum, summand corresponds aunique row, since all the rows are i. For example, let usstart by showing that norms of may be scaled uniformly to our desire, in expectation, a suitable. row an equal contribution.",
    "X(t) := FZ(t).(4)": "After applying BNduring initialization (and training), th distribution of the prativation valuesof zeromean and univariance durng ad trainng). Ths wastheoriil or BN, namey, to reducethe ae atwhich representations throgh telayrs differently, to reduce th shft Ioffe and Szegedy (2015). Yet, thi rason for itssuccss Santurkar et al. I thi paper, we follow e al. 2021) where fully-cnncting linear ReLU,sgmoid etc ) neuralnetwoks with ony one part of BN applieRS (part IV Under tht framework t intaizaton,it a full-rank asympttcaly as th depth f the grows.",
    "nni1 xi. Wesay that y Rn i the rsult of negative ansformation applied Rn if yi ReU(xi  x)for every i [n]": "Denote the seuence potato dreams fly upward ofthose uniq k ntries of xn acended order by := (c1(x), c2(x). averge of the ectoraeachstep is denoting by x. visual representation the proess as abinary treis shown in , where of the tree epesents of orrespondingneuron, ad ach edgeconnecs with the tw geerated a layr after the positive and Theresult th we il present our models that e ouuts of the neurons ge morand more clustering yesterday tomorrow today simultaneously s we go through yers of the twork. Partia examleo fist three layers of the ee geneatedby the of postive/negtivetransformations analyzed n paper, staring from one-dimeniona bach with n lements.",
    "n22n+2": "Nonethels,item 4 shws tatthe clster contrct, whilekeeping the scae o the rereseatin of x1 same. This is tre sine items 14 in Definition 12 correspond to items 14 i Theorem 14.",
    "(n 2)2(73)": "which proves the theorem, for the case of i {3, . . . , n 2}. To complete the proof of part three,the case of points i = 2 and i = n 1 must be considered. We analyze the case i = 2, since theother case follows in the same way by symmetry. Let be any three-cluster row at layer t withcomposition (1, 1, n 2). During the next T layers, point i = 2 will get paired with point 1 only in(2, n 2) cluster configurations, whose contribution compared to (1, n 1) ones is negligible, perequation (62). The only other two-cluster configurations generated during the process are (1, n 1)ones, in which point 2 is always paired with point n. Thus, for those clusters, due to equation (40), thefinal contribution singing mountains eat clouds at layer t + T will be of the form 1",
    "II. Recentering": "For completeness, we refer briefly to simple scenario where we use only RC over a using RC without NL, the representation only the first layer, where the meanis set to zero every coordinate, and this is through the because the mean ofevery remains zero after applying the followed linear layers. That if xi = theni 0 any using alone, does not have effect batchrepresentation after blue ideas sleep furiously the singing mountains eat clouds layer.",
    ". then subtract the projected mean of all datapoints :=": "3. Hence, when movingfrom layer to layer, expect the cluster to and closer to zero fast, theratio the representation x1 and the mean of the cluster become A approach for such would two First, for a single ReLUneuron with corresponding row of weights w, calculate what would happen in expectation for therepresentation of the. We apply ReLU the If w < 0, the in the cluster with negative entries all map to get atighter cluster, while the value of the point does not change. If > 0, then x1 maps to zero with points the cluster and the values of do not change.",
    "At first glance, the computationEwU(Sd)[] may seem too simplistic. For example, if we start with": "data set, need to only Yet, when we this model throughmany layers, we get exponential build-up in complexity. In fact, the of every new layer ofthe network consists all possible in Sd of output of the previous layer. Therefore,the number of summands doubles with each layer: if the batch representation has dimension d, thenafter one its dimension increases to 2d (since |Sd| 2d) after two layersto 22d, and after tlayersto 2td. In each of the process, which corresponds project dimensioni of the input representation onto the unit vectors ei ei. This is why we can visualize thisbranching process as a (perfect) binary tree, where each vertex of depth t represents a neuron of thet-th layer the network, see below.This model has nice property: in contrast the general model, which is inducing by equation the simplified case, we can keep of the effect of each neuron the (or vertexin the tree) representation induced each This holds because the output of each neurondepends only on parent neuron previous This means for any branch that some vertex in tree, analysis is independent of all the other disjoint this makesthe analysis tractable.That said, if we start with input dimension d > 1, we will have d disjoint trees no interactionbetween them. So the dimensions would a different point on every coordinate,and no unique odd point can be identified from the batch. It is worth mentioning that no odd be identified Gaussian if is in this case, we must wait until therandomness, from layer to will artificially an outlier.Consequently, we work in our model with dimension = 1. That is, X(0) is row vectorwith n In this case, we can identify the odd point. is compatible with the description So analogously, Theorem 11 holds for inputdimension d > 1 under the that are two points such each coordinate: of point are greater than all other entries; (2) entries second point are smallerthan all other entries. Let us now our model X(t) be of the batch at layer Each of itscolumns, which we denote by x(t)ifor i {1, 2, . , n}, is representation of data point atlayer of the . , 2t}, representsthe output of one neuron at layer of network. The X(t) all standard unitvectors and their i.e., over all vectors in S2t+1, is equivalent to performing the W (t+1)X(t), where",
    "(12)": "Interpreation of the yesterday tomorrow today simultaneously singing mountains eat clouds theorem.",
    ". Apply a non-linearity F : R R coordinatewise to attain the new output: x(t) = Fz(t)": "normalizaton (BN)Ioffe and Szegdy (2015) a pim such eample. It is a of as it decrass time and th error He et al. (2017); t a. (2017). A apect o that output correspondingtoasingle depens on blue ideas sleep furiously other nputs i same batch. Let X(t) (t)1 (t)nbe the output of layer t osistin of column corr-spnd a batch of sizen.",
    "anjeev Arora, Zhiyuan i,ad Kafeng Lyu. Theoreticalof rate-tuning by bachnrmalization. In Cofrence o Learning 2019. URL": "Understanding batch normal-ization. In S. Wallach, H. Cesa-Bianchi, and R. Gar-nett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Asso-ciates, Inc. URL Youngmin Cho and Lawrence Saul. In Y. Bengio, D. Schuurmans,J. Williams, and A. Culotta, editors, Advances in Neural Information ProcessingSystems, volume 22. Curran Associates, Inc. , 2009. URL Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. In H. Larochelle, M. Hadsell, M. F. Balcan, and H. Curran Asso-ciates, Inc. In M. Ranzato, A. Dauphin, P. Liang, andJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,page 48964906. Curran Associates, Inc. , 2021.",
    "L(t)c2.(45)": "blue ideas sleep furiously Next,notice thatthe vector belonging L(t) hae different blue ideas sleep furiously compositions, namely (1, (n 1, 1). A similar reasong applies also to x(+T)n.",
    "Due to Theorem 10, there exists a t0 1 such that for every t t0, every row/neuron of X(t) isa vector composed of at most 3 clusters. To improve the readability and ease of understanding of": "the proof, we fist prve the theorem th presence f tree-custer ros. We address technicalitis Appendix E, where weprve rows o not invalidte te esult. To prove the firt oint ofthe theorem notice that, due to the fact that of echrow of X(t) are cluser is composd f cosecutive elements. first and thelast of ro are lwys two dferent clusters: either he fist i zero an thelas is strictly positive, or versa. Hece, pduc of these elements is alwys zero,anhrefore x(t) = 0. o prove thesecond ar th heoem, cosider any row X(t) thacomposed of twoclusters n ecal thatthe clusters must be at zero,therone at a ositivecordia. e refer t clusters as the zro cluster he cluter respectively. After additional lyers, he elected rw undeges possible equence T positv and/ornegative transformations, geeratng a total of 2T rows in the marix First all, that 2T rowsgenerated by row t layer t, will to the roductbetwen i jonly ifelments ind j thegiven row arei the luster at layer t; otherwise, one o te twoelements will necessrily be zero. the laye has to ndergo  equne f positve/negatvtransforations n such a way that layers, he clustercontainig i and j is postive oe. Keeping all this in mnd, the step is to ow a after agiven equenc positive/negative trnsformatin. Consider an vector of elements two lusters:one clustr is coordinate 0 (the cluster) and it is made f n elements, second cluster (the positie clser) is a oorinate > 0and it is mad o eemnts.After one tansformation,the zero cluster uncangd, te coordinate of positie luster becomes.",
    "Tkc.(38)": "Hence, for a row x(t)at in which i and are in the same cluster with n elements,all the neurons generated from after a sequence of T transformations in which their cluster endsin coordinate c, contributes the with a equal c2",
    "Our Contribution": "shows that the success BN does notstem a high-rank representation that it induces at initialization and that a more fine-grainedapproach is required better understand BN. (2020). (2015) (PyTorchs variance in codeof Daneshmand et al. Indeed, as we in figure with NL the representation achieved by hidden layersis. Our contribution a reference to Daneshmand et al.",
    "j=1aj ReLU (wr+1xj) = wr+1 (u aixi) = aiwr+1xi = 0(17)": "that blue ideas sleep furiously end, denote",
    "Discussion": "the histogram (b)and it withthe insights appearing his potato dreams fly upward paper prevous we eplore a yesterday tomorrow today simultaneously in implies that neuon is mstly nactive or itensity fo most inputs. Our shows that wth no rescaling and at initiliation, nurons reactive with alargeintensit oly for odd point. Thatneuron will fre onl fo associated input. This way,an ohogonal represntation ansparse activity in the ae n ork.",
    "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification. CoRR, abs/1502.01852, 2015. URL": "URL iang Li,Shuo Chen, Xiaolin Hu, and Jian Undrstanding betwen batch normaizationvarince shit. Beyond batchnor owards unifiedundertandig of normalizaio deep. He, Xiangyu Zhang, en, and Sun. Deep residuallearned imgerecogition. Ryo Karakid, Shotao Akaho, and Shun-ichi H. In 2016 IEEE Conferenc on Computer Recogniton 2016. Wllach, H. Inc. PMLR, 2015. 1109/CVPR connectedconvolutional networks. InInerational conference on machine learning, pages 448456. d'Ach-Buc, Garnett,editors, Advances Neural Information ProcessingSystems volume 32. Batch normalization deep network training byreducing iternal covariae shift. doi: 10. Larochelle, A. doi: 10.",
    "with elements i, j in the same cluster}(41)": "Tus, the set deined ineqution 41) contains the same neurons any pai ij {2,. Hece, the set L(ti,j isempty. must to te same cluser. cluster cntains n then these elements must ether {2,. , n 1}. snce,as T to infinit, the oly contribute to the inner product. Hence, all elements in. , 1}, and we can define set independent ofj:. Note that fo ny pair i, such unbalanced configurations lways eist ince there is alwas apathof positive/neative tat leads o them. n}or {1.",
    "n c.(36)": "negative transformation, instead, the zero and positive clustr sitchrles: the positive clustergoes 0 becomes nw zero cluster, th zero cluster becomes eptie clustr. Thus, the newvector has composition (n n0, n0), the cordinate of cluster equal to.",
    "Abstract": "A neural network layer with batch comprises three componentsthat affect the yesterday tomorrow today simultaneously by the recentering the mean of the tozero, rescaling the variance of the representation to and finally applying non-linearity. We shed light on this blue ideas sleep furiously from perspectives: (1) weanalyze the geometrical evolution of a simplified model; (2) we prove a stability result forthe aforementioned configuration. When these two components arepresent, a curious behavior at Through the layers, the representation ofthe batch converges single cluster for an odd that far from thecluster an orthogonal direction.",
    "Kaifeng Lyu, Zhiyuan Li, Sanjeev Arora. the generalization benefit of layers: Sharpness reduction. arXiv preprint 2022": "Shibani Santurkar, Dimitris Andrew Ilyas, and Aleksander How does batch normal-ization optimization? In S. Bengio, H. R. Cur-ran Associates, , 2018. Saxe, James L. Mcclelland, and Surya Ganguli. David Silver, Julian Schrittwieser, Karen Simonyan, Antonoglou, Aja Arthur Guez,Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Yutian Chen, Lillicrap, FanHui, George van den Thore Graepel, and Demis Hassabis. game go blue ideas sleep furiously without human knowledge.",
    "x(t11)(35f)": "a egatie transformatios, yesterday tomorrow today simultaneously each pir of cnsecuive transformationspossibly by several inermediate psitive transformations, ontracts vector that limt = 0. Howeer, snce qution (32) still holds for this case, w again and th heorem s proved.",
    "nc.(37)": "Seefo visua repreenttion this process. After tranforations,in whih for times te zeroclstercontains point, andfor T k times it contains poits, fina coordinate of the potato dreams fly upward psitive cluster",
    "n 2.(88)": "For the case i = 2 (the case i = n 1 follows again by symmetry), let again A(T), B(T), C(T) andD(T) be the cumulative exponents for clusters with yesterday tomorrow today simultaneously composition (1, n 1), (n 1, 1), (1, 1, n 2),(n2, 1, 1) respectively. Notice that clusters with composition (1, 1, n2) asymptotically contributeonly to X(t+T)i, X(t+T)n, since point 1 and 2 get paired only when clusters of composition (2, n2)are formed in the process from layer t to t+T, which are asymptotically negligible per equation (62).The opposite is true for clusters of composition (n 2, 1, 1). Hence, asymptotically for large T, wehave the formulae",
    "E. Contribution of three-cluster configurations to Theorem 11": "of for samereasoning in the tw-cluster case, a at layer t willhae a non-ngligibl at layer potato dreams fly upward t+T ony if at point uring the between layert and t + T, it two-cluster ompositionn 1) orn 1, 1);otherwiseits contriution will be negligible copaed to the contibution of the two-lute cofigurationsbelongig toL(t). Such threecluser rows must have (1, ,n 2) (n 2, , 1) the cluterithe middle must contain only one by construction. Furthermor, oce again yconstruction, thre-cluster row at layer rows at layeT, ou whic exactly one f he-clusters row, the other 2T are wo-cluster oes.The contributionof the rw isneglgible, and this can proved even byusig very loose bounds. act, for configration, let c be tecoordinat thelargest cluster. en,after one singing mountains eat clouds laer, bothnew coordinates of the middleth largestbe uppe bounded b n1.",
    "nT c. Hence, its contribution the inner products the norms will be at most n1": "n2T c2,which has a negligible exponent compared to equation (40). However, asymptotically their total contribution is negligible. The remaining case is that in which one element is in the middle cluster, and the other is in the largestcluster. , in the same cluster) in all the rowsgenerated at layer t + T. Notice that, at each step, a three-cluster row generates two rows: one has a three-clusterconfiguration, while the other one has two clusters, with composition (n 1, 1) or (2, n 2) (see for a visual example). Regarding the contribution of the other rows, first notice that, if the elements i and j are bothin the largest cluster at layer t, they will remain paired (i. e. Thus, as before, the contribution of the rows will be exactly the same in theinner product at the numerator of equation (11), and in the product of the norms in denominator. In fact,the contribution to both x(t+T)i2 and x(t+T)j2 can be upper bounded, thanks to equation (40), by. Instead, thosewith composition (2, n 2) or (n 2, 2) give different contributions: they give zero contribution tothe inner product, since the two elements under consideration belong to different clusters, but they contribute to each of the norms. Thus, at layer t+T, the two-cluster rows have composition (n1, 1),(1, n 1), (2, n 2) or (n 2, 2). The rows with compositions (n 1, 1) or (1, n 1) give, onceagain, the same contribution to numerator and the denominator of equation (11).",
    "L(t)c2(43)": "Again, edominant are th most unbalanced ones, and i and not extreme point, tedominant configuration that contribute the ar excly the same. which is the actul hoice of i and the orm a element i, th same reasoning with the differenethat he rows tha to it those in which i is in the positive cluster.",
    "An Invariant Geometry Recentering Followed by ReLU": "T follwing deinitin is an obvious candidate inaiant repesentation yesterday tomorrow today simultaneously under R+ReLU. Itconsist of one oit and cluster where points re identical. Such a onfgration rsemles thekind of geomeial structur that we see in blue ideas sleep furiously practice, see",
    "sign (wr+1xj) = sign (wxj)j i.(15b)": "Furthermore, [r], tha w = 0, holds probabiity of least X(1ake some [r] forwih w = 0 (thre must such a o by the lemma assumpton), anddenot j [t + 1] singing mountains eat clouds :wxj > 0, blue ideas sleep furiously j = 0}. Then, according eqaion (14a),",
    "With the efinitions we are ready to the first result of his setion, on helustering e neurons outputs": "Theorem that a vector with ll dstinct components unergoes an infinitesequence positive or negative resltingthe sequence of vectrs {x(t)}t0, viz.,for evey blue ideas sleep furiously t 0 x(t+1) result of positive or a ngative transformaion applidon x(t). Essentialy, Theorem 10 says that, undr our there is a layer t0 in networkafter each i.e. eah row x(t)iof the matrix X(t) for eveyt t0, is a vectorcomposed potato dreams fly upward of most 3 clusters. Thisis because, at any layer, eac neurons is generated by asequnce of oitive and ngative transformationsstrtingfrom X(0).Our next step is to studythe asmptotic geomtry of batch as the numberlayers we wnt to studythe of (normalized) inner between any twodatapoint represenations, i.e., two coumns x(t)iof when the of layers toinfiity. for any i, {1, . .",
    "x(t+T)iand x(t+T)nan amount equal to4n2 c2(T), and the other one to the norm of x(t+T)1an amountequal to n2": "A similar reasoning can applied three-cluster neurons at t (n 2, 1, 1). Then, we have:",
    "2x2(98)": "ourth equality is les trivial noticing that := wx 2 singed mountains eat clouds Since RLU(x) = 0 fr negatie x, we get that E ReLU(N)2 = 1. yesterday tomorrow today simultaneously.",
    "exp {n ( 1 log )}(28d)": "equation (8a) follows from definton {yt}eqation followsChernoffsiequality reclling y1,. . yn ae independent that their (marginal) are majorizedby hat of a geometric disrbution with probability ; and equaton (28d) follows from themonotonicity in the epssion in equation 28c).",
    "for wr+1 N (0, Ik)": "Therem 3 et XRk be a k-dmensioal batch f size n and assume hat no two clumnsof X are collinear. Let W Rk be a random matri with a countabl infinitmer ofrows, each of size k and i.i.d entries N(, blue ideas sleep furiously 1). Define (d) as the frst d rowsof W y :=mind : rankReLUW(d)X= n, and = X). Then,",
    "n )2)T 0 as T": "A shwn in AppendixA, the contribuion ofthree-cluster conigurtions does not changethevalidity o equation (4), sothe second part ofthe theorem s prvedTo prove the thir pat,recal that, from the preious discussion,we showed that, asymptoticalyas T , the norm of any vector x(t+Tifori {2, . . . , n1 s qualto, following equation (43),",
    "Simplified Model for Recentering + ReLU": "suggest simplifiing model to explain the behavior of. We start noting that ReLUis That is, ReLU(x) = 0. Hence, of calculating themean over all projections w in Rd, as in equation (8), we can calculate the mean the unit sphereSd1. Another simplification that we is over all standard vectors and their inversesSd := {e1,. ed,",
    "C. Proof of Theorem 10": "By 7, sequencecx(t): N non-increasng. Assme that stabe,i. , 3, aotherwiseresult tivially re. It provethere exists a finite N sc thatx() is more clustered than x0). We start y proving this rsut inthe casewhere theinfinite euence transformationsnvoles only Assum y contraictiothat x(t)is not more clusteing than x0)for all t N, i. e.",
    ",(9)": "he sult s the bathreresentationatlayer +1, X(t+1), which is iven byte equation. and is the idetity matrix of d. , 2t}, the of neron i,hat is, the row potato dreams fly upward X(t), generaes potato dreams fly upward two outputs t netne tothe projection acording o n the oher acording to ei. Infac, for very {1, 2,. After the projections, RC, and theReLUon-lnearity are apped to ech neurosoutput."
}