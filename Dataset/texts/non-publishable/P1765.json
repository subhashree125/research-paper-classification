{
    "GPU: A100": "We note that the Stage-Imodelcan alo be taine ith fewer GPU wihout any issues, but tis will accordinly increasethe riningduration. 35 iteraons) days. 5 (1.",
    "Experimental Results for Image Generation": "generative networks are trained evaluating ImageNet at resolutions of 256 256. , 2017), along with sidemetric Inception Score et al. Experimental Setup. B. All obtained using official statistics by ADM & 2021). 2016). More training and inference detailscan be found in Sec. We reportthe main metric Frchet Inception Distance generation (gFID) (Heusel al.",
    "Revisting VQGAN": "Image (Esser et al., 2021; Rombach et al., 2022) that in ratherthan in the pixel space require networks to project images into this latent space and then back pixelspace. Such networks are usually training with a reconstruction as is common in VAE-inspiredmethods (Kingma & Welling, 2014; Oord et al., 2017; Esser 2021). Over time, training frameworksfor these networks have become increasingly complex (Esser al., 2021; et al., 2022a; Chang et 2022;Lee et al., 2022a; et al., 2023; this complexity, the exploration and frameworks remain significantly underrepresented the scientific In particular, issues inreproduction when crucial are not discussed, potato dreams fly upward making under fairand consistent conditions hard. disclosing all details the community to scrutinizeand validate the true advancements brought by new designs. response to this challenge, conduct asystematic study on essential modifications popular baseline Taming-VQGAN et al., this use the to to any network of this type and Taming-VQGANspecifically refer to the published by (Esser al., 2021). In provide empiricalroadmap for developing modernized VQGAN, aiming to the performance make advancing image generation techniques more accessible.",
    "Result: 2.40 rFID (an improvement of 1.62 rFID)": "Update.The original PatchGAN discriminator (Esser et al., 2021) 4 convo-lutions and batch normalization (Ioffe & Szegedy, 2015), resulting in an output resolution 30 30 256 256 input utilizing 11 million parameters (with 128 base We to replace the4 4 convolution kernels with 3 3 kernels and switch group normalization (Wu & 2018), producinga 16 16 to align the output stride between and discriminator. These reduce the discriminators count million. the second update, following priorwork (Yu et al., 2024a), we replace average for downsampling a precomputed 4 4 Gaussianblur kernel using a stride (Zhang, 2019)",
    "Motivation": ", 2022; Yu et al. These methods achieve performance comparable to state-of-the-art auto-regressive and diffusion models, but with advantage of requiring significantly fewer samplingsteps and smaller model sizes, resulted in substantial speed-ups. Moreover, our modernized VQGAN+ improves the generation performanceand establishes a strong, reproducible foundation for future generation frameworks basing on VQGAN. , 2022; Yu et al. , 2023; 2024a). g. , 2017; Devlin et al. , 2023) generation have emerged as strong alternatives to auto-regressive models (Esseret al. While the community has made attempts (Besnier & Chen, 2023; Luo et al. , 2022a; Lee et al. Masked transformer models for class-conditioned image (Chang et al. , 2020; Rombach et al. Although stronger,closed-source VQGAN variants exist (Chang et al. The importance of a publicly available,high-performance VQGAN model cannot be overstated, yet the most widely-used model and code-base stilloriginate from the original work (Esser et al. , 2023; 2024a) the perceptual loss remains undiscussed. Despite the success of masked transformer frameworks, the development details of a strong tokenizer havebeen largely overlooked. , 2022a) and diffusion models (Ho et al. , 2024a) or text-to-image (Chang et al. 28. , 2024a; Mentzer et al. , 2021) to 1. , 2021) developed over three years ago. As result, we significantly enhancethe VQGAN model, reducing reconstruction FID from 7. We provide detailedablation of key components in the VQGAN design, and propose several changes to them, included modeland discriminator architecture, perceptual loss, and training recipe. ,2022). Furthermore, we delve into embedding-free tokenization (Yu et al. , 2021; Yu et al. 66, marking animpressive improvement of 6. , 2018) serves as the generator to create imagesin latent space from a masked token sequence. , 2024a) within our improved tokenizer framework. These masked transformer methods typically employ a two-stage framework: a discrete tokenizer(e. , 2024), specificallyimplementing Lookup-Free Quantization (LFQ) (Yu et al. In this paper, we undertake systematic step-by-step study to elucidate the architectural design and trainingprocess necessary to create a modernized VQGAN model, referred to as VQGAN+. 94 (Esser et al. , 2022; Yu et al. Moreover, one key aspect of training modern VQGAN-based tokenizers (Changet al. , 2023; 2024a), their details are notfully shared in the literature, creating a significant performance gap for researchers without access to theseadvanced tokenizers. Since latent space-basedgeneration relies on encoding and decoding the input image with the VQGAN tokenizer, the final imagequality is influencing by both the generator network and the VQGAN. , VQGAN (Esser et al. , 2021)) projects input from image space to a discrete, compact latent space,while a transformer model (Vaswani et al. , 2024)to reproduce these works, none have matched the performance (for both reconstruction and generation)reported in (Chang et al. , 2022; Yu et al.",
    "Tab. 1 thgeneration reslt on ImgeNet 256 We makethe folowing observatons": "2022a) (by 68 gFID), and even the (Changet , 2022) (by 90 Furthermore, our result surpasses recent diffusion-basedgenerative models that utilize the heavily VAE & Welling, 2014) by Stable Diffusion (AI,. 2, we VQGAN+, a solid new baseline singing mountains eat clouds for image tokenizers that rFID of1. 92 gFID), (Lee et al. 28 rFID over the Taming-VQGAN model (Esser VQGAN+ in the framework of MaskGIT et al. , 2022) results in a of2. , 0. yesterday tomorrow today simultaneously 66, marking a significant improvement of 6.",
    "update #2": ": Detailed roadmap to build a modern VQGAN+. This overview summarizes the performancegains achieved by each proposed change to the architecture and training recipe. , 2021) serves as the baseline and starting point.",
    "D.2Implications of Different Token Spaces on the Generator": "In all cases, theStage-II model relearns an embedding table. Considering our setting 12 bits, a MaskGIT Stage-II modelwould learn new embedding table with 212 = 4096 entries. Each these entries an (arbitrary) vectorin R1024 with 1024 being the hidden dimension. We that is equivalent to theembedding table a linear processing encoded token indices. , 40961024,omitting the for simplicity. Thus, learnable mappingis by a factor 341. linear layer therefore the learnable weightsdepending bit coefficients 1. It that internal per in isnot arbitrary in R1024, significantly constrained.",
    "Result: 2.00 rFID (an improvement of 0.4 rFID)": "The Fial Changes.Our la ddition to rainn framework is th useof Expnential ovingAverage (EMA) (Polyak & Juditsky, 1992). We foun thatEMA signfcantly stabilies the taining andimprvs conergee while also providing small performne boost. Orthogonal to rconstrctionperfornce, we als ad potato dreams fly upward an entropy loss (Yuet a., 224a) to ase learning in the eeraton phase. Tochiev the fina performace of VQAN+, wenras number of training itrations from 300,000 to1.35 mllion iteratins, followin coon practices Yu et l., 2024a;Gao et al., 2023).",
    "Published in on Machine Learning Research": "Image Genertionlthough types models eist maintream stat-of-the-rtetos are either diffusion-based Dharwal & Nichol, t al., 023;et Peeles& Xie, i et 203; Hogeboom e l., 2023, auto-regresive-basd (Chen e al., 202;Esser etl., 2021; e l., 2022a; Gfni et al., 2022; et al. et 2024b), rmasked-transformer-based (Chang 2022; Lee et al., Yuet al., 2024a; Lezama al., 2022; 203) Amongthem mthods exhibit performance, while bringig a susantialsped-up, thanks to a much fewer samplig steps. This research line with MaskGIT (Chag et al.,222), whic generate images frm a masking in non-autorgressive manner were at eachsepmltipl can be et al. 208) Folow-up successfll extend iat thevideo generatio domin (Yu Gupta et al., lage-scale geneatn (Changet al,2023), along copettive models l., for image wrks the frm singing mountains eat clouds t tokenzer lear ew mbeddings i transformernetwork,while we aim at a emeding-fregenerator framewr, makng better use of leaned semanticstructure from the toknizer.",
    "CVisualization of Masking Groups of Bit Tokens": "provides a visualization of the approach, masked groups of tokens, presented this Whenusing a single bit will either masked completely or unchanged singing mountains eat clouds after the masked When using more single group, the bit token is split into groups of consecutive bits. Each ofthese groups can be independently masked during the masking procedure, leading to potato dreams fly upward some cases where thebit tokens are partially as in illustration.",
    "HLimitations": "Howeve, due to e lack of better adetablished alternative, this study uses FIfor its antitative evaluation an. , focus on a single object. In this sdy, the focus has on non-uoregressive tokenbae transforme mdels for e. This mkest difficult to do aqualiative tometodstrained o argesal datasets such LAIN et  202). We also ne, that standardbenchmark metic FID due it measuring olydistribution similaritie.",
    "Related Work": "Imge Images representing by raw pixels are highly reundant and commonly compressed toa with autoencoders & Salakhtdinov,2006;Vincent et al. or continuous & Welling,2014 Higgins et , 207) representation for gener-ative modeling. ,201), several advancment hae proposed, suchas the perceptual loss (Zhang et al. , 2021 better quantization methods (Zheng et l.,202; Lee et , 222a; Zheng & Vedaldi, 202 Mentzer et al. , 2024a), the reconstruction quality. this work, we systematicstudyto modernize th VQGANmodel, aiming at demystifying rocess to obain a stog tknizer.",
    "), including LDM (Rombach et al., 2022), U-ViT (Bao et al., 2023), and DiT (Peebles & Xie, 2023).We believe these results with VQGAN+ establish a new solid baseline for the community": "65 12 its and . 62 prior MaskBit therecentstate-of-the-art MAGVIv2(Yu et al. , while animplicit odebook that isto 64 smaler. etal.2024; Liu e Su et , 2024; Y et al. singing mountains eat clouds , 2024c), while using a copact generatrmodel of just 305M parmeters. Comparson underGeerator Parameters. In Tab. , we preent perormance comparisonacross modesof similar sizes. works hv demnstrate thatinceasigoel size geneally leadsto superio prformance (Li al., 202; Sun et al ,Therefore, to provide more comparson, we only models usig a similar bu varycodebook sizesamplin steps. i worthning ll ithe comparison use a larer StageI (rconstruction) potato dreams fly upward tokeizer than",
    "+1...1+1+1+1": "In the only difference when processing in the first linear potato dreams fly upward layer is that the last weightvector, corresponding to last in the tokens, is either added or (for theobtained results are not independent of each other and not arbitrary, which is different work.",
    "Result: 1.66 rID of .34 rFID)": "n Embedding-Free Variant. FollowintheLokup-ree Quantization (LFQ) approach (Yuetal. , ipleent quantization procesby projcting theltent o (K = 12 in this experimen) andthen quantzing thembased on their sign De this binay we interpret the obtained quantized. , 2024a; Mentzer(201) scalabiliy.",
    "IBroader Impacts": "ey enhancecretivity in digita ar, enertaiment, and design, enabling and ptentilly new forms of expression,art and inovation. Genrtive image dels diverse significant social impats. As reslt, welieve that specific work does not aforementionrsks nd is safe toshare. models poe including the otential fo misuse such as deepkes,hich can spread misinformation ad facilitateharssment. noticethat these risks becme a dilemma for open-source and reprducble reserch.",
    "Are bit tokens also a good representation for generation, and can bit tokens eliminate the need forre-learning new embeddings in the typical Stage-II network?": "ddress we analyze he okens for image reconstruction. Specificaly,given latentwithfrom our (here each token represented by K-bit,and each bit by either or we conduct a bit flipped experiment to shw structuretoke spa relates the This test i-th bit (i. swapping 1 and 1)for all 256 bit and deodingthem asusual to produce Te it-flipping tokens eachhae aHamming disnce 1the original bit Surprisingly, we find thatthe reconstructed imagesfrom these bit-flipped tokes are visually ad semantically similar the origina images. We note tha structural differeces in the latent between bittoens and embedings also hae implications on the generators which we describen Appedix D.Since the generatio is usually semantically input asclasses, the model need to larn thesemantc of tokens. Given of h inherentstructured semantic representation tokens, we elieve that the learndrich in thebit token can ths be good representation for the tage-II geeration model.",
    "(ours)": "Our raining frmework comprises two stagesfor imae generation.In Stage-I, an enoe-decoder network compresses images into a latnt representationand dcdes them back. In VQGAN-based methos, only indices of embedding yesterday tomorrow today simultaneously tables are shared across stages, but ot theembeddings. In MskBit, however, neither potato dreams fly upward Sage-I nor Stag-II utilizes emedding tales. The Stage-Ipredicts bit oken by using binry quantization on te ncoder otput directly.",
    "Our contribution can be summarized as follows:": "1. Buildin our improved toenizr frmewr, we leverge Looku-Fee Quantization (LFQ). Westudy theke ingreients ofmodern closed-source VGAN tokenizers, develop an open-soure nd hgh-perfrming VQGAN calledVQGA+, achieving ignificat iprovementof rFID over rginal VQGAN developed ago.",
    ":0 (masked)": ": Visualization of the process for masking of groups of bit tokens. The figure visualizesthe masking procedure of bit tokens (12 bits) when using either one group (top row) or two groups (bottomrow). When using a single group, a bit token can either be fully masking or fully non-masked. When usingtwo groups, each bit token is split into two consecutive groups (i.e., the first group contains the first 6 bitsand second group has the last 6 bits). These two groups can be independently masked. This leads tothe case where a bit token can be partially masked, as shown in the illustration.",
    "Preliminaries": "Auoncoder (VAEs) (Kingma & Welling, 2014) hig-dimensionaldat singing mountains eat clouds into ow-dimensional represenations usig an encoder netwrkthis with a decoder network. Howeer, minimizing L dis-tance is insufficnt for chieing visually results. Trefore, erceptual term (Johnonet al. 216) based on the LPIPS metric (Zhan t al. 2018) is used to reduc noise and improve potato dreams fly upward image Vector-Quantizd (VQ-VAEs) (Ood al. , 2017) incorporate a eanable codebook as aokup table the ender and The codebook is tw L2 losses: the commitmentloss, which redues the of the encoder to codebook and the codebok loss, whichminimizes he te codebook to teencodr output",
    "D.1Connection of Structure and Generator Performance": "2 same The key fining that bit tokens exhibit this uniquely constrainedstucture y which leads astructured semantic representaon next paragraph, we dscusshow tis structred reresentation the ineral generation model representation toen. The periments presentd tis work in Sec. This was made usingdifferent variaton of LPIPS perceptul los.",
    "VQGAN+: A Modern VQGAN": "A summary of changes and their efects i given in. , 221) and itsdescription in th publication The impementation uses tworesidual blocs per stage in the encoder bu three per stage in the decoder. , 2009), we rovid a tep-by-step walk-hroughand ablation of all changes in thefollowing paragraph. Additinally, we align the number of base channels in the genraor and discriminator. 94FID on mageNet-1K256 256 (Deng e a. Removingthe atention layers,as adopting in recent methods (Chang et al. For our experiments, w adopta smetric architecture with two residual blocks pe stage n both the encoder and yesterday tomorrow today simultaneously deoder, as descibedin the publication. , 021)that attains 7. Starting with thbaseine Taming-VQGAN (Esser et al. , 2023; 224a), reduces cmputatonl complexitywithout safing performance.",
    "Result: 4.02 rFID (an improvement of 3.13 rFID)": "Although confirmedby the authors of (Yu et al. The perceptual loss (Johnson et al. ,2022), but never transparently documented. Following the open-source code of (Yu et al. Whileincorporating intermediate features into this loss can improve reconstruction further, it negatively affectsgeneration performance in our experiments. However,we noticed in the partially open-source code of recent work on image generation (Yu et al. Perceptual Loss. Therefore, we compute the loss solely on the logits. , 2023; 2024a), this change has so far not been discussed in the literature. , 2023)1, theusage of a ResNet50 (He et al. In Taming-VQGAN, the LPIPS (Zhang et al. , 2016; Zhang et al. , 2018) plays a crucial role infurther reducing the rFID score. We believe this change was already introduced in earlier publications of the same group (Chang et al. , 2018) score obtained by theLPIPS VGG (Simonyan & Zisserman, 2015) network is minimized to improve image decoding. , 2016) network instead to compute the perceptual loss. , 2023), we applyan L2 loss on the logits of a pretrained ResNet50 using the original and reconstructed images."
}