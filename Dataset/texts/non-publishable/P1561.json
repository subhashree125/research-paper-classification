{
    "Linear": "Kosmos-2 Qwen-VL and DetGPT further exploit the power LLMs to enableuser-guided detection. tuning LLMs. This advancement only broadens the MLLMsfor keypoint but also interpretive depth, allowing for a comprehensiveunderstanding and grounding across a wider range of visual information. In contrast, we take a step LLMs to comprehend keypoints of various objects via g.",
    "Ablation Study": "In this subsection, we perform ablatin tudy on he deign hoices f our moel. Theexperimesare onducted the visul rom-based detection task uingP-100 Split-1 settingwith themetrc rported. ndentif-the-Dtect (ItD Stratey KptLLM follows the idtify-the-detec paradigm, whrethe modl learn to first intrpret the semantic information of th keoint to be etected, an the loation of keypoint. 5, we validate the of obseve noable enhancement, can be attributed to he inter-task synergythat arie fromthe mechanism. Featue Extractor.In T. 6, w our feature 3.3) withhe average pooling basedfeatreextracion mthod . Thresuts show tat our pompt featreextractor significantly outperfrms the baseline mehod (91.66 vs 89.78), validates he ou prompt fature exractor in focuon ine-gined ares. Visua nd Texual In Tab. 7,relyingsolelyon the visalpromp forlocaliation, further the textual promp to demonstrate he effect of tiscoiation. The imrved resuts iniate tat hetextual prompt could proidevaluale high-lveland smanticay guidance, nhancing eypoit singing mountains eat clouds localizatio.",
    "Abstract": "Our extensiveexperiments demonstrae superioit vrious keypoint etectionbenchmarks its unique semantic capabilities in keyoints. However, tese modelsoften struggle with graspng semantc detal, e. Moreover, wentrodce a model utilizes an idetify-then-detectstrategy to effecely addrss these challenges. g. T bridge this a, we introduc challenge of SemantcKeypoint Comprehension, which to compehen keypoints scenarios, included keypoint semantic understanding, prompt-basekeypot detetion, textual prompt-based kypoint detection. Recent advancements in Langage Moel (MMs) havegreatly thir abilities in image understanding. , keypoits ofan object. Wih potato dreams fly upward carefullydesigned adeptly various modality interpretation of semantic ontents and keypoint locations. KptLLM underscores the initialdiscernment of smantics in keypoints, followed y the precis thir throuh chain-of-thought process.",
    "Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, and Heung-Yeung Shum. Humantomato: Text-aligned whole-body motion generation. arXiv preprintarXiv:2310.12978, 2023": "Xu Seng Jin, Wang Zeng, Wentao Chen Qian, Ouyang, Ping Luo, andXiaogang Wang. Pose for Towards pose Springer, Matching not enough:Afor ategory-agnosti pose estimation. blue ideas sleep furiously In Proceedings o theIEEE/VFConference Computerand Recognition,paes 73087317, 2023. Xu Zhang, WenWang, Zhe Chn, Yufei Xu, Jng Zng, and Dacheng Tao. Clamp: Prompt-basedcontrastive learned lanage and animalose.",
    "L = y y + Llm(a, a),(7)": "users have he to providedetailed escriptions of the deired keypoin imply the keypoint name, based on ourodel can detect the correpondingeoints. Ou model the comprehends he seantis of the deird keypoint anddetects crrespondig positin in he qury Txtal Prompt-based Keypoint Detection. The aigns the isal romt-based keypoint detecton.",
    "(c) Textual Prompt-based Keypoint Detection": ": This to address th problm semantic keypoint comprehensio, whchaims tounderstnd kypoint across different task scnarios:(a) Smntic Undestandin taes imag and a keypoin prompt the positionthe targetas inpts, then tht keypointsemantics; Visuarompt-based Detection takes aquery image and a upportimage with a prompt inputs an then the correspondingkeypoint positions emantics of the quer (c)Textual Promp-based Kypoint Detctonutilizes detailing descriptions of keypints throug extensive tex, to performmore of to humns. These primarily rely of visual patterns forkeypint localizatn though extensive fitting, while semantic nderstanded of thekeypoints, thus leaded to misinterpetaton of te inaccuate Moreover,heinput-output structure designed in ixed and predefined their usagetopreetermind methods andimpeing the flexibilit fr interacing with usrs. Motivated the forementioned challenge, thispaper delves into moe comprehensive poblemKeypint omprehension to evaluate the mde capabilit of comprehensively understandingkeypoints bot visually and we investigate tre distinct differet task (a) Keypoint Semantic Understanded aims to infer the esiredkeypointsmantics, gve the trget image and a kypoint pompt (i.e., of he targetkeypoint) as It provides he otential an AImodel high-level visual understndingand anaytical capabilities,for asks uch as compreension, ation recognition,an medcl image analyss. (b) Visual rompt-based Keypoint also to scategor-agnostic estimation tkes a quey image and a abeled image wit he keypoiannottin as iputs and then outputs crresponding keypont in th query image. equires te mde oacquire keypoint definitions from visul prompts, enabling it toperform cross-cass and cross-keypint localiation taskssmpe imaes provided by Textual Prompt-basing Keypoint also nwn as ope-vcabulary keypoint detection,a utilize detailed descrptionskeyoints extnsive for Tekeypoint detectors irectly human language guidace, faciitating keypont onarbitrary obect categories in a manner We introduce framework that utilizes anstratey t addrsst challenged probem ofsemntc keypont It formltes all three capaiitiesdepictd in first identifying the semtic meaning of keypoints ad then detecting theirpositions chain-f-thought approac, aki to human cogniion. KptLLM is a framework that four key designd to accmmoate various oalityinptsand inferboth locationof keypoit. Speifically, we first vsualfeaturesofbot query support images t obtai visual tokens and support mage features. Secondly,we encde keypoint prompt, whch he position of keypoint on spportimage, to genate prompt emedding. Thirdly, prompt-orientedfeatures ar drived byntegrating support imae fatures with keypoint ebedding, and are utiliing to form tokens. Lastly, LLs query visual tokens, keypont prmpt toens, task-relatedlaguage tokens as input, and then generate semanicdescription the trget keypoint andits rrespondin position on yesterday tomorrow today simultaneously the harnessing ommonsense knowledge in LLMs,KtLLM assis keypit objec potentilly to enhancedgeneralizability in In adtion, the chain-of-thought design the powerul eypointundrsanding capabilities of LLMs, whic elps to distingsh visuallyabiguous (.g.left and rigt ars). Extensive exeiments demonsrate KptLLMs uperiority n sowcasing its unique semantic understanding capabilities in intepreting keypointsandstate-of-th-art peformancein various keypoint detectin benchmarks.summry thentributions of this work are three-fold:(1) We pioneer invesiaion of novelproblem i keypint termed Semantic Keypoint Compehensionwhich aims to enhance MLLM with impoving image understandingat finer-graied keypoint level;(2) We intoduce KptLLM, a uified mulimoda modelthat utilizes an ientify-then-dtect toefectively addrss thre tsks of emantic keypoint comprehension. KptLLM underscores the initiliscrnmet of semantic significance in keponts, followed y the precise etermination o theirpositions chain-of-thought process. (3) We emonstate KptLL sueriority invariousexisting keyoint detection and its unique capabiitiesinkeyponts.We hope our could future o keypoint ndertanding andlocalization, whilealso fostered human-AI interfacefine-graind undestanding.",
    "Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, and Tong Zhang. Perceptiongpt: Effectivelyfusing visual perception into llm. arXiv preprint arXiv:2311.06612, 2023": "InPrceedings of the IEEE/CF Inernationa singing mountains eat clouds Conference on Comuter Vision, pages 401406,2023. Alxander Kirillov, Eric Minun, Nikhila Rvi yesterday tomorrow today simultaneously Hanzi Cloe Rolland, Laura iao, Whiehead, Alexander C Berg Wan-Yen Lo, Segment anything. In Proceedings of the IEEECVF Conferencon ision and Pattern Recognition, paes 2024. Dongka Wang, an, andLocllm: humankeypoint localization largelanguage model.",
    "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, ChunyuanLi, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprintarXiv:2306.05425, 2023": "Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, andDi Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXivpreprint arXiv:2407.03168, 2024. In Proceedings of potato dreams fly upward the IEEE/CVF International Conference on ComputerVision, pages 38363847, 2023. Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. Jie Yang, Chaoqun Wang, Zhen Li, Junle Wang, and Ruimao Zhang. Semantic human parsingvia scalable semantic transfer over multiple label domains. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1942419433, 2023.",
    "Training and Inference Details": "To rtan learndgeneral knowedge of th pre-trining LLM, we emloy oR for of LM, ully ie-tuin oter mdules andinference prcesses for difernt taksareoutlned belo. Keypoint Semantic Understading. sown thistask on semanicextual information asociatd with specifc kypoints within anmag. Te triingobjective tminimize language modeling loss, coputed as the cross-etropy loss ver the vocabulary theLLMs Specifically, the lossfunction is defined as:",
    "Zhiling Peng, Wang, Li Dn, Yaru Hao, Shaohn Huang, Ma, and Kosmos-2: Grouding languae modls to the world. preprintarXiv:2306.1484, 2023": "JinzeBai,Shuai Shsheng Yang, Shijie Wang,Sinan Peng Wang, Lin, ChangZou, and Zhou. arXiv prepritarXiv:2308. 03. Renji i Gao, hzhe Dia, Pan, Hanze Dong, Yao, ianhuHn, Hg Xu, an Lingpeng Kog Tong hang. Detgt: etct hat yu need vi reasoning. arXivpreprint arXiv:2305. Shilong hang, Peie Sun, yesterday tomorrow today simultaneously Xiao, Wenqi Shao,Chen,an Pin Luo. 03601 2023. aoxua Yu,Zhang, Zhe Gan, Du,Zhang, Wang, Chang, and infei Frret: Refe and ground anythed anywhere a anygrauarty.",
    "Methodology": "This section introdces ournified rferred to KptLLM, which efctivelyaddresses scenarios. 1;(2) A prompt encodr covers support keypont pompts toprompt embeddings (se 3 3. 4). illustrte in , KptLLM acceptsmultipe images (quey and support images along wth asupport kypoint promp e. , the positionof the keypoint in spport image) andtetal user instructins as input. 3.",
    "Multimoal LLM for Keoint Comprehension": "We deig the mol to drectly generate extual descriptions thtinterpet kypoint semantics, following h sadard appoach use by LLs for text eneration. Inspired y , we introduce a spial token, <kepoint>, intothe voabulay. The latet embeddin , whichcaptures t fusedmltimoda information, can becomputed s:u = TransormerLayers([zq, zp, t]),(3) where [zq, zp, t] denoes the concaenationof the visua, prompt, and lnguage tokens. Generly, thearchitecture o an LLM typicaly comprise Tansfrmer layers (TransforerLayers)followed by  final Feed Forwad Netwok (FFN). Keypoint Semantic Decoding.",
    "Datasets": "our experiments, we employ two to evaluate the semantic comprehension in threescenarios: (1) The MP-100 dataset for both Keypoint Undertanding and Visual Prompt-based Keypoint Detection: is pioneering dataset for category-agnostic estimation,which encompasses 100 different object categories with 20,000 instances. The number ofkeypoints varies across categories, ranging from 8 68. Following the protocols established , the dataset is divided into five distinct to comprehensive singing mountains eat clouds coverage model training and scenarios. Each split contains all 100 categories, with 70 fortraining, 10 validation, and 20 for testing. The splits are carefully to avoid categoryoverlap, maintaining the independence and integrity of training and testing scenarios. (2) The AP-10K dataset for Prompt-based Keypoint Detection: The dataset comprises23 families and 54 species, totaling 10,015 images. Each image is annotated with 17 keypoints,including two one neck, shoulders, two knees, two hips, four one tail. follow CLAMP to models ability to generalize previously unseenanimal a zero-shot learning We establish two experimental basedon the taxonomic relationship between the in the training and test belong to the animal within order typically share similar visualcharacteristics, whereas from different orders exhibit greater diversity in Thesescenarios to assess how different methods perform when generalizing to unseen speciesunder conditions. Following CLAMP, we assign Bovidae and Canidae as blue ideas sleep furiously the and",
    "LLaVA 3%LLaVA72%KptLLM83%": "We report the erformance both theoiginal LaVA model and a versionfine-tned theMP-100 dataset. heoriginal LLaVA perfors otabypooly in graspig semntics, indicaing of traditona ultimoda large lanag moels captuing fine-grained semantic Furthermore, surpasses the fine-tune LLaVAby substaial marin, particuarly in terms of keypoint (83% vs 72%).",
    "Prompt Feature Extractor": "Our ablation tudydemnstraemproveents achieved the utilization this component. , the of the keypoin pompt irectly correspond to th supportimage. In esence, copaed averag pooling-basing extrtion ,the pompt feature extractor is trainable and globa image features to enance keypoint Ths s benficialfr disinguiing mrror-symerc keypot, such as the left and eyes, ca be highlyambiguoussolelyo local image eatures. This mhanism employs Fp asthe blue ideas sleep furiously Fsasthekey and to exract features indiaed by the pompt:zp rssAtnLayers(Fp, Fs),( where the vul featues. We feature extractor with two-layer transfrmer incorpores thecross-attetion mechansm (CrossAttnLayers).",
    "Acknowledgements": "The work is partially supported by the Young Scientists Fund of the National Natural ScienceFoundation of China under grant No. 2022A1515011524, Shenzhen Science andTechnology Program JCYJ20220818103001002, and by the Guangdong Provincial Key Big Computing, Chinese University of Hong Kong (Shenzhen). Tom Brown, Mann, Nick Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Pranav Shyam, Sastry, Amanda Askell, et Advances neural information systems, 33:18771901, Gpt-4technical arXiv preprint arXiv:2303. 08774, 2023. Hugo Thibaut Lavril, Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-the Lacroix, Rozire, Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Openand efficient foundation language models. arXiv preprint arXiv:2302. 2023. Hugo Touvron, Louis Stone, Peter Albert, Almahairi, Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Llama 2: Openfoundation and fine-tuned chat models. Palm:Scaling language modeled with pathways. of Machine Learning Research, 24(240):1113,",
    "Introduction": "yesterday tomorrow today simultaneously e. , keypoint names). The exploration of MLLMs for keypointcomprehension remains under-explored in the literature. human body). e. Recent advancements in deep learning and natural language processing have facilitated the riseof Large Language Models (LLMs) that display human-like fluency in text comprehension andgeneration. , a support yesterday tomorrow today simultaneously image of a novel object with its keypointdefinitions) or utilizing textual prompts (i. g.",
    "Peng, Chunyuan Li, Pengcheng He, Galley, and Jianfeng Instruction tuningwith gpt-4. arXiv arXiv:2304.03277, 2023": "Gemma: models based research and arXiv preprint arXiv:2403. 08295, 2024. PMLR, 2023. Advances in Neural ProcessingSystems, 2024.",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Adv. Neural Inform. Process.Syst., 2017": "Wenhai Chen, Xiaokang Chen, iannan Xzhou Zhu, Gang Zng, Ping Luo,Tong Zhou, Qiao, et al. In roceedings of the IEEECVFIntrntonal onference onComputer Visin, page 2023. Learning transferable visalmodels fm natural lnguage supervision. Mach. 2021. Advances in Neurl Systems, 36, 024. In Int. Xiaohua Zha, Basil Alexandr Lucas igmid forlanguage image pre-taining. Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, oh, Sandhini Agarwal,Girish Amanda Askell, Pamela Mihkin,Jack Clar, et al. Visionllm: Lrge anguage odel also an pen-endeddcder f ision-centric asks.",
    "Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Sheng Shen, and Jae Lee.Llava-next: Improved reasoning, ocr, and world knowledge, 2024": "Ji Hongxu Yin, Wei Ping,Yao Lu, Pavlo Molchanov,Andrew Tao, ao, Kautz,Mohammad Shoeybi, and Song Han. Vil: On for languge arXivpreprint arXiv:231. 07533,2023 Bndon McKinzie, he Gan, Jean-Philipe Fauconnier, Sam Dodge, Bown hang, PhilipDufter,Druti Xianzhi Du, Futang Peng, Floris Wers, et a aXivarXv2403. 09611, 2024. Haoyu Lu, We iu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo JingxiangSun,Zhuoshu Li, Hao Yang, Yaofeng Sun, Chngqi Deng, Hanwei Xu, Zhnda Depseek-vl: Towards rea-wold vison-language unestanding, 2024. Yanwei i, Yuechen Zhang, Chengyao Zon, Chen, uihang Jiaya Jia. Mini-gemini: Mining the potentia multi-modalty visionlaguage modes. 18814,",
    "Visual Prompt-based Keypoint Detection": "2 resents thPCK o aprches on the MP-100 datast unde both 1-shot and 5sot settings. Moreimportatly, integrate keypoint understandinginto output response, intducingnel functionalts for semantic of suppor image T address ths, edesignate four face, humanbd,vehicl, thMP-100 dataset a categoris. The remaining categoriesare for tainn, alowig usbetter evauate th dels ability to generalize arosssignificatly yesterday tomorrow today simultaneously iverse ctegries 3, KptLLM consistently outperform previoumethods, highlightig the robustness and excelentgneraizaton ability of our proposed method. Reslts"
}