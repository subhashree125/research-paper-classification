{
    "Human3.6M 112.9Marker-based-AMASS 34440.0Marker-based11.9HumanML3D 45028.6Marker-based12.3Motion-X -144.2Marker-based38.5BlindWays (Ours)112.8IMU-based44.1": "GT-based text-to-moion modeshave recentlyshown promising results. he KIT-ML datasetfocuses on multi-mdal language-to-tion ranslaionbut lacks motion diversiy. Motio an Language BenchmarksDatasets wt high-uality text decriptions ave furtheradvanced the cntrollability d generation of mutimodl motion, with some aso incorporatingintertionsith objects an otherpople. HumanML3D introduce a large collection of 3D huantions with correspoding naturallanguageanoatons, primarily focused n static ndor setings ith repetitive motions. ecently MotionG has integratedgenerative pre-trained transformerswit joint traiing of motion andlnguae, furter advancing hequlityand diversity of geerated motion. Howeve, the pplicabilit and gneralizai of suchmoels in accessibiity settinsremain underexplord. AMSS unifi  wide rane f MoCap data. BBELand PoseScriptalso incrpoae actionlabels an extual descripions, howeer, sch datsets still lk in motion divesity and ealism. current methods are limitd y ineficiency duringtesting, as tey requir multiple forward epsto generate asinglemtion sequence. Motion-X introduces  coprehense dataet thatincludes dtaildsemantic annotatons and otdooenvironnt (a context largely eglct n prvious datasets).",
    "Conclusion": "Our datasetincludes 3D motion data enriching with and low-level text descriptions, derived from third-person and egocentric RGB videos that capture actions, intentions, environmentalcontexts of motion in individuals use aids interact blue ideas sleep furiously surroundings. experiments show recent state-of-the-art motion-language models struggle to generalize to blind motion, highlighted the unique challenges presentedby this domain.",
    "Motion Capture": "The cne is in rghthand, and she a combintio sweeping tapping the path front her. Hgh-levelLow-Level A blind woman with cane is carefully approaching a cosswalk. uses her cane to the path front of hr and stops whenshe he cub singing mountains eat clouds and tactile paving.",
    "C. Guo, X. Zuo, S. Wang, S. Zou, Q. Sun, A. Deng, M. Gong, and L. Cheng. Action2motion:Conditioned generation of 3d human motions. In ACMMM, 2020": "Blind and pedestrians of i at rodabouts. 2005. Guzov, MrT. umn ystem (HPS): 3D humanose estimation selflocaliatin in largesenes fro body-mounted senors. In CVPR,2021. Harrell. Driver to ped. usin a dangerous croswalk. In JEP, 1992. Effects f blndon motorists In 1994. D. Ceylan, Villegas, J. Sait, J. Back. Stchasicscene-aware prediction",
    "Daa Anotation Pipeline": ", a can and a guide dog). Use of subjective ajectives (e. Hih-evel Descriptions: For high-velanotations, annotators are requeted to focus on describingth oerall cton of the motion, the purpose behind i, an how participants were holing teirmoiy aids(e. g. For novice nnotatrs,we proiding dos of expertanotation amples as references. To ensure piacy, we mosaic te facesof all pepe apearinin te videos, boh the blind prticiants and pssrsby Anotator are given 25 vides t a time,andit took approxmately two hours to anotate each et of25 videos. Annotatrs can fficiently eview and annotate spcificmomets n he videos, enhancing the accurac and detailf their descriptios. Hethen oriets himself in direction he wants to cross street. o facilitate annotaion process, we build a ideo annotaion interface using Tkinter, Python-bsed GUI tookit. o chieve a nuanced understandin of he naviation behavors of blid individuals, we emloya metculus annottion pipeline build in-house that lverages te sychronized thid-person viwRGB vidos alongthe motiondata. In addition to carefully crafted instructions, noice anotatorsare aso give feedbackafter heompetion of their irst set o ensure high uality annotations help the imprvetherefficiency in subsequen annotations. Hemoves forward three steps to orient himself inthe direction he wants to cross the ste, using his anein hisight hand and positiond in fron ofhim. The detaile informtion helps in capturin exactmoton dynmics and interactons between the participant and thesurroundig dynamc enironment.",
    "A blind man with a cane in his right-hand walks confidently with long steps. He momentarily halts at the end of the footpath and keeps walking further": "High-level StartEd To-Lel Text Desriptios : Qualitative Examples From Our Daaset.Annotation language captures both igh-levelinformatio regrded gneral actin, as well a detaildlow-leve motion caracteristics, mbilityaid strategies, goals and environmntal context. BlindWys also encompasses scenaios with careful and deliberate movements,uch as walkin along curbs to avoi fallingffthe sidewalk or colliding withobstacles,stairnavigaion wth motion like tapin te can on eac step to gauge heght and depth and usinghandrailsfo support, an street crosing where participat may pause at thcurb an performprcisecane movements to locat tactile singing mountains eat clouds paving or crb rmps daa is captured in the Xsensjoint representaion, omprising a total of 24 jints. ext Data: As shwn in , high-level motion descriptionanotations provide summary ofthe blind prsons acions and intent, along with their interactions wit mbility aid. Anotations areapproximately 26 words long onaverge, with longest being 111 words.T standard deviationof about 9.45 indicates mderate variability in annotation length. In contrast, ow-level anotationsprovide ore dtailedesciptios f spcific actions along with tep count and detailed use ofmoblity aids. Thus these anotationsare longer, approximately44 words on average, with thelonest being 140 wods",
    "The Guardian. Toyota Pauses Paralympics Self-Driving Buses After One Hits Visually ImpairedAthlete, 2021": "Touvron, L. Martin, K. Almahairi, Y. Batra,P. Bhargava, S. arXivpreprint arXiv:2307.09288, 2023. J. Treviranus. In W4A, 2019. T. Henschel, M. J. Black, B. Pons-Moll",
    "potential obstacles they might encounter, such as a train/tram track in the middle of the route or stairs,to prepare them for critical challenges ahead": "comprehnsivel cpture the navigation we thir-person video of blindpedestrians egocentric view, as wellas motion data.The camer is set to face aound theparticipants meticulously capture cane All data are snchronized, aowin for a in-depth analysis and ofnavigation strtegiesand challenes gain ito participants navigation experiences of each rute,prticiats ae potato dreams fly upward asked to rate on a scale of 1-7 in (i) their to naigatethe (ii) the guidance that tey recived te Google Maps app.",
    "Zero Velocity-0.640.8712.793.241.290.01MotionGPT 23.133.014.760.724.210.570.26CVAE 7.680.470.560.453.890.110.23DLow 11.650.460.590.413.910.120.27MDN 15.140.450.560.404.310.140.28": "on BlindWays, compard to traning on BlindWays alne. This demontraes the efectiveness ofpre-training on a neral datast to nhance mtin diversty, eventhoughit lacks cateries thatcapture th uniqe movement ofbind pedestrians. Per-Keypoint Evaluation: We rther coduct a per-keointevluaionin th textto-oti task toanayze model pefomance on bind motion data, wih a fcus on joint exhbiting unique movementsin blind navigaton. blue ideas sleep furiously Specifically, we evaluate te head joint, arm joints (incdinshoulder, lbow,and wrist, based on paticipats dominant hand), and the mobiity aidjintusing ose-spaceacuracy metics sch as AD and FD. In bindmotion, the armjoints are essetialfor captuingthe use and hadling of mobilit aids, e. g. , for obstacle detectio and naviaon. h mobility aidkeypoint proides nights nto te dynamic and coordinated iteraction btween th uer and theiraid.Specifically, weobserve an verage FDEmprovment of 8% or armjoints and 16. 5%for mobility aid joit n models trained fom scratch onBlndays compared tthose trained on Motion-X hihighting the importance of domainsecifictraining data for accurately modeling nanced blind motin behaviors asociatewith mobility aids. Inerestingly, head movements are modeled or acurately by the Moion-Xtrained model thnte BindWays-traned model uggestin that the wide varit o head moements in th Motin-Xdataset nhances generalization fo these joints. n principle, pretrainingon such a datast couldailitate mode generlization for oh common behavir joints nd unique motion joit. However,in prate, we d this to rult in mixed resultsde to the intoduced bias and domain shift. The odels ar trainedto prdic the next 9. 5 secons of futre motion give 0. To accoutfor surounding context-bsed nteractions,we furher incorporate ext embeddings into yesterday tomorrow today simultaneously stcasticmodelig approach, including CVAE DLow, and MDN. We repetiiely sample from MotionPT to obtainAPD and pose metrics; hwever,we note thatMotionGPT gnerally performs poorly in motionconditioned prediction settings consistent with the riginal tud of ). We findCVAE-ased methods to demonstratebetter accuracy than otionGPT,which generally prdictsdivere but unrealistic motion patns. 68, wieDLw and MN achieve 11. 65 (52% higer) and5. 14(97% highr) AP,respectively. Thisfindinghighlights the beneit of n improved samplig mechanism. Specifically, MD, which ncororatesa transformr-based module in the motion decoding prcess, significtly enhance both samplediversit and reaism (with ADE ecreasing from 0.47 to 0. 5). W oserve consistent improveentsin FID, IV, and NDMS metrcs ith MDN chieving the best results. However, for DLo, theicrease in diversity is shown to result in an accuraytrae-off, where FE is ncreased (from0. 56 to0.Whilhese advancements demonstrate promising sries in modelig diverse andealisicmoton, this work reprents oly an initialstep.",
    "umanML3D Motion- BlindWays0.054 0.0098.612 0.406.260 0.3014.921 0.51MotionGPT BlindWays0.036 0.0310.313 0.1833.74 01642.759 0.100": "We adopt nd MotionGPT our stat-of-the-arttext-to-motion MotionGPT interaes a gnerative re-trained to genrtcompex motion pattens from text escriptions, leveragig lnuage modls. Txt-to-Motion: Weprovide a oparison of text-to-motion usng embeddingbase analy-sis in evalution we afeaur embedding mdel (following HumanML3D. 002 for MotnGPT. In te part our evalati, we focus on a motion-driven motion generation task, wherew stochasti CVAE-based approaches sour baseline. R Precision further nerscors the discrepancy in I contrast, models trained Motion-Xstruggle with this alignment, iely due to the lack of nuanced motion dta corresonding t blndImpact of Pre-taining: We the impact of pretrainingon otio-X, a oon-languag datast hat coves a of rresentations of movementsby people with diablities. We train and testbaselines using the jontrpesentation SMPL (converted the IMU-based motion capture suit)with an additional joint for mobili blin eestrians, enuring and compaa-bility across experiments and datsets. shon in pre-training on rovides thmodewith a strong understadig o motion feature and their singing mountains eat clouds alignment correspondngtextdescritions. This results in IDscores 11203 for HumanML3D 15. and 5. Specifically, an verage mproeent of 6 % in Diersityand 4 9% in Multi-Modality when leveraging on Motio-X by fie-tuning : Analsis Specific. Th flind motion in Motion-these generate yet unrealistic blindmotions,incresing distance etwen generated and lind motions. motions Multi-Modality (MMdalty) to how gnerated motions vary description.",
    "To design our study and data collection, we build on recent advances in in-the-wild 3D human motionestimation, 3D human motion models, and language-based motion generation": "EstimatingIn-th-Wd D Human Motion: Rsearchers have sought to humanmotion i naturalistic settins However, vision-based inerenceof 3D keypoints an be unreliable ino context of mlti-acor dense senes with freuen ocusionand inerctionwith objects. For instance, AlphaPose , widely used keypoint detectionnd liftng model, exhibited freuent failure and por performance on for our settngs (olinesourced nd our in-ouse ons Specfically, ur stuy, we leverage anXsensset oftrckers (oneplaced on the mobility hlethe system have inherentnise, it can be iprov accuracy. In ourstudy, feqentl re-calibrate thesstemthoughout the rote to minimize drift and improve Weaso filteroutnosskeletonsthrough inpection in of system trackng failure. Nnethess, ephasize capture in-the-wild remans n chllenge. 3D Huan Motion: Motion-prediction models (e. fewstudies have analyzed blidigati motion in naure , onl high-levlacont Instead, 3D motin modelseneally leverage sghted participants. Whil understanding encompassing unique ehavios i essentia for motioand geeration , study mpiricallemonstrate prior workssruggle to generalie the nuanced mdeling of blind mtion. Text-riven motingeneratio ha gained inficant to its contrlabil-ity, as as th conse context informaton provided by descriptions. Difusin models,such as MDM ben explored gnrated uman motion sequences from text refining th motion hough a series of forward steps. However, recent diffusion-basedapproaches do not enrate plausibe bind mtio, as shown by our sdy. In addition, : Motion enchmarks. indWays introduces severaldataset dimensions not ex-plored by prior particiants ith mobiliy ais (i. white cae or guide-dog,tracedwithsensr) and in urban strets. We aso provide languageannoationswith two granlart: high-level summries and more detailing dscription.",
    "In this section, we evaluate human motion generation baselines on BlindWays to discuss modelgeneralizability and the role of text labels in blind motion modeling": "First, we discuss limitations in current text-driven motion generation models, which, despite being large datasets, lack specific yesterday tomorrow today simultaneously categories capture unique motions of standard metrics used in previous studies: motion-retrieval precision to evaluate the of matching texts and motions, Frechet Inception Distance(FID) to assess the of motions, Diversity (DIV) to capture of : Text Motion Model Evaluation on BlindWays. experiment is repeated 20 and a statistical intervalwith confidence is reported.",
    "H.-S. Fang, J. Li, H. Tang, C. Xu, H. Zhu, Y. Xiu, Y.-L. Li, and C. Lu. AlphaPose: Whole-bodyregional multi-person pose estimation and tracking in real-time. PAMI, 2022": "D. CVIU, 1999. D. Geruschat. behaior inyielding o sighed and blind peestrians t roundabouts.A. Gopalakrishna, A. Kifer G. Ororbia.",
    "Introduction": "Coputaional modeled of pople and their 3D motion has bee studied extensively bythe machinelearnng community over the past decades. More recntly, the field hsbeen moving beynd single actors performing contrived ations to model interactive behavior, i. ,incrporating objects and suroundingople. However, the cope and diversity of the datstsassoiated with this pri work remain limiting to asmulation , a ab , or simplifiedlayuts. Even more noticebly,there hs nt ben a single 3D human moton dataset reeasetht cmprises obility datafrodividuals with disabiiie. ence, while most human moion model are developedwith assistiveand nteractive pplications inmind, such s social robots and utonomous driving, those who cldbenefit he most fom these technologie are not included. Such sevre biases in existig benchmarkscan arr broadrsocietal imlications.t an exacerbae already widespread concerns in accessiblity,where autonomos vehicles fal to accurtely predict and safey respond to movementsof peoplewith diabilitis; a population that is lready disproportionately impactedby motoists lack ofawaeness.",
    "arXiv:2412.05277v1 [cs.CV] 6 Dec 2024": "Blind pedestrians are known to exhibit significantly different based onpersonal factors, such as their lived experiences with disability or use aids canes,guide-dogs, and orientation mobility apps ). For instance, many do not face forward to signalintent to cross into the road and may take to tactile cues when crossingin various intersections . may significantly in open spacesor unexpectedly step into the road due to obstacles, such a parked obstructed intersectionswith damaged or ambiguous curbs. In such scenarios, reasoning over 3D likehand-aid coordination gestures, improve future prediction in autonomous vehicles and avoidpotential safety-critical . Yet, as far as we are no prior work has investigated prediction pedestrian motion in suchedge inherently subtle, uncertain nature. we aim tounderstand the capabilities of state-of-the-art 3D motion models for futuremotion of to ensure that autonomous systems and vehicles in operate safely around with Our is to enable more robust, accurate, needs-aware pedestrianbehavior models that effectively account disability-related scenarios and key contribution twofold. we introduce BlindWays, multi-modal 3D humanmotion dataset featuring pedestrians who are navigating complex dataset detailing language-basing and non-visualnavigation strategies. Second, we use to motion modelswithin our novel modeling task. the effects of model pre-training and fine-tuning onfuture prediction, we identify fundamental limitations in generalization current datasetsand models, particularly when evaluating within diverse rare attributes.",
    "Limitations": "Nonetheless, real-worldata from diverse pool could help identify futher biases ad model issues yesterday tomorrow today simultaneously (eg. ,arios suchas differe nd backgrounds). To ths bias, we collected realistically complex data ithinan importat but uer-discussed useour severa limiations. Our study underscores the complexit of diverse in cases where pre-training m non-beneficial or even dtrimental to model predic-tios, such as with bind moton. , inertial, vision-basing areconinuously being dvelope andcaneasier and orescaabl capture, laded to moreroustand pactcal motion models across many underepresening use in current human mtionbenchmarks. g. adresss a prevaet bias in motionmodeling datasets, specfically focus on sighteand pedestran motion. Another isthe expensive motion-captre uit, which may hinder large-scale Whil we chosehigher-cost, tracking techology, solutions (e. Te samplesize 11 rovidinga of 1,005 motio after filteringpose trackingfailure is representative in-situ studies."
}