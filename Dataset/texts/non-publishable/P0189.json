{
    "Harold W The method for the assignmentproblem. Naval research logistics quarterly, 2(1-2):8397,1955. 3": "1, 4. Groundedlaguae-mag pre-training. Roberta: A robusly optimizebert pretraining approch. convnet for the2020s. 11692,2019. In Prceedings o heIEEE/CVF conference on com-pter ision and pattern recognitin, pages 1197611986,2022. XiangLi, Jnglu Wang,iaohao Xu, Xiao L,Bhiksh Raj,and Lu. runing ino: Maryig dino wit gondedpr-traiing fr oen-set object etection. In Proceedings of the IEEE/CVFInternational Conference on Coputer Vision, pages222362245,2023. Swin tranformer:Herarccal visio trnsformerusing shifted windows. Robust referrig vieo object segmenation withcycic structural consensus. InProceedings of thIEEE/CVFCoference on Coputer Vision and aternRecognition, yesterday tomorrow today simultaneously pages 109651097, 2022.",
    ". Frme Decoder": "e develop a frame quer decoder to independently gen-eate frame quries Qf RT Nf for ach frame, assown in (b). Each decoder lyerhas a extra tet cross-attention lyer cmpared with thetransformer decode layer ofMask2Former , s we needto inject ext inforation ino queries fr bettr modalityalignment.",
    "Lee, and Seon Joo Kim. Vita: Video instance segmentationvia object token association. Advances in Neural InformationProcessing Systems, 35:2310923120, 2022. 3": "If you usetis software, please itas below. 4. 2021.",
    ". Implementation Details": "We use ConvNeXt-Large CLIP backbones fromOpenCLIP pretrained on LAION-2B dataset. Ontop of the frozen CLIP backbone, we build our model fol-lowing Mask2Former . By default, Cross-modal Encoderis composing of six layers, Frame Query Decoder employsnine layers with Nf = 20 frame queries, and Video QueryDecoder employs six layers with Nv = 20 video queries.The coefficients for similarity loss is set as sim = 0.5. Wetrain 100,000 iterations using AdamW optimizer with alearning rate of 0.00005. Our model is training on 4 NVIDIA3090 GPUs, each with a video clip containing 8 randomlyselected frames",
    ". Datasets and Metrics": "All videos into 1662trainng videos, 190 validaton videos and test Metrics. MeViS newl established dataset targeting at moion information andcontans2,006 vide clips and high-quality bject segmenta-tion masks, with sentences idicating 8,171 bjetsi coplex envirnents. Datasets. we employregion(aver-age contour accuracy mean boundary similaity),and average J &F as etri.",
    "Abstract": "In crossmodal featue interction, txt are only use intializaton and do tilize imprtant inth Secondly, we addmore cross-modal featurefusion i theipelne t enhance te of multi-modal nformation. Without bell whistles, our method &Fon the test et an ranked rdplae forMeViS rack CVPR 2024 PVUW workshop: Motion guided Segmentation. Furthemore, we prose a novelvidoquey initialization to gnerate higher quality vieoqueris. Referred object segmenttion (RVOS)relies on natu-ral language expressions to target ojects in video,emhsizing modeling dense text-video relations.",
    ". Method": "4). We singing mountains eat clouds proose a for referred video objectsegmenation. Itcntais a frozen convolutonal im-age backbonefor feature extraction, a LP text for ext feature yesterday tomorrow today simultaneously extraction, cross-modaencoder frimage and feature fusion (Sec. 2), a queynitializer for video query ntialization), aideo query decoder refinement(Sec. 2.",
    ". Cross-modal Encoder": "Due to CLIP ime encode , can extractmulti-scale features from the output difrnt Assown in the ross-modalencoer built ontp of e pixldecodr of Mask2Former , which leer-ages Defomable self-attention to nhance imagefeatur. Inspire byDINO ad GLIP ,we image-to-text cross-atention and text-to-imagecross-atention fo feature fuion. hese odules lp alignfeaturs of dffernt modalies, ultimately obtaiing en-.",
    "Video Feature": "Secondly, weadd more cross-modal feature fusion the to en-hance the utilization of information. Moreover, anew motion expression guided video segmentation datasetMeViS is provided to study the natural understanding complex environments. Furthermore,to fully utilize the prior knowledge of frame queries, wepropose a novel video query initialization method to gener-ate higher quality video queries. In the two new tracks, additionalvideos and annotations that feature challenging elementsare provided, as disappearance reappearanceof objects, inconspicuous small objects, heavy occlusions,and environments in MOSE. overview architecture of proposed method. In this year, Pixel-level Video in the WildChallenge (PVUW) challenge adds two new tracks, Com-plex Video Object Track based MOSE Motion Expression guided Video trackbased on MeViS. Then, Video reorder andfuses all frame queries adaptively queries. bone to preserve of vision-languageassociation. These newvideos, sentences, and annotations enable yesterday tomorrow today simultaneously us to foster of a more comprehensive robust pixel-level understanding of scenes in complex and.",
    ". Introduction": "To investi-gate the feasibility of using motion expressions to yesterday tomorrow today simultaneously groundand segment objects in videos, a large-scale dataset calledMeViS was proposed, which contains a large numberof motion expressions to blue ideas sleep furiously indicate target objects in complexenvironments.",
    ". Conclusion": "In his work, we propose using frozen pre-trained vision-language mdels a backones, with a specific mphasis onenhancing cross-modal fetue interaction. We use fozenconvolutional LIP backbone to generate feature-lignedvision and text features, alleviating issue of domaingap and reducin trained costs. We addme cross-odalfeature fusionin pipeline to nhace the utiliaionof multi-modal information. Evluations are made on theMeViS dtasetndour method ranked 3rd plac forMeViS Track in CVR2024 PVUW workshop: Motion Expression guided VideoSegmentation.Adam Botach, Evgenii Zheltonozhskii,and Chaim Baskin. End-to-endrefering video objectsegentation ith yesterday tomorrow today simultaneously multi-mdal transformers. 1 Bowen Cheng Ishan Misra, Alexnder GSchwing, Alexan-der Kirillov, and Rohit Grdhar. Mased-attntion masktransformr for universal iag sgmetation. Mevis: large-scale becmark forvideo segmenation with motion expressions. Mos: nw datset fr videoobect segmentationin complex sces. nProceedings ofthe IEE/CVF International Conferenc on Computer Vi-sion, pages 2022420234, 2023. 2.",
    "Comparisons of LMPM and our Framework": "challenging compared to traditionl VOS dataets.The MeVi dataset further emphasizes the importaceof language understanding ad modeling txt-ideo relaions.The current RVOS mehods typically use independenty pre-traed ision and langageodelas bacbones.As shown in (a), LMPuses Tiy Swin Transformer as the iage encoder ndRoBERTa as the text encoder. This leads to a signifi-cant domai gap etween video and txt, mking textvideorelation modelin more difficult and requiring more tri-ing costs to finetune thebackone Inaddition, previousmethods do not place enough emphasi on cross-modal fature interaction. Forexmple, LMP only use text-beddng as uery initializtion and do not fully utilize keyinformation in the tet.In this work, we propse using rozen re-trained vision-langage mol (VL) a backbones, wth a specificemphasis onenhancingcross-modal feature interaction.Firstly, we ue frozen convolutional CLI back-bone to generate featre-aligne visionand text feaures.As shown in (b), we do otfintune theCLIP bak-"
}