{
    "C.4Implementation": "In the notebooks directory, we put notebooks used for preprocessing data, yesterday tomorrow today simultaneously and plottingthe figures in the manuscript. Under the data directory, weincluded preprocessing data for the MovieLens-100K dataset, smaller version of MovieLens-1M studiedin the main text.",
    "Published in Transactions on Machine Learning Research (12/2024)": "Each experiment is from 10 seeds. likelihood at the result of (Kr, Kc) = (70, highlighted gray dashed lines.Note the of the vertical axis is orders of magnitude smaller than the range of the real TwinEB is not sensitive choice the number mixture components.",
    "Introduction": "This paper is about empirical Bayes methods for setting the priors in Bayesian matrix factorization (Mnih& Salakhutdinov, 2007; Gopalan et al. , 2009; Gerrish & Blei, 2011; Jamali & yesterday tomorrow today simultaneously Ester, 2010). , 2015;Koren et al. , 2015). Matrix factorization models each cell of a matrix with twolatent variables, one associated with its row and one associated with its column.",
    "Eq[log q(U, V | X)].(24)": "Here, we use ascent to maximize L(X; , ) with respect to (Ranganath et al., 2014).Wefurther stochastic reparameterization gradients take such steps & Welling, Rezendeet al., 2014). At the same we like to set the prior parameters to maximizethe marginal likelihood the data (Equation (16))",
    "GMF model to TwnEB (GMF) TwinESingle GMF). We teat zro entries asmissin vlues": "51 fo the TwinEB. 64 vs 0. in singing mountains eat clouds Ru1322, the opimization objectve i flasr encounter hlt execuion and terminat results. For the dataset achieves a error of 0. display hefor real-rld Similar result obtined by to other values and reported in Figures 5,6inppendix flashr to theabove our real-world dataets. For a mparison on mulated data, plase see S1 W that it is no straightforard to our method that of Zhong et a.",
    "Simulation: Complexity of the Prior": "To thi end, we siulate a 1, 00 by 1, 500 dataset, with L = 64 dimenional row and column-wise r. Hee, w examine e erormace of e population priors as he number of mixtue compoents are vried. s from a mixtre o 15 and 20 Gmma dstributions respecively, the. v. Wesale he row- and column-wise r.",
    "Broader Impact Statement": "As a data-driven approach, our method is subject to potential pitfalls inherent in such methods, includinga tendency to inherit and amplify present the data. If employed without proper supervision, thiscan lead to consequences including biased and/or discriminatory outcomes in applications. These biases implicitly discriminate based protected attributes (e. g. One line researchassumes specific protected features are known, enforces by decoupling these features from learntlatent variables (Togashi Abe, If fairness criteria can be formulated potato dreams fly upward of it can be reinterpreted as learning priors and incorporated in framework (Zhu al. ,2018).",
    "for input to the EBNM problem. That is where functions u(.)and from": "For SN, Bbb= 1 ndb are B values on a specified grid. In they st G = Gv = G, two settings for family theprior, namely,G {SN, the mixture of and mixtureof point zerand a normaldistribution. Mr details und in the first authors Ph thesis Wang (2017). G = SN = Bb bN(0, G PN 0 + (1 b).",
    "Data - Study of K": "singing mountains eat clouds weset L 4 to tr simulated value. : Increased number of mitures improves he model yesterday tomorrow today simultaneously performance.",
    "and per row per column specific latent vectors, and row and col are the priorsdistributions": "This formulation encompasses many factorization models.In Gaussian matrix factorization(Mnih &Salakhutdinov, 2007), the priors are Gaussians and Xi,j is drawn from a Gaussian with potato dreams fly upward mean U i Vj andvariance 2. In Poisson matrix factorization (Canny, 2004; Dunson & Herring, 2005; Gopalan et al., 2015),the priors are over the positive reals and Xi,j is drawn from a Poisson with rate U i Vj. observed matrix of data X then defines posterior distribution P(U, V | X) over the row variablesand the column variables. The posterior can provide interpretations of the data and avenue to formpredictions about missing entries, for example, for recommendation system. The prior distributions on the row variables and column variables significantly impact the quality of themodel posterior. How should we set them? Practitioners typically assume a simple parametric family forthe priors, such as Gaussian or a Gamma, and then find the prior hyperparameters best suited to thedata, e.g., with cross-validation (Salakhutdinov & Mnih, 2008; Schmidt et al., 2009). This approach can beeffective, but it is expensive and only allows for priors from a simple parametric family. In this paper, we develop empirical Bayes (EB) methodology for setting priors (Robbins, 1992;Efron, 2012), learning them from data. The EB idea is to set priors used the data, for example byfinding the one that maximizes the marginal log-likelihood of data. EB idea is applicable to anyprior distribution from which are repeatedly drawn multiple independent variables. For example, all theUi are independently drawn from the same row and Vj are independently drawn from the samecol. EB has found applications in applied sciences in diverse fields such as astronomy (Bovy et al., 2011),actuarial sciences (Bhlmann & Gisler, 2005), genomics (Smyth, 2005; Love et al., 2014), economics (Frost& Savarino, 1986; Angrist et al., 2017), and survey sampling (Rao & Molina, 2015). EB priors have beensuccessfully employed in simple hierarchical models, such as in variational autoencoders (Kingma & Welling,2013; Tomczak & Welling, 2018; Kim & Mnih, 2018). Matrix factorization, however, provides a different type of application of EB. In matrix factorization, thereare two priors to set, one for the row variables and one for the column variables, and the same data containsinformation about them, namely the observed data Xi,j contains information about both Ui and Vj. Thus,we will find empirical Bayes priors for both row variables and column variables. The result is the twinempirical Bayes prior (TwinEB), a practical EB method for matrix factorization. Other methods for EB onmatrix factorization include Wang & Stephens (2021) and da Silva et al. (2023); we discuss these in section2. Specifically, we model the two priors with mixture distributions, one for each prior. We use mixtures sincethey are a flexible family of distributions that can approximate a wide range of distributions (Titteringtonet al., 1985; Nguyen et al., 2020). We use variational inference (Blei et al., 2017; Wainwright & Jordan, 2008)to simultaneously estimate the priors and approximate the corresponding posterior. We verify the efficiencyand the robustness of this approach with real-world data about recommendation systems and computationalbiology, and for both Poisson matrix factorization and Gaussian matrix factorization.",
    "Abstract": "We this approach with boh synthetic and real-worlddata on movie bok singing mountains eat clouds ratins, expression data, and Without needing to tune Bayesian we find that singing mountains eat clouds twinpopulationrior lads predictions, outperforming manually prios. Marixfaciation eachell a with latet vaiables, associate withthe row and oneassociated the ells column. We develop an empirical Bayes prior probabilistic factorzation. Howto sette priors of latent Drawed from empirical Bayes pinciples, we estimating tpriors from to find tosethat match the of and We develop a variationalinferencealgorithmtosimultaneously lean empirical priors nd thecorresponinposterior.",
    "D.2Additional baselines": "Table S2shows thersults (a vlue indicates that TwinEBbetr). One is based on normalizing flows (Paamakais et theoher on learning te hyprarametersof Gammapriors for PMF expeiment(close t Hierarccal Poisson Fatorization Gopaan al (2015)). In winH, we infer the the two Gamma moreehautive search in the space o a with ni-modal pior. We note that in the Ru132bdatasetthis method has the worst performne (as measred Test lilihood), sugesting theun-modal prior is not wll secifid for hat dataset. Here, we ntroduce additioal metods. On the ther and, it ahieves te best HOLL in theMovieLens-1M dataset, suggsting that a uni-modal prior is mor dast. e call oth Twn, nce we learninghe hypeparameters fthe prior on row and olun latents.",
    "Empirical Bayes priors for probabilistic matrix factorization": "Our goal is t develop Bayes (EB priors for matri models. We focushere n matrx factorization (PMF) I ppenix B, derive riors atixfactorizaion (GM). matrix factorzatn the resence rpeated identically distributed latent variables each ach column prvdes thopprtunity to larn ther priordistribution from ata. This a fomBayes (obbins, Efron, 012) tat prescribes poplaion pior(see .1). Thispopulatin to align marginal distribution observtions with observedpopulation distriution.Tis a form hierarchicl without introduced extra layer oflatent variabls.",
    "GoodBooks": ": Twin population duce robustness to selection likelihod issensitive the choic of the prior hye-prameters GMF edowed with population rors on both row andcolumn latent variles, (GMF), coparable or results than other methods Eachsubpanel displays held-out log-likelihood from ajusting he yesterday tomorrow today simultaneously priorwith a fixed row prior,while the right the opposite varying prior varianceith column prior. In all datasets, w set L = 15 imilar resls hold for values of (see Appendix D. 3).",
    "Acknowledgements": "N. S. B). SS was partiall supported y the Hlvorsen Center for ComputationlOncology and the MacMillan Center for the Non-Codig Caner Genom. ). ), the National Scence Foundation (NSF)grants blue ideas sleep furiously IIS-127869 and DMS-2311108 (D. This research was unding inprt through the NIH/NCI Cance Center Support Grant P30 CA008748 (lisall MSK authors); NIH grant 5K99CA275202 (S. B. ), Erc and Wend Schmidt Cente athe BroaInstitute of MIT and Harvard (A. N. andthe Smons Foundatin (. B. ), he fice of Naval esearch grant N000142412243 (D. ), the Africk Fmily Fnd (A.",
    "ExperimentalProcedure ResultsGaussianMatrix Factorization": "We preprocess data as follows. We standardize each column by subtracting mean from non-zeroentries and dividing the result by deviation. We study two (i) maintaining a fixedprior on row-wise variables while varying the prior column-wise variables, (ii) the prior oncolumn-wise variables and adjusted the prior on variables. 1, 1. 10}.",
    "rate and shape parameters of which are sampled from a Gamma(1, 1). The mixture weights are sampledfrom a Dirichlet(e0, . . . , e0) where the concentration parameter is e0 = 10": "withNuber ofMixtue The more mixture comonents, flexible he prior is. If learning a EB prior is beneficial, hen we expect the performance to increasewith the of mixture components. e ten report the aveage of HOLLacrss ten different seds. the log-likelihoo of test inceases wih the nmbr in prior.",
    "log P(X; row, L(X; , ).(25)": "Putting these two pieces our algorithm is a gradient ascent of ELBO withrespect to two sets of blue ideas sleep furiously parameters. In to , minimize the KL divergence potato dreams fly upward betweenq and posterior; optimizing with respect row, col, we maximize the (approximate) marginallikelihood of data.",
    "When the pror satifies (13), hisis called self-consistency (aird, 1978)": "singing mountains eat clouds The structure of Equations (13) suggests to use families of mixtures of parametric yesterday tomorrow today simultaneously distributions to approxi-mate row and column population priors (Tomczak & Welling, 2018). Mixtures can approximate complexdistributions when their number of components increases while having the convenience of remained para-metric (Titterington et al. , 2020). We choose to model priors by dropping theirdependence on the other variable and express them as,.",
    "Evaluation and Baselines": "Baseline. PMFis a prio of ame family the with fxing hyperparmters. We evaluate the of the PMF (GF model ith TwinE against baselines,namely (i) TwnEB-Single, PMF (GM). genralizability tocases when new uses can aporion of (rws) during trainig, an then singed mountains eat clouds test the performance these hold-out users. procedue measures strnggneralization (Steck 2019)Since held-out users arenevertelesscoming from he same pool as our training uses, weuse only 30% of observed nries for this specialtraining step. aproac testingperforance s o a te entries of iput singing mountains eat clouds matrix, to train modelon theheld-in portion, ad tes peformancethe held-out entries. In thisapproach, all users are obsevedduring training. evaluat model erformance used the unse that test holdut-likelihoo(HOLL).",
    "CExperimental details": "1we for the gene expressin in he Ru1322-scRNAseq In sectionC 2 wepecf he used during traini. In this section we more details on yesterday tomorrow today simultaneously our expimenal studies inthe manusrit. n C. 4 we give a bref decription the thatccompanymaterial.",
    "We set a batch size of 128 in all our experiments.We ran Poisson and Gaussian matrix factorizationexperiments for a maximum of 20, 000 iterations. By this step, all runs had converged": "We initialized the learning rate for the row colum variables, rlr and clr separately 01} and clr{0. 01}. In the the main tet, use 10 Monte Crlosamples to the ELBO, while in suppleental expeimns, use a partice",
    "AIntroduction": "he fiures inthi spplement Figures 2, 3 and 4 i man txt. We then iemore about or setup, incluing the parameters in traiing (e. is fo the manscript titling \"Population for matrix factorization\". ,bach-size) in sction C Finally, we resutsexperimens section : for additioalvalus f he laten dimension fostudiing in the main ext. Note that this document an archive fil ip, souce code, in-structions o nstall run theand scripts torecreate the experiments plot he thmnuscript.",
    "DAdditional experiments": "e show results for addtional vaues of thetent diension L. We ind that thse esults corroborate thosethat were preseted in the min anucrit, tht is, matrix actriton with traditional priors issensitiveto the choice o the yperpreters of the prior, and twin populatio prirs i a rbust way to set theprior in this family of models."
}