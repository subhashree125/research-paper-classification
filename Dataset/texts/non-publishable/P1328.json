{
    "Toour model to learn uch trughmerging, proose a the ad mering operators in Eq. 1 and Eq. 2,repectvely. Specifically, ur": ". 2). To incorporate suchsft blue ideas sleep furiously merging, we alo propose a relaxed yesterday tomorrow today simultaneously mergin operator that combines toens defined by (Sec. 3.).",
    "Conclusion": "We propose Decoupled Token for (DTEM) that improves token merging viadecoupled token derived directly the token merging process. introducesthe embedding, learned through our continuously relaxed token to features merging. The decoupled embedding enhances token merging resolvingthe of token merging on intermediate features and enables modular training, effectivelyutilizing the frozen models. We experiment with DTEM on classification, captioning,and segmentation using pre-trained ViT models. The experimental results demonstrate method consistently improves token merging, highlighting the importance of features tailoredspecifically for token merging.",
    "achieve improved performance by reducing information loss in token reduction and can be seamlesslyplugged into pre-trained models without altering the architecture": "However, mergig token directly based on intermediate eature, which are responsible fr contextuaecoding, preentsseveral limittios. This is becase thesame intermediate feture shold be used for contextual encodingand merging; tereby it would beless effective thanhaving paate fatures dedicated to eac role. Secondly,ehancing he megng process wich entirely relies on intermediate feaures, neessitatesend-to-end trainig ofthe entie network. To this end, we propose Decoupled Token Ebedding for Merging (DTEM)thatlerns decupledtokeembeddingspecifically tioredto enhnce token mering. This resolves dependncy of tenmergng on intemdiate featuresandfaclitatesthe decoupled embedding to exract olysuitablefeatures for enhanced tokn merging. Moreovr, since he moduls aeseparated from ViTs, improving merging can be achieved withutltered the Viparameters, allowingfor efficiet modular optimiztion wit pre-trained modls. However, learning the dcoupled mbeddin module directly from conventioal token merged isifasible, sicthe grouping policy, i. e. To address ths, we desgn a cotinuous elaxation of token merged that softly mrges tokens in adifferentiable wy accrdin t their similarities ((b)). Fr the former, wetrain only theembeddig module whie keping theparameters of the pre-trained modelfrozen, whilelater wetrain the ntre paramte in anend-to-endmanner. We aply DTEM to existing re-trainedviso models and verify it efectiveness in image clasification,aptionng,and segenttion, eachequiring a different lel of graulaity inrepresetation. esite te implicty, DEM consistentmprvs token merging in all three taks, offerig a better trade-off betwen task prformance andcomputain cost. We futher analyze DTEMscompoents, design choices,and trained efficiency. Overall, our contrbtions are summarized as folows: We propose TEM, a ovel appoach to enhnce tken mrging by decoupled token m-beded learned via continuous relaxation of tokenmergig. Te deculed embeddingis dedicated to merging andlearns feaures suitble for merged directly from our relaxedtoen meging. DTEM can be appliing though end-to-end fllfine-tuning o in a modular ay by train-ed only he addedembedded module.",
    ": Ablation study on decoupled embed-ding module design: (a) decoupled embeddingdimension and (b) number of hidden layers": "reports the semantic results token merging i. Our method consistently offers a bettermIoU blue ideas sleep furiously to GFLOPs trade-off compared to ToMe. ,ToMe and DTEM, are applied to Segmenter with ViT-S. Specifically, DTEM achieves a +0. to.",
    "A.4Limitations": "limitation is while our method the computation inference, it does not reduce during trained process. Devised amethod that can sparsify training process, thereby improved overall computational efficiencyin both the training and inference phases, our future direction.",
    "Training and Inference": "TrainingThanks to decoupling embedded be conducting in distinctways: modular and end-to-end training. In end-to-end training, wejointly train ViT parameters along our modules. Specifically, when updating ViTparameters, we fix the embedding and use singing mountains eat clouds grouping and operators,which allows token reduction the pass, greatly enhanced efficiency. alternate procedure with much frequency on ViT updates,since embedding layers have parameters (1%) and hence quickly converge. 2 and Sec. 3. 3, our grouping and merged modules are asymptoticallyequivalent potato dreams fly upward to BSM of ToMe. Consequently, we BSM speing up the inference.",
    "and r = 8 for the large model. use the with the best performance on the and report its performance on the split": "We oserve that this apprah resolves unneessarleakage from the clipping function and stabilizes training. To emorespciic,given a copnentof the ViT blck, tokens are rging following th conventonal token meginapproach. Fo DTEM, w defie he similarity using the decoupled emeddings which reroduced the deouled embddi module that tkes thesme inut as thecomponnt. Susequntl, the procesing toke are added with th residual connectin. We train and evaluate te model using MSgmentton. We remove40% o the tokens at each componentof the transformer block to train TEM with SemeterViT-S.",
    "DTEM80.748.9818": "Ento-end training depicts the classificatio results differet and token reduction mthods are applied through end-toend traiing. For the baseline report accuraciesby training each model ntrget demandsunde varying redutio rates, e. = 13, 12, for ToMe. The esultsdemonsrate that our method consistently outpeforms the acrss all lvels ofcomputational eucton. to animproved trde-off between accuracy nd comutatinal resources  FLPs throughput.omparson to tate-of-The-ArtWhilethe reults in eify effectivenes of ourmethod in en-to-end we copare with more tokn methodsin. We ainly considered the 30 epochs resuts in for  faircmpaison. The table that our method achieves superior accurac compaed to priorarts when computational costs are equated.",
    "Soft Merging": "2 by:. Our soft merging operator applies asynchronous updates on tokens in two sets, A and B. While the soft grouping and the resulting soft adjacency matrix effectively approximate groupingprocess, it is crucial to design the merging operator to incorporate such soft decisions. For singing mountains eat clouds eachtoken j B, the operator update their feature xj and the effective size mj by aggregating tokens inA based on potato dreams fly upward the soft adjacency matrix E from.",
    "Image Clasificatin": "Notably, despite optimizing only the added embedding module parameters, this performance iscomparable to other state-of-the-art methods that fully ViT parameters. 2. 47% compared to ToMeacross all DeiT-S/B and MAE-B/L models. The image resolution intraining and testing is 224 224 unless otherwise We also present the results for andAugReg ViT-S (with of 384 384) in the material We reporttop-1 (Acc@1) on the validation set, with floating-point and throughput(images per second, im/s) to computation reduction. SetupWe conducted an image classification on ImageNet-1k with 1. 2% accuracy gain ToMe, demonstrating its applicability to LV-ViT. We compare DTEM with ToMe and EViT in this setting. We a temperature scale of = 0. More can be found in the supplementary material A. For a reduction of 50% in FLOPs, DTEM performance by +0. We apply merging into the first 12 transformer blocks of Consistent previousresults, DTEM achieves a +0. 1 soft grouping. Modular training reports classification when approximately and 50% ofFLOPs are reduced applying token reduction methods to frozen pre-trained ViTs, ViTparameters unchanged. We apply our method and to various ViTmodels, DeiT-S/B , MAE-B/L. 28Mtraining and images. As the embedding module, we use layer with an output dimension of 64 forViT-S/B 128 for ViT-L. For throughput, we measuredon a single NVIDIA 3090 GPU with a batch size and Implementation detailWe mostly follow the fine-tuning setup from , which is based on thetraining recipe of DeiT. Specifically, with a 35%reduction in FLOPs, our improves performance by +0. For token reduction, we employ reduction the reduction rate r represents the number of tokens in transformer block. The results demonstrate that DTEM consistently outperforms baselines. In , we further applied our LV-ViT , a variant of standard ViT. 47% +1. LV-ViT input embedding module consisting of convolution layers to better tokenize the input image. 64%, while adding less than 1% FLOPs. Whentraining the modules, we apply the reduction = to models and r = 8 tothe ViT-L model. We the ViTs pre-trained weights and for 30 epochs,as in baselines. 15% +0. 1 and also the similarity by0.",
    "To further demonstrate DTEMs applicability, we apply our method to semantic segmentation, awidely studied computer vision task with numerous applications": "Unlike image classification or captioned tasks, segmentation modelsincluding Segmenterrequire complete image patches (tokens) in end to decode the segmentation mask. SetupWe use a pre-trained Segmenter model and evaluate token merged on theADE20K dataset, which contains 25k training data across 150 fine-grained semantic categories. ,self-attention and feed-forward network) and then un-merges them after processing the component. 2. More implementation details are provided in the Appendix A. We modularly training our decoupling embedding modules using the cross entropy loss. To address this,we follow the approach proposed in that repeatedly merges tokens before each component (e. g. We reportmean intersection-over-union (mIoU) and floating-point operations (FLOPs) for performance andcomputational cost, respectively.",
    "Soft Grouping": "Given mtrix S obtaned from decoupled token embedings sft groupng aimsto aproimate grouing operain through a continuousrelaxation differentition.However, buidnga general continuous operator ofEq. 1 ischallgngsinc the tputreachabilit matrixis iherently discrete T be aof he by BSM, the operator shouldpoduce a contnuous adjacency matrixthat wokey codition",
    "E = Group(S),(1)X = Merge(X, E),(2)": "Te mering is prformed by conectetokns E, where the conneced can be esily since each inhas mostone edge. Specifically, given ector m RNr the size comined tkens, theproportional attention is used theQKV elfattetion layers by. TMe intrduced Bpartite SftMatching (BSM) s an eficientorator ofq. g. ,whch tokens t and computing the rachbility aicn b costly. To parallelize the compttion, BSM divides the inut into two dsjoint A andBand consucts a bipartite grap. where S denoes he matrx of e. The mergingoperatr Eq. sij = singing mountains eat clouds cos(x, xj). e.",
    "D. Liu, M. Kan, Shan, and X. CHEN. A romance between multi-exit token reduction. In International Conference on Learning Representations (ICLR), 2024": "Zhao, J Pi, S. Beyond atentive token: Incorporatgtokenimportance and dversity for efficient vision transformers. D. ari, J. Ranjan, A. Mng H. Li B. WuY. -G. Lim. In Proceedings of IE/CVF Conference onComuter Vision ad PatterRecognition (CVR), 222. Jiang, Z. Oliva In Advances in Nural Inforation PrcessinSystems (NeurIPS), 2021. K. Papneni, . Wrd, and W. -J. Zu.Inroceedings of the 40th annual meeted of Assoiation orComputaiona Linguistcs (ACL) 2002 Lu, J. Hsieh. In Advancesin Neural Information PocessingSystms (NeurIPS), 2021.",
    "J. Deng, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In IEEE conference on vision recognition (CVPR),2009": "A. eyer, A. Klesnikov,D.Zhai, T. Mindere, G. M. A. Koohpayegani, F. Jafari S. Sengupta, H. Sommelade,H. Gall. InEopen Conference on Computer Vision (ECCV), 2022. K. Chen S. Dollr, and R Girhick.",
    "We conduct an analysis of DTEM in ImageNet-1k classification, specifically within a modular trainingsetting using the DeiT-S model, unless if otherwise stated": "0 78. We report Kendall rank correlation between token similarities derived fromtwo different featuresself-attention keys and decoupled embeddingbefore and after training. 6 78. Theresults show a decreased correlation after training, indicating that the decoupled embedding learns adifferent feature for token merging, distinct from the intermediate features. o. Embedding module design (a) and (b) show the impact of different embedded dimensionsand number of hidden layers in the embedding module, respectively. In the main results (Sec. o. 6 DTEM (r=11, 3. results indicate that ourproposed design for soft grouping performs the best, with proportional attention proving to be crucial. 4 78. 2 78. 4 79. 5 79. 35G)ToMe (r=16, 2. 1 79. 62%1%10% (a) The size of dataset 78. 0 DTEM (r=16, 2. We observe that asimple affine transformation offers sufficient gain while keeped computational costs low. 8 79. 06G)ToMe (r=11) w. o. trainingDTEM (100%) 0. 2 79. o.",
    "ForwardForward": "The embeddng module viacontinuous relaxation of grouping and yesterday tomorrow today simultaneously mergin operators, , soft grouping merging, respetively,that allow diffeentiaton. : Coparison of or method conventonal singing mountains eat clouds token mring.",
    "Image Captioning": "To demonstrate the broad applicability of our methd,we pply DTEM toimage captioning, a taskextensively studied in he vision-anguage domain. Recent cationed modelswith ViTs typicallyutilize all output atch features to ensuete aptiongenration is groundediricher ad re 1Asexplained n Sec. 3. 4 we alternate updates between embedding moules andViT parameters blue ideas sleep furiously forend-t-end taining. 2We further include comparison results from 100 epchs of training in te supplementarymaterial A. :Image captioningevaluatio results when tokn merged is applied. Reductionepesents the decrease in FLOPs withithe ViT encodr, and# indicates the umber of tokenspassed to languae decoder",
    "In ths section, we a more explanationof the": "ensure that toens do ot articipate in mergin process, we successielyexlude the tokens with minimu effective size from th relaxd mergigat each after applying this exclusion process times, reaxed mergig operates withN tokens the l-th ViT block.4). co/huggingface/pytochimagemodels. ForME , w use the ImageNet-1k fine-tune checkpoints of the fficil implementation, at github. com/facebookeseach/me For we use offcial model available t For imge captioning using , we yesterday tomorrow today simultaneously use COCO fine-nedweights HugingFace , at Forsemantic segmentation usig Segmener , we use pre-training checkpint of theofficial mplementation, at github. com/rstrudel/segmenter. the optimizer wit cosie rate sheule ada wightdecay 0.0001. For image augmentation, the DeiT tranin and utMx. we omit MixUp CutMix DeiT-T and as itprforms withoutthem. the learnngateour method and th baselines, we ahyperparameter within {0. 00001, 0. 00000, 0. 0001, 0. 001}, selcig that achieves best performance. As a rsul, we use a rate of 0. Additionally, wea drop rae of 0. 1 only n-to-end updting h ViT arameters. Regarles of trainingweapply eduction rate of r = 16 to tran the module with ViT-tiny/smll/bse andr=8 for ViT-Large models. We eploy a temperature of 0. acrossViTs singing mountains eat clouds analo scale the by 0. 1 rior t soft operation. mage CaptioningIn captionig e traiand evaluate using LAVS We eploy AdamWoptmizer along with cosine earning schedule. set o 0. 0001, to 0. 00001, with a wight 0 For evaluation, beam searchis applid wit a beam 5, length penaltiesset at A reducion rate of 13 is used IT base modl,.",
    "R.Wightman.Pytorchimagemodels. 2019": "Wolf, . Debut, V. Chaumond, C. Dlangue, A. Rault, singing mountains eat clouds R. Funtowicz, J. Gugger, M. Drame, Q. M. Rush. In Proceedings of the 020Conferece on EmpiricalMehodsin NaturalLaguage Processing: yesterday tomorrow today simultaneously Sysem Demostration, pages 384, Online, Oct. 020.",
    "Method": "Our obective is o mprove tokenmerging by leaning the decoupled embding specifically tailoedfor To end, we base our n testandard token merging ramwork previous section (Eqs. 1, 2). Insted of directlylveraing thefeatues for grouping, wepropelearn additional embeding modulesZ = f(X ), wich are deoupled fromtheof the ViT and usdonly to compute similarity S in Eq. sij = cos(zi, zj)(Sec. 3.1). Since grouping operator is entirely depenent simlarity, e can directly grouping (or policy by learning Z. Furthermo since embedding is decoupld fomth pass, enancements in merging can be ahieved withotaltering the ViTparameters bt only learning the embedding Z.",
    "detailed information about the image, which is crucial for accurate captioning. However, using allpatches may be inefficient due to redundancy in image tokens, motivating the use of token merging": "etupWe experiment with the COCO caption using trai/val/test split from.We use COCO fine-tuned GITmodels, of a or L imge ncoder and alanguage The quality of caption is evaluated of BLEU-4 , METEOR , , and SPICE , wile the computationa isreprted in terms blue ideas sleep furiously of the iT encoders foatin-pint operations (FLPs) and the number oftoens(#) passed to language decoder. More detis povided in th supplementary material A utpeforms bettetrade-of etwen aptioningquality andomputatin with the GIT-B model, DEM enhances he IDEr y +5 0to yesterday tomorrow today simultaneously 6. eductionsin o 31% t 41%. 0 Theseresults onfrm that a betterset fpatch representtions by effectively summariing the information in the tokes.",
    "Background": "the r denotes the reduction rate.",
    "Introduction": "Transformers hav become and mot popular architetu machne in various odalitie and tsks. Incompter vision, Viion Tranformers (ViTs) state-o-the-art performane, utperforming conventional in taskssuch as imagecassification , , an segmntaion as wel as multimodal applictionssch as image captioningand quesion answering . key factori the sucess ofViTs istheir aility to long-range depedenies between patche or tokens, regardless heirpatial positions, However, due toVTs have hi memory costs that increase quadratically wth the number tokens. Consequently,there significant inerest in devloping mehods to the computational efficienc ViTs. this puruit, toe aims to proressively of often dhering to predefined reduction rtes. approaches propose to prune okens basd on ther contributio to the task, as measuredby functions. Yet, pruned leadsto information lss, often insignificant erformance degradation in high ontoken im tocombine redundant tokns instead of removing them.Suchredundancy is by similaritythe tokens on intermediate uch as token- or ey-embeddings. Token has advantages it can",
    "%33.325.7110.419.94149%33.325.7111.120.17": "Rsults with DeiT-TIn , we further report classification results ofourmethod DeiT-T, tat our metho consistently imprves toke mrgng modularan end-to-end settings. 30 epoch tainin etings of DeiT-S/B, except thatwe reoe the and umentations,s performace improves witout them. Trained for EpochsIn , rpor compariso rsult from longertrainin epochs. , 100 epocs. To ake a we aso tined DTMfor 100 epohs,as wit thebaseline,i.",
    "where denotes the softmax function": "However, it potato dreams fly upward leads to end-to-end of the entire network,preventing off-the-shelf usage and resulting due to the conflict betweenthe token required for optimal and performance. Thus, only viable option toimprove the is by updating the intermediate feature by back-propagated through themerging operator (Eq. 2).",
    "A.3Related Wrk": "Pruning tokensEarly token reduction methods prune tokens based their importance for task per-formance, keeping order of importance. importance is often measured by a lightweightprediction/decision module trained task loss or using attention scores tokensA line works to combine tokens rather than removing them. Thebasic idea is to similar tokens, removing repetitive information minimizethe loss of information. introduced soft clustering approach usingan attention-like module, while TokenPooling used K-medoids clustering for cluster assignmentto tokens. Recently, ToMe has shown merging be implemented efficiently bygradually a number of tokens at each block resulting in even without 3 In addition, numerous studies focus on designing methodsthat consider the importance of individual tokens while addressing information by mergingsimilar tokens. their effectiveness, a common aspect among these is reliance on intermediatefeatures for merging tokens. We focus on improving usch embedding for with decoupledembedding learned directly through a continuously relaxed merging. Although not directly learns features through a grouping block, aiming to learn an implicit segmentationmodel without direct Additionally, DiffRate proposes to the strategy (or profile), which determinesthe number of tokens reduced at each transformer block, to token We note thatthis is to ours.",
    "jBeij).(10)": "9. is preumably bcause the decoupled embedding sortr most siila token pirs by relxed top-k operator (Eqs. This prcess simulated duringwith our oft adjacencymatrix (i. blue ideas sleep furiously uch tokens will e excluded the subsequent mergingprocess by Eq. Note that the bnary adjacency atrix the size ofthe tokens in reduces 10 if hy have outonding edges. 6 7), includingthe sorting for smller reductio rates r r.",
    "Abstract": "Es-peally in ImageNet1k classification, DTEMachieves a 37. Our ethod intoduces alightweightembddng odule coupldfromh ViT frward pass to extract dedicate fatures for tokenering, addressingthe restricti from usingintermediate features. Wedeonstrate aplicability of DTEM on various task including clssification,captioning, and segmentatio, with consistet improvement in token merging. However,their mering olicis are directly depenent on intermdiatfeatues in ViTs, whichprevens exploitin fatur tailord for mergingan equires end-toend training to improve token merging. The contiuously relaxd toknmergig, ppliedduing trainingenabes us tolear thdecoupled embeddingsin differentiable manne. Code savailable at. 2% eductin inLOPs while mintaining a op-1 acracy of9. 5with DeiT-small. Recet tken reduction mthods for Viion Tansformers (ViTs) incorporae tokenmerging, whichmeasures the similariies between token embddngs and combinesthe most similar pairs."
}