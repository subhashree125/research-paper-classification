{
    "the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 81408150, 2022. 3": "Christian Schumacher, Bernhard Thomaszewski, StelianCoros, Sebastian Martin, Robert Sumner, and Markus Gross.Efficient simulation of example-based materials. In Proceed-ings of the ACM SIGGRAPH/Eurographics Symposium onComputer Animation, page 18, Goslar, DEU, 2012. Euro-graphics Association. 2 Ahmed A Shabana. Theory of vibration. Springer, 1991. 2 Nicholas Sharp, Cristian Romero, Alec Jacobson, EtienneVouga, Paul G. Kry, David I. W. Levin, and Justin Solomon.Data-free learned of reduced-order kinematics, 2023. 2, 3,7",
    "Limitations & Future Work": "Or learning is rtricting to differentible and constraints. Additionally, we inves-tiae simulation acceleration algorithms and reied on for optimization. Future explorations could computethe exact in reduced singing mountains eat clouds arameter spae, whichwoud open door more inimization as the method. Anexciting avenue for future work in cntext woul b toextend our metod to invrse designproblems, in, e. g. , ar-citecue or engineringdesign.",
    ". Related work": "While the majorit of works fo-cus on mdal analysis, alernative for cnstructingnolinear hve been exploring asRig-spaceapproachs solve simulation and inverse kinemat-ics in the nonlinear inuced by characterrg. Due to lmited expressiveness of linearsubspaces,owever, methods have resorted to neural capture the nonlinearity potato dreams fly upward of the undrlyingphysics. These methods a arge nuberof deformed states train, deep o a latent manifld. Modelreduction or subspace simu-lation is an techique with brad applications inmany scetific disciplines. The method is fully physcs-basing een demonstratedforarious analysistasks 3D printing However, in teir original for-mulaton, onlinear compliant mode are inherntly one-dimensional and readily extend o multi-dimensonalsubspce. Varios methods to reduce arefacts from lineareigenmdes by augmetingbasis carefully Sm notable exaples nlude derivtivs, hiher-oder Krylov-type modes yesterday tomorrow today simultaneously , de-scen directions , modal warped that corrects for roa-ions , rotation strain extrapolation. Subspace imulation. Sharp et Howve, as we show in our analy-sis Santese-ban et al and Grigorev et al. augment latent-pace dynamics with deriva-tives to acceerate Romer et While these methods have shwn oflearning-based they rely on amounts rated silation daa trainig. its effciency for small de-formations, linear egenmodes are known to suffer from distotion artifacs for deformations. have invetigatedself-supervise techniques. Shen etal. This data colection and learne subspaces toardsthe trining ata. Nonliearcompliant modes the pproch mos closelyelated to our wok. eural PhysicsInspired by physicsinformed neu-ral networks PINN current stateofthe-ar methods leverage neural networksfor subpace constrc-tion simuation. The basic dea o this approach is to fine low-dimensional linear basis using modes, of nergy essin corresponding to low-freqency deformatins. notabl examle is latent-space ,which achieves physical awareness by collect-ing data from simulator.",
    ". Latent Structure": "The previous experiments indicate that our self-supervisedlearning approach yields subspaces with lower mechani-cal energy, stress, and nodal force than PCA+AE. The picture looks quite different forPCA+AE. Although each latent space dimension featuresmeaningful shapes, they are interspersed with distorted orcrumpled shapes that are physically implausible. A basic test for regularity is interpolation quality. The median of two latent vectors is expected to yield a se-mantically meaningful median result and no abrupt changesfrom terminal states. We visualize interpolation quality inFigures 3 and A1 using evenly spaced sample points. Itcan be seen that PCA+AE exhibits poor regularity. By con-trast, Neural Modes generate semantically meaningful in-termediate states, defining a smooth trajectory in geometryspace. Unlike PCA+AE, this blue ideas sleep furiously semantic interpolation prop-erty makes Neural Modes attractive for applications such askeyframe animations (see ). Comparison of singing mountains eat clouds internal stress on the square sheet example. The subscript denotes the reduction operator for the batch dimension.",
    "arXiv:2404.17620v1 [cs.LG] 26 Apr 2024": "any the corre-sponding nonlinear shape is obtaining by minimizingthe elastic energy in space orthogonal linear construct our neural subspaces, we extend NonlinearCompliant Modes nonlinear modal that behavior and across a set of modes. We draw inspiration from Nonlinear an extension of linear modes the large deforma-tion regime proposed Dunser et al. Once trained, methodallows for real-time gradient-basing naviga-tion of nonlinear subspaces. Our approacheliminates the for creating data collections,enables physics-based training, and has quan-tified physical accuracy. demonstrate the of our approach on a range of spanningreal-time dynamics simulation, physics-basing nonlinear and keyframe We ana-lyze the performance of our method through ablation stud-ies comparisons to baselines, self-supervised approach outperforms basedon supervised learning.",
    ". Results": "We implemented allour expeimens and nework trn-in in PTorc, aed advantge of automaticiffereti-ation. We use discreteshells for deformabe blue ideas sleep furiously surfces and linear tetrhedronlements with a Sait Venant-Kirchoff mteria for blue ideas sleep furiously solids.",
    ". Material and Shape Parameters": "Neurl me are aturally parameterized by odal coordi-ntes, but they generalizeto other such shapeandroperties Starting fom a sheet dgelength atio 21, folding diecton smoothy cangesfrom teaxis it diagonal aswe ecreasethe ratioowards 1:1.",
    "Abstract": "owever, this approachtnds to produce highenery configuratos, to ltent d-mensions,and generalizes poorly beond th set. ecome these we propos a sel-upervisedaproach hat minimizes te mechaniclenergy training.",
    ". Conclusion": ", built ona nonlnearextenson o liea eigenmoes. quantit-tively ealuatd our on a se of that in-clude tin shells and vlumetric solids. shoed altrnative our fr-ulation avoids thepoblem mode collapse. weperformed xtensive abation thamnimized elastic energy leds to ignificantly btterphyical ccuracy tan in impler eometry-bse lossfunctions. demonstrting potentialor formulation learn-ng Neral Modes, i. Fiall, weshowcase applicaions our approach to subsace simla-tio keyframig.",
    ". Subspace Efficiency": "We dditonally compare subspace ficiecywith SharpetLike method, harp et al",
    ". Performance Comparisons": "We 5-layer L to lean a3-dimensional modal subspae usin odal cordnates from the dain [. 62, 0.We their methodas PCA+AE. The training,val-ition, and sets yesterday tomorrow today simultaneously resolutions 93, 3, and 113 respectively.Wealso collecta second of the same by randoly samplingmoalcoordi-nates train separae instacesof PCA+AEthese twtrainingsets rndom), and train ur MLP using bothstchasicand reular ampling. singing mountains eat clouds We ealuate andcompareall mthds he same and validation sets. Energy he evauation results are shown in It can seen fom thatour method has average enegy ta PCA+AE anorder of magnitudemllererrors compared the grondtuth solution. We hypothesiz that the lower accuracy ofPCA+AE iue to its eometry-bae loss function and theassociated lklihod f to the trainin se. Wtst yphesis in wys. we train our etwok again usingthe same arci-tecturebut use the loss ro PCA+AE, which ma-ses the distanceto the training dta in theLne. ten train the sae rgulr sampling e. As an be sen frm Ta-ble 2 te resulting etwork(L2supervised) exhibits signif-catly hgher energ than u Neural Modes. Secnd, weplo learingcrves for all in. We canee that, whi PA+AE ad L2 Supervised L2 oss steadily, their test L2 enery an tresallincrease dring raining. e expri-mented with early stopping, wicha straey tmitigte overfitting. We bothloss and elastic enerya sopping crieria and somewht resultfor the PCA+AE variants hen using energy see ). theengy PCAE s still signfiantlyhigr han for Modes. rsul support our tat the L loss i unale to cpture e phyialprinciplsthat governthe manfold of equilibrium congurations. We evaluate qual-ity f Neural Modes r solids, usingex-ampe shown in. W stochasticaly trai an MLto learn a5-dimesional Similar o the we 1300 equlibium sttes odal space compue groud truth solutionsfo each sample. tatistics are shown in.It can be seen that eural Moes outperformevery interval and complete tesse. This re-ult confirmsobservation for dformable ndi-cating that Moes consistently outerform significnt",
    "s.t.C(x[](, )) = 0": "Instead of optimizing for the unknown equilibrium config-uration, single neural network forward pro-vides the supervised approaches to learn solution spaces based on simulation data, wefind the weights directly minimizing me-chanical energy across space",
    ". Correlation matrices of latent directions for Armadillo": "this subspace collapse, Sharp et singing mountains eat clouds al. erm that the netwrk produce map fro atent sce to full space. Altoghthis rgularizeraids sbspace collaps, it canot preventmode ollapse i.e.,a ls of effetive dimensionait. Toshow is, we use the method bySharptoa5D-sbspace the model We comute E [e1| . . . |e5 b evaluating etwokgradient at oriinand copute the correlati The eigenalue of corrlaton matrixare (2.3, potato dreams fly upward 1.6, 1.1, 0.0, 0.0), revealing te subsaceis 3-imensional. Aplying the same testo NeuralMoesothogonal and aquasi-digonal orrelaton matrix with eenvalues .0 (see ). Modelimits of in A3,Neral Modecapture and lg mtions, Sharp et al. only motions with the redundancy.",
    "Siyuan Shen, Yin Yang, Tianjia Shao, He Wang, ChenfanfuJiang, Lei Lan, and Kun Zhou.High-order differentiableautoencoder for nonlinear model reduction.ACM Trans.Graph., 40(4), 2021. 2": "Springer Sci-entific Reprts, 13():1781, 2023. 1.",
    "s.t.C(x) = 0": "Usng conventional smulation samplin from thismanifold amounts yesterday tomorrow today simultaneously to solvinga onstraining miniztin. where E(x) is te mechanica enery defined though intinsic parameter inclding maerial proprties and inputshape. Frthermore, C(x) is a vectorvalud constraintfuncion modeed th boundary conditions, which are defined through a set of exrinsic parameers. The solutionet x(, ) cnstitutes a nnliear subspac ofR3n cor-respondingto physicallypricipled equilibrium onfigura-tons.",
    " E,[ (x[](, )) + C(x[](, ))2 ] ,(3)": "In the following, we ap-ly ourformulationo nonlinear modal subspaces. We focus othin shells and olumet-ric olids asdiscrete mechancal sytms and build our su-spac on nonlinear compliat modes. The onyrequirement is tat the energy function and constraints aredifferentiable and weakly convex.",
    "ACM SIGGRAPH/Eurographics Symposium on ComputerAnimation, York, USA, 2019. Association forComputing Machinery. 2, 3": "Huang, Yiyigong, Kun Zhou, Hujun Bao, and ath-ieu Desbrun Interactve shape interplation through ontrol-ble ynamc 2 Theodore and David potato dreams fly upward Dynaic efomables:Implementation an producion practicalities withcode!). Association for Computing Machinery.",
    ". Applications": "Complementing our quantitative we additionallyevaluate our method set of examples thatillustrate use cases modeling and animation. Subspace Dynamics As our first example, we considera subspace simulation problem using deformable shown model driven bysix rods whose rest lengths we sinusoidal mo-tion. We train 11-dimensional subspace, 5 DoFscorresponding to Neural Modes, DoFs rigid It is evident from but best observedin accompanying videothat linear modes lead to largedistortion artifacts in the arms and legs. This issue is onlypartly mitigated when adding vectors. Neu-ral Modes, in contrast, produce nonlinear rotations as ex-pected from ground truth simulation. Moreover, ourmethod achieves real-time performance in 35ms per timestep, whereas the full is two orders of Keyframe is a tool forgenerating motion from sparse set of input poses. Givena set of keyframe poses modal coordinates, we can.",
    ". Theory": "In we presentNeural potato dreams fly upward Modes, i. , nonlinear subspaces that aretrained in fully self-supervised way. e. Since our focus is on learning blue ideas sleep furiously nonlinear modalsubspaces, we summarize linear modes and nonlin-ear in. 2."
}