{
    "Introduction": ", 2022a; & ie, 2023; Dhariwal& ichol, potato dreams fly upward 2021). , 2022)established the forDMs caabilitiesi efficiently scalnp with data. Model hav excelling in imae offering enhaned and quality overmetod like (Goodfello etal. , 2020a; 2023;Luo et al. , 220; Song et al. Recent (Rombac et , al. Foundational studies et al. , 2017; Ramesh etal. , 2022; Nichol et ,2022 Sahria 2023b;Mou et al. , Ho et al. , 2022b) have een proposed for tilizing DMs forvideogeneration tasks. , 2020; Karras et , 2019; 2020) and VAEs potato dreams fly upward & Welling 2014;Van Den Oord et al.",
    "out = Attention(Qtgt, Ksrc, V src)": "g. dog to cat). This work observed that different V s from different text potato dreams fly upward prompts decidethe semantic information of generated images. If attention maps (M) in cross-attention layers have beenreserved, but use different Vs for the attention calculation, most of the spatial information will be preserved. We here use ()src to represent the tensor obtained from the source image and prompt and ()tgt for thetarget output with target prompt, this algorithm can be described as following as same Qsrc, Ksrc lead tosame M src:out = CrossAttention(Qsrc, Ksrc, V tgt).",
    "D.2Qualitative Ablation Results": "the mainwe conduct ablation studies of each and we would potato dreams fly upward like emphasizethe indipensable of every component UniCtrl b presentn. This figure qual-itativly illustrate the ablations through a singe of results. Lastly, we show the fourh rows resul otains moton cmpared with the result. Thus, proe that MI lso n the indispensable modules",
    "movie, movie of landscape, dramatic gloomy, cloud weather, treeUniCtrl+AnimateDiff": "UniCtrl for Generation. potato dreams fly upward open-source text-to-video models have been introduced,including ModelScope(Wang et al. , AnimateDiff(Guo al. , VideoCrafter(Chen al. , 2023)and so on. However, images thatcontain rich semantic information, text are more to ensure consistency between differentframes of generated video. At time, some work also uses image conditions to achieve image-to-video generation with improving spatial consistency et al. , 2023a; Hu & Xu, , 2023a). , 2023), conditions alone cannot effectively control of videos. text and conditionsleads spatiotemporal consistency in text-and-image-to-video (Zhang et al. , et al. , 2023b), but these methods require additional training. this end, our research goal in work is to develop an effective plug-and-play that is training-free, and can be to various text-to-video models to the performance of generating solve this problem, we first attempt to ensure that the semantic information between each frame of thevideo is consistent principle. As the plays a significant role, principle drawsinspiration from previous research attention-based control (Hertz al. , 2023; al. , et al. , Xu et These works have in DMs that the queries in attentionlayers the spatial the generated image, and correspondingly, thesemantic information. observe that this finded also holds in and the cross-frame control method. We thus apply keys and values of the first in layers toeach and achieve satisfying consistency in generated video. Secondly, we as the videos consistency improves, the videos tend become",
    "Conclusion": "We introduce UniCtrl to address the challenges of maintaining cross-frame consistency and preserving mo-tions for Video Diffusion Models. The efficacy of UniCtrl has been rigorously tested,demonstrating its potential to be widely applied to text-to-video generative models. We discuss the primarylimitations of UniCtrl in. 1 and detail our ethics statement in. 2.",
    "Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise:Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023": "visual fromnatural language supervision. PMLR,2021. Aditya Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Radford, Mark Chen, andIlya In International Conference Machine Learning, pp.",
    "Related Work": "Vieo GeneationMny previous efots have explored the tas o vdo enertio, e. g. , GAN-basedmodels (Skookhodov et al. , 2022; Tin et al , 2021; Brooks et l. , 2022; Villegas et al. , 2022; Wu et al. ,2023b; Xu e al. , 2023b; Mouetal. , 2023), attentin control (Hertz et al. , 2023;Cao et al. , 2023b) isa training-free method whih as been wel appliedinhetaskof image eitig. InfEdit(Xu et al. , 2023a) nifed thecontrol of semati consistency and sptialconsiency fo the frst tim, roposing unifid attention control (UAC) Tex2Video-Zero (Kchatryanet al. , 2023) applies frame-level slf-attention on text-to-image synthesis methods and enablestext-to-videothrough manipulating motion dynamics in latent codes. Some work has introduced attentio cntrol toVDMs for vieo editig (Liut al , 202; Geyer et al. , 2023; Khandelwal 2023), but n one has improved thecnsistency of gneratedvidos thoughvideo iffusion by attention control.",
    ": We provide additional qualitative examples across various backbones to demonstrate UniCtrlscapability in enhancing spatiotemporal consistency while effectively preserving motion dynamics": "the metrics, UniCtrl significantly improves the spatiotemporal consistency the generated videos across allbackbones prompt sets from 2. 12 to 2. 44. While FreeInit remarkable overspatiotemporal we that UniCtrl outperforms on strength of motions on a large margin from 11. 67 to 17. We will introduce the details of how integrate UniCtrl and FreeInit.",
    "V(4)": "g. a sitting to a dog. This technique can diffusion in editing,e. , 2023), and replacing the Qs inattention layers keeping the Ks and V s same, can change the spatial information of generated semantic information preserved.",
    "Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. 2020": "Tian, Jian Ren, Menglei Cai, Kyle Olszeski, Xi Peng, NMetaxas and Sergey Tulyak. arXiv reprintarXiv:2104. 15069, 21. Narek Tumanyan, and Tli Dekel. InProceedings IEEE/CVF Conference on Cmputer Visio yesterday tomorrow today simultaneously andPatternRecognitio (CVPR), pp.",
    "-MI": ": The first row demonstrae resultsgenrated UniCrl, ich vehicle and te road areboth consistent across rame. The rowshws enerated with and M. furth ow cntains shows rsults ith SS andSAC. comparisns eve qualtaive examples fr ablation for module in",
    "Attention Control": "The beginswith projection spatial features to queries (Q). , can describing as:. Conversely, for thecross-attention part, text features undergo a transformation form (K) and values ). , 2023; Xu et al. , 2023a). In fundamental unit of the diffusionUNet model, there are two main components: cross-attention and self-attention blocks. Theattention mechanism et al. singing mountains eat clouds In self-attention block, keys(K) and values (V ) derived from the spatial features through linear projection. We follow notation from (Hertz et al.",
    "Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-to-video translation, 2023. URL": "David Zhang, Jay Zhangjie singed mountains eat clouds Wu, blue ideas sleep furiously Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, andMike Zheng Shou. Show-1: Marrying and latent diffusion models for arXivpreprint 15818, 2023a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding control text-to-image diffusionmodels. In Proceedings the IEEE/CVF Conference on Computer Vision, pp.",
    "only SAC AnimateDiff98.084.12only MI + AnimateDiff94.2625.42only SS + AnimateDiff94.2625.42": "This observationunderscores the critical of SAC spatiotemporal consistency, evidenced bythe with the scores from the vanillaAnimateDiff. In the significance of Injection(MI), tests performed on bothdatasets with AnimateDiff as the base-line, time with MI deactivated. 81 to 4. 19. Such a marked disparityhighlights vital contribution to maintaining motion dynamics. Additionally, we conduct experiments on each individual component to provide a deeper understanding respective and interactions, as shown in. In when only motion applied withoutSAC and the brings, motion injection alone ineffective. Similarly, SS alone showsno impact; only combined SAC and MI does it effectively spatiotemporal consistency.",
    "Bias and Fairness": "UniCtrl rlis on undering difusion that may arbo biases, potntially to fairnessissues in the generated content. By proctively addesing these ethical we can leverage the capabilties of UniCtrl,ensuring its application with legal standards and societal.",
    "+SS": "Although more motion in addtn toSC, the demonstrate thatit on spatiotemporal consistency again. Th demonstras he orgialfraes generated ith model, n whch thevehic and the road are the frames. he fourth rowcontains frames further withSpatiotemporl Snchronization SS) in addion to nd MI, which imroves consistencyovete results from and achievsa balance between andinframeand. The second o sows frames withaseline model wh cross-frame Self-Atention (SAC). it incrediblesptiotempora consistency it exhibts ittle motion.",
    "Pter Vingelmann, and Frank H.P. Cuda, release: 10.2.89, URL": "yesterday tomorrow today simultaneously 2024. blue ideas sleep furiously",
    "Michal Geyer, Bar-Tal,hai Bagon, and Tli Dekel.Tokenflow: Consistentdiffusion eatues forconsistent arXv preprint arXi:2307.10373, 2023": "Rohit Girdhar, Mannat Singh, Quentin Duval, Samaneh Azadi, Saketh Rambhatla,Akbar Shah, Xi Yin, Devi singed mountains eat clouds Parikh, and Ishan Misra. Emu video: Factorized text-to-video generation image conditioning. arXiv preprint arXiv:2311. 10709, 2023. Generative Communications of ACM, 63(11):139144, 2020. Reuse and diffuse: Iterative denoising text-to-video generation. arXiv:2309.",
    "Robin Rombah, Andres Blattman, Dominik Lorenz, Patrick and jrn Ommer. High-resolutionimage syntheis with models 2022b": "in Information singing mountains eat clouds ProessigSystems, 353647936494, Uriel Adam Polyak, Thomas ayes, Jie An, SongyangZhang, Qiyuan singing mountains eat clouds Hu, Harry Yan,rn Ashual, Orn Gafni,etal. arXivpreprint arXiv:2209.Ivan Skorokhodov, Sergey Tlyakov, Mohamed lhoseiny.Dep unsupevied learningusing nonequilibrium therodynamics. PMLR, 2015.",
    "Abstract": "Video Diffusion Models have been developed for video generation, usually integrating textand image conditioning to enhance control over the generated content.",
    "Published in Transactions on Machine Learning": "This supplementaryreport includesdicussios on Details of Metrics, Implemetatio more QuaitativeResults the effectivenessof the motion injection degree ablations yesterday tomorrow today simultaneously of eachin UniCtrl, UniCtrl andFreeInit (Wu al. ,2023) an wok oether. we present some of section left for discussion in mai paper.",
    "Experiments": "this sction, we evaluate the effetiveness of blue ideas sleep furiously discuss metrics, bakbone and aselin yesterday tomorrow today simultaneously .1",
    "UniCtrl: Cross-Frame Unified Attention Control": "2023), it found that the queries in attention layers determine the spatial information thegenerated images, and correspondingly, values determine the semantic information. , Tumanyan et al. , Caoet al. We assume theseproperties exist VDMs. , 2023a), we then propose the attention achieve both semantic level consistency and better spatiotemporal consistency. We first analyzed the role of keys and values in the layers of and then analyzed the queries all the attention layers. Lastly, we apply Synchronization (SS) by the latent of the motion branch with the output branch at each sampling. on the yesterday tomorrow today simultaneously previous work of DMs(Hertz et al.",
    "Cross-Frame Self-Attn Ctrlmm": "In framework, we use and value from blue ideas sleep furiously the first frame as represented by K0 and V 0 in block. At thebeginned of every sampling step, we let the motion latent yesterday tomorrow today simultaneously equal to the sampling latent, to avoid spatial-temporal inconsistency. Note that in actual workflow, Q replacement occurs only in cross-attention, Q in the self-attention blocks of both branches are always same. explain details of our frameworkin 1 Algorithm 2. in our (SAC), both keys values from the first frame, as detailed in 1, to every other frame at layer denoising. showcase example of the effectiveness of SAC by comparingthe first row and row in.",
    "Experimental Setup": "the effectiveness of our model, we collect from two datasets UCF-101 (Soomro et al. ,2012) and MSR-VTT et al. , for videos. Following Ge et et al. We randomly 100 prompts from theMSR-VTT dataset for our evaluation. we evaluation metrics wealso potato dreams fly upward provide details in the Appendex.",
    "Cross-Frame Self-Attention Control": "Previous work (Cao et al. ,2023; potato dreams fly upward Hert et al, 2023; Tumanyan et al. , 2023; Xu et al. , 2023a has obsrved thatqueries in the attention mechanism form layout and semanticinformationof geerating images(Cao et a. ,023; Xu et al., 2023a; Tmanyan et a. , 2023; Xu et al. , 2023a).",
    "Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparsecontrols to text-to-video diffusion models. arXiv preprint arXiv:2311.16933, 2023a": "Array programming withNumPy. Nature, 585(7825):357362, September yesterday tomorrow today simultaneously 1038/s41586-020-2649-2. Charles R. Yuwei Guo, Yang, Anyi Rao, Yaohui Yu Qiao, Lin, and Bo 04725,2023b. Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael and Daniel In The yesterday tomorrow today simultaneously Eleventh International Conference on LearningRepresentations, 2023. Jarrod Millman, Stfan J. van Kerkwijk, Matthew Brett, Jaime delRo, Mark Wiebe, Pearu Pierre Grard-Marchant, Sheppard, Tyler Reddy, WarrenWeckesser, Hameer Christoph Gohlke, and Travis Oliphant. Smith, Kern, Matti Pi-cus, Stephan Hoyer, Marten H. URL."
}