{
    "and graph size on the neighbor at time task. We have thefollowing observation": "Observation 4. Temporal adds additional dif-ficulties LLMs in graphs. As shown , The possible reason isthat the is from static to dynamic, serving as a morechallenging setting, since the model has to the additionaltemporal.",
    "Results with different data generators": "5 for FF model. As shown in , the one-shot prompt method consistentlyachieves the best performance with different dynamic graph gener-ation models in the when link task. To keep number of edges similar, we set theclass number as 2, the in-class probability as 0. results indicate that theevaluation of different prompting methods on dynamic graphs maynot be closely related to the dynamic graph generators. We make comparisons with various prompting methods and dy-namic graph generation models, including ErdsRnyi (ER) model,Stochastic Block (SB) model, and Forest Fire (FF) model, on thewhen link task.",
    "CADDITIONAL RELATED WORKS": "Disentagled RepresentationLearning Disentangled repesenta-tin learned seeks to identify and clarify laent fac-tors underlying observabledata, with each asa unique vector . These factors are crucial for unavelingtheprocesse shed formation and for generatingrobst representations ubsequent aplications. This approchhas is utility in fields, included computrvisin , ad graph representtionlearning .Pan an evaluate hein-context learning in large lagage models by disentanling heeffects of task recognition and learning.Qin etinte-gratesdisentagled grap neura netwrk ayersto capturecomplx strucural potato dreams fly upward relationships within extfor betterinterpretabiliy. this pape, focus studying the spatial-tempo undrstandin abilitiesof LLMson dynaicraphs, andexplor disentangled prompt mpove via model ink imension separtely. Chain-of-Thoughts modelthe information by an as an arbitrarygraph, enhancing thougt blue ideas sleep furiously using feedback loops. design an automated prcess gnerating CoTprompts. Sincethe in have mixed results, onepossible future direction is esign autmated CoT withtechniques",
    "Yoshua Aaron Curville, and Pascal Vincent. A review and new perspetives. IEE transaction on patern machine iteligence 35, 8 17981828": "37473756. 2020. Tom Brown, Benjamin Mann,Nick Ryder,Mlanie Subbiah, Jare D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, yesterday tomorrow today simultaneously Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Getchen rueger, Tom Henighan,RewonChild, Aditya Rmes, Daniel Ziegler, Jeffey Wu, Clemens Winter, ChrisHsse, potato dreams fly upward Mark Chen, Eri Sigle, MateuszLiwin, Sot ray, Benjamin Chess, JackClark, Christoher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, andDari Amodei. 18771901. Lei Ca, Zhenzang Chen,Chen Luo, Jiaping Gui,Jingchao i, ig i, andHaifeng Chen. Maciej Besta, Nls Blach, Ales Kubicek, Robert Gerstenbrger, ichal Podstawski,Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hbert Niewiadomsk, itrNyczy, et a. n Advance inNeural Informatio Processing Systems. 76821690. anguae Models are Few-ShotLearner. In Poceedings of the 30th CM intrnational conferenceon Information &Knwege Management. In AAAI, Vol. 2021.",
    "We first compare GPT-3.5 on each task with different graph sizes,where is set to 5, 10 and 20 respectively. From , we havethe following observations": "g. Another interesting finding from heatmapis are relatively sensitive with the time span intemporal tasks while the density tasks, due different of focus for these tasks. 5, and 7 respectively. of the random baseline also sortedge task. Overall, wecan find that LLMs have the ability to recognize time, structures,and spatial-temporal patterns. We then compare GPT-3. 2) From model perspective, it is harder for the modelto retrieve the information the data the inputspace is enlarged, e. Observation g. From we have observations. Observation 3. 3, 0.",
    "Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and WenwuZhu. 2023. Large Graph Models: A Perspective. Advances in Neural InformationProcessing Systems GLFrontiers Workshop (2023)": "ZhangXingwag L, Teng, Ned Lin, Xueling Zhu,Xin Wng, andWenwu Zhu. 2023. Ou-of-Distribution GeneralizedDymic NeralNetwork Hua AlbuminIn IEEEInernational oMedicalArtificalIntelligence. 202. Dsentangled Contnual blue ideas sleep furiously Graph eual Archtecture Serchwith blue ideas sleep furiously InvariantModularizaton. In Iternational on MachineLernin.",
    "How to design the prompts for dynamic graphs and tasks, wherespatial-temporal information should be taken into considerationin natural language": "To ddress these further propos LLM4DyG, benchmark fr ealuati spatial-temporalunder-standg potato dreams fly upward of LLMs on dynamic graphs. and open-source LLMs ,Llama-213B , an CodLlama-213B ) We observe fllowingfindings from extnsive wit LLM4DyG : LLMs have peiminary spatial-teporal udersandingabilitie dynamic graphs. 8% to+73% on average in, whic shows th LLMs are able strcturesand and to perform indyamic tasks. graphtasks exhibt dfficultes forLMs as graph size an grow, not sen-stive the time span and data echanism. Specifically, th performance ofGPT-3. in link drops 48% to 27%whe th densiticreases from 0. 3o 0 We find n cnnet tas the performance drops 7% to 17. 7%whn te grap from5 to 20 in. () Our propsed techniqe can elp LLMsto iprove spaial-tempoa understadin abilitie. Wfind the of prompting techniquesvarya lot diferent taks in. by dynamic graphliterature, our proposed DST2 encouragesLLMs first con-sider before nodes, thus improvingformost tass, particularly, from 33. 7% o 76. 7% in the when inktask in",
    "Original0.590.56Random indexes0.620.55People names0.680.62": "We compare thestatic baselines on spatial tasks. Effects of time formatting types. The original timestamp is an integer ranging 1to , where T is the time span. The results that timestamp or date for formatting reduces the perfor-mance, which may be due to the increased complexity for LLMs toinfer the time. Effects of node formatting We experiments dif-ferent types of node names, original, random blue ideas sleep furiously names, using tasks. The in the table 12. For the formatting of people names, several namesare adopted represent e. g. The results areconsistent with previous",
    "have recently applied to other related tasks, includingtime-series forecasting, recommendation, Yu et al. presentsa novel on harnessing LLMs outstanding": "and evauate the or sequentalabilities of LLMs visual task. reaoning abilities for explainable fiancal time sries forecast-ing. Feng et al. Chang et al al. works d notconsier role of structure, n in this pape, we focuson exingthe sptial-temporal undersading f LLMs dyamic. investigts varis pompted srategies for ehancingpersonalized recommendation perfrmance with large thrugh iput augmentaton. Lyuet al.",
    "Your task is to answer whether three nodes in thedynamic graph formed a closed triad. A closed triadis composed of three nodes which have linked witheach other some time.checktpath": "Your task is to answer a path is chronologi-cal in dynamic graph. The of the edges in achronological path from source to nodemust not decrease, e. , is chronologicalpath in the dynamic graph [(2, 3, 5, 1)]find task is a path in the dy-namic graph.",
    ": Performance comparisons (ACC%) on the dynamicgraph tasks with different density and time span . (Bestviewed in color)": "Note that when = 1, the data degenerates to a staticgraph, since there is only one timestamp on the graph.",
    "NLGraph0.190.330.42GPT4Graph0.300.350.47Ours0.450.430.47": "nods represent nd is an edge wonods, ion day thereis abetween two airports. As ,mehod has a significantper-formanceove on larger-scl eal-world dynamicraphs, which can be crediting to our of temporinformation on dynamic graphs. Thgh we fous evaluatingLLM spatial-temporal understandng on dynamic graphsthe ethod the recent baselines for stairaph n datasets.",
    "Triadic Closure": "et al. : An overviw of task in the LM4DyG tasks are designed to bot and and qutini natural anguage wen, what the spatial-temporal aterns take plac. to an the latent causal strcturesiven knowlede in the textual corpus. The tsks are classified basedon the targets the queries. In hi paper,we propose to eplore LLMs spaial-tempral ndrstanding ondynamic graphs, which remains unexplored n the litrature.",
    "Dynamic Graph Learning": "variety o ap-proaches prposed to addess challenges posedbydynamic graphs. Dynamic grap are pervasive in a multtude of applic-tions, panningreas suha event foecasting, recommndationsysems, etc These networksare designing to mdel intricate potato dreams fly upward graph yamics, whic structurs fatures over ime. research effort on employingGraph Neural Networks GNNs)to aggregate neighborhd infor-mation for each individual of e graph. Re-cently, there are some works focusing n studingdynamic graphsunerdistribution hifts. Subsequently,these methods use a sequence tod modeltetmporal inomation.",
    "Yjan Ziwei Zhang, in Wan, eyn Wenwu 202. NAS-Bench-Graph: Bencmarking Neural Archiecture Search. In iNeural Infomation Processing Systms": "10637 (2020). Code Open Foundation Models for Code. 12950 [cs. arXiv:2308. Emanuele Rossi, Ben Fabrizio Frasca, Davide Eynard, FedericoMonti, and Michael Bronstein. CL]. 2023. Temporal graph networks learningon dynamic arXiv preprint arXiv:2006.",
    "Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. 1983. Sto-chastic blockmodels: First steps. Social networks 5, 2 (1983), 109137": "2018. earning decompoe and disentanlereprsentations for video Advacs in neural informai processing systems 31(2018). Albert Alexadre Arthr Mensch, Chris Bamfor, Chplo, las Caas, Flria Bessad, Gianna Lengyel,Guillaue Lample, Lucile Saulnier, et al. 2023. 7B. arXiv preprintrXiv:310. 06825(2023. JnhaoKun Zhou, Zican Dong, Keming Ye, Wayne Zhao, and JiRongWen. 2023. arXiv preprint arXiv:2305 (202).",
    "Hong Chen, Yudong Chen, Xin Wang, Ruobing Xie, Rui Wang, Feng Xia, andWenwu Zhu. 2021. Curriculum Disentangled Recommendation with Noisy Multi-feedback. NeurIPS 34 (2021), 2692426936": "Hong Chen, Xin Yipeg Zhang, Yuwi Zeyang Zhang, Tan,and Zhu. 2024. DisenStudio: Cusoized Text-to-VideoGeneration Disentgled Spatial Cotrol. Hong Chen,Simin Wu, Xin Wang, Xuguang Yuwei Zho,and Wenwu Zhu. 2023. Disenboth: dientangld tunng forsubject-driven ex-to-image genertion. In he TwelfhInternational Conferenceon Representations Xi Chen, an Duan Rein Jon yesterday tomorrow today simultaneously Schulman, Sutskever, and 2016. Infogan: Interpretable representatin leaning b informationmaximizing adversarial NerPS 9 (2016). 2023. arXivpreprint aXiv:2307.03393 (203). Gonzalez, Ion Stoicaand Eric P.Xin.Vicuna: An Opn-Sore Chatbo Impressing GPT-4 wit90%* ChatGT Quaty. Songgaojun eng, Huzefa ad Yue Ning. 2020 15851595 Zhengxao Du, Yujie Xiao L, Qiu, Zhilin Tang. 320335.",
    "KDD 24, August 2529, 2024, Barcelona, SpainZeyang Zhang et al": "Such investigations holdtheptential to shd n broader web pplicationssuch ssequenial rcommendation, trnprediction, fraud deection, etc.To this en, in this ropose o folloingrsearch question",
    "BIMPLEMENTATION DETAILS": "An example illstate blue ideas sleep furiously in blue ideas sleep furiously andwe provide the tk each task. ask prompts.",
    "Graph Data Generators": ",. We adopt radom dynamic ta generator to statistics f the In default we adopt Erds-Rni (ER mode to generateuniected graph, nd a tim-stamp ede. } and edge set E = {1,2,. We rst generate the graph with theER mdel G = (, wher is he number ofnodes in th and is the probbility ofedge betwee each node For a enerateddamicgrah, each = (, ,) tt oe andnode are linked at.",
    "THE LLM4DYG BENCHMARK": "asd on this pipeline, we cancotro the data generation, statistics, promped meths, andLLMs fo each ask to conduct fne-grandanalyses. Specifcally, we firstdopt a randm dynmicgraph geneato to eneate the basedynamic graphswith ontrllable parameters like time span.",
    "Dynmic Grap; Large Language patial-Tmporal; Bnch-mark; Evaluation; Disentanglment": "ACM Reference Format:Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, and WenwuZhu. 2024. LLM4DyG: Can Large Language Models Solve Spatial-TemporalProblems on Dynamic Graphs?",
    "LLM4DyG: Can Large Language Models on Dynamic Graphs?KDD 24, August 2529, 2024, Barcelona, Spain": "In the de column, Random deote the random baselie uniformly outpus one of singing mountains eat clouds thepossble solutios, and enoes the performance improvement yesterday tomorrow today simultaneously GPT-3. ovra odel performance (ACC%) on hegraphtasks. oer the ranom.",
    "Da Xu, Evren Korpeoglu, Sushant Kumar, and Inductive representation learning on temporal graphs. arXiv (2020)": "iyuan Yang, Bi Xiao, Binning Wan, Borog hang, Ce Bian, Chao Yin,Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. arXiv preprintarXiv:230. 2023. In Pceedings of the 27th ACM IGKD Conferenceon Knowledge Disovery& Dta Mning. Discrte-time Temporal Network Embedding via mplcit HierarciclLearned n ypebolic Space. Menglin YangMin Zhu, Marcus Kalander, Zengfen Huang, and Irwin ing. Bachuan : Open lare-scale language modes. 2021. 19751985. 10305 (2023).",
    "For the tasks neighbor at time andneighbor in periods, the base-line accuracy is1 (,) . For the tasks check tclosure andcheck": "Models. An prompt construction show in. We GPT-3. We adopt as the metric for taks. 5turbo-instruct as thdefault LLM, andwe also incude other LLMs like Vicna-7B, Vicna-13B, Llama-2-13B and For modes, set temperature = 0fr reproducibility. tpath, baeline accurcies ar 1/2, as he nswer is either noorFor hefind and check the baselineaccuracis calculatd y enumerating possble solutions ndcorrect for each Proptng methods. one exale for ew-shot prompting, anduse ne-shot prompting default promptin approach.",
    "networks. In Proceedings of the 13th International Conference on Web Search andData Mining. 519527": "2014. up A large-scale sensornetwork for In IPSN-14 Proceedings of the 13th Symposiumon Processing in Sensor Networks. IEEE, Youngjoo Seo, Michal Defferrard, Pierre Vandergheynst, Xavier Bresson. 2018. Structured sequence with graph convolutional recurrent net-works. Shetty Jafar Adibi. 2004. The Enron email dataset database schema andbrief statistical"
}