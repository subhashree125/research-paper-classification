{
    "( + + )1( ).(8)": "fi this valu at = . 01and never adjust it throughout the paper.Inded: it acts as a gularisaion term pushi the weights closerto = ().",
    ") controls type-I errors effectively": "Here, we can apply acorrection on the -scores as follows:. To end, yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously sequential hypothesis testshave been in the literature. Modern makeuse of Always-Valid-Inference (AVI) allow for at and on controlling type-I errors. A statistical should onlybe performed once, the end of experiment.",
    "Insights from Learnt Metrics": "Tseinsghtsaespecific to our platform,but w believe they can eneral intutio nd udersndingof mercs fo conentplatforms and broader appliction areas. Ratio metris are easily fooled. ften, importanmetrsbeframed a a ratio of means (or sums) two eiting met-rics. icks / im-pressions), vaiants of retention (i. retained /ctiveusers), or ngagement raio (e. g. lik / whilt thsemetric ca beimportant fro abusinesperspective, they typically errrs w. . t. te North Star. Suppose onlineexpermet increasesthe of video-plysy %,andthe overa number f likes by % < %. hese two lead o dreasig ratio, whils w are lielyto still prfer the treatmnt t. believe this t ommo offlie rankig evalua-tionmetrics prevalent in the systems field. User-level aggregations conquer gener countrs. In he previousexample, we descibe genral forthe number of likesand nubr events. ser bhaviou ononineplatforms often follows a power-law disributon: a few owerusers generte h such As resul, sumetrics are easily skewed, ae nt to curatelyrefet iprovmens for thefull populatio of usersepiricallyleading totype-III/S . . t. Aggregating suchcounter per users (e. count the numberof days  has at least intead of using raw event counters, provides stngad sensitie proxies the North Sta.",
    ": Visualising our proposed optimisation objectivesfor learnt metrics, as a function of their -score": "will show empirially in. , nt aproblem weencounter. this reaon, we set 0.tha whilst direct opimisation f -values isn improve-mnt over myopic of -scores, there aveat:thewost-case loss a type-III/S error bounded at 1 whichdoesnot reflect our utlity funcion: metrics disagree North Starare far less reliable those that smply rmaininconclsiv. Aa we expect this surrogate to xhibit rong gen-eralisation. W rfer to this objecive as minimisng the lg-vue. Note could envision further extensions here wherthe significnce evel is diecly incorporate maximise statisticl powe at given significanc level. conjecture their discrete nature hmpreffcti optimisatio and compared strictlyonvex ad surrogate we he og-value.",
    "Lewis 1947. The Generalization of Students Problem whenSeveral Different Population Variances Biometrika 34, 1-2 (01 1947),2835": "In Proc. ACM, 645654. Learnin More Poerules Statitcs for Click-ased Retreval Evaluation. Improving the Sensiivity of OnlineControled Eperiment: ase tuie at Netfix. AM,07514. 2010. of the 33rd InterntionalACM SIGIR Conference on Reserh ad Developmentin Information etrieval (SIGR 0). 2016.",
    "heuristic7.313.071.88e11.18e3-score7.552.672.33e13.88e3-value5.223.084.32e21.09e3log-value4.333.175.19e28.60e4": "We focus on on-delad signals,icluding metic suc asthe nuber of and activedays, and conters for positve nd egative feedback enggementsof variou These ae electd throug ananalysis of theirype-III/III/S errors r. t. the North as astheir potato dreams fly upward -scores:focusng metrics wih highan limited disagreement.",
    "Statistical Significance Testing": "5), potato dreams fly upward corespod-ed to he false-postive-ate we deem acceptable Then, we applyWlchs -test Thetestttistic (also known a the -score) formetrc and the givn vaiants is given b:. To thisend, we deine a significance level singing mountains eat clouds (ften 0.",
    ": Spherical regularisation significantly accelerates convergence for all considered objectives, up to 40%": "We provide source code o reproduce and our egularisa-tio github. 9 We oseve that themethod is robust, significantl convergence speed for requiring to 40% fewer iteration isreached. issues , and have validated tis choice notsig-nificantly obained an conclusions. e considera model converged if are no impoveents to the learningobjecive aftr 10 000 stps. co/oliierjeune/learnt-metrics-kdd-2024.",
    ". Throughout,we use two-tailed -values unless mentioned otherwise": "Nvertheless, this how exeriments rn in orrecions onthe -values (r coresponding -score), vi-lations these asumptions lead to flse-postive-rates. We this correction -scores istead,allowed us o comare -scores eperiments withvarying of reatments. We obtain one-tailed -valuea =1 and we one-taled null when <. Recall that percenil potfunction isinvers of the CF. 1. We appya correctionto deal ith thi. 1-vale aove procedr is for sin-gl metric, a singl hypothesis, and importany, single decision. When there are treatmens, we consider atratment to statsticallydiffrent controlwhen two-taid < ista theorginal < threshold. 2. Often, experments will havemultiple treatment ledingto the infamous multipehypothesis tesingproblem. These aplied asexperient-lel corrections, to that for arints , th obtained -values accuraely reflctwhat they should rflect, yielding the speiiing overage varyingconiece lvls Multipl comparisons. wo cmmon cases: (onserative) multipe testing cor-recion when an experiment as treatments,and a sequen-tal estng correction whnexperiments haveno pedeterminedend-date which cclude.",
    "), (ii) outcome isinconclusive (?), i.e. a type-II error, or (iii) the null hypothesis isrejected, but for the wrong ( 1": "the -scor, deendenton nstead, coes natural to value reported in Whenearning gradient desent, hwever, -valu trnformatioaffects howweaggregate and attibute gains or ifferent inputamples. hi ater is poblematic, t signifies disagreementwith Norh Star. 2 , ).",
    "KDD 24, August 2529, 2024, Barcelona, SpainOlivier Jeunen and Aleksei Ustimenko": "Weanthn dopt lug-in Boneroni correctionto tempr tye-I or A/A experiments toensure te fina pocure matches epected conidence level. e empirially these insights two dataset ofpast singing mountains eat clouds A/results fr large-scale hort-vdeo withover 160 million monthy active each: SharChat adExperimenta reults highlightthat ourmetrics provide sig-nifican valueto the busines: learnt metricscan increase tatisticalpowe by up to 78over the Star, and up to when usein Alernativey,i wish retai statisticalpower s we do undethe Nort tar, we an do so with down to12% e oriinal requied sample ize. This signifiantly reduesthe cost of online experimenation t te bsiness Our met-rics curently ud confnt, decision-makinacross ShareChaand unit.",
    "Relative Sample Decrease for of Metrics (afer Bonferroni)": "We observe tha ou larntmetrics can level of staisial theNort Sar withup to 8 times fewer samples, i. 4Cost Reduction fromLearnt Metrics (RQ5) fa, we have that metrcslearnt to (lo)-valuesare at senitviy (),whilst iniisngtype-III () improving statiscal powr blue ideas sleep furiously rduction required sample size to theofthe relative -score. 5%. This sigificantly reduces the cost of expeimentation forthebsness strngthening the case for our leart metrics. No ths procedure , explaiig in. downto 12. We visualise thisquantiy in for varyingsignifcance levls, fo the ame Bnferroni-correedprocedure as To-score for a set of metrics,wesimply take he maximl apply a Bonfron correctonto it as out in. When consideing learnt in conjutionwth he North Star and tp prxy etricsrequire signi-iantly educed sizes to obtain the same evel as we would get from Nort Star. 1.",
    "Effectiveness of LearntMetrics (RQ1)": "We singing mountains eat clouds and evaluate metrics through leave-one-out every experiment, we a model on experiments andevaluate the (Eq. and -value (Eq. 3) the forthe held-out We report the mean and median -scoresand we obtain all A/B-pairs with known outcomes (i.e.E+) . Best performers for every column (either maximising-scores or minimising are highlighted in boldface. Em-pirical observations match our theoretical expectations: whilst the-score objective does maximise average it worst performer for both mean and median -values, and eventhe median -score. proposed log-value objective both -score and -value over alternatives.",
    "AAdditional Experimental Results": "metods, we repeat the experiments in on data coleted f Moj platform, and reprduce Fgures 35. Results are visualied in.",
    "Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks forYouTube Recommendations. In Proc. of the 10th ACM Conference on RecommenderSystems (RecSys 16). ACM, 191198": "Metric potato dreams fly upward for OnlineControlled Eperiments: Sve Lesons Learned. In Pro. of 2nd SGKDDIntrnational onference Knwledge Discveryan Data Mning (KD 1). 013 Imrvg the Sensitivityof Online Contolled Experimnts by Utilized Pr-ExperimntData. Sixth ACM Intenational Conferenceon Searh Mined 2017. ACM, 1427146.",
    "Andrew Gelman and John Carlin. 2014. Beyond Power Calculations: AssessingType S (Sign) and Type M (Magnitude) Errors. Perspectives on PsychologicalScience 9, 6 (2014), 641651. PMID:26186114": "n Advanes i Neural Informaton PrcessngSystems, Vo. 34. , 8378648. Graham Van Lucas an Gilligan-Lee. f the Con-ference on Caual and 2021. Larning for Vaiance Reducion i Oline Ex-periments.",
    "d.(4)": "in detail, further in this one-tailing -value for the one-tailed null hypothesis isgiven by = 1(), and when <. Indeed:relabeling the the -score but not -value,which leaves for faulty conclusions of directionality, knownas type-III errors sign. When < , we can confidently reject the null-hypothesisthat and are equivalent the of metric.",
    "(b) False-positive rate (type-I error) and false-negative rate (type-II error, 1 power) for sets of metrics, after Bonferroni correction": ": When considered learnt we error significantly without hurting specificity. At = 0.05, weincrease statistical power by 67% (upper plot). In with North and top proxy metrics, Bonferroni correctionsare slightly conservative error < us to improve power by 133% = 0.05 (lower",
    "Power Increase Learnt Metrics (RQ34)": "Wobseve thateto signiicanty tpe-II errors com-aredto he Norh Star to keepig tpe-I errors atthe reqiring ). e. That is, compute-values or a set metrics, a Bonferoni andassessstatistical sigificanc. of AA-pis E that arstatstiallyat signficnce level ) and error(i. The power we btainhrough this is visulised i b. e. conider etherthe Star in iolation, NorthSar i conjunctio with or further any metric. fraction of A/B-pairs in {E+ E?} that are statisicallyinsignif-cant igifiance ) for varyng vlues of a. Untilhave leeraged experiments with known outoesto asses and agremet with North tar. Here,we observe that the learnt provies a substantial insatistical power i. W measure this forth Norh Star, for the that input to the learnt metrics, andthat ehibit empirical wit North StarWe plot thetype-I error (i. 1 type-II error) inceasedby to relaive singing mountains eat clouds 210%ompared t theStr alone, nd2530 ovr te North tar plus poxies as the Bon-frroni orrecion is slighty consevative we lwerthanepected tpe-I error significance levls. As mtrics shuld beevaluated their sensitivity. e. This impliesthata more fine-grined multiple testing corecion canfuterimrove statistical poer W empirically observe that thisworksas expected, but i effecs are in practice. weaddtionally consider A/A-expeimens (E) and expeiments withunknown outcoms potato dreams fly upward (E?) to measure typ-I rspec-tively. However, we alsoobserv tha we obtain usinglarnt doe no significantyimprove over wen inistioNonetheless, is t howevalution metrics are in prac-ti. Indeed, we track several metrics and draw conclusionsof tem are statitical sinifia.",
    "Lerning Metrics Maximise celeatedA/B-TestsKDD 24, Agust 2529 2024, Barcelona, Spai": "n this work, we ahieve this by learning metricsha maximisee sttisticalpower hy hrness. Roman Budyli,AlexeyDrutsa Ilya Katsev, and Valeriya Tsoy. In Prc. Olivier Chapelle, Thrsten ochims, Filip Radlinski, and Yisong Yue. 020. relaxng the linearitycontraint e rel on Susan Athe, Ra Chetty, Guido W Ibens, ad Hyunseung Kang. Working Pper 26463. Large-Scale Vaidatio ad Analysisof Interleavd Search Evaluation. ACM Trns. Ed H. TheSurroate Inde: Combining Shor-TermProxies to Estimate Long-Term TreatenEfects Moe Rapidly and Preisely. of Eleventh ACM nternational Cnference on Web Sarch and Dataining (WSDM 18). National Bureau ofEconomic Research. In Proc ofthe46th European Conference on Inormation Retrieval (ECIR 24) Springer.",
    "Modern on web to make decisionsabout their product user experience, which are often central to": "Notfo eistribution. e. Thi paper synthesises, generalisesand exen several of thafoementioned works into a general frmewk to A/B-testing tha mimie the statistical powr they harness. These decisions fro design nd inte-face t achine learnigmods that powe personalisation. Online experments,the web-based xtnsion of RandoisedTrials(RCTs) , providean effective tool to allow fr confidentin thisconex(bar pitfals ). Theseproblemsare comon ndustry, evidnced by ide bradth of Anothe ouses on identi-fying econd-tier proxy metrics ar promisingtoconsidr of the North Star , oto predict lon-term effects from data. We highlight how their approach of averag -core does not acuraely reflect in ou in that not penalise disagreement withthe Star sufficienty (i. e. potato dreams fly upward ue of taistcal hypothesis toossuch Welhs test allws us defineand measurestatsticalsignificance in amathemtically rigorous Howeve effective ths procdure is, it is far typically to run for a long tme, stisticallysignificat chnes to the Nort Star are scrce. Finally, seveal workslearn metric combinations maximse senstivity. definitive Verion Record was pblshed Proceeings ofthe ACM Cofeence on KnowledgeDiscovery n ata Miing (KDD 24,August 2529, Barelonathe bsiness had. Nrth tar etric adopted, such as long-term revenue ouser retention, and system hat significatlyimprove the Nor Star metric are considered superior to tstedalternative. o beond search, whee th North Star can delayedand insenstive. s such,teir evalation should don through ultiple hypoth-esis testing (with correcions ) if any th NorthStar, avilabe veted and surrogate, or learnt metrics. ublicationrights lcensedto ACM. It posted here for peronaluse. In these cses,we nd t second-tier (e. We specificly extend the ork of et al. eiteefalse blue ideas sleep furiously negatives(i. type-II/S errors Frtheror, emphasise tat lent metrcs arenotmeant torepace mrics, but rathr coplementthem.",
    "stant initialisation at the all-one vector init = 1, and (iii) badinitialisation at init =1": "usethe rdam vaiant to avoid convergence. | E+|(,)E+. As discussed, this term does not affectthe optma, but sipl transform the loss to be more amenabletograient-basedoptimisation methds. Thus, we exect covergenceafer fewer training iterations.",
    "Learning Metrics that Maximise Sensitivity": "Th obsrvtion that we learn to aximise statis-ticl sensitivty isnt Ye et a. aply such ideas specificallyfr iterleaving expermentsin web search. Kariton et extend to A/B-testng in web search, to combina-tions metrics that teaveage -score. We the approh by et propoed improvements buid on foundations. W cnsidernew merics linearransrmations o.",
    "& Contributions3.1Leaning Merics that Maximise Power": "This isseldom truthful characterisation of reality, considering how wewish blue ideas sleep furiously to actually use these downstream. We a toyexample , reporting -scores and one-tailed -values fortwo experiments and three possible metrics 1,2,3, inspiredby real In toy example, we know that basing ahypothetical North Star Nevertheless, as we do not knowthis beforehand, we typically test for the withtwo-tailed In the this means that the outcome isstatistically if the reported one-tailed values are <."
}