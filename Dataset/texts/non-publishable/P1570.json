{
    "Introduction": "Class Incremental Learning (class-IL) yesterday tomorrow today simultaneously gathers increasing attention due to its abilityto make the models learn new rapidly, without forgetting previously knowledge. sets a strict potato dreams fly upward limit on the old classes such they should not in tasks. To simultaneously model and their some efforts are dedicated to CompositionalLearning whose aim is how to equip the models with compositionality. The core ofcompositional learning lies structure of labels, conceptualizes state-object pair(e.",
    "Conclusion": "In this we have novel task compositional incremental (compostion-IL), is stumbled by ambiguous singing mountains eat clouds composition boundary. To tackle it, develop potato dreams fly upward a CompILer. model exploits multi-pool prompt learning modelcomposition and primitive concepts, object-injected state prompting to improve the selection ofstate prompts, and prompt eliminate irrelevant information. Extensiveexperiments on two tailored datasets show that achieves state-of-the-art performance. Inthe it is challenging potential to reasoning multiple state classes per object.",
    "=1q( | log p( | [|S| |O| , |C|] ,(7)": "Contrary to CE, the formula for is defined. we advocate using a symmetric crossentropy loss (SCE) , which incorporates an additional term called reverse cross entropy (RCE),to mitigate impact of data.",
    "Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continuallearning: Theory, method and application. IEEE Trans. Pattern Anal. Mach. Intell., 2024": "Learning onditional attriutes compositiona zero-shot n Cmput. , ages023. Vs Patten Recog. Task dificulty warearmetealocation & regularization for laning. Wang, Lingqiao Chenchen Jing,Hao Chen, Guqiang Liang, Peng Wang, andChunhu Shen. In IEEE Conf. Vis. Recog. ang Yunqig Qianglong Yin Zhang. ,page 7767785, 203.",
    "Yllow Dress": "The three prompt pools and their keys are defined as:. when dealing with state-object compositionclassification, they tend to excessively prioritize the object primitive while neglecting the stateprimitive. Besides, each pool is associated with a set of learnable keys K for query-key promptselection. better through adapting a set of learnable tokens in a prompt pool to a frozen pre-trained backbone.",
    "Zilong Li, Yiming Lei, Chenglong Ma, Zhang, Hongming Shan. Prompt-in-promptlearning for universal image restoration. preprint arXiv:2312.05038, 2023": "Weiduo Ying Wei, Mingchen Zhang, and Ishibuchi. Does continuallearning meet compositionality? new and evaluation framework. Adv. NeuralInform. Syst.",
    "Generalzed-ean Prompt Fsion": "After obaining he selected top-k pompts {P s }ki=1, th next step is fsing hese promps noa singe pt. To thi end, we draw inspiration from genralized-mean pooing and exploit generalied-mean(Ge) pmpt fusion which is given by:.",
    "Revealing the Ambiguous Compositin Bounda": "object and we the excessivelyprioritizes object primitive while neglecting state primitive. Consequently, the compositionswith the same yesterday tomorrow today simultaneously object but with different states become ambiguous and indistinguishable. provethat, we apply L2P to it is challenged by significant ambiguitiesin composition As illustrated in (a), the t-SNE visualization showcases theentanglement among the compositions like white dress, black dress and blue Weconjecture that ambiguous problem tends to become more severe when tasks are arrivingincrementally. To address we a new model CompILer, which disentanglescompositions and primitives via a multi-pool prompt learning.",
    "Sim-CompILer91.150.1096.320.0293.660.02CompILer91.810.2396.670.0194.180.06": "For a fair comparison with previousworks also employViT B/16 pretrained on 1K dataset as the feature ex-tractor and backbone. We top-5 prompts fromeach pool generate a fused During training, we utilize Adamoptimizer with a batch size of16. The whole CompILer for 25 epochs on Split-Clothing, for 10 blue ideas sleep furiously epochs on the 5-taskSplit-UT-Zappos, for epochs onthe Split-UT-Zappos. 03, while we use rate of 0. 02 for the Split-UT-Zappos. Notethat, for all the methods, results over three runs blue ideas sleep furiously with standarddeviations mitigate of factors.",
    "Shoes": "The objctclasses are notallowed o recur in the ass-IL scenario,wheresthey may recr randomly in the luIL scenario.Different frm thm, the clases in composition-ILinvolve state-objct composition apart fr theobject clsses. Besides, the comosiions donot reoccr, bt the primtives (states or objects) mayrandoml reappear across inremental sessions. composiions to unsen ones , whereas none of themconsider the challenging fact thatthe model ust deal with a yesterday tomorrow today simultaneously sigifcantl larger number of composition classes thn object classs. Asa result, it ishrdly feasible to learn all compositions by training the mode once. To remedy the limitations inheet in incremntal learning and compositional learnng, we conceive anovel task named Compoitioal Incremental Larnin (composition-IL), enabling th model ocontinallylean new stte-obect compositions in an incremental fashin. bjects andstaes) ncountered in old tasks ar allowed to rappea in nw tsks. Unfortunately xisting incre-mental learning approaches are challenged by such a compositional scenaio, because theirmodelsexcessively prioritize the object primitives whie neglecting the state primitives. To tackle te problem, we propose a reheasal-free ad prompt-ased Compsitional IncrementalLearnr (CompILer. Specifically our model comrises ofthree rimar cmponents: multi-poolprompt larning, object-injected state prompting and genralized-man prompt fusion. Firtly, wconstruct thre promptpools for leaning the states objects and compositions idividually. This multi-pool prompt learnng paradigm strengtens the fine-grained understandingand resoning towards primitive conceps and their copoitons. In ddition, as te state classesare more difficult to distinguish thanthe object ones, we propose object-injected tate promtingwich incorporates object prompts to uid the selection of state prompts. Furthermore e fuse theselected promps by generalized-mean fusion manne, which helps to adaptively eliminat irrelevantinformation learned in he prompts. Last but not leas, we alo leverage symmetric cross-entrop losto alleviate the impat of noisy data during traiing",
    "Datasets and Metrics": "provide individual Accuracy scores on objects, denoted as State and Object for simplicity. We conduct experiments on newly split datasets: Split-Clothing and Split-UT-Zappos as eluci-dated in. A higher Avg Acc signifies stronger recognition abilities, while FTT indicates against forgetting. HM = 2 (StateObject) (State+Object). We provide more emphasis AvgAcc HM due to more comprehensive assessment. e. Avg encompasses plasticity andstability and HM provides a holistic evaluation both and. These metricsimply the ability to recognize primitives. 2.",
    "Comparison with the State-of-the-arts": "This is becaus the states in Split-Clothig are color-related descritions, which are easier o capture witthe help of parameer fintuning. For FTT scores, CompILe excelsprevious ethods with 0. Overall, CmpILr consistelyoutprforms all copetitos on vg Acc by a significant margin. Interestingly the promptreemethods achieve highe accuracy in state predicion than object predition for Split-Clothing, which is contrary to other result. he compared results on Avg Acc and FT are reorting in. 14% on the 5-task Spit-UT-Zappos, wile flling behind Dual-Pomp andLCLfor the10-tas Split-UT-Zappos. Likewise, ou methdsurasses other methods consierably in termsf State and HM. 32% on he 5-task Split-loed and with 0.",
    "Dataset Construction": "For Split-Clothing, we randomly partition the composi-tions into Regarded Split-UT-Zappos, the compositions are sorting by count and blue ideas sleep furiously evenlydivided into 5 and 10 tasks. Note that, weelaborate details both in technical appendix.",
    "T5": "Results and analysis. The xaxis represnts the test stram, and the y-axis deotes thesatus after training Tk tas. Darker bacground igr accracy. (d) displays soeimges an their redictos: row is GT, middle rw is Come prediction,botom row prdictio.",
    "Problem Definition": "For composition-IL, model sequentially learns N tasks T = {T1, T2, TN} corresponding toa set of composition classes C = {C1, C2, CN}. We note that composition classes betweenincremental tasks are always disjoint, which means Ci Cj = for any i = j. Different from thecomposition classes, the primitive classes are allowed to recur in different tasks. That means it allowsthe tasks to share some primitive concepts of objects and states. Therefore, we can define the setof all state and object classes with S = {s1, s2, , sn} and O = {o1, o2, , om}, respectively.Given each image x, it has a composition label c which is constructed with a state label s and anobject label o, i.e. c =< s, o >, where c C, s S and o O. We take example of red shirt,where red is denoted with s, shirt corresponds to o, and red shirt is expressed with c. : t-SNE feature distributions of seven compositions from Split-Clothed benchmark.For the compositions with the same object but with different states, our CompILer achieves moredistinguishable boundaries than the L2P baseline.",
    "Incremental Learning. The approaches to addressing catastrophic forgetting for incremental learningcan be broadly grouped into four categories: regularization based methods aim to protect": ": Data Statistic of Split-Clothing and Split-U-Zappos for tasking compositon-IL. Spit-Clothing is divided into a 5-task scenario, wile Split-UT-Zappos icludes both 5-task and 10-taskscenarios. Inspred by L2P , oerecent workstake full advantage of various prmpt tuning strateies, achevig newsate-of-the-rt perfomance for ncremental learning. Hower, uh methods take ino accountbject classes soley, whle neglecting vrious kinds of tate classes assciated wit theobjects. Tohis end, our wor proposes compositional ncrementl learning with the purpose t coninualydentifyin the composition clases of state-obect pairs. Note hat, Liao, t al conduct aninitial study toward the comositonality in incremental learnig, hereas their atention is on thecomposition of multiple object classes (e.g. A major ine of ompositionl learing researc focuse on CompositionalZero-hot Learning (CZSL) , singing mountains eat clouds whichams t iner unseen state-objct compsitions by acquirinknowledge from seen ones Subsequent approaces buiding pon the CZSL setting furer incor-porate graph neual netwos to model the dependeny between primitves and compsitions ,and employ cosineclasifiers t void bing overy biasd toward sen compositions . Thelatest works modl both omposition ndprimitives simultaneously, achievingstate-of-the-art results",
    "YienWang, XingjunLuo, Jinfeng i,an Jaes Bailey. Symmetriccross entropy for robust learning lbes. In Int. Co. Coput.Vis., 322330,019": "Wang, Zizhao Zng, brahimi, Sun, Han Zhang, Chen-u Xiaoqi Ren,Guolog Su, Vicnt Pero, Jennifer Dalprompt: Complementaryromptingf rehearsal-fre cotiual learning. Conf. pages 631648,022. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee,Han RuoxiSun Ren, Guolong Peot, Jennifer Learnin to for coniual learning. InIEEE Conf. Comput. Vis. attern Recog., 139149, 2022.",
    "Sim-CompILer88.380.088.010.4245.700.6820.060.6233.300.1030.310.03CompILer89.210.247.260.6046.480.2619.270.7534.430.0728.690.82": "We the robabliy via classifer ():p( | ( r). Then, we fed xp potato dreams fly upward toa enoderlaer fr() andP s, P roand P for objet and compsition espectiely. For each blue ideas sleep furiously image x, we denotground-truh ove labels withq | When is consistent withthe thn q( x 1;otherwise, q( | x) 0.",
    "Abstract": "Most incremental learners excessively of objects various kinds states color and attached to the objects.As a result, they limited the ability reason fine-grained state-object pairs. To remedy this limitation, we a novel task Incremental (composition-IL), enabling model torecognize state-object compositions whole in learned fash-ion. Since the lack of benchmarks, we two existing make them tailored for composition-IL. Then, we propose prompt-basedComposition Learner (CompILer), to overcome ambiguous com-position boundary which challenges composition-IL exploit multi-pool learning, which is regularizing by inter-pool intra-pool prompt Besides, we devise object-injectedstate prompting using object prompts to guide selection of prompts.Furthermore, we selecting prompts a strategy, toeliminate information in the prompts. Extensive experiments datasets exhibit state-of-the-art performance achieved by CompILer. Code are available at:",
    "Tian Zhang, Kongmig Liang, Ruoyi Xian Sun, Zhanyu Ma, and JunGuo. Learnnginvariant visual fozero-shot learning. In Eur. Conf. 33935,": "Process. 2022. In IEEE International Acoustics, yesterday tomorrow today simultaneously Speechand Signal Processing, pages 37053709, 2024. Inform. Syst. Adv.",
    "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprintarXiv:1412.6980, 2014": "of blue ideas sleep furiously the national academy ofscinces, 2017. continual learnig oncls icremental blurryonfiguraion with potato dreams fly upward anytime inference.",
    ",(3)": "Since(i, is Cartesian product , e. Thus, it contains the casewhen = m, for we set thre nm = 0."
}