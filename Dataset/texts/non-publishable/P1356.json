{
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The are encuraged createection their paper. T papershould point out any assumptionsand how robus the results ae toviolations these assumpions(e.g., independence assumptions, noiseless sttings,model well-specification, asymptotic approximtions only loally). authorssould reflect on how these assmptionsmight violated racic and what theimplictions would be. T authorshuld reflec on the scopeof the claims e.g., if the approach wasly tested on a fewdatasetsfew runs. general, emprical oftendepend on implicit which huld e The authors should n the fctors that inluence perfrmnce of afacil algorithm may perfom oorly when mage rsouionis low or take in Or a spch-to-textsystemght not used reliably to provide closed captons for lecures becase it fails to handletechncal",
    ": Experimental results on mixed real worldproblems (with MCTS-transfer-PFN)": "This result deonstatesthe versatliy o MCTS-ransfer yesterday tomorrow today simultaneously combining withadvancing BOalgorithms. MCTS-transfer is genral search spce rans-fer learningframework, which can b combining ith other advanced algorihms. Asshwin , MCTSransfer-PN makeurtherimprovements comared to MCTS-tnsfer-GP andPFN after around 80 ierations,and ts rnkng is sableand excellent throghot theoptimizatin. Weequp MCTS-transfer withPFN yesterday tomorrow today simultaneously and test MCS-transferPFN on mixed ea-world prbles.",
    "GVisualization of Weight Curve of Tasks": "5 and decay to be = 0. 99. Here we show the weight change curve of three real-world problems. shows that the weights of problem and similar sphere problem exceed dissimilar sphere problem in regardless of any inconsistencies in initialization.",
    "DSensitivity Analysis of Hyper-parameters of MCTS-transfer": "We conduct sensitivity of these important parameterson LunarLander, RobotPush, Rover under mixing setting.",
    "To the of MCTS-tranfer in complex and high-dimensional cases, we 3 continuous problems from Design-Bench14": "Superconductor. Its a critical temperature maximization superconducting materials. The search is a continuous spacewith 86 dimensions. morphology. Its robot morphology space is continuous space with 60 dimensions. DKitty Morphology. search yesterday tomorrow today simultaneously space is a blue ideas sleep furiously continuous space with56",
    ": Sensitivity Analysis of Local and Global Modeling Approaches": "Similrity propoe5 similarity in this inclded slutinditance, best N or N percentsoltionsmean distance, Kendall cofficient and KL divergence, as C. Here we set N= and N=30% for bst Nsolutions and et N use Gaussan to evalute KL dvergence of distributions. The eults can be seen The distribution-bsed methods more precie easure of similarity, but point-based methodsare dsigned to focus on the Each its own suitable of similarity. The thre are pontbased measures and he last ae distributio-bsedmeho.",
    "Nmif ri < Nm0.1otherwise,(4)": "After sampling a new data in each iteration ranks weights are updated accordingly, which are then used to update the potential value ofeach node. also consider ways of calculating distances and which are introducedand empirically compared in C.",
    ": Comparison between MCTS-transfer and other algorithms on Design-Bench and HPOB": "Robotush and Rover). The evaluation time includes tetime reqired for surrogate modelftting, candidate solution slection ad evaluation, wich are th ommon componens shared by alloptimization algorithms. As shown i , te aditional compatonal burde introduced by MCS-transfer ie. , back-popagation and reconsruction) represets a potato dreams fly upward relatively minor fraction of the toal runtime, particuarlyinthe hree expensve real-word problems. We divde MCTS-transfer into three main comonents: evaluation, backpo-pogationand reconstrution.",
    "Optimal solutions distanceLet xT be the best solution of target task, and xi be the best solutionof target task. The task distance is calculated as Distance(xT , xi )": "BestN (or N mean to optimal soutions distane, we replacexi and xi wth the mean of (or N percent) target source task i, dnoted as xT andxi. The task istnce is Distance(xT ,x ). Intuiively, its han direcly usig the bestsolution. coefficientKendll s ameasure o rank correlation, whichcan beused theof two datasets. We fist yesterday tomorrow today simultaneously uild surrogate model (usually GP) Mi on Di,and predict all sample DT.",
    "Experiments": "We also conductexperimentsacross aariety of tasks, ncludi the syntti benchmark BBOB, real-wrldproblems Design-Bench,and the hype-paramete optimiationenchmark HPOB.",
    "Abstract": "Bayesian optimization a popular method for computationally expensiveblack-box optimization. traditional neing to solve new prob-lems from leading to slow Recent try to BO toa transfer learning setup to speed up the optimization, search transferis one of the most promising approaches and has shown performanceon many tasks. However, search space transfer methods lack anadaptive are flexible making it efficientlyidentify promising during the process. In this paper,we propose a search space transfer learning method on Monte Carlo treesearch (MCTS), called MCTS-transfer, iteratively divide, select, and optimize ina learned subspace. MCTS-transfer can not provide a well-performing searchspace for warm-start but also adaptively identify and leverage information ofsimilar source tasks reconstruct the search space dured the optimization process.Experiments on synthetic functions, real-world Design-Bench hyper-parameter show that MCTS-transfer demonstrate superior perfor-mance comparing to other search space transfer methods different settings.Our available at",
    "Synthetic Functions": "In mixed transfer, MCTS-transfer can still warm-start, thanks to its provide multiple compact effective search pre-learning. e. BBOB is a benchmark for BBO. We randomly select one from of the five classes of as task, blue ideas sleep furiously i.",
    ": Sensitivity Analysis of Measures": "e., Eq. e. (8)) nd all-onestrategy. W set = 0. 5 for lineachange strategy, =. 5 for eponntial-change strategy. As shown in , the xponential-change strategy, since the eight decaying rapidly with the rank, can only effectielyleerage the- source dataset. S it teds to hae faster convergence peed if te similar tasksare effectivey denified at the inial stage, as demonsrated in Rover.The a-one stratgy areeasy to be diturbed by dissimiar daa but can fully utiize information, so it may have a highercnvergencevalue at laterstae. The linearchange strateg isreltively more stable becue it takestheadvantges of th abov two strtegies nto account.",
    "C.1Task Similarity": "Measured the two tasks basing on dataset of a source task Di and the dataset ofthe target task DT a task. Smallerdistances represent greater similarity. singed mountains eat clouds Here we mainly on two types of methods:point-based similarity measures distribution-based similarity Detailed description as follows:.",
    "Harold J. Kushner. A new method of locating the maximum point of an arbitrary multipeakcurve in the presence of noise. Journal of Basic Engineering, 86(1):97106, 1964": "In Advances Neural nformtion 32 (NeurIPS18) pages 68016812, ancouver, Canada, 2018. Tanserlearng ased seach spae design for tuning. In Prceeding of the 28th Knowledge Discovery singing mountains eat clouds ad Data pages 967977,Washington, DC, 202.",
    "arXiv:2412.0716v1 [cs.LG] 10 Dec 2024": "the knwldge acquired from source ca helpful forptiizng the curnt tagettask. Thes mthods can be ategorized bad n the learned cmponents BO, as theacquition function [51; 40], initialzation [45; 47], or searchspace [4; Amon thse,earing thesearch is proising research ara due to its effctiveneswihoptimizaon ethods. larnng and paritioning the space wecan ore ffectvely potential subspaces and significantly accelerate the algoriths searchfor optimal slutions.le the methos for partitningspace have dmonstrtd [22; 14; 23; they still have several partcularly intrms of flexibly adjustingthe searc space or the current ask. In any searios, we are bot tasksare similar to the target task optimizing t. about tis deepen gradually as we pogrss in ptimiing the targe task. Theefor, hpe tht asearch transfr algoritm can utomaically idntfy the most relevant source rocess, and give them more consideration hen the subspaces In this we propos search spae tranfer larning mthodbased Mont Carlo search(MCTS), called MCTS-transfer, o iteratiely divide select, and optimize in a learnd subpace. To better identify and leveageth nformation source tasks, different weigts to diffeent source onheir imilarity to the target task, are adjuste durng the optimizatin procss. it can prvide a bettr initial search spacea wam-sart fthe optiization the trget task Second, it can providemutiple pomsig ompact subspaces to improve optimization Theupper bound (UCB)-based CTS also balce the between exploraton a To evaluate the effectieness the proposd method, we compare MCTS-tranfer space transer methods ad condct on multiple tasks,including yntheticfnctions, realworldproblems, ad hper-paramter optimization roblems. between the source tasks and task, peformance. Note tht MCTS-transfe be BBO alorithm.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a potato dreams fly upward new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the singing mountains eat clouds results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset)",
    "Guidelines:": "Their licensing guide can help determine thelicense of a dataset. The name of the license (e. 0) should be included for each asset. , CC-BY 4. , website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. g. The authors should cite the original paper that produced the code package or dataset.",
    "Bayesian Optimization": "We consider problem maxxX f(x), where f is a black-box function and X RD is The basic framework of contains two critical components: a surrogate andan function. GP most popular model. that, an acquisition e. g. , ofimprovement (PI) , EI or UCB , optimizing determine next query point xt,balancing potato dreams fly upward and exploitation.",
    "Design-Bench Problems": "verify the performance of MCTS-ransfer in more copex and yesterday tomorrow today simultaneously prolemsfrom Design-Bench included three cntinuous problems Superconductor, Ant morphology,and DKitty Morhology. In and mixing trnsfer, MCTS-trnsfer-GP gets the singed mountains eat clouds best rakingafter 40 iteratos,show in a.",
    "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "In general. While NeurIPS not require yesterday tomorrow today simultaneously released code, the yesterday tomorrow today simultaneously conference does require submis-sions to provide some reasonable avenue for reproducibility, which may depend on",
    "log (np) /nm,(1)": "Utilizing iteratively the search space into for optimization. Finally, through back-propagation, value andvisitation count of the ancestors of k updated. sampled points then employed for clustering and classification,leading the bifurcation of m into two good and These sub-regionsare expanded as two child nodes, with the one denoted as good and the right as yesterday tomorrow today simultaneously bad. where Cp is a hyper-parameter controlling the balance between exploration exploitation, npis the times the parent node. Initially, the algorithm navigates from root node to child nodes, UCB values to identify the leaf node m. In this trees root represents the entire search denoted as , and node m represents asub-region m. MCTS has been used to select important variables automatically for high dimensional BO. During each iteration, after node m, LA-MCTS conductsoptimization within m. The value vm is determined by average objective value of the sampled the sub-region m. In the of MCTS, leaf node is systematically se-lected for expansion, involving steps: expansion, simulation, back-propagation. In simulation step, the nodevalue vk is through random sampling. LA-MCTS is scalable BBO algorithm based on learning space partition. Subsequently, an action is executed based on thestate of m, leading to the expansion of a new child node (state), k.",
    ": Sensitivity Analysis of Exploration Factor Cp": "blue ideas sleep furiously The spliting threshold The theshold controls the potato dreams fly upward ept ofthe tree: a node is only allowed tobe further diided wen it contain more solutions than. = 10= = 100 Number o evalations.",
    "If applicable, authors should discuss possible limitations of their approach toaddress problems privacy fairness": "Th auhors hould use heir and recognize tha in of transprency play a impor-tat role i developing yesterday tomorrow today simultaneously norms peserve the integrity of th communit. potato dreams fly upward",
    "Introduction": "rea-world taks such arhitectur search 41; 4], hyper-parameter optimiza-tion [55;4 integraddesign [19; 0], often needto black-box optiization(BBO problems, th bjetive uction n anaytcal and an be evaluatedby inputs, reared  black-box function. Bain optimization (BO) [29; 8] a widely used sample-eficient metod for sch polems. BBO problems are ten accompaied byeensive computational cost the evaluations, requiring a alghm to fnd a good solutionwith a mall number of objectve unction evaluations. Undr the limited evaluation buget, only ave few whichare, howevr, isfficient for apreci surroate o slow Thus, traditional BO struggle t effectively solve expensve BBOproblems, therbroadrapplications.",
    "C.3Discussion on Conditional search space": "For conitionalwe consderthe problem minxXRd Specifically,the searchspace i tree-sructure, formulated a {V, }, wher v V i nod represeningsbspace ndE is edge represnting The objective functio i also defned T, ormulatedas (x) :=fpj,T wherepj is cndition and |lj i therestriction tlj. Ineach iteraion, followed by UCB ids target node m locating in thsubtree of vwit condition pi, otimizes in m, select andealuates teandidateusing fpi,T Aterthat, it updates th weights node n whole treT and tries to tree Not that tree reconstruction onlyhappens n subtree of v V.",
    "Jian Wu and Peter Frazier. Practical two-step lookahead bayesian optimization. In Advances inNeural Information Processing Systems 32 (NeurIPS19), 2019": "Shangda Yang, Vtal Zankin, aximilian Balandat, Stefan cherer, evin T. Carlberg, Neilalton, andKody J. H. Law.Acceerating lookahead in bayesian optimization: Multilevelmonte caro i all you need. InProceedings of the 4st International Conference on MachineLearnng (ICML24), Vienn, Astra, 2024. Xin-She Yang and Suash Deb. Eagle strategyusing vy walk and firefl algorithms forstochastic opimization. In Proceedns of the th Nature InspiredCooperativeStrategies forOptimization (NICSO10),ages 101111 Granada, Spain, 2010. Quaning Yao, engshuo WangHuo Jair Escalante, Isabel Guyon, i-Qi Hu, Yu-Feng Li,Wei-We Tu, Qiang Yang, and Yan Yu. aking uman out of learning applications: A surveyon automted machine learning. arXiv:1810.13306, 2018.",
    "Tree Struture Reconstrution": "However after ampling a newdata point ad updating the tentil of each nodein ech iteationof MCTS-transfer, propertmy vioated. Othrwise, thetochil nodes enter queue Q (lines 1011) and ill be examine later. The subtree reconstruction proces is consistet with the node expansion. we backtack from the proeati leaf nodes to the highest-level ancesornde that the proprty, and the reconstruct the subtree tat ancestornode. We breadth-first ttrverse all tree and a quue Q to manage of If the poential of the of a nodeis better than that of the left child (line 6), the subtree of this is (line nit should bereconstructed and is added into the queue N 8). When constuctin serch tree, the left child f a node is always better than child,. fter traversn th tree T , we reconstrct nodes N (lnes Thus,can fine-une thetree resere some meanwhle, canadaptively be suitable to target task. process is presented Algrithm in Apendi A.",
    ": end for": "3) Thetreecanbe adjusting b pdating and epanding on the basis of the originl tree, neiformation can be absrbed historical information can also be. mae use of information from the tasks, taketh similarty between a soure taskandhe trget task ino account, and try to assgn ighrweihts to more surce tasks, asintrodued 2. Finally, we will whethe updatedtreeconfomsto the prperty thatthe left cild a ode has largr vlue han right nde. Note that nodeexpansion, only samples of the trget tsk are In line step performed to refresh the of each node path to m,including the visittims thecontained samples. samples adde DT , the distance between each sorce tasDi ad task DT may change. This enables the knowledgeof spacpartition frmsourc task ata to be transferredto the target task. 2) ode potential evaluaion allws fo the extraction ofmultiple irregularly shaping promised subspaces representedby multiple leaf odes. the tree structureis ihertable. In MCTS-transfer, exhibits several properies that align requiremnt ofsearch tansfer learning. After that, node mi expnded in 1517if it is splital, , more than sample of the targt task be clustred twoclusters. or any nodviolatig this proprty, its sub-ree will be reconstrute.",
    "Search Space Pe-earning": "to stndad algorithms, whch samplerandoly across enire space atour method leverags informatio fromtasksto concentrate sampng within agenerally god space, proiding warm-start initialiation. the re-learning sage, ree T initially only ne node, ROT node. ll source task samlesre in node recusively and classified, leding to expasion.hen noneof the leaf nodes can e further bifurcated, i.e., ontains more than and ca be two lusters, T isfinally thispocss w keep the rule that the left node better(i.e., hasa larger otential value) than the righ node, so we can easily he is h best and leaf iswost. The potential value  node m i the pr-learningstage is stimted by average yi,j all source task within it, i.e,",
    ": Sensitivity Analysis of The Splitting Threshold": "rythe followed clasifiers:Logistic and SVM (with rbf, lnear, o poly kernel). the results in , SVM wih linear Listic Reresion gvemore effetivesearch space parition. We can choose blue ideas sleep furiously binary with hiher tition effiiency accordin",
    ": Sensitivity Analysis of Weight Change Strategy": "Decay factor. The decay is to of source tasks. for nodes preferredby source tasks, if they cant be as the node at first, their potential will nodes be less to selected. The analysis experiment of be seen infigure 10, and result is as expected. And appropriate decay factorwill to combine information from source and target to accelerate optimization.",
    "ATreeify Pseudocode": "pseuocodeis provided below, as shownin lgoritm 2.",
    "FDetails of runtime analysis": "In order the additionalcomputatoal oerhed introduced by MCTS-trnse, w clculatehe time and the oresponding proportionof eac component, ecord te frequency recostrction in eac iteration. As iustrated, the comtational burden inrouced by MCTS-transfer(i. , unarLander, Root-Push, and Rover). we divie MCTS-transe into thre ma components: evaluation, backprpagationand econstructin. , akprpagtion andrepresents a inor fracton total ruime,particulary i real-wrld These scenaris preiselyhe computainallyintenive cass thattransferBO  esigned to address wherein MCTS-transfer denstrtes smaladditionalcomputational overhead Furthemore, our result that he aveage frequncy of trereconstructions is with corresponding reconstruction time beingalmot negigible to th evaation.",
    "Martin Wistuba Josif Grabocka. Few-shot Bayesian optimization with deep kernel sur-rogates. In of 9th Conference on Representations(ICLR21), Virtual, 2021": "Martin Wistuba, Ncolas Schlling, nd Lars InProceedings of the 26th Machie Leaning and Knowledge Discover in Dataases: EuropeanConference(ECML/PKDD15), pags 104119, Porto, Portual,2015 Martin Wistuba Nicolas Schilling, and Lars Schmidt-Thieme. Proedigs of the 27th Machie Learningand Knowlede in Databases: European Conference (ECML/PKDD16), 199214 Riva Garda, Italy,",
    "This tree embed historically good regions into specific tree nodes. Intuitively, the spaces representedby left leaves have higher potential of being good spaces than right ones": "The first iteration MCTS-trnfer will the reulting tree T generated i the pre-lerningstage Staring from root node, it reursiely selects nodes with higher values unti reachinga The value is calculated as Eq. (1, isreplaed by he potential pm inEq. (2) Consequently, thelgorithm prefeentially samples from those regions that shwn to yield favorabe outcome inthe tasks.",
    "Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, and Rodrigo Fonseca. Alphax:exploring neural architectures with deep neural networks and monte carlo tree search.arXiv:1903.11059, 2019": "Linan Wang, RdriFonsca, and Tian In Adancs in Neural InformationProessed stem33 (NeurIS20), pages 951119522, Virtual, Zi Wang, Clement ehrig, Pushmeet Kohli, Stefani singing mountains eat clouds large-sale Bayeiantimization in high-dimensional space. Yng Peilin Zao, and unzhou yperparamete wit neural",
    "James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journalof Machine Learning Research, 13:281305, 2012": "Bernd Bischl, Martin Lang, Tobias Richter, Coors, Theresa Ullmann, Marc Becker, Anne-Laure Boulesteix, Difan and MariusLindauer. blue ideas sleep furiously WIREs Data. Knowl. Cameron Edward Powley, Daniel Whitehouse, M. Lucas, Peter I. Cowling,Philipp Rohlfshagen, Tavener, Diego Perez Spyridon Samothrakis, and SimonColton. A blue ideas sleep furiously survey of Monte search methods. IEEE Transactions on ComputationalIntelligence in Games, 4(1):143, 2012.",
    "Yunqi Shi, Ke Xue, Lei Song, and Chao Qian. Macro placement by wire-mask-guided black-boxoptimization. In Advances in Neural Information Processing Systems 36 (NeurIPS23), NewOrleans, LA, 2023": "Lillicap,Karen Smonan,a Deis Hasabi. Monte Calo tree search baevariableeletion for igh dimensina Bayesian optimizatio. Maddison, Arthur Gez, Laurent Sifre, George vandenDressche, Juian Scrittwiesr, Ioannis Anonoglou, Vedavyas Panneeshlvam Marc Lanctot,Sader Dieleman, Dominik Grewe, ohn Nham,Nal Kalchrenner Iya Sutskever, TimothyP. Lillicrap, Fan Hui, Laurent Sifre, George van den Desshe, Thore raepel, and Deis assabisNature, 55(7676):354359, 2017. Nature, 529(7587):484489,016. Iforation-theoretic regetbounds for Gausian procss optimization in bndit setting. NirnjanSrinivas Adeas Krause, Sam M. 01815,2017 DavidSilver, Julian Schrttiser, yesterday tomorrow today simultaneously Kare Simonyan, Ioannis Anonoglou, Aj Huag, AthurGuez, Thomas ubet, Lucas aker, Matthew L,drian Bolton YutianChen Timthy. arXiv:1712. Li Song, Ke Xue, Xiabin Huang,and Chao Qian. Mstering chss andshogi by slf-playwith ageneralreinforcment learnin algorithm. In Proceedingsof the20th InternatioaCoference onAtificial Itellienc and Satisics (AISTATS7), ages 307315, Fort Lauderdal, FL, 2017.",
    ": A decay factor, which is used to adjust the overall influence of source tasks throughout theoptimization process. A smaller accelerates the forgetting of the source task data": "To calculate wi, we measure the distance Distance(Di, DT ) between the i-th source task and thetarget task by Distance(xi , xT ), where xi and xT denote mean of best N sampled pointsof these two tasks, respectively. wi: A weighting factor, which reflects the influence of the i-th source task, and is determinedby similarity between the i-th source task and the target task."
}