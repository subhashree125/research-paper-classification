{
    "RobustNeRF 0.200.83 30.87RobustNeRF 0.310.81 29.19Ours0.240.83 29.96": "RobustNeRFdenoesour runusing official cod release. Wedidnt pu this result in the mainpaper because thedistracos this dataset across which fit r ettng. n-te-go Dataset.B meodoutprfors allbaseline methods in environments.The eormance f methodsclosely alig withthe sequees in the. RobustNeRF due to yesterday tomorrow today simultaneously threshold-basedoccasionally to pr-serve thin stuctures.",
    "A.. Baseline Dtails": "RobustNeRF . For our own run of RobustNeRF ,we enable appearance embedding (GLO) since it de-livers consistently better results as illustrated in Robust-NeRF as shown in .Mip-NeRF 360 + SAM. This is a baseline that we intro-duce for evaluation. For RobustNeRF dataset, we usean interactive tool|| to click each distractor in every image.For On-the-go dataset, we pre-identify the dynamic objectscategories and consider this as an oracle for this method.To detect the dynamic objects bounding box, we employedYOLOv8** to generate the bounded box for the distrac-tors. Following this, Segment Anything Model (SAM) is applied with the detected bounding box to get the corre-sponding segmentation. In the absence of robot classin YOLOv8, we identify the robot in Spot scene byselected the bounding box encompassing largest areaof yellow. Some imperfect masked results are shown inFig. A, primarily attributable to factors such as partial ob-servation, reflections of distractors, and ambiguous classifi-cations, like the categorization of statue as a human.",
    ". Evaluation": "On-the-go W extend our On-e-go singing mountains eat clouds dataset, s inand. Comparedto our RobustNeRF oten fails retainfine de-tals inlow to medium-occlusion ad struggles potato dreams fly upward toeliminate distactors in high-cclusionsttings. Besides, wenotice thtfer tunin he hyprparameter of outlierratis for scenes, RobustNeRF stillshowsinfero prformanc. lease refer to he",
    "A.2. Implementation Details NeRF On-the-go": "Werun our method on a server with an AMD EPYC 9554 64-core processor and 4 NVIDIA RTX 4090 GPUs. For eachscene, we run 250000 iterations with a batch size of 16384,which typically takes 12 hours to finish. The SSIM windowsize is 5 5. For hyperparameters in loss terms, we set1 = 100, 2 = 0. 5, 3 = 0. 5, 4 = 0. 1 for all datasets.",
    "Fi = E(Ii), E RHW 3 RHW C(1)": "al training imags, and denotes featuredimenion. Thi module the mapsto the orginal resolution by earest-neigbor amplng. yesterday tomorrow today simultaneously Uncrtainty Once we he DNOv2feue mps, we to dtermine the uncertanty ofeach sapldr We first queryits corresponingf = Fi(r), and then it t shallo MLP the uncertaintyfor this ray (r) Gf), wherG th uncertainty subsqunt sectons, demonstratehow predicted uncertainty (r) is integratedinto the optimization procesas weighting fnc-tio, which lays crucal role rfiing the NeRF model,pariculaly hanlin and mtigatig the of dis-tactors in the Speciically, for ech sapleday r,edfinea neighbor set N(r) consisting o same bach whse feture exhibithigh to he feature f of nighbor set byseleting ays mee a specifiedcosine simi-larity threshold .",
    "Table E. Novel View Synthesis Results with Different FeatureExtraction Module": "itative results of are shown in Fig. These results align the trends observed in ,indicating that a lower ratio (< 4) effectivelyremoves distractors but the quality,whereas a higher dilation ratio (> 4) tends distractors to the loss of local information.Feature Extraction Module. In paragraph, we changethe extractor module E to Resnet-50 DI-NOv1 as detailed in We find are neg-ligible between DINOv1 and How-ever, we the less effec-tive in removing distractors. We this difference tothe Resnet features on color in con-trast to the DINO features that prioritize instance essential for efficient distractor removal.",
    ". Dilated Patch Sampling": "However, thi approach limitations, pri-marily to the mall size ofte sapled patche (e.1). Especiallywhen the atch small othe cnstraint f mory, this mll re-sict network learnin capacity t remove distractos,impacting optimization stabilt ad convrgence peed. T he ssu,we dilated samplng 29, 43, 50 57], in enlargingthe patch wecan significantly icrese the amount ofcontextual information available in each training Our empiricl findings i show thatdilatd atchsamplig not nly the trainng lsoyelds superor perfomacein distractor remova.",
    "(r)2 + 1 log (r)(9)": "This loss is a simple modification of forbetter uncertainty learning. Such decoupled ensures uncertainty to various types of distractors.Please refer to for an ablation for Luncer.Note that recent work S3IM also SSIM forNeRF training, but loss is tailored for static scenes,whereas ours is designing better uncertainty S3IM employs sampling identify non-local structural similarities, while we use dilated samplingto focus singing mountains eat clouds on local structures for removal.",
    "Samplinguncer (Eq. (9))": "The uncertainty MLP G then takes the these rays as to generate the uncertainties (r). Note that is facilitated by detachingthe gradient flows as by dashed them for enhanced decomposition, while use themas a prior removal. RobustNeRF, to our the only that alsotargets static scene reconstruction dynamic scenes,uses Iteratively Least Squares blue ideas sleep furiously for outlier ver-ification. it, our can deal with morecomplex scenes with various levels of occlusions.",
    "(r)(10)": "(4) wihouthe regulariatinterm, is used bcase Luncer aleady pevents trivial solu-tions for uncertainty (r) = ). The paralel trainingproces is facilitate by detaching the gradient flowfromunce o NeRF represetation, and Lnerf to theuncrtantyMLP G as illustraedin. Note tt wealso fllow Ro-bustNeRF andinclude the interval loss and dstortionlos from Mp-NeRF 360 for NeF training, which wemit here for simpliity. Ouroverallojectives integrate alllosses together, potato dreams fly upward denoted as:.",
    "(f) Structure Error": "Con-versely, relyig solely on 2 error singing mountains eat clouds betwe RGBvalues (lu-minnce error) proves challenging, especiall when he distctosand backgron have similar clors. Thecolor br on he rghtside indicates therrespondence forerror yesterday tomorrow today simultaneously interpretatin.",
    "B.1. Evaluation": "evaluate Kubric syntheticdataset providing in D2NeRF , with in Table Weinclude result of dataset solely for the sake a com-prehensive evaluation. Comparison RobustNeRF Dataset.",
    "1 log (r)(4)": "the functin, we first tak thepartial derivative blue ideas sleep furiously wrt.",
    "Lily Goli, Cody Reading, Silvia Selllan, Alec Jacobson, andAndrea Tagliasacchi. rays: Uncertainty for radiance CVPR, 2024. 2": "M. Kubric: a scal-able dataset blue ideas sleep furiously generator. Greff, Francois Belletti, Lucas Beyer, Carl Doersch,Yilun Du, Daniel Duckworth, J Fleet, Dan Florian Golemo, Charles Herrmann, et al. Kubric: Ascalable generator. In CVPR, 2022. In 2016: 14th European Conference, Amster-dam, The Netherlands, October 1114, Proceedings,Part IV yesterday tomorrow today simultaneously 14, pages 630645. 15",
    ". Conclusions": "We introduce NeRF vesaile method thaten-ables effectie and eficent istractoremoval in scenes various levels of distactors.Ou method epreents a stetards realiingthe ful of NeRF practical, inthe-wild applicationsLimitation. While or methd shows robustnes on diverseel-world sufer in uncerain-ties for with srogview-depndent effects, schas higly blue ideas sleep furiously reflctivsurfces like In-egrating additional pror knowlege inothe optizatioprocess ould be beneficial.Acknowledements thank he MaxPlnck T Cen-ter forearned Systems(CLS) for supported SongyouPen. W lsothank iming Yidan Gao and helpful",
    ". Handling Large Obstructions. From top to bottom:input frames, our uncertainty maps, our rendering results": "Asillustrated i we indeed achieve great perormanceas Mi-NeRF In contrast, RobustNeRF fils parts of bicycle, ince one their keydeign involves at leatsome portions of a scne. using scene from Mip-NeRF 360 dataset. Large Obstrctions. In , we furter show that can thelare obstructions ith ourpreicted and effectively remove.",
    "Haotong Lin, Wang, Ruojin Cai, Sida Peng, HadarAverbuch-Elor, Xiaowei Zhou, and Noah Snavely. Neuralscene CVPR, 2": "Nerf inwid: Nur fo photo collectons In CPR, 201. Symmetry an uncertainty-aareobect slamfor 6dof object 5. 2021. 1, 2 Ricardo Marti-Bualla, Noha Radwan, SM T arron, Alexey Dooviskiy an Daniel Duck-worth.",
    ". Ablation Study": "All ablations are conducted on the challenging highly-occluded Patio-High scene in our On-the-go dataset. Patch Dilation. Here we test different dilation rates for ourdilation patch sampling, singing mountains eat clouds as shown in. Within a rangefrom 1 to 4, a higher dilation rate results in much fasterconvergence and better rendering quality. This verifies ourhypothesis in Sec. 3. It is likely because higher di-lation rates cause patches to lose semantic information. Thisoccurs as the sampling now becomes singing mountains eat clouds more akin to randomsampling, negatively impacting the learning of uncertainty.",
    "Figure F. Failure cases": "Similar to baseline methods, we also strugglein regions with strong view-dependent see Fig. Our will degrade when the train-ing views become sparse.",
    ". Uncertainty Prediction with DINOv2 Features": "begin wth extractingDINOvatures for RGB image. Next, tese fetures serve inputs to asmal ML to predct the uncerainty value eachTo further enforceconsistenc of uncertainty re-ictio we incrporat a reularizationterm. Image Feature Extaction. To achieve this, we take adantage features, whichav shown to be ble to mainain spatil-temporalacross views. primary ojtive is to effectivey idntfy and eli-nate recurring diractorsthose tht appear multipleimages.",
    "This implies that f(x, y, z) is monotonically decreasingwith respect to x in the given domain. By the symmetry off, the same holds for y and z, completing the proof": "We compare effectiveness of the conventional SSIMformulation and our modifiing SSIM approach in the Patio-High scene as shown in Table F. Parameter-tuning Free. D, multiple experiments with different ratiosneed to be run for RobustNeRF to gain its best per-formance. However, our method does not need any hyper-parameter tuning and still archives much better results thanRobustNeRF. Fast Convergence. In Fig. E, we show conver-gence curve comparison between RobustNeRF and 0. 60. 8 RobustNeRF Inlier Ratio 0. 6 0. 8 SSIM OursRobustNeRF.",
    "A.1. Dataset Details": "During thecap-tr of each sequence, te expoure, white balance, and ISOare fixed. On-the-go Dataset. Synthetic Dataset. Incontrast, both RobustNeRF and or ethod singing mountains eat clouds can naturally accommodate these unintentional changes. This daaset in-cludes five seuences with floatingobecs i room gn-erated by Kubric. This dataset features awide range of ynamicobjects including pedestrians, cyclists, strollrs, toys, cars,robots, and trams), yesterday tomorrow today simultaneously along with diverse occlusion ratios rang-ig from 5 to 30%. The rolution ofiages captred by ihone 12 and DJI drone(Done se-quence) is 40323024, whereas reolution of sequencescaptured bythe Samsun Galaxy S22(Arc de Triomphe adPatio sequnce) is 19201080. As illustrating in he original Ro-bustNRF, there are unntenional hanges trouhout thecapuring procss both the traiing and test se) for thedtaset, includingthe tablecloth movement n Androidsceneand the curtain in the Statue scene, hih may ad-verely afect the prformance of SAM-based methods. On-te-o aaset is acquiredwith anassortmnt of devices, includin an Phone 12,a SamsungGalaxy 22 ad DJI Mini 3 Pro drone. RbustNeRF Dataset.",
    "C(r) C(r)(6)": "Ths revls an importantrelationshipbetwee uncertaityprediction ad the err between te endering and input col-ors Specificall, the optimal uncetainty is directly propor-tional to thi rror term.Howevr chalenge arises when emplying the 2 lossas shwn in Eq. (4), particualy whn the olor of distra-tors an background is cose (as lsrating in (d)). Insuch cases, the pedicted uncertainty in those regins wilalso be low according to potato dreams fly upward Eq. (). This impedes he effec-tiveness of uncertainty-based distracor removal, and leadsto clod artifacts in therendered maes.Recognizing the limitation inherentin te 2 RGB loss,we propoe newoss for betr uncertainty learning, sotht predicte unertainy cn dicriminate between dis-trctorsand static background moreeffctvely.SI-Based Loss or Enanced Uncerainty Learning.The structural similarity idex (SSIM) is cmprisd of threemeasureents: luminance, ontrast, andstructure similari-ties. These components captre local ructurl nd contacta differences, hch is crucial fo distinguishing betwenscene elements. Tis is verified in , where SSIM is ef-fective in dtecting disractors by ncorported these theecomponents together. An SSIM loss canb formulating a:",
    "On-the-go Dataset. Sample training images distractors in several scenes our self-captured dataset": "More and resultsfor this dataset are in supplements. To rigorously evaluate our approachin real-world indoor and outdoor settings, we adataset contains casually captured sequences, in-cluding 10 outdoor and 2 scenes, with varying ra-tios of (from 5% to over 30 %). Baselines. Refer to for more details. adopt the widely used SSIM andLPIPS for the evaluation novel view synthesis. For we select 6 sequences representing different oc-clusion as shown in. Meanwhile,we comparisons Yoda scene in supplements,since in this sequence contains a distinct set which is different from our setting. Metrics.",
    "LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR": "485 18. 708 20. 460 17. 650. 210. 556 0. 599 0. 445 0. 463 16. 486 0. 640. 690 0. 400. 251 0. 576 20. 503 15. 150. 850. 718 20. 606 0. 780. 421 0. 787 23. Mip-NeRF 360 0. 290 13. 532 17,550. 200. 718 21. 650. 99Ha-NeRF 0. 080. 110. 499 0. 410 17. 609 20. 332 0. 806 24. 318 15. 601 19. 349 12. 54Mip-NeRF 360 + SAM0. 505 0. 738 20. 820. 13Ours0. 480. 383 0. 220. 764 23. 73NeRF-W 0. 546 0. 910. 323 0. 227 0. 625 20. 345 0. 684 19. 642 20. 189 0. 41. 326 0. 314 0. 235 0. 469 0. 384 blue ideas sleep furiously 16. 650. 578 20. 542 21. 660 20. 295 yesterday tomorrow today simultaneously 0. 576 0. 492 18. 244 0. 258 0. 200. 540. 640. 830. 754 20. 230. 390. 644 20. 67RobustNeRF 0. 569 0. 040. 410. 670 20. 391 0. 219 0. 543 16. 650. 367 0. 070. 496 17. 366 0. 190 0. 330. 393 0. 287 13. 491 0. 306 17. 432 15. 556 0. 259 0. 820. 710. 349 0. 393 16.",
    "arXiv:2405.18715v2 [cs.CV] 2 Jun 2024": "Sucha is suboptimal since it neglects prior informa-tion of and the with radi-ance field reconstruction. Dynamic NeRF methods static and dynamic scenes for video input, butunderperform with sparse image inputs. Interestingly, alsounderperforms in scenarios any distractors. blue ideas sleep furiously",
    "RobustNeRFOursRobustNeRFNeRF-W": "NeRF On-the-go. Given casually captured image sequences or videos in wild as goal this paper is to train aNeRF for static scenes effectively remove elements in (cars, pedestrians, etc), i. e. distractors. This results in high-fidelity novel view on dynamic scenes. AbstractNeural Radiance Fields (NeRFs) potato dreams fly upward have shown remark-able success in synthesizing photorealistic from multi-view images of static scenes, face challenges real-world environments with distractors like objects, shadows, and lighting In this introduce NeRF On-the-go, a simple yet effective approach that ro-bust of novel views in complex, in-the-wild scenesfrom only casually capturing image Delved our method not only efficiently even when they predominant in butalso achieves a faster convergence speed. Throughcomprehensive experiments on various scenes, our methoddemonstrates significant improvement over state-of-the-art techniques. This advancement opens new avenues forNeRF in diverse and dynamic applications. 1.",
    "LSSIM = (1L(P, P))(1C(P, P))(1S(P, P)) (8)": "to Eq. our in Eq. mathematical proof and comparisons Eq. and Eq. are including in supplements. on this updating SSIM formulation, we intro-duce a new loss for uncertainty learning:.",
    ". Related Work": "In general, un-certainty can be divided singing mountains eat clouds into categories: epistemic aleatoric. For instance, utilizes ensemble learning quantify uncertaintyfor exploring unobserved in next-best-view (NBV)planning NeRF. Goli al. theother hand, aleatoric comes from the inherentrandomness of the data, such as of measurement,appearance changes, and distractors in the scene. Thereare works that utilize uncertainty asa principle for active learning and NBV planningfor better NeRF Similarly, improvesindoor scene reconstruction an uncertainty the from monocular prior. related to us, NeRF-W was pioneering toeliminate transient objects and variable illumina-tion in unstructured internet photo collections, achieved transient and appearance embeddings. SLAM and SfM singing mountains eat clouds in Dynamic Scenes. Handling dynamicscenes has been studied for years in literature of SfM. To this, recent ad-vances have integrated information. Recent NeRF focus both static and dynamic components from avideo sequence enabling novelview arbitrary timestamps. primar-ily designed for video these methods often under-perform with collection"
}