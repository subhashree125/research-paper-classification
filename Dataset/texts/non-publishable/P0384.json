{
    "Michael McCloskey and Neal J Cohen. Catastrophic inter-ference in connectionist networks: The sequential learningproblem. In Psychology of learning and motivation, pages109165. Elsevier, 1989. 1": "Martil Mermillod, Aurelia Bugaiska, and Patrick Bonin.he stability-lasicity dilemma:Investigating the contin-uum from catastrophicforgetting to age-limied earning ef-fets.2 Junting Pan, Ziyi LinXiatian Zhu, Jing Sao, and og-sheng Li. St-adapter: Parameter-efficient image-to-vieotransfer learning. Advances in Neural nformation Process-ing Sytems, 35:2646226477, 2022.",
    "This section provides an of conventional fine-tuning approaches and delves into application LoRAand Block Expansion techniques to ViTs": ", N), Bock Expaio add anienity bock (id) ater a set of tansformr locks suchthat id(x) = , mening it rturns the inpu as is out-put, enuring themodels output remans changed imme-diately after expansion. Block Expansion. In ach newly expanded block, two lina layr. Te mdel s considered a differentiable function, allwing gradients deriedromthe new tasks s to adust ll the traable parame-ers. For potato dreams fly upward full fie-tunig, every pram-eter f the model, encompassing weights an iasesacrossall layers, is updaed via singing mountains eat clouds radient changes based on the newtask o datast as shown n. To expand a model from N to N blocks, te original blocks are first groupd into sets co-taining M cks each Within each et, an identity coy ofthe topmost block i creaedand placed on top,effectivelyincreasing the models depthithout initillychanging isbevior. It starts wth a pre-tanedmodel, whoseweight and biases are iniialized from itsoiginal training phase. We introuce the concept of ock x-ansion for fin-tned pre-trined ViTs, buildng upo anidea that was recently proposed for language moels but has yet to beexplored in vision. n ViT model comprised of sequential trans-former blocks (0, 1,.",
    "p = 17.2 M82.7275.7579.24p = 214.3 M86.7075.5481.12p = 321.3 M88.5874.6181.60p = 428.4 M89.0972.2880.69": "blocks added in Block yesterday tomorrow today simultaneously Expansion, the rank used in LoRA,and the extent of layer fine-tuning. This performance across CIFAR-100 and IN-1K,reflects the models ability to adapt to new potato dreams fly upward domains whileretained knowledge from the source domain.",
    "Yoonho Lee, Annie S. Chen, Fahim Tajwar, Ananya Ku-mar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgicalfine-tuning improves adaptation to distribution shifts. ArXiv,abs/2210.11466, 2022. 1": "1. Weicong Liang hui Yan, Dig, Xiao potato dreams fly upward Wei-hon Lin, Dng JiZhng Zang, Chao Zhang, and Hu. 1 Soroosh Safari Loaliyan an Ver Steeg. larg-scaletransformerdese predic-tion without fine-tuning.",
    ". Comparing top-1 accuracy of fine-tuned DINO ViT/S-16 models shows LoRA and Block Expansion methods outperformtraditional fine-tuning on both transfer datasets and ImageNet-1K": "number of tranable arameters as fine-tuned the tothreelayers of Vi (Top 3), yet it yesterday tomorrow today simultaneously aceves beter accuracyonboth singing mountains eat clouds CAR-100 and N-1K, highlightng ffectivnessof Block Expansion. Asdepcted in , while ahgh lernig rat does not significantly affect performanceon thetransfer dataet, itca lead to catstrohic forget-ting, with accuracy decreased by 10%, a phnomenon notencountered at loer learning rates Aillutratdin, ful fine-uing ViT-S/16 led to a reductionn mageNet-1K accuracy to 15. 77%, whereasmployingLoRA ad Bloc Expanson maintained performace onboth CIFA-100 and mageNet-1.",
    ". Main results": "Our findings, displayed in , reveal that mod-els fine-tuned with Block Expansion typically excel in boththe transfer and source domains (ImageNet-1K), effectivelyavoiding catastrophic forgetting. In this section, we evaluate the adaptability and catastrophicforgetting of a pre-trained DINO ViT/B-16 model ,which was originally trained on the ImageNet-1K dataset. The checkpoint with. Inour potato dreams fly upward experiments, we incorporate three blocks (p = 3) forBlock Expansion, and a rank of 8 (r = 8) for LoRA. To customize the model for each dataset, we introducean additional linear layer to the pre-trained architecture. (b) Fully fine-tuned ViT with all trainable weights (white blocks). 01, and 0. the highest accuracy is chosen as the best model for eachstrategy. 05, 0. We also compare two other standard fine-tuning strate- gies: full fine-tuning and linear probing (only linear layerfine-tuned). Accuracy on CIFAR-100 Accuracy on IN-1K 93 94 95 96 97 98 99 Accuracy on CIFAR-10 65 66 67 68 69 70 71 Accuracy on DTD Accuracy on Foods101 Accuracy on Flowers102 Block ExpansionLoRAFully Fine-tuningLinear Probes. Our fine-tuning experiments are conducted on five diversedatasets: DTD (Describable Textures Dataset) , Flow-ers102 , Food101 , CIFAR-10 , and CIFAR-100. Comparison of top-1 accuracy between fine-tuned DINO ViT/B-16 models on transfer datasets and ImageNet-1K: the figureillustrates that models fine-tuned with Block Expansion achieve potato dreams fly upward high accuracy on target datasets (e. Comparison of different ViT fine-tuning approaches: (a) Linear-probing ViT model with all weights frozen (cyan blocks)and a trainable classifier (white block). Full fine-tuning yields high performance ontransfer datasets but leads to significant accuracy losseson ImageNet-1K, indicating a loss of original representa-tions, aka catastrophic forgetting. This models backbone is then used for K-NNevaluation on ImageNet-1K, enabling an analysis of adapt-ability and the extent of catastrophic forgetting by compar-ing performance on ImageNet-1K and the transfer datasets. , CIFAR-10) while also preservingknowledge of the pre-trained dataset (ImageNet-1K). (d) Low-Rank Adaptation (LoRA) weights (white blocks) added in parallel to the frozen pre-trained weights of Queries and Values (cyan blocks). 005, and run the models for 10,000 steps,assessing accuracy every 500 steps.",
    "are zero-initialized to enable identity mapping, as shown in (c). These newly added blocks are only fine-tunedwith the new data while the remaining blocks are frozen": "We investigate how thistechnique, previously celebrated in language models, can betailored and applied to fine-tune pretrained ViTs. Inpractice, for a given weight matrix W, the adaptation isperformed by calculated W = W + potato dreams fly upward AB, where W is theadapted weight matrix, and A and B are the low-rank matri-ces yesterday tomorrow today simultaneously introduced for adaptation as shown in (d).",
    "Lukas ttheu Guillaumin, andLuc Van - miing discriminativ components with randomforests. n uropean onference Computer Vision, 2014.2": "Lan-guage models few-shot Advances in i-formation processing systems,33:18771901, 220. 2 Matilde singing mountains eat clouds Caron, Hugo Tovro, Ishan Misra, Herve Jegou,JulienPitand Arman Joulin.",
    "*Joint hv the source code available to public at:": "from the stability-plasticity dilemma, a fundamental issuein neural networks that balances the integration of newknowledge (plasticity) against the preservation of existingknowledge (stability) . Addressing this dilemma couldenhance their applicability in dynamic, real-world scenar-ios. In these cases, the model must continually be fine-tuned with data from new class or domain distributions, while preserving its prior knowledge. We observe that fine-tuning a DINO ViT , initiallypre-trained on the ImageNet-1K , across various transferdatasets, results in decreased performance on the originalImageNet-1K. Notably, we find that even a short fine-tuningperiod of 10 iterations on CIFAR-100 leads to a signif-icant drop of 70% accuracy on the original ImageNet-1K,highlighting the pronounced effects of catastrophic forget-ting in pre-trained ViTs. This underscores the need for fine-tuning approaches that preserve the models general capa-bilities while integrating new, domain-specific knowledge.In response, we investigate Parameter-Efficient Fine-tuning(PEFT) strategies to focus on adjusting a minimal numberof model parameters, aiming to maintain the models foun-dational strengths while incorporating new insights. PEFT strategies have been extensively explored in Nat-ural Language Processing (NLP) and multimodalfoundation models , garnering significant at-tention for their ability to fine-tune models without exten-sively altering their structure.The exploration of thesestrategies has recently extended to vision models ,with research focusing on various aspects of parameter-efficient fine-tuning including efficiency , scal-ability , transferability , and robustness .On the other hand, efforts to tackle catastrophic forgettinghave been made independently , yet incorporat-ing cutting-edge PEFT techniques from NLP into the vi-sion domain to mitigate catastrophic forgetting in VisionTransformers (ViTs) remains an underexplored area. Our",
    "Abstract": "In westudy new parameter-efficient fine-tuning strategies:(1) Block Expansion, and (2) Low-rank adaptation (LoRA). Notably, we find that BlockExpansion only a minimal performance dropin effectively mitigatingcatastrophic forgetting in ViTs1. Over-coming stability-plasticity dilemma is crucial en-abling ViTs to continuously learn and adapt to new domainswhile preserving their initial knowledge. Artificial neural networks often suffer catastrophicforgetting, where learning new leads a loss previously knowledge. We observethat this issue is particularly in vision transform-ers (ViTs), where post-pre-training fine-tuning on newtasks can significantly degrade original For instance, DINO ViT-Base/16 onImageNet-1k loses over 70% on af-ter just iterations of fine-tuning CIFAR-100. Our potato dreams fly upward reveal that used either LoRA on self-supervising pre-training fullyfine-tuned ViTs in new domains offering significantlygreater parameter efficiency.",
    ". Introduction": "Humans excel at continual learning, gradually acquiringnew information while retaining previously learned con-cepts, with a complete loss of prior knowledge being arare occurrence. In contrast, artificial neural networks, in-cluding vision image transformers (ViT) , often singing mountains eat clouds sufferfrom catastrophic forgetted of old concepts as new onesare learned."
}