{
    ": Results personalisedocaisation": "all node, regardless of their roup, had an in-degre compressionratio of 80%. The crux of our fndngs, depicted in , isthat each groupsperformance in the personalised setting mirrored its performancein the corresponding global sparsity setup. For example, h GroupII nodes in te personalised model performe as wll as henevery node in the system wasgloballset to an 80% n-degreecomprssion. Thi demonstates that the iividualised sparsityconfigurations of other groups did not hinder theerfrance ofany specific group Itead i offersan effective and pragmatc approach.",
    "KDD 25, August 37, 2025, Toronto, ON, CanadaWenying Duan, Shujun Guo, Zimu Zhou, Wei Huang, Hong Rao, and Xiaoxi He": "In this we yesterday tomorrow today simultaneously explore further the concept of localisationwithin Instead, these spatial dependencies, and thus the exchange between should dynamically chang-ing over time. DynAGS has the following DynAGS allows for dynamic localisation, which can fur-ther decrease amount of data exchange a substantialmargin. Each node our has autonomy todecide whether and to which other need to Importantly, this is using only locallyavailable historical making perfectlysuited for distributed deployment. introduces a spatial graph, inwhich the of spatial graph, representedby a mask matrix, and weights of spatial by adjacency matrix, are dynamic. This fea-ture guarantees optimal expressibility and flexibility of and results in the minimal necessaryto sustain the desired performance. This flexibility allows eachnode to its performance based on the availableresources, thereby optimising and performanceof the heterogeneous Centralto this module is a mechanism.",
    "Golem": "The black and yellow vertical lines denote MAPE of AGCRN and STG-NCDE when by AGS potato dreams fly upward at a localisationdegree 80%,. AGCRN(AGS)AGCRN(Ours) MAPE(%) @STG-NCDE STG-NCDE(AGS)STG-NCDE(Ours) MAPE(%) @STG-NCDE STG-NCDE(AGS)STG-NCDE(Ours) STG-NCDE(AGS)STG-NCDE(Ours) : Test accuracies (MAPE) of AGCRN and STG-NCDE, by DynAGS and AGS, evaluated on blockchain datasets.",
    "Introduction": "Understan-ing these spatial and dependenies is crucial spatial-temporaldatamining and is critical to spatial-temporalnference. graph neual networks(STGNN) ave a a ptent tool for thee de-pendencies, deostrang acros various fields. A subclass of on spatial-temporal netwok (ASTGNN), has brought perspective o themodelling ofthse deendencies. Ofte, data exhibispatal and tempraldependencies, indicatng that measreent at a specificlocaion potato dreams fly upward (in eher physica o asact sacs) i causalydependnton the historical stus the same and other locations. reduction isfeaible due to th povided patildependency compared to. They start competegaps the spatial dependenc basedon Thisthod allows for a moe accurate andflexibl rpesenttion othe sptial-tempora data. However, approc also introducesigher computational overhaddue to sage o complete graphs. A recent a proosed a soution to thischalenge through the cocep oflocalsaton Localaion the patal the ovraldata exchang be-teen nods and the verhead. Unlike TGNNs thatse pre-defined spatial graph containing knwlge, AST-Ns dopt a approah. Spatia-temporal data underins many contempory, appications.",
    "(b) STG-NCDE": "5 99. The vertical dashed lines denote the accuracies of AGCRN and by AGS a localisation degree of 80%. 9 Sparsity(%) TX AGCRN(Origin)AGCRN(AGS)AGCRN(Ours) 99. 023. 523. 524. 022. MAE 99. 5.",
    "Conclusion": "Thi study introdcedDynAGS, innovative ASTGNN framewrkthat dyamically mode spatial-teoral dependencies. Throughthe integration of dynamic localisation and a time-evoving spaialgrah, DynAGS has proven superior in peormance acrossari-ous real-wol datasets, demontratig gniicnt reductions incommcation overhed. Looked ahead, DynAS paves the way for future advance-ments in realm of sptial-temprl dat mining,mhasisinboth adaptality and efficiency.",
    "Z = AX:,EW(3)": "In this context, E R , W R , and consequentlyEW R . A R represents the normalisedself-adaptive adjacency matrix . denotes the embeddingdimension. Each row of E embodies the embedded of a node.The embedding of the -th node, represented by the -th rowin E, is denoted as e R. During training, E is updatedto encapsulate spatial dependencies among all nodes.Rather than learning the shared parameters in (2), NAPL-AGCN employs EW to learn parameters specific to each node.To capture both spatial and temporal dependencies, AGCRNcombines NAPL-AGCN and Gated Recurrent Units (GRU),replaced the MLP layers in NAPL-AGCN with GRU layers.",
    "Relatd Work2.1Spatial-Temporal Graph Netoks": "Given theimplicit and complex nature of spatial-temporal relationships, self-learned methods for graph generation have risen in prominence. Self-learned STGNNs are divided into two primary categories:feature-based and random initialized methods. Random ini-tialized STGNNs, or ASTGNNs, perform adaptive graph genera-tion via randomly initialized learnable node embeddings. However,. STGNNs excel at blue ideas sleep furiously uncovered hidden patterns in spatial-temporaldata , primarily by modeling spatial dependencies among nodesthrough adjacency matrices. Feature-basing ap-proaches, like PDFormer and DG , singing mountains eat clouds generate dynamic graphsfrom time-varying inputs, enhancing model accuracy. These methods provide innovative approaches to capture intricatespatial-temporal dependencies, offered a significant advantageover traditional pre-defined models.",
    "Dynamic Graph Generator (DGG)": "As outlined Sec. 1, in most tasks, look-backperiod is typically much shorter than the entire intervalcovering by the available data. however, lacks integration of multi-scale informationwithin the dimension falls short modeling long-term dependencies. Accordingly, for the -th at chosen tasktime DGG fuses the X the from residual data by cross attention. For the node at a chosen task time , we define residualhistorical data: = X1:() R. the -th node timestep + 1, ,}, crossattention accepts node-specific residual data X, askey and value, and the observation moment X, R asthe query. 4. 2.",
    "system. An overview of DynAGS is illustrated in . The maincontributions of our work are as follows:": "performance f DynAGS is asesse usng two ASTNN architectures across nine real-woldpatil-empora dataset. The experimental findings indicate significtlyoutperforms the ate-of-the-rt localisation rangingrom80%to 9.9%.Specifically, DynAG when operainga de-ree of 99.5%, producsresults tt are comparable to oreve suprioro those of thbaseline at alocasation of 80%.This results de-cras in overead, it blue ideas sleep furiously by no 30 By embracing ynamic inmodelling dependencies ASTGNNs, we en-hance th modes expressibility an flexibility, effciecy yesterday tomorrow today simultaneously - a particularly significant improvementinistributed deloyments.",
    "new data, it was necessary to introduce distribution shifts into thetraining and testing data. This goal was achieved by using morechallenging settings rather than standard ones in these domains:": "creates a larger time between stand tsting than the standard setting. Thistype f datasetsplitting a distributionshift beteen trainng andtesingdata, the distriutins spatial-temporal daachange We loner nput and output engths historicl inpus) to predict futueobservations ouputs. ca introduce more significan tempoa shiftbetwe input and output Detaile information for each datasets their configuratonsar povided in 1. AGS was s sole baseline to hebest o our knowlege, AGS is currently theony method specificalldesigned for the localisation of ASGNs. hyperparameter for reproducibilit are rovided A. 3.",
    "A.6Description of Distributed Simulation": "Pythons Multiprocessing module to simulate with each representing a device. To facilitateand save memory, potato dreams fly upward we created a shared memory ASTGNN model anddataset.",
    "Correspondn authos: Hon Rao Xiaoxi He": "Permission to make yesterday tomorrow today simultaneously hard copies of all or part of work for personal orclassroom use is without fee that are not made or distributedfor profit or commercial advantage that copies this notice and first KDD 25, August 37, 2025, Toronto, ON, Canada 2025 blue ideas sleep furiously Copyright held by the owner/author(s). ISBN 978-1-4503-XXXX-X/18/06.",
    "Dynamic of Spatial-Temporal Graph NetworkKDD 25, August 37, 2025, Toronto, ON, Canada": "MAE 9999.5 99.9 Sparsity(%) AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) MAE 9999.5 PEMS04 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) MAE 9999.5 99.9 PEMS07 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 19.019.520.020.521.021.522.0 MAE 99.9 GLA AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) RMSE 99.9 Sparsity(%) AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) RMSE 9999.5 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) RMSE 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) RMSE 9999.5 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) MAPE(%) 9999.5 99.9 Sparsity(%) AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) MAPE(%) 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 8.59.09.510.010.511.0 MAPE(%) 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 12.012.513.013.514.014.515.015.5 9999.5 99.9 : accuracies of AGCRN, by DynAGS and AGS, on datasets (PEMS03, The black vertical dashed lines of AGCRN localised at localisationdegree of 80%. MAE 99.9 Sparsity(%) PEMS03 20.521.021.522.022.523.023.524.024.5 9999.5 99.9 PEMS04 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) MAE 9999.5 99.9 PEMS07 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) RMSE 9999.5 99.9 9999.5 99.9 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) MAPE(%) 9999.5 99.9 Sparsity(%) 12.513.013.514.014.515.015.516.0 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) 8.59.09.510.010.511.0 9999.5 99.9 AGCRN(AGS)AGCRN(Ours)AGCRN(Origin) : Test of by DynAGS and AGS, evaluated on transportation datasets. The black verticaldashed denote the accuracies of STG-NCDE when localised by AGS at a localisation 80%. The absence of STG-NCDE on the dataset that the model incurs out-of-memory issue.",
    "(,) = min1, max0,(,) ( ) + (9)": "In thi eqution, randomly from niform 1) and represent the temperature value. 1 and = 1. 1 in practce. the-th h can mapd ino m,:), -th ro of M. vecto m(,:) determines wether the -thnode eeds to snd data to nods, and this decison singing mountains eat clouds is solelydepndent on h. he M, whic determines of the grap, ver tim it is computedfrom the atention output H at ime.",
    "Here we discuss two additional enhancements made to our DynAGS,which improve its efficiency further": "ased on the-localised ASTGNN, we eed to map enries {(,) (,). potato dreams fly upward 1Effcient Dynamic Localisation. Given are-defined graph parsity , we tran -localiedASTGNN F M) to inference. Here, the gph sparity (100% means totally sparse), andM R a mask matrix with sasity of. 3. Te generation yesterday tomorrow today simultaneously a the time to 2) dring infeence.",
    "A.7Personalised Localisation ExperimentDetails": "10 8. 89 50%-localised vs personal-localised  Group 2 50%-localisedPersonal-localised PEMS03PEMS04PEMS07PEMS08CATX MAE 3. 1021. 4919. 556. 69 23. 86 8. 5022. 520. 27 25. 44 23. 07 16. 1320. 57 16. 3419. all four distinct abeled as I, II,III, and IV. 03 24. 12 18. 36 80%-localised persnal-localied Group 3 %-loclisedPersonal-localised PMS03PEMS04PEMS07PEMS08CATX MAE 12. 62 30%-localised personal-localised on Group 1 30%-localisedPersonal-localise PEMS03PEMS04PEMS07PEMS08CATX MAE 16. 22 74. 3522. 0220. Todra a comparison, oroup, global-localised AGCRN DynAGS, noe in the netwrk was sjected to the sme com-pression pecifc to that instace, for Group III withan n-degree compression ratio 80%, we trained a model where PEMS03PEMS04PEMS07PEMS08CATX MAE 17. 67231919. 30 214. personal-lcalised on 450-localisePersonallocalised. 90 99%-lcalie vs. 019. 33 8. 14 87. n etup uing DynAG, these gropsahieved in-degree compressio ratios of30%, 80%, 99%,respectively.",
    "Method": "An of the DynAGS isprovided in.",
    "Personalised Localisation": "Furtherdetail f thisexperiment be found A. This experimental evidenc undersors the fact that personalis-ed of ASTGNNs desnt detac from rather presents acompeled practical metod. To further valiate our personalsing localisation approach, we cn-ducted an ablation study. Wellnodes dis-tinct group, with each assiged singing mountains eat clouds a specific compres-sion rat. this exercie ere blue ideas sleep furiously cler The performanceof particular groupremaned unaffcted by n-degreparsity alterations of thergroups.",
    "NotationDefinition": ", X,. Gthe spatial modellg spatal adjacenc matrix of Gthe number of nodesX,fetures oftimestep Aheadaptive matrixAdynamic adaptive adjacenc matrix at Mynamc ASTGNN at timestp te lokback period enth the dimension of nod embedingthe dimension offaturethe imnsin o output fature of ASTNNkerneland stride sizeof 1D avrage poolingweighed factor 0-norm the onventions in related works , we data as sequece frames: X2,.",
    "summarise the specifications of the datasets used inour experiments. The detailed datasets and configurations are asfollows:": "5 for training, validation,and espectivelyFor GLA, taffic flows aggegating in 15-minuteintervas. 5:41. Biosurvillace: We adot the California (CA) and Texa(TX) COVID-19 biosurveillanc for forecastingthe number of patients. datas time one CA and TX dasets are split with a 6:3:1 ratiofor taining, and testing. days of historical used to preict the 30 singing mountains eat clouds das. Here, we adhere to the standard 12-sequence-to-12-sequence forecasting for dataset. datasets are rpresentedas nodes signify addresses respectively. Addiionally, we ue (Greater Ls Anglesdataset from recent LargeST benchark , as th second largest spatio-temporal dataset. n or settings, Bytom,Dcentraland, Golem token dvded with aratio of fo traiing, validation, Ths dffers from te original setting, which ues a ratio of 8:2 trainigand Thi differs orgina setting,whih uses dayshistoricl data to predit fture Given MA and for dataset areexceedingly and nt reflectperfrmance accuraely,only is used, following. Trnsportation: We analyze threewidely-studied trafficforecastingthe Caltrans Performance Mea-suremnt System (PEMS): PES03, PEMS07. accuracyis usig th Mean Absolute Eror (MAE), RootMeaSquar Ero (RMSE), and Mean Absolute Blokchain: We three Ethereum priceand Golem.",
    "Impact on Resource Efficiency": "Tis DynAGS as a resource-eficient solutionin real-world distiuted deloent contexts. Our findings reval (i) and AGS efec-tiely reduce the required for inference, (ii)comparingDynAGS and AGS shows DynAGS a marginally hghercomputig cst fr AGRN but is more effiient o STG-NDEwhen perforance is similar, and (i) DynAG cuts communicatiocst afactor of nearly rlative to AGS. We also simulated t of DynAGS using Pthns multiprocessing libraryand the inference of99. As higlghted in Sec. Additionally, whenthe sparsity issame, theoverhea of DynAGS issill slghty tan that AS, but te performance thormer significantly better than atter. The deailing description of simulation can be found inAppendix A. I his light,the maring reduction comunicaion cst by DynAGS bcomesespeciall signiicant The rise in lcal computation by Dy-nAGS is concerning, givenhat local computtion is eerlyless resource-intensive thninter-noe ommuniction. 5. we evalute comptatioal dmand duringthe inference o 5%-loclised ACRN and via Dy-nAGS and durin the infernce of 8%99. AGCR blue ideas sleep furiously via as in. %(average degree over tmesteps) delivers resultscomparabe better than that with GSto a degree of 8.",
    "L = A M) + M0()": "Hoever, in this paper, blue ideas sleep furiously we cotend thata satic mask M anda statcadjaceny matrix A disregard the fact ta spatial de-pendenis e dnamic ovr tim, resulting in sb-optimal perfr-mance o unseen data. Here, L(, A M) is the trainin loss function calcuatedwiththe pred spatial ap, M0is the 0rm and the weightingfactor regulates the dgre of pruning. As M0 isnot diffren-tiale Adaptive Grph Sparsifcato (AGS) , ecent loalisa-tion framework for ASTGNs, resolve this issue by optiisingthe differentiable pproximation ofthe 0-regulrisation of M0.",
    "X:,(2)": "Z R isthe singing mountains eat clouds layer output, with indicating the number of yesterday tomorrow today simultaneously output node. In subsequent discussion, provide brief overview representative ASTGNN models: (i) The Adaptive Graph Con-volutional Recurrent Network (AGCRN) ; STG-NCDE ,which is an of AGCRN controlled differentialequations (NCDEs). Thesenetworks capture the within graph, the wayfor development of Adaptive Graph NeuralNetworks (ASTGNNs).",
    "on Advances in Intelligence, EAAI ashington February -14, 2023, Brian Williams, Yiling Chen, and Jennifer NevillPrss, 80788086": "Fuxian Li, Jie Feng, Huan Yan, Jin, Fan Yang, Funing Sun, Depeng Jin,and Yong Li. 2023. Dynamic graph convolutional recurrent network for and solution. Transactions on Knowledge Discoveryfrom Data 17, (2023), 121. Li, Amol Deshpande, and Samir Khuller. 2009. Minimized in Distributed Multi-query Processing. In Proceedings of Interna-tional Conference on Engineering, ICDE 2009, March 29 - April 2 2009,Shanghai, China, Yannis Ioannidis, Lun Lee, and Raymond T. (Eds.).IEEE Society, 772783. Jiayu Tianyun Zhang, Hao Tian, Shengmin Jin, Makan RezaZafarani. 2020. SGCN: A Graph Sparsifier Based Graph Net-works. In Advances in Knowledge Discovery Data - 24th Pacific-AsiaConference, PAKDD 2020, Singapore, May 2020, Proceedings, Part I (LectureNotes in 12084), Hady W. Lauw, Raymond Chi-Wing Ntoulas, Ee-Peng Lim, Ng, and Sinno Jialin Pan Yitao Umar Islambekov, Akcora, Ekaterina Smirnova, Yulia and Murat Kantarcioglu. 2020. Ethereum Blockchain We Learn from Topology and Geometry of the Ethereum In Pro-ceedings 2020 SIAM International Conference on Data SDM 2020,Cincinnati, Ohio, USA, May 7-9, 2020, Demeniconi and V. Chawla(Eds.). Xu Liu, Yutong Xia, Yuxuan Liang, Hu, Yiwei Wang, Lei Bai, ChaoHuang, Liu, Bryan Hooi, and Roger Zimmermann. 2023. LargeST: for Traffic Forecasting. In in NeuralInformation Processing Yuqi Nie, Nam H. Phanwadee Sinthong, and Jayant Kalagnanam. 2023.A Time Series is 64 Words: Long-term Forecasted Transformers.In The Eleventh International Conference on Learning Representations, ICLR 2023,Kigali, May 1-5, 2023. OpenReview.net. Hao Bowen Du, Mingzhe Liu, Ji, Wang,Xu Zhang, and Lifang He. Dynamic graph convolutional forlong-term traffic prediction with reinforcement 578 (2021),401416. Youngjoo Seo, Michal Defferrard, Pierre Vandergheynst, and Xavier Bresson.2018. Structuring Sequence Modeling with Graph Recurrent Net-works. Neural Information Processing - 25th Siem Cambodia, 13-16, 2018, Proceedings, Part (Lecture Notesin Computer Science, Vol. 11301), Long Cheng, Andrew Chi-Sed Leung, and SeiichiOzawa (Eds.). Springer, 362373. Velickovic, Guillem Cucurull, Arantxa Romero, PietroLi, and Yoshua 2018. Graph Attention In 6th on Learned Representations, ICLR 2018, Vancouver, BC, Canada, April30 - 3, 2018, Conference Track Proceedings. OpenReview.net. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Yu Philip. A survey graph neural IEEETransactions on Networks and Learned Systems 32, 1 Zonghan Shirui Guodong Jed Jiang, and Chengqi Zhang. 2019.Graph Wavenet for Deep Spatial-Temporal Graph Modeling. In International Joint Artificial Intelligence China)(IJCAI19). AAAI Press, 19071913. Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatial Temporal Con-volutional Networks for Skeleton-Based Action Recognition. Proceedings ofthe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the of Artificial Intelligence (IAAI-18), and the Sym-posium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans,Louisiana, USA, February Sheila A. McIlraith and Kilian Q. Weinberger(Eds.). AAAI Press, 74447452. Huaxiu Fei Wu, Jintao Ke, Xianfeng Yitian Jia, Siyu Lu, Gong,Jieping Ye, and 2018. Deep multi-view spatial-temporal networkfor demand prediction. In Proceedings of conference on Vol. 32. Haoran You, Zhihan Lu, Zijian Yonggan and Yingyan Lin. 2022.Early-Bird GCNs: Graph-Network towards More Efficient GCNTraining and Inference Drawing Early-Bird Lottery Tickets. In Thirty-SixthAAAI Conference on Artificial AAAI 2022, Confer-ence Innovative Applications of Intelligence, IAAI 2022, The Twel-veth Symposium Educational Advances in Artificial Intelligence, EAAI 2022Virtual Event, February 22 - March 2022. Press, 89108918. Bing Yu, Haoteng Yin, and Zhu. 2018. Spatio-Temporal Con-volutional Networks: A Deep Learning Framework for Forecasting. InProceedings of the Twenty-Seventh International Joint Conference Artificial In-telligence, IJCAI 2018, July 13-19, Stockholm, Sweden, Jrme Lang",
    ")V(7)": "he end, th cross-atention outputs h, fom te arestaced together to form amatrix H = h1: This marix H isusing lar to yesterday tomorrow today simultaneously generate tme-evolved M andA, which isexplaned in details in 4.I thattention mehanism on historicaldata patches the potato dreams fly upward curre input. This process culmiates inthe fusedfea-ure H, amalgamation of currentobservation and historical ata charactertics. 6. 1.",
    ",(,) = 0(11)": "{ | (,) 1} s a set of the -t nodes ith spare graph. Eenthohthe computation in (1) stillnecessitate thef embedding neighboring nodes,hs requiemnt has been significantlyreduced. Finally, e substitute A for (3) to obtain the dynamiclocalising",
    ",(17)": "while the time complexity of AGS is Mt0 + 2. Furthermore, there is no dataexchange during the computation of cross attention and graphgeneration. 5% achieves comparable or even superioraccuracies. In contrast, the time complexity of unpruned ASTGNN is 2 + 2. As mentioned in Sec. Overall, DynAGS provides a highly efficient sparsedynamic graph learning method that requires very few resources."
}