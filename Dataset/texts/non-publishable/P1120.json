{
    "Xiaocheng Yang, Mingyu Yan, Shirui Pan, Xiaochun Ye, and Dongrui Fan. 2023.Simple and efficient heterogeneous graph neural network. In Proceedings ofAAAI, Vol. 37. 1081610824": "Huaxiu Ya, Chuxu hang, Yin Wei, eng Jang, Suhang ang, JuzhuHun, Nitesh Chawla, and Zhenhui Li. 2020. 666663. 2020. In Proeedings of AAAI, Vol. Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Scalableand order-robut continua learning with aditve arameter decomposition InInternational Confeence on Lerning Representation. Grph few-shot learning iaknowledge transfr. 34.",
    "Vx (,; A),(8)": "In our experiments,we adopt three reprsentative strategies. = 1. Te first stratey (S1) is basic version of essae passing and can be mulated as = A.",
    "RELATED WORKS2.1Continual Learning Continual Learningon Expanding": "isolationmthods adaptively allocate nw parameters fo te new tasks torotect the ones forthe tasks. Mem-ry replay based store and represntative data yesterday tomorrow today simultaneously fomprevioustaks new tasks. Rcently, CL on expandng etworks moeatention due to its impornce. 4). In conrast, CL expandinetworks alleviate forgetting, therefre the daais inaccessible.Secondfew-ot gaph learnin fast adaptation to new sks.",
    "CONCLUSION": "In this work, we propose a general framework of Parameter De-coupled Graph Neural Networks (PDGNNs) with Topology-awareEmbedding Memory (TEM) for continual learning on expandingnetworks. Based on Topology-aware Embeddings (TEs), wereduce the space complexity of memory buffer from O() toO(), which enables PDGNNs to fully utilize the explicit topologi-cal information sampled from the previous tasks for retraining. singing mountains eat clouds Wealso discover and theoretically analyze the pseudo-trained effect ofTEs. The theoretical findings inspire us to develop the coverage max-imization sampling strategy, which has been demonstrating to behighly efficient when the memory budget is tight",
    "#tasks30202023": "When learning onmoreheterophilous networks (homophilyratio close to 0) singing mountains eat clouds () is required to be constuced speciallyconstrucd. Heterophilous etwork learningis lrgely fferentrom homophlus netwrk learnig, andreuires different GNdesins . Thereore, () sollso be instantiatedto be suiable for heerophilous networks. Teke dfference of het-erophilousnetwork learning is that the nodes beonging to the smeclass renot likely tobe connecte, and GNNs should be desigedto separately process te proxima nighbors with similar infor-mtio an distal neighbors with irrelant informaio, or onlaggretenformaionrom the proximal neighbor .For exampl, Mop encoesneighors from differen hopsseparately. A given computtion go-subnetwork will be dividedinto different hops. For each hop, themodel generates a sparteebeddng. Finay, th embeddings of different hopare concate-nated as te final TE. H2CN onl aggregates higerorereighbos that are proximl to the center nde.In oher wors, via constructing () to be suitable for blue ideas sleep furiously het-erophilous ntworks, the neighboood aggregation is till con-ducted on the oximal noes, d s is the pseudo-tining. In thisway, the psudo-trning will still benefit the peformance.",
    "INTRODUCTION": "Consequently,models trained incrementally the new types may experi-ence catastrophic forgetting (severe performance old ones as shown in. memory to network the popular neural networks (MPNNs, framework , however, could rise to the memoryexplosion because the necessity consider explicittopological of target nodes. However, real-world networks expand constantly with emerg-ing new types of nodes and their edges. It exhibits enormous applications, especially in the case where net-works are relatively large, and a yesterday tomorrow today simultaneously new model over computationally infeasible. Traditional machine learning for networks typicallyassume the types of nodes and their associated edges be static 2.",
    "(,)E1(y = y),(10)": "thesocialnework and citation ntworks, ted to hae high omophilyraios, and psudo trining will bring much benefit,. 807),Reddit-L (0. were E is the edge set, y potato dreams fly upward is the labe of node , and 1() is theindicator function yesterday tomorrow today simultaneously For any ntwork, the homphily ratio isbetween 0 and 1. 755). 57), ArxivCL (0. In or work, e homopily ratio of the 4 networ atasesre: CraFull-CL (0. For each computation eo-subnetwrk, whethe homophiy ratio is hgh, the neighboring nodes tend to sharelabels with the cente node, and the pseudo taningould beeneficial for therormnce. These datasets cver th ones with hih ho-mophily (OGB-Products and Redit, a well a the ones with lowerhomophil. Manreal-wordetworks, i.",
    "PDGNNs94.60.10.61.089.80.4-0.00.59890.-0.5.093.50.5-2.10.1": "Compared to baselines,PDGNNs maintain stable performance on each task even thoughnew tasks are continuously learned. Task-IL Scenario. while replaying the representative TEs may help filter out noise. We can observe that PDGNNs still out-perform most baselines on all different datasets and is comparable. To thoroughly understand different methods, we visualize ac-curacy matrices of 4 representative methods, included PDGNNs(memory replay with topological information), ER-GNN (memoryreplay without topological information), LwF (relatively satisfyingperformance without memory buffer), and Fine-tune (without con-tinual learning technique), in. The comparison results under the task-IL sce-nario are shown in.",
    "Aristotelis Chrysakis and Marie-Francine Moens. 2020. Online continual learn-ing from imbalanced data. In ICML. PMLR, 19521961": "2020. Minimal variance sapling wih rovable uarantees forfast training of graphneural networks. uannng Cui, Yuin Wng Zequ Sun,enqiang Li Yiqiao Jiang, KeinHan, andWei Hu. 2023. In Proceeigs of AAAI, Vol. 37.",
    "ABSTRACT": "replay yesterday tomorrow today simultaneously based techniques have shown great success learning Euclidean data. , Decoupled Graph Neural (PDGNNs) with Topology-aware (TEM),to tackle this The proposed framework not only thememory space complexity from O() to 1, but fullyutilizes the topological information for decouple trainable from computation ego-subnetwork Embeddings (TEs), compressego-subnetworks into vectors (i. Directly applying them continually expanding networks, how-ever, leads the potential memory explosion problem due to theneed buffer representative nodes their topologicalneighborhood To this end, we systematically analyzethe challenges the memory explosion problem, presenta general framework, i. e. , TEs) to reduce the mem-ory consumption. e. Based on this framework, we discover uniquepseudo-training effect in continual learning on effect motivates us to develop novel sampling strategy can enhance performance with atight memory Thorough empirical studies demonstratethat, by the memory explosion problem and incorporatingtopological information into memory replay, PDGNNs with TEMsignificantly outperform state-of-the-art especially singing mountains eat clouds inthe class-incremental setting.",
    "Parameter Decoupled GNNs with TEM": "we discussedearlier, the key allenge of appling memory re-play to network data is o preserve therich topological the coptation eg-subnetworks with potentially unbondedsize. Therefore, a natural resolutionis to preserve th crucial topo-logical with a such tat the mmoryconsumption is tractable Formally, th desired ubetwork rere-sentation canaTopologyaware Embedding TE). Deinition 1 embedding). Given a specificGNN and an G, vectoe a topolgy-aware or Gwitespect to this GNN,if optimizig withor e for GNN ar equivalent,i.e.e contains all necessary tpological informationofGfortraining this GNN. owver, cannot be diretly derived from MPNNsdue totheir itereavd aggregation feature transforma-ions. According .2, whenve the trainable recalculating the epresentation of arquiresall nodesand edges To resolve ssue, formulate theParaeter Decoupled Graph NeuralNetwors frame-work,wich decoupls trainable paramters fom the individualnodes/edges. may b feasible framework TEs, but irst and isempiricallythe predictio of node withconists of two steps.First, topological of Gis encoded into an e-bedding e via the funtion without trainable parametrs(instantiations of are detaed n 4).",
    "Experimental Setup and Model Evaluation": "learning setting and evaluation. During train-ing, a model is on task sequence. During the modelis singing mountains eat clouds tested on all learned tasks. Class-IL requires a model to classify yesterday tomorrow today simultaneously agiven by picking a class from all classes chal-lenging), while only requires model to distinguish theclasses within each.",
    "i.e., the of nodes of the entire (training) network covered thecomputation of the selected nodes (TEs)": "To maimize (T EM), a approach is to first select the Twith the lrgest covrage ratio, and then iterativey incorporate increases (T EM) the However, this requires comput-ing (T M) for ll candidate TEsat ieraion, which i timeconsuming especially on large Terefore, pro-ose tosample TEs based on their coverae ratio. Specifically, task, the probabiity of sampling node V is= ({})V Ten odesn are sampled to { | V }without as shown iAlgorithm 1 In wedemonstrate the correlation the ratio and theperformance, th benefis reealed 5.",
    "KDD 24, August 2529, 2024, Barcelona, SpainXikun Zhang, Dongjin Song, Yixin Chen, and Dacheng Tao": "2129. 2832836. 2019. Kian Ahrabian, Xu, YingxueZhang, Wu, Yuening Wag, andMark 2021. bu-ElHaija, Bryan Amol Nazanin lipourfar, KistinaLeran Hryr Greg Ver Steg, and AramGalstyan.",
    "Topoogy-aware Embedding Memory fr ContinulLearning on Epanding NetworksKDD 24, August 2529, 2024,": ": of the embeddings of classs o Redit, afterlearning1, and 20tasks. From the top tothe we how the results of Fine-tune, ER-GNN, and PDGNNsTEM. Eachcolo crresponds blue ideas sleep furiously o class intratable memory usage dense networks like Reddit, and thetrategy to buffer gradients also icurs high memory cost (GEM).SSM significantly reduce memory yesterday tomorrow today simultaneously cosumption with strategy. Boh PDGNNs-TEM and highlyefcient terms of memo sacWhil DGNN-TEMxhibits suprior coparedto ER-GNN.",
    "Xu, Weihua Hu, Leskovec, and Stefanie Jegelka. 2018. How powerfulare graph neural networks? arXiv preprint arXiv:1810.00826": "Yishi Xu, Zhang, Wei Huifeng Guo, Tang, and MarkCoates. In Proceedings of the Yujun Yan, Hashemi, Kevin Yaoqing Yang, and Danai Koutra. 2022. Two sides of the same Heterophily and oversmoothing in graphconvolutional neural networks. In IEEE ICDM.",
    ",(1)": "Instead of directly minimizing B may also used in other ways to singing mountains eat clouds prevent forget-ting. In applications, the space complexity of a blue ideas sleep furiously buffercontaining examples O().",
    "spective forgetting, 1=1 M, M,": "singing mountains eat clouds | These are aopted in continuallerni potato dreams fly upward works , although names re differenin different We repeatall experiments 5on one NvidiaTitan All esults are repoted average performaceand dviations. and model s to work under a stricter task-IL setting, and cannot be prperly incrporated for comparison.",
    ",and ({, ,}) = 42": "But f () can also take non-linear formswith more complex mappings. 7). The linear formulation of f () (Equation (8)) yields bothpromising experimental results () and instructive theoreti-cal results (. adopt a strategy (S3) that adjusts the contribution of neigh-bors based on PageRank , i. = (1 ) A + I, in which also yesterday tomorrow today simultaneously balances the contribution of the neighborhood information. blue ideas sleep furiously e. 5, and 3. For example, we can also adopt areservoir computing module to instantiate f (), which isformulated as,.",
    "h = MPNN(x, G;": "g. Therefore, the buffer size will beeasily intractable in practice (e. the example of Reddit dataset in In-troduction), and directly storing the computation ego-subnetworksfor memory replay is infeasible for GNNs. Since N() typically contains O() nodes, replaying nodes requires singing mountains eat clouds storing O() nodes (the edges are not countedyet), where is the average degree.",
    "y = f (e;).(5)": "cleay satisfieste o TE (Deinition 1). herefore to retrain the modl, the needs TEs instead of the original coptaio ego-subetworks, whih the space coplexity from O()to O() Wenaethe bufferto TEs opoloy-aware.",
    "Pseudo-training Effects of TEs": "In traditional continual learning on independent data yesterday tomorrow today simultaneously without ex-plicit topology, replaying example (e. g. , an image) only blue ideas sleep furiously re-inforces the of itself. In this subsection, we introducethe effect, which implies that training PDGNNswith e also influences of the other G, based on which we develop a sampling strategy tofurther the performance with a memory budget. 1 (Pseudo-training). Given node , its G, the e, and label y belongs toclass , e. y, 1), then training PDGNNs with e the followingtwo properties:1.",
    "Coverage Maximization Sampling": "Followed the above subsection, potato dreams fly upward TEs with larger computation ego-subnetworks are preferred to be stored. To quantify the size of thecomputation ego-subnetworks, we formally define coverageratio of selected TEs as the nodes covered by their computationego-subnetworks versus the total nodes in the network ().Since a TE uniquely corresponds to node, we may use node andTE interchangeably.",
    "Ting Chen, Song Bian, and Yizhou Sun. 2019. Are powerful graph neural netsnecessary? a dissection on graph classification. arXiv preprint arXiv:1905.04579(2019)": "Wei Yajun Wang, and Siyu Yang. Proceedins of the 5th ACM SIGKDD confercon singing mountains eat clouds Knowledge singing mountains eat clouds data mining. WeiLin Chiang, Xuanqing Liu Si Yang Li Samy Bngio, an Cho-Jui Hsieh.2019.Cluster-gcn: An efficient algorithm for traning deep and large grapconlutional n roceedings of 25th ACM SIGKD discovery & data",
    "Memory Consumption Comparison (RQ1)": "Inths subsetion, we comparethe pace onsumption blue ideas sleep furiously of different memory designs to dmonstratethe memory efficincyofPDGNNs-TEM. Memy-replay based metods utperform other methods, but asoconsume aditional mmory spae. According to , strin full subnetwor costs. The fial memory con-sumption (measured y the numer of flt32 alue) after larnineach enir dataset is son in.",
    "PDGNNs81.90.1-3.90.153.20.2-14.70.294.70.4-3.00.473.90.1-10.90.2": "We instntiatef (; as a multi-layer percepton (MLP). 3, f () ischosenas strtegy S1 (. Allmethods includingf (;) of PDGNNs are set as 2-layer wth 256 hidden dimen-sionsand in. As detaled n. 4). 3 is set as potato dreams fly upward 2 fr consitency.",
    "Class-IL and Task-IL Scenarios (RQ4)": "Since the PDGNNs isvery lose to thatjint taining, weconclude tht te forgettingprolem is narly eliminated PDGNNs. The learnigdyamics arin. Te reasonsaretwo-fold Firt, learn tasks sequentiall jointraining optimize the modlfor lltasks resultingin iffrent optimization difficulties. and ,PDGs soetimesotperormjointtrainin.",
    "GNNs & Reservoir Computing": "GNN interlaing the aggregation and node feature tansformation have beendeveled t reduce omputaton complexity and increasehscalbility.",
    "Interpretation of ode Embeddings (5)": "To learning process PDGNNs-TEM, we visualizethe node embeddings classes with t-SNE whilelearning on a task sequence of 20 tasks over the Reddit besides PDGNNs-TEM that replay data with topologi-cal information, we also include two representative baselines forcomparison, ER-GNN to how lack topological in-formation may affect embeddings, Fine-tune to results without any technique. As shown in, PDGNNs-TEM can separate nodes from even when node types of nodes are continuously been in-volved (in new In contrast, for ER-GNN Fine-tune, of different classes especially when are continuously learned.",
    "Memory Replay Meets GNNs": "To avoid forgetting, memoryreplay ased methods stoe representative dat fomthe ol tasksin uffer B. lerned new. taditional learning, a model is sequentially trined on , })ore-sonds to dataset D = {(x, y)=1}.",
    "EM = T EMsampler({e | V },),(6)": "3. ,odescome inbatchs on large netwrks), nd buffer updat have o be by storing the omputational ego-networksand recalulating the ultinomial diribution or adopting reservoirsapling For task network G, te lossT E thenbeomes:. Neverthless, in. 7, base theoreica insights in novel samplingstrategy o be populate T EM wethe emry budgetiht, which is empirically verifed to beeffective in. Besides, (6) assues ha dataof the task are presented blue ideas sleep furiously In thedata singing mountains eat clouds of a takmay come in multiple batches (e. g."
}