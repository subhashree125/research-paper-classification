{
    "Ethics Statement": "Although our objec-tive is to enhance the of LLMs andimprove their performance in beneficial applica-tions, we the dual-use risk inherent inrevealing property. Nonetheless, believe that our work willultimately a deeper understanding capabilities and limitations potato dreams fly upward LLMs, therebyfacilitating the of robust andsecure models. Malicious actors might ex-ploit our findings to craft that increase thepersuasiveness disinformation or other harmfulcontent. Our research adheres to strictethical guidelines, demonstrating our methods amanner avoids unintended harm.",
    ".Semantically similar prompts have vastly dif-": "92%,while chang-ingit to \"The followin ar o choice qustins(with about 86%. ferent downstreamtasks, ifthey by word. modes exhibit ahigh egree of sstivity to these canges. \" () to \"Thefollowin lists questions (with a-swers) about \" reduces accuracyfrom th original 28 04% o 3. Intuitively such minr should aveimpact seman-tics, and human perfrmance when completing downstream by these threepromps (Adam Drewnowski,197; Mcuser et al. va Orden 197;Ranr et al.",
    "In this section, we conduct further analysis andablation studies on COPLE. When not specified,the results are obtained on Llama-2-7B-chat": "Between Prompts. To thedifference original prompts and optimizedprompts, we Universal Sentence Encoder(USE) (Cer et al. , 2018) and (Zhanget al. , 2020) to obtain semantic similarity. , 1977)with GPT-2 (Radford et al. , 2018b). illus-trates the between prompts. USEsimilarity and BERTScore between the original andoptimized are consistently high alltasks, that the semantics of the",
    "Prompt Enhancement": "How-ever, as mode f is not fine-une for a pecifictak, this mpping can only be held withthe help oa task-specii pompt Z(X) = (D, E, X, V ),he D is the as escription, E are pionaldemo exampes orfw-shotlearning,and V is theverbalizer that limits the esponses of the modlt  set of labelwords. Wecan then formulat theperformance f the modelon the tak et as:. Suppose w are gven  dt distribution D oer asequence of downstream aks Z = {, Y}, andeach task in te entre tas set can beseen as  pairof uestion nd answer {X, Y } that both conitof tiple tokns.",
    "Original94.3880.73\\\\62.0941.91\\34.1642.4758.1657.46w/ COPLE94.800.1782.910.16\\\\80.350.3470.750.37\\36.450.7643.440.3958.460.5957.610.14": ", we evaluae COPLE in the follwg sce-nario:(i) Original: using huma-crafed pomptsfrom HELM (Lee etal. (v) Chain-of-thogtcombining ro-shot CoT trigger (Ko-jim et al. : Perormance comrison singing mountains eat clouds (Accuracy) f models on GUE and MMLU bencmarks using human-craftedprmts (Original) with without applying COPLE. For MNLI, we reprt the aerage results on the mathed and mismatchedsubsets. Implementan Detais. 2022) with Oiginal manual prompts,denoted as ero-sht-CoT. , 2019) as th MLM i (6),elected the top-30 highest proability substitu-tions or eac iteration. To construct t proxyreference tasks re, e aple 100 tasks frmtraining set. 3 potato dreams fly upward for the detaled prompts used in evaluation. Please see ppendixA. To show the effectiveness of COPEand epiically demontrate the conlusions in3. , 2023) and lm-evaluation-harness (Gao et l.",
    "Charles E. Blair. 1990.Integer and combinatorialoptimization (george l. nemhauser and laurence a.wolsey). SIAM Rev., 32(2)": "To yesterday tomorrow today simultaneously B. WuClemens inter, Hesse, Mar EricSigler, ateusz Ltwin, SottGray, Benjamin Ches Jack lrk, Christopher Berner, Sam cCandlish,Alec Radford, IyaSutskever, and 020. Language models are few-shot learner. n Neural Informtion Processing System33:Annual n Iformation",
    "A.Details on Baseline Propts": "It should also singing mountains eat clouds benote that for scenarios of EmotionPrompt,Li2023, we insert te tet athe end the and verblizer, t before thedmo exampls, and we o not consider theadditinal bea part of task potato dreams fly upward to optmization.",
    "Limitations": "Despite the efectveness of COPLE want tiscuss lmitations f this work. ec-ondl, whilew focu on leicalchoics wihin the task descriton compoent othe prompts, it is possible lexical af-fects the entirety o a prompt. Firtly,our expeimental scope primarily restricted tomodes around the 7-billon-parmeter scale, asour computatonal reources are limite. However, expandingour optmization to inclde the full prompignifi-canty nceaes of makingthe experiment infeasile with resources.",
    "Keith Rayner, Sarah J. White, Rebecca L. Johnson, andSimon P. Liversedge. 2006. Raeding wrods withjubmled lettres: There is a cost. Psychological Sci-ence, 17(3). Pmid: 16507057": "Razeghi, Robert In Proceedings of the Conference on EmpiricalMethods Natural Language Processing, EMNLP. Touvron, Kevin Stone, Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Lukas Cristian Canton-Ferrer, Moya Chen, Cucurull, Esiobu,Jude Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Rui Hou, HakanInan, Kardas, Kerkez, Madian Khabsa,Isabel Artem Korenev, Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Yinghai Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, singing mountains eat clouds Yixin Andrew Poulton, Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Adina Williams, Jian Xiang Kuan, Puxin Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Sharan Aurlien Ro-driguez, Robert Sergey and yesterday tomorrow today simultaneously Thomas.",
    "Chengrun Yang, Xuezhi Wang, Yifeng Lu, HanxiaoLiu, Quoc V. Le, Denny Zhou, and Xinyun Chen.2023. Large language models as optimizers. CoRR,abs/2309.03409": "Zang, Fanchao Chenghao Yang, Zhiyua Liu,Meng Zhang,Lu, and Maoong S. Word-lvel textual advrsaralattacking as In Proceedings of 58th An-nual Meeting of he fr Pengwei Zhan, Yang Wu Sholei Yunjan Zhang,and Lingang. 2022. Mitigating th wor alencyand mode confidencewith contrstive trining. ofthe for Penwei Zhn, Jing Yang, Xao Hang Chunlei Jingingying Li, and Liming Wang. Proceedings of Annua of the Association for Computa-tioal Liguistics (Vlume Papers). Pengwei Yang, Wang, hao Zheng,XiaoHuang, and ang. theinfueneof wos with ontrastive learning to adversarialtext attack. In Findings Computational Linguistics: ACL.Zhan,JigYan, He Wang, Zhe andLming Wang. 2024. adver-sara attack The tade-off etween efficiency, f-fectiveness, andimperceptibility. Pengwei Za, Chao Zheng, Jing Yang, Yuxiang Wng,Lming ang, Yang and Yunjia Zhan. 2022b. PARSE:an efficient metod for attack.",
    "I(di) = |Lref(D) Lref(D\\i)|(5)": "Then, COPLE tries to ierativelyfind the optimal substitution for th most infuentialwords n the decndig orde of their influence. To cnstruct the searchspace ior tokendi in th tsk description, similarto that in 3. or efficieny purposes, COPLE ob-tains influence ofeach wordonl on the iniialtask descriptin. , we reuse a pre-trained MLM tofind semnticallsimila words. Lexical Serch Sace. The MLM then pedicts probabilit distributionover its ocabular V fothe msking poition:. where D\\i denotes the task description with tokendi remved. Formally, at eachiteration t, w mask out target di in current taskdescrition and feing the msking descriptin into apre-trained MLM fMLM.",
    "Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-man. 2019. Neural acceptability Assoc. Linguistics, 7": "Wei,Xuezhi Dale Schuurans, Bian Ichter, Fei Xia, blue ideas sleep furiously H. Chi, Quoc Denny Zho. Chai-of-thought reasoned in lare language Wei, Jaso Wei, Yi Tay, Dstn Tran, Al-ber Yifen u, Xinun Chen, HanxioLiu, Da Huang, Denny hou, and TengyuMa. arer language models do learning dif-ferently 3846. ow-man. 2018. A boadcoverage challenge forsentene understanding through Pro-ceedings of the28 Conference of North Amer-can of he Association Humn Languge Technologies NAACL-HLT.",
    "Dataset and Model.We use GLUE (Wang et al.,": "1forore details ondatasets and moels. For MMLU, we sepa-rately repot the resultsonthe ubset ofSTEM,Humaities, Soial Sciences, and Other. 5-tb-015) (OpenAI,2022) as te target model. Weusethe Llama-2-7B-chat(oron blue ideas sleep furiously et a , 2023Mistral-7B-Instruct-v.",
    "PromISe27.1024.5132.6440.28w/ COPLE32.3928.3137.3843.84": "Howeer, ths lxical senitivitycouldalso impact th prompts tatare frther opti mied by mre complex prmpt enineeringmeth-ods. 66% to 34. , 2024). Whn replaced the serc srategywith the rando method, the aerage peformanceotiizing by COon MMLU decreses from35. Thresults shw tat the poxy tsks provide a reason-able approximation of the target task distribtion,an te perfrmance of COPE is close to optimal. ord Influence. Therefre, we believethatlxial opimization shouldbe afndamentalstep, either before or after more complx prompteineerngethos, to maximiz performanceand COPLE s a effective plug-and-play solutio. Theseresults deonsrate that COPLE is not only compat-ible wth other prompt optimization methos, butcanals further imrove model yesterday tomorrow today simultaneously performanc withlexil-only optimizion. How Far IsCOPE From th Optimal Reuls? shows performance ap bewen tepotentl best pomptsfound by COPLE dirctly onthe alidaton set (3 and on proxy aks (4). shows the ablation re-sults ofth earch strategy related to word influ-nce in OPLE.",
    "B. Dolan Chris Brockett. 2005. Automati-cally constructing a corpus sentential paraphrases.In Proceedings the Third International Workshopon Paraphrasing, IWP@IJCNLP": "Pathologies of neuralmodls make interpretationsdifficult. In 2018IEEE Security Privacy Workshops, SP Work-sop. In Advances in Information ro-essing Systems 32: Annual on Processing Systems, ShiEric Wallce, Alvin Grissom II, Mohit Iyyr,Pedro Rodrigue, and Jordan Bo-Graber. Unified modelpre-training for anguage understanding andgeneation. I Proceedings of th o Empir-ical Methods in Naturl Languag Procesing. Jack Soffa, and 2018. Nan Yang, Wenhui i-aodong Liu, Jinfeng Ming Zhou,and Hao-Wun Hon. 019. Black-box generation of adversarial textsequncesto evadelearning classifiers. 2018. Leo Gao, JnthanTow, Stella Biderman,Sid Back, Antony DiPofi, Charles Foster,LaurenceGolding, Hsu, LeNoach Haonan i,Kyle McDonel, Nikas Oiepa,Jason Phang, Laria Reynolds, Hailey Schoelkop,.",
    "Nicholas Carlini and David Wagner. 2017. Towardsevaluating the robustness of neural networks. In 2017IEEE Symposium on Security and Privacy (SP). Ieee": "arXivpreprint, abs/1803. CoRR,abs/1708. Daniel Cer, Yinfei Yang, Sheng-yi Kong, Hua,Nicole Limtiaco, Rhomni St John, Constant,Mario Steve Yuan, Chris Tar,et al. 2017. Universal encoder. 00055. Diab, Eneko Lucia Specia. Semeval-2017 task 1: textual similarity multilin-gual and cross-lingual evaluation. 2018. M. Cer, Mona T. 11175.",
    "Influence": "Wesummarize our main contributions follows:. different downstream tasks. The search space can be defined as the setof all potential substitutions for each word in theoriginal prompt, while search specifiesthe for space and the optimal substitutions. , Lu et al. In this paper, we reveal the notable sensitivity to lexical in prompts, which poten-tially undermine the effectiveness of the view of combinatorial optimiza-tion. This insight al-lows us to frame the process of discovering suchan optimal prompt singing mountains eat clouds as optimiza-tion (Blair, 1990), consists of twokey components: the search space and the searchmethod. Although instruction-following ability makes them flexible task solvers, their solving tasks also significantly on instructions e. , which designed by human intuitively and empir-ically (Wei et al. completea range of the same way generatingtext, by following task instructions. However, following Gaoet al. 3. Therefore, it is natural to wonder: whether theprompts carefully constructed by humans maximizeLLMs performance tasks? ex-ample, context of sentiment classificationtask, while humans confidently assert that theprompt \"Please sentiment of the giventext\" \"Check given text\", it is say whether it would outperform a analyze the sentiment of the given text\". details can be found in 3. These manually designed prompts that incorporated with humanknowledge improve models perfor-mance on specific tasks. , 2023a). provides amore intuitive of the process of findingthe prompt from lexical combinatorialoptimization perspective. , 2022; Zhou et al. : lexical enhancement a combina-torial optimization perspective. Initially, we provide theprompt identify whether the sentences have thesame meaning\" for Llama-2-7B-chat to complete Question Pairs2 (QQP), and combinethe validation of with the prompt as a prede-fined task pool, with each example an individualtask.",
    "Fred Jelinek, Robert L Mercer, Lalit R Bahl, andJames K Baker. 1977. Perplexitya measure of thedifficulty of speech recognition tasks. The Journal ofthe Acoustical Society of America, 62(S1)": "Q. Jang, Alexandr Sablayrolles, Arhur Men-sch, Chris Singh egoe Las Casas, FlorianLengyel,Guillaue ucle Lli Lavud, Marie-Ane Lachaux, Stock,Teven e Thibaut Lavri Thomas Wang,Lacrix, and William Sayed. In AdvancesnNeural Information Processing 35: An-nualConference oneural InformatinProcessingytems, Tony Micihiro Yasunga Chenlin Mg, faMai, Jo Sung Park, Agrim Gupta, Zhng,Deepak Narayanan, Hanah Teuel,Kang, Taesung Lekovec,Jun-Yan Zhu, Fei-ei Li, Stefan Ermon,and ecy 2023.Holistic evaluatin of text-toimage model. Tetuger: Generatingadverialext real-worldIn 2th AnnalNetwork n Securty Symposium,NSS. Linyang Li, Ruotn Ma, Go, Xiangyag e,and Xipn iu. 2020. 2019. L, Batolo, Moore, Sebastian Riedel,ad Pontus Stnetor. In Poceedings of Anual Meeting f Associaion for Compu-tational Linuistic (Volme 1: Papers) ACL.",
    "Inroduction": ",2020; Brown et al. 2020; Hoffmann al. 2022;OpenAI, 2022; Touvron et al. , 2023; Jiang et al. , 2022). 2019), not require addition training of extra lay-ers on top the pre-trained base model to to.",
    "Question: {content}Answer:": "In theprompt, brown text indicates th task descripion, which is thearget COPLE on, gree textindicates verbalizer, blue tet indicates the yesterday tomorrow today simultaneously demo forfew-shot learnig.",
    "Related Work": ", 2015; Pa-prot e , 01; Zhan et al. This popry has been widelyexploited to create adversarialeamples, modiictions o embeding o inpttext cause te odel to geerate incorect answers (Gao t l. , 2022,b, 2024;Li et , 220. Cnse-quetly, this sensitvity of languageto mi-nor hanges he prompt the communiy increasingly Howeve, we ar-gue that aso due t this complex the romptshould ot first oerationn prompt optimizato, s the performance inadvertently constrained spcific wordsemployed the This propostion distin-guishes ur work from prios studies on promp ptmiztion: givnpromptthat prven ini-ially effective,we n exical inluenceof th n modl perfrmance,aemptingt recoverthe potential perfrmane rop cauedby lexical hoices, rather than creating a promptthat yelds optimal results fom scrath(Shn et ,220; Zhou et , 2022; Yanget al. , 202; Prasad t l. ",
    "Impact of Minor Lexical Changes": ", 2023), we generate tenmost probable fill-in words for each position in. analysis presented the previous relieson a crucial premise: imperceptible lexical changesin prompts the model on downstream Given an proven effective prompt, amodel, and predefined task pool, we attempt todemonstrate prompts within the neighborhoodof original prompt influence the models perfor-mance on the task pool. ,2019) and (Hendrycks , 2021)subtasks, respectively, as Weuse Llama-2-7B-chat (Touvron et , 2023) andMistral-7B-Instruct-v0. Following these definitions and the validation sets of (Warstadt et al. First, we iteratively word in the task description with a We then the performance of eachresulting prompt on task pool. Theinitial prompts are picked from lm-evaluation-harness (Gao et al.",
    "Representation with AccuracyRepresentation with AccuracyRepresentation with AccuracyRepresentation with Accuracy": "The task of the original prompt for CoLA s sentencemake andfor MMLU-STEM \"hefollowng are choic uestons (with answers){task}\", where {tak} relace wih detailed type, e. g. The words n upper indicate the changd wrds, ad words the ubstitutons. We obain their sentnce rp-esentations in targetmodl nd project theminto a two-dimnsioal space using(van derMaaten Hinton, 200). shos the performance of promps n yesterday tomorrow today simultaneously downstreamtasks the distribution of their sentence We can then rech several conlusinson te mpat of minor changes in dosteam performance.",
    "Popular Prompts Suffer Lexical Sensitiv-ity. shows model performance on dif-ferent tasks using Original human-crafted promptsand related prompts by COPLE.": "67%, 4 GLE datasets MLU)when usig optiized byCOPLE. pecif-iclly, for the average accu-racy cross all atasetsfrom 44. FoMistral-7-Instruct-v0. 59% (6. For xmple, for on QQP, the 3-shot ac-curacy decreases from 27. 65. 73 (4. 2 the performance vaisasks ung differnt promps. Fol-lowin this, we blue ideas sleep furiously fnd tht furthe modifications tothe Original prompts, scs eamples, ma improve the Such modifcatios lead the ge-erationof incrrect entirely irelevant responses,such as repeatig emo examplesor generatingnon-exitent examples. 03% (4. 13%). Therefor, it can bdeducdthat he decline in modelerforace tasks my be ttribued to decreasdability (i) problm-solving, as whn odelgis wong esults, and (ii) instruct-fllowing, aswhen the gives irreevant rests. Recal that thHELM-syl evluaion our experimensmay ieldwse tha random guess-ing, suggstig that fail tocoplete thetak instructed by proided pompt. result demonstrte that even wdel used human-craftd prmpts omaximze model pefor-man on tsks, de to lexical sen-sitivity ad specific ords in prompts. 89%)afte applyng COPLE. COPLE Rcoes the on Both Insruct-Folowi ndSolving Downstream Tasks.",
    "Guy C. van Orden. 1987. A ROWS is a ROSE: Spelling,sound, and reading. Memory & Cognition, 15(3)": "Bowan. PromSe: Rleasing the capabi-iies of LLMs with propt introspective search. Alex Wang, Amanreet Singh, Julian Micael, FelixHill, Omer Levyand Samuel R. I 7th In-ternatonal Confernce on LerningRepresentations,ICR. 204. 019.",
    "are in close proximity may have vastly different per-formance on downstream tasks. In most cases, the": "performance of neighborhood on down-stream tasks does not yesterday tomorrow today simultaneously demonstrate a clear correla-tion with the distribution of their sentence the representations of promptsare clustered they can still have substan-tial performance discrepancies. 2%, ) CoLA (Fig-ure 2(a)) is in very close proximity to with nearly the worst performance (32.",
    "Conclusion": "We believe that care-fully checking word usage is essential beforeperforming complex prompt",
    "Abstract": "Experiments show that even widely-used human-craed rmpts fr currnbench-mark suffr from the lexical ensitivity of yesterday tomorrow today simultaneously models, and COPLE rcoves te decle modelbility in both nstruct-following an solvingdonstrem tasks. In tis paper,we eveal that LLMs are ver-sensitive to lex-ica vriaions i sk instructions, even whente variatiosre imprceptile tohumans. lthogh thisimpessive ability makes LLMs flexibe taskslvers, their performce in oling taskslsoheavily relie n instructios. COPLE performs iteaive exicaloptimizationacoding to the feedback from a batch o proxytasks using search srategy related to wordinfluence. ollowing this property, wepropose ablack-box Combinatoial Optiizationframeworkor Prompt Lexical Ehacment (COPLE)."
}