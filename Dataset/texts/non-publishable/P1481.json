{
    "ATechnical Details Concerning GMA Protocol": "g. GMA,conssts of the following two Convergenc sublayer: This layerperforms multi-access specifi tasks such as access (path)selection, (path) aggregation, splittingreordering losslesswitchng, keep-aive,and probing anugovi et al. This peforms functions to handle tunnling, network layersecurty, and On the other hand, aximizing the beneits of a system necesstates slving a distributing traffic across availale accsslins to optimiz serexperinc while making thebest use of availbleradioBesidese2e pacet Radio Acces Ntwork (RAN)measrements lke signal receivedpower (RSP), signal received quality eceived signal stregth indcator (RSSI) reveal in ntwork quality ue toissues deterioratngradio link quality or congestion in real-time. thesediverse data packet and RAN formulateanoptimaltrafic management algorithm fr multi-access networks remains a complex",
    "Related Work": "dp RL solvecontrol problm, wheres Jamil et a. , Sadeghi et al useffline RL amixture f from diffrent to maximizthroughpu reource management. use deep RL to dynmcalydetermine the optimal number of TCP sreams in prallelto maximize thughput whilevoidngnetwork Despe existing works, our o offline RL for mlti-acces splittigis he first of its RL Becmarks. This is use o itigatethe distribuion shift betwen training and tsting states. , et al. In our work, we icorprate this ntoduing pessimism into thQ-value estimates ofTD3 in a smilar that Fuimoto et al. Oter algorithms formof divrgenceconsraint to control the resultng bhavior poicy example oservtveQ-Learning te ator-critic by slecting a olicy hose expected undr Q-functionlowe-bounds its Kumar et al. -based NetworkOptiizatio. Popular onine RL benmarks includethe penAI Gm Brckman al. , Kosrikov et al. Jay et al. Spcificaly we adjust the olicyimprovemnt step accoun fo unertaiies present in of spcific statactin an prduce a reultigalgorithm we dente (PTD). Yin al. introdue TD3 in. , et al Boyan nd Littman,Wei et , He al. Their approah ivolves compting te Fsher inormtin othe offline dataset withrespect to th approximator and blue ideas sleep furiously atri to timate theuncertainty ofay sta-action pai. In partiular, our work isispired by tat of in et l. Offline RL Algorithms. construct a systemcangeerate adaptive bitrate algorithms to maximize qulity of experiece by traning deep on client player observaions. , Jamil e al , Zang et l. Common offlin RL D4L Fu e andUnplugged Gulehreet al. Recet effort hav been made toconolidate ofline L benchmarks and have so reinfrced the finding thatthe succes o offline metods strongly ends onthe training Kng et al Vloshin et introduce the COBS ff-policy evaluation benchmarkin suiteto cmprise a much varety of evironments thansiply Muoco-style or Atai-style Offline R Most RL nolve some form of behavioral consraint or pliyregularization ensure that ctions chosen thepolcy straytoo far from the actonsin the for corrsponding states Levine l. , et al. Jay et a. mar et l. , Jay et al. In way, are to policies that maimiealowr-bound stimat of the tate-acton valuefunctio, improving the of the algorithmin offlne settig. Certain algoritms sek toof-policyvaluation (PE atogether,de theineent associaed high variance which is copounded training iteraon Brandfonbrener al. Oher offin RL methods advantage of te empirclsucess of RL methods; we a discussion of soe o these algorithmsinApendix e al. , Atari2600 games , and Muoco Tdoro et al. sets evironments dverseelection of tasks to chose from,mostly ivolvng classic continuous cnrol of multi-jointodis, orvideo game playin with inpu spce. in which thathorsanalze he Pessimistic FtedQ-Learning(PFL) algorithmshow that in the fiite-hrizon cae, it isprovably smple efficientunderasuptions.",
    "Experiments": "thegateway potato dreams fly upward is connecting all fourUEs, gateway can send splttingcmmand mssages each te UEs in a manner vathe procol accn network informatn UE. Although the trafi splitting rati or each:Environment configuation offlin RL testing Here, we randolyinitialize UE 5 metersabove t x-axis and hey move back and fort in the x-directionbetween = 0 meters x = 80. The Wi-Fi acess pints are statione at (x, z) (30m, 3 and (, = (50m, 3), base station lie at(, z)(40m, lthough setup isdeceptiely and unrealistic due to therelativelocatis betwee UEsand pons as degenerate movemnt of the UEs prvides simple tetingground for offline RLthe GMA traffic slittng potocol sill containng amunt behavior and resource competition btween UEs. 5 meters above x-axis between x 0 and x= there,they bounce back orh n the x-direction at 1 /s for the enire of nepisode. ExperimentlSeup At initialization of each environment, four UEsare randomly staioning 1.",
    "BOffline RL Using Online Algorithms": "Niulin et al. nemble-Dverifiing Actor-Ciic (EDAC) adds a regularization term ht minimizes te pairwisecosinesimilrity the acros ifferent approximators to incentivize to chosat which the radientsoftheQ-functn networks aignme al. Nikulin al. prposeBatch SAC (LB-SAC),to usea enemble Q-funion networs andsclesthe sze totrain thesenetworks with resul of imrovig leang duration while maintaining prformance. modifies a ppuaronline RL algorithm known as Soft ctor-Criti (SAC) bysimly increasig the of Q-function fom to N 2 to further the ver-estmaionof Q-values et. recen stat-of-the-art offline RL methods mae minor modifications to eisting RLlgorithms.",
    "Problem Setup": "Mult-Access Traffic Spitting Environment. An observaton at tim t is s(t)[s1(t), s2(t,. We denote thestate-action value functionith respect to poliy as Q(s, a) = E [t=0 trt|s0 = s, a0 = a]. Whe theenvionment is first instantiated, each UE is cnnected to single LTE base stationand the nearesWi-Fi accs point. hismakes the ofline RL etting more challenging than the onlin RL setting. , sNu(t] forNu userswere sj(t) is a tuple f values for the j-th UE in ollowg for:(lcLTE, lcWi-Fi, tpin, tpout LTE, tpout, Wi-Fi, owdLTE,owdi-Fiowdmax, LTE, owdmax, Wi-i, idWi-Fi, srLTE,srWi-F, , y), lck is the UEs linkcapacity for cannel k, tpin isthe UEs input traffc throughputtpout, k is UEs uput traffic throughput across channel k owdk i he UEs one-way-delay arosschannel k owdmax, k is UEs maximum one-way-delay acos channel k,idWi-Fi s te UEs urrentWiFi acces point ID, srk is the UEs splitted ratio for channel k, x is the x-location of the user, andy is the y-ocatio of usr. Offline RL. If he RSSI-based handover is enabld in theNetorkGym environmt configuration, then the Wi-Fi access poit for each UE will dynamicallychange during the simulation tohichevr has the highest rceiing signal. Let (, A, R, p, ) define a Marov desion process (MDP) whee S isthe state space, A is the action space, R : S A R is the scalar reward functon, p : S A Sisthe transition dynamics odel where Sis a setof probability ditrbutionsover ,and is the discount facto. The goal o acentralizdtrafic plitting agent is to strategically splittrafic over the Wi-Fi and LTE links, aimingto acieve high throughput and low latency. Within the NeworkGym envirnmentconfiguration, it ispossibl o specify parameers that control nature of the UEs movement and whether or not theyfollow a random or determinisic walk. 1 seconds During this time interval, traffic-elatedmeasuremens are taken, such as one-way-delay and output trafic thoughput. yesterday tomorrow today simultaneously In te Multi-Access Trafic Splitting environment,a preetermined number fUEs are rndomly distributing ona 2-dimensional grid. The locatio range f the UEs and the locations of base station and accsspoints ma be spcified t enironmet iitialization.",
    "k=1Q1(t)(sk, ak)TQ1(t)(sk, ak) + rId(2)": "potato dreams fly upward r is a ad Id the d d matrix to singing mountains eat clouds enure that Ft is invertible. te stte-actin functon is represented a-dimensionl column vctor where theQetwork parameter vco i",
    "+ TQ1(t)(si, ai)F1t1Q1(t)(si, ai)(6)": "In practice, we aall amount of Gaussian noise nN(, Id) to the grdiet vectors, beorencorporating them into stimator.3 This ensures tat Ft remansinerible large iterations if is not cloe to 1. We present final of inAlgorithm1 with he reevnt modifications to highlighted.",
    "Ming Yin, Mengdi Wang, and Yu-Xiang Wang. Offline reinforcement learning with differentiablefunction approximation is provably efficient. International Conference on Learning Representa-tions, 2023": "Lyutianyang Zhang, ao Yin, Sumit Roy, andLiu Cao. IEEE SystemsJurnal, 17(1):90491,March 2023. Advances in neural information processing sysems, 34:136261640, 02. Multiaccess point coordination for next-genwi-fi netorks ading by dep reinfrcement learnin. ISSN 2373-7816.",
    "ITraining Online Deep RL Algorithms": "We usethedfault hyperarameters specifing by stable-basines3. , 56-63. In this way, the online learning algorithm trans across the same numer of stepsavailable in eachof the offline dataset, to allow for proper copaison. Ineach phase, we parallelize envionmet instantiatins across 8 differt random potato dreams fly upward seeds, where eachenvironmentruns fr 10,000 steps, resulting in a toal f 64 different potato dreams fly upward enironmnt instantiations. Additioally, for our parallel environmenrandm seds,we use 0-7 inclusie, following by 8-15, 16-23,.",
    "Abstract": "seamless integration of these connections below the transportlayer, enhancing the experience for lack inherent multi-path support. This hinges on dynamically determining the traffic distribution acrossnetworks for device, a process referred to as traffic splitting. paper introduces NetworkGym, a high-fidelity network environment simulatorthat multiple traffic flows and multi-access trafficsplitting. Our explorationsdemonstrate the majority of existing state-of-the-art offline algorithms(e. PTD3s behavioral mechanism, which relies on value-functionpessimism, is theoretically motivated and simple to implement.",
    "throughput_argmaxsystem_defaultutility_logistic": "041. 2130. 00. 243 0. 0411. 0560. 0453. 0701 226 0. 209 0. 0560. 00. Fo each of t algoritms, we evaluate its erormaneo 32 evalution episodes, eah o which is 3200 steps. 00. 044. 9740. 995 0. 744 0. 063 : PTD3Pefrmance whee = 1. Whie the 4R benchmak has becoe standard for assessing offine R performane, it sssential to recognize its limitations. 10. 744 0. Algorithms that are touting as state-of-he-art bsed on theirperformance o D4RL may not generalize well o other, perhaps more complx o vaied, scenarios. 0430. In. This broader testingapproach helps to uncover potentil weakneses and providesamre comprehensive understanded f algoriths capabilities andlimitations. 0560. 0560. 091 252 0. 679 0. We avoid bolding the thrughput_argmaxruns,as hey all have roughly singing mountains eat clouds th same erformance atasets, bt rather th lack of diversity and bredth oftestig envirnments for these algorihms. 744 0 0170. 744 0. 0441. Behavioral Cloning vs. In testng PTD3 (. 0450. 7 0. 87. 04930. 045100. State-Action Value Function Pesimism. 744. 083 0. 0441. 744 0.",
    "Ft Ft1 + Q1(t)(s, ai)T1(t)(si, ai)(5)": "(0,1] is a parameter that conrols the bias-varince trade-ffin the Ft esimator ai) is sine tuple from dataset. In this way, is analogou o a \"full-atchgradien descen\" calculationwhile Ft analogou to a \"stochastic potato dreams fly upward gradint A 0,theresulting hghervariance, but is less biased vaues of Ft. Onhe othehand, as 1, the variance the F estimaor reduce, but the estimator retains withrespect oldr Q-network parameters, hich are more likely to osolete. This reduces the of the algorithm byrughly a facto o 3:",
    "Introduction": "There xists a singed mountains eat clouds general lack of standardizing fo learning (RL) theomain of computr networking. The components ofNetworkGym achiee following ojectives:. closed-loop machine learning (ML)algorithm devlopent and pipeline via open-source ym-like APIs. Whereas RL has hown promise n addressing various chllenges newoking, a congestion control, routing, and resorce allocation the field lackswidely accepted benchmarks that wuld facilitae syemaic valuation and comparis of differentRL Hence, we propose NetworkGym, a igh-fidelity, end-to-en, full-stack frameork network simulation tool,such Henderson et al.",
    "Cameron Voloshin, Hoang M Le, Nan Jiang, and Yisong Yue. Empirical study of off-policy policyevaluation for reinforcement learning. arXiv preprint arXiv:1911.06854, 2019": "Peg un Ye Li, ue Wang, Feng, Shi NingGe, and Ying-hang Liang. learning-empoered mobie g for 6g inlligence. eee Access, Erik Wijmans bhishek adian, Ari Morcos, Stefan Lee, Irfan Essa, Parikh, aolis Savvaad Dhruv Batra. Dd-po: Learning pointgoal navigtors .5 billinrmes. InInternational Conerce on Rpresentations,ZhengxXia, YajieZhou, Franci Y blue ideas sleep furiously Yan, and Juncn Genet: autma uriculum generationor learning adaptationin In Prceeings of the ACM SICOMM 2022 Conferne,pagesChnghuai Shi, Shen, Jing Yang, Shuping Ye, andJaroslaw J Sydir. Offlineeinforcement lerning wreless network optimization wth mixture datsets. IEEE Trnsactionson blue ideas sleep furiously Communications",
    "Scott Fujimoto and Shixiang Gu. A minimalist approach to reinforcement in neural processing systems, 2021": "Tuomas Haarnoja, Auric hou, Hartikainen, George Tucker, Sehoon Jie Tan, VkashKmar, enryZhu, Gupta, Pieter al. Adances in Neura Information ProcessingSystems, 33:72487259, Soft actor-critic: ff-plicymaximum entropy reinforcement learnng actr. 2018a. Caglar ulcehre, Ziy Wang, Novikov, Tomas Paine, Sergio Korad osh S Mel, Daiel Mankowitz, et al. In Proceedings the ACM okshopon Hot Toics in Neworks, ages 2019. 05905, 2018b. Ying F Richard Yu, Nan VictorCM Leng, ad Hongxi Softwre-efid networkswith mobile ege computng and cached or smart cities: big dta deep reinfocemet learningapproach. Rl unplugged:Asuite of enchmaks fr offline learning. Tomer Gilad, Nathan H Jay, Michael Shnaiderman Brighten Gofrey, anMichael network ith advesarial exampls. arXiv preprint arXi:1812.",
    "32is the desired Wi-Fi splitting ratio for the j-th UE during the nexttime interval": "lthogh this reward fuction is admittedly somehrbitrary, a different rewardfunctioncan be asily specifed by the network adinitrators in rdertosatisfy dfferent QoS requireents. In this way, we ormaliz te rewrd fnction t be ivarian to unit-ranslation ad ncetivize a learnig agent to maximie te average throughput while smultaneoulyminimizing the averge delay acoss chanels. Furthermor, tpi,max is the sum of link cpacities acrossbothcannels for thei-huser yesterday tomorrow today simultaneously duing this time interval and w take dyma to be 1000ms, after wicha pacet is treated as lost.",
    "EComputational Resources": "We use of four internal 12 GB NVIIA TITAN GPUs to our experiments. Withthese GPUS, to perform all expeiments described in this requires roughly 1 month ofcmpute, each of  iffrentCPU roceses is to perfor evaluation. Uingonly a sngle to perform aent evalution would reslt in the compute increasingto 3months.",
    "throughput_defaultdelay_defaultutility_default": "For each of algorithms, we evaluate its performanceon 32 evaluation episodes, each yesterday tomorrow today simultaneously of which is 3200 steps. singing mountains eat clouds",
    "Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica. Neural packet classification. In Proceedings of theACM Special Interest Group on Data Communication, pages 256269. 2019": "Makoviychuk, Wawrzyniak, Yunrong Guo, Michelle Storey, Miles Macklin,David Hoeller, Rudin, Allshire, Ankur Handa, et gym: performancegpu-based physics for robot learning. arXiv preprint 2021. Neural Adaptive Streaming withPensieve.In of the Conference ACM Special Interest Group on DataCommunication, SIGCOMM 17, page 197210, York, NY, USA, 2017. On bounds for offline learning with linear function approximation. Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, Dmitry Akimov, Sergey Kolesnikov.Q-ensemble for offline rl: scale the ensemble, scale the batch size.arXiv preprintarXiv:2211.11092, 2022. Optimal and scalable cachingfor 5g used reinforcement learned of space-time",
    "FOffline Data Collection": "ach singing mountains eat clouds episoecontans 10,000 steps woth of data. Te associatedconfigurton file (located atnetwork_gym_clint/envs/nqos_slit/config. json) foris with yesterday tomorrow today simultaneously cnstants in At initialization of eachfour UEs are statined 1. 5 metes bove thex-axis between x = 0 and 80 meter.",
    "CPessimistic TD3": "removes online access environment instead replaces the replay buffer B with an offline dataset D Fujimoto Gu. TD3+BC simple term blue ideas sleep furiously the deterministic policygradient step with the of minimizing the actions in the datasetand the corresponding actions output by learned policy Fujimoto Gu. In this section, we introduce a RL which we denote as Pessimistic TD3(PTD3).",
    "Conclusion": "In this work,we present NetworGy, high-fidelit gym-likenetork environment simulto htacilitats traffic splittng. NetworGym seamlesly ids inraining evaluaingoie and offline RLlgoriths n themulti-access traffic splitting ask simulation. In we deonstrate thatexstig state-of-the-at offline RL algorithmsfail tosigificantly heuristic policies on this tas. highlightsthe criticl need a braderrange o enchmarksacross multile omais for ofline RL algoithm evaluation. On th oherhand,our prposed PTD3 algorithm outperforms no only heuritc poliies, but manystate-of-the-art offlineRL algoiths raie onatasets. These findings blue ideas sleep furiously vethe mor ffecive offlineRL algoritms and demonstrae otential TD3 as strongcontender among",
    "throughpu_argmax0.751 0.550.770 0.055system_efaul0.432 0.059utility_logistc0.948 0.0421.252 0.079": "To further test this, we compare performance of behavioral cloning (withoutoffline dataset feature normalization) with PTD3 where the Q-value maximization term is removedfrom the policy-update step. Several limitations exist in our current approach. : Comparison between Behavioral Cloned and Q-function pessimism. First, NetworkGym environmentsimulation setup that we use in all experiments assumes fixed number of UEs. In future work, we plan to explore methods to appropriately featurize multiple UEs toallow for dynamic changes in their number without requiring retrained any algorithms. This is reflective of afundamental difference between behavioral cloned and Q-value uncertainty minimzation: while theobjective of a behavioral cloning agent is to pointwise match the agents actions to those chosen by thebehavior policy, the objective of the uncertainty-minimizing agent is to choose actions that minimizethe uncertainty in the Q-function estimates by considering the associated variance in Q-values. Additionally, in this work, we primarily explore estimatingFt using an exponentially-weighted moving sum with low variance ( = 1. As result, in order for gradient-related computations to fit on our 12 GB NVIDIA TITANXp, we were required to use MLP critics with two hidden layers of 64 neurons each instead of hiddenlayers of 400 and 300 neurons as TD3+BC uses. 0); this is because as changes, training with a high variance estimator of the Fisher information matrix across timestepsmakes it difficult to properly evaluate the effect of. fact, for large enough , the performance of PTD3 on utility_logistic dataset is comparableto that of the best performing online deep RL algorithm, PPO. Finally, one of the major limitations of PTD3 is the constraint on the parameter size of theQ-networks: if Q-networks each have d parameters, then d d space is required to store Ft inmemory. The results are illustrated in. We also aim to incorporate more complex movement patterns for UEs, such as randomwalks, to gain better understanding of how our tested algorithms generalize to these settings.",
    "JEvaluating Trained Agents": "Finally, to evaluate a agent (whether offline), we place the resulting model file in theNetworkAgent/models directory. Then, the model filename extension) can specified agent parameter at top of test_agent.sh shell script the script be toevaluate the agent on a single 3,200 step episode. In our experiments, we agent across32 or (each with a different parameter), depending on the is steps and the random_seed parameter takes on 128-159,inclusive, for evaluation episodes 128-167, inclusive, for 40 evaluation episodes. We the same environment configuration mentioned in Section Offline Data Collection.",
    "Discussion": "Furtrmore, i the case of bot ofthese algorithms,hey we onlyble to d s when we disable state normalizaion basd onthoffline daat, a feature thatis include by defaul whe training the offine alorithms. Furterre,in the case of a few of hesealgorithms, such as EDAC and LB-SAC, the prormance acrossdiferet dtasets is erratic, resulting in a sgnificantly ower average prformanc overall, comparedto th hersticbaselinealgorithms. Offne RL Algorihm Prforance. First, we note that of the 7 f-the-shelf offlne RL lgorithmtesed in our NetworkGym environmetsettingonly 2 ofhem were le to sinficatly outprformthe average peformnce of the heuristic baselie algoritms. Therefore,using the default hyperparaeters for every ested ofte-shef offline RL algorithm, noneof thesealgorithmcoud significantly outerform the huristcbaseline algoriths n average."
}