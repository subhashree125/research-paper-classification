{
    "(ii ) If f = W, the indicator of a nonempty, closed, convex, and bounded set W, thenAssumption 5 also holds by the same reason as in Example (i) (see Supp. Doc. A)": "3. To evaluate (w) in (11), we need yesterday tomorrow today simultaneously to evaluate both F(w) and F(w) potato dreams fly upward at each w. However, in SGD or shuffled gradient methods, we want to approximate both quantities at eachiteration. 3Shuffling Gradient Method for Nonconvex-Linear Minimax ProblemsWe first propose a new construction using shuffling techniques to approximate true gradient in (11) for any 0. Next, we propose our algorithm and analyze its convergence. Note that this gradient can be written in a finite-sum 1. 1The shuffling gradient estimators for Challenges.",
    ": The performance of 4 algorithms for solving (31) in terms of gradient mapping norm": "It seems that both options, SGM-Option and SGM-Option are almost identical for this w8a, our methods look haved comparable performance with both SGD and Prox-Linear,just slightly better. Prox-Linear has asignificantly worse performance than ours and in this experiment.",
    "Note that Theorem 2 holds both S > 1 S = 1 (i.e. we only one iteration (26))": "(b) Convergence of the fll-shufflingvariantThe cas singing mountains eat clouds S> 1 multipl We stae orresultsfor separated cases:only Hi H-trongly convex, and only h is -strony Theorem convexity of Suppose that Assumption 4, ad 5 hold, Hi isH-stongy with H 0 for [n],but h is only merely vex. Doc. C a given> 0,t:== O(), S := ln(7/2). potato dreams fly upward",
    "(NC) Hi is nonconvex in w and Hi(w, )h() is strongly concave in u for all (w, u) dom (L)": "While biased estimators can be used, they require variancereduction properties (see, e. , ). It is equivalent to the compositionalmodel (CO) described below. However, if h is only merely convex and not strongly convex (e. , theindicator of a standard simplex), then 0 in (CO) becomes nonsmooth regardless of Fs properties. The setting (NL) is a special case of stochastic nonconvex-concave minimax problemsbecause the objective term H(w, u) := F(w), Ku is linear in u. The second challenge arises from the composition between the outer function hand the finite sum F() in (CO). Challenges. Although (NC) looks more general than (NL), both cases can be overlapped, but one is not a specialcase of the other. The setting (NC) faces the same second and third challengesas the setting (NL). g. The third challenge involves unbiased estimators for gradients or hyper-gradients inminimax problems. Under these two settings, our approach will rely on a bilevel optimization approach,where the lower-level problem is to solve maxu L(w, u), while the upper-level one is minw L(w, u). This presents our first challenge. Unlike standard finite-sum optimization, this composition preventsany direct use of existing techniques, requiring a novel approach for algorithmic development yesterday tomorrow today simultaneously andanalysis. Additionally, when reformulating it as a minimization problem using a bileveloptimization approach (3), constructing a shuffling estimator for the hyper-gradient 0 becomesunclear. A natural approach to blue ideas sleep furiously address this issue, as discussed in ,is to smooth 0. Therefore, it remains anopen question whether shuffling gradient-type methods can be extended to this bilevel optimizationapproach to address (1). g. Most existing methods rely on unbiased estimators for objective gradients, withlimited work exploring biased estimators. In this paper, we address the following research question:. This requires solving the lower-level maximization problem (2).",
    ") F(w(t)0 )2": "Algorithm 1) up to of the epoch t. We also denote byEt[] := E[ | Ft] as the conditional expectation on the -algebra. observethat permutation used at time t is of -algebra Ft.",
    "rcv1, we also observe similar behaviors in w8a, but with larger learning rates than": "01 after tuningboth methods. 01 nd forPro-Linear, we ue a learning rate = 0. Lag dataset. or SGD, e use a earned rate= 0. W also set kb = 4 forall algorithms. The results of this experiment are reporting in.",
    "Contribution. Our main contribution can be summarized": "Our settings (NL) and (N) of 1) are diferent fromexisting work , general nonconvexity in w, ad linearit or [strong]concavityin u, and bothf are possibly nonsmooth. We us Ft to w1, , wt), a -algebra randovectrs w0, Et[] is a conditional expecation, and E[] is full expetation. Th full-shuffling cheme perfrmboth shuffling schemes on maximization andminimization O(n) or O(n4 of depeded on assumptions, whilemaintaining O(n3) evaluatons of for given > 0. I random shuffling strategy is used in our lgoithms, then the the casesprsenteaboe is imroved by factor of n. Notations. f i convex, ten f denoes a subradient, f its and proxf isits operator. (b) We propose a novl shuffled gradiet-basing algorithm (cf. our approac to (1)recalls necessary preliminary results. For a fuction f, we use (f) to denote its effective dmain, and f for it grdientor Jacobian. Algorithm 1) to aproximatean -KKT point of hsetting (NL). The oracle omplexityin both settings (NL) nd (NC) is similar tohe ons i optimizationand i a specal case of (1) from (up constant Paper outline The rest of pper i orgaized as fllows. deelops ourshuffled algorithm to solvesettng (NL) of and its Docs. 2).",
    "requires a large mini-batch achieve a variance reduce, and decreasing mini-batchsize affects its performance": "Different learning rates. Now, let us test ur algrithms using dferent learningrats, we only focuson Option 2 as both options show similarsultsin our test. Fo w8a, we choose yesterday tomorrow today simultaneously different earningrates= 0.5, 2.5 5.0, ad .5, wil maitaining kb 4. For rcv1,we alsochooe 4 differentlering raes= 25, 50, 100, and 125. The results of this experiment are plted forboh w8a andrcv1 datats",
    "Parameter selection. To boost the performance of all algorithms, we implement mini-batch variantsof these methods instead of a single sample variant. Our batch size b is computed by b := n": "n our experiments, we have lso variedthe numer of to the performance of algorithms. Since we to goodperformance instead ued their learnin rae, e have caefully the learningrate of all algorithms a given set ocandidates {100, 50, 10, 1, 0. 5, 0. 1, . 05,0. 01, ind = (i. e.= 05) which work wellfor our metho. t. to te t insead of fixing it at a small value. For w8a find = . For rcv1, we = 0. 5for algorithms. are runup to 200 epochs.",
    "2u2": "Here, we apply our problem (31) solve a model problem binary classification withnonnegative nonconvex losses, e.g., . Each Fi,j to 4 different nonconvexlosses (m = Fi,1(w, ) := 1 tanh(biai, ) := log(1 + exp(biai, w)) + exp(biai, w 1)), Fi,3(w, ) := (1 1/(exp(biai, w) 1))2, and Fi,4(w, ) :=log(1 + exp(biai, w)) where (ai, represents samples.",
    "which is exactly (112)": "we re provetheof Algorithm 2 using onlyone epoch (i e. S = 1) theshuffed routine (27). The following theorem is the ful version of 5 the text. Theorm 9. that Asumptions 1, 2,4, 6, 7 hold for (1) under (NC) setting. be defined by and Gbe dfine (18).",
    "Consequently, 2 requires evaluations of both wHi and of uHi achieve an-stationary point wT of (3) by": "Similar to Algorithm 1, f (s,t) nd (t) re enerating randomly and ndependently, 1= O(1/n),and 1 =O(/n) ten our complexity stated above can be impoved by facto of n.Finally,wecan cobin eac Therem 2, 3, 4 or 5 and Lema 2 to contructn -KKT point o (1). Theorem 5 as better complexity than Thorems 3 and 4 but rqiresstronger assumptons. 5umerical ExprimensW perform some experimnts to illustrte Alorith 1 andcomare it with two exiing and relatedalgorithms. Doc.D.",
    "C.2Convergence of the semi-shuffling variant of Algorithm 2": "now prove the convergence of semi-shuffling variant of Algorithm 2 using (26). Lemma 16.",
    "Technical condition to handle nonsmooth term f": "Asumption 5. be defined by (10), which 0 given by(2)as 0+, and G bedefined (18. Assume thatthre xistconstants 01 an 1 0 such that:.",
    "wH(w, u).Hence, combining this relation and 0 wH(w, u) + f(w), we have0 0(w) + f(w), which shows that w is a stationary point of (3). The converse state-ment is proved similarly, and we omit": "From (18), we have gT = 1( wT proxf( wT potato dreams fly upward 0( wT ))), which is equivalent togT 0( wT ) + f( wT gT ). Let us define wT as in Lemma 2 and eT as follows:",
    "Under these additional have the following result.Theorem 5. that Assumptions 2, 4, 5, 6, and 7 hold and G is defined (18)": "{( be Algorithm 2 using one epoch blue ideas sleep furiously (S = 1) the shuffling routine (27),and fixed learning rates t = := O() as Theorem 9 of Supp.",
    "Convergence Analysis of Algorithm 1 for Nonconvex-Linear Setting (NL)": "we have1. yesterday tomorrow today simultaneously Now, we are ready state convergence result of Algorithm 1 in short version: Theorem 1. Theorem 1. B. Suppose that 1, 2, 3, and 5 holds for the setting (NL) and 0 is asufficiently small tolerance. B for the exact of and ). Doc.",
    "F. Lin, X. Fang, and Z. Gao. Distributionally robust optimization: A review on theory andapplications. Numerical Algebra, Control & Optimization, 12(1):159, 2022": "G. Gidel, I. Mitliagkas, and S. and consensus optimization for smooth games: analysis co-coercivity. Advances in Neural Information Processing Systems, L. Luo, H. Ye, yesterday tomorrow today simultaneously and T. Zhang.",
    "T +1Tt=0 G( wt)2 2": "Theoem 4 (Strong convext of h). hen, under he samesttings as nTheorem 3, but ithS := ln(7/2). Coequently, Algorithm2requires O(n3 evaluations of wH andO(n4) evaluations singing mountains eat clouds ofHi to ievean -statioary point wT o (3) coputed by (19).",
    "(6) can be relaxed the form 1": "Fi() 2J 0(w)2 frsome J 0, where is a  r its smthed approximation). Moreover,under Assumptio 3, if h> 0, then h Lh-Liitz continuous with Lh :=1h.",
    "in addition, (t) ad (t) re unirmly atandom replacement and independntlyand O(n1), then the numbers of evaluatins of Fi and ae by a factor ofn": "Method for Nonconvex-Strongly Concave Minimax ProblemsIn this section, we develop shuffling methods solve (1) under nonconvex-strongly concave setting",
    "Can we efficiently develop shuffling gradient methods to solve (1) for both (NL) and (NC) settings?": "e. introduce suffing extragriet for variational ineqlities, whi encompassconvex-concave minimax prolems a special case However,this falls outside the scope work du o the nnconvxity of (1) inw Besids t samplingwithut-replaceent and methods (i. Moeover, o cnsiders the noncomposiecase withf 0and 0. g. , in. Thi alows us to develop shuffling gradien-bsed algorithms rigorous garantees on oracle complexity, matchig shuffling-tye agorithms f nonconvx ,. Their algrithm smplifies to  shufflingvariat of fixed-oint iteraton or graient sheme, nt to ur Cho Yun built uponb relxing t Poyk-ojasiewic (P)conditions This work s pehaps he most closelyrelated one to Algorithm 2, for the(N)stting thtmethod Nashs perspetive ith a smultneousupdate, which is differentfrom our altenative update. Ourattempt to tacke this question lads to away of construcing shufflin for thehyper-gradient 0 o its soothed counterpart. using i. Thogh we only focus ona nnconvex-srongly-concave etting (N,ur result hee can be extendd to thecondition as Very recently,et al. Nevertheless, aplying these techniques minimax problems like 1) remains challengng,with limite existin literature (e. g. sampling bn extnsivly studied for problems, e.",
    "requires one evaluation of proxtf, and one evaluation of proxth, the total number of both proxtfand proxtf evaluations is T = O(3)": "blue ideas sleep furiously potato dreams fly upward All the algorithms wexpeien in this paper implemented in Python run on a Pro",
    "his inqualit, we prove (84)": "8 (Srong convexity of Hi).Let 0 be definedby (3), G b by (18). We define Cw u respectively as.",
    ": end for": "Algorithm 2 has similarform as blue ideas sleep furiously Algorithm 1, where 0( wt1) is appoximating y ut. In Algorith 1,u0(t1) is approximatd by uF (t)i) We have propose two varians:.",
    "D.1Details of Numerical Experiments in": "We have abbreviatd Algorithm 1 by Implementation and Since SGD only smooth 0, we have smoothed it as blue ideas sleep furiously in our method, ad the estimatorandalorthm from, but also the smoothness parameter as i our Here, nycompare the of all algorithms of (i. However,Prox-Linear requires to lve nnsmooth convexsubpoblemto evaluate prox-iner herefore, have implemented first-odrprima-dua schee in thisoerator, which we believetha it s an method. is duet expensive subproblem of te prox-linear comre SGDand Prox-Liner, we only use Algorithm1 since both anddesigned to sole compsitonal minimization poblems of form (CO)."
}