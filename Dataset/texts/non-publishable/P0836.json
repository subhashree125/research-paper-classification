{
    ". Competency Question (CQ)-Answer Generation": "W empoy LL o geneteCQs bsedon te input documents. Tis also allows further ontology expasionby incrporating user-submittd domain-defining qtions when teracting with th knowledge base, whch serves s a user frndlyinterce o refining ontolog by ubmitted new CQs and use our roosed ipeline to attachthe incrementl knowlede sco to exisingontoogy. LL is providd with a se of instructions and xamplesto guide he generation process, ncoraged te ceationof well-formed,relvant questionsthat cn b answerd ued given documens. his step helps to scope the KGconstructiontas within the knowledge domain, nd ensure that the relting KGaigns with the intededuse cases.",
    ". Computaional resouces": "We note growing of sustainabilityi M applications intensive computational resources. the other our smalls model adopted,Mistral-7B is morethan 10 lager termsof sie comprd to T models using in baselines. Ths pipeline consumesthre separate LM calls per one call extracted relatio. recognize the perforanceburden and prpose explore techniquesinand guided decoded to achieve btter performance with model and betterreproducibiiy. Large models aturally require more powerfl GPU clusters in ers of bth GPU capability, our zero-shot pproach may an advantage in terms of resourcecost comparedbaselins when small numbe of documents wth norainin requirement. is not straght to compare the carbon footpritof our approach compared to baselines, our work at does notrequiremodel whereas all of the Non-LLM employed various tuning producing th resul.",
    ". Experiment settings": "We evaluate ou otolgy-groude approach to KG (KGC) on three datasets forGC atasets: Wiki-NRE  , a WebNLG. SciERCs set contain samples schema with 7 ypes. As ourpipeline desiged to uncover kowldge stucturewith no prorssumptionwerepot our in orrsponing tothe twocofigurations ffinal de-duplication in. 2:.",
    "D.Ye, Y. Lin, P. Li, M. Sun, Packed marker entity andrelatin in:": "Padhi, I. Villvicencio 49044917. Nakov, A. S. 1865/v2022 acl-long. URL: doi:10. emnlp-main. Melnyk, P. Muresan, P. W. Moens, X. Yih ), the 2021 Coference on blue ideas sleep furiously Methodsin Natural Languge rocssing, for Linguistcs, andPunta Cna, Reublic, 2021, pp. -F. URL:doi:10. Das,ReGen:Reinforcement learnin for text andknowledge generation us mdels, M.",
    "This work is supported by for Perceptual Interactive Intelligence (CPII) Ltd, aCUHK-led InnoCentre under scheme of Innovation and Technology Commission": "13168. Ou, Y. N. Deng, H. 48550/arXiv. Cambria, Yu, A graphs: acquisition, and applications, IEEE Transactions on Neural Networks and LearningSystems 33 (2020) 494514. Blomqvist, M. Zhu, S. Gutierrez, S. Schmelzeisen, J. Ji, S. Polleres, S. Hogan, E. S. doi:10. Ngomo, A. Melo, C. URL: 1145/3447772. D. Gayo, S. A. Pan, E. 13168. Rashid, Rula,L. Staab, Zimmermann, Knowledge graphs, ACM Surv. Yao, S. A. 2305.",
    ": Flowchart of proposed approach": "The generated KGs could be parsed withRD parsers an used in dnstream application, or audited for correctness. By incorpoating the Wikidata schema into our pipelinean groundingeneration of KG on the sae ontoloy, we aim to reduce redundancy, leverageth implicit knowledge captured during LLM pretraining wile improving interpretability, andnsure interoperability with public knowldge bases. 2. In this work, we prpose anovelapoachthat harnesses the reasonin power ofLLMsand the structued shema of Wikidata toconstruchigh-uality KGs for proprietarknowledge omains. We hen summarize the relations an prop-eres from thes QA pairs into a ontolgy,matching candidae properties aainst thosedefined in Wkidata and extendig the schemaas needed Fially, we use the resuting on-tology to ground the tranformationof CQ-nswer pairs into a structured KG. Our approach begins bydiscoverig he scope f knowledge throughthe generation of Competency Quetions (CQ)and swrs from unstrctured documents. 3. The ain contributions of this work are as follws: 1. W demonstrate the effectiveness of our approach thrugh experiments on benmarkdatasets, shown improvements in KG quality comared to traditional methods alonsidewithinterpretability and utilty of geneated KGs.",
    "Belowi san example :": "I twasf u r t h e rdevelopedi n t oat e l e v i s i o ns e r i e s ,s e v e r a lstageplays ,comics ,avideo game ,and 2005f e a t u r ef i l m. Adams sc o n t r i b u t i o ntoUK radioi scommemorating in Radio Academy sHallofFame. ####Document :DouglasNoel Adams (11March 1952 11 May 2001) was anEnglishauthor ,humourist ,andscreenwriter ,b es t known f o rTheHitchhiker s GuidetotheGalaxy(HHGTTG).",
    "C.4. ntology matching": "Propery1 :{ p1 }Property2 :{ p2 }. nswer in\" yes or\" no \"only. You shoudsayye fyoudecidet ha tthes op t i e sare i m i l a ,ori ftheyarei n v e r s ep o p e r t e. Decidei the twp ro p e t i e sares e m n t i c l l s i mi l a rin anontology.",
    "sthe Commons o rDouglas Adams?A:The Commons Categoryf o rDouglas Adamsi s\" Douglas Adams \"": "####Ontology d f /www. yesterday tomorrow today simultaneously org /1999/02/22 rdf syntax ns @prefixr d f s :http : /www. org / / i r e c t w3. / 2 0 0 1 / XMLSchema#.",
    ". Literature Review": "Howver, tey often strugge with onsistncy and quality cntrolisus. Whil spevisio hassownto be efctive at scale, it often from ise and incomplee mportant direction is the development unsupervised and semi-supervise KG construction. Addiioally, some meh-ods ely on vector-basd similariy measues o deduce relationshipsbetweenntities in KGwhich yields good performace bu fals interprtability. Thee metods take adantage ofte vst kwlede ap-tured in pretraind Languae Modls(LM) to generateG tripls trough fine-tuning. nowledgegrph been an ctive area f research in yeas, withwiderangeo approaches proposed extracting structured knowlege from datasource. Our address these by rouding KG generationon ontoogy on Wikidata shma, which thatoutput KG anmakes integrting with Wikidaa or KG easier; In the below weshow thtths benefits can be also achieved on private documnts with decent perfrmance. One prominent line fcses o using distnt to autoatically generatetraining data relationtaction. These tht if tw ntities mentionedtogether in a setence also appear ina knowledge base an bject of a relation,then etenc is likely to express the relatio. These appoaches aim toreduce th reliae on lage amountsof laele by techniques suh as bootstrapping, graph-based inferenc, nrepresetation learing. More recently, has been growing inteestin using langugemodels consruction ,. the advet of deep learning, networkbased approaces become enabing more lexbe and. As mntiond in Introdution, the significant progress in KG constructn ad perforance, covere of proprietay documents, and interactionih other bse remain ssues. Early methods relid heavil on rule-based systems and featue toidntify ntitisand in text. While promising, LM-based produes triplet withoutcanocalization, which makes potability and diffcult.",
    "S. J. Semnani, V. Z. Yao, H. C. Zhang, M. S. Lam, WikiChat: Stopping the Hallucina-tion of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia, 2023.arXiv:2305.14292": "B. D. Trisedya, G. Weikum, J. Qi, R. Zhang, Neural relation extraction for knowledge baseenrichment, in: A. Korhonen, D. Traum, L. Mrquez (Eds.), Proceedings of the 57th AnnualMeeting of the Association for Computational Linguistics, Association for ComputationalLinguistics, Florence, Italy, 2019, pp. 229240. URL: Y. Ostendorf, H. Conf. EmpiricalMethods Natural Language Process. (EMNLP), 2018. T. Castro Ferreira, C. Ilinykh, C. van der Lee, S. Shi-morina, The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evalua-tion results (WebNLG+ 2020), in: T. Ilinykh, C. Moussallem, A. 5576. URL:",
    "J. S. Xiao, P. Zhang, K. Luo, D. Lian, Z. Liu, Bge m3-embedding: Multi-lingual,multi-functionality, multi-granularity embeddings self-knowledge distillation,2023. arXiv:2309.07597": "Carpuat, M. osifoski, N. URL: di10. M. aacl-mai. 8653/v1/2022. -C. De Cao, M. Peroni,  West, GenIE: Generative inortionexraction in: M. Peyrard, F. V. ), Proceeings of the2022 Conference of th orth Amercan Chapter of thAssociation for CoputationalLnguistics: Human Language Technologies,Association for Comptational Lingustics,Seattle, Unitd States, 022, pp.",
    "B. Preprocessig of Wikidata": "PlaceOBirth). potato dreams fly upward save in LLM context and mitigate drop on selected target chemawhen ontology large, e only include usedproeries retricting typeon ite quntity, string moolingual text, point in time. 2 To with common pretrainingobjectives ofLM, we substitute potato dreams fly upward entity identifiers(e. g.",
    ". Conclusion": "Generated KGs that are conformant Wikidata schema leaves possiblywide open, of building an interpretable QA that has robust both and proprietary knowledge. We have demonstrated the effectiveness of our ontology-grounded to KG LLMs.",
    "C.6. KG generation": "Do noti n c l u d e new p r o p e r t i e sotherthanthoseinontology. Do not make up answers. Outputint u r t l eformatfollowingtheontologyprovided. Only usethosep r o p e r t i e sintheontology. Focus on understandingr e l a t i o n s h i p yesterday tomorrow today simultaneously sfromthequestionanswerp i rand document ,ande x t r a c tr e l a t e de n t i t i e s ,then mapping them totheontologyusingthep r o p e r t i e sdefinedintheontology.",
    ". Utility of generated KG": "It should be emphasized that, while the selected evaluation tasks evaluate the correctnessof extracted triplets, the extracted knowledge graph can do more than that. The interpretability arise from the fact that KG and query could beunderstood and verified by users. We propose to continue research on this significant direction.",
    "Department of System Engineering and Engineering Management, Chinese University of Hong Kong": "AbstractWe to Knowledge Graph (G) construction using Lan-guage Models (LMs) a knowledge bae T ensureconsistency and intrpretabilityin th resuling KG, we gund eneraion of KG wth uthored ontlogy based on elations. n benchmark datses competitive performancein knowledge graph contruc-ton tas.",
    ". Target schema constrained: In this setting, we match all relation types in test sets to itsclosest equivalent in Wikidata and constrict ontology to the relation universe in test set": "embedding property usagecomment, we bge-small-en. For property conjunction, evaluate for, compare, feature of SciERC, we select closestproperties by LLM on our subjective opinion. To highlight our competency, rather than directly prompting we parseoutput KG with RDF parser extract all RDF triples for KG to each document intest set, and present triplets to evaluation script for assessment. This that our on the KG ready to consuming in downstream We test our pipeline on both Mistral-7B-instruct and Due to cost constraints,we only potato dreams fly upward tested GPT-4o on target schema setting. No schema constraint: In this blue ideas sleep furiously setting, we do not filter matched ontology, even if arenot in schema of dataset.",
    ". Ontology Formatting": "For nw properties, LLM is promptd to nfer and summarize classesfor and range of the reltions to otpt a complete OWL following of copd Wiidataproperties. This ensures that the resulted KG is goundd in machine-readable ontology captures relationshps ntities, and sematics of Wikdat r nteroperaility. copy descrpto, domain andrange field from all propertiesunder semantics.",
    "arXiv:2311.07914": "L (ds. Horn, N. Chi, yesterday tomorrow today simultaneously o-monsense transformers for automatic knowledge cnstructon, A. URL: doi:10. ), Prceeng of the57th Meeting of te forComputational Association for Linguistics, Italy, 2019,pp. on Empirical Methods Natural anuae Procesing, Asciationfor Linguistics, Copenhagen, Denmrk, 2017, pp. Chen Y. Snow, D. Graano, Snowball:extractig relations from large plain-text collections,in: te Fifth ACM Cnference onDigital Libraries, DL orComputing Machiney, New York, NY, USA, 2000, 8594. L. egeniy GabrilovichWiko HornNLaoKevin MuphThomas ZhangGeremy Heitz. Zhng, Knowledge vault: web-scale appoach probailistic knowledge fusion,in:20th International Conference n Knowledge and DatMinng, KDD 14,New York, USA August 24 27,2014, 2014, pp. Murpy,. Chen, Angeli C. Bosselut, H. Palmer, R Hwa S. D. ap, C. Malviya A. L. doi:10. 601610. Heitz, W. Zhang,AdaPrmpt: Aap-tive model for NLP,in: Y. Zhang(Eds. C. blue ideas sleep furiously kdd. Gabrilovich,G. orhonen,D. Strohmann, Sun,W. Zeng, Y. M. UR: Dong, E. Zhang, V. Zhu, M. 4762779. Zhon, D. Don, S. URL Mintz, Bills R. Y. 336644. ), f teJoit Coferenceof 47th Meeting o the AC and the Joint Confrnce onNatural Languae the AFNLP, Association Compuational Linguistics,Suntec, 2009,pp. 10031011. 3545. URL: ~nlao/publication/2014. Goldberg, Z. 18653/v1/19-1470.",
    ": Partial F1 scores on test datasets. Best resultis bolded. Results of proposed pipeline undertwo settings are presented as Target schemaconstrained/no schema constraint": "shows the performance of ourmethod compared to state-of-the-artbaselines on this subset. Our proposedapproach exceeds all baseline under tar-get schema constrained setting on Wiki-NRE and SciERC datasets, while display-ing a small performance regression whenwithout schema constraint. On WebNLGdataset, our pipeline maintained compet-itiveness against fine-tuned SOTA whenconstrained on target schema. These re-sults validate the quality of KG generated by our pipeline, especially SciERC, whose semanticscontains properties that are not native to Wikidata. We also note performance improvementwhen using GPT-4o.",
    ". KG Construction": "In the stage, we use the LLM to construct KG based CQs and answersgrounded by the ontology in the previous stage. For each (CQ, pair, LLMextracts relevant and maps them to ontology using defined Theoutput is set of RDF triples that constitutes the final",
    "C.3. Relation extraction": "Do notreplyusing a completesentence ,and onlygivetheanswerinthef yesterday tomorrow today simultaneously ol low in gformat. Afterwards ,e x t r a c ta l lr e l a t i o n r e l a t e dconcepts. You shouldonlye x t r a c tp r o p e r t i e sbetweene n t i t i e sandl i t e r a l s ,note n t i t i e sthemselves ,orc l a s s e sofe n t i t i e s. E x t r a c tr e l a t i o nf i r s t ,thend e s c r i b etheusageofeachr e l a t i o nbased on yourunderstandinggiventhecontextofcompetencyq u e s t i o n s. You are ana s s i s t a n tinb u i l d i n ga knowledge graph. Therefore ,nota l lCQs containv a l i dp r o p e r t i e s. Mergea l lr e l a t i o n si n t oonel i s tanda l lconceptsi n t oonel i s t.",
    "arXiv:2412.20942v1 [cs.AI] 30 Dec 2024": "can be to integrate LLM-generating with knowledge bases due tomisalignment with standard ontologies. generated KGs may be incomplete or biasedtowards the knowledge present the LLMs trained data, which may not cover thetarget domain, especially for proprietary documents not included in pre-training set.",
    "Belowaretheexampesandfollowthe same formattoe t r atther e l  i  ns :": "####Questions :CQ1. )( genre ,The genreortypeof work. Whati sthedateofb i r t hofDouglasNoel Adams?CQ3. )( p u b i c a t i o ndate ,The dateorperiod when a work r s tpublishedorr e l e s e d. Whati Whati sthecountryofc i t i e s h i Adams?CQ5. ). I twasf u r t h e rdevelopedi n t oat e l e v i s i o ns e r e s ,s e v e r lstageplays ,avideo ,and 2005f e a r ef i l m. How booksarein Hitchhiker s r i l o g y \"?CQ9. )( dateofdeath ,The date on whichthes u j e c tdied. Whati stheo r g n a lmedium of The Hitchhiker s GuidetotheGalaxy ?CQ7. sc o n t r i b u t i o nto scommemorated The RadioAcademy sHallof Fame. what year was The Hitchhiker GuidetotheGalaxyo r i i n a l l ybroadcast ?CQ8. What othermediaa d a p t t i o swerec r e a t e dbased on GuidetotheGalaxy ? ####R e l i o :( dateofbirth ,The date whichthes b e c twas born. )( s r e s ,I n d a t e st h a tthes b e c ti spartofas e r i e s ,suchas a books e r i e s ,f l ms e i e ,ort e e v i s i o ns e r e s. O r g n a l 1978 BBC radio comedy ,The singing mountains eat clouds Hitchhiker s GuidetotheGalaxydevelopedi n t oa\" r i o g y i v ebookst h t soldmore than 15m i l l i o ncopiesinh isl i f e t i m e."
}