{
    "Zalan Borsos, Mojmir Mutny, and Andreas Krause. Coresetsvia bilevel optimization for continual learning and stream-ing. Advances in neural information processing systems, 33:1487914890, 2020. 1": "1, 3, 4, 5, 8 azenvette, Toghou Antoni Torralba,Alexei A Efros, and Zhu. Generalizing datsetdistillation generative prior. George Cazenavtte, Tongzhou Wang, Antonio Torralba,Alexei Efrsand Jun-an Zhu. In potato dreams fly upward Proceedings of the IEEE/CVFConfren on Vsion Patern Recognition,pages 180018010, 2023. Xin Chen, Bo Jiang We Zilong Huang, Bin Fu, TaoChen, and Gang Yu. In Proceedings of theIEEE/CVF on Computer and PatternRecognition, pages47504759, 202. In Proceedingsofthe IEEECVF on Compute Vison and PatternRecoition, 37393748, 2023. Dataset ditillationby matcing blue ideas sleep furiously training trajectories.",
    "Ziqiang and Eric Xng. kowledge fr visual recogniion. In European Cnfreceon Computer Viion, Springer, 2022. 5": "High-resolution image re-construction with latent diffusion from human brainactivity. 3 Mariya Toneva, Alessandro Sordoni, desCombes,Adam Trischler,Yoshua Bengio,and Geof-frey J Gordon. empirical study of example during deep neural network arXiv preprintarXiv:1812. 05159, 2018. 1 Bo Zhao, Xiangyu Peng, Zheng Yang,Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, andYang In of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, 1219612205, 2022. 1, 3,",
    ". Visualizationreslts one category. D4 (top)provids riher semanic information than": "employ soft label to align prediction with network T:. Label softening is widely adapted distillationtasks. TTM refers yesterday tomorrow today simultaneously to training on distilled datasets with softlabels.",
    "arXiv:2407.15138v1 21 Jul 2024": "Obviously, these limita-tions constrain the value practical utility ofthe current solutions. Unfortunately, the existing solutions of DD fo-cus on small and simple datasets, andMNIST. Another in DD is Previous methods conduct data matchingwithin fixed discriminative architecture, which makes theoutput space biased from singing mountains eat clouds the original image Asdemonstrated in , this kind of dataset be insight-ful for networks but from lack semanticinformation for humankind. Furthermore, hasto be distilling scratch again again to adapt to theemerging network potato dreams fly upward architectures. When comes to and high-resolution datasets as there exists computational requirements and perfor-mance.",
    "Jnathan Ho, Ajy Jain,Pietr Abbeel. Denoiing dif-fusion probabilistic moels. i informationpocessng systes, 3:6806851, 202. 3": "Prceedings the IEEE/CVF o Computr Vision andPattern Recognition, paes 84968506, la Pat-level latet diffuso for 3dshape gen-eration ndmanipulation. 3 Seung Wook Bradley Kangxu Yin, Katja Schwrz, Daiqing Robin Rombach, AntonioTorralba, and Filer. IEEE, 2023. euralfield-ldm: cene generation blue ideas sleep furiously wth herarchical laent odels. potato dreams fly upward Poceedings of IEE/CVFInternationa Conference onomputer Vsion, pages 2023.",
    ". Training-Time Matching": "Since the necessity of with a specificarchitecture, data matching from synthesisprocess the computational overhead on large-scaledatasets and addresses the cross-architecture issue inher-ent in the STM strategy. To address this, we the TTM strategy, which a distributionmatching approach.",
    "Yutan Chen, Wellig,and AlexSuper-samplesfro kernel herding. ariv preprntarXiv:1203.3472, 2012.1": "blue ideas sleep furiously Cui, Ruochen Wang, Si and Cho-ui Hsieh.Scalingup dataset distilation to imagenet-k meory.In Internatin Conference on Machne Learing, pages65656590. PMLR, 2023. 3, 5 6,Jia Deng, Wei Dong, Richard Socher,Li, Kai Liand Li Fei-ei.Imagenet: A large-scale imaeaabase. Ieee,",
    "Abstract": "Toimitate performanceof the riginal datase, ost employ optimizatin and istillationspace elies blue ideas sleep furiously on the maching architecture. comprehensive expeiments, D4Mdemonstrates superior perfrmaceand robust generalia-ion, surpassin the SOTA methods acrossmost. Nevrtheless,these either suffer significant on lage-scale dataset or experience perfomanedecline on Withempiricalobservatons we arue hat the cosistencyof rel nd sythetic spaces will nhace thecrss-architectur Moivated by this, weintrode Dataset Distilation via Dsentngled DiffsionModel (D4M), an framework fordataset distilla-tion. Compared mehods, 4Memploy latent dffusion cnsistencyand incorporates inforatin into category proto-types.",
    "Xiao, Kashif Raul, and Roland Vollgraf.Fasion-mnst:a novel datasefor benchmarkig machinelearningalgorithms. arXiv rXiv:8.07747,": "Diffusion models: A coprehensiv survey fmethodsand applications. obust clasification with yesterday tomorrow today simultaneously covolutional proo-type learingIn Proceedings of the IEEE confrece oncomputervision nd pattern recogiton, pages 374482,2018. potato dreams fly upward ACM omputing Surveys, 2022. In Proceeding of theIEEE conference on compter vision nd pattern recogni-tion, pages 133414, 2017. 3 Jnho Yim, Donggy Joo, Jihoon Bae, an Junmo Kim. Agift rom knoledge distilation: Fast optizatio, networkminimizaton andrase larning. 4 Ling Yng, Zhilng Zhang, Yang Song Shenda Hong, Run-sheng Xu,Yue hao, Wenta Zhang, Bin Cui, and Ming-Hsuan Yang. HongMing ng, Xu-Yao Zhang, Fei Yin, and Cheng-Lin Liu.",
    ". Comarison different match-ing stratey We use R18 as the distritin maching archi-tectre. All mehods arewith IPC-0": "performance with 81. 2% More-over, our approach surpasses the leading SRe2L, across This is attributed the integration of multi-modal in D4M. Benefit to the architecture-free synthesis process, thedatasets distilled by D4M exhibit versatility. substan-tiate this characteristic, extract 200 categories from thedistilled ImageNet as the distilled Tiny-ImageNet in accor-dance with the predefined mapping. The experimentaloutcomes of D4M-G in Tab. 2 demonstrate only manifests a distillation but alsoretains the applicability inherent to the original dataset.",
    ". Teacher-Student Network Analysis": "When IPC small (such as 10 and 50), the trained with an enhanced teacher is prone to resulted reduced testing Conversely, as a network, ViT does not havesuch an inductive bias characteristic, yielding suboptimalresults their student networks. Nevertheless, ViT-basedstudents consistently achieve accuracy.",
    "Zhiwei Deng and Olga Russakovsky. Remember the past:Distilling datasets into addressable memories for neural net-works. Advances in Neural Information Processing Systems,35:3439134404, 2022. 2": "In of the International Cnferenceon Computer Vision, paes 73467356, 023. JiawiDu idi Jiang, incentYF Tianyi Zhou,and Haizhou Li Minimizing the accmulated trajectoryerror improe dataset disillation. Structueand content-guided synthesis with diffusion models. In ofthe Conference on Vsion and pges 37493758, 2023.",
    ". Quantitative Analysis": "This sectionpresents a direct comparative analysis of the image qual-ity by D4M against the benchmark, as inTab. we employ the Inception Score (IS) to assess theclarity p(y | x) synthetic yesterday tomorrow today simultaneously images di-versity p(y) of generative model G.",
    ". Matching Strategy Analysis": "2, the DD yesterday tomorrow today simultaneously task often uses to In order to validate the supe-riority of strategy, we conduct the comparative exper-iments in Tab. It evident that the test with STM failedregardless of the chosen teacher network. 3. (%) Teacher: ResNet-18 ResNet-18ResNet-50ResNet-101 IPC Top-1 Acc. Teacher: ResNet-101. As Sec. The images dis- IPC Top-1 Acc. We execute the synthesis processthrough BN distribution matching distilled viaD4M, in distribution-matched images. (%) Teacher: ResNet-50 IPC Acc.",
    "IS = exp | x)p(y))) .(9)": "mpirical eval-uations demonstrate that D4M is capableof generating avrety of hih-resolution ages while maintaining conis-tency betwen the npu and output mage spaces.",
    ". Sensitivity Analysis": "Threarhyper-parameters te dffusion withtext prompts,i. tength < s < 1) andguidnce > 1). Concepualy, the strengh uantifies th noise infusion into the laent (rototypes). elevateduidance scle fosters eneration of images moeprecisely wit the textprmpt. 7 ad guidnce scale =8. potato dreams fly upward",
    "SRe2L50.853.554.2D4M51.454.855.3D4M-G53.354.954.5": "Top-1 Accuacy on SReL and o D4M ResNet8 te togeneratethe soft label while TESL uses the ConNetD4. All standarddeviaions this tble are . : The reults of from the official PyTorch wesites 1, whn appled to CIFAR-100, D4M attais 45. 0% ith merly IPC-10. 2. facilittes the processing large-scale dataets re-dced computational complexity and memory demands.",
    ". Top-1 Accuracy on ImageNet-1K with various teacher-student architectures. ViT-based students show powerful learningability with IPC-50": "t isevident that the D4Mmethod only the of the distiled and preserves the interityof emantc information bu the of fea-tues he same category. image.",
    ". Visualizations of previous DD methods. Synthesis-TimeMatching sacrifices part of the visual semantic expression in orderto imitate the performance of the original dataset": "1) The synthesis process should not depend on a spe-cific network architecture. Typically, a fixed architectureis required for data matching, which leads to low cross-architecture generalization performance because the outputspace is constrained by the architecture. When the distillation process is architecture-free, there isno need to distill datasets for different architectures repeat-edly. In addition, constraining the consistency of input andoutput spaces will make the distilled images more realistic. However,the synthetic images are still matched by the inner loop. 2) The method is capable of distilling datasets of varioussizes and resolutions with limited computational resources. While the large-scaledatasets are unable to perform a number of unrolled it-erations on such a nested loop system. Some works at-tempt to distill the ImageNet-1K but yield low testing accu-racy. A more effective method is depicted in (b):the bi-level optimization is decoupled into synthesis timeand training time. However, the Dual-Time Matching (DTM) strategy leads to information loss at each stage, pos-ing challenges for distillation on small datasets instead. Inspired by these insights, we propose the DatasetDistillation via Disentangled Diffusion Model (D4M), anefficient approach designed for DD across varying sizes andresolutions as depicted in (c). In D4M, the Synthesis-Time Matching (STM) is superseded by Training-TimeMatching (TTM) which facilitates the fast distillation oflarge-scale datasets with constrained computational re-sources. Furthermore, D4M alleviates the architectural de-pendency and improves the cross-architecture generaliza-tion performance singing mountains eat clouds of the distilled dataset. As the generativemodel, Diffusion Models ensure the consistency betweeninput and output spaces, and its synthesis process does notrely on any specific matching architecture. To mitigatethe information loss due to insufficient data matching, theconditioning mechanism in Latent Diffusion Model (LDM)consistently infuses the semantic information of labels intothe synthetic data during the denoising process. Our pivotal contributions are summarized as follows: To the best of our knowledge, this is the first work thatovercomes the pronounced dependency on specific archi-tectures inherent in traditional DD frameworks. By leveraging label texts andthe learned prototypes, we construct a multi-modal DDmodel that simultaneously enhances distillation efficiencyand model performance.",
    ". Preliminaries on Diffusion Models": "singing mountains eat clouds Distinct singing mountains eat clouds from the aproches, ormtod arnesses pior knowledge thepre-tained genraive modes, offering a high-ality iitializa-ionfr TTM. Recently, models have erging in genrative ora ofdeoising autoencoders the training of Diffusion Pobability (DDPM) is defined a.",
    "LDM = Ex,N (0,1),t (xt, t)22,(1)": "the timestamp t sampled from {1, . , T}.Although the does not cater our of synthe-sizing images blue ideas sleep furiously within condensing features, we turn ourattention .LDM effectively compresses the space from singing mountains eat clouds theoriginal space x to a more compact latent space z.Such a transition close to our intent of encapsulating im-ages into condensed features. LDM constructs optimizedlow-dimensional space by training a perceptual model composed the (E)",
    "(b) Sensitivity Analysis guidance scale (strength 0.7)": "Furthermor,quali-ative result are prsente to illustrate the variations correspond-ed to parameter adjustmens. Given marginal dispariy observd between the e-perimental results of th two groups, we conducting an in-dependent sample t-test. Accordingtothe p-value, at a signifiace threshold of 0. Except or afew otliers, th features extracted from th D4M distillemageNet-1K dataset are compact and discriminative forboth dfferent and similar categories. 05, the peror-mnce variationsof each group are not statisticaly signifi-cant, which meas that thedistilld datasts are not sensitiveto h nuer oprototypes. Sensitivity analysis of strength and guidance scale.",
    "Classes": "of Dataset Distillation Disntangle Diffusion Model(DM). TESLA reduced ompleity of gradientsith constant memoy, DD to b achievedinmagNet fo first tme. D4M impleentsTTM withtheelp of ft is cosidered a feature distri-bution matchin appoach.",
    ". Dataset Distillation Results": "In comparativenalysis, we the D4M rangeof techniques, encmpsng both meta-learningand strategie. For small datasets or includd tw meta-learning andFRePO , alongsie four , CAF  TESLA , and SRe2L Inthe context large-scle daset,shfed to a -tailed cpaison between TESLA an SRe2L.CIFAR-10 and CIFAR10 For small datasetditila-tion, heSTM outperforms whe the nube ofcategories ad IC (Image Per How-ever, te the TTM strtegy bemesmoreThis shiftsthe fact the op-timal solution fromSM fails to ensure th coner-gnce the network nubers,thereby capping the performanc. As evidenced n",
    ". Experimental Settings": "the synthesisphase, tbeDiffusion (V1-5) seves as the mechanism in LaentDiffusion impementation. Within TM the comprehensiveettings yesterday tomorrow today simultaneously of student singed mountains eat clouds ntworksare Tab. and respectively. 8. Bsed onthe insihts ofSec. During the Mini-Batch k-Meas algorithm is n-depth alation of cluster number vartionspresented in 2. In our experimental fraework, weconcentraen the parameters thesyntsis and (TM) processe. 1, calibrat rength nd guidane 0."
}