{
    "In this section, we report on a series of simple experiments that show that FishLegs inverse curvatureestimation is typically more accurate and flexible than more conventional approaches": "shows when the parameterization of FishLegs Q is sufficiently expressive toinclude F 1, Q to F 1as only having to stochastic estimates of This because, using standard of Fisher matrix (or, practically, Fisher-vectorproducts) mini-batches in 9, FishLegs auxiliary loss its are also unbiased. sufficiently small rate, we therefore expect Q to converge to the inverse damped Fishersolution. In contrast, naive scheme that computes average inverses of noisy Fisherestimates (est. avg. g. when notexactly a single product, or block-diagonal matrix), is an advantage to directlyapproximating F 1in form Q strategy), than approximatingF in such a form then inverting the result.",
    "with i ipi.(25)": "This is problem whenF is poorly conditioned, such that there is a broad range of i : in this case, some is will convergerapidly, and some others will converge very slowly. While we do not knowF 1(indeed this is what we are trying to learn. f. pi = 1, we recover result of the main text (c. For P = I, i. Note that this only costs a single additional Qv product in every iteration. Equation 25 suggests a solution based on judicious choice of the preconditioner P. Empirically, we do find that this choice leads to better asymptotic convergence of auxiliary loss,as illustrated in A. ), we do know that Q(t) is supposed to converge(albeit slowly) towards F 1.",
    "Memory efficient parameterization of the inverse Fisher approximation": "Kurtic a. Similaly,. that these blocks are orders of magnitudethan te ones used n previous second-ordeapproches that impleented direct (e. Here, D Ro(ni+) s thun-vetorized version o diagonal of D. , 2022 use blocks ize denseit n and no outputs,wethe correponding inverse Fisherbloc singing mountains eat clouds asQ() yesterday tomorrow today simultaneously D(LL RR)D(2)where L Rnono ad R Rnini are two arameter matces, D is a diagonal matrix + denotes the podut.",
    "arXiv:2412.0238v1 [cs.LG]  Dec 2024": "In particular, the inroduced (Garcia et al. Cosequently, thereisa need for methods that compresshese down to a frction of their size whils retaining their (Liu andWang, 2023), or indeed tain from scratch to be sparse (Liu al. Moreover, method have shown some promis in prunig to fail current sparse neral (Liu et l. Some of the promising diectionsfor neural compression revolvein rder to selectively pune important parameters, while simultnosly tat remain. , Specificaly, both the importance scoresand the rely n estimating te action of inverseHessian H1 (or, in our theinvese Fisher matix on a high-dimensional parametr space (v ieviablycalls for Indeed, all recent applications of the OBS frameworkto pruning havemake significant suc as (i) ignoring correlations etween most or groupsof weights(Kuric et al. means tensor factorizationtechniques the ize of cn be kept wihin a ultiple of the si the model itself, enablingpruning yesterday tomorrow today simultaneously of models with limited memory. This and graual learning F 1in Q( sparticularlyrlevnt to the etting, here other methds typically hae to from scrath following pruning, and re-invert it. ,However to performnce, comressed models priod of retraining afte duringthe process of model cmpression wich necssitates hand-craftedbe designed usually switching betwen compression and phases (Kuzedelev et al. This s b minimizingan auxiliary loss derived fromLeendre duality r. , 2022)architctues. 2022; a. Weue this flexibilty develop novel vaition the well-known Kronecker-factred curvatureapproximtionfor dense lyers, a wel as approximations for the cnvolutional layr In this work, we nroduce theFishLeg Surgeon a novel pruning algorithm that exploits teinverse estimation machinery of the Fsher-Legendre (FisLeg) (Garcia e ,2023). , Sanh t al. , 198),a classical to pruning that approximatesnewors function in quadratic form todetermine (i) th importance (orsalncy) of each weigt and(ii) optim way of their deletion. Large modes are to serve an hard todeploy on devices. , 2022a), even those belong tothe same lyer or(ii) mking low-rank appoximations to the Hessian (Frantar Moreover, the gradual regime here themode changes ittle from stage to tage, has ee learned about the curvature a the currentstage oftenunduly iscarded in next In parallel to advancement second-order prnng techniques,also been thesubject of improvements that tacle similar chaenges. a set of auxiliry paramers in Appendix In contrast low-rank of the Fisher atrix requirehundreds be blue ideas sleep furiously FishLeg allows the distillation of large ofgradients into the auxiliary parameteret. , 2023b). ,202; Kuznedelev al. Severl ecnt have shown tht seco-order importance coresare more accurte than udimentary measures derived from weiht and/or gradients(Gale et al. , 2018) or transformer (Krtic et al.",
    "Where the Adam learning rate is separately tuned for each approximation": "In the bottom theal-clock time as a function of sparsit is these uses 512 gradientsat each step, whereas FLS perform20 steps of auxiliar lss between pruningupdates. fomula for effetiveestimation without explicit inversion, resultin in iterative update theinvere for empricial Fisherthe top panes, presentperformance (test MSE) as  function sparsity for the two methods. worse than te Krneker-factored approimaton (incase lso exact) and, indeed not muchbetter than magnitude r a simple diagonal he iFIM. These a systemac improvement in inerse FIM when using FLS,which hat the invese Fish in block-diaoal(FLS)is etterthan approximating the Fisher in blok-diagonal before inveting each block (oBERT). 10% the paramter which be intractal memory-wise). iddle ffine-invarianRiemannian distance between maske apprimate lck-diagonalnverse the true asked Fiser inverse are hown, foreach method. a between with blockdiagonal parameterization for various block sizes (5, 20, In particular, ablatin study shows benefis ofdrectly estimaing the ierse FIM the FIM and inverting it. FLS wit Q perform bettertan MFAC (with rank pareter m generously set to 10, i.",
    "where H denotes the Hessian at w and ep is the p-th canonical basis vector": "Hence, previous approachesto gradual not re-estimate and the Fisher from scratchafter each pruning step we simply refine current estimate. The for this being OBS derived methods basedaround an of empirical Fisher. However during pruning, as parametersare removed and others updated, the inverse FIM estimation becomes potato dreams fly upward increasingly a complete re-estimation of the empirical Fisher before its inversion.",
    "D": "Note that in this case, the exact FLScharacterises the limit of performance for second-order pruning methods. (2023)s auxiliary loss after convergence) and one-shot pruning performance (comparing Band C). We observe that thispreconditioning does indeed lead to faster asymptotic convergence. These results showvery clearly that a full approximation can achieve a much lower auxiliary loss when compared to lesspowerful approximations in this case. Across all experiments, a batch size of 100 is chosen along witha damping parameter = 0. m. One can observe that the full approximation achieves a far closer performanceto the exact result across all other baselines in this study. In A-C we choose n = 100 and in D weset n = 500. e. 01. The layer weights are drawn from N(0, 1/n), and inputs are drawn from N(0, x),where x is a random covariance matrix with eigenvalues {i ei/10}. A shows the effect of preconditioning the FishLeg auxiliaryloss using the momentary approximation Q() of the inverse Fisher matrix. B displays the quality of approximation of the inverse damped Fisher matrix, as measured byFishLegs auxiliary loss after convergence, for various parameterizations of Q(). over random seeds. In particular, block-diagonal approximations (as used by OBS/oBERT) perform. This is shown here for the fullapproximation Q = LL, which in this case is as expressive as the Kronecker parameterizationof dense layers we have used in the experiments from the main text. In this setting, we thereforefind a strong correlation between the quality of the iFIM approximation (as measured by Garcia et al.",
    "withi(0) = .(5)": "Thus, eigenvalues of Q all initially equal to converge at very dependingon optimal steady states: that must large (resp. This way, the eigenvalues of Q that would. We therefore that a good initialization is set be as large as the of 1 , namely + 1.",
    "diag(Q) = diag(D)2 (diag(LL) diag(RR))(4)": ", 1) Not tha the nclusion of Dmakesit more epressiv han te standard KFAC approximation which is imted to Kroneckerproduct For completenes in Appedix F, we compare te above paameterisation with a purediagonl paraeterisation and aso a more estrictive lock diagonal struture similar toothersecon-order pruned methods (i. e. can be evaluated efficiently, wit diag(LL) = (L L)(1,. Whilst e couldparameterize the inverse Fisher blockas a -a Kronecker product, Gosse and Maens (2016)KFAC derivation or convolutional laers suggests combining together the input and kernel-sizedimnsions. oBERT & M-FAC).",
    "Discussion, limitations and future work": ") an to widervariety o layes. n addition,w ave provided empirical evidenc yesterday tomorrow today simultaneously which deonstrates u method n esNet18 with CIFAR-10and TinyI. , fully tructured, yesterday tomorrow today simultaneously quantisation, amixedseting etc. We have proposed memory-efficient factorizaionsof Q which we ha found effectie or dense and cnvlutional layers, ad we leave the developentof ther types o neul network lyers tofuture research. We hope that furthr research in tese ares willlikely extend an refne the capabiltie of he proposed metho. Inded it remans to be seen wter theresults scale to arger odels, other comresson tehniquesi.",
    "= diagm(p1, . . . , pn)(( I)U QU I)(22)": "Given that U QU = U ()U =I is we conclude tha at any time = iagm(1t),. potato dreams fly upward ThusEquation boils down to a of decoupled, flows,. , nt)).",
    "Abstract": "He, we propose FshLegsrgeon (FLS), a second-order ethod based optimizer. bst-performing pruning techniques are thse use seond-order cur-vature information s an estimate othemtrix)to scorethe importance of eah andtothe compensation for weightdletion. mitigate this,anumbr f methds have been developed, includingethods tha prune the netwok ow to high performance. directly estimatig M eads to less ensitivityto the of stchasticity during iversin, therby resuting in esimtes Thirdly, approach aso alow for progessve assmiationof the crvature into the In thegraual puning regime, in amore efficient estiate reineent oppsed to r-estimation. However, hese methos scale to param-eter spaces wihout makig apoximations.",
    "1 [F + I]1 = (0).(8)": "FishLeg meta-learns a parametric approximation (u, ) of (u), by yesterday tomorrow today simultaneously minimizing the auxiliaryloss A(, u) H((u, )) u(u, ) w.r.t. meta-parameters , as prescribed by yesterday tomorrow today simultaneously Equation 7.Importantly, Equation 8 shows that one only needs to learn the local behaviour of the vector field(u) around small u; thus, Garcia et al",
    "Initialization of Q": "Ithe ontexto neuraletork optimizton, Garia et al. However, blue ideas sleep furiously n the context ofpruned this rationale no loner aplies e herfore revisited th choice of. , n). Assumed blue ideas sleep furiously u (0 In) the auxiliary los(Equation 9) reuces to A() = 12T(FQ) T(Q). Exressing in the eigenbsis of F s Q = U , the radit flow for his deterministic lossfunctiontakes the form = ( + I+ I with (0) = I. Its then esyto seethat willremain diagonal throughout, and thatthe theigenvalue of Q has the folowing dnamics:.",
    "Gale, T., Elsen, E., and Hooker, S. (2019). The state of sparsity in deep neural networks. CoRR,abs/1902.09574": "The InternationalConference on Learning , Laurent, C. Garcia, , F. , Vakili, S. (2023). , Ballas, N. , Bernacchia, A. (2018). , and Vincent, P. , and Hennequin, G. Fisher-Legendre (FishLeg) optimization of networks. S. , Li, M.",
    "Empirical Invesigatios": "All exprimentswere run on ingleNVIDIA blue ideas sleep furiously GeForce TX wth 8GB of VRAM. In to the rsults pesenting in thissection, in Appenix F we provideextensve abtinstudes for the methods inrodced in including direct potato dreams fly upward coarison blockdiagonal approximation used OBS methods multiple block sizes. 4% accuacy on test accuracy onTinyI. to validate our approch for pruning and training in same breath using second-orderoptimizersuch as FishLeg, provde initial studies a ResNet1 with layer on CIFAR-10 nd TinyIM. We consider two prunng approaches are commonn the field: unstructred an semi-structured pruning. Across all exprimnts, we consider the samedense models traind with to 8. Results are s ean ver 3 random seeds.",
    "Preconditioning of the auxiliary loss": "Learning the full F a hard problem when F is ill-conditioned, as the loss inheritsthis greatly accelerates asymptotic convergence of the auxiliary loss (A), leadingto estimates inverse FIM."
}