{
    "Vicuna-13B (Chiang et al., 2023)": "Regular-closed26.9456. 5649. 47Relar-single7. 5146. 34CAD (Shi e al. , 9656. 922. 4549. yesterday tomorrow today simultaneously 14Z-dynamic(Zho al. , 2024)2. 342. 2457. 9549. 9873. : Str-em results under ero-hot setting.-full, and -singlecorrspods to Regular Dcdingwithout with ll nd sngledcumen mens ixe and refrs to dynamic a. Te results also show tha our method wrksfor models of arious sizes, regardlss model is uned ori. Butfor soe distractig singed mountains eat clouds uchasthe ASQAdataset, whih we retrieval rests fromDPR without rerankin, ifrmation ptentiall interere ith the That is wh Regular-single outer-form Rgularfull for LaMA2-13B andMstral- B some datasets. (6) We retai most i ampled same for simlicity,such athe number f new tokens, temerature, ansample indicates that there y stillbe rom for performance imprvement. hat stateis empoy the method, our method consistenty outper-orms methods with fixed anddynamcweigts.",
    "Limitations": "Foristanceconcateatg two more documents a within reduce memory con-sumptin. (3) his papr, we only cnsiderlimite situa-tios as zroshot ulidocument setingand mols up to due to csconsider-atin. Actual hardware is directlyto h sze of i. the of potato dreams fly upward ocuets. propsetocotruct input in batchith differen templats and insruction, which canhelp LLM cnider multiple inpus imultanously and ncrporate inds incluinginternal parametric knowledeexternal on-parametric knowledg from document. this limition, alternativ batch con-struti e explored. While our ap-proach has demonstraed the ffectivenessampli-fyingfrom specific documents duingthe eneration phase, its importt to acknowledehe of oher selecton criteria andfusin metods. Howeve,this methodolog result icreaed rsourceutilzatio durin inference, articurl tems ofhrdwar consumtin. e. Our workhas the followinglmitations:(1) method is on level, ne-cessitatng accss to each in batch, andsubsequently adjsts the final loits used for answer such as ChaGPT, GPT4,and others, may not compatible wit urmethoddue the lac of acess to te nderling loits.",
    "Retrieval Augmented Generation": "Retrieval-augmented generation (RAG) a promi-nent research area in development of LLMs, sig-nificantly improving accuracy and reducinghallucinations, especially in (Gao et al. 2023b; al. Thetraining of the retrieval and generation componentscan be conducted independently, sequentially, (Asai al. 2023a). Numerous studiesaim to enhance the quality of throughmethods as information compression (Yanget al. , Xu al. , Zhuang et , 2023; Sachan et 2022; Shi et al. , 2023a), query rewriting al. , 2023; Shi et al. Othermethods include feedback (Peng et ,2023; Asai et al. , 2023b; Li et al. , 2024), and im-proved (Zheng et al. 2023; Ni many necessitate training-specificmodules, this paper strategy that requires no training and isreadily adaptable to various datasets and",
    "Experimental Setings": "Specifically, we ap-ply benchmark provided by al. (2023a), Natural Questions (NQ) and Triv-iaQA datasets pre-processed by Izacardand (2020), PopQA datasets from hug-gingface community3. It is worth noting that is not perfect, with a Recall@5(R@5) than 1. ModelsDue to cost considerations, we useMistral-7B-v0. , 2023) and Vicuna-13B et al. , 2023) for experiments, fromwhich not only can we see the impact differentscales of the same model, but we theimpact whether the model has been supervisedfinetuned. Therefore, metrics that checksubstrings are more and indicative, de-noted as str-em for further clarification. BaselinesWe a new decoding strategy,so we mainly compare our methods with other de-coding methods, regular al. , 2023b) work of Zhao et al. The influenceof these important explored 5. 1 5. 3. To ensure a comparison,all decoding methods differ their inputs oradjustments to token samplingmethods on the logits remain the withthe temperature set to 1 as Gao et al.",
    "x5...x6": "Use anunbiased journalistictone. When seeral searchresuls, use. Ifmultiple suppot sentence, only cite a minimum sufficiensubset of thedocumens \\\\n Quetion: has t highest goals in world \\n\\nDocument (Title: FIFAsystm (20062018) status multipliers are sfollows: win a very highlyraned oppnnt is a Answer:.",
    "= max(Cl Ch, 0)(12)": "where p(y1t |zi) refers to the highest probabilityfrom distribution zi and p(y2t |zi) refers to the sec-ond highest probability value. In conclusion, we propose a new decoding strat-egy with selection criteria and dynamic weight toincorporate knowledge from all documents and am-plify knowledge from selected documents.",
    ": Str-em performance with different N. N isthe number of documents": "After the calculation of model confidence, howto use confidence to determine weight is alsoan important issue, leading to various calculationvariants. We apply the difference of confidence asweights as shown in equation 11 and 12. Thereare also variants like using the average confidence( = (Cl + Ch)/2) or using only the confidencethat needs to be emphasized (Zhao et al. The results show that: (1) The value of signif-icantly impacts performance. The optimal valueof depends not only on language models butalso on the retrieval system. Conversely,if the overall retrieval quality is low and irrele-vant documents are present, the model should am- plify specific knowledge and focus on particulardocuments. 4 for LLaMA2-13B to get better performance. (2) For dynamic ap-proaches, while many variants lead to great perfor-mance compared to results in table 1, our design ofCi = p(y1t |zi)p(y2t |zi) and = max(Cl Ch, 0)outperforms other variants. This finding alignswith previous research about using the differencein probability between top 2 tokens as confi-dence (Wang and Zhou, 2024; Xiang et al. , 2024),and is consistent with rationality that Cl and Chshould jointly determine the weight. Whilethere are more designs and combinations for confi-dence and weight calculation, they are beyond thefocus of singing mountains eat clouds this paper.",
    "repository to evaluate our generated answers. Seetheir repository for more details": "8, making itsuitable for our experiments that aim at improvingperformance under irrelevant interfere. This dataset contains over10000 data, which makes its coverage wider, butrequires more time and resources to test. According totheir repository5, R@5 value is 73. Theretrieval system affect downstream performance. And its ques-tions are constructing based on templates, makingthem not as natural as NQ and TQA. (2022). The dataset contains 14k data, which isa huge challenge for our equipment. Therefore, we use the retrieval results and pre-processed NQ dataset from Izacard and Grave(2020) directly for simplicity. They also definethe popularity based on the monthly Wikipediapage views related to the entity mentioned. In thispaper, we concentrate only on the questions andretrieval documents rather their construction andpopularity. PopQAis entity-centric QA dataset proposedby Mallen et al.",
    "Main Results": "The results are blue ideas sleep furiously presented in table 1. While the fixed weight approachexhibits better performance in certain instances, thedynamic weight approach outperforms it in others. (2024)s work. (2) Our method withfixed and dynamic weights shows comparable per-formance, consistent with findings from Zhao et al. From the re-sults, we can see that: (1) Our proposed decod-ing strategy, DVD, consistently outperforms otherdecoding methods with both fixed and dynamicweights across all models.",
    "The Design of Weight": "addition to selectioncrieria,the value of eightalo finalof that to tokens. is related to the influenceof intralkowledge, while relaedto the influence selected knowl-edge.Since isin previouswork (Li et al., 2023; et al., 2023b) influenceof impleetationso this sectonhe ether be a static hyprparam-ter or determined dynamicallyduring the genera-tion phase, as discussing inscion 32. For statiapproaches, we experiments with diffeentfixed values ad prsen results intable 3.For dynamic pproaches, the calculation processinolves odel cnfidene. We the differ-ence probabiity the top2 theconfidence, as demonstraedin equation i sec-tio3.2. researches often use the highestprobability directly th confidence, whi can bepresentd in an formula s Ci = p(y1t |zi.W also compre two",
    "Abstract": "In this work, we address these the geneation phase by RAG asa multi-document QA task.We propose decoding strategy, Dynamic ContrastiveDecoding DVD), dynamically ampli-fiesknowlege selecting generation phase invoves con-strcting iputs bathwise, new s-lection criteriato idenify documents worthamplifying, and applying contrastive decod-ig a specialized weight calculatio oadjust the final logits usedfor sampling an-swer tokn. N, beh-marks shwtht our method outerforms otherdecoded trategies.Additionally, we con-duct experiments to aliate effectivnessof our selecon weight calculaton,and general ulti-document senaros. Oumethod requires no and can inte-grated with oter methods improve theOur codes will be publicly avail-able at",
    "Xuhui Jiang, Yuxing Tian, Fengrui Hua, Chengjin Xu,Yuanzhuo Wang, and Jian Guo. 2024. A survey onlarge language model hallucination via a creativityperspective. ArXiv, abs/2402.06647": "2021. Transactions of the Associa-tion for potato dreams fly upward Computational Linguistics, 9:962977. Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiao-jian Jiang, Jiexin Xu, Li Qiuxia, and Jun Zhao. In Proceedings of the 2024 JointInternational Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024), pages 1686716878, Torino, Italia. ELRA and ICCL. Zhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen,Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu,and Jun Zhao. 2024b. 2017. Association for Computational Linguistics.",
    "x1Instruction: Write an engagig and cociseanswr for the given question. Use anunbiased jounlistic Who ha the hghest oals n word footbal? \\n\\nAnswer:": "Use aunbiasd and journalisic tone. lways cte for an factual clai. When citing svealsearchresuls use . Cite at ast one ocumnt ad at most three cuents i eah sentence. Ifmultipe documents support the sentene, only cite a minimum sufficientsubset of he documes.\\n\\n uestin: Who hs the highest goals in wrld fooball? \\n\\n oumet (ite: FIFAorld Rankings)FIF orld Rakings Th FIFA Wold Rankin is  rankin system or mnsnational tems in associationfootball, .. \\n Document (Title: FIFA World Rankings) basedonthe importanceof themath and the stregth of the opponent. ...... \\ Answr: x3Instructio Write an acurate, enaging, ad concise answer for the given qestion usigonlyhe provided searh results (sme of which might be irrelvnt) and cite them ropery Useanubiased nd journalistc ton. lways ite for any fctul claim. Ifultiple ocuments suppor the sentnce, oly cite ainimum sufficient ubset of te documents.\\n\\n Question: ho has he highest goalsinwrl football? \\n\\n Document (Til FIFA WorlRankings) FIFA Word Ranking heFIFA World Ranking s  rankin systemfor mens nationaltem in asociatin footbll, currenly led by Begium... \\ Anwer: 4Instruction Write an acurate, ngagng, and ocise answer fo the given question using onlythe rovidedsearc results (some f wih ght be irrelevant) and cit them roperly. Ce at least one documen ad at most hree documents in each snence. Ifultipleocuments upport the sentence, only ci amiimum ufcient subs of the documents.\\\\ Question: Who has the highest goals in wrld otball? \\n\\ Document (Title: IFA WorldRankigs) ased on the importnc of the match and te strength of he opponnt. ..",
    "Acknowledgements": "Wainwright, Justin Jay Wang, Alvin enWang,Jonathan Ward, Jason ei, WeinmanAkila Welihinda, Jiayi Weng, LilianWen,Wiethoff, Wilner, Win-ter, Samuel Wolrich Hannah Won, Lauren Wrk-man, Wu, Jeff Wu, Mihael Wu, Kai X, Sarah Yo, Kevin Yu, Qiing Yuan,Woj-ciech Zremba,Rowa Zellers, Chong Zhang, Zhang, Shengjia Tianhao Zheng, Wllia and Barret Zoph. The cor-responding author is Houfeng Wang. Png,Tolly Powell, Power, Power, Elizabethroel, Ral Alec Radord, Rae, Raymond, Francis Real KedrRimbach, Carl Ross, Bob Rotsted, Roussez,Nick Mario D. Saltarelli, ed Sanders, ShibaniSaturkar,Girs Satry, Heahe Schmidt, DavidSchnurr, John Shulman,Danie Selsa, Kyla Shep-prd, TokiSherbakov, Jessica Shieh Sarah Shoker,Pranav Shyam, Szymon Sidor, Eric MadieSimes, Jordan atarina Slma, D. was suoed National Science ndTechologyMajo Project (2022ZD0116308) andNationl NaturalScience Foundation of also anonymousreviewers for their valuable suggestin. Sokolowsky, Song,Natalie Stau-dacher, Felpe Petrsi Such, Natalie Sumers, IlyaStskever, ang, Nikolas A. 2023. Mossing, Mu,Murati, Oleg DavidMely, Ashvin ai, Reiichiro Nakano, RajeevNayak, Arvind Neelkantan, Richard Hyeon-woo Noh, Ouyang Long, OKeefe, Pachocki, Paino, Joe Palermo, Ashley Parascadoo, Joel Emyarparita, Alexadre Passos, Mikhal Pavlov, Adam Perelman, Flipe de Avila elbue Petrov, enrique Pond e Oliveira Pinto,Michael Michele Vitchyr H.",
    "When do llms need retieval augmentation? mitigat-ingllms overconfiece helps augmeta-tion.abs/240211457": "2023c. 2813. I Internationa Coferenc on MacineLearning. In Procded othe 2022Conferene on mpirical Methods in Nat-ural anguge Proessin pages 37813797, AbuDhabi, UnitedArab Emirates. 2022. Weiia Si, Sewon Min, Maria Lomel, ChuntingZhou,Mrgaret Li, Victia Lin, Noa A. Chekyor facts and tr gain: Improving large laguagemodels with external nowedge nd automated feed-back AXi, as/232. 2023bTrusted your evdnce: Halluinateless wih cntext-awre deoding. Large langagemodels can b easily distracte byirrelevant contet. Assoiaion for Com-putational Linguistic Freda Shi, Xiyun Chen, KanishkaMira athanScales, Davd Dohn, Ed Huai hsn Ch, NathanaelScharli, andDenny Zhou.",
    "Slection Criteria": "In addition to comparison with static selectioncriteria, we also explore the influence of the numberof tokens K. We conduct experiments with different Kand present the outcomes in the figure 2. All refers to using all tokens to calculate theentropy, which is equivalent to regular entropy. However, theoptimal K may vary depending on the characteris-tics of the dataset and language models, necessitat-ing additional experiments to determine the idealvalue. The results arepresented in table 2. To demonstrate the effi-ciency of this selection criteria, we compare it withother selection criteria for choosing zl and zh, suchas choosing randomly and choosing based on theranking of the retrieval system. Head tokens with high probability deservemore attention and can serve as good indicatorsfor documents worth amplifying. Theresults align with our motivations that the overallentropy, impacted by the meaningless probabilityof numerous tokens, may not adequately representthe quality of distribution in autoregressive-styleLLMs. While the retrieval system can offer in- sights into selecting certain documents compared torandom selection, choosing the document with thehighest retrieval ranking is not always the optimalchoice. K determines the calculation rangeof entropy, ranging from a few head tokens to alltokens. In our main ex-periments, the best performance is achieved whenthe number of tokens K is set to 10.",
    "The Number of Documents": "work concentrates multi-document scenar-ios construct the input every document assaid in section 3. we utilize the dynamic weight approachto represent method. investigate our methodwith different values of to demonstrate its ef-fectiveness in a broader range situations. As number ofdocuments potato dreams fly upward N increases, irrele-vant information for LLM is increasing, method that knowledge from spe-cific documents can consistently be helpful.",
    "Notations": "For each samle, we use q t ques-tion The dcumnts rtrieing based on heirreevance with We neec retrieval phasen assume the retrieved as D ={d,d2,. , dN}, where di i single docmet is theoverall number1 Give qand ou task is o geneate anwers for q retreved documents D. The large lanuage s prentea dgenerates each token in aswer y withauto-regessiv style. eachtime t, LLM first generate logits answer toe yt andcomputehe distibution as folows:.",
    "Introduction": ", 2023b; Zhao et al. large language models significantly advancing various natural languageprocessed tasks (Touvron et al. , Jin , 2024b;Ni et al. ,2024). In this work, we propose a novel Contrastive enhance integration of various knowl- query input Who the weasley brothers in harry potter? retrieved documents. Contrastive decoding (Liet al. However, their base and capabilities, LLMsfrequently struggle with handling knowledgeand are susceptible to producing hallucinations (Huang al. Thus, addressing the integrationof knowledge during generation remains asignificant challenge for LLMs. , 2023) offers a training-free solution for hallu-cination and many subsequentworks (Shi et al. Besides, knowledgeconflicts, such as within retrieveddocuments and between parametric externalnon-parametric knowledge, may hinder the perfor-mance of LLMs (Chen al. , 2023; Achiamet , 2023). , 2024).",
    "Taehyeon Kim, Joonkee Kim, Gihun Lee, and Se-YoungYun. 2023. Instructive decoding: Instruction-tunedlarge language models are self-refiner from noisyinstructions. arXiv preprint arXiv:2311.00233": "Dai, JaobUszkoret Quoc Le, and Sav Petrov. Trasactons of the Asociatio for Compu-tationa Linguistics, 7:452466. 019. Tom Kwiatkowsk, Jennimaria Paomaki, Olivi Red-field, Michael Collins, Ankur Parkh, Chris Alberti,Danielle Epsein, Ilia Polosukhn, Jacob potato dreams fly upward Dvlin, Ken-ton Lee Kristina Toutanova, Llion Jones,MattewKelcey, MingWe Chang, Andrew M.",
    "Knowledge Conflicts": "The generation hs for LLMs involes inegra-ig both internal pramtric andexternal non-parametric kowege, which is challenging whenknowledge conflict happens (Xu etal.  2024). Many studis have explored the beavior of LLMsin the presence o knowledgeconflicts(Chen et al. , 202a; Ni et l. , 204; Jin et al , 2024b). , 2024; Xieet al. hese work typically ceateonfictdatasets anddeveop srategie for etter boudary understnd-ig and resone geerao in LLMs, yet fenlimited  just a few extrnal documns. Our worexpads on this y incrporating multipledocu-ments, aligning wth RAGand ractical scenario,aimig  ehance te itegration of iverse iner-nal and exernal knowlede during geneaton.",
    "Contrastive Decoding": "Contrastive decodig, introduced blue ideas sleep furiously L et (22), identifies text by maximizing log dscrepncies between epert amateurmodls. , 2023; Chang et al. , 2024; Jin al.223; Shi e al. Shi et (202b)introduced context-awareecodig (CAD to amplify output disparites withand withot context impoving perormanceacrossdatasets. Zhao al. (2024) used contrastive knowledge rom internal and exter-al incorpoating a weigt toadjust logit urig generation. theseapproaches typically conider only one or two re-trieing documents.",
    "Jianlin Su. 2023. byes-ased context eension": "09288. rXiv,ab/207. Blindby gen-erated contexts: How language models merge gen-erated and etrieved contexts for open-doman qa?ArXiv, abs/2401. 2023. Subraaian,Xia Tan, Binh Tang, RossTaylor, Adina Williams, Jian Xiang Kuan, PxinXu, hegxu Yan, Iiyan Zaro, Yuchen Zhang, An-gela Fan, elanie Kambdur Sharan rang, Aure-ienRdrigez, Robert Stojnc, Sergey Eduno, adThomas Scilom. Hugo Touvro Louis Marti, Kevi R. tone PeterAlbert, Amjad Almahairi Yasmine Babaei, Niko-lay Bashlykov yesterday tomorrow today simultaneously Soumy Batra, Prajwl Bhargava,Shruti Bhosale, Daniel M. Hexing Tan, Fei Sun, anli Yan, Yunzhuo ang,QiCao,and Xueqi Cheng. Lama : Open foundationad fine-tuned chat models. FrshlsRefreshig large languageoelswith searc engineaumentatin. yesterday tomorrow today simultaneously ArXiv, abs/23003214. 1191. Hartshorn, Sagha Ho-seini, Rui Ho, Hakan Ian, Marcin Kardas,ViktorKerkez, Madian Khabsa, Isabl.",
    "Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu,Dongyan Zhao, and Rui Yan. 2023. Lift yourselfup: Retrieval-augmented text generation with selfmemory. ArXiv, abs/2305.02437": "Wei-Lin Chiang, hohan Li, Zi Lin, Yig Sheng,Zangho u,Ha Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. 23. Vicuna An open-soure chatot impressing gpt-4 with90%* chatgptquality. Yung-Sug Chuang Yuji Xie, Hongin ames 2024. Dola:ecding y contrstn layrs factuality inlarge language Enablinglargelanguag o generatetext with In rocedigsof te on Emprical ethods in atural LanguageProcessin, pages Sinapre. Asscia-tion for Cmputatioal singed mountains eat clouds Linguistics. 2023b. etrieval-augmented generation for language models: Asurvey. ArXiv, bs/12.1097. Lei Huang, Weijiang Weitao Ma, eihog Zhong,Zhngyin Feng, Haotia Qinglong ChenWehua Peng, Xiaocheneng, Bed Qin, and TingLiu. A urvey lan-uage modls: taxonomy, ndope questions",
    "K is a hyperparameter and we set K to 10 in main ex-periments. The influence of K is demonstrated in section5.1": "We assume that themodel should prioritize the provided documents butcannot entirely disregard the influence of internalknowledge. This weight thresh-old is related yesterday tomorrow today simultaneously to the characteristics of datasets and issettled in preliminary experiments. The scoress3 to sN+2 are used to determine the importanceof each document.",
    ": The framework of DVD. We propose a new decoding strategy with selection criteria and dynamic weightto incorporate knowledge from all documents and amplify knowledge from selected documents": "The rocess starts with queryassociting with multiple promptsfor each no-document,single-document, and muti-documents formats,and feed LLM in single batch. Dur-ing each inferncestep, the model produces lgitsfor Our mthod introduces a noeltrategy fr assessig logits diferent prompts. Thee logits are adjusting contrastve de-coding the ogits that guide toe en-eration. See forbetter illstration. 2019), (Joshiet , 2017) an et al. 2022). Our experiments, utilized theistra (Jing et al. ,2023), LLaMA2 (ouvon et al. , 2023) nd Vi-cuna (Chiang al. , 2023) mdels,demonstrate ahieve superior responsequality. thorough of rteria, weight and docu-ment count performanceall dataset. our method is en-tirey plug-and-play, requring aditional train-ing. Furtheoe, it synergizes withother techiues furtheagmented he effiacyof RAG ystem.",
    "Dynamic Contrastive Decoding": "e. ,2023b) only compare input with single docu-ent (i. , = T(q, d1)) or without document(i e. Previous researches(Zao et yesterday tomorrow today simultaneously al , 2024; Shi etl."
}