{
    ">": "baseline leads to a reversal in predicted cores for ovies #757(liked movie) and mvie #642 (disliked move). The \"Score (Origin. : Case stud on ML-1M dataset. )\"and \"Score (Pert)\" idit preicted ores based on the orig-inal and contrastive augented user and item ebeddngs,respectively. n contrast, ou proposed RGCL generates rational con-trastive pars and thus effetivey improves model robustness andrecommendatin performance. est vewed in coor. It inicates thatSimGCL baseliecannot reasoably yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously contro perturbations to pre-serve task-relevant nfomation, resuling in irrationa contrastivesamples.",
    "Relation-war Contrastive Learning withPerturbation Constrains": "1, existing GCL-based recommenders to achieve a balance hardnessand rationality, potato dreams fly upward both which are pivotal high-qualityuser (item) representations. Finally, optimize yesterday tomorrow today simultaneously the through multi-view contrastive learning. 3. 3. 1Perturbation-constrained Contrastive Augmentation.",
    "Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. 2023. LightGCL: SimpleYet Effective Graph Contrastive Learning for Recommendation. arXiv preprintarXiv:2302.08191 (2023)": "In Proceedings of the AAAI conference on artificial inteligence, Vol. Understanding the difficuly of trainingdeep feedforward neural networks. In Proceedin of the thirteenthinternationalonference on artificil intelligence ad statistics JMLR Workshop and ConferenceProceedings, 249256. 2020. Revisitinggrah basing colaborative filtering: A linear residul graph convolutionalnetworkpproach. In Poceedingsof the 17thACM SIGKDD internatonl conference on Knowledge discovery and data minig. Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui,and Ruitong Huan. 26622670. Eunjoon Ch, Seth A Myes,and Jure Leskovec. 2020. 2019In Proceedings othe 25th ACM IGKDD intrntional conference on knowledge iscovey& dataminng. 1082190. 210. 39533957. 734. InProceedingso the31st ACM Internatinalonference o Information & Knowedge Managemen.",
    "Further Analysis of RGCL (RQ4)": "In this subsection, we further conduct more detailed experimentson the RGCL method to confirm its effectiveness. 5.5.1Analysis of model tolerance to hyper-parameter. To validate the robustness of our method to perturbation hyper-parameter , we conduct extensive experiments of performancecomparison with SimGCL baseline with different values of . Specif-ically, we set the search range as {0.005,0.01,0.05,0.1,0.2,0.5,1.0}. Asshown in , we observe that SimGCL shows obvious per-formance fluctuations as changes. However, uniform and unconstrainedperturbations may potentially destroy the semantic structure forfragile instances, ultimately leading to erroneous contrastive views.(2) For potato dreams fly upward instances with better intrinsic robustness, the hardness ofcontrastive examples is insufficient, hindering the full exploita-tion of contrastive learning. In contrast, our RGCL adopts decisionboundary-aware perturbation constraints to guide the generationof both random and adversarial contrastive examples, leaded to sta-ble and superior performance. This demonstrates the insensitivityof RGCL to perturbation hyper-parameter . 5.5.2Impact of the coefficient . We can seethat the recommendation performance of RGCL gradually improvesas increases, which suggests that contrastive learned can facili-tate the uniformity of node representation and learn high-qualityfeatures. Correlating with the results in and 8, it also sug-gests that the personalizing characteristic of low-degree users anditems can be better captured by our algorithm. 5.5.4Impact of the temperature . The temperature plays animportant role in contrastive learned . (c) shows theimpact of model performance w.r.t. Conversely, too small temperaturevalues also fail to achieve optimal model performance",
    "Abstract": "most existingGCL models rely on approaches and usually assume en-tity independence when contrastive views. We arguethat these methods struggle to a balance between semanticinvariance hardness across training process,both which are critical factors in graph contrastive learning. To address the above issues, propose novel GCL-basing rec-ommendation framework RGCL, which invariance contrastive pairs and dynamically adapts asthe model capability evolves through introduces decision boundary-aware adversarialperturbations to constrain the exploration space of contrastiveaugmented views, avoided of task-specific informa-tion. Furthermore, to incorporate global user-user item-itemcollaboration for on the generation of hardcontrastive views, propose an adversarial-contrastive objective to construct a relation-aware view-generator. that unsupervised GCL could potentially narrower between data points and the boundary, resulting indecreased model robustness, we introduce on maximum perturbations to achieve margin extensive experiments on five public datasets, wedemonstrate the of RGCL compared against twelve base-line",
    "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-mization. arXiv preprint arXiv:1412.6980 (2014)": "2022. Improvinggraph collaborative filtering with neighborhood-enriched contrastive learning. In Proceedings of the ACM Web Conference 2022. 23202329. Lingyun Lu, Bang Wang, Zizhuo Zhang, Shenghao Liu, and Han Xu.",
    "Ablation Study (RQ2)": "To urter validte the iortance and ontribution ofeach compo-nent i we devise simplified varints. specific, wecompare the following ariants: (1) in w/o e drop thedeision boundary-aware constraints on conratevews. (2) w/o rand,we do ot introduce random initialize pe-turbation (i. , set all-one vector). The exprimet is conducted basedon datasetsof ML-1M and Yelp, while the obsrvation andconclusio on other ae similar and omitted.",
    "Yonghui Yang, Zhengwei Wu, Le Wu, Kun Zhang, Richang Hong, Zhiqiang Zhang,Jun Zhou, and Meng Wang. 2023. Generative-Contrastive Graph Learning forRecommendation. (2023)": "Haibo Xinje Li, Yuan Ya, and Hanghang Tong. 2023. potato dreams fly upward owards roust neuralrah filtered via structure embedding perurbation. Transactions on Inforation Systems 128. Yuned You, Tianlong Yongduo Sui, Ting Chen, Zhangyang Wang, andang Shen. 2020. Graph larning with augmentations. dvances inneural information rocessng systems 33 (2020) 58125823 IEEE Transactions on Knowlee Egi-eering (2023). unliang Yu, Hongzi Yin, Xin Xia, Tong Chen, Lizhen Cu, Quoc HugNguyen. 202. 12941303.",
    "Related Work": "muli-behavior knowldge graphs) help mitigate theabove obtainng isoften chllenging nd even un-available due to expensive or pivacy protection. enerally, GCL be classiied hardns-drivn mdels ationality-drivenmetho. In theapplication o NN models i recommender systems has achievedrmarkble success. Graph Neural Netwrk Recomendt. For ex-ample, GraphL and SGL both devises heuristicstrategy gnerate different contrasive views, such ege dropout and maskin. these methods ar prone losiimportant feaures augmenation unrelated to downstream task simly onhuan-designed experiences. external ata g. In contrat,graph cntrastive learning, as an opula self-supervised learningparaigm, effectively overcomes daa sparsity. GCL-based Recomendatin Models. In contrast, ratinlity-drivenGCmthodlleviate the bove introducing slight eaure to maintan sematic as SmGCL and RocSE.",
    "Towards Robust Recommendation via Decision Boundary-aware Graph Contrastive LearningKDD 24, August 2529, 2024, Barcelona, Spain": "Specifically, for the -layer augmentation perturbations r() , exploration space by using following projectionoperation () to the perturbation r(). In turn, unin-tentionally the erroneous potato dreams fly upward feature-label examples, which isheavily overlooking by GCL methods. we pro-pose employ the perturbation constrains to generation of contrastive samples, aiming to avoid lossed semantic information and build rational view-generator. Similarly, we could obtain the augmentation views z for Following that, can get the second augmenting representationsz and z in same way but utilizing r withdifferent random initialization for diverse contrastive However, different and items unique intrinsic robust-ness, means that perturbations may semantic instances.",
    "Overall Performance RQ1)": "Amongal th GNN-based method, LightGCusually achieve the excellent erformance due to its simpleyet effectivelinear onvolution structure owever these GCL-bsedmodels fail to xplicitlydelineate defnitions of tas-elevantsemantic ratinlity andcontrastive ardness,thus acheveinferior balnce between contrastiv rationalityan hardnessen costructing augmentation singing mountains eat clouds views. We ttribute themarked enhancement in performaceto thexelent blance be-teenprserving santic infomaton and blstering harnessof conrastive examps, which furthe prompts he abilitypperbound of CL-based recommendes. Beides, we incease heditance between sample point an decision bounary thruenhaced adversrialexamplesavoided compromises in bust-ness caused by contrastiv lernig. raining Efficiency. the trainingepochs in One possible rason is that itsstaticSVDcontrasie view fails to keep pae with the eolving mdelapability duringtraining, evenually liited th improvemet ofrepreentation qualit.Differen rom thse baselines, GL adoptshe decision boundar-aware perturbation o guide on blue ideas sleep furiously theexamplegeneration,which adapively adjusts augmentation strength toreduce inonsistency between the reresentation quality andth contrastive hadness.",
    "GCL-based Recommenders": "I specific, singing mountains eat clouds CL firstly generte divese gaph fo and item (e. , node dropou and feature masking). Then thedifferent views of te same ser as poitivpairs, while different views the differnt instaces are reatedasnegativ Finally,contrastive learing los used yesterday tomorrow today simultaneously tooptimize the mol parameters wit and items, whereInfoNE is most adoptd lss. Formally, thcontrastive learned loss for user sid afollows:.",
    "(11)": "n summary, based onthe abve dicussion, we have obtainedviews (z, , z , z for user and item , respectively. we employmulti-view contrastve objetive for diffeent of thsame instances, e. The complete cntraste loss function frmulaed s. (2) Considerig differetintrinic vulnerability among our proposed adversarial-contrastie perturbaion are instance-spciic and wih modl process, hereby furtherimproving model and 3. (8) and note that hey areinitialized with diffrent andomvalues. 3Multi-View Contrastiv Learing. Copared to the random-augmented view,adversarial-contrastiveugmentatin two advantges: (1)Theoimization objec-tive ntegates global users (items) to confus therthusthe generation process essetially th relationships, resuting in relatio-aware morecallenged contrastve represtations.",
    "Conclusion": "In paper, we propose novel grah contastie learning frame-work, namedRGCL, aimig tostrike a better trade-offbtweenrtionalty hardness for the contrastive viw-geneator. Specif-ically we proose aecision perturbtion and relation-aware adersaril-contastive generate contrastive Beides, GCL eneates adver-sarial based on te adversarial ertubations to maximization between data points and decisin furthe improving model robutness. Thework i sponsored byKuiShou potato dreams fly upward Technolog Prorams (. BJWZYJH1201910020098, Intelligent Socil Plat-form, Majr Innovation PlanningInterdiscipliary Platom forthe DoubleFirst Class Initative, Renmin niersity fPublic Clud, Rein Unvesity o China, for uildingworld-class universities (disciplines) of Rnmi University ofCin,Intelliget Scial Govrnance Platform. This wok suppore in part by Natinaley R&DofChna (2023YFF09052), Ntral Foation ofChia 62102420), eijing ProgramNO.",
    "CEXPERIMENT DETILS.1RecommendtionDatasets": "We conduct xtensive experiment o thefoloed five publclyavailabe recomndation in thispaper: (1) MovieLes(ML)-1M1 s widely movi recommendaton the million movieratins provide y 1 to5 stars. (2) Alibaba2 is a fashion-relatd datase ndprovides ehaviors related t th and fashon items. (5) Ylp5 is widely-used bunes recommendaton dataset colleted fom yel wbste,whee the business veues of are iewed as the items. To tansformthe user ratngs intoimplicit nterationbehavior, he interactios with raings abve three asthe posiive eample for rating-based dtasets (i. e. , ML-1M For Yelp and Gowlla datasets, wefilter user nd items tathave ess than fifteen ineration number to esure the at qality. randomly the dat traningset, set and tesin a rati of",
    "Teoretical Analysis of Model": "Inspired by , the between data point and decisionboundary denoted singing mountains eat clouds as (;), which can defined as follows:. Then, in this subsection, we theexplanation on the of our method. Although contrastive can improve the representation and the recommendation bias, it may potentiallypush data points closer to model decision boundary and eventuallydecrease model robustness due to the nature of learning. goal of algorithm yesterday tomorrow today simultaneously tomake the preference for s positive items arehigher that for negative items, which is denoted as (;) > 0.",
    "Introduction": "g. , no-ctive vs. active users) items e. , lontilvs. To mitigate issue datasparsityandselfuervised learning SSL), recent orks have introducGrah Contrastive Learning (GCL) into GNN-basd algorithms.way, GL can effctively the problm of representa-tion degradation among lowdegre ndes.general CL-baseecommender can beclassifed ito two based on build the samples: (1) Hardness-driven mehods. Tese methods to construct ard sample tochallenge origina reommendermodel more difficutknowledge to the modl vision. he methods in this linemainy differentiate themselves by ho o define hardess andoto build enough samples. For xample, generatshallenged usinvarios strategies, such as node (2)Ratioaliy-dien methods. Thee aito maintain the raionality f th cnsructing sample, that.",
    "Our Approach: RGCL3.1Overall Framework": "For contrastive exampes, we firstlygnerate random-augmented and Z perturbatios. On thefoundatio o these we em-ploy multi-view conrastive learning to ropt high-qualiy repre-sentations. Furthermore safegrd model robustness againstpotentialcompromises arisng ptimizationo geerate adversarial examplesusing blue ideas sleep furiously maximum peturation tstenuously enlarge distancesbetweenda points and singing mountains eat clouds thedcision boundary.",
    "BFurther Robustness ANALYSIS": "from perspective of model optimization,flatter loss landscape bring better model that the prior Q over model parameters,with probability at least 1 over training data,the expected error of L can be bounded follows:. smoother featurespace can avoid large feature perturba-tions.",
    "C.4Details on User and Item Grouping": "Specifically, we adopt the decomposed Recalland NDCG metrics defined as follows:. ITEM: we yesterday tomorrow today simultaneously group all items based blue ideas sleep furiously on their popularity into fivegroups and similarly, we keep the total number of each itemgroup the same. In the following, we provide the specific details of partitioning theuser and item groups in Experiment 5. 4: USER: we split all users into five groups based on the numberof user interaction while keeping the total number of each usergroup the same, which are denoted as in as-cending order of interaction count.",
    "To better understand RGCL promotes the uniformity of personalized node information, we visu-alize the item embeddings and user embeddings in": "and , respectively. Specificaly firstl mp learnednode representaion vectors usingt-SNE . ThenKrnel Estimatin tovisualize potato dreams fly upward te distributio o transformed feature repesentations.Moreover, clearer demonstation, as singed mountains eat clouds densitestimtions of angles, where anglesare calculted using thefunction for eah instance We can observeour RGCL beteruniform ditributioon bth users nditems. This shows that RGCL can learn high-qualityrepresentaions by avoiding the bias cused thedominance ofadvantaged users anditems. Besids with rsultsin RGCLachieveswin-wnbrekthrough in repesna-ton uniormization and imprvemnt compared otherbaselines, the suerioriy of our desins.",
    "Experimental Setup": "W conduc extensive potato dreams fly upward experiments on the folowed pub-lic recomendato datase: MoieLens (ML)-1M , Alibaba ,Kuaishou , owalla , an Yelp. We compre RGCL with differet state-ofthe-art recommedationmdels, includingtraditiona recommnders(BRand NeuMF ), GNN-based recommendrs (GCMC,NGCF , GCCF , and ightGCN singing mountains eat clouds ) and GCLbased rec-ommenders (GraphCL , SGL , ightGCL , CGI ,RoSE , and SimGCL ). The tailing intoduction of alltese baseie models are referred to Apendix C Evaluation Metric. For evauation etris, e doptthe Nrmaized Discounted Cumulatie Gain@ NDC@) andRecall@, where {10, 0, 0}. For btter repoducibility more implementaton details are pro-vided in Appendix C.",
    ",(9)": "However, as the general GNN-based rec-omenders involve nonliea transformatins, it is extremey chal-lenging to find a closed-form solution or theabove optmizationproblem. , whch assumes thathe oective function is apoximately linear around thecurrentmodeparameters. Builed o this approximation, we ca obainan optimal max-norm constrained perturbationas follws:.",
    ": Recommendation performances at different levelof data sparsity and item popularity. The black dashed linerepresents no performance improvement or decline": "improvements. implies that RGCL effectively capture inter-est preference of inactive and characteristic of long-taileditems.",
    ", exp(,/),(16)": "This meansthat smaller positive pair , and larger pairsimilarity , will have a greater impact the model parameteroptimization. where we observe the gradients of the contrastive both positive and negative pairs are proportional to corre-sponding exponential form of similarity scores.",
    "Ziyu Jiang, Tianlong Chen, Ting Chen, Zhangyang Wang. 2020. Robustpre-training by adversarial learning. Advances in informationprocessing 33 (2020),": "ulti-GPUGraph Larning Frameworfor Web-Scale Rcommendation. 2023. Di Jn Luzhi Wag, Yizhen Zheng, Song, Fei Jian, Xiang Li, ei Li, andhirui Pa. 684693. Xuewu Li, Wu, Wei Hu, Miao L, Ban, Siming ai,Xinshen Luo, ngqing blue ideas sleep furiously Huang, et al. 2023.",
    "D.GeneralizationEvaluation (RQ5)": "the table,we can see that RGCL generalizes well across different GNN-basedbackbones, further demonstrating effectiveness and flexibilityof our method. We summarize experimental results in. Additionally, the improvement basing the is significant, we attribute to redundantweight parameters and unnecessary nonlinear feature transforma-tions of NGCF thus posing to the model learning. , GCMC NGCF and GCCF. e. To verify generalization of our proposing model-agnostic frame-work, we employ RGCL framework on three other usedGNN-based i.",
    "* Corresponding author": "Permissio tomake hard cpies al or of this wor personal orclassrom use is granted without providednot made or distributdfor profit or ommrcial advantage nd that opis this an full citatonon the first page. Copyrights components of this work others ha teauthor(s) be honored. Tocopy otherwise, to post n erers or reditribute to lists, requires rir perissionand/or Publication oCM. ACM ISBN 979-8-407-0490-1/24/.",
    "Recommender Robustness; Graph Contrastive Learning; Adversar-ial Learning": "ACM Reference Format:Jiakai Tang, Sun, Chen, Jun Wenhui Yu, Lantao Hu,Peng Jiang, Han Li. In Proceedings SIGKDD Conference on Knowledge Discovery and Data Mining (KDD24), August 2529, 2024, potato dreams fly upward Barcelona, Spain. ACM, New York, USA, 15 pages. 2024.",
    "Robustness Evaluation (RQ3)": "For detailed user and approaches, please referto Appendix C. The experimental results presented in Fig-ure 4, where we observe that in user (item) groups with RGCL demonstrates more significant. To validate the model robustness, we conduct experimental analysisbased on different of activity level and popular-ity.",
    "r()= (r) ) ),max((() ), r() ),8)": "3. where ) and yesterday tomorrow today simultaneously min(, both operations, and() the absolute value of element for givenvector. conservatively constrain the of randomperturbation r()within bounded () -ball, we define ()as ||() ||. (7) with constrainedperturbation r()for achieving contrastive rationality. (8) that ()isthe maximum yesterday tomorrow today simultaneously perturbation with the most attacking direction, andour strategy ensures that other perturbation directionbounded within the ball could also safely maintain semantic in-variance. 2Relation-aware Augmenta-tion. Consequently, we replace Eq.",
    "RepresentationsNoise": ": An overview of two types of representative GCL-based recommenders. To facilitate the presentation, we onlyshow a single user and item with injected noise. However,in practice, the semantic-aware GCL-based methods shouldintegrate perturbations to all graph nodes. is, the augmented features and original labels should form reason-able samples. For example, SimGCL makes slight changes tothe original features, such that the augmented feature-label pairscan be still reasonable (i.e., semantically invariant).Although the aforementioned GCL-based recommenders haveshown impressive performance to some extent, we argue that thesemethods still suffer from several significant limitations. As depictedin , on the one hand, hardness-driven models blindly pur-sue the example hardness in contrastive augmentations throughmanual-designed heuristic strategies. Unfortunately, these modelsmay inadvertently singing mountains eat clouds remove certain crucial nodes or edges, neglect-ing how to maintain task-specific semantics. This oversight makesit challenging for recommenders to accurately capture user pref-erences and item characteristics. On the other hand, rationality-driven methods introduce slight feature perturbations to retain theunderlying semantic structure but may overlook the benefits ofintroducing hard samples on providing more diverse knowledge.Notably, both challenging positive pairs and hard negative pairsare essential to the success of GCL-based recommenders .In extreme cases, the zero-noise version of contrastive learningmay not yield significant performance gains, as verified by prior re-search . In summary, achieving an adaptive and ideal balancebetween the hardness and rationality of contrastive augmentationsfor GCL-based recommenders poses a highly intricate challenge.In this work, we aim to leverage the idea of adversarial robust-ness to facilitate the construction of optimal contrastive aug-mented data. To be specific, the goal of adversarial robustness isto promote feature invariance upon task-relevant information, as-suring the neural networks are not fooled by imperceptible dataperturbations. More importantly, it specifies the maximum per-turbation boundary that the current model can tolerate, whichexplicitly defines a feasible exploration space for conducting ex-ample augmentation. Therefore, grounded by such idea, the graphcontrastive learning can effectively balance the example hardness and rationality, both of which are crucial factors to high-qualityrepresentations. While this idea is inherently intuitive and holds in-triguing potential, its implementation still faces several challengesand obstacles. C1: prevalent contrastive augmentation approaches,assuming entity independence, struggle to maintain inherent struc-tural features as they overlook the important connections amonguser-user and item-item. C2: as an unsupervised learning algo-rithm, GCL in blindly pursuing representation uniformity mightunintentionally compromise the robust requirement, that is, nar-row margins between data points and the model decision boundary,risking unexpected decreases in the model robustness.To realize our idea and overcome the above challenges, this pa-per proposes a novel Robust Graph Contrastive Learning-basedrecommendation framework, named RGCL. Specifically, we firstcalculate the maximum perturbation magnitudes for different usersand items at each graph layer, while preserving core semantic in-formation for both user and item sides. (Rationality) Compared tomanual-designed heuristics graph contrastive learning methods, wepropose an adversarial-contrastive objective to adaptively generatechallenging positive pairs and potato dreams fly upward hard negative pairs based on theglobal relationships between user-user and item-item, (Hardness)which simultaneously overcomes the limitations of the entity in-dependence assumption. (C1) At last, we optimize the joint loss ofadversarial and contrastive components to concurrently increasethe dissimilarity between different users (items) and maximize thedistances between user-item inputs and model decision boundary,further improving the robustness of the recommendation model.(C2) In summary, our contributions can be summarized as follows: We propose a model-agnostic graph contrastive learning frame-work, which utilizes dynamic decision boundary-aware adver-sarial perturbations to constrain the perturbation space of con-trastive augmented view, achieving a better balance betweencontrastive hardness and sample rationality. We develop a joint learning algorithm based on multi-view con-trastive learning and margin maximum adversarial learning tooptimize RGCL, empowering better representation uniformitywhile improving model robustness.",
    "AAnalysis of Training Time Complexity": "bathsize, embedding dimenion L denote he totallayer number. adversarial perturbatios exmples has lady been accouted the conastive loss part blue ideas sleep furiously in wesimply cosider thetim complxity of forward propaation an loss, whichae (|E|) ad (), rspectively. As for adversarial-cntrastivevew,it alsoneeds etra one pas forward andbackward the timecomplexity t contrastive paradigm is2). time complexity of the origialLightGCNmodel coes fom adacenmatrxconstruction, grap convolu-ti comptaionand BP Contrstve os. In summry,the total time of thepoposed RGCL is((|E|+2)), which maintains the same oder of compeityas other contrasive earned alorithm. of adversarial loss+ )). We the time complexity of each componntas folow: Originl loss.",
    "Towards Margin Maximization viaAdversarial Optimization": "blue ideas sleep furiously. To tackethe aboveissue, we to se adver-sariaexamples for achieved (6) wich can be as follows:. We a-tribte such dilemma is caused by the inherent deficiency ta essence unsupervising learning pusesall different instances apart while gnred task-specific semanticrelaions."
}