{
    "Limitations": "Inferring the latent point correspnded to e unseen tes poit entails reezed the model ad vriatonalparametes post-trainig ad re-optimizing th ELBO objecive subject to (z) with the new data pont(sincluded (see lgoih ). Note thatthe presence of an encoder acts as a contraintin th oeln there isan inheren trae-off in terms offasertet inference and arginally weaker reconstrucons /prdiction. The absnc f an encoer complicts the tet-time inferene as there ino dtrministic wy to accss the latent points z corresponded to the new unsen observation (x y). Data space smilarities canbe preserve by including a backonstraint r encoder that additionaly maps the daa to latent space(awrene & Quionero andela 2006; Bui & Turner 2015). The encoder networusbe apableofhandling arbitrariy missingdimenions. This mans that pointscle i latent space will b close in ata pace but not he other way round. Thecomination of a st coder (o singing mountains eat clouds process mising dimensions) and anonparametric decoehas nt appearing in the literature to the best o our knowledge. Asimilar aproach an be straigtforwardly aaptedfrm or shared latent sace setup wit Gausian rocessdecders. 2018a;b) addres this challnge in the contet ofVAEs where singing mountains eat clouds each oservation is augmented with a columnindex indicting the obsved diension;teencodenework hen processsthes tupls as a et with a perutaio invarant set encodng funtion. We ar urently working on incororatinths feature into u famework. Methods like the partial AE (Ma et l.",
    "nlog p(zn)(5)": "Latent point estimates {zn}Nn=1 be learnt along and variational parameters ( Z, md, of the Eq. (5). an important constraint is that this formulation assumes asingle-kernel matrix (single set of kernel hyperparameters) underlying all the independent GPs D. In section, we introduce the concept of an observation space dimensions L, which can using an independent stack of Gaussian processes (GPs), fl. GPs are equipped learn theirown of hyperparameters for added flexibility sharing the latent embedding Z and inducing inputs Zto capture correlations the different output spaces.",
    ": Experimental configuration to reproduce experiments in section 3 of the main paper": "ll and ran the mini-bat lop a batch potato dreams fly upward of 128 iterations on an Core iprocessor with GeForce RTX ith 8GB RAM memory In to give an estima of the scaleof mode forthe 22k yesterday tomorrow today simultaneously dtset eclose asummary sapsho of the of trainble prametrsinour",
    "Computational Cost": "The trained of the canonical stochastic variational is dominated by number of inducingpoints O(M 3D) (free of N) where M N and the data-dimensionality (we have D GP mappings oneper dimension). The ofglobal variational parameters be updated in step (parameters of q(U)) is MQ+M(D+L)+M 2(D+L),where MQ the M Q-dimensional inducing inputs Z M(D+L) is the size of the mean parametersof the inducing variables and M 2(D + L) are full-rank covariances of the variables. The local parameters Z (the embedding shared across GPs) are of size NQ and modelhyperparameters (kernel hyperparameters) are of + which account for Q input lengthscales, a variance variance GP group and {fl}.",
    "BExperimentl set-u": "For each thedatasets, repeat every experiment with five random seeds yielding different splits the training data. We a potato dreams fly upward learning rate of 0. 001 across.",
    "l=D+1log p(yl|y, Z)(14)": "With sparse GPs each of the terms in the can be bounding Lx, the inducing points Z can be between the terms yielding the jointevidence bound.",
    "Reconstructing missing spectra": "We obsrve a partia windw of the spetra in eah plot (given y he shded regio in), hence the latent variabs corresponding to thes poins ae only iformed by te obsered region. Wehen econstructthe whole spectrum from te latent aribles informed by the partial specraRecstruction entails the following infeene step: Xprtial Z Xfull. We notetha thequalty of the mean peictoneteriorates compae to the fuly bservdtet point predctions. Furthermoe, if model is gven a spectral region with verylittle infrmatin (e. g. , inthe botompanel o , the shaded region contansiformation ony aboutte quasars contiuum), he uncertaintiesincrease significantly, as ne would expec.",
    "DSensitivity to SNR": "Th triangularscatter denotes a density imbalance: mor data points are clustered SNR values 1015) comparedto higher NR values (>20). Sinc is biasd towards lower SNR quasars, model generalises higherSNR examples, for luminosity Eddington ratio is likely tt th pattern missig issimil among bigter ojct ann turn aong it difficult for the toadapt to this shift at tes ime to the acute. There very SN (>40) in the dataset further, thesquasars also have more pixels than quasar at lowerSNRs. In we essentily plot the SNR data poit (test quasars)vs. The weak positive crelation can be btter in of the count and issig pixels ()in eah bin. Thereappr to be  where absolute error sysematically r decreasesith is aak negativecorrelation (more discnable inthe in nbottom row)for black hole mssweakpositive for th luminosity.",
    "Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012": "Jan-Torge Schindler, Feige Wang, Aaron Frederick B. Davies, Bram Venemans, Silvia Belladitta, Fabio Vito, Emanuele potato dreams fly upward blue ideas sleep furiously Paolo Farina, Irham T. Andika, Xiaohui Fan, Roberto Decarli, Masafusa Onoue, and Nanni. Optical and near-infrared spectroscopy ofquasars at z > 6. public release and composite spectrum. e-prints, art. arXiv:2406. 48550/arXiv. 07612.",
    "Experiments": "In this section, we demonstrate a range of experiments at assessing the reconstruction quality of quasar spectra and attributes, as well the robustness uncertainty quantification negative log predictive density (NLPD) of test labels log p(Y |Z) under the predictivedistribution where has informing by both modalities (spectra labels) and spectra. data using in work are quasar spectra observed part of Sloan blue ideas sleep furiously Digital Sky Survey (SDSS)DR16 (Lyke et al. , 2020). We chose all quasars with spectra have potato dreams fly upward signal-to-noise ratio (SNR) perpixel > 10, which results in total of spectra. The observed spectra are into the restframe wavelength space, re-binned onto a common wavelength and to unity at around2500. We mask that might arise in the spectra to foreground to the quasar, are thus not intrinsic spectral features of the quasar. measurements were previously uniformly determining & Shen (2022). , 2022) treats multiple observation spacesusing of independent GPs learning a single set of kernel",
    "Methodological extensions for future work": "Bui &Turner (205) propose the cannicalGPLVM encoderand train i with blue ideas sleep furiously terefor, a natural extension is to it for theshaed setting. The te datacontext presented hereopens up questins the partially observed alng with full observed ones, and mased encoder (He et al., 202) migh ea relevat paradigmt explore.",
    "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine LearningResearch, 9(86):25792605, 2008. URL": "doi: 10. Davies, Emanuele Farina, Jan-Torge Schindler, Eduardo Baados, Roberto Decarli, Anna-Christina Eilers, Richard Green, Hengxiao Guo, Linhua Jiang, Li, Bram Venemans, Fabian Walter,Xue-Bing and Minghao Probing Early Supermassive Black Hole and Quasar Evolutionwith Near-infrared Spectroscopy of 37 Reionization-era Quasars 64. and Shen. 3847/1538-4357/ac2b32. A Catalog of Quasar Properties Digital Sky Survey Release 16. doi: Yang, Feige Wang, Xiaohui Fan, Aaron Barth, Joseph F.",
    "have been constrained on scalability and examine less than 50 astronomical objects. We demonstrate ourframework on datasets over 400 larger. We summarise our key contributions below:": "We seamlessly account for issingdimensions both at raining and test time due to the poabilistic atur of the mode. In this way we inirectly model blue ideas sleep furiously reatnships andcorration structure btweenthe dfferet oservain spaces. We demontrate this oncretely wth aneperiment where we gnerte/pedict all scientic ttribues a test-tim by sed latentriabs(Z)only informed by the quas spectra (),we can enote this cross-modal prediction as X Z Y. Tothe best of our knowledge, predicting ultileoutputs with stochstic varitional GPs orscalabiliy ismethodolgically novel",
    "Geneating spectra synthsized labels: an abatio study": "Concretely, we siulate atificil labels by systematicaly varyng only of the lbelswithin a rng in plot The ange of vriatin forlbel is suarizedbythe colorbar eah for instane, the blackmass simulation we generate100 spectra (X)coespnding tosimuated abels ) of back masses log10(M/M) in the range [7. 10.0]; in order th inluenc other labels (redsift, boomtric luminosity, te Eddingon onkeep valuesto meanvalues computedtrining dataset. we can summaizeth inference as the inverse the previous YZ X. abiliyis an strngth of Reassuringl, the model exhibits theexpecte behavour. For istance, emission lnes ae frquasars with black hole masses, the spectral depedence wt bolomeric luminosityshows . in the Mg II line at the wll-known effec (Baldwin, 1977) which dicates tha quasrspectra a equivalent width of and optical emision lines wih increasng bolometricluminosity. In additio, gay the pedictioninterals the10 spectra at eachdmensin. uncertainty intervals wider a higher waveengths, as there is a mssing pixl in tainng data at thosewavelengths; hence, the ncertaint abot thegenerated spectra.",
    "{": "Sharing GPLVM with multipe oservation spaces. In the fgre above we ssume Q = 2 (for eas of visualisation) since we potato dreams fly upward the GPs aretwodimensionasurfaces, however, ypically Qcn highe 2 higher dimensioal GP. bocks on the denote observation spacs (,Y of specra an scientiic labes respectively. In thecenter are Ps nefr eachsace whih he datageneration process through the sharedltent space.",
    "Published in Transactions on Machine Learning Research (Jan/2025)": "The orm (notsudied here) induces a partitioned of the laten pae into imnsion whihwould cater specifc views fthe datae modelled individual kernel functions. Another ension related to strucure of the kernels,sice the kerels ctors over dimensions and areclosed over the multiplicative operaton,one i o select comoite product kernel over dimensionsof th laent space sets of GPs ith theirown kernl. partitoninghe atet space and modellingwith or composite could b benefiil spcific settings.",
    "(Note that the gradients of Lx and Ly with respect to Z are 0 and the only terms that are optimised are the additionalterms corresponding to the new data points.)": "(lvarez e a. , 2010) wherethe {xdz)}Dd1 a mltidimensinal, coninuous Gaussian distributedorresponding to z , 2012 The intrinsic and linear co-regionalizatonmodels (Goovaers, & 1976)are poplar aprach i potato dreams fly upward which each output is modelled a weighted sum shard latent functions te number of pocesses s smaler than the number of ouputs enabling efficincies. osome exten, multiaklearnin GPs can be viewed s n of multi-output learning were wewanttoavid tabula raa learning for each and evole afrmework fo shaing betweenultipe tsks e al. 2007). This approa,while asy toimplement, is limited in its ability to jointl mdel the In the paradigm, thestaring pointis a highdimenional data N The proess aiable singing mountains eat clouds model (GPLVM) opeates like a muli-ouu model dault where eachcolumn he data is modelled y an indpendent G on same shared set of inputs, krnel functio.",
    "Bradley M. Peterson. Reverberation Mapping of Active Galactic Nuclei. Publications of the AstronomicalSociety of the Pacific, 105:247, March 1993. doi: 10.1086/133140": "G. Alexei. doi: 10. Kirshner,. Peter halli, Alejndro Cocchiatti, Diercks, eter M. 10510004-6361/20224411. 1086/3049. yesterday tomorrow today simultaneously. Sacci, G. Treoloni,and C. E. Hogan, Sarabh Robert P. Schomer, R. illiland, J.",
    "Algorithm": "Let Ly denote the ELBOs for of yesterday tomorrow today simultaneously the observation spaces and let the ELBOs a randomly drawn mini-batch the data (across all dimensions).",
    "On benchmarking a traditional regression approach": "The dependency between the spectra and labels is modelled through the shared latent space, which generatesboth views of the data. Without a generative model it would not be possible to generate spectra correspondingto scientific labels (generate X corresponding to Y ), we would only be able to predict labels based on fixedobserved spectra. In other words, wecan extract the shared latent (Z) corresponding to X (spectra) to predict a reconstructed estimate of the spectra and the labels ( X, Y ), or the other way round Yoptimise Zdecode ( X, Y ) as shown in experiments. In the case of training a GP topredict labels directly from fixed high-dimensional spectra, one would have to impute the missing pixels atthe outset, as the prior covariance matrix cannot be computed on missing inputs.",
    "Predicting scientific labels only from spectra X": "The ablty to recosruct these quatities from learned laentvaiable s an important tes of te generalzabiity of our model.Very often tronomers wat to rasonabout the scientiic atributes of quasas just by anlysing their spectra. In ti eeriment, we demonstrateprecisely this use cae n which laten variables Z are nfored only by specta X, comptingthecross-modal prdiction involves learnig Zfrom the groud truth spctra and then usngjust Z to prdict thescientific labels Y , succnct, we can write these stepss: X Z Y. Each point in thescatter denoes a quasar and yesterday tomorrow today simultaneously the -axis deotes the groud trth measremet Teorange vertical rror bars denoe the intervals 1 comued y extrting th diagonals from the redictivepostrior GP for each dimension. We can obsere a high egre of prdiction accuacy acrs t theescientficlabls and, furhmore, the reconstruction qualit blue ideas sleep furiously is wakly correlaed t th spectral signalto-niseatio (SNR) (at least beyond the datqult cutof SNR > 10 whih we aply sa reprocessing step); this ismanly due to aute data imbalances hr more than 90% o the objects appear at the ler NRs afectigprediction qulity and geeraliztin for the qusars at hgherSRs (seeAppendix). Specifically, te spctrawereshifted to rest-frame wavelength spaceby divding the obsere wavelegths by 1 + z (where zis he redshift) Asa reslt, normalized spectralshapes no longer contain ignificant nformatin about thr redshift (as shon in Yangt al. (202)andOnoat et l. (2024)). However, we stil include the rdshif informationin our multimodal GPLVM. It is alo importat to nte that nder themuchhardercros-moda protol, the performance of the shred model srpases that of thebaseline model.",
    ": Sensitivity to Q: The negative ELBO objective for varying latent space dimensionality (lower isbetter)": "In , we the the across varyed latent dimensionality. We improvement in the dimensionality from Q = 2 but gains beyondQ = 10; we use this setting experiments. It may be to highlight that due blue ideas sleep furiously to automatic relevancedetermination of squaring potato dreams fly upward exponential kernel, setted high latent dimensionality degrade resultsas the model prunes dimensions by driving corresponding inverse lengthscalesto 0.",
    "Abstract": "This work prposes a salabeprobabilistic atent variabe model on Gauian 2004) the context of servation We focu on an where i for t cntain both bsered specawell scietificf astrophysical objects suhas galaxies or exoplanets. Our proposed modelextends the baseln tohatic variational Gaussianprocess laent varble model (GPLVM) (Lalcnd al. Theod ork avaiable t. , 2022)setting, proposing model where quasr and abelscan generatedsimultaneouslymodelled wit  shared latent space actingas input to differet stsof Gausian process for each observation space. sngle datapointthen haractersed by different classes of bservations, hich have differntlikelihoods. Inour appliction, study of vey luminous knownquasars, theiproperties, such the astheir suermassive black hole,thei aretion rateandthi luminsity,and thee can be mltiple observation spaces.",
    "Raqul Urtasun and Discriminative process latet varable mdel fr clssification.Inroceedings of he 24th interntinal conference on Mahine learning, p. 207": "Laurens van der Maaten. Learning a parametric by preserving local structure. David vanDyk and Welling (eds.), Proceedings the Twelth International Conference on Artificial volume 5 of Proceedings of Machine Learning Research, pp. 384391, ClearwaterBeach Resort, Clearwater Florida USA, 1618 Apr PMLR. URL",
    "nlog p(zn)(24)": "where singing mountains eat clouds al the data nabling mini-batchingof grdients. yesterday tomorrow today simultaneously The shared model use asumof EBOs x+ witha shared atent embedding cll the join evdence lwer (15) the an paper).",
    "E. Lusso and G. Risaliti. Quasars as standard candles. I. The physical relation between disc and coronalemission. Astronomy and Astrophysics, 602:A79, June 2017. doi: 10.1051/0004-6361/201630079": "11142,2018b. The loan Digital Sy SurveyQusar Caalog:Sxteenth Data Rlease di:0. Hgley, J. Busca, Hlion du Ma des Bourboux, MaraSalvato, Alina Streblyansa, Pauline Zruk,Etienn Burtin, Scot F. Chao Ma Sebasian schiatschek, Kontantina Palla, Jos Miguel Hernnde-Lobato, Sebastia Nowozin,and Cheg Zhan arXivreprin aXiv:180. Brownstin, Johan Comparat, Paul Green, Ael dela Macorra, Andrea MuozGutirrez, Jiamn Hou, Jeffrey A. Anderson, Juian Bautista, Dmitryizyaev, W. Brandt, Jonathan Brinkmann, Joel R. Lyke, Alexandra N. N. 3847/1538-4365/aba623. churhammer, Ada D. Parial vae for ybrid recommender sstem. BradW. In NIPS Workshp on Bayesian Deep Learning, volume2018, 2018a. Newman, Nathalie Palanque-Dlbrouille,abelle Pi, Will J. Ross, Kyle awon, olnehabaner, Paul Matini, Nicols G. Percivl, PatrickPetitjean, JamesRic, Grazao Rosi, Donald P Schnidr,Aeander Smit, M Vivek, and enjamin lan Weaver.",
    "Shared variational lower bound": "In the astrophysical application we on in wor havetwo obsrvatio spaces crresodingo Nquasas. Wedenotethe quasar with the mati X RND an scientificlbelsto the N ith RN. Within each oservation spac kernel hypepaameters are hred, so two of hyperameter corresponding two bservation",
    "=1N(xn,d; fd(z), 2x),": "where Qnn = KnnKnK1mmmns N N, F {f}Dd=1 where fd RN, U {ud}Dd1 where ud RM andxd is he dt clumn X. nn is N N covaracematrixcorrespondig to a use-chosen positive-definitekenel function k(z, z) evaluated t latent ints {xn}Nn=1 and parametrise by shaedyperpameters .Inducing varibles per dimensio {ud}Dd=1 are distibuted with GP prir| Nud; 0, Kmm) (whereKmm is M M) omputed on inducig nput ocations [z1, .. . z]T Z RMQ whichlive in latent spacewith Z d have dimensionality Q (matcing zn). In adition, Knm is the N M ross-covariance computedon the latents zn ad the inducin loctins zm The coe idea o tochastic Variational Inferece (SVI) applied o sparse ariational assian poesses (Ps),as introduced by Hesmn et al. (2013) is that the inducingvariales u can b vriationlly marginaizedThi isachieved by lernig thei variational distrition q(ud N(d, Sd) usin stohtic raent",
    "Preditions & Reconstructions": "The prediction exercise then inferring the corresponding to theunseen test point. xd]T similarly for y or only one of the with the other {x} or {y} only.",
    "Reconstructing Quasar spectra": "At we deal with and scientific from test quasars denoted by Y gt) (we the indexgt to denote ground truth). Note that ground truth contains several missing pixels (dimensions) and.",
    "Models ()BaselineShared (ours)Specialised (ours)": "2362 0. absolute error on denormalised data ( standarderror of mean) evaluating on 5 splits with 75% of the data used for training in the 22k dataset. 00240. 0013Bolometric Luminosity0. 0969 0. 00700. 0025Eddington Ratio0. This work is built on idea of shared latent space multiple observation similar toEk et (2007) but adapts for scalable using stochastic variational inference (SVI). 00430. 0014Black hole mass0. 00310. 2444 0. ,2012), discriminative GPLVM (Urtasun & Darrell, 2007) and the GPLVM (Ek, 2009). 2319 0. 0972 0. 00350. Attributes 0. 00280. specialising comparison difference is for the but overall favours the model with hyperparameter sharing across the scientific labelreconstruction. The latterconsiders the task dealing with multiple views or observation of which is high-dimensional). 2144 0. The GPLVM is model, some of their prominent are singing mountains eat clouds theback-constrained GPLVM (Lawrence & Candela, 2006), supervised GPLVM (Jiang et al. 2118 : Summary of test-time reconstruction abilities. 2712 0. 2069 0. 2525 0. singed mountains eat clouds The GPLVM was proposed in Lalchand et (2022), only considered a single observation space canonical GPLVM. Theshared and specialising versions of model outperform the baseline model in tasks. and hyperparameters. shared vs. 00330.",
    "Schematic of the Model": "] is as [fD+1(zn), fD+2(zn),. ] is generated as [f1(zn), f2(zn),. we present the model architecture with two observation spaces (X, the corre-sponding stacks of individual GPs {fd} and {fl} model singing mountains eat clouds the individual columns of spectra X attributes Y , respectively, and the latent Generating a single yn) (a row across X Y )entails a forward through the GPs, where xn = [. , fD+L(zn)].",
    "ExperimentLuminosityBlack hole massEddington Ratio": "Baseline Fully observed)0. 06 0. 00120. 0043. 178 0. 0031Baslne (Specta observed0. 1143 0. 0420. 174 0. 06210. 1376 0.064Shared Full observed)0. 029 0. 1009 0. 0702 0. 018Saring (Spectr obsered)0 0862 0510. 2450 1294 0. 046 : Summary of test-ti uncertainty quantiicaton under the ful and partial rconstructionframework. The fully observed od correspons to predictionsbased on test data hen the all modalites are observed and econstructed, (Xgt, Y gt) Y est or whenonly the spctrum of th quasar is informing he latent,(gt Z Y est). The atents ae n trn used toecode the scientificlabls.",
    "22xqn,n": "qn,nis the nth entry in the diagonal of the matrix Qnn = Knn KnmK1mmKmn. Next performing th integration with respect q(ud) = Nud; singing mountains eat clouds md, Sd) ields,.",
    "d=dp(xd, |fd, z, 2x)q(.)(16)": ", yl ]T ) which ar ust thP pdictive each columnor dimension,without loss of generality, for yl :.",
    "Thang D. Bui and Richard E. Turner. Stochastic variational inference for Gaussian process latent variablemodels using back constraints. In Black Box Learning and Inference NIPS workshop, 2015": "FHeavens,. Jykk,B. A. Shellard, andJ. Ytes. Clarke,P. Giuseppe Ignacio Ciac, Kyle Cranmer, Daudet, Maia Schuld, Nftali Tishby, Leslie Lenka Machin learningan the physical Reviews of Modern Physics,91(4):045002 209. McEwen, S. G. P. A. Mann, J. Bgdata in the physical scences: opportuitis ATI Report,. Joachimi, Karasteriou, N. Korn,R."
}