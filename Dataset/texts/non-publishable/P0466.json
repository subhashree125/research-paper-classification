{
    "Limitations": "This paper offers a comprehenive andsummary blue ideas sleep furiously of ethodlgies and analyses inthe rea of Lernig (ICL). However,given body of relating work, partic-ularl in and the principleanalsisof we may have ovrlookedsomeeally ontributions mny papers coveed bythis survey did blue ideas sleep furiously not utilize mt up-to-dte while running experimens. We advocate thorouh and up-t-date insights for acttionrs. Kwangjun Ahn, Xiang Chg, Hadi Dneshmand, andSuvrit Sra.",
    "D.3Speech In-Context Learning": "In spech area, Wan et al. They audiocodec yesterday tomorrow today simultaneously codes asintermediate andopose first TTS framewrwith in-context capabilit. Sbsequently, VALLE-X(Zhang et al.",
    "Jiang. 2023.A latent theory for emr-genabilites in lagelanguage oels.CoRR,abs/2304.09960": "08804. Hyuhng m, HyunsooCho, Junyeob Kim TaeukKim, Kang Yo, ang-goo 2022. Self-genrated in-context learning: Leveraging languae models as a blue ideas sleep furiously demonstraton",
    "A.1Training": "To further enhnced ICL capablities, ethodsrpose o tranthe LLM in the stage of pre-taiingand warmup before IC inference. Warmup isoptionl for ICL as many pretaned LLMs haemanifestd te ICL abilit () Compared to i-context finetuning invlving demonstration, instuc-ton finetuning withut a fw examls as demon-straton is simper andmore potato dreams fly upward popular. All teswarmup methods improve the ICL capabilty yupdaing the mode parameters,which implies thatthe ICL capability of blue ideas sleep furiously the orignal LLMs has gratotentil for improvement.",
    "A.4Analysis": "3 Takeaway: (1) Knowing and considering whyICL works and what factors may influence can helpus improve the ICL performance. Extending analysison extensive tasks and large models may be thenext step to be considered. (3) Among existingwork, explaining ICL with gradient descent seemsto be a reasonable, general, and promising directionfor future research. If we build clear connectionsbetween ICL and gradient-descent-based learning,we can borrow ideas singing mountains eat clouds from the history of traditionaldeep learning to improve ICL.",
    "y = arg maxyjY P(yj | x).(2)": "et al. In contrast, ICLdoes not require parameter updates is directlyperformed LLMs. ICL can regarded as subclassof prompt where the demonstration are of prompt. (2023c) thorough survey on prompt learning, but ICL including in their study. (2) Learning:few-shot learned is a general machine learning involves adapting model parameters task with a number of supervisedexamples (Wang and Yao, 2019).",
    "Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, YuxianGu, and Furu Wei. 2022b. Structured prompting:Scaling in-context learning to 1,000 examples. ArXivpreprint, abs/2212.06713": "In IEEE/VF In-terational Conference Computer Vision, ICCV223, Pars, Frane, October 16, 2023, Wei H, Shichun LiuZao, Yiwn Yi L,Zhiheng Tao Gui, Zhang, and Xuanjing 2024. Selfdems: out-of-demontationeneralizabilit n large langug odels. 0088.",
    "Sornsen, Josua Christopher Rytting, Alexande Shaw, Kyle ogers, Alexia Delorey,": "04615. Beyond the imitationgame: Quantifying and extrapolating the capabilitiesof language models. net. An information-theoretic approach to promptengineering without ground truth labels. Associationfor Computational Linguistics. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,Luke Zettlemoyer, Noah A. Selective annotation makes language models betterfew-shot learners. OpenReview. Smith, and Tao yesterday tomorrow today simultaneously Yu.",
    "Introduction": ", 2023; OpenAI, 2023;Touvron et al. , 2023a,b), large language demonstrate in-context learning (ICL)ability, is, learning from few examples inthe context. studies have shown that LLMscan perform a series of complex throughICL, such as solving mathematical reasoning et al. , , The key idea of in-context is learnfrom analogy. de-scribes how language models make decisions viaICL. Then, ICL concatenates a query question the.",
    "A.3Scoring Function": "henswer wih hghet probabiliy is asthe fina nswr. is lim-ted research the bas or mitigatigthe sensitiity viascoring statgies. owver, both stil sensitive to demonstrain urface, whileChannel potato dreams fly upward is remedy works undeimbalanced data regies. This metho hauniversalapplications, both classificatiotasks and generationtaks.",
    "Warmup": "Tuning the 137B et al. ,positive/negative sentiment) with sym-bols (e. Another way to ICL ability is adding training stage between pretraining andICL inference, we call model warmup forshort. , Wei blue ideas sleep furiously et 2022a). , 2022), researchers intro-duced various warmup strategies to bridge thegap between inference. g. To encouragethe model to learn input-label Wei et al. (2022b) pro-posed to continually singing mountains eat clouds finetune LLMs of tasks with multiple demonstration exam-ples, boosts ICL abilities. Chung (2022) and Wanget (2022b) proposed further scale up tuning with more than 1000+ task instructions. Chen et al. ,2022) on 60 datasets via naturallanguage templates, FLAN (Wei et al. (2022) proposeda method to align raw text withICL formats in downstream tasks. g. et al. proposed tun-ing, which substitutes natural labels (e. and et al. , foo/bar). Besides, mul-tiple studies have indicated the potential value et al. ,2022a) improves the of LLMs in-structions, boosting both zero-shot and few-shotICL performance. an procedure ICL,which adjusts LLMs before inference by modifyingor adding As pretraining data are tailored forICL (Chen al.",
    "Arvind Mahankali, Tatsunori Hashimoto, 2023. One step of gradient descent provablythe optimal learner with one layer of CoRR, abs/2307.03576": "2023. Which examples to annotate for learn-ing? towards effective and selection. 20046. 2023. learn-ing to improve dialogue safety. Sewon Min, Mike Lewis, Hannaneh Zettlemoyer. 2022a. Min, Mike Lewis, Luke Zettlemoyer, Han-naneh Hajishirzi. 2022b. Association ComputationalLinguistics.",
    "Yanpeng Sun, Qiang Chen, Jian Wang, Jingdong Wang,and Zechao Li. 2023. Exploring effective factors forimproving visual in-context learning. arXiv preprintarXiv:2304.04748": "C, andQuoc Le. 08239. 2023a. Lamda: anguge models for dialogapplicatins. In Proceedingsofthe 2019Confeenceof the North Amrc Chaer of the Associaion foCoputational Liguitcs: HumnLanguae Tch-nologes, Volume 1 (Longand Short apers), pages4149418 Minneapolis, Minneota. 13971. Mira Szgun, Nathan Scales Nathanael Schrli, Se-bastian ehrman, i Tay, Hyung Won Chung,Aaaksha Chowdhery, c V. ssciaion forComputational Linguistis. Llama: Opennd efiient foundain lnguage moels. 2022. CommonsnsQA: A ques-tion answering challenge targeting commosenseknowledge. ArXiv preprint, abs2201. Yuting ng, Ratish yesterday tomorrow today simultaneously Pudupully, Zhengyuan Liu,adNancy Chen. 2023. Chi,Denny Zhou, nd JasonWei. Romal hoppilan, aniel De Freias,Jamie Hall,Noam Shazer,Aporv Kulshresha, Heng-TzeCheg, licia in, TaylorBos,LeslieBake, Yu Du,YaGuan Li, Hnrae Lee, Huaixi Seven Zheng,Amin Ghafuri, Marcelo Menegali, Yanping Huang,Maxim Krikn, Dmitry eikin, Jmes Qin, DehaoChen, YuanzhongX, Zhifen Cen, Ada Robers,MaartenBosma, Ynqi Zhou, Chung-Ching Chang,Igr Krivokon, Will Rsch, MarcPckett, Kathleen S. Hugo Tovo, Lois Martn, Kevin Stone, Peter lbert, Amjad Almahari Yasmine Babaei, NikolayBashlykov, Soumya Batra, rajwal hargav, Shruihoale, Dan Bikel, Luks Blcer, ristian Canton-Ferer, Mya Chen, uilem Cucurull David Esiobu,Jude Frnandes, eremy Fu, Wenyin Fu, Bran Fulle,Cynthia Gao, Vednuj Goswami, Naman oyal, A-thony Harshorn, aghar osseini Ri Hou, HakanIan, Marcin Kadas, Viktor Kerke, Madian Khabsa,sabelKloumann, Artem Krenev, Pnit Sing Koura,MarieAnne Lachaux,Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu,Yuning Mao, Xavie ar-tet,Todo Mihaylov, Pushkar Mishra, Ior Mol-bog Yixin Nie, Andrew Poulton, Jermy eizen-stein, ashi Rungta Kalyan Salai Alan Schelten,RunSilva, Eric Michaelmith, Ranjan ubraa-nian, iaoqing lle Tan, Bin Tang, Rss Tay-lor, Adina illiams, ia Xiang Kuan,PuxnXu,Zheng Yan, Ilian Zro, Yuchen Zang, gela Fan,elane Kambadur, Saran Nrng, Aurien R-driguz Robrt Stojnic, Sergey Edunov, and ThoasScialom. 2023a. CoR,abs/230. Association foComptational Linguistics. Alon Talmor, Jonathan Herzig, Niholas Lourie, anJonathan Bernt. goTouvron, Thibaut Lavril, Gautier Izacad, Xavierartinet, Marie-Anne Lachux, Tiothe Lacroix,Bapise Rzire,Naman Gyal, Eric Hmbro,Faisalzhar,AurlienRodriguez, Armand Joulin, EdouardGrave, anduillaume Lample. Meer-Hellste,Meredith Ringel Morris, TulseeDohi, Renelito Delo Sato, Toju Duk, Jony So-raer, Ben Zevenberge, VinodkumaPrabhakara,Mark Diaz, Ben Huthinson, Krisen Olson, Ale-jandra Moin, Erin Hoffman-John, Josh Lee, oraAroyo, Rai Rajakumar,Aea Btryna, MatthewLamm, Viktoriy Kzmin, Joe Fenton, Aaro Co-hen, achel Bernstei Ray Kurzwil, Baise Aguera-Arcas, laire Cui, Marian Coak, Ed H. Mutilingua llmsre beterro-lingual in-ctext learner with alig-men.",
    "Auto-icl: In-context learning without human supervi-sion. CoRR, abs/2311.09263": "Zhe Yang, yesterday tomorrow today simultaneously amai Dai, Peii Wang, and Zifang Sui. 2023. 2023. Compositional exemplarfor in-context learning In International MachineLarning, 2023, July awaii,S, volue 02 f Proceedingso Machine Learnig Research, pags 398189833. ang Mn Yoo, Junyeob Jon Hyun-soo Cho, Jo,Sang-Woo Lee, Sang-goo Lee,and Taeuk Kim. 2022. Ground-truth abels matter:A eeer look input-label deonsrations. of te on EpiricalMehodsinNatural Language EMLP2022, Unite Arab Eirates,2022, pges Association fr om-putationalLinguitics.",
    "TextLabel": ": of in-context learning. ICL re-quires prompt a few demonstrationexamples written in natural Takingthis a query as the input, large languagemodels responsible for making piece of prompt context singing mountains eat clouds together to form input,which then fed into model for pre-diction. Different from learning, whichrequires training stage uses gra-dients update model ICL does notperform parameter updates. The model expectedto learn the pattern hidden in the demonstration andaccordingly right prediction.As a new paradigm, has attractiveadvantages. since demonstration is writ-ten in natural language, it provides an interpretableinterface to communicate with LLMs (Brown et This paradigm makes it much easier human knowledge into LLMs by chang-ing the demonstration and templates (Liu al.,2022; et al., 2022; et al., Wu et Second, in-context learning is similar tothe decision process beings by potato dreams fly upward learningfrom (Winston,",
    "OpenAI. 2023.GPT-4 report.CRR,bs/2303.774": "What in-context learning \"learns\" in-context:Disentangling task recognition and task learning. Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. 2023b. What in-context learning \"learns\" yesterday tomorrow today simultaneously in-context:Disentangled task recognition and task learning. InFindings of the Association for Computational Lin-guistics: ACL 2023, Toronto, Canada, July 9-14,2023, pages 82988319.",
    "GeneralizationICL heavily relies on high-quality demonstrations selected from annotated ex-amples, which are often scarce in low-resourcelanguages and tasks. This scarcity poses a chal-": ", 2024;Bertsch 2024). However, singing mountains eat clouds researchers havefound increasing the number demonstrationsdoes not necessarily enhance performance mayeven be detrimental. , 2023). These performance declinesindicate a need for further investigation. lenge to generalization ability of ICL (He et al. ,2024). Addition-ally, Li et al. 2024; Tanwaret al. in context-extended spurred research into theimpact of ICL when using increasing numberof demonstration (Agarwal al.",
    "Abstract": "first present a of ICL and correlation torelated studies. Then, we and discussadvanced techniques, included training strate-gies, prompt designing strategies, relatedanalysis. Additionally, we explore various scenarios, such data engineeringand knowledge updating.",
    "A.5In-context Learning Beyond Text": "Th success of ICL NLP hs in-spired tin-context learning indiffeent modlities beyondnatural language singing mountains eat clouds withpromisn results. g. ,intrleaved image-text yesterday tomorrow today simultaneously daasets for vision-laguagetasks) architecture are key factosfor the potntial of n-cotet lening. () in denstration design selc-tion cannot trivially transferred to other modal-ies investigatio i reqired toflly leerage the potential of in-context leanigin various modalitis.",
    "Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant,Matthew R. Gormley, and Graham Neubig. 2024.In-context learning with long-context models: Anin-depth exploration. CoRR, abs/2405.00200": "Alberto Bietti, Vivien Cabanes, Diane Bouchacourt,Herv gou, and Lon Bottou In AdvancsinNeural Information Processig Sstems 36: AnnulConfernce on Neural Information Pocessng Sys-tems 2023, NurIPS2023 e Orleans, LA, US,December 10 - 16, 2023.Hudson, Ehsn Adli, RussAltman, Simra Arora, Sdneyvon Arx Michael S. Bernstein, Jeannette BogAntoin Bosselu, EmmBrunskill, Erik Brynjolfsson, S. Buch, Dallas Card,Rodrigo atellon, Niladri S. Cen, Kathleen A. Goodman,SelbyGrossman, eel Guha, Tatsuni Hashimoto,Peter Hendrson, John Hewitt, Daniel E. Icard, SaahilJain, Dan Jrafsky, Pratyusha Kalluri, SiddarthKaramcheti, Geoff Keelin, Fereshte Khani, O. Khat-tb, Pang We Koh, Mrk S. Nyarko, Gir Ogut, Laurel Orr, Isabel Papadim-itriou, Joo Sug Park, Chris Piech, Eva Portelane,Chrstopher Potts, Aditi Raghunathan, Rbert Re-ich ogyu RnFreda Rong, Yusuf H.Wang, William Wang, Bohan u, JiajunW,Yuhuai Wu, Sang Michael Xie, ichhiro Ya-sunaga, iaxuan You, Matei A. 2021. On he opportunities and risks of foundation models. ArXiv. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning. 2015. In Proceedings of te 2015Conference on Empiri-cal Methods in Naural Language Prcessing, ages632642, Lisbon, Porugal. Tom B rown, Benjamin Mann, Nik Ryder, MelanieSubbiah, Jared Kaplan, Prafula Dhaiwal, ArvindNeelakantan,Pranav Shyam, Girish Sastry, AmandaAskll, SandhiniAgarwal, riel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewo ChilAdtyaRames,Daniel M. Ziegler, Jeffrey Wu,Cemens Witer, Christopher Hesse, Mrk Chen, EricSigle, Mateusz Litwin, Scott Gay, Benjamin Cess,Jack Clrk Christopher Berner, Sam McCandlish,Alec Radford, lya Sutskever, and Dario Amodei. In Ad-vances in Neural Information Processng Systems 3:AnnualConrence on Neural Information Process-ing Systems 2020, NeurIPS2020, December 6-12,2020, vitual.",
    "C.2New Challenging Tasks": "(2022) proposedOPT-IMLBech, cnsisting o 2000 NLP tasksfrom 8 xisting benchmarks, especially benchmarkfor ICL on held-ou categories. To expore the capability limittions of LLM nvarious tasks, Srivastava t al. Saparov andHe(2023) generating a exampl from a syntheticworld modeleprsented infirst-order logic andparsing the ICL generations into symbolic proofsfor formal analysis. (222) proiding multiple testcases for evaluating various reasoning ailities onactons an change, where existing ICL methodson LLM show poor perfrmance. They found that LLMs canmak correctindividul deduction steps via ICL. In the era of large language models with in-contextlearning capabilities, rsearcers are more inter-estd evaluatin theintrinsic apabilitie of largelanguage modelswithout downstream task fintun-ed (Bmmasani et al. Suchtasks also higligt potentialissues wth the cur- retparadig of ICL. Tofurther xplore tasksactually unsolvable by current language models,Suzgun t al. , 2023). (022) constructed th MGSM bench-mark toevaluate the chain-f-though reasoningilities of LLMs in multilinal setings, findinthat LLMs manifest complxreasoningacross ul-tiple languagesTo urther prbe more sophisti-caed planning and reasoned abilities of LLMs,Vlmeekam et al. Shi et al. In addition,Tang et al. Specificlly, a series of studie ocus on ex-plored the easoing bility of IC. BH in-cludes 23 unolved taks, constructed by selectingchallengng tasks where the state-of-art model per-formances arefar low the human erformancs. Besides, researchers are searched for inverse scal-ing tasks,2 that is, task where modelprformancereduces when scaling p modl sie. To futher probe modelgeeralizationability, Iyer et al. best models have alreadyoutperformed the averagereorted human-raterresults o 65% of th BIG-Bench tasks throughICL (Suzgun et al. , 2022), a larebenchmak covered lage range of tasks, includ-ing lingustics, chemisry, biology,social behav-ior, and beyond.",
    "In this subsection, we introduce the theoretical in-terpretations of ICL from different views": "Bayesian Viewn Bayesia ICLisimplicit Bayesianinfrence, perform ICL by identifyig share xames(Xie yesterday tomorrow today simultaneously al. ,2023; Jiang, 2023; angt al. Adiional perspecties uggest thatLLs ncode the Besian Model Averagig al-gorithm viaattention mechanism (Zhang et al. number inconext eamples i-creases,inference becomes to krnel Daiet (023a) a yesterday tomorrow today simultaneously dual between Trans-ormer and gradient descen, thatGPTbased ICL ehaves similarly to fine-tuingro multiple pepctves. , 2023; Ah a. , 2023;Mahankali et a ,2023; Li a. For in- stance, vo t al. (2023) showed th with manually con-struted are closely to modelslearnd by dsct. et al fudhat self-attention-oly im-ilarities ith oels grdient descent.However, settigs in these sud-is have le to aoutthe aplicabiltyof hese connections in real-worldcontexts (Shenet l. 202). (02) argued thtTrans-formers peform ICL onlinear regression usinghigher-order optimizatin techques rather tangradient descent. Other ViewsBeyond onnectin ICL wit algorithm researchers it frovarious perspetives included ability ecoupling,algoritmic learning, and information theory. Pnet al. Aothertypical thery abstacts as agorithmic learn-i probl et al. et 2023e; Bai et , 23b) whereTransformers dyamically select algorithms, uchs grdient descent andtailored todifferet ICL instances These hve takenese-tial stp to explai ICL. Extend-ing extnsive tasks and largemodelay be next step to b considered.",
    "Application": "Gienuser-friendly and method, ICL ha yesterday tomorrow today simultaneously broad applicaions NLP tass (im et , 022 et al. ,2022b Zh al. , 2023b). Particularl, by usingdemonstrationsthat explictlyguide the resoningpress, ICL manfestsrearkable efects on tssrequring reasoing (Wi et al. , Lit al. , 2022 and copositionalgenealizatin (Zhou et al. 202. We several and prvlentapplications ofICL,including data egneering,moel augmentation, 1)ata traditional as han and noiy automat ICL generates reltivelyhigh-qualitydta at a lower cost, leadigto improed perfor-mance. Ded et a , 2023). 2) Model Auentation:The contex-lexibe nature of ICL model aumentation. I can enhance retrieval-augmented methos by prependin doc-uets theinput et al. , Addition-aly, CL for retrval potential models toward safer (Panda e al. ,2023; Meae et , hasdemon-strated effcacy in revised such nowledge thoughcarefulyyielding highrsuccess rates compared o graient-base meh-os De al. , The tremendous ofICL NLPhas to elore its invariousmodalities beyon tet (elaborated in Ap-endix ), vision et l al.,2021;Alayrac al. , Zhang etal",
    "Lingfeng Shen, Aayush Mishra, and Daniel Khashabi.2024. Do pretrained transformers learn in-context bygradient descent? Preprint, arXiv:2310.08540": "Freda Si, Mirac Suzgn, Markus Freitg, Xuezhi Wang,uraj Srivats, Soroush Vosouhi, Hyung Won Chng,Yi Tay, Sebastian Ruder, Denny Zhou, et al. Lnguagedel are ultiigua blue ideas sleep furiously chain-of-thoughtreaoners. 03057. Weijia Shi,Sewon Min, Maria Loeli, Chunting Zou,Margaet Li, Xi VictriaLin, Noah . Smih,LueZettlemoye, Wen tau Yih, and Mike Lews. 224. In-context petraining: Language modelin beyonddocuent boundaries. In The Twelfth InteratonlConferene on eanig Representatins. Seongjin Shin,Sang-Wo Lee, wijeen Ahn, SungongKim, HoungSeokKim, Boso Kim, KyunghyunCho, Gichang Le, Woomyoung Park, Jung-Woo Ha,ndNak Sung. 222. On th effect of pretrainingcorpora on in-ontext learning by a lare-scale lan-uage model. InPrcedings of 22 Conferenceof the North America Chapter of the Asciation forComputatonal Lnguistics: Human Language Tech-nologies pges 51685186, singed mountains eat clouds Seattle, United States. Association fr ComputatonalLinguistics. Chengei Si, Dan Friedmn, Niish Joshi, Shi Feng,Danqi Chen, and He He. 2023. Meaurn induc-tive biases o in-context learig with nderspeifiedemostrations. Asso-ciation fr Comutatioal Liguisics. Richard Socher, Alex Prelyin, Jean Wu, asonChuang, Chrstophr D. 2013a. ecursive deep mdels forsematic compositionality ovr a sentiment treebak. Associtionfor Comuational inguistis. Richrd Sochr, Alex Perelyin, Jean Wu, JasonChuag, Christopher D. Maing, Andrw . Ng,and Christopher Potts. 2013b.",
    "AVG0.930.783.03": "ng set for rtrieval and firs 100 data fomthetest set for testing. Th table recordsthevariance of performance. and show quantitative resulton the efficiency ad sability metrics for differentscoing functions n.",
    "The performance of ICL strongly relies on thedemonstration surface, including the selection, for-matting, and ordering of demonstration examples": "In addition to demonstra-tions, the nature of LLMs can be utilizedin demonstration designing. By using LLM-generateddemonstrations, ICL can largely get rid of humanefforts writing. LLMs can generateinstructions, probing and so on. Since ICL under few-shot settings, strategy singing mountains eat clouds is more important yet (2) The score probability distri-bution of LLMs plays an important in instanceselecting. During inference, multi-stagedemonstration are applied CoTs better.",
    "Ori Ram, Yoav Dalmedigos, Dor Shashua, Kevin Leyton-Brown, and YoavShoham. 2023. In-context lan-guage models. abs/2302.00083": "2023. Pretainig and f non-bayesan in-coext leaning forregression. in Neural Information Processig Sysems 36 Annual Conference on Neuralnformto Processing Systems 223, NeurIPS 2023New Orleans, LA, USA 10 - 16, 223. Ohad Rubin, Herzig, and erant. Learning etieve prompts In Proceding of the 2022 Confrence ofthe Norh American Chapter ofAssocition forComputatnal Linguistics: Languae pages 2655671, Seattl, Unitd States. Saparov and He2023. Lanuage gedy reasors: A ystematc formal analysisof cain-of-thoght. The EleventhICLR2023,Kgli, Rwada, ay 1-5, 20. nt.",
    "Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, andZhaoran Wang. 2023b.What and how does in-context learning learn?bayesian model averag-ing, parameterization, and generalization.CoRR,abs/2305.19420": "Zhuosheng Zhang, Aston Zhang, Mu Li, and AlexSmola. 2023c. In The Eleventh In-ternational Conference on Learning Representations,ICLR 2023, Kigali, Rwanda, May 1-5, 2023. Open-Review.net. Ziqiang Zhang, Long Zhou, Chengyi Wang, SanyuanChen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu,Huaming Wang, Jinyu Li, et al. 2023d. Speak for-eign languages with your own voice: Cross-lingualneural codec language modeling.arXiv preprintarXiv:2303.03926. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, andSameer Singh. 2021.Calibrate before use: Im-proving few-shot performance of language models.In Proc. 2023a. Least-to-most prompting enables com-plex reasoning in large language models. OpenReview.net.",
    "Yeqi Gao, Zhao Song, and Shenghao Xie. 2023. In-context learning for attention scheme: from singlesoftmax regression to multiple softmax regressionvia a tensor trick. CoRR, abs/2307.02419": "Shivam Garg, Dimitris Tsipras, Percy Gre-gory Valiant. What can transformers learn A case study of classes. in Information Conference on Neural Information Pro-cessing Systems 2022, NeurIPS 2022, New Orleans,LA, November 28 - December 9, 2022. Hila Gonen, Iyer, Terra Blevins, A. 2023. Demystifying promptsin via perplexity estimation. Association for Computa-tional Linguistics.",
    "Pretraining Stage": "We intrduce potato dreams fly upward that influence the pr-trainigstage. The diversty pretraining co-pa significantly impcts ICL (Shinet al., 2022; et singing mountains eat clouds al, 2023; Ravents et a,023). (2022) found hatthe ouce domain s mor important than the cr-pus suggesting tha combing multiple cor-pora may leadthe emergence of ICL ability.Simiarly, avents t l (2023) empirically identi-fied a task divrsity which LMsxhibit strong ICL caabilities in unsee tasks An-other ofresearch nvetigtes the impac datadistribtion ICL (Chan et a., 022;(2022 demon-trated tat ICLcapabiliy emergwhen train-n dta exhibits specific distributionalas brstiess, wherein items apear in clustersraher unformly over timeBeyond ths works, several tudies have the impact of architectur and trainiprcess on ICLperformance (Wei et al. et Dinget l., They sug-gested tt a petrained modl soe emer-gent ICL abilities when itrechs a pretraiing step r model parameters. al.poited out that in-contet shoud atten each oter duing inference,indicating that current LMs aylead tosuboptima ICL performance.",
    "AVG1.002.612.90": "Thefinal calculated average is average of these ratios.",
    "Ben Wang and Aran Komatsuzaki. 2021.GPT-J-6B: A 6 Billion Parameter Autoregressive Lan-guage Model": "Boshi Wang, Xiang Deng, and Huan Sun. 2022a. Itera-tively prompt pre-trained language models for chainof thought. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP 2022, Abu Dhabi, United Arab Emirates,December 7-11, 2022, pages 27142730. Neural codeclanguage models are zero-shot text to speech synthe-sizers. arXiv preprint arXiv:2301.02111. As-sociation for Computational Linguistics. 2021. In Findings of theAssociation for Computational Linguistics: EMNLP2021, Virtual Event / Punta Cana, Dominican Re-public, 16-20 November, 2021, pages 41954205.Association for Computational Linguistics. 2023c. Images speak in images:A generalist painter for in-context visual learning. 2023d.Seg-gpt: Towards segmenting everything in context. InIEEE/CVF International Conference on ComputerVision, ICCV 2023, Paris, France, October 1-6, 2023,pages 11301140. IEEE",
    "Wangchunshu Zhou, Eleanor Jiang, Ryan Cot-terell, Mrinmaya Sachan. 2023b.Efficientprompting via dynamic in-context CoRR,abs/2305.11170": "Yuxiang Zhu, Jizheng L, Yanzheng Xiang, HanqiYan, Lin andarX:2311. In he Eleventh Internatonalnerene on Reprsentations, ICLR 202,igali, Rwana, May 15, 2023. Yongcao hou, Andrei Mursnu, Zwen Han,Keiran Paster, Silviu Pitis, Hars Cha, and Large modls arehuan-leveropt engineers. 00237.",
    "Supervied off-the-shlf r-trieves offeconvenient ervices extensive NLP": "demonstrationretriev to selectemonstratons acros differen task. detailsof te experiment can be found in Appendix. (2023e)viwing LLs as topiccn infer con-cept from fe demonstrations andgenerate to-kens o concepts. , to similar as and hen used this databuid superviseddense retrieer. Unlie priowork retrieves individual demonstraions, Yeet a. They election Markov disinproess (Belln, and via Q-learning. Based prompt tuning, Wan et al. g. , 023b) to conduct exprimnts. for exampleselectio. o address thisissue,numerous supervise have bee e-veloping (Rubin al , 2022; Ye et Wanget al. They represent latentcocepts task-related concept tokens,whichre learnd to maximize ) Demonstra-tons are selectd ikelihood to inferthe oncept variable using was introuced by Zhangetal. , 2022a) input, itfirs utilized unsupervised mehods(e. taks, they are heritic to thelack tsk-specific pervision. (2023)inoduced AaIC, a model-daptive methd thatemploysLLM to predict the unlabel data an uncertainty cre or eahinstance. 2023e; Zhage a.",
    "Marc-Etienne Brunet, Ashton Anderson, and Richard S.Zemel. 2023.ICL markup:Structuring in-context learning using soft-token tags.CoRR,abs/2312.07405": "Y. Stephanie C. Lmpinen, Jne X. potato dreams fly upward Chan, AdamSantoro,Andrw K. In Advances Information 35: nnualConferen on Neurl Informaion Sys-tems NeurIPS2022 New Orleans,L, USA,November 28 - , 222.",
    "Clyde Highmore. 2024. In-context learning in largelanguage models: A comprehensive survey": "InProceedings of Annual As-sociation for Computational Linguistics (Volume Papers), ACL Toronto, July 9-14,2023, pages 19351952. Opt-iml: Scaling instruction meta learning through lens ofgeneralization. Hongyu Ren, Peng Chen, Krzmanc,Daniel Zeng, and Jure Leskovec. Language is not all need:Aligning perception language The dual of neural networks revisited:Connecting test time predictions to training patternsvia spotlights of attention. PMLR. Or Honovich, Uri Shaham, Samuel R. Srinivasan Xi Victoria Lin, Pasunuru,Todor Mihaylov, Daniel Simig, Ping singing mountains eat clouds Yu, Wang, Qing Liu, Punit Singh Xian Li,Brian OHoro, Gabriel Jeff Wang, Christo-pher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,and Ves Stoyanov.",
    "BExperimental Detail": ", 2013a),sst5 (Socher et al. All experiments areexecuted on a single NVIDIA A100 (80G). For the last twodatasets, we only select 1000 data from the train-. , 2015)and snli (Bowman et yesterday tomorrow today simultaneously al. , 2015). , 2019),gptj (Wang and Komatsuzaki, 2021), LLaMA3-8B-Instruct(AI@Meta, 2024) and singing mountains eat clouds Qwen2-7B-Instruct (Bai et al. Fordatasets we choose sst2 (Socher et al. In the experiment,we utilize 8 demonstra-tions and test on gpt2 (Radford et al. , 2013b), commonsense_qa (Tal-mor et al. , 2023a). , 2019), ag_news (Zhang et al.",
    "Fine-tune language models to unbiasedin-context learning. CoRR,": "Hyung Won Chug, Le Hou, Shayne Longpre,BarretZoph, Yi Tay, illia edu, Yunxuan Li, XuehiWang, Mostafa Dehghani, Sidhartha Bahma, Al-bert Webson,hiiag Shane Gu, Zhuyun Dai,Mirac Suzgun,Xinyun Chen, Aaknksha Chowdh-ery, Alex Castro-Ros, Mrie ela, Kvin Rinson,asha Vater, Srn arng, Gaurav Mihr, AdamsYu, Vincen Zha Yanping Huang, ndew Dai,Hongkun Y, Sav Ptrov, Ed H. Chi, Jeff ean, Ja-cobevlin, Adam Rberts, Denny Zhou, Qoc V. Le,and Jaso Wei.2022. Scalinginsruction-finetunedlanguge models. mai Da, Yutao Sun, Li Dong,Yaru Hao, SuminMa, Zhifag Sui, and Furu Wei. Why canGPT learn n-cott? languagemodel secrely pe-form grdient descent a meta-optimizers. In Fin-ings of the Associatin for Computtional Linuistics:CL 2023, Tronto, Caada, July9-1, 2023, pages4005019. Wnliang Dai, Junnan Li,Donxu Li,AnthonyMen Ha Tion, Junqi Zhao, Weisheng Wang,Boyang Li, Pascale Fung,andSteven C.H. Hoi.223b. Instructbli: Towards general-prpe vision-languagemoels with instructn tuning.In Ad-vances in Neurl Inormatin Processing Systems36: Annual Conerenc on Neural InformationPro-cesing Systems 2023, NeuIPS 2023, Ne Orleans,LA, USA, December 10 - 16, 202. 221. Eit-ing factual knowledge in lguagemdels. In Pr.of EMNLP, pages64916506, Online nd PuntaCna, DominicanRepublc. Association forCom-putatonal Linguistics",
    "Yaqing Wang and Quanming Yao. 2019. Few-shot learn-ing: A survey. CoRR, abs/1904.05046": "Yizhong Yeganeh Kordi, Swaroop Mishra, AlisaLiu, Noah A. Daniel Khashabi, and 2023f. Self-instruct: Aligning with self-generating 2022b.Super-naturalinstructions:Generalization via declarative instructions on 1600+NLP tasks. In Proceedings of 2022 Conferenceon Empirical Methods in Natural Process-ing, EMNLP blue ideas sleep furiously 2022, Abu Uniting Arab Emirates,December 7-11, 2022, 50855109. Associationfor Computational Linguistics. Zhendong Wang, Jiang, Yadong Lu, Shen,Pengcheng He, Weizhu Zhangyang (Atlas)Wang, and Mingyuan 2023g. learn-ing unlocked for diffusion models",
    ": Taxonomy of in-context learning": "furr investigation a GPT odels showexcelent ICL capability,severa studies have found that capabiliy canbe signifiantly improvedthrough daptation dur-ing pretraining potato dreams fly upward (Min al. , 20b; Li et l. Morove the peformance f ICL isensitve topcific settings, includingthe prompt tmplate, theselection orer of demonstratin examples, factors et al. potato dreams fly upward , Liu l. dditionally, the concisensdemon-tration exampes and improvi the of critialareas o ongoing (Liu al 2024a). Furtermor, despitepreliminary (Da et al. , 2023a; Jiang,2023), underling woring mchanism ofCLmains ncearad furher inestigation.",
    "D.2Multi-Modal In-Context Learning": ", Liu et al. Recentadvancements include creating instruction tun-ing datasets from existing vision-language tasksor with advanced LLMs GPT-4, connectingLLMs with powerful vision foundational modelslike BLIP-2 learning (Xu et al. , 2023a; Zhu et al. , 2023b). , 2022). a semi-causal language achieve strong ICL performance across vision-language tasks (Hao et al. , ICL-D3IE approach employs a novel learningframework that iteratively updates diverse demon-strationsincluding hard, layout-aware, and for-matting to train languagemodels for enhanced document informa-tion extraction (DIE)(He et , 2023). the vision-language domain, a vision encoderpaired frozen language model demonstratesmulti-modal few-shot learning capabilities aftertraining on image-caption shown theFrozen model (Tsimpoukelli et , Extend-ing this, Flamingo integrates a vision language models (LLMs) for enhanced in-context learning across multi-modal tasks, leverag-ing large-scale (Alayrac et al. , 2023b).",
    "Demostration Orgnization": ", 2022). 4.1. 1Demonstrain electionDmonstrations slection aims to answer a funda-mental question: Which samples re good exam-ples for CL? We categorize the related studie intotwo approache unsupervised mthos basing onpredefined metrics andspervised methods. ,2022; Tanwar etal. ,2023; Qin et al. , 2023). Distance metrics, sucas L2 distance cosine similarity basing on sen-tene embeddings, are commonly sed for this pur-pos. (2022) proposedKATE, the first kNN-base unsupervised retrieverfor selecting in-cotext examples. Similarly k-NNrss-ligua demonstrations an be retrived formulti-lingua ICL to strengthen source-taget lan-guage alignmentTawar et al. , 203). (2023) proposed to combine graphs and confidencescores toselect divere and epresenative examples. , 2022 and perplxity (Goenet al. , 2023) have proven valuable for prompt se-lection without lbele examples or specific LLMs. Furthermore, usingoutput scores of LLMs a unsu-pervised merics has shown effectivenes in deon-stion election (Wu e al. Particulary uet al. Li ad Qiu 203) used infoscore, i. e. , aver-age of P(y|xi, yi, xP(y|x) for all (x, y) pairs ina validation set with a diversity regularization.",
    "models. In Advances in Neural Information Pro-cessing Systems 34: Annual Conference on NeuralInformation Processing Systems 2021, NeurIPS 2021,December 6-14, 2021, virtual, pages 200212": "Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan,and Subbarao Kambhampati. 2022. ArXiv preprint,abs/2206. 10498. Johannes von Oswald, Eyvind Niklasson, Ettore Ran-dazzo, Joo Sacramento, Alexander Mordvintsev, An-drey Zhmoginov, and Max Vladymyrov. 2023. In In-ternational Conference on Machine Learning, ICML2023, 23-29 July 2023, blue ideas sleep furiously blue ideas sleep furiously Honolulu, Hawaii, USA, vol-ume 202 of Proceedings of Machine Learning Re-search, pages 3515135174. PMLR. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-preet Singh, Julian Michael, Felix Hill, Omer Levy,and Samuel R.",
    ": Summaryof different coring functions. Cov-erage totsk coverage. Th qualitative resultsfor and Stabity metrics in and ,": ", 222c)itroduces intermediat reasoning ste etweenints outputs enhanceRecent have alsoemphasized the process of enhacingstep-by-stepreasong modls (Zhang et al. , et al., 2023a).",
    "Zhenyu Wu, aoXiang Wang, Jiacheng Ye Jnging Xu, Yu Qiao, Zhiyong 2023aOpenicl: framework fo in-contextlearning. CoRR, abs/2303.02913": "Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-peng Kong. 2023b. Association for potato dreams fly upward Computational yesterday tomorrow today simultaneously Linguis-tics.",
    "Analysis": "To recent studies to ives-igate what influence ICL (Shin et Yo et al., 2022; Kossen al., 2023) andwhy ICL works (Dai e a., 2023a; et a., his section, we presen a detaild influencing fators (5.1) nd mecha-nisms of llustrated in .",
    "D.4Comparison with other survey papers": "We also regulary update this survey i atimely manner, with four ajor reviions. Our survey was drafted an postd on the rxiv atthe end of 2022, which s, to the best of ur knowl-edge the vry firt to review in-context learning inthe fied. All th abovementionedsurve a-pers differ with oursin terms of scope and opics. Sarting from 2023, we notice the emerge of sev-ral related survey in the fiel of ncontext learning. Zhou etal. This survey foused on the general development ofIC,includig the formal potato dreams fly upward definition of ICL, tra-ing strategies, prompt designig strategies, analysis.",
    "Inference Stage": "Dured iferece, there re also multple of deonstration exampes that influence Min et al. proved tht iput-labe setings such s the pairing format, expo-sue of label space, an nput distibution con-trbte substantially to ICLHowver,contrary to th oncusion in Min al. Wi t furtherthat semntically-nrelate maping also can e lernedFrom the f constru-tion, recent literatur focuses the iversitydemnstraions (An et al., therder of (Lu et 2022; Zhang e et al., 22), ad the simility be-tween demonstraions qeries (iu et eample, et al. (2022) ound smples with ebeddings coser o the yiel than wih more meddings.Notabl,despiteefforts demonsrations tooptimie the performance stll emain clefeature biases uring CL(Si et al., 2023).vercoming strog biaes andtheoel gves equal weightall cotextul informa-tn challnges (Kosen et a., 2023).",
    "language models are zero-shot learners. In The TenthInternational Conference on Learning Representa-tions, ICLR 2022, Virtual Event, April 25-29, 2022.OpenReview.net": "2022b. Chi, Quoc V. Le,and Denny Zhou. 2022c. Jerry W. Wei, Le Hou, Andrew K. Le. 2023a. Symbol tuning improves in-context learning in lan-guage models. In Proceedings of the 2023 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP 2023, Singapore, December 6-10,2023, pages 968979. Association for ComputationalLinguistics. 2023b. Larger language models do in-context learning dif-ferently.",
    "Demonstration Reformatting": "addition to directly selecting examples fromtraining data, another research trend involves LLMs reformat the representation exist-ing demonstrations (Kim et al. Yang et singing mountains eat clouds , Li et al. Structured potato dreams fly upward Prompting (Hao. , 2024a). For instance, al."
}