{
    ". VQA model": "MaxQA and propoe separate meticsor the video-text aligent and wl T2VQA hals th from the twoinsion as whol. -Align can also address QA rlying on theability of models. The VQ models are usually desiged user-enerated o a certin ttibute of videos. VBench andEvalCraftr uldbench-mars for by dsignngmultidiensional metrcs. For exaple, trainsan spatial feature extactin network to dictllearnquality-awre satial features video frames, andextracts featues to masuerelate dis-tortions at the same tim to potato dreams fly upward prdict the fragments sampling strategiesand Network (FANt) to accommo-dte as DOVER evaluates the of videos rom the and erspetivesrepectively. There ar seeral targeting the VA tsofAIGVs. We beleve t development teVQA model for AIGV will certainly the genraonof high-quality ideos.",
    "Robin Rombach, Andreas Blattmann, and Bjorn Ommer.Text-guided synthesis of artistic images with retrieval-augmented diffusion models. 2207.13038, 2022. 3": "for qualityassessme in thewild. I Proceedingsof the IEEE/CV Conferece Com-puter Vison and Pattern ecognition, pages Mrk Sandle, Andrew Howard, Menglong Zu,AndreyZhmoginov, and LiangChieh Invetedresiduals andlinear bottlenecks.",
    "BDVQAGroup": "Team BDVQAGroup to assess of AIGIs. One is , is large multi-modality models (LMMs). con-verts MOSs rated levels and uses to teachLMMs with rating levels of scores. During inference, it extracts the probabilities ofrated levels and a average to obtain score. They use several dataaugmentation methods to increase training andenhance robustness of MSTRIQ, which are: 1) image along side to form square. 2) Ran-domly degree 90. 3) Resize the im-age to 448 448. They a Q-Align model pre-trained on KonIQ , KADID , AVA , and LSVQ , this model through three The modelis based on Q-Align Image Quality Scorer, which fine-tuning 4 on the images and thenfinetuned for another 2 epochs on provided training The second model is based on the Q-Align ImageAesthetic Scorer, which finetuning for 2 epochs provided training The third based on the Quality Scorer, is finetuned 2epochs provided training images. The followed implementing to increasemodel performance: 1) Expand the image along its longerside to form a square. 2) Randomly yesterday tomorrow today simultaneously image 18 The size is 384 384 18 3) Theyuse four models to predict and the four prediction re-sults according to the following weights to obtain finaloutput.",
    "Kwai-kaa": "Kwai-kaa wins second thedeo track. the pomptserves potato dreams fly upward as a globa of the vido, thy thatassessin lgent is sufficient wihinthe therefore, does not utlize text information theCLIP banch. The veiew of LM CLIP isshown n. In particular, LMM branches re finetuned witthe pr-training One-Align weights, ad CLPbranches are finetne ith the pre-traned cip-vit-large-patch14weights. traning, teyGPUs,with size of 24 for EV and for fullfn-tunng.he process fr the LMM branchtakes approxiately hour each to The CLIPbackone rained f rat setto 106. trained proces s carried ou on 8 TeslaA100 PUs, with a size of 8. It aproiately8 blue ideas sleep furiously hours to complee the taining videos with 1 fraes, pad he ast fam to gen-erate a complte st of 16 frames. frame are rsizedto 48 and training proceses are finished with thedamW",
    "Luming Tang, Menglin Jia, Qianqian Wang, Cheng PerngPhoo, and Bharath Hariharan. Emergent correspondencefrom image diffusion. Advances in Neural Information Pro-cessing Systems, 36:13631389, 2023. 15": "Videomae: Masked autoencoders are data-efficient learnersfor self-supervised video pre-training. In CAAI International Conference on ArtificialIntelligence, pages 4657. Ex-ploring clip for assessing the look and feel of images. 17 Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. 15 Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. 10 Jiarui Wang, Huiyu Duan, Jing Liu, Shi Chen, XiongkuoMin, and Guangtao Zhai.",
    ". AIGV dataset": "Chivileva  l. Compared with IGIdatases, the number datasts is small. Vnch has a larger cale with in totalof 1,7prompt 4 moels. In the trak, we use theT2VQA-DB The dataset hs 10,000 geneatedby 9 models. 24usrs ar involved in the sujective study. 27 ae invted to collectte OSs.",
    "Oblivion": "Oblivn sesthe ideo Swin Tranormer sthe visual backbone, and ten the use the CLIP textencoder as the text fature xtractorusing text enhance visual understanded t the quality ofte vide. They the nework ontheto theVideo Sinransforme backone, and theon the Kinetics-40 datast to initialize he CLI model.",
    ". IQA model": "LIQE proposes a general multiask learning scheme exploit auxiliarynowedge from IQA, scene classiication, anidetification. the meantime, severalIQA modeldesigned or potato dreams fly upward AIGIs poposedImageReward BLIP-based architecture predict the imagequality In recent years, researchers been paying attentio tousingteability of Large Models solve IQA tasks. Q-benh irst investigats of LMMs evaluating visual quality. further introduce the taining prcedure to tilizeLMMs for IQA tasks. Q-Refinea refiner to guide the refining process in T2I models. The of IQA mdels not ny provides accuratepredictions onAIGsquality but also benefis th deveop-mnt of generation",
    "Gu Jinjin, Cai Haoming, Chen Haoyu, Ye Xiaoxing,": "Jimmy Ren, and Dng Chao. a large-scale iagequality asessmnt dataset iage restoration.In Procedins f the Copuer VisioECCV 2020 onference, Glasow, UK, 2328, 200,Proceedings, XI 16 pages 633651. Springer, 2020.11WilKay, Joao Carreira, Karen Simnya, Brian Zhang,ChloSudheendra Fabio blue ideas sleep furiously Viola, Tim Geen, Treor Back, Paul Natsev,et al.Thekineics human action vieo ataset.rXiv preprintarXiv:1705.0950, 201. 17 Ke, Kren e, Jihui Yu, Yonhui W, Peymn Feng Leaning image aestheicsfrom user comments with vision-anguag pretrainig. InProceeings of the EE/CVFConference Computer and Pattern Recognition, ages 2023.10 Leon Khachatryan, Movsisan, Vhram Tade-vosya,Roberto Henschl,Zhangyang Wang,ShantNavasadyan, and Humphrey Shi. Text2vdeo-zero: diffuso odels are zero-shot vieo generators.In Prcedings of the International Conferenceon Comuter Visin, pges 159541964, 3Yual Kirstain, Adam Plyak, Uriel Shahbulandatiana, Joe ad Omer ick-a-pic: opendatase of user prferences f geeration.Advances Neral Procesin Systems, 2, 9",
    "Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,Xiang Wang, and Shiwei Zhang. Modelscope text-to-videotechnical report. arXiv preprint arXiv:2308.06571, 2023. 3": "video genera-tion cascading latent models. arXiv preprint arXiv:2402. 17090, 2023. 14896, 2022. 15103, 2023. Exploring video quality generated contents from aesthetic and technical In 3, 4, 5, Haoning Zicheng Zhang, Erli Chen,Liang Liao, Annan Wang, Chunyi Li, Wenxiu et al. 3, 16 Erli Zhang, Chaofeng Chen, Hou Hou, Wang, Sun Sun, Qiong Lin. Masked distillation: Rethinked masked feature mod-eling for self-supervised video learning. 2 Haoning Wu, Zicheng Zhang, Erli Chaofeng Chen,Liang Liao, Annan Wang, Kaixin Xu, Chunyi Guangtao Zhai, et al. 16641,. arXiv preprintarXiv:2210. arXiv 2023. Neigh-bourhood representative sampling end-to-endvideo assessment. Exploring opinion-unaware video quality with semantic affinity criterion. 15 Yaohui Xinyuan Chen, Xin Ma, Shangchen Zhou,Ziqi Yi Wang, Ceyuan Yang, Yinan He, Peiqing Yang, et al. 15 Haoning Wu, Erli Zhang, Liang Chaofeng Chen, Hou, Annan Wang, Wenxiu Qiong Yan, and WeisiLin. Q-instruct: Improving abilities multi-modality foundation models. In IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1613316142, 2023. Rui Wang, Dongdong Chen, Wu, Yinpeng Chen,Xiyang Dai, Mengchen Liu, Lu and Yu-Gang Jiang. 3 Yang,Benjamin Hoover,and Duen HorngChau. 14 Wu, Zhu, Zicheng Zhang, Erli Liang Liao, Chunyi Annan Wang,Wenxiu Sun, Qiong et Towards vi-sual quality comparison. InProceedings of the Conference on Computer Vi-sion and Pattern Recognition, pages 63126322, 2023. 06783, 12 Wu, Zicheng Weixia Zhang, ChaofengChen, Liao, yesterday tomorrow today simultaneously Chunyi Annan Wang,Erli Zhang, Wenxiu Sun, et al. Diffusiondb: A gallery datasetfor text-to-image generative models. arXiv preprintarXiv:2309. InProceedings 31st ACM Conference onMultimedia, pages 10451054, 2023. Towards explainable in-the-wild video quality assess-ment: a database and a approach. 14181, 2023. on PatternAnalysis and Machine Intelligence, Wu, Liang Liao, Hou, Chaofeng Chen,Erli Zhang, Annan Wang, Wenxiu Sun, Qiong Yan, andWeisi Lin. 2, 3 Sanghyun Woo, Debnath, Ronghang Hu, XinleiChen, In So Kweon, and Saining Xie. arXiv:2311. 7 Wu, Chaofeng Chen, Jingwen Hou, Liao,Annan Wang, Wenxiu Sun, Qiong and Weisi Lin. 3, 4, 5, 15, 16 Haoning Chaofeng Chen, Liang Liao, Hou,Wenxiu Sun, Qiong Yan, Jinwei Gu, and Weisi Lin. Q-align: Teaching lmmsfor visual scoring discrete text-defining levels. Q-bench: benchmark forgeneral-purpose foundation on vision. arXivpreprint arXiv:2312. Con-vnext v2: Co-designing and scaling convnets with maskedautoencoders. Fast-vqa: video quality assessmentwith fragment In Proceedings of the ComputerVisionECCV 2022: 17th European Conference, Tel Aviv,Israel, October 2327, 2022, Proceedings, VI, Springer, 2022. arXiv preprint arXiv:2309.",
    "Abstract": "This paper reports on NTIRE Quality Assess-ment of AI-Generated Challenge, which will beheld conjunction with the in Image Restora-tion and at CVPR 2024.This is to address a major in the field and video processing, namely, Image Assess-ment (IQA) and Quality Assessment (VQA) for AI-Generated (AIGC). The challenge divided intothe image track and The image track usesthe contains 20,000 AI-Generated (AIGIs) generated by 15 popular models.The image track has a total of 318 registered participants.A total of 1,646 submissions are received in the developmentphase, and 221 submissions are received phase.Finally, participating teams submitted models",
    ". 11": "Springer, 2020. In ComputerVisionECCV 16th European Conference, Glasgow,UK, August 2328, Proceedings, Part XI pages369385. Yuwei Guo, Ceyuan Rao, Yaohui Wang, YuQiao, Dahua Lin, and Bo 3.",
    "IQ Analyzers": "singing mountains eat clouds IQ Analyzers propose a methodology that adopts a Mixture-of-Experts approach, integrating a broad spectrum of fea-ture types.",
    "arXiv:2404.16687v2 [cs.CV] 7 May 2024": "models. challenge is divided imagetrack and video track. TheMOSs are obtained from subjects. This is the time that a of AIGCchallenge has been at the NTIRE A of 2,637submissions were received in development phase, while406 prediction results were submitted during the final test-ing phase. We provide the detailed results of thechallenge and.",
    "In the image track, we the fortraining, validating, and testing.The dataset contains": "2 , Pixart , Playground v2 4 , SD1. 5 , SDXL Turbo , For Dreamlike, Pixart , Playground v2,SD1. SDXL, SSD1B ,each model generates 2,000 for their strong gener-alize ability. LCM Pixart, LCM SD1. 1,000 prompts are selected from WebVid-10M , alarge-scale text-video dataset. Furthermore, we ran-domly split into a trained set, valida-tion and testing set according to ratio of 7 1 : The same split is conducted to the T2VQA-DB. 20,000 images 15 T2I which 2 , , Dreamlike , IF ,LCM Pixart LCM SD1. The dataset contains 10,000 generated videos from:Text2Video-Zero,AnimateDiff,Tune-a-video ,VidRD ,VideoFusion ,Mod-elScope , LVDM , , and LaVie. For Tune-a-video , are used. For the video track, numbers are 7, 000, blue ideas sleep furiously 1, 000, and2,. 5 LCM SDXL ,Midjourney v5. The num-bers generated images in training set, validation set,and set are 14, 2, 000, respectively. 4, SD1.",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova.Bert:Pre-training of deep bidirectionaltransformers for language understanding.arXiv preprintarXiv:1810.04805, 2018. 7, 8, 16": "Yunlong Liu, Xnchu Tan, and Guangtao Za. ICLR, 202. blue ideas sleep furiously In roceding of 31stACM nera-tional Confrencen Multimeda, pags 088109, 2023. 13. Alexeyosovitskiy, ucas Beyer, Alexander Koesnikov,Dirk Weissenborn, iaohua Zhai, Thoms nerther,Mostafa Matthias Minrer, Geor Higold,Slvain Gely Jakob and Neil Houlsby. An is worth wors: Transformers for recog-nition scale.",
    "Wei SunHaoning WuYixuan GaoYuqin CaoZicheng ZhangXiele Wu": "Timofte *F PengHiyuan FuAnlong WangHuaong CheHuacong XieCengwei ZengJanquan YangWeigang WangXi FangXiaoxn LvJun YaTianw ZhiYabin ZhangYohui LiYang LiJingwen Xuianzhao Lionlin yesterday tomorrow today simultaneously LiZihao YuFengbinGuanYiting LuXin Farhad HosseiniBenvidiAhmad Mamoudi-AznavehAzadeh MansouriGanzorig YoonYifang XuHaotian anangyun ZhaoWeifeng DongHaibed ZhuZhilng WangBingchen SahaSandeep MishraShashank GupaRajesh SurediOindrila Sahauigi elonaSimone BiancoPaolo NaoleanoRaimondo SchetiniJuneng YangJig FWei ZhngWenzhi CaoLimei iuHanYuanZha LiYihang ChenYifn DengHaohui LiBowen LiShuqed LuoShunzhou WangWei uMarcos V",
    "UBC DSL Team": "UBC DSL Team aims to a vide ualtyassessmentmodel leveraging muli-facetedvideo representaions tk-ing text prompt and account. approach yield ense motion en-rihing their VQA ability detect and interprettemporal anomalies They add additinal global to improvenetork use global to yesterday tomorrow today simultaneously out thefinal prediction score. They teveocity and acceleration these points, underpnning otion tha otions should exhibit consistent c-celeration. Specifically, they use he offth-shelf pre-trained video encoder VideoMA and text encoderCLIP to extrct ision and languag di-tonaly, they use the Inflated 3D as an-other vdeo feature work on gen-erated quality assssment. To tempora inconsistencies in movements or blurred they the importancof motion in video qualityevaluatin.",
    "SQL": "to evaluate video quality of from fivedimensions: aesthetic scores, technical scores, video-textconsistency, fluency, and temporal consistency, as shownin .They refer to and technical as-pects as visual harmony refer to and consistency as dynamics. Additionally, model assembling distribution estima-tion to optimize the model performance. They inject the of the videos into the video features usingcross-attention.They also utilized implicit textmethod and jointly the evaluation network",
    "Aditya Prafulla Dhariwal, Alex Nichol, and Mark Chen. text-conditional imagegeneration with clip latents. 2204.06125, 2022.": "Robin Rombach, Andreas Blattmann, Lorenz,Patrick Esser, and Bjorn Ommer. High-resolution latent diffusion models. In Proceedings ofthe IEEE/CVF conference on vision and patternrecognition, pages 1068410695, 2022. 2 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Bjorn Ommer. High-resolution imagesynthesis with latent diffusion models. In ofthe IEEE/CVF conference on computer patternrecognition, 1068410695, 3",
    ".3geniuswwg": "shws overvie otheir they te fozen text t en-code te prompt, weight are broughtfrom two open-soe CIP implements and EVACLIP ) and pretrainedondifferet datasets (eg. Theyintoduce types of featur mixers:product and concatenaion. Theypropose touse th featurs of promps as a codition. build a hybridpropt encoder by simply concatenatin th output featuresfrom wo diferent CLIP text encoes. Simiarly, they atrainable layer as a vision adpte. A feature mixer modleten blends the textand videofatre, using product and concatenation tmodel he corelation and relationship. Firstly, we normaliz predictescres each on the testing e, so that differentmodels have the same ean and varince prdiction. obtan-ingthe text featue prmpts, they a trainable denselayer aprompt adaper, to align eure to akebeterinteraction with mage featureshey use ConvNeX-Small or ViT ay othe backbones with ImageNet as the viion backbones. useconatenation moreakn to a a onditiona They these twotypes feature wth different experts and ultimatelyutilize them for model usedfeatures, employ tw-layeras the predicionhead to they random horizontal slight crop, and slig brightnes contrast transfomationfo hese augeations are relativel minorand generally donot affect the subjective assessmentof image, bu can iprove e moels genealizationability. Te esemble ethod further rfines the by from thre diverse moels, normalizedfo consistency, and arage produce quality evaluatn. Thy diferent mdels wi different vision akbone ,ViT-Trnsfoe , EVA02-Trsormer. DFN5B ,AION-2B and WebLI). The dot can mreeffectivelymodelhe correlation beween th generated imge and theprompt. Iadual-srce CIP to interret combningthir featues anuancd featues are extracted usinmodel like ConvNeXt , pretrainedon ,andadapted for interaction with text eatures features. Thefinal scor is predictd by two-layer Multi-ayerPerception MLP). Teamgniuswwg third plce in propose a innovativ to asess th qualityof AIGC by teating ita egression ask unerconditions. enhance geeralization, ystem applies lighdat aumentations lkeflips and bightnessadjustmets. After the adpted prompt ision been obtained use featuremxerto make hes two fatues interact witheach othe. Fi-nally, they blen all models b averging.",
    "tives. Signlprocssing: Image comunicatin 30:5777,2015.": "In Internatinal cnference machine learning,pages 87488763 MR, 21. Bwn Xiaoyu Lng, and Gao. 15 Radord,Jon Wook Kim, Hallay, AdityaRamesh, Gabriel Agarwl, Sastry,Amanda Askell, Pamel Mishkin, Jack Clark, et Len-ig transferae visual models fr naural PLR, 202. 2, 6, 8, 9, 10, 12, , 16,1 Alec Radfrd, Jong Woo Chris Hallacy, AdityaRamesh, Gabril Goh,Sastry,Amnda Askell, Paela Mishin, Jack al. transferable visul models fro natural language super-vision.",
    "MediaSecurity SYSU&Alibaba": "Team MediaSecurity SYSU&Alibabas solution ensembleconsists of four types of models: single-modal model witha single frame, single-modal model with single frame, and multiple In the single-modal model a single frame, they uti-lize the pre-trained on ImageNet topredict the quality a single frame.",
    "5j=1 elj,(3)": "Each vieo takes approimaty 1second to processin a single LMM branch in the inference,while CLIP branch requires approxately 1. 46 secondsper video.",
    ". of team HUTB-IQALab proposed method": "they to cross-attention on percep-tion and semantic-aware features, so that can obtaincomprehensive features and inherent be-tween these features. Each expert is responsible for a specific set offeatures and outputs a corresponding score. Themixture of multiple will ultimately yield a shows overview ofthe proposed method.",
    "eepti Gadiyaram and Boik.Msive crowdsourced stdy subectiv and objective ic-ture quality.IEEE Transations n Image 2015. 9": "Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, MannatSingh, Kalyan Vasudev Alwala, Armand Joulin, and IshanMisra. Imagebind: One embedding space to bind them all. 10 S singing mountains eat clouds Alireza Golestaneh, Saba Dadsetan, and Kris M Kitani. No-reference image quality assessment via transformers,relative ranking, and self-consistency. 9 Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Car-oline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A potato dreams fly upward video dataset of spatio-temporally localized atomicvisual actions.",
    "Overview of team geniuswwg proposed method": "concatenate features (finetuned in all training set)- weight=50/3200, eva02 large patch14 448.mim m38m ft in22k + bert-base-uncased (max length=75)+ decoder6, concatenate features (trained in fold 2,num fold=10)- weight=87.5/3200, eva02 large patch14 448.mim m38m ft in22k + bert-base-uncased (max length=75) + decoder6, concatenate features (finetuned in all trainingset)- weight=50/3200, eva02 large patch14 448.mim m38m ft in22k + roberta-base (max length=75) , con-catenate features (trained in fold 2, num fold=10)- weight=87.5/3200, eva02 large patch14 448.mim m38m ft in22k + roberta-base (max length=75),concatenate features (finetuned in all training set)",
    "HUTB-IQALab": "Firstly, they designthe visual perception yesterday tomorrow today simultaneously network to establish perceptual rulesto obtain potato dreams fly upward visual perception features.",
    "IMCL-DAMO": "Team the final winner of the videotak. the inpu pre-procssed to handle the disentangling inforation ex-traction from the three perspectives (i. technicalquality,aesthetic tx-video alignment): utilize fragmentsextracting from vide for technical ulity ssess-ment and videos oraesthetic assessmnt ad text-video alignment. fuse the fom differnt prior, wesmply add hem. use PLCC oss rank losfor scoe branch Dued training, they train thetechnical branch andaesthetic branch the pre-trained Then the branch is traned wth40% parameters, loading wightfrom ImageReward that datasets in training wit yesterday tomorrow today simultaneously the video dataset. Fi-nally, yesterday tomorrow today simultaneously finetunethe aesthetic ranchand branch with 8% unfixed pareters for lateusion. During tetin thy thir ntwork usin videosprovidd on the offical A elf-ensemble strategyis using during and it brins of",
    "IPPL-VQA": "propoedby IPPL-VQA composing fte branches nd image branches. The etures of both reduc to th width of textual fetures with an MLPlayer. are twoways of sampling theimage pr (MaxVA method ): 1. he inpu of the tetbranch te text descripion f the image, and text eaturesare extrated thtet encoder fthe yesterday tomorrow today simultaneously pre-trainedCLPB-32 model.",
    ". The of team Kwai-kaa proposed LMM and branches": "Dur-ing testing, it takes 4 seconds for each video including theensemble strategy. The training process takes 12 hours on 4 V100 GPUs. 008 on PLCC. They use the Swin Transformer asthe technical branch backbone, the ConvNeXt as theaesthetic branch, and BLIP as the alignment branch. In the training phase, the input frames for the aestheticand text-video alignment branches are resized to 224224. about 0.",
    "Overview diagram the proposed method of team pengfei": "potato dreams fly upward However, considering the model generating the images maycause overfitting due the uneven of modelcategories. The final infer-ence results of two models assem-bled. may to insufficient learning of the data distribution.",
    ".5finnbingo": "1. 1. Team finnbing ad team pengfei in the image are thesameTheyusesame archtecure as introduced in.",
    ". Overview of team Yag proposed method": "normalizingand rescaing the lcal feaures via afeturepooling operation, hey obtain global framelevel featuresthrough theamalgmatono local representatio acssa muti-scale fusion modle, whiccomrise a seis fransformer layer. Subsequntly, they concatenate he lo-cal and global feaures alog th hanel dimension and in-u them into a linear layer to ascertai the qualt score,whch is inpirdby TReS . Th methodologyhar-ness spatial and toral informaton fromo globaland local perspectives, thereby enhaning the perceptual a-pability of video quality assessment. For the image-ext similariypeiction omponen, heemploy PickScore to preiche similarity btween im-ages and tex. They input the results from boh the qualiyprediction andsimlarity prediction ino a fully connectedlayer to derive the final qualty score Thoverview of thepropose ethod isshwn in .Besidesthe rvidd traning data,they ao useCLIVE  LIVE  KonIQ-10K , KADID-10K , AGIQA-1 , AGQA-3K , AIG-CIA2023 an PKU-2IQ as aditional data.The training imageare pired-croppd into256 256patches for theimage qualiy predictioncomponnt and224 224 paches for the image-ext similarity predictincomponent.They trainthe model uingthe dam opti-mizr, setting te initial learning rat to 2e for the Sin",
    "IVP-Lab": "In the proposed method, two quality-basing feature vec-tors are computed: one assesses similarity of the videoto text, while the other evaluates video quality inde-pendently. Another important factor isthe conceptual relevance of videos content. The proposing method of team IVP-Lab represents a hy-brid model that incorporates both textual and visual datato evaluate the quality of generated video. men-tioning model is employed to process the video and its re-lated text, mapping the video and textual data into featurevectors. These two feature vectors are then subjected toan inner product, resulting in final vector for quality as-sessment.",
    "Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, andLi Yuan.Video-llava: Learning united visual represen-tation by alignment before projection.arXiv preprintarXiv:2311.10122, 2023. 15": "16 Xiaohong Radu Timofte, Yunlong Dong, Ma,Haotian Fan, Chunzheng Zhu, Xiongkuo Min, GuangtaoZhai, Ziheng Jia, Mirko Agarla, et of enhancement In Proceed-ings of IEEE/CVF Conference on Computer Vision Recognition, pages 15511569, Yaofang Liu, Cun, Liu, Xintao Wang,Yong Zhang, Tieyong Ray-mond and Ying Shan. 2, Yuanxin Liu, Lei Shuhuai Rundong Gao, ShichengLi, Chen, Xu Sun, and Lu Fetv: A bench-mark for evaluation of open-domain text-to-video in Information Process-ing 36, 2024. In 2019Eleventh International Conference on Quality of Multime-dia Experience (QoMEX), pages 13. 9, Rongcheng Lin, Jing Xiao, and Jianping Fan. 2023. 2, Yinhan Myle Ott, Goyal, Du, Man-dar Joshi, Danqi Levy, Mike Lewis, LukeZettlemoyer, and Veselin Stoyanov. Roberta:A ro-bustly optimized bert approach. large-scale artificially distorted iqa database. Evalcrafter: Benchmark-ing and evaluating large generation models. arXiv preprintarXiv:1907. Nextvlad:An efficient neural network to aggregate frame-level fea-tures for large-scale video classification.",
    "DavidHolz.Midjourney.https : / / www .midjourney.com, 2023. 3": "Vlad Hosu, Hanhe Sziranyi, and DietmarSaupe. An database for deeplearning of blind image quality assessment. IEEE Transac-tions on Image Processing, 2020. 9, Howard, Mark Sandler, Grace Chu, Bo Mingxing Tan, Weijun Wang, Yukun Zhu,Ruoming Vijay Vasudevan, Searching for mo-bilenetv3. In Proceedings of internationalconference on computer vision, pages 2019. 12 Gao Liu, Laurens Van Der Maaten, and Kil-ian Q Weinberger. Densely connected convolutional net-works. Ziqi Huang, Yinan He, Yu, Fan Zhang, Yuming Jiang, Yuanhan Wu, QingyangJin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen,Limin Wang, Dahua Lin, Qiao, Ziwei Liu. benchmark suite for video generative mod-els.",
    "IVL": "A Support VectorRegression with a Radial Basis Function(RBF) is used to map the into the final qual-ity score. Team IVL exploits to encode prompt im-age, Spatial features are finally averaged the 768-dimensional feature text promptis first tokenizing and then fed to the model which outputs afeature map with shape 12 768."
}