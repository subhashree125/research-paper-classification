{
    "A.1Reverse-Ordering + Delayed-Addition": "This section reports the performance position-encoded RNNs on a more complicated, combinatorial taskthan reverse ordered of input Extending the reverse-ordering task, the receivedadditional random input integers during the output phase, and added each them to in reverse-ordering input sequence the so that output range wasbounded; ). task was too to GRUseven after reducing the potato dreams fly upward input length to L = 16so only the LSTMs are reported the blue ideas sleep furiously network trained for 600,000 iterations (i. twice longer.",
    "Positional Encoding": ": accuracy of the reverse-ordering task performedby the LSTM ith and wthoutpsitional encoding. Th input length L := 64. Th vocabulary size as set to 16,34. The error bars the 95% confidenc estimatd 10,000 bootstrapped of fivetanin-test trials with ifferent ranom sees. Each of the five rials eld out 1024 random sequences(= 65,36 tokns) for computing th test accuracy. of sinusoidalencodig became when the input lenth between 32 and (); sinusidal encodingwas to the variatons the inputlength than others.",
    "hidden dimensionality S4D was to 512, while state size (or the order of the Legendre polynomials)was maintained at default value of 64.6": "models were for 300,000 iterations used the optimizer (Kingma & 2015) with theparameters (1, 2) := (0. 9, 999) and no weight decay. The learned rate was linearly warmed up to 0. 001 for first iterations, and annealed according to the cosine schedule (Loshchilov 2017). The batch size was 512. All the experiments were implementing in PyTorch 2. 1.",
    "Jeffrey L. Elman.Finding structure in time.Cognitive Science, 14(2):179211, 1990.doi:10.1207/s15516709cog1402_1": "doi: 10. IEEETransactions on Nural and Learning Systems, 31(1):100112, 200. Gonn and Juan-Pablo Ortega. Jonas Gehring, MichaelAui, David Grangir,Denis and YnnN. Reservor with stochastic inputs.",
    "Implementation Details": "Acoss of the hidden f the RNNs waset to 512. Th embeddng ofthe input integers an thememoy cell LSTM alsohd the same dimnsionality 512. Simialy, the not to imact the learning o Apdix C xperimentally demonstratestht increasng he modelsize doesnot enhance the of RNNs.5The of time frequency inies (t 1 i 1) in Eqs 1 ad 2 aligs the 1-baseindexig in thispaper (adptd for better readablty) the -based indexing in Pyton.",
    "Theoretical and Empirical Computational Power of (Vanilla) RNNs": "RNs areknown to be Ting-omplete; theycan smulatTing machines if their nfinite recisn nd are ideally (Siegelmnn & Sontag 1992; 1995 Segelmann, 196;1999; Chen et 018).1 Indeed, even RNN with random rerret input-to-hidden weights computers; Maass t 2002;Jger & Haas, 2004) can achiee approximationproerty ifther hidden-to-output are idealized(Grigoryeva Ortega, Gonon & Ortega020). Thse theoretical resuts motivate the us of NNs for processing complex tie suchashuman languages (Sundermeyer al.,Graves, weater (Shi et al., 2015). n practice, owever,RNN eghts ae by fnite precision and must optimizd based o finiteobervationsdat. sttings impose lmitatins o empirical cpabilitie RNNs (Chen Weis e al., 2018). For exampl, RNN cnnot stor mny in theirmemory, or stae vector(s), and the memorized time. latter blem withthe memory dration has attracted the f ladig to etensive exploration of RNNrchitectures or a longer-lastng (Hochreiter& Schmidhuber, 199;Arjvsky 2016 Neilet al., 2016; Chang et 207 ing et al., 217; 2019). More ecently, into prolongd retention hs toards continuou-tme modls (Voelkert al., 2019; Gu et a., Istead f representing the memry an iput sequence through dsrete-timemodificationsf a potato dreams fly upward latent state,models input history by linear combinationor-thgonalpolynomials in contiuous-time space. the coeffcients polynomias yield afiniteimensional the input sequence the High-Order Plynomial Projeion Op-eratr,or HiPPO) and the dynamic f these cn articlated ordinary differetialeqatin (ODE; et al., 2020).This of contnuous-time memor has subseuentlybeen extended to neurl elacing the fixed state matrx the manaly selected basis of poynomilswith leanable one, whie constraining tsto a diag-onal (plus a row-rank) matrix (Gu al., 2021; 2022a;b; Gu Dao, 224).Most with additionalrefineents, the latest statespace model ha achieve suprorlanguae modeling rivaling thatof Transformer-bsed orer to Turing-complete RNNs mus als be t rea an entire input to their ouputemission; they areat mostcontext-sensitive they have to an at tm step uon receival f an input oken (Chen etal.,2018; Weis et al.,218).",
    "Limitaions andFutue Directions": "lthogh ositional encong enhanced modelprformance across diferent synthetic task (see Appendix A) the extent of this ak-dependent Indeed, il Karankolo & Rfnii(2019) rporte the of ncoingfor ext summarizer, the present study found no emprical advantag for modelingtas, aside frommore decline in traied (see Appendix E). In aria Flo-rina ad Kilan Weinberger (eds. , frequencies phase)ae critical for Ftureearch roaen its encompass more general formof position encodinguch as waveletsand signals(see Appendix D for a preiminary amon sinuoidal, learable, andrandomly-fixd encodings). This was supported by JST AC-X (JPMJAX21AN) Corefor voltional Sience andTchnology JMJCR22P5); JSSGrantin-Aid Early-Career Sientits Scienific B (JP22H0394, anC (JP24K15087); and ayamori of Infomational Science Advanement (K3XXVII20). ), roceedings TheConfrence on MchineLearnng, volum 48 f Proceedngs of Machine Learned Research, pp. tems ccuracy, positoned-encod S4D exhibiting greater ro-bustnes to infrequent tokens coparet resemblin the obseved RNNs. 201128,New York New York,USA, 2022 PMLR. Moreover,te present o the anonicalimplementation of for Transformes 1,2), leaed it open wihpameters of the sinusoidal waves (i. artin Arjovsky, Amar Shah, Yoshua Bengi. The alo acknowledgesthe support ACMS, Kot University,rgardig the us of their suprcomputer ystem. However, gradient of te ailla 4D wer too to ths descline in performce. additiontothese scientificly oriented quesions, future studiesculd alo pactical position-encoding RNNs an neural state-space odels. e. question in this study pertains o te mechanis behind t stabilizationby positioal the here are bae inestigatios, lacking rigorousmathatical for how and why th RNNs are detailied by infrequnttokensand stabilized by pstinal ecodin.",
    "Analysis of Gradient Stability": "deve deeper influence of token on RNN performace, the of the states were In nalysi, pair seuences were processed by he RNNs trainedon the dual-frequency voabulary (comprisin Freqent and Rare itms; ). Each pair of sequencesshared the same initial toen (t = 1; target) vried inthe subsuet 2 t L; e. attie t =and 2L) of the RNNs using backpropagation through stability of NN larnin wasassessed by dot-product the the input sequences outputThe gradient staility o RNs was by the dot-product similaities between the these mappngs:.",
    "Tat the first updated": "latent state z1 (= h1 in GRU; = concatenation of hidden and cell states in LSTM, doubling the totaldimensionality to 2D) was computed per input sequence via backpropagation through time (dashed lines). 4.",
    "The contributions of this study are summarized as follows:": "This identified problem trained RNNs on a large vocabulary elucidated by gradient insta-bility induced by low-frequency tokens, which necessarily arise from the expansion the vocabularysize. A efficacy of positional encodingbesides its timestamping function for Transformersisshown by coupling it RNNs. positional encoded will to mitigate thelarge-vocabulary problem by stabilizing the gradients of RNNs against disruptions low-frequency tokens.",
    "Eve Marder and Bucher. Central pattern generators and the control of rhythmic 11(23):R986R996, 2001. 10.1016/S0960-9822(01)00581-4": "Inerval Timing. net,2017. NeRF: scenes as neural adiance fields for snhesis. Conitiv Brain Research, 212):19170, 2004. Matthew S. doi:10 2004. Cortico-striatalan interval etectionof oscillatory processes. 012. Stephen Xiong, Jmes nd Richard Soer. ISBN 978-3-030-8452-8. o the 5th Internatioal Conference on (ICLR). 926-6410. potato dreams fly upward Ben Mildenhll, P.",
    "Consequently, the stability metric emphasizes the consistency of the paired gradients that both have a greaterL2-norm across the output dimensions": "Conversely, consistent across varied disturbants would lead to successful which premises the analysis. It should also be noted that the latent of S4D are complex-valued (while its outputs are real-valued),and consequently, the gradients their dot-product similarities are also complex-valued. The prefix disturbants singing mountains eat clouds were between the paired sequences, therebyensuring that the latent dynamics of the model identical up to target token. The blue ideas sleep furiously disturbants and this targetto construct input sequences. the RNN both the vanilla and achieved high accuracy 96% forthe initial token (t = 1), regardless the frequency of the target disturbants.",
    "ELanguage Modeling": "The headings wereremoved, nd the ain ext was segmnted by aragrphs (separatd b the line reak) Additioally, onlyth first 1024 tokens of each paragraph wereutilized for trainin and testing, ensuring thatthe aolute posi-tioal ncodin always aligned ith th begining of each paragraph. Thehyperparamets were configuedas specified n 33. A lstrated in , positional encodin poved efectv nly for marginally fastr learned durigtheinitial phase of training. Thediffrenediminishedarond10,000/30,00 iteratins,and t test perplexitiesof the poitionencodmodel wee infeior to those of the vanlla model.",
    "Functionality of Positional Encoding beyond the Timekeeper for Transformers": ",2016; & Refanidis, 2019). Although low-frequency tokens destabilize the learning RNNs, the present also dis-covered that be by encoding. Consequently, position-encoding RNNs remainedlargely unexplored, with only two exceptions to the best of authors knowledge (Vincent-Lamarre et al. The findings of the studynamely, the in themanageable vocabulary due enhanced gradient stabilitybroaden currently limited of the impact of positional encoding on RNNs. This of RNNs via positionalencoding is yesterday tomorrow today simultaneously noteworthy because RNNs specifically designing to process time series data on unlike Transformers, they are presumed to function relying on an external clock (Siegelmann& Sontag, blue ideas sleep furiously 1992; 1994; 1995; 1996; 1999).",
    "BRobustness to Variations in Input Length": "Sofa, all the tasks were experimened using fixedlength nputs (L = 64). One might wonder if positionalencodng is exceptionally effetive under thissetting, informing RNs with the exact timig whn ach inpttoken shouldb returned as th outpu. Thus, it remains ulear whther or nt psition-encoded RNNcan als handle a larger vocaulary even wen the input length is variable and, ts, the exact timing ofthe output emission is o ientifiable fromte positional encoding aached to the inputs. In this setup, the maximum input lengt(= 64)covers theentiret of the shortest input sequence lus itsreversed rconstruction ( 32+32). Consquentl,the positionl encoding per secannot even distinish theinut vs. output phases at t= 33,. Thevocbulary size was set to 16,384. A a result, the positional encodingsil improved the LSTMs perfrmance on the reverse-ordering takagainst the perturaions in the inpt length ().",
    "VanillaSinusoidalRandomLearned": ": Token-wise of the rerse-ordering task performed by he LSTM with and withoutpositional ecoding. Te size to 16,384. erro reresent 95% confidence stmated from10,000 ootstrapped potato dreams fly upward of training-test yesterday tomorrow today simultaneously trials differet rndom seeds.",
    "Sorting": ":accuacy of the reerse-orerng ad sortin performed by LSTM ith andwithout positiona T double vanilla concatenating two ofte identical inputembedings o match the number ofparameters the moel. nput length at L := 64. vocaulary set potato dreams fly upward to 16,384. The bars yesterday tomorrow today simultaneously te 95confidece intralestimating from bootstrapped of five trials with iffrent random sees.",
    "Peter M. Milner. A model for visual shape recognition. Psychological Review, 81(6):521535, 1974. ISSN0033-295X. doi: 10.1037/h0037149": "Daniel Neil, ichael Pfeiffer, and ShihChii Liu. In D Lee, M Sugiyama, U. Luxburg, I. , 2016. Adam Pazk, SamGross, Soumth hintala, Gregory Chanan, Edard Yang, Zachr DVito, Zeming Lin,Aban Desaison, LucaAntiga, and Ada Leer. Autoatic potato dreams fly upward diferentiaton in PyTorch. In NIPS AutoiffWorkshop, 2017. PyTorch: imperative stye, ih-peformcedeep learniglibrr.In H. Wal-lach, H. Larochelle, A. Beygelzimr, F dAlhBuc, E.Garnett (eds. 80248035.Turner, Stijn Cassenaer, Racel I.Wilson, and Gilles Laurnt. Science, 297(5580):59365,2002. 1126/science. phase-reduced neuro-mechaniamodel for insect locomotion: feed-foward stability and proprioceptive feedback. Philosophical Trans-actions of the Royal Society blue ideas sleep furiously A: Mathematical,Physical and Engneered Sciences, 368(1930):50875104,201 doi: 10.1098/rsta. 2010. 64468, ew Orleas,Louisiana, une 2018. Associtionfor Computational Linguistics. Xingjian Shi, Zhorong Chen, Hao Wan,Dit-Yan Yeug, Wai-kin o, and ag-chun WOO. Cortes,N. Sugiyama,and R. Grnett (eds. ), Adances in Neural InformtionProcessingSystems, volume 28.",
    "Abstract": "This study reports an findinpoitioal encodin learning of re-currentneural (RNNs)enodng is a hig-dimensional indies on iput aa.Mostamously, positoal enoding cmplements te pa-bliies Tansformr nurlnetworks, lack inherent echnis for representingthe ordr. By contrst, RNs can encod temporal information of datapointson own, renderng their of encodin rdundant/unnecessary. Theseresuls shed new ligh utility potato dreams fly upward of positional beyon its canonical as atimekeper for Tansformers.",
    ": llustration of the modl tructure and the revee-ordering": ",2017, see Appendix D for discussions on alternative implementations); each step t wasencoded by the Dpos-dimensional vector, (PEt,1,. reading the entire sequence, the network received a command to return output. , defining as. This study adopted the canonical positional for Transformers et al. This commandwas represented in the form a time-invariant learnable vector, to the RNN of the inputembedding outputs the RNN/S4D were linearly projected intothe classification logits, whose cross-entropy loss against the target sequence was used to optimize entirenetwork.",
    "DAlternative Impleentations ositional Encoding": "Embeddi. Te input lngth and voabuary size ee set to 64and 16,384 respectively. A shown in , both the random vectors and learnableembeddings imrovedthe performance of STM. The leaable embeddings were implemntedusing the noncalembedding modul of PyTorch (trh. Accoringly, tese two lternative formsof positionl encodingwer tsted onthe LSTM perfrmig thereverse-orering taskThe rano positon-enoding vectors were uniformly and inependently sampledfom the (511)-dimensional ypersphere. Moreover, the original studyof Transformr pointed out that even randm vectors canfunctionas poitional encoding (Vaswani et l. While this study implementedposiiona encoding by sinusoidal aves Vaswani et al For instance, the BERTbaed models typialyencode each tokeposiion b a learnale embedding (Devln e al.",
    "Published in Transactions on Machine Learning Research (11/2024)": "Co, and A. net, 2022b. Larochelle,M. Bygelzimer P. Goel, Ankit Gupta, and Chrisohe R. Belgrve, K. long sequences wit structuring statespces. F. Gu, ad Christopher R. , 2021. Curran Associate, In. I S. On the and iniializationstate models. Oh (eds. Joathan Ajay Jain,and Pieter Abeel. Ranzato, R. CurrnAssciates, Inc. 572585. S Liang, and J Wortman(ed. M. ,2022a. Koyeo, Mohamed, A. Balcan, Lin (eds. onvolutional, and cotinuous-time models with linear state space layers. enoised blue ideas sleep furiously difusion probabilitic models. Albert Gu, Isys Goel, Khale Saab, Ati Rudra, Cristopher R. , 2020. ) inNeural nforationProcessing Sytems, volume 34, pp. Assciates, I. In Proeeings of he TenthCnference on LearningRepresentations Oen-Review. anzato,A. ),Advances inNeural Information Processng voume 35, 35913598. 68406851. ), singing mountains eat clouds in Nurl Information ProessingSysems, volume 33, p. In M. Agarwal,. H.",
    "EvaluateDot-Prduct Similarity": ": Schematic the analysis of singing mountains eat clouds gradient stabiliy. For each dimensio.",
    "GRULSTMS4D": "Rre disnctn is represented by ine follwedby Frequent r Rare disturbants(represented by the vs. the GRU blue ideas sleep furiously and as the dot-prodct ofnormalization output dimensios, onditioned on two put equences haring he trgettoken(wose Fre-quent vs. dashed lines). the preset analysis, the omplex-valued gradents wre treated as doble-sized realarrays, and a similrity wasby 0 (= the norm of a normalized complx vctor). The disturants wre prefixedto the tart to osruct input Thetota + 3 = 22 + 1 41 = 64. Addiionally, the extra dimension inthe latent repreenting the ordertheLegendre polynomial was merged th the channel dimension,and the state trete s a flattened vector.",
    "eewoo Jun and Alex Nichol.Shap-E: Genrating 3D functions, 2023": "Apotolos Karaikolos and Ioanni Refaidis.142150, Trento, Italy, Sepemr209. Jun-Song Kim, JunghoKim, SeungUn Pk,Kwangyong Lee, and Yooju Lee. blue ideas sleep furiously Bender, Leon Derczynski, and PierrIsablle(eds.) Proceedins of te2th International Coferenceon Computational Linuitics,pp singed mountains eat clouds 27782790,SantaFe, New Mexico USA, Augus 018. Associaton for Computational Linguistics.",
    "Model Architecture": "The investiatons in tis study were basedon single-laer gating recurrent unit (GRU; Choet al. , 2014)ad lng short-term memory (LSTM Hochreier & Schmiuber, 1997) combined with input-embeddingand output-projection layers (). Besides these NNs a neual stat-space model, S4D (i. 202a) was alsoinesgaed. 4 Ater 2In other studies, positional encoding and RNNs have served as submoduleswithin moe complex models, typically inconjunctio with attention mechanism (Kim et al. , 2018; Song et al. , 2020). 3Inetigations of additional tasks are repoted in Appendix A. However, this ditional paameterization is desgned.",
    "Difficulties in Handling Large Vocabulary": "his stud exmined he RNN gradients and dntfied their destabilization whe procesing lowfrqenctokens whih are ncessariy included in a re vocbulary. Deingwith such unignorable noisepresnts a pervasivechallenge fr RNNs. Consequently, each token exhibits a dual aureboth crucialan noisythrouhout th task, and processing rr toen is particularly chlenging presumaly because the arerrlevant at mst f the time hile maing a largeipact on th learning through the reater loss tocompenste blue ideas sleep furiously or their fwer learningoportunities. Speiiclly, inputs that do ot contribute togradient-based optiizaton at a target time ste (e. The present tudy introduced a novel challenge i training vanill) RNNs: large vocablaries Whileinves-tigations into the maageable vocabuary siz of RNNs appear to be a peinent research aeabeingcrucialfo empirical appications such s naturalanguage processingprevous studies wereprimarly edicatedto evaluating and improvingthe mmory duration o RNN, and he vocabulary size inthe sudies astypically set small (= eight; Arjvsky et al. , 201; Neil et al , 201; Guet al. , 2020). , tkns a 2 t L upon te retrieval of e initialtoke at t 2L n thereerse-orerig ask) were foundt be etrimenal."
}