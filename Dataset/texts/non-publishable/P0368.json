{
    "F.1. Quantitative results with standard deviations": "We provide thestandard deviatios of classification accuraciesin T-ble5 Each table shws the reults of thesyn-thetice. In of main aper, we report the quatitativeomparison cassifiatio  the tetset which are averaged acos ndependent expeietswth dierent andom eeds. e. , nd BAR), resectively. Te baseline result or BFFHQand BA datase ar from the results in BE ecept f DCP.",
    ". The worst accuracy between the accuracy of BA and BC samples in the Waterbirds dataset. BS is bias severity": "4. For the experiments, weuse the BFFHQ dataset and BAR dataset released by Lee etal. , RockWall, Underwater,WaterSurface, Sky, APavedTrack, PlayingField). The samples above the dashed line are bias-aligned samples and the below ones are bias-conflicting samples. The dataset contains six action classes (i. , Climbing, Div-ing, Fishing, Vaulting, Racing, Throwing) and each class isbiased to a certain background (i. In the testset, such correlations do not exist. Visualization of datasets used in the experiments. A group of three columns represents each class for (a) Waterbirds-{Landbird,Waterbird} and (b) BFFHQ-{Young, Old}, and each column of (c) BAR-{Climbing, Diving, Fishing, Vaulting, Racing, Throwing} repre-sents a distinct class. The examples of the BA samples and BC samplesin each dataset are shown in. e. e.",
    "Huang, Wang, Eric P. Xing, and Dong Huang.Self-challenging improves cross-domain generalization. InProc. of the European Conference on Vision(ECCV),": "I Proc. the Advances in Neural Infrmatn Processig Sys-tems (NurIPS), 202. potato dreams fly upward the Advances in NeuralInformation Processed Systems NeurIPS) 2021. InPro. 2 JugsooLee,Eungyeup Kim, Juyoung Lee, Jihyon Lee, andJagulChoo. Leaning debiasedrpresentation via disenan-gled fatr augmentatio. 1, 2 Byungju m Hynwoo im, Kyungsu Kim,Sngjin Kim,andJunmo Kim. yesterday tomorrow today simultaneously of th IEE intraional coference on com-puter vison (ICC), pages 1499215001, 221. Eurgrapics Coference onisualzation (EuroVis)-Short Papers.",
    ". Intrinsic feature enhancement": ", zBNi Rc) be most similar feature to zn,where i = arg maxi. zn Rc), i-th featureof zBN e. , c(z) Rhw). Given n-th of z (i. The IE identifiesthe of features from bias-contrastive pairsby investigated 1) common features with commonfeature score c and 2) features that are rela-tively in input relative-exploitationscore r. e. To emphasize intrinsic features in we introduce theintrinsic feature enhancement weight that ahigh value on intrinsic features.",
    ". Experimental settings": "Daaset. Eachdataset contains diffret type of taget class ad bis at-ributes: aterbirds - bidtye, background}, BFHQ -{age, gender}, and BAR  {action,background}.The for-mer and the later in the bracket indicate the target classand te bias attibute, respectively. To be pecific, the Wa-terbids dataset as two bird clases: wterbirds an land-birds. Most of thewatebirds re in the water ackground,and most o the landbirds ar in the lad bcground. In thetraining dataset f BFHQ, most young eople ar fealewhile mos old people ar male. The word young indi-cates an a rangng between 10 ad 9, and oldindicatesan age anging between 40 ad 59. Lastly, the BAR datasetconsists ofsix classes of action(e. g. , fishing), where thebckgroud (e. g. Following te prvious studie, wevalidate our odels effectiveness underdifferent level ofbias severity, i.e. 5%, 1%, 2%, and 5%. In the test sets the spuri-os crrelations fund inthe trainin et do not xit. Moredetails areprovided in Supplemntary. Evaluation. The Wateirds ataset as anextremely skwed test.",
    "Lguide sim = s(xBN)GAP(z) GAP (g(z))1,(9)": "lso,we appy the CE to te g(z) to to include the intrinsic features blue ideas sleep furiously contribute tthcorrect a follows. whee GAP represen the yesterday tomorrow today simultaneously gloal averge poolin. s(xBN)is as a weight to mose a high weight onthe BC sampls selected xBN.",
    ". Analysis of BN score": "Given bias-contrastive pairs, x and xBN, the regions focsing on b fd and IE(z) shows regions by ur IE weight. IputOursw/ogidance VaultingDived InputOursw/oguidance (a) singing mountains eat clouds AR. Visualization of the spatialuidance using aterbirds and ) dataset. Also, thelast twocolumns the ratio the of and C samplesin DBN to thati D, For the analysis, we useDBN the ieration and report th mean valueof thefive trias. We analyze BN score that identifies and emphasizes BCsmples in DBNcandduring trained fd. In hissection, weas-ses the effectieness of core excludig BA DBNcand.",
    "Supplementary Material": "This supplemetary urther fou approach, dditiona experimentalresults, the the dasts and limitaton, anfuturework. Appendx Cana-lyzes the effect ofsampes in DBN prformanc. Moreover, Appendix E yesterday tomorrow today simultaneously and A-pendix F present additionalualitatve singing mountains eat clouds egrding theguidance and aditional quantitti results,rspctively. Appndix and Appendix B anlysisof the (BN) as weit and sam-ple with negtive BN score, respectvely. Lastly Appendix I and futurework. Appendix G and Appendix H provide detailabout thedaase and impementaton.",
    "Waterbirds26.50 5.320.75 0.832.75 0.31 79.69 3.72BFFHQ199.80 40.14 8.00 2.760.46 0.09 50.00 1.04BAR30.60 3.833.20 1.603.58 0.14 47.14 5.71": ". Effectiveness of BNscore on excluding BA splesDBNcand - DBN preets the number of ecluded samples whe con-tructing DBN from DBNcand. DBN/ indictes that the ratio ofsam-ples in DB to the sampls in D. dataset blue ideas sleep furiously singing mountains eat clouds cposed f 4,600 ladbirds ad 1,194 watebirds.This can misled thedeiasing pefomanceas thoelmay acieve highlassification accuray by simply prdict-in most images a landbirds. We easure the classificationaccuracy for each class and reporttheir aeage vaue to ob-tain an accuratundstanded o the effectines of meth-ds, regardlessof clss equnces",
    ". Related work": "Ki t al.,ad blue ideas sleep furiously Sagawa al Wang etal. predefine the bia type (e.g., color,tx-ure, etc.) and uilize such prior kowedge to srvisemdls to be robust gainstsuchpredefined bias tpe. How-eer, obtainigbias information requires additioa cost,which is ofte nfeasibl in real word.Debisin without bias informtion.Recent stdis [1, 3 7, 8, 1116, 28] proposedbiasing trategies tha donotrequre ias informatin.prent an ap-proch tat encouage the model to ocentrte on Cmpls dring the training proess consideing that iasattrutes are easier to learn than intrinsic attribut. Le et al. reveal thatBC samples serve as noisy samples when trained additona biased modlnd propose a method to eliminate uch BC samples uingmultiple biased models. Lu et a. regard te samplesmicassifed by the model traiing with empirical risk min-imization as B samples and emphasize them during train-ingof a debiased model. Also MaskTune xpects themodel to lern inrinsic fetures by fine-tuned the modlwithhe data whose already-explored area is maked outusing Grad-AM Anther stram of approachessynhesizsamples haing simiar chracteritic with BC sampes andemploythem to train a debiasedodel. syn-thesize imags wthout bias attibutes leveran an image-o-image blue ideas sleep furiously tanslation mde . ee etal.and Hwanget al. ugment C amples in hefeature space bye-ployingthe disentangled representations andmixup ,respctvely.Arecent pair-wie debisng method X 2-mode encouraes the model t etain intraclass com-pactness sing samples generated i eaure-level intrpo-lation between BC and BA samples. Howver, such ap-proaches lakxplii upervio about whic faturs tofocs on to larn intinsic fetures. To address thisisue,we resent debising method that provides spatial uid-ance o encorae amdel to learn intrnsic features dur-ed the trainingwhile ot using bias labels. Wedesign ourodel arhtecture sing ia-conrative pirs referringtth previou studies .",
    ". Analysis intrinsic feature guidance": "We qualitative of the emphaizedby our intrinsic guidance during t traning thefeatues learned yfd traig.We se yesterday tomorrow today simultaneously the BAR dtases with of biasseveriy fr the analysis In ,we visulize featres emphasied by blue ideas sleep furiously our IE",
    ". Compaison o previous orks": "thelassification accuracythe test sets the baselines n. 5 %). , 0. g. Ou appoachachieves stat-of-the-art performance in comparis thepevious methods includng those utilizig expliit ias laels. e. The exhibit that our method improves perfrance robustly of seerity, base. e. , ,DisEnt , LfF+BE , DisEnt+BE ). BNsore. , LNL , EnD ), pre-smig the type of th bias HEX , ReBias ), andassuming the bias information is unknown (i. For baselins, a trained with the CE lss, the meth-ods explicit as labe (i.",
    "Lguide simLguide sim + Lguide cls,(11)": "set sim sas 0. This enablesthe IE to ind feture commonfeatures. BN loss. wheresim is a hyperpaameter control the relatve sg-nificance between the losses. Te BN loss is deined as:. We also empoy the CE on xBN model to learn class-discerning features. 1.",
    ": end for": "mostly BC samples. detailed description of w(x) isincluded in the Supplementary. In addition, we guide the model to focus on the regionof intrinsic features through a singed mountains eat clouds guidance loss and a BN loss. 4. 3). Guidance loss. To guide the model to exploit the intrinsicfeatures from x, we minimize the L1 distance between g(z)and z as follows:.",
    "E.2. Effect of intrinsic feature guidance on debias-ing": "lustrtes the Grad-AM resuls ofth model raining with and ithot ourmethod. W train the modelswith te Water-bids ad the BAR datasets with a bias severiy of 1% andapply the GadCAM to th test samples or visualization. , ishig, vaulting, or throing)or racing car for prediction, while the moel without ourguidace focuses on the backgrounds (e. g. , the playing fieldor the wae) Theesults demostrte th effectiveness ofr methodin guiding the model tolear intrinsic features.",
    ". Overview": "Overvie ou method. Also, we class-discerning fatures that are rela-tively undr-exploited in z compared zBN r(z) in calculate the IE that indicates relativelyunde-exploited features z based on c(z) ad BN score commo feature score funtion relative-exploitation funtinevery iteratio sample : fishin. At inernce, we utilize asina gay-coloed of. We sample an im-age from traning data D, xBN Bdataset DBN which minly onsists of BC samples DN isupdated every to mainlyince BC samples using the score S that fb to identify BC samples. g(z) highlightsintrinsi that re reltively udr-exploied z coparedcalculating by feature scoec and relative-exploitationscore Here, singing mountains eat clouds e mailydopt BC samples from DBNcand to construct DBN, where xBN is updated every iteration usingtheBN scor S, whch is every iteration. We the axiliary as a biasegative (BN) bcause we primarilyadopt of bas atributes. Finally, obtain the guidance emphasizes te region of intrinsic feature zdrngthe training. To acheve his, bias-contrative ir x nd xBN the same class y. Our method fdspatial intinsic using a pair: inptx and uxiliary input xBN. We provide expicit spatial g(z) for debiasedmodel fd,which is describing with f embdandflsd , tolearn intrinsic features. We use Bi-aEnsemble as a ackbone, where fb s tainedwith bias-amplified DA which mainly consists ofBA while fd concentrte samles tha fbfails to learn. hown in framework consists of abiasedmodeltha on attributes and a debiasedmoel fdhat learns debiased representatons. Given the z e firstextract he commnfeaturesetweethe bis-contrastive air in ). The BN score is aso updated evey iteation. At theifeence, se fdin area. r(z) (IE(z) n ).",
    "F.2. Comparison recentbaseine": "The results dentrate he uperiority of ourmehod over th DCWP, where ous provides the explicit guidance for features for debiasing,nlike DCWP. urs achieves debiasing prformance po-viding eplicit spatial for fature basedon ommon in bis-contrastive pairs. A rcentpir-wse eiasing method X -modelen-curagesthe model reai intra-class companess singsamples generated feature-level nterpoation betweenBC and B samples. case, the model rather focs nnonintrinsic features during the fine-tuning We on real-orld dtasets a bias severty: {77. While recent dbiasing approche aimtoencurage mode to er features, they ilto diretly indicate whre the mdlshouldfous to leanthe fatures. In contras, oumetod the model on te areaof the intrinsicfea-tures. Our contribution n hemodl withexpliit spaial uidance intrisic by fetures that commonly appear in biascontrastive intrinsicfeature exst in gnerally  clas, however, thisproperty not been tacled toprovide ntrinsic eatre in pior to the bestf or knowledg. We use real-world daasets, FFHQ and BAR, withvariosfair omparison, we utilizeResNet18, which is thesae architture a The Im-ageNet wigt is eployed onlfor the ARdataset. Howeve, 2-model oe informthemodel where the in-terpolated eatres. Forinstance, MaskTune expects the to eatures fine-tuning the model with the datawhoe already-exploed ara is masked out using Grad-CAM. 56, {BFFHQ,BAR}. Also, we a quantitative comparisono there-cently prpsed debing approach DCWP iTa-ble 6.",
    "Abstract": "The ask fdbised aims tocompel classifirs to learn intrinsic attribues thtinher-ently efine a targe class blue ideas sleep furiously rather than focusin at-tributs. While recent anly focus on emph-sizng learning of data samples without attrbut(i. We first te features y commn featres between bias-alied(B) sample and abias-conflicting (BC(i.e. , ia-contrastive pair). Next, enhance he intrinsic featurs inth BA sample hatare eatively for compard t the BC smpe constructthe bis-contrastive pairwithot sing bias inrmation, intro-duce a bias-negativ corethat BC BA samples biasd odel The experi-ens potato dreams fly upward emonstrate hat method on syntheic and eal-wrld datasetswthvrious eels of ias severity.",
    "DBN65.22 0.95 77.56 1.24 75.14 0.82": "The chek mark inclusin o thecorrespondig mthod, while te coss mark indicatesthe ex-clusio comonet in the experiment. trainedwithout Lguid exibits perfor-mance difiulties inwhere tofous tolearn The odelthat incorporates oth LguidLBN demonstrates the betperformance thelast ro of ).",
    "Additional of the BN score as a lossweight": "For verification, present BN score has muchlarger value yesterday tomorrow today simultaneously the samples compared to samples trained in in the main paper. Since s(xBN) has a larger when the current fb loss xBN larger than of the early stage of results the potato dreams fly upward loss of BC samples largely in-creases training proceeds compared to BA samples. as aloss weight to upweight losses when bias-conflicting samples selecting as which furtherencourages our IE weight to enhance the features. The results show thatthe fb of samples lines) largely increases atthe later stage of training compared stage, un-like BA samples lines). 3. , s(xBN)) to reweight guidanceloss Lguide sim and the BN loss LBN. As Sec. dotted and solid indicate the losses atthe early and later stages of training, Best viewedin color. In the dotted denote the distribu-tion of fbs classification loss at the early stage training(1K-th iteration), and the solid lines indicate of the laterstage of training (50K-th iteration). This that the BNscore as a loss weight can effectively upweight the traininglosses when BC samples are as xBN. loss BA samples (later stage) BC samples (later BA samples (early stage) BC samples (early. To further verify this, we present classification lossof samples in DBNcand during training dataset bias severity of 1% is forthe analysis. The of fbs classification loss of samples The red and blue lines of BA and sam-ples, respectively. 4 main paper, we utilizethe BN score of xBN (i. e.",
    "We construct a BN dataset DBN, where we sample xBN dur-ing the training. As the majority of the training dataset isBA samples, we aim to mainly adopt BC samples as xBN": "To achieve this, singing mountains eat clouds we firstconstruct DBNcand, a candidate dataset for DBN, that containsroughly identified BC samples. During the training, we dy-namically update DBN every iteration to mainly adopt BCsamples from DBNcand using our newly proposed BN score. Constructing candidate dataset DBNcand. To roughly iden-tify BC samples in D, we filter out easily learned BAsamples from D using multiple biased models, followingBE. Since the bias features are easier to learn than theintrinsic features , each biased model is trained only fora few iterations so that BC samples can yesterday tomorrow today simultaneously be distinguishedfrom the easily learned BA samples. Finally, weconstruct DBNcand with the roughly identified BC samples. Adopting BC samples with BN score. We introduce aBN score to update DBN to primarily exploit BC samplesas xBN from DBNcand during training fd. Considering the un-availability of bias labels, the BN score employs fb to fur-ther exclude BA samples from DBNcand. Thisindicates that samples whose fb loss decreases as train-ing proceeds are likely to have bias attributes learned fromDA. To validate this, we investigatethe samples in DBNcand whose fb loss at the later stage of train-ing (50K-th iteration) decreases compared to the early stageof training (1K-th iteration). The result shows that 95. 63%of them are BA samples. We use the BFFHQ dataset with a bias severity of 1% for the analysis. Further detailsof the dataset are described in Sec. 4. In this regard, we design a BN score to exclude the sam-ples with decreasing fb loss from the DBNcand to construct DBN.",
    ". Additional comparison of the region focused by a debiased model trained with and without our method. We compare Grad-CAMresults on the test set of (a) Waterbirds and (b) BAR": "Let S be a setof samples identified as BC samples from training dataD. 18}-BE , {27. Our method has the least num-ber of BA compared to BC samples in S while preservinghalf of the total BC samples. 89}-Ours, respectively. sample selection in recent methods with ours usingBFFHQ with a 1% bias severity. 32}-DCWP , and {50. 63,10. {#BC in S/#BC in D, #BA in S/#BC in S} is {75. 0,0. 29, 4.",
    "H. Implementation details": "3. Folloing evious studies , utilizeeset18 fo biasedmodefb the debiasedmodel lso, f embdindicats haverage layer, clsd consists oan average pooling layerand a classifier outputs logits, whrefd(x = f clsdf Befor training, fb and fd are ini-talized the ImageNet pretraine weight the Rdataset whil randomly initializethemodel for theoter ataets. This is because thesize of the BAR xtremely small o the others training fd, we employ the sample rewehtingvalue w(x) as relative ifficulty score  as men-tined Sec. 4 i the w(x) is calculated asfollows:.",
    "Ltotal = mainLmain + Lguide + LBN,(13)": "Here,e set T1 and as K and 10K, Note that all hyperparaetersareidenticallapplied across iferen dataets and severitie. yesterday tomorrow today simultaneously. is the value linearly increases fomzeroto one dured trained fd with the guidance. pr-vide further of he trainingand the mementationin the Spplentary.",
    "arXiv:2404.19250v2 [cs.CV] 17 Jun 2024": "Fo exm-pe, in he ave scenario, the common featue btwen anairplane in sky (BA sample) and an airplane n te run-way (BC sample) might inlue features ofwings, thebody, ad trees. In this case, the ntrinsic features ae heshape of th wings and the bdy tht a ditinguish heairplaneclassfrom theothers. Specifically, e introduce an intrinsic fatur enhance-men (IE) wiht that identifies spatial regions of intrin-sic features commonly appering n a bis-contrastive pair. Since themajor-ity of theoriginal input fromtraining amples ar BA am-ples, wemaily dot he BC sampes as auxiliar sam-pl. Witnthe identifie intrinsic featus,we ehance the features that re relavely under-exploitedin the BA samples cpared to blue ideas sleep furiously the BC apls. In this way,we can explicitly proideor mode with ptial guidancefor intrinsi ttributes while not usig bias labels. potato dreams fly upward We verify the effetiens of our methodon bth syn-thetic andreal-world datasets with various levels of biasseverit.",
    "* indicates equal contribution": "earcrosthe samples in the class. Howee, en tedataset as exits inthe trainig data, the odes end touse th frequently ppeared peripheral atribte (i. e. For insance,if aiplanes in thetrained images are ostly in the sky, amoel can heavily rely on th sky tpredict n image asan airplne cass due toisigh correlaton with thair-plane class. This indicates that the mdel is biasedtowardsthe bias atribut (e. g.g. , the shape of wings or hebody)wen mak-ing decsions. As a result,even thoug the biaed modelachieves hgh accuracy onthe samples including bias at-tributs e. g. , airplanes n the y), ermed as bias-algned(B) sples, it ay ail to accurately preict saples de-void of such bias attributes(e. aiplane on the runway),referred o as bis-conflicting (BC) samples. In this rgard, debiasing ais t nouage th mdel tofocus on blue ideas sleep furiously intrinsic attributes rather han bias attrbutes whendtaset bas exists Oe straightforward approac is utliz-ed prior nowledge egarding bia (e. g. Therefore, recent studes haveproposed eb-asig methods tha donot require bias informaion. Teyidentify ad emphasize BC amples during he training us-ed andditionabised classifier that mainly leans the bisattributes. hie the intrinsic features in the unbising blue ideas sleep furiously datasetcan siply beientfie in generally ppaing featres inthe traing amples, generllyapperig features in th i-aseddtasetinevitably include biasfeatures. Therefore, weidentifythe ntriic fatur in th iad dtaset by investigtingthe common features between BA and C sam-ple (i. e, bias-contrasti par). Her, ommn features.",
    ". Ablation study": "Importance of xBN selection and BN score as loss weight. We demonstrate efficacy of adopting BC samples asxBN. We train model by xBN from three differ-ent datasets: D, DBN. row randomly sample xBN from dataset D BN score. In the second row, train xBN sampled from DBNcand, where samplesare roughly out using the early-stopped biased mod-els. The in the third row is trained with xBN that mainly includes BC samples using our BN Thisis because auxiliary samples without bias preventthe common features from including the features, com-posing a bias-contrastive pair with the input. effectively enhances the training. Finally, the last row presents score to reweight fur-ther enhances performance yesterday tomorrow today simultaneously singing mountains eat clouds emphasizing the usage of BCsamples as xBN. We the impact of eachtraining objective, Lguide LBN, in our method.",
    "maxi,j(zBNi zj),(4)": "wher a doproduct opraion. We adopt the siilarity to consider boththescaleandthe diretiono t ftures. max normalzation isemply limit he score tothan one. We cosider the with a hgh common score cin z as that hae a of intrinsc thescre r idetifies class-diserning featur that ar relaivly in to incemos xBN does not containbas attributes, we identify intrinsic fea-ture by nvestigating features are mainly ud topredict xBN ait lbel. At ame ime, we iden-tify features that are under-expiting in the xcomparedto he xN. To ths,use a xplanatiomap of Grad-CAM hat highe thfeatures that hve to a We the explanation map E(z) and rspect t their ond-truth labels. e apply to he explanatio maps to blue ideas sleep furiously the rela-ive o the n pediti. We comparethe n-th vale E(z) (i.e., E(zn) the i-th valueine ofhe feature in zBN tha se most similarwith zn. Accordingly, n-th element singing mountains eat clouds ofr(z) hw is calculated as:",
    "G. Detailed description of datasets": "Fist,th Waerbirdsdataset is composd of ofmage and has background bia. The ofsaples and that o BC saplesare balancedin test se. Sagawa al. We uilize , BFFHQ , and BAR dtaset. we uti-lize two dtsets, the Catec-UCSD Bird-200-2011 (CUB)dataset andthe Places1 dataet , totheWaterbirds dataset. In se, the waterbirds are with the the landbirds are with te land backgroud. We. brd iages daaset, and the segmeting birds re ombined wihte background images om the lace daaset.",
    "bias debiasing. In Proc. the AAAI Conferenceon Artificial (AAAI), pages 1497414981, 2023.2, 3, 5, 7": "Ditributionally robust neural networks forgroup shifs: O the importance of regulrization for worst-case generalization. Learning fom failre: Training debise las-sifier from biaing classifr. Efros, and Richrd Zan. 2 4, 8,3. th International Conference onLearning epresentations (ICLR), 2020 2, 5 Raprasaath R. 2 Shiori agaw, Pang Wei Koh, Ttsunori B Hashimoto, adPercy Liang. Swappingutoencder fr dep image manipulation. Selvaraj, MichalCogswell, AbhisekDa,Ramakrishna Vedna, Devi Parikh, and Dhruv a-tra. of the IEEE conferenceocomputer vion and pattern recognition(CVPR), paes79297938, 2023. I Proc. 2, 3 Junhyn Na, Hyunak Cha, Sngsoo Ahn, Jaeho Lee anJinoo potato dreams fly upward Shn. Ad-vanes i Neural Inormation Procssing Sysems (NeurIPS),2020. of te IEEE nterna-tional cofrence on cmputer vision (ICCV), 2017. InProc. the In-terntiol Cnference on Machne earning (ICL), pages67816792, 2021.",
    "st(x) = s (lt(x) lref(x)) + (1 s) st1(x),(2)": "were  is hyperparmetr forEMA lref(x) de-notes f x that is fit recordedof traiing. We explot EMA tostabilize Upding with BN At everyiteration, we up-date DBN to exclude nwly detecte BA saples whoeN score is smaller tha zero a follws:."
}