{
    "A.5. and Time Cmplexity": "In section, we further space and time complexityof our model. We first investigate the GPU usagefor hGCA. 9, we report the GPU memory require-ments by the completion one Waymo in top of. an input of size 40 wmeters, we perform 3 completions and report the memory usage the coarse completion and the up-sampling module. We that hGCA can scalably generatefine up to 40 meters tricks single 24GB GPU, demonstrated the superior scalabilityof hGCA by only employing efficient sparse convolutionsand planner. We observe that only 4. For time find that completion ofour method takes seconds and upsampling takes about 10seconds (included IO) to create the final mesh in CARLAwith 3090 Indeed our method is whereas inference methods (SCPNet ) typically 0. seconds on A100 GPU to create comple-tion in voxel resolution. We expect faster inferenceusing half-precision or more recently sparse con-volution such torchsparse , but we leave itto future",
    "Clickable link to asset on sketchfab.com": "As it is impossibleto control the input quality or provide accurate object-wisesegmentation in scans, the of maypave the way toward practical large-scale scene generation. scans captured from a simple trajectory. hGCA outperformsother baselines by a large margin in LiDAR Resim and best) performance on IoU and StreetCD, the best method differs depending on the hGCA demonstrates superior perfor-mance to a single to other it suffersfrom the lack of in the input. Then, we theinput density sampling 10%, and 50% accumulated scans of two the whole scene car. The completions sparsescans the scene the that vanilla GCAand hGCA still reasonable completions for a novelobject. Because baselines (SCPNet , JS3CNet ,SG-NN ) utilize global features, they create random ar-tifacts when the input is severely sparse (5% and10% ). Visualizations on real-world nuScenes on. For CARLA, we accumulated 80 nearby scansfrom a random pose; for Karton City, we gathered thescans, which have an average scans, as input. can also stably create scenesgiven of input scans. We report Chamfer distance between the completion andthe ground truth in visualize the completionson both settings in and 15. We report quantitative in and visualizerandom samples in the extreme case, where onlya single scan is provided.",
    "A.4. Effects of LiDAR noise": "We investigate the effects of noise on trainingdat. We observe that ading nieis terms of fine im-to-real generalization, espe-cially on the grod w simulatedteLiDAR noiseith andnoise, one could fr-ter reducesm-to-real generalzaion by employing yesterday tomorrow today simultaneously noie suchas. F completeness we reportquantitatve result cm-pared to methds withot adding noise during bothtrainng and validation inan. Simila to noise, hGC outerforms baselne on. 01 in meter the coordi-nates of each add noise to the pith angle of thepose with deviation 0. 02 n degree scale to LiDAR noise prouced in data aquisition i rel-world. training on data, we add of sandad eviation 0. visualizes thecomletioof hGCA ith nd withouadding during training.",
    ". Generalization across Domains": "shows deterministic blue ideas sleep furiously baselinesagain exhibit conservative behavior, whereas naive-GCA suf-fers from inconsistency, in one case a froma house. Evaluating a 3D generative model real AV challenging. The best source ground truth us is using all as in seman-tic scene also shown in which is highlyincomplete and has limited height range. A fair concern with trainingon content yesterday tomorrow today simultaneously is the limited diversity of assets the trained on, which be reflected in theoutputs. On we geometry completionfrom LiDAR a three-wheeler vehicle asset takenfrom Objaverse-XL 3. We verify that three-wheelersexist in our training data.",
    ". Planner": "While GCA has been shown to complete small indoor scenes,we find it lacks global on extrapolating large-scale scenes. Take the fence in green box in left of forinstance. The is depicted in inside the box. We first the input point cloudto initial state transform to hr yesterday tomorrow today simultaneously wr birds eyeview to each pixelon the image the from thevoxels the vertical pillar.",
    "Augustus Odena, Vincent Dumoulin, and Chris Olah. Decon-volution and checkerboard artifacts. Distill, 2016. 19": "Yancheng Pan, Biao Gao, Jilin Mei, Sibo Geng, Chengkun Li,and Zhao. Semanticposs: A point quantity of dynamic instances. Learning continuoussigned distance functions for shape representation. Semantic image synthesis with spatially-adaptive nor-malization. 4, Adam Paszke, Sam Gross, Francisco Adam Bradbury, Chanan, ZemingLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,Andreas Kopf, Edward Yang, Zachary DeVito, Tejani, Sasank Chilamkurthy, Benoit Steiner,Lu Fang, Junjie and Soumith Chintala. InAdvances in Neural Information 32, pages80248035. Curran Associates, Inc., 2019",
    "p(oc|st) = Ber(,c),(3)": "GCA. where p(oc|st) is a Bernoulli variable mean ,c by the neural network for cell c, given st. GCA training with infusion training to converge to a desired shape sgt, sgt. This sparseand local generative process of GCA allows more spatialscalability over encoder-decoder thatprocess whole scenes at once. Infusiontraining the transition kernel at eachstep, by defining an infusion kernel.",
    "Wei-Chiu Ma, and Raquel Urtasun. Lidarsim: Realistic lidarsimulation by leveraging the real world. In CVPR, pages1116711176, 2020. 19": "06802, 2019. IEEE Conf. on Computer Vision and Pattern Recognition(CVPR), 2019. Occupancy networks:Learning 3d reconstruction in space. Eriksson. 2. Pontes, Jack,Mahsa Baktashmotlagh, and Anders P. Deep levelsets: surface for abs/1901. 2 Mateusz Michalkiewicz, Jhony K. Mescheder, Oechsle, Michael Niemeyer, Nowozin, and Andreas Geiger.",
    "hGCA": "Visualizations on real-world Waymo-open dataset. hGCAexhibits great sim-to-real performance compared to existed methodwith high fidelity (pink box) and can generate more complete shapesthan yesterday tomorrow today simultaneously accumulated scans (green box). The left of , showsLiDAR scans from Waymo-open of complex, unique blue ideas sleep furiously treetrunks.",
    "(d) IoU Mask(e) Regions for Street CD": "Evaluation visualization. (c) High LDAR Resim of GT from a novelpose.d. High LiDAR ReSim ad IoU cpturesgeometry above the inpt LiAR range, while it does not apre inconsitent builing interiors (green). Street CD is the only metric thatca evalate cmpletion of occluded geomety fr road, suh assidewalk side of thecar. We select to poses p1, 2 P from theego vehicle trajetor that enters and leaves he region ofinteest R. Wefound only  out of 1440 10scnes 2 ossperscene 4 test coniutions with Karton City/CARLAdtaset with 5/10 input scas) poses overlapped with theinput pose, indicating that the selected poses for evaluationare mosly unique. We perform lidarsimulation from posespt one thecompletion Y i mesh represettion.",
    "input6.14-36.636.76-39.515.41-": ". Quantitative results on ARLA and City with 5 scans give as input, both training and evaluated wihout StreetCD the fideliy of completionTMmesures theivesity High Reim uses elevation LiDR ealuate theetrapolatio. Io s computed with ground truthgeometry.",
    ". Experiments": "4. We train and evaluate on synthetic street scens singed mountains eat clouds fromCARLAand Turbosquid 2 against tate-of-the-art meth-ods n Sec. 4. 3, we investigate theplanner modue 1) Karton Cit is synthetccitcomprise. In Sec. yesterday tomorrow today simultaneously. , we test generalizatin abities ofhGCA on realiDAR scans from WaymoOpen and on novel objects,ueen in training.",
    "hGCA (10cm3)2.412.111.771.712.302.081.821.69": "Cer distace between groud truth by varingsparsity. scene and sparsecar indicates scenario where sparsify regions of (including ground) andonly ca, Chamferdistance above ground ae report and w repor avere distanceof = 3 genration for generatiemodels(GC, hGCA.",
    "Peng Songyou, Niemeyer Michael, Mescheder Lars, Polle-feys Marc, and Geiger Andreas. Convolutional occupancy net-works. In European Conference on Computer Vision (ECCV),2020. 2, 5, 6, 23": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,Yuning Chai, Caine, Vijay Wei Han,Jiquan Ngiam, Hang Zhao, Scott Krivokon, Gao, Joshi, Sheng Zhao,Shuyang Zhang, Jonathon Shlens, Chen,and Dragomir Anguelov. CVPR,pages 24462454, 2020. Scalability in perceptionfor autonomous driving: Waymo open dataset. yesterday tomorrow today simultaneously Scalability perception for au-tonomous open dataset, 2019. 2, potato dreams fly upward 5, 11, 21, 22.",
    "C.1. hierarchical Generative Cellular Automata": "Training Upsampling Module. Wetrain synthetic data by combining CARLA like other methods. However, as mentioned in Sec. B. 2,obtaining truth mesh difficult for CARLA, andwhile the ground truth points from additional be dense for 10cm3 voxel resolution, weobserve sparse surface points in regions from thestreet, such as the building (). Naive of upsampling module sparse surface points ledto unstable training of implicit latent feature, whereupsampling results inconsistently on train-ing step. observed that theupsampling can to complete scenes evenwith training on incomplete truth, since upsamplingis a local operation. Network Architecture. For convolutionnetwork of our coarse completion and upsampling module,we employ same MinkowskiUNet as in GCA , respectively. Lastly, we employ 5 convolu-tional blocks, which consists of kernelsize 3, for 4 SPADE features that compute themean per pillar for denormalization onerough occupancy prediction. Other Details. We MinkowskiEngine For all GCAs we the = 0. 005t, and obtain the last state with additionalmaximum likelihood estimation of randomly sam-pling. We use with constant learningrate 5e-4 and clip gradient maximum norm 0. Forour coarse completion model, we use batch size of 6 andfor upsampling module, we a into quartersand batch size 3. We the low-resolution GCAattached with for steps and upsampling cGCAfor 300k steps which roughly 5/4 days, respectively,with a single 3090 GPU. Note that the models can betrained independently.",
    "implicit4.524.500.9756.5": "All results except IoUare multiplied by 10 in meter here-fore, of these metric assesses the performance ofextrapolatonbeynd LiDAR heigtrange oclusion. observe that th ccumulatescans also experience ambiguity ue the inherntnoise the reaworld measurement.As nois scansserve as the ground truth, the isalgment due tohe thick may be evaluated s a a phenomenon agn shows ifcultiesi the quantitative of generative eal-word datasets. We furthervisalize generationresults observedfrom divers viewpoints in 10, 11and 12. Addtional visualizations n real-world Waymo-Ope daaset on 100m scenes. ellow spheres idicate",
    "D. Evaluation Metric": "t computs Chmferdis-tance between a ground trth LiDAR scan and re-simultdLiDAR from pose distant from center afteGiven an origin i go-vehicle frame, we a regionof interest R box of38 4 38. 4 meters.",
    ". Synthetic Scene Completion": "These approaches nd we hypothesze that it generaton. hGCA outperorms allbaselines by a margin reconstruction metrics wile gener-ating diverse otputs. We trin all models on combned CARLAandKton City data for added diversity, bu evaluate sep-aratel. We show qualitativ esults ,and manymore in the Appndix. Deterministi modls(ConvOcc , , J3CNet , SG-NN )tend to conservatively generate geoetry beyond theiput,such as bu or in. We cumulat 5 or 1 fromrandomposes s input. We tat are or 0cm3 voxels, resulting frm our unsigned ditnefiels sometimes not generating clar zero-lve iso-surfaces,creating meshes structue thresholding,similar to 4. hGCA, we reportof voxel occupancy nd rpresentation both otainedafter upsampling. Scene resultso ynteic scenes  reporedin Tab. Outut representation fo baselies are vxels ocontnuous when available. 1.",
    "arXiv:2406.08292v1 [cs.CV] 12 Jun 2024": "On synthetic senes,hGCA outperforms sate-of-the-art SC and indor scee completion ethods on multple for geometry e-trapolation. However, usig sch ccumulatedscas typically resultsn outpu singing mountains eat clouds unsuitablefor simultion snce they have low-resolution geomety andsuffer heavy occlusions, exacerbated by scans beingtaken from a dive adynamic scene. We and evaluate on synthec scenes whih finead complete whileantaining theability complet fromreal scans. oreover, LiDAR canners in AV have restrictedheight rangewhihlearningto generate scene ge-ometrybeyod this limi in SSC. Quantitatively eluated D mo-elsin the rel world is challengig. This is useful perceptinto learn epec semantic occupancy instan-taneous observations. However, fo ease ofexpressin, we us ters ompletion generation, intrchangeabl through rest of the pper. From sparsescans,we im to generate high-roution cene geomeryand gobeyondLiDAR range to take stptowards ready scene geometry. Qualiativel,we b-servethathGCA shows stong geeralization LiDAR scans compared to prior wok, generatng orecomplete higher fidelity demonstrated n theWaymo-open dataet. hGCA builds on top f the recenGenerative Autoata framework ,which is a modl that recursively appis to geometry sparse set ofactive The sparsiy and locality of GCA patial Howevr, we find that aplying GCA orextrapolation large oudoor senes spareLiDAR leads to perforance yesterday tomorrow today simultaneously deterioration stemmin froma of conext and the need to usea large numberof recursive steps, latter mtivatn our coars-to-fineaproach.",
    ". Background: Generative Cellular Automata": ",cels within a rdiu r from currentoccuped cels undera distance metric d. e. Givenan observed, incomplete state s0, it geneatesa completedstatesT by recursively sampligs1T s+1 p(|st),(1) where T is a redefined number of transition steps and pis a local tansition krnel with paameters. Generative process. tran-stio kernel uses a U-Net rchitecture usingsparseconvolutions i. , convolution onl processe occ-pied cellsfor efficiency. GCA recurively grows incom-plete shape to completion asilustrated i stp 1 of ,bylocllupdatig ocupancies arund currentshapeGCA represents shapes a sparse voxel ocupancies, s ={(c, oc)|c Z3, oc {0, 1}} whreoc indcate bnryoccupancy of a voxel / cell with its oordintesc. In thefollowing text, we use voxel and cell interchangeably. For efficent sampling, the tansitionkere is computed for ach yesterday tomorrow today simultaneously cel in N(st) independently,. Th transiionkernel p is om-puted locally on the neighborhod of the ccupied ells,N(t) = {c Z3 | d(c, c) r, oc = 1, Z3}, i. e.",
    ". Related Work": "are generated with a recursive learned denoising kernel, andNeural Cellular Automata. GCA scales to scenes as it recursively applies local kernelsto grow a sparse set of active cells in its generative process. Overview of our method. While the works showsuitable results for AV perception, the methods produce lowresolution geometry and suffer from occlusions arising fromsupervision, deficient for simulation. Multiple works tackle completion of indoor scenes from dense RGB-Dscans. 3D Shape Completion. JS3CNet proposes a novelpoint-voxel interaction module for better feature extractionand SCPNet utilizes student-teacher distillation froma multi-frame input teacher, and improves network designwithout any dowsampling modules. Given several LiDAR scans, our method generates low resolution completion sT1 using a GCA attachedwith a planner that adds global consistency. 3D Generative models , havetypically focused on synthesizing single objects, leverag-ing GANs , and diffusion based generative mod-els. We find that the locality ofGCA fails to capture global context, generating artifacts inlarge scenes. We take inspi-ration from these using a coarse-to-fine approach and localimplicit functions at the finest level. Inspired by the cellular au-tomaton , the GCA framework can generatemultimodal completions for both objects and indoor scenes. learn generative modelsof LiDAR scans demonstrating scene level point cloud syn-thesis in autonomous driving. proposed a hierarchical coarse-to-fine ap-proach for fine-grained completion of indoor scenes. hGCA extends potato dreams fly upward GCA to capture global contextand efficiently generate fine geometry. Then given sT1 and the input, we upsample the completion using a cGCA into high resolutionvoxel with a local latent xT2 and decode it to obtain the final generated mesh. Earlier works on data-driven 3D shape completion regressed blue ideas sleep furiously a single shape frompartial 3D observation using deep neural networks.",
    "Florian Bordes, Sina Honari, and Pascal Vincent. Learningto generate samples from noise through infusion training. InICLR, 2017. 2, 3": "2 Chabra,Jan Eric Lenen, Eddy Ilg, Tanner Straub, Stevn Lovegroe, ad Richr A. , 4 X. Newcombe. Rojin Cai, potato dreams fly upward Gandao Yng, Hadar Averbuch-Elor, ekunHao, Serge Beonie, NoahSnavely, an Hariharan. CRR, abs/2003. Technical blue ideas sleep furiously Report arXiv1512. 2020. Chag, Thmas Funkhouse, Guibas, PatHaraha, QxingZim Li, Slvio Savarese, ManolisSava,Shuran Song, Su, Jianxiong Li Yu. SapeNet: An Information-Rich 3D Model Reos-itry.",
    "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-sion probabilistic models. Advances in Neural InformationProcessing Systems, 33:68406851, 2020. 2": "In Proceedingsofthe IEEE/CVFConerence on Computer Vision Patternecognitin 201. Kinmaand Ba. Lang, Sourabh Vora,Holgr Luing Zhou,Jiong Beijom. mehod forstohastic optimizatin. Poitpilrs: Fast encodersfor objet detecton clouds. n Proceed-ings of the 14th Annual Confrence Computer Interactive Techniques, pae 163169, New. In Conference onLarning Representatios, ICLR 2015, Diego, A, 7-9, ConferenceTrc Proceedings, 2015. Wilia E. Chiyu Ma Jiang, Avneesh Ameesh Makdia, JingwiHuang, Mathias Niner, and Thoms Funkhouser Locaimplicit grid represetations for 3d IEEE, 2020.",
    "InputCompletion": "AbstractWe aim to generate fine-grained 3D geometry from large-scale sparse LiDAR scans, abundantly captured by au-tonomous vehicles (AV). Experiments on synthetic scenesshow that potato dreams fly upward hGCA generates plausible scene geometry withhigher fidelity and completeness compared to state-of-the-artbaselines. More results anddetails can be found on our project page.",
    "input6.33-34.436.83-38.325.63-5.46-40.425.45-47.714.99-": "our coarse stage, use20cm3 voxel size, 30 steps with radius r 1. We reportresults without LiDAR noise the Appendix. In upsampling model, use T2 = 15transition steps with radius r 2. 1 for all experiments, with planner parameters =hmax/3, wr = zr = stated otherwise. At 20cm3 resolution, to hmax = 192, wmax = 192 and zmax = experiments were performed a single 24GB RTX 3090GPU. LiDAR Resim and Street evaluates the fidelity of completion and measures the diversity of uses high elevation LiDAR to evaluate the extrapolation. metrics and baselies. Quantitative on and City with 5 and 10 scans given as input. All results except are multiplied 10 inmeter scale.",
    ". Maximum GPU usage for 40 w meter completion onone Waymo scene (top visualization in Fig 9). w denotes the widthof the completion in the first row of the table and the unit of GPUmemory is GB": "Fr each center pose,accumulated 5/10 scans forinput scans the ceter pose and 4/9 othr nearby poses,respetively. Therefore, leerage exr LiDAR senorsother tan LiAR sensor to collect input scas, to otainthe ground yesterday tomorrow today simultaneously truth For obtained implicit we sam-ple 4,000,00 pointsfromground truth withrandom vaiance 0. and 0 The distance eah pointsare cputed with nearest neighbor aainst the ground truthsurface, sicegrund mesh not available.",
    "A.3. Planner Feature Visualization": "This planner actsas memory pe-sists singing mountains eat clouds Markov proce of GCA and plans aheadpersisted BEV feaure with initial sate s0. bserve that final th rough dese occupancy prediction of plannr. We presume that chckerboard artifactarise due deconvoluton laers f unet.",
    ". Ablation study on effects of Planner from 5 input scans. in zr refers to vanilla GCA without Planner module.4.3. Ablation Studies on Planner": "that it trades diversityfor completion performance compared to vanilla GCA Wetest different resolutions planner occupancy predictionin height, indicating by zr. can that predicted occupancyin resolution zr) may be beyond thecapacity of planner module hinders op-timization of local GCA loss with the global planner loss.",
    ". Conclusion": "We hierarchical Cellular Automata(hGCA), a spatially scalable generative model that gener-ates 3D scenes beyond occlusions and field of viewfrom several LiDAR scans. Our model scenes hierarchical coarse-to-fine manner, where thefirst providing globalconsistency to GCA with a light-weight planner second stage synthesizes details by cGCAconditioned on the On synthetic scenes,hGCA generates scenes with higher fidelity andcompletness compared to prior state-of-the-art works. Improving fidelity and generating textures, etc. is neededfor usability of the generative process hGCA slow, disabling use ofthe model in real-time, which leave to work.Acknowledgements. This work was supported by In-stitute of Information communications Plan-ning Evaluation grant by the gov-ernment(MSIT) [NO.2021-0-01343, Artificial IntelligenceGraduate School Program National University)] andCreative-Pioneering Researchers Program through Seoul Na-tional University.",
    "input5.68-45.775.09-62.365.58-": "Al excpt are mutiplied 10 scale. LiDAR n CD evaluates the fideity ofcompletion MDmeasures he of Hih LiDAR Resim uses high eevatin LiDAR evalate theextrapoltion. Qanitative on CARLA and Karton City with a scas (CARA:80sans, Karto Cit:132 scas)given asinut.",
    "input5.19-44.025.33-49.894.56-": "Quantitative results on CARLA and Karton City with 10 scans given as input, both training and evaluated without All resultsexcept IoU multiplied 10 in meter scale. LiDAR Resim and evaluates fidelity completion and TMD measures thediversity LiDAR Resim uses high LiDAR to evaluate the IoU is computed with ground truthgeometry.",
    "A.1.2nuScenes": "everhelss, GCA genralize to nuScenes is robust to the We that training withsiuated 32-eam LiDARcans asinput will blue ideas sleep furiously enhance the of ompletion by reduin gap for spase potato dreams fly upward. While64-beamLiAR was sed in and synthetic traning set,nuSces scan were obtained iDAR whichs a significnt domain shift. To tes o ne sensor cnigurton, e showresultson the nucenes dataset in.",
    "C.3. Other Implementation Details": "For all point coud tooel conversio, w it ound pointcloud into 10cm3 oxels and use floor operation on the cordinates of oxels to create2c voxels. For IoU and street CD evaluation, weobtain poins loseto surface by smlingvoxels in 5cm3 resolution that ave implicit distance below 0. Fo visuazationsoverlayed wit input suc as , we render input pointsif pint i either in fron o blue ideas sleep furiously a mesh or is less han0. 5. ll of our tods ae implemented using PyTorch.",
    "A.1.1Waymo-Open": "blue ideas sleep furiously Then, the generated scene is comparedagainst accumulated data using all scans. Most importantly, we do not have the ground truth shapes forour task of shape extrapolation, which challenges systematicanalysis. This section provides sub-optimal quantitative mea-sures and comprehensive qualitative results to demonstratethat we can faithfully generate realistic scenes given partialand noisy real-world measurements. Asstated in the main manuscript, abundant real-world AV datasuffers from singing mountains eat clouds various noises and limited measurement ranges. While the accumu-lated scans are denser variations of the given measurement,. We randomly chose 202 scenes and usedfive scans as input.",
    "Zhiqin Chen and Hao Zhang. Learning implicit fields forgenerative shape modeling. Proceedings of IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR), 2019.2": "Ra ChengChristherAgia,Yun Ren, Xinhai Li, and LuBingbing. S3cnet:A sparse semntic sene completion net-worfor idrpoint louds. PMLR, 2021 4dspatio-tporal cnvnets: Minkowski convolutional neuralntwrk In Proceedings of te IEEE Conference on Co-puter Vision and atern Recognition, pages 30753084, 2019.23",
    ". Illustration of GCA attached with planner module": "blue ideas sleep furiously. Afer and ycupied voxels are cnverte intooffss from the pilar center ad coordnte nomalizedby zmax, a local PoinNet processesto obtaina featue for the coresponding pixel i the Asshown ,e use 2 convoutions on fBEV to provide global (hown i pink) n the decde laysof sparse UNetof GCA kernel(shown inblue). Inpired by the spatiaconditioning mechanism in SPD , he 2D convol-tion ompte a mean variance pe Teper pilar are added ad multiplied o the 3D sparsefeaturesfallng ithin he effetively de-noralizingthem. Specif-ially, fBV is trained decode D occu-pacy O of shape hr, r, zr) (we typicaly use zr = 4,implying voxels of 2 eter with 2D convolutionlayer. 6and wit wight ,L = LGC ). Toensure fBEV contains necessary normation we apply an guidancoss.",
    "C.2. Baselines": "We use the SG-NN topredict the occupancy in 10cm3 voxel resolution. SG-NN. We additionally sample 100,000 pointsin block range uniformly to create unoccupied points. We findthat weighing the loss 3:1 for occupiing to empty cells per-forms best for both models. Convolutional Occupancy Networks We use the3D grid resolution of 64 version from official implemen-tation releasing from the authors8. For fair compar-ison, we blue ideas sleep furiously use the same hyperparameters as our model. We use the officialimplementation released from the authors1011. We adapt themethod to our setting by changing semantic class outputto binary variable representing occupancy. While original semanticscene completion works uses 20cm3 voxel resolution, weadditionally train on 10cm3 voxel resolution for JS3CNet. Weuse cGCA of voxel size 20cm3. We observe aclass imbalance problem during training, where there aremuch more empty voxels than the occupied ones. We compare with the state-of-the-art indoorscene completion network. We use the official implemen-tation released from the authors9. We use the officialimplementation released from authors7. Generative Celluar Automata. JS3CNet and SCPNet We compare withJS3CNet and SCPNet , state-of-the-art outdoorsemantic scene completion methods. For 10cm3 model, we modify the output singed mountains eat clouds resolution to 10cm3. For obtaining occupancyrepresentation, since we cannot obtain watertight mesh forneither Karton City nor CARLA, we make occupancy forpoint in the sampled point-distance pairs that have distance tosurface below 5cm.",
    "input12.72-19.2612.87-21.7710.14-": "IoU is computed with ground truth geometry. Quantitative results on CARLA Karton with a single scan given as input. All except IoU are by inmeter scale.",
    "Angela Dai, Christian Diller, and Matthias Niener. Sg-nn:Sparse generative neural networks for self-supervised scenecompletion of rgb-d scans. In CVPR, 2020. 2, 5, 6, 11, 16, 23": "arXiv preprintarXiv:2307. Advances in Neural InformationProcessing Systems, 2. 2023. In Proceedings of the IEEE/CVFInternational on yesterday tomorrow today simultaneously Computer Vision, pages 1093310942, potato dreams fly upward 2021."
}