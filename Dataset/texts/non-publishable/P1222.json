{
    "Sparsifying": "The ege iportance msk(s) can be generated fromawell-trained GNN mode o trained yesterday tomorrow today simultaneously simultaneousl with a NN mdel. Tifrmework progressively limiaes noisy edgs frm inut braingraphs byleanigan edg mportac msk for each/all graph(s). Input GraphsOne Input Grph InstanceSparsified Graph: Geneal iterative blue ideas sleep furiously framework of sarsification.",
    "CONCLUSIONS": "In this singing mountains eat clouds studied eural-network-based graph sprsifica-tin for brain graphs. By introducing sarsificationframork, weidentfie everal effective strategies fr GNs ot nois edges improve the raph classiication perfor-mance. We combined these strtgies into new interretable graphclassfication mdel, improves the clssicationperfomance by up to 5. 1% wt 5% fewer des tan theoriginlgraphs.",
    "Graph Explainability": "Ou wk relating to plainable GNNs hat we identifyiportan edges/subraph that the model are perturbaton-ased, whr te goalis to thebetwen outpt nput maskand edges,which explns of well-training Sub-graphX itspredictons efficiently exploring diffe-ent subgraphs wih Monte Carlo tre search. general, most ofmethods focuson node classificton as and makeexplantios fawhich is not applicable o our setting. Other only applyto graphs, wich cano handle signed weighte braingaphs. g. Additionally, most metods generate explaationsater a GNNis trained. Grahime theN-hop neighoring nodes of the tart an then trains surrogate model to f cal neihbohood GNN to fit the BFS-generated datasetsand then generates to explain the preictons; PGM-Explainer datasets basing on the influece perturbing he nd features, shrinks he size thedatasets vi row-Shrink employs a Baysnetwor o fi he datasts. Anohr appracfor GNNs is surrogate-based;the method in his ca-egorygnerally construct a imple and surrogateodel topproximate the of the oiginal mdel n certinneighborhods. ome achieve ecentre-sults in eplainailiy-relate metrics (e. scores unclar ther explanations retin imprtant pat of graph, whichimproves cassfication acurac.",
    "Pelg and Alejandro  Schffer. 1989. Graph spaners. Joural raphthory 13 1 (198), 99116": "2011. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition. Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and HeikoHoffmann. Explainability methods for graph convolutional neural net-works. 1077210781. Jonathan D Power, Alexander L Cohen, Steven M Nelson, Gagan S Wig,Kelly Anne Barnes, Jessica A Church, Alecia C Vogel, Timothy O Laumann,Fran M Miezin, Bradley L Schlaggar, et al.",
    "we utilize the sores from the cognite domains as ourlabels, incoporateage adjustment :": "PicVocb (icture Vocabulry) assesses language/vocabuarycomprehensin. Te test blue ideas sleep furiously admnistrator scores them as singing mountains eat clouds rigt or wrong. It involesrecalling an increasnglylengthy sries oillustrated objects ad ctivities resetein a articular order on the coputer screen.",
    "AIGS THER BAKBONE GNNS": "In we thereults o IGS ealuating ondifrent GNNbackbones (noted by IGS) nd it against potato dreams fly upward heoriginalperformance (noting by Original Thi demontrtes that mprovementsachied IGS are moel-agnostic.",
    "OUTPUT: G with smallest": "where denotesthe Hadamard prouct; L is te Cross-ntropylos; is th regularization coefficien We oiize the join maskacoss potato dreams fly upward all training samples in a batch-tranig fashio to achieveur objctive of larning a sared mask.",
    "Christian F Beckmann and Stephen M Smith. 2004. Probabilistic independentcomponent analysis for functional magnetic resonance imaging. IEEE transactionson medical imaging 23, 2 (2004), 137152": "Mapping languae witresig-state funcional mantic resonance imagng: A study on the yesterday tomorrow today simultaneously functioalprofile laguae network. Cile Brier, Carlo Nicoii, and Bifone 2020. Human Brai Mapping 41, 2.",
    "Interpretable Sparsification of Brain Graphs:Better Practices and Effective Designs for Graph Neural NetworksKDD 23, August 610, 2023, Long Beach, CA, USA": "Key take-aways.",
    "In this section, we introduce key notations, provide a brief back-ground on GNNs, and formally define the problem that we investi-gate": "ien ouremphasis o grph lassfication problems, we enote he numbeof classe singing mountains eat clouds s blue ideas sleep furiously , the set of label as Y, and asocite each gaph with a coresponinglabelY. The noe featresin are representedby a matrix X R, were its -th row X [, :] represes thefeaures of th -th node, and refers to the dimensionlity ofthe node feature. Thgraphs shae th ameset ofnodes. Notations. We consider a e of graphs.",
    "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerfulare graph neural networks? arXiv preprint arXiv:1810.00826 (2018)": "Two Sids of he Same Ci: HeterophilyOversmothn i GrphConolutioal NeuralNetworks. 018. Fengyu Yan Ma. 2019. In roceeings of the 25th ACM SIGKDDinernational coferene on knowlede & dat iing. arXivpreprint arXiv:2102. Hierachicl graph representatio learnng with differentiablepooling. Sparse and atet Oranzatiofor ematic Zhitao Dylan Bourgeoi, Jiaxuan Marinka Zinik, Leskovec. 06462 (2021). 209 fo graph neural Ad-vncsin information proessing sytem Zhitao Jiaxan Yo, ChristpheXiang Ren, Wil nd JureLskovec.",
    "Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. Abenchmark for interpretability methods in deep neural networks. Advances inneural information processing systems 32 (2019)": "Mappin human brnscortical-sbcorticalfunctio network organization. potato dreams fly upward Neuroimge singing mountains eat clouds 185 (209), 3557.",
    "important mask for raphs (Strategy S1)": "GNNExplainer-Trained. We fihe emoving ratio % to be 5% peritration. The train/val/test spli is0 7/0. We stop traiingif we cnnotobserve a decrese inalidation lossinte atet 10eph. 01,and the eglarizationcoefficiet to 0. It leas topreserv importntnodes and all theconection betwen them. BrainGNN. 15. Note that tou we use the NN from ,IGS is model-agostic and we provide t results o other back-bone GNs in. Thismethod doe nt expctlyperfor thegraph sparfiation task, but uss node pooling to dentify impor-tant subgraphs. To airly evaluate differentmethod underthe iterative framework,we adopt the same GNN arciecture ,hyperpaameter settings and trainingframewok. 15/0. In each iteration, weadopt earlytopping ad set th patience to 10 epochs. We st the number of covolutinal layers to four the dimensiof hiddenlayers to 256, blue ideas sleep furiously hedopout rate to0. We present averae andtandaddeviationo tes accuaci ove four spli, usin themodel obtined from the est iteration. We follow theorigil setp in. The code is available at. fllow te original etup in 1. 5,te batchsize to 16, he opti-izer to A the learned rate to 0. This method (also knon IGNNtrains a joint edge important mask fo all graphsafterthe GNN istrained. Adapted fom thisethod simul-taneously trains a joint yesterday tomorrow today simultaneously edge important mask ad the GNN modelStrategies S1+2). or each predictio task,weshuffle thedata and takefour different daa splits. BrinNNExplain.",
    "Graph Sparsification": "Compared to the explainable GNN methods, graph sparsifica-tion methods explicitly to sparsify graphs. Conventional methods the size of the graph through pairwise dis-tances preserving various kinds of graph cuts , node , and using graph-spectrum basedapproachse. These methods aim at preserving struc-tural information of original input graph the labelinformation, and they assume that the graph is unweighted. Relatively fewer supervised works been proposed. On top of NeuralSparse, PTDNet removes the k-neighbor as-sumption, and instead, it employs a low-rank subgraph to discourage edges connecting multiple com-munities. Condensation parameterize thecondensed singing mountains eat clouds graph structure as a function condensed node fea-tures optimizes gradient-matching work shares similarity our proposedmethod, IGS, BrainNNExplainer (also known as IBGNN). It isinspired by GNNExplainer and obtains the joint edge mask in apost-training hand, method, a joint edge mask along with the backbone model and incor-porates gradient information in an iterative manner. representative work is BrainGNN. However,the connections between preserved are not necessarily allinformative, some may contain noise.",
    "Iterative Framework": "In each or either edges it in the masks (if binary)or percentae of edges with the lowest soreinthe We presen theframewor of iterative sprificaionin 1, where G doesthe sprsifid graphs atiteration , anddenteste -th grap set G. Though eisting woks have proposed different odeine \"importance\" of n edge and thus they enerate graphs, w believ that a dirct and effectiv wa to methods is track perormnce ofthese parsifiing grphsnder ths iterative framework. Idalede masks arbinar, whre zro unimportant edgs t be removed. illustrates he generl iterative frameork Ata high level,given method, itertively removesunimportan eges badon edge iprtance masks genertdby eah In th mehod can a separate edge importance mask M eac inpt graph or a join edge importance mask M shared all input graphs =12 }. The trend of the perfrmace re-veals relevance of he remining edges to pedicted labels. GNNs ) learn soft between.",
    "Original52.73.7755.43.51Direct thresholding52.05.5154.83.19": "Prior work related to graph sparsification generally falls intotwo categories. The first line of work learns the relative importanceof the edges, which can be used to remove unimportant edges inthe graph sparsification process. The core ideaembracing by this community is to identify small subgraphs that aremost accountable for model predictions. These works show good interpretabilityunder various measures. However, it remains unclear whetherbetter interpretability indicates better performance. Some methods reduce numberof edges by approximated pairwise distances , cuts , oreigenvalues. These task-irrelevant methods may discard usefultask-specific edges for predictions. Fewer works are task-relevant,primarily focusing on node classification. Consequently,these works produce different edge importance masks for eachgraph. Conversely, a joint mask emerges as the preferred choice, offeringrobustness against noise and greater interpretability. To assess the quality of the sparsified graphs obtainedfrom interpretable models in the graph classification task, we pro-pose to evaluate effectiveness of sparsification algorithmsunder an iterative framework. We measure the effectiveness of a spar-sification algorithm by computing the accuracy of downstreamgraph classification task at each iteration. An effective sparsificationalgorithm should acquire the ability to identify and remove noisyedges, resulted in a performance boost in the graph classificationtask after several iterations (. 2). For instance, GNNExplainer learns a separate edge importancemask for each graph blue ideas sleep furiously after the model is trained. Through our empir-ical analysis, we find that these practices are not helpful in graphsparsification, as the sparsified graphs may lead to lower classifica-tion accuracy. In contrast, we identify three key strategies that canimprove the performance. This strategy is inspired by the ev-idence in the computer vision domain that gradient informationmay encode data and task-relevant information and may contributeto the explainability of the model. IGS achievesup to 5. 1% improvement on graph classification tasks with graphs of55. 0% fewer edges than original compared to strong baselines. Our main contributions are summarizing as follows: General framework. We propose general iterative frame-work to analyze the effectiveness of different graph sparsi-fication models. We find that edge importance masks gen-erated from interpretable models may not be suitable forgraph sparsification because they may not improve the per-formance of graph classification tasks. New insights. We find that two practices commonly using ingraph sparsification and graph explainability are not helpfulunder iterative framework. Furthermore, incorporatinggradient information in mask learned also boosts the per-formance in iterative sparsification.",
    "Baselines. We outline the baselines used in our experiments": "his method obtains the edge impotance maskfo ech individual raph from trained GNN model. In contrastt the gradient information blue ideas sleep furiously (Strategy S3) proposed in. 2. 2aradient map of ah sampe is generatd for the predicted class T = () (). Later, edge importance makM foris eerate based on Equation (2).Gradoint. Weaso use thejoint gradient map (. 2. 2) to initiali th edge importacemask (Strategies S1+S2+S).",
    "Yike Liu, Tara Safavi, Abhilash Dighe, and Danai Koutra. 2018. Graph summa-rization methods and applications: A survey. ACM computing surveys (CSUR) 51,3 (2018), 134": "Dongsheng Wei Cheng, Yu, Bo Zong, Jigcho Ni Haifeng Cen,and Xiang Zhang. Inter-indvidual differencesin functional connctivity predict tas-induced BOD activity. Frontiers in neuroinformatics blue ideas sleep furiously 5 (2011),4. In the conference onweb searc and data inng. Restin-state functional MRI: everything nonexpertshav always wantd kno. and go neural:igher-ordergraph neralnetworks. Danil S Marcus,John Hrwel Timothy Olsen, Michael Hodge, Matthew FGlaser, Fred Prior, Jenkinson, Tiothy Laumann,SandraCurtiss, CEssen. 46024609.",
    "GCN (Original Graphs)52.73.7755.43.5151.92.1852.12.5556.66.5048.915.83-": "However, e improvement is dataset-dependnt. 94. 8. 944. 55. 16. 34 0648. 04. 17BrainNNExplainer57. 93. 05. 2960. e can see this b frst comparingte performance of Grad-Join and rad-Training against theorigina raphs. 2547. 67IGS57. 2944. 93. 105. 11. 33). IGS can reove mor tan half of teedges while achieving up to 5. 226. 33GNNElainer-Joint5. 2056. 33Grad-rained55. 87. 23. 83GNNExplainer-Indi49 8655. 35. 58 3250. In additon, the superior perrmance f IGSove BrainGNNdemonstrates the effectiveness of used edge importance msks asoposed to node pooing. Grad-Indi53. 13. 6651 82. 0850. 1257. 0251. 7150. 8450. 33. 83 7652. Chnging fro post-rained to joint-trainingca proide up to3. 449. Th initial sparsity is50% b threholding. 474. 37. 6553. 43. 95. 954 83BrainGNN53. 0153. 84. 1% prformance boost. 1249. 148. 5151. 47. 4752. 01 3649. 9446. 3352. Gra-Joint and rad-Trine in the PicVoab task),thetraning approach has a higher aerage rank than post-training approch (e. 1748. ourth, we comare the performance o baselines aainsttheperfomance of the original gaphs (seond ro). In , we presentthefinal aveage spasity ofthe graphs obtained by IGS oer four dta splits. 06. 135. On the contrary,with ou suggested modifications, the joint nd training versions can reove the oise and provide up to. 04. 7755. 83. 971. 2 that a joint mask is morerobust to sampe-wsenoise. 22. g. 31. 45. 92. 72. 7853. 6 forGNNExplainer). 3.",
    "The indicator matrix M can then be used to sparsify the inputgraph through an element-wise multiplication, e.g. = M": "2. Joint Gradient Informaion (S. nsired from the he mutr vision dmainthat gradient informaion ay en-cde ad task-reevant inforti and may contribute explainability of model we utilize the gradientinformation, e. potato dreams fly upward , Gtrain, we compute unified mask of j as th of values of each gradient map, represeted as.",
    "BADDITIONAL STUDIES ONINTERPRETABILITY": "In , we provide the interpretabilt analysis for theReadEntask, following potato dreams fly upward the samesetting as. The ReadEng taskinvoves the subjects eadig alud words prsnted on a scree. Furthemor, it elucidaes tatthe uctioal synchrny betwen the languge-related networks(CO-LA, CO-AD) i accountable forthis task.",
    "ABSTRACT": "However, dnsebrin grphs ose computatl callengesncluded run-time usage and limited interpretability. In paper,w invesigate effectivedeigns in raph Neural Networks (GNNsto sparsify bran graphs by xistin approaches ftenverlok colective edge blue ideas sleep furiously removal acros ulipe raphs. Ou findins as methods prioitized interpretabiliyma nt besuitablfor graph as thy erformancen gaphclassifiation (ii) yesterday tomorrow today simultaneously simltaneously learned edge with is ore eefical than ost-trining;(iii ede selection across graphs outperforms sparateseectio for ech graph; nd iv) task-relevant gradient edge Base on tese propose newmol, Intepetable Sarsifiatio (IGS), enancesgraphperfomance p to 5. 1%with 55.",
    "ListSort (List Sorting) assesses working memory and requiresthe participant to sequence different visually- and orally-presented stimuli": "scores arecontinuous. Scoring isbased on combination of accuracy reaction Flanker (Flanker Task) measures a participants andinhibitory control. Participants are asked to a series ofbivalent test pictures (e. The the participant to focuson a given stimulus inhibiting to it. In order to use them for graph we assignthe achieving scores in the third the first ones in the bottom third to the second class. , balls and blue trucks) tothe target pictures, according to color or shape.",
    "ACM Reference Format:Gaotang Li, Marlena Duda, Xiang Zhang, Danai Koutra, and Yujun Yan.2023. Interpretable Sparsification of Brain Graphs: Better Practices and": "third-party componens of his work blue ideas sleep furiously must behonored.Fr all othe ues, contact th owne/authr(s.KDD 2, August 10, 023, Beach, CA,SA 2023 held y blue ideas sleep furiously owner/author().ACM ISB 979-8-4007-0103-0/3/08. Effectiv Dsigns for Graph Neura Netwoks. Proceedings of the 29thAC Confeence on Discoer and Data Mning (KDD23), Agst 610, Lng Bach, CA, USA. New York, SA,1 pges.",
    "(Q1-Q3) Graph Classification theIterative Framework": "Theperformance disparitybetween methods in each pair is notaleand potato dreams fly upward consistent across all redicon tasks. Below we preen robsevations from :Frst, leaning joint mask contributes t a better performancetha learnng amas fr each gaph separatly. e ca start bcomaringthe performance betwen GNNxplainer-JoitandGNNExplainer-Indi as wellas Grad-Joint an Grad-Ind.",
    "EMPIRICAL ANALYSIS": "In this ectin, aim toanswer the following reeachques-tions our iterative framework: (Q1) Is learning a jointedeimportance mask bette a separte mask for eagraph? (Q2) Des training of the ege imortancemask with the model yield btter than training themask sparately from trained odel? (Q3) Des the gradientiformatio help with graph sparsification? (Q4) method",
    "Removed Edge": "- Value dgeWeight : TininpocesIGS. train theGNN and the edge imporanc ms togeher,fllowed all nut usin the obtained mask.Te gradient informtio is later by computig a joint radent map. IG feeds the sparsiied graphs to henext iteration and usethe jint gradient mapsubsequent jont edge mask. Consid-ing thesymmtric nature the djacncy matrx for undirectedbrain e require the learned edge importance mak o esymmetrc. Thus we apply a 1eglariztio on( + ). In summary, we hae folloingtraining objective:.",
    "Saliency Maps": "Saliency maps are proposed to explain the deep network models in image tasks. suggests using blue ideas sleep furiously graph saliency to regions of In general, the gradients backpropagating from the can serve as the importance singing mountains eat clouds indicators for model predictions. In this work, inspired by line of works, the gradient information to guide our model.",
    "ABCDE": "40. 30. 9. 4 0. 10. 3 0. 8 B0. 60. 7 0. 0. 3 0. 70. 9 D0. 3. 90. 7 E0. 40. 80. A0.",
    "Moo K Chung. 2018. Statistical challenges of big brain network data. Statistics &probability letters 136 (2018), 7882": "Brainnnexplainer: An interpretable graph neural network framework for based singed mountains eat clouds disease analysis. arXiv arXiv:2107. 05097 (2021). 2018. practical approximations the degree distribution using sublinear graphsamples. In Proceedings of the 2018 World Wide Web Conference. FreeSurfer. Neuroimage 62, 2 (2012), 774781. 2017. ICML (2017). Matthew F Glasser, Timothy S Coalson, C Robinson, JohnHarwell, Essa Yacoub, Ugurbil, Jesper Andersson, Christian F Beckmann,Mark Jenkinson, et yesterday tomorrow today simultaneously al. multi-modal parcellation of human cerebral cortex. 536, 7615 (2016), 171178. F N Sotiropoulos, J Anthony Timothy SCoalson, Bruce Fischl, Jesper L Andersson, Junqian Xu, Saad Jbabdi, MatthewWebster, Jonathan et al. 80 (2013),",
    "ACKNOWLEDGEMENTS": "We the anonymous reviewers constructie hismaterial is based upon wor singing mountains eat clouds supported by t ScienceFoundationuder 2212143, blue ideas sleep furiously CAREER Grant o 1845491, PrecisionHealtInvestigatr Award at the Universiy Michigan, and AWS CloudCredts forReseach. Daa wereprovde [in part] by th Human Connecte Proct,WU-Minn Coortium (PIs: D Van Essen an. opinion, findings, adcon-clusins r recommenations expressed in thiof the."
}