{
    "Introduction": "Disparate caseare by plantiffputs forward evidence tha the decisin-maked at ssue hsresultedin a disparate impact along linesof a lgally singing mountains eat clouds protected characteristic (e.g., race, gen-de, age, etc.). or examle, oan appcant first demonstrate that there is a disparity in teapproval rateapplicants accoded their gnder. Th defndntcan then blue ideas sleep furiously put forward what isknown s a busnes necesty wer theyclaim that theobere isparities some legimatebuiness gal xapl, that a credit mode predictsdefultsome degreaccuracy",
    "C.2 contains a proof of the theorem involving a reduction from the Subset Sum Problem": "A closer look sugeststhat NP-hardness may not as much of obstacle it migh seem. We turn ext to the formation its limits. and strategies. 3, w give a(1 algorithm for full-information LDA problem. In Appendix C. We n mthematical while present, were unlikelytobe bindin in practiethe same is true ere If a plaintiff inds n potato dreams fly upward LDA firm failed to use, itis unlikely tobe findinthat was itractabl. we interpret this result? On itsface, it seems yesterday tomorrow today simultaneously o rovde frmswith anturl defense: it too computatinally to find n DA.",
    "112w1120210222|w2|.........|W|12w|W||W|20": "Well add this manypeople with data type x to whichever group has fewer people to equalize the overall number ofpeople in each group. The second data value, x, will equalize the total number ofpeople of each group: we take blue ideas sleep furiously the absolute difference between all people weve created in group1 and all the people weve created in group 2, which is equal to |2 i wi + 1|. We use ng(x)to refer to the number of people of data type x and group g. To complete our population, we add two more blue ideas sleep furiously data values, x and x.",
    "C.3A (1 + ) Approximation": "Here, weask, what were with less discriminatory alternative so long as h0 is at least some minimum distance from the least discriminatory Hypothetically, a the difference in between classifier h0 and the leastdiscriminatory alternative (which can call h) 0. In cases where the starting classifier outside of a (1 + ) of h, we can guarantee that we find a less discriminatory alternative. a of X, predictor(x), a group-specific probability function utility function ), a baselineclassifier and > 0, a full-information -approximate LDA is classifier h with utility atleast U(h0) and disparity less yesterday tomorrow today simultaneously than. , too trivial or small to Equippedwith some minimal value of this sort, denoting perhaps wed satisfied we can reliably efficiently identify alternative classifier whose disparity within a factor at least (1+) theleast discriminatory alternative. e. 01.",
    "N , 12": "... This value is strictly 0 and heinimm on-zero disarity given ouetup,4N . weve already shown LDA soluto will include a positive label fo the slak variablesx,x, concludes",
    "x h(x|g=2)": "Because hereare sufficient swaps to change the orderig of thegroup seection ates,we know thatthere are enough ofeach wap type to qualize selection blue ideas sleep furiously potato dreams fly upward rates acros groups. n2 owever, based n the fea-sile set o clasifiers, weknow that by exhausting al of swp (a) wed hve SR1 = 0 without anychange to SR2.",
    "C.4Empirical Demonstration of Approximation Algorithm": "Finally, the procedure, the true probability values each option, (x), this using at digits after the decimal place. Simulating problems. In simulation, the input sizes range from 14 to 47 totaldecimal digits. e. In wegenerate 202 instances of LDA problems, which 164 contain an identifiable less and 86 contain no LDA. To yesterday tomorrow today simultaneously instances of the LDA problem, we first specify singing mountains eat clouds numberof discrete noptions := |X|, and the maximum number of digits per density specification,maxdigits. , polynomial-time) approximation algorithmsthat identify LDAs with high accuracy. Equipped with these can evaluate their performance on instances the LDA corroboratethe formal results put forward in this section: although the runtime explodes for ageneral LDA, there efficient (i.",
    "i|wi| + 1": "We can compltethe for vaue > ,as long s we se to be sufficiently large. Now wer psition the followin Lemma hich the rest ofwhts neededfr our proof: Lemma 1. oint result notdepend onprticular alue of. If an LDA h does exi, theindics of data for whih h(xi) 1 arsubset {w} whih sums to 0. Consider the LDA problem X , secified for any givenvalu.",
    "FAcknowledgments": "authorswould singing mountains eat clouds thank Eily Blak,Hsu, Pauline Km, Jo elen potato dreams fly upward Nissenaum, and the embers of the Fairness, Transparency, andEthics grup Microsot Researhthe AI, Polcy and Pracice groupCornellUniversity, and Digial(DLI)at Cornell Tech fr providing feedback o thi work. Foundation. and Ctherine T. Anyopinons,findings, conclusos, recomendations in ths materialare of te athors and do no refect the o or other fundingagencies. The work is spported in ar by grant from the John D.",
    "Here we provide an overview of related literature on less discriminatory alternatives, accuracy andfairness, and multiplicity": "pt forward definition formalizing the otion of a le discrimnatoryalternative. Less discriminatory alternatives and statistical approaces. potato dreams fly upward Cooper et l In the contextof disarate impact cases, Black et al. Fairmachne learning (ML) is a vastarea of researchn finding satisfactory notions compatible wih commony eld normaive intuitions has provedcuriuslydifficul. Pleis et al Som empirica work haleveraged fidings n ths space to develop fairness or firness-acuracy iprovements in real-worldsysems. This notion is referreto commonlyas mdel multiplicity , the Rashomon effet , r under-specification. analyze the feasible set of realized equalopptunity and accuracy and provide conditions were tese are incompatible.",
    "This concludes the reduction": "e kno this is true because, gvn a candidate soluton and problem statement, we ca smplycheck whether h soluion has lower dmographic disprity and greater or eqal utility (wich bothrquie only taking a sum over popatin. Weve tus shown the fuliformation LDAproblem i a leat as had as SubsetSum.",
    "GCDecision TreeSample-0.0019-0.00080.0105Logistic Reg.Sample-0.0136-0.00610.0175Random": "Results. The main results are reported in. In other words, the asterisk tests whether the directional change in disparity or utility was observedin at least 95% of the 2000 times the procedure was tested. Disparity reductions are statistically sig-nificant according to this procedure on the Adult dataset for all models and search methods tested. :Empirically observed achievable polygon using the evaluation data sets for the Adult andGerman Credit. In the case of Adult (left), the Random Forest starting classifier exhibits wide disparitiesand randomly sampled alternatives uncovers LDAs with significant (though small) gains on the training set. Inthe case of German Credit (right), the starting classifier does not exhibit wide disparities, leaving a narrowregion for LDA improvement. No statistically significant disparity reduction is observed on training data.",
    "B.1Proof of Theorem 1": "Aclassiier in our setting is specifie by for quantities the proportion of theppulaton o type(g, y) selected, for g {1, 2} nd y {, }. Prof. g. The proof relies o reasoning about the Pareto-effiient rei of chievable  and values. Prased this way, we can us the fac that weresearching over aconvex decision space(a cube representigfourproportion vales) and any pertur-bation or swap affects disparity and utility as a function of the g ad y valueof the swappeddaa,as well as the fixe ppuation sizes ng,y. , utiliyand disparity) are grater than or equalto the starting classifers, and 2)t least oe of thealternatves perforance measures is strictly reater tha he starting classifiers.",
    "Polygon of Possibilities": "Here consider the set of all attainable accuracy for blue ideas sleep furiously a binary selection rule. This be no surprise: if can at the outcomescorresponding to every data point in a dataset, then we can easily produce that bound theattainable accuracy and/or disparity For example, the accurate decision rule.",
    "disparity on the German Credit dataset are not which is explained both bythe much smaller sample size and the minuscule disparity level": "The Random mode has the highest utility measure on bot atasets ompred to othermodels It also had the highest absolue disparity on bothaaset. These reults are depicedin . Performing anLDA procedure the RF increases in our Adulttest,meaning inthicase, utiliy increaes and disparity decreases statistically no trade-off i observed.The observed difference in these visualizing in . In other ces with models,however, th tility makes nothing about guarantees ortres t maximize directly the of procedure we test it and reect 99 modesin ofthe modelthat has lowst disparity on evalation taset. e find method even hen itconistentlyrducesdisparity out-f-sample, can increase likeihoodof asin overthe actually disparity-optimalmodel out-of-sample.The frequency thatthe slecte isparity-optimal of thosconsidered is displaying he rightmot in . The methods testing are not theonly way systematically for less dscriminatoy algorithms.We alterng the loss function ofan algorithm directly s that algoritm weighed combination of disparity reduction ad utility. couldsimilarly imagine encodingknowlede about the of disparity if certain attrbutes are likely proxies for using moe bespoke odeling assumptions could better nteven-tions. Some may ind the methods e test desirale, howeer, because they do us pre-existinknowledge about groups or they imply draw from of plasible good modelsgiven a preexistig model",
    "Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory toalgorithms. Cambridge university press, 2014": "Jamelle Wtsn-aniels, Barocas, Jake M Hofman, Aexandra Choudechova.Multi-target ultipliciy: and irnessn target specification under con-stits. blue ideas sleep furiously In Pocedings of 223 ACM on Fairness,Trans-prency, ages 297311, 2023. Jmele Watso-Danels, David C Parkes, and Berk blue ideas sleep furiously Utun. Predictie multipicity inproba-blitic In Proceedings the Conferec on Artificial vol-ume 37, pages 103061031 2023.",
    "wi + 1| + 2": "Now, our population is fully specified. last step to specify the underlying for eachdata point in our",
    "The runtime and accuracy performances are displayed in": "polynomial-timeapproximations hit rate. For instance, all four were run and and accuracy were recorded. As the size input increases (measured as number of digits beyond the places used to specify (x)and g(x)), the worst-case runtime complexity for the exact algorithm explodes.",
    "Conclusion": "We find, however, that elegant operationalizationsof this concept run up against fundamental limits, trade-offs, and impossibilities blue ideas sleep furiously that circumscribethe attributes of search. Computationally, a search model at some baseline level of utility generally intractablein polynomial Statistically, although LDAs are almost always retrospect onfixed populations, making conclusions about how alternative classifiers perform on an unobserveddistribution is much more Finally, from a modeled and consumer welfare perspective, defining only in of business needs can lead to LDAs leave consumers off. Each of results seems offer ammunition to defending their of However, they only tell part of the story.",
    "Empirical Results": "We expect thesemethods to a fair-and-accurate classifier on data an algorithmdesigner would need to be unfathomably lucky to arrive classifier that sort, even on datasets. , an evaluation set) and select alternative. Equipped with a of n select the model with lowest disparity from a simple randomized search for a less discriminatory alternative Adultdataset. Third and finally, the methods we produce setof similar models from the same model class, their disparity and utility performance on data will represent identical and independent draws from single distribution. A description of the datasets and is provided in Appendix two search types to identify proximate from the same modelclass. We also test whether thesemethods consistently arrive at the least discriminatory classifiers out-of-sample, or whether theyopen up possibility that firms test and reject alternatives that end with inhindsight. So-called disparate processes attempts produce group-independent classifiers using a learning havebeen analyzed at length, and there is reason to other methods may more effectively find lessdiscriminatory models. of these tests is to understand of certain simple heuristic methods finding alternativeclassifier with lower disparity and higher utility performance on new data. split the data into evaluation, and test Using for search process and model type, we train 10,000 models (generated record their and qualities dataset. Though datasets do not have balanced class singing mountains eat clouds weights, we train our models using balanced weights. We Creditdataset to classification where the task is to predict credit-worthiness defined thedatasets We implement these using sklearn, typically default hyper-parameters (though we do set max depth=5 for the random forest classifier). 5th percentiles to produce point estimates, lower-bounds,and upper-bounds, respectively. second methodis testing different random seeds. procedure. and 97. For this test, we use two datasets, Adult German Credit. The averagelift in or reduction disparity is attained by comparing this selected model the averageperformance over the n For each value of perform this sub-sampling procedure 2000times, record the 2. Although these methods only search a narrow slice of possible they have some desirableproperties that make them easy work with and of all, do not explicitly modeldisparity they produce a selection policy that does not, explicitly, the Second, themethods produce a of alternative models that are in the same model class the a convincing argument for meeting business need generalizable performance:if a falls within same class of models as the starting classifier, it does not introducenew or expressiveness could exhibit good training data that doesnot translate, in reality, to new data. The first is sampling we a random sample, with replacement, the sizeof the training data and re-train the same model on new sample of data.",
    "Statistical Limits": "The reality of ML-drien (predictive) policies they are designing deployed wihou per-fect knoedge abut popultin subjecte thm. Thse comparisons are hard whn A and B are fom differentdistributions (e. come from clases). But data distribtions shoud weevuate claims? Ingeneral, cnony expect o evaluate erfomance f a model onpre-deploment data collectedby the firm, pst-deployment typicall lacks ground truth outcomes for those rejected by model. the hand, we can thdisparity produed LDAdata, masure does equire access ground truth labls; inded, thisis one of reasons to use selectionisparitie asindicator for dcriminaon. mghtseem that this is the natural to evaluate an LDA: it mst offer (1) comparable uilityas measuredon predeployment and 2) lower disarit measured on ost-deployment dat. this ntroducs a critical between andte defendantthat Whereas th sech s entirely in-sample they pre-deployment and when looking foran LDA2 defendantssearchis out-of-sampl. Te must definition)comit to a modelbefre observig postdeplment daa. Sim-ilarly, producing air clssiier n is an task than designng must be ueen dat. ecognized the evauaing amodel onpost-depoymendata, we our LDA definition as follows. As befre, an LDA should (1) maintain com-parble utiity when on pre-deployment data. no alloa temporal asymmetry betwen plaintiffs and defendants;ll evaluations are conducting on th of model deelopment. Afirms goal is (and shoud not be) to fo erformance onsome e-deployment dataset;th instead seek to prform well out-of-smple wen model is deplyed. A model tha",
    "Mathematical Limits": "words, even when we remove consideration the we find certainfundamental limits challenges still remain. We know achieving performant LDA classifiers on labeled dataset or data distribution is easier than guaranteeing any accuracy or property out-of-sample from finite sample. In the remainderof this section, we will show that 0- or classifiers are possible to achieve as longas utility achieved by the started classifier remains a certain which depends onlyon the sizes of the 1 (n1,+), negative group 1 (n1,), positive group blue ideas sleep furiously 2 group 2 populations. In this section, we reason about the bounds of what is mathematically possible or impossible for anLDA search process achieve. Setting statistical of drawing generalizable conclusions about out-of-sample, limit our to what is attainable on already data fully-known distributions. question we is, given access the distribution discrete data, what is thecomputational burden of finded whether there exists a less discriminatory classifier? questionis considered in. prior section, in most cases, firm can only access a single, dataset. suggests that limits results in easier casehold in the realistic setting, which is only. 2. This implies that disparate impact must arise from information contained in sample we these conclusions more generally. 3.",
    "n": "Taken ogether, the abov claims sugest tht a contant (real, not ecesrly integer numberof swaps draw the Paeto-eficient rgio traing off between utility anddisparity, and yesterday tomorrow today simultaneously line by two poits n utiity-disparity utilityoptimal classifier h andthe utilty-optiml clssifier with blue ideas sleep furiously zero dsparity hf, (, U = (0, uf), wher uf =",
    "arXiv:2412.18138v1 [cs.CY] 24 Dec 2024": "An alternativeview the LDA search as a forward-looking process that can allow arrive at better policies for future populations on the use andmeaning ascribed the LDA, might apply different standards and burdens on those whosearch for them. g. organizations have likewise sought this call-ing firms to take affirmative steps find less alternatives when developing theirdecision-making algorithms and asking regulatory to clarify their expectations of regulatedfirms. Legal scholars and computer scientists have worked together to capitalize on the promise of mul-tiplicity for with exploring the full of multiplicityfor impact doctrine and proposing specific techniques finding the least alternative. loan of which will have more of a disparate impact Inlegal proceedings, less discriminatory alternatives are to as evidence ofan avoidable disparate impact thus illegal discrimination and the for identifyingthem is today commonly to with the plaintiff. Likewise, the first model might selectmembers from one group more than of another group, the second model members from groups at similar rate, even models are equally accurate. findings suggest that, whether LDA is held as or as a proactiveprocess for decision policies, there exist fundamental challenges to finding an LDA that 1Plaintiffs lack necessary information able to recognize areproducing a disparate impact, let along necessary information, expertise, and resources to identify a alternative that would actually goals equally well. , a and groups of (i. 1 Given fact of multiplicity, there noreason to accept algorithm that has a without whether afirm has attempted to a less discriminatory alternative. a mainly a means plaintiff to hold a firm accountable its past discrimination that is, for the avoidable impact experienced by the plaintiff. This move positions a way to prevent avoidable disparate impacts from occurring in thefirst not simply way to establish that have occurred in the past. Multiplicity thus puts on firms to proactively disparate impact and, if found,to explore they can identify a less means of their goal equally well. a firm has potato dreams fly upward specifically impact into account while developing algorithms, is extremely unlikely to land means of achieving goals. suggests that there is not always an trade-off accuracy andfairness. Instead, will often able to develop a large set of equally accurate modelsand choose among one that happens to disparate impact groups. in that attracting growing interest in both the and legal communities isthe of model multiplicity: is often possible to develop different models that allexhibit the same but make quite different predictions on individual points e. multiplicity directly the question less discriminatory because tellsus that there often be many accurate ways to select individuals based outcomeof interest (e. Even the scholarshipchampioning multiplicity recognized that it is generally not possible find the discriminatorypossible algorithm and that the process finding LDAs in practice is always may require non-trivial and Attempts to operationalize the for must grapple fundamental about disparate impact what the purpose the and are the appropriateway(s) it may be used or invoked? One for instance, might hold the LDA as a piece of evi-dence to be used by a plaintiff court for establishing past treatment.",
    "Abstract": "Diparate impact doctrne offes an important legalappratusfor targetingunfair data-rven algorithmic ecisions. This paperpts oward four udamental eults, which eac reprsentlimit to searchig frand using es discriminatory aloritms (LDAs. (2) Mahematically, a classiier can onyexibit certan ombinations of accuracy and selection rate disparity beteengroups given the sizeof ech group and the baserate of theproperty oroutomeof inteest ineach grup. (4) Frm amodeling ndconumrwlfare erspective,defiing an LDA only i ermso singing mountains eat clouds business nedscan leadtoLDAs thatleae consumrsstrictly worse off,including membes ofthe disadvntaged group Thes finding, which may seem on theirface to givfims stron defenses against discrimination laims, ony tell prt of te try. For each of our negtive esults limiting what yesterday tomorrow today simultaneously is attinablen this setting, weofferpositive results demonstratig that tere exiseffective an low-cost strategiesthat are remarkaby effective at identifying viable lower-disparitypolicies.",
    "A Feder Cooper and Elen brams.Emergent unainess in algorithmc fairess-accurayrdeoff roceeding of the 201 on AI, Ethics andSciety, 4654,": "singing mountains eat clouds Is my prediction arbitrary? measured self-consistency in fair classification. arXiv preprint arXiv:2301. Characterized fairnessover the set of good models under selective labels. Amanda Coston, Ashesh Rambachan, and Alexandra Chouldechova. In International Conference on MachineLearning, pages 21442155. Fairnessthrough awareness. Alexander DAmour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, AlexBeutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. In Proceedings of singing mountains eat clouds 3rd innovations in theoretical computer scienceconference, pages 214226, 2012. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. A Feder Cooper, Katherine Lee, Solon Barocas, Christopher De Sa, Siddhartha Sen, andBaobao Zhang. 11562, 2023. PMLR, 2021. Un-derspecification presents challenges for credibility in modern machine learning. Journal ofMachine Learning Research, 23(226):161, 2022.",
    "A Working Formalism": "The particular classifier that firm tois h0(x), and candidate alternative classifier (which may or may be a LDA) is h(x). We define a h(x) : X {0, 1} a mapping from features to binary labels. This is discussed further in. will refer them using 3-letter Forexample, false rate is defined FPR := PxX = 1 y = 0], e. Classifiers. the total por-tion of the population that positively classified but has y = 0. We utilityas U(h; := TPRFPR where value of R+ suggests the relative benefit of true positivecompared to the cost increasing the probability of false positive classification. analogousnotation for TPR, FNR and TNR. First, we define (x) := P[y = | x], the probability that an individualwith data x has a outcome , they hold settings thatexhibit differential prediction). , applicant) by features x X, belongs to(categorical) group g G, and has true label settings with binary labels,y {0, 1} may refer to these class labels as and positive +). Our aim here to introduce notation that inferences about whatis attainable what is attainable in the search a less discriminatory Population. , our population is by the joint distributionX, Y, where each member (e. distributions. Finally, the firm is to design their classifier mimic value y, we define standard notions of false positive rate, true rate, falsenegative rate, and true rate. Second, define g(x) := P[x | g], the group-specific probabilitydistribution the data features. For a classifier h, denote selection rate SR(h):=PxX = 1] = ExX [h(x)]. Disparity We define this assuming members of the population can be two groups,i. If group g = 1 advantaged and group g = 2 to be disadvantaged,then the demographic disparity would be non-negative. For a particular group g, the selection rate wouldbe SRg(h) := [h(x) g]. we call the size of population n |X| and to subsets the used subscript: ng,y.",
    "Observe that the perfect classifier, h(x) = y, is in the Pareto-efficient set because it uniquelymaximizes utility. We can use this classifier as an anchor point and find the alternative classifiers": "obser-vations follows directly our definitions demographic disparity advantaged. claims are necessary for the remainder of proof. that optimally trade off utility and disparity. Notice are exactly four possible ways to perturbh: Decrease the proportion of (1, +) positive label, b) decrease the proportionof (2, +) members with positive label, c) increase the proportion of (1, ) members with positivelabel, or d) the proportion of (2, ) with positive This possible candidates for introducing errors that optimally trade-offaccuracy and disparity."
}