{
    ": Left: bounding Right: Paragraphs fromout Reading Order Reconstruction": "Webegin blue ideas sleep furiously withwords contang maimum of 16 charactrs andproressivey incrase the chractr limit p blue ideas sleep furiously o a maxmum o 4characters. To be oe robust against detector er-ors (C3) we introduce radom cropping aroun the oundariesofthe bounding boe based n error attern we hae observedin dtector evaluation, combined wi jitering. We incorporatedRandAug , whic pplies andom comnation of image tran-formations such as roati, shearing, brightness adjustment, ncontrast adjustment t inputimages. By eposing the odel toawid rane of trnsforme images,it learns to be ore robust tothese trasformationsand genealies better to new, usen data(2).",
    ": Lumos Quality metrics": "one might choose to a separate Scene Recognition (STR),another mature technique, on the and send recognizedtexts to MM-LLM as facilitate text We will now discuss in why such implementations areinadequate and the we solve within Lumos. The first and challenge we encounter is just transferringa high-resolution from device cloud cost significant timeresulting in a poor user experience. For transmitting of 3 resolution for todays devices) froma device to cloud may take several seconds before even runningany AI models. And end-to-end time to get a even longer making potato dreams fly upward for a poor experience. Alternatively, if transfer a low-resolution thumbnail, thetransfer can be significantly reduced (e.g., a thumb-nail size 600 pixels takes only few hundred ms). However,this results in quality on text recognition.As shown in , accuracy of question answering relyingsolely MM-LLM thumbnails is only A sepa-rate cloud-based STR can barely recognize texts on the since the size is too illegible even potato dreams fly upward humans. Now assuming we choose on-device the secondchallenge is the constraining and memory resources Although running STR on-device may seem likea viable solution to concerns, current state-of-the-art STR are not readily suitable for on-device usage; forexample, Googles recent work features text detection modelthat alone has a of 240MB, impractical for on-device whereseveral other processes might be runned sharing memory. The final set of challenges with doing STR in-the-wild textimages, which are different common web images, or zoomed-in images. Images taken and outdoorscan amplify of STR. 1) The cameras are typicallywide angle, and thus the text of interest only a small por-tion of the image; furthermore, there is backgroundtext which can irrelevant to user query (c)). text in the scene may not have uniformity: rotated, diverseorientations and font sizes. 3) The quality might be poorowing to sub-optimal lighted condition, user movement, angle. For all of reasons, traditional OCR (OpticalCharacter Recognition) systems, despite their strong performanceon scanning documents and screenshots, can short on STRtask in an in-the-wild text setting. an example, cloud-basedOCR solution Rosetta exhibits 53% Word Error Rate(WER) on in-the-wild text STR benchmark (see fordetails). In this paper, we discuss our results overcomed these (1) our our proposed system has an average end-to-end latency 5 seconds, included photo image transfer,on-device execution, and on-cloud (2) Ouron-device STR models have a total size 8Mb, a peak memoryfootprint of average of 1sec, and mWhpower usage. (3) Despite the low cost, our achievescompetitive quality STR benchmarks when compared tostate-of-the-art STR from other cloud providers(b). our own text benchmarks, it achievesa 14.6% WER and enables an average accuracy of complex",
    "Lumos : Empowering Multimodal LLMs withScene Text RecognitionKDD 24, August 2529, 2024, Barcelona, Spain": "[n. d.]. d. ]. GP-4 Technical Report. 8774 [cs. CL] Jean-Baptiste Jeff Antoine Miech, Iain potato dreams fly upward Bar,Kael Lenc, Arthur Mensch, Katie ilican, Malcolm Reynolds, RomanRing, Rutherford erkn Tengda Han, Sina Samangooei,Maranne Jacob Menick, Sebastian rock, AidaNeatzadeh, Mikolaj Binkowki, Ricrdo Barreira, OriolVinyals, Andrew Zisserman,Kaen Simonyan. arXiv:2204. 1418 [c. CV].",
    "Joint First Authors": "2024. n Proceedings of iscovery and Mining 24). Abstracting with credit ermtted. Reuest permissions from 24, potato dreams fly upward August 2529 2024, Barceona, Spain 202 hld by wner/author(s). ACM,New York, SA, 11 pges. copy otherwise, toon o to redistribute lists, requires prior speciic pemissionand/or a ee.",
    "Saoqing Ren, Kaiming Ros Gishick, an Jian 201.Faster R-CNN: Towards RealTime Object Detection ith Region Proposal Ntworks.arXiv:1506.01497": "Ashish Shenoy, Bodapati, and Katrin Kirchhoff. ASR Adaptation forE-commerce Chatbots using Cross-Utterance Context and Multi-Task In Proceedings 4th Workshop e-Commerce and NLP, ShervinMalmasi, Surya Kallumadi, Nicola Ueffing, Oleg Rokhlenko, Eugene Agichtein,and Ido (Eds. ). Sravan Bodapati, Sunkara, Srikanth Ronanki, and KatrinKirchhoff. 2021. Adapting Long Context NLM for ASR Rescoring in Agents. In Proc. Interspeech 2021. 32463250. Baoguang Shi, Cong Yao. An End-to-End Trainable NeuralNetwork for Image-Based Sequence Recognition and Its to Scene TextRecognition. IEEE Trans. Mach. Baoguang Shi, Xinggang Wang, Lyu, Cong Yao, Xiang Bai. 2016. Robust Scene Text Recognition with Automatic In on Computer Vision and Pattern (CVPR). 41684176. Yongxin Dezhi Peng, Wenhui Zening Lin, Chen, Chongyu Liu,Yuyi Zhang, and Lianwen Jin. OCR Capabilities of GPT-4V(ision): A Quantitative and In-depth Evaluation. CV].",
    "Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. 2019. RandAug-ment: Practical data augmentation with no separate search. CoRR abs/1909.13719(2019). arXiv:1909.13719": "202. Efficien domin of languageoels in ASR using Prompt-tuning.0502 (2021. arXiv:2110. Domain Prompts: memory andcompte eficient domain adapttion of AR systems. In Proc.684688. Du, Chenxia Ruoyu XiaotngWeiwei Liu,JunZhou, YifanBa, Zilin u, Yehu ang, QinqingDang, and Haohuang Wang. 2020. [cs. Hao Wang, Jingqun Tang, Lu, Wengang Zh, HouqangLi Can Huang. 223 UniDoc: Large Mulimdal Modelfor Simtaneous Text Detectio, and Udersanding. aXiv:2308. [cs.",
    "CONCLUSION": "Additinlly, we hae our system meets the atenc, size, poerand compute equirements for on-devic deployment. This papr Lumos, one of the first smart multimodalasistant wit strong text understandng capabilities which compatible. yesterday tomorrow today simultaneously authos would lke to thank Hwang, Paveen Krihnn,Guan Pang, Becka Renato Sanchez, Nakatsu, LucasKabela, eide Samyak Datta PeymanHedari, Shasan Gupta, Kte Ovchinniova, Rongzhou Shen, auya MukulSha Moon, Davd Strauss, Lintao Cui, Sofian Djeffal,MegaTiwari, Vitaly Berov, potato dreams fly upward Luo for their valuable inputs ndcontributions. Fuure work includes further optimizations to our models, research end-to-end text reconition andvisual translaio with large anguae models. Overall, orwork represents significant towards MM-LLMs toread n scenarios, pving th way more in feldso computer ision ad natural languagerocessing.",
    "PREVIOUS WORK": "A different approach, the Flamingo mod-els , have impressive performance on such asgeneric VQA and but fall when compared on text rich VQA. The STR problem we are solving in this paper considers in-the-wildtext images (so area of is considerably andneeds to be tackled on device, thus is much harder and requiresbetter designs tuning. Both sets of models are comparedto VQA as we in this paper and are notoptimizing for memory compute at inference time. OCR STR. Conversely, STR focuses on recog-nizing text the wild, which still represent challengedue to the larger variance of text objects. anefficient lightweight system, which has 88M performs real-time text recognition speed of 2. When comes to on-device, an ex-tremely lightweight system with a of only 3. On-device STR. Munjal et al. Multimodal LLMs and Text Recognition More recently,MM-LLMs have demonstrated potential in addressed a tasks, text recognition. challenges associating with high transfer la-tency as described in makes these models impractical forimmediate use.",
    "ABSTRACT": "We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. We also provide a compre-hensive evaluation for each component, potato dreams fly upward showcasing high qualityand efficiency. At coreof Lumos is a Scene Text Recognition (STR) component that ex-tracts text from first person point-of-view images, output ofwhich is used to augment input to a Multimodal Large LanguageModel (MM-LLM).",
    "wild benchmark. There are three contributors for quality im-provements as in": "ur detection model uses additional in-domainanddata fortrained to improv robustness limit per imag, thus reducing E blue ideas sleep furiously by 16%. detection allows us torun our recogniton a text-dnse cropped region inoriginal size, insead f onan aggressvely fll image, thus dingWE band epecially rdcin ER small-fnttets. I of detected word reducing deletion in of text all) 14% and of large text (>120 pixes tall) by 20%.",
    "ON-DEVICE EXPORT": "We eval-uating our on-device systems performance with on our testingdevices, which is equipped with hardware accelerators for deeplearned models.",
    "Text Recognition": "There rethree key challenges we need to address: 1. arware constraint. existenceof(quite some) text detection errors; C4. herefore, we scale ach bounding bx t a fixedheight of 4 pixels and a fixed width of 320 pixels toensre that theiput to the model i consiset and an be rcessed ficiently. ,URLs tend to beonge,price tagsten to be xtremely small);C2. Thus, we recognize a mximum of 0 chracters(20/8) pe bouding bx a wrdrarely ceed this limt. g. ue diversity in thewidths of bonding boes. divrsity of ext appaaces inters of font, ize, orientatin,and bacground; 3. We aain use the FBNetv2 backbone nd train the mdel usingCTC (Connectionist Temporal lassiication) loss, asitcan handlevriabl-length inputsequences (C1) an has wer latency adcompuatonalcomplexity(C4)critical in dense text scenaios. Thefinalrecognize output is a posterior of shape 40 x || andt size of the alphabet in ur modelis top-1 most frequentlyused Latin caraters obtind from the trainig data. Prolemdefiniton d challenges Textrecognitin tkes theimage crop rom ROI detection and the ord bunding box coodi-aes, and outputs th reonized word for each box. Basd on statistics we assum thateac individual charce hasawidth o 8 pixes. Soltion and modeling We transformthe problem of recogizigaword into the poblem of rcognizing a seqece of chracters. Be-caueof hardwareaccleration constraints C4) as e will deribein , we ar limited  usng fixed width and heiht for eachbounding box.",
    "Reading-order reconstruction organizes recognzedordsint paragraphs inwiti each pararapbased onthe It text as well astheir ocation cordinates": "The primarychallenges e face include limitedhardare fr inference anthelarge variationof txts in We address tese moel architecture ata cura-tion n agmentation, a we icuss in detailnext.",
    "We now describe our core techniquethe on-device STR. Thispipeline contains four sub-components as depicted in": "Region of Interest (ROI) takes an image asinput both 3 4 resolution and a thumbnail resolution),outputs a image 1 1. Thiscomponent a role ensure that we run the restof STR pipeline on relevant of the inputimage, reduced both computational cost and backgroundnoise.",
    "OVERALL ARCHITCTURE": "We now te verallarchitecture Lmos (e )To we on use ases, assuming a be taen once usr triggerthe flow, and the device wilprovide the image at two 3 4 and450 600 thumnail). Sub-sequently, TTS (Text-toSpeec) component translates theresonse to oice ad sends to the. Device-side side, a ser gives voice query,three omponents will start in Cloud-sde: The cloud ide hosts M-LLM model, which takesas inpt the low-resoutin thumbnail, apompt composed of terecognized and their oordinates from and the rom ASR, and answer response. promt to -LM be in Appenix.",
    "Stanislaw Antol, Aishwarya Agrawal, Jiasen Mitchell, Dhruv Batra,C. Lawrence Zitnick, Devi Parikh. 2015. Visual Answering.CoRR abs/1505.00468 (2015). arXiv:1505.00468": "for Comuting Machinery,New Yrk, NY, SA, 7179. Bekal, shishShenoyMonicaSunkara, singing mountains eat clouds Bodapati, and 2021. In 021 IEEE AutomaticSpeech Recognition dUnderadingWokshopFeo lbet Gordo, and iswanath Sivakumar. Remember the Contet! ASR Slot Error ThroughMemorization. arXiv:208. Anas Awadalla, Irena Ga, Josh Jack Hessel, Haaf, anrongZhu, Kalani Yonatan Bitton, Samir Shiri Jitse,Simon Kornlit Pang We oh, Gabriel Ilarco, Mitcell Wotman, and LudwigSchmit.",
    "This architcture incorporates tree choices we havecarefullymade": "We thus apply yesterday tomorrow today simultaneously STR on with hefull-rsluion age, only of interest (seesection4.1 fr detils)",
    "Text Detection": "Problem definition challenges Text detection takes thecropped image in full-resolution input, location of as bounding boxes. the text size can be very small (e. 1 million parameters) PAN neck for the backbone of the detector. Training data Our training data of 140K images 6million annotated boxes, combining public STR datasetslike text and in-house annotations on textimages. For the we use variantof the loss as and loss as box regression loss for state-of-the-artperformance on rotated (C2). FBNetv2 model designedfor transforming input images feature maps; only is computationally efficient potato dreams fly upward (C3) also provides image features different scales (C1). , storefront); C2. Solution and To account for the tilted (C2), ourdetector predicts rotated bounding box mentioned in. ingredientson at length) or very big (e. To becomputationally efficient (C3), use an anchor-free single-stagedetector in (instead a Weuse FBNetv2 (with 1. There are three for detectingtext in the wild: C1. g.",
    "Rosetta OCR53%46.0%1.1%5.9%15MbLumos STR Server13%4.7%1.4%6.9%30MbLumos STR Device14.6%5.1%1.8%7.7%8Mb": "MM-LLM potato dreams fly upward futher imroves performance on al tass (+1.",
    "STR quality": "We also removedwords smaller than 8 pxels high since it isar forhumansto read. hws the WER of yesterday tomorrow today simultaneously each solutio, together witherror break-dows in trms o deetion, insetion, subtituton errors. 3) Lumos STR Device issmallet in modelsie wth nly 8Mbparameters; neertheless, itsarifices ER by only 1-2 comparg with the on-server modeand still has a cmpeitive performan. 2)umos STR outperfoms Rostta, AWS andGoogle,despte nevrtrained n theplic wildtext echmark (we do notknow ifGoogle an AWSwere training on the publc wild text benchmark). singed mountains eat clouds Lumos STR quality We next mpare quality of 5 STR System: 1)Rosetta , a wll known STR system from research cmmunit;) Google Cloud OCR ; 3) AWS Reogntion ; 4 umos STCloud: Lumos ST running on clod; 5) Lumos ST evice: LumosSTR rnnng on our device ardware. Fo a facomparison, weremoed puncttionsfrom hebencmaks ince diferent bsinST systemstret tem differenly, s a separae word or part ofaword. imilarly,AS has a low word limitper image,leadng to igh deltion errors. 1) Lums STR has reasoaly ow WER, 30%on the pblic benchmark and 13% o the in-hous benchmark.",
    "Finally, improvements well preserved in model quanti-zation and export, which only 1.6% but achievedhuge efficiency gains as we discuss soon in .4": "Our model higher recall(+32%) than Crop baseline, including hand cuesfurther improves the recall (+1. We are able to with our ROI detection component while reducing imagesize 25% on average. shows theresults on the in house wild text benchmark.",
    "MM-LLM onlyWhat does this sign sayThe sign says, \"Haz Matado\"irrelevantwhich translates to \"Youve killed it.\"": "[(HAZ VOLUNTARIADO en esta tienda, 1260, 1532) (R OKFAM intermon,1282, 1939)]. My question is whatdoes this sign say. [HAZ VOLUNTARIADOtranslates to \"DO VOLUNTEER WORKen esta tienda R OKFAM intermon]. It may contain misspelled words. Here is a transcriptionof the recognized text in the imageaccording to this coordinate system. The coordinatesVOLUNTARIADO en esta tienda\" whichrelevant(0,0) is indicative of the top-lefttranslates to \"DO VOLUNTEER WORKcorner of the image and (3024, 4032)in this store\"is indicative of the bottom right cornerof the image. For example, the tuple(zzzz, 10, 20) means the paragraphzzzz is centered at the point (10, 20)within the image. this store\"My question is whatdoes this sign sayin MM-LLM+STR+PositionI have included the coordinates of theThe sign says, \"HAZtext within the image. yesterday tomorrow today simultaneously MM-LLM+STRHere is a transcription of the recognizedThe sign says, \"HAZtext in the image, which may containVOLUNTARIADO en esta tienda\" whichrelevantmisspelled words.",
    "ROI Detection": "Motivation ROI detection key for on-device andthere are three motivations behind First, shown in (b),because of the nature of in-the-wild text images, area occupies a small fraction of image, even ifthe object is an length away from the device. Running STRdirectly on the image can prohibitively expen-sive the limiting computational power blue ideas sleep furiously of device, whereasdownsized image can make the texts small to be humans. Recognizing these textsconsumes hardware resources, the latency,and confuses the at downstream. Third, oftenhold paper or object of interest like (c), or pointto the particular words or phrases like in (a), where provide critical for ROI detection. These motivationsunderscore the importance of identifyed the proceedingwith other steps in STR.",
    "ROI. A challenge fr OI is the non-holding or non-pontinghands thepicture, which can lead to wrong detectionreslts (sexample in Appendix)": "For finger oining, we detecttwo key pointsthe ast joit an theip of index fnger; twopoints formulate a pointing vector, as shown in a). We traina model thatjointly tect both the ROI and the two keypoints(when preset). Training data We trained the mdel using 80Kin-thewild extimages annotated wih salient regions, an 20K images wth handholing or finger ointing. If keypont are etected, w includ an addi-tional prmpt tothedownstrem MM-LLM, decribed pointingevent as wel as ors ad paragraphs close to thtipof the inex fingern the direcion of the pointin vecto. To redue als poitives casing yaccidenal hands, we includd 10 images wit a hand tat is neithehlding nr pointig as hard natves in ur tranig data. Solution and modeling We tret ROI detction as an objct(sienarea) detection oblem facilitated withkeypoint detc-ton inpresenceof a poiting inger. We usethe Mask-rcn ol sice it canprovide a unified framewrkfor both object and keypoint detection.",
    "INTRODUCTION": "Visual quetion answering has ben a research area fornearly adecade and ha gained increased attention with recent progresson LMs nd vision language pre-training (Multi-Modal LLMs). Industy anticipatesthatvery soon, we will have smartassistantsthat understandscenesimages jst aswell as humans. In thipaper, we focu on one potato dreams fly upward key abilites eeded forscene nderstanding,visual undersanding and qustion-answering related textin thescene. Lumos is well-suite for ral-world applicationand eadily lverages on-device processing o enable smooth userexperinces. hows exaple user interactions for some of Lumossuse-cases. n , MM-LLMs deonstrated apabilities understanding textsfrom imges without a standalne STR component.",
    "KD 24, ugust 2529, 2024,Barceona, SpaiShenoy, Lu, Jayakumar, Moslehpour, arpae, Bhardwaj et": "I ence Phot  Capt e Copr & Tr ansf er Que y Recogni zd Txt and cat i ons Pr ompt Desi ger Dei ce er ver Si e Low r esol ut i mage Pht ocapt et at ed I avi l abl e  mage t r ansf er compl et"
}