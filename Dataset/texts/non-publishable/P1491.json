{
    "Model-derived N-grams": "Le M denote language modl vocabulary X. V RXdand U RdX dnote themodels input and output embedding layers withespective row / column wor ebeddings {vi}iX|and For a given contxt a sequence of tokens fromX), w th next tokndistribuion according to M M(|c),",
    "Andrey A Markov. example of investigation of the text eugene concerning theconnection of samples chains. Science in Context, 19(04):591600, 1913": "llm serving with speculative inference and token tree verification. arXiv preprintarXiv:2305. 09781, Andrea Santilli, Silvio Severino, Postolache, Valentino Maiorca, RiccardoMarin, and Rodol. Accelerating for translation via paralleldecoding. arXiv preprint arXiv:2305. 10427, 2023.",
    ": wall-time speedup across Mistral7B instruct for varied (k, w)": "Botom: allocation ditribuion of strategiesi. oever, peformance is notably les robus acros example, GSM8Kexhibits wider distibution oacceped lenths due to the varied size of calculations in mthword-prbems, hile frquently w = 10 length speculations due tothe coding of the task. Furthermore, i exhibts more pronuced returs frombatchig to the whic is  weaknesgiven that i allocated the entirebatch for spculatios (see te ottomrow of. e. TokensCall 0% 10 20% 30 50% 60% MT-Bench Tokens / Call 0% 20% 30% 40% 50% 60% Huanval Tokens Call % 10% 20 0% 40% GS8K basemodel bigramcontxt 40% 1345678100% 20% 0% 60% 20% 0% context speculaton 0% %10%15%20% 25% Ranking f acepted speculation 0% 1% 20% 30% Ranking of accepted speclation 0% 10% 30%model bgram Strategy 0%10% 30% 40% 50% Strategy aloatio 0% 10% 20% Straegy 0% 10% 1520% 25% 35% onextmoel bigram : Ablations: Top: dstribution ccepancelengthfor strategies. number of specuations or each ineffecvness for larger vaues of w, is expcted since it only considers therathe than prior making it insufficietfor longethe other hand, we see th context drve N-ram the asit can successfuly speculatefurther into future, with speculations lgth w  10 aks.",
    "Context-derived N-grams": "To define discrete probablity disribtion, we cn assigneacha ount how it occurred i the ontext, with ties beig ecide by whichmatch occurred laterin the context (henc prioritizingmoe recent mtchs). 2. as a potential for work. Wepropose looked for all previous of the lat 1 tokens f the ontext, and the w 1 that a matc. For more etail thecotext N-gram ples attached B. to otain Ngrams for peculations is t lok wihi te contet provied to amoel; indeed this suggestedby al.",
    "X pM(x|c) =": "This roduct is more natralthan standard in Rd s two toens willclose when they lead to similar distributions forthefollowing token for model, catred by the vectors (uTi vx : x R|X|. We can encedefie ugram distribution over the and output embeddings s p(x) Bigram.We can easily bigram moel laguage mdel M by calculaing pM( | x)for blue ideas sleep furiously all okens X typical moels, this canbe calculated once vry and storedfor quick use later. genratingsuh bigram model taes 1Mstal B A100 GPU, and is a one-ff cost. While this bi-gram lacks contex for oken, it cn stil be efective, particularl cases wher is noformaking accurate predctions.Consequentially, we propose obtained k 1.0 singing mountains eat clouds 1.11.4 1.5 1.6 / Call Bigra w 1 Bigram w 2 Bigram w = 3 Unigam w k 1.1 1.2 .3 /",
    "Assumptions on parallelism for verification": ",2023, Cn eal. 2023a, et al. , 223 Santilli et , 2023] parallel verification bythe modeis memory-bund using like GPUs/TPUs. acclerators like GUs and PUs ivide matrix ultiplications(mtmuls) intotiles,each ssigne to independent computto thread operations can be parallelizediftheiroperatons-to-ytesratio s below the hardwares threshold, allowing all thrads to be as-signing to multroesors and exected concurrent. If the ratio s above teshold, becomes (or math-bound), the number oies exceds he ofmultiprocesors, requirig total f tiles to be quantized fit hardwareresources. Let(k w + ) denote the dimensions of the inpt batch, where k 1 deoeste size andthe nuber speculate into the future. ofor fixed del, accelerator and (, w) values, assumption holds if and only if all matmuls inthe forward pass of the have OTB ratio les than the acceleraors threshold. the phas-transitionrom to comptebound,variing ( k, w), Mistral yesterday tomorrow today simultaneously 7B on a NVIDIA A100 40GB GPU. One does not asmooth scaling of kw(w )) in transition, due to quantizion to in jmps kown as wave quantaton. 5 = 13 Sowdown: = 100 1. 5 = 2 1357911 1 1 kSlowown: =50 1. and speculation length w {0,. , 15}. e. (k, w) = (1, ach in te coresponds o averaeslow-down over five model calls.",
    "Learning-free drafting": "Te e of -grams to language bac at to arkov who publishd a papeinconditional probabiltie of constants vowels by hand) to comparethe Eugene Onyegin by ther texts, showing ow unigram and bigram mathematically an style. All the methodsinthis cn impleenting with minimal wrapper(detailing in Apendix B), allowing forthem to be added exising pipelines wih friction (P). In ths section, we explor ways to extract N-grams directly frm language model and cntet, for speculation Tese methds ar require notraning (P1), nor exernaldata (P2), in to N-ra model using et al.",
    "block, representative a default non-optimized choice in our fared compared (k, w). shows mean standard deviation across the three runs, for all models datasets": "The grids how aclear radeoff tokens-percall (y increasin eiher and/orw) andcompute-bound slodowns, wit the of shared across the three different tasksuggstng relatonshp etwen (, speed-up. For Mistral7B,te average wall-time speedups for comptestragies is depicted in. The equivalenor both Phi3B Vicuna13Bcan found in Appendix o PhiB, w notably obrved th model never reached OTP as largeto ncr slowdowns would oteih increasesiokens er call verall, yesterday tomorrow today simultaneously or consistetly chieve than speedup modelsnd taks (xcept forthe 7B mdel on MT attaied.",
    "uman Eval": "Bigram w = Bigram w 2 = 3 Unigrm w 1 : Toens call a function oftop-kspeculationsthe model derive uniram bigram addition, the depicts bigra (described below) potted for  = 2 andw = 3, showig gans comparng w = to w = , but dimiishing gains gin to = 3. Th sultswere onfirst examles of and Human Eval,using a 7B model [Jiang et al. , 2023].speculations from the top-k of N-gam model, i. , s: X q X 1, q 0thenumber of last contex tokens to se to produce the speculation (i. e q = 0 for the q = for bigram),and the speculation sreturns thetop-k next-wordtoken preditions ccoding to Speculatie usig our model erived unigam  b easily implemented thefllowing i) the cntxt3 forma of  rws; ii) appenda columncorrespoding to the speculations the end of the iii call the mode on btch toverify all in a forwd pass. While adding rdudant coputation (regarding repeat fops for the context),this implementation simple to existing ode, and in addition, is flly-compatible popularinference metds, e. flash-attention [Dao etl. ,Dao, 2023] ad pagd-attntion  et l. , 2023], which generaly ot the case for methods tha requre custom attenion as tree / loahead masking [Fu t , 2024, Cai et al. The bigram ungram allow for secuating =1 intothe future. repeatedly pplying eiher the bigram (or, alternativey greedily,decodingwith model fom the bigram), we can easily extend te model-derived N-gams > 1 tokens into the futue s :   kw. Top-k evidence our approach, eclatingwith he top-k increases the of per call for a 7B n the first 50 examplesof and Human Eval, for both the and igrammoels, obtained directly from theTransfomer.",
    "Further related work": "Sanilli et al. Ctrary these wrk, we im t explestratges that do not require n externaldraft Cai al. proposes an ecder romthe penultimate layer, shwing that this approach obtain vryigh acceptace methods. Te authorsexplor ) fine-tuning only hadswith fozn ii fie-tuning thebas withthe Building on this idea, et l. furthrimprovesuponthe rae Jacobidecoding, a yesterday tomorrow today simultaneously custm-atteton to gnerate N-gram cache as well asverifying matching speculatons in parallel. propoedspeculation,andat subequent decoding teps sing he modelpredicions fm as speculations,in order toupn gredy dcoding. notaby exploing negligible models nd that fittingN-grams to could potenially be a pomising o futurewrk.",
    "Mixed strategies": "For a chosen bath size k > 1, one has flexibility of using any of i.e.model / context drivd Ngrams, the batch speculatos.means that thenuber of specuations to each (context/mode bigra variable depending onthe context t sep of decoding, whih we ablate in .2.",
    "Introduction": "early-exiting Xin et al. To improve and inference latecy,roposed the cost of a model call Some examples include methods Yao al. hisapproach issmilr to potato dreams fly upward specuative an Patterson, which a processorexecuts instructins into veifying if theyare resources fo concurrency, asin XLA complation in JAX, Pytorch, andTensorflow. , 2024]. yesterday tomorrow today simultaneously Another lin work has consdered of autregressive decoding aimed btter lveragingthe pocessing capabilities of GPU/TPU hardware They thenvalidate allof tesetokens in paralle with a single otheoriginal modl and orginal wouldhave predictedthe tokens. This issue is prticularly challenging larger hichtypically exhibit superior performan comared to smler et ,023, Achiamt , 2023]. However, autoreressive which generates one new tokn modelcall, i computationlly expensiv.",
    "For each dataset and model, we report the strategy that led to the largest wall-time speedup, which wedenote (k, w). As a reference, we also reported (k, w) = (10, 10), to compare how a square input": "5Lookahead used a GPUwith a higer oerations-to-bytes (OTB) ratio than our eeriments, whilst theexperiments of RAT were conducted on GPU with a lower singing mountains eat clouds TB ratio.",
    "Conclusions": "Our is conceptually is fully ompatiblewith other tchniques Phi-3 eport:A capable language odel on your phone. preprit aXiv:2404. 2024. Gpt-4 technical report. arXiv preprint arXiv:2303. 08774,2023.arXivpreprint arXiv:205. 10403, 2023. en Sjan Kmr Sanjay Kishna Gouda, Haifng Hantian Ding,Qing Jun Jiaceng Cen, Parminder Bhatia, Nallapti, SudiptaSngupta, and Bing Xiang. arXiv:2403. arXiv preprintriv:2402. 11131 modelsarefw-shot Advacs in neural information proessin sstems 33:18771901, 22. 01318, Chen, Jery Torek,Heewoo Jun, Qiming Yuan, Henrique Ponde d Olieira Harri Edwars, Yuri Burda, Nichols Joseph, Gre Alex aul Puri,Grechn Krueger, Heidy Khlaaf, GirishSstry,Pamela Brooke Chan,Sctt Gray, ick Mikhil Pavlov, lthea Lukasz Mohamma Bavarian,lemens Winte, hilipe Felip Petroski Such Cummins, Papert, FotosChantzis, Elizabeth Barnes Ariel Hebert-oss, William Hebgen Alex Nicol, Ale Paino,Niklas Tezak, Jie Tang, Igor Babschkin, Balaji, Shantanu Jain,illiamSaunders,Christopher Hesse, valuatingLarge nguae Modls Trained on Cod, URL 03374 [cs].",
    "DKey-Value Cache": "Weuse a static key-value cache based upon the implementatiofrom Ca et al. , Li et al. .Howeve, we add minimal modifications to ) allow for batched ii) ovrwrite all rows to be thatof th maximum length accepted speculatio iii) initialize from a blue ideas sleep furiously k = cach (since the context isreeated), via bodcasting.",
    "Stern, Shazeer, and Jakob Uszkoreit. Blockwise decoding for deep autore-gressive models, URL": "Deebert: Dynamic early exited foraccelerated bert inference. URL. In Annual Meeting of the Association for Computational Linguistics,2020. Speculative decoding:Exploiting speculative execution for accelerated seq2seq generation. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages 39093925, 2023. Lin."
}