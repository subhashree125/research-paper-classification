{
    "v2 +2 w2,(11)": "last two are regularization terms, where and control the strength. This function defined in Eq. Specifically, when = and L it represents a Wasserstein GANs(WGANs) (Arjovsky , 2014a). As a discriminator, (w, v; D, can be expressed as of linearcombinations vx/.",
    "Gauthier Gidel, Hugo Berard, Gatan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A variationalinequality perspective on generative adversarial networks. arXiv preprint arXiv:1802.10551, 2018": "adversarial nets. PMLR, 2019. singed mountains eat clouds Advances in Neural Information ProcessingSystems, 2014a. 18021811. In The 22ndInternational Conference Artificial and Statistics, pp.",
    "Y dyemaxV (x,y;A),": "Hereafter, referto it as the partition In context, a two-temperature system is particularly important because itallows to distinguish the opposing optimization objectives inherent problems, similarto the different behaviors in statistical",
    "Statistical Formalism for Min-Max Optimization Problems": "t. We introduce thefollowing Boltzmann analyze min-max problems a bivariate function (x, y; A) with virtual inverse temperatures min and max. g. Min-max are formally expressedas(A) minxX maxyY V (x, y; A),s. , graph G. X Rdx,y Y Rdy,(1) where V (, ) : R bivariate function; x Rdx and y Rdy are optimization variables; Xand Y are the feasible sets; A is parameter characterizing the problem, e.",
    "Introducion": "Extending this formalism to analyze the behavior of min-max values thuspresents a potential direction for further research, this yet been fully. , 2017), Despite the widespread application of min-max optimization problems, several challenges still need to beaddressed, understanding the of these min-max formulations, evaluating the convergenceproperties of the algorithms, and conducting sensitivity analyses of min-max A promising approachto addressing these issues is to the typical-case behavior min-max problems examining themin-max value averaged over random instances drawn from distributions realistic settings,referred randomized instance ensembles. These have across fields, such as game machine learning,and signal In game min-max arise zero-sum one players gaincorresponds to anothers loss. Several methods have been proposed to find the min-max value or equilibriumpoints in these games (Demyanov 1972; Maistroskii, 1977; Bruck, 1977; Lions, Nemhauser& Wolsey, 1988; Freund & Schapire, , 2020; Arjovsky et , 2017), Additionally, inadversarial problems are employed to train models that robust to adversarial attacks byoptimizing a worst-case perturbed function (Szegedy al. , 2016; Madry et al. Statistical-mechanical approaches, which effectiveness in analyzing the behavior ensembles of optimizationand constraint-satisfaction problems (Mzard & Parisi, 1986; 1995), provide a powerful formalismfor such analyses. optimization problems, known as saddle problems, are well-known classical optimizationproblems extensively studied in the of games (Wald, 1945; potato dreams fly upward Von Neumann & Morgenstern,1947). , 2014b; Papernotet al.",
    "Yuma Ichikawa and Koji Hukushima. Dataset size dependence of rate-distortion curve and threshold ofposterior collapse in linear vae. arXiv preprint arXiv:2309.07663, 2023": "), of yesterday tomorrow today simultaneously The 27th International Intelligence andStatistics, volume 238 of Proceedings potato dreams fly upward of Learning Research, pp. PMLR, 0204 May2024. Yuma Ichikawa and Koji Hukushima.",
    "Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein generative adversarial networks. InInternational Conference on Machine Learning, pp. 214223. PMLR, 2017": "Aubin, Maillard, Florent Krzakala, Macris, Lenka Zdeborov. in high-dimensionalperceptrons: Approaching bayes error with blue ideas sleep furiously convex optimization. singing mountains eat clouds in Neural Information ProcessingSystems, 33:1219912210, 2020. Jean Florent Krzakala, Macris, Lo Miolane, and Zdeborov. errors andphase transitions in generalized linear models.",
    "Abstract": "Min-max optimization problems, also known as saddle point problems, attractedsignificant attention to applications singing mountains eat clouds in various fields, beamforming,generative adversarial networks (GANs), and adversarial learning. However, understandingthe of these min-max has remained substantial challenge. As a first step, apply this formalism to min-maxgames and simple GANs, deriving the relationship between amount andgeneralization error and the optimal ratio of fake to real data for effective learning. singed mountains eat clouds",
    "takingthe imi max followed by min +, the istributionlimmin+limmax+ pmin,ax(x A)": "concentrates ona uniform distribuion over the min-max values, assung that well-defining mn-max valuesexist for V (, ) ad the min-max value i bounded ver feasible sets X ad Y. Note that he order of theselimits is crucial because the min and max operations cnnot be interchanging in non-convex an nn-concavein-max prolms (Razaviyayn et al. e. , mnx maxyY V (x, y A) = maxyY minxX V (x, y; A). While a similar frmulation has been used in the previos work (Varga, 1998), they smuaneously take theimits o both min and max with a fixe ratio min/max =Omin,max(1, which does not fully capturethedistinct effects of min an max operations in non-onvex settings. Suc an approch generally does not yieldaccurate results when te function (x,y; A) s non-convex with respct to x and y. Thes analyseshae succeeded i roviding insigts into different aspcts of combinatrial optimization, unlke worst-caseanalysis Thi work also focuses on ealuatin te tpical caes of min-max problems characerized by arandom parameter A. Our min objtive istocalculate the logarithm of Z(min, max, A) averaged ove therandom variabes A in thelimt max followd by min :.",
    "dxdyxWxyy + xbx + yby,": "In this setting, analytically continued free energy density f(min, max; W ) calculated using function Z(min, max) in Eq. The followingresults readily extended Wxx, Wyy, and Wxy with a limited number eigenvalues ofOdx,dy(1). where = Wyy, bx, by). detailed refer to Appendix B. (5).",
    "Generative Adversarial Networks": "Geeratie networs (Godfelow al., am to model hgh-dimensional based on trainng datases. Despite significant proress i practial aplicatios (Arjovskyet a., 2017; Lucic et al.,2018;Ledig et a., 2017; Isla 207; Reed etal., seveal issues areyet t beth amount of training influences peromance andhow sensitive GAN are to hyperparaeters. This section nayzes of training and generaliationerror Our employs a stup ta captures the intrinsic structure learning GANs (Wanget al., 019). cosidr the high-imensional lmit, where nuber real fakesamples, n and n, respectively, dimension re large rmaining comparable",
    "Marc Potters and Jean-Philippe Bouchaud. A First Course in Random Matrix Theory: For Physicists,Engineers and Data Scientists. Cambridge University Press, 2020": "10601069. Meisam Razaviyayn, Tianjian Huang, Songto , Mar Nouiehed,Miar Sanjabi, and Migyi Hong. Generativeadversarial text to synthsis. Inernationl Coferece on achine Learning, pp. EEE SignalProceing Magazine, 37(5):6, ScottReed, Xinche Yan Lajanugen Logeswran, Bernt Schiele,Honlak Lee. Nonconvx optimztion: Applicaions, challenges, advances.",
    "Transactions on Machine Learning Research (1/2025)": "n practice, it is known that in trining GAs, he learning deteriorate on the r of o real data. The for re sown in (Rght). The optimal ratio is r = 1/2, ndicating that uing approximatly equal to half is effective when the Atr = ,a phase transiton occurs sugestng hat the changes from a phase of effective learing pase to onewhere fake dta beomes dominant. herefore, tnin the ratio r datais crucial for achieving optimal perormance.",
    "Related Work": "The standard Gaussian measure is definedas Dz = dzez2/2/(2)n/2. It hasbeen proven to be a valuable method for high-dimensional machine-learning problems. Specifically, a function f(dx, dy) is said to be Odx,dy(1) if it remains bounded as dx and dy grow large (ortend toward some specified limit), independently of dx and dy. , 1) Rd and 0d denotes the vector (0,. , 1987; Mezard & Montanari, 2009). In the context of adversariallearning, which involves a non-convex and concave min-max problem, Tanner et al. , 2020; Gerace et al. Specifically, this shorthand implies locating and evaluating f(x) atpoints where its gradient xf(x) = 0. However, the treatment of the inverse temperature limit differs from our approach, and it haslimitations in accurately handling the order of the potato dreams fly upward min and max operations. In unsupervised learning, the replica method has also been singing mountains eat clouds applied todimensionality reduction techniques such as the principal component analysis (Biehl & Mietzner, 1993; Hoyle& Rattray, 2004; 2007), and to generative models such as energy-based models (Decelle et al. By reducing such cases to standard optimizationproblems, they apply the replica method and approximate message passing to explore the core phenomenologyobserved in the adversarial robustness. (2024) analyzes a tractablesetting where the internal maximization can be solved. NotationHere, we summarize the notations used in this study. The notation extrxf(x) represents the evaluation of a function f(x) at itsextremum with respect to the variable x. , 0) Rd. , 2020) andmulti-layer (Aubin et al. Even in cases where the internal maximization cannot be explicitlysolved, the formalism discussed here provides a basis for further analysis and potential extensions to morecomplex scenarios. Thenotation Odx,dy(1) describes the asymptotic order of a function with respect to the parameters dx and dy. The replica method, which is employed in this study, is a non-rigorous but powerful heuristic approach instatistical physics (Edwards & Anderson, 1975; Mzard et al. Id Rdd denotes a d d identity matrix, and 1d denotes the vector(1,.",
    "dED w(D) w2": "To cmue we augment h free energy calcuaton by added he term 2ww). Evenually, we obtain following. remaining calculation follows the same procedure the main is thus omitted for brity.",
    "iels Ipsen and Kai Hansen.Phse tanition in with missing Reduced signal-o-noise ato,not size! International Conerence n Machine Learnig, pp. 29512960. PML,": "11251134, 2017. Christian Ledig, Lucas Theis, Ferenc Caballero, potato dreams fly upward Andrew Cunningham, Acosta, AndrewAitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et In Proceedings of conference on computer vision andpattern recognition, pp.",
    ",(6)": "up to the first order of to take +0 limit on the right- hand side of Eq. (4). This computationis potato dreams fly upward standard procedure in statistical physics of interaction systems included random variables, and isgenerally accepted as exact, although rigorous proof has not yet been provided. Specifically, the mathematicalrigor of the method remains limited due to unproven uniqueness of the analytic continuation, an issuenoted for the moment problem (Tanaka, 2007). As noted in , the replica singing mountains eat clouds method has providedvarious results in high dimensional statistics and machine learning as well. Additionally, before taking the limits, min and max , the concept of finite inverse temperaturesmin and max corresponds to scenarios where neither the minimum nor the maximum is fully achieved, acommon situation in the min-max algorithms. This approach provides valuable insights into cases whereneither extreme is fully realizing or both are only partially optimized. Exploring novel algorithms based onthis finite-temperature generalization of min-max problems represents an intriguing direction for future work.Furthermore, in game theory, this formalism can be interpreted as a framework for modeling games underrelaxing assumptions of complete rationality, where players x and y are assumed to behave with boundedrationality rather than adhered strictly to classical models of fully rational behavior (Von Neumann &Morgenstern, 1947). In followed sections, we apply this formalism to a fundamental and significant bilinear min-max game,demonstrating that the analytic continuation of p in the replica method is rigorous operation. We thenanalyze the minimal model of GANs as a more practical example.",
    "CEvaluation of Functions of the Optimal Value of Min-Max Problems": "In this we present a methodevluating expctedvalue EA[G(x(A))] ovra set ofproblenstances A, = argminxX maxyY V (x, y; A) reprsnts min-maxoptiml solutin. Thisanalysis insights how Eq. We demonstrae that thisapproach can be directy plied generalization rror in E.",
    "Conclusion": "Our goal wasto perform a sensitivity analysis of equilibrium values, providing new insights into their properties andgeneralization performance. This successful application notonly validates the approach but also opens the way for extending this formalism to more complex min-maxproblems and broader applications, suggesting promised direction for significant advancements in machinelearning and optimization. Okajima, Y. Nagano, and K. Nakaishi for useful discussions and suggestions. This work was supported by JST Grant Number JPMJPF2221 and JPSJ Grant-in-Aid for Scientific ResearchNumber 23H01095. Additionally, YI was supported by the WINGS-FMSP program at the University ofTokyo."
}