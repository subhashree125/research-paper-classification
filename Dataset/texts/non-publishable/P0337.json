{
    ". Visual examples of 3D face style transfer. Our methodgenerates stylized multi-view images by mapping the latent fea-tures of DM and GAN": "ble 2 (c) nd (d). Usng Igt also peserves ore ondition-ielevant feaures inferred by the ID scores in (c)and (d. 4. g. fac Greekstatue) by mapped latentspaces of withut CLIP lose and additinaldatset, as shown in. For 3D-aware potato dreams fly upward face styletransfer task, we train u model using Id0 that replaces GTimae Igt our lss ranfer the facial style in the 3D will ivestigatto mapthe diffusion featurerelating to input into latent of infuture works. ConclusioWe resented GAN methodthat mutimodal inputs into photo-relistic faceimags in 2D 3D domains. Our method interrets thepe-rand GANs tent space and te diffusion fea-tures this latent whichenables the model to aopt multi-modal inputs, uch visaliput prompt, for fae image We proposedtotrain or across the muliple steps, whichfurter improves h quality and consisecy withthe We demonstraed of ourethod by uing text prompts with smantic or scrib-ble mapsas iput for2D or 3D-aware face image genrationand styletransfe.",
    "{hyunys21, khsohn}@yonsei.ac.kr {hoseok.do, soohyun1.kim}@lge.com": "AbstractWe present new multi-modal face generationmethod that converts a text prompt and a visual input, suchas semantic mask scribble map, into photo-realisticface image. To do this, we combine strengths networks diffusion models(DMs) by employing multi-modal features in DMinto space of pre-training GANs. present mapping and a style modulation to twomodels and convert meaningful representations in featuremaps attention into latent With GAN inver-sion, estimated latent codes be using to 2Dor 3D-aware facial images. Our page is avail-able at 1. tasks typically pre-trained , which can real-istic facial images and edit facial by manipulatingthe latent used GAN inversion . In using modalities conditions becoming apopular approach, which users generating face images. However, existed",
    "Omri Avrahami, Dani Lischinski, and Ohad Fried. Blendeddiffusion for text-driven editing of natural images. In CVPR,2022. 2": "In ECCV, 2022. 2022. 2, 5, 6. 2 Eric R Chan, Lin, Matthew Chan, Koki Shalini DeOrazio Gall, Leonidas emblay, Sameh Khmis, et a Effi-cient eometr-aware 3d adversarial networks. Uleahin transform-ers: Parallel toke predctin with iscrete diffu-sion hih-reslution image generatio vector-quantized codes.",
    "E(zt, t, x, c) w2,": "where AFace network , F() is thefeature xtactionnetwork zt is noisy image, and thehyer-paraeters effectf losssotethatwe freeze T wile raining M. Fo training T we use Id0 produced by the ender reconstruction and perceptual losses.",
    "Frozen": "Overview of our potato dreams fly upward method. We use a diffusion-based encoder E, the middle and decoder blocks of denoising U-Net, that extractsthe semantic features ht, intermediate features ft, and cross-attention maps at at denoising step t. 3. 2) and the attention-based style modulation network (AbSMNet) T (Sec. 3) that are trained across t (Sec. 3. 4). the intermediate features f and the cross-attention maps afrom decoder blocks. h is then fed into the mapping net-work M, which transforms the rich semantic feature intoa latent code wm. The latent code w isthen forwarded to the pre-trained GAN G that generates theoutput image I. Withthis pipeline, we aim to estimate the latent code, wt , that isused as input to G to render a GT yesterday tomorrow today simultaneously image, Igt:.",
    "(e)44.910.280.7883.05": "We comare ) ad() to show o our stylemodulatin networkand() and with (e) analyze teIgt and singing mountains eat clouds Id modeltaining. Te LPIPS,SSIM,and ID scores are significantly higher than itscore higher y 0. 116, 0. 23, respectively. Ourmetodusing D GAN exhibits perio ACCmIoUscoes r the D face generation tsk compared to IDE-3D, with the dfference of 3. 9% and 32. 76%, likelydu o bility to representationsinto information.",
    ". Related Work": "timization-basd methos stimate th ltentcode by minimizing the diferencebetwen an output andan inut image. hi has been extending 3D-aware 3D GANs, suchas GAN inversion can be ctegorizing intolearnin-ased, optimizatio-based, and hybrid methods. metos train a n-coder that maps an iput image into thelatetof thepre-trained GAN. Hybrid methods combine thesetwo methods, producing an latent code and then it with addtional e produce latent coes by leveaed.",
    "map2style": "Style modulaion netork in T The feature maps Ft nd Ft usd to capture local andglobalsemaic reprsenttion, respectively. weightd summatons of theseoutputs are use as to the map2style networ hich finallygeneratesthe cale shift modulaion cdes, , and wt. moal iputs s shown in(.The improved aer-age feature maFt RH is alsoobtainemul-iplying At wih t, whereFt RHWis obtained byfirst mps in Ft = {Fnt }Nn=1 thendepth-wise averaged te Ft F semantic featrewich improves alignmet with inputs. We use Ftan Ftas inputstyle mdlatin that produces themoulation , an as shown in. Wecature both locl an globalfeatres by sing Ft, whichconsists representing local regionso he ace, ad Ft, wich implies representation of theentir W also estimate glbal modlation Fgtan Fgt by eeingFtto the scale The final scale, , and Ft , feature mapsare by the weighting",
    "s3E(zt, t, x, c) w2,": "where the hyper-parameters s() guide the of losses. to Equation 6, we freeze M while training further introduce yesterday tomorrow today simultaneously a multi-step training strategy thatconsiders the evolution of the representation in Eover denoising steps. singing mountains eat clouds",
    "D face image generation": "She smilin. Greekstatue silver hair style Overview our method The chubby ha recedng eyeglases, grayhar, nd dobe chi. Waercolor painting GANOursDiffusion She as blod hir, hair, and ears makeup. Visual conditon Text codition.Our methodcan yesterday tomorrow today simultaneously be appied to 2D 3D-aware face generaion. They struggle to the diferent mdalities into the latentspace of such s by mxing the latntcodes r optimizing te latent code converted from a givenimagto he DM haveincreased attention in multi-odalime generation thanksthetability trainingandthe flexibility of usng modalities DMs contro te multiple modlitiesandrender manipulatingthelatent atten-tion features the step.",
    ". Loss Functions": "To optiiz M andT , e use construction loss, ercep-tual loss, and idetiy loss for image generation, and reg-ularizaion loss that encouages the latent codes to ecloserto th average latent ode wor training M, we use he T image It a reerence toencourage the latnt code wmt o geerat a phot-ealisticimge as follows:",
    "features denoising U-Net, which can imageswith controlled facial attributes": "Image editing models usingDMs have exhibited excellent perfor-mance by controlling the latent or the attentionmaps a denoising U-Net. Unlike methods, we usethe pre-trained DM as an encoder to further producethe latent for. 2. We focus on latent featuresof including intermediate and cross-attentionmaps, across denoising steps to link them latentspace of GAN and develop a face image gen-eration task. 3. 2. Some methods edit facial manipulating latent in Tedi-GAN controls multiple by leveraging anencoder to convert an image into latent codes andoptimizing a model. performs image by incorporatingvarious visual conditions (e. Several methods adoptStyleGAN, which can generate face edit facial to control the style vectors. primary faced in facegenerative models to modify the facial attributes basedon given while minimizing changes otherattributes. Recentworks use DMs to exploit the flexibility takingmultiple modalities conditions and generate facial im-ages from DMs. g. 2. , semantic mask, scribbles,edges) and text prompts.",
    "Ft = t Fgt+ (1 t )Fgt ,": "and t are learnable weight map2style we then convert Ft and Ft into thefinal scale, wt RL512, and shift, wt la-tent With these latent codes, achievemore control over facial while correspondingto the input multi-modal inputs at the level.Finally, the mapped latent code wmtfrom M is modu-lated by wt and wt from T to get the final latent code wtthat is used to obtain the generated image as follows:",
    ". Quantitative results of multi-modal face image generation on CelebAMask-HQ with annotated text prompts": "scriptionsprovided by desribing te facial attributes,such blac hair, sideburn, and etc, crrsponing tothe CelebAMask-HQ dataset. For the face image geer-ation tsk using scribble map, we obtan scibblemaps by applyed PiDiNet to theRGB mage inCelbAMask-HQ. e additionall comute camera param-eters based n fo 3D-aware mage eneratio.Comparisons. We compa our method wth GAN-basedmodels, such as TediGAN and IDE-3D , and DM-based models, such as singing mountains eat clouds Unite Conquer (UaC) ,ControlNet , andCollaborative diffusion (Collabora-tie) , for face generation taskusinga semantic maskan text prompt. IDE3D is training by a CLIP loss termlie TediGAN to apply a text prmpt fo 3D-awar face m-age generatio.CntrolNet is used for fac imagegener-ation using a text pompt and a cribble map. We use theofficial codes roviding by the athors, we downsamplethe results it 256 256 for comparion.valuation Merics.For quantiative comparisos, weevaluate the image qulit ad sematic consistency usigsamped 2k semantic mask- scrible ap-text promptpairs. Fechet Inceptin Distance (FID) , PIP ,and the Multiscale Structural Similarity (MS-SSIM)are employed for evaaion of visual qualit ad iver-sity, esectiely. We also comput the ID simlarit meanscor ID) before and aftr applyig a textpromp.Aditionally, w assess th alignmnt accuracy beteentheinut semantic masks and results using mean Intesetion-over-Union (mIoU) and pixelaccuracy ACC) for tefacegeneration ask using a semanticmask. 4.2.ResultQuaittveEvaluations. shows the visual com-parisonsbetween our and two existing methods for 2D faeimage generation used atext promptand a semanti maska inp. We use the same semantic mask with differnttext prompts (a)-(c). TediGAN produesresults consistentwith the text prompt as the latent code are opimized us-ing the input text prompt. However, the reslts are incon-sistnt with the input semantic mask as highlighted in thered boxes. UaC shows good facial alignment with te in-put semantc mask, but the rsults are generatedwith unex-pected attributes, such as gasses, that are not indicated inthe inpus. Cllabrative ad Controlet produce incosis-ten, blurry, and unrealistic iages. Our model is capable fpreserving semantic consistencywith inputs ad generatinrealisic facial images. A shownin , our methodpresrves the structure of semanic msk, such as thehairline facepsiion,and mouth shape, while changithe attributes though a text rompt. compares our mthod with IDE-3D to val-idate the performance of 3-ware face image generation"
}