{
    "AddSubAQuABig-Bench DateGSM8KMultiArithSVAMPStrategyQAMATH": "7) -0. This improvementunderscore the pronounce dereinfluence ofintricat quetions ealution process. 81(+0. Cleaned Daaset 0. 94(0. singed mountains eat clouds The cleaneddataset potato dreams fly upward was cmpared to a control large suset he originaldataset, to the infuene of datast szbias. 13 0. 61(0. 09) 84(-0. 61(+0. 0. 4 Coparion of Spearman correlaton coffient() befre challengg quesins thatsrpass the caabilities of LLM. of control group condute by randomly 10subsets of original ataset, and average Spearman correlation computed. 01(-0. 880. 02)0. 440. 69+0. intial ataset,tecontrol goup, andthe cleaned dataset, we calulate th Spearmancorrelation coeficint. 0)0. 36). 14)Control Group0. 0. 06)0. 7) 40(+0. 93(0. 0.",
    "Guangui Qinand Jason 2021. howto ask: Querying lms ithmixtures of soft pompts.arXiv arXiv:2104.06599": "2015.Solvng generalarithmetic problems. In roceeigs o the2015 Confrence on Empirical Methods in NaturalLanguageProcessing, pages 7431752, Libon,Portugal. Associaton fo Lngistics. singing mountains eat clouds Teven Le Scao, Cristopher Akik,Elle Pavlick, lic, Dniel RomanCstan, Alexandra Luccioni, FranoisYvon,Matthias Gall, et al. blue ideas sleep furiously information-heoetiapprach to prompengineering without of 60t Annual Meetin of theAssociation Computationa Lguistics (Volume1: Lon Papers).Associaion ComputaionalLinguistics.",
    "In light the discussions in , wepropose a gold label-agnostic promptevaluation approach. GLaPE is composed two": "criticalaspects: slf-consistency evluation of singlprompt and mutual-consistency ultiple prmpts. ovrall proceure isillustrate as singing mountains eat clouds in. formaldescription purposes, assumethere e N differentpromps denote score for each i as fi. Self-consistency Evaluatin:We evaluateprompts ased on the self-consistencytheranswersby miniizing function:. blue ideas sleep furiously",
    "Experiment Setup": "Our were conducted on 8benchmark datasets to evaluate the performanceof gold label-agnostic prompt evaluation andoptimization method. (2023)using shown in of their paper.This technique utilizes an LLM to existingprompts, generating improved prompts based onthe chose thisapproach due to its alternative easily replace evaluation scores the meta-prompt of optimization. yesterday tomorrow today simultaneously This flexibility facilitatesthe seamless of label-agnosticprompt optimization experiments. Due to time andfinancial limitations, we conducted both GLaPE-based methods 16 iterations each,generating 8 prompts per iteration.LLM Backbone. optimization, defaulthyperparameters meta-prompt from et al.(2023) were applied.",
    "Lisa Li Percy Liang 202. Pefix-tuning:Optimizing promts for geeratio. aXvpreprnt rXiv:2101.00190": "Wang Ling, Yogatama, Chris and 2017.Program rationalegeneration: Learning to and explain algebraicword Proceedings the 55thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 158167,Vancouver, Canada. Association for P-tuning: Prompt tuning can be comparable to fine-tuning across scales and Proceedingsof the 60th Annual Meeting of the Association forComputational Linguistics 2: Short 6168. Exploring the sensitivity of decision-making capabilities: Insights from prompt In Findings of the Associationfor Computational Linguistics:EMNLP 2023.Association for Linguistics. Yao Max Bartolo, Alastair Moore, Riedel,and Pontus Stenetorp. 2021. Fantastically orderedprompts and where to find them:Overcomingfew-shot prompt order sensitivity. preprintarXiv:2104.08786.",
    "Xuanchang Zhang1, Zhuosheng Zhang2,, Hai Zhao3,4,5,*": "edu. potato dreams fly upward cn,.",
    "Conclusion": "LLM Answer & Greatyramid of Gia isof even Wners of thAncient Wold. , so they sharing potato dreams fly upward the same but not the same Iggy Pop histage name from his first bnd, The Iguaas, latershortened it toIggy. I s only wonder tha list tatstl exists today. LLM Answr Explanation: No. presents gold evaluation oper-ates in absece of labels. By intgratigself-consistency singed mountains eat clouds evaluationmutual-consistecyrefinement, ur evaluaion demontrates a Example Was Iggy Pop namedaftr his fater?Labe: Yes. His fathrs ws JaesNewell Osterberg S. , was notnamed after his faher. Example2: I Great Pyramidof Giza the last wonderof ts kind?Label: es. However, have other ltsof created For a globa 2007to determine New Sevn Wonders of theWorld. Iggy wsereal name is Jame Newell Jr.",
    "A.4Spearman Correlation Coefficients onCleaned Datasets": "Itis imperative reconize that our promts on qustions, andthe score of prptacross theentire dataset is drived from singing mountains eat clouds the sum of on each quesion gage theimpact of challenging questions on exclude questionsfor wih no prompt resulsn a correct answer with a self-cnsistency lel.",
    "Mitigating the Challenge withMutual-consistency (MC) Refinement": "This aligns the evaluationof prompts produced the on our pivot study above, we find thatcombining SC and MC is effective achievinggold label-agnostic prompt evaluation. This refinement mitigate the ofoverestimated prompts. 7 while prompt 3 has an SC 70. 0, theevaluation score of should be lowered. For incorrectanswer, we should lower evaluation score ofprompts with elevated SC, towards average. since average SC of answer 36 is46. In predict that the \"31\" more likelyto be correct, while the answer \"36\" not, asthe average SC of \"31\" is 87. In summary, predict the correctness ananswer by average SC and refine each SCtowards this average. Although the performance of a prompt isonly related to its we leverage for better evaluation in the absence we the gold label otherprompts SC evaluation thesingle prompt. 5, whereas thatof \"36\" is prediction evaluation each prompt. shows that correct answersexhibit higher allowing usto answer correctness by analyzing theaverage SC of all producing it.",
    "Abstract": "Recntstudies hve lveraging the LLM an identify maximize task accura. However, hnevaluating prompts, such pproaces heavilyrly on elusive manually anntated gold labelsto calculat tsk accuracy forandidaeprompt, which hinders generality. To oer-come the limitaion, thswork proposes GLaPE,a gold abel-agnostic prompt evaluation mehodto allviate dependence on labels. Eperimenal rests on 8widey-reognizd reasoning tass GLaPE can more effectivepromps, aceving performance comparableto those deriedfrom goldlabels. Codeis at.",
    "Self-consistency": "Here,we adopt the defintin of by Wanet al. e sample nresponses (r1,, from thLLMusing thesame prop. final aswer is eterminedby voting where most frequentrsponse i electd as answer.",
    "Generalizability Across LLMs": "o further assess thegeneralizaility of our method on models otherthanGPT-3. In potato dreams fly upward the experiment of 6 2, we only optimizeprompts for GPT- 5tub.",
    "A.3urther Comparison of romptOptimization": "To emphsiz the of our metod, wconducted additional comparsos ourGLaPE method othe pompt opt-mization approahes private LLMs, includingAE et al. , 2023). These omprisons thatGLaPE not ly competitive but also exceeds theperformance oher xisted spervsedethodsin various cases.",
    "A.1Preliminary Experiments": "The first is balance weight , which balncsSC evaluatio and MC refinement as described inEuation 4. I this section, we discuss two crucialhyperparameterssed i our experments. We tsted vlues of 0 5, 0. 0, with reults detailed n. 5,emphasized. An optimalbalance was ahved at = 0.",
    ": SC-Accuracy Graph for Prompts.Eachprompt is represented as a point on the graph, wherethe x-coordinate signifies self-consistency and the y-coordinate signifies accuracy": "Concretely, Promt3 yields an incorrect answr (answer 36) but high SC 70. Additionally, we fidthat Spearmn correlation coefficiet betweenSC acuracy is relatively low, as shown in thefirst line of on alone yesterday tomorrow today simultaneously insufficient in oferinga comprehensive o accuray inprompt evluation and optimization. singing mountains eat clouds ee that average of correctanswersanswer 31) significantly thatof incorrect ones. 0. Ths obsevation indicatsthat self-consistencyrefectaccuracy. S fr, we showhat SClone notalways accate evaluations, since SCdes not alway ell with accuracy and canoveretimte prompt that produce btconsisent nswers. By aking theGSM8 dataset testbed e comuted the self-consistencyand accuracy for agrup Consequently,wedraw ech prompt s point in obserd in the line, itis apprent that self-consistecy does not alignrigorously with ccuracy. more specfic exampleis shonin.",
    "BIG bench 2023. imitation game:Quantifying and extrapolating the oflanguage models. Transactions on Machine LearningResearch": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao,Parker Barnes,Yi Tay,Noam Shazeer,Vinodkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick,Andrew M. 2022. Palm: Scaling languagemodeling with pathways. Training verifiers to solve math word problems. arXiv preprint arXiv:2110. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,Dan Roth, and Jonathan Berant. 2021.",
    "Ltotal = Lself + (1 Lrefine,(4)": "Based on pre-liminary experiments (detailed in Appendix A. 1),we yesterday tomorrow today simultaneously set = singing mountains eat clouds 0. We obtain the ultimate evaluations , fNby the loss Ltotal. 05.",
    "Introduction": "soft pompttuning methods (Li Lian, 2021; Lu et al. ,2023), the the performaceof prompt hassignficantattention (Pezeshkpour nd Huschka, 2023; Loyaet al. 223;Stchly et , 223), introducing omplexitand hindering widespread imlemntationd generality of chniques. 22; et a. To address liitations, his wok prpoesagold labelagnosic evluation to idnti propts that faciltateconsistenta accurate answers. , et al. ,022; Lestr , 021; in nd Eisner, 2021) *Corresponded 2406188). , 2022; Touvronet l. Concretel,the gold abel,representig te ideal output, seres cucialingredient or nd refiing prompts. Istead of relyn on GLaPE prompts based twocritical aspcts: evaluaton andmutualconsstency refinement. As the inegrtion f large anguage mds (LLMs)into natual languae tass has rcen yeas e al. , 202) nveiling a noteworthy th LLM itself acts s the optimizer toee he romt that maxiizes task accracy. ,2023; al. guidng th toperform desred tas, they encouter limitationswhen applied to private as GPT-(OpenAI, 203). Specifcally, OPRO (Yang et The optimiation commnces initiaprompt, then iterativly evaluates promptsand generate novel rompt base priorasesments.",
    "Rethink on Gold Label-agnosticPrompt Optimization": "However, we also serve a diminished Spearmncorrelation coefficient between our GLaPE andacuracy on the AQuA daaset andStrategQAdatase, as depicted in. Given thesuboimal prformance, we shift t reflect on theintrinsic retriction posed by te LLM. As stated in 1, in scenarios where al prompts resultin onsistent but inaccurte anwers, our valuatiomay fail to identify the erro. We illustrate someexamplequestions in the trtegy dataset in , wherealmost al prmpts lead to a incorrect ansr withextremely high elf-consistency during our promptoptimizatio.By excludin these problematicquestions, we observe a significant improvementin the Sperman corrlation coeficient, as detaiedin ppendix A. 4. OurGLaPEevaluation method aligns well withaccuracy on therigorous MATH dataset, as shownin. Thislignment can battributedto the significant dispaity in self-consistencybetween correct and incorrect answes, as depictedn. Conversely, on the StrategyQAdataset,the self-consistency level of corrct andincorrect answer arermarkably similar."
}