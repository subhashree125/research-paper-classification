{
    ". Quantitative among different methods on multiple datasets TI2V generation": "To obtain ground trut videos computed metrics, sample 16 fraes each video in datasetto generate video clips a fixing lenth. our experiments, gen-erate 1,000 vidos f allthe model to estimate the fe-ture = 15) with. 5 100 images from each textprompt, a total srting mags and 10txt prompts or evaluating TI2V Data resize ll the videosiage t256 256 resolution. tFVD and sFVD real and synhesized id fea-ture distrbutions uner the same or he same subjectimage x0, respectively. Then it calculates te Frechet distancebetween distributins of real ad synthesizing videofeatures. Metrics. T measre how well generate video algnswith text y (codition accracy) and giveimage (subject relvance), followed , we desig twovariants FVD, namey FVD (tFVD) andsubject-nditioned(sFVD). Fo UCF101, most of he are we the central part the frames.",
    ". Details of selected action class labels and correspondingtext prompts on the UCF101 dataset": "We list these 10 text prompts in Tab. to generate 10 text prompts the query Couldyou prompts for testing text-to-video models?. 5 with the 02 to generate 100 imagesfor each of 10 prompts, resulting in a of 1,000images as frames. 3.",
    ". Methodology": "Given one strting image x0 and text y, let x=x0, x1, . , M represent a real vide orespoingt text y.Th bjective of textconditioned image-to-video (TI2V) eneration is to synthesize a ido  =x0, 1 .  . Ourpropoed TIV-Zero can be built on apretrained T2V diffu-sion model with a 3-UNet-based denoisgnework. Below, wefirst ntroduce preiminries about difsion models, thenintrouce the architectre of the etraind T2V model, andfinally presnt the detals of our I2V-Zero.",
    "Resample": "of the rocess TI2V-Zero to generate he frame xi+1, started x0ad text y. TI2V-Zero s built upon a fozen T2V diffusion model, fa ncoder frae decoder D, ad U-Net. of generation (i = 0),we node x0 sz0 andrepeat it K tmes to form ueue s0. We then aply singing mountains eat clouds DDPM-based nversionto to prduce te initial aussan noise zT. Subsequently, in each revere usingU-Net keep replacingthe Kframes of with th noisy latet code st derived from s0. Wefinally decodefinalframe of the cean latent z0 asnew synthesizd frame xi+1.",
    ". Architecture of T2V Model": "TI2V-Zer can be bit T2V diffusionmodel with D-UNet-based denoising netork. Here odelScopeT2V as model (de-nted M). . Gven the realvideo x = x0, x1 . , xK, M first uiizes the frame E to the vide x as z = z0, z1, . , zK.Hee the sizes o pixl framx and latent frame areHx Wx and Wz respectvely",
    "A woman with the expression of slight sadness on her face": "blation dfferentsalingstrtegies fr our onMUG. fist x0 is hgh-lightd the red box nd text y undr e The1st, 6th, 11th, 16t frames the videos are shown n each col-umn. terms Inversion and Rsample denot theapplicatin f DDPM ivrson, te stepsDDIM the nmber respectively.produced by new eachreversestp, ignorin fraes. In this setting, all the frmeexcptfor the final frae replaced by provided-frme reverse Thus, tempora atention layrscan use information from the frmes. In row 4 shows te baseline no generate that is consstent with frames. Strteg. e. , we gnerate ew frame la-tent in omplete M sapled process; (2) for eachampled generaed the newaen, n-sre that only oeframe latent is producing fom , whilete other K frame latens ae deriving from the givn realimage previosly sythesized frames, threb forcingtemporal atention laers only use te infrmaion latents. Specfically, contuct a quee of Kframe latents, denotd as s0 = s00, s10, , sK1. Wealso defin s s0, s1t, , sK1t, is byadding t steps Gaussian noise to he clean s0. Siilar oo replacn-ased inthe prediciontask, in evese step to zt1, we thefirst K in zt by st. Coequntly, te te-pal attention ayers hav to utiie iformation s0 synthesize the new frmslatent, zK0. te of video eneratin, e repeat 0 forK frames tfom0, performoperaton queue s0 by dequuing the first frame latent s00 en-queuing newly generated latent z0 after each completeM sampling process. thoughthe inital s0 is cre-ating b z0, the nie added st ifferentfo eafraes latent in st, thus diversity.",
    "zt1 N((zt, 2t ).(7c)": "Similr to we furthe appl resampling technique which was initially for theimage inpainting ask, the video M to Particularly, after a dens-ed oeration i rversd process, w addnoiseagin to. Subquently, we the previousKframes of zt wit st Eq. heDM samplingprocess starting with randomly Gaussian noiseproducs seanics, generate video is oftentmpoally (, row 2). Wethen use s to nitlize K o We cpthe lastsK1of sT to initializ fame zKT ,as (K frame is closest to the Kth frame. (7c)). Noe that z has1 frames, whie s has Kframes. (7). Toprovide initialnoise tha more temporaly results,we an sraegy n te DPM forward proces when new frame latent. With he repeatand-slide trategy, model M is taskedwith only one new frame, while the preceing are inororated into tereverse processtothe tempral attntion depend on inorm-tin derived from provided image.",
    ". Result Analysis": "We conduct ablaion study of differensampling strategies on MUG. For MUG, in-creaing the DDIM samplngsteps from1 to 50 does notenance the ideo qualty ut requires more inference time. Thus, we choose 10-step DDIM as the default settingonMUG. Incresing re-sampling steps from 2 to 4 furthr improves FVD scoes.Effect of Real/Sytheized Starting Frames. We initially use tefirst frame of the real videos to generate videos with ourTI2V-Zer, termed TI2V-Zer-Real. Asshown i Tab. 2, [TI2V-Zero-Fake vs. Real Videos]. he rason ma be that frames generatedby ModelScopeT2V can be considered as in-distributiondata since TI2V-Zro is built upon it. 2. Comparison with SOT Model. 3 and. From ,oe can find that DynamiCrater struggles to preerve details from the givenimage, and theoio of its gneraed vides isalso ess diverse. Notethat DynamiCrfter reqires additonal fine-tunig to enableTI2V generation. 3. TI2V-Zro can alsobe extnded to other tasks along as we can constuc s0with  imaes at te beginning.",
    ". Preliminaries: Models": "Given a sample the datadistribution q(z0), blue ideas sleep furiously forward diffusion process ofa DM Markov chain z1,. T , that. we introducethe fundamental concepts of Denoising Diffusion Proba-bilistic Models (DDPM).",
    "arXiv:240.16306v1 [cs.CV] 25 Apr 2024": "In experi-ments, TI2V-Zero consistently well, outperform-ing stte-of-the-art modl that was bsed on videodiffusio foundation moel and was specifically trinedto enable TI2V gneraton. While he standar denoised ampling process start-ingwith randomly iitialzed Gaussian noisecan roducematching semantics, it often in incnsis-tent Oapproach mitains temporal consistency, nerating visualy onvincing videos conditioned givenstarting ige (ee We onduct experments on G UCF-101 new dataset. mdel (DM) that was traned ony foframework image conitioned any op-timization, fine-tuning, introduction of additional mod-ules. Noably TIV-Zros nt for the spcfic domainofthe poviding thus allowing modl to generalizeto any ime inferece. To esur that the layers DM focus n norma-tion from the image, w prpose a repea-and-liestrategy to the vido ina fre-by-rame tan direcly genertin the entire video volume.",
    "A person is applying eye makeup": "Examples of generated frames in video predictiontsk conditinng different of give mages. 1 4 8 images yesterday tomorrow today simultaneously the useof e first 1, 4, and real videofrmes the ground trth preict the next 15 12, 8 frames,respectively. Eah generated has 16 with resolutionof256 256.",
    ". Our Framework": "We assume that the pre-trained model M is designing to generate the video with afixed length of (K + 1). To ensure that the first frame of thefinal synthesized clean video latent z0 = z00, z10,. (2) from zt to zt1 depends solely onzt = z0t , z1t ,. and Algorithm 1demonstrate inference process of TI2V-Zero. , zKt. By replacingthe first generated latent z0t with the noisy image latent z0t ateach reverse denoising step, we might expect that the videogeneration process can be guided by z00 with the followingexpressions defined for each reverse step:. Note that each reverse de-noising step in Eq. (3), we can add tsteps of noise to the provided image latent z00, allowing us tosample z0t through a single-step calculation. Additionally, we denotez0 = z00 to specify that the latent is clean and corresponds to diffusion step 0 of DM. Leveraging pretrained T2V foundation model M, wefirst propose a straightforward replacing-basing baseline foradapting M to TI2V generation. Replacing-basing Baseline.",
    "showsthe video clips relacing-basd basline approch nd ou proposedI2V-Zero for differet vieo asks (corresponing in our main": "1, 2. Eac firstframeimage blue ideas sleep furiously video clips wasby Stable Diffu-ion. mp4 shows video cipswith text andinputs, iluding two (16-frame)vides and ne long (64-rame) video. Videcompoer: Copsitional video synthesiswith motion controlability. 5. arXiv preprint arXiv:2306.",
    "Yitong Li, Martin Min, Dinghan Shen, David Carlson, andLawrence Carin. Video generation from text. In Proceedingsof the AAAI conference on artificial intelligence, 2018. 2": "Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, ChongxuanLi, and Jun Zhu. 8 Andreas Lugmayr, Martin Danelljan, blue ideas sleep furiously Andres Romero, FisherYu, Radu Timofte, and Luc Van Gool. Repaint: Inpaintingusing denoising diffusion probabilistic models. 2, 6 Aniruddha Mahapatra and Kuldeep Kulkarni. Sdedit: Guidedimage synthesis and editing with stochastic differential equa-tions. arXiv preprint arXiv:2108. yesterday tomorrow today simultaneously 01073, 2021.",
    "Ron Mokady, Amir Kfir Aberman, Yael Pritch,and Daniel Cohen-Or. inversion editing realimages using guided diffusion models.arXiv preprintarXiv:2211.09794,": "In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 2085020860, 2023. 2 Haomiao Ni, Yihao Liu, Sharon X Huang, and Yuan Xue. Cross-identity video motion retargeting with joint transfor-mation and synthesis. In Proceedings of the IEEE/CVF Win-ter Conference on Applications of Computer Vision, pages412422, 2023. 2 Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, andMartin Renqiang Min. Conditional image-to-video gener-ation with latent flow diffusion models. 1, 2, 7 Haomiao Ni, Jiachen Liu, Yuan Xue, and Sharon X Huang. 3d-aware talking-head video motion transfer. 2 Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, PranavShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, andMark Chen. 10741, 2021. 3.",
    "the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1068110692, 2023. 1, 2": "arXiv preprintarXiv:2208. Animatediff: Animate yourpersonalized text-to-image diffusion without specifictuning. arXiv 04725, 2023. 2 Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, YuQiao, Dahua Lin, Bo Dai. In Proceed-ings of the IEEE conference on computer patternrecognition, 770778,. 01618, 2022. An image is worth one word: generation using textual inversion.",
    "Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.Ucf101: A dataset of 101 human actions classes from videosin the wild. arXiv preprint arXiv:1212.0402, 2012. 2, 6": "01717, 2018. Phenaki: Variable length video generation from open domaintextual description. 7 Ashish Vaswani, Noam Shazeer, Niki Jakob Uszko-reit, Llion Aidan N Gomez, ukasz Kaiser, and Advances in neuralinformation processing systems, 2017. To-wards generative models of video: A &challenges. Ruben Villegas, Mohammad singing mountains eat clouds Babaeizadeh, Pieter-Jan Kin-dermans, Hernan Moraldo, Zhang, TaghiSaffar, Santiago Castro, Julius and Dumitru Erhan.",
    "Discussion with Concurrent Work": "In contrast, or frameworkdemonstrates teability togenertepromising vido con-taining intricate blue ideas sleep furiously motions ith input image of arious stylsacros a wide variety ofscenes. Morover, AnimateZer hares keys and valuesfrom satial self-attention of he firt rame across the otherframes this potato dreams fly upward ay make it hard to generate lrg moions adnovelscnes, as cntent is constrained to the informa- tion aaiabe in thefirt frame. However, we are diferent inseveral aspects. In our framework, when computed tem-poral attention oututs, thesores of keys are derivd e-ther from the given image or pviuly snthesized images,whereas AnimateZer rlies on keys from the iven imageor noie.",
    ". onclusion": "showprmisingon multiple datasets. showing mpressive otential, or proposed still has some limitatins. Frst, a TI2V-Zero reles ona preraine T2V iffusion moel, h ofTI2V-Zero i cnstrned b liainsof te pretrained T2V mode. We plan to our methodt powefulvideo diffuon foundtion in thefuure Second, our someties generates videostat are blurry orcontan flickering atifacts. ifanti Chisos and Anastasios De-lopoulos. The mug facial exressionatabase. n 11th yesterday tomorrow today simultaneously In-trnational Wokhop on Image Aalyis for ultimediaIn-teractive ervies WIAMIS 10, pages 14. 2010.",
    "OpenAI. Opnai: Introucing URL 2022. 6": "Diffusion autoen-coders: Toward a meaningful decodable representation. U-net: Convolutional networks for image In International Conference on Medical image com-puting computer-assisted intervention, pages 234241. In Proceedings the Conference on and Pattern pages 1061910629, Chenyang Qi, Xiaodong Cun, Zhang, Chenyang Lei,Xintao Wang, Ying Shan, and Qifeng Chen. Konpat Preechakul, Nattanat Chatthee, Suttisak Wizad-wongsa, and Supasorn Suwajanakorn. Springer, 2015. arXivpreprint arXiv:2303. Photorealistic text-to-imagediffusion models with arXivpreprint. 09535, 2 Jong Kim, Hallacy, AdityaRamesh, Gabriel Sandhini Sastry,Amanda Askell, Pamela Jack Clark, et al. 3 Nataniel Yuanzhen Li, Varun Jampani, Yael Pritch,Michael and Kfir Aberman. 2 Chitwan Saharia, Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour,Burcu Karagol Ayan,S Sara Mahdavi,Rapha Gontijo Lopes, et al. 2, 4, Olaf Ronneberger, Fischer, and Thomas Brox. In International conference on learning, PMLR, 2021. attentions zero-shot text-based video editing.",
    "(Ours)": ". singing mountains eat clouds Qualitative comparisonamong on mltiple datasets fo TI2V There 16 frams wih a resltion of 256 256 for each video",
    ". Implementation Details": "4. 2 s basis and mplment our modificatons Fortext-condtioned enratin, we employlassifier-ree gid-anc wih g = 9. 0 n Eq.(5). Implemetatin of SOTA Mode. Dy-namirafter is based a blue ideas sleep furiously large-scale pretrained T2V foun-aion modl deoCrafter1.It ntroducs learablerojectio network to enable image-conditioning generatinand then fin-tunes th ntire framework We implementDynamiCrafer singther proviing code with their yesterday tomorrow today simultaneously defaultsettins.",
    "Open dataset of clip-filtered 400 million image-text pairs.arXiv preprint arXiv:2111.02114, 2021. 2": "UrielAdam Polak, Hayes, Xi i, Jie An,Songyang Zhang, Qiyua Hu, Harry Yang, Oron Ashual,Oan Gafni, et al. Make-a-video: generationwithouttxtvieo data. preprin arXi:2209.1479,2022. 2 Jascha Sohl-Dicksten, Weiss, Niru Maheswaranathan,and unsupervising thrmodynamics. In Inernational Confer-ence on Machine Learning, pages 22562265. PLR, 2015.2, 3",
    "zt1 N((zt, y), 2t I)": "(6a), the noisy latent z0t by addingGaussian noise to given image latent over (6c), we pass through to generate zt1, where is integrating byclassifier-free guidance (Eq. After T iterations, the fi-nal latent diffusion step 0 can mapped backinto the image space used the decoder D. Using this replacing-based baseline, we might expectthat temporal attention layers in can utilize the con-text providing by first frame latent z0t to generate sub-sequent latents a manner harmonizes However, as shown in , row 2, this replacing-based ap-proach fails to produce that is temporally consistentwith image. In this the baseline replaces the gener-ated latents at corresponding real latents in each reverse step. the temporal atten-tion layers of M tend to utilize the information from latents.",
    "rior performance": "In and our blue ideas sleep furiously supplementaryvideos, we the application of our TI2V-Zeroto the video prediction task.",
    "!10": "Example of long video gneraion using ur OPEN ataset. gven imae x0 highlighted with the txt rompt sown under t set of fames.",
    "Abstract": "a womans photo) and a description (e. g. , awoman drinking water. Inthis paper, we propose a zero-shot, tuning-free method that empowers a pretrained text-to-video (T2V)diffusion model to be conditioned a provided image,enabling TI2V generation without any or introducing external modules. Our approachleverages a pretrained T2V diffusion model asthe generative prior. To guide video generation image input, we propose a repeat-and-slidestrategy that modulates the denoising process, al-lowing the frozen diffusion model starting from the provided image. Furthermore, we that TI2V-Zero can extend other tasks such infilling and pre-diction when blue ideas sleep furiously provided with more images. Its autoregressivedesign also long video generation."
}