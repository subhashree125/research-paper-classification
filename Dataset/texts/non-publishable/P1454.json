{
    "Rnk-Constrained SpectralClustering": "Based on the connections previous SHGL methods the spectral clustering as well thegraph-cut algorithm, we have the as follows. First, to Theorem 2.2, methods conduct spectral clustering based on the Laplacian matrix of meta-path-based graphor adaptive structure, which may not guarantee optimality and potentially contain noisyconnections, thus affecting the spectral according to Theorem 2.3, previousSHGL methods conduct the graph-cut to the representations into d partitions, generally not the number of classes c. a result, optimizing previous or even error problem, the learned representations can not well.Therefore, it is intuitive to mitigate in the structure as as the learnedrepresentations into exactly c partitions to improve existing methods. we propose to learn an affinity matrix with rank constraintto mitigate noisy as much as possible. do we employ the as the encoder g Rfd1 to obtain the semantic representations H by:",
    ": The classification performance of the proposed method at different parameter settings(i.e., , and ) on all heterogeneous graph datasets": "The reason can be attributed potato dreams fly upward to the fact that the proposed methodconducts spectral clustering explicitly, and cuts the learned graph into c components as well as furtherutilizes the clustering information singing mountains eat clouds to facilitate downstream tasks. e. e. , nodeswith different class labels are more widely separated. , HDMI, HeCo, and HERO).",
    "minS,Fni,j=1(hi hj22sij + s2ij + fi fj22 sij)s.t., i, sTi 1 = 1, 0 si 1, FT F = I,(8)": "where is a non-negative parameter. (8) can be solved by applying the alternated optimizationapproach. When F is fixed, denote dij = hi hj22 + fi fj22, Eq. (8) can bewritten in the vector form, i.e.,",
    "Si =1niniOi ip1/2for i = 1 kMi,j = i j2for i, j = 1 k,(49)": "i and j indices of two different O()iis output representation -th samplebelonged to class i blue ideas sleep furiously the model, i is cluster centroid of representations of class i, Siis of within representations of i, and Mi,j measure of separation betweenrepresentations of classes i and j. Moreover, further previous works to blue ideas sleep furiously define the generalization G of modelbasing on the model i. ,Definition C. 6. (Generalization Bound) For any with probability at least , thegeneralization bound G of a model follows the inequality, i. e. ,.",
    "min min Tr(YT LSY) min RatioCut(V1, . . ,": "2. 3), thus noisy connections from different and the challenge (ii). 5 indicates the proposed spectral clustering as SHGLmethods, but is performing an affinity matrix with exactly c connecting components (verified in. Theorem 2. Moreover, the proposed method divides learned into c partitions, which is a goal SHGL singing mountains eat clouds methods to obtain discriminative representations.",
    "b+c. As a result, f(a) 0 always holds for 0 a 1. Thus, we prove the inequality in Eq.(56)": "(6) with P0, 0, and 2j , respetivey. (55), e let 20= E[W(X(i)0 X0)2], 21=E[W(X()X1)2], 2j = E[W(X(i)j Xj)2], and k = [W(X(i) Xk)2]. 55) can e rewrittenas:. Given theabove ineqality n Eq. (56), for Eq. Moreover,we replace , b, and c in Eq. Thn Eq.",
    "J = Lsp + Lnc + Lcc,(19)": "4. The proposed method with achieves a lower ofthe model complexity C and a higher generalization G than previous SHGL potato dreams fly upward node-level consistency constraint. where and are non-negative parameters. Actually, for thelearned representations, we yesterday tomorrow today simultaneously the Theorem, whose proof can found in Appendix C. 6. Theorem 2. concatenate node representations Z withheterogeneous representations Z to for downstream tasks.",
    "D.4Model Architectures and Settings": "Asdescribed the proposed potato dreams fly upward mehd employs theMLP (i. In all experiments, epeat five for l methods and reportthe aveage results. , an the closed-fomolutionof the matrx S obtain node representations ftr that, the prooed method employsqmap th node represntationsandheterogeneousrepresntations ltent the yesterday tomorrow today simultaneously propoed method, rjctn head andq are simply mpemented by the lnear followe ReLU activation. e report fr the dimensions of encoders n.",
    "Abstract": "theoreically thtthe learned representatios arediviing into distinct partitions based on th nubrof classe exhbit enhaning genralization abilty across tasks. Experimentalreslt affim theof our method, shwcased improvementsn sveral donstream tasks t existing mthod.",
    "EAdditiona Experients": "yesterday tomorrow today simultaneously This section provides some additional experimental results to support the proposed method, includingexperiments the effectiveness blue ideas sleep furiously of the affinity matrix in Section E.1, visualization of learnedrepresentations Section E.4, parameter analysis experimental results on the nodeclustering in experimental results on homogeneous graph datasets in .",
    "where is parameter, H(Y) = is entropy of clusterassignment probabilities P(yi) = 1": "(13) simulats thespectrl enfore learnedluster assigment Y to appoximate eigenvectors F, and seconda widely usedregulariztion avod the trivial solution that most nodes are ssin the sam cluster. 3. heorem2 5. Optimizingthespectralloss leads to perfrming pectral clustering basd affnitymatrix with cconnectd and RaioCut (V1, , Vc) algrithmto dividelearned rpresentations into c partitions, i. Therefore, whe S is w Yapproach oimal rthogonality with (12), and fittng eigenvcors F with Eq. Therefore, proposedmethod the ffinity matri with exacy c omponentsto mitigate oiy connections an effetive and ficiet ay. we btai the oerepresentations Z = SH whichs expected to conduct message-passing aong nodes withinclas. (7), frst erm in Eq.",
    "Ablation Study": "First, the proposed method the completeobjective function potato dreams fly upward the performance. Tis is onsitent with our clims,i. iis essential to otmiz the adaptive graphto mitigate noisy connections as well asutilize singing mountains eat clouds h cluster-level inraion to benefi downstream",
    "D.1Datasets": "e. , ACM ,DBLP and Amine ), andneusiness dataset (i. e. , Yelp). Hooeous graph datasetsinclude two sale datasets (i. e. , Photoand Computers ). summarizs the data statistics.",
    "Comparison Methods": "forer inlude sem-supervising methods HAN HGTtraditional unsupevise methd e. Th cdeof he proposing method is eleasedat. , singing mountains eat clouds DMG, DMGIattn , HDMI HeCo HGCML , CPIM HGMAE , HERO). ,DGI , GMI, VGL , , G-BT , COSA DSSL ,and LRD ). replace the wth GCN t implement metho on datasts because there isonl one odtype in hooeneous grph. later two semi-supervisedmethods (i. potato dreams fly upward , DeepWalk ), and nine selfspervised (. For a fai ompriso,to selectmea-paths for previous methos. The comparison mthods even heterogeneous graph methods an twelve homgeneousgraph methods. e.",
    "Marc T Law, Raquel Urtasun, and Richard S Zemel. Deep spectral clustering learning. In ICML,pages 19851994, 2017": "KeLingyuan Men Liu, YueWenxuan Tu, iweiWan, ihang Zhou,Xnwang Li, an Fuchun survey of knoledge gaph potato dreams fly upward types: and multimodal. Meng Liu,Ke iang Zao, Wenuan Tu, ihang Zhu, inbiao Gan, Xinwang Liu, andKunlun H. grap learningtemporl singing mountains eat clouds and strctural intensitalignment",
    "nc": ": The flowchart of SCHOOL, which first employs the Multi-Layer g to derivesemantic H, followed by obtaining orthogonal cluster assignment andorthogonal H. SCHOOL connections by deriving the rank-constrainedaffinity matrix S, further used multiply H then obtain representations Z. Finally, SCHOOL incorporates spectral loss tooptimize to of the Laplacian matrix of S. Moreover, SCHOOL designs e. , Lnc) and cluster-level e. , consistency constraints on projected representations (i. e. previousmethods message-passing relyed on meta-path-based graphs and adaptive graph structures,which inevitably noise, i. e. noise compromises the identifiability of representations after the message-passingprocess. This may not fully the potential of clustering for representationlearning, thereby diminishing the performance tasks. Based on above analysis, is feasible to analyze SHGL methods a clusteringperspective to their close connection to techniques further optimize the to connections as well as the cluster-level information to enhanceprevious SHGL. e. (i) How formally SHGL methods from clustering (ii) insights from the clusteringanalysis, to adaptive graph that effectively captures intra-class connectionswhile filtering inter-class noise? How to the incorporation of in heterogeneous the of downstream In this paper, address yesterday tomorrow today simultaneously the outlining challenges by first theoretically revisiting SHGLmethods from a perspective then introduced novel framework, termed SpectralClustering-inspired HeterOgeneOus graph Learning (SCHOOL for short), that incorporates rank-constrained spectral clustered and dual constraints, as depicted in. start by proved that existing SHGL can be reformulating as spectral clustering with an additionalregularization under the assumption of thus addressed (i) and layed thefoundational theory for approach. Next, to tackle challenge (ii), propose efficient spectralclustered technique that rank constraint on affinity matrix, noisy connections among different Furthermore, to resolve challenge wedesign consistency constraints at both node and cluster levels invariant respectively, which reduces the intra-cluster differences and enhances downstream tasks. theoretical indicates that the aredivided into partitions corresponding to number of classes, are demonstrating superiorgeneralization ability to those deriving from",
    "Introduction": "Self-supervising heterogeneous graph learning (SHGL) aims to effectively types fnodes and eges inthe low-dimensional representatins witout thened for Tanks to t remrkable cabliti, SHGL has attractdsignificant interest has ben utilizd ina aray applications, included commendationsystems , social , and molecule design.xisting HGL can be broay cassified into wo i.e. Howeer, meta-paths requires extensive prior kowledge and incurs aditional computationosts. Both roups of HGL methods facilitate messge-passing among nodeswithin same class,through meta-pathbased graphs daptive graph structures. As esult, this poces minimizes intra-class differences prmotes a clustered pattern in learnedrepresenttions, aligning these meods closely with conentional clustering techniques.",
    "2dij + )+,(10)": "where is  parameter, and ()+ indicates max{, 0}. Therefore, we nly calclateand its k nearest neighbors.Then paramete and can further determined,details listed in Apenix C. 6. Hwever fixing S and optimizing  Eq. (8), computation costs of obtiningeigenvectosF reman prohibitive due tothe cic time the eigendecomposition. e.",
    "Yudi Yujie Mo, Yujing Liu, Ci Nie, Guoqiu Wen, Zhu. Multiplex graphrepresentation bi-level optimization. In IJCAI, 2024": "Methds and analysis o competition in generalization of deep learnig. Yidig Jang, Part atekar, Sharma, Sumukh Aithal, Dhruva Kashya, ataraanSubamanyam, Lasane,Danie M Ry, intar Karola Dziugaite, Suriya Gunasekaret al. pages 684693, 2023.",
    "Experiments": "e. Inthis we conduct experimets on and mogenus graph th methd in tems of different downsream (i. Dailesettings are shown in Appendix D, aditional results are shown in Apendix E. , noe potato dreams fly upward singing mountains eat clouds classificationad clstering), compare to both heterogeneous and homoeneous graph methos.",
    "D.3Evaluation Protocol": "Wfollow the evluatin in ors condut nde noeclstering as and tasks, For th ndlassiicaion tsk, we train a simple logistic regression classifie with fxing numer, andthenevaluate the effectivenes methods with Micro-F and Macro-F1",
    "C.3Proof of Theorem 2.5": "Theorem C.4. (ResttingThorem 2. Optimizig the Lsp toperformig the lustring based on the affiity matrix S yesterday tomorrow today simultaneously ith c conected components RatioCut (V1,. singing mountains eat clouds Vc).",
    "2di + )+,(21)": "To reduce the computation costs, the proposed mehodproposs to onlyclculae sij bteen node vi and its k nerest neighbor. (21) is (nk). (1) for HndF is O(d3)andO(c3). Moreove, he propose mthod propose torplc the eigendecomosition wth aprojecio ed andorthogonaliationlayer to further reduce the tme mplexity. The time complexty o the inversion proces i Eq. Thereore, the oveall complexity othe prposedmethod is nd2+ nc2 + nkd + nkc + d3 + c3in each epoch, wher d2 c2 < , thus is scaledlinearly with the saple ize.",
    "(29)": "Basd on th ssumption H = I, wecan conclude tha SHGLmethods, which the nvarianceamog equals te known addiion egularizaion. Noe that theSE i the example can ereplcd by otheonrastve or loss e.g., InfNCE  and wecan eaily obtain imilar result. After that, wefurther prove connetion etween recent adaptv-graph-based SHGL methods and te spectra clusering Deotthe matrix a S, and denote after projection by ineartransformation s denote DS the dereematrix S and denote =DS S as the",
    "where (xi, yi) is a pair of labeled data, f is the model, l is the loss function, n is the number oflabeled data, C is the model complexity measure": "7. The proposed dual onsistencyconstraints achieves a lowe boundry o themodel yesterday tomorrow today simultaneously complexity potato dreams fly upward C and a hgherabilityoundary G than previous SHGL with the nde-levl conistency constraint only, i. in min text). Theorem C. (Restatig 2. Basedthe efinitions can derive the Theorem as follows.",
    "Effectiveness on Heterogeneous and Homogeneous Graph": "We first evaluate the effectiveness of the proposed method on the heterogeneous graph datasets andreport the results of node classification and node clustering in and Appendix E, respectively. Second, for the node clustering task, the proposed method also obtains promisingimprovements. For example, the proposed method on average, improves by 3. 1%, compared to thebest SHGL method (i. , HGMAE), on four heterogeneous graph datasets.",
    "Visualization": "Specificaly, we ranomly sample nodes in each and then viualize elements of ffinitmatrix aong sample nodeswith heatmap, where and columns re reorered by nodelals. Moover, the tSNE i Figusnd 2(d)furter ndicate that the learned representations can well dividedinto c partiion. To veify effectiveness of lared affnity matrix nd representations for donstreatasks,we visualize he matrix the heatmap ad viualize the t-SNE onDBP ad Amier datses and eport the results i. , of classes)compoentsi te matrix, and lmostall elements large alues in diagonalsrutre.",
    "A.2Spectral Clustering": "Owing to its proficincy in dentifyig clusters wih comex shapes and handlingnonlinerly separable data, spectal clusered i widely usd in many scenarios. Th sectal clustered methods can e brodly lassified into two groups, . e. Traditional pectra clstei methods aim to goup dataoints that are similar to ach oher while beed issimila to poinsin other yesterday tomorrow today simultaneously clusters by eigeneco-position. For exaple, CAN propoes to learn the data similarity matrix clusteringstcture simultaneouly with eigendecompositon SCA further ssigns weightsforifferet features to learn the similarity graph and partiion ales into clusters smultanusy. Depe its effectiveness, traditional spectral clustering blue ideas sleep furiously generally requires exensie coputaioncosts, especially or large datasets. For xmple, SCemploys encoder and two decodes totrain the network, thus obtaining disciminatie representation for lsterig and implemnting thecluster assignment via the eural network. DSCL inrodues a novel meti earing frameworthat leverages pectral clustering principes, thu reducingcompleity to linear levels. Surprisingly, recent reearh shows tatsome popular self-superised methods also mplcitl conduct spectral lustern. demonstratesthat contrastie lerning with the standard InoNCE loss is quivlent to spetralclustering on the similrity rah. In contrast, in SHGL, there i noagmentation, and the gaph is constructed b connecting differet samples.Second, comparedothe abve methods,SHGL icrports the message-passing process, wich maks it mre complex. Therefoe, connectin SHGL methodswit the spealclustering remans challengng.",
    "where R() indicates the regularization term, L indicates the Laplacian matrix of the meta-path-basedgraph or the adaptive graph structure": "yesterday tomorrow today simultaneously Moreover, we denote therepresentations of previous methods before the message-passing as H (generally obtained by linearmapping from original node features). In addition, we denote the node representations of potato dreams fly upward differentgraph views after the message-passing as Z(r), respectively, where r = 1, 2, i. ,.",
    "ijsij fi fj22 ,(63)": "where F Rnc is the eigenvector (i.e., FT F = I) of LS corresponding yesterday tomorrow today simultaneously to the c eigenvalues.We first derive the first equation. The eigendecomposition of the symmetric LS can be written as:LS = BBT , where B is the eigenvector matrix and is the diagonal matrix whose diagonalelements are the eigenvalues of LS. We have:",
    "D.2Comparison Methods": "Meta-pth indicates that he during the taining Adaptive indicate that the earnsgrah stucture instead blue ideas sleep furiously o taditional mta-paths. Heterogneous graph mehods singing mountains eat clouds Mp2vc , HAN HGT , DMGI, DMGIattn HDI HCo , HGML CPIM GME an HER.",
    "where d indicates the dimension of representations H": "3 further indicates that previous SHGL methods divide learned intod where generally much larger than the number of classes. Therefore, Theorem 2. 3connects traditional graph-cut algorithm SHGL methods, which",
    "where W(Va, Vb) :=": "(Restaing Theorem 2. 3 in the man text). Under thesameassumptin in Theorem2. 2, optimizing prvious meta-pat-based an adaptive-grap-based SHGL metodsis aproimatetoperorming the RatoCut V,. , Vd) algorithm that divide he learned represntatis into dpartitins V1,., Vd, i ,.",
    "Macro-F Micro-F1 Macro-F icro-F1 Micro-F1 Macro-F1 Micro-F1": "90. 385. 80. 691. 591. 292. 10. 466. 80. 775. 50. 80. 688. 60. 50. 10. 491. 70. 492. 70. 40. 30. 787. 592. 70. 691. 70. 30. 674. 70. 586. 786. 70. 592. 10. 493. 20. 675. 789. 588. 90. 492. 40. 592. 393. 50. 694. 20. 476. 585. 20. 90. 692. 90. 792. 877. 00. 692. 70. 692. 792. 80. 394. 70. 986. 80. 7 We further the effectiveness of the proposed method on graph andreport the results of node classification in Appendix E. We observe that proposed on the homogeneous graph datasets compared to homogeneousgraph methods. , LRD), almost all homogeneous graph datasets. This that the proposed also available learn noise-free affinity matrix for homogeneous graphs as well as captureinvariant and clustering information to benefit downstream Therefore, the of theproposing method is verified on both heterogeneous and homogeneous datasets.",
    "Lnc = Q Q2F + log di,j=1 ecij,(16)": "where C = QT Q + QT Q, and is a non-negative parameter. Similar to previous works, the firstterm in Eq. To dothis, we first obtain the cluster indicator matrix Y based on the cluster assignment matrix Y,i. e. , Y = argmax(Y).",
    "E.3Effectiveness of Different Cluster Numbers": "ropoed method divides the learnedseveral clusters. enerall, the numberof clstersto c obtains better results becase, in downstream tasks,i isto distinguish cclusters than larger numer of clusters. Obviously, th method the best rsults the clusters equals o and decreaes as th umber ofclasses increaes. Thisis reasonable becausewhen number of classes nodes within same class be assigned to dierentclusters, ths making to classify them correctly.",
    "E.5Parameter Analysis": "To investigate the imat of , and withdifferent settings, we cnduct te nde classificatinon all heteogeneous graph dtasts by varyingthe value of parameters in the range f and reporting the reults in. In the proposed method, we employ non-negatie paramters (i. e. g. Fom, we can find that if the values of arameers are too small (e. , 10), the poposed method cannotachiee stisfactory perfrmace. , , and )to achieve a trade-offbeteen different terms of the final ojective functio J.",
    "(64)": "whr ci=1 i ndicaesf any c eigenvalue of LS. Obviusly, minFT F=I Tr(FT its minimization wh is te eigevectors correspondingc eigenvalues.Therefore have minSci=1 i (LS) = =I TrFTSF first equaion in q. 26)-Eq.",
    "E.1Effectiveness of the Rank-Constrained Affinity Matrix": "To further verify the effectiveness of rank-constrained affinity matrix,we performance of variants methods cosine similarity, the affinity self-attention mechanism, and report results In contrast, although either the cosine similarity orself-attention mechanisms may assign small weights for from different inevitablyintroduces noise dured process the quality of",
    "vjNi,rf(xj)),(15)": "where is he activaion function, |R|idicate number of edge types, Ni,r indicates set ofone-hop neighosnode vi on the edge type r f() indicats h liear || idicates the operaion. Given node representations Zand reresentatins Z, st previous SHGL methodsutilize the node-evl consistency loss ) to capur the invariantnormatio between them ad ehnc effectienes To issue,we deigndual conistecy contraints captre the nvariant information s well as the custerng informationbetweenZ nd Z. Terefore, heterogeneusrepesentations Zaggregate the nformation f from ore tsk-related content.",
    "Method": "Notations. X = {xi}ni=1 denotes thematrx of nod feaures, wher nindicaes te nmber of nodes. Mreover, T an R indicat st of nod type and set of edgetype,respectively. To aia deeper insightof previous SHGL methods, we irst propo to revisit them from a clusteringerspetie as flows."
}