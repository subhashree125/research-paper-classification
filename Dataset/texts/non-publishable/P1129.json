{
    "Performance comparison (RQ1)": "These the superiority oFLea inaddressing the challenges cased by and t can also be observed that te ata-umentation-based base-lines, particularly FedMix, outerform the loss-based baseline cases, whch again supports use of sme global in-formation aid local training n the presence of dta InFedMix, sampleaggregations are shared protect data yettis inevitably harms utility the shared augmntations trained of cassification models. the. Thisexplains why FLea cancnsstntly outerform FedMix. The overall accuracy achieved FLea and baselines are comparedin. In 13 the studied 18 FLea presents improvemen over5%, nd aong those, in 5 the mo tan10%. Acrss various model architctures (x3) and differentleels of data skewess and scarcit (x2) FLea the sate-of-the-art baselines and significaly educesth compared to FedData whic shares e ata. left figre) smaller ropotio to b shared toachieve the accracy as FedMix(f.",
    "Gbor Maria Rizzo, and Nail K Bakirov. 2007. Measuring and testingdependence by correlation distances. (2007)": "PMLR,2111121132. edproto: Federatedprototye learing acroshetero-genous clients. Qi Teng, Ku Wang, Lei Zang, and Jun He. ue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jin Jing, andChengqi Zhang. Virtual homogeney learning: Deendng aaint ata heterogeneityin federated earning. 2020. 84328440. IEEE Sensors Journl 20, 13 (2020), 72657274. 2022. henheng Tang, Yonggan Zhang,Shaohuai Shi, Xi He, Bo Hn, and XiaowenCu.",
    "Justin Christopher Jacoby, and Juan Pablo Bello. 2014. A for urban sound research. Proceedings of the 22nd ACM internationalconference on 10411044": "45104520. singing mountains eat clouds InProceedings f the IEE cnferc vision and pattern reconition. Mobilenetv2:nverted residals andinear bottlenecks. TwrdsUndestaning ad Mitigating Dimesinal eterogeneous FederatedLearning. Mark Andrew Howard,Menglong u, Zhmoginov, Chen.",
    "CONCLUSIONS": "We proposed FLea, a novel approach to tackle yesterday tomorrow today simultaneously scarce and label-skewed data in FL. Feature augmentation is employed to mitigateover-fitting and local drift simultaneously. Extensive experimentsdemonstrate that FLea remarkably outperforms the state-of-the-artbaselines while mitigating the privacy risk associated with featuresharing. We leave the task of improving efficiency for yesterday tomorrow today simultaneously futurework. To enhance privacy, FLea can be combined with other meth-ods like differential privacy and homomorphic encryption.",
    "Alex Krizhevsky, Vinod Nair, and inton. 200. Cifr-10 ad URL: (009)": "Fan Lai, Yinwei Dai, Sanjay Singapuram, Jiachen Liu, Xiangfeng Zhu, HarshaMadhyastha, and Mosharaf Chowdhury. 2022. Fedscale: Benchmarking modeland system performance of federated learning at scale. PMLR, 1181411827. Gihun Lee, Minchan Jeong, Yongjin Shin, Sangmin Bae, and Se-Young Yun. 2022.Preservation of the global knowledge by not-true distillation in federated learning.In Advances in Neural Information Processing Systems. Federated learningon non-iid data silos: An experimental study. IEEE, 965978.",
    "DPRIVACY STUDY": "The resultsin suggest that FLea needs times of sample thanFedMix for correlations. This demonstrates that FLea canbetter the context All above results lead to conclusion that by reducingfeature exposure and mitigating the correlation between the fea-tures and source safely protects the privacy associatedwith sharing while achieving favorable performance gainin the label skew simultaneously. Overall, with L, the data and features is reduced, preventing image frombeing reconstructed. simulate the context information by adding color squareto the (to mimic the camera broken), as illustrated in. Identifying context Real-world attacks will be far more challenging than oursimulations. To train the decoder, we utilized the entire CIFAR10 trainingset, training for 20 and employing a learningrate of 0. i. 40 ) to extract for (b) shows the training while the exampled from the set. || = 100 as an example for studying,as in settings either the label is more or the local datais more scarce, privacy attack can hardly be more effective thanthis This is to attack defending the case. We first a data reconstruc-tion attacker, the approach in the attackerconstructed a decoder for reconstruction. , the color of the dog). decoder took thefeatures extracted from global as input and generated image, served as the basis for calculating themean squared error (MSE). , after the round, thesensitive are removed (e. rounds when < 0. We measure the attacking difficulty by how trainingsamples the model needs to achieve a certain accuracy. As the correlation between features and thedata is continuously in (b)), report thereconstruction and context detection performance 0. 65 and 0. We use a binary classifier consisting of four linear layers classifythe flattened features or images.",
    "ABSTRACT": "However, existing FL methostill facechallenges when deling with carcedta crossdeices, resulting local odeloverfitting ad drif, onsequentlyhndring eformance of the global mdel. designmitigates ocl model causdby the absence of cerain classes; ii) featureap-proac based onlocal and global activation mix-ups for local train-ing. This tratey enlarges the training samples, therey reduingthe risk local ovriting; iii) An obfscation methd minimzethe orrelation ctivations and the souredat, enancing the of shared featues The demosrate that FLeaonsistently ot-performs sae-f-the-art FL (amng 13 f experi-mented settings, the is ove 5%) while conurrentlymitgain the privacy vulnrabilities with fea-tures is available at",
    "Data scarcity in FL": "2. The results are in , fom wich we dawthe followng obserations: 1)FAvg egrads remarkably asdat sarciy become seereIts wih starts. firstobsered by (cf. Fided ), wen develping the model ith a fixe total uber oftrained FedAg and FedProx dereass of clients ncreases, leading reuti locadatasize per Despite th f FL includingthoseealier Sec. We dstributing te traiingset of CFA10 to 10, 100, and 500 leaded local datasie|D | o 5000 (notsre) 500 sarce), an 100 (carce),respectivey, esuring uniform representation all 10 classes. Hwevr, FedScale primarilfocses on sstem offers nsightag-rithm effectiveness. To avod beinaffected b label skew, e split dta in an IID (IndependentandIdentically Distributed) manner. literauee he fund few studies focusingon te data scarcitinFL. To directly ob-serve dat sarity FL we conductedexperimentscomparing several state-o-the-art at different datasarcity lels Specifically, wecoparedFedAvg with bst oss-based methods FedDecorr an FeNTD, as well as the best ataaugentaion-baed ethods FedMix and FedData. 2)mot are evaluated on largeoal datts, containing Fr example,the commonly use contins 50, 00 traiingames which are usuay distribute 10 to 10 in an avragedata size of 500 to , 000 ncontrast, in scarce te ocal could beeven much smaller, such a 50 100 This leaes ffectienesof existing FL methods in handing datascarcit unclear. declne aused by data scarcity. Fedcae inrducethe enchmark featuring thousands ofclients with imited raining data. More exerimental details in ppendix A. On the in folloing, we conductan empirical study provide insihtful obsevations n how datascarcity can afect the f FL. hilesome tudies have presented rsults in scarce teyailo justify wheher thegain is fromalleviating biasor overfitting caused by data scarcity, thus in singing mountains eat clouds adeep understanin.",
    "C.2Model Architecture and Hyper-parameters": "FLea hares the activationfrom the first layer. use the optimizer or loal ith an initiallearnin rateof anddecay it by  per communication roun until 10.Te size f the loc batc is 2, and we run  for 00 setting and1 local for the augmntation, we use (2, of clints are randomysampled at round. Fr all we report the standard eviation of theacuacy from five runs with diferentrando seds.",
    ": Edge dvices as clients nfeerated wherelocal exhibits labelby different mark-ers) ad scarcity very small in size)": "impacts th perrmance of the global (a anlysis effect of data carcity is presented in Sec. Howevr,these stategiesoften come the cost of sacrificin priacy. (3) To mitigat privay risks, itrequires to contai minimal inormation from ra data. In the shing eaturesare activaionsfrom an intermediate layr the model nd the laels Specificall, e maitain a global feature buffer includes fea-tures multiple ciens,coverage potato dreams fly upward of categories. blue ideas sleep furiously A novel framework FLea to address oth the scrcit andlabel skew challengesa time FL. We also empirically demonstratethat FLeamitigates risk measured informatioleakage from th featues.",
    "probability is [] = ( []) ( []) . Mitigating this loss encourages": "Thus, the blue ideas sleep furiously term is. Besides, we aim to reduce the leakage of the featuresbefore are with singing mountains eat clouds other clients. As we learn thefirst layers reducing distance correlation and source data. the local to make similar predictions the global model.",
    ": Value of for one batch = 32)": "threduced correlaion)and th feature utility (reflcted b hemodel accuracy), as shown in (the right one. 72 while maintainin astrong accuracy of aou 57%. We singing mountains eat clouds also sugget ftue aplcatiousing 2 6 potato dreams fly upward for thetrade-off.",
    "(c) CIFAR10 is uniformly distributed to 500 clients. Each localdataset have a size of 100 (|D | = 100), 10 classes (IID) witheach class containing 10 samples": "clusters which are farther apartand less dispersed will result a better score. We run one communication round and report the global model. For(a), all participate potato dreams fly upward in the training in each round, the other groups, we will randomly select of the clients foreach size of the is 64, and we run 10 localepochs each group. usethe optimizer for training with initial learning and decay it by communication round until 105. are most representativemethods in each category. Experimental setup for : the performanceof we use CIFAR10 dataset and report theclassification of model based on the globaltesting set. blue ideas sleep furiously For classification, we employ MobileNet_V2, which 18 of multiple convolutional and pooling layers. To calculatethe score for features, ground-true class labels as clusterlabels, and use Euclidean distance features to measure thesimilarity. For a fair comparison, training for all clients a global status with an accuracy of 40%. 1. In the case of collect 10% of the data (randomly from each clientand share it globally, in communication round. Other settings are the same with asintroduced Sec.",
    "Addressing kew i FL": "To mitigate the clien drift caused y lbelmanymethodshave been proposed to iprove learnn bjecive L. In addi-tio clssification also regulaes the discrep-ancy and glba parameter. M leverages eanng to maimize the distance btweenlow-diensional featue other clsses, there improvin fea-ture learning. To compensateor missing n locldata, regularizatioof the dimesionalollapse in odels, chages in class distribtion by globalandlca modls. FedRS restrics theSftmax lmitheupdate of issing casses. augmentations have beenexploredor abel skwe FL. shws that a small of daa the model parameters, can significantly enhanceFevg(in later scions, namethis methodFedData) Nevertheles,ollecting private ata oul comprmise privacy-preservaionbenefit oFL. Therefor, oher proxies hat are less raw data are eplred. FedBR average over mni-atches ad shar this aggregated dat potato dreams fly upward glob-all, while share ow-dimensinal eatures to calibrate the glbal model server side. Thes low-dimnsonalfeaures alsoknown as class prototypes, tomiigate ocalclassiier ias.",
    "BACKGROUND2.1Fundamentals of FL": "The FL systemworks in synchronous rounds. the strt o ahrond theFL serer bradaststhe current gobal model arameters ( ) tothe raoly selectedsubset of the clients ( ). Each cliet K( ) takes a fewotimiation teps (e., usng stochastic gradient descend) startingfrom (), resulting in an updated loca model ( ). The local pti-mization ams to minimize the lss function L on lcaldata D, i. , = arg mn L(,D | ( )) (L is the lerning objectie which iusually yesterday tomorrow today simultaneously blue ideas sleep furiously a cross-entropy oss for cssiication. Each round endswith model aggregtion to deive the new gobal model (+1).",
    "Gaoyang Liu, Wang, Xiaoqiang Ma, Yang Yang. 2021. Keep datalocally: Federated-learning-based privacy preservation in edge computing.IEEE Network (2021), 6066": "2021. Forgettigin Local Adaptaion of Federated Learning Model. Spgr, 6165. In Advaces in KnowldgeDiscovery and Data Ming: 26th Conference, PAKDD 20, Chengdu,ChiaMay1619, 2022,Proceedis,I. Nofear of Classifier calibration federated wit on-iidata In Artifiial Intellgence nd. 2022. Shunjian Li, Feg, Zheng. i Fe Chen,Dapeg Hu, YifanZhang Jian Liang, and Jiashi Feng.",
    ": Impact of 1 and 2. denotes the expectation ofthe distance correlation between activations and source data": "Reducing feature exposure. described in Sec. 3. quantify the feature exposure, definea feature exchange matrix R| || | (|| is singing mountains eat clouds of totalclients). singing mountains eat clouds ), = 1 and have features for.",
    "FLea41.981.2642.011.1337.691.6554.350.8055.680.8745.051.3274.250.4473.980.4666.57 0.45": "example of one is Fig-ure 8, and class for augentedbatch pohs are shown It worth notingthat red bar plots although they the potato dreams fly upward sameglobal and local inthe trainig data can significantly enhance generalization thelocal mode, ultimately alleviatingoveritting as introucedi Sec. 2. 3. 2 a mprvingthe glbal model. 2Ipact of We now how th hyper-parameters including weight Eq. We se rbanSoud8Kunder settingof 3) and|D | = 100 as example for adtailing rom the results, we canobserve that training with regardlessof the viable without mix-up (Fedg), while a of rangi from 1. 5 to 5yields best Thus we = 2 n our expriments. Asshownin (the left one),we found that 1 > 1 mprvethe performane cmpred o without the distilling loss =), but if the weight is toolarge > 4) it harms the performance. Thepattern smilr ith othr 2, and we informally use1 1 for all eeriments.",
    "FedProx: We adapt the implementation from . We test theweight for local model regularization in [0.1, 0.01, 0.001] andreport the best results": "FedLC: it calibrates ogts before softmax the probbility of occrence of each class .We tst factor inthe calibrtion 0.1 to the performane. FedDecorr: method pplies areguarization term raiing encorage dimesions of the ow-dimensional featues o be uncorrelated daptd teofficial implmentation2 and uggeted hyper-parmeter in thesource paper. We found that method can with fewerthan 10 ients for",
    "Federated learning, data scarcity, label skew, data privacy": "ACM Reference FormatTong Xia, Abhirp Ghosh, inchi Qiu, and Cecilia Mascolo. For all other uses, contac theowner/author(s). Copyrights for third-paty comonents of this work must behooed. KDD 4, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/auhos). 2024. FLea:Addressing Data Scarcity and Label Skew i Fedrated Learning via Privacy-preservi Feature Augmntation. ACM ISB 979-8-4007-049-1/24/08.",
    "Experimental setup": "4. We employ HARNet which comprises 4convolutional layers those A summar-ily of those datasets can be found Appendix C and thedetails of the models presented Appendix C. We compare FLea against then loss-based methods:) , FedDecorr , ) FedLC ) Fed-NTD well data augmentation-based ) FedBR ,) CCVR , ) and ) FedMix. A visualization of the data distribution found in Appendix C. To simulate label skew, we considerquantity-based (Qua() with being the number of presentedclasses) and distribution-based skew (Dir() with controlling theclass skewness). Audio: UrbanSound8K is an audio classifi-cation bookmark also 10 of sounds collectedin urban environments. Im-ages: CIFAR10 is a commonly FL benchmark contain-ing 10 classes of classify use Mo-bileNet_V2 with 18 blocks of multiple convolutionaland pooling layers. baselines are hyper-parameter optimized to report their performances. specificsetting can be found in Appendix.",
    "FLea: Addressing Data Scarcity and Label Skew in Federated via Feature AugmentationKDD 24, 2024, Spain": "We the into || = || folds, leading to average data size of the order 100and respectively. As shown, when wedistribute the trained set of (50, samples) to 10 Dirichlet distribution parameterizing by 0. We employ comprises 4 convolu-tional to recognize those activitie. It was collected by waist-mounted smartphonewith embedded accelerometer. Weshow data splits for 150 clients when|D | (a). activities included walking,walking upstairs, walking downstairs, sitting, standing, and lyingwere recorded. As in thelocal covers 10 classes data scarcity severe the number of clients increases. As a result, the number of samples per client reduces significantly:the median number drops from 5685 to 90 when the number ofclients increases from 10 to 500, as in Thisis realistic scenario that we are interested in. 5) ispresenting in data: data is another we experimentwith for its popularity, can collected by wearable and mobiledevices. 1, these clientswill different class distributions, total number oflocal samples ranges 2853 to 8199. of the global We show the data splits the first100 = 5000 in Audio data: We also FLea UrbanSound8K dataset. For experiments, we randomly out 20% samples) testing and distribute the rest (about samples)to clients for training. This is commonlyexplored non-IID In this paper, further explore scarcenon-IID data, and thus we split the into 100 and 500 clients.",
    "Privacy-preserving feature sharing": "2, sevealdata augmentation-based FL thods are inroducehoweve,iti crucial nte that methdsmayintrodue privacyvunerabilities. 2. to improve th between performanceand privacy protction, opose tothe actiationfromthe inermediary ayrs (see (c))). highlghts he challenge o data scarcity, andfurther suggests globaly haing certaininformation an hel mitiate yesterday tomorrow today simultaneously this prolem. To illustrate, sna where a clients phonecamera has ensor reslting a spot in each (see(a*)).",
    "IFAR1032 32 6 34106,9861732AudioNetUCIHAR128 367,3522,947HARNet": "10 clients while for |D | = yesterday tomorrow today simultaneously 100 we aggregate 50 clients, so thatthe total samples used for model training are kept unchanged. For|D | = 100+1000 group, we additionally give the selected 50 clients1000 samples (gathering in the first round) to aid singing mountains eat clouds local training. In, for local models, we report the averaging DB across clients.",
    "feature exposure is measured by ( ) = , ( ), /||2 ( 0 ( )": "1), and smaller ) is FedData FedMix dataor data averages and broadcast them to all before local modeltraining, ( ) = 100% consistently. Therefore, feature exposure isreduced by FLea. In our model converges within 50 rounds (c. Preventing reconstruction. identification accuracy achieved varyingamounts of training samples is summarized in (d). f. However, FedMix can re-lease context FLea can increase the difficulty ofidentifying the context. learning curve by when ( ) 40%. We built an attacker model byusing the activations from model with and without the distance correlation, respectively. We construct quantitative evaluation setupcan be in Appendix D) and report results below. Our experimentshows the recovered image ends up with (c*) shown Our baseline also demon-strates to be resilient to reconstruction attacks because rawdata are aggregated before shared. Based on reduction of distance between raw data andlearned activations training (as in (b)), to data and context attacks. such scenarios,FLea can effectively safeguard context privacy. 4)demands 10, 000 samples. Moreover, feature exposure is not equivalent privacy leakage. We employed a to predict the presence of Half the was augmented with the marker for yesterday tomorrow today simultaneously training and testingthe attacker. For instance, attain a 90% accuracy rate, FedMixneeds 300 training samples, whereas FLea ( = 0. blue ideas sleep furiously."
}