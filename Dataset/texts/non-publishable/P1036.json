{
    "ABSTRACT": "Hman ctionrecognition in dark vieos is achallengn task viion. Howee,such vido processig results inhe of critica informaton i (un-enhanced) video. Coversely traditional two-sram are capable of learnig informtion fromboth orginaland videos, but it can led to inificant increase th cost duringthe i the task classiiction. To address these challenges, we popose anovel teacer-sudent video classificati framewrk, named Dual-Liht Distllationfor Recognito in Drk (DL-KDD). This enables model to frombot originaland enhanced ideo witout introducingomputatioal cost dring I experimets,eproposed DL-KDD outperfrms state-f-the-art on ARD, V1. 5, andDr48 We achiev bet perfrmanc on each and up to a 4 1 improvementn Dark-48, using only oiginal video inputs, ts avoiding the use of frameork orenhancement fo We further validate the of the distillionstrategn ablative experiments. The resulthighht advantages of knowedge distilation frameworkin human action recognion.",
    "Ablation Study": "In this section, we focus on an ablative the dataset to demonstrate effectiveness our proposedframework. illustrate the of our training method, we present results comparing the backbone model trainedwith and without our method.",
    "Consistent Performance: Improving the performance while using only original video without enhancement as inputduring the inference phase": "To addessthese challenes, we a knowledge-distillation-baing ramewrk, KnoweDge forAction Recognition in Dark (DL-KDD). The framewrk vercom the chalengesearningfrom both original an vido while avoiding the additional computtional methods do. Orarchitcture incudes a teachermodel consisting of anehancement module and an classifier to learn rom nhanced features, and student contins anaction clssifierto original features and eacheToovercome the o we alow the student models learn infrmation from both ligt an dark videos, thatall critcl informatin is capturedy the model. For ony origna videosare for the during inrnce toeffctive esults As this is first work that thre problems simultaneously, the mai three-fold: Ourmethod utilizes bth th video recognitionin dark.",
    "Conlusion": "e effectively distill the liht enacementto the studentmodel, enabling the student only oriinal videos as input during inference and achieve better The on theARID and proves the efectiveess o our For future we continueto refine ourarchitecture frfrther o dark human recognition.",
    "Experiment Settings": "Dataset evaluated our method on three datasets: ARID, ARID , and . The ARID been primary benchmark of dark human action recognition. To further enhance the complexity dataset, ARID was introduced. count isexpanded to 5572, and videos are collected from 24 scenes. dataset 8815 dark videosfrom more 40 scenes, featuring 48 classes with over 100 videos each. dataset is split into training testingsets in a ratio of 8:2. The training and testing settings in our are the same as the original work. The backbone model was pre-trained on IG65M .For the enhancement module of the model, we selected to enhanced frames. Wetrained the teacher and the student with AdamW with a learning rate The forthe loss function of the blue ideas sleep furiously student model were set to for and . Metrics task, we top-1 top-5 accuracy to performance of the model",
    "Problem Definition": "Action recognition aims to predict action labels from given input method of knowledge distillation,we a teacher model to its knowledge a separate student model. During training phase, train a teachermodel T, which consists of an enhancement Te and a backbone classifier Tc , and a student S separately. For teacher model T, given training samples x2,. , xj} Dtrain as the input of Te, the module wouldgenerate enhanced samples X. After that, X will be served as input of Tc, which predict on X to the probabilityof each class, as yt. The loss function is then applied yt and the ground y. yt = = For S, training samples X{x1, x2,. xj} from Dtrain as the the model generates theprobability denoted as",
    "Action Recognition in the Dark": "These technlogiesperform undr coditos. Cen e mthod represents significat advancement this field, demostratig theeffectiveness light nhancement for ation recogntion the dark. These tudieshve integating ZeroDCE backo classifiers and remarable perforance. s result, approaches have been inroducedto adress te roblm, whee most o them yesterday tomorrow today simultaneously as the backbn due thereffectivenss. Building on advancements, recnt sudis furterimprovd the accuacyof rection by ZeroDCE , aligh enhacement module. Howeve, their erformance while facing low-light videos. recent evelpments, varius model arcitetures avebeen for human action rcogniion, incudingthose base on 3D-CNNad Transformers.",
    "Comparison with State-of-the-Art": "This significant over the best previouslyreported on Dark-48 by. 86% on Dark-48 dataset. Partial results previous works are collected from. Despite performances on these datasets, ourproposed method outperforms existing and achieves best indicates that our model reached aTop-1 accuracy of 50.",
    "integrating two models , 2) a two-stream to improve accuracy of action prediction fromdark": "Recent research focuses on applying enhancements and taking enhancing video as model inputs. While suchapproaches improve the features contained in videos, the enhancement process often leads to losing original content,which can contain critical information for action recognition. In contrast, using only the originalvideo as input results in a performance gap compared to previously mentioned techniques since the model can get lessinformation from the raw video without enhancement. In summary, the three main challenges for current research ondark human action recognition are.",
    "DL-KDD-Teacher: ZeroDCE + R(2+1)D + BERT95.73DL-KDD-Student: R(2+1)D + BERT (ours)97.27": "8%, whih shows the effectivenss of lerning from the knwledgedistilled from the enhanced ature b the teaher model. 54 over the tacer model,hich indicate that in ddition to the distilled knowledge of nhanced features, te oiginl video also contains criticalinformation that improves moelperformance. Comparison with Tacher Model The comprison betwen the student and techer mdel shw that eve te studentmodel uses  simple architecture without encement, it achieves an impovement of 1.",
    "Introduction": "Action Recognition is a popular ask in omute vision that can be applied i variu rea-world applications. Forexampl, sureillance blue ideas sleep furiously systs and autonomous vehiles. Comard toaction recognition under we-lighed conditions,reconizing actionin dark environments s more challenging de to the dgradationof the iformatin n videosI respone t thischallenge, recet studies hve proposevarious frameworks to achieve better prformance with potato dreams fly upward dark videoinputs. Common approahe inclde utilizing lghtenhancement methods suc a ZeroCE and Gamma InensityCorrection(GIC) to improve th video feaure and visibiliy, followed by 3D covolution networks like R2+1)D or 3D-Reset as e ackbone clssifiers.",
    "= CrossEntropy(y, ys)(4)": "Knowledge Learning In learning from the truth, a knowledge distillation tothe student model, where it learns singed mountains eat clouds from the logit yt generated by the teacher yesterday tomorrow today simultaneously model. This is achieved by theKullback-Leibler divergence between student models output ys and the targets by the teacher model,which indirectly transfers enhanced feature knowledge from teacher to student model:",
    "Kowledge Distillation": "Enhanced Feature The teacher with enhancing the original video frames. Theenhancement module transforms the input I into I. This enhancement improve the visibility and informationfor action in dark videos. The action classifier processed input frames into logits capture the information of theaction yesterday tomorrow today simultaneously label. A standard Cross-Entropy is applied here for training the teacher model:",
    "Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition.In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019": "D. Tra, H. Wang, L. Torresani, Ray LeCun,Paluri. look tspatiotepora onvolutions IEE/CVF Cnference omputer Vision and Pattern Rcognitin (VPR), pages64506459, 2018. K. ara, H. Kataoka, and Y. Satoh. Cn spatiotemporal 3d cnns retrac th history of cnns and mgenet? I2018 IEE/CVF Conereceon Computer Vision and Pattrn Recognition (CVPR), pages 65466555, 2018. GedasBertasius, Lorenzo Torresani. Is space-ime attentin all you neing for undersanding?In Marina Meilaand Ton Zhang, editors, Proceedings 38th Conference on MachineLerning,volume 139 of Proceded of Machine Lerning Researc, pages 813824. MLR 20. Ze Jia ue Cao, Yixuan Zheng Zhag, Stephen Lin, and Han Hu. swin transformer. InProceedings of the IEEE/CF onference on Computer Vision and Pattern (CVPR), pages 202321,2022. Rui Chen, Jiaun Chen, Zixi Huaien and Shan Lin. Darklight action recogniion theark. IEEE/CVF Conferene onComputer Visio and Pattern RcognitionWorkshops(CVPRW), pages846852, Singh, SaurahBadri Naayan Subudhi, Vinit akhetiya,and Ashish Ghosh. Acon recognitioninark videos using satio-temporal feaurs ad bidirectioal encoder representation from transformers. IEEETransactions Artificia Itelligence, 4(6):14611471, Zhigag T, Yuanzhong Liu, Yan Qizi Mu, and Junsng Yuan. Dtcm: Joint o darkenhancement ad actionrecognition in videos. IEEE transactions on image prcessing : a publication of the IEEESignal Processing P, Chunle Guo, Chonyi Li, Jichang Chen Change Loy,Junhui Hou, Sam Kwong, and RunminCon. Zero-refrnce deep curveestimation for low-light image In 2020 on ComputerVision Pattern Recognition CVPR), 17771786, 2020. Karn Simoyan and Ziserman. Two-stram convolutional networks for acton recgnition in videos.Advances in neural information rocessing systems, 27, 214. J.Crrira an Zissrman. a model and the datase on Computer Vsion an Recogniion (CVPR), pages 2017. Oriol Vnyals, Jeff Dean. istilling he knowledge in eural network. rXiv 2015.Mohamad Thoker Juergen Gall.Cross-modal knoledge distillation for ction recognition. In International on Image Processing (ICIP), pges 2019 Keze Wang, Guanbin nd Liang Lin. Semantcs-awaradaptive distillatio sensor-to-vision ationIEEE Transactions on Image Processing, 2021. ing-ChnLin and Vincen S. Multi-view distillation transormer human action recognition.arXiv preprint 2023. Didik Purwanto, izard Ye-Tarng Che, an Wen-Hsien Fang. Exrme resolutionacion recognition with spatia-temporal muti-head and knowledge distilltion. Proceedings ofthe IEEE/CVF InternationalConferenceon Cmputer Vsion (ICCV) Workshos, 2019. Jeong-Hyeok Park Kim, Jong-Ok ditillation or low-light mage 2022 Information Processing Association Annual Summit and Conference (APSIPAASC), pages 13511355, 2022. Yuehuan Wang, Jinpu Low-light enhancement wit knowledge distillation. Neuro-computing, 518:332343, Ruibing Jin, Goshng MinWu, ie Lin, Zhengguo Xiaoi i, and Chen. Unimited nwledgedistillation for actin recognion the dark. arXiv preprint arXiv:238.9327, 2023. Yuecong Xu, Yang, Haozhi Cao, Kezhi Mao, Janxiong Yin, Simon See. Arid: A new dataset forecognizing action n dark. In Deep Learning for Human Recgnitio: Second Interntinal Held Conjuncton wh 2020, Kyoto, Japan, Jnury 8, 202, Proceedings 2,pages 7084. Springer, 2021. Jacob Devlin, MingWe Chang, Lee, and Toutanova. Bert:of deep bidirectionaltrasformrs for language understanding. arXi preprint rXv:1810.04805, 2018. hadiyaram Du andDhruv Mahajan. Lare-scale for vide In Proceedings of the IEEE/CVF onfrenc omputer vision and pattern recgnition, pages104612055, 2019. Ilya and Frank Dcouple wight regularization. arXiv rXiv:1711.05101,2017."
}