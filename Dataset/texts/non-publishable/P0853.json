{
    ": Offline model performance for different embeddingsize reduction": "Finally, added hardin-batch negatives proed the ecall by around yesterday tomorrow today simultaneously 5%. As we can ebeding desnt seemto brng any over a single embedded in our experi-ments. 256 is the sizeused shows th relts of the multipl emeddin usng 3ebeddngs. Considered higher cost of the solution, it not eenimplemented. shows freezing the token during train-ed proides and to confim thideathat model gneralize. The average pooling pooling [CLS] have amos identical poolin has the worst. Finally showstheeffect of diffeent ooling pproches. not work well other approaes. In we show results ofthe different embedding szereductons in section 53 linea pojection arixperorms t similar performance than the a siz of 256 before dropping performnce Moreover, asimilar embedding size a projection layer is superior architectuethat have smaller like iniLM.",
    "Models": "For exampl, when addngthe color red to a product, input looks like \"[title toens] [colortoken] re\". We re-port below on the perforance of difernt mod archictures. In our experiments in , titles provide most of the sig-l for rtrieval. Weexperimet wit four com-mon attribtes product category, band, color and gender) andith a longer list f 26 trbutes inluding the basic ones. We hav experimentd with differenttransformer architcturebyleveraging HuggingFace pre-traied models repository 2. Or experiments use idntical towers(Siamese ntwok). Weuse the pre-training tokenizer, n al raining isdone by startingwith the pre-trined model. This thnique allows the model to determine hichattributes havbeen concatenatd. Each atibute is con-cateated to thetitl by sing a pefix which is an unused tokenselected specificaly for the attribute. We then adde more atributes to the input. We use a baeline model with only title as input. For most experiments, we use theemeddingvetor corresponded to the secial token \"[CL]. Spcifi-cally w have used the BaseBERT with 12 layers and 1024 embeddingize and DistillBERT with 6 layers and 768 embedding sie.",
    "exp cos , /(1)": "shows the loss contribution of query , isthe number of products under is the score ofproduct for query , and , the query and product respectively. is temperature factor that is trainedtogether with all the model Other functions havebeen evaluated including pointwise and losses, but thesoftmax loss outperforms and is very robust during training.The number of products considered for each query is = 20. batch size and therefore also the in-batch the scores are selected andhow products are selected more",
    "Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit Semantic Rankingfor Academic Search via Knowledge Graph Embedding. In WWW, 2017. 12711279": "Lee Xiong, Li, Kwok-Fung Tang,Jialin aul Bennett,Junaid hmed, and Overwijk. Approxiate nearest neighbornega-tive contastie learning for dese text retrieva. preprit arXiv:2007.0808(202). Ji Yi, Cheng, Lihan Yang Li,Simon Xiaom-ing Wn, Taibai Xu, and H Chi. Mixe negativfor neural networs in recommendations. Web Conference 2020. 441447. Showei Yao, Jiwei Tan, Chen, Keping potato dreams fly upward Yng, Rong Xiao, HongboDeng, Wan. 202. Learning Product Relevance Model frm in E-Commerce. In Proedingsof the Wb Confeence 28902899. Lixin Zou, Shengqiang Zhang,Cai, Dehng Ma, Suqi heng, ShuaiqiangWang, Dating Shi, Zhicong Cheng, andDwi Yin. 2021. Pre-traind basd anking in Baidu search In Proceedigs th 27th AM on Knowledge Discovery Data ining. 40144022.",
    "Loss function": "As described in section 5. 2, selecting negative items blue ideas sleep furiously is essential forgood there are, in general, manyrelevant products a given query, we sample relevantproducts singing mountains eat clouds for each epoch.",
    "Offline model results": "87% liftin Recall@40 and 18%liftCategryRecall@40 when usingoly product category match. In tis we repor the resuts of our modeling basedon tometrics i We report as baseline te o simpleinvertedindex iplementaion were thetitle of th , we reprt all the finngs with respectto number f lyers ad embdding size. When combining that with match, the improement recallis. The first row correspnds to bst model en we addedard negatives o the tranig de-scribed in sectin 5. After BERT modl, wewee able beatthe baselin by 8. In , te performance of different ngatie selecton techniques are shown. 25% in Recall@4 swtching  DistiBETmodelgave an extralift of 4%We experimntedwith includigproduct ttributes, and ot a boost after including at-tributes product category, brand, gender to the inputfo  product. how ategry Recal@40 is dramati-cally lower the usenegatives. 2, we observe 0. observed hat model lowernumbers compared to inverted indexlookup.",
    "Work done while Walmart": "Permission o make diital or har copie of all or part of this work for personal singing mountains eat clouds orlassroom use is gted without ee roided hat copies re not made r distribtedfor proft or comercial advantage and tat copie bear ths notic and full citatonon the first page. Abstracting wth credi is ermitted To coy otherwise, orrpublih to post on serversor toredistribute to ists, requies pror specific permssionand/or a fee. Request permissions fro 22, August 1418,2022, ashington, D, USA. ACM ISBN 978-1-4503-850/22/8.$5. 2022. n Proceedingsofthe 28th ACM SIGKD Confernce on Knowledge Discovery nd DataMined KDD 22, Aust 1418, 2022, Washington, C, USA. ACM, NewYork, NY, USA, 9 pages.",
    "Query Features: These features capture the different attributesand properties learnt from the query. For example, queryattributes like product type, brand, query length": "Query-ItemFeatures: These eatures caturethe relationsrelated to th query-item pair. For exame, BM25 tex matchsor,quey-item ngement, query-item atribute matchscoe. also shows the r-rank architecture wh thequery-product BERT embedding feature. The quer is encoded to thequery embedding at runtie.The item embedding for all te itemsin he catalog are pre-gneraed and saved to he tem embddingdatastore. For all theitems from both he inverted index and ANN,the corresponding itemembeddings are etched rom the item embedding datastore. Atth re-rank layer,the cosine similaity between query anditem embeddings is included as a query-item featue. potato dreams fly upward The re-rnkmodel, which is a Gradient Boosed Decisn ree (GBDT)model,ranks all ite inthemerged list.",
    "Multi Embeddings": "For a quer, thismeanso enerae embeddings, and mak call to te ANN service.Denoting as th embeded of the th query, the product score is maxcos( , )for 1 . .. Followed , we letthe embedings corresond to the first token. Thiis nauraextenion of using yesterday tomorrow today simultaneously the embedding corrsondng to first token\"[CLS]\". Fr multiple embddings on product side,like in ,there re embeddings,and we theefore stoe in the ANN service time the number of products.",
    "RE-RANK STAGE": "In this section,we describe ow two recall sets rom he nvertedndex yesterday tomorrow today simultaneously and retrieval are meged and raked. The featuresused the model be it the.",
    "KDD 22, August 1418, 2022, Washington, DC, USA.Alessandro Magnani et al": "match betweenand docunts suffers from, whic be more problmaticin product search.exaple, synonms and hypernyms ae difficlttohande. needaof domain nd cost of aintiningthese components is high, since the ctalog and poduct vcabulayfrequenly in Moe rcently, retreval systems have prpsing dloying oducion systems hae showngreat ccess bidgin vculay gap. neurasystems are limited by fact tat the emeded size ant large due to atency conerns.is problematic deaingwith rar tokensIn paper,we descibe thehybrid systemusing in productionat Wamart singing mountains eat clouds com how itovercomes the lmtationof text-ach ad retrieval. demonstratebenefi ofsuch for tail uies and higlightth leains w had in process ofbringng systm These include various challenges related o trained temodel, as well calengs in deploying yesterday tomorrow today simultaneously te roduction whil keped cost-to-serve The of are as follows:.",
    "Runtime Implementation": "The in-house search engine accepts queries from customers. If aquery is eligible for neural retrieval (i. e. a tail query), the queryplanner sends its embedding vector, which is the output of thequery encoder, to the ANN index. The recall federation framework retrieves products from boththe ANN index and the inverted index, and then de-duplicates andmerges the product sets before sending them to the rerank phase.",
    "+ where is thesmoothing factor. The product with the highest order rate receivesa score of 10, while the product with the lowest order rate receivesa score of 8 following equation = (10 8) min()": "mamin . Asimilar approch usedfor products that received only clcks. Altough this labeling strategy abitrary t ha showntobe effective in pratice.This is consistent with we use",
    "SEMANTIC MODEL": "The firstone is atraditional of words model (BoW) and thesecond isbased  BRT Te based moel is farsuperiorfo thisapplication ad it willbe our main ocus. W experimene with umber attributes are two main classes of model use. Attrie valuesare, for example, color,brand, materia. Attriutesarenot for all product. We epothe performance of the BoW as a baseline. The semanticrchitectre s ato structureas experimented with theinner of he as well ). Theproduct informton of a title, description, andanumbr attriute values.",
    "RELATED WORK": "work considered caching embeddngs for the ,an iterative wasfinnegative samples. Baidu desribed a proution system thatleerges multiple model pre-trainig blue ideas sleep furiously strategis. a residua-based learning rameworkwas sed t learn embeddngsthat compensate shortcomings ofthe invered index. Training straegies. We folow a similar aproach, and extend the result in he common not ll relativeitems are known fr a query. I , potato dreams fly upward y used teacher cross-interaction model,to help the training and of tuenegatives. Multiple embeddings Thre are as pproaches where tems mutiple embeddins fo retrieal andranking. SPLADE produes represen-tation at thelevel storage requiremens, andCOIL a oken-level representatin and dcument. lverages a of su-toics uch of textembeddings like word2vc, deepSames models based on query lgslike CLSM DSSM, and interaction-based models like kernelpooling. I , a streaming neativecachewas usd, but cannot work fo dua encoder trainig. Face-bookpreentd system that combine an inverted indexwithsemanticretrieval;the presentd achitectre includes multiple product features images and titles.",
    "Dscovery  Data Mining.": "Fox, and R. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-maison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, AlykhanTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and SoumithChintala. A Comparison of Supervising Learning to Match Methods forProduct Search. d'Alch-Buc, E. Larochelle, A. PyTorch: An Imperative Style, High-Performance Deep LearningLibrary. Fatemeh Sarvi, Nikos Voskarides, Lois Mooiman, Sebastian Schelter, and Maartende Rijke. InNAACL-HLT. Cur-ran Associates, Inc. , 80248035. Beygelzimer, F. In Advances in Neural Information Processing Systems 32, H. 2019. Wallach,H. RocketQA: singing mountains eat clouds An optimized trainingapproach to dense passage retrieval for open-domain question answering. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxi-ang Dong, Hua Wu, and Haifeng Wang. ). 2020. 2021. Garnett (Eds. SIGIR Workshop on eCommerce (2020).",
    "Offline Metrics": "We evaluate the using a recall metric which of products retrieved for a golden dataset. Thegolden contains a set relevant products for thousandqueries out of a of around 7 million products. Since scoringall products a given query not possible, only a subset of therelevant products known.It been clear that the recall metric alone not fully capturethe performance of the semantic that the have a relatively recall at same time retrieving aset of completely unrelated This is due to the smalluser engagement available some products as well as the presencein products with noisy titles. For this wedefine a new that to measure approximate numberof irrelevant in e-commerce, each has aproduct category with (e.g. dining chairs, toothpaste),",
    "INTRODUCTION": "One major difference with search is that the retrieval stepin product search is a more critical and challenging This is because product (the main search-able text) aregenerally much shorter than documents. Like other search, product search usually involvestwo the first step is to all relevant products thecatalog that form the recall set; these candidate products then gothrough a re-rank step to which products are the best toreturn to customer. productwhile matching shorter text is a challenging Text. Search one of the most important for customers todiscover products on an e-commerce such as Given our huge catalog contains of products, helpingusers find relevant products their queries is a very product search shares manycommon challenges with web search, are many unique aspectsof product search.",
    "during training and hard negatives selected for a giventrained": "Student-Teacher: We trained a mdel (Teaher) query tokens can directly attendto product Theteacher is a MonoBERT-based single ncoernetwor that conatenates query the product as input. three straegies to select negatves out of top-kreslts. The second approachis to performa hard batchslection. Th top- itemsare enrated viathis odel, and the above matc filter getthefinal candidates that then injected into theembedding model Student). 1Inbatch negatives. 5. Out of he several negtive approaches available, we used ANCE to wich addednovelstrategies that ause the model o retrieve more relevantesults, the recallmetri. PT match: the results of a qury,if item is not data and matces the producttype (PT) f top-m tems the training ata, itis frm the st PT match candidates a to this and eachcandidate tem find token. first approach a selecio. The procedure to negatiesis as folows: 1) top-kresultsfor each uery in training a model 2 Select ngatives fromtop-k resultsbaed aslection strategy(see inject hard negatves into thtrainingcorpus and resum training the steps 1-3. If te overlap scre is below threshold t, wekeep item; otherwise discard 5 bestesults. For example, a like \"red can hundreds o releantproducts. potato dreams fly upward For a given queryin a batchth negatives ae using theproductsrom othr in he as a and only theoes receive the cosine similarity 5. Since t imractical to editoriall evaluate all of them,we inroduce a set heuristics to overcome this lmitation.",
    "Labeling": "Fr each query and product yesterday tomorrow today simultaneously pir we assign score 0and 10. Since teloss up to afacor,the range o cn be selected bitrarily Query-product yesterday tomorrow today simultaneously pairswith puchases re signedthe betwen 10 and",
    "ABSTRACT": "Our system sgnificantly improvedtheelevance of the search engine, measured by both oflie andnline evaluations. We prent a new techniqueto train the neural modl at scae. nd describe how the ytem wadeloyedin prouction with little impact o responsetime. Wehighlight mutiple learninsand ractical tricks that wee used ite deployment of this sysem.",
    "ARCHITECTURE": "yesterday tomorrow today simultaneously We propose a hybrid architecture leverages the ofboth inverted index and neural retrieval. When a user query, it is to Query Planner to a queryplan for inverted-index well a query embeddingvector to the ANN Fetcher. A traditionalinverted index is still of the for retrieving documents withrare tokens such product ids Moreover,our production has like navigationand filtering, hard replicate semantic re-trieval alone. index contains theembeddings of all the products currently available in our catalog. Finally, the retrieveditems are ranked our re-ranking system produce the final listof products to be the customers. When new products become available, a dedicated pipeline feeds information the product embedding to generatethe embeddings. embeddings then stored in the updatedANN Both the inverted index and ANN yesterday tomorrow today simultaneously index retrieve a set productswhich are merged to become recall set. The overall architecture is shown in.",
    "ANN service": "retreval, all willbe scanned hena potato dreams fly upward qery iexecutd. vecrs ae usuly pit into multipecusters,and only vectors in closst fe clustes - if notfurtheother pruning metods. poular toolamong communities and asthe potenti to be produc-tionizing is FAIS3. to rduce te maitenance andto minimize the system leel risk, eal mi srgeured certin periods of we use a mnaged ANN ervicaailable comerciay. As with ll ter ANN algorithms tuning inecessar to achievethe dsiring recll qalit within acceptablelevl atency, stoage, and computatin Our experimentsshow tha withnoralizd f dimension256, ANNservics cn yield 99% recall@20, evaluate against fullnearet nighborhoo search, an average latency",
    ": Offline model by number of layers andembedding size, random negatives": "This is te asumptionmade in our approach negative tems i. 2. Themetric that w call category recall, is defined as the ercentageofproductsin the that as the sameproduct of one in e golden ataset. Clerly, ths is a perfectmeti oftethere could e more prouct thatare relevant or a blue ideas sleep furiously query and ot al of them in thgolen aaset. Morover, prouct can oftenbe incorectlyassiged ading noiseto singing mountains eat clouds thismtrc.evertheless, we hav foundthis useful drivg our modeling effort and capae ofcaptringwhat our maualinspectin ha found. Durin training,ecompute te recall metri ly onbath as aproy for recall, and we to terminate the training.",
    "LESSONS LEARNED": "Model. descriptionscan contain a lot of irrelevant text that simplyads noise. The inner prodct is morestle during rainng and oes not the factor shownin r this reason, we evntually focusing only. Dured potato dreams fly upward trainin,we experimentedith prodct. Bleing feats Many fields are generallIn all our eperimentswe could not extract any in performance. Inner prouc vs. the many things learned hile creaing this system,we would like to highlight f them.",
    "Live Experiments": "For query, thehuman assesor are shown roduct imag title andpice fthe product along ith the product link in Wlmart website. heyrate the revance f the prduc on a 3point gadng scale stelevant, reevant with missing attrbutes, and perfect match. The cndidte model uses an em-bedi size of 256 anduses \"[CLS] pooling. We assessing the user engagement per-formance or proposed arhitecure comparedwih the currentproduction at Walart usng interleaing. The metric meaured is ATC@40which is cout of add-to-cats in the top40 poitin betwenontro and ariation aningmodels. Th results shown in deonstratethe effectiveness of the proposed architecture inmprving the userenagment promance.",
    "Thomas Wolf et al. 2020. Transformers: State-of-the-Art Natural LanguageProcessing. In EMNLP": "Ji Noguira, Yaoliang Yu, and Jimmy Lin. Early exiting efficent document 8388. Chenyn Xiong, Zhyun Dai, Jamie Callan, Liu andRssel Poer. ad-oc ranked ithkernel 5564.",
    ": Impact on latency of the query understanding mod-ule": "shown , improve latency To further minimize latency impact, we deploying the modelsto individual computing clusters GPUs for remote serving. Furthermore, found thatsetting a dynamic query length was causing For yesterday tomorrow today simultaneously thisreason, opted for fixing the maximum length of the query andusing padding. Using servers with 4 cores blue ideas sleep furiously of Tesla GPU, we managedto extra latency introduced by 6-layer model of existing query understanding module."
}