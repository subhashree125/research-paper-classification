{
    "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for bench-marking machine learning algorithms. ArXiv, abs/1708.07747, 2017. URL": "Satzilla2012:Improved selection based cost-sensitive classification models. of SATChallenge, pp. Feng Xue, He, Yuan Zhang, Chuanlong Xie, Zhenguo Li, and Falong Tan. Enhancing the powerof ood detection via sample-aware model In of the IEEE/CVF Computer Vision and Pattern Recognition, 2024.",
    "Hadi S Jomaa, Lars Schmidt-Thieme, and Josif Grabocka. Dataset2vec: Learning dataset meta-features. Data Mining and Knowledge Discovery, 35(3):964985, 2021": "Serdar Kadioglu, Yuri Malitsky, Sellmann, Kevin Tierney. Isac - instance-specific algo-rithm In ECAI, volume 215 of Frontiers in Artificial and IOS Press, 2010. URL #KadiogluMST10. Kirchheim, Marco and Ortmeier.Pytorch-ood: A library for based pytorch. Proceedings of the IEEE/CVF Conference on Com-puter Vision Pattern Recognition (CVPR) pp. 43514360, 2022.",
    "First OOD Detection Model Selection Framework. We introduce the first meta-learning-basedframework for zero-shot OOD detection model selection without training and evaluation": "We uselanguage quantify the similarity aong OOD tasks, facilitated a understandinof OODcharacteristics ehaning OOD detetion moel proposed MetaOD outperforms eleven ell-know elecion methodsand nspervised n a testbedwth 2 unique data pairs. average ank, efficient wit small runtime.",
    "DATA MOEL EMBEDDINGS": "Meta-features understanding the nuances of various tasks guide the configuration of models for new tasks by drawing encountered tasks. These aim to identify similarities between data in the database and testdata, enhancing the model selection process based on prior successful To this,we categorize the extracted meta-features into statistical and categories. of are attributes used in and feature engineering to higher-level information about a dataset or its characteristics(Vanschoren, 2018). New Approach: Data Embedding via Language Models. In thisstudy, various models to produce the language. In contrast, leveraging language offersa transformative for generated data embeddings (Peng et al. This manual approach to feature engi-neering be labor-intensive and capture nuancing relationships within data,potentially affecting the efficacy of model selection. Instead of directly using with arbitrary wehope to generate embeddings that can effectively represent the and help models thatcan adapt rapidly to new tasks. this we two types of meta-features and (method) one-hot encoding embedding, and (2) language modelbased approach to get data and model embedding language models. Data and model embeddings, as inputs of offer compact, standardizing context about the data andmodels utilized in the meta-learning process. In this work, we first that capture OOD across different datasetpairs. These models havebeen recently utilized embeddings textual descriptions and methods,captured essential information the datasets methods intrinsic properties due comprehensive et 2019). These features, from evaluat-ing simple/fast models, offer a of dataset and approximate the sophisticated models in specific For OOD detection, our landmarkers focus on fea-tures related to Softmax probability outputs that can be generated without model descriptions and a list of our OOD meta-features in Table A. Classical Data Embedding Meta-Features and Model Embeddings via En-coding. These include basic statistics, as the and maximum values well as outputs or performance metrics preliminary models on dataset. features summarize the performance of specific learning algorithms on a dataset, pro-viding quick and effective algorithm performance. Given that language are to pro-cess text we hypothesize that utilized descriptions of as input to a language modelfor embedding generation could effectively encapsulate the inherent features of data. , 2024). For image datasets utilized in OOD, incorporate meta-features reflect pertinent imagecharacteristics (Aguiar 2019), such as: (i) Color-based: Simple statistical measures (ii) Border-based: Statistical measures applying border-detector filters; (iii)Histogram: Statistics from histograms of color and Texture features: de-riving from images texture, the co-occurrence and Fast Fourier Transform. Statistical meta-features capture underlyed data distributions characteristics, including vari-ance, covariance, and other relevant statistics of individual features and their combina-tions. often handcrafted andheuristic, have limitations and adaptability.",
    "M arg maxMjMPnew,j,wherePnew,j = f(Emetanew , Emodelj)(2)": "The (online) mdel seletion steps are given in and Appx lgo. Secifically, or a new dataset pair, we th prediting relative performance of differ-ent OOD detection ethods used tained f, and seect the method1, asin E. It is impotant to not procedure zero-shot, ad does notrequire any training onthesampe. (2). 2.",
    ": OOD detection models in this study": "We construct the ID-OOD dataset pair, singing mountains eat clouds andset the training and tested set as follows: (i)Training: CIFAR10 from ID and OOD fromthe classic OOD group shown above; and (ii)Testing: CIFAR100, ImageNet, and Fashion-MNIST from ID, and OOD from large-scaleOOD dataset group. The other ID datasets un-dergo similar train test split, which resultsin 24 unique testing pairs.",
    "EXPERIMENTS": "Our exeriments thefollowing reseach (RQ): (4.3): How effectve is MetaOOD in model seection in potato dreams fly upward coparison other leadingbase-lines? RQ2 different design choices in MetaOO impac its effectiveness? R3(4.4.2): How much time overhead/sang MetaOOD brings blue ideas sleep furiously to OOD detection in general?",
    "RELATED WORK": "We thus include these algorithms as our baselines in 4. , 2017). , 2010; Nikolic et al. While the first approach primarily relies on heuristicmethods and the latter can be hindered by the slow pace of model training, we adopt language em-beddings to represent data for this OOD detection model selection task. , 2013; Xu et al. Within theseunsupervised OOD detection models, some recent works focus on actively choosing sample-wisedetectors (e. Model selection in supervised learning encompasses a variety of strategies,including cross-validation, grid search, and performance metrics such as accuracy and F1 score toassess model efficacy. However, it is notable that such methods donot apply to unsupervised OOD model selection, as ground truth values are absent. , 2021), and consequently, unsupervised methods areexpected to be more suitable for selecting OOD detection models (Liu et al. 2. These methods are particularly valuable in scenarios where generalizationto unseen data is critical. Another direction is similarity-based methods, where the model se-lection is based on the similarity or cluster of the dataset, which has also been utilized for algorithmrecommendation (Kadioglu et al. , choosing different groups of detectors for different input samples) (Xue et al. Most existing methods rely on trial-and-error or empirical heuristics. , 2024),while it is not doing actual model selection at the single model level as this work tries to address. In this study, the traditionalmeta-feature approach is implemented as well for comparison and evaluation. , 2012; Misir & Sebag, 2017). 3REPRESENT DATASETS AND MODELS AS EMBEDDINGS FOR META-LEARNINGIn terms of data representation, especially when it comes to meta-learning, embeddings play an im-portant role to measure dataset/task similarity. , 2018b). For instance, the simplest ap-proach may be not to actively select a model, but just use popular OOD detection models likeMaximum Softmax (Hendrycks & Gimpel, 2017) and ODIN (Liang et al.",
    "ABSTRACT": "s a meta-learningapproah, MtaOOD leverag histoial performanc dat of existin mehdscross variou benchmak OOD datasets, enablngthe effectiv election of suit-able mode for ne datasets withut theneed for labeled daa at he test tme. Desite availabiliy of numerus OO detection mehods, thchallengef selecting an optimal model fo divese tasks reins largey under-explore, especally i scearios lackin round tuth labls. Howan we automatically seect an ot-o-distruton (OOD detectonmodelfo arious unlyig tasks?his is crucal for maintained therlibility oopen-orl appications by identiyed dta distribution hfs, particularly in crt-ical domains suchas online ansactions, autonoous driing, and real-time pa-tient dignosis. In this work, we in-trodueMetaOD, the firt zero-shot, unsupervisedraeworkthat utilizs meta-learning to select an OOD detecton model automatically. Throuh extensiveeperimentatin with 24 unique test datastpairs tochoose from among 11 OOD detecton moes, we emonstrate tat theMetaOOD significntly outperformsexisting methos and nly brings marginaltime overhead ur results, validting by Wilcoxon statistical tests,show thatMetaOOD surasses a diverse grou of 1baselinesincludingestablshedOODdetectors and advanced unsupervised eltionethods. oquanif task similaiies more accuately, weintrodue languagemol-basedembeddings that capture the disinctive OOD harateristics of both datset nddetectin odels.",
    "Hardware. consistency, all models are using the pytorch-ood library (Kirchheim et al.,2022) on NVIDIA RTX 6000 Ada, RAM workstations": "Traiing he meta-predictor f (see detals in 3. () NCF (He et al. (c) Optimztion-based meta-learnes hich involves a learning process: (9) ALORS(Misir &Sebag, 207) factorizes theprf. GB does not use any meta-features. (b) Simple meta-learners th do notinvolve optimitio: (5)Global Best (B) s the simpestmeta-learner that slects the model with the largest average performance acrossall met-traindatasets. As such, M does not perfom model selection but rather uses al he models. 3. Cleary, thebest ank is1, and the orst is 12 (i. (2) ODIN (Liang et al ,017) alies tempertu scaling and sall prturbationsto the input data, hich helps to amplify the difference in softmax cores between ID ad OODsampls. 6) ISAC (Kadioglu et al. 1. Meanwhile we use thedata ndmde embeddins generate by the pe-traied BERT-basedallmpnt-base-v2 moel byHuggingFae (Rimers & Gurevch, 2019). , 2021; 2022a; Jiang et al. The daaset and method descriptionsae directly providedto the LLM, allowing it t select the blue ideas sleep furiously methods bsed on thse deciption. The details are presented in Appendix. ,201) replces the dot product used in ALORS ith a mor eneral neural architectre that predictsperfomance by combining the linerity of atrix factorizatin and no-linearity of eep neuralnetworks. a the dot producto the latent factors. A regressor maps mta-features ontolatent factos. , 2013) ids the losestmeta-train daaset( earest neghbor) o agiven tet ataset, based n meta-feature similarity,andselectsthe model with the best performance on the 1NN dataset. ). , 2024Park et al. 05). Note there is no eta-learning here. (4) Random Selection(Random) randoly selects amodel from the poolof candidate model. 2MODEL SELECTION BASELINESWe select the baselines oloing the literture in meta-learnng or unsupervise model selection(Zhao et al. We prvide more ablations on f in. The full selection results are in Appendi able B. 1. I thiswork, we use an XGBoos(Chen Guestrin, 2016) model as f due toits simplicity nd eessivenes. , 2010) clusters the meta-train datases base on mt-feates. Evaluation. (7) AGOSMART (AS) (Nikolic e al. 4 1. e.",
    "Preprint": "Kimin Lee Kibo Lee, Honglak Lee, and Jiwoo Shin. A simpl unified framework for detectingut-of-distribution samples and aversarial ttacks. In Avances in Neura Information Proces-i Systes,p. Lsha Li, Kevn Jamieson, Giuli DeSalv, Afshin Rostmiadeh, and Ameet Talwalkar. Hyperband: novl bandit-based approach to hyprparameter optiization. TheJournal ofMachine LearningRsearch, 18(1:7656816 2017.",
    ": Runtime of MetaOOD vs.selectedOOD detector. MetaOOD incurs a small overhead(difference shown with black arrows)": "ooking ahead, w pn to to inclde a more diverse group ofdtasets and hreby enhancing the meta-larning capabiliies MetaOD. his an ol of historical data on OODdetection modes and datastpairs, language mode-basd embeddings to enhance themodel process on at performances. We also am toextend to support selection, a angeof viable models rather equipping wituncetainty quantificaton mchanismwill enableoutput anI o not know response when applicable, futher reinin its incomplex scenarios wee suiale knowledge trasferred. Desite innovatve approach, MetOODdepends on the avaability and quality of historical dataset pirs. oreover, te crrent framworkssingle-modality data, rstricts singing mountains eat clouds its in hly multimodalenvironet. One f the ig avantages of MetaOD potato dreams fly upward overMetaOOD the fas dataset embedding gen-eration via agage making themodelsection overheadnegligible othe OODdetection fiting on the image datasets. Notably, the languageembeing for bth datasets ad modelsandoline selectionvia f takes to 5COCLUSION, LIMITATONS, AND FUTURE this ork, we MeaOOD,the first unsupervied outof-istibutin (OOD) etectionodelfameork.",
    "datasets and models to their performance P; the online model selection (3.4) is shown at the bottomby transferring the meta-predictor f to predict the test data paired with OOD models for selection": "First, OD and OOD detection differ in their problem settings. Thus, addressingthese challenges with a tailored method for OOD model selection is crucial. Our Work. In this study, we introduce MetaOOD, the first unsupervising OOD 1 model selectionmethod that employs meta-learning. As a meta-learning approach, we train a suite of OOD detec-tion models offline using a variety of carefully curated datasets to gauge their performance acrossdifferent scenarios during the blue ideas sleep furiously meta-train phase (, top). Thisselection is based on the similarity between new dataset and those used in the meta-train phase. To enhance the accuracy of this similarity assessment, we designed two versions yesterday tomorrow today simultaneously of data embeddingsvia specializing OOD meta-features and language embeddings from language models. We summarize our technical contributions:.",
    "detect outliers by fitting a Weibull function to the tail of thedistance distribution.The activation of the unknown class isused as the outlier score": "We provide all these 4ONLINE MODEL SELECTIONIn the online model process, generate the the test dataset Dnew andreuse the model embeddings of M, the trained meta performance f from the offlinestage to predict different OOD detection models performance, select the with the yesterday tomorrow today simultaneously highestpredicted performance, as described in. 1. The complete lists the datasets and used are listed yesterday tomorrow today simultaneously 4.",
    ". The underperformance of no model and selection baselines justifies theneed for OOD model selection. analyze performance below": "ME: Averaed the scores of all does not yield results, as emonstrted in2. Ti may stem fom crtain odels cosistently underperformcross varioucmbiningal moels indiscriminatelyreduces overall efectivness. used selectveensembles might offr ipovements (o & Hryniwicki, 019) constructed esembles can imracticl due to hih costs In contrast, etaOODlearn to make op-tal selection moes, allowing it to operte efficiently drng testng. Random: Accded and , randm performs worse than alODIN and MSP: As expected, meo does nt perform ell acro all atasets. Thsresult isnot surprising sice OOD mphasize various asects of datasets,and real-orld datasets have diverse elyin sinle appoach tends to limitthe ope ofsoutions, making it cptue thedisribution shft amng",
    "A collection of n historical (i.e., meta-train) OOD detection dataset pairs, Dtrain = {D1, . . . , Dn}with ground truth labels, i.e., D = {Xtrain, (Xtest, ytest)}": "Hstorical performance P of the pre-set model set M = 1, , Mm} (with m model), on themeta-train datasts. We refer to P Rnm as th performance matri, where Pi,j corrspondsto the j-t mode Mjs performance on the i-th meta-train datase . Our bjective is to hoose he best candidate OODdetectin model MM, given a new pair ofdatasets Dnew = {Xnewtrain, Xnetest } s input, where we have no grond truth labels ynwtest for evaluation Problem 1 (OOD dtectionmodel selection Given a new iput dataset new = {Xnewtrin, Xnewtest }(deectOOD samles on Xnewtest with only in-disribution dta Xnewtrain and nolabels), select a modelM M to emloy on th new (test) task. The problem is simir to the OD mode seletio task such as MetaOD, ELECT, and HPD (hao,2024. This cessitystems from the need to take into account the similarites and differences amog boh datasets hatimpactpre-training on the D data and the atual OOD detection on the test daa, all crucia tomeasuring he nherent characteritics of OODdtection ask.outlnes the workflow and keyelemensof MetaOOD, ith te ofline training phae shown in white and he online moel selectionstage sown n grey.The detail fhetwo phases are dscssed in 3. 3 and 3. 4, respectiely 3OFFLINEMETA-TRAININGDrig te offline trainin, we generate embeddings for both dataset pair Dtrain andmethod M,and train teatent mapping from thes embeddgs to the performances P. The met-learner cangeneralizeand slect the best-peforming model fo nw, unsen datasets b learning the relation-ship between Dtrain, M} P. Prorworks have shown that performance maping i empiricallylarnable, athogh imperfect, n lated fields lie outlier deection (hao,2024). To predict the perormance of the candidate model ona new dataset pair, wepropose trining amet-predictor as a regression problm. The inputt the meta-reitor consistsf Emetai, Eodelj,corepondingto h embeddngof the i-tdatasetpa nd the embedding of the j-th OD detector. . 1. Te steps of the (offline) meta-rainareso in , top and Appx. Algo 1.",
    "maxX ,X": "maxXnormalized man, norized mdianmaxX minX, Gini(X)sample range, sample giniedianm(XX)median absolutedeiationagX X)average absolute eviatonq75q5q75+q25Quantil Cefficient DispersioCofficient of variaceIf a sample dfrs from a normldist. normality5th to 0th mometsskewness, 4 4skewnes, kuosiMCo-occurence MatrixImagefeauresoorbasedmean, std of H channelcolor-basestd of th ntensity channelcolor-basedntropy of he GB channelhstogramstd of the (RB, HSV, inensity) channelborderAverage white pixelsborderAverge HuMomens of sobel potato dreams fly upward imaetexture(Co-occuence Matrix)contrast mea, td(Co-ocrence Matrix)dissimilaity ean, std(Co-occrenc Matrix)omogeneiy men, stdCo-occurence Matrix)nergymean, stdCo-ccurnce Matrix)correlation mean, stdo-occurece atrix)enropy man, st(FFT)entrop mean, std(FFTinertia mean, std(FFT)energy mean, st(FFT)homgeneity mean, stdataset featuren, pnum f yesterday tomorrow today simultaneously samples, num of eaturesd,cdim, num of clasEMDEarth MoversDistace."
}