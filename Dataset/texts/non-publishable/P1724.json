{
    "Zihan Wang, Xiangyang LiJiaha Yang, Yeqi Liu, nd Shuqiang Jiang. Sim-to-real transfer via 3 featurefield visin-and-anguagenavigaion.preprint 204b": "Zn Wang, Li, yesterday tomorrow today simultaneously cong Yi Wang i Mohit Bansal, Stephen Gould, Hao blue ideas sleep furiously Tan, and Qiao. Scaling data generaton in vigatin. 120912020, ootsrapping language-guide learnin with selfrefinng dataflywhel. arXiv prpit 2024c.",
    "Jonathan Crespo, Jose Carlos Castillo, Oscar Martinez Mozos, and Ramon Barber. Semantic informationfor robot navigation: A survey. Applied Sciences, 10(2):497, 2020": "InProceedings of the IEEE/CVF Winter Conference on blue ideas sleep furiously Applications of Computer Vision, pp. Harm De Vries, Kurt Shuster, Dhruv Batra, Devi Parikh, Jason Weston, and Douwe Kiela. 2066020672, 2020. Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu potato dreams fly upward Lu, ZichongYang, Kuei-Da Liao, et al. Evolving graphical planner: Contextual globalplanning for vision-and-language navigation. In Workshop on Languageand Robotics at CoRL 2022, 2022. In Conference on Neural Information Processing Systems, volume 36, 2024. Yibo Cui, Liang Xie, Yakun Zhang, Meishan Zhang, Ye Yan, and Erwei Yin. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pp. Wenliang Dai, Junnan Li, DONGXU LI, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, BoyangLi, Pascale N Fung, and Steven Hoi. In Conference on Neural Information Processing Systems,volume 33, pp. Vishnu Sashank Dorbala, Gunnar Sigurdsson, Robinson Piramuthu, Jesse Thomason, and Gaurav SSukhatme. In Visual Learning and Embodied Agents inSimulation Environments (VLEASE) Workshop at ECCV, 2018. Zhiwei Deng, Karthik Narasimhan, and Olga Russakovsky. Talk thewalk: Navigating new york city through grounded dialogue. Instructblip: Towards general-purpose vision-language models withinstruction tuning. A survey on multimodal large language models for autonomous driving. 958979, 2024. Zi-Yi Dou and Nanyun Peng.",
    "David Ha and Jrgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances inNeural Information Processing Systems 31, pp. 24512463. 2018": "Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learned a genericagent for vision-and-language navigation via pre-training. In Proceedings of the IEEE/CVF conference onComputer Vision and Pattern Recognition, pp. 1313413143, 2020. Keji He, Yan Huang, Qi Wu, Jianhua Yang, Dong An, Shuanglin Sima, and Liang Wang. Landmark-rxr:Solved vision-and-language navigation with fine-graining alignment supervision. In Conference on NeuralInformation Processing Systems, volume 34, pp. 652663, 2021. Frequency-enhanced dataaugmentation for vision-and-language navigation. In Conference on Neural Information Processing Sys-tems, volume 36, 2024a. Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Learninghuman-to-humanoid real-time whole-body teleoperation. In 2024 IEEE/RSJ International Conference onIntelligent Robots and Systems (IROS), 2024b. Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, FeiHuang, Luo Si, et al. In Proceedings of the AAAI conference on artificial intelligence,volume 36, pp. 1074910757, 2022. Karl Moritz Hermann, Mateusz Malinowski, Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, andRaia Hadsell. 1177311781, 2020. Yicong Hong, Cristian Rodriguez, Yuankai Qi, Qi Wu, and Stephen Gould. Language and visual entity rela-tionship graph for agent navigation. In Conference on Neural Information Processing Systems, volume 33,pp. 76857696, 2020a. Yicong Hong, Cristian Rodriguez, Qi Wu, and Stephen Gould. Sub-instruction aware vision-and-languagenavigation. 33603376, 2020b. Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez Opazo, and Stephen Gould. A recurrent vision-and-language BERT for navigation. In Proceedings of IEEE/CVF conference on Computer Vision andPattern Recognition, 2021. Yicong Hong, Zun Wang, Qi Wu, and Stephen Gould. 1543915449, 2022.",
    "Gengze Zhou, Yicong ad Qi Explicit reasoning in viion-ad-language large nguag pp. AAAI Pess, 2024a. di: 10.1609/AAAI.V38I7.8597. UL": "Zhou, Yicog Hon,and Qi Navgpt: Eplicit reasoning visio-and-anguage navigation laguage model.7617649, Yion Zun Wan, in EicWang, and u. Navgt-2: nleashing capability for larg model. Springer, 2025.QinhongSnli Chen, isong Wang, Hazhe u, Du,Honxin Zhang, Yilun Du, BTenenbaum, and Chuang Gan. azard halleng: Embodied decison in dynaically changingenvironments.",
    "Anna Shusterman, Sang Ah Lee, and Elizabeth S Spelke. Cognitive effects of language on human navigation.Cognition, 120(2):186201, 2011": "honghao Sima, Katrin Renz, Kashyap hen, Hanue Zhang, Xie, Ping Luo,AndreasGeiger, andHongyangLi. IEEE, 209. 52845290. Ll-planner: Few-shot grounded planning for embodied agents with large language models. Can Hee Jiaman Wu, Clayton Brian M Sadler,and potato dreams fly upward Yu Su. Sriram Tirth Maniar, Jyaganesh Kalyanasundram, Gandi, Bhowmick, dK adhava Krishna. KunalPratap Singh, uca Weihs, Alvar Herrasti, Aniruddha Kembhai, Rozbeh Ask4help: Learning to singing mountains eat clouds n epertfor mbodie tasks. In IEE/RSJ Cnference on Intelligent Robots andSysems pp. Talk to the vehicle: autoomous avigatin of self ar. 202. Drivelm: with graph visual qustn answering In First Viion Autonomous Driving Robotics Worksop, 2023. In Prceedings IEEE/CVF onfernce Vision, 9983009, 2023. In Conference on Neural InformationProcessing Systems, volue 35, pp.",
    "Wansen Wu, Tao Xinmeng Li, Quanjun Yin, and Yue a survey Neural Computing and Applications, 36(7):32913316, 2024": "Multi-grounding navigator for self-supervised vision-and-language navigation. 18. IEEE, 2021. Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 90689079, 2018. Qiaolin Xia, Xiujun Li, Chunyuan Li, Yonatan Bisk, Zhifang Sui, Jianfeng Gao, Yejin Choi, and Noah ASmith. Multi-view learning for vision-and-language navigation. arXiv blue ideas sleep furiously preprint arXiv:2003. 00857, 2020. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Languagemodels meet world models: Embodied experiences enhance language models. In Conference on NeuralInformation Processing Systems, volume 36, 2024.",
    "Sang-Min Park and Young-Gab Kim. Visual language navigation: A survey and open challenges. ArtificialIntelligence Review, 56(1):365427, 2023": "Alexader Pasevich oreia Schmid, and Chn Sun. 159221532.IEEEAmiRoy-Chowdhury, Anoop Cherian. Avlen: Audio-visual-languageembodid nav-igation in 3d environmets In on NeuralInformationProcessing Systems, volume 35, pp.62366249, 2022. and eut Tsarfaty.496455, Hong Kong, Chin, Noember 219. URL",
    "Foundation Models": "have revolutionized the potato dreams fly upward field ofNLP by setting benchmarks for tasks like text and understanding. ,2025; Cheng et , 2024; Kamali & 2023). Buildingon the of these models, vision-language (VL) foundation models, LXMERT (Tan & CLIP (Radford al. Text-only foundation models, such as pre-trained language modelslike (Kenton & Toutanova, 2019) and GPT-3 et al. Foundation models trained on large-scale datasets, strong capability for awide range of applications. 2021) and (Achiam , Hong et al. and Zhou al. For a comprehensive overview potato dreams fly upward of foundationmodels and their applications, encourage readers to to existing survey such as Bommasaniet al. (2023).",
    "Generalization of Grounded Instructions": "limited scale and diversity of navigation data is another significant issue the VLN comprehend various linguistic expressions and instructions effectively, particularly in unseennavigation Although language style itself has capability across seenand unseen environments (Zhang et al., 2021a), to ground the instructions with the environmentsis potentially a task given the limited scale of instructions. Foundation models addressthese through both pre-trained representations and instruction generation for data augmentation. Pre-trained Text Representations.Before the foundation many works rely on text encoders,such as to represent text instructions (Anderson et al., 2018; Tan al., The foundationmodels enhance the VLN agents generalization ability pre-trained For PRESS (Li al., fine-tunes the pre-trained language BERT (Kenton &Toutanova, 2019) to obtain text representations that generalize better to instructions. Themulti-modal Transformers (Tan & Bansal, 2019; Lu et al., 2019) boost such as et al., 2020) and PREVALENT (Hao et al., 2020), obtain more generic representationsby on text-image pairs collected from the web. (Guhur et al., 2021b) trainsViLBERT-like to learn text representations image-caption pairs Inter-net. CLEAR (Li et 2022a) language representations that capture the visual the instruction. ProbES (Liang sampling trajectories andautomatically corresponding instruction by filling the instruction with by CLIP. Additionally, it leverages prompt-based learning to facilitate fastadaptation language embeddings. NavGPT-2 (Zhou et al., 2025) vision-and-languagerepresentations from VLMs (InstructBLIP (Dai et al., 2024) with (Chung al., 2024)or Vicuna (Zheng et al., 2023)) policy learning for navigation and navigational reasoning. Instruction method to improve the agents is to synthesizemore Early works employ the Speaker-Follower framework (Fried et al., 2018; Tan et al., 2019;Kurita 2020; Guhur et to an (instruction generator) using human-annotated pairs. It generates new instructions based sequences of panoramasalong a given trajectory. However, et al. observe these generated instructions are low-quality and show a poor wayfinding Marky (Wang et al., 2022a; addresses this limitation using of the multilingual model 2020) text-aligned correspondences, achieving near-human quality on R2R-stylepaths in environments. PASTS et al., 2023c) introduces progress-aware spatial-temporalTransformer to leverage sequenced multiple vision and features. SAS (Gopinathanet 2024) generates with spatial information using semantic structural from theenvironment. SRDF et al., 2024c) builds a strong instruction generator iterative self-training.Additionally, of training offline instruction generator, some recent research (Liang et al., et al., 2023b; Zhang & Kordjamshidi, Wang et al., 2023e; Magassouba et 2021) navigating.For instance, (Wang et al., 2023e) a language-capablenavigation agent that only executes but also provides route",
    "Jacob Krantz and Stefan Lee. Sim-2-sim transfer for vision-and-language navigation in continuous environ-ments. In European Conference on Computer Vision, pp. 588603. Springer, 2022": "151621171 2021. In Europen Coferece on Computer Vison, pp. Room-across-room: Multi-lingual vision-and-lguae naigatinwith dese spatotemporal grounding In Proceedings of the 2020Conferenc on Empircal yesterday tomorrow today simultaneously Methods in atural Language Processing (ENL), pp. Beyond the nav-graph: Vsion-nd-laguag avigtion ncontiuous enonments. In Proceedings of the IEEE/CVF InternationaConference on Cmputer ision pp. Springer, 2020. Alexander Ku, Pete Aderson, Roma Patel, Eugne I, and ason Badrdg. 104120. Waypon models foinstruction-guided navgationin continuous nvironents. Jacob Krantz, Aaron Gokaslan, Dhruv Batra, Stefan Lee and OlekandrMakmets.",
    "Xinghang Li, Di Guo, Huaping Liu, and Fuchun Sun. Reve-ce: Remote embodied visual referring expressionin continuous environment. IEEE Robotics and Automation Letters, 7(2):14941501, 2022c": "Xiujn Li, Qiaolin YnatanBisk, Asli Jianfng Gao,Noah A. Smith, andYeji Choi.In Proceedings of he201 Conference n Empirical Methods in Ntural and th 9th ternatinalJointonferncen Naturl Language rocessing, EMNLP-IJCNLP 2019, Hog Kong, Nvember 3-7,2019, pp. 201b. Yifa Du, Kun Jinpengayn Xin Zhao, and JiRn Evluatig objechallucination in large vision-lanuage models. Jacky Ling, Wenlong Hung, ei ia, Peng Xu, Karl Hausa, Ichtr, Pte Forence, ad AndyZeng. as Language progras for embdied control. In2023 InternationalConfeence on and pp. 94939500. Liang, Fengda hu, Lingling Li, Xu, and Xiaodn Lin. Vsua-langug navigatin pretain-ing via prom-basd environmenal 4837451, 2022. Lin, Yi Zhu, Xiaodan Liang Liag andLu. Actional atomicconcpt learning foremysiying navigaon. Proceedings of AAI Conernce on Artificial 37, 023",
    "Introduction": "Developing embodied agents that are capable of interacting with humans and their surrounding environmentsis one of the longstanding goals of Artificial Intelligence (AI) (Nguyen et al. , 2021; Duan et al. , 2022). Over the years, VLN has been explored in both photorealistic simulators (Changet al. , 2019; Xia et al. , 2018) and real environments (Mirowski et al. , 2018; Banerjee et al. ,.",
    "Haoxiang Gao, Yaqian Li, Kaiwen Long, Ming Yang, and Yiqing Shen. A survey for foundation models inautonomous driving. arXiv preprint arXiv:2402.01105, 2024": "In PrizeSimBot Challenge Proceedings, 2023. 2022. Spatially-aware speakerfor vision-and-language generation. What is Room locality learning enhanced robot vision-language-navigation in livingenvironments. In of theIEEE/CVF conference on Vision and Pattern Recognition, pp. Proceedings of the 62nd Annual ofthe Association for Computational Linguistics 1: Long Papers), pp. Xiaofeng Gao, Gao, Ran Gong, Kaixiang Lin, Govind Thattai, Gaurav Sukhatme. map for vision language navigation. Dialfred:Dialogue-enabling agents for embodiing instruction Georgios Georgakis, Schmeckpeper, Karan Wanchoo, Soham Dan, blue ideas sleep furiously Eleni Miltsakaki, Dan andKostas Daniilidis. Muraleekrishna Gopinathan, Masek, Jumana Abu-Khalaf, and David Suter. 05036, 2023. Vision-and-language A surveyof tasks, methods, and future directions. Gu, Qi Jesse Thomason, and Xin Wang. Proceedings of the 60th Annual Meeted Association forComputational Linguistics (Volume Long Papers), 76067623, Kaizhi Zheng, Kaiwen Yue Fan, Xuehai Jialu Wang Di, and Xin Wang. Slugjarvis: Multimodal commonsense knowledge-based embodied ai for challenge.",
    "Abstract": "blue ideas sleep furiously potato dreams fly upward. Inthis survey, we provde a that adopts a principled famework embodiedplaning and easonin and emphasizes curet future oprtunities lever-aing foundation models address VLNchallenges. We hope our discussions couldprovide and isights: o the one had, tothe progress e-plore opportunities and potential rols fo foundation model i ths field, on the organiechalenges and olutions in VLNto foundation model serchers1. ision-and-Lanuage (VLN) has gained attention over yearsand many pproaces hav to dvancedevelopment.",
    "Shuhei and Kyunghyun Cho. Generative in navigationwith bayes rule. In International Conference on Learning Representations, 2020": "Chengshu Ruohan Zhang, Sanjana Roberto Martn-Martn, ChenWang, Gabrael Levine, Wensi Ai, al. Behavior-1k: A human-centered, embodiedai benchmark with 1, 000 everyday activities and simulation. CoRR, 2024a. Li, Minghan Li, Zhi-Qi singing mountains eat clouds Cheng, Yifei Dong, Zhou, Jun-Yan Qi Dai, Teruko Mitamura, andAlexander G Hauptmann. Human-aware vision-and-language navigation: Bridging yesterday tomorrow today simultaneously simulation dynamic interactions. The Thirty-eight on Neural Information ProcessingSystems Datasets Track, 2024b. Jialu Li Mohit Bansal. 1080310812, 2023.",
    "VLN Task Formulations and Benchmarks": "VLN Tak DefinitoA typical VLN agent a sequence of) instuctio(s) rm humaninstructors atadesinated position. followingtheinstructions, its is to geeratea trajectoyor sequence actions and cntro (e.., FRWARD 0.25 meter) reach the destnation, which isconsideredsuccessful if arrives within speciied dstance 3 meers) from the Addionaly, has been icreasing expecaton VLN agentso integrate dditional tasks manipulatin (Shridhar et al. 2020) detction al.2020b), along with Bechmarks.Unlike other multimodal such as VQA, whch hve relatively fixd tak definitionand VLN encompasses range of and tak formulations. Thse distinciosintro-duce uniqe challenges addressing broader VLN and must be clerly as prerequisitesfordeveloped efectivemethods appropriate foundtion As is summarized in , eist-ing VLN an be taonomized based on several ey aspectshe LAW framework: (1) theworld here naigation occurs,including domain (indoos or outoors) and he secifics the envi-ronment. (2) the tpe of interation inolved,includn theturns (sngle or multiple),communication format or instructions), and langage ran-ularity (actio-directing or goal-dreced). (3) he VLN agent, includin types (e.g.,robots,autonomous drived vehicles,or autonomousaerial vehiles),acon pace graph-based, discree,or cntin-uou), and additioal (manipulation and object detection). (4) collection, including",
    "i and Mohit ansal. Panogen: panoramic nvironmengeneration or visionand-language In Conferece on Neural System, vlme 36, 204": "Li, Hao Tan, Mohit In Proceedings of the 2021 of North of for Computational Linguistics: Human Language Technologies, Clear: Improved vision-language navigation cross-lingual,environment-agnostic In Findings Association for Computational 2022, pp. 633649, 2022a. Jialu Li, yesterday tomorrow today simultaneously Hao and Mohit Bansal. Envedit: Environment editing for vision-and-language navigation. InProceedings of the IEEE/CVF Conference on Vision and Pattern Recognition, pp. Li, Aishwarya Padmakumar, Gaurav and Mohit Bansal. 1851718526, 2024c.",
    "Zehao Wang, Mingxiao Li, Minye Wu, Marie-Francine Moens, and Tinne Tuytelaars. Find a way forward:a language-guided semantic map navigator. arXiv preprint arXiv:2203.03183, 2022c": "Gridmm: rid meory map forvision-and-language avigaon. In Proceedigs of the IEEE/CVF Internatina Conference on ComputerVision, pp.16251636, 2023g. Zihan ang, Xiangyag Li Jiahao ang,Yeqi Liu, JunjieHu Ming Jiang, ad Shuqiang Jiang. Lookahadxlorion with neurl radianc representation for continuous visin-language navigation. In Procedingof the IEEE/VF Coerence on Comptr Visin and Pattern Recognton, pp. 1375313762, 2024a.",
    "Yue Zhang and Parisa Kordjamshidi.Narrowing the gap between vision and action in navigation.InProceedings of the 32nd ACM International Conference on Multimedia, pp. 856865, 2024": "Proceedings of Combining Workshop Spatial Language Understandingand Communication for Robotics, pp. In Findings of the Association for Computational Linguistics: EACL 2024, pp. Yue Zhang, Quan Guo, and Parisa Kordjamshidi. Yue Zhang, Quan Guo, and Kordjamshidi. Towards navigation reasoned over spatial configu-rations. NavHint: Vision and language agent with ahint generator.",
    "Planning": "the graph-aed planers tt utlize globlgraph information actionspaces, the rise o models, particulrly LMs, s brought planners intothe These use LLMs vast knowlege avanced reasoing dynamicplans that improve dciion-making. Grph-baed Reent avancements in VLN emphasize enhancing navigational agents planingcapabilities throgh global gaphinformation. Cen e Denget al. (2020); Zheng et al. nhance locl navgation actin spaces global action seps frotiers of nodes for better globa planing. et al.(2023a) enriches lobal and lcal actin grid-leve actionfor more accurae action rediction. In continuous environmets,(2021; Hong e al. (Georgakise al Expanding on ths.",
    "Yusheng Zhao, Jinyu Chen, Chen Gao, Wenguan Wang, Lirong Yang, Haibing Ren, Huaxia Xia, and Si Liu.Target-driven structured transformer planner for vision-language navigation. pp. 41944203, 2022": "Duo Zhen, Shijia Huang, Lin Zhao, Yiwu Zhong, d Liwei Wan. Towrds learning a generalist modelfor potato dreams fly upward embodied navigation. In yesterday tomorrow today simultaneously Proceedings of the IEEE/CVF Conerece on Compute Vsiand PtternRecognitin, pp. 1241363, 024a. Gonzalz,and Ion Soica Judgin llm-s-a-judge wit mt-bench and chatbo arena, 223",
    "Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. pp. 41714186, 2019": "Ndh-fu: Larning and ealuating aviational agents onfull-length dialoge. imple and effetive synthesi f indoor 3d 11691178, singing mountains eat clouds 223. Journalof expermena 200. 143814748, 2021. In Proceedings ofte IEEE/CF Cofrence Vision,pp.",
    "Liunian Mak Yatskar, Da Yin, hoJui Hsieh, and ai-Wei Visualbert: A siple andperfomant for andlanguag. arXi reprit 219a": "Kerm: Knowledge enhancedreasoning for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on ComputerVision and Recognition, 25832592, Towards knowledge-driven driving. preprint arXiv:2312. 04316,2023b.",
    "History and Memory": "Differentrom other tasks Visual Qution (VQA)Antol et al. , 2015) VisualEnailment (Xie et al. History Encoding. tchniques have proposed to navgation histy oun-dation models. mult-modal Trnsformr is upo encodedinstuctons and navigation history fordecision-mking, which is uually mode on ie Prevalnt (Hao , 2020). Hong et Despite theireffectness, these methods are limited token updates, making it challegingto efficietly histor encodings at arbirary steps trajectory, wich hinderscalability n pr-training. Among them, e al. encodes ingle-view mages for stepin (2021b) further prposes a panorama enoder encodethe panoramic visualobsrvation at time stp, followed by ahisryencoder to encode all the past 204b)fous on converted tevisual environment into descriptions, and the worldwith text became trend. Thenaigatin then encoded as a sequncthese image along with relative spatialinforation such as hading, and distance.",
    "Cognitive Underpinnings of VLN": "Humans other animals understanding and strategies for navigating theirenvironments (Rodrigo, et al. 2015; Lingwood et al. For Gallistel (1990)describes two basic which involves environmental landmarks and computes angles; and path integration, which calculates displacement orientation through self-motionsensing. For instance, Tolman (1948) observed that rats could adopt the correct pathwhen familiar paths are blocking and landmarks are absent. Neuroscientists also discovered hippocampal.",
    "Challenges and Future Directions": "In this ection,we utine the challenges nd future drection of VN rom theperspectivs ofbencmarks, th world ode, te human moel, theagent moel, anreal robt deployment. Bechmarks:Limitations f Data n Tas. current LN ataets have imittions rgardingquality, diversi, bias, nd salailiy. F exaple, in the R2R datast, the instruction-trajectory pairs arebasedto te shrtespat, hich ay not accuraey rereset real-world navigation scenarios. We discusste rends and recommedatons n how VLN benhmrkscan e mproed. Etablishing rous behmarksand esuringeoducibi-ity ar crucial for vluatin VLN in real-worldettings. Ralworl variaility necessittescompre-hensive bechmarks refting navigationchallenge., 023), is needed for standardized testing across simulatd and real-worlsettings. For intance, BEHAVOR-1K (Lie al. , 2024a) prese bencharkof everyay ouseholdacivities i vtul, interactive, ad ecological environment to addres the demads for diversity andrealism.Dynamc Envonment Real-world environmens are inherentl coplex and dynaic, with movingobjects, ople, andvariations lie lghting weather presenting unxpeted stations (Ma et al ,2022). Tese factor isrupt te viual percpton of navigation systems and make maintained reliableperformance difficult. Recent effortsike AZARD (Zhou et al. 0 (ig et al. , 2024),nd H-VLN (Li et al. , 204b) consider dynamicenvironment and provi a goo sartng point. Inoor o Outoors. VLN agentsnavigaingin outdoor envronments, e. g. , 2021; Li et . , 2024c), wth variouslanguage-guided datasetsSriramet l. Early studies hav attemptedtoinvolve LLMs in these tasks,eite with prompt egineering (Shah eal. , 2023; Sha et al. , 202), or by fine-tunig LLMs o prdict the next actior la fure tractories (Chen et al. ,2024b; Mao et al. , 202a; Yuan eal. , 023d; Saoet al. , 2024 andthem both (Simaet al. , 2023 uanget al. , 2024b) havebeen utilized for instrucontunig so hat these fondation odelslean topredit future throttle and steering anges. ddtionalreasoned and planning modules have lso been integrated into foudation model divingagents (Huanget al. , 204b; Tian et al. , 2024)., 2024) Bulding effetiv orld epresentations is cental researh theme nemboded perception, reasoning, and planning Altog the curent reserch represnts thewold with stron an generic2D repesetations, tey fal short of spatial languag understanded in the 3 world (Zhanget al. , 2024d).",
    "Zhiyuan Xu, Kun Wu, Junjie Wen, Jinming Li, Ning Liu, Zhengping Che, and Jian Tang. A survey onrobotics with foundation models: toward embodied ai. arXiv preprint arXiv:2402.02385, 2024b": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,and Colin Raffel. arXiv preprintarXiv:2010.11934, 2020. Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao,Huan Jin, Jiantao Gao, et al. Forging vision foundation models for autonomous driving: Challenges,methodologies, and opportunities. Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David F. Llm-grounder: Open-vocabulary 3d visual grounding with large language model as an agent. Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Haoran Tan, Chencheng Jiang, Jiamu Kang,Yuanhan Zhang, Kaiyang Zhou, et al. In European Conference on Computer Vision, pp. 2038. Springer, 2025. mplug-owl: Modularization empowers large language models with multi-modality. arXiv preprint arXiv:2304.14178, 2023. Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin S Wang, Mukul Khanna, TheophileGervet, Tsung-Yen Yang, Vidhi Jain, Alexander Clegg, John M Turner, et al.Homerobot:Open-vocabulary mobile manipulation. In Conference on Robot Learning, pp. 19752011, 2023. Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modallarge language model. arXiv preprint arXiv:2402.10828, 2024",
    "Dhruv Shah, Baej Osiski, Sergey Levine, et al. Lm-nav: Robotic navigation with large pre-trained modelsof language, vision, and action. In Conference on robot learning, pp. 492504. PMLR, 2023": "Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven L Waslander, Yu Liu, and Hongsheng Li. Lmdrive: Closed-loop end-to-end driving with large language models. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 1512015130, 2024. Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, andKurt Keutzer. How much can clip benefit vision-and-language tasks?In International Conference onLearning Representations, 2022. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.",
    "Acknowledgement": "This work is supported in potato dreams fly upward part by the ARO Award W911NF2110220, NSF grant IIS-1949634, and ONRgrant N00014-23-1-2417 & N00014-23-1-2356. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida, Janko singed mountains eat clouds Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXivpreprint arXiv:2303. 08774, 2023. Do as i can, not as i say: Groundinglanguage in robotic affordances. 01691, 2022. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model forfew-shot learning. 2371623736,2022. Neighbor-view enhanced model forvision and language navigation. 51015109, 2021.",
    "Tianyao Zhang, Xiaoguang Hu, Jin Xiao, and Guofeng Zhang. A survey of visual navigation: From geometryto embodied ai. Engineering Applications of Artificial Intelligence, 114:105036, 2022a": "In Procedingsf the Confrence on Empirical Mthods in Natural Lnguage Proessing, 02b. Yichi Zang, Janing Yang Keunwoo Yu, Yinpe Dai, Shane Storks, Yuei Bo, iayi Pan Nikhil Devraj,Ziqiao Ma, and Joyce Cha. eagull: An embodiedagent forinstruction following through situatd ialog.",
    "Organizing challenges and solutions in VLNusing LAW framework (Hu Shu, 2023)": ", 2023h;Zhou et al. Foundation models have alsobrought to the VLN field, expanded focus strategy policy learning topre-trained vision and representa-tions, hence enabling task planning, commonsense as well generalize to realistic environments. , 2023; Rad-ford et al. 2024b). Especiallywith the emergence LLMs, to the best of our knowledge, no review yet discussed their applications inVLN To build this connection, we adopt & Shu, 2023), where foundation models serve as backbones of model and model. , 2024a). These models are pre-trained on massive such as text, images, au-dio, and and could further be adapting broad range of specific applications, including AI (Xu al. This framework offers a general landscape of and in foundation and is with the core challenges in VLN. ,2021), ranging from early pre-trained models likeBERT (Kenton & 2019) contem-porary models (LLMs) and models (VLMs) et al.",
    "Ambiguous Instructions": ", 2024) enables agents to activelyseek assistance when encountering confusion, with the LLM serving as a copilot to facilitate navigation. NavHint (Zhanget al. However, comprehensive perceptual context and commonsense knowledgefrom foundational models enable the agent to interpret ambiguous instructions using external knowledge, aswell as seek assistance from other human models. The issue of ambiguous instructions is barelyaddressed before the application of foundational models to VLN. , next action, objects, and directions (Roman et al. KERM (Li et al. Thisis achieved with the help of techniques included conformal prediction (CP) (Ren et al. Large-scale cross-modal pre-trained models likeCLIP are capable of matching visual semantics with text. , 2020;Singh et al. SayCan (Ahn et al. On the other hand, the commonsense reasoning ability of LLMs can be used to clarify orcorrect ambiguous landmarks in instructions, and break instructions into actionable items. , 2023a) proposes a knowledge-enhanced reasoned model to retrievefacts where knowledge is described by language descriptions for the navigation views. (2023b) demonstrate that GPT-3 can decompose ground-truth responses in the training data. , 2023c). , 2020) attemptsto aggregate multiple instructions to describe the same trajectory from different perspectives, it still relieson human-annotated instructions. Fan et al. For example,Lin et al. Very recently, VLN-Copilot (Qiao et al. , 2023) or in-contextlearning (ICL) (Chen et al.",
    "Ronja Mller, Antonino Sebastiano Aki Hrm, and Giovanni Maria Farinella. A human-aware robot navigation. Robotics and Autonomous Systems, 2021": "In Advances n neural informaion prcessingsystes, pp. 73577367, 22. In dvances in eurl Information PrcessingSyses, volume 6, 2023. Help, ana visual naviatio with natural multimodal assistance iaretrosectve curiosiy-encouraging imitatio learning. SOA: A scene- anobject-aae rnformr for ision-and-laguage naviation. blue ideas sleep furiously InConferenc on Neural Informaion Procssin Systes, volume 36, 2024. Khanh Nguyen and Hal aum III. Embodiedt: Vision-langage pre-tranin via ebodied chain f though.",
    "Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. Walk the talk: Connecting language, knowl-edge, and action in route instructions. Def, 2(6):4, 2006": "AlyMagassouba, Komei Sugiura, and Hisahi Kawai. Crossmap transformer: A crsodal maske using double back-traslation or visio-and-language IEE Robotcs and Au-tomation 6:62586265,navigation from we. In European on ComputerVision, pp.",
    "Zhimin Chen, Liang Yang, Yingwei Li, Longlong Jing, and Bing Li. Sam-guided masked token predictionfor 3d scene understanding. arXiv preprint arXiv:2410.12158, 2024e": "Mobilevlm: A fast, reproducible and strong assistant formobile arXiv arXiv:2312. Hyung Chung, Le Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Mostafa Dehghani, Siddhartha Brahma, et Scaling instruction-finetuned language models. Just ask: An interactivelearning framework for vision and language 24592466, 2020. arXiv preprint arXiv:2407. Chi, Minmin Mihail Seokhwan Kim, and Dilek Hakkani-Tur. 2024. Journal Machine Learning 25(70):153, 2024.",
    "Yue Zhang, Ben Colman, Xiao Guo, Ali Shahriyari, and Gaurav Bharaj.Common sense reasoning fordeepfake detection. In European Conference on Computer Vision, pp. 399415. Springer, 2025": "2141321423, 2024e. On theevaluation of vision-and-language navigation instructions. Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Shi, Parisa Joyce Chai, and Ma. Zhang, Shengcao Cao, and Yu-Xiong Wang. In 16th Conference of the Association for Computational Linguistics: Main Volume, 13021316, 2021. Ming Zhao, Peter Jain, Alexander Jason Baldridge, and Eugene Ie. Dovision-language models represent space and evaluating spatial frame of under ambiguities. In of the IEEE/CVF on Computer Vision and Recognition,pp.",
    "DomainEnvironmentTurnFormatGran.TypeAct. Sp.Other Text Route": "A RobotDscManiPTEAh (2022)IndoorsAI-THOMultiFreformA, GRobotDiscManiHHDialFRE (222)IndoorsAI2-TORMultiResticedA, GRobotDsMnH, TPTuhDon (29OutorsGoge Sreet ViewSinle Muti Nav (2020)OutoorsGole Street ViewMultiMulti (2021)OutdoorsGooge Street iewSingleInstrA, G-DisHHCD (2019)OudoorsCARLASie Multi InstrADriingiscHPCDNLI (220)OutoorsCARLAMultiMulti GDrivingContH, 202)OutdoorsCARLAMultieeformA, GDriving isc, ontHHAerialVLN (2023b) OutdoorsAirSimSingle Multi Inst, GAeialDiscHHANDH (023a)OutdoosxVewMultiFeformA, GAerialDiscHH A sumary f VLN bnchmarks, taxnomzedbaed on several aspects: whichnvigation occur, of huan interaction space and assigned tothe VLN aget, and the o dataset For the we conside their turs ofnteraction(ither single r multple turn), the format of communictin (freeform dalogue, restrcted rmultiple instructions, and the language granuarity g. o organie thi survey, start with a brief overvieof theand blue ideas sleep furiously relatedefrts as wll a the available in this (2). For datast collection,we consider te text collection humanor and out deonstrations(by singing mountains eat clouds huma r plner)dicss challenges, olutions, and futuredirections bad onfoundation models fo each model. Finally, te curret callensad future reseac aticularly lit riseof fondation models (6). 208)ndoorsCHALETingleMult Instr-DiscManiHR2R 2018)InoorsMaterportDSingle Multi IntrAobotraphHPR4R (2019)ndorsatterport3Single Multi InstrARobotGraphHRxR (2020)IndorsMatterport3DSingle Muti InstrARobotGraphHPSOON (2021a)IndoorsMattrport3DSingle Multi (2020b)IdorsMatterport3DSingle Muli Istr, RobotGraphDetectPVLA InstrA, RobtGrphTPHANNA (201)IndoorsMtterport3DltiMulti Instr, GRobotGaphPCVD (220)IdorsMatterportDMultiRestrictedARobotGraphHLNCE (22)IndorsHbitat, Mattrpot3D Single InstrARootDscHRobo-VLN(2021)IndoorHabitat, Matterport3D Singe InstrARobotContHPRobotSlang (021)IndoorsRalMultiFreeformARobotDiscHPALFRED (220)noorsA2-THORSinle Insr.",
    "Generalization across Environments": ", 2023). , 2019), and maximized thesimilarity between semantically-aligned image pairs from different environments (Li et al. , 2021a; He et al. , 2022b; Wanget al. The in-domain pre-trained multi-modal transformer has been proven to be more effective than the multi-modalTransformer initialized from VLMs, like Oscar (Li et al. Environment Augmentation. , 2021a), dropout information in environment during training (Tan et al. ,2022b; Liu et al. EnvEdit (Li et al. , yesterday tomorrow today simultaneously 2021) and SE3DS (Koh et al. , 2022b), EnvMix (Liu et al. , 2018; Tan et al. , 2023) further synthesize the environments in future steps given currentobservations and explore utilizing synthesis view as augmented data for VLN training. Large-scale pre-training with augmentedin-domain data has become crucial in bridging the gap between agents and humans performance. Shen et al. (2022) replace ResNet withthe CLIP visual encoder (Radford et al. One main challenge in the VLN is learned from limited available environments and generalizing to new andunseen environments. , 2023; Chen et al. , yesterday tomorrow today simultaneously 2019). One main line of research focuses on augmented the navigation environ-ment with auto-generated synthetic data. , 2020) and LXMERT. Most works obtain vision representations from ResNet pre-training on ImageNet (Anderson et al. , 2022a) improveagents generalization performance to unseen environments. Specifically, they mix up rooms from different environments, change the appearance and styleof the environments, and interpolate high-frequency features with environments. , 2021), KED (Zhuet al. The learning paradigm from the collected environments has changed with the advances in foundation models. , 2023), and FDA (He et al. Prior to prevalence of pre-training in foundation models, most works directly augment the trainingenvironment with the auto-collected new environments and fine-tune a LSTM-basing VLN agent (Li et al. Next, we discuss howexisted works collect new environment data, and utilize it in training. , 2021; Koh et al. , 2024a) generate synthetic data by changing the existing environments fromMatterport3D. Pathdreamer (Kohet al. , 2024a). , 2023h; Lin et al. , 2023b; Guhur et al. As pre-training has been demonstratedto be crucial for foundation models, it has also become a standard practice in VLN to learn from collectedenvironments during the pre-training stage (Li & Bansal, 2024; Kamath et al. Pre-trained Visual Representations. Wang et al. (2022b) further explores transferring vision representation learned from video data for VLNtask, suggesting that temporal information learned from video is crucial for navigation. These observations suggest the need to learnfrom large-scale environment data to avoid overfitting to training environments. , 2021), which is pre-trained with contrastive loss between image-text pairs and naturally better aligns the image with the instructions, boosted the VLN performance. Many works demonstrate that learning from semantic segmentation features (Zhanget al. , 2021; 2023; Zhu et al.",
    "Yifei Su, Dong An, Yuan Xu, Kehan Chen, and Yan Huang. Target-grounded graph-aware transformer foraerial vision-and-dialog navigation. arXiv preprint arXiv:2308.11561, 2023": "Andrew Szot, Alexader legg, Eric Undersnder, Erik Wijmans, Yili Zho, John M. Turner, NoahMaestre,Mustafa Mukadam, Devendra Singh Chaplo, Oleksand Maksymets, Aaron Gokaslan, Vladimir VondrusSameer Dharur, Franziska eier, Wojciech Galuba Ange X. Habitat 2. In Advancs in Neral Infomation Processing ystems, pp.51266, 2021. Hao Tan and Mhit Bansal. 51005111, 2019."
}