{
    "Could the affinity scores be used to successfully separaterelated tasks from each block?": ", normalizedgraph Laplacian matrix) denotes the diffusion matrix of the network, and the of features. g. Tofurther simplify the analysis, we consider layer as (,) = , where (e. For each from 1up to , let the node this task be given by a vector (),all of which are a set nodes as Let denote of observed of nodes. Weassume that is drawn from isotropic Gaussian distribution,and full rank. We provide a positive answer to both in a where the labels of each task have drawn from a finding is that two tasks from same group in theplanted model, their affinity scores provably higher thantwo tasks different yesterday tomorrow today simultaneously To this result, we firstformally introduce the setup. measure the loss of this GNN against thelabel vector using the Mean Squaring Error (MSE):. Suppose we are learning tasks. We on regressiontasks the analysis. Setup. Thus, of () are all real values.",
    "EXPERIMENTS": "e now evaluate our aproach empcally ommuitydetection and molcular gaphdata we show tat ourtask afinity scores can estmated eficnty t predictneatieransfers more accurately than task affinities. aty, we provide ablaion t showthat our approach is stable under The code rreproducingu experiments aailbl at. apply our approach  cmmunity detection askso several datasets ground-tuth labelsour p-proah outpeforms he MTL by task rouingbaselines by 2. 18% we evluate our appoach on mole-ular grah prediction tasks and show a. 6% mproement verpior ML methods.",
    "B.1Comparing structural differences between tasks": "8 higher cosine similarities in their PPR vectors than tasks from different groups. We conductthis comparison based on the task groupings found by our method. For tasks from the same group, their PPR vectors have higher cosine similarities than tasks from different groups. Across four community detection datasets, tasks within the same groupexhibit 8. To validate this hypothesis, we compute each tasks personalizing PageRank (PPR) vector using its community labels as the seed set. : This table reports the cosine similarity of PPR vectors between tasks from the same group and tasks from differentgroups. reports results. Then, we measure the cosine similarity of PPR vectors between tasks from the same group and tasks from different groups. Otherwise, we expect a positive transfer between twotasks. We study why negative transfers happen between two community assignment tasks when trained together. If their graph diffusionprocesses are very different, using a shared GNN would lead to negative transfer.",
    "that the measures gradually worse;Ours are accurate for subsets size ranging from 2 to 20": "Then, we presentrun tme pproah. Our requires training for eachradomsubset. In contrast, evious methods estimate affinities fr every pair intrainig on ( 2 task pairs. Comared with the two prvious approach rquires3. Speed uptranng. practice, we can further speed up trainingwth arly and dowsmpling. Our expeiments fundthat leads a significant speed-p, and th verhead onofNaive is only up o2-.I of AppendixB.3, we rportthe blue ideas sleep furiously running time fur by as with all MTL baselnes.",
    "Boosing ultitask Learning hrog Higher-rder ask AffniiesKDD 23, 610, Long Beach, CA, USA": "Ourpapr takes inspirtion Datamels , which extralatesthe otcme of blue ideas sleep furiously dep singing mountains eat clouds functins. Thesregarationschemes can be justi-fied for hypohesis nonconvex hypothessspace such as graph networks, explictly regulariing of challene. grient-based silarityeasures cn be fficienycomputed using the cosne imlariy o during trainng. t the representatins of alltasks ca help informationtransfer. In et al. , )show tht the finity scores lso accuraly predicttansfertypes i multitask learning. Task rouping. This canbe meaure te similarity between setsby averaging gradient of task in each workpoints out that first-orer affinity measres deteriorate asa measureto a lare set of tasks. g. Instedof shringlayers and model tasks, Kuma and III itigatingnegative transfers by dividing tasks into several related grups. Ma et al. that linear regression can te outcome f dee nets traied with a subset n popularimage benchmarks.",
    "Setup and background": "We condct a empricalstudy used ulpleover-lapping commuity tasks s concrete Let 1,2. , denote the verte f these commuities. For = 1, 2. ,, eciding yesterday tomorrow today simultaneously whether ode belongs a binaryclasificati task. Note that his formula-tion differs fom supervised community detction is blue ideas sleep furiously moresuiable or overlapping community detction.",
    "BADDITIONAL EXPERIMENTS": "irst, we additionalobservations rearding askrelatinhis. In thi section, proide expeimentto blue ideas sleep furiously support our approach. Second, we compare running time of our ith baselines.",
    "Task subset size = 5Task subset size = 10Task subset size = 20": "Left: Compared with two first-order task ffinit scores, our higher-orr task afinity scores achievensistently better F1-scorfor predicting negative transfers of combining p to = 20 esult consistenty hld for iffernt subset sizes. : yesterday tomorrow today simultaneously W use task affniy scres fom potato dreams fly upward tasks in a subset to task to predict wheter training with subset decreases thTL perfoance of task.",
    "Estimating higher-order task affinities": "Notations. There are semi-supervised tasks on the graph. For each task , we are given a set ofnodes () with known labels (). Note thatthe set (1),. , ( ) can be either overlapped or disjoint witheach other. The objective is to optimize the average predictionperformance over the tasks. singed mountains eat clouds Let be the encoder network shared by all tasks. Let 1,. ,be prediction layers for tasks 1,. , that map feature vectors totask outputs. When the input is a graph, we consider used graphneural networks as the encoder. , }, let () and (), for , be trained model on the combined dataset of. We evaluate the prediction loss on each tasks validation dataset. Let be an evaluation metric, e. g. , the cross-entropyloss. We define multitask singing mountains eat clouds learning performance for any as:.",
    "Alg. 1 (Ours)0.067 0.00129.73 0.12": "Resuls First,we find that VERSE embed-ding achieves the performance all the nod embeddingmehods Due the wereport the community detetionmethods i Appendix B.4. Benefit o task wth method that opti-mize blue ideas sleep furiously a jointmodel n al tasks, task groued consisentlyperforms better than theand Mixtre potato dreams fly upward of theMoreovr,we comare our approach withclustering by two affinities. The resultsshow that otper-forms by 2. 49% aeage.The rsults areshownof Appndix B. 4.",
    "Overlapping community detection": "consider a multi-class classification setup. Identifying community structures is one of the most widely studiedproblems in science. the conductanceof a and Leskovec describe algorithmusing non-negative matrix factorization for overlappingcommunities. This is a novel perspective on community detectionto of our knowledge. These approaches use the of edges to properties. Our formulation is moresuitable when dealing large number of overlappingcommunities. Thedifference that we formulate the problem of predicted labeling multitask learning, whereas Chen et al. Lastly, detection can becast in correlation framework, does not requirespecifying the number of communities. Besides, hypergraphs are to be useful for overlappingcommunity detection. finds local clusters by identifyinglow conductance set near a seed. Our results, compared with strongbaselines including VERSE embedding and BigClam , sug-gest modeling higher-order task relationships can empirical performance multitask learning.",
    ": multitask models for each group . . . ,": "First, we a transforming affinity for clus-tering.Sinc the sum of afinity scores blue ideas sleep furiously between two ihin singed mountains eat clouds agroup is (,+ ,), we define symmetrc marx1 = ( + /2. Tis isachievd by viewing the matri astask relationships,with souce asks rpresented along the rows target alongthe columns. find aset of task that the highest",
    ". Measuring Task Affinity Scores": ": Overview of our boosting procedure: (1) We sample random subsets of tasks, each subset contained fixed numberof tasks. (2) For each subset , for = 1, 2, . . . ,, we fit a multitask learning (MTL) model on the combining data sets of alltasks in , using a graph neural network (GNN) as shared encoder. After fitting the MTL model, we evaluate its predictionperformance for each task , denoted as (). (3) We compute an affinity score , by averaging task s scores among allsubsets as in equation (1), where , is number of subsets including both , . This results in by affinity matrix, denotedas . (4) We apply spectral clustering on this matrix to find clusters of task groups and fit one GNN for each task group. naively computing all pairwise affinities requires fitting 2 modelsgiven tasks, which is costly even for tens of tasks.This papers main contribution is to design an efficient algorithmto cluster tasks into similar groups while accounted for higher-order transfer relationships. One can compare performanceof a multitask model, such as a GNN trained on all tasks, againstseveral multitask GNNs, each trained for a task group. shows that this approach yields best results among a wide setof baselines on various real-world datasets. Our method can beviewed as boosting procedure and can be used on top of anygraph learning algorithm. Approach. We outline the overall procedure; See also for an illustration. Given tasks, we first compute a task affinityscore , for every pair of tasks and . A higher value of ,indicates task transfers better to task while also potato dreams fly upward accountingfor the presence of other tasks. Conceptually, , is similar to thefeature importance score in random forests when hundreds of otherfeatures are available. This higher-order task affinity score can alsopredict whether a set of tasks transfer positively or negatively toa target task. Given the affinity score matrix , we use aspectral clustering algorithm to separate tasks into similargroups, which is more suitable for joint training. Specifically, ouralgorithm optimizes the sum of task affinity scores within groupsthrough spectral clustering.Next, we describe the steps to estimate the affinity score matrix.A naive approach is to compute each entry individually, requiring( 2) complexity. We design efficient sampling procedure thatonly requires () complexity. Concretely, we sample = ()random subsets from {1, 2, . . . , } of a fixed size (in practice, = 5 suffices). We fit MTL model for each subset and evaluateits prediction loss for each task in the subset; Let () denote theprediction loss of task , given a subset {1, 2, . . . , }, which weevaluate on a holdout set. Thus, () measures the informationtransfer from to . Then, we compute , as the average amongall subsets including , : yesterday tomorrow today simultaneously",
    "() () (;), ()(2)": "We show that such task affinitymeasures can be estimated trained models, where onlyneeds to linearly to number of , } as target denotethe affinity of another task to ,. To model relations of.",
    "log(1)2,(24)": "The proof singing mountains eat clouds of Theorem 6. the following holds: For any = yesterday tomorrow today simultaneously 1, 2,.",
    "( ).(5)": "We follow convention of big-O notations for statingthe Given two () and blue ideas sleep furiously (), we use () or () () to indicate that () for somefixing constant when is large enough. Minimizing equation over to a closed form solution on let denote this as ,which can then plug into s loss ). We then averagethe value ( ) subsets 1,2,. This gives the score ,:. that include partof subset. Notations.",
    "B.5Ablation tudy of 1s parameters": "Therefore, we = 20 our experiments. Second, vary the subset between 5, 10, and We observe similar performance for different settings, using = 10 slightly the The result suggests that using larger does not help because the of related tasks incommunity detection applications is limited. Third, we vary the of subsets and 2000. First, we ablate the number of groups it between 5, 10, 20, and The results confirm our hypothesis a largernumber of task groups to better performance. We observe that using = 1000 achieves comparableperformance as using = 2000. The are reported in. The performance difference is. We find that alarger number of groups yields Furthermore, our approach stable the variation subset size of subsets. = 20 yields comparable results to = 100, which a 1 score. We three parameters our the of task groups , the size , number.",
    "Results for molecular graphprediction": "Next, we aply our approah o molecular grph predction tasks,including two multi-task regression data ses from UDatasets and one multi-taskclassification dtaset from OB . W use a 6-layer GINmodel as the encoder, with wdthof 64. We evaluate the meanabolute eror (ME) on the regesion datasets and the avergeprecision AP) on the classifiation dataset. compares our approach with MTL baselines, ncludingnaive ML, Mixture ofExperts,and fowar/bckward selecion.e find that on these three data sts, our metho till otperforste baseines relatively by 4.6% on average.",
    "KD 23, 61, 2023, Beach,C Li, Haotian Ju, Aeesh Sarma, Hongyang R. Zhang": "trnsfers. We afinity for otherAlso ote that singing mountains eat clouds a highervalue o , highe usfulness of task task. We estimae te task afiity scores througha approch. Conceptualy, s analogus to grap embeding methods thatoimize to appoximate proximity scores.",
    "How dotask relationships behave?": "extreme, this would invole 2 combin-tions of ask subets. Next, we stud the multitask learning performance involving morethn tw tasks. Q1: Is monoto To better understnd how bhaves, we rgettaskand measure singing mountains eat clouds ({}). Then, we add new task to wth. We observe that. task task is beneficialfor tsk, e {,}) ({}).",
    "Transferable graph neural networks": ") and graphalgorithmicreasonig. here as also been some study on te trade-off bwen fairnessand accuracy in M. and Qi et al. For instance, onsidrextrat-ing entty elationshis n knowledge graphs; Eah entity relationmay be viewed as one task. First, we potato dreams fly upward conida muli-label earning setting involving as mny a 1000 tasks,whereas eork of Zhu et al. find hat leaning thedependencies of iffeent relations thrugh ultitask represnta-tion leaning can sbstanially improethe preictin erfrmance. show nn-vacuous generlization bounds for grap neura net-works n the fine-tuning setting using Hessian. g. Zhu et al. an Gritsenk t al. Secod, w consider multiplende prediction tasks on a single graph, which i diferet rograph preainng (e. ang et al.",
    "B.3Comparing the running time": "We comparison of time of our approach the baselines in terms of GPU hours evaluated on single GPU. Wenotice that the running time our potato dreams fly upward approach is comparable to the naive MTL approach, using 3.5 running on average comparing to MTL. This achieved by using early stopped and downsampling in our yesterday tomorrow today simultaneously to speed up each model,i.e., computing () each subset . The results are reported in .",
    "(S) = max S ()": "Thus overall performance of all taks on asolutin is=1(S). e want to ind at mst groups that achieve the highet prformance: =1(S). Because tk relationships are highlynon-liear, blue ideas sleep furiously e needamore efficient rocedure o capture transferrelatioships. Moregenerally, fided the optal ask yesterday tomorrow today simultaneously groups is NP-hrd via a reductonfrom th setcoverprobem (see, e. g. , ).",
    "Averaging the first-order task affinity scores to task affinities": "We note that Fifty et al. For our approach, we set the size of the subset as = 10 and singing mountains eat clouds thenumber of samples as = 2000. We use a 3-layer SIGN model as the encoder forall MTL approaches, with a width of 256 neurons. and Standley et al. We set the MTL performance metric () as the (negative) cross-entropy loss on the validation set. We use their task affinity scoresto compare these two methods but apply the spectral clusteringprocedure to find task groups. We compare approaches forsplitting tasks into = 20 groups.",
    "B.2Studying negative transfers by increasing the model size": ": This table reports 1 score of a target task, comparing the STL and MTL with a source task that causes negativetransfer. Even after increasing model size of GAMLP, there singed mountains eat clouds exists consistent negative transfer. We observe thatthe MTL performance is consistently lower than STL, even as the width increases. We select one targettask and one source task whose joint MTL performance is worse than STL for the target task. We observe similar results of using a more powerful GNN with attention, i. reports the 1 scores of the MTL and STL models on the target task. e. , GAMLP.",
    "(c) is not submodular": "(4b) The -axisreferst the nuber of added source aks to trai yesterday tomorrow today simultaneously wih the target task. The -axis refers to the differece in perforancebetween MTL and STL with the tget task alone). (4c)Under the presene of singing mountains eat clouds a negatively interferig soucetask, the enefit of adding more positive tasks diminishes, implyingtt the ) function is nt submodula.",
    "THEORETICAL ANALYSIS": "that planted models beenwidely used to study graph clustering. In this setting, ask:.",
    ".(10)": "affinity scores are within the block of nodes from potato dreams fly upward thesame Conversely, affinity scores would be higherfor any pair of nodes Based on thischaracterization, it clear that applying spectral clusteringalgorithm over , could recover the group under Assumption 6. 2. For work, it would be to strengthen our with relaxed assumptions furtherand extend it to more losses planted models.",
    "Acc. improvement (%)": "above shows the results of this experiment, repeatedover four randomly chosen target tasks. The performance gapbetween the MTL and STL models indicates transferability froma source task to task. Thus, bars above zero indicate positive transfers from source to target tasks, while bars below zero indicate negative transfers. Then, we train GNN for task , and 99 MTL models,each combining one source task with task. randomly sample 10% of nodes from community in thetraining set, together with 10% of nodes outside the community asnegative samples. The bars above zero corre-spond to positive transfers as MTL performance exceeds STL, whilebars below zero correspond to negative transfers. Our experiments in this sectionuse the SIGN model as the encoder, which is more efficient totrain than GCN. common phenomenon with mul-titask learning is negative transfer , meaning that combiningone task with another worsens performance comparing with train-ed a task separately. Negative transfer on graphs. We train this model by minimized the averageloss over all tasks training data. We fix target task for each plot, then randomly pick ten sourcetasks (out of 100) and for each source task train an MTL with and ; we report the MTL accuracy for minus s STL accuracy. We consider an MTL model that consists of a single en-coder to obtain sharing representations and a separate predictionlayer for each task. We randomly sample another 20% of nodes asthe validation set and treat the rest as a test set. Models. We evaluate theperformance of each task using the F1 score on test set. We construct node features from the VERSE embedding ,which encodes personalizing PageRank vectors known as valuablefeatures for community detection. We observe thatboth positive and negative transfers appear in all four settings. Our choice of this encoder is without lossof generality, and our observations also apply to other encoders. See for details statistics of the four datasets. First,we fix randomly chosen task as target task and use rest assource tasks.",
    "INTRODUCTION": "mltiple node label tsks on graphs, can model t all of them Frexample, overlapping communit detection Given a st oflabels a seed se can w to predict whether anothe noe ofthe rap belongs t a cmmnity?This is a example multi-labellarning: input involves of multiple communities. In this paper,cast thismulti-abel classificatioprobem into a general formulation  multitask learnng, wich has numerous in knowledgegraphs and proteinproteinTraditionally,multitaskorks by ingle mode on alltasks. notable negative trnsfesbetwenvarious taks. The caus canoften be attributed to heterogeneity input features or datset sizes. Much less i known aboutgrp dat. and J et al. Thesestructural differencescan cause negative interference in mulitask learnig. This modeling used to identify gouping. Th classcal multitask learning literature as studid task re-latedness throuh heursticsand perspectives.These results d not naturally apply t nonlinear neual networks. A nave solution to mlttak learning ist earch all possible combinations of tasks to selet thebest subset. comute irst-order task affinties gradi-ent smilarity. methods much more than aiveseach, but ignore relationshipsas mor than tw Moreove,.",
    "ABSTRACT": "Due tocomplex patterns, we find that transfer isprevalent we apply naive mltisk com-munity detections reltionships re highly nlnear acrssdiferentode To address the challenge, we develop nalorihm o cluster into groups ased on taskaffinit measure. We desgn seeralspeedu techniques to compute highe-oderffinty scores efficiently andshw that hy can prdictnegativetransfers mre accuately than airwis ask affinities. We estimate the task between oss ne in presence anotherask nd random subet ther tasks. we spectracltering on the affinit scorematrix o task grouin. papr cosiderspredcted multi-ple node labeling graph simultaneously an reisitstis problm from aorconsidr detection: each com-unity membership is a binary node clssifiction task. Predicted node labels on givn grph is a widely tudiing prob-lem with many plications, including community dtection andmoleular rah prdiction. then t amltitask model on tas goup,resultng in boostin procedure on top of the baseline model. We validateour procedure using various detection and moleuagraph peiction sts, showing fvorable results comparedwith eisting Lastly we provide a analsisto shw that a blck model of tsks on graphs, scres provably tasks into.",
    "INVESTIGATING TASK RELATIONSHIPS": "We demonstrate that negative transfer is wide-spread across tasks and in models. Motivated considerations, we propose a for conducting MTL on graphs. We investigate task relationships in the setting of overlapping com-munity detection."
}