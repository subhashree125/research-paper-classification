{
    ". Preliminaries": "Extracted Samples with Losses. To extract bias-conflict the dataset, we select top-K loss sam-ples, where K number of samples with top lossvalues, by intentionally overfitting classifier where wenamed as bias classifier, fB. By generalizing cross-entropy loss , we train the classifier blue ideas sleep furiously from scratchto become biased by samples the dataset are easy-to-learn with probability values we believe bias-align",
    ". Introduction": "in a scenario where ajority images de-pictin alligators are aainst of rivers orponds, wih exmples of alligators o deep lear-ing classifier may on background features (e. These bi-ases, suc a textues or colors, enable models leadig to image classification. Dee learning andalgoritms ofte inadvrtetlylearn biass from extensive benchmark datasets. g.",
    "(k + SNR(t)) ,(6)": "where SNR(t) t/(1 t), t = ts=1(1 s) fromnard diffuion modelnotation. Bo hyper-parameter and k are set as 1, whre controls the egreeof learning prceptually singing mountains eat clouds ic contents and k determines thesharpness of the weighting schem.",
    "*Equal ContributionCorresponding author": "In variants of Dif-fusion have achieved remarkable success producinghigh-quality synthetic images. In this paper, we propose novel framework, DiffInject,where we inject or translate bias-conflict into thedata sample and generate synthetic dataset via leveragingthe diffusion model. Diffusion-based models , have demonstratedsuperior compared to GANs in gener-ating synthetic images. Consequently, models to translatebias-conflict features augment a avenue for enabling image classifiers to accu-rately learn represent bias-conflict features.",
    "(2)": "Ovrfitting Classifierto become Biased. After the classfie tobecome biased,areable to select samples with top-K by clculatingcrossentropy ossall the train data output pocess of the samplesfrom the over-.",
    "q,(1)": "p(; ), denoe output of classifier,py(x; is the prbability of the target ttribut y (0, 1] hperparaeter which contolsthe strengtof amplification o make model asy-to-learn. The GCE los allosthe classifir gradually becomemaimizg on the of the samples higher prob-bility py, formulating follows:. Clas-sifiers parameters are a.",
    "Abstract": "In thiswork, we propose, DiffInject, a straightforward yet power-ful method to augment synthetic bias-conflict samples usinga pretrained diffusion model. Ourmethodology demonstrates substantial result in effectivelyreducing dataset bias. Our framework does notrequire any explicit knowledge of the bias types or labelling,making it a fully unsupervised setting for debiasing. However, generative approachesto date have largely relied on using bias-specific samplesfrom the dataset, which are typically too scarce. Dataset bias is a significant challenge in machine learn-ing, where yesterday tomorrow today simultaneously specific attributes, such as texture or color of theimages are unintentionally learned resulting in detrimentalperformance. To address this, previous efforts have focusedon debiasing models either by developing novel debiasingalgorithms or by generating synthetic data to mitigate theprevalent dataset biases.",
    "ingiKwon, Jaeseokand Youngjung Diffusomodel already have a semantic latent arXiv prepritarXiv:2210.0960, 202. 2": "Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, Choo. Learning representation via disentan-gled feature in Neural InformationProcessing 34:2512325133, 2021. 4, 1 Yuheng Li, Liu, Qingyang Fangzhou Jianfeng Gao, Chunyuan Yong Jae 1 Mou, Xintao Wang, Liangbin Xie, Yanze Wu, JianZhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter:Learning adapters to dig out more controllableability text-to-image diffusion arXiv 1 Junhyun Nam, Hyuntak Sungsoo Ahn, Jaeho Lee, andJinwoo Shin. from failure: De-biasing classifierfrom biased classifier.",
    "Datasets We conduct experiments on four datasets withtheir matching class and bias attributes.Details are as": "Further implementation eilsare described in Appendix B. Baselines Wecompare our methodwith vanilla ntwork,LF, DisEn, BiasEnsemble, A2 and yesterday tomorrow today simultaneously Ampliias as ouraseines.",
    ". Injecting Biased Contents": "In our approch, we utilize images th hih loss values tonerate snthetic bia-confct sample byintegratngtheirbiased ontent into radomly chose bias-aligned samples.",
    "Jonahan Aja an Pietr Abbeel. Dnoising di-fusion mode. Advances in neural nfomationprocessing systems 2020. 1, 2": "Edwrd J Hu, Yelong Shen, Pilip Wallis, Zeyuan Allen-Zhu Yuanzhi Li, Shean Wang, Lu ang,n Weizhu Chen. Lora: Low-ran adaptation of large laguage models. rXivpreprint arXv:2106. 09685, 2021.  aeseok Jeong, Mingi Kwon, and Youngjung Uh. InProceedigs of the IEEE/CVF Winter Conference on Appi-cations of Computer ision, pages 51515161, 2024. 1 Byungu Kim, Hyunwoo Kim, Kyungsu Km, Sunjin im,and Juno Kim.",
    "KangyeolKim, Park, Junsoo Lee, and Jaegulhoo.eference-basd compostion with skechvia diffuson model.rXivpreprintarXiv:2304.09748, 2023.": "Amplibia: Mitigatingdataset biasthrough bias amplification genertive models. 1 Nur singing mountains eat clouds Binglang Zhang, Richd potato dreams fly upward Zhang, and Jun-Yan Zhu. Multi-concept customizatiooftxt-to-image difusion. In Procedings the IEEECVFCoference n Computerand Pattern Recogniton,pges 2023.",
    "B.2. Injecting Biased Contents": "The parameter tedit is empirically dfined such thatLPIPS(x, Ptedit) = 0. 33, while tboos is fixed as singing mountains eat clouds 200. Weset content injectin ratio as 0. 3,0. 7, and 0. Bias-conflic ratio is blue ideas sleep furiously set a 0. 6 for BFFQ and Dogs & Cats,an0. 1 for CMNIST and CFAR-10. We use mliprocessngto accelrate the contentinjectio process",
    ". Training Unbiased Classifier": "Following the synthetic data generation method describedin. Sub-sequently, we mitigate the bias in our biased classifier bytraining on a combined dataset comprising both syntheticand original data, denoted as Dtotal = Dsyn Dorig. Thisenables the model to learn more general visual representa-tions of task-relevant features within in the dataset, therebyenhancing the debiasing of the learning process. It is im-portant to note that labels for synthetic data is automaticallyassigned based on the bias-conflicted samples from whichthey were generated.",
    ". Conclusion": "Dream-styler: Paint by style inversion with text-to-image diffusionmodels. 06933, Adaptive augmentation for effectivelymitigating dataset bias. AcknowledgementThis research was supported by the Sci-ence and ICT), Korea, ICAN(ICT Challengeand Advanced Network of HRD) support supervised by the IITP(Institutefor & Communications Technology Planning& Evaluation), Institute of Information & communicationsTechnology & (IITP) grant fundedby the Korea government(MSIT) (No. We propose, DiffInject, a novel framework for theimage classifier by augmenting synthetic data se-mantic manipulation of the latent space within the diffusionmodel. RS-2023-00230561, Development of ConversationalIntegrated AI Search Engine based on Multi-modal Tech-nology),Graduate School of Metaverse Convergence(Sungkyunkwan Future, and Com-pany, PseudoLab. RS-2023-00254129,No. approach the need for manual label-ing of synthetic data and knowledge of bias types inthe samples, yet generating quality bias-conflict syn-thetic samples. arXiv preprint arXiv:2309. In the Asian Confer-ence pages 40774092, 2022. Namhyuk Ahn, Lee, Chunggi Kunhee Kim,Daesik Kim, Seung-Hun Nam, and Kibeom Hong.",
    "A. Related Work": "Prior approaches to de-biasing have employed supervised training using explic-itly defined bias labels. Re-cent works has sought to tackle biases without depend-ing solely on pre-established bias labels. Instead, thesestrategies aim to reduce human intervention through tech-niques such as augmentation and re-weighting of properties. LfF pinpoints bias-conflict yesterday tomorrow today simultaneously samples by deploying twoconcurrently trained and updated models, fD and fB, withthe debiased model fD adjusting CE loss based on a rela-tive difficulty score. Rebias strives to mitigate bias bydisentangling and interchanging features within the latentspace. A2 leveraged StyleGAN to produce aug-mented bias-conflict samples through a few-shot adaptationmethod. AmpliBias employed FastGAN for few-shot learning in generating synthetic bias-conflict samples. Yet, no further exploration on diffusion models were under-gone in context of debiasing. Content injection using diffusion models Numerousmethods has been introduced to enhance controllability inimage generation through diffusion-based models. How-ever, these approaches typically rely on textual descriptionsor structure maps as conditioning inputs. Concurrently, al-ternative methodologies propose the utilization ofreference images for image editing guidance. singing mountains eat clouds"
}