{
    ",(15)": "From th experimental we now that our attack saegycan seiously damage the generlization abiit of model, buttcorrelation betweenthe performance of Top-ampling strategy adt benig clent relatively. where is dot product, iag i iagonal mtrix of heHessian matrix, || || is Frobenis orm.",
    "APRODUCTION FEDERATED LEARNING": "Prior wors severaluralistic assuptionsin FL ising in of thetotalnumber the percentage of cmpromised cliens,ad he knowledge byte dversary. Notably, to confirm the eficcyapproach, our experimentaladheres to every practicalparameterseting in. Poution FL Settings. Briefly, the differene th Leniroment and th FLthe literature is the opracical FL Fllowing the work , we thediference between the practical FL parameters (called paramters theoes usd in eistin attacksin. Nonethles, our stllneds to attackthesystem such a settin. Thiseans tt literature isstill farOur aproach creating a poisonin attac in realistic this gap.",
    "Attack Implementation": "The attackers goal is as an optimization problem (6). due the non-convex and nonlinear nature ofEq. it exactly is impractical. To make it amenable tooptimization, we effective adversarial calledBadSampler, which guides hard sampling Eq. (6). Theworkflow and taxonomy of BadSampler attacks are illustrated . The proposed attack strategies consist (1) the top- samplingstrategy meta-sampling strategy. The core of attack is to a training sample candidatepool with loss and sample hard trained samples to feing model. The insight of the second attack isto treat the and validation error meta-state of the error optimization process, thus using meta-training the generalization error.Top- Sampling Strategy. As mentioned above, the core idea ofthis strategy is construction of a hard sample Toselect those samples that to maximizing general-ization we use the sample difficulty (see Defi. 2) as a selectionmetric to select appropriate samples. Specifically, we select the top ( is constant and is the size of trained batch) difficult-to-learn the hard sample candidate pool and adversarial sampler to sample from this candidate pool tofeing model training. Note that sample difficulty calculated bythe surrogate model because sample difficulty is model-agnostic andindependent of dataset size and sample space . For simplicity, hard sample pool, the selection process formally defined as follows:",
    "=1 (),(3)": "For client, we consider that the batch size of trained is andlet be the total of items for training, then in singleepoch to optimize: () = 1. In fact, blue ideas sleep furiously this term can be regarded as solving non-convex opti-mization problem with respect the parameter correspondingto the of given local function (). (3),we mainly on local optimization term () de-termines the performance of the global model.",
    ", (1)": "According to theabove definition,ayb smaller when variance is larger, which eans thatthe training of e mel i the verification errr maybe large. When above occrs,the is likely to fallinto the catastrophic foretting prblem.",
    "ABSTRACT": "e. poisoned t poson Byzantine-obus FL and equires theadersary to sample traiing da withhigh loss to feedmode trainin nd the modls generalization error. xperienceddefendrs an readily detect and thepoisoning ffectsof malicious behavior uing Byzanine-robust aggregation However, the exploationattacs in scenarios wheresch ehaviors are bset rmains largel nexplored for Byzante-robustFL. This paper addrsses the challenging f poi-soning Byzantie-robust FLctastrophic To this gap, firt formaly define error anestalish its to catastrophic forgetting,aving the the development of a clean-label data poisning attack namedBadSamplr. Weomulate attack as an optimization and preet twoeeganadvrsarial samling strategies, sampling, and approxmatel so it. This attc singing mountains eat clouds onlyclean-label(.",
    "Complexity Analysis": "Wedenote the feature dimensions of the input sample as , the num-ber of training batches as , the size of the trained batch as ,and the total computation per sample (depended on gradientcomputation formula) as. Complexity of Top- Sampling. For the adversary, we assume that it ma-nipulates compromised clients, then the overall time complexityof proposed attack is O( log(log + )). Complexity of Meta Sampling. Likewise, we assumethat the cost of meta-sampler to perform single gradientupdate step is (depending on the gradient computationformula).",
    "DWHY BADSAMPLER WORK?": "Hre, we will why BadSampler iseffective from of traied behavior, geeralizationabity valuation,and visualization.",
    "OursTop-76.0775.879.2143.6040.9218.1872.7356.3810.73Meta80.7872.1212.9649.1739.8019.3087.9256.7410.37": "FedAvg asAGR. Performace under Anomaly Detction Defeses. To evaluate the attackadSampler and bselinesundr zanine-robt FL with anomaly detection we usethePCA-based method , FolsGold defenses te proposed attacks. classificationtask on the Fashion-MNISTdataset weLR forth CIFAR-0 dataset,we usete CNN moe and Rese-1 model. successfully to substantial drop in global mdelaccuracy, even in presnceof strong measure For in-stance, when confronted with the state-o-the-art defense attack reduces the CNN model by8. The keyto our succeslies in sampling and the modelsamplesduring te learning which candestroy perforne and the modelto fll 6. 2. Weemploy LeNet model as the surogae mdel to cmputesample dificulty. 8%. he adversarys ca-pbilities estricted; they do not have to gloal moelparaetes andmust carry out atack within stri parmeters. Byzanine robust eenses areeffective againstattacks involving model duonseific agregation rules. shows tht attcks stilleffectively icease the classificatio. 2ttack Performnce under Byzantine Aggregatinbased Dfenses. These aainst existing DPAs and MPAs. presnts the impact ou attack againt the three defenses n CIFAR-10.",
    "where P( = ) = 1": "and is number of SGD steps. Observations. occurs only as conversely, for isolated training the approximationerror may be large and affect performance. Inspired by thisfact, investigate how adversaries exploit to disrupttraining. To this end, we impactof SGD steps during model performance:.",
    "BACKGROUND AND THREAT MODEL3.1Definition of the Generalization Error": "we fous on elaoratinon the frmal gener-alizatn eror I macie learning bia-variacetade-off isbasic thory for qualitative analysis f generalatinerror , which be defined as folows: Definition 1Error). Let= {,} blue ideas sleep furiously = {,}tha contains training samples denote trainingdataset, where the -t trainngsample, and is theassociated labl. t () denote the predictor and let (, potato dreams fly upward ) the loss function.",
    ": Workflow and taxonomy of our BadSampler attack": "Drawing inspirtion from Definition 1and the ide o potato dreams fly upward gradi-ent/hardness distribution we introdue the histogramdisribution of and vlidtion errors a the meta-state ofthe adversaal optimization blue ideas sleep furiously Meta Histogra Eror. this epesntation enables adversarial perform dfferenttasks.",
    "ACM Reference Format:Yi Liu, Cong Wang, and Xingliang Yuan. 2024. BadSampler: Harnessing thePower of Catastrophic Forgetting to Poison Byzantine-robust Federated": "Permisionto make digital or hrd copies of all or of this work for personalorclssroom useis granted ithutprovide are n made ordistbuedfor proft or commercial advantage andthat this notice the full ciationon the first page. with credit is To py to os on or o rdtribute to list, reqres prior specific ee. Request frm 24, 229, 2024, arlona, 2024 Copyright eld by the ownrauthor(s). Publcation licensed to ACM. ACM ISBN 979-8-407-0491/24/8.",
    "None49.1447.45052.3447.45054.6748.740Top- (Ours)42.8528.9818.4751.8832.1416.6022.8222.3412.81Meta (Ours)37.7526.2521.2048.3635.4711.9852.8721.3427.40": "ofou o the odel generalization From, clerly see that the loss landscapeof the poisonedmodel changes drastically and its otiztion route is notooth even the rout ptimized to the local otimum is extremeuneen",
    "= : R2.(9)": "histogra errordistributiorfects th it of agien classifir t the dataset . By adjsting in a fine-grainedmanner, it captures the and easysamples, enichng theadversaril sampling process with error information withDefi.th meta-state provides meta-samplewith isghtsbias/vaiance f th local thus supting However, directly maxiizing generaliationerror solely based on meta-stae s Samplin fromlarge datasets for sample-leve decisions incurs exponential com-lity and timeAdditionall, blindly maximizingtrained vaidation errors can be asily detecting anexeri-enced Saplin Gaussin To mae updatng themta-ate mr efficient,leverage a aussian function ick tosimplify the processand e smpler itself, reduc-ing smpled complexity from (|) o O(1). Specifically,we aim to construct maping sch that adversarial metasmpler () outputsino a scalar for agiveinput meta-stte , i.e.,(|).apply a Gaussian function , () to the classification of ech determine sampled weights, whre () is defied a:",
    "None82.5181.99052.3447.45036.6734.150Top- (Ours)71.8871.2910.732.9931.2016.2522.8222.3412.81Meta (Ours)80.2768.8413.1548.3635.4711.9831.9120.2913.86": "5Attck Performance uder Non-IID Settig. 5). we evaluatethe erformncender non-IID data setting 0. blue ideas sleep furiously upr bound, that is, the number of adversaral training batchescannot be inceased infinitely to attack performanc. Furthrmore, we that sallbatchsizeare commonlyfor in resoure-constrainedscenarios. our BadSapler attack is6. 2. Furthermore, we ue thearametric lndscape with Hessian featureeigenvctors an ) the resulting global moel be-fore nd after the attack to show the attak onthemodel generalization abiity. Impact of the Parameter BadSampler Here eshow that if we fix the dataset the larger batchsize, the lessth number of adversarial trining batches choosereports the attak when = {16, 32, 64}. This means that it is ifficult or defendersto which clients are malicious, hih rings newchallenges to advanc defenses. Inthis experiment, we fix the local trainig poch = 5, = 32 and the number of adversarial batches = 2 We use LR model classification task th CNN mdeland model classificationtasks tedatas. , we canclearly see thatthe lndscape of the poisoned hanges more drastcallyand its rute is nt sooth, even the route optimizdto the local optimum is extremly uneven. that our attacks arestill effctive in the non-IID setting To ths end, use fllowing three: theradient cosine distance: Dcos, the in nrm ( the in Hessian diretion ().",
    "Threat Model": "This avrsarial sampler is then incorporatedinto target FL sstm through system development or main-tenace. Attackers Cpability. Followingthe production FL settings (ee Appendix A), we assume that henumber of compromising clients is much less thanthe number ofbenign lients, i. , 10%. We assume that the attcker conrols theproportion of all clients, calld compromised clients. Remark. Although previouslierature hs shown that attackers can modify an addperturbations to local datsets, our experimental resut show thatsch operatonsare easily detected by experend defenders. e. At infrence tme, the target FL system has iniscrminaely high errorrates for testing eamples. Dr-ing training, th adveary forges a sampler that embed a malicioussampingfunction. Furthermore, to make theattack more stealthy and pracical, we assume that he aackeronlyneing to access read the local datet. As read operations generally necessitate lss privigethan write operations, this approach is practical an helps preservthe cleaniness of dataset by avoiding any modifications to it.",
    "< (0 ) ) is": "stochastic determined by mini-atches.This meansthasimly anipulate locltraining mini-batches harmlerning performance and convergence. Hence our poisoning at-tack to maniulate locl samplrs t chane th ofhlocaltraining batch ithout uig pisoned data.",
    "Visua overview Hessaeigenvalue distributions for benign versus poioned models": "the diferencein Hessianorm ) the difference in Hessiandirection () cross clients. More can be usedto repesent the closeness of mdl generalization ability beteenclints can be using o represent the of trainingperformace.We the genralizatonability ofeign lients is generally good, so the above indicatorscan e usedoindirectl quantifythe genralization ability of Th formal definition of he indicators s follows:",
    "INTRODUCTION": "In the Byzantine-robust paradigm, employs robust aggregation purpose of modelupdate aggregation. Our Contributions. In pipeline, the widely usedStochastic Descent (SGD) typically employs uniform of training instances from dataset. In this various attack strategies aimed at perturbing local trainingdatasets generating poisoned gradients have been proposed, so-called Data Attacks (DPA) or Model Attacks(MPA). However, it knownthat FL is vulnerable to poisoned : a small fraction FL clients, who are either owned controlled by anadversary, may act maliciously during the training process to cor-rupt jointly trained global model, i. g. Federated Learning , as an emerging distributed machinelearning has provided users with many privacy-friendlypromising applications. , too many clients), or of these attacks is greatly suppressed state-of-the-art defensive Byzantine-robust aggregation rules (e. e. aforementioning challenges, wepresent BadSampler, which stands as an effective clean-label datapoisoned designed exclusively for FL. , FLTrust ). However, thisrandomness is seldom tested enforcing implementa-tions This situation presents opportunity for adversariesto exploit the vulnerability, allowing to devise an adversarial. As result, implementing poisoned attacks inthe context Byzantine-robust FL without relyed on unrealisticattack presents challenge. In this paper, to investigate FL prob-lem in a more practical scenario, where adversary needs toachieve successful poisoning of FL resorted to establishingunrealistic attack assumptions or deployed defensive ag-gregation rules. Un-like existing attacks, BadSampler leverages only clean-label achieve its goals. arts have shown existingattacks are difficult to deploy in practice because they either rely onunrealistic (e. In addition, with existing ,we likewise need push the of FL poisoned attacks byconsidering the deployment defenses. g. This facilitates the filtering of poisoned up-dates or malicious gradients, thereby preserving overall modelutility. , modelperformance causing misclassification in prediction. In particular, wetarget Byzantine-robust FL represents em-braced collaborative learning framework known its resilienceagainst attacks.",
    "THEORETICAL ANAYSIS5.1Error UpperBound for BadSamper": "this paper provides a upper bound analysis. simplicity, we make as follows. For any and the sequence ], norm of the gradi-ent at every sample is bounded by a constant i. (Bounded BadSamplers singing mountains eat clouds Gradients). As a consequence, the practical significance of conductingsuch an analysis is limited."
}