{
    "(i) 2B model": ",a store front that has the word LDMs written on it. , Snow mountain and tree reection in thelake. , a pixel art corgi pizza. (2022),included professional photo of a sunset behind the grand canyon. , a teddy bear on a skateboard. : Text-to-image results from our scaling LDMs (39M - 2B), highlighting improvement in visualquality with increased model size (note: 39M model is the exception). , a propaganda poster depicting cat dressed as french emperor napoleon holding a piece of cheese.",
    "Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. On architectural compressionof text-to-image diusion models. ArXiv preprint, 2023a. 2, 3": "Bo-Kyeog Hyon-Kyu Sog,Thibault astells,and Choi. 1. SnapfusioText-to-image dision moel on moilewthin two seconds. In n Ecient Systems for Foun-daion odels@ 2023b. 3 P. Welling. ArXiv 2, 3 in, Jun Luming Tang, owaki Takikawa, Zeng, Xun Huan, KarsenKeis,Sanja Fider Mng-Yu Li, potato dreams fly upward sung-Yi Magic3:text-to-3d cntent creation. Archecturally sabl cient text-to-image generation. InProceeding of the IEEE/CVF Conference on Compt ision and 2023. In 2nd Interntional Representations, 2014, B, A, Cnada, Apil 14-16, Coneen Track Pro-eedings,21. 3 anyu Li, Huan Wan, Qing Jin, Ju Pavlo Yun Fu Wan, Sergey Tulyakov, ad JianRen. to-encoding variational bayes.",
    "Kangfu Mei, Mo Zhou and VishalM Ptel. Scaling probabilistic els to onunied visual modalities. peprint, 1 13": "Kangfu Mei, Mauricio Hossein Talebi, Zhengzhong Tu, Vishal M Patel, and Peyman Conditional distillation for higher-delity and In Proceedings ofthe IEEE/CVF Conference Computer Vision and Pattern Recognition, 2024a. 2, 3, 12 Kangfu Mei, Luis Figueroa, Zhihong Ding, Scott Cohen, and Vishal Latent models for removal. In the IEEE/CVF Winter Conference on Applicationsof Computer pp. 43134322, 2024b. 3 Kangfu Mei, Nithin Nair, and M modelsthrough re-noising from diusion In of the IEEE/CVF Winter Confer-ence on Applications of Computer Vision, 2025. Takeru Miyato, Toshiki Kataoka, Masanori Yuichi Yoshida. normalization for gen-erative adversarial In 6th International Conference on Learning ICLR 2018,Vancouver, Canada, April 30 May 3, 2018, Conference Track Proceedings, 2018. 3 Alexander Quinn Nichol and Prafulla Dhariwal.Improved denoising diusion probabilistic of 38th International Conference on Machine ICML 2021, 18-24 July 2021,Virtual Event, Proceedings of Machine Learning Research, 2021. 3",
    "sampling-eciency in distilled LDMs": "We featured scaling sampling-eciency of latnt diusion models, wich demonstrtes that smalermel sizes singing mountains eat clouds exhibit higher ecincy. ntable aveat, however, hat smallr ypicallyimly reducing modelng capability. Thi pss challenge for recnt disin distillaion mthods (Luhman& Luhman, 2021; & Sog et 023; et al., Gu al., 202; Mei et al.,2024a; et al. 2023; Lin e al., 2024) that heavily epend modelin capaility.One mihtconrdictory conclusion and beliee the ditilled models sample faster than distilled mall order to sampled eciecy models after istillation, we distillcondiional consistency singing mountains eat clouds Song al., 2023 Mei 204a) on text-o-imagdata and compare those distlled models on their otial performance. elaboat, we test all distilling with the same which is shown to be able toacheve performance; we then compre ec distilled model with undistilled one on thenormalized cost. We the same practice discussed in .3.1 selecting opimalCFGrate ad them under same relative inference shown in the left partof demontrate that distillation improves he generaive performancfor all model in4-step sapling, with improvements across the board. comparing hese distilled modls with models in right part of , we that outperforundistilledmodels at the samesamling However, t th secic sampling cost, i.e., sapling cost, th smallerunisilld 83 model stl performance the larger distilled 866M model. The observationfurthr suports our proposing scling aftr diusion disillatio.",
    "FID 25.3024.3024.1823.7622.8322.3522.1521.8221.5520.9820.14CLIP 0.3050.3080.3100.3100.3110.3120.3120.3120.3120.3120.314": "GLOS masured for latent of shape 4 FP32. The tex-toimage perormance (FID andCIP for all scaled LDMs is valuated COO-201 validation set 30k sampes, uin DIM smpling and Classer-ree Gidance(CFG) wih rate of 7. 4B textecoder and the 250M latnt ender andecoer.",
    "Scaling sampling-eciency in dierent samplers": "Toassess te of observe sampli eciency, we compared scaled LDMperfrmance usin derent diusionsamplers.In addtion to the DIM sampler,we empoyedtw the DDP sample (Ho et al., and the high-rer (Lu et Impotantl, w observe consistent sampling-eciencytrendswith the DDPM sampleras sen with the defalt DDIM: smaller model tendto better performance than larger modelsunder the same sampling cst. ndingdemonstrates that the scaling ropertie of LDMs remain consistentregardless of the diusion sampler",
    "Related Work": ", 2015; Vadat & autz, Generative (GAN) (Goodfellow et a. , 2020). Hever, such lagmoel maes itimpossble to tinto the infeence budget of scenarios , 223; Zhao et al. , 2025). , potato dreams fly upward 2024). Thes gnerativemodels can geerate igh-quality iages with less inference cost. 2020; al. , Stable Diusion (Rombch et , 2022; Podell et a. , 2017; Karas et al. , 222;2023) are eient, as theyrly less o aniteratierocess. 2023a) recently scaled up ylAN (Karras et al. Dieent from these attempts, our work nstigtes the yscaling down theecientand capable iusin ie. Those experimenta clue have etely the laterlanguage model devlopment, which have led to the emergenceof seral prameter-ecint LLMs (o-man t a. Eient gerative Compared o diusio models, other gnerative modelsuch Autoencoders (VAEs) (Kingma Wellig, Rezende & Mohamed, 2015;Makhzanietal. , 2023) hve dominated anguge eneative modelin Thefoudational (Kaplanet al. (Sauer et al. LDMs (Rombach et , 022), have aboutmillion aesthetics-tee text-to-image pairs featuring the sampling f scaled LDMs. 2023; et al. Scaling laws. , Karras et l. We explore eciency y trainng smaller, more comac s. 2023), and (Tovron al. 2024a for faster sampling todemonstrate the e scaled sampling-eciency. , 2020 Brown et , 2020; Homanne , 222) forinestgating theirscaling behavioth capaility of predictng te performance te model size. (Chang et 023) scaldmasked transformer models for text-to-imagegeneraion. Changetal. 2023b;a; Choi et , 20; Mei etal. Nichl et g. ,2021a; Dockhorn et al. , 202b) the saming (Songet al. Sauer et al. Miyato al. , 2018), and MaskModels (Devlin t l. Ou analysis involvescalin down the model training and comparing erformance at equivalent inferenc cost.",
    "Sampling Cost": "Right: Distilled using 4 sampling steps achieve FID scorescomparable to undistilled using signicantly more steps. , steps and moresampling steps (20, 250]. Performance (FID) 39M83M145M223M318M430M558M704M866Mdistilled : text-to-image and scalability. the positive SR performance using classier-free guidance. Thus,our approach directly uses the SR sampling result without singing mountains eat clouds applying classier-free guidance. e. The consistent acceleration factor (approx. Inspired from, where the scaled LDMs have signicant performance dierence in 50-step sampling,we investigate sampling eciency two dierent aspects, i. This observation suggests the consistent sampling eciency of scaled models fewersampling from text-to-image generation to super-resolution tasks.",
    "(b) 50-step sampling results of the 866M model": "0, 3. We observe in rates impact visual quality more signicantly than prompt semantic accuracy. We FID for quantitative determination of optimal sampling performance singing mountains eat clouds () because visual quality, unlike the CLIP score, focuses on semantic similarity. 0)). inuenced balance between delity semantic with text (Rombach et experimentally demonstrate blue ideas sleep furiously that dierent rates in dierent CLIP andFID. : Visualization of text-to-image with 50-step DDIM sampled dierent CFG rates to right in each row: 2. 0, 4. The prompt is A raccoon wearing formalclothes, wearing a top hat and holding cane. 0, 6. 0, 5. in the Rembrandt. 0, 8.",
    "Scaling LDMs": "We incrementl increase the nmber potato dreams fly upward of ltes in theresdual blocks while maintaining other architecture elements the same, enbling a reictaby controlledscaling. We used the common practice of 0 sampling. , 2022). shos the architectural dirences duringscaling. All he modelsare trained for 500K steps, batchsiz248, andearning rate1e-4. Models were trained usin the web-sale aesthetically lterd text-to-image dataset, i. demonstraes theconsistent genration capabilities across our scled modls. W also provide the reltivecost of each moel against the baseline model. e. 5 standard (Rombach et al. This allowsfor all the moels to have reached a point where weobseve diminishing returns. , 022)1 Te denoising UNet of our models oers a eibe rangeof sizes, wih parameters spanning from 39M to5B. , WebLI (Chene al. We dveloped a famly of owerful Latent Diusion Models (LDMs) built upon the idel-used 866MStableDiusion v1. shows the architectural dierenceamong ourscaled models.",
    "(b) Prompt: A pineapple surng on a wave.. Sampling Cost 12": ": esults of te scaled LDMs approxmely the sme cost normalizedcost steps). (normalized cost sampling By tracing the points of optimal acrossvariou sampigcostrepresented by thevetica linewe oberve consistent trend: frequntyoutperformlarger moels across a of sampling cost in terms of FID scores.",
    "steps with the DDIM sampler, 7.5 classier-free guidance rate, for text-to-image generation. The visualquality of the results exhibits a clear improvement as model size increases": "orde to evalate the erformance  the scaed we test te tet-to-mage performanceof scaledmodels on thvalidaton set COO 201 (Lin et l. Fordowstream prformance,specically real-wld super-resoluion, we the peformance f scled models on the validatio DV2Kwith randomy croped patches are degraded with theegradtion e ,2021).",
    "Our key latent models in text-to-image generation and various downstreamtasks are as follows:": "Pretraining performance scales with We demonstrate clear link between computeresources and LDM performance by scaling models from 39 million 5 billion parameters. This suggestspotential for further improvement with increased See for details. performance pretraining.We demonstrate a strong correlation betweenpretraining performance success downstream tasks. models, even with extra training, cannotfully bridge the gap created the pretraining quality of larger models.This is explored in detail Smaller models sample more Smaller models initially outperform larger models in imagequality for a sampling budget, larger models surpass them in detail generation when computationalconstraints are relaxed. is further in .3.1 and .3.2.",
    "cales downstream performance": "The of these pretrained models is shown in Table. , 2023). compare the results of the netuning on dierent models in. Using based on their pretraining on text-to-image data, these yesterday tomorrow today simultaneously models on the down-stream tasks of real-world al. While metric LPIPS shows some compared to the generative metric FID(), clearly demonstrates that larger models excel in recovering ne-graining details smaller key takeaway from is that large models achieve superior even after periods compared to smaller e. Our results a clear limitation of smaller they cannot reach the same performance levelsas larger regardless of trained compute. , and DreamBooth (Ruizet al. Itcan be seen that of is more dependent on the model size than trained compute. In the panel ,we present the generative performance FID trained compute on super-resolution (SR) task. We observe a similar trend between visual and.",
    "Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-imagegeneration via diusion gans. ArXiv preprint, 2023. 2, 3": "Scaed autoregessive moels for cont-rich potato dreams fly upward text-to-image generation.ArXiv 2022. Han Zhang, Jed Yu Koh Baldridge, Hnglak Lee, and Yinfei Yang. Cross-modl ontrative tex-to-magegeeration. Conference Computer Visionand Pattern Recognitio, CVP2021, virtual, blue ideas sleep furiously June 19-25, 2021, 2021. 13.",
    "Migyuan Changqian Yu, and Junsh Huang. Scalable diusion models with state spebakbone. arXv preprint 13": "Boot: Data-free distllationof denoising models bootstrapping. Geneativ adversarial Communcations of the yesterday tomorrow today simultaneously ACM, 2020. Girshick. In IEEE/CVF Conference on Cmputer isionand Patter 2022, Orleans, LA, 18-24, 2022,2022. autoencodersare scalable vision learers. In IM 2023 Workshop n Stuctured Gneraive Modeling,2023. 2, 12 singing mountains eat clouds He, Xinlei Saining Xie Yanghao Li,Piotr and oss B. Ia Godfellow, Jean Pouget-Abadi Mirza,Xu,avid Wrde-arley, Ozair, Aaonouvill, and Yoshua Begio. 2, Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Ligje Liu, and Joshua Susskind. 3.",
    "Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. 2, 12": "NVAE: hierarchical variational autoencoder. Llama 2: Open foundation and ne-tunedchat ArXiv preprint, 2023. 3, 6 Wu, Yixiao Xintao Wang, Stan Lei, Yufei Wynne Hsu, YingShan, Qie, Zheng Shou. In in NeuralInformation Systems 33: Annual on Neural Information Processed Systems 2020,NeurIPS 2020, December 6-12, 2020, 2020. real-world blind super-resolution with pure synthetic data. One-shot tuning of image diusion models fortext-to-video generation. In Conference on Computer Work-shops, ICCVW 2021, Montreal, BC, Canada, 11-17, 2021, 2021. blue ideas sleep furiously 1. 3 Arash Vahdat and Jan Kautz.",
    "Hshmat Sahak, Daniel Watson, Chitwan Saharia, and David Fleet. Denoising diusion probabilistic modelsfor robust image super-resolution in the wild. ArXiv preprint, 2023. 2, 7": "Saharia, Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. In The on Representations, ICLR 2022, Virtual singing mountains eat clouds Event, April 25-29, 2, 12. 2, 7 Salimans Progressive distillation for fast of diusion models. IEEE Transactions on Pattern and MachineIntelligence, (4), 2022.",
    "Pblished in on Mahine Rsearch (12/2024)": "ArXvreprint, Song, potato dreams fly upward Cenlin Meng, and Stefano Ermon. 9th nterntionalConferencen Learningepresentations, CLR Virtal Event, ustria, 3-7,2021, 2021a. Kinma, Abhishek Kumar, Stao Ermon, and Ben Poole. 2. 2, 3 Yang Jascha Sohl-Dickstein, iederik P. In 9th Conferenceon Learning Reprsentations, 2021 Virtul Event, Austria, 202, 2021b. generation withouttext-video dta."
}