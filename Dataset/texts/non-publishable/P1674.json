{
    "for C1(), C2(B) independnt F, D2(F) of B, with 0 C1(B)C2(B) 0 D1(F D2(F)": "By(1), decrease of f(f; ) to change in yesterday tomorrow today simultaneously f correspondsto equivalent decreaes of KL(M F) fra fixd b, or B. each optimiation step, decreases of ) Lb(; correspondto equivalent of KL(M B0,1() and KL(M potato dreams fly upward F0,1) B)). th lsses )and Lb(; ) cnnot be decreasing in , i. e. at and if F() = B(), then B() = S.",
    "Conclusions": "In this work Bridge Matching a novel approach learned Schrdingerbridges from samples. BM2 blue ideas sleep furiously builds on principles of Bridge while key limitations ofexisting iterative Our offers advantages, included simple single-loop optimizationprocedure, exactness blue ideas sleep furiously idealized setting, modest memory requirements, and a straightforward loss The experiments demonstrate that BM2 is with and often outperforms existing iterativediffusion-based methods like I-BM and DIPF across various dimensions and entropic regularization Firstly, while some tothe standard convergence result for the EM a quantity analogous to the maximizing in algorithm. It unclear whether decreases in KL(M B0,1() andKL(M B()) can be linked to decreases in KL(B()S). Secondly, the requirementthat = B(), equivalently that (F()) (B()) time-reversals of each other, appears unnecessary. all numerical simulations conducted not explicitly enforce this condition, which emerges the trained process. it would be to study problem (16) where KL divergencesare partially aligned more closely with BM2 algorithm. this scenario, Lemma 4 nolonger holds, it may necessary to impose a corresponding additional constraint to maintain tractableanalytical of (19), of system arising fromreverse KL minimization, can be investigating to assess further properties of BM2. On the empirical front, the applications of BM2 generative machine learned tasks remainunexplored. promising results previous studies employing Bridge Matching, such as those byLiu et Future work could investigatethe and of BM2 in these domains.",
    "Bridge Matching (BM)": "We succinctly review Bridge Matching, and refer to Peluchetti (2021; 2023); Shi et al. Firstly, astochastic process Q0,1 is constructed as mixture of diffusion bridges (R|0,1), such that the endpoints(X0, X1) of X Q0,1 are distributed according to Q0,1. Consequently,X M Q0,1 is a diffusion process for which X0 Q0 and X1 Q1, i.e. it defines dynamic transport fromQ0 to Q1.",
    "Published in Transactions on Machine Learning Research (12/2024)": "Forward-Backrd SD: Chn al. (2022) propose tw training algorthms addressing the prolem.Bth approaches loss functins that equire divergene(violating dsideraa(iv)) and the use of two distinct networks. Oncethis achieved, solutions to dynamic prble are obained hrough ecompositionS = Athough these wor ffer i nature and objectives,we include tem here due to thei witBM2 the non-iteratie nature of the algoritm. Light B: in tw Korotin et ushchin (024) upon ths approximation and introduces an additionalsample-basedtaining objective that takes s input C0,1 C(, whreas Koroine (2023); Gschinet Consequenty thesemay face challenges incalng tomodern generative",
    "(12)": "The subsequentoptimization step emplos. For completenes, w outline the standarSG trainig loopin Agorihm 2, were sgdstep) reersto an updte step vi a geneic grdent descent optimizer.The introduction of themiig procss s crucial in ensuring this propert.",
    ": Mote Carlo of KL(S ) function of nd devation in gray": "1, a neiter aproachdemonsrated cnsistent performane improvements acrss onsideing bechmark. EOT: amplin from the EOT solution, accuntng forth bias due to Monte Carlo estimation. 99. In, e additionally include three baselines. SB(disc): sampling from Solution vi he SB-optimal drifts, additionally ccunting or EulerMaruyama schee discretiztion rror. Results for additional discretization intervals are repotedin Apendix C. , 2021; Shi et al. For I-BM and DPF, ach outer oop iteration comprises 5, 000 SG steps, totaling 10 outer loop (algorithmic)ierations. We alsoconsider BM2, a variat of BM2 tat learns Schrdinger bridges fr , 1 acrss multple values. This amortized version leverages M2s non-iterative nature. 1, we iplemet pah ahed and aexponentialmving average for parameters usedin ath sampling The cache contains 5, 000 inital-erminal aluesfrom boh (F()) and (B()), refreshed every 200 traiing step. 2, and weillustrate in teevoluton of (21) dured rainig for a representative benhmark setting. swithBM2, we implement path caching (forDIPF entire discretize paths are cached) and EM for ampling,with an EM decay rate f 0. Folowing best pactices (Bortoli et al. , 223, we alernate time directions overiteraions for both algorithms Each method employs two sepaae neural neworks for forward and backwardtime direction, mantaining a total paameter count close to 1 illion, matching BM2s moel size. The neural netwok implemented drft functions f(x, t, ) and b(x, t, )is modiiing to accept asan additional input, resulting in conitioal drift functions f(,t, , ) adb(x t, , ). r BM2, we employ a single feedfoward neual networ with 3 layersofwidth 78 and ReLU activation,resulting in approximately 1 milionparameters.",
    "Complete Minimization": "(2023); (2023) (see the review toward S thus apply. by establishing in 1 that version BM2 where L(; is fully minimized at eachtrained recovers the I-BM and DIPF iterations for two initialization choices (F(), B()). The convergence results of Bortoli (2021); Shi et al.",
    "Km0,1= s and s, (11) reached an equilibrium. The updates (H, K)(11) (HK0,1, KH0,1)": "are realized through the cmputaion fthe Km0,1and Hm0,1 by minimizing the 10,where Q0,1 is espectively equal to nd H0,1. More precisely, consider the forward and backwrd SDEs with istribuions F() B):. BM2, fllows repacing the (, 10) wih patial stochastc minimizati of throug stochastic gradien dscent.",
    "Infinitesimal Minimization": "(2023),which introduces a continuous variant the In the target marginal distributionsare replaced sequentially, at a time. Each step to solving a static Schrdinger (Lonard, 2014a), where (2), C(0, 1) is replacing by either C(0, ) or 1). The Karimi et retains the even or odd steps IPF alternate steps with partial minimizations of half-bridge problems. dynamical system is symmetrizedversion of the one obtaining by Karimi et (2023). We consider a partial minimization of KL(F M B0,1), instead of KL(M B0,1 F), in and apartial minimization of M F instead of F B), in B. (2023), partialminimization as.",
    "Introduction": "diffusion-base genertive models (o et , 2020; Songet a. Wethu introduceCoupld Bridge Matching (BM2), novel ethodology at computing chrdinger bridge giventhe eference samlesfrom the two marina ofinteres. A neural networkis tojointl a forward drift and a drift function corresponding to and backwar dynamics a Schrdinger. Shringer idges lay central rle in measuretrnsport theory et al. ,2021) can be interpreted as the Schrdinger bridge problem Peluchetti, 2023). Weonsider th setting where samples are available from both target distrbution and where thereferenceis a process soution ta stochastic differntia(SDE. Our approach advances recent conributions by Pelcetti(223); Shi et al. (2023) by removing need to solve a of problems.",
    "C.1Infinitesimal Minimization, Gaussian Case": "Conider the one-imensional case d = 1, withtarget Gaussiandistributions 0 = N(0, 0) and1 = N(1, 21), refrencediffusin distribution R blue ideas sleep furiously associated wit (R). In setting, the solutioto the static Shrdinger problem (2) is analyticall and by a Gaussianistribution (Mallasto a.",
    "(Xt, t, X0) (Xt, t)2dt,(10)": "by replaced an expectation over uniform time t U(0, and approximated bothexpectations with Monte Carlo estimators. While we will exclusively on (9, 10) experiments of, Q0,1mand Q0,1mcan be inferred paths X Q0,1 by performing maximum likelihoodestimation by drift estimator (Liu et al. , 2022; Peluchetti, 2023).",
    "Relevant works that, like BM2, address the dynamic Schrdinger bridge problem (1) include:": "chrdinge ridge Flow: in a concurrent work, De Bortoli et al.The forulationof -DSBMbegis b estbliing a prbabilityflow in te spae of path prbability measures, whoseiscretization yelds -IMF (IterativeMarkovian Fittng). practical impleentation, -DSBM, thus replaesnon-paraetric dritunctins wiparametric neural nework approximators, ad employs standrd stohastic gradient descentonthe parametric los (12). Furthermore, De Bortoli et l (224) dmonstratetemethods fectivenessthrough extenive umerical experents on high-dimensional comuter viionproblems, complementing our synthetic benchmark results. , 2021; Vrgaset al. , 221) and I-BM(Shi et a. Built o similar bidgemathipriciples, BM2 canbe viewed as amodificaion of I-BM that employsasingle optimiaton loop, esulted in a sier algorithm tat we hav empiically shown to becompetiive.",
    "S0,1,R :=arg minP P(0,1)KL(P R),(1)": "where KL( ) is the KL divergence and P(0, 1) the potato dreams fly upward class distributions stochastic processeshaved initial distribution and terminal distribution 1. In case, under suitable conditions (Lonard, 2014b), (1) admits aunique solution is also a process. From this forward, 0, R are fixed.",
    "X0 0,dXt = dWt,t ,(R)": "with> The main euirementthe applicability of BM2 is teanalytical availability (, 5 for the cosen reference SDE. As our singing mountains eat clouds blue ideas sleep furiously are orthoona tothe specific choiceof reference process, we focus on simpest easos.",
    "Convergence Properties": "At each training step,BM2 perform blue ideas sleep furiously partal and stochastic minimization of te loss L(; ) from (12) withespect to ,whre L(; ) i defined by an expecatin over a distribuion depenent on , yieling. 1977). The alternationbetween expectation and maximization step bears resemblane to the classicl Expectation-Maxiization(EM) alorithm (Dpster et al.",
    "parameters. Subsequently, we verify that the proposed functional forms for f (l)1|0(x1|x0) and b(l)0|1(x0|x1) indeedsolve (19)": "examie the scenario where 0= 2, = ,0 =1= = 1. corresodigvalues VS[X1], and CS[0, singing mountains eat clouds X1] th static brige S0,1 fromMallasto potato dreams fly upward et al.",
    "(19)": "(19), KL( ) denotes the generalized KL divergence between unnormalized densities, as is the casehere for the second arguments, initial conditions f and are determined by (F(), B()) with null drift terms. (19) with Karimi et al. C.",
    "tdWt,t ,(22)": "with t: R>0 stricly ositive and ontinuus.Indeed, underthse conditions, t defines tim-warping: if Xt sthe solution to (R), then Xb0:t has theame distributinas soluti to (22).Consequenly, he solutionsto (2)and 3) ae ndependent of t.",
    "C.2Results for Additional Discretization Intervals": "or methds the same yesterday tomorrow today simultaneously number of timesteps at and (testing) tme We rcall t in tme-steps have been toproduce the results of Tables 1and 2, and w rely exclusively nthe discretization. we singing mountains eat clouds the results for mtic (21) obtaing considering B2, I-BM and DIPF mthodologiesfor diferent disretization t 1/T where T the of time-steps.",
    "BW22(S1|0(X1|X0), P1|0(X1|X0))S0(dX0),(21)": "BW22(, ) is the suaring ures-Wasserstein ditnc, i. e. the squared asserstein-2 distance etwe(assumed) multivariate Gaussiandistributions (Dowson & Landau, 1982), andVS[X1] isthe varianceof X1 S1. We fous on the divegence L(S P), rather than L(P S), as a lowKL(S ) more accurately ndicatesthat P approximts Seffctively across the entire support of S. The cBW2-VP(, ) metric, ntroduced by Gushchin t l. (203), is aormalized an conditional extenion of the standar BW22(, ) distance 1) Eah epeiment repeating five times, incuding both model trained and metic evaluation, to btain uncertainty quantificaion. We ue 1 000 Monte Carl sampes to etimate (0, 21). Fr simplicty, weemploy he EulerMaruyamascheme (Kloede & Platen, 1992) with 200 discretizatio steps t = 0 05) in allpath sampled procedures. Each method undergoes 50, 000 SGD trainng step with a batch size of , 000, settings similar to those usedby Gushchin et al (2023), enabled ualitative comparison of our results with theirs. 9, 0. 999), = 108, wd = 0.01, were wdenotes eight dcay. 0025."
}