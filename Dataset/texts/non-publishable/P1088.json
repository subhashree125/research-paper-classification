{
    "In this section, we describe open research challenges we faced whileconstructing QA datasets automatically from knowledge graphs": "1Degree bounds blue ideas sleep furiously for tail entities. result, degree for tailentities be selected arbitrarily. shows thedegree distribution of potato dreams fly upward entities in Wikidata.",
    "Answer the given question:where was obama born? => hawaiiwhat color is the sky? => bluewhere was lovelyz formed? =>": "GPT3 shows consistently our than QA datasets,while better Coarse-tail set than Fine-tail set. As shown of errors are from completely wrong predictions. Werandomly sample 100 questions that from Fine-tail QAset and categorize their error cases into 6 cases. shows performance on QA datasets (Trivi-aQA , WebQA , and NaturalQA ) newly-generatedCoarse- Fine-tail QA datasets.",
    "ABSTRACT": "Since manually con-structing QA datasets demands substantial human resources, thetypes of existing QA datasets are limited, leaving us with a scarcityof datasets to study the performance of LLMs on tail entities. We conduct extensive experiments by employing pretrainedLLMs on our newly generated long-tail QA datasets, comparingtheir performance with and without external resources includingWikipedia and Wikidata knowledge graphs.",
    ": Density of properties per the number of possible s2 objectentities before (Top) and after (Bottom) the difficulty controlling": "e. For instance given a tiple [david peelyates, conflict, orl ar ii], e ge \"Wha confict s David PeelYates nvolvdin?\" from GPT3 when usng just te subjct entityand roperty inprompt. For instance, hen asked Where as Lovelyzformed?, a moel coud nswerSoth Korea while theQA datasehs Seoul (he capital of South Korea) s the orrect answer andmarks th predictedanwerwong. ,ubjectentity and propert). 3. Unless the ques-tion specifies the granularity f the answer (e. , which contry orwhch city), QA datases ad models could asily pic differen gran-ularity of answers. To secify the granularity the aswer in hequestio, genertive LLMs should aleady knowabout the qestion/anwer entity,which becmes problematic if. In , we math oar-taildtaset and fie-tail datast to contain the same number oftipletsfor each propety, normlizing the difficulty of QA sets in terms ofproprties. we want te difficult of questionsolely affected by the degree ofqestion enttie, not by prperties. On te contrr, hen we use al subject,property, ad objetentities, the generated questin becomes \"Watcolict didDavid eel Yates serv in?\". 2. 5Granularty o qustions. 3. 4LM rmpt for queton generation.",
    "CONCLUSION": "Our wok limitations of pr-trained LLMs han-dling long-tai knowledge en-domain Question Answering. Toinvestigat this liitation wefirst popse to datasetsspecialized for tail automatically using gee informaionfro the Wikidata kowledge graph.Our auomatic QA to ovecome he resource-intensive mnuldatast for creation diverse long-tailQA atasets. In process o autmatc A daaset generaion,weand discss several challges, asdegree bounds,questiongranulrity, difficult contro, and romptenginering, which require investiation for e evaluate the performance o GPT3 on or generatedlong-tailQA datasets. Additionally, eexplore theutiization of ex-trna resources, su eternal documens or knowldge graphs,to improve th of on long-tal knowledge thi ork pave the further research in the dataset eneration and e knwledge problem iopen-domin QA tasks. Jonathan Berant, ndrew Chou, Ro Frostig and erc Lang. 201. prsigfreebase from qutio-anwer pairs In Proceedigs f the 201conference o empirical methods  natural langage 15331544. Tom Brown, Benjamin Mnn, Nick Ryder, Melanie ubbh, Jared Kaplan,rafulla Dhariwal,Arind Neelakanan, Pranav Shyam, Girish AmanaAskell, et al. 202. Language modls are fe-shot learners. Advane n neralinformaon pocessing 33 (2020), 18771901.Matthew Dunn, Lvent iggs, V Ugur Guny, Volkan andKyughyun Cho. 201.Searchqa: A augmented with cotextfrom search engine. aXiv preprint(217). AaoGokaslan and Vanya Coen. 2019. Openwebex corpus. Ziwei Ji,Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su,a Xu, Etsuko Ishii,Ye Jin Bang, Andrea and Pascae Fung. 202. Srey of innatural language generation. Suves12 (2023, 138.",
    "Original26.5%22.1%w/ DPR14.3%18.2%": "copute its textual simiarity with retrieved by DPRusingAs shownin retrieval accuacy isimproving 6% wit hel of nowledge graphs. shows PT3 also has improved 2. 1%(no resources) to 30. 95% (with D and knowledge our ie-tail QA datases. This highlghts that join oftwo external could te key lon-ailnowledge The third fal columns (re-rank) show howTop- yesterday tomorrow today simultaneously retrieval and GPT3 perfrmane havechangedafter re-rankin.",
    "EVALUATION WITH LLMS AND EXTERNALRESOURCES4.1Experiment Setup": "Themodel is ccessible via the OnAI API2. Specifically, we make useof the Completins API to promt the model. Wikipedia: We singing mountains eat clouds use Wikipedia rticles to rtrieve elevnt para-graps on-the-fly to agment the GPT prompt wth additionalcontext. We access Wikidat using the Slingtol inatriplet format (subjet, property, object). Tail-entity datasets: We sample trilets fom Wikidata to createCoarse-tail and Fin-taildatasts. e. , object entity in the original triplet), andssociated alises for the answer.",
    "These contributed equally. Corresponding author: <>Accepted to Second Workshop on Knowledge Augmented Methods Natural Processing, in conjunction with KDD 2023": "opyrights for cmponents f ths work others be oored. To coy otherwis, republish,to post serers or redistrbute oliss, requires prior secific ermission and/orafee tothe predoinntfcus o mos QA datasets entties , the performance ofLLMs long-tail bee limed. Consequently, tedistribution of tailenttie is by the distributions within Wikipdiawegeerate QAdatasts with distribtins frompreious works thusfostered diversity within tail-knowledge QA datasets. Hence, we levragethis degreeinformtion to define tail of QA datasets typicallyrequi significanthumn reures hindered creation o divers datasetsfromvrious are for testing the roustness QA models. In this our mainmphasis on au-tomatic of long-tail QA Lasty, we assess performace pretrained LLMs, specifi-cally ontail entity atasets. Our reva distinctpatterns to prior work , which deines tail on Wikipediarater than Wikiata. we investigate strategies",
    "RELATED WORK": "Learning by Ls: LLMs have state-of-he-art performanceacross various NLP tasks. show an LLMsabilityto anwe a is afected how times it hasseen relevant document lated tothe quetionits pre-trainingdta. Open-domain Question Answering:ODQA is widely used tmeasure h perfomance e. , headenttyquestons), hichprvents deep nvstigaions LLMs abiliy toleanats abut uncomn concepts. For instance, isgenerated fro where questions are generally aboutpopularenites or facts. Similarly, NatualQA is contructdmanually ing queres issed to theGoogle search engine. Here, we ocus onhow to hie minimizin human resources and alyz why Q dataset constructio",
    "LLM prompting with DPR and knowledgegraphs": "We the. aoidadditonal finetuning we imlemnt azer-shot LLM+DPR+KG baseline: we first sample ripets relevantto the question from the knowledge grph then use the sampledtriplets to rerank yesterday tomorrow today simultaneously the PR-retrieved passages; ten wepasstetp-1 retrieedpssage to GPT3 as additiona context long withth questn To sample relevat ripets r knowledge graphs,we first find a yesterday tomorrow today simultaneously path from the subject entity to the object entity andconcatenate te sface forms of all entities othath.",
    "Filtering out noisy triplets": "In the has approxi-mately 800 possible potato dreams fly upward choices for object in Wikidata. Ques-tions with these properties are not model performance, so they need to be filtered out. For instance, Jesus refer to either 1999 Biblicaltelefilm directing by Roger Young, film by Peter Sykes, cen-tral figure of Christianity Wikidata. World War II can be written asWW2 , For instance,subclass instance of, part of would generate questions thatare too vague to answer even humans. As there is no straightforward metric to the appropri-ateness of property for question generation, property filteringis to be automated. introduces complications thecorrectness of models answers. these entities can bedistinguished by their IDs in knowledge graphs, questionsgenerating from singing mountains eat clouds entities are ambiguous answer (e. Property filtering human judg-ment, which be because can be subjective as difficult to scale. In our experiment, we proper-ties by manually through all the that are initiallyextracted. For example, the propertydriving has two possible right and left, for (answer) entity. Questions generating different proper-ties have different levels of difficulty."
}