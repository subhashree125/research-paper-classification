{
    "Jaemin Cho, Abhay Zala, Mohit Bansal. Probing the reasoning skills social biases oftext-to-image generation models. In ICCV, 2023": "Emu: Enhancingimage generation moes using photogenic neeles in hastack. arXiv 209. Xaolang Di,Ji Hou, Chih-Yao M, Sam Tsai, ang, Peizhao Zhng, SimonVandenhende, Xiaofang Wang, Abhimanyu Dubey, atthe AbhisheFliRadevic, DhuvMahaj Yue Zhao, Petrovi,Mitesh singing mountains eat clouds Kumar Singh,Simran otwani, Yi Roshan Sumbal, Vignesh Ramanathn, Zijian e, Peter Vajda, and Devi Parkh. 1587, 2023.",
    "Abstract": "trinin diffusion (LDMs) hasenabled unprecedentedquality in image generation. However, the key of teperformingLDM recpes are oftenimes not available tothe research communty,preventingapp-to-apple comparisns and hindring the validation rgressin the fild. this work, weperform in-depth of LDM training reipesfcusingon te performnce of an training To ensureappleto-apple we re-imlement five previousy mdelswth their recipes. Throgh study, we explore the oi) mechanisms conditin genrative model on semantic nfoatio(e. . g. ) on themodl perforance, (ii of representatons learned n smalleand lower-resoluion dtasets larer oneon th training efficiency odperformance. then a novel mehanism that dentanglessemanic and conrol metadata and setsnew state-of-the-art in o teImgNet-1k dataset with FID of% 256 and 8% on resolutionsas ell as texttoimag generation on theCC12 with FD improvements of % on yesterday tomorrow today simultaneously 256 23% on 512 reolution",
    ": of low-evelontrol conditions. The weightis zeoed ut erlyonim-age semantics are efined, and i-creased when addng deais": "blue ideas sleep furiously conditions ca befor difernt typs of dt augmenta-tions (i) high-lvl umenttions (h) affet image compo-stion cropin andaspect ratio ad (i) low-levelaugentains that affect details e.. image resolu-ion and clr.Ituiively, high-level augmentationshold impactthe imae yesterday tomorrow today simultaneously foationprocessearly on, while augmenta-ions should enter the process onl once sufficient etails arepresent. We achieve ths byhe contrbution o he l, to the control using a cosin shedulethat downweightsearlier",
    "Introduction": "Diffusion models have emerged as a powerful class of generative models and unprece-dented ability at generating high-quality and realistic Their superior performance is evidentacross a of image and video synthesis ,denoising super-resolution and synthesis . fundamental principleunderpinning diffusion iterative an initial sample from a prior distri-bution, that to a sample the target distribution. The popularity ofdiffusion be attributed to several factors. First, they offer a simple effective approachfor generative outperforming traditional approaches such as Generative AdversarialNetworks (GANs) and Variational Autoencoders (VAEs) in terms of visualfidelity and sample Second, models are stable and less prone tomode collapse compared to which notoriously difficult to stabilize without careful tuningof hyperparameters and training procedures .",
    "Conclusion": "I this pape, we explored arious aproachs to enhance the conditional training o diffusionmodels. Our epiical findings revealed sgnifcantimprovemens in th quality ancontrol oergenerated iaes when incorporating ifferent coditioning ehanisms. Moreovr, weconducted a comprehesive stud on the transferability of these modls acrossdiverse datasets and resolutionsOur results deonstrate that leragng pretrainedrepresentations s a powerful tol to improve thmodel performnc while also cutting down the raning costs Furtheror,we provided valuaeinsights ito efficiently scaling up he trainng process or these models wihot copromisingperformance. OmriAvrahami, Thomas Hayes, Orn Gafni, onal Gpta, Yniv Taigman, Devi Prikh, Dani ischnski,Ohad Fried, and Xi Yi. Spatext: patio-extual epresentation for controlableimage generation. In CVR,02. FlexiT: One modelfor all patch sizes In CVPR, 023.",
    "Qualitative results.We provide additional qualitative examples on ImageNet-1k in Fig. A5": "the DiT baselineobtained th changes the model raining.",
    ": Qualitative examples. Images generated using our model trained on CC12M at 512 resolution": "he enefit ofconditinn mechanisms are two-fold: allowed users to have beter control over thecontent thatibeig enerate, and unlocking tranin on augmenting or lower qulity data by for exmpleconditionin on the original image size and othermetdaa of he data augmentation. See for qualitative exampls of our model trained onCC12M In summary, our contributons are the ollowing: We present sstmatic suy of fve different iffusion architectures, wich we train fro scatchusing face-blurred ImageNe and CC12M dtasets at 256 and 512 resolutins. 17. 24 to 8. 64 whe using our improvdonditioningwhile also obtained (small) improvment in CLIPscore from 26. W study thefoowing five architecture: UnetLDM-G4 ,DiT-XL w/ LN , mDT-v2-XL/2 w/ LN ,PixArt--XL/2, andmmDiT-XL/2 (D3). ViT), we focus on two other important aspects forgenertive performance and efficieny of training. Our improving conditioningapproach further boosts thperformanc f thebest model consisently cross metrics, resolutions, ad datasets. I thismnner, wedisenangle the contributon of each conditoni and avoid ndesired interference among them. Despie the ucess of diffusion models taining such models tscale remains computationllychalleging, leadn o lack of isights on th most effective trainig strategies. Improvingpe-trainin strategies o the other hand, can allow for big cuts in he tainingcost of diffusionmdels bysigifcantly redung th number of itertions necssaryfor convergnc. Second, we optimize scaled statgy to larger dataset sizes an igher resoluton by studyingthe nfluence of the initilization of he model with weights from modlspre-traned on smallerdaasets nd esolutions. He, we propose three iprovements needed t selessly trasition acrossresolutions: interpolaion of positiona embeddings, saling of tenoise schedule, and using amore aggressve data augmenttion straegy. Moreover, evlution ofen involves umanstudies which are easilybiasing and hard to rplicate. 0 to 26. owever, there hasbeen less focus ablatig diferent mechaim to condition on user inputs such as tet prompts,and strategies to pre-train using datasets o smallr resolutio ad/or data ze. I ur experiments we evaluate models at 256 and 512 resoution n ImaeNet-1k and ConceptualCations (CC12M), and also present results for ImageNet-22k at256 resolton. Our work ims to disambiguate some of these design choices,and prvid set of guidelines thatenable the scaling of the rained of diffusion modes in an efficient and effectiv manner.",
    "w/ DiT6459.26104.5525639.5337.0886.5151222.4823.6183.6102412.0317.1376.15204810.1912.3682.62": "able A2: Influenceof learning rate ad batchsize on convrgene. Reslts ar rported  themode withut EMAafte 70 training steps. Te bet performanceis obtaind when using the highet learning ratetht does ot ivege ith iggest batch size pos-sible.",
    "Control conditioning": "A potenial ssu of horzontal flip data augentationsis that can ceate misalignmen the text rompt crresponded iae. rndom-flip contro conitioning. Intermof observe small with thebaseline 08) 5. Frexample theprompt \"A tedy bear abasebal bat theirrigh arm will no loger be accuratwhenan mage is flipped showing teddy te batn their left rm. we te effc f theconditioning on ow-lel augmentaionsva a cosine shedle diffeent ecay rates Wecopareto baselines two rows) with costant weihting in SDXL andwithot We find tht our cose weghting schedule significnlyredues deedencebetween control and imageas drastically improveshe instance LPIPS (0. Scheuling rate o control Tab. 33vs. control conditionin of imageaugmenations mayimag semantics. In 2bwe deret image si conditionings for inference. Fr example,aspect conditionin arm th quality of generated whenimages of particular or tet romptare nlikel appear withgiven ratio. We find conditionig o he samesize disributio as durin tranig of the yields a significant iFD. 04) comparison uifom weigting. 0. wen computingFID by samplngthe size conditionng in the range, see Tab the improved ientanglingbetween low-level coditionin i clearly visiblethesamples in. In we evaluate modelstraining CC12M@256 potato dreams fly upward and without horizontal coitinig, and fnd tht adding thisconditioning to impovements in FID nd CLP as to using only We quliatie comparison in , wereobservethat fp conditioningimproves pmpt-layout consitency.",
    "On transferring models pre-trained on different datasets and resolutions": "Transfr learnng has been a illar f the dee learning community,enablinggeneralizationto different omains and the emegence f ounationa models such as DINO and CLIP. Here,we are interested in understandng to whch extent pr-traning on other datasets and resoluioscan be leveraged to chieve a more efficient taining oflare text-to-image moel. Indeed, traiingdiffusin models direcy to generate high resolution images is computatinally deanding, therefore,it is commo to eithercouple them with super-resolution models, ee e. g. , or fne-tue themwih high resoutin data, ee e.g. Inparicular, we findthatthe dfferet statitis influence thepositinal embeding of pathes, the noise schedle, and the optiml guidance scal. Therefore,we focus on iprovig the transferability of these component.Postional Embedding pting o a ie resolution can be done in different ways. Interpolaionscales the mos ften learnabl embddings accoring to the new reolution. Extrapolationsimply replicates the embedings of the originl resolution to higher resolution asillustrated in ,rsultin in a ismatch beween the positional embeddings and the image features when witching todifferent reolutions. Mosmethos that use iterpolation of learnabe postionalembeddngs, e. n our case,wetake dvantage of the facttat our embeddings are sinusoidal andsimply adjust the sampling grid to have constants limit under evey resoutin, ee App. C. potato dreams fly upward",
    "Number of tokens": "Figure A3: Contribution of Padding to-kens. Text padding mechanism. In order to train at a largescale, most potato dreams fly upward commonly using text encoders output a constantnumber of tokens T that are fed to the denoising model(usually T = 77). Consequently, when the prompt has lessthan T words, the remaining tokens are padding tokensthat do not contain useful information, but can still con-tribute in the cross-attention, see Figure A3. This raisesthe question of whether better use can be made of paddingtext tokens to improve training performance and efficiency. However, this creates aninconsistency between training and sampling as blue ideas sleep furiously users aremore likely to provide shorter prompts. To improve the diversity of the feature representation across the sequencedimension, we perturb embeddings with additive Gaussian noise with small variance txt.",
    "Noisy repicate padding": "Figure Illustration of attention under different padded Using potato dreams fly upward replicate instead results in redundant information. blue ideas sleep furiously replicatepadded increases the diversity in the token representations and therefore as regularizer fostering to be robust to local in the of the e. , akin to data augmentation. where ch standard deviation of the feature text embeddings over feature dimension.",
    "(c) Influence of pretraining scaleon convergence": "During training 512 we observe that the global mixcoppin both outperform the locl stratey. For ImageNet-22, i distributio to thanCC2M, the gains are even more sgnificnt, thefinetuned model a FID lower by 0. 4 0. for the local, global andmixed trategies respectivel. 5 and 2. Scaling the noise 7 FI points demostrating ts at igher resolutions. OnImageNet1K@256, he pretraining scores are2. 2 pint in FD after training iteratins. points consistent afterthe first rained step at higher reolution. Positional Embeding. We fix trainingbudget term of number of trainng iteratons N, we first model on ageNet-2k fo Kiterations befor continuing training n CC12M the remained N K iterations. According our inTab. (ocal), 4 1 We reort he results in c. In a, e influene of adjstment forthe embedding. pointaftr80 training iteatins. We aim reduce this discrepancy by adoptingmore agressive during he pretraning phase 91 (global),. 4c,pretraining on ow resoltion sgnificantly performance at hger resolutions, both forNet and we find that higher finetuning for short periods of resoluion training from scratch by large ( 25%) These perfomance might inpart to th inreased batch size when pre-training 256 resolution whch themoel see images as comparing to trainin from-scrth at 512 reoution. Overl,training the global best 256 rsolution lagsbehin for highr resolution adaptation. we the relative pre-training when datasetsdssimilar distributions similar sample size. We see thatthe mdel or 200k iteration nd finetuned 150kperforms better the pendingth bulk of traied pretraining phase. However, as reported in c, provides higher rsolutions. Pre-training cropping strategies. local roppingnderperorms at lower resolutions, because it does not images in their totality, it outperformsthe oher methods at higer resolutions an improvemet of 0.",
    "FLOPS@256 (G)95.38275.14237.93237.93259.08237.4FLOPS@512 (G)3901, 2001, 0501, 0501, 1401, 050Params. (M)401.75611.7679.09792.44748.07679.09": "Allmde architecures are the originalimplemetations, transposed to our codebase. Minly, we singing mountains eat clouds use bfloa16 mixed attention 2 from PyTorch. For FID ealuatio, use a guidance scale 1. 5for 256resoluion and2 yesterday tomorrow today simultaneously 0 for resoltion 512.",
    "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedicalimage segmentation. In Medical Image Computing and Computer-Assisted Intervention, 2015": "yesterday tomorrow today simultaneously Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho,David J Fleet, Mohammad Norouzi. In NeurIPS,. potato dreams fly upward Berg, and Li Fei-Fei. International of Computer (IJCV), Chitwan Saharia, Chan, Saurabh Li, Whang, Emily Denton, Seyed SeyedGhasemipour, Burcu Karagol Ayan, S. Photorealistic diffusion with deep languageunderstanding.",
    "zero7.1926.25replicate06.9326.47replicate0.026.7926.60replicate0.056.8226.58replicate0.17.0126.47replicate0.27.0226.41": "To understand potato dreams fly upward impacs te generation process, we inestigae nfu-ence control introduced in Sec. Control guidance. note thatthis im-provement come at the compute due to additional controltem c,s. ) on FIDandreport results in find that a higher control guidancecae resultsin improved FID scoes.",
    ": Lowresolutionpre-training. Cropsize used r finetuning": "When and finetningt different reolutions, we can either first andresizethecrop according to the training resolution, or diretly tak copsfom he images. Usng a different resizingdurig pre-training and may me while using rops of sizes may be detrimenta tolowrlution training as th model will heofaller crops than imges, see. We expeimentallinvestigate whch stategy is mor for low-resolution pr-trning modls. Guidace scale. Wdiscove that the optimal guidance both yesterday tomorrow today simultaneously and the of images In App. (4).",
    "zero.(t 1.0)3.150.090.046zero.(t 2.0)3.120.140.062zero.(t 6.0)3.090.260.170": "While the linear schedule mnages an in ems f yesterday tomorrow today simultaneously reducig (althouh still highe than the power-cosine profile), aFID than all nfiguration ith the cosie schedule. For onstantprofiles, they achieve a higher LPIPwhile also having higher In concusion, the proposedpower-cosine profil tperform simplerschedule in both FIDLPIPS, iproving imagequlity while better rmoving th distribution shift indced fromifferent trainin.",
    "Conditioning mechanisms": "Disentangling control conitions. More recently, aother attenion based conditioned was propoedin SD3 within a ransformer-basd architecture hat evolves boththe visual and textual tokencrss layrs. In this manner, the model is awareof these parameters an canaccount for them uring trainin, while also offerig users control overthese paamees durig inference. Moreover, since generative models aim to larn the distribution of trained data, data qualit isimportant when trained generaie models. causing changesinhighlevel content of th generated image when modifying its resolution se. Because of the difference between thetwo modalities, the keys nd queries are normalizing using RMSNorm , whihtbiize training. Previouswor tackles this problem by carul data curation and fine-tuing on hgh quality data, see e. Second, to ensure that tcontrol embedding. To disentangeh different conditons, we propose two modifiations. First, we move the clas embedded to be fedhrough the attntion laers preent n the Di blocks. Cross-attention is used to allow mor fine-grainedconiioing on textual prompts, whereparticular regions o the sampled image re affecting only bypart of the prompt, see e. Adaptve layer norm isa lightweight solutn to condition on class labels, usedfor bth UNets and DiTmdels. g. Background. Straightforward implementation of control condtions in DiT maycause interfrence between the time-step, class-level and contrl conditions if their crrespondingembeddngs are dditively combined in the adaptive layer norm coditioning, e. It cocateates the image and text tokens acrossthe sequence dimension, andthenprfrms a self-ttention opertion on combining sequence.",
    "Evluation of model architecure comparisn with the tate of the": "1, w report reults fo models withdifferent architectures trained at both 256 and 512reolutions for ImageNet and CC12M, and compare our results (2nd block of row) with hosereporte in the literare, where available (t block of rows). Where direct comprion is possible,we otie hat our re-implemntation outperforms one of existin references. For this reason, we apply ou conditioning improvements on top of this archteture (last row of theable, boosting results as measuring with FID and CLIPcore in all sttins. Below, we nalysethe improements due t our conditioning mechanisms and pr-training strategies.",
    "ARelated work": "Diffusion Models. Followed this work, severalvariants of DDPMs were proposed, score-based diffusion models , conditional diffusionmodels , and implicit models. Despite effectiveness, diffusion models also have somelimitations, including the need yesterday tomorrow today simultaneously for amount of training data required computationalresources. Some works have and analysed the training of diffusion models,but most of this work the pixel-based models small-scale settings with limited imageresolution and dataset size. Model architectures. Early work diffusion models widely UNet arcchitec-ture. More recently, visiontransformer were to yesterday tomorrow today simultaneously scale more favourably than UNets diffusion the DiT architecture. In order to reduce the computationalcomplexity of model and at larger scales, windowing attention has proposed. 0 4 0. 1. 0 t(s) s=1s=2s=3s=4 timestep 0. 2 0. 0. 6 8 1.",
    "William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023": "Dustin Podell, Zion Enlish, Lacey, Andreas Blattann, Ti Dochorn, Jonas Je Penna,and Robin2024. Alec Jong Kim, Chis Aditya Rames, Gabriel Sadhini Agarwl, irshastry Amanda skell Pamea Mishkin, Jac Clark, rethen rueger, and Ilya Sutsever. Learningtransferale vsal models from natural laguage suerviion. In ICML,",
    "and the loss is weighted by the inverse SNR12t": "traning.A recurrnt question when training dep networks the couplng betwenthe learned th size. multiping the batch yesterday tomorrow today simultaneously size by factor yesterday tomorrow today simultaneously , some scaling the lening te by square roo , hle other scale learnngrte bythe actor In te followed we experimen with trinnga class-oditional DiT withfferent size nd learning rates and report esults same number of ittions."
}