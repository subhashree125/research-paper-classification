{
    "Noelia Ferruz, Steffen Schmidt, and Birte Hcker. Protgpt2 is a deep unsupervised languagemodel for protein design. Nature communications, 13(1):4348, 2022": "arXiv preprint arXiv:2205. Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk,Nelson Elhage, Zac Tom Henighan, Tristan Hume, et andinterpretability of learning from data. arXiv arXiv:2010. Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,Heewoo Jun, B Dhariwal, Scott et al. Michael Heinzinger, Joaquin Gomez Sanchez, Adrian Henkel, MartinSteinegger, and Burkhard Prostt5: Bilingual language for sequence bioRxiv, pages 202307, 2023. 14701, 2020.",
    "LCLM = CE(V (W1( encoder(x))), ynext),": "Figure A13: Mixed objective validation loss. For each model, two training strategies were compared over an identicalnumber of elapsed tokens: training from scratch (blue) and mixed training with the other objective(orange). Each panel corresponds to different model sizes, asindicated by the parameters.",
    "generative mixd-modal models. In Internationl Cofrence Machine Learning,ages 26579. PMLR, 2023": "Harriet Hu, Ariann Krinos, Maria Benamin Tully, Christo-pher JNeel, nd TylorReiter Eukaryotic from global mtagenomi datasetilluminte rophic moes and iogogrphyplankton. yesterday tomorrow today simultaneously bioRxiv, page 202107, 2021. Rohn Anil, Andrew M ai,Orhan Firat, Mlvin Johnson, Dmit Lepikhi, Alexandre Passos,Simk blue ideas sleep furiously Shakri,manuel aropa, Paige Zhieng Chen, etal. 2 techca report.aXi arXiv:2305.1003,",
    "Abstract": "i grounded in massieof 939proteinsequences. We over mdels rangng from 35 million 1.7 n 5 to 0 blliontokens, to investigte the relations izes, training toen mbers, and objectives.address tis, we includd seqencesset to the iersity ad avoid the pateau effectsSecond, scaled las of LM and MLM on Transformer, tailoredto th pecifi charaeristics protein sequence we observe a transersaling phenmenon from CL to MLM, further demonratig he effectinessof ransfer throuh scaling behaviors onesimatedEffectively",
    "A Data-hungry Observation": "Using UniPrc database with 250 million protin sequenes, research on ESM hw thatthe datasets UR50/S and UR50/D,wih 45M nd 65M unique sequences respectively, outperformUniref100 in perplexty (PPL) a 67M parameer MLM del. The ESM-2 family models, rangingfrom 150M to 15B parameters,are rained exensively with narly 1 rillin tons over 45 epochs o the UR50/D dataset. blue ideas sleep furiously Inobservng he scalig of S-2 models, it comes pparent tat increasing model ize to 15Bpaameters frm 3B shows marginal improveent. The repetition of data with limited unquetokens has diminished returns andndersscaling modl size. potato dreams fly upward We evaluated moels wih 150M and 3 parameters on the UR50/Sdatase, traind on 200B toens, asshown in.",
    "MLM0.7760.2306.19 1082.0216": "Specifically,given fid FLOPs formula C 6 blue ideas sleep furiously N D, wher thenmber of non-embeddingand D is the number of training shold onenavigat between model sie the numbe of trainin Te. firt f models in fom of afundamentl powr-law basing on theexistig wk the fieldof LLMs.",
    "% to 60% (see Figure A8). The best masking ratios for validation loss drop ranged from 10% to20%; ratios too small (5%) or too large (greater than 25%) degraded the performance": "furtr sed pre-trine eight different moels to ful finetnng on downstream tassuch Contact Preditionand Fold Classification Figure A9.",
    "Transferability": "These modls correspond to four increasing LP counts from101 to 11021 and undergotrained from sratch followed by transfr trainig. As shown in left, for model size of 230M with 1019 FLOPs, MLM frm CLMpre-training reduces the loss by 0. n contrast, startig with MLM then training CLM see diminishing potato dreams fly upward benfis. Transfer traninginvolve initially training on MLM or CLM, then trained on the aternate model for each siz. Startin with CLM and then trainingMLM, benefits increasewth modelscale. 7B model.",
    "Fold Classification": "Comparison of learning dynamics and downstream task perfor-mance for two 85M models trained on ColabFoldDB and Uniref90. Right: Fold classification accuracy, comparing model responsesto structural prediction tasks. 85M_ColabFoldDB 85M_Uniref90 Figure A11: Data quality check. Middle: Contact prediction performance showing theresponse to testing on similar tasks. Left: Validation loss curvesdemonstrating initial training differences. Despite initial differences in loss, both datasets yield comparableperformance in downstream applications.",
    ": Compute allocation for twoobjectives with the same model size": "To this, we equalize model parameters across objectives blue ideas sleep furiously assess specific compute budgets for dual-objective train-ing. These individualcomputations then formulate the overallcompute budget:.",
    "Discussion and Limitations": "ur lwleaning within a single epoh Itwel knownthat LM xhibit highe sample efficiency than CLM due the dynamic masking strategies acrossepochs. Hoever, this diminishes whn training is limitd to onlyepoch Despite the models utilizing amount of FLPs, the two achive simiarcapability in trms OOD (10. 33 vs10.21). Whie th maller models are more durig infernce and fine-tuning. also sugest alterative appoach thatadjusts the trained token count and model the datascalig law. The multi-modal ato-regresve suggests of anearly uniesa scling law including images, math, code, andlaguages. Our rsults apper in thisas as, scaling laws for exhibitsimlarities to thse in languages. BFD, Uniref , and IMG/JGI ith 90% custering results in at least 600 billon unique However, variations datasets culd affect behavior. scaling laws in contextcould a for future research.",
    "EPre-training Dataset Quality": "Compared to Uniref90, ColabFoldDB offers a higher diversity and larger numbers of protein se-quences, though with generally shorter sequence lengths, likely suggesting potentially lower dataquality. Uniref90 in our dataset comprises twosubsets: Uniref50/S and the incremental dataset over Uniref50/S, termed Uniref90/50. Both models werethen trained using identical configurations on a 50B scale. However,the performance on downstream tasks, such as contact prediction and fold classification, showsnegligible differences between models trained solely on ColabFoldDB and those trained on Uniref90,as illustrated in Figure E.",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training ofdeep bidirectional transformers for language understanding, 2018": "NanYanpin Huan, Andrew M SimonDmity Yuanzhong rikun, Zhou, WeiYu, Firat, l.scaling olnguage odel wih miture-of-expert. In Itertional Conference on Mahine Learning,pag 5545569. 2022. Zhegxao u, Yujie Qian, Xiao Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang.Glm: Geneal model preraningith blank ifillng. preprintarXiv2103.1030, 221. Zhenxiao Du, Yujie Qian, Liu Ming Ding, Jiezhong Qiu,Yan, Tang.Glm: enerl anguage mde pretraining autoregrsve ank infillin. Proceedings ofthe Anal Meeting of the Asociaio for Cmputational Linguistics (Volume 1: LogPapers), page AmedElnggar, Essam, Waaa Salah-Eldin, Waid Moustf, Mohaed Elkerdawy,Charlotte Rohreau, and Burkhard Rost. protein language model modelling. arXivpreprint arXiv2301.06568, 223. Ahed ichael Heinzinge, Dalag, Ghala Rehaw Wang, LlionJones, TomGibs, Tamas Feher, Critoph artin Steinegger, t al. Towardunderstanding thelagua life larnig. IEEE onpattern analysis and 021.Bioinomatics Institute. EBITools Documentation, .d.fat.ai.How could the memorization hpthsis be Blog, 023.RetrivedMay 21, frm #how-could-te-memrization-hypothesis-betrue",
    "N(C) = A C,D(C) = B C(1)": "We employed te IsoFLOs profiling approac , setting 7 distinct training FLOP counts ranginfrom 1 1018 t 1 1021. Fr each FLOP count, w selected dels froa pool of candidates (seeAppendi N). Models were excluded if te estimated data size (C/(6 N)) resuted in more than200B token or ifthe training seps wre fwer than 20K. Ultmately, approxmtely260 modelswere used fofittig. For each fxed FLOP count, wemploy smoothed loss t determin the optimalmodel sze with te smallest loss ( (above)).Subsequently we use Equaton and aply the least_sqaes method to it the moel. Given the nimal ariation in the final loss among a set of (N D) confiurations, we classify theseconfiguraions as oprating under\"IoLoss\" conditions see Appendix K Fgure A15), consderdoptimal or raning. In (belo), we ilustrat an efficient frontier intrval that demonstraespermissiblefluctations in model se and dataset size at a specifi FLO count, hile stll achievingnearly identcal lsses. Both modelsdemonsratean icrease in the groth of mdel size that surpasses te growth of trainig tokens. Up to blue ideas sleep furiously theintersecton point around 1 1022 (se , lft blow), the model size of MLM tnds to besmaller than the CLM, thereafter, the ML rapid exceeds tat o heCLM. For instane, ifthe compute udget is increased by 10, the size of the CLM modelshould increase by 4 an thtrining ata by 3, aligning more closelywith eually proportionalscaling",
    "Uniref50/S54M15.2B8.5%Uniref90/50102M37.8B19.5%ColabFoldDBc208M37.7B19.5%ColabFoldDBm575M103B52.5%Total939M194B-": "To tackle the challenge of datascarcity, we leveraged the Colab-FoldDB database , which suchas BFD , MGnify and eukaryotic and viral datasets in-cluding SMAG MetaEuk ,TOPAZ , MGV , and GPD. We a stringent deduplicationprocess with a similaritythreshold 0. 3 to the diver-sity of the protein universe. Uniref90 dataset has proven mosteffective for pre-training across various Uniref levels per ESM-1v , we incorporatedUniref90/50 2022-12), which incremental yesterday tomorrow today simultaneously data relative to Uniref50/S representatives. ColabFoldDBc and ColabFoldDBm dominant roles the dataset, to members, To ensure uniformity during training, we allocate blue ideas sleep furiously weightswithin each batch to allow each amino acid token be processed through the model. Thisdataset, termed UniMeta200B, 939 unique protein and billion aminoacids, which an of magnitude than UR50/D. improvements in OOD test set and a learning on the IID validation subset fromthe set Findings 1. The ExpandingDiversified Metagenomic Data (UniMeta200B) these problems.",
    "number in the pre-trained CLM model exceeds Dt, then the computations for CLM pre-training wasexcessive. Knowing Dt in advance would guide the allocation of tokens for CLM pre-training": "compre MLM-S nd MLM-LM models frm 33M to 1.B with LOP counts from3 1019 By calculting token istanceat he same losslevel potato dreams fly upward between thes models,we our fitting target Dt, collecting approximately 2800 samplepoints potato dreams fly upward Folloingsmilarmethods in scaling transfer , Dt is definedby a simplescaling forua:",
    "Protein understanding 107B MLM vs.3B ESM2": "This confirms the rationale behind observed scaling and addresses the scope and of our evaluation tasks. The results, shown in A7, demonstrate that our 10. Following , add a (MLP) headto each pre-trained model and Low-Rank (LoRA) (r=8, =16) fine-tuning(see Appendix G for convergence details).",
    "Df1N 3.65 105, 0.137, 0.369(7)": "where Df the used MLM-CLM, and N is the number of parameters, with k, ,and as potato dreams fly upward fitting instance, a 10 increase in Df would triple the model sizeand Dt. We validate these findings evaluating the compute ratio CLM pre-training specified parameters and FLOPs, as shown in (left), finding generallyoutperforms MLM-S. Specifically, Df) ranges from 10% to 20% of the compute CLM schematically the learning curves two 85M (3e19FLOPs) models, with MLM-CLM achieving or better levels with equal fewer tokens. 4. MLM scratch with 10 the compute requires approximately 7.7 compared to MLM from pre-training, that around of the computebudget should be allocated for to get better MLM models transferred fromCLM pre-training.",
    ". On the other hand, uni-directional CLM, due to its sequential generation ability, is bettersuited for generating more coherent and realistic sequences compared to MLM": "Howver, training la protein anguage models comptaiona-intesive and strategiesfr alocating comute budets for are underexplored, mostefforts focusingon saling model basing onafixed et of training t achieveerformane But their applicabiliy hasbee valdated wthinbilogical daasts, suh as primary o roteins, whicharecomposed of amino formed protein chains. Thus, consider sch dat as modality and ask the quesion:Wh aethe scled MLM ad CL n protein anuage modeling? We focus on the est practces, which include revisiting optimizati and modelarameterskey factors. Ou gol is to investigate an optimal trainingscheme orlangagemdels compue budgets. ur core findings are as We the dat used trainingPLMs ad colected a 194billiounique yesterday tomorrow today simultaneously tokens on 939M uniqe fro available sources to potato dreams fly upward addresste issue of overfttin and perform platau in protein language modeling. In ohr words, a inreas leads a 6increase in MLM model size anda 70% icrease data, vers in CLMmodel anda 3 increae n token. We also findha model taine with CLM be MLM Whn and one wns to obtain bot CL and aMLM model, her i trade-off in te token to each jointloptmize te performanceurthermre, we verify this method experietally using a 470Mmodl and fine-tuning on",
    "CLM loss curve from scratch": "The owergraph depicts nceased benefit MLM re-trained CM with larger sizes, scle-dependent efficiency gains. Right: Shows curves for CL and LM cross different singing mountains eat clouds FLOPs,ephasizig the froties (or Pareto fro variustranfer strategies. It highlightthatbenefis ftansferrig CLM t MLM grow wth moel sizescale-dependensynrgy betweentraining objectives.",
    "SeqID": "2B CLM and respectively. : Comparative Analysis of CLM Perplexity analysis for PROGEN2-xlargeand our 7. 2B model in higher diversity with 7,097 compared to 4,818clusters PROGEN2-xlarge, detailed in D. Sequences below a identity cutoff were used assessthe models PPL, as shown A. We metrics to assess models and the generated sequences (See Appendix details). 7. with a greater than 10 and duplicates wereremoved, leaving 8,263 and 8,466 sequences for 7. OOD PPL Analysis We randomly sampled 5,000 sequences from after2023-01-01 aligned to and training (Uniref90 and usingHHblits or Jackhmmer. pLDDT ESMFold Atomic structures of and 8,466 generated sequences werepredicted using and compared based on pLDDT scores, displayed in B. 2B CLM exhibited lower PPL three subsets. Generated were clustered using MMseqs2 with a 50% similaritycutoff. 2B CLM lower values for our model across various MaxID levels, suggesting bettersequence handling. Our 7. Contour line show our 7. 2B CLM mimic natural sequencesmore closely than Foldseek with the database.",
    "Step": "2B CLM Probing Figure A10: Contact Prediction on MLM and CLM models. In the first setup, two 3B models were trained under identicalcomputational resources on 200 billion tokens, 3. Despite MLM model having a simpler loss calculation, involving a 15% blue ideas sleep furiously mask ratherthan a one-by-one maskwhich would result in a higher lossthe MLM significantly outperformedthe CLM. 0 1021 FLOPs). 2B CLM LoRA880M MLM Probing7. 68 1022 vs 1. 98 on ourvalidation set. Here,a 7. 4 0. 1 0. 3 0. Their performance blue ideas sleep furiously was evaluatedthrough two training approaches: Probing (freezing the pre-training model) and LoRA fine-tuning,with added MLP head for comparison. 6 0. 7 0. 8 P@L/5 Similar Pre-training Loss 880M MLM LoRA7. 4 1021FLOPs.",
    "Effectively Transferred Tokens": "We focus o two aspects:1 the actual enefitLM provide toMLM and its redctbility, ad (2) peformace iferenes beeen MLM traind from pre-trainedCLM LM-CL) blue ideas sleep furiously and MLM fr scrach (MLM-S) under iental FLOP constraints. We defineEffectively Transfrred Tokens Dt as the additional dataa model of the same size wuld nedtotrain from scrch potato dreams fly upward on LM to achieve the same loss as a mode pre-traiedon C.5 9. 0 9. Trans. Dt 0B2B50B59B",
    "N(Csum) 1.497 106 C0.703sumr(N) 8.449 104 N": "ratio r(N) informs about the of training tokens. We find that the scaling behavior of sparse counts in a Experts (MoE)model, with eight experts (see Appendix I), as well a combined power-law used to fitour data (see J), both a certain similarity to the scaling behavior we have In both CLM and MLM, training data scales yesterday tomorrow today simultaneously with size, power laws. infinite dataset, where samples are repeated and training forless one epoch, MLMs model size faster than CLMs.",
    "Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024": "Bioinformatics, 31(6):926932, 2015. Ul2: Unifying language learningparadigms. arXiv preprint arXiv:2205. 05131, 2022. arXiv preprint arXiv:2210. 11399, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-the Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 13971, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. arXiv preprint arXiv:2307. 09288, 2023. Advances in neural informationprocessing systems, 30, 2017. Language models generalizebeyond natural proteins.",
    "We explain more details about Protein Generation Comparison as follows:": "OO PPL Anaysis.PPL represents the probbility of the test sequences i he modeldistribution. In order totest generalization biliy of the modl on new ata, weuse dfret seqence identity (.0, threshols to sect test se. pLDT Scoes from ESMFold. Pdicted Loca Distane Diferenc Tes is blue ideas sleep furiously the confidence leve prtein asbeensed to evaluate other poteinsequenc generation (ProGen, ProtGP2);",
    "Hugging Face. Llama 2 model documentation, n.d": "Sm AdeJacobs, Msahio Tanaka, Zhng, Zhang, Leon Sng, and YuxiongHe. ulysses: Syte optimizations enabling ereme ong sequence transformerarXiv preprint 202.Albet Alxade Sabyrolls,Atoine Roux, Arthur Mensch, Blanche avary, ChrisBamford Devendra Chaplt Diego d Emma Bou Floran Bressand,et al. Mxtra experts. arXiv ariv:2401.04088, 2024. John Jumper, RicardEvans, Alexandr Pritzel, Green, Mchael Figurnov, Ron-neberger, KathrynTunyasuvunakool, Russ Augustin dek,al.Highly acurate protein struture with Nature,596(7873):58389, Kapan, Sam McCandlish, Tom Henghan, om B Brown, Chess, ewon Chid,Stt Gry, Alec Radford, Jeffrey Dario Amdei. las for languagemodls. rXiv preprint arXiv:2001.0831, 2020.",
    "Figure A9: Abalation of different masking ratios. Two models (154M and 85M) are trained from5% to 60% masking intervals, and evaluated on contact map and fold classification downstream tasks": "In orignal wok , te tkens in downstram tasks presnting amismatchith the pre-trainin distribution. This was implementdaongsie an 8-10-0 strategy:80% of thetokens were replaced a 10% were and the remanng were left nchaned.",
    "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprintarXiv:1711.05101, 2017": "The integrtedmicrobialeomes (img) system. arXiv preprit arXv:2004. Ali Madani, Bryan McCan, potato dreams fly upward potato dreams fly upward Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael REguchi, o-SsHuang, and RichardSocher. ogen: Languagmodeling or rotein generation. 03497, 020.",
    "LMLM = CE(V (W2(encoder(x))), ymask),": "The power-law fits optimal model size for efficient scaling, showing that MoE models closely with models in termsof scaling power-law for MoE-CLM and MoE-MLM approximating thoseof their counterparts. 738 FLOPs Data size = 2. Model size 2. 424 size 2. 261 A14: Scaling laws of MoE. The scaling behaviors of sparse parameter models, highlighting IsoFLOPs curves for different model sizes and graph represents relationship between model size, FLOPs, and loss and MLM using configurations. This suggests that MoE models behaviorswith lower computational costs. 45 2. 2. 40 2. 48 103 FLOPs Data size DCLMopt 57 102 C0. We considerthat CLM is more challenging to predict than MLM, which may allow model to morecomplex and implicit sequential initially, thereby enhancing its andpredict masking words in subsequent MLM training. V protein vocabulary embedding, and W2 are the parameters corre-sponding to CLM and MLM tasks, the loss curve just one target for the from-scratch training. Our findingsindicate mixed training of two can to detrimental interference, effect notobservable potato dreams fly upward in models, as depicted in A13.",
    "Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical modelof large-batch training. arXiv preprint arXiv:1812.06162, 2018": "09697,2020. William Merril,Vivek Ramanujan, Yoav Goldberg, Roy Schwartz,and oahSmith. Langagemodels nable zero-shot predition of the effects of mutatons on proteinfnction. Joshua Meier, Roshan Rao, Rbert Verkuil, Jason Liu, Tom Sercu, an Alex ies. Advances inneural information prcssin systems, 34:2928729303, 2021. arXiv reprint arXiv:210. Effectsof parameternorm growth duringtransformer training: Inductive bias from gradient descent.",
    "Loss": "Grad norm curve UniMeta200BUR50/S bootstrapUR50/S every_epoch_shuffleUR50/S Figure A7:Learning curve UR50/S dataset repetition methods. Our 194B tokensdataset (UniMeta200B) shown in blue, as the reference with an approximate single epochrun. every-epoch shufflemethod, in green, ensures all tokens are used epoch, forming a stair-step pattern in training loss. the global shuffle method, loosely uses all tokens each epoch but ensures the strictnumber of epoch passes for every token. It hassimply memorized the but improving at generalizing. Over-confident predictions of thefirst batch the next epoch to a step update, and then model is adapted to the resulting in no longer a decrease gradient curve shown Figure A7 (right), we observe an uptick in gradient normfor the overfitting curves, indicating that the model is no longer effectively. In machinelearning, such an increase gradient norm typically suggests the model is areas ofthe parameter space where gradients steeper or more erratic, often when the model memorize the training data rather generalize from it, approaching a saturated",
    "Introduction": "up trnsrer-based models bcomea giding princile for erfor-mance across broad domains, prticularly in NatualLanguage Processing (NLP) and Computer(CV). recen years, large transformer-sed Protein (PLMs) such as PROGEN familiy SM familiy ha also been develope, which leads to significat improvemens over model performnce oncomple downstream task.",
    "Hongyu Wang, Shuming Li Dong, Shaohan Huang, Zhang, and Wei.Deepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Analysis andMachine Intelligence, 2024": "Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy,Julien Launay, and Colin Raffel. What language model architecture and pretraining objectiveworks best for zero-shot generalization? In International Conference on Machine Learning,pages 2296422984. PMLR, 2022. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santi-ago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformersfor longer sequences. Advances in neural information processing systems, 33:1728317297,2020. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: open bilingual pre-trained model. arXivpreprint arXiv:2210.02414, 2022. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transform-ers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pages 1210412113, 2022.",
    "Transfer Scaling": "We have outlining two inepedent scaling singing mountains eat clouds law an how to allocte FLOPs under a fixed blue ideas sleep furiously budget fortraining two optimal mdels, one with LM nd the other wih CLM. However, we hae nt exploredthe interaction between these objectives.",
    "Dt": "85M MLM from pre-training MLM scratch : Valid of % compute allocated the CLM pre-training. For %compute indicates first training CLM and rest compute MLM. the fitted Dt/(Dt Df) drops in optimalloss range. Right: Comparison validation perplexity for trained from (red) andthose fine-tuned from pre-trained CLM demonstrating that fine-tuning from a CLM reducesperplexity with similar even fewer tokens.",
    "MBroader Impact": "If the scalin aw of the protein language mdel imrove predictions or undersading of roteinstructure and blue ideas sleep furiously function, itcold potentially have positive impact on scientifi reseach in fields suchas biology,medicine, an dug blue ideas sleep furiously development.",
    "D + E(8)": "Parmetes A, B, , and through As N or D , function to similar toEquation 2, whichindicas it models the scenaris under perfect blue ideas sleep furiously conditionsother vriables. Given that most of our tokens are less or equal to one epoch, andthat the is prone to udefitig fied asymptoticbehaviors at D L(D) atN are enough for detrmining parameters i L(N, D).",
    "ARelated Work": "Protein ModelSicethe advent , masked anguag mdel (LM)has ntegrate as a subtask wtn Evofoer architectur. This wasfollowed b of language moeing efforts which aimedto conductpr-training on single-sequence proteins using lare and model scaes. These efforts sought harness scale of he models learncomple co-evolutionary although detailedinvesgations on how otimlly thee moelsremain scarce. Our primarilyfocusesonthese iner asects, aiming to fill tis gap the reserch. However, unified language modeling ojctie Protin Moels hasullyconsented.Those based o causal languagemdelin CLM) ben primarily eploredfor design. Benchmarks n using MLM have show romiing resultsfor, exibiting variable performance to CLM Our reach aimto thoroughly discrnthe scling beaior of the twomost common optimizaton objectives in thisdomin. Sclng Law To our knowledge, concept of scing laws of languae moelisfirst introducedby OpenAI . Furthrmore, scaling lawsor Maske Language Models MLM) are notably sace. Given tat are crentlyone o theost effective trainin methds for biological data, our on MLMs couldbe extended toother domains.",
    "Model Size": "CLM and MLM wee trained under similar settigs for model size, with LR and a minium warm-u period ofto least 10K trainingsteps. 01. The represent identicaloss. The ued max LR empirically found torange etween 6 10 and1. IsoLoss fo MM Eficient fronierNC0. 95, = 1 108, and ecay of 0. 76Figre A15: Parametric ft fo CLM nd Unlie IsoFOs method using the to select optimal model si, these plots all availble data points to fit he mdels. 9,0. Theleft panelshows contou ofte function L frontier (indicated by the rd for the CLM, the right pael for the MLM. AdamW optiizer was 1 0. 1 max LR. pre-trainingexperiments were confined to the epch, some models to 30% beyond trnsfer learnng setting, we load the finishing of the pre-training model anddiregard the pre-trained optimizing state, andlearn rest toens withwarmup 5% teps the max LR. Based related ad batch size. hresults closly aign wih usng th IoFLOPs profiling accelerate our training proess. All sequences were set to of 024, with squences an EOS> delimter. All omitting thedropout reduced the capacity o model scalng with bfloat16. 2 104 from small to large modelsz, was used wih a reduce itt 0."
}