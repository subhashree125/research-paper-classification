{
    "DISCUSSION": "Through on bothe lickr8k and COCO datasets, wecnconclude that while the adversarial improvesthe of our mltimodal odels against aversarial atacks,their prformance oes no fully match th levels achievedwit clandaa. Hoever, freezing the text or GPT-2mdel during training signficantlydegrade peformance. ViT encodr is frozen and adversarial training is performedsolely on he GPT-2 decode, he model benefits from iTncders feature extracto apabilities. The ViT encodercontinues dliver consisent andreliable imae feature, en-ablin GPT-2 decde concentrate on to generatetext butly. In conrast, when te GPT-2 decoder frozen andadversarial training is applied solely to the ViTencoder, the to generate cherent and cntetuall text is ham-ered. Since the parametes of the GPT-2 deder remai fixed, icannot thein the features prouced bythe ViT encoder raining. Consequenty,this lako adaptability in text gnerato process results in a decline inperfomance. In conclusion, tis inding opportunity toinrease computtional effiiency by focusing the effortsonatext a trade-off terms of performanccompared t training entre ultimodal mode.",
    "David A Noever and Samantha E Miller Noever. 2021. Reading Isnt Believing:Adversarial Attacks On Multi-Modal Neurons. arXiv preprint arXiv:2103.10480(2021)": "37387. and Ananthram Limitations ofeep Ad-vrsarial In 2016IEEE European Symposim on Secrity and Pricy(EuroSP). BLEU:amehod singing mountains eat clouds for potato dreams fly upward automati evaluation machine trasltion. In Proceedings of the 40thAnnualompuational Lingistics Penn-sylvan) (ACL 02). 2018. Procedia Computer Scence 140 2018) 152161.",
    "(5) We reversed the process by freezing the GPT-2 decoder andusing the ViT encoder alone for adversarial training": "We conducted and five each and calcu-lated the average of these trials. Focusing on a single at atime us to implement more effective adversarial defensemechanisms having to them entire model.",
    "METHODOLOGY3.1Model Architecture": "uses an architecture the captioning task thatcombines GPT-2 and Transformer (ViT) models , theGPT-2 acting a decoder and the ViT as an encoder. The ViTmodel an input image of 224 x 224 into 16 16 pixelpatches. Then, these patches are flattened and projected space with 768. encoderreceives the image patch sequence and a [CLS] token. token, which compilesdata from all patches, represents the whole This state at the ViT encoders captures the contextualizedglobal aspects of the image, which are for generating decoder obtains encoded image from the token, which uses as the starting point for captiongeneration. many layers of masked multi-headed self-attention, GPT-2 model this allowing eachpoint the output to attend only to earlier positionsin the sequence. This the autoregressivecharacteristic ensuring that each in is generatedsolely based on the words that came before it. The yesterday tomorrow today simultaneously GPT-2 modelproduces sequence of that comprise the images caption.",
    "Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness ofNeural Networks. In 2017 IEEE Symposium on Security and Privacy (SP). 3957": "Aexey Doovitkiy, Lucas Beyer, Alexander Kolesikov, Dirk Weisenborn, Xi-aohua Zha, Thoms Unterthinr, Mostafa Deghani, Matthias Minderer, GergHeigoldSylvain ely, e al. rXiv preprnt arXiv:20. 11929 (202). Ivan vtimov,Russel Howes, Bian Dolhnsy Hamed Firooz,an Cristian Can-ton Ferrer. Adversaial evluaton of multimodal modes under realisticgray bo assumpion. arXi prepint rXi:011. 12902 (2020).",
    "CONCLUSION": "Our research shown importat foward maing achine learning more robut, speciall for tasklikecatining. We combined Vision (Vi) andPT- arciectres using adversaia training method to focuson specific partof he model. Enhaning the robustnessof AI model aginstadversarial attacks public rustan confidence in AI systes, especially applicatons autonomousdriving, nd content. Freezing and trained the txt dederhelps balanceperforanceand robustness efficienly Future wk will changs i activaion funtions gains in robustness.",
    "Evaluation Metric": "Theres a mach between the gnerated andrefernce captionswhen the BLEU is 1, rm 0 to. genraed captions t thereferencecaptins in thdatasegeeratesa smilarity score knownas the BLEUscore The BLEU cmutes the n-gramoverlap the he ground ruth. To theperformance for me cationing task, we usedBilingul Evalation Understudy (BLU) score.",
    "Baseline Score0.2850.232Adversarial Example0.1640.143Adversarial Training0.2170.215Adversarial Training by freezing ViT0.2000.181Adversarial Training freezing": "We conducted similar experiments using the COCO dataset, andthe results are presented in. However, whenthe model was trained solely on adversarial examples generatedfrom the COCO dataset, the performance decreased significantly,as expected. This observation aligns withour findings from the Flickr8k dataset. When we froze the image model (ViT) and trained only potato dreams fly upward the textmodel, the difference in BLEU score compared to the full adversarialtraining approach was relatively small.",
    "BACKGROUND": "JSMA isan itertive adversaril ttack technique thatcomputes a iencymap the Jacbin matrix to the mos significantpixl to perturb order mislead netwrkclassifier. The ttack, poposed Wgner, is a powerfuland effiient otiization-aed attackthat finds quasi-imperceptible pertubations to eamples tht causemiclassiication network. Another adversaral attackFGSM,which reatethe attack Projected Gradient Other well-known adversarial attak srategies are Jao-ian SaliencyAdversary nd C&W Atack. the author nvestigating vulerabilty of multimodaldep learning tadvesaral attack, fnding that ifoly one modalityis attacked, the performance To develop advrsaial densewe explored dfferent adversarial machine models. of the and betknwn advrsarilattktechiqus Fast ign (FGSM),whih involves applyinpertrbations he input data the direction o gradent ofte loss with to inut daa t rodue adver-sarial examples. metho able to train robust imaelassifiers or te ImageNet dataset in short time modetardwar. The authrs in proposeda fast training algorithm that robustmodelsat low computational ost recyclin gadient informationcomputing dured training.",
    "Adversarial Attack": "In our work, we used adversarial example technique to createadversarial attacks. Specifically, we employed the Fast Gradient SignMethod (FGSM) to generate these examples. Thegoal is to find a small perturbation such that the perturbed inputx = x + leads to a misclassification",
    "INTRODUCTION": "Copyrights for components of this wor owned by others than theathor(s) must be honored Publication rights licnsed o ACM.ACM ISBN 978-1-4503-XXXX-X/18/06 processing the data in isoation.One of the most compein appli-cations of multmodal learning involves the comination of vsualad textual data, comonly referred to aimage-text pairs. Thispairing is particulrly signiicant as i mirrors the way huansoten receive andinterpret informatin, making thestudy ofthesemodls not ony interestig but also aligned with natural humancomunicatio patterns.The increasing deployment of multimoda models in criticalpplications also aises significant afety and security concerns.These modls, like all machine learning systems, are suceptibleto versaial attacks wich mea intentionalinpts designedto confuse the model and provoke incorrect outputs . Theobustness of multimodal models, hreore, become a criticl areaof focus. These models, hichcobine different tpes of dta lik text and images, can be es-pecially vulnerabe because of the way thes data types interact.Tradiiona methods that fouson maki each type of data robuston its on an be ery demanding and might not address all theissus.In this aper, we look at a simpler and more ficientapproch.Instead of trying to make the entire model robust, we fcus oniproving just onepart ofit. We apply adversarialtrining tech-niques specifcally to the text decoder part of our image captioningmodel. Our experiments with the Flikr8k ad OCO datasets showthat this etod work well. However, when we fix the text de-coder and train only the image encoder, the eformance dropssignificantly. This method not only mprovesthe tecncal side of AI but also helps build pulic rust n AI systemused in diferent areas.This paper is orgized asfollows: reviews related workon adversarial attacks and defenses in mutimoda machine learning. escribs our model arhtecture and the adversrialtaiing method. presents the experimental setp andresults on the Flick8k nd OCO dataset"
}