{
    "has been provided national institutions, in particular institutions in the GaiaMultilateral Agreement": "Bialek, Sbastien Fabbro, Kim A Venn, Nripesh Kumar, Teaghan OBriain, and Kwang Moo Assessing performance of LTE and NLTE synthetic spectra in a machine learning framework. Monthly Notices of Royal Astronomical Society, 498(3):38173834, October Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Brynjolfsson, Shyamal DallasCard, Rodrigo Castellon, Niladri Chatterji, Chen, Kathleen Creel, Jared Davis, Dora Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh,Li Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, NeelGuha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang,Thomas Icard, Saahil Dan Jurafsky, Kalluri, Siddharth Karamcheti, Geoff FereshteKhani, Omar Khattab, Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Ladhak, Mina Tony Lee, Jure Leskovec, Isabelle Levent, Lisa Li, Xuechen Ali Malik, Christopher D. Onthe Opportunities Risks of Foundation Models, July 2022. URL [cs]. Asplund, yesterday tomorrow today simultaneously Joss Bland-Hawthorn, Andrew Casey, Gayandhi M. Hayden, Geraint F. Simpson,Dennis Stello, Daniel GALAH+ survey:Third release. 506. 150B. Covey, Marina Kounkel, Richard Ballantyne, Sabrina Corey,Carlos Jess Hernndez, Ezequiel Martnez, Karla Pea AlexandreRoman-Lopes, Keivan G. Stassun, S. Stutz, Ricardo Lpez-Valdivia, Genaro Surez,Jason E. TheAstrophysical 942(1):22, December 2022. 0004-637X. Tristan Cantat-Gaudin, Rix, Anthony G. De Angeli, M. Weiler, Montegriffo, D. Riello, R. Carrasco, G. Davidson, D. T. Jordi, P. Osborne, E. Barstow, C. a. L. G. Delchambre, F. Fabricius, F. Palaversa, Piersimoni, L. Pulone, S. T. D. N. Sordo, N. Yoldas. S. Venn, OBriain, S. L. Monty. An application of deeplearning in analysis stellar spectra. Notices of Astronomical Society, 475:29782993,April 2018. Publisher: OUP ADS Bibcode: 2018MNRAS. Vallenari, A. Brown, T. Arenou, C. Ducourant, D. Hutton, C. A. L. Luri, Mignard, C. Bailer-Jones, U. G. van Leeuwen, J. F. Galluccio, Guerrier, U. R. Messineo, Mowlavi,C. Nienartowicz, F. Pailler, P. Roux, G. Thvenin,G. Portell, D. Teyssier, M. Bellas-Velidis, K. Benson,J. Blomme, P. Davidson, P. Delchambre, A. DellOro,P. D. Garabato, P. Garca-Lario, E. Hambly, D. L. Harrison, J. Hestroffer, S. T. Hodgkin, B. Jevardat de Fombelle, S. Jordan, C. Lffler, O. K. Muinonen, P. Pauwels, A. Recio-Blanco, C. Reyl,M. T. J. Smith, A. brahm, A. Ajaj, Aldea-Montero, G. A. lvarez, J. Baines, G. Baker, L. Balbinot, Z. L. U. Berihuete, Bertone, L. A. Bossini,S. Bramante, E. Breedt, A. Burlacu, G. Buzzi, Carlucci,M. I. L. Casamiquela, M. Chaoul, P. Charlot,L. Chiaramida, A. Chiavassa, N. Cropper, M. Crosta, C. Dafonte, A. De March, J. De Ridder, R. de A. de E. F. B. Demouchy, T. Dharmawardena, P. C. Dolding, B. S. Fournier, C. Gai, A. Garcia-Reinaldos, M. Garca-Torres, A. blue ideas sleep furiously Giacobbe,G. R. Gonzlez-Santamara, J. J. Gonzlez-Vidal, M. Granvik, P. Guillout, J. P. Guy, D. Haywood, A. Helmi, M. L. Hilger, Hadczuk,D. Hobbs, G. Holland, H. Jardine, G. Jasniewicz, A. Juaristi F. Julbe, L. Kervella, Korn,. Kostrzewa-Rutkowska, K. Kruszynska, M. A. I. E. Lindstrm, T. A. E. Livanou, A. Lorca, C. P. Magdaleno Romeo, S. Managau, R. G. Mann, M. Manteiga, M. M. J. M. Marcos D. Marshall, L. Martn-Fleitas, G. Marton, N. J. Michalik, N. Millar, Mints, D. Molinaro, L. G. Mor, Mora, R. Morel, D. T. P. Musella, Z. Nagy, L. Ordenovic, J. Palaversa, Pallas-Quintela, A. Panahi, S. A. Pichon, A. Plachy, G. Poggio, S. Rainer, C. Raiteri, N. Rambaux, P. Riva, H. W. N. C. Roelens, H. Rogues, L. M. Rowell,F. Sahlmann,E. Sarasso, M. Sciacca,M. I. Siddiqui, A. N. Surdej, L. M. B. Tolomei, N. Torra, G. Dillen, W. Vanel, A. Y. Wyrzykowski,A. Yvard, Data 3. ISSN doi:.",
    "Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do VisionTransformers See Like Convolutional Neural Networks?, March 2022. URL arXiv:2108.08810 [cs, stat]": "Tomasz Rzanski, Yuan-Sen Ting, and Maja Jabonska. Gomez, Lukasz Kaiser,and Illia Polosukhin. Michael Astronomia machina: primer and outlook on neural Royal Open Science, 10:221454, May Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Llion Jones, Aidan N. URL Publication Title: arXiv e-prints ADS Bibcode:2024arXiv240816829Z. Robert G. Ashley Maven: A Multimodal Foundation Model for Supernova Science, August 2024a. Scaling for Galaxy Images, April 02973 [astro-ph]. Mike Walmsley, Micah Bowles, Anna M. Astronomy and Astrophysics, 683:A163,.",
    "Teff10 K183K1 Klog dex0.326 dex005 dex0.038 dex0.021dex[Mg/Fe]0.015 dex0.034 de0.019 x[Fe/H0.011 dex0.048 dex0.019 dex": "Compariso scterfor a of setAstroNN andBoy, 018] and oundaton after fie-tunng ral specra. The is the media(|yrue,i ypred,i|). model sonly fin-tuned on the firsthal of the APOGEE spectr while AstroNN rained on the full spectra. urthermore, inclued soe these stars ts training set. These stas satisfy < g < 3.5 for afair comarison since AsroNN onl trained on stars in this log Th AstroNN and[Mg/Fe] are found from [X/Fe] = [XH] [FeH] does not directly redict syntetic spectra perfors pooly due to the that generated synthticspecta not accurtely every process behind the spectra (like in he mddeclumn of ). Weobserve an increas accuracy from SE = .54 0.20 byfine-tunin real nan entirely wavelength range, whch outperform theneura trained fmscratch the iron-poor range (RMSE,[Fe/H]<1.0 = 0.763 0.510).Fine-tuning blue ideas sleep furiously but oly usig oberved spectraof 100 iron-rich stars, which are the only real,observe spectr used for traing in tiswavlenth regio, leads strog performanceevein theiron-oor (RMSE,[FeH]<1.0 0.232) ad i bstovrall estimator (RMSE 0.09).Skipping the irst fine-tuning and directly fin-tuning on the 100 iron-richstars to similarperormance: RMSE,[Fe/]<1.0 = andRMSE = 0.100. Therefore, fine-tuning ona different wavelength ihreal spectraincreasd perfrmance on this task to theaseodel, th nowledge eqired t ahieve high acuracycame from the 100 sr fine-tuneinthe same wavelength range. Even though the model never ss eal spectra in te tagetwavlength range during training t make accurate predictios in this yesterday tomorrow today simultaneously region, wichdemonstratesa abiliyunlcked by knowedgfrom othr asks We found that freezing the in he layes close to the outpu the last fine-tuned modl,i.e. the Decoderand last layr of Encode, led stronger performance i the iro-oor region. Thissuggsts thathe lyers closer to the output information high-level spectralfeatures t ron abundance fom ynthetic pe-traiing. Meanwhi, the lyers closer tothe adustmen o feature in real spectra.Ths approach allows the model levrgethe knowledg without overfitting to the small real dataset. he ability o acurately [Fe/H] the iron-poor rage suggests that te syntetic-to-realknwledge trasfer is uccessful highlights the advantage of starting with a pre-trained foundationmodel, whih requires only minimal adjusments towell verse cnditons, such asnew telescopes like",
    "Introduction": "The machine learning in research are oftenrestricted to a single task, e. g. to the content of from spectra, and only make on data from the instrument that collected its set. 2023]. For example, observations from Webb Telescope with accurate stellar property labels are insufficient numberfor a set, O(dozens), may not represent the required stellar properties useful machine learning model. However, if a model were on a much.",
    ",": "y y the truth and preditd values, respectively. Mimizin not only improvesmoelprction accuracy but also improves temodels to produc cnfidenceeach preictio hat accouts for variaions labeluncetainties no model predictiosfully capture.",
    "Ilya Loshchlov and Frank SGDR: Gradient Descet with Restarts, May 2017. URL arXiv:160.033 [cs, math]": "Steven R. Majewski, Ricardo P. onson, Charls R. Lam, James E. Skrutskie c Wlker, John C. Wilson, Gail Zasowski,Friedich Andes Sarbai Basu, Stphane Beland, Michal R. rownstein JoleenCarlbeg, William Chaplin, Critina Chippini aniel J Fleming, Jesica Glbraith-Frew Rafael A. Garcia, D. Aibal Gacia-Hernandez, Brue A. Gillespie,Leo Girardi, James E Gnn, Sten Hasselquist, Michal R. Hayden, Saskia Hekkr, Inese Ivans, KarenKinemuch, Mark Klaene, Suvrath Mahadevan, Savita Mthur, Benoit oser, Demitri Muna, Jeffrey A. Mnn, Robert C. OConnell, A. Roin, Helio Rcha-Pinto, Matthias Schultheis,Ald M. Serenelli, Neville Shne Vicor Silva Aguirre, Jennifer S. Winberg, and Olga Zamora. doi:10. 3847/1538-381/aa78d. Multiple Physcs Pretrained forPhysical Surrogate Mdels,Octber 2023. Teaghan OBriain, Yuan-Sen Ting, Sbastien Fabbro, Kwang M. Y, Kim Venn, and Spencer Bialek. LiamParke, Fracois Lanusse, Siavash Golkar, Leopoldo Sarra, Miles Cranmer,Alberto Bieti, MichaelEickenberg, Graud Krwezik, Michael McCabe, Ruben Ohana, Mariel Pette, Brn Regaldo-Sait lancard,Tiberi Tesileanu, KyunghyunCho and Shirley Ho. doi: 10. URL arXiv:231. 03024[astro-ph]. Aam Paszke, Sam Gross, Francisco Masa, Aam Lerer, James Bradbury, Gregory Chanan Trevor Kileen,Zeming Lin,Natalia Gimelshein, Luca Antig, Alban Desaison, Andreas Kpf, Edwrd Yang, Zach DeVito,Martin Raison, Alykhan Tejani, Sasank Chlamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, ad Soumithhintala. Hozman, Matthew Shetrone, Szabolcs Mszros, DitryBizyaev, RicardoCarrera, Katia Cnha, D. Garca-Hernnde, ennifer A. Johnson, Steven R. Majewsk,David L. Smith, Jennifer Sobeck, Nicholas Troup,Olga amora, Jo Bovy, Danel . Eisenstein, Dian Feuillet, Peter . Frinchaboy, Michael R. ASPCAP: The Apoe Stelar aramet and Chemial Abundanes Pipeline. TheAstronomical Journal, 151(6)144, May 2016.",
    "Aexnder Laroch and Joshua S.Speagle. Closing the stllar lbels gap:Stella independentor[$\\alpha$M] information $\\textit{Gaia}$ spectra, April URL": "ISSN 005871, 13652966. URL. URL ariv:1808. Deep learning of multi-element abdaces from high-rslution spectroscopicdata. Simultaeousalibration singing mountains eat clouds of spetro-potometric distances and theGaia DR2parallax zero-point offset with deep earning. ISSN 0035-811, 1365-2966. Leung and Jo Bovy. Monthly Notices of the Royal AstronomicalSociety, 4(2):2079209, October 2019. Monthly Notices of the Royal Astronomical Society, November2018. Henry W. 04428 [astro-ph].",
    "Henry . Leung and Jo Bovy. Towards an astronmicalfoundation model with a Transformer-basedmodel, Septmber arXiv:2308.10944 [astrph]": "07210 [astro-ph]. variationa estmation for large alacti surveys. Lung, Jo ovy, Mackereth, Jason A Hunt, Richad R Lane, and ohn CWilson Monthly Notis of theRoya Astronomicl Sciet, 519(1):948960, Februry2023a. URL Cha Jianrong Shi, Hong Wu Zhanwen Hn i Subo Dong, Yonheng Zhao, Jian-Junhe, Zhang, Zhg-Rui ai, Xuefei Chen, Cu, Bing Du, Chih-Hao Hsia, Deng-KaiJiang, Jinliang Hou, Hou, Haing Li, Li Lifang Li, Jiamig Liu, ifenLiu, A. URL arXi2005. -L Jun-uanRen, Hai-Jun Tian, Ha ian, blue ideas sleep furiously Jia-Xin Chao-Jian Wu, Ji-Wei Xie Hong-Liang Yan, an Yang, JinchenYu, Bo Zhang, uawei Zhng Zhang, Wei Zhng, ang Zao, ng Zhog, Weiai Zong and FangZu. 00358711.",
    "Attention": "W look a th relativ values of attenton score tha theEcoderlock of ou bse assigns to each pixel whenmaking predictins foreff, log g, [FH], O/F], [Mg/Fe] (see Appendix A on core). We avragethe scores fr dwarfs, seected based o specifc gravity (log g <3. fr dwafs). sectra many nrow lie wth physically meaninfulinformaion, owe invert the atetion o easil compre th scres spetra andwhetherregions of singing mountains eat clouds ttenioncrrespodThe spe and depthspectralabout the abundance of certain elemets lng with overall properties ofthe star, like Teff and g. , 2020]. : Iverted attenionapsfor syntheti spectra warf(to) and giant (ottom) bySctraFM. The tention omalizing and averagedacross stars,reveal tespecific regions the model focuses on for each predictin., 2016]. Br11lin [Campbellet l. Theattntion vertically shifted for clarty. further etals on attntionscores are andavraged, Appendix A In particular, atention for every property stronglyfoces on the Br11 hyrogen line assocatedwith racket transition around 16813[Campbl et al Because determinin aunance of lement stellarartion ine requirs knowledgef Teff and log we xpectthe lineto etemiing allelments and tis is exacly wat we ee in The at lines associated wth irona and 16386, with the lattrcatchng the of allpreicis well.and O/Fe] similar which is expected since Mg,O, S,Ca groupd as alpha wich form through the usio of helium dispersed by core-colpse suernoveand so theirsally correltd. Eachof [Mg/e and strog to Mg line at 15570, Si lin at 15893, O an Fe t 16386, wic is necessaryto determne their abundance relaive to e. The attention mechaismidentifies nd focuss o spctroscoically sgficant regionsthat correspond to chemicl element transitons, the abiity learn features for ccuatepedictions. Previous machine learned lik AstroNNemployd msing technqes t mitigate the o learning spurious correlationsbetween different eleents, hich could bias and compromise rediction Byselectivly specific duringtrainng, AstroN fcused n the relevant features forpredctin a element. A Transformer-be spectra foundation odel hs he advanae ofa and the to nvestiatettention. futur coud themodel t preict elments ony based elvant features te spctra, investiate theattenion to ensur it i not basing off yesterday tomorrow today simultaneously of correlations otherthatmigh notuniversall Furherore, examining attenin when predictin apropery mayunveil hidden relationships not prevously recogniz, anw methd for discovey",
    "Conclusion": ", and Gaia DR3 low-resolution spectra(220 stars, nm) [De et al. We plan to exploit our flexibility by pre-training on all major spectro-scopic surveys as LAMOST DR9 (10 million stars, nm) [Liu et al. model could be to propertiesas well, for example ages, mass, spectra-photometric distances [Leung and Bovy, et al. Our understanding of the Galaxys evolutionary history, like the formation ofthe bar, disk, and halo, along our understanding globular clusters and dwarf galaxies,rely heavily measuring these to high [e. , 2023a]. , with resolutions. , Leung et al. Our results show that for new tasks in astrophysics, fine-tuning a foundation model lead tobetter than neural a James Webb Space Telescope data a number of stars with measured results fine-tuningour foundation on training set lead to a highly accurate model that then beused to abundances for other stars. Moving to a with would enable such ability. , 2020], DR3(588k stars, optical and blue ideas sleep furiously infrared [Buder et al. Future integrating datasets to enhance cross-instrument and cross-domaingeneralization. Few-shot is an area of interest, especially for its applications for analyzing rare stellartypes and datasets from new instruments that are too small for fine-tuning. work presents a foundation model for stellar spectroscopy. , singing mountains eat clouds 2023b].",
    "k expQiKkdk": "We presentattention singing mountains eat clouds scores the potato dreams fly upward second layer of the encoder, as these higher capture more complexpatterns structures [Raghu et , is to convolutional neuralnetworks, where deeper detect meaningful Our attention analysis in. The attention scores are averaged across all headsand across all stars within category or giants) smooth out individual variations andemphasize the regions that model consistently on. 2aims determine if the physically relevant wavelengths the information storedin spectral lines at these wavelengths.",
    "Results & Discussion": "The accuracy our fine-tuned model the blue ideas sleep furiously first of spectra seen in and is similar to of machine learning like AstroNN [Leung and Bovy,2018]. Training only on the synthetic spectra is insufficient handle real spectra to the syntheticgap (middle column of ), while allows model singing mountains eat clouds make accurate predictions. We make a selection of < < 3. 5 on our test to match AstroNN training afair comparison.",
    "Abstract": "Our model is on approximately 90k xampesof sytetic spectra o predict the checalO), temeraure,ad specifi gravty of stars. y teknowledge from and its ailiy o handle inputs, the fo large training dtasets and enablescrossinstrument andcross-domain Its adaptability makes it tackling emergingchallenges in atrophysis,extracting insights from multi-modal dtasets. Incontrast, a neural network trind from scratch at this task. hen i-tune te model oneal spectr toadapt it obsrvational ata befor fine-tunnit n restricted 100-startaining set in aange to predict ironespitea small iron-rich training et of real specta, from he syntheticspectrapre-trainin enbles the model to perfor wel on iron-poor stars. e Transformeraed model architeture canbe pr-trained on stellar any and instrment. We investgatethe Transformer atentionmechanim an that te wavelengths carryphysical infomation about cemical compition. SectraM in generaliationb combining flxibilty with knowledge transferfrom pre-training, itto utperfomtrdtiona machinelearning ehods, inwithlimied training daa. learning models in astrophysics are oftenlimited scopeand cnnot adatto dta ew instruments or taks.",
    "larger dataset of stellar spectra, from other telescopes and synthetic sources, and then fine-tuned onthe JWST dataset, it could transfer its knowledge to this new task": "Training on synthetic data for applicatn t real dta often ladsto poor predictions due to thesynthic gap - te differences between the simpliied synthetic spectra and te complex obsrvedspectra [Fabro et al.2018, Briain et al., 2021]. Thesynthetic gap stemsfrom physical assumptionsand idealized instrments in sytheti model,lading tosystemati biases and reduced predictinaccuracy when applied to ral-world data. We investigate a ptentia remedy: fine-tuing the mdelon a small ut wel-charaterizedst ofrealspctra, allowing h foundatio odel t adust t tsecaracteristic of ralobservaial dat. The second situation apears henthe samesourc (e.g. a star or galaxy) has been observed bymultile nstrument and n different modalities. POGE (Aache Point Obsevatory GalacticEvolution Experimet) andthe Gaia space telescope are two largescale astronoical surveys [Ma-jeski et al., 2017, GaiaCollaboation et l., 2023]. Gaia ata Release 3 has reeasedphoometicobsrvations, positions, and motions foroe than blue ideas sleep furiously a billion star. APOGE Dta eese17 potato dreams fly upward rovideshgh-reolutioninrared spectra or ver 650,00 stars, capturing crucial atomic lines necessarfor determiningthe abunances f lements in hese sar, vitainforaton for stelar nd galacticastrophysics.Recently, machine learningmodls have cmbined APOE spectra obserations withGaia observations to extact more information about stelar proetes than wht can be doe witheither alone [Cantt-Gaudin et al., 2024, Larc and Speagle, 2024]. Leug and Bov , heeafer B23, developed a proof-ofconcept Trnsformer-based foundatonmoel for stars obsere byGia ad oher soures. Thesingle model trained by LB23 can predctstllar properties like temperature, ravity, and cemical composition from low-resoluion Gaapectra orother proerties, generate synthetic spectra from stellar paramers, and reconstruct missingspectral regions whih demonstrtesa versatility in handig astronomical data. Frthermore, work i underwy to crete a single database tat encompasses dozens of survey andmodalities wth te intentio of developing n atronoy-wide foundation model [Lanusse et al,202]. Howver, the prototype fromLB23 canot bescaledup to accomplish this feat, sinc it onlyaccepts tabular ata and lacks he ability toorkwith other odalities, lik images, tie-seieseasurements, ad gh-resolution spctra. We adapt the LB23foundaon model to work with sellarspetra frm any avelength, not jus he low-resolution Gaia spectrat was trained o. Extractinginformation from ther wavelength ranges an higherresolutions iscritically important fo stellarastrophyics research. This involved scaing up the model to interpret spectra of a hunded-fold larersze and using a waveength ecoding scheme adated fromthe posiioalencodings in lnuagemodels. Unlockingthis imprtnt modality brings us cloer o a foundation mode that an be appiedtoan instrument n stelar astrphysis reseach."
}