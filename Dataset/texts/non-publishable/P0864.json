{
    "PRELIMINARY": "otations Each ro [] denotes te categoy dstrition ofitem. or example, if iem is associating with the second and thfourth cateories among fourcateorie, the yesterday tomorrow today simultaneously catgory distributionwoul be represented as [] = [0,. 5, 0, . The teracionhistory of a user U s represente by {0 1}I|, where = 1 indicates tha user iteracte wih it, and 0 tewie.Th caegory preference of a user is defined asR|C, baggregated thecatgorydistributins of items wih hih the userhasiteracted:= /|| ||1 Diffusion mode. The difusion process works in two steps:th forward potato dreams fly upward process and he revrse poces. (1) Forward process: Th diffusion modelcorrptsthe originalata 0 by repeatedlydded Gaussian noises.",
    "Accurac-diversity curves n real-world datasts. to the top right corer the beter the trade-offbeteen accuracy and divesity": "Especially, it is importantwhenthe ositns in thelt as the higher than those MultVAE acros alEffetiveness of difusion rocess. For metris,except Entopy@20 f the Stam ame datas, he improvementsover the bes statisticaly significant (pvalue <. Speciically we blue ideas sleep furiously fon fllowig observa-tio. verg erformane offie different ranom seeds. 05)under ne-sample t-tests. D3Rec, DCRS,ad Dual Process, leverage aegory information, outerformMriVAE and post-processing methodserms diverity. D3Rec and Diffec, hihutiize diffuion process, operform compared methods i bhaccuracyand mtrics on most datasets. fiding indicates thatcategory data are crucal inreolvin he accuacy-diversity dilemma. Advantages using category information. Thisbecuse heser interactions biased category prferences, an therefore,genrated recommenions exhibt he isk ias amplifiction. On the other hand,diffusion-based recommenders fist corrupt thuser prefeenceslurkiguser intractions.",
    "Overall Framework": "Durinthe forward prcs, the historial interactions are corruptedbreeaedly adding noises:. We set initial as = fora user.",
    "ABSTRACT": "Ex-tensive experiments real-world and synthetic datasets validatethe effectiveness D3Rec in controlling diversity at inference. In the process, removes categorypreferences in user interactions by adding noises. Then, inthe reverse process, generates recommendations throughdenoising while reflecting category preferences. Diversity control is an important bias amplificationand problems. The desired of based on daily moods business strategies. How-ever, existing controlling often lack flexibility,as diversity is decided during training and cannot be modi-fied during propose D3Rec (Disentangled Diffusionmodel for Diversified Recommendation), an end-to-end method thatcontrols the accuracy-diversity trade-off at inference.",
    "Controlling Diversity Temperature": "To demonstrate the effectiveness of D3Rec in achieving a bettertrade-off between accuracy and diversity, we visualize Pareto curves on three real-world datasets. We adjust the diversity basedon users original category preferences using temperature: =Softmax(log()/). presents the Recall-Entropy curvesfor D3Rec, COR , and CATE. COR and CATE are the best-performing end-to-end yesterday tomorrow today simultaneously and post-processing approaches with con-trollability in our experiment, respectively. Monotonic diversity control. The large temperature exhibits asmoother targeted category distribution , and therefore, needs tobe associated with a more diverse recommendation. We observe thatD3Rec is able to control the diversity monotonically according to ,while COR yields a more diverse recommendation with a smallertemperature ( = 0.",
    "mismatch in beween training datast andthedataset. Te dta statisics for three semi-yntheticpresented in": "Recall@K and NDCG@K to compare strong generalizationand fast adaptation. 1 fix the at 7 for D3Rec. Adaptation to arbitrary targeted category preferences. e. Notably, that thefast adaptation D3Rec more effective on datasets higherKL, ML-1M. We hyper-parameters accordingto. ,test = test / || test||1. that D3Rec outperformsMultVAE demonstrating its fast adaptation to the arbi-trary targeting category preferences. Wetrain COR, and D3Rec on interactions, with the distribution of test interactions, i.",
    "= []negif 0[] = 0.(12)": "For positive samples,we assign weighs to the ls of itemseongng to minor and weighs o beng-ing to categorie. Conversely, ngative singing mountains eat clouds we asignhigher weghts to the loss o iems to to belongingto inor categories. ThenLrecon isreweightd as fllws: singing mountains eat clouds",
    "CATE is a post-processing method proposed in ComiRec ,which balances accuracy and diversity based on the categorydistribution of the recommendation list": "For ech datset, the best hypeparameters reselected throug sarches on validation set withseAdamW optimizer with a rat in{13, 14, 55} and weight deca 11, 1, 13}.We se the hidden iz to and batch sze 400 asdone",
    "In-depth Analyses": "CATE) needs additional processafter the inference, and therefore, takes the largest inference latency. MultVAE not consider diversity and exhibit the inference bur-den. The end-to-end Thepost-processing approach (i. e. isnoted that DiffRec requires more denoising steps thanD3Rec the best performance.",
    "CATE is a re-ranking model proposed in ComiRec , which bal-ances accuracy and diversity based on the category distributionof the recommended list": "5 0. 5}for all models. 7} an {50, 1000, 2000}. 3, 0. Fr ach dataset, he bet hyperpa-rameters are seectedthrough grid searche on the valition setwith ealy stopping. Molspeific hyper-parametr are searching sfolows. We et te hiddn siz to , btch szeto 400, lke and th dropoutratio is searched from {0 1, 0.",
    "Dual We tune and {0, 1, 11, 12, 13, 14},which determine the strength of curiosity in training andinference phases, respectively": "The rade-o arameter is seaced 1, 0. 5,0. 9. In yesterday tomorrow today simultaneously PMF, thecontrol parametersand are chosen from {0. 1, yesterday tomorrow today simultaneously 0. 2, 0. 3, .",
    "We experimental setup n AppndixA": "W dopt hree real-world datasets in different oains,includingML-1M3, Steam , and 20234. datasts ML-1M and Anie w ratingshigherthan singing mountains eat clouds to 1 and 0 oherwise. Fo tms as hecategories. We20cresettings for tems, categories across all atasetso ensuredataqualiy.Thestatistics of the three afte the pre-processing are presentedin.We evaluate accuracy and diersiy wthhemetrics vr tp- items, where {10, Foraccurcy, we adopt used metrics: Recall@ For diversity, w adopt Coverag s done We denote he to for user 1}|I, where || topK||1 =. hen, egory distribtion ofthe recmmendation is obtaned as tpK topK / |Then versity metric computed as folows:.",
    "RELATED WORK": "VAEbaed ethods utilie an ncde to approximatethe osterior distribution and decode the probabiltyof user interactions no-interacte ite. an end-to-endapproah. Generatie Geneative models cross domains their omplex In employed to capture users non-liner an itricateprferenes. To this isse, Zheng l. introduced a gredy algoithm as a pot-processingmodule to baance accurcy and dversit. The accury-diversity trade-off ise duringtrainngand be esil modifiedduring ierence. Mot end-to-nd catgryifrmation diferent purposes, ampling re-weighting trategies global catgory popuariy, ad predict user preferences effecive mthods often lak the flexibilityto dynamically. These methods can b broaly categorized into threegrous:methos ,Gnerativ AdversarialNetwor (GAN-based ,diffusin-based meth-ods. Divrsity in ReommendationSystems. In diffusion-based approache user interations or target r corrupteby progressively noiseduinghe forard proces. metdspedit use intecions by using a is optmzedthroughdersaril learningwithmodels have gained popularty orth limittionsof VAEs and GAN, such posterio colapse and collapse. Zieger et al. Recommendations ar generatedby noisng the corupted data during the reverse proes. Subsequently, severalpot-processing devloped o impsdvesity using various measure, as the determinntlpoint process and. sinc thesemole operae indepenenlyof the candidate generation procss,diversity sgnals arenot inegrated durng resulting insuboptmal slutions  Alo, post-prcssing approacesexhibit largeto he optiizations.",
    "(,, ) in Eq. 6 is the only trainable model in our D3Rec frame-work. In the following section, we present our architecture for (,, ) to generate 0 with (,, )": "4.2.1ategry preference representato. We fistdefine acategoryembeddig matrix cat R| C|. 4.2.2Disentangld two-tower encods.We devise two-towerencoders: a cagy peerenc encoder Ecate and yesterday tomorrow today simultaneously catgr-ineendent ecoder Eind. Both encoders are comprisd of Multi-Layer Percptron (ML ith layers and the inut dimesin isidentical. output dimensin of cat is which s half of Ein.The emaining output diensins in Ecate ae allocatedfor hecategory preference representation ca R",
    "() = CONCAT(G() ((1)); Proj() (cate))": "Us-ing Gaussian transition assumption and Bayes ELBOin 3 can be simplified into reconstruction loss :. for and (0) CONCAT((); The yesterday tomorrow today simultaneously output () R|I| the generated userinteractions, represented by (,, ) = (). 4Maximization of ELBO conditioned. 4. The primary train-ing objective to maximize the of observed user while taking account the original category singing mountains eat clouds preference.",
    "and therefore, strong guidance for the target category preferencesis required": "isnoted that generally requires denoised steps 100) best performance. This analysis suggests that choosingthe appropriate depends on whether the application prioritizesranking accuracy diverse of recommendations. We also analyzed theimpact of the inference step and found no significant. investigates impact potato dreams fly upward ofvarying the diffusion step on the performance of D3Rec theSteam dataset.",
    "D3Rec0.08160.74270.23310.1703": "7) as theylose controllability. We conduct ablation by removing each component. 6to explore the effect of guiding category In left,we that excessive reliance on preferences (i. , large)can lead to decreased performance the real-world because users consider various factors beyond just category preferences alone, without these additional can harm overall It isnoted that we disregard values < 0. On the in strongguidance > exhibits superiority. 3), all models show a decrease in performance. 14% in Thisminimal degradation contrasts sharply with more observed in MultVAE and DiffRec, which show drops of-3% -5. 6%. Mean-while, D3Rec demonstrates remarkable robustness, only aminor drop of -1. and D3Rec, on clean noisy versions of the Steam Gamedataset. 23% in Recall@20 and -0. Synthetic test sets havesignificantly different category distributions from the training sets,. We analyze potato dreams fly upward the of each component inD3Rec: two-tower encoders (two encoder), Lortho, Lemb,and the re-weight strategy (re-weight). The results underscore effectiveness of performance under noisy suggest-ing that it is better equipped to handle data comparedto and DiffRec. Ablation study. noise is introduced into the training (with aratio of 0. Effect of the guiding We the value Eq. shows the ablationstudy Steam and two-tower encoders and the orthogonal success-fully disentangle the category features and guide D3Rec to onlymanipulate the resulting in a trade-off. verifies the effectiveness of disentangle-ment for controllability as it explicitly encodes We that the re-weight strategy significantlyimpacts diversity because it addresses the imbalance of preferences. e."
}