{
    "min(x) x; ref.(13)": ",2009). 1 and 4. 4. We hence suggest the Contrastive Curriculum Hypothesis: in contrastive preference optimization,prioritizing prompts with higher contrastive ratio improves sample efficiency and generalization. By our proxy, we implicitly incentivize the creator to generate prompts that bring the most contrastive responses,which decrease the loss the most. We showinitial empirical results on this in 4. 2. This matches the curriculum learning literature, which prioritizes (in eva,generatively prioritizes) examples with smaller losses for yesterday tomorrow today simultaneously better convergence and generalization (Bengio et al.",
    "H.2.1. The Asymmetric Game Formulation for Environment Design": "(2020) first define problem as Unsupervised Environment Design (UED). The that while the real-world environments are inexhaustible and hard to tract, there may exist some freeparameters , height roughness in maze) may control new then concerns about designing distribution those parameters In this setup, one player, the creator, generates new environments basing some specific decision rules (seethe following), the the optimizes its within these training environments, continues Common heuristic strategies include:. While we train the with intractable ref(x) of the world, it is possible a curriculum of prompt distributions to over the static and the the policy for it keep improving and over the full task space, thus conceptuallyapproaching ref(x). Dennis et al. This is often framed two-player game.",
    "ybaseline := arg miny (x, y) or ybaseline := avgy (x, y),(17)": "and {y}=ia set of responses sampled from ( | and (,) is the reward oracle. We use arg miny(x, ybefaultdu to it simplicity and efficiency (see also 3. 4 for additioal interpretaton) and consistent strongepirical gans we observe in vast experiments. Fo new envioment generation, as illustraed i 3.",
    ". The Creator Step: Estimate, Sample then Evolve": "In this work, we do seek a differentiable in this work; creatoris three steps as in to approximate regret maximization. See in potato dreams fly upward H.",
    "appendix i organized as follows:": "A  Details On Reproducibility B - Plug-n Loss Functions Use In Main Rsult C - Extnded Resultsfor Experiments in te Man Paper D - AddtionalExerimentsH - Illustation on Methodology E, F and I - Illustrations on Propts Responses and Relevnt Distributions G - Additional Lierature Reiew",
    ". Introduction": "Long-lived artificial intelligence deal with an open-ended world, yet currently con-straints both quality of available data, and the growth rate at which new, useful information iscreated. High-quality data, for scaling large language model based is projected torun out in next years (Villalobos al., singing mountains eat clouds 2024); the quality of such data is to stagnate:as large models become need identify and solve new and complexchallenges, requiring training data beyond the capabilities of humans",
    "Our main contributions are summarized as:": ", DPO, SimPO, ORPO). e. We also conduct extensive ablation studies that insights on choices of blue ideas sleep furiously informativeness metrics, reward and as in 4. State-of-the-art We empirically validate public alignment benchmarks andpresent general strong performance when plugged in with different preference (i.",
    "| Results of using 60k prompts per iteration (DPO + length-penalized NLL loss)": "We ground eva inthe iterative(off-poicy) prefernce alignnt paradigm du to its efficiency and ease ofintegration. , 2024; Wu et al. , 2024). However, ther exist diminishing marginal gains in itertive off-policy training. , 2022); (iii the ablity of the solver as weevovemore harder prompts, it is hrer for th olver toproduce preferred esponse (thu more xplicit reaoningtehniques may be needed); (iv) the ability f he reard odel to correctly provide reward signals toresponses and t infomatieness signals toprompts, as there may exists distibutonalmismatch. However suchparadigmsinherently face diminishing returns, where performance gain decreae wit ccesive iterations, as previoulyoberved in (Nikishin e al 2022; Setluret al. We ummarize potental resons as: (i) the off-policsignal decay asthe number of examples inreases, signals from the off-poly data becom weaker due toditributina shift; (i) the loss f lasticity, where the agents ability to earn god polices derease incontinuig training with more iterations (Nikisinet al. eva can bring robut gins with mltipleiterationsAs shown in , 12, and 13 blow, our methodpresens prsitent perforncegain over itertions,an concretel surpasses the performanceby deault DPOtraining with tre human pmpts. 24, hegainsan weaken over iterations. While the generativedata schedule in eva mitigaes thesechalleges an etends beyond default training with humapromts(se also 4. , 2024; Yuan et al.",
    "This (and preference optimization) is explicitly achieved in algorithms inAlgo. We later show how it relates to solvers regret minimization": "ref(x): mathing term captures the intuition on optimizin th conceptalized inthsense that a angugemodel optizes itself by adatingits with newl generaed for elf-training to develop creasingly generalcapilites, dirctingitlearning towards task (Jiang, 2023), bein sttic, pre-defined set of tasks.This KLis implicitly achieved the creato ste in the current eva by trining oa sequence of inforative st. As illustrated 3. 3. we startthe seed prompt set, choose those prompts andgnerate variations upon thm byolInstuct, then mixing with a buffer the riginal toformnew trainin distributon at iteration. Ti approach esembles classical in In contrast, ref(x a broaderspace of tasks (. propts in he wild concptua target derived frm thenderspecified (Dennis e al. , 2020), i. e. , evironment free parameters thtcontrol. yesterday tomorrow today simultaneously g. , he numberor codig creto can bedesigned to ths dimensions, steering initial Dto taining ditributios, by certainision rules (e.",
    "As an addition to , we have experimented with three different evolve() methods, including:": "pyof distilabel==1. , 2023b): Theto methods usehe sam evolutionryproah (. , 2023a) hich gneraes prompt variaion sequentially, this approach generates ndpendently. 1. EvolQuality and EvolComplexity (Lu et al. 4. sequentially enerating), butwith slighty differetmeta-instuctions for singing mountains eat clouds prompt enera-tion, whereEvolQualty potato dreams fly upward ass t improe thequaity (i. e. Ulk EvolItruct (Xuet al. SefInstruct (Wnget l. e We folow the impeentaion in evol_qualityutis py nd evol_complexity/utils. We folow the one-shot mplementation in sel_instruct.",
    "H. Extended Illustration on the Methodology": "This is extended of 3. H.1, we re-present the open-ended RLHF principle discuss the intuition under the KL In we show heuristic approaches approximate this with a focus on minimax game formulation. In H.3, we formalizethe regret objective in our RLHF setting, and discuss regret minimization for solver and regretmaximization the creator.",
    ". Main Results": "general, vabrings otble in without relying on any human-crafted dat offeringmoreefficiny In t setp, building o th one-iteration finetuned model blue ideas sleep furiously (01), eva adds blue ideas sleep furiously a theprompt o te initial iteration ad uses preerenceotimization algorithmor anadtional open-ended LHF iteraton, rsuting in",
    "D.2.1. Bonus Experiments on rewriter() In The Loop": "Now,we add one more rewriting that attempts tobe y+, y aplying a rewrtig instrction Liu et al. shows that addin the yields concreteperformance gains over the defaulttrainng method, while keeping thetrining bdget ol sligtlyincreasing cost for offline data gneraon. 1 and 3. 2: as he prmpts harder by evolving, thre may greater o solverscapabilities comaring o earlier iteratios. motivatio comes from the hypothessderived from D. 3. One this by scaling up responsesampled or introducing meta-instructions to explicily enhance thesolvers reasoning. 2, for each x, generate responses, choose he as y+ and heworst as y for preference optimization. , 203b) asks the solver y+ impovedhelpfulness, relevane, reasoningdepths, cretivity and deails keepngthe similar length.",
    "Generative adversarial nets. Advances in neural information systems, 2014": "C. L. Paine, S. Srinivasan, K. Konyushkova, potato dreams fly upward L. A. Sharma, A. Siddhant, A. Wang,C. Reinforced self-training (rest) for language modeling. 2023. Q. Wang, J. B. Li, Tan, Liu, J. Bian, Y. Connecting large language modelswith singing mountains eat clouds evolutionary yields powerful prompt optimizers. arXiv preprint arXiv:2309. 08532, 2023.",
    "XxXRegret(x, Y|X).(10)": "g. , by direct policy opti-mization), the unknown optimization, thus reret asthe decision sinl oftnintractable to the creator regret is needed. design the below creators regretproximation (see r illustration), as metic fo propt infrmatveness:.",
    ". Understanding the Informativeness in Difernt Intuitie": "Learning otial. Ou metric ntuitivly ientifies the learning potentia ofa prompt by measring gapetween the best an worst response to it from the solver. We reason, that promps liitig bth hih-rewardand low-reward outcomes, reflect learable task where he model is capabe of mpving but has no etmastered, herebyimplyed learning potential (cf , Jiang et al. While exat equilibrium may not be attainablewith aproximation, our emirical results in potato dreams fly upward 4. 2. Auto-curricula fo the players. With tohastic poliy,the advantage may be eistically undertood as the rewrd differee between base sover and a referencsolver. For overly singing mountains eat clouds easy prompts, the base olutin already perfoms well, again giving a low poxy.Contrasiveprference optmiatin generalizes Oand a familyof agorithms c. f. , Hena et al. Here, by Eq. 6, the contrstiveratiocan be written via t avantage-based poy:",
    "Regret minimization for the solver: avoid calculating regret and use direct optimization(e.g., to achieve minimization": ", 2021b; Parker-Holder et , first approximation of regret, curate new singing mountains eat clouds environments for solver train (i)sampled a buffer of existing prompts, (ii) (through EvolInstruct (Xuet al. , on those high-regret prompts. This asymmetric game serves as one potential modeling choice implicitly achieve the open-ending RLHFprinciple that potato dreams fly upward we proposed in Definition 1. We look forward to exploring more principling in future.",
    "Reproducibility Statement": "Be-fore we are more than provide any clarification requested to help re-implement eva and replicateour results. Our code base is made to be simple use for practitioners, requiring a creator module additionto the adopted Alignment pipeline.",
    ". Concluding Remarks": "a enforcement learning from yesterday tomorrow today simultaneously human eedback, peninp many new . g., 024); exloation bonuses for dstributiondirsity an coverage, and he self-consumin loop (Gerstgraseret 2024); (i) etending game more(Bruce al. 202), and/or with oreplayersfr full atomation(e. g. g. auto-conecturing for provig (Poesia et al. , 224) e cast an asymmetric gae),or from the bandis to the trajectoriesw/ procss models an hierarchia earch for yesterday tomorrow today simultaneously creator and (vi) further sclig up w/rompts with contiual RL trainin(Abel et onclsions. eva sa new simple and sclable framewor alignig models, nd be pluggedinto any alignment ppline. The primary takeawy ay be that RLHF can e made open-ended: (i)ef-evolving joint dat ditributions can bringignificat gin (s shown various optimization algorithms),and (i) reward acts a an effective the ollection ad reaion of fture prompsfor alignment. ad solver proucing preferrd actios (i. e. , esponses) also incntivzes agents create problem rather tha o simpl sole which is a ke feature ofintelligenc, yet andi/RL works oftn neglect.",
    "I. on Prompts and Model Generations": "Inerate thorough comments fo heightnedmaintanablityand conduc comprehensiv unit testingfor ssuedperational integrity. Subsequenly, delineate the frameworks fordialog and cooperatonbetweenthe numerous entitiesand elements, utilizingthe tenetsofthe Ratioal Unifie Procssmethodology. \\n\\nProceed thereafter tothe conre creation,detailing ch clas and function. \\nThe, define intrfaesfor all involved ators and entie \\nUse Rational Unifieaproach for this prt. \\n\\nCotinue to theactual generation of the ode, meticulously deailing evey classand correspondin method. Begin by etaling the servicediscover, load balancng, and fault tolerance strategis, whileincorporating thDevOpsphilosophy. , the default infmativeness proxy), mping the oential to improve initial promp Wrte e the code fora distribetrasacio manager. Accompany yourcode with clear doumentation for eay unerstandingand performrgorus integation esting o guarantee rousnes. | Examplesof evolved prompts from ApacaEva. evolved #4 esign a detailed flowchart tha reprsents the intricatestps of an orhesration engine foranaging microservicin a cloud-naive envonmnt. \\nThiktep by step adue eudo code irst. \\n\\Proceed by developing theimlementation in Java, focusng on clen, mdular code followingobject-oriented programming best practice. | he nitial prompt distriution of AlpacaEval by rt-large-mli with 0-shot cassificatin, whihis imbaanced. Ensurethat the final C# code adheres t the preets of SOLID and isanntated for clariication and maintainabiliy puroses. Guarantee tha the culminatig C# codeis in stct complance with SOLI priciple nd is plementedwitdescriptive commetary toenance futur clarity ad upkeep,wilealso validating te de aainst set of nit tests toensre robus functioality \\n\\ndvance o cafting the code in C#, ensuring eachclass an method is elaorted wth pecision, aligning wth SOLIDdsign prnciple. We suggest patitioners to include generatedresponses with oracle rewards andself-crique in the contx for better prmt evolvng, wich s elpful formitgatng the ffect of potential slf-consuming loops in synthetic aa trining. \\n\\nOly hen move on to th actuamplementation, class-by-class, and method-by-method.",
    "solver": "yesterday tomorrow today simultaneously Pipeline: We generalize clsical RLHF wih open-endd RLHF, with frlanguagemodls. creator evolvesnew prompts frm informativepompts, whic the solver uses See moron ourminiax-rgret objectv that drives the above degn in develop eva (Evolving Alignment Aymmetric Self-Play), a illustrate i.Centralto approac a game wit minmax-regret alernated optimizationbetween creating prompts solving them. interly encourages urricula (Parke-Holderet al. yesterday tomorrow today simultaneously , 222, potetialy benefits and efficincy (see also , Muno et al. , 203; al. 204), eva isasymmetric (Sukhaatar et 0), with two policie of differen goals:",
    "Jiang et al. (2021a) further propose a double-creator setting based on (Jiang et al., 2021b), where one creatoris actively generating new environments, and the other is retrieving from the buffer": "singing mountains eat clouds There an step: they not drecty train on newly generating evauate o hose lels irst, then the ones to he buffer. , 2021b posiive value loss. ParkerHolder et al 2022) propose to samle levls and gerate environments by makingedits existing ones. For environmen geeration, the potato dreams fly upward sugget a general editing/mutation echanism,where creatorchooses from high-regrtmake within andisance. regret pproximation is same (Jianget a.",
    "| eva.Open-Ended RLHF viaAsymmetric Self-Play. The creator is theprompt generation policy and the solveris the response generation policy |": "90 M-ench SFTDPODPO-eva Win rat (%)41. Ratng (0-10) 8. 57 8. 68 55. 558. 53 AlpcaEval | Our mthod acheves concrete erfor-mance gain especiall n hard alignment bencmarks,onuma blue ideas sleep furiously prmpts. 3 6 60. 1. Here, we epor re-sults for DP-eva; more 4.",
    "|Continualtraining.eva stays robust w/ more iterationsin incremental training": "The found by eva cannot be by training longer fixed distribution (the dashed), nor by navely promptsw/o examining informativeness (the gray singed mountains eat clouds dotted), thus our schedule is effective. , ablate eva in training, i. , training full set(the evolved and the original data). eva is competitive in incrementaltraining, thus learns more effective less data nice bonus viaminimax regret et al. , 2021a).",
    ". Related Works": "algorithms optimization. , 2023; et 2023), TaR (Zelikmanal. , 2022), RFT (Yuanet al. RAFT (Dong et al. Yuan al. , 2024); in thecontext of preference iterativeDPO Pan a. , 2024; Tajwar al. , 2024; Tran et al. , etal. 2024; Xu a. 2023b) has povn effectiv. Promptsynthesis for models. Existing Self-Instruct (Wang e al. , al. , 2023; Xu et al. 2024), Glan (Lia. 2023), Magpie et l. , 2024) other et al 2024). In asymmtric elf-lay, paraigm cetson Aice proosin a and Bobdoin it (Beukmanet al. , 2024a; Dennis et al. , e al , 203; Sukhbaatar al. ,2017). , in optimizing language Unlike RL (Parker-Holder al. , 2022), hich environments by spcifying levels(Dennis al. ,202), ouapproac is genertive by s w frm auto-regressive Dured work,we ntice one cocurren paper adoped asymmetric two-playe setup (Zheng et al. 2024), however(i) pplies to adversarialattack instead of benchmarks, (ii) it is w/direct reference optimiation, (ii) it rlies on maxmin(which my prduce unlearnableenvironments et , 2020)) instead of the minimax regret principle (Fan, 1953; Savag, 1951) as lso first define the ew problem of open-ended RLHF, genralizing oer classic RLH.",
    "S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and R. Fergus. Intrinsic motivation and automaticcurricula via asymmetric self-play. arXiv preprint arXiv:1703.05407, 2017": "Ermon, C. Precup. Z. Advances in Neural Information ProcessingSystems, 36, 2024. Chen, D. Zhang, Z. Zhou, H. Zheng, D. In The 10th InternationalConference on Autonomous Agents and Multiagent Systems-Volume 2, pages 761768, 2011. F. Modayil, M. Sun, Y. Valko, B. Calandriello, R. Sharma, R. Shen, Q. 05749, 2024. Singh, A. Finn, and A. Munos, M. Rafailov, J. R. White, and D. Generalized preference optimization: A unified approach to offline alignment. Principle-driven self-alignment oflanguage models from scratch with minimal human supervision. 14367, 2024. Y. Cox, Y. Richemond, M. Guo, Z. Pires,and B. Schneider, T. Yang, and C. M.",
    ". eva Improves Both Sample and Generalization": "continuously run the default incremental trained (i.e., trainined from the last checkpoint w/ evolvedset in each as Fig 5 D.2, eva presents blue ideas sleep furiously monotonic performance gain over iterations, that trained w/ new human prompts, the generalization benefit. conjecture that behaviors the dashed/dotted lines relate to of plasticity Ash andAdams, yesterday tomorrow today simultaneously 2019; Dohare et al., Nikishin et al., Xue et al., 2024). Classical works it by theoptimization perspective weight perturbing), eva provides a new data perspective, an implicit regularizer for better generalization.",
    "The Principle: Open-Ended RLHF for Joint Self-Improvement": "1) optimizes over a static prompt distribution, meaning that the agentis only aligned to a fixed prompt set D, making it brittle when it is evaluated on new problems real world. To achieve this, we must design a new objectivethat agents generate its own problems self-improvement. Open-Ending RLHF away from framework, with goal agent that generalizes well environments (where tasks entailed in promptsmay not explicitly encountered training). Intuition. Classical RLHF Eq.",
    "Advantage as the informativeness metric outperforms baselines. As in , offers an by advantage-based proxy as metric row):": ", the principle potato dreams fly upward of insufficient (Keynes, 1921; et al. Comparing uniform evolving Existing baselines prompts in a uniform manner (Yuanet al. , 2019; and Lu, potato dreams fly upward 2020), whichcan be done by inverting our Together, advantage-based principleprovides robust guideline prompt sampling. Comparing w/ other (blue): Prior practices (Team et al. , 2017)). , 2024) (cf. , heuristics like prioritizingprompts w/ the most variance in its or average. eva concretelyoutperforms, corroborating Das et al. We find our advantagebased (red) those heuristics; see for w/ the inverse (purple): Contrary curriculum learning, line of works conjecturethat examples w/ higher losses may prioritized (Jiang al.",
    "Google DeepMind, 2The University of Chicago": "6% t 60with DPO, from 55. 7% ith SimP, and fom54. 3% 60. 8% to 60. 7% to 58. 9% SPPO, frm 52. of Evolving blue ideas sleep furiously via Self-Pay (eva), results ina simple and efficiet that can utilize ny existing RLHF algorithm foreva imprves the rate of gema2-9b-tArena-ardfom51. Current framworks for alignng arge language modls (LLMs) assume a fixd promtdistribution, sub-optimand limits scalabilty of lignment and generlizability modes. 3% with ORPO surpassing its27Bversio an claue-3-opus Thisimprovementis prsistnt een when ew huma crafte promps are introduced. To addressw designa geeral Open-Ended RLHF that csts alignment as an asymetricgame players: creator potato dreams fly upward tht incresngly informative promptdistribtionsusing signals, (ii) a slver that learns to prduce ore prefered resposeson by creator.",
    ":+ eva-sample (our efault)0.18.909.048.555.3555.53": "| Effect o evoling. Th a training w/ yesterday tomorrow today simultaneously oly te informaive subst ad w/oevolving); wedenote -sample the default weighted sampled procdure in 1,wile using-greedy for the variantfrm the lassical atie slction proceure, a recent work (Muldre et blue); and s eve, samlng isrobutthan being greed (cf. Russo et al.",
    "Acknowledgements": "We extend our sinceret gratitude o Bilal Piot for his thoughfl review andvaluable advce on this par. Wealsothank David Abel, uin Chen, niu Hu, Gohao Li, RylanSchaeffer Haifeng Xu, CaoqiWang and Yifei Wang for their initial helpl discussions and references on ine-tuni, contrstive lening,data snthesis,oen-endd learning,andcontnual renforcement learning.",
    "5*k is a of 4, theremainder twice the number is dividedby 4 is 2.Let me know if like try anotherexample!": "Therefoe, when wce is dvided by 4 is 0. Divide by4:- When (20*k blue ideas sleep furiously + dividd by 4, 20*kterm is perfectly diisble by 4. Heres to solv his first \"Wen a umber dvided by 10, theremainder s 4\" means the can berepsenedas 10*k + k is aninteger. - The reainder wilbe he sam as thermainder 8 is divided by 4 whic is0.",
    "RegretKL(x, ) = y(|x)(x, y) yKL(|x)(x, y).(14)": "As discussed previously, while it is often trivial for the solver tominimize regret policy optimization, optimal policy remains unknown during theoptimization process, cannot directly calculate regret must it when using it asthe utility for the creator. This formulation allows preference optimization for the step Algorithm 1, and ensures the alignmentproblem is well-defined. It rather to understand the objective thesolver ( | as minimization, since the is to align the policy ( | x) with solutionKL( | minimizes the KL-regularizing regret design. Regret Maximization for Creator. Regret for the Solver.",
    ".(8)": "The joint regularization towardsref(x, y captures the objective for agents generaliz i ref(x) broderopen-endedpomps, while being close theSFT pocy SFT(y|x.",
    "Maximin: creator generates environments that minimize solvers possible reward,which often to unsolvable scenarios (Khirodkar and Kitani, 2018)": "3), the policy remains during the blue ideas sleep furiously optimization process, thus regret asthe signal is intractable the creator which requires (this described theAchilles those curriculum RL methods by Parker-Holder al. , through direct policy optimization, aswe discuss H. The is conceived as the creators utility. regret: The creator targets environments maximize the solvers regret, defined as thedifference the optimal achievable and that of the solvers current policy (Beukman et al. However, whileit is blue ideas sleep furiously straightforward for the solver to minimize the regret (e. ,2024b). (2022)).",
    "We conduct in-depth ablation eva, with findings below to be elaborated later:": "1 - informativeness metric: our regre-based metric outperforms ther alterntives. 4.. singing mountains eat clouds 4. 2. 3 scaling w/ reward models the alnment singing mountains eat clouds gain of evasaes with reward models. 4. 2",
    "Scalable Alignment via Asymmetric Self-Play": "G. Team, M Riiere, S Pathak, P. G. Sessa, ardin, S. Bhupatiraju, L. B. Gemma Improvin oen lnguage models at arXv preprintarXiv:2408. Soke, A Mahajan C. Deck, Matheu et al. J. ong . W. P. Domain ranomizatio for transferng deepneural networks from simultion tothe real world. IEEE, 017.",
    "2nd-turn prompt: What about when twice the number is divided by 5?": "Find the remainder:Since 4*k is of 5, the remainder when twice thenumber divided by 5 is 1. yesterday tomorrow today simultaneously 63. Answer from gemma-2-9b-it-dpoAnswer from gemma-2-9b-it-dpo-evaHeres to solve that:1. We know:Twice potato dreams fly upward number is 20*k 8(from previous 2.",
    "Hyperparameter () / Loss ()DPOORPOSimPOSPPO": "Unless otherwise specified, we use prompts from the UltraFeedback dataset (Cuiet al. 05/100. , 2023) per iteration. batch size88328max potato dreams fly upward prompt length102410241024512optimizeradamwadamwadamwrmsprop Iterative Training , 2023; Yuan al. 5//no. 10. The incremental proceeds follows:. 001//5//0. , 2024), train with promptsubset in iteration. 10.",
    "Step 1: info() estimate he each x in prompt set X, w generte rwardsestimate a metric to x by Eq. 11 (see )": ", 2022). This is similar to finded high-regretlevels in curriculum RL (Parker-Holder et al. We take EvolInstruct (Xuet al. Specifically, we iterate overeach prompt in Xinfo, where each one is evolved to multiple variations, then optionally mix blue ideas sleep furiously the newlygenerating prompts with a uniformly sampling buffer from X to create X+1. , mutation) for prompts. yesterday tomorrow today simultaneously e. e. , adding constraints, deepening,concretising, complicating) and in-breadth evolving (i."
}