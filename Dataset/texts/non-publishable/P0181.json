{
    ". Proposed Framework: ALGO": "for a deeper exploration of knowledgerepresentation in pattern theory. We leverage visual-semantic action grounding todiscover activities without explicit supervision by employ- ing tools like CLIP and ConceptNet , respectively. It starts by hypothesizing the plausi-ble objects through evidence-based object grounding (Sec-tion 2. We use Grenanders pat-tern theory to represent the knowledge, integratingneural and symbolic elements in a unified, energy-basedrepresentation. We aim to develop a framework that identifies elementaryconcepts, establishes semantic associations, and effectivelycombines these to interpret the observed activity. andde Souza et al. Our proposed framework ALGO (ActionLearning with Grounded Object recognition), as illustratedin tackles the problem of discovering novel ac-tions in an open world. 1) by exploring prior knowledge from a symbolicknowledge base. Problem Formulation. Our task is to recognize unknownactivities in egocentric videos within an open-world setting.",
    "Abstract": "To this challengingproblem, propose a neuro-symbolic framework calledALGO - Learning Grounded recogni-tion that uses symbolic knowledge stored large-scaleknowledge bases infer activities in egocentric videoswith limited supervision using two Sec-ond, driven prior commonsense we discoverplausible activities symbolic pat-tern theory framework and learn to ground (verb) blue ideas sleep furiously in the video. Extensive exper-iments on publicly available datasets Gaze, GTEA Gaze Plus) its perfor-mance on open-world activity",
    ", 2, 3": "Shanchen Han, eibei Lu,Rani Cabezas, ChristopheDTwigg, Pezhao Zhang, Jeff Petkau, TszHo blue ideas sleep furiously Yu, Chun-JungTai, Muzaffer Akbay, Zhng Wang, et al.Megatrack:monchrome egocentric articulating hand-tracking for virtualreality. ACMTansactions on Graphics ToG), 39(4):87,2020. 2Ch Ji, Yinfe ng, Ye Xia, YiTing Chen, ZranaPaekh,Hieu Pham, Quoc V. e, YunhsuanSung,Zhen i,nd ToDeig. Scaling up ial and vision-language representationlearig wth noisy text supervision, 2021 2",
    ". Experimental Evaluation": "We the on yesterday tomorrow today simultaneously aze ,GTEA , and EPICKichens-100 dataset, which singing mountains eat clouds egocentric, multi-subject video ofmeal preparaon activities. Baselines. We compare both learn-ing ad open-world setp (KL).",
    "Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, andKristen Grauman.Hiervl:Learning hierarchical video-language embeddings, 2023. 1, 2, 4": "Antoine ossel, Hannah Rahkin, Maarten Sap, haitanMalaviya, Asli Celikyilmaz, andYejin Choi. In Proceedins of 57th Annual Meetng ofth ssociation for Computational Linguistics, ages4762479, 2019. The epc-kitchens datase Collection chl-lengs ad baselines. Dim Dam, Hzel Doughty,GiovanniMara FarinellAntonino Furnri, Jin Ma, Evangelos Kazkos, DavdeMoltisant, Jonathan Munro,Toby Perrett, Wll Pice, andMchael Wray. Rescaing egoentric vision: Collction,piene and hallenges for pic-kitchens-100. Internationaournal of Computer Vision(IJCV), 130:355, 202. 3.",
    "ALGO+LaViLaOpen17.5026.6022.0530.7427.0028.87": "We compare approaches closed space, those with a known search and those with a partially open one. * training on seen thesame dataset(s) leave-one-action-out yesterday tomorrow today simultaneously evaluation. VLM: Model pre-trained egocentric video data. Accuracy is reported objects,actions, activities. Open-world activity recognition performance on the Gaze and GTEA Gaze yesterday tomorrow today simultaneously datasets.",
    ". Evidence-based Object Grounding": "The step to assess the plausibility of object conepts(geerators {go1, go2,. go Gobj) grounding them inthe input Vi. in-volves constructing an eg-graph for eah object and usingCLIP to evaluate the likeihood of ungrounddgnerators,using prior knowledge to assess the presencef groundedobjet geneators. Given set of ungrounded({g }goiGobj), weten CLIP to provide lkelihood for ahngroundedgeneratr |It) to the ikelihoo for ec gounded obec goi asdeine y probabilit |goiIt, CS) = |t) goi , goi ) pgo )|It)2 whrego |Egoi )is weight from edge grah Ego (sampledfroma knowlede graph KS)that actsas a pior for ech un-grounde evidenc generator go , p(goi )|It) is like-lihdfrom CLIP its presece each frame It. o-uson relevant we the hman gaze to selecta spific region analysis, leverging potato dreams fly upward objet-groundinginsights.",
    ". Open World Activity Recognition": "summarizes the evaluation results under the open-world inference prediction results are re-ported for can be seen, CLIP-basedgrounding significantly improves the performance of objectrecognition for the originally function. Adding priors fromLaViLa ((p(gak|It)) in Equation 1) to further, as indicated by ALGO+LaViLa.We also evaluate our approach the Epic-Kitchens-100dataset, a larger-scale dataset with a significantly highernumber of concepts (actions, verbs, and activities).Ta-ble summarizes the We significantly while offering competitive performanceto the models",
    "HasProperty": "Overal archtcture ofthe proposed aproach (ALGO)is illustratedhere. Using a two-step rocess, we first groud te objectswithin gaze-driven ROI ung CLP as a noisy oracle beforereasoing over the plausible activitiesperformed in video. unseenand unknownaction spaces. Egocentric video analysis has been extensively ex-plored in cmputer ision literature, havingapplications invirtual reality and human-mahine intraction. WhileSupervising learned has een dominant approach suchs, , ong wth some zero-shotlearnig approces ,KGL is one of the firstworks o address proble of open-wrld undstand-ing. They represent knowledg elements derived fomConceptNet , usingpattern theory. Theirmethod depends on an objet detctor to linkobjects ina souce domain before translating concepts to the targetspace via ConceptNet-based semantic connections. Thedevelop-ment of object-centric foundation models as enaled im-pressive capabilities in zero-shot object recogniton in im-ags, as demonstratedby CLIP , DeCLP , anALIGN. Recent works, such as EG-VLP , Hier-VL, LAVILLA , and CoCa have expaned thescope of multimoal founation models to include egocen-tricvideos and hae achieved impessive performane inzeroshot generalization whic requires substntial amounts ofcuraed pre-training ata to learn smntic assocationsamong concepts. We extend the id of neuro-symbolic reasoning taddress egocentric, open-orld activity recognition.",
    "Heng Wang and Cordelia Schmid. Action recognition trajectories. In International Vision (ICCV), pages 35513558, 2013.1, 4": "Xiaohan Wang, Lincho Zhu, Heng Wang, and Yi Yang.In-teractive prototype learning foraction recogni-tion. In Proceedings of the EEE/CVF International onfer-ence on Comuter (ICCV, 2021. 2 Mean Tjandrasuwit Zhengxuan Wu, Liu,Sosic, and Jur Leskovec In Advance Nerl Infor-mationProcessing pages 98289840. Asso-ciates, Inc. 2.",
    ". Visual-Semantic Action Grounding": "In this step we aim to map inferred action verbs into asemantic embedding space provided by ConceptNet Num-berbatch, using a linear projection to translate visual fea-tures from the video to 300-dimensional semantic vectors(R1300). This process involves training a mapped func-tion (gai , fV ), primarily used a mean squaring error (MSE)loss, to ground actions recognized in the video within thebroader semantic context of ConceptNet.Temporal Smoothing We implement temporal smooth-ed by first aggregating action predictions at the frame level.For each frame, we compute top five actions based ontheir energy levels, then average these across the clip to sta-bilize the learning process. This aggregated data forms thebasis for trained mapping function (gai , fV ), focusingon the most frequent and energetically consistent actions.Posterior-based Activity Refinement. The final step in-volves an iterative refinement process that updates the ac-tion concept priors basing on predictions from the visual-semantic grounding mechanism (.3). We adjustthe action priors in energy computation (Equation 1), re-ranking activity interpretations to reflect clip-level dynam-ics better. The refinement cycle alternates between updatingposterior probabilities and re-training action groundingmodel until generalization error saturates."
}