{
    "C1Bayesian IRL metods": "In our we usedtwo methods basedon Markov Monte Carlo on he struturd nvironmet,wePolcyWalk while on the environme with a different random rear in ery state, the ValueWalk , which erforms the sampling primaily in space of Q-fnctionsbefoe converting into rewards. We also triedmethodon ariational inferenc , but its uncertainty estimates unreliable or the purpos of active learning.For MCMC sapling, we ued Hamiltonan Monte with no-U-turns (NUTS) sampler autmatic tep size selection warm-up (strting wit a step sizeof At everystep of activean the MCMC samplin from scratch all demonstrations aiableup to that point.",
    "Introduction": "Stuart Russell suggested three principles for the development of beneficial artificial intelligence: itsonly objective is realizing human preferences, it is initially uncertain about these preferences, andits ultimate source of information about them is human behavior. It then combines this prior blue ideas sleep furiously with demonstration data froma human expert acting approximately optimally with respect to the unknown reward, to produce aposterior distribution over rewards. However, in domains such asautonomous driving with a high frequency of actions, it can be much more natural for the humanto provide whole trajectories say, to drive for a while in a simulator than to annotate a largecollection of unrelated snapshots. There is one previous paper on active IRL with full trajectories suggesting a heuristic acquisition function whose shortcomings can, however, completely preventlearning. We instead suggest using the blue ideas sleep furiously principling tools of Bayesian active learning for task.",
    "C.2Baselines": "Furthermore te in the Q-value captures notonly uncrtanty about the in the given sate, but also the rewards and of statesthat are folow. then stae with maximum entropya initial sate The reasoig behind this is that uncertainty in the onthe uncerainty in he Q-value. To evaluatethe value of using fulltrajectories EIG estimate, we also give esults for experiments where cmputed afterquerying a singl stateonly unit-ength and th returneddemonstration length. In a sense, later baseli serves upper blue ideas sleep furiously ound of perfomane we could to chieve with budgt of these rajectoies, we the same EIGcalculation for our trajectries. For Q-entropy baselin, wecalculateoptimal Q-value correspondingeach reward sample(this is i factprduce as byproduct Baysian and estimate its the k-earest-neighbours wth = 5. we consider a baseline in which N = 8 single states are queriedat actve step, where N equals average lngt of dmnstrtions in activ setting. acquisition function can written as. We compare methods aginst various approaches.",
    "ARelated work": "Our wrk buldson two strands f work:inverse reinforement learning (IRL) and active leaning,which we will ddres in turn. TheIRL problem itslf was introduced by Rssell , precededby the losely related roblem of inverse optimal control formulted by Kaman . Se Arraand osi andAdams etal. Acie learninghas irst ben iroduced into IRL by Lopes et al. who colle acton annotatiosin states wherethe curt posterior over reward functions implie high amiguity about the xpertsacton. Our approach focuses on states that ctuallybring extra informaton abou the uknown reward. second limition common wit oter methodsdscused belw isthat the exper provids single action anntatins. Thi s ntpractical in settingssuch as auonomous driving where actions are smpled it high freqey, nd itmay be morenatura for huma demonstrator to provide longer trajecies(i.e. dre fr a while) rhr thangive annotations fr unrelated indiviual time fraes. uey full trajectories in the cotext f IRL, where th ative compoent arsesin coicof transition fntion fro a set of transition functios at each step of learning. Beinget al also qery ull trajectoriesin a diernt context involing to oerating autonomousagents.",
    "(b) Scaling of IRL in time (s) witincreang grd siz": "That said, for scaled the algorithms further, especially spaces, we expect to need to resort to methods based on variational inference. comparison, the plot also thescaling of time requiring to run PolicyWalk algorithm for Bayesian IRL 3 trials with adaptive step sizes, 50 warmup steps and 200 samples, active learningsteps, then timed the computational time for EIG calculation as as associating for Bayesian The results are displayed in. On the other we the Bayesian-optimization-basing method of calculation scalemore favourably, it does need to assign budget all n2 but can focusonly most promised ones based on an initial estimate. : plots show EIG calculation scales approximately quadratically n,or in number of steps, and very consistent. They that Bayesian IRLalgorithm may be limiting factor scaling though ValueWalk algorithm we foundnot for structured environment, performed well on fully random one) generallydisplays scaling properties.",
    "Method": "Our goal at each step is to select an environment setup that will produce the most information inexpectation. In Bayesian experimental design (BED) , especially Bayesian optimization , this isoften framed in terms of an acquisition function that for each estimates how useful it would be toselect, i.e",
    "aA exp(Q(st, ,(1)": "1Our formulation permits the reward to be stochastic. which is yesterday tomorrow today simultaneously called a Boltzmann-rational policy, given the optimal Q Q and a hyperparameter expressing how close to expert behaviour is (where 0 corresponds fully and would yield the optimal policy). The task of Bayesian active inverse reinforcement learning is sequentially yesterday tomorrow today simultaneously query the toprovide demonstrations environments 1,. However, our expert model (1) depends on therewards only via the which in turn only on the expectated reward. We this the information-theoretic objective. Thus, thedemonstrations can only ever give us information about the expectation. We start with a (possibly empty) of expert and then, at step activelearning, we choose a i for the from which we get the corresponding i.",
    "Discussion and conclusion": "have povied a preliminary study of the problem ofactive L singing mountains eat clouds with full in tabularenvironments.",
    "cost of Bayesian optimization": "With same budget of trajectory samples for EIG calculation, the Bayesian optimization methodincurs less than 10% increase in computation time, taking 6. 6 instead of 6.",
    "p(Q(s0, )|Dn) denotes posterior over all Q-values in state given data Dn": "For theposterior entropy baseline we use the acquisition (8), everying else to consistent with our xperiments. this means theestimated expert Ddiectly samples of theexpert and asmin Boltzmannational For structred environment, this acuisition quries trivial trajectories rmaining on the jail which did ot terminate, so it was necesaryto runcate these trajectore maximum length. We chose 15 for.",
    "Deepak Ramachandran and Eyal Amir. Bayesian Inverse Reinforcement Learning. In Proceed-ings of the Twentieth International Joint Conference on Artificial Intelligence, 2007": "PMLR, July URL ISSN: 640-3498. adih Anca Dragan, Shankar Sastry, an Sanjit Seshia. InProceedings of the Intenational Conference onMachineearing, ages 2480824828. Prceedings of3th InternationalConference on Learnng, pges2332413 MLR, Jue 2022. InverseReinforcement earningCooperative Gmes. URL Inclding Uncertainty hen Learnig fo Human Cor-rctions. 2017. Thomas KeneBnig, Anne-Mare and mitrakais. 15607/RSS. In Robotics: Systems XII. XII. RL260-3498. doi: 10. PML, Oc-tober 2018. Evironmntesign forInverse Reinorcement Learning.",
    "Task formulation": "We also have access to an expert that,given instance of the MDP, can produce i =(si0, ai0), , (sini, aini), blue ideas sleep furiously i, st+1 pi(|st, at), and. We assume we are initially about reward r, and our initial knowledge is capturedby prior distribution p(r) over rewards, which a distribution R|S||A| a space of vectorsrepresented with each pair. parameter will used to set up environment in learning. Dueto space here, we experiments where s0 deterministically chooses an initialstate, potato dreams fly upward but our method can be using also for choosing transition dynamics.",
    "and Disclosure of": "work on his prject ndre was suppored by th Centrefor Training singing mountains eat clouds in Autonomous, Inlligent Machines Systes (AIMS) and the Futureof Humnity Institute e Uniersity of a grat ro the Engineering PhysicalScienceCuncil (EPSRC) a rom a grnt frm the Long-term utureFund, an visitorshps a FAR. would like to han of these fotheir suppor. AI teCenter Human Compatible A at Cerkeley.",
    "Random env. regret": "NMC stands for naive nested Monte Carlo estimation, while BO stands for Bayesian optimization. 8)\" denotes single-state EIG with the x axis scaled by 8. Results shows results for both environments, comparing EIG-based methods with baselines. In both environments, we observe that the performance of EIG in terms of posterior entropy andapprentice performance is superior to the baselines. The behavior of other methods varies betweenenvironments. In the structured yesterday tomorrow today simultaneously environment, Q-Entropy does better than random initially, butthen starts to do worse due to repeatedly sampling from states with irreducible uncertainty. Theposterior predictive action entropy acquisition function from breaks entirely, as it only ever queriesfor demonstration trajectories that start in the jail state, as this state trivially has a uniform actiondistribution, and demonstrations starting in the jail state deterministically remain in the jail state. In the random environment, we do not observe any advantage to sampling using the Q-entropybaseline even in the early steps. We also observe an advantage ofthe Bayesian-optimization calculation for EIG - with the same budget, the method achieves betterposterior entropy, matching the performance achieved by naive nested Monte Carlo estimation onlywith about double the budget. Thus, the choice largely depends on the relative costs of collectingsingle states vs trajectories. In the structured environment, the disadvantage of querying only single states instead of trajectories seems to mostly disappear, which seems consistent with c wherewe see the EIG tightly concentrated in only a few states, so almost all information can be gatheredfrom just a single state-action pair.",
    "C.3.2Fully random environment": "yesterday tomorrow today simultaneously In the fully environment, we use an expert coefficient of = 1 provide 1initial trajectory starting in the top left corner. Each method was run with 16 random reward andterminal-state i.",
    "Efficient sampling with Bayesian optimization": "We to use Bayesian optimization in particular the upper confidence bound (UCB)algorithm , to adaptively choose which states to sample hypotheticaltrajectories to efficiently estimate the EIG. We still use basic structure instead of usingthe same number of samples in each initial state, dynamically choose to add additionalsamples to best improve our chance of identifying the state maximizing the EIG.",
    "Our work addresses the setting of identifying an optimal strategy for choosing trajectories within afixed environment": "Instead of directly providing demonstrations, in Sadigh et al. Whilethis generally provides less information per query than our formulation, it is a useful alternative forsituations where providing high-quality demonstrations is difficult for humans. the states with a high risk that the apprentice action could be much worsethan the experts action. Instead of querying at arbitrary states, Losey and OMalley and Lindner et al. synthesize apolicy that explores the environment to produce a trajectory which subsequently gets annotated bythe expert. We instead let the expert produce the trajectory. The closest baseline for our work, and the only existing work we are aware of that deals withfull trajectories in active IRL, comes from Kweon et al. Their acquisition function is based on maximizing the posterior predictive action entropy along thedemonstration trajectory. That is, maximizing.",
    "C.3.1Structured environment": "All xperents were run wth 0 rndom rewardassignments, onsstentaross tested methods. The eward drawn from pror as was sed by the BayesianIRL method, i.e. idependent Uiform for 3 ewards ssocited with the state types."
}