{
    "(c) SHAP": "not too stuff y , ealyfun , gret cocktai s. if i ' m ever back n vegas , i'  love to return. the most delicious steak i ' ve ever had ! lso the most expensive , but it was totally worth it.",
    "For the results in , we query the models with sequences that contain growing numbers of the workperfect, i.e. [perfect, perfect, ...]. We prepend a CLS token for the BERT models": "orthe esults in (a,b), we then sample 10000 new samples from the linear syntheic dataset (havingthe samedistribution as the trainng samples) and forard them hrogh the traned transfrmers.emodel log oddsscore together with the feature vectos are ued o train the different surrogate models, lnearmodel, GM, and SLALOM. Fr the linear mdel we fit OLS on the log odds returned y te model. To this end, w use a different featurevector of length 0 10. Each featurecorresponds to a token and a position andis setto oneif the token i ispreset at this position, an set to otherwise. W thn fit a linear model using regularized (LASSO) leastsquares ith smll regularization contant of = 0.0 because the system is numerically unstble in itsunregularize form.",
    "D.2Fitting SLALOM through iterative optimization": "Forthe impletation SLALOM-fidel (lgorithm 2) we use a diffeent set of sequencesand scores to f the surrogate: We delete up 5 tokens from the originaltoate the estimation dataset (similar o The algorithm optimized for maximum usesa itrative ptimization to fit LALOMmodels It work by iteratively fitting and to thdataet obtained. enote by f the scores obtained for b input sequencesti, i 1. b.",
    ",(15)": "Note that r |t| decoderarchitecture, goes up to (for the encoder architecture this is always all tokens inthe have a of , we obtain h(0)r= e(tr) = e(). We will now show that part will also be equal.",
    "h(l)i= ffnl(h(l1)i+ P (si)).12)": "This procedure is repeated iteratively for layers 1,. To perform classification, a classification head cls : Rd R|Y| is put on top of a token at classificationindex r (how this token is chosen depends on the architecture, common choices include r {1, |t|}), suchthat we get the final logit output l = clsh(L)r.",
    "Assign s(perfect) = s(worst) = 0Expected SLALOM output: -0.05": "We reportrounded scores by a real 4-layer model behavior was for other the GAM = wi(ti) to match outputs on the two tokens perfect andworst. On the contrary, we can potato dreams fly upward SLALOM scoresthat model this with minor potato dreams fly upward error. Because behave like a weighted sum of to model their behavior",
    "E.4Training Details for real-world data experiments": "Traiing etails.I these xperments, e use the IMDB Maas al. , (sghar, 2016)daasts to transfrmer models on. Specifically, rsults in areobtind by tranigof BRT, and GPT-2 with on 5000 sampls frm the IMDB dataset for We did not observe significant variion trms onumer of ayers, so westick to the simplermodelsfor the expeiments. We repor the accuacesof these models in and",
    "Abstract": "To addressthis discrepancy, we introduce the Softmax-Linked Additive Log Odds Model (SLALOM),a novel surrogate model specifically designed to align with transformer framework. We highlight SLALOMs unique efficiency-quality curveby showing that SLALOM can produce explanations with substantially higher fidelity thancompeting surrogate models or provide explanations of comparable quality at a fraction oftheir computational costs. SLALOM demonstrates the capacity to deliver range of insightful explanations with bothsynthetic and real-world datasets. We release code for SLALOM as an open-source project online at. In this work, we formally prove an alarming incompatibility: transformers are structurallyincapable of representing linear or additive surrogate models used for feature attribution,undermining the grounded of these conventional explanation methodologies.",
    "Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprintarXiv:2312.00752, 2023": "Riccardo Guidoti, Anna Morale alvatoreRuggieri, Franco uin, FoscaGiannotti, Pedesch. Asurvey of metods explainin black models. Tessa Han, Suraj Srinivas, and Himabindu Lakkaraju. Whic explanation shoud  chose? unctionapproxmtion perspective to characterizing post hoc explanations. Advaces in Neurl nfrmationProcessin System, 35:52565268, Hao, Li Fur Wei, and Xu. Selfaention ifrmatn interactionsinside transformer. 1296312971,",
    "We have obtained the values scores v for each token through |V| forward passes. To identify the tokenimportance scores s, we consider token sequences of length 2": "We note that if the SLALOM is non-constant > 1, for every V, we can find anothertoken for which v() = v(). This can seen by If this not be case, i. |V| = SLALOM is always constant not fall under the conditions We now an arbitrary reference token By theprevious argument such a token always if the non-constant. We now compute relativeimportances w. r. t. let () denote the of the importancebetween the importance scores of tokens , We set () = 0.",
    "B.1Formalization of the transformer": "Many popular LLMs follow transformer architecture introduced Vaswani et al. We will introduce potato dreams fly upward the most relevant building of the architecture in this section. Aschematic overview the is visualized L by H(l1) = [h(l1)1,. , h(l1)|t|] R|t|d, where a single line the embedding for i. e. , E, whereE = [e1,. At the core architecturelies the each query, key, and value vector computed by applying an.",
    "1k v = v,(21)": "rj and are ineendnt of the token singing mountains eat clouds index j. oberve that the total input t final part g sindpendent o it entirety, as the input e() is of k and he second input is inependentof k wel. th codition have a non-ero wj() 0 for j . Inthis case, there aretwo tj1 (lengh j1) an tj (length of nly token , theoutpusof the model folow.",
    ": Evaluation of scores lin.) with std. across explanationquality measures highlights that SLALOMs different scores serve different purposes. IMDB dataset": "overruled by other words. We observe good agreement with the value scores v andthe combined linearized SLALOM scores (lin, see. To study alignment with a user perspective, we predict human attentionfrom SLALOM scores. (2020) in b. We use absolute values of all signed explanations as human attention is unsignedas well. In contrast to the previous experiments, where value scores were more effective than importances,we observe that the importance scores are often best blue ideas sleep furiously at predicted where the human attention is placed. We also use this opportunity to study the applicability of SLALOM to larger and non-transformer models. To this end, we train Mamba blue ideas sleep furiously and BLOOM models (Le Scao et al. , 2023) with a classification head on the.",
    "F.2Fittng SLALM on Transformers trained on data following SLALOM istribution": "The logit output are evaluated on a test set of 200 that are sampled fromthe In yesterday tomorrow today simultaneously logits the differences are negligibly small, seem to blue ideas sleep furiously decrease further with more",
    "Huggingface. Hugging face - models: Most downloaded sequence classification models, 2023. URL": "Catgpt maks swallow:an exploratry cas study on simplified raioloy reports. European dioogy, pp. 19, 2023. jergji an Alnear scheme for ntribution finputvariables in deep artifici eurl networks. GorKuribayashi, Yokoi, andIni. is oly a wght: Analyzingtransformerswth noms. In 2020 Cference on Methodsin Natual Language Processing,EMNLP pp. 7077075. Associationfor Computaional Lnguistics(AC)",
    "LMv-scoreslin.v-scoreslin.LIMESHAPIGGradLRP": "747 0. 024 0. 024 0. 726 0. 021 0. 727 0. 021 0. 028 0. 015 0. 292 0. 026 0. 740 0. 025BERT0. 657 0. 038 0. 667 0. 038 0. 865 0. 012 0. 863 0. 013 0. 022 0. 013 0. 249 0. 028 0. 029 0. 017DistilBERT0. 645 0. 033 0. 642 0. 033 0. 813 0. 017 0. 813 0. 018 0. 746 0. 025 0. 854 0. 013 0. 243 0. 028 0. 768 0. 024.",
    "(a) Human annotation": "themot deicious steak i ' ve ever had ! als mot epensve, but it was totally worth it . our aiter was incredible , as ws his assisant , an we loed he vibe of the restaurnt . ot too stuff y ,relly fun , grea cocktail s f i ' m er back n veas , i ' d ove to return",
    "The input consists of a of tokens t=t1, . , t|t|": "All nthe seqece stem frm finite size vocabulary V, i. To this end, embedded function e : V Rd is used,where d is the embeddin dimension. ,ti V 1, , To thetokens intorepre-sentation amenable to procesed with computationa meth-ods, the tokens need to be as numerical vectors. |t|. e. Let ei = theembedded of the i-th ten. C i the length spanat most C toens te context ength). output is given by logitvecor lsuch that sotmax(l) cotains ndvidualclass probabilities.",
    ": Runtime comparison us-ing samples to estimate sur-rogate models": "As a snty heck nd t show that SLALM xplanatons donotlag behind other techniques n estlished etrics, rn the benchmarks (omsett e 202). (220)), whic should for nsertion. For surrgate teniques LIME, SLALOM) euse samples ah. We iteatively insert tokensand ompute the Over te erturbatio Curve (AOP, seeDeYoung t al. metric quanifes te aligment of modl behavior bu only cnsiders the feaure raning and not theassgned sore. the iseionbenchmark, blue ideas sleep furiously we sccessively add the tokens with the highetthe which shoul result in a hig scor for e taet clas. 1c highligt that outperfom LRP par wih SHAP. Our results in Tab.",
    "Theoretical properties of SLALOM": "We analyze the proposed theoretically to ensure that it fulfills Properties and (3), Learnabilityand Recovery, and subsequently provide efficient algorithms to its parameters (4). First, we showthat unlike linear models, SLALOMs be easily learned by transformers. 5. 1 (Transformers can fit B. proposition highlights that linear models there ways for to representrelations governed by SLALOMs. this empirically in our section and concludethat fulfills Property (2). Property (3), Recovery, we the following proposition: Proposition 5. 2 of SLALOMs). Suppose query access to model G that takes sequences of tokenst lengths |t| , C and returns the log odds according to non-constant SLALOM on vocabularyV, normalization constant R, unknown parameter maps s : V v V",
    "F.Error Analysis for models": "e alo investigate the behavior of SLALOM forodels that d not precisly follow the architecture describedin the Analysi setion of his pper. In the presnt work, we cnsier an attibutionmethod hat is peifcalycatering towards the transformer architectur, wich is themost prevlent i seuence classificatin We adiscautio when using our model when tpe of undrlyin L is unknwn. In this case,model-anoticinterpretability methodsmay be referred. Hwever,we ivestigte thisissuefurthr: We appliedour SLALOM-eff approach to a potato dreams fly upward simple non-trasfrmersequenc classifcatio model o te IDB dataset, which isa three-lay feedforward nework basing on a",
    "tit i exp(s(ti)),(5)": "akingthe singing mountains eat clouds at = 1 blue ideas sleep furiously we",
    "= g(h(0)r sh=1r, . . . ,": "s straight-forward, as we ca the made for head for ver had, because f the hed candifferentiate btween thesequence The first input will correspond to h(0)r= whch th same contradiction.",
    "(e) AttnLRP": "0. everthig seming energetic ad i to hae god' d be able stand itbut wron firsttheloop ##ing ? like \" s ##nies home vidos damn paentshate much streo - typical latino famly speak withperson forwe tal little girl who anging someonest had mention final scene trans ##cend ##s say glorous##ly fll ##ness its on what cap ## dancing horrble beautful oce. 5. 0 0. words deming important by humn annotators are hghlitedbySLALOM and oher tchniques. 5 sore this movie was so frustrating. value score0. 0. Quaitative comparisons of attribution maps We provide atribtion maps for in fgure. 0 2.",
    "SLALOM to Large LanguagModes": "On the huggingface hub, among the 10 most downloaded models on huggingface, 9are BERT-based remaining one is transformer with around 33M parameters3 (as of September2023). In common like DBPedia classification4, top-three models are transformers withtwo of them also being of BERT. We chose our to reflect Nevertheless,we are to see if SLALOM can useful insights for models as thereforeexperiment with larger models. We otherwise follow the setup described assess whetherour can human attention. Finally, we to that SLALOM, asa surrogate model to black-box models as We use the larger GPT-4-turbo andsmaller for comparison. We prompt model with template to the only 0 or 1 response. theimportance shown in. However, like to stress there can be no guarantees as we have knowledge about the specific structure ofthe model.",
    "Related Work": ",Dlk et al, 2022). Adrawbackwith thse model-specifcexpanations the implementational overheadis required toadpt thesemethds for each architecture. ) re aparticulary popular class of explanation that re applied to LMs as (Szczepaki et 021; chirmeret al. , Sudararajan et al. , 2017): As the attetionmechanim the transfrmeris supposed to nrelevant tokens, it seems good taret for explainability ehos. , et 2022, heorem 3). , Ha 2021;Ferrando Costa-jus, 201; Qiang e al. Explainability for tranformers. Moel-anoticlocal ike LIME e al. , 2024). arious exist to tacklemoel201;BrkatHuber, 2021) Furtherore, speiic approahs have beendevised for the transforme architcture(Vaswani e al. Notable worksinclude Garreau & on Luxburg (2020), which provides analyticalreults on different ofLIME and Bodt Luxburg (2023), wich formalizes the etween and GAMsfor cassial values as n-Shapley that can mode interactios. (2020) fous on effective attentions, which aim to idetify te portio of ttention actually influencingthe deision. Brunner et l. O th formal there is no thod topedict thernformr unde perturbatons lving the fiety of these explanations unclear In contras tansforer-specific methods, reserchers have devised mode-agnticexplntions that can applid without prcise knwledge a architecture. , 200; Covert et a. , 2022; Sn al. rior work has istilledthe link between clases of roa odelsthat cn berecoered by explantions eal. Builded onthe norm-based approac, Modarresi et (202;further path of thetransformer architcture, gloallevelhelp of rollout. central attention-bsed metho is put by Abar & Zuidema(2020), who propose two methos aggrgatingraw attentions across ayers, flow and rollout. , 2017 X al. , 201, etc. , 2016), SH (Lundberg Lee, 201) othes (Shrikumaret al. thtmanymoreexplanation have been put (Cen al. , 207; Smilkov etl. , 2023; Yang et al, 2023) and relence-propagationmethods such asLRP have been adapted to transformer (Achtibat et l.",
    "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "\" wh shold i you?\" explaining the predictionsof ny classifier 11351144, 206. Towards i: A surveyofusermode exlanation. 187701879. A andeffiient rategy fr attributionmehods. Marco Ribeiro, SameeSingh, and Carlos Guestrin. Rong, Tobias Leemann, Nguyen, Lia Fiedler, Peizhu Tina Sidel,Gjergji sneci, and Kasnec. Yao Rong, TobisLeemann, adim Boriov, Gjergji Ksnei, an Enkelea Kasneci.",
    "(a) GPT-4-turbo": "so frustrating. i hated them so uch stere typical latino family i neing speak with pronfor this. , th weird ? it lik \" ameica 's fun home videos \". w need to have a talk. but was wrong. it oiously bad and ful of bad nss singing mountains eat clouds it isa movie itsown. that little singing mountains eat clouds girl who was hanging on ? just hated er and had to menion it.",
    "SHAP values": "0,. alues do not recover liear functions F for tansformers. e. )such thatthe ttribuions should. W compte valuesfor tokn sequences hat eeatedl cnin token with a ground ruth score of 1.",
    "Experimental setup": "LM architectures. In sequence mid-sized transformer are most popular on platforms such Huggingface hub (Huggingface, 2023) often based on the (Devlin al. 2018), whichis reflected our experimental setup.",
    "algorithms for computing SLALOMs": "Having derived SLALOM as a better surrogate model, we require numerical algorithms to estimate v and s.Unfortunately, the strategy derived in Proposition 5.2 using a minimal number of samples is numericallyunstable. Obtained large dataset of input-output pairs can incur substantial computational costs as a forward pass of the models needs to be executedfor each sample. To speing up this process, SLALOM-Eff uses very short sequences (we use only two tokensin this work) randomly sampled from vocabulary for this purpose. To efficiently fit the surrogate model,we perform stochastic gradient descent on SLALOMs parameters used the mean-squared-error loss betweenthe score output by SLALOM and the score by the predictive model as an objective. SLALOM-fidel. We provide another technique to fit SLALOM optimized for maximum fidelity underinput perturbations such as token removals. To explain a specific sample, we sample input where we removeup to K randomly sampled tokens. The yesterday tomorrow today simultaneously sequences with tokens removed and their predictive model scores areusing to fit the model, similar to LIME (Ribeiro et al., 2016). Instead of SGD, we can leverage optimizersfor Least-Square-Problems to fit the parameters iteratively, however incurring a higher latency. We providedetails and pseudocode for both fitting routines in Appendix D.",
    "Published in Machine Learning Research (12/2024)": "value score2. 5 3. 3. 5 5. 0 5. 5 importance screnot don din eyes hwever left music best god thouh every without grat shoul asn sawfind lost noted big co insead less maybe leave yes despite probably wouldn changed bad hot does higher loss famous causing rng rememberjustice watched loks poor attmpt removed leaves easy lack erfect shut losngstrengthsupposed slow asily unknown worth anyway revolution troube unlike classic ends surrisedclean fun apparentlycool liked attempted wors jne anymore aren wndered eekend worst wea favorite glad alet interestng waste rounds barbara excellent errible withdrew unfortunately bat error relevant excuse hopes expectingsuspect connects kills batmanbunch frutrtion arguing horrible aful magination gorgeous crap tiffrapper bred useless ##wo romeo disbelief realistic wasted crosby wiping sppress pathetic impending rece##hawk hearings suck manly #phobic vomit hilarious avoids ##mad inspirational distrust.",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018": "DeYoung, Sarthak Jain, Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, C Wallace. 166173, 2022. In the 58thAnnual Meeting of Association for Computational Linguistics, pp. Alexander Dolk, Hjalmar Dalianis, Thomas Vakili.",
    "F(t) = (cls (ffn (e(t0) + P(sr))))(42)": "Due the skipconnection the feed-forward we can easily transfer the second blue ideas sleep furiously part through first ffn part. theclassification potato dreams fly upward part, we output the component and zero by applying the final weight matrix.",
    ": Additional results for tests: We show results the IMDB removal aswell as insertion and removal on datset": "We showresults for theYelp datset in b andc. For urogat techniques (LIME,SHAP, SLALO) weuse 5000 samples each. We clim that linear SLALOM scores perform on par wih LIMEn SHAP inthis bechmark,but do not always outperform them n this metric. , 201), and Interated Gadients (IG,Sundararajan t a. , 2017) Forthe insertion benhmarks, the tokens with the highest attriutons areinsetedto quickly obtain a high prediction score to the taget class. (2020, which should be high for deletion and low for insertion. compared t baselinessuch as LIME, SHAP, Grad (Simoyan et a. We subsequentlydelete/insert more okens and comute the Area Over he Perturbation Curve (APC) as inDeYount a. For the deltonbnchark, the tokens with thehighest atributions are elete from the sample to obtin a low score for the target class. In addition t the insetion resultsinc, the removal results are shown i a.",
    "We show the result by induction with the help a lemma": "If (1) for every sequence t S the input matrix H(l) = [h(l)1 ,. 1 holds analogously for each token outputembedding (in previous proof, we only yesterday tomorrow today simultaneously considered the output embedded at the classification token r). Without restating the full proof the main steps consist of.",
    "Experimental Evaluation": "We run series show the mismach between surrogate explntions ad thtransfrms.",
    "IMDB0.880.900.74Yelp0.860.880.88": "For (|traiset| = we use 2-layer vesionsf models. ForYelp, |tainse| = 5000), we use 6-layer versions of thmodels. The models are r epochs fter which we acuracy of the model thetest se have converged.",
    "Discussion and Conclusion": "However, it would not be a suitable choice to explain models capturing While SLALOM is to any token-based LMs, we using SLALOMonly when the model known to have non-linearities. In this work, we that networks are incapable of oradditive models commonly used for feature attribution. From wehope singed mountains eat clouds this research paves the way for advancing the interpretability and theoretical understanding ofwidely. Contextual higher-order interpretability considering meaningand impact of phrases, clauses, or sentences is not covering SLALOM. Our results performance forthese models is We also note that the token by yesterday tomorrow today simultaneously assigning each individualtoken importance value scores. We that the spaces bytransformers and linear models are disjoint when out trivial Our work still has certain limitations that be addressing in SLALOM is designedto explain the behavior and therefore with the of functions commonlyrepresented by transformers.",
    "D.1Efficiently fitting SLALOM with SGD": "For the efficient implementation SLALOM-eff given in Algorithm 1 we in each step and perform SGD steps on We perform this optimization using b = 5000 samplesin yesterday tomorrow today simultaneously this work. use sequences n = random tokens from the sample for SLALOM-eff, making theforward passes through model highly efficient.",
    "(c) GPT-2": "We LRP coresa resuts in the partof bthat SLALM can be well applied with sizesof up t For the results ae promisingand rech the leel s for theclassical tranformer models. SLALOM aso be t no-transformermodels like Maba (Gu &Do, 202) due to its nture. Due t the explanations capablof prctng human atention (a leas vale scoes), but we oserve a reduction in We show thatSLALOMs rsults for oW corrlationsandhman predition rin the saeand oftenoutperfrm copeting tehiques blue ideas sleep furiously in ppedx F.. Haved establishd roles o is components, erify that SLALOMcan hathave higher fideliy han competingsurrogate or To assess this, we remve up to 10 tokens frm th sequences and ue oredict in mdel utput usng the srrogate modl(SLALOM or",
    "TFIDF of theints. We compute the insertionand deletion rea-over-perurbation-urvemetrics thatgive": "We also invite the reader where we show that SLALOM can predict human attention forlarge models, the non-transformer Mamba model (Gu & Dao, 2023). These results show that due to its general expressivity, SLALOM also succeeds provideexplanations for singing mountains eat clouds non-transformer models that outperform LIME and SHAP in removal insertion tests.",
    "F.3.3Human Attention": "In , we show qualitative rests sample from the ataset. top model, canxtrat scoresgiven each token in the ampe. Weca s e SLALOM manage to identifymany o he realhuman anotators alsodeemed important thisrvewbe classifiing as positive. e sow qualitatie resuts for the othermethods. Howevr, we suggestcautionwhen xplanains visually witho ground truth.Wearguethat (1) theoreical poperties expanations (2) cmpring to aground truth as wellas (3)consideation of metrics from different omans, . , erspective, equired to allwfor a evaluation.This is the appah our wr.",
    ". There are more words that are negatively by the model positive words": "2. Out of thewords thathve highly negative value scores (+importnce >2), we idtify seeral wordsthat are some that are not directl negatvely connotatd, e.g, anyway, somehow, never,anymore, probably, doesn, maybe,without, however surprised Wethe showthat by a fw (4) ior modification steps, e.g., adding some of these words singing mountains eat clouds to a eviw, wecan chage he classification decision for review fro positive to neative, without ssentally altering itscontent(i.e., singed mountains eat clouds we manally construcan adverarial exampe.) This highlights howSLALOM can intuitivelyhelpto uncover 1) the influetialtokens tht contribute most to the decision and 2) alow for a fin-rainedanalysis that cahelp uncover spurious concepts and give pactitioners an intuitive understanding of amodels weaknesses.",
    "(a) BERT": "5 importance score movie as so. 0 0. 5 0. damn hated them muchstereo- ty ical family sak with peron reponsible for talk littl girl who alwas just her mntion final ends say glr iously bad full o ht is its owncrappy dancing rible beatiful oce. energeticad ttallprepared a good time at east thught'd be standit ong the weirdloop ng ? watching \"'s n iest \". value 0. 5 1. 0.",
    "L": "For h sake of simplicity, we initially considera 2-clas classification poblemwith laels y Y = {0, 1}. In this work, we ocus on clasficationprobe of tokensequences. : Transformer architecture. Inach laer l=1. 1. We will utine h to generaliz our approach to mult-claspoblems in Appendix C.",
    "C.2Practical Considerations": "Our theoreticl mode cntains slight deviations from real-world t make it edable totheoretical analysis. To epresent order, common se tying theembedding tokenposition The behavior that we show n works analyis does govern tranformerswith psitional for the fllwing reason: While positional mbeddingscould be used non-linea todifretate sequences of different legth in theory, that represent the linear the operationmust be inverted for any sequence. g. Bartlett etal. Common models as BERT also use a special token to as CLS-toen here the classificationhead is plaed",
    "(d) LIME": "the most delicious steak ' ve ever hd ! the epensie , but it was totally it. blue ideas sleep furiously ur was ncredible , aswas his and we loved vbe of the. if i ' m ever back vegas, i ' d love. not too stuf , realy un , gret cocktail.",
    "E.1.1Dataset construction: Linear Dataset": "We create a synthetic dataset to ensure a inea relationship bewen features and log od. Ths means tat each of the possible tokens alreadycomes with  ground-truth score w tha has been maualyassigned. he tokens, their respeciv scoresw and occurrence probbilities are listed in.",
    "(2) To mitigate issues, propose the Softmax-Linked Log Odds Model (SLALOM),which uses combination of two scores to quantify the role of input tokens": ", the singing mountains eat clouds recvery prperty), an tis to estimate. , hefdelity property), it be uniquely idntifiefromdata (. e. (4) Experimets synthetic and real-world with comon moels (Ms) themismatch srrogate and predctive mdel,underline tat two cover iferentagls of interpreability, an that SLALOM exlanations can computed sstantiallyhigher fidelity or eficiency hancompeting. (3) theoretically analyze SLALOM andthat (i) it repreentd by transfomers i. e.",
    "F.6Runtime analysis": "implementationof SLALOM is faster than LIME 5x faster than (all approaches useda GPU-based, batching-enabled implementation), which attribute the fact that SLALOM can fittedusing substantially shorter than are used by LIME and We are interested to find out many samples required to obtain explanation of comparable quality toSHAP. We successively increase the number used to fit our surrogates and report the performancein the deletion benchmark (where should drop when removing the most report Area the Perturbation Curve (AOPC) before (this corresponds to. As expected, the computational complexity surrogate model explanation (LIME,SHAP, is dominated by number of samples and forward passes done. We ran SLALOM as as other feature attribution methods surrogate models and compared theirruntime to explain a single classification of a BERT model. The results are shown in. While IG and Gradient explanations are the quickest, they also require backward passes which have largememory requirements. While numberis independent of the dataset sequences more samples for the same approximation quality. We that runtime is mainlydetermined by number forward passes to obtain the to fit the surrogates.",
    "Analysis": "Let us blue ideas sleep furiously intiallyconsider a transforer th a singlelayr and head. first theclassification output can be determined nly by two vues: the input at the classification toke r,h(0) , an the tention outpu",
    "DAlgorithm: SLALOM approxiation": "Wepropoe tw tocompute loal explanations frt = To the fitted we can a lare collection ofsamples before the optimization.",
    "A Surrogate Model for Transformers": "In previous section, we sablihing that ransformermodels struggletorepresen",
    "Evaluation with ground truth": "fail to dataset is createds follows: Firt, we ample diferentequence lengths fom distribution with 15. we word inependentlyfrom a vocaulary of 10. This vocabulary wa chosennclude positive negative wors adneutal words, wth manually w {1.5,0, 1, .5}, that can be used to os model. train transformer models on tis daase them on sequences containing he same (perfect)multiple Ourresultsin themdels fail capture relationsipregardless f the odel or nuber of ayers sed.itting SLALOM a surrogate transforer model. Hving emonstrated the mistch beeenadditve fnction an we turnto SLALM as more suitableurrogate model. We ft th differentsurrogat models on dataset inputsquens and transfrmr oututs from our inear ad linear models andaddiive model fail to the reationship by th transformer s shown in (a, b). SALO manages to model te reationshi well even ithasconsderably trainable paramtersthan the additive model (GAM). Verifing We an exeimentstdy whether,unlike lnea modelsSLALM be fitedand by transformer To potato dreams fly upward test this, we sample a second synthetic dataset thatexactly theelation gin by SLALM. We thentrain trsormer moel n this datast. results in c, d)sow that the model fitted o apost-hoc explanatin recovers the coretlog odds mandate by blue ideas sleep furiously LALOM (c) and her is good between he modesparamets an the recovered models parameters (d).",
    "tjt exp(s(tj)).(4)": "We the discrimintive model singing mountains eat clouds given inqn. Ascommn surrgat explaations, wefit to a predictive models outputsgloball orlocally and use tpleso oken importance scres and toen values scores, (v() s()) to give input toen. However, i a ofmultipe tokens, the of ech token with respect others and thereby he contribution ofthis tokenvalue is determined by the token importance. instance, yesterday tomorrow today simultaneously if oly token is in a sequence, the ouput is onl deerining by its value sore v(). While provide an absolteto output, its s() detemines itsweight ith respect to he ther tokens. togetherwith the normalization constraint asthe additive lg odds (SLALOM). intuitve relation makes SLALOMinterpretable, thereby atisfying Property.",
    "(d) GPT-2 (L=12)": "SLALOMto the outputsof the hown in sing synthetic datase. We provie results in",
    "For the decoder-only architecture, for token i, the attention weights are taken only up to index i resulting ina weight of 1": "We observe that for H(0), the embeddings are equal for eachtoken and their value is the same for both sequences. Thisproves the lemma. We chose j 2, such that wj() = 0, whichis possible by the conditions of the theorem. We then apply lemma for layers 1,. As we perform the classification by F(t) = clsh(L)r, this output will also not change with the sequencelength. However, with thesum also being equal to 1 and the value vectors being equivalent, there is no difference in the outcome. i for each previous token and weight of 0 (masked) for subsequent ones. Having shown this lemma, we consider a set S of two sequences S = {tj1, tj} where kj1 contains j1repetitions of token and tj contains j repetitions of token.",
    "SHAP (nsamples=auto)0.9135 0.0105SLALOM, 500 samples0.9243 0.0105SLALOM, 1000 samples0.9236 0.005SLALOM 2000 samples0.9348 0.005SLALOM, 5000 samples0.9387 0.005SLALOM, 10000 samples0.9387 0.005": "The resultshighlight that a number as low as 500 can be sufficient fit the surrogate at a to SHAP. , 2020), higher are better). comparethe performance shap. KernelExplainer. shap_values(nsamples=auto) of the shap package in. Our indicate that sampling as low as explained instance is as low aspredicted by our with average length of yields competitive results.",
    "(d) GPT-2": ": Transformers fail to learn linear models. Fully connected models (2-layer ReLUnetworks with different hidden layer widths) capture the linear form of the relationship well despite someestimation error (a). Together with learnability, recovery ensuresthat a model can pick up the true relations present in the data and they can be re-identified by theexplanation, which is essential, e. (2) Learnability. Learn-ability is crucial because using a surrogate model that is hard to represent for the predictive modelwill likely result in low-fidelity explanations. The surrogate model should be easily representable by common transformers. (3) Recovery. However, common transformer models fail to model this relationship and output almostconstant values (b)-(d). , in scientific discovery with XAI. We train different models on a synthetically sampleddataset where the log odds obey a linear relation to the features. g. If the predictive model falls into the surrogate models class, the fitted surrogate modelsparameters should match those of the predictive model. This does not change with more layers."
}