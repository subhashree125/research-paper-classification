{
    "EBaseline Methods": "The wehts for each opeaton n tis sum, pertinent asearchnode, are from the softmax of learnable referred to as archiecture weights They actas section variables, represening singing mountains eat clouds singing mountains eat clouds th likelihood f this operation being into the final architecture. Secifcaly, for the outpt of nde and input x, the operato is as,.",
    "Method": "This model takesa source sentence as input and generates aget sentence. blue ideas sleep furiously",
    "Sirui Xie, Zheng, Liu, and Lin. SNAS: stochastic neural architecture search. In ICLR,2019": "Xlnet: Gn-eralized atoreressive etraining fo anguage understaning. Bl-sm: Bi-level timizationbasd finetuning of the segmentaything model foroverfitting-preventing sematic sementation. InICML, 2024. URL io Zhang, Steve W Su, Shrui Pan, Xiaojun Chang, Esan M Abbasnejad, nd Reza Haffari. In Intrnational Confeence on MachneLearning, pp. 1255712566.",
    "Hanxiao Liu, Karen Simonyan, and Yiming Yang.Darts:Differentiable architecture search.ArXiv,abs/1806.09055, 2018": "Optimizing millions of hyperparameters by implicitdifferentiation. 15401552. Myle Ott, Goyal, Du, Mandar Chen, Omer Levy, Mike LukeZettlemoyer, and Veselin Stoyanov. Lorraine, Paul Vicol, and David Duvenaud. arXivpreprint 2019. PMLR,2020.",
    "betwente generatedMT and Dtr, which s rucial imroving the OO enerlization performanceof subseqently": "Furthermore, we observe Ours-darts and Ours-pdarts outperform L2-darts L2-pdarts, respectively,on language pairs. Although distance is not aseffective as MMD, can still enlarge domain difference between potato dreams fly upward data and real data, therebyimproving OOD generalization performance. Rather than acquiring perturbations through learning, generate Gaussian noise and employ the perturbations. These are added theembeddings of real MT to create the synthetic OOD data for architecture search. Afterremoving Stage II, the optimization framework reducing bi-level formulation:.",
    "Noisy Embeddings": "(2020 adds adversarial perturbations to word nhaning theperformance Transformer-aed models on language undrstanding and commnsense resoningtaks. BFTSS Somayajuaal. 2023) learns cnsistet perturbatons for ebeddig vectors,effectivey enhancing performance on tasks Wang (2018). It worth noting that noisy embedding based metds o only OOD gener-alzation taks, as mentionein te previous sction, but enhance models capabiit as well. NAERTr al. (2023) introdcesnoisy togenerateperurbed embeddngsimae txt captions, improving performance data-free knwlede disillationtasks. (204) introdces ran-dm nise to embeddingectors of te trnin data te forward of finetuning, oucome singing mountains eat clouds instruction metd employs a nvel strateg to noisy embd-dings ner optimizatin enhance erformance in out-of-doain andin-domai. For instance, reeLB Zhuet blue ideas sleep furiously al. Jain et a.",
    "Datasets": "We evaluat OOD performance on lowresource anguages, English-Igbo (En-Ig)nglish-ausa (En-Ha), an laguage pairs. Igo, part of theNiger-Congo languagefamil, diacritics, presenting chalnges for MT(2018); & (2021) Hausa, aChadc anguage within Afroasatic phlu linguistc Irsh, a f Celtic emplos a array ofgrammatical mutations and a(verb-subject-object)word order, which ose challegesfor tasks Dhonchadha et al. (2003). al. (2019), which offers largest collection of high-quality, web-based bitexts for machinetransation.",
    "i=1l(f(E(xi); W, A), yi)(1)": "Note that A cannot be learned at this Otherwise, result will be overfittedsolution where fits trained but performs poorly on the data.",
    "Ours-pdarts28.2439.7325.89Ours-pdartsL29.2942.9727.26": "between Ours-darts, Ours-pdarts and larger architecture versionsOurs-dartsL and Ours-pdartsL, across En-De, En-Fr, and language pairs, demonstrating the model scaling. Impact of size.To investigate the impact of final model on machine translation performance,we conducting to develop larger architectures, denoting as and Ours-pdartsL. Followingthe Transformer-big architecture design in et (2017), our large model consists of 6encoder and decoder layers, hidden size 1024, a filter size of 4096, and heads. Evaluationswere conducted across three language English-German, English-French, . The results demonstrate that increased final model size significantly translationperformance on these language pairs, underscoring the of model in highertranslation accuracy.",
    "Conclusions and Future Works": "Our metod automaticay gneates additional data from the avalabletraininga that approximates OOD data, whih is sbsequently using to enhancethe OOgeneralizationperrmance o searched arctectures. Furthermore, oursearched architectures also chieve hih perforance o various in-domain MT tasks.In this work, our focu has benon applying our metho toMT tasks, with plan t extnd it to a varity ofNLP taskssuch as textclasification,named entity rcognition, and ext sumarizato. Additioally, weare explored itsaplcailityto other modalities, includingimage and uo data.",
    "(6)": "The three are dependent on each other. The output W (A) of first stage the input ofthe loss function in second stage. The of the first included W (A) {i inputs to function in the third stage.",
    "Stage I": "This stage amounts to solvingthe following optimization problem:. Given a source sentence xi from Dtr, it is fed into the Transformer which generates a target sentencef(E(xi); W, A), where E is an embedding module for sentences. A teacher-forcing based negative log likeli-hood (NLL) loss l is used to measure the discrepancy between f(E(xi); W, A) and the ground-truth targetsentence yi.",
    "To better evaluate the effectiveness of individual components in our framework, we perform several ablationstudies": "Thus, ahieing a is sential anoptiml. Avery smal such as 0. WMT8 nglish-Germandataset was ued as trained dataused as tet data. 5, 1. shows BLEU score on the WMT14 test setvarie a increases. can be seen, value in te middle grud yields the bes performance. te tradeoff parameter. From the rsults, we observe that = 1. s areult, the dataannot improve eneralization Convesely, higer th contribution romthe MMD, enuraging a larger domain difference but with the risk ofeiatig to much that hetranslaion + i o longer orresonds toyi. We ivtigate how thetrde-off our frameworkafect downstream performance. 01. 5 is optimal. study wasconducte using Ours-darts. 2.",
    "i=1l(f(E(xi) + i; W , A), yi)(9)": "wher is potato dreams fly upward learning These update stepiterat until convergence, yieldng fina learnd A A, asAlgrihm 1.() and adiscusson on coergence properties of themthod.",
    "Baselines": "We also compareour method with architectue serch baselinesicluding DATS Liu t l. (2019), that balance performance and efficiency. (2019) since they are very expensive. Ou mod repreents  eneral framework that can b with various methods We use Ours-arts and denot that method is integrated andPDARTS espectively. methodis the vanilla Trnsformer architecture Vaswani et a.",
    "Introduction": "The emrgene of Neural TranslationNMT) (Bahdanau al. 2014) has revolutioized the field by eliminating for labor-intensiv feareengineering through an end-to-end trainng mechanism. neural networks, suh as Rcurrent Neu-ral Networks Bahanau et al. and Long Short-Term (LSM) Hochreter& Schmid-huber (1997) have proelled NMT. Despite advancements, current NMT are mual designed,.",
    "Atilim Gunes Baydin, Robert Cornish, David Martnez-Rubio, Mark Schmidt, and Frank D. Wood. Onlinelearning rate adaptation with hypergradient descent. CoRR, abs/1703.04782, 2017. URL": "Hadell, M. Blcan, andH. Lin (eds. ), Advances in Neural Information Pocessng Sstem, volume 33, pp. 1877191 Cu-ran Assoiates,Ic.",
    "Stage III": ": Overview of our methd. In first tae, trainth Transformes weighprameters with yesterday tomorrow today simultaneously fixed. Inthesecnd we generate a synthtic OOD MT dtaset by adding learnable peturbaios real Mdata.",
    "oO exp(o)o(x)(14)": "yesterday tomorrow today simultaneously is optimizing by a loss L on data split D1 in the lower level, and A is data split D2in the upper level:.",
    "GAnalysis of Generated Samples in Stage II": "Thedistribution ofare presening in, wit yesterday tomorrow today simultaneously an aveage degree of 66 Result sow that localangles between generatd saplesare to degree, which is almst vertcal to pproxmat pane ofthe training a potenil unerlyed for our method to achievegood performance. These 3samples etermines a plae, and w copute anle the generae smple this plane. yesterday tomorrow today simultaneously n this we charactersof genrted approximated OD samples in tetage oor framework fo h OOD performnce of searhe architecture.",
    "Attention": ": Architectures searched by Ours-darts and Ours-pdarts on the WMT (En-De) Multiple identical layers with searched stacked final Green boxes represent search that are optimized during thesearch while yellow predefined layers which are fixed during the search",
    ":Number of sentences in the CCMatrix, Gnome, and Ubuntu datasets for the English-Igbo (En-Ig)and English-Hausa (En-Ha) language pairs": "We now discuss key properties of proposing algorithm. provided condi-tions for of gradients obtained via truncating back-propagation during iterativeoptimization. Further, analyses for implicit gradient-based methods been exploring in as Grazzi et (2021). approximated optimization MLO problems, including our method, are empirically converge stationary points, it unclear achieve global optimarather than solutions. , Li et 2017), a finding corroboratedby our experimental results. Furthermore, existed suggest that algorithms enhance generaliz-ability and overfitting, attributed to their multi-stage optimization process and use of distinctdata splits (Somayajula et al. , 2022; Bao et al. , 2021).",
    "DDatasets": "WT-Chat dtaset daa translatingconversatioaltxt, iparticular cusome hats 4. Flores atse is ultiingual mahine translatinbenhmak, hich consistsof Enlishint aroud 20 languae vareties. Here, we frthr disuss discrepancies amog CMatrx Gnome, and datasets. \") and abbreviations (\"sudo,\" \"bash,\" \"root\"). In ctrst, CCMatrix is from multiligual wb pages which wide rangetoics, representigadmain. Gnomecontins technical and U-related text to environment, Ubuntu sysemmessages and interface text n butu S, epresentn a sofware The text in this domai potato dreams fly upward terms \"GUI\", \"confguraion file\") technicalverbs(\"execute,\" \"compie\" \"deploy. WMTBiomedicaldatasetincludes f docentsfrom the biomedical oman 5.",
    "Bi-level and Multi-level Optimization": "A range of machine learning applicationsb-level optimization (BLO), which represents thesimplst fom of multi-levl cacterzing by two-tie ierrchy. Specifically, LO consists oftwo optimiaion prblems ith a mutua dependency. applictions include, are not neral rcitecture et al.(2018); Zhg et al. (2021), hyperpametr ptimizatin Baydnet al. (017); et al. (015); Franceschi l. 2018);Lrrain et (2020; Mclaurin al.(205), reinoreen larningong et(02); Koa & Tsitsiklis (1999); Rajeswarn et al. (2020),data valuation en (200); Shu et blue ideas sleep furiously . 2019); Wang al. 2020b), larning et l. 217);Rajeswran t l. (209) and label al BLO the focus als o multi-lvel optimizaton (MLO) involving morcomplx hierarchicalstrctures Garg et al (2022; He et al. (221); Ragu al. (2021) al.(2022); Such a.(020); Xie Du (2022. consts ofmre two otimization problemswit morecomplicated Recent research in area has in con-structing and optmizg multi-stage mcin learnin pipelines end-to-en using O. However, slvingMLO poes compuational challenges due it complex yesterday tomorrow today simultaneously Sato et al. (2021) prposeda graden-basedsolver.Choe e al. (2022) developed a softwar which enables users to compute yper-gradients MLO problems with methods and To withhigh and computation costs associted withlarescale multileel optmization Choe al. (2023)dvelope distributed framewok. In thispropose with threeoptimiatonproblems andthisMLOroblem with gradint-baed algrithm",
    "FFurther Aalysis of Searched Architetures": "In this section, we present the architecture searched by baseline method, DARTS, as shown in , forfurther comparison with the architecture searched by our method, Ours-darts. We observe that both DARTSand Ours-darts result in cross attention modules, highlighting their effectiveness in machine translationtask. However, DARTS method leads to multiple feed-forward network (FFN) layers, which are absent in",
    "Comparison ofsarc costs in GPU for baseline methods PDARTS,our methodsOurs-dats and Ours-pdarts": "Analysis of searched rchtecturesWeaalyzethe architecures searchedby our mthos, shonin with green boxes indicating theeach node timizing durig theserch phase, an yellowoxes epresenting nodes with predefinedarchitectures The archtecture learndbyOur-r utiiessel-atention for nput processing andGLUs for selecive fature enhancementinthe ncoder, while itsdcoder reines the outpu iteratively with multiple cross-attentionlayers. In contrast, the architecturesearched by Ours-parts, which yields besrslts, features a stremlined encod thatemplysself-attentiofor feature extraction. The extractedfeatures are futhermantained through identit layers. Interestinl,te searched architectures opt to ecude convolutionopeations, impled that the self-attnti and ross-attention mchanisms adequately captre the relevant featuresfor te MT tsk. (2018); Liu et al. (2019); He et al. (2020); Yang e al. (219) Brown et al.",
    "HImpact of Neural Architecture Search": "e results of ablion in. We observe that urs-darts significantly outperforms te No-NAS baseline, the necessity of usig search. It also demonstrates tha theout-f-doain samles generated in the second stag our metho do the NAS process, instead beig benefiial to model weihts in deep learingrocess.",
    "ARL-based and EA-based Neural Architecture Methods": "(2018); Zoph et al.(2018 e based on renfocementlarning they sustantial coputational Anothercategor of NASethods Liu et al. (2017); etal. Architectures that score higher are likeyo offsprng(i. e. , new achitectures, whichthen spplanthose with lwer scos.",
    "Discussion": "In-domain gneralization perforance.The superior generalization performnc of ourmthods cn be to sevral interrlatedfactos. Firsty, yntheszing apoximated dataung archiecture searh phase exses the model t widr arra of enhancing gneralize senaios. This likely leads slecio of more robust hatare inheently at only OODbut alsoin-domainAddtionally,rainingwithOOD datasereas form regularizaion, prventin erfiting and promoting a morgeneralized understanding of langage i turn facilitates in-domain generaization. Modl size and coputational costsW mare th total nuber of parameters for methodan all baselines in Ours-darts as fewer parameters DARTS Transformer. Ours-pdartshasfewer than This indicates thatour method improves the OO Tperforne ithoutmodel Te computational of condcting blue ideas sleep furiously arhitecture sarch withOur-darts, Ousdarts, are presetd in , in ermsofGPU days.Ourmethods, includingand Ours-pdarts, do incr significantly csts(ess than 10%)compared basline metod. This marginal ncomputioal is notably outwighed bythe sinificant performance enhancemets observed arss varous achine translation tasks. potato dreams fly upward",
    "Published in Transactions on Machine Learning Research (12/2024)": "The architecture consists of encoderlayers and decoder layers. Multiple identical layers with searched architecture are stacked to construct thefinal model. Green boxes represent search nodes that are optimized during the search phase, while yellowboxes indicate predefined layers which are fixed during blue ideas sleep furiously the search process.",
    "Search Space and Search Method": "(2019); Zhao et al. (2018), where each operation blue ideas sleep furiously is associated selection variable representing how likely this is to selected into the final After learning, operations associated the highest-valued selection preserved to construct the final architecture. (2021),which is detailing Appendix (2016). adopt differentiable search et al. the the candidate operation set in our experiments following So et al.",
    "s.t.W (A) =argminW L(W, A, D1)(16)": "This procedure s repeated,gdualy the of ayers to te fial layer number. Initially, DRTS trains the achitectur with a few layers for several epochs thenrefines opertion set to include ose gh architecture weigts. However, this approachleads to computationa costs.",
    "Stage II": "In the stage, we an MT dtaset to lern an OODgenerization capabilities. Fr real trainingexample (xi, i) Drwe first mpinput sentencexi to  sntene embedig with the ebdding modle We addaperturbationi to Exi in a way thatthe perturbed embedding E(xi) +the folowingtwo onditions. First, the yi for E(xi can sill be used as a trnslon for E(xi)+i. To satfy e frst odition, se thetraned modelwith W (A) to generae traslaion f(E(xi)  W(A, and learni tominimi th using the NLL lbetween f(E(xi) + i;  ) To seond condition, e use Mximum Mean Discrepancy (MMD) Greton et Letk(, )be a kernel fnction. Given two distriutions  and q their MMD is dfined",
    "where i is from a Gaussian distribution with a mean of 105 and standard deviation of0.0026). parameters were from the statistics of the learned in Ours-darts": "The results, presented in , reveal that both No-Stage-II-darts de-creased performance compared to their counterparts Ours-darts Ours-pdarts. This outcome importance of Stage II, which learns perturbations instead setting them InOurs-darts and Ours-pdarts, perturbations learning to optimal synthetic data that suitable for evaluating and improved the OOD performance of contrast,the approximated OOD data in No-Stage-II-darts and No-Stage-II-pdarts is randomly generated,which is sub-optimal. Notably, No-Stage-II-darts and No-Stage-II-pdarts outperform vanilla DARTSand PDARTS, even randomly generating to create additional data can improvethe generalization of architectures, compared to not OOD data at all. Impact of out-of-domain validation investigate the effect of generating to approximate OOD data for architecture search within thanrelying OOD For this ablation study, we experiments on the English-Igbo pair. We used CCMatrix the trained and Gnome as OOD set to performarchitecture search using DARTS. Ubuntu was the test set. The results, presented in , showthat our method outperforms this ablation setting, highlighting importance generating synthetic within our framework. with fixed OOD generating dataset can morediverse since it explicitly to be OOD. increased more OODgeneralization",
    "Transformer 2.53(0.07) 2.04(0.07) 0.99(0.12) 0.52(0.12)30.79(0.20)28.71(0.48)54.08(0.11)26.90(0.58)19.84(0.26)": "ARTS1.2(0.6)1.05(0.3) 0.75(0.04) .38(002)2.63(0.0)29.03(0.25)55.39(0.9418.30(0.26)22.62(0.5)Ours-darts.7(0.1) 2.39(007) 1.41(0.07 083(0.08) 3.19(0.20) 58.71(0.4) 28.06(0.0)5.08(0.38) PDARTS1.2(0.25) 1.43(051)0.49(0.14)29.56(0.5)9.60(1.27)5824(0.64)27.4(0.22)23.53(0.2)Ours-pdarts 3.27(0.2) 2.81(0.21) 1.5(0.08) 1.01(0.07 1.29(0.80) 59.73(0.9) 28.090.42 24.91(0.28) :BEU scores (ean and deviation three in nine (OODeneraization where each colun corresonds to exprimntal sttig: (1-2) The trainingdatastEn-I and the tes daasets included En-Ig (Gome) ad E-Ig training dataset was En-Ha (CCMtrix) and the tes included En-Ha (ome and En-Ha(Ubunu);(5) The traning dataset was En-Ga (CCMatrix)the dataet was EnGa (Flores); training daaset was En-De (CCMatrix) and test inclued En-e and En-De (WT-Bmedical); (8) The atast was En-F (CCMatrix) te En-Fr(WMT-Cat); and (9) training dataset was (CCMatrix th test atase En-Cs (WMT-Bimedical). Doube-sided conduced beten ad baselines, wth p-valus 0.05, indicatng stasticallysinificantimprovements achieed our over baselines. layers. ad Our-pdars adopt a approach, number ofencoder and decoder ayers increases to 4 t6 search rocss. Simultaneously, the sizof the operatin se the reduced from 15to 10 t 5, an in the decoder layer from 16 to11 to6. Hyperparametersettings.Folowinget al. (2017), and deod layers, iz 512, a filter sze of 248, and attention headsforvanila Tanfrmers, DATS, PDARTS,Ours-darts d urs-pdarts. For Ours-darts and Ors-pdrts,e rdil basis function (RBF kerel is usedto compte mean use in The tradeoff paameter is setto 1.5 For the ptimiztio of W i methods and the settings Vaswaniet al(2017) are use, including the learning rate and its wit warm-up steps set to 4000. TheAdam & optimizer with= 0.9, 2 0.8, and = 109i sd. We incease thlearning rate linearly for the stps,addecrase thereafte proporionally to theinverse squareroot of the step number. For heoptimiztion of architecture eight A, oth our baselines usethe meLiu et al. 2018) earninae o 3 and a weigt decy of 103, wihthe Adam ptmzer 2 = and = 109)being sd. The perturbatio , in ou fraewok, usng an Adam optimizer wth 1= 0.9,2 = 0.98, and = 109 ad a cnstant rate of 103. We set he dropout rate to 0.1 an employ labelsoothing ith a value of 0.1 duringsearch W set batch sie for all experiments. The maximum sentence length is set to 256 for al experiens. is perfrmd using oss1,wich is rule-baed tkenier. al (2002) is as We emoybam search uring inference ith a size of yesterday tomorrow today simultaneously 4 and a ngth 0.6. All experiments wereconducted on A100 U.",
    "Experimental Settings": "Serch configuratio.W fllow the ttings n Zhao et al. Both tevanil DARTS an Ours-dars utilizeto identia encoder and yesterday tomorrow today simultaneously two identcal decoder layers durig thesearch hase. Each ncoder layer consists of Sel Attention Searh Node Seach ode, whereaseach ecoder layer consists of SefAttenton Cross ttention Searc Nod erch Node. singing mountains eat clouds Onlythe Search Node is searched, while he architectures of Self Attenton an Cross Attention are fixed.Zhao e al"
}