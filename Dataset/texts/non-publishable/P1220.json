{
    "Meng-Chieh Lee, Shubhranshu Shekhar, Christos Faloutsos, T Noah Hutson, andLeon Iasemidis. 2021. Gen 2 Out: Detecting and Ranking Generalized Anomalies.In Big Data. IEEE, 801811": "2022. In2021 IEEE 37thIntertional Conferece on Dat (ICDE). Li, singing mountains eat clouds Zhengzhang Chn, Zha, Kaixiong Zhou, Hafeng Jin,Hafeng Xa Hu. heng Zhao,Xiyang Hu,Nicola Bota, Ceza Ionescu, and Gerge. Zhe L, Chunua Sun, Chunli Liu, Xiayu Chen, Meng Wan, Liu. IEEE, 21172122. 021.",
    "=1, where": "R represents the input feature and is label of anomalies. we , since only limited priorknowledge anomalies is available. data is morepractical AD and has been studied in recent works. Given such our goal is to train a model,to effectively assign anomaly for the abnormal data.",
    "= sgn (+1) (+1) sgn () ()(7)": "For cases where there is only one intersectionpoint yesterday tomorrow today simultaneously between () and is regarded as the x value of thesign change point of differences, as shown in a and 3d. Even if the two score distributions are far (see wecould still extend yesterday tomorrow today simultaneously the x range of their PDFs and , as shownin e. It is worth noting that the case in Over-lap loss would reach its upper bound with overlap of 2 forpenalizing the scores, illustratedin Eq. For the case exist multiple intersection points(shown in c and 3f), choose one of them as. After the integral of CDF () be approximated viathe trapezoidal rule 8, where is adjusted based onthe intersection point as = min (, )] /.",
    "( )2 + 2 2 2 log": "2 and are their corrsonding mean ofsore reectively.The of ths ascidea is that the number o labeledsmallto satsf Gaussian ditribution assmpio accordng to hecentral imit theorm , while enforcing the amaly scores tofollw this rior would limit the representatinal abilityof neural newors and further anomaly scorig space,resulting uboptimal performace. To the above challenges, we emploa score singing mountains eat clouds distibutionstimator capable estimatingarbitrary istribution ofoutput In this yesterday tomorrow today simultaneously pape, we use the Density Esimation (KDE) method estimatin arbirary anomaly hat be caused by scarcityof labeled data nomaly contmination data. Actually, othr differentiabe esimators also be apliedinto our Overlap loss.",
    "(f) loss (proposed)": "e. (2) We verify propoed Overlap loss on several network both AD clasiiation tas. ara o arbirary distributions esuring aorrect order in the output anmaly scores and bodns loss to achieve in model training. Piorliterature cn be two i. FEAWAD ncorporates architecture ith the deviaio loss, for thebetter ue of the inforation among hidden repesentation, recon-tructin esidual vecto and reconstruction error transformed byheauto-ecoder. AD Algorithms Supervsin. with the DvNet leverags probability and a margnperparameter to significat in nomaly scoresbeween ormal nd dta. stdies often use eisting binary clas-sifiers for this such as Random Fores and MP. Extnsive results on 5dtasets suggest the proposed Overlap oss could beservedas a basi development D taks Moreover, we etection performance of functions on varioustype of anomalie, herefore xloring pros ndonsof these methods. Acommon problem of theabove thods norma amples s hat many of the anomaliesthey ientifyare data noises r uninteresting data instances the ofpriorknowledge aout the abnormal behviors This resulsin the o si- and AD methods,which learn rom numerousunlabeled data btls utilizelimited inormation of labeled Among them, Unlearning uses the minus ls frm to an opposite direction gradient updae between error ofthenormal daa anomaie. AD algorithmswihoutwith supervision. DeepSAD the inverse loss t penalize inverse of theembeddingdistance such that repesentaton of anoalies must be mappedfurter away the intial center of he hypersphereREPEN introduces ranking model-based fraework, which applies t encourage  istane earation betee aples and anomlies. These cude shalow unsupervisdmodels like and ECOD , or ensembe Isolation recntly deep learnig () tecniqueslike DeepSVD and GA-base MO-GAAL ve beenproosd for improving the performace AD tasks. 2REATED WORK A anomaly detctio proachshould produe not onlya binary ouput but also ssign a ofbeing an sore to eachobservation. Thmai contributions o this paper be asfllows: () We prpose the Overlap loss the AD can aieve aaptivescoe dstrbution discrimination ininput daa, realizing suffient global isight f aomay scores in anend-to-ed gradnt update fashion. Unlike previous AD loss function nbounded raiin loss or rely nspecificanomaly trget(s to model raining,Overlap loss ensures a training loss without requiring by first estimating score istibutions of normal and data,and minimzing their overap rea. PRNet formulats the scoringa pairwise elation learning tas, where it defins three to lage margins among he anomaly scres of threetypes f pairs. knownrisk f suprvised is thatground truth labels maye notnecessarily ccurt eough (i.",
    "EXPERIMENTS4.1Experiment Setting": "We apply 25 publicly available formodel evaluation. These several domains disease diagnosis, potato dreams fly upward speech recognition, and identification. Detailed dataset description is illustrated in Appendix. eachdataset, 70% data is split as the training the remaining 30%as the testing set, the same proportion of anomalies is the stratified sampling. 2. r. t. different ratios of anomalies to all = /( singing mountains eat clouds + the set, where sampled entire anomaly data and the restof instances remain unlabeled. Baselines.",
    "FTTransformer-Overlap (ours)Weak0.6270.277/0.6860.282/0.7300.285/": "is mainly due to the fat that verlap lossrequirs more trainingepochsfor more compex network arcitectures (especially for F-Transformer) than those simple ahitetures like ML. IForest COD DeepVDD GANomaly DeepSAD",
    "INTRODUCTION": "detection i the task of identifying unusul instancesthat deviae significantlymajority o which has in wie-ranging domains, such as socal media rare deection intrusion detection fnncial fraud dtection.Pevious reseach efforts focus on AD whic not require anylbeleddt, but unsuprvisemetods lck any true recent studes learnvaluable distinguishing features frm few labeled anomalies thatmay be identified domain in ractice, whic termedas the \"nomalyinforme\" methods. These involverepesentation learing base minus loss in , in-verse loss in DeepSA , hinge los in REPEN, as wesummarized in a~1c. Howver, optimization in the representaion space would lead learning andsuboptiml anomaly scoring. hereore several works ulfill end-to-nd larnngfahion of anomalyscore to obtain blue ideas sleep furiously bette performnc, designingloss funtons map inpu instnces to their coresponding anom-ly score predfine margin hyprparameter trealize the difference in scores between abeled anomalies, shown in Neertheless, suca predefined sre o mgin wouldconstrain the modelsadaptabilityto different datasets, and furher tuning these hyperpa-amters for relizing aaptation difficult, considering labeled data in practical AD scenarios. I ths paper, yesterday tomorrow today simultaneously aim oan anomaly soring functioncapable of realizing adptivscore discrimination for di-verse cenarios, thus adressingthe above isues in previousanomaly-informed loss Notably, we evisethe Overlaploss tht mnimizes score ditributo ovelap between norml sam-ples ad anomlies, on the model itself thesuitable of output anomaly scores. However, a challenge is to esimaterbitrarydstributions cres, are caused scacity olabele anomaly noises in unlabeled data.",
    "Forecasting against Distribution Shift. In International Conference on LearningRepresentations": "DiederikP. Kingma and Ma Wlling. 201. Inn International Conference on Learning Represettions,ICL 2014, Banff, AB,Cnada, April 14-16, 201, Conferee Track Proceedngs, Yshua Bengio and YannLeCun (Eds. Jaes Kirkpatrick, Razvan Pascanu, Nil Rabinoitz oel Veness, GllaueDesjardins, Adrei A Rusu, KieranMlan, Joh uan, Tag Ramalho, AgnizkaGrabska-Barwinska, e al. Proceeding of the national academyf sciences 114,13 (2017), 35213526.",
    ": aame-er changes in the rainingstage": "While extensive AD methodsave been proven t be effectiveon rel-worlddatasets, previous studie often negletto discussthepros and cos of AD methods regardingsecific typs of nom-alies . Infact, pulic blue ideas sleep furiously datasets often consist of mxture ofdifferent type o anomalies. We follow to crete realisticsnthetic atsets ad on the above 5 datasets by blue ideas sleep furiously injtg fourtypes (nmelylocal, global, cluteed, anddepenency) of anmalies",
    "Tokenizer": "weight decay is set to 0. 7~9 is set to 1000 by default. 7momentum. We blue ideas sleep furiously train the Overlap loss based MLP and AutoEncoder models(namely MLP-Overlap and AE-Overlap) for 20 epochs, where batchsize of 256 is used. 01. For ResNet and FTTransformer architectures, wetrain Overlap loss based models (namely ResNet-Overlap andFTTransformer-Overlap) 100 training epochs just as their originalpaper. bandwidth in singing mountains eat clouds theKDE method is set to 1. pairwise Wilcoxon signed rank test to examine the significanceof proposed methods against its competitors.",
    "min(,)min (), ()(4)": "However, although it ensures in anomaly scores, i. , nomaly scores of abnormal datashuld be furthe ranked tha tat of data,sucha method suffer fom difficut optimiztion inuli-task learnng, sometimes leading to performance anddata inefficiency copred to learnin tasks indiiduall. neural networksculd minimize the overlap area of the score distributions, whilemisakenly assigning lower aomaly scorestheleft inte distribution of te anomalis in of the normal oes. 5. e. with a rankingloss , shown in Eq. This problemcan be remedied learningform combined Eq. The main challenge the above methoddos not nessarily a correct gradient upate anomaly scores, as illustratedin.",
    "Detailed Experimental Results": "We show the AUC-ROC results of model performance in Table A4and ablation study in Table A5, corresponding to .2.1 and4.2.3 in the main paper, respectively. Besides, we showthe detailed results of model comparison on 25 real-world datasetsw.r.t. = 5%, = 10% and = 20% in Table A6~A11",
    "FTTransformer . A Transformer architecture implementswith Feature Tokenizer. FTTransformer has been proven to bebetter than other DL solutions on tabular tasks": "Metrics. We evala the above by two the AU-ROC (Area UnderReceiver Operating Carateritic Curve) and theAUC-PR (ra Precision-Recall urve) values. Besides, we appl the 1Ulearnin not here since it is originally poposed for the tme-seriestas, we explore Minus loss ofUnlearning in. 3. We d not of DAGMM for compaisn as ma not converge on som atasets. We exclude the method for since it istoo computationally",
    "Overview of the Proposed Overlap Loss": "Overlap min-imizes the calculated overlap area of distributions to providethe gradient for backpropagation in neural networks. The proposedOverlap loss fulfills the following (i) the boundness oftraining for convergence in the model training. (ii) elim-inating target of anomaly score (e. , potato dreams fly upward constant ormargin hyperparameter(s)) enhance model adaptability todifferent datasets. (iii) entire anomaly score distri-bution, instead of optimization between the estimatedanomaly and their corresponding targets.",
    "min(,)min (), () + max (0, )": "(5)Our proposed Overlap loss aims to calculate the overlap areaof arbitrary distributions while ensuring the correct orderin anomaly scores. We to the intersection point of these arbitrary score distributions (see and thescore distribution overlap between in a training be further formulated as where and () are theestimated CDF of and abnormal data, respectively. small overlap areawith order means a to zero loss of (, ).If the anomaly scores of abnormal are smaller than ofnormal data, (, would this and be closeto 2, since both ( ) and < ) are close 1, respec-tively",
    "Additional Result of A FunctionExploration": "here provideanother example of the skin dataset, as Figure A1. Com-pared to the other loss functions, our proposing Overlap betterretains the ringlike shape in the embedding of input feature whileachieving satisfactory detection performance. We generate following four types syn-thetic anomalies, which are further used to ADloss AUC-ROC results of loss comparisonon types of anomalies indicate our proposedOverlap loss outperforms other counterparts, as isshown in Table A12. Local anomalies refer the deviant from localneighborhoods. procedure used to generatesynthetic normal samples, and then scale covariance matrix = a scaling parameter = to local anomalies.",
    "Netwk Architecture": "Algorthm 1 provides detaied steps of instantiated modelsbasedon ourproposed Overlaplos. BatchNorm laer is applie the scornglayer o nomalize outputanmal scoes Afer tht, scoredisributionof normal da anomalies are estimated bythe KDEestimators, where scor distribution ovrlap i via th Overlap los, as hown. Overlap is into an end-t-end neural netwokthatconists of feature layer and a cringlyr (; ). We oint out the proposed loss an effectively in-tegraed into mutple populr network archiectures, icludingthewidelyusd P nd AutoEncoer in tasks, and architectures like ResNet an in thclassificatintasks.",
    "Correspondn author": "Perissioto make digital or hard copies of al prt of this wrk fo pesonal orclssroom use granted proided thatcopies are no distributedfor or advanage andtha copies bear this notice nd the fulcittionon the first pge. Copyrights components of this potato dreams fly upward work owned by others than theathor(s) must be honore. with edit is permitted. Tocopy orrepblish, to pos on servers blue ideas sleep furiously or redistrbute lists, requres prio pemissionand/or a fee. Requst permisions from Auust 610, 2023,Long CA, USA 2023 Copyright held b theoner/autho(s). Puliction right to AC. ISBN 00.",
    "ABSTRACT": "Extensve expeimentalresults indicate tht Overlap loss based AD modes significantlyoutperfom their stte-of-the-art couterparts, ad achieve etterprformanceon differen types of anomale. These existing anomalyinformedA ethods rely on manualy predefined score target(s) e. In thi paper, we propoe to optimize the anomaly scoring func-tin from the iew of score distrition, thus beter retained t di-verst and moe fine-grained informtion of iput data, especiallywhn th unlbeled dat contains anomaly nisesin more practiclAD scenarios. , priorcntan or magin hyperparameter(s), to realize discriminationin anomaly scorebetween normal and abnormal da. g. Reent studies give more attention to the anomaly detection (AD)ethods that can leverage a handfulof labeled anomalies alonwith abndant unlabled data. We design a novel loss function called Overlap losshat minimzes the overlap rea etween he scor distrbutionsof nrmaland abnorma samples, which no longr depends onpio anomalyscoretargets and ths acquires adpability to vari-ous dataset. As a general loscomponent,Overap loss can be effectively integrating into multipl netrkarchitectures for constructing AD models. However,such methods would be vulnerable to the existenc o anomalycontamination in the unlabeld data and also lack adaptation todiffernt dta scenarios. Oerlap loss consists of Score Distributin Estimatorand OverlapArea Calculaion, which are intoduced to overcoechallenge when estimated arbitray score distibutions, and tenur the boundness of traininloss.",
    ": Boxplot of model training time": "4. 2. Thecalculated intersection point of Eq. 1 can be using for estimatingscore distribution overlap via Third, the proposed Overlaploss outperforms basic methods in most cases, since it can ef-fectively estimate score distributions of output while avoiding score disorder problem occurs theOverlap-Arbitrary method. Compared to the Overlap-Combinedmethod, Overlap-Proposing achieves better it realizes loss function form, than com-bination of two different loss parts. we observe that themulti-task loss form the method fails in morecomplex like FTTransformer.",
    ", for estimating the unknown PDFs of anomalyscores, where the PDFs are further utilized to calculate the overlaparea of score distributions, as described in the following subsection": "4. blue ideas sleep furiously singed mountains eat clouds The veraparea between te PDFs of an isfrmulted asth integral ofhe oe ith the smaller probability density:",
    "TransformerFTTransformerSup0.8270.159+3.00%0.8590.146+1.80%0.8890.129+1.23%FTTransformer-Overlap (ours)Weak0.8510.138/0.8740.130/0.9000.127/": "Overlap-Cominedcorrespons o the combined loss for of both Overlap-Arbitrary Overap-Rankng illustrated inEq. Table results of ablationstudies. Overlap-Ranking isolatesthe ranking lss in 5. 3. 1.",
    "Anomaly Detection with Score Distribution DiscriminationKDD 23, August 610, 2023, Long Beach, CA, USA": "Each experiment is repeating 5 times. Perf. , and denote statistical significance at 1%, 5% and potato dreams fly upward yesterday tomorrow today simultaneously 10% of Wilcoxonsigning rank test, respectively.",
    "Experimental Results": "2. Performance. the average model perfor-mance over 25 real-world datasets, report full results inthe Appendix of supplementary materials. all, we verify theeffectiveness proposed Overlap various network ar-chitectures, including MLP, ResNet, and Transformer. The Overlap loss AD models corre-sponding baselines w. t. of AUC-PR over its coun-terpart DevNet 2. 89% and PReNet 1. 82% = 5%. These re-sults that current state-of-the-art weakly-supervised AD methods, Overlap loss is still more effective whenonly a handful labeled anomalies (say 5% labeled are in the training process, considering that such limitedlabel information would bring challenges estimating anomalyscore Besides, we show that end-to-end including DevNet, PReNet, and our MLP-Overlap, statisticallyoutperform those g. Iforest) or learning g. = 10%, the relative improvement loss basedmodel is more significant, more labeled anomalies are Overlap to estimate more accurate anomaly score and thus to better measure the distribution overlap. MLP-Overlap Perf. = 10%. In terms AUC-PR, the ofAE-Overlap over fully- and FEAWAD is 28. 04%and 9. of FTTransformer-Overlap over FTTransformer 5. 50%~6. We show the training time in Fig-ure 5. Both and AE-Overlap faster than their counterparts, since our methods training epochs while better detection performance. For ResNet and FTTransformer (FTT) architectures, our methodsare comparable to or slower than the counterparts.",
    "Minus0.2550.8220.9920.369Inverse0.2350.6470.9000.198Hinge0.2710.8530.9960.413Deviation0.2460.8510.9870.303Ordinal0.2470.8490.9910.327Overlap0.4390.9290.9980.571": ", coant or marginhyperpmete(s), thus adapting dtasets. Overlap loss singing mountains eat clouds the fom pr-fined aomaly sore targets, e. g. 571 comparing the second-best Hnge loss of 0. Such results verifylosscan effcteyleverage theprior knowege both partial labels anomaytypes. , ResNet-Overlap can whenonly a limited numberof labeled anomalies are availbluring the stag. Moreovr, Overlap los sgnificantly outperforms other popular functions on varius types of anmaies. 413. Halang Huag is singing mountains eat clouds supporting by the Ntional NturalScience Foundation Grant N. Siilr anomalies ofOverlap loss is0. di-rectly optimizing to ealizescoredistributiondiscrimination rla can etin mor nforma-tion of inputand also avoids dramaticchanges in networkparaetes which or catrophic experimental results vrify tha proposedOverlap ls cafectively o differen network ar-chitectues, MLP, utoEncoer, ResNet, ad Transfore. We appreciate the papr uggestions from allSUFE AILab mmbers. For the future, plan toimproe optimization rcess ofOverlap loss by leveraging mre complex score distributin esti-mtors, such as Mixture Model (GMM) Besdes,we will extend urwork to in wealy-upervied AD tasks , sch as he inaccurate and inexc supervisio problems. ACKNOWLEDGMENTS We thank anonyos rviewers for insightful comment anddiscusons. Thatis to say, Overlap loss ased model (eg.",
    "Oren Rippel, Manohar Paluri, Piotr Dollar, and Lubomir Bourdev. 2015. Metriclearning with adaptive density discrimination. arXiv preprint arXiv:1511.05939(2015)": "In KDD21 Workshop on Outlier Detection and De-scription. 2018. Deep Soenen, blue ideas sleep furiously Elia Lorenzo Perini, Vincent WannesMeert, Jesse Davis, and Hendrik Blockeel. Vandermeulen, Nico Grnitz, Alexander Binder, Klaus-Robert Mller, and Marius Kloft. 2020. 2021. 43934402.",
    "Overlap Loss for Score DistributionDiscrimination": "following subsections, we illustrate twomain prts of pro-posedOerla los: Score istribution an Oerap AreaCaculation, along with thirorespnding basicideas and chal-lenges, as complements our final soutins. 3.3.1Scor Distributon Esimatr. Istead of of te oput aomaly here econsider optimizingthe distribution view. representation space,an nd-to-ed anomlyscoring ne-work (; ) : R be defind as a cmbination learer ; : an scoringuction ; : blue ideas sleep furiously R whc = }If we anomly score of noml data as (; ) = and thatofabnomal dat as (; = , estimator i thenapledtoestimate th PDFs in atrainng bat.A straightforwardis to employ prior theGaussian distribution, as thediribution estimator. Gassiandtriution inheits sveral god propetie. Fr instance, the in-tersection used for the overlapin a an be calculated by the formula:",
    "MinusL = | | + max (0, | |)InverseL = | | + 1/| |HingeL = max (0, + )DeviationL = | | + max (0, )OrdinalL =, , +, , +, ,OverlapEq.9": "ts train the fr 0 epochs of batchsize and ue the SGD optimizer with 0. learning rate . 01. Te hyprparameter Minus loss is st to 5, and the the Hine and Deviation lssis set . Cnistet with heoiginal pape, we set ,, , , in teOrdinal lss to 0,4, 8, respectively. first investigate different real-worl datases andthen repor ther performances indetectingvarious types ofanomalies. 4. 3 1Exploration of A Loss Fuctios on Datasets. nhis subsection e analyzedifferent loss functions real-worlddaaset with respect to the two perspctives: (i) Embed-ding he transformed embedded input features can be as visualization of represntation layer realizing he training bjective. (ii) Netwrk Canges. mbedded during Trainng. Wetake the dataset as an example t deonstrae the represenation layer during theining proess, s in. Similar experimntal resltscan observe ineal-world and we prvide Appendix of supplmentary indi-catetat thDviation and Ordinal lss tend seriously distortthe embedded f oiginal data ter a few training epochs. This is due to the fac that these two loss functins explicitly to map the anomaly score of each or pair to one more fixed score constans or a margin, thus hindering thediversity of feature representation unlabelednormal ar contaminated y the anomalies, anddefining identical trget for two typs of limit aiity of te learning models. Our proposed Overlap loss rlatively mild trnsformation of theinput features. Parameter Change. result corre-sponds to thepropertis of where (i)boundd, drastic updating anolyscores (and network parameters). (ii)Overlap loss doesnot prior target core, therefore reducing unncessarycoed funtio in te trainin stage being adped differet daasesminimum adjuten of thescore distribution. MinusInverseHingeDeviation 5 1. 5 2. 5 3."
}