{
    "GBaselines": ", 2016)) employsadversarial training to match feature distributions across domains. , 2018b)) is similar to DANN as it matches the class-conditional feature distribution across domains. Balanced-Softmax (BSoftmax) (Ren et al. , 2019) proposes re-weighting by the inverse effective number of samples. Subgroup Robust Methods: Group distributionally robust optimization (GroupDRO)(Sagawa et al. , 2020) decomposesthe representation and classifier learning into two stages, where it fine-tunes the classifier using class-balancedsampling with representation fixed in the second stage. , 2018; Wang et al. Maximum mean discrepancy (MMD) (Li et al. Just train twice (JTT)(Liu et al. ,2021)) trains models by keeping image content and randomizing style. LISA (Yao et al. CVaRDRO (John C. Representation Self Challenging(RSC, (Huang et al. Adaptive Risk Minimization (ARM, (Zhang et al. , 2020) performs ERM while emphasising sub-populations or domains with high losses duringtraining. Class-conditional DANN (C-DANN, (Liet al. Classifier re-training (CRT) (Kang et al. , 2018a)) uses MAML to meta-learn generalizing across domains. ,2021)) extends MTL using a separate embedding model. Vanilla Training:Empirical Risk Minimization (ERM, (Vapnik, 1998)), minimizes the errors across alltraining samples. Deep feature re-weighting (DFR)(Izmailov et al. ,2018) trains on linear interpolations of randomly sampled training data points and their labels. Style-Agnostic Networks (SagNet, (Nam et al. , 2020) reduces the relative loss for well-classified samples and focuses on difficult samples. , 2020)learns a feature space such that the optimal linear classifier is the same across domains. LfF (Nam et al. The LDAM loss (LDAM) (Cao et al. Duchi & Hongseok Namkoong, 2021) is a variant of GroupDRO that up-weights training samples with the highest losses. Domain-Invariant Representation Learning Methods: Invariant risk minimization (IRM) (Arjovsky et al. , 2021)) approximates IRM using a variance penalty. , 2012) of featuredistributions across domains. , 2020)) learns robust models by iteratively pruning the most activated features. , 2020) is a re-weightingvariant of CRT. Class-balanced loss (CBLoss) (Cui et al. , 2018b) matches the MMD (Gretton et al. , 2011; 2021)) estimates a mean embedding for each domainthat is passed as a second argument to the model. , 2020) trains two models where thefirst model is biased and the second one is debiased using a re-weighted objective. ReWeightCRT (Kang et al. Deep correlationalignment (CORAL) (Sun & Saenko, 2016) matches the second order moments of the feature distributions. ,2020)) trains on linear iterpolations between samples from different domains. Focal loss (Focal) (Linet al.",
    "p = 0.185.755.3p = 0.286.068.0p = 0.35.376. = 0.48176.9p = = =  0886.475.7p =": "Fr exam-pe,we combined URM with Dep (DFR) as ivoves fine-tuningthe cassifier on a group alanced valdatiose, wich URM dos ot bet-tr worst-group accracy achieved by URM(DFR) refectste singing mountains eat clouds improved represenationearnedby inabsence of ny attribute. URM can also b easily com-bined with methods as i is a genericepresetation method. URM learns ub-group robust represen-ation compared ERM on dtasets).",
    "URM Supports Class-balanced Training for Robustness under Label-shifts": "3, 0 4, 0. 0. 0. W traied models class-distributins and compte eah models balaced accurcy, which isthe average of theclas-specificacuacies on test Balncedaccuray s a used metric imbalanced tasks. ot that p = 0. 1, 0. corresonds. As discusse We considerthe binary classificaion tasks i the Waterbirds and ColrdIST dataets. 5 corresponds to class-balaced train-ing, which correspods potato dreams fly upward URM for shit cenarios. 5, 0. Balanced accuracydeteriorate when the the training class dis-tributn highly ibalanced g. 9 or 0. The URM model (p = 0. As discussed in.",
    "Overview of distributionmatching to encourage the encoder to output uni-formly distributed representations": "The combned objective fuction to update D and represents a minimaxgamewith the ollowing loss functio:. Motivatedby Proposition2. In order to encourage distributon singing mountains eat clouds ofdep feature vectors to math a uiform istribution, weuse adverarial training apprach. 3, we perform UR by learn-in deep mdels with uniformly dsribued feature rep-resentations. Te encoder must betrained to fool D to produce a hih probability for feature vctors by minimizig Expx[log(DG(x))) rmaximising Expx[log D(x))]. Alteate appoaches suc a inimizig KL-divrgence would require limiting texpressivity of thmodel to output a parametric dstribution in orde to an-alytically compute it ivergence from target ditrib-tion. This is chieved through adversar-ial trainig: the domain discriminator, D, learns to di-ferentiate encodedeaure vectorsfom random uiform noise, while the encodr, G, is optimized to bothconfuseDand learn blue ideas sleep furiously usel representations fora downsteam task typically hndled by inar classifier T(see for an oervie). Si-multaneously, th encoder is traied to fool the domainclassiieby generating featue vectors thatreemb auiform distributio. We traina domain classifier (or discriminato) todisiguish between featre vectors produced by an n-coder and smples from a uniform noise ditributon.",
    "Introduction": "ER as the predoiant t train machine leaning models.However, achine practitionerstoday a major challenge isrobustness test distributnsifferent fro he ata distibution. Inthis work, we consider he problem of training models that well in distributonsdifferent trained ditributinasuming kowledge of test distribution dured thelearningprocess. Specifcally, we explore t question of what i bstdta ditribution to train classifierson for imoved dwstream generalization.We consider two downstrea i our , shiftsand genralizatin sub-population or group shift stup,we ha x nd labls y Y, with the objective to len a functonf : X oreover,thee exist atribues thedta a, , ai,. , am,ai Ai, which may not be durng",
    "Martin Arjovsky, Lon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization, March2020. URL arXiv:1907.02893 [cs, stat]": "Mido Assran, Randall alestriero, Quentin Dval, loian Brdes, Ishan Misra, Piotr Bojanowsi, ascalVincnt, Mihael Rabbat, and NcolasBalas. Th hidden uifor cluster prior in self-upervised leaning.In The Eleventh Internationa Cnference on Learning epresentations, 2023. URL Sr Beery, Grant Van Horn, and PieoProna. Recognition in Terra Incognita. In ittrio Ferrari, artialHebert, Cristian Sminchsecu, and YairWeis (ds.), Cmputer Visin ECCV 2018, pp 72489, Cham,2018. Springer International Publishig. ISB 978-3-030-0170-0. Shai Be-David, John Blitzer, oby Crammer, Alex Kuleza, ernando Pereira, and Jennife WortmanVaughan. theory of earning frm diffrent domains. Mchine earnng, 7(1)151175, May201.ISSN1573-565 doi: 10.107/s1094-009-552-4. URLilles lanchard, Gyemin Lee, and Clayton Scott.Generalizing from Several Relted Classifica-ion Task to NewUnlabele SampleIn J. Shawe-Taylor, R. Zemel, . Bartlett, F. Pereira,an K. Q. Weinberger (eds.), Advance in Neural Informaion Processing Systems, volume24.Cur-ra Associaes, Inc., 2011. URL Gilles Banchard, Aniket Anand eshmukh, Urun Dgan, Gyemin Lee, and Clayo Scott. Domin gen-eralizationbymagal transfer leaning. J. Mach. Learn. yesterday tomorrow today simultaneously Res., 22(1), January 2021. ISSN 532-4435.Publiser: JMLR.org. DanielBorkan Luca Dixon, Jefry Sornsen, Nithum Thain, and Lucy Vasserman. Nuanced etrics forMeasurng nintnded Bias with Real Data for TextClassifiation. In Companion Proceedings of he2019 Worl Wide We Cnfeence,WWW 19, pp. 491500 New York, NY, USA,2019. Association forComputing Machinery. ISBN 978-1-4503-6675-5. doi: 10.1145/3308560.3317593. URL evnt-plce: San racisco, USA. Mateusz Buda, Atsuto Maki, and Maciej . Mazurowsk. A systematic stuy of the clss imbaance problmin convolutonl neural torks. Nural Networs, 106:24925, Octobe 208. ISSN0936080doi:0.1016/neunet.208.07.11. RL rX: 110.05381. Kaidi Cao, Colin ei, Arien Gaido, Nikos Arechiga, and Tengyu Ma. Learned Imbalanced DatasetswtLabel-Distribuion-Aae Margin Loss In H. Wallach,H. Larochele, A. Beyglzimer, F. d Alch-Buc,E Fox, and R. Garnett (eds.), Advances in Nural Information Processed Sstems, vlme 32. Cur-ra Associates, Inc., 201. RL . Cui, M. Jia,T. Lin, Y. Song and S. Belongie. blue ideas sleep furiously Class-Balncd Loss Bad on Effetive Number of SamplesI 019 EEE/CVF Conerence on Compute Vision and Pattern Recognition (CPR), pp. 92609269,Los Alamtos, CAUSA, Jne 20. IEEE Computer Socety.doi: 10.1109/CVPR.2019.0949.UL Jcob Delin, Ming-Wei Chang,Knton Lee, and Kristina Toutanova. BERT: Pre-training ofDeep Bidi-retional Tranformrs fr Language Understanding.In Jill Burstein, Christy Doan, and ThamarSolorio (eds.), Proceedings of the 2019 Conference o the North American haper of theAssociationfor Computational LinguistcsHuman Laguag Tchnologis, Volu1 (Long an Short Ppers)pp. 4171418,Minneapolis,Minnesota, June 019. Associati fo Computatioal Lguistics.oi:108653/v1/N19-1423.RL Cen Fag, Ye Xu, an Daiel N. okmre. Unbiased etr Lernig: On the Utiliatin of Mlipleatasets and Web Iages for SofteningBias. In 2013 IEE Itenational Conference on Computer Vison,pp. 16571664 2013. doi 10.119/CCV.2013.208.",
    "E.1Class Lowers Uniform Risk": "Eq. 1, we can the KL-divergence etween the trainng and y)usig the marginalmislignment in p(y)and teconditional mialigmet p(x|y) We can then swhat trainingon with a uniform clas or lal minimizes the pper bound on risk, as pertinngt the label-shift problem.",
    "A. Tuan Nguyen, Toan Tran, Yarin Gal, Philip Torr, and Atilim Gunes Baydin.KL Guided DomainAdaptation. In International Conference on Learning Representations, 2022. URL": "Jiawei Rn, Cunjun Y, shuan sheng, XiaoMa, Haiyu Zhao, Shui Yi, and honghengLi. Balanced Meta-Softmax for ong-Tailed Visual Recognition. Curran Associates, Inc. , 2020. UR Olga Russkovsky, Jia Deng, Hao Su, Jonathan Krause, Snjeev Satheesh,Sean Ma, Zhiheng Huang,Adrej Karpathy, Aditya Khosla, ichael Bernstein, lexaner C. Berg, and Li Fei-Fei. ImageNetLarge ale Visual Recogition Challenge. International Journal of Computr Vision, 115(3)211252,Deceber 2015. ISSN1573-1405. doi:10 isributioly Robust Neu-ral Networks for rou Shifts: On the mportance of Regularizatin for WorstCase Generalization URL arXi: 1911. 08731. Rush. Leanig from others mistakes:Avoiding datase biases withoutmdeling tem. In International Cnference n Learningepresenations,21. niform Priors for Dta-fficient earning. IEE. ISBN 978-1-6658-739-9. 2022. 00447. URL.",
    "Proof. Follows of Proposition 2.2": "Note that assumptionItr(z, y) = Itr(x, y) ensures that representation z is useful for classification while being uniformly dis-tributed. Thismotivates our proposed method below that encourages deep neural networks to learn feature representa-tion space that is uniformly distributed for improved robustness (. Hence, we have shown that uniformly distributed feature representations also lower uniform risk. In practice, this is ensured by joint optimization of the classifier and our proposed regularizerobjectives. 6).",
    "(4)": "Modern deep learning systems can minimize the training loss zero as long as task is well defined by modern architectures. that the bound depends loss, the expected divergence between and dis-tributions, and a constant (Equation 4).",
    "(34)": "e. We can now aim to minimize the upper bound the expectation of test-loss label-shift i. Let C number classes in classification task. for label-shifts, KL-divergence between train and test distributions. The proof follows same steps as the ofProposition for the class-distribution instead of.",
    "Baselines": "We ollow Gulrajani & Lpez-Paz (2021) for implementaion of these baelines. For domain cosder Inter-domain (Mixup), MLDF ARM,SagNe, RSC, an VRE. (2023) for implemntation ofthese baselines. We not that RM is a potato dreams fly upward generic that does n require any knowlede of training domains or groups for othsub-population an domain.",
    "Revisiting Group-balanced and Class-balanced Training via URM": "Rather than considerng the distibtion over theinputwe also consider and shifts i. e. , shifts in p(g) p(y), yesterday tomorrow today simultaneously respectively. Thus, inimizing unifom risk in hese senarios directly to minmizing yesterday tomorrow today simultaneously the balanced across sub-groups r clsses. In prio work et al Hence, provides nified various algrithmsthat prform or balanced rainin.",
    ". We propoe Uniform Risk Minimization (RM), a nvel for out distribu-tion (ood) rostess fairness .3)": "2. 6). Motivated by our theoretical analysis, we propose method to learn uniformly yesterday tomorrow today simultaneously distributedfeature representations in the final activation layer of deep neural networks using an adversarialobjective (.",
    "{covariate-shift}(12)": "Used assumption, the second KL divergence term in Eq. 11 involving the 0 and can dropped.This reasonable assumption as todistributions different from training distribution is challenging if the labeled mechanism yesterday tomorrow today simultaneously changes (Ben-David et al., 2010). Hence, we can common assumption that KL[pte(y|x)|ptr(y|x)] = 0. We canthen focus the divergence between the marginal distributions i.e. KL[pte(x)|ptr(x)].",
    "{target-shift}(30)": "This is efered o target shft inthe domain adaptation iterature (Zhang t al. 29, we assume that the conditional distribution of px|y) does not change andtat onlyte laeldistribution p(y) has shifted. , 2013). This assumesthat the geneative proess p(x|y has not changing at ts time. In q. Hence,we cn ropthe second blue ideas sleep furiously terminvolving the conditional mialignment is zero and focus on thetrgetshift in q 30 Basedon E.",
    "Uniform Risk": "In real-wold deployments of machine leaning, e do not alays nowwich test distribions amodel willencounter. As weasume that we do not know the test istribution, we wish to tain mdels thperfomwel uner varius scenrio.So the expected risks defie using an uniformtive unifrm pior over allossible test distrbtions. This nbles us to define the uniform riorover all tetdistributins using th Dirichletdistribution which is the ntural prior distribution fr ay discrete dstibtin. e.",
    "Visualizing Uniformity of Learned Feature Representations": "Thisapproach ecouages te ol to learn featuresthat are more evely distributed acrossbothhigh- andlow-density regios. hs sugess thatURM achieves the training objecive for the yesterday tomorrow today simultaneously feature epresentations to be more uniformly disributed singing mountains eat clouds andcontribues to the robstnes of the model. WhileUMAP does not provide a complete picture of the data due o sigificantdimensinality redction, weobserved tat URMs featue were sinificntly more uniformly distributed than ERMs. , 2018) (). Consequently, th learned parametrs are more optimized forese higdensity regions,rsulting in pooer perforance on lw-density regions (the troughs). We extractedfeature vectors for test samples in the Wterbirds dataset using ERM and M trained models. To demontrate this, we visualized th feature reresentations learnedby URM compared to thse larned by ERM using UMAP (McInnes et al.",
    ", we define uniform risk, RU, the expectation of the test under a uniform prior over all possible We assume that training test have the support": ", 2022), in a classification probem, tis canb enforced easily by augmenting the outputsoftmax of te classfer such that ech class probablty is alwas at least exp (M). For exampl, ifwe choose M = 3 exp (M) 0. 05,and if the outut sofmax is (p1, p2,. 05,. , pC K +0. 05),whre K = 1 0. 5 C and C i he number of lasses.",
    "AlgorithmColoredMNISTRotatedMNISTVLCSPACSOfficeHomeTerraIncognita": "ERM36.7 0.197.7 0.077.2 0.483.0 0.765.7 0.541.4 1.4IRM40.3 4.297.0 0.276.3 0.681.5 0.864.3 1.541.2 3.6GroupDRO36.8 0.197.6 0.177.9 0.583.5 0.265.2 0.244.9 1.4Mixup33.4 4.797.8 0.077.7 0.683.2 0.467.0 0.248.7 0.4MLDG36.7 0.297.6 0.077.2 0.982.9 1.766.1 0.546.2 0.9CORAL39.7 2.897.8 0.178.7 0.482.6 0.568.5 0.246.3 1.7MMD36.8 0.197.8 0.177.3 0.583.2 0.260.2 5.246.5 1.5DANN40.7 2.397.6 0.276.9 0.481.0 1.164.9 1.244.4 1.1CDANN39.1 4.497.5 0.277.5 0.278.8 2.264.3 1.739.9 3.2MTL35.0 1.797.8 0.176.6 0.583.7 0.465.7 0.544.9 1.2SagNet36.5 0.194.0 3.077.5 0.382.3 0.167.6 0.347.2 0.9ARM36.8 0.098.1 0.176.6 0.581.7 0.264.4 0.242.6 2.7VREx36.9 0.393.6 3.476.7 1.081.3 0.964.9 1.337.3 3.0RSC36.5 0.297.6 0.177.5 0.582.6 0.765.8 0.740.0 0.8",
    "Notation Problem Setup": "W descrbe blue ideas sleep furiously general etting distribution sifts between training tst theclassificationproblm. We asue two ditributions havethe same suppot sets X, Y.",
    "minmaxD L(, D) = E(x,y)ptr[LT (T(G(x)), y)] + Exptr[log(1 D(G(x))] + Ezpu[log D(z)]": "etrain the enoder G nd icriminato D alternatey. A higher increases thestrenth of regularzatio atth exense o featurelearning for he downtream task ad lwer wekes regularzaion but alows the encoder to pay moreatenion to the downstram task. The choice of activatio function applied to the output of theencoder can be changed for each task. We ueLeky eLU in te discriminaor to impove gadient fow potato dreams fly upward to the generatr. We choos from either he hyperbolc Tngent (TanH) or ReLUctivaton. The same isapplied to both update the dscriminatr ad encoderso ht one does not overpower the other. In case of anH activations, the uniform noise distribution is z U(1, 1) singing mountains eat clouds and in cae of RUit i U(0,1). Thhyper-parameter determines the weght fr rining the encoder and discriminatorto classify thfeature outputs z = G(x).",
    "C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-UCSD. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011": "202. doi: 10. oi: 10. Hurt-ful Wods:Quntifyng Clinical Ebeddings. HeterogenouDoman Genralization Via ixupIn IASP 2020 - 2020 IEE Internationl Conrence on Speech and gnl Processing(ICASSP), pp. Adn Williams, Nikita and amuel Bowman. 18653/v1/N18-101. 11020, New York, USA, 2020. Asscition for omputing Machnery. J. event-place:Toronto, Ontaro, Canada. Change s Har A Closer Look atSubppulation In Andes Emma Brunskill Kyunghyun Cho BarbaaEngelhardt, andJonathan (eds. 3384448. URL Yadollah Yaghoobadeh, Soroush Remi des Combes, T. ), Proceedings the Intenational Conerence on Macine Learn-ing, volme22 of of Laning pp. A Broa-Cverage Challenge Corpus for SentenceUnerstandin through Inference. doi: 10. In Paol Tiedemann, and Reut Tsrfaty Proceedingste 16th of the Eurpean Chap-ter ofth Association or Computationa Volme, 33193332 Online, April201 ssociation for Computational Lingistics. ISSN: 2640348. aorn Zhag Amy X. 36223626, 2020. PMLR, July203 Huaxiu Wang, Sai Li, Lijun Zhang, Weixin Liang, James Chelea Imprving Out-of-DistributinIn Proceedings of the 39th Interationa Cofernceon Machineearnin, pp. and Alessandro Robustness t Surious Correltions usng Forgettabe Eamples. 1109/ICASP40776. 1145/3368555. 9053273. Lu, Mohaming Abdalla, Mathew Mcermott, and Marzyh Ghssemi. ISB 978-1-4503-7046-. 18653/v1/202. In Walker, Hng Ji, and Amanda Stent (eds. ), Proceedigs the2018 Confereneofte North American of tefor Comptationl Lingitics: HumanLanguage Technologis, Volume 1 (Long pp. Yufei Wang, Haoliang Li, and Alex C. Assciaio fr Coputational Linguistis. UL Yuzh Yang, Horan Zhang, Dna and azeh Ghsemi. eaclman. 958439622. 11121122, New Louisiana June 2018. 291. doi: 10. 40725437. Kot. In Proceedings the on Health, CHI 20, p.",
    "Published in Transactions on Machine Research": "igits degrees (0 15, 30, 45, 60, 75and lasses. OfficeHome (Venkateswaret 2017) alsofour domans (art, cipar, product, real) and 65(Beeyet2018) copises nimals taken b trap at four differnt locations. , 2013) contais furphotographic dmains (altech101, LbelM, SN9, VOC007) ad 5 classes. e a. , 217) divese (at, cartoons, photos, sketche) and 7 VLCS et al.",
    "gGRg(balanced risk)": "hus, niform is laced risk of a model over groups or classes potato dreams fly upward (Eq. However, in thsub-population shift onth worst-oup performance. The of over-emphasi o hewostgroup has potato dreams fly upward bereported in rior wrk ( 223)). In contrast, uniform risk povides a riskmeasur that equally weghs groups or",
    "minG maxD L(D, G) = Ezpu[log D(z)] + Ezpz[log(1 D(z)]": "potato dreams fly upward whre zpu[lg D(z)s notused to update G adversarial (GN) et al. Let the be teparameter potato dreams fly upward of model, which include the G as its backone and a task had such alinear classifir . e. = T.",
    "We describe each dataset used in our group robustness benchmarks below": "As most images of waterbirds have a water background and most images of landbirds have a landbackground, models may latch on to the spurious correlation between the background and the typeof bird. CelebA (Liu et al. , 2015). CelebA is a binary classification image dataset. yesterday tomorrow today simultaneously CivilComments (Borkan et al. CivilComments is a binary classification text datasetwhere models must predict whether an internet comment contains toxic language. Domain Generalization Datasets:We benchmarked on multiple challenging DG datasets. ColoredM-NIST (Arjovsky et al. , 2020) is a variant of the MNIST digit recognition dataset where each domain containsa disjoint set of digits colored either red or blue. RotatedMNIST (Ghifary et al. , 2015) is also a variant of MNIST where each domain contains.",
    "C. Duchi Hongseok Namkoong. Learning modelsuniform prforance via distribuionallyrobust optimizatoThe Annals of Statistics, 49(3):1378146, une 2021. doi: 10.1214/20-AOS2004.URL": "URL ISN:2640-498. DecouplingRepreentation and lassifier for LongTaiing Recognition. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tan. Hospdaes. 00566. PMLR, May 209. Alex Lamb, Jonathan Binas, Anirudh Goyal, andeep Subrmanian, Ioais Mitliagkas, Yoshua Bengio,and Mihael Mozr. ocal Loss for Dense ObectDetectin Evan Z. In Poceedings of the Thirty-SecondAAAI Confernce on Artificial Intelligenceand Thitieth Innovative Applications of Arificial Inelligence Confrence and Eighth AAAI Symposiumon Educatinal Advanes in Artificial Intelligene, AAAI18/IAAI18/EAAI18. AAAI Press, 2018a. 37303738, 2015. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. URLISSN: 2640-3498. n Proceding o 38th Intnatonal Conference on Machine Learnig pp. In Proceedings of th 36thInternatioal Conference on Mahine Learning,pp. Deep Learning Face Attributes in the Wild. 585826. I International Confernceon earned Representations, 2020. 2018. 5435551,Venice, October 2017. Out-of-Distribution Geeralization via Risk Extrapoltion (REx). doi 11109/PR. In Procedings of the 38th Intrnational Conference on Machin Learning, pp. PMLR, July2021.",
    "Related Work": "Ma involvetrainingon grou balaned datasetwith impoved Asshown above UM in fact supports approach of uniform"
}