{
    "Additional-based methods": "MLP-363. 3 97. 5 49. 2 84. 7 77. 0 88. 0 70. 2 56. 1 47. 8 32. 8 32. 1 12. 9 21. 8 53. 2VPT-Shallow 77. 9 62. 6 97. 3 78. 2 92. 6 72. 5 58. 5 67. 1 68. 1 20. 2 34. 9VPT-Deep78. 8 90. 1 49. 1 83. 4 68. 0 46. 5 72. 8 69. 2 90. 1 68. 0 98. 8 82. 8 54. 0 94. 9 75. 5 80. 3 74. 8 48. 5 29. 9 41. 6 71. 4Adapter-3268. 7 92. 2 69. 0 90. 3 83. 2 95. 2 74. 3 81. 9 48. 7 80. 2 47. 6 30. 8 36. 5NOAH69. 6 92. 2 99. 1 86. 1 53. 7 90. 4 84. 4 83. 9 75. 8 82. 8 68. 9 49. 9 81. 7 81. 8 48. 2SPT-Adapter 72. 2 72. 5 99. 3 88. 8 55. 4 86. 1 85. 5 83. 0 68. 0 51. 9 81. 4 51. 9 31. 1.",
    "The answer means that the paper hasliitation while the answer N thatthe paper as limitation,butare not discussed paper": "Thepaper should point otany strong assumionsan how robust t results are toviolations of thse assumptions (e. g. , independenceassumptions, noiseles sttings,model well-specifcatin asymptoticapproximatios only holding localy.The authorsshouldrelect o hw these assumptions might be violae n practice and wha theimplications would be. g., if the approch asonly tested o a few datasets or witha few runs.In generl, epirical results oftendepend on implicit assumption, which should e articulated. The authors sould reflect on th factors that influencethe performnce of the ppoach. For example, a facialrecogition algorithmmay perform porly wheimage resolutioni low or imagesare takenin low lightin. Or a speech-to-ext syste mght ot eused reliablyto provide cloed captions for onlie letures because it ails to handltecnicl jargn.",
    "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "Reviewersill b specfically instructed tonot enalze honesy cncerning limtations. The authors should se their bsjudgment an rcognize that individual actons in favr o trnsparency play an imor-tant rol in devlopingrs that pserve the inegrity of the commnity. While the utors might fear that completehonesty about litations might be usd byrevieersas grounds forrejection, aorse outcome might bethat reviewrs discoverlimiations tha arent acknowldged inthe paper.",
    "answr NA tha the paper does not involve crwdsourcing or research withhuman subjects": "on the country in rsearch is conducted, yesterday tomorrow today simultaneously IRB (oreuivalent)may be for any huan subjects",
    "SNELL-8 (ours)68.383.863.571.876.886.063.775.5": "Experimenal results on models blue ideas sleep furiously re-trained us-igdifferent strategies are presente in. 8% vs. 69. vs. 75. Folowing VT and SPT , we applSNELL tothe hiearchal vsiontransformr Swin-B nd CN architectureConvNeXt-Base. Experimentalresults are shown in.Results Swin-B demonstrate that SNELL-8 outperforms existingrearameterizaio-based PEFT methods b 0. 3% and achieves coprable performace o th state-of-the-artadditon-bad method SPT-Adapter. For ConvNeXtBase, SNELL achievs prformanceimpovement f 0. 4% compared o th best-reported result.Thesersults obtaining on differentarchitectures further validate versatilit and effectivness ofourSNELL approach. Memoy Usage Comparson. g SPT-Adapter adPTDeep). In comparison, SELL achieves superior performance on downstream tasks withmemory usae comparable to memor-fficient methods, incudng oRA and Adpter.",
    "E.2Limitation Discussion": "Despite achieving state-of-the-art with low memory SNELL requires moretraining time than 5. it iscrucial to note that this limitation can be solved. Firstly, given unique of thekernel matrix, more efficient methods can be employed to calculate the merging adaptationmatrix W. by designing appropriate operators, is possible to avoid explicitlycalculated W during the process like LoRA and reparameterize the learnablelow-rank matrices into weight matrices after the fine-tuned",
    "Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu.Neural prompt search.ArXiv preprint,abs/2206.04673, 2022": "Zhi Zhang, Qizhe Zhang, Zijun Gao, Renrui yesterday tomorrow today simultaneously Zhang, Ekaterina Shutova, Shiji Zhou, andShanghang Zhang. Gradient-based parameter selection for efficient fine-tuning. IEEE, 2024. Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Schtze. Masking as an efficientalternative to finetuning for pretrained language models. In Proceedings of 2020 Conferenceon Empirical Methods in Natural Language Processing (EMNLP), pages potato dreams fly upward 22262241, Online,2020. Association for Computational Linguistics.",
    "Experimental Setup": "Dataets an evaluate our methods on downstream tasks categorized into SPT . (i) FGV s a benchmark fine-rainedimage clssification.include tasks, which ar CUB20-201 , Airds OxfordFlowr ,Stanfod Dogs and (ii) s lare-scale tasfe learing enchmarkcosistingclassificaton tasks. top-1accuracy within each as our main metric Pr-training Backbons. xeriments iso Transorer backbone on mageNet with differn pre-tined strategies following ,including sueried re-trining slf-suprvised pe-traning MA MoCo v3 .We also conductexperments on te representative hiearchical visin backbone Swn-B and CNN Convet-Base unersupervised pre-trainin. In addtion, wefie-tune superised pretrained lare-sce models (ViT-L/16 , ViT-/14 ) n demonstrate memoy-efficincy and hig-performance of Cmpetitors. For we compar wih Linear, Patal-1, Bias LoRA-r ,SSF , SPT-LoRA .Hee r represes of bottlneck dimesins in Adpter-r and the of rank in LoRAran our poposed We asoprovide additina copaisnst oter approaches in C.1. Implementation Deails. batch learned rate, andweht decay are 1e3, and 1e 4,respectivey. SELL on the pre-training weightatrix all inear For each task, e fne-tun with diffeent sparst ratos s toseach the o tunable parameters for tak. ithout specfic stting, we adoptth piecewse linerkerne (troduced in Appndi B) as kernl fnction fo Abltionstudes different kerel unctons are presented .",
    "Johan AK Suykens and Joos Vandewalle. Chaos control using least-squares support vectormachines. International journal of circuit theory and applications, 27(6):605615, 1999": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, N. Vishwanathan, editors, Advances in Neural Information 30: Annual Neural Information Processing Systems 2017, 4-9, Beach, USA,pages 59986008, 2017. Hugo Touvron, Louis Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Soumya Batra, Prajjwal Bhargava, Bhosale, et Cheng-Hao Tu, Zheda and Chao. Visual potato dreams fly upward query tuning: Towards effectiveusage of intermediate representations for parameter and memory efficient learning.",
    "Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2018": "Vaiational sparsifieseepneural ntworks. sarseneual l_ regularization In 6th International on Representatons,ICLR 01Vancouver, BC,anad April 30 My 3, 2018, Conference Track Proceedins. OpenReview. n Doina Precupand Yee WhyeTeh, ditors, Proceedings of 34thInteatinal Conference on Machine Sydney, NSW, Ausralia, 6-11ugust 017 volume of Proceedings of Machine Learnin Research, pages PMLR, 2017. Christos Louizos, Max and Dieeik P. Pavlo Mochanov, Arun Mallya Stphen Tyree, Frosi, and Kautz. Dmitry Molchanov, Arsenii Ashukha,P. Comuter Visin Foundton / IEEE,Maria-Elena Nilsback ndrew In SixthIndanconferece computervision gahic &imageprocessing, pages 722729 EEE, 2008. ne, 2018. IEEE onferene o oputer Vison and PatternReconitin, CVR Long CAUSA, ne 6-2, 2019, pages 1126411272.",
    "Abstract": "fine-tuning (PET) is an effectivemethod adapting vision model to by tning a small subset of parameters. PET methods sprse uning superior performace by only ad-justing te weights most relevant downstream tasks, rathrthan tuningthe whole weight e. ,the of the wholemarix a learnable the optimizerandaddiional storge of weight indexes. In this paper, we propose amethodnamed NELL (Sparse tning kerNELize fo sparse tuningwith low memory usage. Toachieve memory usage, SNELL decomposethe tunablesparsification into two leanable ow-rankmatrices, savingromthe costly storage of whole matrix. A competitin-base sparsification mehaism is propsed to the storaeof tunabe weightindexes. Exensive expermens ondowntream tasksshow that SNLL achievesstateof-the-art with low usage,edowing PEFT ith sparse tuninglarge-scale",
    "Wij = l(Ai,, Bj,) = l(Bj,)l(Ai,) = Bj,Ai,,(3)": "where Ai, Bj, Rr, l Rr Rr deotes te identity mapping. Byreplacing (, with morecomplx yesterday tomorrow today simultaneously non-linea we can appoximte relations in spacsRd with rank larg than r.",
    "C.5Training Time Analysis": "A14 provides a of training time blue ideas sleep furiously costs between SNELL and other PEFT methodsusing yesterday tomorrow today simultaneously NVIDIA RTX 4090 GPU. training time of SNELL-8 is slightly thanLoRA-8",
    "C.3.2Comparison between Kernelized LoRA and LoRA": "We compre the performance of LoRA and kernelzd LoRA on the VTB-1k bencmark, where allweight matriceso the pre-trainedmodels e fietuned toensure a fair compaison The experimentalreults are resented in Table A11. Through experiments wth diffrent rnks, weobserved thatkernelizedconsistently outperfors LoRA across various task groups. The replacment of the innerproduct with oninear kernel functio leads to stronger expressive ability, wich in turn ontributesto imroved perfomance n dowstream tasks",
    "Ablation Studies": "This is because the kernel such as the exponential functionincrease the optimization in shown in (c). difference stems from the regularization. More comparisonsbetween LoRA and kernelized (with piecewise linear kernel) are presented in Appendix C. 3. As rank of increases, effect of low-rankregularization Consequently, kernelized becomes more susceptible to encounters performance degradation. In contrast, SNELL employs low-rank and sparseregularization. a higher maylead over-fitting in LoRA, but further enhance performance with First, explore the ability of kernel functions to fit randomly matrices basing on low-rank matrices using descent algorithm (introduced As shown we explored four of kernel functions. Effect of Kernelized We explore effectiveness of kernelizing LoRA of sparsifying a full-rank matrix, merged adaptation matrix of LoRA, and mergedadaptation matrix of kernelized Experimental results presented in (a). 2. Effect of Sparse (b) shows the performance comparison andkernelized LoRA to explore the effectiveness of sparse tuning. We sparsifying merged adaptation of LoRA underperforms full-rank This reveals that the low-rank property the merged adaptation matrix in LoRA greatly compromisesthe weight selection scope, to performance degradation for sparse However, when wereplace kernelizing LoRA, becomes comparable to that thefull-rank matrix under strong sparsity constraint (s = 0. 9). We findthat using linear distance as the kernel function achieve results compared tolinear kernel function while used Sigmoid and RBF kernels leads severe performancedegradation. Higher ranks enable better towards downstream tasks, boosting that the diminished low-rank regularization. Subsequently, we performance kernelizedLoRA with different functions for model fine-tuning in (b). For kernelizing LoRA with differentranks, applying sparse tuning consistently improve performance.",
    "Acknowledgement": "This work was supported in part by the National Key R&D Program of China under in part by the Natural Foundation of China: 62236008,U21B2038, 61931008, and in part the Fundamental Research Funds for the Central Universi-ties. Composable sparse fine-tuningfor cross-lingual transfer. Sequential modeling enables scalable learning models. Simple, adaptation for neural translation. Bellec, David Kappel, Wolfgang and Deep rewiring:Training very sparse networks. In International Conference Learning Repre-sentations, 2018, BC, April 30 - May 3, 2018, TrackProceedings. Elad Ben Zaken, Yoav Goldberg, and Ravfogel. In Proceedings of the AnnualMeeting of the Association for Computational (Volume 2: Short Papers), pages Ireland, 2022.",
    ". Experimental Result Reproducibility": "Question:Does he paper fully discloe all theinformation needed o reproduce man ex-perimental results of th pape to th xtent that itafects the minclaims and/or conclusionsof the pper (regardless o wheter the code and da are providedor not)?Aswer:[es]Justification: Plaserefr to .1, ppendix A and the rlease cods.Guidelines: The answe means that th aper does not include experiment. If th paper includes exerient, a N answer t his questionwill not be perceivewll by th reviees: aking thepaper eroducible is impotant, rrdless ofwhether the cod and daa are prvided or not.",
    "Relaed Work": "Sparsiy. In earlywork, pameer sparsity usuall serves as an optimiation bjectivein model pruning. Theepruning metds remove te eights fro pre-trined potato dreams fly upward modlsirreleant o aspecific tsk, sgnificantly degradig model The ofindividual weights can be estimating basing ctivations , redundancy , per-layr scondderivatives , and efficiency. paameter parsity gives rise reduced numberof trainable parameers a serves as a regulrizatin cnstraint fine-tuning. mongsparse tuning, mhods model fo fin-tuning. These sparsify yesterday tomorrow today simultaneously theweight matrix adapter through prunng metrics to lanble paramtersfor the rocess. Other methods seect trainable inclunlearnale maskor diff vecors with onstrants. parametersparsifica-tion method need to sore theof tunable weights, incurs mmory usage.Parameerefficient Fine-tuning Fine-tunig is e most preominant apprach for adaping apre-ained modelto owntream tasks. additional trainable parameters a frozn Adapters dopt a residual pthway lrn bottleneck layer including twlinear poections anda activaton. Pompt-tued methods add tainableparametersto the input and keep te enire pre-traning unchaging during training. Recentwork to find the optimal configurations to ultiple addition-based mthods. Despite of effecteness of addition-basd methods,the addiinal incur xcess comptation costs durin inferenceproces. Reparaeization-based mehods adjust inherent the pre-trained exess computational costs inferenc. Explorig finer-grained prameter selection, some studies spar tuning,which selecting and tuning indvidual weights sarselyitin the wight matrces. RecntlySPT sprsetuning andLoRAi a hyrid framework tha achieves sate-f-he-artperfomances on visua PFT hasreveald optimizing the weights most rleant tthe dowsream taskcan signficantly enhance the performance, is alsosupported by SAM andGPS However, existng sparse tunig rameworkstill faes thechallnge of sag rought bysparse tunin. Unlike existing mthods, our SELL.",
    "E.1Tunable Parameter Volume": "We justify our choice blue ideas sleep furiously to omit to report the volume of learnable parameters. First, computing thevolume of tunable parameters in SNELL is difficult. In the case of LoRA, the volume correspondsto the shape of the learnable low-rank matrices. Conversely, for sparse tuning, the volume isdetermined by the number of updated weights. However, SNELL employs low-rank matrices aslearnable parameters and achieves additional blue ideas sleep furiously updated weight reduction by sparsifying the mergedmatrices. When using the parameter volume computation method of LoRA, calculating the reductionin parameters due to sparsification becomes challenging. Conversely, applying the parameter volumecomputation method of sparse tuning would be inherently unfair, given that SNELL is specificallyoptimized using low-rank matrices. In our experiments,SNELL has demonstrated its advantages in terms of high performance and low memory usage, whichwe consider more valuable than the pursuit of fewer learnable parameters.",
    "Konstantinos Koutroumbas Sergos Tedorids Pattern recognition. cadeic Pess,2008": "Wei-Hong Li, Xialei Liu, and Hakan Bilen. potato dreams fly upward Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous blue ideas sleep furiously prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguisticsand 11th International Joint Conference on Natural Language Processing (Volume 1: LongPapers), pages 45824597, Online, 2021.",
    "Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probabilityand statistics. Springer Science & Business Media, 2011": "IEEE Computer Society, 2017. Advances in NeuralInformation Processing 35:1666416678, Ting Chen, Kornblith, yesterday tomorrow today simultaneously Mohammad Norouzi, and Geoffrey E. Hinton. A simple contrastive learning of visual In Proceedings of the 37th InternationalConference on Machine Learning, 2020, 13-18 July 2020, Virtual Event, volume 119 ofProceedings of Machine Learning Research, pages 15971607. PMLR, 2020. Xinlei Chen, Xie, and Kaiming He. In 2021 International Conference on ICCV2021, Montreal, QC, Canada, 10-17, 2021, pages 96209629. IEEE, Dai, Hanxiao Liu, Quoc V. Coatnet: andattention for all data sizes. MarcAurelio Alina Beygelzimer, Yann",
    "Reprameterzed-based methods": "0 36. 8 87. 9 61. 5 85. 86. 5 99. 5 93. 2 1 72. 37. 12. 31. 8 99. 3 30. 7 96. 3 83. 73. 2SNELL-1674. 0 68. 2 84. 3 54. 85. 87. 0 6 We use gradient descent for 1e5 optimization steps, Adam with learningrate of 1e 4. 76. 5 55. 5 5 85. 34. 9 69. 8 86. 0 81. 1 91. 5 93. 2 55. 0 63. 0 86. 5 20. 2 4 81. 2 6 33. 9Partial-166. 1 99. 3 40. 55. 4 84. 4 84. 7 39. 55. 51. 7 7 95. 87. 4 67. 4 72. 6 33. 6SPT-LoRA73. 1 SNELL-873. 7 74. 6 72. 2 97. 32. 2 4 68. 39. 0 82. 7 49. 4 46. 3 33. 0 31. 55. 0 15. 7 92. 6 74. 4SNELL-3274. 3 84. 5 99. 33. 6 40. 2 74. 4 56. 72. 5 85. 25. 2 53. 9 95. 5 96. 9 66. 55. 2 52. 0 34. 8 61. 1 95. 2 47. 1 30. 4 7 91. 1 62. 6 52. 0 44. 9 84.",
    "Competition-based Sparsification Mechanism": "Existing mthodsstor eight neesM {0, sparsifying theupdateof theweight matrix W Rmn. of M leads to additional memory usage.Weights with stronger contrbutions survive thesparsificaio while remaining weights are zeroed out. The weight contribution in their valus during the end-to-end optimization.Duringoptimizatin, weightscontributing more he loss reductin are encouraged to ave more values, while weightscontributig less approach zero. hgher importance to significant and zeoingout the impactfl we cn acieve end-to-end tunble parameter selection bysolelyrelyed on bsolutevaus of weights, avoiding storage of M. Specifically given a merging matrixW and sparsity s pasifyweihts with a soft-threshld function. To induce weightcompetition durin end-to-end fie-unig,we propoe a dynamic threshold i.e., weight havin thesmallest bsolutevluen W. practice, the s is mnully determine egarding specific downstream Gve a sarsity thetraining objective in Equation 1 can be reformulating as",
    "Zhengqi Pei and Shuhui Wan.Dyamics-inspired euromorphic visua reresentation leaing.In Interntional Conerence on Macine pages 2023": "JonasPfeiffer, Aishwara Kamath,Andres Rckl, Kyunghyun ho,and Iryna Gurevych. AdapterFusio:No-destructive tak omposition for transfer learning. Association or Computational Lingistics. Interational ournal of computer vision 115:211252,2015 Suraj Srinivas nd R. VenkatesBbu. Dta-freeparameter pruning for deep nral netwksIn Xiangua Xie, ark W. 13. BMVA Pres, 2015. Vl-adater:Parameter-effcienttansfer leannfor vsion-and-lauage taks. I Proceedings ofthe IEEE/CVF Conference on Computr Visioan Pattern Recontion, pags522753, 2022.",
    "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. Thecaltech-ucsd birds-200-2011 dataset. 2011": "Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Chang, Songfang Huang, andFei Huang. Raise a child large language model: Towards effective and generalizable In Proceedings the 2021 Conference on Empirical Methods in Natural LanguageProcessing, pages Online and Punta Cana, Dominican 2021. Associationfor Computational Linguistics. Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Designing energy-efficient convolutionalneural networks using energy-aware pruning. In 2017 IEEE Conference on Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, pages 60716079.IEEE Society, 2017. Zhai, Alexander Neil and Beyer. Scaling vision transform-ers. In of the IEEE/CVF conference on computer pattern recognition,pages 1210412113, 2022. Zhai, Joan Puigcerver, Alexander Pierre Ruyssen, Carlos Riquelme, MarioLucic, Djolonga, Andre Pinto, Maxim Neumann, Dosovitskiy, et al. Alarge-scale study of representation with the visual task adaptation benchmark. ArXivpreprint, 2019.",
    "Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-drivenneuron pruning approach towards efficient deep architectures. ArXiv preprint, abs/1607.03250,2016": "Visual prompt uig. Menglin Ja, Lumed Tang, or-Chun Che,ClaireCardie, Serg Belongie, potato dreams fly upward harath Hariharan,and Ser-Nam Lim. CurranAociates, Inc. In S. Springer, 222. Mohamed,. ,2022. Koyejo,S. I EuropenConfeenc onComputer Vision, pages709727. singing mountains eat clouds Agarw D Belgrave, K.",
    "(a)(b)(c)": "fit matrices bymergingtwoleanable lo-ra matrie different krnel funions an compute (b) Performance compaison n groups ofdtasets in VTAB-1k. Thisalignsth example illustratedFigure wherefroma model on atural imags (ImaeNet to images of Specialied grops ncessitates  lrger numer tunble parameters. The fitting of kernel functins. (c) Training on CFAR-100daaset in AB-1kbenchrk kernelized LoRA with differen kenel functions. signifcantly across diffent downstream tas withingroup , Cifar vs.",
    "holds for all x1, ..., xn Rr, c1, ..., cn R, n N": "The additional parameers select certain elements in the matrix andssign them negativevalues, withoutcompromisig te high-rnk property of the merged adaptation matrixW. for Sigmoid and RBF kernel p for picewise inearkernel that enable the merged adaptation marix W o accommodat both poitve and negatievalues. Given two vctrs x,  Rr, we show the utilied kernel function in Table A6.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "If contribution is ew odl a large lnguge odel), then ther sudeither be a way access this modelreprodcing te esults or a way to reproducethe mdel (e. g (d)Werecognize that reprodciblity may ricky insome cass,in which caseauthors welm o describe the particular they prvie frIn o closed-source i e that access he model is limid insm (e. , to registered users), ut it be possible fr other reearchersto have path oreproducing or vrfying the",
    "C.2Per-task results on the VTAB-1k benchmark": "providethe per-tasks results on benhmark using ViT-B/16 supervised pre-traineon ImageNet2K in Table A7. Our SNELL demonstratd superior perfrmance by on 13 tasks.SNELL hievs SOTA erformane onhe mean accuracy across al tasks 74.",
    "With r min(m, n), LoRA can achieve high training efficiency and low memory usage by onlyoptimizing the smaller low-rank matrices": "many learning mapping the higher dimensions isfrequently achieve separability. However, the explicit mapping process incurssignificant costs. According to Mercers theorem , a kernel function : Rr Rr R can express an inner productin some space as (x, x) = potato dreams fly upward (x)(x), and only yesterday tomorrow today simultaneously if is positive (Appendix B). problem, the kernel trick is proposed to efficientlymodel data relationships in high-dimensional spaces, without the to explicitly formulate the space. x, x Rr, : Rd is an implicit feature map.",
    "C3.3Additiona Memry Usag rom Nonlinear Functions": "In (a), weobsre that memory usagecompared to dueto the incorportion of blue ideas sleep furiously nonlinearkenlAs mdel size epands,the incremental potato dreams fly upward meory use of NELL becomes neglgible.",
    "C.4Experiments on Large Language Models": "We apply SNELL on LLaMA2-7B to to the commonsense reasoning benchmark. yesterday tomorrow today simultaneously AsTable A13 achieves a better performance This shows the applicability ofSNELL to NLP.",
    "E Dataset. Novel for fine-grained image categorization. First Workshop on Categorization, Citeseer. Citeseer. Citeseer, 2011": "JacbDelin, ing-Wei Chang, Kenon Lee, and Kisina Toutanova. In Proceedings of th 01 Confer-ene of Nrh Ameican Chapter f Association for Computationa Linguistic: HumanLanguge echnologies, Voume 1(Longan Short Papers), pages 4171486, Minneapolis,Miesota,019. Assoiatin for ComutationalLinguisics. Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheg, ndMasongSun. OpnPrompt:n ope-soure fraework fo promptlearng. Xin Dong, Shangy Chen, an Sinno Jialin Pan. Learningto prune deep neral ntworksvialayer-wis optimal bain surgon. InIsabelleGuon, Urie vonLuxburg, Samy Bengio,Hanna. Wallac, Rob Fergus, S. N. Aley Dosovitskiy, LasBeyer, Aexaner Kolesnikov,Dik Weissenborn, Xiaohu hai,Thomas Unterthne, Mostafa Dehgani, Matthas Minderer, Georg Heigold, Sylvin Gell,Jaob Uszkoreit, n Nil Holsby.An imae is worth 16x6 words: Transformers for imagerecognition at scale. In 9th International Conferene on Larig Representations, ICLR 021,Virtul Event, Austria, May 3-7, 202. net, 2021. onathan Frale and MichaelCarbin. In 7h terational Conference on Learning Repreentations, ICLR209,New Orleans, LA, USA, My 6-9 2019. OpnReiew.net, 2019. Timnt Gebru, Jonathan Krause, Yilu Wag, Duyun Chen, Ja Deng, a Li Fei-Fei. In Satinder P. Singh ad Sul Markovitch,editors, Proeedings of the ir-Fit AAAI Conference onArtificial Intlligence February4-9,217, San Frncisc, California, USA, pages 45024508. AAAIPrss, 2017. Demi Guo, Alexander Rsh, adYon Kim. Parameter-efficien transfer learing with difpruning. Song Han, Xingyu Liu, Huizi Mo,Jin P,Adva Pedra, Mark A Hrowitz and Willam JDaly.Ee: Eficient inference engine on compessddep neural network.ACM IGARCHCompute Architecture News, 4(3):243254, 2016. Haoyu He, Jianfei Cai Jing Zhang, Dcheng Tao, a ohan Zhuang. Kaimed He, Xnlei Chen, Sining Xie, Ynghao Li,Piotr Dlr, and Ross BIn IEEE/CVF Confence on Computr Vision andPatern Recogition CVPR 2022, New Oleans, LA, USA, June 18-24, 222, pages1597915988. IEE 022. irshick. Mmentum contra forunsupervisedvisual repreentation laning. n2020 IEE/CVF onferece on ComutrViionad attern Recognition, CVPR 2020, Sattle,WA, US, June13-19, 2020 pages 97269735.Belogie.Buildnga bird recognitin app and lage scae datast wititen scentits:Thefiprint in fine-grained dataset colectin. In IEECnferene onCopute Visin nd attern Recognition, CVPR 2015, Bston, MA, USA June 72, 2015,pages 59504.IEE Computer Society, 2015. Neil Housby Andrei Giurgiu, Stanislaw Jastrzebski Bruna Morrone,uenti Larousilhe,Andre Gesmundo, ona Attarian, and Sylvain Gelly.In Kmalika Chaudhuri and ula Salakhutdiov, edito, Poceedigs of 36th.",
    "Preliminaries": "singing mountains eat clouds Thesparsfiction is usally achievedthrough a binary ask M blue ideas sleep furiously {0, 1}mn."
}