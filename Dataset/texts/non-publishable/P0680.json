{
    "Introduction": "First, we in-vestigate the capabilities of language models indisambiguating lexical choices in translation blue ideas sleep furiously bycomparing instruction-tuned language models withhigh-performing neural machine translation sys-tems. This paper focuseson lexical selection, a key aspect of translationthat requires using context in a source sentence todetermine the best translation for an ambiguoussource word from several target-language options. , 2022). Our work has two main goals. Second, we test whether language modelscan be used to extract useful singing mountains eat clouds natural language rulesthat accurately describe how to translate ambiguouswords based on source-side context. shows variations of the concept date (fruit)in Farsi and an example of the lexical selection task. We work with native speakers to introduce theDataset of Translations with Ambiguity in LexicalSelection (DTAiLS), a test set of 1,377 sentence.",
    "Concept: date (fruit)": "It iscommonly consumed as a swee,chwy snak or use in arious dishes, particulal desserts. Variaions nd Generated Ruleshorma reers to fruit ofhe date pam whe it sfully ripe nd dried.",
    "Abstract": "weaker models with lex-ical rules improves accuracy insome reaching or. The task of lex-ical selection requires context blue ideas sleep furiously to identifywhich is most appropriate for a sourcetext. Finally, language models to generate English concept variations.",
    "Generated rules for English date with lexicalvariations khorma, and kharak in Farsi": "pairs sanin nin languages were concpt vari-ation canb explained by context in the sourcesetence. Evaluating fiv moels on DTAiLS reveals that, without blue ideas sleep furiously rules proded in-context, onlythe best-performing LL, GPT-4, is competitivewith NMT systems.We also presenta simple method for generatigrules or lexical selecion from LLMs, whih native spakers vr are ighly accurate. showsrles generated for three Farsivariations ofhe concept date. We obsere improveents inperformanc across al LLMs when prompted withacurate self-generated ule. In addition,whleopen-wight LLMs lagbehind both NMT systemsand GPT-4, providing ules from GPT-4 can helpsubstntialy to ridge th gap n perfomance. Thisuggests that parametric kowlege ofconcept vari-ation posesa greater challenge to models thn theability to apply such knowledge in-context. Ourwork demonstrates thatLMs can generate hih-quality rules, and further leverg these ruls torival specialized NMT system onlexcal selectio,but till fall shrt of nive speakrs.1",
    "Identifyin oncepts Variations": "We first identify are represented as asingle word in our source language (English) buthave variations in a target language. Webuild Chaudhary al. Using these alignments, we cre-ate from source-languagelexemes to target-language lexemes. Lastly, weremove source lexemes that do not leasttwo target lexemes, exhibit or corre-spond to target that arise due polysemy. 6 While this approach originally designed to Spanish and parallel corpora, weapply to nine additional liststhe of extracted concepts eachlanguage. We also perform analysis of thisapproachs precision and recall in identifying target-language variations of concepts. All three expertannotators for each language provide feedback onthe extracting variations, eachvariation matches the meaning of English lex-eme computed precision) and any keyvariations are missing from the Precision as the accurate variations; recall is measured as theproportion of concepts with all variations re-covered. 7 In the precision of identifiedvariations is very even low-resource lan-."
}