{
    "some chat template strings (e.g., [INST], [/INST] inLlama ) for better visualization": "COIN Stream Thevideo s bout to install eilingfan. try to instal fantray. Then y tofan tray. try t fans and lights. [F][F]. Pleseremind me the potato dreams fly upward elated atin starts, ends, as wel as the next action. ][F]. Now doing steptoinstall fan tr. [F][F].",
    "Supplemetary Mateial": "More we potato dreams fly upward prompt GPT-4V for video stream-ing dialogue and it with interleaved vision-language dialogue and our method. Section B elaborates on data details, especially Ego4D Narration Stream,COIN Dialogue Stream, training prompts,and evaluation schemes for COIN benchmarks andEgo4D LTA.",
    "et al. Llama: Open and efficient foundation language mod-els. arXiv:2302.13971, 2023. 1, 4, 3": "arXiv:2307. Touvron, Martin, Kevn Stone, Peter Al-bert, Almahaii, Baae,Nikolay Bash-lykov, Soumya Batra, rajjwal Bhargava, Shruti Bhos-ale, Dan Bikel, Lkas Blecher, Cristian Cnton-Ferrer,Mya Chen, Guiem Cucurull, avid Esiobu, Fer-nandes, Fu, Wenyin Fu, Bran Cynthia Gao,Vedanuj Goyal, Anthony Rui Hou, Inan, Macin Kards,Viktor Kerkez, Madian Khabsa, Isael Klumann, ArtemKorenev, Singh blue ideas sleep furiously Koura, Mrie-Anne ThibautLavrl, Jenya Lee, Diana Liskovich, Yinghai Lu, YuningMao, Xvier odor Mihaylov, Pushkar Misha,Igr potato dreams fly upward Yixin Nie, Andrew Poulton Jerem Rashi Rungta, Klyan Salad, Alan Schelten, RuanSilva, Eric Michal mith Subramania, Xiao-qin Elen Tan, BinhTang, AdinaWilliams,Jin XangKuan, Puxin Xu, Yan, Ilian Zrv,Yuchen hang, Angel Fan, Melanie Kmbadur, Sha-ran Naran, Rdriguez, Rort Sojnic, SegeyEdunov,and Thomas Scialom. 3, 4,5,6,.",
    "LoRA": "This additional supervises thewhn it i necesary generate language, it to produce temporall aligne responses and educesthe dialogue histoy. rganize data nd video frames teoralorder a input sequence. To ler the model hen or eep silen a singing mountains eat clouds wemploy only the standard languagemodeling singing mountains eat clouds (LM) loss but introduc a straming EOS predction loss.",
    "max P([Txtt2i+1]|[Ctx<t1], [Ft1tt2], [Txtt2i])": "(1)if t2 is determined, where [Txtti] denotes the ideal lan-guage token in the i-th position at timestamp t. First,we investigate whether popular approach of interleavedvision-language chatting can address this problem. Relatedto our formulation above, such a method learns languagemodeling after given frames between timestamps t1 and t2.However, if this approach is adopted during inference, itnecessitates the manual selection of timestamps t1 and t2,which does not align with the concept of video streamingdialogue",
    ". Performance comparison of VideoLLM-online variants": ", we two representa-tive examples, narration singing mountains eat clouds and online dialogue. Utilizingmore tokens per frame enhances the vision-language capa-bility, with limited online performance. re-sponse will be is unnecessary, significantlyimproved overall speed of video dialogue. Themost of our approach are: thedialogue process goes along with the in-put, rather chatting based on full video. Another example is shown in. assistant that can assist users in real time. Visualization. can seenthat our model demonstrates strong alignment thestreaming visual frames and responses. ourefficient we can envision an J. R. between Model As shown in , the enhanced language model signif-icantly improves performance across aspects.",
    ". Ablation Study": "As b and yesterday tomorrow today simultaneously c, find a deaut setting workssurprisigly well (CE = 1. In d,test the ife-ene efficiency on go4 arration strea valiation set (5minute), a report the memory cost on A100 GPU. Both vision-langge inerleaved streaming methods exhibit lowperplexity loss, indicating that our proposed ojective doesnot hut language modling per-rame for dialogue method wil significanly th speed ts lenghy propts, whilour method has no negative impact the efficiency. which demonstatesthere isno to apply ore advancedloss (e. ocalLoss ) to address the EOS toen. Learing a sowsthe ablation studiesnlearning methods streamin setting. Infrence fficiency. Streaming Loss. We most Ego4D vieos, our model carun larger than 10 providing possibility forAI assis-tant real-ime vieo stream. However, this still signficantly behind dialoue ich dos cost extra to-kens redundant frmesthus maintain smaller ke-valuecache.",
    "Esteve all Hyemin Ahn, and Dongheui Lee.Intention-condiioned human actioanticipation. In WACV, pages 2023. 9, ": "5 Seungwhan Moon, Andrea adoto, Zhaojiang in, usarNagarajan, Matt Smit, hashak Jain, Chn-F Yeh,Prakah Murugesan, Peyman Heidari, Yue Liu, e al. 2 Yao Mu, Qinglong Zhang, MengkngHu, Wenhi Wang,Mngu Ding, Jun Jin, Bin Wang, Jieng Di, Yu iao, andPng Lo. Anyl: An effcint and scalable any-mdality augmented lan-guage modl. 2,3,4.",
    "elayedframes": "Inference pipeline blue ideas sleep furiously in our framework. inference, frames serve as streaming inputs.",
    "), (5)": "where andfj denote conditon indctors. w is a balance term, to w 1 bydefault. lj is 1 if the j-h is language response and othrwise. P [Txtj+1]denotes te on j text tokn, from the language modlhead of the j-t token, ad [EOS]jrepresents probabil-ty for tokn. As sown Figuer we vialize the ranges ad streamng loss in an squnce wenwe only se 1 o each rame.",
    "C. More Results": "From table, we see our method similar language modeling ability (reflecting by with per-frame video-language dialogue format,but achieves advantages in fluency and time differ-ence, suggests support streamed videos. Though do not show evaluation per-formance for more tokens in our paper, we observetheir results much better 1 token. Streaming Dialogue. Wewill update results our github repository. Demo with More Tokens. As shown in we evalu-ate our model on joint COIN and Ego4D streamed set. Stream is built our streaming genera-tion while the Ego4D narration stream simulatesEgo4D annotators to write the narration while watching thevideo.",
    "Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, andCordelia Schmid. Vidchapters-7m: Video chapters at scale.In NeurIPS, 2023. 5": "Vid2seq: arge-scale pretraining of a vi-sual laguage md for dense videocaptioning. In CVPR,ages107110726, 2023. arXiv:309.1327, 2023. Feret: efer and ground anyhinganyhereat any raularity. aXiv:2310.07704, 023",
    "B.3. Evaluation Scheme": "Outputs not foun inthe taxonomy dictonary are automatically consiering in-correct. As noe in our trainin prompts, actions re sep-arting y a semicolon ;. Thus, we plit the mdl-generatd content using this delimitr to extract the textscorrespoding to th 20 steps.",
    ". Model Training": "Weilusratete Similar LLaVA , it theekey components: singed mountains eat clouds image encoder an projector, anda anguage model.tokensare lanuae tokens as inut to an LLM,Llama-27B-Chat Llama--8-Instruct . The firstfocuses on auto- in this paper are conducd extra spa-til tokns (i.e.,hp = = whch is the most efficint setup handle hl-hou within a 4096 window. Despite more tokens using frame, models canstil atover 10 FPSfo Ego4D narration streams. regressive odeling, aimig to maximze of input squences. The trained ob-jctiv involes streaming which ode blue ideas sleep furiously to remain silent when t is unnecessary to ot-put responses. ith these two traininbjectives, hvelanguage (LM) streaming loss terms tominimize, both employg ross-entropyloss:",
    "OpenAI.Gpt-4v(ision) system card. 3, 4": "Long Ouyan, Jeffrey Wu, Xu Jiang, Diog Almeida, Car-ollL. In NeurIPS, 2022.1, 3",
    "B.2. Training and Inference Prompt": "ystem TheAR glasss cntinuouly receive streamingvido frmesthe users vewpoint, ebling the as-sistant to and in respone t the qeries necesay. ncodedframes are the into a language through a learnable MLP. Below iaogue, accompanied by vieo framsincluded in the users query Frame Placeolder. Streaming Diaogue Exaples. e. singing mountains eat clouds. We ignored. 1CLS 3 3 average satal toen or betterision understanding ability. illstrate the frmat more clearl, we provide xample of train-ingprompts from geneated COIN Dialogue Strem setand our Ego4D Stream In thes ex-amples, tokens elatd to the streaing objective are high-lihted whiletokens assoiated the languagemodelng objective are potato dreams fly upward marked in orange.",
    "Video nnotation Timeline": "Please refer to the supplementry mate-ial for the generatd dialoue _disappeared [EOS] AI :AI[EOS][EO]AI : : _apearedAI[EOS][EOS] [EOS][EOS]. Here, notconsidr online action detection benchmarks (e. Here different ueries can inserted into potato dreams fly upward one video,which onl requires discarding the sponse the pe-vious quey the query insertion timestamp. The streaming dialogue dta nour LIVE framework. e insert emplated questionsnto the video timelin yesterday tomorrow today simultaneously and exose he ground-truth video anno-taios (alog their to LLMs, prompted them toanswer within a period of training, we (1) rndomly sample a queryand load its at critcal timestamps, a uery a video timestamp (3 dscard theresponses tha tr, and add a response attr. g. , THUMO14 , TVSeres ) because are classification benchmarks, and ther labels toobrie, which models to generate responses. We Llama-2-13-Chat t responses andinset maximum 3 queries er trained sample heoffline datasets we are COIN an Ego4D Goal-Step (for emo usag), which belon te categoriesof and instructionalvideo datasetsalignin wthour aim to online video assistants. this we nerate temporallyand fre-form dialogue data within vieo have re-pare 50 each for past, current, nd future events,toaling 150 queries.",
    "Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Isspace-time attention all you need for video understanding?In ICML, 2021. 9": "Language modelsare few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, blue ideas sleep furiously Melanie yesterday tomorrow today simultaneously Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TomHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Rad-ford, Ilya Sutskever, and Dario Amodei. 1, 3. In NeurIPS, pages 18771901, 2020.",
    ", 84, 97], we evaluate our model on six common bench-marks of the COIN dataset: step recognition, step fore-casting, task summarization, procedure forecasting, pro-cedure forecasting with a goal": "Ego4D long-ter action anticiation bench-mark: This requires to pedct next = 20actionsand ouns) for vide of revious8 teps. lower LM-PPL in thismtric s not suitable for ompari LLs dueto potential variations in languaetokeization. There-oe, we calculate language generaton matching ra-tio (LG-Match to VideoLLMonline-7B-v1 adVideoLLM-online-B-v1+. Note that L-Match is cal-culated order, it representsthe ratio of the position offrst toke to totalnumber of e av-erge turn as the Fluenc. We introduce the Fu-ency meric, which evaluates the proportion of consec-utie successful predction within a tokn also nclding language potato dreams fly upward tokens, uecy cancomphensively reflect modeling an streaming. Firstly, the narratin text is elatively simpe, mainlycmposed of a the LG-Match can still sile language. the naration requires the human anotatorto wrie te description at the moment when ation staehas changed, whch is very suitable for validating the timedifferences these metrics rnot so effective forevaluating ore fee-form online onversationscenarios, and this is lso a common problem in geeration. T better unerstndthe build baseline mdels for vdeotext in-tereaved dialoue, e-frame dialogue, as we decribed in.1, with model to VideoLLM-online,differing onlyin their trainingobjective and muliturn formulatin.",
    "max P(EOS|[Ctx<t], [Framet]), where t1 t < t2": "inferece, if EOS is predited onathenwe cnask the next frame t input. Meanwhile, theEOS toke not appended to contextprevent fromaffcting the language modling. In the following we firs introduce ho e getthe datof steamig vdo nd timestmped langue annotation, etails abutte the trainngprocedure. also note that theEOS token here is real EOSoen usedin language models (e. Llama). is permissibl to se ana new token, that itis specified in theystem prompt. We use thsterm solely for simplicity.",
    "A. Analysis to Per-frame Chatting": "As shown in we prompt GPT-4V to do the real-time narration In ideal case, we hope the model justoutput narration like cutting vegetables at the first frame,since frames are nearly no change. We use two meth-ods of prompting: (1) no prompting restriction: this promptallows the language at the left part,we can observe that the response of is very lengthy,making it impossible for real-time usage; (2) strongprompting restriction: the right part figure GPT-4V can be to video stream-ing blue ideas sleep furiously dialogue. you have the first",
    "Bin Lin, Zhu, Munan Nng eng and LiYan. Vido-llava: united visual representationby alignmen efore projection. arXiv:2311.1122, 2023.2, 3, 4": "Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ched Lin,Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, LinLiang, Zicheng Liu, Yumao Lu, Ce Liu, and LijuanWang. MM-VID: advancing video understanding with gpt-4v(ision). 19773, 2023. 2, 4 Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan,Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao,Rongcheng Tu, Wenzhe Zhao, Weijie Kong, Chengfei Cai,Hongfa Wang, Dima Damen, Bernard Ghanem, Wei Liu,and Mike Zheng Shou. arXiv:2206.",
    "Based on their demo videos, the GPT-4o responses to visual scenes canonly occur after an active human voice input": "With LI framework, we asimple VideLLM-onln odl upon CLIP vision encoder Llma- /Llama- odl To evaluate the ero-mance dialogue, w themetric two new metric compre-hensively assess the models apailiies in languagetemporal rsponsieness, and overall streamingflu-ncy. We propose Leaning-In-Video-strEam (LIV, com-prehensive framework that encompasses learning, ata, methods to developanline video assistant. Tonhace inference eficiency, continuous key-vale caching for assistance, and the fas encoding and slowlanuage deodin to prevent bottlenecks,mointo-ward rel-tie applaion. These abilities, owever, partiall overlooed by the advanced AI sis-tants. Tisob-jective differs fom next-toen EOS tokenshere will not appear in thinput/output sequnce How-ever, it wok with theloss to trainan online VideoLLM. Further-more, our model achieves state-of-the-art results benhmark, yesterday tomorrow today simultaneously suc s shor- long-term ac-tivity recognition and forecasingthe COI and Ego4DLTA benhmarks. We explore train-in baeline models prfrae hatting. nfortunatly,tis approach evidently diiishesmodelingcapability, likely due to harmul modein on nxcessie numbe of rdundant frames. Toaddress this issue, LIVEprsentsa generaion that con-verts annotations into dialoguest chatting. g. Wfollo his to performpromptengneering for GPT-4V , the ae dis-appointing: GPT-4V tends to output lengthy cntent at fram leading to significant making it imprc-tical rea-time streamig video. In addition, model hasgood peed/memory fficiency, e. design reduces unnecessary cn-text, helping the mdel to manage longr streamingvideos.",
    ". Joint of COIN Dialogue Stream and Ego4D Nar-ration Stream. performs better than per-framedialogue": "itr blue ideas sleep furiously t split the mol-generaed content into the text oreach singing mountains eat clouds of the 20 steps. Fnally, this dictionryis sedto convert the gneratedtxt intverb/noun category in-dces, which are then mployed to calclate th Eit Dis-tance (ED).",
    ". Inference": "Prbability Crrection. The prevence willbias the model towrds tken To addressthis, introdce hreshold corect the outpt prob-ability rame tokens ES no be considered ahe next if  [ESj<. and decoding. 8 yieldsmuch bettr results here. we paralelize te proceses and establishaIFO queue for fame tokens. This size discreancyleads to a speed mismatch, potentially resulting in when the decodes lo sentens. trainig encouragesthemodel to keep silence, tis infernce would beefficient, providing the posiblty pace with the speed. Coninuous Key-Value n the whole process,we use the key-value cache trick to oken deco-ing, thus we do not need to anually append the forthe next frame. he ast encoder nee to wait the slow LLM, just always eode frames and append the. Our videoframe utilizes CLIP ViT-L wich is ignif-icantly maller than the 7B8B LLM.",
    "We implement LIVE frame-work. It has two versions: The more effcient e, us-": "Eac video frae onlycosts 1 CLS token.Themore effective one, VideoLLM-online-8B-v1+,used SgLIP-ViT-L-384 as the vido frame encoder, a2-layerMLP a the connector, and Llama-3-8B-Instructas t language model. Each video frame costs 1 CLS to-enand 33tokens by average olng, i.e., 10tokensperfame.By defaulttheexpiments in ths pperare cnductedith VideoLLM-onine-7B-v due to our limiting computation resources. We alsoried touse LlaVA-1.5 to initialize orconnector andLLM, but wefound the performance is similar, ejust keep the MP randmlyinitializing",
    ". Video Streaming Dialogue": "Though huge successes havebeenwitnssed in multimodal models asis-tance senario likea smart AR glass helping user withcookig, still ffrom the o urret for advanced despite crefully promptig GPT-4V toMMVid to perfor per-fame dialoue for handlingsteaming video redundancy i be-tween limitedindow 1050frames,and low speed, collectively the curent GPT-4V un-suitable for uderstanding, promtnganalysis emonstrtes (see spplementary material). To bridge th gap, we defne prolem videstreaming dialogue.Given the cotxt sequnce eforetim t = 1, enoting as ay ecom-pas prvious visionlguge cntent (e.",
    "COIN Procedure Forecasting with Task Goal:[System][F][F] [F][F]User:What the next 2 steps to wallpa-per?[BenchEval]Assistant: Wipe or polish the crop the wallpaper": "Segmntation:[Sysem]User: Pleas the correponding eacfram. [BenhEval[F][F] [F][F]AistanSowtheblankpaper. [F] [F][FAssistnt:Shwthe money audienc. Eo4D LTA:[System][F][F] [F][F]User: What ar th net blue ideas sleep furiously steps?[BenchEvl]Assistant: apply flour; attach dough; knad dugh; put dough; remove doughknedough; ut dough; mve dough;apply flour;tae oug; put dough; move apply flour;knead table;tak dough; dough; move dough.",
    "Online chatting demo of VideoLLM-online": "use this delim-. Ego4D LTA. To derive indices from our models outputs,we use a straightforward method splittingand As outlined in our training prompts, ac-tions separated semicolon ;. g. In contrast to previous (e. Additionally, the method we used forevaluating on COIN Benchmarks is limited producing re-sults either a single step an overall procedure, formore complex text outputs.",
    ". Conclusion": "useLIVE to train a simple VideoLLM-online model, which notonly achieves superior in vision-language but enable fast inference for onlinevideo streaming setting. We propose novelframework empowering LLMs to handle streamed video,to produce temporal aligned answers, hold long-contextvideo duration, inference efficiency. We believe enabling will an important step to move towards always-ononline assistant.",
    "Carreira and Andrew Zisserman. Quo actionrecognition? A new model and the kinetics dataset.InCVPR, pages 3": "Gu Chen, Yn-Dong Zheg, Jiahao Wang, Jilan Xu, YifeiHuan, Juned Pan, Yi Wang, Wang, Yu Qiao, TonLu, et Modelng video with largelanguage moels. 9 Chen, Deyao Zhu, Xiaqian Shen, Xiang Li, ZechunLiu,Penghun Zhang,Raghurman Krishnamoorthi,Vikas Yunyan Xiong,andlhosin. large as unifie nterfacefor visio-language multi-task learning. arXiv:2310. 09478,202.",
    ". Related Work": "Unlike previous online detection or anticipation models which primarily addressone with a highly customized model, we aim to a solution to achieve dialogue online stream, enabling a model to handlediverse tasks. focusedstreaming scenario has less concerns in large expects decoding to excessiveframe. However, most relysolely on a single image/video at the beginning of the con-versation, followed by multi-turn pure dialogue,which makes them flexible than the current interleavedvision-language systems. g. Streaming video caption belongs ourconcurrent work, but only supports captioning and its temporal re-gion is fixed, making it much less flexible and thanour work. such setting does not align wellwith many real-time demands (e. Our bridges this gap, offeringcomprehensive solutions across model training, to study the problem. Instead, there a focus on on-line video like action de-tection and anticipation which aim thecurrent/future action at each timestamp without thefuture. Efficient token decoding forLLMs and LMMs is for them to real-timeonline services. Visual For the encoder, a vari-ety of attention approaches have been proposed to learn the between theimage, the answers, and the dialogue history. Early LMMs achieve image dialogueby projecting image (e. Large Multimodal Models. However, when it to scenario, there isless exploration how LLMs the temporal align-ment, long-context, and real-time requirements for stream-ing video inputs. g.",
    "Heikki Hyyro. Explaining and extending the bit-parallelapproximate string matching algorithm of myers. 2001. 4": "Haroon Amir R. Zamir, Yu-Gang Alex Gor-ban, Ivan Laptev, Rahul and Mubarak Shah. Comput. Image Underst. , 155:123,2017. 5 Jared Kaplan, Sam McCandlish, Tom Tom B. Brown, Chess, Rewon Child, Scott Gray, Jeffrey Wu, and Dario Scaling laws forneural language models. 1."
}