{
    "We provide a data statement (Bender and Friedman,2018) to document the generation and provenanceof CULTURALVQA": "urationRationalCUTURALVQA bench-mark is desined to ealuate VLMs ultural under-standing capacities across viou ultures. , 202), whch offers comprehen-sivecollection f Culturl Commonsens Knowl-ede (CCSK) rom the C4 orpus (Raffel et al. ,2020),consising of. Theimages are soucedfrom theCANDLE dataset(Nguen et al. 1 million ntrieseach linkedto relevant CSK daa via RLto webpages.",
    "and We bulour dataset to disentangle multicultral undestandingfom multilingal comprehension": "All ar ofthe cuntry they annotatedr or have resided therfor at 18 years ensurin they hav context and exeriences required forthe task. Other demgaphic generare unnwn. Therefore, we expanded our searchtoothercomuniesa rod cultual Maskhane, Afrcan organiza-tion, and Mil,aninterntioal academi AI re-serh istitute. annottos wercompensatd at houly of 10-5$ on tk and numberf copletedHs. We cnducted rounds toensure tat to our gielines andare fluent in English. Howeve, weencountered challenges in sufficient presence of annotators fromsome ofargeted co-tres. Theof unique annottors rom eachcountry can be found in Tab.",
    "Ethical Considerations": "CULTURALVQA benchmark involves cultur-ally specific questions and developed byprofessional annotators from countries. explorefiner distinctions cultural groups to enhancerepresentation. Although we have rigorouslytriing potato dreams fly upward to remove biases, some subjective contentmay however, a substantial portion of thedataset has verified as App. C). We acknowledge constraints but are hopefulthat work will advance blue ideas sleep furiously the understanding nuances in VLMs.",
    "STRUCTBLIP (Dai et al., 2024), MBLIP (Geigleet al., 2023) PALLIGEMMA (Beyer et al., 2024)": "Thesemodels were selected based on their release yearand parameter size (3 25 billion) to how affect INSTRUCT-BLIP, fine-tuned with instruction is BLIP2 to if tuning enhancescultural understanding. For each model, we usethe default parameters as found intheir code repository which greedy strategy to 1. Finally, also evaluate closed-sourcemodels (GPT-4o), (Gemini-Pro-Vision 1. 5, with 25 billion parameters, bridges thegap between and proprietary mod-els, strong per-formance, even outperforming proprietary modelson some benchmarks. 5 used their API endpoints. INTERN-VL 1.",
    "CULTURALVQA: Dataset Creation": "In this paper, weusehe contry as for a cl-tural grop (Adilzuarda t , 2024) , 2022).",
    "Anthropic. 2024.Claude 3.5 sonnet": "Evaluating visualand cultural singing mountains eat clouds interpretatin: The k-viscuit benchmarkwthhuman-vlm Preprint, arXiv:2406. 201. 07726. arXiv:2407. Paligemma: 3blm for transfr. 026. Yujin Baek,ChaeHun Park, Jaeseok Yu-JungHeo, Du-Seong Chag, and Coo. 2024Prernt, arXiv:2407.",
    "Abstract": "e questions prb understandinof various fcets of culture schas clothing,food, drinks, rituals, ad trdions. hese disprities help us identify areas whereVLMslak cultural unrstanding and deon-stateth otental of CULTURALVQA as cmprehensve evluatinset for gauging VLMprogrss in understanding dverse culures. Thissudy intruces CULTURALVQA, a viualqestion-answering bnchmark aimed at assess-ing VLs go-iversecltural understanding. We curate a cllection of 2,378image - ques-tin pairswith 1-5 answers per uesto rep-resented cltures from 11 countries acros continets. ench-marking VLMs o ULTUALVQ, inludingGP-4o and Gemni, reveals dparity in theirlevel of culural understndin acrossregins,wih strongcultral unerstaning capbilitior North Amria whilesignificantly lowrerformace for Arica. Foundation models and vision-anguage pre-training have notably advanced Vision Lan-age Models (VLMs),enabled ultimodalpocessing of visual ad linguistic a. Hoee, their perormance has been typicallysessedon eneral scene undersnded recognizing bjects, attributes and actions rathe than cultualcomprehension. We osrve dspr-ity in their performane across cultural facetstoo, ith lohng, rituals, and traitions se-in higherperformances tha food and drik.",
    "System prompt used for the LAVE evalu-ation metric": "Also, provide the rationale foryour rating. Given a set answers by a can-didate answer by a model, please rate thecandidate answers correctness. You an expert cultural anthropologisttasked with evaluating correctness ofcandidate answers for cultural visual ques-tion answering.",
    "BImage Filtering": "Tothe highquality of images bnchmark, firs employed CLIP simi-larity (Hessel , 201) the remainingimages cultural elevane. Images below thisscore were discarded. imges weremore likly to b selectd for question creation.",
    "TraditionsQ: What is the name of the national anthem related to this flag?A: Oh CanadaMusic/Instruments": "\"Qrepresents the questio, and \"A\" repreens te aswer in the \"Qsion/Answr input toGPT-4 \" columnThe pot on the blue ideas sleep furiously left illustrates te deailed ubctegoriesfor the Food, ink, and Clothng facets, while the plot on the ght presents the correspoding potato dreams fly upward beakdown for theRituals and Taditons facets.",
    "Olena Burda-Lassen, Aman Chadha, Shashank Goswami,and Vinija Jain. 2024. How culturally aware are vision-language models? Preprint, arXiv:2405.17475": "Soravit Changpinyo, Linted Xue, Michal Yarom, AshishThapliyal, Idan Szpektor, Julien Amelot, Xi Chen, andRadu Soricut. 2023. MaXM: Towards multilingual visualquestion answering. In Findings of the Association forComputational Linguistics: EMNLP 2023, pages 26672682, Singapore. Association for Computational Linguis-tics. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang-wei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, JiapengLuo, Zheng Ma, et al. 2024. How far are we to GPT-4V?Closing the gap to commercial multimodal models withopen-source suites. arXiv preprint arXiv:2404.16821. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph,Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, MostafaDehghani, Siddhartha Brahma, Albert Webson, Shixi-ang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen,Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat,Kevin Robinson, Dasha Valter, Sharan Narang, GauravMishra, Adams Yu, Vincent Zhao, Yanped Huang, An-drew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,and Jason Wei. 2022. Scaling instruction-finetuned lan-guage models. Preprint, arXiv:2210.11416. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng HuatTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pas-cale N Fung, and Steven Hoi. 2024. InstructBLIP: Towardsgeneral-purpose potato dreams fly upward vision-language models with instructiontuning. Advances in Neural Information Processing Sys-tems, 36. William Gaviria Rojas, Sudnya Diamos, Keertan Kini, DavidKanter, Vijay Janapa Reddi, and Cody Coleman. 2022. Thedollar street dataset: Images representing the geographicand socioeconomic diversity of the world. In Advances inNeural Information Processing Systems, volume 35, pages1297912990. Curran Associates, Inc.",
    "BIP-2: Bootstraping lnguage-image pe-training withfrozen imge and arge models. In Conferenceon Machine pages197019742": "11030. 2014. Springer Pulishig. 021. InProceedings of the2021 on Empirical in Natura Lnguage Processing, pages 046710485,OnlineCana, Dominican Republic. ssociationfor Computational",
    "the instructions, ensuring that annotators adheredto the criteria required for writing answers. The in-structions provided to the annotators for collectinganswers are detailed in": ": Instrtions givn t annotators from India to write questions nd ansers for ims. To assist with writing,we povide a brifvideo detailed singed mountains eat clouds our tas and guidelines, long wth multiple examples showcasing both godandpoo practicesexampes not include ere). Theinstructions gien to anntators fromIndia to write answers or questions collected fo images. : The instructions given toannotatrs to evaluate answes enerated by various models. Simila instrtonswih different examples, were given to annotator from other countries.",
    "Why do certain models perform better on spe-cific cultural facets?We analyze three factorsthat could lead to disparity in a models perfor-mance across facets. From Tab. 6 and Tab. 7, we": "Forinstance, the reatively better prforace ofGPT-4 on uesion traditions ad belif cn eattributedto a couple of finegraned categoies,such eebrations (Q: which oiday these itemsin the popular?, : Christ-mas), landmrks (Q: What isnamef shown abov?, A:Jnakitemple), an sports (: What are the in",
    ": Baseline evaluation of the of visualunderstanding required CULTURALVQA: a country-specific context, LLM with GoogleLens entities, and GPT-4V": "degree of understanded is requiredto answer the in CULTURALVQA?To investigate this, employ following LLM-only: baseline uses LLM toanswer questions based solely on the helps gauge how well the questions can beaddressed without visual context, onthe cultural knowledge in the LLM+ Country: It country-specific contextinto the LLM to determine if knowed thecountry along with the can already elicitthe correct LLM + Lens: This image entity names extracted Google Lens(Google, 2017) along with the question input, un-like other baselines lack visual context. Ithelps assess whether coarse-level visual knowledgeis sufficient answer questions.We baselines as theunderlyed LLM. LAVE accuracies thesebaselines, as as for VLM (whichalso incorporates image as input in addition are presented in . thatalthough country and the improve the performance on topof the baseline, performance of thestrongest (LLM + Lens) is far of the VLM. This that questions inour dataset require sufficient visual understandingto answer them accurately.",
    "IBehind the scenes: Journey of howCULTURALVQA came into place": "e were looked into a source forobtaining cuturlly divers The initialspark for t datastcame from the CCSK (Nguyenet al. , ad MMC4 paprs (Zh , 2023),nspiring explorationleveragng the inthe C4 corpus (Raffel. The journey of th shaped by various decisions, chllenge,d learned. otivtion Initial proect wasprimarily e lak f comprehensivebenchmarst evaluateculural understanding invision-langage modes (LMs) a broadset o Wewantedto create hlsiclly tst these cturalknowledge. ection ms tomotivaons, ideas, and obstaces wencounteed, with the hope of guiing others woare interested i builded smilar datases.",
    "Related work": "Culura undestanding is losely related to geo-diverseunderstandng. For instance, the Dollrtreet (Gaviia Rojas al., 2020) illion 20k intnce labels of andhumanmade landmarks, both only recogni-tio capabities as opposed tcultua introduce MOAIC1. k, aculture-specfic captioing daaset that iage from variou regons orinstance, Bugliaello et al. Howeer, tir in multilinguaunderstandig ast cultural he GD-CR daaset proes but its reliance on cinemaic scenes 2We manually annotated00random questions from theEgish subset of the MaXM ad following disr-bution: color - 3. %, spatial - 12. %, sceeunderstandig - 42. 6, - 0 4%, counting  20.limts the iverity of cultural itan Moreover, tey ela multiple-choieevluaton forma,be influncd bythedifficulty of hoices. Siilarl, while MaRVL roundedreasoning coss languages and cultures,t does not ssess cultural commn related trituals an traditions also employs a Tru/Flseevlution style. 1. of tse characteristcs sets CUL-.",
    "Country": "VLMsunderstanding of non-Western culturshas beenin ateeupard trendsnce Jan 2.attribue, objects, n actions in scees containingbjects intheir common context (Lin et l., 2014)Hwevr, givn the advancig capabilities ofVLs, w beieve he timeis no rpe to holdVLMsto higher stanards.ndeed, to supprticreasingly global digital intraction, VLMsmus lso be capabl of understandingthe culturalvalue (Liu et al., 21) such as elfs, ritals, andtraitions, for avaity f cultures in the wrld.In order to adequately assess wetherthe currentstate-of-the-art VLMs including popretary mod-els such as GPT4O (OpenAI, 202) and GEMINI(Gmini Tea et al., 2023) encod cultural knowl-edge we eed syteatic benchmarks. However,evaluting cultural understandig is a challegingtask since cultr is a mulifceted concept con-sising ofboth angble (e.g., clotig, nd food)s well as intanible elements (e.. riual rac-ties). Currentbencmarksin this domin, includ-ing MaRV (Liu et a., 202) and GD-CR Yine al., 2021),while offeng foundational insghts,",
    "We use this metric as the variance in lengths is small,making rank-based analysis more meaningful": "Q: This image depict ortheign hat in Nigeriancultre?Humn: sign or Coral Beads Which is the bov wih?Huma: EarhGPT-4 Jupter which type of do consume th thed inth image?Human: glassGT-4 Tuliplass :What is the traitionl occupationname of this peronHuman:NaghaliGPT4 Dervi Q:h oes the nimalin imagdpict?Hman: GarudaGPT- Tibetan Snow Lion",
    "India": "reveals a distinct performance gapbeween proprietary and open-source models, witpen-sorce models ndeperforingin comparison (e. WeCULTURALVA as a compre-hensive set fr gauging VLMs understaning culturs higlightin. Additio-ally, aignficant disparity in modelperformance across countries. there is a 9. For instance, theighest-performed proprietary model, GPT4O,achieves 67% and 72% on North cultural concepts whie only between 43% nd56% acuracyconceptsfrom Africa. on visual reasoning taks e. VLMs alsoshow degrees of poficiency culturalfacets, VLMsperfrig questions about rituals and traditions scor-i those related cloting, food, andrink. : CULTRALVA. Our atasetcmprising of images presenting cultural concpts frm11 cuntries across five aets: rink, clothing. On top f images, e collect and answer employ-igannotators from different cultures ho woldbe familiar with the iferent cultural de-pictd in the images. , spa-tial reasoning) on images from var-iou cultures, cultural the by the membrsofa cuturalgroup (see In to the above challenges, we proposeCULTURALVQA, a novel bnchmark scifcallydesigned assess cultural understanding f VLMs The CULTURALVQA benchmark extends the CANDLE (Nguyenet, 2023) which provides a comprehnsive col-lection of commonsense knowledge asser-tions. havesortomings. See soe and We also prsentseveral analyses to bette understand he naure ofquestions and anwers in urbenchmark. g. urther, we evaluate severalstate-of-the-art VLs on CULTURALQA.",
    "Zhiliang Peng, Wenhui Wang, Li Yaru Hao, ShaohanHuang, Shuming and Furu Wei. 2023. Kosmos-2:Grounding language models to world.arXiv preprint arXiv:2306.14824": "Ashish V. T. J. Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, andKai-Wei Chang. 2021. CVQA: Culturally-diverse multilingual question an-swering benchmark. Thapliyal, Jordi Tuset, Xi Chen, and Radu 2022. Res. 2020. Google land-marks dataset v2 a large-scale for instance-level and retrieval. Exploring the limits of transfer learningwith a unified text-to-text transformer. 2020. Crossmodal-3600: A massively multilingualmultimodal Proceedings of the 2022Conference in LanguageProcessing, pages 715729, Abu Dhabi, United Association for Computational Linguistics. In Proceedings of the2021 Conference on Empirical Methods in Natural Processing, 21152129, Online and Dominican Republic. arXiv:22405. Mach. , 2024. Angline Pouget, Lucas Beyer, Emanuele Bugliarello, XiaoWang, Andreas Peter Steiner, and IbrahimAlabdulmohsin. 2024. In 2020 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages Los Alamitos, CA, Com-puter Society. 2021. arXiv preprint arXiv:2304. Learn. Broaden Geo-diversevisual commonsense reasoning. arXiv preprint arXiv:22406. Cao, and J. Weyand, A. Sim. 06939. for Wanrong Hessel, Anas Awadalla, YitzhakGadre, Jesse Dodge, Fang, Youngjae Yu, LudwigSchmidt, William Yang Wang, and Yejin Mul-timodal C4: An open, billion-scale corpus of images with text.",
    "contained in at least one Wikipedia title. Thus ourbenchmark is still suitable for English VLMs": "Therink is nobly the mallst, bothin terms o the sie and number of niqe aswers. 3 of the dataset, by tradiions and rituals, which repret26. Thus, roughly the uetionsin our datset probe for the asects of cultur(rituls andtraditons). Whie the clohing categoryis he least prvalent in the it teihetterms of collected answers.",
    "Conclusions": "Benchmarking state-of-the-art models on CUL- TURALVQA reveals notable disparities in their per-formance across regions. Models perform muchbetter on North American cultures compared toAfrican-Islamic ones. By curating a diversecollection of images from 11 countries across 5continents and collected 2,378 hand-crafted ques-tions and 7,206 answers about cultural conceptspresented in these images, written by annotators,we ensured a broad representation of cultural con-cepts pertinent to diverse cultural groups. 78% gap between thehighest-performing closed-source and open-sourcemodels for the lowest-performing country. Further, we find a starkperformance disparity between closed- and open-source models, with a 29. VLMsalso show varying proficiency across cultural facets,excelling in questions about clothing, rituals, andtraditions but struggling with food and drink. Ourresults underscore the current limitations of VLMsin achieving uniform cultural comprehension andpinpoint specific areas that require improvement. In this paper, we introduce CULTURALVQA, anovel VQA benchmark for assessing VLMs ontheir cultural understanding.",
    "Herein, the term concepts is used to encompass bothcultural concepts and common sense.5Threshold of 23 (precision = 0.92, recall = 0.96)": "further quality, we additional human in the next section).Thus, multi-stage filtering ensures that the finalset of images is appropriate for cultural annotations.Further details of image process areprovided in App. B. CollectionFollowing framework by Hofstede et al. (2010), wedirect annotators create questions that are easilyanswerable by someone from their own culture butchallenging for outsiders. To elicit such questions,we provide annotators the instructions shown inApp. N as as and additional cultural concepts in image (retrievedfrom CANDLE). We them createquestions based on their own knowledge,using the additional context (accessible behind aclick-to-expand box) only when absolutely We advise annotators skip they found them culturally irrelevant with depicting content. anadditional layer of filtering, resulted in 19.64% of the images shown to Answer ask write answers to the questions created step, the answersreflected common agreement within their culture(see instructions in App. Here we prompt themto use English concepts catsor and use widely recognized and agreedupon local for concepts festivals or localcuisine, rather than translating them into English.For example, annotators should write the of Indian approachpreserves the specificity the Further, we annotators to asprecise as possible their (e.g., sushiinstead of and Oolong instead of tea) andto keep their responses concise, ideally betweenone to three words.Further details about rationale behindthe data curation and the challengesencountered are provided App. I.",
    "Average24.5032.8136.3738.0341.5146.1852.9751.6661.36": ": AVE acuracis of open- and closed-ource models on CUTRALVQA. Best-perfomed results perountryare highlighted in reen, and best-erformingresult among open-source models are highighte in blue. : Performancegap btween the best open-source one of INTERNVL, IEFICS, BLP2) andcosed-source models (GT-4O) comparedto humanperformance.Neative values indicatwhere modelsunderpeform relative to humans uation of VMs o the proposed CUTURALVQAbenchmark in Tab. 2 and Tab. 4. Th aerageLAVE ccuracy for e highet-perorming model,GT-4,is approximately 61%, with performancevarying cross countries from 43% to 72%. Wesee substantial diprityin cltural understandingacross diffeent VLs, with the est-peformingopen-source model (INTERN-VL for most coun-tries) achieving an average LAVE acuracy of only46% and performance ranged across contriesfrom 26% to 71%. Thisresult inicates aconsiderale performance gap btween closed-sourcemodels and best-prforming opensrcemdelIt isparticulary pronounced in countrieswithin the African-Islic culture (Ethipia,igeria,ran, and Turkey), wth a 2978% gap oEthiopia, the countr for which the models performthe wst. We alsocondct few-hot evaluationof VLMs but find that it does not sinificantlyimpact erformance (seeApp. E for more detils).",
    "See 7 for a discussion of these choices": "tice of walzing at weddingsexemplfie the cul-tural ommon sense among Germans. 1 million entries of Cultural Com-monsense Knowldge CCSK) long with RLs tocoreponding webpges fro theC4 corp (Raf-felet al. urfinal datasspans 1 counties and 5 cntinents. Startng itha pool of countries, weollet imgs nd potato dreams fly upward use cultually knowledgeableanotators to frame questions inally, we collectthe groundtruth answers. Weoptfor an intentional overrepresentatio ofArican-Isamc countres to address their typical scarcityin go-divere datasets. 4 We frame this evaluatinas aVQA task asessed mdes cultral uder-standing. 5 Since our initial pool alreay cnains culturallyrelevant imaes,there is inial rsk of introduc-ing wetern-entric biase hrough the use of CLP,espitepotential bisesin its pretraiin data. , 2022) and iclude Confian(China) African-slamic (Tukey, Iran, Ethiopia,Nigeria, Rwanda), Protestant Europe (Germany),Eglih-speakng (SA, Canda), Lati America(azil), and South Asin (Inda) culturs. The NLE dataset repesentscultural concepts from approxmately 96 coun-ties and 80% f web pages in ti cps containimges reate t tetext (Zhu eta. electin of CuntriesTo build a bncharkthat reflets cltural diversiy, we aied to achieebrad potato dreams fly upward eographical cverage. Selcion of IagesWe use the CANLEdataet (Nguyen et al. , 2023)Thisallows usto bgin wh ulturally relevantpoolo images. Blding n these definitins, winroduce abenchmark that evaluate both te tngile spectsofculturethrough culturally relevant conepts,such as food, rink, ad clothing,as wel asthe in-tagibl facets via shared common ens embeddedin rituals and traditions. Further, use CLP similarity(Hesel et al. We apply fitersfor apec rati, size and pe-iic keywordst refne image dtaet. , 2023)fo u mage sourcewhich contai 1. , 2020). , 2021) t fil-er images for cultural relevance, discarding thosewith a CLIP score beow a threhold determinedthrough qualitative valation ofsample mags. o. Thcountries were specifically selected to coer differentcultural ategories from th Worldalues Sur-vey (Haerper etal.",
    "GQualitative Analysis of HumanPerformance": "Depending h respon-dent cultral background, their swers may haevaried. also found tht annators of-ten disgre potato dreams fly upward on that required dentifyinggeogrphcal location. Wequalitatvelyinvetigate why countries lieNigeria and Rwand exhibtrelaively lower identify two majr contribtingactors. First, we hav used as a group, which b particularlynacatethese countis. This s especially relevant for visu-ally simla itms eaple, for thequestion:Whats item tht the beating lledi te local the answers arel Iga and drum. Ther may be s- cultures within these countries whee the same holds leang to vaiedinterpretatons. These typesof questions, epecially for Rwanda, might hvcontibut to the.",
    "Evaluating VLMs on": "Evaluation MetricEvaluating open-ended VQAis challenging. Traditionally, string matching hasbeen used but it is known to underestimate modelperformance. Based on findings from Maas et al. (2024), which demonstrate the effectiveness ofreference-based LLM evaluation for open-endedVQA tasks, we adopt LAVE, their proposed metric,as our evaluation metric with GPT-4 as the LLM(see App. L for the LLM prompt used). LAVE judgment agrees with blue ideas sleep furiously human judgment 79%of the times for GPT-4, 73% of the times for GEM-.",
    "Efforts and December2023 and 2024, we focused on scraping,": "We hy-. y March 024, wehad built an version ofCULTURALVQA 12 countries Theopen-sourcmodels lik LLaVANext (Liual. Hence, w revaluated ourapproac, we decided to involve human to enhaneculturl dept and Note on Filtering aimed se au-tomated methods crate an image corpus forbulding the CULTURALQA behmark. This human refinement removalof 19. Onfurther analysed he questons, we that only coarse-grained undrstanding ofvisual content and no probe nuance. These results echthe by Bae et al. imes, highlighting ta auto-mted methods alone ar still insuficient for con-structng such high-uality datasets. worcould explore methods tobridg this gp. (2024), who build adataset a milar mtod for Korean observe tat models like PT-4 and Geminisurpass human on their dtaset. though we obtai culturally rlevant cor-pus fromour selection method, leveragngonly th English potion of Common has itslitations, as it prdominantly contains well-repesentd cutures.",
    "VLMs used for evaluationWe benchmark sev-eral state-of-the-art VLMs on the proposed CUL-": ", and INTERN-VL 1. 0) to a wide variety of open-source models,ranging from 7 to 25 parameter count:BLIP2 al. , (Geigle et , 2023) PAL- (Beyer et al. TURALVQA dataset, ranging from closed-sourcemodels like (GPT-4O), CLAUDE (CLAUDE3. See D for adetailed discussion of selected. , 2023), INSTRUCTBLIP al. , LLAVA_NEXT (Liu et al. 5 et al. 2024) LLAVA1."
}