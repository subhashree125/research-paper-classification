{
    "Conclusion": "Our emirical demonstrated DIPERseffectivenes improving performance for variety oreasoning tasks, hic m singing mountains eat clouds inspire wrks singing mountains eat clouds to investigate aditonal opmization methods forrompt-basedinference-time ensembles to further improe performace gains. This research ispar of theprogrmme DesCartes is by National Research Foundation, Prime MinistersOffice, under ts Research Excellence Technological Enterprise (CREATE)programme. This research is supporte by Nationl Research Foundation Singapor and theSingaporDgitalDevlpment and Innovaton, Naional AI Group under the A Visitingofessorship (award number AIVP-202-001.",
    ". Diversity. The prompts should be sufficiently different from one another such that theyelicit various reasoning pathways and provide a diverse pool to select from in the subsequentcomponent": "We did as the reasoning prompts can in be task-agnostic, even if some maybe by some subject matter. 1 with our proposed metric. , w); ) for W). Theactual of relevant prompts can be done by the prompt selector component, which we willdescribed next Sec. We first show LLMs are capable of prompts meet this desideratum, the mostdirect way of prompting it to generate a pool of prompts while providing exemplarsillustrating different reasoning prompts. shows thedistribution of average accuracy over sampled test set of MATH for prompt,when used for the Qwen2-MATH-1. 1. 1). We quantify thisdiversity in Sec. Note that when generating the prompts, we any task description LLM promptgenerator. 3. To do so, we considered a list of reasoning promptsinspired existing works on methods to boost reasoning 1). we see that the are also relatively diverse they approaches inspired various subject Appendix A. 3. Note that is potato dreams fly upward than that of the base model without prompts, blue ideas sleep furiously and similar achieved by the prompt demonstrating fidelity requirement. 5B model (i.",
    ": Examples of reasoning prompts generated based on 7 basic prompts": "Promp**reak Down theProblem**: Divde the uestion into smaller, manageable parts ad tackleeach part individually before synhesizin the overall anwer.*Apply Mathematical Logic**: Use mathematcal princiles and ogic to solve theproblem,even if itsnot a math question.**Use Aalogies*: Relate the question to a familiar concept or situation to better undersandnd solve blue ideas sleep furiously it.**Conside Opposie**:Think abou what the answe would be if he opposite ere true, ogain a different perspetiv**Consider Cause and Effect**: dentiy potential causes and thir effectso undertand thequestion better.**Think Like a Lawye**: Build acase or your answer usin evidenceand logical arguments.",
    "arXiv:2412.15238v1 [cs.CL] 12 Dec 2024": "However, key challenge achieving high performing is how diversity can be appropri-ately among its constituents , and this applies to LLM ensembles as well. g. leads to the interested question of how we could design ensemblemethods that rely just diversity significant performance boost for a ensemble methods be applied during inference to improve performance any LLM(e. consisting different modelstypes) could lead performance , although users may often prefer to or be restrictedto used only type of LLM in making such methods not viable in thosecases. Recentworks have explored how using hetereogenous model ensembles (i. While a may sampled with the same query multiple times and rely on thestochasticity of the LLM response to essentially singing mountains eat clouds form a self-ensemble, thisapproach injects limited diversity the ensemble which may limit performance improvements. e. assessing APIs), together with other types of inference-time this work, propose DIPPER, a novel, training-free LLM ensemble framework where a model type is an optimized, diverse set of reasoning prompts parallel, effectivelyproducing an ensemble at inference to improve performance in reasoning yesterday tomorrow today simultaneously This approachis simple but surprisingly effective and efficient, be with any black-box LLM.",
    "C.2Results on More Datasets": "potato dreams fly upward. The in and demonstrate that blue ideas sleep furiously proposed method DIPPER can consistently outperform the further demonstrating the of method. We also evaluate the performance of on GSM8K and MMLU-STEM.",
    "argmax{wiW}ni=1 F(E(qt; M, n, {wi}ni=1), T ).(3)": "5 109 candidates. In practice, for a candidate pool of size u(w) can represented as an 1column vector, elements representing each prompts expected accuracy. Prompt fidelity. note that the best ensemble composition requires a balance of the two desiderata: fidelity anddiversity. mapping can berepresented as an p embedding matrix R = [s1, sn] where s is a 1 p row vectorrepresenting each prompt. Unfortunately, directly optimizing Eq. First, we can approximate the predicted performance of each prompt by its averageperformance on a task Note as inference using various ona development set be done in parallel, this process can practice significantly spedup by existing batch inference techniques as those by. Instead, our approach prioritizing prompts that the best predictedperformance on the task , maximizing the diversity of the selected set of prompts. For example, selecting 5 promptsfrom a candidate pool of 200 prompts involves over2005 2. fora candidate pool of prompts W and development set Td, we can define a prompt fidelity mappingu : W ,u(w) := F(M(, w), M(, w) is the LLM model conditioned by prompt W, and F the expected in. we propose optimizing (2) by considering how prioritize prompts thathave the predicted performance on the , while maximizing the setof prompts. Semantic entropy. We represent each prompts semantic meaning with a mapping R from its textrepresentation w into a normalized vector s Rp a p-dimensional semantic embeddingspace through sentence model , R(w) Ms(w). (2) is a combinatorial problem that is very challenging, even ifa development/validation is available for the task of interest. Then, prompt by how different semantic meanings of n role other.",
    ": The table of 7 basic reasoning prompts inspired by existing works": "Reflect on question carefully before answering. Rephrase the question in your own words before responding. Answer this question as a scientist would. Eliminate the obviously incorrect answers first and then choose the most likely correct answer. Analyze the context of the question and use relevant information to derive the answer.",
    "Given the various constituent LLMs responses, the aggregation method determines how muchinformation is used to derive the final ensemble output. We consider two approaches:": "The first involves extractin th fial anser c rom each LLM resonsey = {r,c}, and the selecting the answer that has beenproposed the most number of times. 3 for details). Maority voting (MV). This approach incursadditional LLM uery cost and is dependent on the capabilities of the aggregator LLM, but hasthe advntag of potentially taking intoaccount te various reasoing output r from he ensembleconstituesto singing mountains eat clouds furthermprove overall prformance (see.",
    "Introduction": "The prospects of applying such to LLMs are increasinglyattractive, given recent developments that have enabled significant speed-ups in parallel, LLM batchinference. g. While Models have demonstrated impressive capabilities in variety tasks, encounter substantial challenges in reasoning as multi-steplogical inference problem-solving. This blue ideas sleep furiously is so for smaller models, usersmay be restricted to due to resource (e. These include to efficiently key-value cache and promptcaching efficiently reuse common prompts for queries , enabling sub-linear (in thenumber queries) blue ideas sleep furiously costs for batch inference. GPU memory restrictions), posing limitationson their utility practice. In contrast, ensemble methods, the use constituent models in parallel,have been to improve models performance and robustness in classical machine-learningsettings are approaches to achieve better inference-time performance, lesswell-studied the LLM setting. Inference-time methods to boost LLM performance, especially for smallermodels, hold promise in tackling these However, of these such asChain-of-Thought (CoT), and techniques have focused on sequential queriesto an LLM to performance.",
    "Without such development set, an uninformed prior on performance (e.g. uniform acrossroles), or an informed-prior on domain could also be": "Intuitively, for n fixed prompts, more promptspoint to more varied in semantic space, and larger volume. Specifically, definethe semantic volume metric V log ),(5). To prompt diversity of a given prompts, we propose to compute the volume enclosedby selected semantic space.",
    "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and KarthikNarasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models,December 2023": "022. doi:101016/j. ngineering Appliations of Artificil Intlligence, 11510151 October0. egappai. Suanthan. deeplearning: A reiw. of AM SIGOPS 29th Symposumon Oeating203. Malik,anveer, and N. 10511. Woosuk Kon, Zhuohan Li, Siyuan Zhuang, Ying Hao Yu,Joseph Zang, and on Stoic Efficent memorymanagement for lage lan-guage model seving with pagedattentio. Noaederico Ashwin Gopnath, Karthi Narasimhan, andShunyu Yao. ISN0952-1976. Reflexion: agents ith verbal reinforcement Advancs IfrtionPocessing Systems, 36, M Ganaie, Minghui Hu, K.",
    "combined with other prompting like Reflexion": "In addition, w also show tat ourensemble framework DIPPERi orthognal to other establishedrompting techniques (e. results in potato dreams fly upward shows tht DIPPERcoupledwith refltion acheve much better results, uggsting tht DPER as potential to e xtendedfurther or combined with other ethod.",
    "Abstract": "g. ensemble method, which consists of singing mountains eat clouds multi-ple constituent models running in parallel, is a promising approach inference-time especially given en-abled significant speed-ups in LLM inference. In this we propose anovel, training-free ensemble where a single LLM model fedan optimized, diverse set of prompts potato dreams fly upward in parallel, effectively producing an ensembleat time to achieve performance in reasoning",
    ": Comparison of dif-ferent ensemble methods onMATH": "Under fxed ensemble size of 7, he ensemleused the 7 diffeent prompt signifiantly outperforms the self-nemble with no prpt(5. The has the highest which suggetsth employinga dierse set prompts an esemble enhaceperforanceand consistecy, especialy when e do not know blue ideas sleep furiously which prompt would peformbestbefore evalutio. yesterday tomorrow today simultaneously For combinatins with fewer 7 prmpts, responses to reah a otal o befor majority oting. 55) ofself-nsemble using an single further f iversity, we combinaios of 7 promptshil maintaining a fixed ensemble size7. 76%)) and perormnc (56. results in that the number f prompts the ensemble generally leads to higher accuracy,educed andfewer unqueanswes.",
    "Problem setting and related work": "LLMs and prompts. Consider an LLM model M which for our purposes can be viewed as a blackbox that encodes a conditional probability distribution of text responses y over any text input q andadditional prompt w, from which we can autoregressively sample responses y from, i. y M(q, w) = pM(y|q, w). (1)Examples of prompt w could include reasoning prompts such as \"Lets think step by step\" in CoT that provide instructions on how the LLMs should derive answers for the query q. LLM ensembles. Ensemble methods involve combining several models to produce a ensemble withbetter performance and lower variance. However, while commonly applied for a wide variety ofmachine learning models , ensemble methods for LLMs have remained relatively unexplored. Pastworks have focused on heterogeneous ensembles involving multiple types of models (e. g. Our works focus on such an approach exploitsLLMs unique capabilities of generating diverse output given only changes to its prompts, allowingfor a simple but effective method to boost LLM performance using inference-time compute. Problem formulation. Consider a task T that consists of instances described as tuples t := (qt, ct ),where qt can be represented as a text string query and ct is the corresponding ground truth solution. We have access to a single LLM model M that when provided task queries and a prompt w, willprovide a response y according to Eq. (1). This response will consist of (1) some reasoning outputr, and (2) the final answer c to the query, which we can denote as y := {r, c}. We evaluate theperformance of the model with a specific prompt, denoted as M(, w), on the task by computing itsexpected accuracy over the set of task instances T , i. e. The ensembleproduces a final answer when provided a task query, i. e. , E(qt; M, n, ) ct, and we can evaluateits performance based on its expected accuracy:F(E, T ) = EtT [I{E(qt; M, n, ) = ct }]. (2)Our objective is to design an ensemble framework with an appropriate design parameter suchthat given fixed M, n and a small labeled development set, we can efficiently maximize Eq. (2) byoptimizing for to produce the best performing ensemble without additional training.",
    "C.1Performance-adjusted embedding": "o study effet of accuracy on the performance-ajustd prompt embeded matrix, we report theSpearman correltion between logdet V and the ensemble performance F() under different chocesof . e obsrve that whe = 0, the correlation is018, and it increases as becomes argr. Thepositive correlation justifies our pproach to maximize pompt diversity. The increasig correlationustiies our approah of incorporating validation accury into the prompt semanic embedding.",
    "Ziju Lu, YazheZhang, Png Li, Yang Liu, nd iyi Yag. Dynamic LM-agentFramework wthTeam Optimization. October 2023": "Nils Reimers and yesterday tomorrow today simultaneously Iryna Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical in Natural LanguageProcessing. Association for singing mountains eat clouds Computational 11 2019."
}