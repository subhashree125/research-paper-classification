{
    "(L + Lrandom).(12)": "Lack of training stabilit: sop-gradient. When it end-t-end rining, and fG are simultaneously udated. In thiscse, observe that the * \"S gradients fro with the learnin fG, to unstabletrainin with low convergec speed. Wefid thatthis prolem can b solve by simply stopping the the inputs of.",
    "(4.6%)53.4% (4.9%)62.8% (4.6%)70.8% (4.0%)76.6% (3.0%)": "of dynamic computation strategis. variationsin lcations, scals, thrfundamental to visual Intriguingly,introducing more flexible dynamic transformations (e. of thse studies, here we furthr design a seresof more in-depth analyses, aiming to highlight the important ofthispaper, and larify novel contribuions over existingworks. Desin of the spatialcomptation mechanism. phemnon caused by th sufficiently demandsfor utilizing these iagarpn/rectification techique to pocess the task-relevantregion our prblem. g. It be oberved that model-ing any one he three types of ynamic computationsignificantly improves the cmputational fficiency of Uni-AdFocus, while the compatile with ach other inan arbitray o cmbination. ,affine homography and thnplate spline) slightly accuracy.",
    "Grat 6232100, in part b the BNRist project under and i part by Tsingha Universty-hina Mobile Groupompany, Ltd.": "2, pp. 5,o. eldjoo,M. Cremonesi, F. Nandy, T. , The yotuevdeo recommendatio syste, Proceedings of the fourh ACMconferenon Recmmendr systems, 2010, 293296. Quadran, ontent-based recommendation systembased on stylistic visual featues, Journa on Data Semantics, vol. B. Van Vleet, U. Livingston al.",
    "Ablation Studies": "Effectiveness of the techniques for training(introduced * \"S in .3.2) is validated in Onecan observe that all of significantlyimprove the across different The of them allows our method tobe trained in end-to-end Importantly, TABLE 5Performance of Uni-AdaFocus-X3D and representative efficientvideo * \"S understanding models on Kinetics-400. MN2/R18/R34/R50/ENdenotes MobileNet-V2/ResNet-18/34/50/EfficientNet. The blue textshighlight the comparisons with the baselines.",
    "AdaFocusV1: Three-stage Learning": "As foremenioned, he policy network the patches {v1 v2, However,the operation is inerently",
    "(15)": "here HW sh sze of the orgial ideo frames. In (15),we ntroduc rgulrization term [(HHtp)2+(WW tp)2]toexplicitly romote larger tp, * \"S W tp, an is a pre-definedcoeffiient to * \"S conrol trength of regularizatin. Despiteis simplicty, Eq.",
    "H.3Training Hyper-parameters": "values of hyper-parameters are determined with the following procedure: wehold out 25% of the training as a mini-validation setto search the hyper-parameters, then we put thesesamples back to train our model the our optimal hyper-parameters, with finalperformance reported on the official validation or test set. OnActivityNet, the size of the mini-batch is set to 32, and regularization co-efficient is set to 2e-4. L2 regularization co-efficient is set to 5e-5. 005, 0. All experiments on Kinetics-400 adopt training configurations. 0004, respectively. The initial learning rates of fL, fCand are set to 0. 5, 0. initial learned ratesof fG, fL, fC and are set to 0. 0025, 0. All other training settings are same as experiments onFCVID Mini-Kinetics. The on ActivityNet, FCVID Mini-Kinetics all patch use the same aforementioned trainingconfigurations. 005 0. 002,respectively. Sth-Sth and Jester. When X3D is implemented ourbackbones, we initialize fG and fL with official pre-trained provided by. with cosine learningrate annealing a momentum of is adopted. 002, 0. 75, on aper-dataset On FCVID and Mini-Kinetics, therelatively larger size data, we reduce the L2 regu-larization 1e-4, and increase the batch size to 64(the initial learned rates linearly scaled correspondingly). L2 regularization is set 1e-3 onSth-Sth and Jester, and 5e-4 on Sth-Sth V2. The value of is within 0. ActivityNet, FCVID and Mini-Kinetics. 04 and 0.",
    "rt(vt|v1, . . . , vt1)= pty(vt|v1, . . . , vt1) EvtRandomCrop(vt) [pty(vt|v1, . . . , vt1)] ,(26)": "Inourexrimens e esimate this termwiha single tme o Monte sampling. secnd termin Eq. Byintroducng it we ensuesEvt[rt] 0, which s empirically to yeld more stabletraning procedure. (26) encouragesthe mdel to sect te patches that of producingconfident orrectlabels with as fewer rames as possible. Intuitively, Eq. , vt1} have bendeterind, whileonly can be changed. imprves performance f our. , * \"S eGt ). Finally, we ine-tune fL ad fC(or fC) with learning ntwork from stage II,namely minimiing (24) ith vt (|eG1 ,. Stag III: Fine-tuning.",
    "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION12": "* \"S efficent video uderstanding models on Sth-Sth VV2 and refes to the TSM basline withsame networ architecture as ourethod for the policy ntwork. throughput is benchmarked * \"S on an3090 GPU.",
    "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION5": "Howee, this procedureis ufriendly for practitioners. It requires cnsiderble efforts for proerlydesgning he ke components (e. g. g.Second, the hreestae alternatieagorithm * \"S i an indrect formulaton or optimizing therecgnitio bjective, whihtendsto be time-consuming,a mayresult in sub-optimal slution.",
    "Network Architecture": "Withoutloss of gerality, we a onineideo cnario, where a stream f frames comein equentially whil a redictin b aftr pro-cessing any numberof frame. hen featuesae ed into a policy to aggregatethe inforaton across frames acordingly dtermine.",
    "Building on Top of Existing Efficient Backbones": "We uniormy samplT0 = 24/36/48/96 frames fromach vieo, and set TL =8/1216/32correspondingly, withTG =8/16. 5% v. 35. Fo examle or methodreucs GFOPs by 3. 1) o top of TSM+, butoupeforms itby * \"S. 596%) on Sth-th V2inerms of To-1 acuacy Comared with the recentlyproposed S2DNet famework, Uni-AdaFocs i ble tachieve considerably bettr performance (e. In this subsection, w implement Uni-AdaFocs onop of he recely roposed ffient network achtecturesConvNets with tempral shift mdule (TSM) and X3Dntworks , to emostrae ha ou method can furtherimpre the efficiency of sch state-of-the-rt ligtweghtmodels For TSM,we still se MobileNe-V2 nd ResNet-50as fG and fL, but add TSM to them. Setups. s. 99 (. Uni-aFocus enables TSM to oncentrate the ma-jorty o ompuation on themot ask-relevant video frmesan iage reions,while it allows allocating computationunevenly across eaer and more difficult videos Asa cnseuence,the overall computational efficiency durininferce is dramatically improved. % (62. For r comparison, we agmet the vanilaTSM by introducin te same two ackbone networks asours (namedas S+), were their outut featurs aealso concatenatd obe fed into a linearclassifier. Following th xperimental setings inthe originalpapers of TSM and 3D , he vie recognitiontask onSh-th V1V2, Jester an Kietics-400 is conideredher. ForX3D, we simply replace MobileNet-V/ResNet-50 by X3D-SXD-L on top of the settings of TSM Here we directly cmpare te performance of our method with X3D, sincebot our two bacbons com from thefamily of XDnetworks. g. I otherwords, TSM+ diferentiates itself from ourmethodoly ithatit feeds the unifml-sampling whole videofraesinto ResNe-50, while wefeing the dynamically-electedforatie image patches fom task-relative frames. 3-1. % using less inference cost with 2. In , w also report th actalinerence peed of our mehodon n NIDIA 3090GP,which i bnchmarked using a batch ize f 128. Folowig he originaldesg of TSM , fully-conectedlayer is deploye as theclassifier fC, and we averagethe frame-ise predictions asthe outputs. 4xfer trang epochs.",
    "t=1 LCE(pt, y),(10)": "example, the twoencoders (i. techniques donot introduce additional whileachieving consistent improvements across datasets,backbone architectures, model configurations. , fG, fL) separately a direct (by a fully-connectedlayer). In specific, weattach two linear classifiers, FCG() and FCL(), outputsof fG and and replace loss L in by L:. e. Lack of supervision: supervision. However, when solving problem (10), we notintroduce such pre-training mechanisms, which hurts theoverall training efficiency of our method. We attribute this issue to theabsence of some appealing optimization properties intro-duced training procedure, namely the input diversity and training stability. In other words, fGand fL are trained without specialized initialization, are only indirectly supervised by the gradients from theclassifier this end, that introducingauxiliary supervision fG and fL effectively facilitates theefficient end-to-end of AdaFocus.",
    "Y. Han, G. Huang, S. Song, L. Yang, H. Wang, and Y. Wang,Dynamic neural networks: A survey, IEEE Transactions on PatternAnalysis and Machine Intelligence, vol. 44, no. 11, pp. 74367456,2022": "* \"S G. Y. Rao, Z. * \"S Qi, and S. 114, 2023. Huang, W. 46054621, 2023.",
    "Easy VideosHard Videos": "One an observe that our learned poicieshave considerably eter performancecompred wth allthe baseie. xample ofthetask-relevant patces and informativvideos frames selectd by Uni-AdaFocus (zom in for detals). We assue tat Uni-AdaFocu proesse a fixed nuber offrmes fr al videos, and eport the correspondng mAPo ActiityNet. Wepresen a variety of reresentative input videos, where easy/hard videosrefer to the samples to whch Uni-AdaFocus allocats a relatively salrorlarger numer of computation resourcs. For 3), although therecently proposed MG Samler is also able to improve the accuracy, our dynmic framsamplig algorithm outperforms t by large margins. Fr an isolatedand focused reflecon of the effec of our policy with clearcomparisons, here we do not reuse the gloal feature eGtfor ecognition (its effects have been studied in. (15) isan impotant tehnique to learn efective deformable-patchpolicies. For 2), weobserve that introducig the reguarizatio term n Eq. For 1), an interesting phenomeon s thatrandom policy appears srong ad outerfrms the centralpolcy, hich mayb ttriued to the spatial similaritybetween frames.",
    "Frame Sampling": "Wehave improving earlier works substntially in whihar in A. This paper exends preious papers into-duced te AaFocus frmework nd preliminariydiscusing itsed-o-end training. Incoporated the foreentioned methodolg inno- vations, we present unifiedaFocs (Uni-AdaFocus, ee d)), holistic framewok that seamlesslyintegratesspaial, empra, n samplewise g. Most existing orks aim to reducecomputational csts by selecting a fe informative frmes to proces. Ormethodallocates unevenly acros the spatialimension video rames accoringo the contributions tote recognitio ask, to signifcant imprvemnts inefciencyapreerved accuracy. betwen emporal-basedmethodand ourapproaches. Anillustration of AdaFocu found in (c). attendingto the most valuablespatial reions of os task-relevant vdeo frames. he inferene cst f Ui-AdaFous can nline withou additinal training modiith for sample-conditioal computatio). , TSM and X3D ), whichcan b conveniently deploye s the extractor inUni-AaFocus for thei computational effiency. On to of valla AdaFocus framework, we elvedeep thoptimlof eficient satial algorithms, a further improve AaFocus inseveral imprta aspect This yiedsreducdtraining cost, impoved tet greater aessibilityfor pactitoners Secondl, we discussio on howto introuce supervson sgnals for learnint selec akrelevant regios, anaproach for traiingregio seectionpolicies. ) coceptuly elevant to, improvedupon sefor coparson). Emprically,the effectiveness of Uni-AdFocus is vaidating on seve wdely-sed datasetsapplication scenarios. local encoder roess oly theegons. Therefoe, our metis compatible with te idas oftepora-aaptive and sampleaaptive dynamcinference. Moreover, our deep-feture-basedfor training the selection policy(. In this we demonstrat that these be byintroduced a dynamic fram smplingalgoritmas well asa conditial-eit echanism.",
    "UNI-ADAFOUS: SPATIAL-TEMORAL DYNAIC FOR VIDEO RECOGNITION3": "Some preliminay studies , hvebeguto underscre potental benefits of tsapprach. Several effective lgrithms have beenproposed along his dretin. The spatial transfome networksar traned basedon ainterplation-based echanism, which is smilar to thediferentiabl pach selection ecnique in AdaFocus. ur workis reated o inthebs paradigm of frmulted frame selection as a sequentialweighted sampling probem wthout replacment, werethe ditribion is dynaicllyarameterized ondiionedon each video utilizing a poicy etrk. Ithe ralm f video undertandng, the exploiation ofspatial redndancy as a means t redce computationalcstremains a relatively unxlored area. VideoIQ pocesses videorames using diffent pecision ccorded to tei relatieimpotnce. For example,recent invetigations have revealed that 2D mages can beefficently processed via atendingto the task-elevant omore information-rich imae regions , ,. Compared to,our metho des not rely on reinforcement learningor mult-sag training, considerably redces bot the theo-retial complexity and te practicaltrinn wall-time, yetignificantly improve the prfrmance). Adaptiversolutio netork ARNet) processes different frameswit aaptiveresolutions to save unecessary cmptationon ss iportant frmes. 2). FameExit learns t conclude he iferenceroess aftr seeing a few suficienty informative frames. Many f thespaialy adaptive neworks ae designed fom the lns ofinference eficiency , , , ,. TheAdFocus newokstudied inthi apercan be classifiedinto thi category as well. However they focuson actively transfrmig eaturempsfor leaning spatially variant representations, whreas urobectives to localize an attend to te task-relevantregin of te ideo frams fo improved the computationlefficiency. For exaple, LiteEval dapively selects an SM model with apprpit size ateach time step in a recurrent recgniton rocedue. In particlar, OCSampler propose a ovel andeffctive framework that learns toset task-relevant frameswith reinocementlearing. Since not all fres areequally importat for a gien task, te model should ideallyallcte fewer computtional resours ords less nfr-mative frames. temporaledudancy within videos ,, , ,, , ,,. Morever,we demonstrate that a straghtfrwardimplementation of this mechanism fails to yield compet-itive results inour oblem.",
    "= E{v1,v2,...}LCE(SoftMax(FCAux(eGt)), y),(14)": "Howeer, comparedto  ou method provides moe effective guidace fr based on the global information within eG , introducesnegigibl coputatnl overhead duringtrainin (i. See Appendix E fo more comparisons. It should be notedthat only s updated folowing Eq. The underlying logic behind EqIn he contex of training , e employ the chnge of eG t estimat the hange of the deep fatures correspondngto vt. (14), while the tainin ofall the other components emains unchaned. Notably, Eq. Furthermore,heprocess of obtaining vt from vt and feedig vt into thelocal ecodr fL is not altere. e. That is to say, thelocation-peservin nature of deepfeatures ensues that Gt represents the contnts at he location of v. See for an illustration. (7 9). Theonly ifference is thtthegradients of are calculated using (14 rather tan bingback-propagaed from vt. (14) isconeptally reevant to AaFocusV3 in training usng dep features.",
    "Reducing Temporal Redundancy Dynamic FrameSampling": "Hence,nsteadof processingieos on a frame-by-frame basis, heewe assume the lightweigt global ecoderfG first akes a quick glance atthe whole video. Mathmatical formlation for dynamic fram samplingTo slect T tasrelevat frame fom original T0 frames,we start by fomuating a weightd sampling problem. W seek to develo a unifiedframework that considers boh spatial and tempoal redun-dancies. A illustratn of ourunified framewor is show in. Weassume that generate eightfor each o te 0 frae:{w1,. Infact, AdaFocus cn befurther mprd by educing thetemporal-wise redundancy, namely aaptivly identifyigand attending to the mot task-relevan video fames. To bespecific supose tht ideo comprises T0 frames n ttal. , nTL, which yields. In theollowing, we wilist introducehw to acquire 1), an then demonstrate that our previouslydicussed patial dynaic coputation techniques can beeffortlessly adapted for obtaining). A unified framework. We unifomly samle TG (TG < T0) frames from the video,and feed them int fG a the sametime, directly obtainingthe inexpensive and cos global features correspondingto te entir video. In this subsection, we will demonstrate that this objectiv can eachieved with a dyamc rae sampling algoritm tailoredfor our aproach.",
    "Real-world Appication Senarios": "evaluate Uni-AdaFocus thee represenativerealstic aplication scenarios: ) fine-grained diving actionclassificaton; 2) Azheimers and Parkinss i-gnosis with rain mgnetic images(MR); and3 vioence recognition nline Due tospatialimaions, these an be in Appendix D. senarios, Uni-AdaFus outperforms existig works bylarge margs, yielng  ew state-f-the-ar perormance.",
    ". (21)": "(21) has a totacomplexityof O(L(T0 + L log 0 )hich is dramatically moreefficient thansolving Eq. (21) isiffereniabe with espect to {w1,. , wT0, alowng forthe end-to-end training of.Smilar to us, OCamplr also fomulats frameselction as a video-cnditionalsequentialweighted sam-pling prolem.Wepropose to consider he expected losover dynic framesampling asthe tainingobjective, and reval that it n bedecmposed into adfferentiable form soving with the MontCalo mthd, yildingan efficient ed-t-en optimzationprocedure. ur implementation of * \"S dynic framesampling mnimizes Eq. (21) wihout nobe additional osts. See Apendix Cformore details (due to satial limiations). Unified satial-emporal dynamic computation. For example, oe my train to identfy theifrmativefame patches forthe G frames processed by fGin accordanc with 15), and interplae between he oupsfof ajcent * \"S framest acquire eormable task-reevantpatch for each ofthe Tslecting frames. I summary, theoveall training objecive of our method can be witten a.",
    "J. Jang and D. Hwang, M3t: three-dimensional medical imageclassifier using multi-plane and multi-slice transformer, in CVPR,2022, pp. 20 71820 729": "S. Figueiredo, temporalfusion for video classification with convolutional andlstm neural networks applied to violence detection, InteligenciaArtificial, vol. M. 24, no. 4050, 2021. Yoon, and -I. Suk, reasoning and its guidance to reinforce an alzheimersdisease model, IEEE Transactions on Analysisand Machine vol. de Oliveira Lima C. A. 48434857, 2023. 45, pp. Akhloufi, bidirectional gated recurrentunit convolutional neural networks for end-to-end violence detec-tion in videos, in Image and Recognition: 17th ICIAR Pvoa de Varzim, Portugal, June 2426, 2020,Proceedings, Part I, 2020, J.",
    "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION4": "the location of an mage patch to befocusedon, under thegoal of maximizing its ontributio to video recogniion. In the ollowin, we describe the four mpnentof ourmethod in details. lobal encoder fG and local encoder fL are both back-bone networks that extact dep fatures from te inputs, butwith istinct aims. Therefore, a lightweight netwok s adopted for fG. Since fLonly needs o proces a series of relativel sall regonsintead of the fullimages, this stage enjoys high efficiecyas well. Iportany, the fomulatio o fG an fLis generaland feible,i. e. Representatiexmples are iven in. Formally, given video frams {v1 v2,. } with size H, G directly takes them as inpts and produces he coarseglobal feture mapseGt :.",
    "Jester dataset consists of 148,092 videos in 27action categories. The average duration is 3 seconds": "our aim toimprovethe omputational efficiecy f te wedo not consider the setins views (. On Kinetics-400, also report the result f 3-crop prediction , ,where take 224224 crops the spatialdimensions, an aerge SoftMax scoresfor experintal settings are adopted under theconideration of ensuring efficiency. Data pre-processg. includes 240k videos fortraining ndvideos for validation. g. , 30views) which dramatically increase the inference cost. Following  , ,train-ing ata is ugmented via random scaling followed y2x224 random cropping, ater which random isperformed on except for Sth-Sth At since we consider the ifereceefficiency video recognition, we the short sideof video frames to 56and perform 224x224centre-crop,obaining a ingle clip video for evaluation. is aroud 10 seconds. Kinetics-400 is alarg-scale human ation collected fromYouTube, which conains400uman action classes.",
    ". (17)": "}[] wil lead to a complexty f O(P T0TL ), whereP TTL denots he nubrof TL-permutatios * \"S of T0 Thi inef-ficiency will make Ltemporalintractable when T is not verysmal (e. As a consequence, canotreceiveradients from Ltmporalas supervisin signals. Herein,Lfame(ni) representsthe loss correponding thenthi frame vni, which easues the quantity of he aluabletask-relevant information ecompassedby vni or a shortvideo clip centred at vni. g. ,with T>8). On th otrhand, a more efficientapproah istoestimteE{n1,n2,. }] uilzing onte Carlosamplig However, this will eliminate the diferentiabilityofLtemporalwith respect to * \"S. Calculating the exc vaueof E{n1,n2,. Te two expectations inLtempralare taen over the single-video framesampled disribuion,and the different videos withintraining data, respctively. Importantly, it is difficult t soleprblem (1) drectlywith gradient-based methos.",
    "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION13": "th ample that met the early-temnatin crierion will beoutput, wt the rmaing videos coninuig to be rocssed.It can be oservd that ou practical speedup is ignificant aswell wit a slight drop compar ith the theoreticalesults.We entatively attribute this to he inadequte hadware-oriented optimizatin in our implementation.Comparisons with the preiminar versions of AdaFo-cus on Sh-Sth are shown in . Whenachievingthe same accuracy, ni-AdaFocs reduces the computionalcost by 2.7-.5x. Furhermore, when levraging the ameamount of computatio at tst time, Ui-AdaFocu improvsth test acracy by 2-3.5% compared wth AdaocusV2.Results wih X3D on Kinetics-400. In , weimplement our mthod on top o te lightweight 3D backnes, and report its prformance on the large scaleKietics-400benchmark. For a comprehensive comprisn,we also present the esuts of representativ state-f-the-art effcient video understanding odels o Kinetcs400.Note that we ainly compre UnAdaFocus-X3D wit thesebaselies under smilar valiation ccuraciesor inferenceosts One can clearly bserve that our method contibutesto a ignificanly improved accuracefficiencyradeoff. Foreample, compared to X3DL,Uni-AdaFocus-X3D reducesthe computationl cost by376x (.9vs. 18.4), whieipv-ing th accuracy by 0.5% 76.2% v.s. 7.7%) inthe meantie.Furthermore, comared to the best baseline, MoVNet-A2,Uni-AaFocus-X3D ha .10x less GLOPs/video (4.9 vs 10.3)whe achievin te same accuracyof 75.0%.",
    "vt RandomCrop(vt).(24)": ", eGt for the fame vt (see Eq will receivea rewar whether is beneficial. Hre T and y to the lengt and th label the video {v1, v2,. Atthisstage, we fix he two encoder (fG and fL)theclassifier obtined from stage and evoke a rndominitalized policy network to e tained with Specificall, sampling a * \"S loation vt. Wetrain maximize th of discounted * \"S rewrds:. In temodel extract from anrbitrary seqence of frae patces, laying the basis fortraining the poliy network. }, repectively. Stae II: earning to informaive patce.",
    "Uni-AdaFocsOL(T0 + TL log M3.8h (.5x)77.%": "As shown in ,our method works wthout themultistagetrined reinfrcement lerning tchniques utlied whle i eliminates the factoria time complexity fthealgrihm. 74. s. Empiricall, th training time 5x et the mAP considerably v. 9%). 2) propose the expeced ossover frme samplingas th trainingobective anreveal thtit can be ecompose into diferentiale formsolved theMoteCarlo method, leading t an fficientend-toed ptimization procedure. Uni-AaFocusand frame selcton asa sequential eightedspling prolem without wheredynamially parameterzing conditioned on eah Compare , ur cntriutiolies in a novelsolvig agorithm for this basic frmulatn. ur theoreticalanalyses (. In the size of taining/test inputsof wich nt only prforance by liminting fbut lso avoids additional trained cost thesame It ca obtane wit-out etra cost, the global-level semantic informationncompassed by Gt efficcythan thelocal eatureeLt in uiin t trainin of. critical f , asin.",
    ",(25)": "where (,1) is a discount factorfor lg-term rewards. 7 and sole Eq Notaby, here we directly train on the basisof thefeatures extracted by fG, ine previos works  ave demonstrated that he vision backbones lerned forrecognition generaly excel at localizing task-rlevant regionsith teir dep representations. Ideally, the reward rt is expected to mesure the vue ofselecting vt in terms of video recogntion.",
    "Uni-AdaFocus (1282)w/o sample-wise dynamicMN2+R5098.5% (2.2%) 27.2 (23.0x)": "Violence recognitionfor nline videos. g. ,detect harful on the Internet in * \"S real-tim or to * \"S build surveillancesysms in thworld increse safety. 5% on RLVS , outerorms the competitivebaselines, and anew stateof-the-artperformance. cost of metho is 0x lessthan he prevous best baselin in , is beneficialfor developin real-ime video processing appications.",
    ".8x": "76. The mAP and wall-clock training time on ActivityNetare reported. Comparisons with state-of-the-art baselines on Activ-ityNet, FCVID and mini-Kinetics are summarized in. When achieving the same state-of-the-art levelmAP, the number of the required GFLOPs/video for Uni- TABLE * \"S 3Comparisons of Uni-AdaFocus and AdaFocusV1/V2 in terms oftraining efficiency. More implementation details can be found in Appendix H. We consider P 2 {962, 1282, 1602}. 21. Thebest results are bold-faced. , AR-Net , AdaFrame , AdaFuse , VideoIQ ,Dynamic-STE , AdaMML , SMART , FrameExit, OCSampler , NSNet , TSQNet , AFNet ,and the preliminary versions of AdaFocus , ,. 1 v. Following the common practice of these baselines, we adoptMobileNet-V2 and ResNet-50 as the global encoderfG and local encoder fL in Uni-AdaFocus. 9%)than the strongest baseline, OCSampler , with smallerGFLOPs (18. 7). s. In ,we further present the variants of the baselines and Uni-AdaFocus with different computational costs for a morecomprehensive comparison. E2E refers to End-to-End. Comparisons of Uni-AdaFocus and state-of-the-art efficient video understanding approaches on ActivityNet in terms of inferenceefficiency.",
    "CONCLUSION": "This paper presented Uni-AdaFocus, an that en-ables deep networks computation dynamicallyacross dimensions: temporal, and differentsamples. The major motivation behind is computational resources on most task-relevant informative video frames, and relatively difficulttest thus attain a superior accuracy with total cost. We deep into themodel design, training, and efficient implementation Uni-AdaFocus. With our proposed techniques, Uni-AdaFocus canbe easily deployed top of popular lightweight and X3D) and considerably improves their infer-ence efficiency through adaptive computation. Moreover, itcan be training in an fashion. Empiricalresults basing seven large-scale benchmark datasets andthree real-world application scenarios demonstrate that achieves state-of-the-art performance in termsof performance, computationalefficiency, and practical inference speed.In future, would be interesting to further explorehow proposing dynamic computation techniques can beemployed to the inference efficiency of multi-modallarge language models. one may consideradaptively identifying task-relevant video frames on the input text or audio prompts.Given expensive cost of training large models, it wouldalso be important to investigate pre-training multi-modalmodels can be to acquire such capabilities ofdynamic computation.",
    "[P 2": "s. 5%73. 1%ni-AdaFocus [E epochs] [supervison from eGt ] [P 2 P 2]45. ( [supervision from ] [(P + 32) 2]44. 4%65. 6% [supervisionrom eGt [(P 32)2 P 245. 5%79. 1%578%66. 758. 3%78. OCSmpler in terms of the algorithm for learingemporal dynamc comutation strategies. 3%67. %7. 9%57. 9%73. 8%79. 6% TABLE 16Uni-AdaFocus v.",
    "Defomable Patches": "imitatios of fied-size patches. Inboth AaFocusV1 anV2, t inormative image patces {v1, v2,. e he model is onl trainedto dynamically deermine teir * \"S locaions to capture themosttask-relevant patial regionsof eachvideo frae. Howevr, in contrast, t copromises the flexibilityof AdFous.As a matter of fact, the shapes andsizes ofdiscriminative regonsmay vay across iffeen ideos anddfeent frams. By design, the patches with a fixed sie arenherently unable to accommodte the inomative regionsadatively conditiondon each indvidual ideo fme. In othe * \"S words, {v1, v2. } may either mitcucial task-relevant details or include undesirable redundan contents. Incorporating deformable patches. To address this isue,werpose to allow theshpe and size of the patchs{v1, v2. } to be dynamically confiurd accring to thechaateristics n semntics o each frame. Thendirctly tak th HtpW tp patc at (xtc, ytc) to obtain v. It iworh noting thatbeforefeeding vt into the local encderfL, we resize alhe patches to P P, such that they cn beconvenietly proesed in parallel on hardwares.",
    "Adaptive Early-exit (ours)Fixed Early-exitRandom Early-exit": "The resuts of Uni-daFocus dynamic)are On Sth-Sh, we #rames=8+. TABLE 8ffectsof Reusig eGt for Representative ondtion wthdifeent networks, atasets ad (input size for fL)areconsieed.",
    "Index Neural Networks, Efficient Deep Learning, Video Recognition": "isalso Academy of Artificial Intelligence, Beijing, China. and J. H. G. In recent years, remarkable success in videorecognition has achieved by leveraging deep networks, , , ,. Yue, Song, and Huang are with the Department ofAutomation, BNRist, Tsinghua University, Beijing, China. with ChinaMobile Research Institute, Beijing,. Y. 1INTRODUCTIONThe proliferation of online videos, by plat-forms such as YouTube and has necessitated thedevelopment of automated methods for identifying humanactions, events, and other elements within This is cru-cial for the applications * \"S such as recommendation, , , , and content-based searching. Zhang is International School, Guangdong, China. However, noteworthy perfor-mance of models usually comes at of highcomputational costs. In real-world computationdirectly translates power consumption, carbon emissionand latency, which should be minimized undereconomic, environmental or safety address this issue, number of works propose toreduce the inherent temporal redundancy in recognition, , , ,. As * \"S illustrating in Y.",
    "et = = 1, . . . ,2)": "where eLt denoes he feature maps. Importantly,the is localized to capture th most nformativergions for the given task, and this procedure is fulfilled bythe policy netwo , which is introduced the following. g. The detaile and trained lgorithms related to willbe dicssing in Sctions 2 an 3. Classifie is predictionnetwork to aggregatethe information * \"S fom te frames that procesedby the and utput curentrecognition resultstep. To be specific, we performglobal averagepooed onthe feature maps eG , eLt from the two ecodersto get feature vecorseGt, eLt , and concatenaetem as the inputs",
    ".Additional visualization results focusing primarily onvideos that incorporate multi-person/object interactions (zoom infor details). Blue boxes indicate the patches selected by Uni-AdaFocus": "the model. Forexample, n the lower lef video of Uni-daFocussucceds in ientifying theinteractions betwenhumanhand and raw potaes, yet ails infer that actinsare part of pearation. imilarly, right ni-AdaFocus succeeds task-relevant gins, yet in the Future works may focus on adressingthese limitatins of our Notably, we particular differene the pterns of failure asscross different dataset. Hence, we slect the eneral andinteresting casesof ActivityNet video (whichincoporates a ange of various interacions) as",
    "Y. Wang, Pan, S. H. Zhang, Huang, and C. Wu,Implicit data for deep networks, inNeurIPS, 2019": "pp. S. 132, no. 7, pp. B. Li, M. 14171441, 2024. S. Liu, Wang, Sepio:ematic-guided pixel contrast domain daptie semanticsegmenation, Transactions n Patten Anyis and MachineIntelligence,vol. * \"S ang, G. o. Adaptng acrossdomains target-oriented transfrabl sematic prototype constraint, Iternational Journa ofComuterVision, vol. Li, K. 7, p.",
    "Yulin Wang, Haoji Zhang, Yang Yue, Shiji Song, Senior Member, IEEE,Chao Deng, Junlan Feng, Fellow, IEEE, and Gao Huang , Member, IEEE": "ith examinatin of spatial redundancy, thobservation that the ost each video usually orresos to sall path, whose shap, size adloation siftsmoohly acros frames. Motivatd by thi fomuate the patch localization prblem as a dynmic decisiontask and a spatially adaptive video apprach, termed Sueuetly,the selected nferred y a deep network for the final The complete model can in an end-to-endmanne. Duing infeence, oce patch sequence generated ofcoputatio can be executed iparalel, it efficient GPU devices. g, nd X3D), which can be readilydeployed ourfeature extractor, yilding a signifianty improved compuationaleffency. Empirically, experiments based onseven wide-sing benchrk datasets (i. e , FCVID, Mini-inetics, Something-oeting V1&V2, Jester, and Kinetics-400)and three real-orld applcatio scenarios (i.",
    "FADDITIONAL VISUALIZATION RESULTS": "We fin tha o currentmdelexhibitefficacy in understanding thenon-typicalare rare nd may lie within the tal ofthe istributio. Fr instanc, videos such as a cffee or ridng in the pose chalenges.",
    "%74.3%77.0%78.5%79.0%79.5%": "We investigate elminated it three major components (two of which areintroced by the deformable patch mechanism proposed in. 1. 2), or further incorported moreflexible dynamic ransformations. Characeristics of Patch election Mechanism within Uni-AdaFocus (P =1282)ActivityNet AP after roesing t Frames(.e.",
    "ActivityNet, FCVID and Mini-Kinetics. Following thecommon practice our baselines, adopt": "3. Following the original design , fully-connecedayer asthe fC, and we thefrae-wis prdictions as the For fair comparison,we augment TSM introducing the sam twobackbone networks as or as thioput features are lso concatenated t be int a In other words, TSM+ differentiates itelffrom ourmtho in it he unifrly-sampled wholevide frames into ResNet-50,while we feed t dynamicaly-selecting informative ptches task-relative frames. and esNet-50 as the global encoder fG ad fL in Uni-AaFocus. Notably as he vdeos in these datasets are vey short(average duraon 3-4s for Sth-Sth V1&V2 nd for Kinetics-400), w the networks reuirethe visalangle inputs (frames/patches to be smilar forhigh eneralization Wealso oserve lo-cations and tsk-elevnt regions do significatlychange cross frames in the same ieo (due to the shortduration). such simplification des not affect main ideaof our method snce differe atch. Her we directlyompare the perfomance of ourmethod with X3D, our two backbones cmefrom family X3Dnetwork. Comparing irectlyprocessing C T W this dsign eliminatesthe redundant informaion in the coresponding computationaloverheads, which regenerally negligible. The teporl accumulated ax-pooling moule proposedin is as the classifierfC. The training hyper-parmetes ofourapproh e found in Appendx H. policyan efficient two-rancharchitectre, correponding to produced temporl andspatial dynamc comutation Given fetures wit thesize C H W, the temporalbranch pools them CT1 (since here we do ot nedto preerve the spaial and themwith 11 convolutional folowed by a ull-onnected layer. For X3D, simply replace MobleNet-V2ResNt-50 byX3D-S/X3D-L on to of sttings TSM. In addition,as he networks usually necessitat rocessing simultaneously, here oursample-wise is performing adaptvely actiating/deactivating L. V1&V2,Jestr and Kinetics-400 implemenUi-daFocus on top recently propsing efficientnetwork architectures, ConvNets with temoral shift mdle(TS) and urmethod can the efficiency of uc state-f-the-art liheight models For TSM, we till se MobeNet-V2 and ResNet-50 as and fL, but ad TSM them.",
    "M. Kim, H. Kwon, C. S. Kwak, and M. Cho, self-attention: Whats missing in video understanding,in NeurIPS, pp.": "C.42, no. 80893, Cao, M. Zhan, * \"S Y. Han, Xu, J. 107110, Zhang, Y. Yang, H. Wg, S. nealnetworks with broad views for parkinos screening, IEEE International on Bioinfomatics and Biomdicine(BIBM), 219, pp.",
    "EXPERIMENTS": "Overview. 2,we Uni-AdaFocus on top of representative recentlyproposed lightweight deep networks (i. In. 3, we further evaluate Uni-AdaFocusin context of three representative real-world applicationscenarios. In. 4, comprehensive analytical visualizations are provided to method. also con-sider real-world scenarios i. e. fine-graineddiving action on Diving48 , Alzheimersand diseases diagnosis with brain magneticresonance images (MRI) ADNI OASIS , and PPMI, and violence recognition online videos RLVS. adopt the same pipeline as ,,.",
    "Reducing Sample-wise Redundancy Conditional-exit": "n fact, nuerous studis , , ,, have reported the existence of  significant numberof easier sampleswihin dataset, hch can be acuratelyreconied ih considerabl less computation compared. Apart from the forementied spatialand temporl redun-ncies, n addtional factor contibuting to  significantmount of redundant computation canbe ttributed to theequivalen treatment of dverse samles concerning theircomputational cost.",
    "(ours)45.7% 58.3% 67.4% 74.8% 79.6%": "Effectiveness of reusing eGt recognition. As fore-mentioned, method the global boh activating the policy network and recognition,aiming to facilitate efficient rusig see. effectiveness of this design is studedin. can thatthis mechanism is abl to improvetheby 0. 8-2. 8%. scific, ourbaselinesinclude 1) patch loclizati 1-a) randomly samplingpatche, 1-b cropping patches from the centres of fames,1-c) patches from a distribton centredat fram centre; polcy to determine te sizes/sapesof patches: 2-a) fixed patch size, 2-b) randomly determinigpatchszes, 2-c) * \"S te frames as2-d).",
    "t )": "Illustration of the policy in training, wesample from , while at test time, selec thpatch with the larget Inaddtion, the achitecturof fC my ave different choices, such recurren networs, , averagingthe frame-wise predictions , ,, ad accumlated feature pooling. qW/qh11dIAPzrwKBXh70hcWEP2Hm3AeJS0uGOZIQrsWfwGKROXyFj/nW86PMzvpyfxFe+Q+7AD7XyjtJ4+nN8+XLycBVSTt5n+ecS+z1DP/wvNTpFW3TS0hKzqNqjl9P/HLeUZzPidxSwjWQ+uVuWT9VNz5Q8AuJo3MEm+7ohZ5a/OTi3nE9vHnUO+/hcy42ht5CXRx8JzJ1eiLjDnFL8/cbeJK74/lnKObYTEyRI93RXsqEwQs7s769E05KxsV+uvqzsfdiqnaLrtMek4v0LNqEHvaA8hIMfKVv9L3wo3BV+Fn4lUJXVzKbpzTxFP7+A/yJk/4=</latexit>v.",
    "Hidden StateCConcatenation": ". Overview f AdaFocus. It irst aglance ateachframe v sing globa encode fG. a policy networkis built on top * \"S of fG to seectmst imporant image regin vt in termsof recognition. A high-capacity local encoder fL adoted to from vt. Finaly, a classifier aggregates the fatues acrossframes to obtin pt.",
    "BTHREE STAGE TRAINING OF": "werandomly sample patches to minimize thecross-entropy loss LCE() over training set Dtrain:. , video recognition) and discrete (i. Therefore, training algorithm to solve continuous anddiscrete optimization problems Stage I: Warm-up. , patchlocalization) optimization, the cannot be applied. We first initialize fG, and fC, the network this stage. the formulation of AdaFocusV1 includes both (i. e. e.",
    "IOS": "Setus. subsection evaluates ni-AdaFcs intreerepresentative realstic application The details ofour setups cn be fond in H.2.Fine-gained lssification on Diving48.results are in . Our metod ith bot basicbaselieTSM/SM+)an recently proosed methods state-of-the-art performance. Uni-Adaocus attainsthe test accuracy ith a minimal computtional ost. it improe he 7.7% v.s. 80.4%)with 1.87x les computation onto of TSM+, astatic compuationalTis my be by more effctive for less biasd scenarios likeiving48, tak-relevant is intensve,anit is lossless beeficial filter ot lessinformativ image regions and video frames.Alzheimrsand Parkinsons iseases diagnosis withbrai magntic images Alzheimersand disasestw of the disorders the elderly , , charactrized by te irreversle oss of neuros aswel as impairment cognitie and otor diagnosis of and interventin atthe prodromal stage is ofgreat impotance, h whichthe onset the isases can b delayed Toaddres this isue, analyzig the MRI ptenial patients isa widely tchnique, where MRI depict structurof human brains in a non-invasive can be as effcie approach for thedata-diven nlysis 3D MRI under hego ofimprvin the precision and reucing therelce on experiencd which typically shon in , Un-AdaFocus is o outperformthe state-of-th-at imaging analysis rameworkson ADNI , OASIS , and PPMin terms o theaccury ofdetcing Alzheimers and diseases.Noably, of the baselies leverage generative models,Transformr networks, orDhich and computationally intensie than our metho.",
    "utilizing the features of the TG uniformly sampled frames pro-cessed by the global encoder fG. We approximate the T0 TLsampling process with TG TL": ", y) (see: Eq. g. , wT0}. Subsequently, Lframe(vni) can be effortlesslyobtained with the off-the-shelf of the auxiliary linearclassifier, i. e. * \"S TG weighted weights are to acquired down-sampling{w1,. , wT0}, andtake uniform on * \"S the to corresponding the horizontal axis. , 3D or temporal self-attention), Lframe(vni) actually represents the loss of a shortvideo clip centred at Moreover, at time, to deterministic inference procedure, we compute thecumulative distribution function of a single-time weightedsampling distribution parameterized by {w1,. (11)). Notethat is as video backbones with tem-poral fusion operations (e."
}