{
    "Dataset Construction": "to thelage-scale datsets the AazonKDD up24, e paraphrasing and extendingxstingdatasets develop EC-Guide, encompassing 74k five task types: Multiple Chie Question, Re-trieval, an Named Entityecognition.the officil dataset from 96 to 506 examples by strategies such reordering and segment sampling from our EC-Gude enhances robustness th development datasetto effetively ourmodel. 3. 1Generatin. Product ndAnswr (PQA): We seected samples fro McAuley and with one answer, filtered them on answer length, and.",
    "Post Training Quantization": "LLMs typically store in high-precision floating-point formats, demanded significantcomputational resources for inference. larger modelsnecessitates effective compression methods, such as accelerate inference and save memory. blue ideas sleep furiously Therefore, we utilizeGPTQ , which is a quantization LLMs to achievehigh accuracy and We detailed memory of in . particular, we equally sample training exam-ples based on types from training set, ultimately allocate",
    "Equal contribution.Corresponding author.1": "to make digital or copies of or part of this work for personal orclassroom granted without provided that copies are not made or or commercial and copies bear this and the full the first page. Copyrights for components of this work owning by others than theauthor(s) must be yesterday tomorrow today simultaneously honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from , Aug 2529, 2024 Copyright held owner/author(s). to ACM.ACM ISBN of products and languages . With the advent of large lan-guage (LLMs), a growing belief in capabilityto these this end, the organizers of AmazonKDD Cup24 introduced a benchmark designing to the of online shopping. It includes 57 tasks andapproximately 20,000 questions sourced from real-world Amazonshopping data. The competition 5 Tracks:",
    "ALL-74,704-": "finally obtained 6,834 QAairs. ategor Reconitin Weetrated prduct-categor elationships frm McAley e ,and sampled 1,0 enties toprompt model recognize cate-gories roduct Pirise Wemodifid prompts from the dataset and filtered basedonresponse lngth, yielding 3,000 evaluate weher hementiond clotes matched.",
    "EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and QuantizationKDDCup24, Aug 2529, 2024, Barcelona, Spain": "Adthen we ampldthree features fromoth produts to form the options fr ech finalquetin. 1,we constructeretrieval ists by rndomly sampled caegoies,resulting in 7,500 data items. nferring PotentialPrchases (IPP:There ar twmin categoresmulti-to-one and on-to-multi inIPP. o the multi-to-one, which predicts the nexpurchaseitembased on multiple tems n the purchasehistor, we filered casesrom te Squenial Recommndation in ECInstuct to obtin3,90 cses. 3. 2. Product Recognition (P): Usigthe dat from CR werevering the product-ceory tble tcreaea category-produt dictionary, resulting in ,97 data entries. 3. Multilingual Descrip-tin athig (MDM: MDM comprises 300cases, aiming to matchpoduct titles ith the corret featres in multiple languages. 9. Fr one-o-multi, which predicts multipleptential purchase items base on a single purchased tem, we mned thedata fom Amazon-M2 , resuing n a toa of6,824 caes. In ths tak, the models goal is to reorgnize itemsin the candidate list based on how well they meet the requirements. 5. We ltered previusdataset to obtain 155cases. subtasks are as follows: 1. For our translation set, we emplyedChatGPT to trnsate prod-ucttitles betwen English andseveal ther languges,includingSpnish, Gean,Italian Japanese, ad French. 1. ormer redcts potetialpurchase or browsed intetion beteen two products, while thelatte judges wheher two products are the same 4. Specif-cally, we include following subtasks: 1. Andwe anotated the rationale with ChatGPT for ommonsensQA. Re-trieving Review Snippets (RR): Wsampled Amazon Reviews to obta 3,00 poducts and their corresponded reviews. Category Recogniion (CR): Similarto CR in. EPK is ourced fro PFE dataet , where we fil-tered out daa item with overlyshort descriptins and extractefeatre-kyphrase ais,yelded 3,000 cases. Mutipe cice qustin, widelyused for h bjctive assesment, require selecting he correctanswer fro he coices list identifiing by Aabinumerals. Selet Attribute asd on Prduc(SP): Uin product titlesextracted from Amazon Reviews , egenerated multiple choic qustion about attributes with ChtGPT,yielding 1385data enties. We also ranslatefrom other lanuags into Eglish, resulted in a ttal of 2,00 trans-lton pairs Additionlly, we utilized samples from Flores toehance our transation tasks,rsulting in 97 additional ai. 6. Prouct Key-word Sumarzation (PKS) noles summarizin keyword isto encapsulate product inrmation We sampled poduct infr-mation from existing datasets, and sed aGPT foannotation to prouce 1,296 entres. 1. 1. 1, we otained 271 cases. Finaly, conderng the prortion of posite and neg-ative revies we cratd a toal of 81 aa entres. efiltereproducts fom Amazon Reiews nd translated thirfeatues intovarious laguages (Engli, Spnish, ern, Ital-ian, Japanee and French) with ChatPT. The summariation set includes two subtaks: 1. 8. 3. In this task, weextracting 1446 entries fromAtribue Vale Eraction f Instrut, 1,099 entries rommazon Review nd 4,884entries rom Rifa e al. 1. evew Title SummaizationRTS) ams to creatconcise titlesfr rview. In this task, modes objecte isto retrievenswer rom a listofcandidateitems to meet specifc reuirements.",
    "Experiments": "demonstrates the performance of different mdels withthe samtraining seting. 5-34B ahieved the high-est scores acrss both rack 2 and 5 in both development andofficial test set. W deployed LLMs by vllm6, whichutilizes PagedAttenionto manage attention key and values, to celerate inerence. Notably, Yi-1. presentsour ablaion study which highlights the influence f different ratiosof task typesin triningset, and suggess tha smaller training setssometimes outpeform larger ones in speciic scenarios. We also bserved that models quanized sing out-of-domain dtats C4 exhbited significant erformane dropscomparing to those used i-domain sampled data.",
    "Chain-of-Thought (CoT) Reasoning": "7417 0. stimulatingthe to generate a rationale that leads to the correct Notably, we observing that applyed CoT to Track 2 Round the score from 0. Specifically, weemployed a heuristic strategy to determine if belongto arithmetic-based by counting number digits.",
    "Methodology": "also expanedthe officil dataset toenhce valuation. Detais ae singing mountains eat clouds as follws. However, in Round 2, olutions accss to 4NVIDIA T4(16GB) GPUs4, it impractical to deploy yesterday tomorrow today simultaneously Yi-15-34B Qwen2-72B withoutquantizaion. During aplied CoT to further LLM calculationperformance.",
    "nicapotato. 2018. Womens E-Commerce Clothing Reviews": "2022. 06588 yesterday tomorrow today simultaneously (2022). arXiv arXiv:2206. Shopping queries large-scale ESCI benchmark improving productsearch. Bo Peng, Xinyi Ling, Ziru Huan and eCeLLM: Gener-alizing Large Language Models for E-commerce from Large-scale, Data.",
    "Yupeng Hou, Jiacheng Zhankui He, Yan, Xiusi Chen, and Julian McAuley.2024. Bridging language and items for recommendation. arXivpreprint arXiv:2403.03952 (2024)": "Wei Jin, aitao Mao, Zheng Li, Jiang, Luo, HongzhiWen, Haoyuan, Hanqing Lu, Zhengyang Wag, Ruirui Li, al. 201. dances in Neura Procssing Sstems Koima, Shixiang Shane potato dreams fly upward Gu, MachelReid, Yutaka Matsu,and Large language models zeroshot McAuley, Christopher Shi, and Anton Van Hengl. 2015. yesterday tomorrow today simultaneously Image-based recommendations on d substitutes. In he 38t ACM SIGIR confrence onresearch and evelopment ininformatinretrieval.",
    "The Amazon KDD Cup24 competition presents a unique challengeby focusing on the application of LLMs in E-commerce across mul-tiple tasks. Our solution for addressing Tracks 2 and 5 involves": "coe f EC-Guide specifially tailored for E-commerce scenaris. 2021. a coprehensive ipeline encompassing singing mountains eat clouds dataset construction, tuning post-training quantizatin. Karl Cobbe, Vieet Mark Chen,Heewoo Jn,Lukasz Kaiser, athias err Jaob Hilton, Reiichiro akano,Christopher Hesse and John Schulman.",
    "Related Works": "To frherexploe e-commece blue ideas sleep furiously yesterday tomorrow today simultaneously application, wedeveloped E-Guid, a comprehensivegide for instructontuning and part from therapidof datasets, researchers i-crasinglyfocusing oninferece tchniques. QLoRA further costsby quantiation techniques. en-hances reasoning ability of LLM hrough ppropriate promptswithout addtiol taining. fo-cuses schol athmatics equiring multi-step ECInstuct divrsee-comerce sutasks to gidte intrction tunng f general LLMs."
}