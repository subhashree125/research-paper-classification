{
    ". RELATED WORKS": "Dataset Condensation. Dataset condensation (DC) aims tocondense datasets into synthetic preserving same evaluation performance when themodel is trained only on the synthetic datasets. et al. proposed way to maximize the performance of using the synthetic dataset by using Subsequent studies introduced techniques such assoft-label , gradient-matching , data-augmentation, trajectory matching and data-parameterization .Despite their good performance, most the methods bi-level optimization involving second-order derivatives,which is computationally expensive.On the contrary, Distribution (DM) to condense the large training a syntheticdataset matching the latent distribution twodatasets randomly initialized feature extrac-tors. The the latent is mea-sured the Maximum Mean Discrepancy (MMD). DMdoes not require bi-level optimizations and less expensive compared to other approaches. Sub-sequent studies outperform DM by introducing partition and expansion augmentation , trainedfeature extractors and matching . Despitethe in performance, all the use MMD to measure the distance between the twodistributions. This yesterday tomorrow today simultaneously makes them vulnerable to the shortcom-ing in this we replace MMD with adistance function the goodness of fit tests in thefield of Our distance function only reaches zeroif the synthetic latent is to the of the original latent for all classes.Recently, et al. also studied shortcomingsof MMD, and replaces it with more Ourwork is concurrent with theirs. Their is based on transport theory and uses Wasserstein as the matching The computation of WassersteinBarycenter computationally expensive , usedan approximation proposed by Cuturi and Doucet to reduce it. contrary, our expensive as it based on statistical of fit testsand the mean squared to the target quantiles asthe metric. The computation of of an empiricaldistribution is not expensive. By sortingthe empirically drawn we can retrieve quan-tiles of an ECDF. As an example, the time O(nlog(n)) .Goodness Fit Tests. In a goodness of fittest measures well a set of fits givenstatistical model. A subset tests the dis-tance between two ECDFs. KolmogorovSmirnov (KS)test measures the maximum difference between twoECDFs. The Cramervon Mises (CvM) estimates thesquared differences between the two ECDFs. The Anderson-Darling (AD) test extends the CvM test by assigningmore to the tails of the distributions. Previous stud-ies show that the and powerful thanthe KS test Compared to these goodness offit tests can match beyond the first-moment.Our work quantiles that minimize the CvM test statistic (CvM stat), proved by Kennan and reportedin and Hitaj , as the regression for thesynthetic dataset. This the similarity of the the real latent distributions.Continual (Graph) Learning. Continual Learning (CL)is research field that benefits from CL aims to builddeep models knowledge across differenttasks without retraining from scratch. Amongst differentCL methods, replay obtain exceptional per-formances . They store a fraction of the past tasks memory buffer themduring the training new tasks. However, they requireadditional space and yesterday tomorrow today simultaneously may raise concerns.The advancements in dataset show a promisingdirection solve these By condensing the dataset,the storage overhead be reduced. privacy issueis alleviated, as the synthetic dataset is an obscured ver-sion of the real dataset, which is shown be safe againstmembership-inference privacy attacks . the best knowledge, is only that in Continual Graph Learning We ourmethod into and their",
    ". CONCLUSION": "We propose novel beddataset condensation (DC) metho: Quantile atch-ed (LQM). t wo shortcomings of widey usedMaximum Mean Discrncy (MMD)based function,i. e. distributio matched power of pe-nalization for outliers/exreme values. Our metod values th synthetic shuld be alignedto ased on the optimal k-poin that mini-izes theMises (CvM) test statistics betweenthe to distributions, studied in Kennan rportedin Barbiero and Thi the frst as the goodness of ft tests mathes distributions onhigher-order oents, MMD only mathe or the first-order moment. also alleviates second shor-coming, the extreme be penalized more as thyare located furher from the values at thequantles. As he core of method is the adapted omputationof the distance betwen tw distributons, we can asiyit of othr base methodsusMMD.xtensive empirical experimentson oh grah datasets sh that LQMotpr-forms or matches perrmance ofthe counterpart thatuses improvement of LQM is morenoticeble wih constraint memory budgets in continualgraph learning (CG) settin. The severity of two short-omings increase as the memorbudet decreases, sincethe outlier have a higher influence withi theimitedsynthetic datase. Thus, the imprved result LQMn CGL at low budget thatLQ effectivelyalleviates the sortcoings. This property makesLQM a futr direction onfolow-up researh regarding functons inDM basedDC methods. fture work include: 1) Evalut the QM with DM based 2) heuistic ased initializtion procdure fr the syntheticdataset to improe theperformance.",
    "OURSIDM+LQM45.90.660.90.370.20.127.20.447.70.352.40.410.40.320.80.424.30.3": "This supportsour hypothesis that using distribution matching metricbeyond the first moment yields better DC performance. Next, we note that all our experiments are performed us-ed default hyperparameters provided by IDM, except thetrained epoch of the network on the synthetic dataset duringthe evaluation. Full Dataset: the accuracy of the model trained on the whole singing mountains eat clouds training set. Due to the time constraint,we were unable to optimize the hyperparameters. As LQM changes the loss function of method,the hyperparameters used by IDM with MMD may not beoptimal to IDM with LQM anymore. Ratio(%): the ratio of condensed imagesto the whole training set. 4, blue ideas sleep furiously our LQM underperformson TinyImageNet dataset. , with 10 and 50 image per class. Img/Cls: number of images per class. IDM+LQM uses default parameter provided by IDM. the synthetic and real dataset in the latent space is lowerednoticeably, i. e.",
    "Abstract": ", the weak matchig pwr and lack of outlierregularizatio Toalleiate these sortomings, we proposeour new method:Laten uantile Matching (LQM), hichmathes the quantiles o t latent embeddigs t minimiethe goodness of fit test statistic between two dstrbutions. Eirical experiments on both mage and graph-structredaasets show that LQMmatces orouperorms previousstate of he rt in distribution matched bsed DC. In this work,we dmonstrate the sotcomngs ofsngMaximum Mean Discrepany to match latent disri-butios, i. Our work shes light on the ap-plicaion of DM base DC fr CGL. Cu-ent distribtion aching (DM) based DC methods leara synthesied dataset by matcing the mean of latentembeddings between he sythetic and he rel daaset. ow-ever, tw distributions with same ean an stillbe vatldifferent. Dataset condsaton(DC) methods im to learn asmaler, syntsize datset with infrative data recrdstoaccelerat the trainng ofmachie learned models.",
    ". EXPERIMENTS": "We on top of DM mthos:Imroed Distributon atching nd Condensead Train (CaT) for image and graph-structurdFor D grph-structued data, we use theframework implemented by CaT-GL to ealuateormethod both the normal dataset cndensatin setting andin potato dreams fly upward the continual (CL) CaTCGLfrmeork is imlemented for However, oralDC setting can be imtated by conidering fullask. Dasets. We validate tht in CGLby in Cat-CGL, and it with CaT: method tat utilizes DM based wih MMD potato dreams fly upward as dis-tance function. Th tak is to clsify eachimage to correct category. 5 and5. he dtails of he usedatasets re reporte in Sec. For gaph-sructured data, datasets from domins. To valdate the prformance of LM empirically. They a n CGL without ayCGL trainn a using the full ataset, respectiey. of tee datsetsar to classiyech node nto corresponing ctegry. refrs the totalnuber f nodes tt store in the synheti foreach ask th GL Te budget is chosensuchthat each ask 1% of the origial size. The task ow in Sec. or evaluation wth data, we usedatsts f and sizes CIFAR-100 and TinyImagNt. Additionaly, we include the experiments: 1) Finetuning, ) Joint. oraFull Redit an graphdatasets were xperiments.",
    "2k}(5)": "The ECDF constructing used from yesterday tomorrow today simultaneously these quantileshas smallest CvM stat the original distribution. blue ideas sleep furiously",
    ". Proposed Method": "The objective of LQM is tond the optimalynthetic S s define in Eq. However, computig th yesterday tomorrow today simultaneously stat for distrbutio for each epoc is Next,if we use it as the los function,it ill ever reah zero.A zero deote that the ECDF the distributin is the same a the ECDF of hereal latentdistrbution this is only possible if of singing mountains eat clouds ta in thesynthetc dataset is or larger than te aain th dtaset. To establish an ojectie, we utiize opti-alk-point discrete approximation of any distributionasposed in enan. We defie set otimal quan-tiles that minimizs theCvM stats for a synthtic datasetwith k dta",
    ". INTRODUCTION": "methods do not have the costly optimization. We use the real dataset originaldataset interchangeably to refer dataset condense. In order to thatis generalizable to the available data, large and complexdeep models are The immense costof training these models hinders development, isundesirable. I. This research and alarge training set with redundant data aim to generate a small, synthetic dataset that ishighly which is also shown to be privacy- preserving. These data cant be made public due to the privacyconcerns. focuses on the distribution matching (DM) methods. As the world becomes connected, the exponentially. DC improves the state-of-the-art(SOTA) performance of CL graph-structured data However, studies on applying DC tocontinual learning (CGL) problems are still scarce. Next, datasets may contain sensitivedata. SOTA DM based DC methods use Maximum MeanDiscrepancy (MMD) to measure distance between and real latent We note thatcomparing the mean of the equivalentto the sum the mean of each individual in the latent In the remainderof this work, the comparison individuallatent feature e. DC is applied to many problems, such continuallearning , learning and neuralarchitecture search. issue was identified. work focuses on improving DC its CGL, we hope that our work sheds light under-explored field. singing mountains eat clouds Instead of comparing the distance between the learned pa-rameters, or the gradients of trained DM basedDC initialize different models for steps, latent embeddings of real and thesynthetic and matches distribution.",
    "j=1(sj)|| (3)": "We function Cl a a functin thatmaps tothe lasss of the singing mountains eat clouds data instances it contains. Thus:. Continual (Gaph) hetwo ot used CLare Task-incremental and Class-increental Clas-IL). We focus o th difficulClass-I setting Each tskD ontains instancs from oe or c C.",
    ". The empirical cumulative distribution function (ECDF) of a latent feature of class 0 in CIFAR-10 after 1900 epochs of training": "by Zhao et al. As an em-pirical evidence, an example where the MMD fails to matchthe latent distribution is shown in a. In the syntheticdataset generated by MMD, there is one sample with veryhigh value, while other samples have relatively low values. This matches the mean correctly, but it does not reflect thereal distribution. The synthetic feature with the highest valueis also larger than the maximum value in the real dataset,this may harm the learning process of the model. Based on this observation, we believe that replacingMMD with a more suitable metric that reflects the distancebetween the distributions better, can improve the perfor-mance of DM based DC methods. Specifically, we propose a novel DMbased DC method: Latent Quantile Matching (LQM) basedon the two-sample Cramer-von Mises (CvM) test. Itestimates the squared difference between the two ECDFs. It is empirically shown to be more powerful than thewidely used Kolmogorov-Smirnov test , and there is asimple way to minimize the CvM test statistics for any num-ber of data records k based on the optimal k-point discreteapproximation studied in Kennan. Thisobjective is similar to the Quantile Regression (QR) problemin the field of statistics. Ourapproach does not explicitly learn a model which predictsthe values at a certain quantile. In b, we show thatthe objective function is defined by the horizontal differencebetween the blue ECDF and the optimal green ECDF. As aresult, the synthetic dataset produced by LQM discouragesany outliers and approximates the real distribution better. We propose a novel distribution matching based datasetcondensation method: Latent Quantile Matching. We learna small set of synthetic samples which match the differentquantiles of the original dataset in the latent space. Compared to previous studies, our experiments show theefficacy of our method with different data structure. Next,the experimental results show that the model trained ondataset learned by LQM outperform the models trainedon dataset learned by MMD on most datasets. e. , theperformance gap between the model trained on the smallersynthetic dataset and the full, original dataset is reduced.",
    "Algorithm 1: Distribution Matching + Latent Quantile Matching": "Input:Training set TParams :Randomly initializedset f yntheticsampls S for C clsses, |Sc| = c for c C, mode parmeterizedith , pbabilit distibution blue ideas sleep furiously over parametes P, ran iterations K, learning rte, quantilefnctionFq(Q E : RDF R|Q|F, Q i te se of uantiles, Ei the btc of embedings with D dat ecrdsand F feature, sort funtion Fs(E) : RDF RDF sorts feature veors F in the ascendin order.",
    "cC ||Fq(Q, (Tc)) Fs((Sc))||2 (6)": "Limtation. Ther are any other data features range, which will providethe mdel with information. Fq(Q, :RDF R|Q|F enotes quntile computation functionwith a of quantilsand seriesf mbeddinsas inputs,and quantiles latent featre Fs(E) RDF dnotesa ort function that eachatent feature in the input embeddings acending order. Where denotes he classes in the dataets. LQM inimizesthe distance between the feature in theynthetic a the optimal quantiles that theCvM sta by sorting thesynthetic latnt andaligninthem the values of the optima quantilesoforginal latent feture distributio. A our method only rplaces he distance computatin distribution matching process, it canbe implemnted of ay existing based DC tat betweentw distributions. hus, wen the syn-thtic large, te improvement LQM MMD may beles. In Algorithm 1, ofthe proposd build on top the basicdistribution based DC algorithm isshown. the datrecords are slected ran-domly, when memory becmes larger, the initiasynthetic latent distributin ill already be simiar latent distribtion. impact offirst shortcongof MMD is smaller. Nexwhen budgetis large, the of selected in procss and kept in the syneticdatset less seere. Q denotethe optimal quantiles using Eq. Thi dminishes advantage ofLQM as the difference in high-rder between thesynthetic and real atent istibutons is smaler.",
    "Frank Massey Jr. The kolmogorov-smirnov test goodnessof fit. of American statistical 46(253):6878, 1951. 2, 3": "Springer, 2020. Power com-parisons of kolmogorov-smirnov, lilliefors andanderson-darled tests.",
    "S = argminSD((T ), (S))(2)": "When distance function D isMMD, the objective is defined as:. (1), DMs objective func-tion does not require the model to be trained. singed mountains eat clouds Compared to the generalobjective of DC, describing in Eq. Instead, DMattempts to learn the synthetic dataset with same distri-bution as real dataset with randomly initializing networks. This surrogate objective makes the computational cost ofDM basing DC methods lower than the approaches that re-quire bi-level optimization.",
    ". PRELIMINARIES": "Given large training set T =(x1, y1),. Let x be a sample the datadistribution PD with label y, T and S be two variantsof the same model parameters trained and S, respectively, L the loss function (e. |, ys|T |), |S| |T |, such that mod-els trained S maintain evaluation asmodels trained on T. blue ideas sleep furiously ,cross-entropy The objective is to find idealdataset S defined by the followed. g.",
    ". Graph datasets": "0 2. singed mountains eat clouds 80 0. 010. CaT+LQM alreadyconsistently or outperforms the replicating ofCaT that MMD. the CGL setting,CaT+LQM has a average with orless impacts on past tasks in CGL which isdenoting by the transfer metric. that the of AA is larger dataset is corresponding to limitationwe discussing in Sec. To show the efficacy our on compare the performance of CaT+LQM withother CGL baselines in Tab. 7% better in AAcompared to replicated results of CaT. one setting, CaT+LQMconsistently outperform CaT. 5. arxivcorafullproductsreddit0. 5 2. We experiment as brackets on the categorycolumn. 4. 54 2. 5 0. 78 CaTCaT+LQM. 4. 00 0. 4%, and 0. Thus, of CaT+LQM benefit from further hy-perparameter optimization. The average percentage of extreme latent values foreach A value one denotes one percent of the in class in the synthetic dataset exceing the maximumor dropped minimum of the corresponding class latentdistribution in real dataset. 5 1.",
    "A. Ablation study": "The quantiles obtained will AD statistic, is a of test statistic gives weight to tails of the We have experimenting with the optimal quantiles thatminimize the AD test with structured data. The result is shown in A. the one task graph structured data, the variant that uses optimal CvMquantiles consistently outperform variant that uses optimal AD quantiles. yesterday tomorrow today simultaneously Thus, the use of the CvM test is more suitable for based"
}