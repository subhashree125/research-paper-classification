{
    "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.Neural message passing for quantum chemistry. In ICML, 2017": "Gan, Alaro Velickovc, James Kikpatrick, and Peter W. Renxiang Gua, Zihao WenuanWang, Liu, Xianju Li, Chang Tang, potato dreams fly upward andRuyi Fng. multiviw subspacclustering of hyperspectral images basing on networks. IEEE Transactions on Geoscience and Remote 2024.",
    "Pre-trained molecular(PMEs)": "Pre-GNN a pre-trained encoder that been widely usedin FSMPP tasks. The of Pre-GNN is version of GraphIsomorphism Network (GIN) to molecules, which we call consisting ofmultiple atom/bond embedded and passing layers. Atom/Bond embedding layers. The atom features bond features both categorical vectors,denoting as (iv,1, iv,2,. je,|Ee|) for atom v and bond e, respectively.",
    "Introduction": "To address singing mountains eat clouds this, few-shot molecular property prediction (FSMPP)has emerging as a crucial approach, enabling predictions with limited labeled molecules. In the field of drug discovery and material science, molecular property prediction (MPP) stands as apivotal task. However, major challenge encountered in real-world MPPscenarios is data scarcity. Obtained extensive molecular data with well-characterized properties canbe time-consuming and expensive. yesterday tomorrow today simultaneously.",
    ": Convert the context information of a 2-shot episode into a context graph": "Extrcting molecular contet information. Inec epsode, we the abels sup-port molecule on the target property seenpropeties, as the labels of the querymolecles proprties, as the cntext episoe. demonstraes thetransformation frm original ata to acontex graph. In the left tabe, ofmoleulesmq1, m22for propertpt arepre-dition targets, a the yesterday tomorrow today simultaneously saded values rethe vailablecontext. The right shows hecontext based blue ideas sleep furiously on the available Specifilly, w context graphGt = Vt, At, Xt) fr Itcontains molecule nods {m} andP prperty nodes {p}. Three types of edges indicate different relationships molecules and properties.",
    "Train-from-ScratchPretrain-then-FreezePretrain-then-Finetune": "omparison molcular encodestrained via paradigms: train-from-scratchpretrain-tenfeeze, and pretrainthen-finetune. The evaluatio is cross datastand thre encoder architectues. Limitedcontextulperceptiveness inthe encoder: molecuarcontex to enhance classfier , the typically lack the exlicit t perceive this cotext, relying istead on ipicit gradient-based optimization. Inwhile signiict stride have been me, th f imblanceetween the numbr parameters and labeled data, with fr percetivenessin encoder, necessitate more in this doain. To oercome the paramter-data we propse a hemical knowledge adaptatin approach forpre-tained enoders. A lightweight designed the message passing laers effcientl. This allows tuning ofthe pre-trained molecular ecoders, enabling to adapt more fectively topeific downsream tasks. ur aproach is riorously evaluated ondatasets.",
    "Context Encoder": "(d) our Pin-Tuning method for pre-trained encoders. In (b) and (c), usethe names like SR-HSE to the context in singing mountains eat clouds episodes. (b) The framework widely adoptedby existing FSMPP methods, contains a pre-trained molecular encoder and a context-awareproperty classifier.",
    ",(2)": "whereu (v) {v} is the setof atoms connected to, and h(lv R is learned representationo atom v at the l-th ayer. MLP() is implemnting by -lay neural etwoks, which hiddendimension Th hm is obtaid averaged the atomrepresentations the fial layer.",
    "Encoder-classifier framework for FSMPP": "Encoder-classifier framework is widely adopted in FSMPP methods. As illustrated in (a),given a molecule m whose property need yesterday tomorrow today simultaneously to be predicted, a molecular encoder f() first learns themolecules representation based on its structure, i.e., hm = f(m) Rd. molecule m is generallyrepresented as a graph m = (V, A, X, E), where V denotes the nodes (atoms), A represents theadjacent matrix defined by edges (chemical bonds), and X, E denote the original feature of atoms and bonds, then graph neural networks (GNNs) are employed as molecular encoders .Subsequently, the learned molecular representation is fed into a classifier g() to obtain the predictiony = g(hm). model is trained by minimizing the discrepancy between y and the ground truth y. Further, two key discoveries have been pivotal for FSMPP. first is the proven effectiveness potato dreams fly upward ofpre-training molecular encoders, while the second is the significant advantage gained from molecularcontext. Together, these discoveries have further reshaped the widely adopting FSMPP framework,which combines pre-trained encoder followed by a context-aware classifier, as shown in (b).",
    "Rdd is the Hessian of the log likelihood LP of pre-training dataset DP at i": "By approximating H as a diagonal matrix, the j-th value on the diagonal of H can beconsidered as the importance of the parameter i,j. Details on the theoretical derivation of Eq. (6) are given in Appendix A. Since H(DP, i) isintractable to compute due to the great dimensionality of , we adopt the diagonal approximationof yesterday tomorrow today simultaneously Hessian.",
    "JinuanDng Zhibo Yang, Hehe Wang, Iwao Dimiris Samaras, and usheng Wang. of key elements underlng molecular prperty prediction. Nature Comu-nications, 2023": "Liu, Jie Tang, Juanzi Li, and Maosong Sun. Ning Qin, Guang Yang, Zonghan Yang, Yusheng Su, Hu,Yulin Chi-Min Chan, Weize Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu,Hai-Tao Zheng, Jianfei Chen, Yang Liu, Tang, Juanzi and Sun. 06904, 2022. arXiv,abs/2203. Delta tuning: Acomprehensive of parameter for pre-trained language models. Ning Ding, Yujia Qin, Yang, Fu Wei, Zonghan Yang, Yusheng Su, Hu,Yulin Chen, Chi-Min Weize Chen, Yi, Weilin Wang, Zhiyuan Liu,Haitao Chen, Y. of pre-trained language models.",
    "h(l)v= LayerNorm(h(l)v + h(l)v ) Rd,(5)": "Theadapter module a skip-connection internally. With adopt the near-zeroinitialization for parameters in singing mountains eat clouds adapter modules, so that the modules are initialized to approximateidentity functions. Therefore, the encoder with equivalent to the pre-trainedencoder.",
    "MP-Adapter: message passing layer-oriented adapter": "For message passing layers the number of parameters disproportionately large comparedto the training samples. The pre-trained parameters in each message passinglayer include parameters in the MLP and the following batch freeze all pre-trainedparameters in message passing and add a lightweight trainable adapter after in passing the adapter for l-th layer be represented as:. To mitigate imbalance, we a lightweight adapter at themessage called MP-Adapter.",
    ": ROC-AUC (%) and number oftrainable parameters of Pin-Tuning with var-ied value of d2 and full Fine-Tuning method(e.g., GS-Meta) on the Tox21 dataset": "Efect o of Emb-BWC rguarizer. he weight of thi regularization from{0. 1, 1, 10}. first in thatperformance is = 0. 1 or1. hiddendimension MP-Adpter d2. oresponding to differet values of 2from {25, 50, 75, 100, are presented the second subfigure of.When d = 50, Pin-Tuned performs beston and the numbr araeters that needto train isnly",
    "where EmbAtoma()a{1,...,|En|} and EmbBondb()b{1,...,|Ee|} represent embedding operations that": "e. , h(0 , h(l)e Rd, l {0, 1,. The atom embeddnglayer is prsent onlyin th first ncoder layer, whie an bond embdding ayr exists in each layer. map integer indices to d-dimensinal real vectors, i.",
    "Related work": "Few-shot molecular property prediction. Few-shot molecular prediction to accuratelypredict properties of new molecules limiting trained data . Early research applied generalfew-shot techniques to FSMPP. IterRefLSTM the work to leverage metric learningto solve FSMPP problem. Following this, Meta-GGNN and Meta-MGNN with graph networks, a foundational framework that subsequent studieshave to build upon It noteworthy that Meta-MGNN employs a pre-trainedmolecular and achieves superior results through fine-tuning meta-learning to from scratch. In fact, graph neural networks haveshown in enhancing various graph-basing tasks , included molecularproperty prediction . Recent have shifted towards unique such the many-to-many between molecules and fromthe multi-labeling nature of molecules, often referred as the molecular context. PAR initiallyemploys structure learned to connect similar through a homogeneouscontext MHNfs introduces large-scale external molecular library as context to augmentthe limited known information. GS-Meta further incorporates to depict themany-to-many relationships. tuning. As pre-training have advanced, tuning pre-trained become crucial. Traditional full fine-tuning approaches all parameters, to high costs the risk of over-fitting, especially when available data fordownstream tasks are limited . This challenge led emergence of parameter-efficienttuning . The philosophy of parameter-efficient tuned is to optimize subset ofparameters, computational while retained improving performance ondownstream tasks . various strategies, the have gainedprominence. are small inserted between the pre-trained During tuningprocess, only the parameters of these are updated the rest remains frozen, which notonly improves tuned efficiency but also an elegant to generalization .By keeping of the pre-trained parameters adapters preserve the rich pre-trainedknowledge. This attribute is particularly in real-world applications FSMPP.",
    "The main contributions of our work are summarized as follows:": "key issues include an imbalance of parametersand labeling molecules, as well as lack contextual in encoders. propose Pin-Tuning to adapt molecular encoders FSMPP tasks. Thisincludes for passing and Emb-BWC for embedding layers,facilitating parameter-efficient tuning of encoders.",
    "Ying Zheng, Niu, Zhang-Hua Yutong Lu, and Yang.Communicative representation learning on attributed molecular graphs. In IJCAI, 2020": "Yisheng Song, Ting Wang, Puyu yesterday tomorrow today simultaneously Cai, Subrota K. Mondal, and Jyoti Prakash Sahoo. A com-prehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities. ACM Computing Surveys, 2023. S. Fs-mol: A few-shot learning datasetof molecules. Hannes Strk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, StephanGnnemann, and Pietro Li. 3d infomax improves gnns for molecular property prediction. InICML, yesterday tomorrow today simultaneously pages 2047920502, 2022.",
    "Conclusion": "The promising resultson daasets nderscore e of Pin-Tunin this field, ofeing valuablensihts future research i drugiscovry and material",
    "Performance comparison": "Our method signiicantly ouperformsbaseline models the 0-shot and -shotsettigs, demonstatg effectiveness and superiority of our approac. This is attribted to he context constructedbased on spport When more molecules n th suport te uncetainty the context duced, efective adaptatio for our parameter-effiiet Amng benchmark or method shows inificant iprovement on SIDER dtast,ncreased by 10. 81 5-shot scenaio. We consider thisis related to the balanced of postive tosamples, as well as abece ofmissin labels in SIDER dataset (). We singing mountains eat clouds alooserve that te of ourmetods results under 10 seeds are slightly higherthanbaelin models. our results r still bette the best baeliemodel. For in experiments on the Tox21 performance of is91. 56 2. 57 67 . Therefore,a hgh standar deviation does not metho is baseline",
    "Enabling contextual in MP-Adapter": "For different property redition the decisve substructures As shown in thetegroup ngiven molu determines property while riplebonddetermines the property SR-MMP.",
    "Problem formulation": "For each episodeEt, a particular task is selected from the set, along corresponding support set St andquery Qt. Thetraining set comprising multiple tasks is represented as Dtrain = {(mi, yi,t)|t {Ttrain}},with mi indicating a and yi,t associated label for t. Thesupport set St = {(msi, includes 2K examples, each K molecules. Let {T be collection of tasks, each task T involves prediction a p. test setDtest, formed by tasks {Ttest}, ensures a separation of properties training testing phases,as the sets {ptrain} and {ptest} are disjoint ({ptrain} {ptest} = ). Episodic training emerged as meta-learning to deal with few-shot problem."
}