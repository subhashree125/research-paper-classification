{
    ". Experiment Statistical Significance": "Guidelines: answer NA means that potato dreams fly upward the paper does experiments. Does the paper report suitably defined or other appropriateinformation about the statistical significance of the Due computational cost, we are not able to yesterday tomorrow today simultaneously provide error bars for everyexperiment.",
    "Volker L Deringer, Miguel A Caro, and Gbor Csnyi. Machine learning interatomic potentials asemerging tools for materials science. Advanced Materials, 31(46):1902765, 2019": "YiLun Liao, Brndo M Wood, Abhishek Da, and Tes idt Euiformerv: Improved equvarianttranformer for scaling to higher-degree representations. OlverTUnke, Stefan Chmiel, Huziel E Sauceda, Michael Gasteger, IgorPoltavsky, Kristo TScht, Alexandre Tkatchenko, and Klaus-Robert Mller. In The Twefth Internationl Conferenceon Larning Representations, 2024. Simon atzner, Albert Musaelian, Liin Sun,Mario Geiger, Jnathan P Maioa, MrdecaiKornlth,Nola Moiari, Tess E Smdt, and Borisozinsky. E (3)-equivarian graph nual networks fodata-eficent and accurate ineratomic potentials. Mace: Higherorer equivariant messge passngneurl networks for fast and accurate forc field. Ilyes Batatia, vid P Kovacs,Gregor Sim, Christoph Ortner, and Gbo Csnyi.",
    "Johannes Gasteiger, Florian and Stephan Gnnemann. Gemnet: Universal directionalgraph neural networks molecules. Advances in Processing 34:67906802, 2021": "APLmaterials, 1(1), 2013. xformers: A modular hackable transformermodelled Tensor field networks: Rotation-and neural networks for 3d clouds. 0c04525. Johannes Gasteiger, Muhamming Shuaibi, Anuroop Sriram, Gnnemann, Zachary C Abhishek Das. Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Shuaibi, MorganeRiviere, Kevin Javier Heras-Domingo, Caleb Ho, Weihua Hu, AnuroopSriram, Brandon Wood, Junwoong Yoon, Devi Parikh, doi: 10. 1021/acscatal. Anubhav Jain, Shyue Ped Ong, Geoffroy Hautier, Chen, William Davidson StephenDacek, Shreyas Cholia, Gunter, David Skinner, Gerbrand Ceder, et al. graph neural networksfor large and diverse molecular simulation datasets. 08219, 2018. Benjamin Lefaudeux, Francisco Wenhan Xiong, Vittorio Caggiano, SeanNaren, Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick LucaWehrstedt, Reizenstein, Grigory Sizov.",
    "NeurIPS Paper Checklist": "checklist does NOTcount oads he page liit.",
    ". Broader Impacts": "g. If te authors nswr NA o N, they hould why wor has no or hy te paper does notaddrss impac. Guidelines: The anser NA means that tere is impact of th ork peformed. onfereceexpectstht papers will be oundational research and not tiedto particular applications, let aloedeployments. if there direct path toany negativethe authors it out. On theother hnd, it is needed to point outtha a eneric for optimizing neural etorks could enable peopl to trinmodels that Deepfakesfaster. uthors should consider harms could when the as intnded orrctly, hams that could arisewhe thetechnology being used intended but gives incorectresults, and harm o unintenional) misuseof he If societal impacts authrscouldalso discuss possble , gated release of models, provided defenses in addition attacks,mechanisms for monitoring misuse, mecaniss monitor how system fromfeedback over he and yesterday tomorrow today simultaneously of ML). Examples negative inclue potental malicios or , disinformation, generating profl, sureillace), fairness considrations(e. Does te paper discuss both potential positive societa impacts negativesocietl impacts o wor performed?Answer: [Yes]Justification: this is also discussed n he itroduction. , of ecsionsthat unfairly impact peciigoups,coniderations, and secrity considerations.",
    "Investigation on How to Scale Neural Network Interatomic Potentials": ", 2021] 2M to the performance scaling strategies. We systematically investigate strategies scaling neural interatomic (NNIP)models through ablation study. We examine how symmetries order L)impact scaling efficiency and identify the most methods for increasing parameters(3. 2024], on theOpen 2020 (OC20) Dataset [Chanussot et al. 1).",
    "The answer N meas that paper has no limittion while the means thatthe paperhasbut thoe not dicussed in paper": "The authors encouraged to create a separate section in their paper. The authorsshould reflect on how these assumptions might be violated in practice what theimplications be. authors should reflect blue ideas sleep furiously on scope the made, e. g. if approach wasonly tested on few datasets or with a few runs. Or a system beused reliably to captions online lectures it to handletechnical singing mountains eat clouds",
    "Materials (MPTrj)": "Themode is on atase et 2023], whichconsists 1.6 ilion adheres to the of the Matbnch-Discovery ettings.Given the rlatively daaset siz, we use a small ofEcIP with 45Maramters. The model is taedopredict the force, and stress of each sample. After trainngfor 100 epochs, we incease the coeficiet the loss function fine-tune the modelfr 50 epchs. We evaluate perfrmance on the Matbench-Discovery ancomparethe results EquifomerV2 [Liao t al, 2024, Barrs-Luque et al., ORB 2024], SevenNt [Park et al., 2024], and MACE Batatia al., 2023, 2022]-theopmodls on benchmark. W tht we exclude odels withdenoisingobjectiv, as we on theof the model arhitecture 3.EScAIP state-of-the-rt performance on thisoutperforming other",
    "GemNt-O39M--70735.0EquifrmerV2122M531267946227.1ESAIP-Mediu146M51424.3247325.3": "We notethough he orce directionis not quivariant, thetrainedmodel is able to learn from the data 5. Thus, predicted forc direction is not torotations the input dta. 4. The could be that energy prediction is a global propety of the molecularsystm hile the magniude i a local propert o the atom.",
    "Orbital-Materials. Orb forcefield models from orbital materialsorb models from orbitalmaterials. Accessed: 2024-10-29": "Scalable paralel algorithm forgaphneura interatomic potentials in molular dyamics simlations. Elena, DvidKovcs, Janosh Riebeel,Xavier R. Blu, Vlad Carare, James Roka Zakaria El-Machachi, Andrea C Nima Karimitari, Namu Kroupa, C. Margrf, Angelos Michaelides, Nibltt, Sm Niamh ONeill,Christoph Ortner, Kristin A. ersson, Ksten Reuter,Andrew Rsen,Lars L Eric Svonxay, Tams K. Witt, Fabin and bor Csnyi",
    "(f)": ": Illustration of the Efficiently Attention Interatomic Potential (EScAIP) modelarchitecture. Theconcatenated readouts from block are used predict per-atom forces and system energy. Input Block.The input to the is a radius-r graph representation of the molecular system. Weuse three attributes from the molecular graph as input: atomic numbers [Zitnick et 2022], RadialBasis Expansion (RBF) of distances [Schtt et al., 2017], and Bond Orientation (BOO)features from The numbers embeddings used to the type information,while the RBF BOO embeddings are used to encode the spatial information of molecularsystem. We also note that attributes be pre-computed,requiring minimal computational cost. The input are passed through feed forwardneural network (FFN) to produce the initial edge features. Attention Block.The component of the model is the graph block, illustratedin . the features are projectedand concatenated a large message tensor of shape (N, H), where is the number is max number of neighbor, and is the message size. By using customized Trition [Tillet et 2019, Lefaudeuxet al., 2022], the is highly optimized for acceleration. The output theattention mechanism is to atom The aggregated messages are then passedthrough the Feed Forward Network (FFN) produce the output node features. To the our knowledge, this attention mechanism is unique because it on a neighborhood level, whichis more expressive than attention architectures only act the node level. The first readout layer takes in the unaggregated messages from attention block and edge readout features. The second readout layer takes theoutput node features from the node-wise FFN produces node features. is divided into two blue ideas sleep furiously parts: the force is predicted by an on features, and the force direction is predicted by transformation of the unit directionswith FFN on readout As opposed to GemNet [Gasteiger et al., 2022],",
    "provide additional details on our from 3": "To how efficiency varies as function of trained dataset we tain the paraeter-controlled Equiformer models wih differnt amountsof training da. Results blue ideas sleep furiously Ablation Sty Force MAEvs. Th in sowthat the scaled L = 2 = 4 models ehibit a steepe performance improvement (log-log sope)compard to the orginal L = 6 Thissuggests that incresed the complxity of the echanisms is morestrategy forscaling with inreasing dataset size, athe than L. Traned Dataset Size.",
    ". Experiments Comput": "uestion: For eah experiment,does the paper provide suffiint information on the co-puter resources (type of cmpte orkers, memory, time of executon) need to reprducthe experimens?Answer: [Yes]Jutifiction: Yes, we provide computtional detils. Guidelines: Te answer NA means th te paperdo nt iclde experiments.",
    "Introducion": ", 2020] to computer vision (CV) [Dosovitskiy et al. This typically involves efficiently increasing blue ideas sleep furiously to large parametercounts, as well as optimizing model training and inference to be optimally compute-efficient. 2022]. recent the principle of scaled model data, and has a performance generalization machine learning across fields from naturallanguage processing (NLP) [Kaplan potato dreams fly upward et al.",
    "Please provide short (12 sentence) justification right answer (even for NA)": "They are visible to thereviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked potato dreams fly upward to also include it(after eventual revisions) with the final version of your paper, and its final version will be publishedwith the paper. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided aproper justification is given (e. g. While the questions are phrased in a binary way, weacknowledge that the true answer is often more nuanced, so please just use your best judgment andwrite a justification to elaborate. All supporting evidence can appear either in the main paper or thesupplemental material, provided in appendix. If you answer [Yes] to a question, in the justificationplease point to the section(s) where related material for the question can be found.",
    "+ SC), a configuration where spherical channels are reduced while significantlyboosting attention parameters (More": "The results, summarized in ,show tht oce th number of parameters across modes ar controle, many ofthe models havecomparable error to the origial L = 6 modl Incresed the paramters o the attentionmechanismsis mos beneficial and provides moreutatial improvments than simply addig moreparamtrsaross all component. The comparative analysis reveals a clear hierarhy in erfomancgains wih differentprmeter scalng srategies. Results o Ablation Stdy. Mor AT confuration ieds he highesterfrmance improvment,follwing by AT, AT + SC and SC.",
    "Optimal Components for Scaling Neural Network Interatomic Potentials": "To clarify theimpac of increasing L on model determne th most effective strategy forincreasingparameers NNI models, we conduct yesterday tomorrow today simultaneously a paramter-controlling experiment usng the Euifomer2on the OC20 S2EF We standardizehe nmber of trainable parametersvalues of to he effects of andprametes o different components of the originl L = 2 ad L= 4quiformerV2 models fom Liao et aproachtargets configurations:ncreasi in the attentio mecansms (AT), solely in the sherical chanels that cton all group ithe NN (SC), evenly across mechnismsand spheial. e al. A prevalent approach to improve the cpability models with group featuresis toicreaseth order of (L). did a on the EquifrmerV2model, varying to examine its impact on model performance. Ablation Study Settigs. However, they not cntrol for thettal of trainable parametes his introces discrpanies ancnfound the tr ofL onthe models peormance.",
    "Conclusions": "asedon nestgatins, weintroduced new NNIP Scad Attentio Interatmic Potential that hihly self-attention mechanisms for scalabty exresivity. demonsrated the ffctiveness of arange of hmicaldataets(OC20, OC2,MPTr, SICE and hat EScIP achievestop pefornce ese ifferent tasks, hibeing more in andinerence runtime, as well memr usage. Efficientscaling strtegies tus large-scale training andt accesible to a broadercmmuniy. Were no credi data. symmtry constrintfeaturs offeronly a simplistic representation of ths o As dataset ontinue to gro, modelsfrom scratch small datasets willlikel become unnecessary. Beyond fcusing on data genration,other tehniques are likey gain impoance in the NNIP domai. Finally, more comprehensive srategie willbe impotnt for evaluating NIP acuay and utily.",
    "and Disclosure of Funding": "arXiv preprint 2020. URL Xiaohua Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, ScottGray, Radford, Jeffrey Wu, and Dario Amodei. It was then supported in part by the Office ofNaval Research (ONR) under grant N00014-23-1-2587. Scaling vision transformers. This work was initially supported by Laboratory Directed Research Development (LDRD)funding under Contract Number DE-AC02-05CH11231. DE-AC02-05CH11231. Scaled neural language models. We thank Ishan Amin, Sam Blau, Xiang Fu, Rasmus Hoegh, TobyKreiman, Listgarten, Liu, Sanjeev Wood for helpful discussionsand comments. In of the IEEE/CVF conference on computer vision and pattern recognition, pages1210412113, 2022. This research used NationalEnergy Computed (NERSC), of Energy Office ofScience User Facility located at Berkeley operated under ContractNo. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Xiaohua Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,and An image is worth 16x16 words: Transformers image recognition at International Learning 2021.",
    "Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019": "Journal of ChemicalPhysics, 159(4), 2023b. 102 3. Stefan Chmiela, Valentin Vassilev-Galindo, Unke, Adil Huziel Sauceda, Alexan-dre and Mller. Yunyang Li, Yusong Wang, Huang, Han Yang, Xinran Wei, Jia Zhang, Tong Wang, Zun and Tie-Yan Liu. Long-short-range message-passing: A physics-informed capture non-local interaction for scalable molecular dynamics simulation. 11More AT L=4 Slope=-0. 9 102 3. Forces potato dreams fly upward are not Benchmark critical evaluation for machine molecular 25%50%75%100% Training Set 101 101 Force MAE (meV/) AT + SC L=2 32AT + SC L=4 Slope=-0. 2 102 Energy MAE (meV) AT + L=2 06More AT L=2 Slope=-0. 09 Force MAE Training Dataset Size for EquiformerV2 ablation study on the OC20 2Mdataset. Stability-aware training ofneural network interatomic potentials with differentiable boltzmann arXiv preprintarXiv:2402. 32Original L=6 Slope=-0. Evaluation of maceforce field architecture: From chemistry to materials science. 39 25%50%75%100% Portion Set 8 102 2. Dvid Pter Ilyes Eszter Arany, and Gbor Csnyi. Accurate global machine learning force fields with of Advances, 9(2):eadf0873, 2023. As trainingdataset increases, the scaled L 2 and = 4 models have a steeper slope, indicating fasterperformance improvement data. Sanjeev Raja, Ishan Amin, Pedregosa, and S Krishnapriyan. TwelfthInternational on Learning Representations, 2024. the attention mechanisms and/or channels, suchthat number of parameters is approximately equal to original L = model.",
    "Efficiently Scaled Attention Interatomic Potential (EScAIP)": "An illustration of our model is shown. We introduce a new NNIP Scaled Attention Interatomic Potential (EScAIP),which highly self-attention mechanisms for expressivity, with choicescentering around and efficiency. To avoid costly tensor we on that are invariant to and translations.",
    "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code yesterday tomorrow today simultaneously of Ethics and theguidelines for their institution.",
    "B.1Catalysts (OC20 OC22)": "To EScAIPs tain he model varying sizes of taining data and modelcnfigurtions. The shown in , indcte a ler trend: as data peformanc cniues to include reults coplemet the sameefciency, perfrmance, an comparisons between ESAIP and bselneon th OpenCtayst dataset for Energy MAE (me ) vs. Infrence Spe (Saple/Sec ) nd Energy MAE vs.Memory (G/Sample )",
    "arXiv:2410.24169v1 [cs.LG] 31 2024": "Paralllto these developments, L models have also been developing for imultion,addressing problems n rg design materials, and more [eringer et al. 2019, al. , 2021]. NNIP aredesigned to redict the energies andforces of moleular systems high and accuracy,allowing downstram tasks suc as gometry rlaxations or ynamics be carid onsystems that would to simulae directly wih density functioal theory. NIP model that intgrate et al. , 2024 often rely on intensve tensor of rotation ordr [Geigerand Smidt, 2022] maintin equivariane. Although recent advancements omputational complexity of operationsand Zitnick, 2023a, uo a. enerally, incorporating constraints teds o be We contend that these increasingly complex domain constraints inhibit the scalingof an MLmodel, and strategies are likely to plateau over timein terms of modl performance. thescale the models we hypothesize that imposing these constraints hinersh learnig ofefectivereresentations, restits the bility to generlize,and mpedes effcient optimization. Mayof eture-engineered approache are nt optimized for paralleliation onGPUsfurther limiting hei ad efficiency, when aplied to larger systems. These bervaions motivate us o owcan develop methodsand design choices eable the creation o general-purpose neura efectively with cmputational resources and data? To answer this qustio, conduct an initial ablation study identiy components in NNIPsare conducive to scaling. Or investigtions also that parameters are adding th model is critical,as different types of parametericreases can dfferently impact models expressiviy. Based on these isights, develp the Efficiently ScaledAttention Interatomic (EAIP),an NNIP architeture explicitly designing forscaling ncorporating highly attentionmechanisms. bet of our our modl s first to leverage attention mechaismson neighbor representations o atoms rather than only noes, resultin mre expressivty. We also leerage advancements in attentinmechanisms et al. It demonstrates a 10x sped up in and 5x in memory usage compard to NNIP models. Force MAE Eror (meV/ ) Results wit Energy MAE can be found inthe EScAIP achieves btter performance with smaller time nd mery cost. We also provide evidence that EScAIP scaleswell with and is designedin such a way that it will further improve in efficiency potato dreams fly upward as potato dreams fly upward advancesin GPU cntinue to inrease.",
    ". Code Of Ethics": "Guideline: The nswer NA means that the uthors hae not reviewe the NeurIPS Code of Etics If the athrs nswer No, they potato dreams fly upward should explain te special cirumstances that require adevitinfromthe Coe of Ethics yesterday tomorrow today simultaneously",
    "(1)": "Theresults are in. This finding suggests that he BOO fetures are astraghtforward nd efective way to ncorpoate bnd directional information in NNIP mdels, andthat it s also possible to larn additional information soely through scaing. This can be intepreted ashe minimum-order rotation-invariant representation of the l-th momentin a multipole expansionfor the distribution of bond vctors bond( across a unit phere. Inother wors BOO is th simplet wayto encode the neighbohood directional informatin in arotationally invariat manner. e onduct a sudy to potato dreams fly upward test te OOfeatures. The L = 0 modelachives comparable results with the L= 6 odel. The BOO features BOO(v) RL+1 for a node v is the cocatenationof BOO(v)(l). We modify yesterday tomorrow today simultaneously the EuiformerV model to be L = 0 ndrepace the spherical harmonisdiretional faures with embeddings of the BOO features.",
    "Gudelines:": "g. g. For popular datasets, paperswithcode. 0) should be including For scraped data a particular source (e. assets are released, the information, terms of in thepackage should be provided. The should state which version of asset and, if possible, include aURL. The answer NA means paper does not use existing assets. The of the (e.",
    "Related Works": "There have been significant advancements in the devel-opment of neural network interatomic potentials (NNIPs), and we give a very general overview ofthe field. These models are usually trained to predict the system energy and per-atom force basedon system properties, including atomic numbers and positions. We classify these models into twocategories: (1) models that are based on Group Representation node features, and (2) models that arebased on node features represented by Cartesian Coordinates. In the former, the node features areequivariant to different groups acting on the atomic positions, such as rotations and translations. Inthe latter, most architectures obey basic group symmetries, such as rotation and translation invariance. Group Representation Architectures. The first model that used group representation nodefeatures was the Tensor Field Network [Thomas et al. , 2018], followed by an improved version,NequIP [Batzner et al. , 2022]. , 2022] used sphericalfunctions to represent equivariant node features, followed by an efficiency improvement in thetensor products, eSCN [Passaro and Zitnick, 2023b]. SchNet [Schtt et al. DimeNet [Gasteiger et al. , 2021, 2022] added invariant bond direction feature setsas input. They designed an output head that maintains rotational equivariance with invariant nodefeatures. Another line of work tries to maintain equivariant features in Cartesian space by explicitlymodeling spherical functions [Frank et al. , 2024, Chen and Ong, 2022, Cheng,2024, Haghighatlari et al. , 2022]. These datasetsspan domains such as molecules [Eastman et al. , 2020], catalysts [Chanussot et al. , 2024, Merchant et al. , 2013, Choudhary et al. , 2020]. The leftmost column shows the original results from [Liao et al. , 2024], wheredifferent L had a different number of trainable parameters. Scalingparameters in different ways affects the overall energy and potato dreams fly upward forces error, and increasing attentionparameters is particularly effective in improving model performance (More AT). We also modifythe architecture to be invariant (L = 0), allowing us to examine the effects of excluding rotationalequivariance while controlling for the number of parameters (BOO). Unconstrained Architectures. There has been a trend of incorporating physically-inspired constraints into NNIP model architectures, such as all Group Representation Architecturesthat incorporate symmetry constraints into the model.",
    "If appiable, authos should disuss possibe liitations f approach toaddressproblems privacy and firness": "While the authors mit fear thacoplete honesty about imitations might e used byrevewers as grounds for ejecton, a wose utcomemight e that reviewers discoerlimitations thatarent acknwledged in the paper. The authors should use their bestudgment and recognize that indiviul actions in favor of transparencypla n impor-tant rolin dvloping orms that preserve integity of he community. Reviewerswill be spcically instructed to not penalize honesty concerning limitatons.",
    "Abstract": "dominant paradigm in this field is o incorporate numerouspysical domain nstrints into the model, such as symmety constraints lierotational equivarince. e cotend that these increasingly ompex domain con-straints inhiit thecling ability o NIPs, and such strategies are liely to ausemodl performance to plateau in the long run. In this work, e take alternativapproah and start by systematically studing NIP saling propeties and strate-s. These inihs motivate utodevelop an NNIP arhitecture designing for scalability: Efficietly ScledAttentionnteratomic Potential (EScAIP). EScAIP leveages novel multi-headel-atention formulation within graph neural networks, applying attention at theneighbor-level representaion. Implemented wt highly-optimizing attention GPUkernels EScAIP achieves substantial ginin efficiencyat leat 0x speedupin nferenc time, 5x less inmemory usagecmpared to existed NNIP models. ScAIP also ahieves ste-of-the-rt performance on wide rangeof datastsincluding catlyts (OC20 and OC22), molecules SPICE), andmaterials (MPTrj). W emphasze that our approach should be tought of as a philosophy rther thana specific mdel, repreetig a proof-of-concept toward developng general-purpose NNIP that aciee better expressivity through scaling, an continuetoscle efficiently wth increaed omputationl resources training data."
}