{
    "Jigyi Yu  light-field jorney reality. IEEE Mul-tiMei, 2017. 1": "* \"S 1 incet itzann, Semon Rezchiko, Bill Freeman, JoshTenenbaum, nd Durand. In Proceedings of the IEEE/CVF Con-ference Computer Vision Pattern 2022. Springer, 202. In European Conference on pages 6262. Lighnetworks:Neralscene representations single-ealuation Advance in Nerl Ifomation Processing Sstem,34931319325, 202. Hua Wang,Jian Zeng Hung, Kyle Me-glei Chai, Yu F, nd Sergey Tulyakov. Learnin neura ight felds wihra-space embedding. Gochang Yein Li, * \"S Fng, and Tianyo Chai field enderng with deep anti-aliasing neuraletwork. IEE Transactins on Anlysis and Ma-chn Intelligence, 4(9):54305444, 2021.",
    ". Ablation Study": "In thisseion, we further proe theeffectiveness o severalcr of the proposed LGFN modelthrugh ablaionepriments. 1) Connection of ECAM ESAM module. connecto modes of AM a are classi-fied into cascade connection and connectin. As in , the LGFN-P is better thnLGFN-C. The model this ape is LGFNP. The DCEis used toextract local feature.To * \"S demnstrate the effctiveness ofthe DGCE module, we remov this module and use ECAM ad modules. As sown in , thePSNR isdereased dramaically 31 807 to28. 7796 dB for 4x SR without DCE module, and dropvalue is 3. 028d. The EAM module is used ex-tract LF image spatial featur. demonstrat theeffectiveness o he ESAM module,we th module. CAM. The ECAM modul is sed feature. To demostrate efectiveness ofthe modue, this modle. shon in, nd PSNRvalue is decreased from 7804 for withou ECAM odue the dropvalue 0 72dB. LGFN w/o ECA and SA.Te ESAM andECAM module are used o extract global odemonstrate the efectiveness of the EAM and ESAM modules, wethee modules. to 1 for SR without them, the drop vale s 0. 1614dB,which the ffectieness of atentin",
    ". Network Architecture": "Meanwhile, t is convertedt F0 R1UV HW and passed 133spatal convolution to extract the feture Finit, umber of is increased 1 to 64:. Specifialy, as illsrting in , our F SR modelainly compoents shallow feature ex-tractio, deepfeature extractin and up-samplng mod-ul. in the shallow feature extraction the low-resoluton4D is upsampling bilinar nte-polation to the ize sW. Givn an input LF low-rsoluton FLRRUV denote SAI array with Uresoluion.",
    ". We design an efficient channel attention module namedECAM and use the statistical information of channel di-rection to model the correlation between different chan-nels": "We propose light-weight imagSR which hs aparamterof 0. 33G. 45M an FLOPs of19.",
    ". NTIRE 2024 LFSR Challenge Results": "As shown in , we proposed model the second place n the rak 2 & Effi-ciency of NTIRE2024 Light Field Super Resotion 30. Theset ofLFSR challene including 16synthetic LF and 16 rel-world LFs captured by ytocam-era.",
    ". Introduction": "LF cameras ca capture varying intensities directionsof liht within th scene, encodin 3D sceneces 4D LF (comprising and angulardimensions). However,du to the ltation ofsensor exists betwee the spatial resoution andangular resolu-tion of images How to improvethe LFimages i pominent research challenge. traditional LF SR method ocuseson how to ind sub-pixel inforaion and wrp multi-viewimagesestimate dispaties. Howver, the per-formnce of these methods depeds on accuaestimated disparities, which is difficult to cieve in LF magesand imging as occlusio non-Labert n ecent deep learning-basing methods hve benwidey",
    "Ours": "51/0. 84 Perforated_Metal_3. The center view images presented for detailed texture comparison. The corre-sponding PSNR/SSIM of methods on the presented scenes are also reported below.",
    "Methods#PramsLytroSntheticAverage": "900128. 935429. 25 / / 1. 11 0. 902829. 936929. 8378VDSR 0. 40 / 0. 67 M27. 898128. 9198MEG-Net 1. 49 / M27. 875727. 24 / 0. 54 / / 0. 21 / 0. 09 0. 34 / 0. 31 / 0. 877327. 40 / 0. 23 / 0. 9183LF-InterNet M29. * \"S Bicubic25. 78 0. 9203. 901229. 900829. 61 / 0. 8869RCAN 15. 36 M27. 870327. 65 / 0. 46 / / 0. 36 M29. 840426. 933729. 54 / 0. 87 / 0. 96 / 0. 8887resLF 8. 926029. 21 / 1. 05 / / * \"S 0. 77 M29. 03 0.",
    ". CNN-based Methods": "Zhang et al. te fist CNN-baseLF image (i. charac-teristics ointeral similarity and of fusd these two complemntary featue SR. proposed a learing framwork. Yon al. , LFNN) which used SRCNNto ser-rsolve each SI. et l usesEDSR to eac SAI. proposed the firt real-world LF SR datasetcalle proposed nomi-frequency * \"S pro-jection network(OFNet) which with the spatiallyvriant degrdatin by diiding features into differen fre-quncy cmonents and trativelythm. addion to directly prcessingte 4D LF data, some disentangld the LFsio diffeentsubpace for SR. popsed a bidirecional rcurrnt NN iteratively spatial eltins between horion-tlly r verticaly adjacent SAIs. et al. hey dividdthe odel tranin * \"S task into three subsks:pre-upampling, ignmen multi-view aggregation, tackled ac f these tasks spa-ratly by using simple yet efficint NN networks. methoddopted the methds to inegrae informtion to improve perormance of et al. o aress he of thomain gap LF image Cheng al. proposed a 3D covolutionsnetwork features frm SAI image along differentangular diretions. e.",
    "fficiet Spatil Attention Module": "wing the positionbeyond te boundaries in im-agresets * \"S a lrge requires aggregate con-textfeatures SAIs, therefore we ye efficient spatial ttention * \"S module, as shown n(c. In order to icreasethe fiedattetionthe large-kernel decomposed int a depth-we dilated convoluion and a 11 point conolution,wichcan capture long-range relationsips wie matini lo",
    "ECAM": "overview our LGFN network (a) Loal global feature extraction module (LGFM); (b) ouble-ated convoltionetracton (DGCE); (c) Efficient spatial atention (ESAM); (d) Eficen channel attention Given SAIas inputs, we adopt upsamping intial content the original images. depth eature module (DFEM) incldes seven local and global ature extaction hichar composed o DGE, ESAMand comptatinal e parmeters. thespatialresoltion to scale up-sampling,and umber of cannels is rtoring o the oriinal num-ber by a convoluton. Therfor, ttention a large re-ceptie feld isobtained,hich convenient for next at-tention",
    ". Traditional Methods": "adopted opimization solve LF SR problem basd sparsiyprior. ed EPI o estimated aps and proposed framework for LF image SR. Alain et al. Beides, optiization-bsedmethodhave benpoposed. Wanner et al. pro-posed an example-ased LF image meh, enhancingspatial resolutio across SAIs through learn-ing lier proectios from educe-dimension angular via multivariate ridge regres-sion. The traditional LF SR mainly focuses onhow find sub-pixe information and warp ult-view im-ages on estimateddiparities. ouped informationwih a graph adopt cnvex ptimizationmethod to olve image problem. However, * \"S the methods on stimated disparities, it difficult toachieve LF images an environmensas non-Lambertan surfaces orocclusions.",
    "Mode#ParamDGCEESAMECAMEPFLHCLnewHCLoldINRIASTFgantryAveragePSNR": "6k30. 514536. 285330. 082330. 8076BaselineCascade453. 6k30. 048130. 053331. 8014-0. 0062-452. 550936. 447031. 783830. 302031. 7804-0. 0272-409. 549036. 077832. 911431. 1614Parallel147. 0k27. 750428. 956426. 700328. 7796-3. 0280 Data Augmentation. LFs in released datasetsused the downsampling approach LFpatches of * \"S size 3232. We performed random horizontalflipping, flipping, and 90-degree rotation to aug-ment the training times. Note the spatialand angular dimension need to be flipped or rotated maintain LF structures. Regularization. 01 and 1respectively. Optimized the Adam method =0. 9, 2 = 0. 999 and a batch size of 1. learning rate was initially set to 2x10-4 factor of for every 15 epochs. The training wasstopped after 100 epochs. To compute the metric scores for a dataset con-taining scenes, computed the average score ofeach scene by separately averaging the scores all Then metric score for the dataset determined by averagingthe scores over the M scenes.",
    "Yuan, Ziqi andLijua Su Light-fiel iage super-resolution usig adeep cnn based ei. IEEESigal Letters 25(9):13591363, 2018. 3, 7": "Zhen Cheng, Xiong, Chang Chen, Dong Liu, andZheng-Jun Zha. In Proceedings of the computer vision and pattern recognition, pages 1001010019, 2021. 3 Zeyu Xiao, Ruisheng Yutong Liu, Yueyi Zhang,and Xiong. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 34073417, 2023. Wang, Zhengyu Liang, Longguang Wang, and Yulan Real-world light field imagesuper-resolution via degradation modulation. 3.",
    ", ESR, RCANand recnt LF im-age SR methods: resF, LFSSR , LF-ATO, and EG Net": "Results.As shown in , com-pared with other models larger our modelis very lightweight has achieved competitive results.Specifically, the our are smallest,our is only of the of MEG-Netmodel, but it achieved better average PSNR the lightweight characteristics of our model has achieved remarkable results onEPFL and INRIA datasets. Qualitative shown in , regarding qual-itative performance, the propose has ability produce trustworthy details and sharp struc-tures. SISR methods, VDSR, EDSR and RCAN tendsto artifacts, and the texture details are notclear LFSR methods, the ability discriminate more dense Specifically,in figure ISO Chart, the figure recovered by our model than other and there are fewer artifacts. Inthe Perforated 3, the graph restored ourmodel has more material texture. . Our team achieved second the leader board (last rows) in the NTIRE-2024 2 Fidelity & Efficiency test dataset,with quantitative results of PSNR (average) and 0.924 SSIM (average).",
    "A21EKYN00884B03, in part by the Nat-urlScience Foundation of Fujianunder GranNo.2023J01083": "Epinet A fully-convolutional neural net-work used epipolar gemetry for depth fromlight field im-ages. In Proceedings of th IEEE onferene on computevision and pattern recognition, pages 748475,2018. Ocluio-aware cost con-structor for it field depth estimation. Leaned su-pixel disparity di-tribution or lght feld dpth estmaion. IE Traactionon Cmputational maging, 9:11261138, 2023. ACM Transactions onGraphics TOG), 37(6):115,218. 1.",
    "Methods#ParamEPFLHInewHCIldINRIASTFganryAverage": "Biliner-24.5 / .815827.09 / .83971.69 / 0.925626.23 / 0.875725.20 / .82616.95 / 08566Bicubic-25.14 0.832427.61 / .85732.2 / 0.93442.82 / 0.845227.58 / 0.87729.31 0.88334.81 / * \"S 095529.19 / 0.92042851 0.900929.81 / 0.9066EDSR 3889M27.84/ / 0.88635180.953629.66 / 0.92578.70 0.907230.20 / 0.9118RCAN 15.36M2788 / 0.888635.20 / / 0.927628.90 0.913130.2 / 0.9141 resL / 0.903530.3 / 0.90736.71 / .968230.34 / 0.94123.19 / 0.37235 / 0.9322LSS 1.774M28.2 / 0.911830.72 / 0.914536.70 0969630.31 / 0.946730.5 / 0.942631.23 / 0.9370L-ATO 1.364M28.52 0.911530.88 0.91337.00 / 0.969930.71 /0948430.61 / 0.943031.54 / 0.916230.98 0.91613.11 / 0.97630.61 / 0.49130.3 / 0.9388",
    ". Method": "As mentioned above, the LF image SR needs to considerthe local similarity between SAI subgraphs on the one hand,and the disparity problem behind different subgraphs on theother hand, which urges us to consider the methods of localand global feature extraction. In order to design a lightweight model with fewer param-eters and FLOPs, we choose to reduce the high-dimensionalfeature space to the low-dimensional feature subspace, anddesign an efficient local and global feature extraction modelto achieve LF image SR.",
    "Mattia Rossi and Pascal Frossard. Graph-based light fieldsuper-resolution. In 2017 IEEE 19th International Workshopon Multimedia Signal Processing (MMSP), pages 16. IEEE,2017. 3": "2, 3 ShuoZhag, Youfang ndHao esual net-works for lght image super-rsoluion. Marti Alain an Aljosa field denoising bysarse 5d transform domin collaborative filtering. 3Yingqian ng, Jungang Yang, ongguang ng, XiyiYing, Wu, An, an uan Gu. 3,5, 6. IEEE transatins onpattern analysi and macine intelligence, 43(3):873886,2019. 1, 3 an Meng, K-HXing ad Emund Y High-dimensional denseresidual convolutional neural net-worfor recnstruction. Light field i-age ser-resoutiondeformable convolution. IEEE n Circuts Systemsfor 2019. IEEETranactions on Image Processing, 30:1051071, 220. Prceeingsof the IEEE/CVF cnrence on compue viio and patternrecognition, pages 110411055 2019. In Procedingsof the EEE international conerece n copute pages 2432, Lfnet: Anovel bidirectionalrecurent covolutional ntwork oright-field imagesuper-reslution. 2017IEE19th International Workshopo Multimedia (MMSP), pages 16.",
    "ur contributioncan be summaried as:": "1. We design a lightweight convolution modulation modulenamed DGCE the local spatial features of LFimages. A lightweight spatial module namedESAM with enlarged field is designed to ex-tract features. In order further featureextraction, we extract the local global features alongthe vertical directions",
    "Double-Gated onvolution Extraction Mdule": "haf of the eatures underoes 33 deph-wise convoution nd ELU function e halfof the features undergos multipliction wth pixels to * \"S obtain he cl ftures. Sme studieindiate ht moduation mecanimprovdes perforance an is theoeticall (n ters o FLOPs). After are adde to they are fused y a. Owigto neighboring regions same ixel positio ndifferent SAIs exhibit imilar structural relatioships, whiis suitable for withlocal fature extrcionmod-ule. Therefore, weesin fetur extractionmdule based on featuremodulation, a In order to etract thelocal featues better, the shallow eatres firt ndero a11 convolution, then ino two alongthechannel.",
    "shown in adjacent areas at the same pixe po-sition cross SAIs exhibit similarstructural rl-": "To consider these two apets and the reqirement oflightweight mdel, we a lighweiht local learningmodl namd LGFN. Dfferent from the existing CNN-bsed methodswhich complex etwork structure, e propose yet efficientcnolutin * \"S designed to features trough featre modulation convolutio branches. hnd, the position ut-side the boundar LF exibis a parallax,whicagggaion of contexual features acrosSAIs or presn. ourmoelchooes convolution oca representation. Inaddition,wechoseattntionmechanismoextractcotxtul features. By integra-i local and global features of views of dfferent channels, ur lightweiht achieve results coparing with exst-ing mode parameters. Different fr the with quadraticomplexity over of isual proposea siple ye attention module,whose wghtbranch decomposabl convo-lution to obtain an enlrged eceptive field, multiplie itwith identity brnch to exract contextual In to further refine the feaure extaction, extract helcal and globalfeturs along the horizontal and verticaldietons respectiely."
}