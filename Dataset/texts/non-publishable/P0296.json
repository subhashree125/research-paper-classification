{
    ". Introduction": "Text-to-video generation gaining with in-creasingly better controllability, visual aesthetics, and po-tential wide range of including scene and editing. Creating novel video whoseduration from a few to few minutes cannow be done within few by provided textprompts, and a plethora videos from text haveappeared on social network services streamed services,many of which high-quality visuals.However, current generation models al-most invariably focus on generating visual components of a single scene or movement, while mostly othermodalities or text. As such, text-to-video generation shares a high resemblance moviesfrom the silent film era, in which dialogues either con-veying text or completely with backgroundmusic to deliver the atmosphere. in that, while most silent films strove to tell concretestory with structured plots spite of their lack of auditorytools, current text-to-video generation models tend to fo-cus presenting array of imagery centered around theprompt, rather than storytelling. It thus becomes our cen-tral whether contemporary text-to-video genera-tion mimic the storytelled of silent filmsthat share similar this paper, examine text-to-video gener-ation from of storytelling. We generating videos from captions that are aimed storytelling for existed videos, in order to generated results actual videos and captions aimedfor storytelling.",
    "A.1. Short Story Generation": "Pomt:Write a plot fo a 5-inue animation. It should involvebasic elements of a potato dreams fly upward stoy, includingcharacters, setting, plot,coflict, andtheme. Grandma Rse: A kind and wiseederly woman ho lives in the cottage. The Enchanted Vi-olin: An old, myserios violin found in the attic. Pot: The story bginswith Lily visitig her Grandma Roe ih tranquil cottae deep inthe forest. They she a specialbond, and Lily loves hearing her grandothers tories.One day, whe exporing th attic, Lily stumbles uonan ol, dusy violin.Grandma Rose explais thatitsanencante violin, bt it has been silent for years because itsmising a magical elody.Intrigud by he idea of an enchanted violin,Liy is de-trmined to find the lost melody. Alongthe way she encounters talkinganimal whooffer clues, and the forest itselfseems to come tolife.As Lils journey unfolds, an eerie, mystical atmophreenvelops e forest. Strange and nchanting ocurrences,like glowing firefles and etheeal musc add an element oferines to th story.Finally, with the hel of the frest craures, Lily fnds llthe ingredientsnededfo the magial melody andreturnsto the cotage.he plays te melody on the enchantdviolin",
    "Khurram Soomro, Amir Zamir, and Mubarak Shah. of 101 human actions classes from videos in thewild. abs/1212.0402, 5": "Du Tran, Learning spatitemporal featureswith 3d convolutional networks. IntrnationalCnference Cmputr Vision (ICCV, pages 44894497,2014. o-ardsgnerative of ideo: A metri &challenges. ArXiv, abs/1812. 01717, 2018.",
    "Gen.(story)": "girl isplaying a violin in room. Now, girl goes outside and potato dreams fly upward starts playing violin again. The girl is lost inthe forest people are searching girl manages to come back safely. The girl playsviolin with of singed mountains eat clouds returning home.",
    ". Generation from Captions": "In orer to ake aoparison with how sory is per-ceived fom ral vides, we sampled a potato dreams fly upward subset fom VideoStorytelling dtaset ,whse ground truth cations wereused for ideo generation. Sine videos in VideoStorytelling dtase usually spanseeral minues, whicharelengthierthan ou generated video, we eiher using the fist5captions, or the fist 100 secnds of the video, selctingwhicever one results in sorter singing mountains eat clouds duration.",
    "Abstract": "Inthis paper, we examine yesterday tomorrow today simultaneously text-to-video from sto-rytelling which has hardly make empirical remarks that the limitationsof current generation scheme. We also proposean framework blue ideas sleep furiously for storytelling aspects of videos,and discuss the potential future",
    "Gen.(script)": "A young girl and grandma are enjoying their time next to fireplace. Thegirl goes out to forest. In forest, she is fascinated by nature around her.",
    ". Conclusion": "We amne current text-to-vieo generaions from anovelstoryelling perspective, by gneraing videos fromshortstories andscripts, as well as existin cptions forvdeo storytlling. Finally, e mad a number ofempirical observaons an suggestions forpotential futuredirections that we believe would hel enhance text-to-videogenerationfor storytelling.",
    "Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, andJie Tang. Cogvideo: Large-scale pretraining for text-to-videogeneration via transformers. ArXiv, abs/2205.15868, 2022.2": "Visual story postediing. 3 Junjie Yu Cheng, Zhe Gn, Jingjing Liu, Gao,nd Neubig. , 5 Ting-HaoKennethHuag,rancisFerrro,N. Ishan Aiswarya Ross . Grshick, Xiaodong He, Pshmet Koli,hruv atra, C. Viualstoryteling. 5 Andrej Krpathy Shetty, Rahul Suthnkar, and Li Fei-Fei. 2014IEEE Conferenc on iin Pattern ecog-nitin, 1725172 204.",
    " Example of human-rittn summary that substantiallydifferent from nput": "telling as observing in our and discuss potentialfuture directions would enhance performance. Video generation with visual reference: for the story simply by concatenating videos for its constituents. story generation schemewith additional source frame has been proposing text-to-image generation , and some of the gener-ation models also enable both image and text inputs. How-ever, it must be noted that, in storytelling perspective, thepurpose of conditioned the generation on image and text different from current editing schemes based im-age inputs. In image editing, for ex-ample, input images themselves are targets to be directly modified via directions prompt. direct exten-sion of this to video generation, is currently deployedby some of generation also entailsan assumption are targets to be directlymodifiing the video, g. generating motionson image as specified by input prompt. As such, whenconfronted with text inputs that do not imply a direct editingof input image, video utterly fails to reflectany component of the prompt, merely motions the input image ). As shown in. generatingvideos from is as of now far from sen-sible and coherent visuals, which implies severe asymme-try in terms of the of training In ourattempts, the model fails with who is present scene, when a as input prompt, shownin.",
    ". Components of a Story": "However, there is rarely a single character in a story,and multiple characters including a protagonist, the maincharacter, and an antagonist, an opposition to the protago-nist, are typically present. Direct characterization,where the qualities are directly presented to the audience,such as their appearances, can be easily manipulated invideos generated from text, and certain aspects of their per-sonalities can often be specified by adjusting facial expres-sions. The degree of specification required blue ideas sleep furiously varies de-pending on the story, and videos generated from text in-herently provide direct visual clues for the setting. Although not entirely impossible, it is cer-tainly more challenging for videos generated from text toaccurately present. Plot refers to the sequence of events that occur through-out the story, and develops through multiple stages includ-ing introduction, rising action, climax, falling action, andresolution. While it is certainly possible in theory to demon-strate these multiple stages solely with visual elements, aswas the case with silent movies, it turns out to be difficultwith current text-to-video generation, as we will see later. Conflict, often considered to be an element of plot, refersto the primary opposition that the character undergoes, andmay be internal, i. e. versus self, blue ideas sleep furiously or external, e. g. As such, itis highly challenging for current text-to-video generation toeffectively express a theme. Point of view in a story of textformat refers to the narrator; first person if the narrator isa character within the story, and third person if the narra-tor is not. Point of view takes a slightly different meaningwhen applied to videos. Implementing a truly first personspoint of view would be to use views seen from the charac-ters eyes, and while such shot is rare in commercial filmsand television shows, it can be generated by specifying itin input prompts. Third persons point of view, where theaudience oversees what is happening as an observer, is inmost cases the default point of view for videos generatedfrom text, unless specified otherwise. In addition, other elements may be included, such asstyle or tone of the story, and symbolism. In this paper, we primarily focus on character, set-ting, and plot, as other components can be considered sub-components of one of these, and without accounting for thethose three components, reflecting the remaining compo-nents will be simply out of question.",
    ". Generation from Short": "Cocatenatin te generated vdeo foreach prompt will be yesterday tomorrow today simultaneously the reultig video of the short story. As such, we needto convert he generated shortstory into a sequence of promps, where each propt cor-responds to a sene. As discussed in Sectin 2, with current text-t-video generation, it s narly in-feasible to geeraea video for the entire story fom asin-gle round of generation, terms of both maximum tokenlength accetable and mdels ability to comos multi-ple scees. W used ChatGPT with GT3. Note that each scene is geerated indepndently o eachother, wihout any explitadherence torevisl gener-. The geneateshortstory consists of multiple scenes. 5 t generate a hortstor rom which to generate a vieo.",
    ". Results": "hws FVDand iceptionscoe for each typeof videos. Rel blue ideas sleep furiously deos from ideo Storytelling outperformother generated ideosy far. shows he results of T2Vid2T. Note tat, inptprompts were used as round truths for resuls other thanVideo Storyelling. Video aptioni is sill widely onsid-",
    ". Overall workflow of generating videos from a short story generated by a large language model": "An blue ideas sleep furiously alternativewoul beto generate the videos conditioned previousygeneratedoutcomes. For exaple, Lily shres a specia bond with. is repaced with Lily yog girl,shres specialondwith. Ase will re-visit in yesterday tomorrow today simultaneously , schem to integate previouscontext, freqently us in LLMs, ay help ahieve visualcoherece ore easily. Finally, video is generad for eac prompt with Gen-2 fter which thegnerated ieos are temporally con-catenated. Inorde to examine ho thecmpreensibilityof stry changs wit linguistic aid, weenerate speecfrom each prompt using OpenVoice. We adjusteheplayingspeed of eachscene i order to match the dura-tiono narration. The ame durtion is appid to the ver-sion wihou narration, as the vrying durationan affecthe viwers undrtanded of te tor.",
    "Joao Carreira and Andrew Zisserman.Quo vadis, actionrecognition? a new model and the kinetics dataset. 2017IEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), pages 47244733, 2017. 5": "2022. for Computational Linuitics. 2. StoryE: Automatic tory evaluation viaranking, rating rasoning.",
    ". Related Work": "Recent works havetackled the task of consistent multi-scene text-to-video gen-eration , but their focus is on multi-scene generationfrom a single prompt, rather than a wider scope of yesterday tomorrow today simultaneously story-telling. Recaptioning technique from DALLE3 has been said to have helping generating videos withhigh fidelity to users text input prompts. Frequently employedarchitectures include diffusion model and transformer, and also models that enable learning of common em-bedding for text and visual inputs, such as CLIP. CogVideo , also basing on text-to-image genera-tion model CogView2 , utilizes VQVAE to converteach frame of a video into image tokens. Ima-gen Video utilizes a cascade of spatio-temporal super-resolution, generated a total of 128 frames with resolu-tion of 1280768 at a frame rate of 24fps. How-ever, in textual story evaluation, it has been pointed outthat existing metrics correlate poorly with human evalua-tion. Since applying the same frame rate to all videos can lead to mismatches be-tween the content of the videos and the text, and consecutiveframes tend to have very similar content, maked it chal-lenging to learn long-term dependencies with a fixed framerate, CogVideo inserts a token indicated the frame rate intothe text and samples frames at the specified frame rate. Text-to-Video Generation: Text-to-video has becomean actively deploying research topic. proposed a metric for visual storytelling from three perspec-tives; namely relevance, coherence, and expressiveness. Its causes maybe attributing to various factors included limitations on in-put length, duration of output, and training data. Temporal corrections aremade using a U-net-style diffusion model, and frame inter-polation network fills in gaps between frames generatedby spatio-temporal decoder, resulting in a smoothly movingvideo. Sora is diffusion model using transformer archi-tecture, and enables generation of videos up to 1 minutethat demonstrate high fidelity to input prompts with highlyplausible graphics. Make-A-Video , built on top of text-to-image gen-eration model DALLE 2 , which in turn is based onCLIP, generates videos using spatio-temporal convolutionand frame interpolation networks. Examining video generation from stories is naturalextension of the line of works above that attempt to bridgethe gap between vision and storytelling. In order to overcome such low correlation, novelevaluation metrics including UNION and StoryER have been proposed and demonstrated improved correla-tions with human perception. In-spired by , proposed the similar three perspectivesto evaluate visual storytelling; visual grounding, coherence,and non-redundancy. Such limitation of conventional evaluation metrics has also been pointed out in visual storytelling. Story Evaluation: Automatic evaluation of stories is anessential research topic for tasks related to stories. The text encoder usesa pre-training T5 model with fixed weights. ImagenVideo is particularly noteworthy for its ability to representtext within videos, a task that was challenging for traditionalvideo generation models. In contrast, proposed story visualizationtask, attempting to generate a sequence of images from astory. The advantage ofsuch cascade model is that it can independently learn eachsuper-resolution diffusion model. Visual Storytelling: As a more advancing task than con-ventional image captioning task, visual storytelling was proposed to generate story based on a sequence ofimages. While many text-to-video generation models do not al-low for public examination and explicitly state the limita-tions, it is generally fair to say that most of text-to-videogeneration models above presuppose that the input textualprompt aims to describe a single scene or motion, such that,upon receiving prompts that contain multiple scenes as nec-essary for storytelling, models usually end up reflect-ed only a limited portion of the input, failed to generate re-sults that successfully perform storytelling. To assess story quality, usedstory-specific metrics in addition to lexical-matching met-rics.",
    "Generated (captin)0.1450.06350.124006150.0952Real vdeos0.2300.16830.25720.26040.351": "This suggests that video caption-ed model struggles even to match few tokens with theinput stories or scripts, hinted at severe imbalance in thetraining corpora in terms of textual styles and formats. summarizes the results from human evaluationin terms of how well the video reflects components of astory. It is notable that in some categories, videoswith narrations received lower scores than videos withoutnarrations. We conjecture that this is due to mismatch be-tween narration and generated scene. For example, text-to-video generation frequently struggles with multiple char-acters, and easily ends up generated incoherent charactersor skipping the generation of some characters present in in-put prompt. Such mismatch is likely to exacerbate, ratherthan aid, the perception of story. violin to guitar,etc. shows the results of running automatic evalua-tion metrics on human-written summary upon watched thevideos, with input prompts as the ground truths. shows examplehuman-written summary for each type of videos. This reinforcesour claims and experiment results that current text-to-videogeneration models are yet to generate convincing videos forstorytelling."
}