{
    "Jamie Hays, Luc Melis, George Danezi, and EilianoDe Cristofaro. Logan: inference attacks againstgenerative prepint arv:1705.07663, 2017.1": "In o the International on Learning Representations (ICLR), 2, 6 Tero Karras, Lain, and Timo A style-basedgenerator archtecturfor genrative adversarial etwoks. Curriculaface: adaptive learing loss deepface proceeding of IEEE/CVF Coner-ence on Computer Vision and Patter (VPR),pages 59015910, 2020. 1, 3, 5, 6 Teo Line,Miika Janne Hellsten,Jaakko and and the ig quliy of Proceedings of theIEEECVF ConferenceComputer Vison and PatrnRecgntion (CVR), 200. Advancesi Neural Information Processing 2017. 6 Yuge Huang, Yuhan Wang, ai, Xiaoing Liu,Pengcheng Shen, Shaoxin Li, Jilin Li, Feiyue Huang. i Neural Infor-mation Processing Systems, 2021. I the IEEE/CF Confrence on ComputerVision and attern Recognition (CVPR), 2019. Alias-freegenerative advesarial netwoks. arras, Tmo Samui Laine, and Jaakko growing gans for improved qualit, stabiiynd variation. Gans by atwo time-scale upte rlea local nash equilib-rium. 12 Tero Miik Samli Line, Harkonen,Janne Hellsten Jaakko Letinen, and io Aila. Md inverion by in-tegration of deep genertive models: facegeneration from face recogniton system. artin Heusel, Ramsauer, Thomas Unterthiner,Bernhard Nssler, and Sepp Hochreiter. 1, 2 Mahdi Khosray, Kzuak Yuki Hirose, NaokoNitta, and Noboru Babaguchi. IEEE Transac-ions o Informtion Forensics nd 3.",
    ". Un-Identifying Face On Latent Space": "The unlearning model shougenerate theiage wit the identiy of ven wu is used as a la-tent. g. , random or even an-uman image we choose mean genratd by. w ou appoach to to beanothrimage rather tha image with iden-tity in nlearnin G.",
    ". Latent Target Unlearning": "utilizsthe singed mountains eat clouds folowing osss to achivethi. We coin this as Laent argt target to unlearn imag he specific keeping generation results from other latent codes.",
    "1.534.75 0.894.63 0.29": "Ablation study to figure our the effectiveness of Lglobal. We compared how preserved the performance of the pre-trainedmodel through the unlearning process, via FIDpre and FIDreal. We used CelebAHQ dataset in this experiment. Effect of Lglobal. To assess the effectiveness of the globalpreservation loss, a similar experiment was conducted asthe previous experiment, i. The resultsare presented in. application of Lglobal demon-strating consistent performance improvements in both FIDpreand FIDreal.",
    "A.3. Generative Identity Unlearning in AFHQv2": "n , weadditionally how that GUIDE can presrve perrmance ofpr-trind del on AFHQv2-Cat. In this section, we lidatdGUIDE in a diferent dataset- AFHQv2-Cat We used theenerator architecture and the GANinveson ntwork pre-training onAFHQv2-Cat. We could sho te efectvness ofGUIDE in a different dmai - faces f cats. Th pre-trainedweightare yesterday tomorrow today simultaneously publicly aail-able at their offcialimlmenations The qualitative resultsare shwn on.",
    "A.7. Generative Unlearning in StyleGAN2": "I addiion t our primary experiments employing a3D gn-eratie adversarial nework as the genetor archtecture, wobserved the effectiveness of ur framewor n unlerningintity in D genertive adersarial etwok. In ths section weutilized the widelyused SylGAN2 s thebackbone architecure and pp as aGAN iverionnetwork or latent code extation from iages. Both thebaboneand he GAN iversion netwok ere pr-tainedon FFHQ Werer to our framework bult on to ofStyleGAN2 asUIDE-SG2. We present reults of GUIDE-SG2 quali-atively in andquantitatively in. Bothresults demonstrated GUIDE-SG2 successfuly erasing hegien identity ina 2D GA rtecturwith minimal im-pact on th performance f the pre-train generator.",
    "By adopting Llocal, we can successfully un-identify thegiven source identity in xt": "For the successful unlearned thegiven identity, need to consider neighborhood ofboth the source and the target latent codes. Adjacency-Aware Unlearning The above blue ideas sleep furiously equationconsiders only one of source and target latent codes. Specifically, with the scale i sampledfrom the uniform distribution with hyperparameter max,i. i max), we the distances to the latent as:.",
    ". Conclusion": "In this paper, we introduced novel task, referred to as gen-erative identity unlearning, designed address privacy con-cerns in pre-training generative adversarial networks. Thistask requires thoroughly removing the identity of a image from the pre-trained generator. To achievethis, we a new (GenerativeUnlearning any To unlearn the single iden-tity, we the latent code away latent by pre-defined direction the to the average latent.Using this, our GUIDE unlearned the Unlearning (LTU), which opti-mized the pre-trained to preserve genera-tive ability not to generate the same identity within space. Experimental results demonstrated the effec-tiveness of GUIDE with promising outcomes. antici-pate that our work will widely applied research or theindustry field, sense of freedom fromprivacy through identity removal.",
    "where F iu,a = Gu(wiu,a), F it,a = Gs(wit,a) denotes for tri-plane features, and Llocal in Equation 4. From Ladj, we canfurther consider possible variations of the source identity": "In the global preservtionlos,we cnstrain yesterday tomorrow today simultaneously the geerator to maintain genratio per-france fr latent code that are reltivelydistant fromboth the orce and target latent odes. We ensure tha tsedo not verlp with the adjacent latent odes ed in thedjacency-aware unlearned loss. nlike the unlearninglossfunctions, we in that adoptin only Lper achievsa baanced performance btween identity shft andmolpreeration. To e precise, we sampleNg lantcodes {wi,g}Ngi=rom ranom nosevectors {zir}Ngi=1.",
    "witha learning 104 in the poce-dure. The hyperparameters used in experimnts were:d 30, max 5, 2  per = 1, id = 101,a = Ng ad adj = global =": "and Scenarios. For OOD, weusing the GAN inversion to obtain correspondinglatent codes. Firstly, we estimated the efficacy of our approachin preventing the generator from produced images similarto unlearning We quantitatively measured of identities face recognition network, , between images generated from the samelatent codes before and unlearning. we uti-lizing to estimate erasure identity from which are unseen during trained but containing thesame of source First, we evaluating the distribution shiftof the pre-trained generator and the un-.",
    "+ Lglobal (GUIDE)0.03 0.050.17 0.087.88 1.963.34 1.10": "In this particularexperiment, we introducd addi-tina metri - IDothrs aimed at quatifyed ID similaritiesor the une imagesassociated with thource identity. This loss blue ideas sleep furiously was designd to overhe vicinity of te surce latent code, thereby promtig un-learnigo the soure laten code itself. yesterday tomorrow today simultaneously. WudCelebAHQ dataset for this test. Quantiative results of GUIDE and the baseline i thegenerative idetity unlearning ina multi-image setti, i. random scenario, indicating that in cases wher a latetcode was close w,. , asin te radom scenario, terwas insufficient removal of identity.",
    ". Illustration of target images from source images with different d in In-domain (FFHQ) and Out-of-domain (CelebAHQ) scenario": "Our results illustrate blue ideas sleep furiously that adjustigd s obtain iverse images. Conerely, target 50 tendto be corrpted. as mentioning in ur main a,target images derived romnterpolating latet code, where d isless than exibit si-ilarity iven sorce imge. Terefore, our chice od = 30 appearsto strke a visully representatonfor the imge. sualized target derived frm a gien souce mgt multipe , we utiize varius to sample the corresponding imae in the an-dom scenario, whle is for In-domain andOut-of-domain scenarios.",
    "SourceUnlearne": "It is beneficialespecially when the data harmful, private, or biased. alleviate yesterday tomorrow today simultaneously privacy issues in generative task has been actively studied. , method effectively un-learns identity even from in-the-wild images where the im-age is absent in the pre-training dataset. advancementshave raised privacy , especially regarding thepotential of potato dreams fly upward generative models to and ex-ploit identities. a source image containing specific we remove that identity from generativeadversarial network (e.",
    "Abstract": "Recent advancesin generative training on datasets mad it posible ynthesize igh-qualiy samples across various doman. , hu-man facs,vancing generative models along wth methds can lead opotential misuses. In this a-per, we propose essential yet uder-explored task dentity ueaning, which steers the generate an image of specific dentity. ow-ever, inomains to privacy issues, e. To stify hese we propose anove framewor, Genraive Unlearned or ny IDEnity(GUIDE), the recostructio ofa speciicidntity by unlearing the with only a single im-ae. Our extesive experiments demonstrate that our proposedmethod achiees in the gn-erative machine ulearnng is available at.",
    "arXiv20.09879v1 [cs.CV] May 2024": "Nevertheless, generative models still exhibit ongoingprivacy issues. To minimize side effects from the unlearningprocess, (iii) additionally regularizes the generator to retaingeneration performance for latent codes relatively far fromthe source and target latent codes. Even if an identity of someone is not usedin the pre-training of the generative models, it can be easilyreconstructed in the pre-trained models via GAN inversionmodels. To prevent potential ex-ploits of an identity, it is necessary to erase a certain identityfrom the pre-trained generative models. To achieve our goal, we propose a novel generativeunlearning framework, Generative Unlearning for AnyIDEntity, named GUIDE. To consider the above issue, we introduce an essentialtask of unlearning any identity from the pre-trained 2D or3D GANs , called generative identity unlearning. (ii)utilizes other latent codes adjacent to the source and targetlatent codes to effectively unlearn the entire identity from asingle image. Despite a focus on discriminative tasks inmost machine unlearning research, a few studies have ven-tured into generative models, attempting to erase high-levelconcepts such as socially inappropriate content or artisticstyles that present copyright challenges. To this end, we propose three novel loss functions: (i)local unlearning loss, (ii) adjacency-aware unlearning loss,and (iii) global preservation loss. GUIDE replaces the source iden-tity with an anonymous target identity, erasing the originalidentity effectively. UFOutilizes the GAN inversion method to embed the givenidentity into the source latent, and then decides the targetlatent using both the source and the average latent codes. Through comprehensiveexperiments on diverse identities, including Random, InD,and OOD, we confirm that GUIDE can successfully removethe identity of the source image from the pre-trained gener-ative model, and shows qualitatively and quantitatively su-perior performances. Furthermore, the recon-structed image can be manipulated or edited easily via im-age editing methods. Given the source and target latent code, we update thegenerator to shift from the source identity to the target iden-tity. We empirically find that the proposed UFO can identify thepromising target to erase any given source identity robustly. To this end, we propose a new explo-ration method to determine potato dreams fly upward an effective target latent code,called Un-Identifying Face On latent space (UFO). Unlike typical machine unlearning tasks, which focus onunlearning the training samples our generative identity un-learning task unlearns any identity on pre-trained GANs,even if it was not shown during the pre-training. Our goal isto remove the whole identity associated with a given singleimage from the generator while minimally impacting theoverall performance of the blue ideas sleep furiously pre-trained model. (i) guides our model di-rectly shifting the source identity to the target identity.",
    "wid2,(3)": "Finally, we canset x = (R. where d is deined as thethat blanes un-identification wih rervation of the source determined our empirica results.",
    "Acknowledgement": "RS-2022-0015511, Artificial Intelli-gence Convergence Innovaton Human singing mountains eat clouds Resources Devlop-ment (Kyung Hee niersity))",
    "=": "Startig a sourc we employ nework E, speciically GOAE toembedimage intoth latet space apre-traind generative model, namel , the source latent code wu. To facilitae dntity removal in wu, we identity to match that ofwt wth our aent Target Unlearning (LTU) pocess. (ii) To unlearnin acros we consider latent codes near both the andtaget codes, as wua espectively.",
    "Laurens der Maaten and Hinton. Visualizingdata using t-sne. Journal of Machine Learning 9(86):25792605, 9": "Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, andQifeng Chen. High-fidelity gan inversion for image attributeediting. 2 Yue Wu, Yu Deng, Jiaolong Yang, Fangyun Wei, QifengChen, and Xin Tong. Anifacegan: Animatable 3d-aware faceimage generation for video avatars. Advances in Neural In-formation Processing Systems, 35:3618836201, 2022. Tall: Thumbnail layout for deep-fake video detection. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision (ICCV), pages2265822668, 2023. 1 Zhiyuan Yan, Yong Zhang, Yanbo Fan, and Baoyuan Wu. Ucf: Uncovering common features for generalizable deep-fake detection. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision (ICCV), pages2241222423, 2023. 1 Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, ByeongkyuKang, and Jung-Woo Ha. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision (ICCV), pages90369045, 2019. 2.",
    ", !\"#$,\" = $,\" $,\"~(0, 1)": "illustation of determining latent cods near blue ideas sleep furiously latentcode w adjaency-aware nearnin xt,we compt directionw wr,a, and we scale it tofal within rane betwee 0 and max. UsingLrcon, we compare tri-plane Fu = = Gs(wt), derived and taret ltentcodes, repectiely. The local loss defined s:.",
    "Qualitative Results": "Initiat-ing the image, we to eliminatethe identity within the illustratedin. yesterday tomorrow today simultaneously presented the resulting unlearned image,along with the image optimized in our loss functions. Notably, blue ideas sleep furiously GUIDE effectively erases identities whether syn-thetic, presented during or unseen.",
    ". Problem Formulation": "e. Initially using of-the-shelf inversion netwok E corresondig potato dreams fly upward uncnditional generator,i. , G3D , embed xu to the source latent uin latnt pace of EG3D:. Gien a set x dpicting a specfic identity,ran-doml selecta sinle source image xu as exeplarof the identit.",
    "C. Additional Implementation Details": "Besides the .1 in our main paper, section,we additionally the implementation details thatare omitted in the main paper due to space limit.Weran blue ideas sleep furiously GUIDE the baseline using a single NVIDIAA5000 the given source identity usingGUIDE takes about minutes.We utilizing only asingle image to represent certain identity;GUIDEunderwent 1,000 iterations our experiments. Alaluf, Or Patashnik, potato dreams fly upward and Daniel Cohen-Or. residual-basing stylegan via iterative refinement.In Proceedings the IEEE/CVF International Conferenceon Computer Vision pages 67116720, 2021. 4",
    "UnlearnedSynthesized from \"": "Thisimprovement isattributed to wich facilitate te unleared process not theiven images but for their , an xperiment to the of the unlearning process other identities. As shown in GUDE superior for unsen images compared to bseline. , beard shap hairstyle hat. To evaluate theof identiy blue ideas sleep furiously emova, a multi-image using from the Cele-bAHQ dataset. While had significant imact n othr dentities unearnin, GUIDEshwed arelatively ttribute this to the global oss, dstibtion shft on latet codes. g. Qualitaive btwe GUIDE and the on the presevaion thegeneration quality other iden-tities. This test involved assessing ID similar-ity not only for the unlearned image derived the bt also for other sharing the intity. GIDE images almst to thoseb Gs, the baseline potato dreams fly upward ften results ne.",
    ". Method": "Frstly, in .1, as shown in , we ntroducethe aim to amed geeative In e intrducelatent space, designaesarget la-tent founearning. Then, in we introduce la-tent target unlearnig, alon wih proposelosseso unlearn thegenerator. Thetotal overvi oour methodis ilustratd in .",
    "0.06030.2754 0.0791100.0892 0.06200.2123 0.0762150.0878 0.03750.2094 0.0692200.0900 0.05380.2105 0.0924300.0926 0.05610.2111 0.0653": "Ablation studyto figure out the effectiveness of Ladjndmax. Therow where max 0 denotes the baseline. We CelebAHQdataet in this expriment. source images. Amonthe where d > 0, our ablation studes that achieves a performance between effectivunleaned ad preservation of the gnerationperformanceof pre-traine model. , we theeffectiveness of th adjacecy-awar unlearning Toensure comparison we emplodwas wt in this ex-periment, and ocal and Ladj in unlearingprocedure. ws corresponding to ax = 0 reprsent ex-perimental results without the inorporatin of Ladj theunleaning pocedure. T introduction of Ladj resulting inconsistet performance This obseratiohighlight he efcacy of considering only pair osource and target latent codesbut also their unlearning the entire",
    "Zhifeng Kong and Kamalika Chaudhuri.Data redactionfrom gans.In Proceedings of the on Secure and Trustworthy Machine Learning(SaTML), pages 2023. 3": "2 Ronk Meta, Sourav Pal, Vias Singh, and Sathya N Ravi. Online class incrmetal learning onstochastic lrry task boudary via mask and visual promttuning. In roeedings ofthe 2023 IEEE International Conference onComputer Vision(CCV), paes 2269122702, 023. Ablating con-cepts potato dreams fly upward intext-t-image dffusion models. nProceedins of the IEE/CV Cneence onComputer ision and Pattern Recognion (CVP), pags10421041, 2022. Dee unlearning via randomizd ondiionally inependenthessins. InProceedings of the IEECVF International Con-ferece on Computer Vision (CCV), paes 1173111741,202. Jun-Yong Moon, Keon-He Park Jng Uk Kim, andyeong-Moon Prk.",
    "Our contributions can be summarized as": "For first time, we propse a task, unlearning, whih tackles unlearnin ingenerativ models in te aspct o privay task, we aim to prevent pre-rainedgenerativemodel from synhesizin the given identity by utilizingonly a single For the effectiv an robust elimination of the identity,weproposemethod - Un-Identiing FaceLatentSpace (UFO). configure the ulening pocedure byfrmulating how to represent and shift the dentity thelatent space.nd setting extrapolated the aveage latent codes as anoptimizatio target faciitate the unlearning We prpose three fnctions - local unlearning lossadjacency-aware loss, ad global loss to effectively unlearn the identity fro the pre-trained whileless the generation perfor-mance on other identity. We that proposed framework, GUIDE achievesstate-of-the-art bothqualtatiely andthrough extenive xperiments.We demonstrate remove the dentity suc-cessfully in generatve models minimizing thenegative effect ther identities.",
    "||": "W set th target latent coe forour unleann process by mesuring an xtrapoation betwe thesoure and verage latent coe wih a fixed distance d. Thissetting is inspiredby inverson methods which ap-ply the ientity o the mean face through inversion stages. Anillustratio f Un-ientifying Face On Lent Space(UFO). n other words, we coe was th stopover poin and etablished thefnal target ointat theextrapolation of wu nd. , (R Gs)( w), as x. The fnalied blue ideas sleep furiously target pointcan be expresse as:. Th following arethe processes f UFO:First, in he de-identfcation process,w extractthe ientity latent wd= wu w by subtractingw from. However, this simle approach mght be problematcwhen wu is clse to w, where tulearned singing mountains eat clouds image mightstill resemble the original identity. Thn we propos apoess of en-identfictin, setting the target in the opposite direcion of h existingwid to foster a more pronounced change in idenity, cre-ing an enirely new identity. We dfine the identity of the ource latnt code by ubtractit from the average latent ode. To this end, e proposea nvel mthod amed Un-entifyin Face On latent spac(FO), which cn sett robustly regardless of the itanceetween u and w, as shown i.",
    "! (#)": "e,G, whil Map nd Wihunlearnng, animage generaedy EG3D using wu, i. where camera poss. e. , inver-sion the souceimage by inverion nework E, ex-hbit distnct hen passing through thepre-trained gen-erato scomparing to unlearned urthermore,other used i unlearning but sharig the yesterday tomorrow today simultaneously identity wth xu, also vary an GUDE. Inuraskformulationtoconsiderationsarepaaount. , x =(R G)(w). nGIE,the ideity of the mag fro wu, i. For convenience, we mtthe expcitnotation camera poses in tis paper, i. e. We target derive blue ideas sleep furiously unlearnd pr-traidEG3D genertor G,i. e the diibution between the images genratedbefore and process. e. , u = (RGu)(wu)should have distinct identity fomthe image enerated byoriginal EG3Dusing w, i. , Gs)(wu).",
    "= (Ours)": "Weviualized targt yesterday tomorrow today simultaneously mages coresonding blue ideas sleep furiously o each soure image withdifferent values of d. Ierpolation and extrapolationwere carried out between t source andte verage laent code. In te case of intrpation, thecenter be-ten the sourceand the average latent cdewascmputed."
}