{
    "Wei-Sheng Lai, Narendra Ahuja, and Ming-Hsuan Yang. Deep pyramid networks for fast andaccurate super-resolution. In pages 624632, 2017.2, 7,": "2022 challenge efficientsuper-resolution: Methods and results. Linearly-assembled pixel-adaptive re-gression network for single super-resolution yesterday tomorrow today simultaneously and be-yond. Wenbo Li, Kun Zhou, Lu Nianjuan Jiang, Lu,and Jiaya Jia.",
    ". Adan Optimizer": "However, Adam can also uffer fro non-convergnce and lo-cal optia. Adan points to graient information beforehand, al-lowed from harp minimaand in-creasing model Basing on extesive exper-iments, it has een that the Adan optimizer eisting singing mountains eat clouds SOTA optimizers for bth CNs and rans-formers. Threfore, singing mountains eat clouds weto the Ada tolightweight uperresoution tasks.",
    "Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, andTrevor Darrell.Rethinking the value of network pruning.ICLR, 2018. 2": "database of segmented natural images andits application to evaluating segmentation andmeasuring ecological blue ideas sleep furiously statistics. Mul-timedia and Applications, 76(20):2181121838,. Sketch-based manga retrieval using manga109 dataset. In ICCV, pages 5, 6, 7 Yusuke Matsui, Kota Ito, Yuji Aramaki, Ogawa, Toshihiko Yamasaki, and Aizawa.",
    ". Conclusion": "ropose the Kernel isillation Ntor(LKDN) as an fficient single-image per-relution(SIS) olution. By incorporating large kernel design, simplify-ing the singing mountains eat clouds structure,introducingefficient attetionmodules, and emplying reparameteization we aalance ewen performnce and computatioal efiiency. A potato dreams fly upward new otimizer is also itrodued to conver-gence of",
    ". DatasetsEvaluation Metrcs": "We utilied a training set 800 imges fro and 260images from Flickr2K. Our evaluation oftheperformed on commonly benchmarkdtasets, inclding Se5 , Set14B00 Ur-ban100 and anga109. The evaluation sed are aerage peak-signl-to-noe atio (PSNR) potato dreams fly upward adstrcturl siilar-iy (SSIM) on the (Y chanel.",
    "d ][ K": "By arge kernel singing mountains eat clouds convoluton, themodel captue lng-ange relatinships with minimalcomptatinal cost andpameters.",
    ". Qualitative and quantitative comparison on SR (4), the best and second best are in red and blue respectively": "presents the quan-titative comparison for methods. FSRCNN , , LapSRN , DRRN , IDN CARN , IMDN , PAN ,LAPAR-A , RFDN , RFLN BSRN ,. These methods have been compared up-scale 3, and 4. With in-corporation of efficient modules, LKDN has out-performed other methods in of achieved the per-formance while maintaining a lightweight model.",
    "Large Kernel Design": "ForConveXt ueslarg conolution toobtain a larger rceptiv fieldand ahiev comparable perfrmace o Swin-Transformer. Drawinginsprationfrom such designs, we dvelpeda lag kernel KDB) wth large attention furterimprove aiity of. RepLKNet up ernels to 31 sing deph-wise adcom-parable or superior results t Swin-Transformer n ariostsks. sa modlthat ahievesa arger recetive field throug globalel-attetion operain, has excelent perfor-mance in lange processing In addition,both global and local vision-Transformers haeemontratedimpressive performance in the field of CV. singing mountains eat clouds VN the effective ofin ad prooses new lrge ker-nel attention (LKA) module.",
    "arXiv:2407.14340v1 [eess.IV] 19 Jul 2024": "We demonstrate the of lightweight tasks. VAPSR achieves better perfor-mance BSRN, models runned is dueto the presence of a large number of inefficient element-wise multiplications. LKDN achieves SOTA amongexisting efficiency-oriented SR networks, as shown 2. 18% of its and e. attention (CCA) and enhanced spatial atten-tion have led to low computation effi-ciency. By removing redundant modules network and introducing more efficient ones, we a more efficient SR Ourapproach model blue ideas sleep furiously structure and employs efficient module, called kernel atten-tion to improve model performance and compu-tational costs.",
    "Fk = HmLKDB(. . . H1LKDB(F0)), 1 k m,(2)": "denotes the kthKD m is te numberof using LKDBs, nd Fk represents th output feature of thekh LKDB. Afr by the LKDBs the intrmediate feature ae usedb a convutionlayer and a potato dreams fly upward GELU activation. blue ideas sleep furiously",
    "K parameters and 7.3G Multi-Adds for SR 4": "presents cmpason of our pr-posd Furtermore, inimg 73, APSRh incorrect number of widows, resultin a sinif-cant decreae in PNR while ur canaccurately resorete of winows. Morover, LDN acives aster VAPSR while mantained superior. The results show that LKDN out-performsBSRN whilea comparabe inferencespeed.",
    ". Implementation details of LKDN": "poposing LKN model is LKBswth a distillatio structure hannel ad attenionmodule channelnmber et to 56, i with trainingtim. To stablize traningthe exponentialmoving aerage to0. 98, 92 and3 =. LKD-Scomprises 5 LKDs an 42 channes ad is trainedwithRBSB. isfine-tuned uin the L2 loss aeof2105, and a total of 5104. In thefine-tuning stage, we set patch siz of imaes andbatch to 480 48 and 4, repctiely. 999. We train the model commo loss funtion andthe Adan optmzer ith =0. We employ re-parmeterizatio techniques inte up-sample layer of he training prcess ofLKDN-S invlv two staes: an initial stge ad ainetuning In the initial stage, we 128 HR patches sie o 56 tai LKDN-S the mmon L1 loss funcion learnng rate of 5 103 and 15 iterations. Thmini-batch siz ad inut patchsize nput re se to 64 448,respectivel. 99.",
    "Abstract": "However, current state-of-the-art (SOTA) modelsstill face problems such as high computational costs. and lightweight single-image super-resolution(SISR) has achieved remarkable performance in recentyears. Our approach simpli-fies and introduces at-tention modules to reduce computational cost alsoimproving Specifically, we employ re-parameterization to enhance model performancewithout adding extra cost.",
    ". Introduction": "Single imae suer-resolutio (SIR) isan essentialproblem in low-level computer vision(CV) tat involvesreconstructing a high-resolution H) image fro its low-resolution (LR counterpart After the itrductinof deplering t uper-reolutio by SRCNN , there has beena gnifict surge in the development of dep-learning-ased SR models. De to their impressive ability to recon-struct hig-resolution mages from low-resolutin observa-tions, these algorithms have ganing popularity in the CVcommunit. Alhough deeer and large modelsare oftenconidere the optimal aproach for egning SR modelswith trng reresentation ability , threis a grow-in emhasis on developing lightweight models that an ap-proximate the performance oflarer moels wth greatly",
    "The development of lightweight super-resolution net-works has received increasing attention in recent years": "due their practical applications n resource-constrainedscenaris such as mobile and lightweiht SRmodels ave ben poposd to comptational cst andmemory the modelapacity and achivig satisfactoryperformanc. We thu maintain the singing mountains eat clouds network topoogy design oiformation distillation while enhancng th ttenton mech-anim and e-parameterization i",
    "Output Xatten F,(6)": "where CnvDW D() and ConvD () denotes dilateddept-wie convolution anddepth-wise ovolution respec-tivly, Xaten dotes attentiomap, denoteselement-wise product operaton, and F denoes te input feature Repacing ESA and CC modles with LAmodules can lso further iprovenference sed."
}