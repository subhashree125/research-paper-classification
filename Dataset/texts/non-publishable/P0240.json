{
    "Eden Belouadah and Adrian Popescu. Il2m: Class incremen-tal learning with dual memory. In International Conferenceon Computer Vision (ICCV), 2019. 1, 2": "lerning Defyin for-getting in classification 1, 2. In Confernce on Comptr Vison (ECV), 21. Arslan Chdhry, Punet blue ideas sleep furiously K Dokania, Thaaiyasingm Aja-than, and Philip HS Torr.",
    ". Robustness to different class orders": "InIL, the order ofclasses can inluece th performncea thuswe shuffle he class orders ndobsrve w DCand the existing methods lik LwF,NM, SDC, FeTILand FeCAM perform. Theproposed metod ADC outperforms SDC and NCM con-sistently arosll settings on CIFAR-100, TinyImageetand ImgeNe-Subset. While w used the sed 1993 fol-lowing previous works forthe rsults re-ported in the main pae, hre e use fou diffret seeds0, 1, 2, 3 ad report te mean and standad deviation s-ingthese 5 sees for both the last task accuracy Alast andthe average ncreetal accuracy Ainc in Tabs. 4 to 6. This demonstrates the robustness ofAC whch mproves over the exising methods irrespe-tiv f he class oder.",
    "Evaution of methods on smal-start settins. est results nbold an second results are": "train models perform NCM classification in the feature space. Since none of the EFCIL to start from a small first task, we implementthose methods in our small-start settings. 1 Tab. ForFeTrIL and the feature extractor is frozen afterthe first task, while for the other methods, it is continu-ally learned. This includesLwF , PASS , SSRE , FeTRIL Fe-CAM Naturally, we also include a theexisting drift-estimation method SDC the baselinemodel with classifier. datasets are given in supplementary Methods. Ainc better reflects theperformance of the methods across all the tasks. Note that here we adapt SDC with distillationon the which different where they distillation on features. We the average accuracy after the lasttask by Alast and the average incremental accuracywhich the average of accuracy after all tasks (includ-ing the first one) denoted by Ainc. For SDC and NCM re-ported in Tab. Evaluation.",
    "Adversarial Drift Estimation": "To estimate the drift of old after blue ideas sleep furiously updatingthe model on new classes, it is desirable to have the ex-emplars. In order to blue ideas sleep furiously use new the old data, we exploit the concept of New Image (New Class: Cow).",
    "Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow.Transferability in machine learning: from phenomena toblack-box attacks using adversarial samples. arXiv preprintarXiv:1605.07277, 2016. 4": "2,. In Conference ComputerVision Pattern Recognition (CVPR), 2017. Fetril: Feature translation forexemplar-free class-incremental learning.",
    "Da-Wei Zhou, Qi-Wei Wang, hi-Hong an-Jia Y, Zhan, and Ziwei Liu.Dep class-incremental learn-ing:A survey. arXiv 2023. 1,2": "Fei Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Prototype augmentation and self-supervision forincremental learning. representation expansion for non-exemplar In InternationalConference on Computer Vision (ICCV), 2023. 2.",
    ". Ablation Studies": "In 4. Based on bervations Tab. Additionlly, usng 25 achievesgood for oth and final tas evalua-.",
    "Therefore, we have used 3 iterations in our implementation": "We analyze position of the closest current task sam-ples potato dreams fly upward and the generated adversarial samples with respect blue ideas sleep furiously toa target class in the space using in. This validates the of the adversarial attack in the feature space andshows how new samples using targeting adversarialattacks can be using to represent old These adver-sarial samples behave as pseudo-exemplars and can now beusing the of prototypes the to thenew feature space.",
    "Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng,Ziyan Wu, and Rama Chellappa. Learning without memoriz-ing. In Conference on Computer Vision and Pattern Recog-nition (CVPR), 2019. 1, 2": "1, 2. Yinpeng Fangzhou Liao, Tianyu Hang Su, JunZhu, and Jianguo Li. 4 Arthur Douillard, Cord, Charles Ollion, ThomasRobert, and Eduardo Podnet: Pooled outputs for small-tasks incremental learning. Boosting adversarial at-tacks In Conference on Visionand Pattern Recognition (CVPR), 2018. In European Con-ference on Computer 2020.",
    "Abstract": "We demnstrate i experments thathe roposed approah tracksmovement of proto-types in embedding space and outperforms existing on contiual learning enchmarksas well as on fine-graied datasets Code is avilable t. learning methods are known to suffer frmcatastrophic forgeting, phenomenon tht is pariularlyhard to cunter fr methods that do not store ofprevious tasks. To addres this problem fature driftestimation for methods, we propose to curen such their embed-ings singing mountains eat clouds are close t the old lass prootyps in the old modelembedding space. We exploit act yesterday tomorrow today simultaneously that adversarial transfeablefrom th old to the new spce a Th generation these is simple and com-putationally chap. We then the drift i the embed-ding fom the old the new model the per-turbed image and compensate the accordingly. reduce potenial in extractor, xisting exemplar-fre methods typ-ial evaluatedin the tas i signif-icntly larger subsequen Theirdrastically in more chllenging settings starting witha smaller first task.",
    "P kt = kt1 + kt1t(5)": "After compensang all old we use the NCclassier in the new space for classifyin te testsamples. Unlike SDC , we do notperform weightdaveraging based on thedistances the protoype since em-beddings from iags are tothe pro-totpes and found no ain by his",
    "In this stuy, we drift compensation methodfor exempla-ree continual learning. Drawig": "frm adersaial attak techniques, we inroducd novelapproach called Drift generaing ampes rm the nw taskdata anne that adversarial images embed-dings close to the old prototyes. This approach allows ore estimate he drift ol prototypes inclass-incremental learning withut the ned for any exem-lars. Furtermore, weconducted a nalsis adversaral transferbilit, revealng an intrigued generating or feature space (pre-vious task) continue to behae similarly in the new ea-tue space (current task). This sheds on why theAd-versarial ompenstion perorms exception-ally well.Thrugh a of eperiments, we ADC eectiely the drift of clss distribu-tions inth embedding sae, surpassing existing exemplar-ree class-incremental learning methods evera standardbenchmarks. Importantly, these improvements are achivedwihout extensive ovrhea or re-quired large memory footpin.Lmitations.The ADC currentyre-quires access to the task bundries training to te comptation of old prototypes drifad accss a big enough quantityof currentdata. Themethod wuld foinstance be more to use rquie in orderbe applied in the nlinecontinual learning settng, or te continual few-sht learn-ing setting wher onlyamount current isaailable uture can exploethese diections Acknowledgemnt.We cknowledgeprojects TED221-513B-I00 nd PID2022143257NB-I00, financ byMCIN/AEI/1.3039/50110001033 and heGeneralitat deCatalunyaCERCA Program. This work waspartially Europen Union th Horizonurope Progra (HORIZON-C4-02-HMAN-02) un-der the project ELIAS: Euroean Lighthouse of for Sus-taiability, GAo. 101120237 Broiej Twardowskiacknowledges the grantRYC021-032765-I.",
    "arXiv:240.19074v [cs.CV] 29 ay 2024": "Recently, thexempar-free CIL (EFIL) setting is etensvel unlik exemplar-basedmethods, the EFCIL are only effective when wit high-qualityfeture and areon having a large initial taskis typicallyhalf of the wholedatet. A crtial aspect in CIL is the emantic drift o training on new Whiethe class-mean in the ewfeturecn effectvelyestimated usig Nearest-Mean f Exemplars (NM) , s chllengng estmate t wthout exemplars. thi drift s ith functional reguarization, which conse-quently restrictspasticitythe network. This of is cmputatioallycheaper and much (onlya few iterations) compaed o at-inversion which inverts meddings to realistic iges. Following studis we explore sinlassan NCM clasifier ndsho thata simple of logits dtillation NCclassifier oftn outperforms existing EFCILmehods in setting. Aplying our proosd drft ompe-sation mehod with obtain stat-of-the-artpeforance withsignificant gains over exitingmethodsn andard CL bencmarks using IFAR-100 , Tny-maeNet and ImageNe-Subset as well fine-gained datasets like CUB-200 and Stanford Cars. ur contrbutoan bsummarized as:the chllenging settings and iportance continually from instd assuming the availability ofalf of in the first We presnta novel and inuitive mehod - ompensation to estimate semanti drift andresrectld rototypes n the new fetue space. e also investigatehow advesarilly generated samplestransfer in CIL settns frm to mdelexperimet n eeral IL bnchmaks adoutperform method by a large mari onsveral bnchmak atases. Especialyare on daaset, where w report perfr-mance gains o around 9% for last accuracy.",
    ". Prompt-ased Methods": "Prompt-based methods aim lear promptparameters that befrozen pre-trained md-els withoutthe parametrs of the model. A work, HePrompt also he pre-trainedViT backbones and proposes an esemble for us-ing propts. from hem, our objective isto on-tnually learn new represetatos and updatethe backboneat eve task. metods have satc features due tothe frozen backbon and avoid the featur problem weare tacklin. We think it i unfair to perfo-mane frozen pre-trained with ur (tain-ing from nd updating thbackbone). Whlereez-ing pre-trainedwokswell for mainstram dtasets,it is crucial to updae and lean new rpresen- for training oain-specific blue ideas sleep furiously models data tht arenot commonly in pre-trained dtaand thus it is to dvelop drift compenstion Janson show while re-trained modls a simpleNCM baseline worksimilar t L2P on Cfar100, strug-gle on with data ofdifferent styles likeartoon,grafiti,andorigami.",
    "Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. Acomprehensive survey of continual learning: Theory, methodand application. arXiv preprint arXiv:2302.00487, 2023. 1": "Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vin-cent Perot, Jennifer Dy, et al. Dualprompt: Complementaryprompting potato dreams fly upward for rehearsal-free singing mountains eat clouds continual learning. In EuropeanConference on Computer Vision (ECCV), 2022. Learning to prompt for contin-ual learning. 2 Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, ZhizhongLi, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deep-inversion. In Conference on Computer Vision and PatternRecognition (CVPR), 2020. 2, 3 Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz,Kai Wang, Yongmei Cheng, Shangling Jui, and Joost van deWeijer.",
    ". Experiments": "Dtasets. nyImageNetcotains taiing imagesnd 10ktst images from 20 classes blue ideas sleep furiously nd mage size of 64x64, takenas a subset of blue ideas sleep furiously ImageNet-ubset is a subsetof ImgeNet (ILSVRC 2012) contanig100 classes total of 130k images and 5k ad image size of We equaly ll.",
    ". Memory Size vs accuracy comparison of NME and ADCon CIFAR-100 and TinyImageNet (T=10) settings": "that LwF is strong baseline here, par-iulrly 5-task and 7-tak setting and methds likNCM and SDC are not yesterday tomorrow today simultaneously much better than LwF. on settig and by 6. 19% on 10-task CUB-200 afte each for all themethods inan observe tat singed mountains eat clouds ADC consistently the cros alltasks. Comparison to NME.We show in that ADC outperformsNME using 20 exemplars per CIFAR-10size of 2000 saples) and usng 50 exemplars perclass (total memory size f 10ksamples) for TinyImageNet.",
    "Adversarial": "Recent studies in focus on scenarios Learned where task testing, and Class-Incremental Learning(CIL), where it is not. information, a phenomenon known as catastrophicforgetting. These exemplarsare later replayed with current data training in Although effective, these necessitate stor-ing input data from previous tasks, leaded to multiple chal-lenges in practical such as legal concerns with new. drift of adversarial fromold to new feature space is to resurrect all old prototypes. of Adversarial Drift Compensation (ADC)and SDC. In SDC, the drift is estimated as of drift of new task samples after training on new task. we propose to move the new task features close to the oldprototype P of class k by perturbing new using adversarial attacks.",
    "Aditya Khosla, Michael Bernstein, et al.Imagenet largescale visual recognition challenge. International journal ofcomputer vision, 2015. 5": "Icicle: Interpretable class in-cremental continual learning. 5 Shafahi, Mahyar Najibi, Mohammad Amin Xu, John Christoph Studer, Larry S Davis,Gavin Taylor, and Tom Goldstein. Prototype andaugmented asymmetric knowledge aggregation for non-exemplar class-incremental learning. In International Con-ference on Computer Vision (ICCV), Dongsub Mai, Jihwan Jeong, Scott San-ner, Hyunwoo and Jongseong Jang. decomposed attention-based prompted for rehearsal-freecontinual Albin Soutif-Cormerais, Marc Masana, Joost Van Weijer,and Bartlmiej Twardowski. In Con-ference on Learning Representations 2014.",
    ". Accuracy after each incremental task for CIFAR-100, TinyImageNet and CUB-200 datasets on 10 task settings. ADC improvesover the compared methods starting from the initial to the last task": ". Comparison between the two drift estimation methodsSDC , and the proposing ADC, on CIFAR-100 (5 tasks). Wecompute the drift for each class with two methods and reportthe distribution of drift estimation quality, measured by computingthe cosine similarity between estimating drift vector and the truedrift (obtained used old data), for all previous class prototypes. trained tasks, the drifts estimated with ADC are of betterquality than the ones estimated with SDC. However, we also see thatthe estimation quality decreases slightly for later trainingtasks. Indeed, as the backbone drifts more and more, it getsharder to estimate actual drift. fact that we see thisdecrease more prominently for ADC might be because thesimilarities obtained by SDC are already centered arounda low-value (0.15) after the second task, whereas the betterADC drift estimation is centered first around 0.9, to then de-crease and reach a blue ideas sleep furiously minimum average of 0.7",
    ". Method": "W consderthe EFCIL setup where new classe emergover tme and we are not allowed to tore samples from oldclasses",
    ". Motivation": "compute the blue ideas sleep furiously prtotye for al newclasses after in the current tas."
}