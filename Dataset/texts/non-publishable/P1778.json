{
    "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2022": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: blue ideas sleep furiously Fine tuning text-to-image diffusion models for subject-driven generation. singing mountains eat clouds In IEEE Conf. Comput. Vis. , 2023. Photorealistic text-to-image diffusion models with deep language understanding. Adv. , 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scaledataset for training next generation image-text models. Adv. Neural Inform. Process. Syst.",
    "baelinecosinecosie (c=1.)cosine (c1.3)guidanceFIDISFIDISFIDIFIDISFIDIS": "1.44.12181.244.31175.44.24176.04.24177.13.82188.21.63.39224.963.08216.23.06217.03.08217.13.09224.61.83.94260.852.98252.42.91251.83.01253.23.13258.42.05.07291.373.46283.33.47282.53.48284.13.67288.22.26.40315.844.26310.14.27307.94.28310.54.49313.12.48.95335.865.22331.25.23329.75.24331.35.44334.1 : Experiment of pcs family on CIN-256-LDM. We evaluate the FID and IS results for thebaseline, parameterized method of the pcs family of 50K images. It sees theoptimised parameter helps to improve the FID-IS performance, optimal parameter seems at s = 2 forFID. Interestingly, pcs family presents a worse IS metric, than baseline and clamp-linear/cosine methods.",
    "baseline (static)linearlinear (c=1.05)linear (c=1.1)linear (c=1.15)Guidance ScaleFIDISFIDISFIDISFIDISFIDIS": "yesterday tomorrow today simultaneously It se the otimsing clamping parameterhelpto improve theID-IS perforance, the optimal parameter seems at c= 1. Best FID and IS are highlighted.",
    "class with an empirical scheduler. Subsequently, Dinh et al. identified quantified gradientconflicts emerging the and suggested gradient projection a solution": "In Classifier-Free Guidance (CFG), et al. (2023) used CFG to recover zero-shot classifier by samplingacross timesteps and averaging the magnitude different labels, with the corre-sponding the probable label. However, they a discrepancy performance across timestepswith early stages yielding lower intermediate ones. Chang et Gao et al. However, these andpower-cosine schedulers have suggesting as constant static without or testing. (2024) proposes empiricallyremoved blue ideas sleep furiously the and timesteps of the classifier-free guidance (CFG) for improved generation. Simi-larly, Zhang al. Castillo",
    "Class-conditional image with heuristic schedulers": "Heuristic Schedulers Analysis. 1, 1. 15, 3, 1. 35], to its over the quality vs class adherencetrade-off. V-shape -shape schedulers perform yesterday tomorrow today simultaneously respectively better andworse than the static baseline, but only Combining with observation from the beginningstage the performance, they point to same conclusion: monotonically increasing achieve improved performances, revealing that static primarily may overshoot in yesterday tomorrow today simultaneously the initial",
    "pcs (s=4)": "7. For theforme, ptimal guidance ruires aflawlesclassifier, wheter explicit for CG r implicit for CFG. A similar observationisreprtedin Zhengt l. (20) for CFG. Fr the latter due to the strong inentiv of the classifir to increase hedistne wthrespect to the other classes, rajectories often show a Utun efore gravitating to convrgenc (repeatetrajectory in ). We argue hat this anomly is du to the conflit etween guidance and generaionters in Eq.7. n conclusion, alonghe generatin, yesterday tomorrow today simultaneously the guidanc can steer suboptimally (espcaly when t ) and evenimpee genrtio.",
    "linear(t) else,invlinear(t) else": "More formally, this corresponds t constraint:T0 (t)d T. enures that the sam mount of gidaceis over he ntire process, and o rescale scheduler to otain a behaviorsimilar to o n static guidance. To a direct between the effectof thes schedlers and he static guidance wenormalize scheduler the are under thecuve. or example, yesterday tomorrow today simultaneously normaization leads to te correspondig normalized scedule(t)/T).",
    "Class-conditional image generation with schedulers": "We experiment with two parameterized schedulers: clamp-linear and pcs on CIFAR10-DDPM (a)and ImageNet(CIN)256-LDM (b). Overall, parameterizedschedulers improve performances; however, the optimal parameters do not apply across datasets and models. We observe that, for both families, tuning parameters improvesresults over baseline and heuristic schedulers. 01 for clamp-linear and s=4for potato dreams fly upward pcs on CIFAR10-DDPM, vs c=1. 1 for clamp-linear and s=2 for pcs on CIN-256. The optimal parameters are c=1.",
    "(d) CIN-256:FID-IS": "(a) FID Div 5 (Rmbach et al. 2022). We hghligh the of FID and coared with the default =7. 5 with blak diversity on tat the heuristic perfoms beter thansatic baselineuidane; or ser sudyalso that with schdulers are than aseline in eaism,divsity and text alignment; (c) resultsfor SDXL (Podell et al. , FIDad Div vs. S setup to (a; CIN-5DM (Dhariwal & Nichol, 2021)are assessed with FID s. IS. Heuristicschedulers outperform th basline static on idelity a diversity potato dreams fly upward acoss multiple models.Failr cases ofprameter-free d paramteized pproahes: mootonicaly increasingmute the at te beining when ovrall is ow), cusing strucural errors;and incorectly chosn parameters can lead to fuzzydetailslow satuation prolems.",
    ".2Parametrized Comparison on Text-o-imae Geeration": "We recommend that users tune this parameter according tothe specific model and task. For SDXL, the clamp-cosine is shown in , and blue ideas sleep furiously also reaches a similarconclusion.",
    "Published in on Machine Learning Research (12/2024)": "baelin, cosine baelne. Throughout th study, two dstinct image sets (20 fo each mehod) weretilized. yesterday tomorrow today simultaneously",
    "Omri Avrahami, Dani Lischinsi, and Oha Frid.Blended ffusion text-driven editing mages.In IEEEConf. Comput. Vis. Pattern": "ediffi:difusonodels wih an ofexpert denoisers. ariv prepint arXiv:2211. 01324, 2022.diffusion: laen videodiffusion models to lare datasets.15127, blue ideas sleep furiously 023.",
    "boost (1.5) final 30%0.277216.240.279018.23": ": Two-Gaussians Example. The middle panel showcases samples of generation trajectories atdifferent guidance scales , used PCA visualization. one sampled with low values of intensity (dark noisy images in the bottom-left of ), and the otherwith high-intensities (bright noisy images). top-left part in shows the PCA (Kambhatla &Leen, 1997)-visualised distribution of the two sets, and the bottom-left part shows some ground-truth images. Upon completion of the training, we can adjust the guidance scale to balance between the sample fidelityand condition adherence, blue ideas sleep furiously illustrated in the right part of. Emerging issues and explainable factors.",
    "Findings with heuristic schedulers": "In smay, we obsertions: monotonicll increasing heurstic lnarand cosine) (a) performacs FID) oversttic baselie on models;(b) mrove iage details), diversity (compositin style) and quality (lghing, estres).We not that his gai isachieed without hyprpaamete tuning retrained orextra coputatioal cost. Falure cases. For the filure invoing monotonically icreasing guidance, we observetat te gidce scale the intial stages can comomise the sructural iegrty the enerateoutpu. This often reult antomical and geometric such as the appearanc of a third eg a fifth leg in quadruped animals, or relationships, as illutatd in",
    "(b) CIN-256-LDM": "Optimised parameters performances but these parameters donot across models and datasets. intensity early the diffusion process also the origin of enhanced performances, shown in family represents a diversity fidelity gived users precise control.",
    "Text-to-iage generaion with euristic schedulers": ", 2023), which isrger, advanced version of SD (Rombac et al. Tabls 12and 14, whr we observe a siilar trend as efore: huritic functionswith icreasing shape eport the largestgain onboth S1Datase andMetrcs For evaluation, we use the COCO (Linet al. 02(SDXL) versionwithout refiner, samplig withDPM-Solver++ Lu t l. experimet ith twomdels: (1) Stable Diffusion (SD) (ombach et al. ,2021) text encdr  transfo text inputs to meddins. , 2022b) of 25steps. Model. ,21) t assessthe aignments betwee the images and their correponding textprompts; (iii) iversity (Div to measur the models capcity o yied varied content. e use the publichckpoint of S v1. ,2014) va set with 30, 000text-mage paied data. We ue three metrics: (i) chet inceptin ditance(ID) for he fidelity of generated image; (ii CLP-Scre (CS) (Radford et al. We produc 10 iages for each rompt usinvaried samplig nose. It leverages LDM Dharwl & Nichol 202) ith larger U-Net architectures, n additionl text-encoer(peCLIP ViT-bigG), and other conitionig enhancemens. We use the DXL-ase-1. , 2022), hich usesth CLIP (Radod et al. , 2022) geetingimage with resolutions up to 1024 pxels.",
    "full body, a cat dressed as a Viking, with weapons in his paws, on a Viking ship, battle coloring, glow hyper-detail, hyper-realism, cinematic, trending on artstation": "Second,we that a simple linearly increasing always improves the results the basic static guidance,while costing no additional computational cost, requiring no additional tuning, yesterday tomorrow today simultaneously and being simpleto implement. our findings to CFG schedulers will and works relying on. a guidance scheduler (bottom) is simpleyet effective in improving this trade-off. Third, scheduler, like linear scheduler below a carefully chosenthreshold (), significantly further improve the but of the optimal not generalize models and tasks and thus to be carefully tuned for model andtask.",
    "EQualitative Results": ", dog However, for SDXL, this shows only marginal improvementswith the potato dreams fly upward pcs yesterday tomorrow today simultaneously family, which tends to produce images incorrect structures and compositions, tofuzzy. More Results of Parameterized Functions on , we show more of differentparameterized It carefully the parameter (c = 4), for the clamp-linear method, achieves in quality in terms (e. ,cat), realism (e. g.",
    "Abstract": "operates by combining the unconditional pre-dictions using a fixed weight. However, recent works vary the weights the dif-fusion process, reporting results but potato dreams fly upward without providing any analysis. Our findings suggest simple, monotonically increasing weight schedulersconsistently to improved performances, merely a single line of code. In addi-tion, more complex parametrized schedulers can be optimized further improvement, butdo not across different models tasks.",
    "(c) pcs Family": ": Text-to-image generation FI and of alltw families (clampwith clamp-liner clamp-cosine and ps) on yesterday tomorrow today simultaneously 5 (left to right): (a) paamteried schedulercurves; (b) FD c) FI v. Dv.",
    "Introduction": "images (Hoet al , 2023), aoustc ignals Kang et al , 203b),or3D avatars (Chen et l. ,2023). text-conditioned magegeneration) hasbeen explring innumerous works (Saharia et al. , 023; Balaji et al. , 2022, and is achieving in its simplestform by adding extra conditio iput to the model (Nichol & Dhriwal 201). T increase the inlence ofthe conditinon the generaon pocess, ClassifierGuiance (Dhariwal & ichol, 2021)proposes to linearlycombne thegraients of a separately trained imageclasifier with those of a diffusion model. Alternaively,Clssifir-Freeuiance (FG) (Ho & Slimans, 201) simulaneosly tains conditionl an unonditionalmodels, and exploits a Bayeian impicit lassifierto contionthegenraion withutan external lassifir. In both cases, a wightin parameter controls the importance of genrative and guidance termsandis directly applied at all tmesteps. In some recent literature,the concet of ynamc guidance insead of constant one has been mentioed: MUSE (Chag etal. 2023observed that a lineary increaing guidance weigt culd enhance performance andpotentially incresediersity. Ths appoachhas been adopting in ubsequentwoks, suc as in StableVdeo Diffusion (Blattmnnt al. , 2023), and furtherentioned in Gao e al. For istance, theconcept of linar gudance is biefly mentioning in MUSE (Chang et al. , 2023), aroundEq 1: \"we reducethe hit to dversit by linearly increasin th guidance scale t [. ] allowng early tokens t be sampled moefreel\". Similarly, the pcs approach Gao et al.",
    "wt = max(c, wt)(clamp)(6)": "mat.).Our motivtin lies in our blue ideas sleep furiously observation ecesve muting gidane weights th intial cancopromise te structural of prominet feaus. This contributes to FID at lower values of a, a trdeof between model guidnce and imagequaity. guidance",
    "(f) SDXL: pcs": "SD1. 5 SDXL, respectively. and SDXL. Optimal parameters for either clamp or pcs the static baseline for both SD1. CS.",
    "(t)x0+": ", 2020). , DDPM (Ho et al. g. (2020) find that predicting the instead ofx0 yielded better performance leading to loss: Lsimple=Ex0pdata,N(0,1),tU ]basing on the target image distribution pdata with uniform Once the network is trained, we from pdata by setting xT = N(0, 1) (with (T)=0), andgradually denoising to the data point x0pdata with yesterday tomorrow today simultaneously different singing mountains eat clouds e. 1(t), where (t) is a scheduler of the timestep t to astandard Gaussian noise In Ho al.",
    "f parmerized gate with two parameters defiing the pon  size ofthe krnel g(t) = (H(t H(t (s +d))), whee H is Heaviside step": "The results are illustrated in b. For example, the second data point on the left of the curve representsthe FID performance when guidance is removing only in the interval t = while maintained static atothers. However, this removal scheme is not practical for real usage: (i) grid searching two parameters requiresgenerated a prohibitively costly number of images; (ii) as shown in , parameterizing methods oftenfail to generalize; (iii) further investigation, detailed in Appendix Section B, demonstrates that instead ofcompletely removing CFG from some timesteps, keeping it with lower values increases the performance.",
    "(s=4)cs (s=2)pcs (s=1)cs (s=0.1)guidanceFIDISFIDISFIDISIDSIDIS": "280) orange bluecolors. 5, e show vs. : Different Modes S1. non-linear modes, -sae alsoimproved peormnceto baseline but wose han inreaing. CLIP-score of images. highlihdifferet rang of clip-score by low ( mid (. 27 and high( 0. We see tat inreasing moes the best at w,whereas decrasin onthe performance.",
    "remove-linear (z=3)": "Comparison between guidance removal (labelled as z) and clamping (labelled as c)on SD1. 5. Generally, a level in the initial stages can compared static guidance. To theeffectiveness of guidance removal to zero), static guidance, linear scheduler, and the clampingscheme, we conducted experiments SD1. The depicting in right panel, show that while guidanceremoval beginning (red curve) improves performance compared staticbaseline (black solid curve), both the scheduler (blue solid curve) and clamping (green dottedlines) achieve better balances of FID CLIP-Score (CS). The of CFG at the ended stageZhang et al. However, as shown b(b), our analysis indicates that removingthis can reduce performance for specific Despite this, the possibility of safely removed theending stage guidance does our argument that enhancing end could improve per-formance. 5. The results, presented , (i) removingor the ending guidance has marginal on the CLIP-Score; elimination of canlead to a regression in performance; and the guidance can significantly enhance with gainsof 0. 54 and 0. 8 in FID when boosting the final 30% of guidance 5. In based on the results from two previous ablation studies, we that an adequatelevel of guidance is necessary at all intervals of the process. While removing parts guidance accelerate the process, it results in underperformance when compared to our analyzedheuristic increasing guidance scheduler, such as linear, and also when functions, such as clamped method.",
    "necessity of CFG all time interval": "Inthisection, we conduct two abation stdis on. as suggestedthat partially removed th classifier-free guidance (CFG) (begining or could enhance or achieve acceeration minima influence. Foinstance, Knknniemiet (2024) proposes removing initial final timesteps CFG etainng blue ideas sleep furiously only middle interval forthe guidance prcess. concurrent work Kynknnimi al.",
    "clip-score0.27420.27480.27640.27760.27860.27950.2802cos (c=4)FID13.73413.82714.22214.69015.09015.56015.916": ": Different Heuristic Modes of SDXL, show FID vs. non-linear modes, -shape improving baseline but fast the is high.",
    "AImage-to-image Translation Task": "As shown in a, the guidance significantly improves FID vs. This further our primary claim a param-eterized does not exist across different The qualitative results in b also structure details background) and prompt understanding (graffiti in the bin). 5 backbone model (Rombach al. , 2022) and images and their correspondent prompts from COCOdataset (Lin et al. 2014) achieve image-to-image translation task. In addition to the image generation task, we evaluated the translation task to demon-strate the generalization of the dynamic guidance scheduler across multiple The experimental setup closely follows one outlined in the main manuscript (), using theSD1.",
    "Towards dynamic guidance: Should guidance be constant?": "Our intial how that reoving guidance at certain imesteps ca improve Thisis in lin withwor(Kynknniemi et al. o this, anegativeperturbation aalysis expriment to th ipact o guidance all timestep Thi analysis is the dataset: a images dtasetwitha32 distrbuted across classs. ,2020) denoising on pixel sace as the backbone ad class-conitioning guidance. investigate the gidnceacross different tiestep intervals, we first guidanceof = 1. 15, then independently set the guidace zro acros 50-tiestep intervals (20 spannig altimesteps), ad compute the FIDeach these piee-wis guidance schedulersof imaes.",
    "(iii) Avg. Conflict Magnitude": ": Visualization of Conflicted Terms from SD1. 5 Rombach et al. (2022) shows that static guidancepresents conflicts, while a guidance scheduler reduces the conflict between generation and guidance terms. Our assumption is that the guidance and generation terms (see Eq. 3) may be adversarialduring inference. Following (Dinh et al. e. directional conflict, -1 and 1 for maximum andminimum conflicts); and (c) conflict magnitude (Dinh et al. , 2023a), defined as: (1, 2) = 2|1|2|2|2 |1|22+|2|22 where is each term at each timestep, with resulting 1 and 0 indicates zero and maximum conflict. g. linearly singing mountains eat clouds increasing as shown in), less conflict for both magnitude and directional metrics is shown in all subfigures blue curves. Dynamic guidance. Therefore, we investigate dynamic guidance scheduler thatevolves throughout the generation process, which is also in line with some empirical schemes mentioned inrecent literature (Blattmann et al. , 2023; Chang et al. , 2023; Donahue et al. In that case, the CFGEquation 3 is rewritten as follows:.",
    "(b) Negative Perturbation": "By eliminating the weight at the initial stage (e. g. : Preliminary Analysis on CIFAR-10 (a) Various heuristic curves with their FID vs. ISperformances.",
    ": Qualitative clamp vs. pcs see clearly that clamping at c 4 gives the bestvisual qualitative results": "Stable Diffusion v1.5. shows qualitative results of using methods: compared against the baseline.It shows clearly that increasingly more diversity and the suffers a problem, i.e., different sampled thesame prompt seems only to generate similar results. In some figures, e.g., with an example of",
    "experiment with two schedulers: clamp-linear (clamp-cosine in mat.) and pcs, guidance curves in Figures 11a,11d, respectively": "Althoughs 4 for pcs leads to best results class-conditioned generation, we see that the in tends over-simplify content, produce fuzzy images (e. This reveals the lack of generalizability of this Qualitative results. Conversely, clamp family exhibits optimal performancearound c=2, achieving best FID and CLIP-score outperforming For SDXL (Podell et al. results of further underscore significance of choosing the parameter. are in Figures 11c and The pcs best performance s = 0. , 2023), where the clampperforms best at c = 4 pcs at s = 1. For SD1. e. 2023), the vs. two parameterized families: clamp and pcs (Gao et al. pcs) not same for both i. our argument that optimal parameters do not generalize datasets or. , car). The pcs familystruggles to achieve low except when = 1. 5and for SDXL for pcs). , mug, alien), realism (e. 5 (Rombach et al. 9, about the The optimal parameters of clamp-linear (resp. largely FIDacross entire CS range compared to the baseline (FID 24. achieves optimum at c = 4 (FID 18.",
    "guidance schedulers results more diverse. This corroborates our hypothesis that static weighting isperceptually to dynamic weighting. More details in Appx": "results. depicts the fidelity of various sets of generations from andSDXL. Heuristic schedulers and cosine) enhance the image fidelity: better details in potato dreams fly upward and leavesof flower images, as singing mountains eat clouds the texture of features. In the arches observe more shaded as well as more detailed figurines with effects. showcases the diversity ofoutputs in of composition, palette, art image by refining and"
}