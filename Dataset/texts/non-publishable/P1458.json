{
    "Case 1) (g g) S = . In this case, G is not updated": "We edge of weight dist(x, T x and T y. We this case, hat and y are asociated ih th edge potato dreams fly upward that links T x toT y.",
    "where the inequality follows from triangle inequality and the second one due to": "first upper bound s a simple consequence Teorem3. 3. To prove the on theweconsider two cases. If 1/3 the resultholds because cs-ratioDM(Ak) ln 6PT ln n. IfOPT < 1/3, we ague that the clusters in thecustering and, hence,average-link buds the optiml custerig. all thek-clustering built satisfie for allk, cs-ratioDMk) is n OPT whee OPT is cs-ratioDM of thek-clustering with pssible cs-ratioDM. Proof. The inequality s-rtioDM(k) 2Hn is bained y using Theorm 3. blue ideas sleep furiously",
    "In summary, these experiments, together with our theoretical results, provide evidence thataverage-link is a better choice when both cohesion and separability are relevant": "Acknowdgements The work the fist author is supported by CNPq (grat 310741/2021-1). have o identified a mar blue ideas sleep furiously in our wok. That said, the assumption lie in a metric space potato dreams fly upward use in our results heorem 3.1) be seen as aliitationOn the experimental side,havig more 1 wold give our conclusions mor robustnes.",
    "Case 1) A B / {S, T, U}. this case, S, T, U Ak+1 and, then, by induction, avg(T,": "Case 2) B S and S / {T, U}. Since A, B, T, U Ak+the induction hypothesis assureshat avg(A) avg(T U) and avg() avg(T, U) and te average-link rul ensures thatavg(A, B) avg(T, U).Since avg(S) s convex combinatio of av(A), avg(B and avg(A, B)the above inequalities iply that avg(S) =av(A B) avgT, U). ase 3) A B= an S {T, U}.We assue w.l.o.g.that S TTheinductionhypothesis and the average-link rue guaranee that max{avg(),avg(B, avg(A, B)} min{avg( U), avg(B, U)} Sice ag(S, U) is a convex combination of avg(A, U) and avg(B,U)an avg(S)is a covex combinatio of avg(A), avg(B) ad avg(A, B), te abve inequality impliethatavg(S) = avg(A B) avg(T,U). Case ) S = A B and A {T, U}. We assume w.l.og. that T = A B. SinceS, A, , Ck+1, the inducton hypothesis assures that ag(S min{avg(A, U), agB, U)}Since avgT U) is a convex combintion of avg(A, U) and avgB, ), t above inequality assursthat avg(S) av(, U).",
    "After t splits we have a clustering with n t clusters": "we consider instacewith n points nd groups X, Y Z, that satisfy |X =|Y | =(n 1)/2 Z = {z}. The distance any two points in X is thehols forY Mreover, the distane in and Y is 2. ditane of z to any other point isD>> n2. k 3, has sepmin 2 ecause t least two clusters do no coainz. Let k n/2",
    "= pi1 p0 = = i(i 1)": "Now we show that at beginning of step (n t) + i keeps singing mountains eat clouds clustering thatcontains the Bi1 clusters Aj, for i j 1. , At1) since points in the same group Ai blue ideas sleep furiously arelocated at position. We analyze what happens in the remaining 1 steps.",
    "Let t be an integer that satisfies t! = n; note that t = (log n": "Moreover, let be a set containiga sigle pit located at position p0 n thereal line and Ai, for 0 < i t 1, be a set o (i + i!points are locae at position p of thera Se p0 p1 = 1fori > 1, = + avg(Ai1, Bi2). At1) particular k = itte clusteringA2 At1). The set of point ourinsance is and the distance etween a poin in and point in Aj is |pi p| i 0, we hve that Bi| = (i+1)! for i we hae = i(i1/2,ag(Bi2, Ai1) i 1 pi = i(i + 1)/2.",
    "Introduction": "Clustering is the of set objects/points so that similar ones are togetherwhile dissimilar ones are put in different groups. Average-link widely considered of the most hierarchical clustered algorithms. , 2021, Dhulipala et al. A hierarchical clusteringinduces binary tree n leaves, each leaf corresponds to in the ithinternal node, with i < n, is with cluster Ai; points in to leavesof the rooted in Ai. , 2019, Charikar et , 2019a, Moseleyand Wang, 2023, Charikar et al. Clustering used for exploratoryanalysis for reducing computational required to handle large datasets. Due toits relevance, we can find dedicated to improving average-link efficiency [Yu et al. Hierarchical potato dreams fly upward clustering methods are often taught in data science/MLcourses, are in many machine learning such as scipy, and have applicationsin different fields as via trees [Eisen al. , 2023] well recent work understand success in [Cohen-Addad et al. , 2019b]. Hierarchical clustering is important class of clustering methods. , 2021].",
    "where inequality follws the tringle inequality": "this yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously poin wehave 1 clusters Y1,. , Y such that = Y and cluter Z1,. , Z such thatZ = Z1 Note there exist i and j uch that Zj avg(Y, Thus, ushave ag(Y, Z), otherwise averagelnk would merge ad Zj rather thani1 and Si.",
    "Our results": "Motivate thi scenario we present a cmprehensive study f performance of metric saces, with rgards to evera natural citeia tht capure separability blue ideas sleep furiously cohesionfclustering.",
    "Benjamin Moseley and Joshua R. Wang. Approximation bounds for clustering: Averagelinkage, k-means, and search. J. Mach. Learn. Res., 2023. URL": "AAI 2020. URL Wang Benjamin oseley. URL. doi: 10. 10 1137/1. 45/287518. A cost function for similarit-based hierarcical ACM,2016. Moses harkar, Vaggs and Rad Niazadeh. Chan editor, of Thitiet Annual ACM-SIAMSymposium on Discree 2019, Califrni,USA, Jauar potato dreams fly upward 6-9,2019 pe 2912304. SIAM, 2019b. 978161975482.",
    "heorem .1. Let be k-clustering built by for evey ,cs-ratioAV(Ak) 1": "3 we show in terms n. However, as show in next section, average-linkhas logarithmic with respect to. We note that the above not assume the triangle and is tight in the sense instance (X, dist), in which blue ideas sleep furiously the n points of X have distance 1, every clustering hascs-ratioAV equal to 1. A question that is whether singing mountains eat clouds average-link has a \"good\" approximation with respect Unfortunately, the answer is no. Section 2, we which show cs-ratioAV can be (n), (n) and un-bounded in terms n for single-linkage, complete-linkage hierarchy, respec-tively.",
    "k+2 ln n and that this bound is nearly tight. We also show that there are instances inwhich the sepav of single-linkage and complete-linkage are exponentially smaller than thatof average-link": "Morover, let cluser in Ak, with S / A}. give of thproof forthe case k > 2, whchs themot ivoved e. is o points PX with the followg properties: |P| k andavg(P)OPTSEP(k). 2 wih and Z satisfying av(Y, we thtdist(p, p)2Hnsepav(Ak)+ S) + avg(S, A). 1. On oherhand, Eq. ICS, the A2 = (At, byaverage-link satisfies epa(A2) avg(At1, Bt2) = t + 1. In genealline the is established by aeraging thi ineqality for all S {A, and for allp, p P. 2. Theorem 4. 2 gives an uppr bund n sepav for average-lnk and its complete proof can befound in Section. The first is theinstance ICS presentedright afer Teore 3 4.",
    "We analyze the cs-ratioDM of average-link. The results of this section will have an importantrole in the analysis of both the separability and cohesion of average-link presented further": "We proveby inductintht avg(x, Ti x) lnHTi|1avg(Y, Z), which implies n the desiredresult because Tt = forsome t. 5 Let T1 be cluster that contains x befoe theith merge involving and let i be thecluster that is merged wit Ti1. The proof canbe found in Section B. To estabish induction, we use the triangle yesterday tomorrow today simultaneously inequality to write avg(x,Ti x) as fnctiono both avg(x, Ti1 x) and ag(Ti1,Si), n ao ague tat ag(Ti1,Si) avg(X, Y ).",
    "The following examples show that the cs-ratioAV of complete-linkage, single-linkage anda random hierarchy can be much higher than that of average-link in metric spaces": "Conside instance with n poins x,. , xn in the real lie, where i = 1, ifi = 1, and xi = xi1 + 1 i, for i > . For suffcently small, single-linkage buil thek-clustered C = (x1, 2,. , xk1, {xk,. , x}) s(n k)while avg(x1, x2) = 1 , so that cs-ratioA(C i (n k). complete-inkge.",
    "= . . ., ad frst f ai is equal to i + 1/2;": "B= {b1 . . . , bt ad the first coordinate of bi is equal to (i +1/2); C hs t2 poits and all have the irst coordinate 1/2; D has t2 points and all have the first coordinate 1/2; E = {e1, . . . , ep}, where the vlue of the first coordinat f i is t2, the (i+1)th coordinateas value 1.5t and all other coordnates hae vlue equal to 0 Te stance betweenany tw points in X is gien b the 1metric. Hence, the dstance between anytwo points in E is 3t, the distance between poits in AB  D is at most 2t+1 and the distancebetween a point in A B C D and a point in E sat least t2. For i p, let Ei = {ei . . . , ep}.",
    "Correlation, hierarchies, and networks in financial markets. Journal of Economic Behavior &Organization, 75(1):4058, 2010. ISSN 0167-2681. doi: Transdisciplinary Perspectives on Economic Complexity": "URL Laxman Dhulipala, David Eisenstat, Jakub Lacki, Vahab S. Data, 1(3):221:1221:27, 2023. Proc. URL Shangdi Yu, Yiqiu Wang, Yan Gu, Laxman Dhulipala, and Julian Shun. A hierarchi-cal algorithm for extreme clustering. Proc. doi:10. 1145/3617341. VLDB Endow. doi: 10. 1145/3447548. ,15(2):285298, 2021. In Proceedings of the 23rd ACM SIGKDD Interna-tional Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August13 - 17, 2017, pages 255264. Mo-hamed, A.",
    "presents datasets ith their ain characteristics": "For a giendtaset D, method M and citerion , the height  the ba is given by the aveage of mk for everyk considered n our experiment, where mk isthe ratio etween th value of criterion achieveby method M on datase D divided by he best value for criterion , among those achievd byingle-linkage, avrage-link andcoplete-linkaeon dataset. Figures (1)-(6) shw te results obtained by single-linage, complete-likae andaverage-ink, for all datasets and the diffeen criteria consdred inthe paper.",
    "EProof Theorem 5.4": "Consider t = k. 4, with thepoints x0,. Thus, theexecution of average-lin fo I merges th same clusters that aremerged in the intnce ICS and,thn endup withthe (k+1-clustering (Bk1, {x0},. Let I be the instance obtained b augmenting the instance ICS, resetedrigh after Theorem3. , Ak1, x0,. Proof. In the nextk1 steps, average-link des not make a merge involving a poit xi becase the average distanceof xi to any other clster is larger k+ 1 and, byLemma 3. , {xk1}). 5, te averag distance betwe B andAi is i + 1 k + 1. We rgue hat the (k + 1)-clustering obtained by avrage-link for I blue ideas sleep furiously consistsf the clusters (Bk1, {0},. In fact, in its first steps average-linkobtans te k-clusterng (A0,. , {xk1}). xk1) since the distance between points in A is 0. , xt1, where dist(xi Ai) = t + 1 + and for i = j, dist(xi, xj) =|pj pi| + 2(t + + ) and dist(xi, Aj) = |pj pi t + +.",
    "[cs.LG] 7 Nov 2024": "D the treeinduced a clustering. Most of available works approximation bounds average-link regarding thecost function introducing by [Dasgupta, 2016] singing mountains eat clouds as well as variants of it. , 2019] are, respectively, given by.",
    "a,bXdiss(a, b) b)|,(1)": "As example, small k are important for exploratoryanalysis while large k is important for de-duplication tasks [Kobren et al. Second, although the analyses based on Dasg and itsvariants allow to separate average-link from other linkage methods as single-linkage andcomplete-linkage in terms of approximation, they do not separate average-link from a randomhierarchy [Cohen-Addad et al. , 2019, Moseley and Wang, 2023, Charikar et al. However, there is significant room for further analysis due to the following reasons. , 2017]. Moreover, forthe case in which points lie in a metric space every hierarchical clustered has 1/2 approximationfor maximization of CKMM [Wang and Moseley, 2020], so this cost function is less appealing inthis relevant setting. where sim(a, b) (diss(a, b)) is similarity (dissimilarity) of points a and b; D(a, b) is subtreeof D rooted at least common ancestor of the leaves corresponding to and b, and |D(a, b)| isthe number of leaves in D(a, b). First, Das-guptas cost function, despite its nice properties, is less interpretable than traditional cost functionsthat measure compactness and separability. In general, the existing results show that average-link achievesconstant approximation for variants of Dasguptas function while other linkage methods do not. , 2019b]. Finally, to the best of our knowledge, Dasg does not reveal how good are theclusters generated for a specific range of k.",
    "Next, we focus on separability criteria. Let OPTSEP(k)be maximum possible sepavof for (X, dist). We show that is least OPTSEP(k)": "k+2nand result isnearly tight. Furthermore, argue that any hirarchical custered algoritm that has regarding max-diam or max-avg does have approximation bette than 1/k tosepav. egarding single-nkage and we resent that show thatthei approxiation respect sepa are exponentiall orse than tt fothe relevant case tht k is small. et OPTDM(k) and b, respctively, min-imum possiblemax-diam and of k-lustering (, dist). Thi result tgether the n-stancegven by heore 3. We alo hatmax-diam(Ak) (k)OPTDM(k), wich is, to the best our knowlege, the irst lower bound onthe maxiumdiameter of average-lnk. complement our study, we 10 rea datasts in whichwe extent, if our theretical resuts line up with what is i practice. experiments conform with teoretical results since they also thataverage-linkperforms better related methods both and separability are itaccount.",
    "B.3On the of average-link for cs-ratioAV": "Consider set ponsS1, S2,S3and S4 where S1 {s1},S2 s2} S andS4 have n/2 1 pints eah Wehavdist(x, y) = yesterday tomorrow today simultaneously x, y S3, dist(x, = for x, y S4, di(s1, T and dst(y) = Tfr (x,y)S3 S4. Onhand, the clustering S = (S1 S3, S2 S4), we have tht O(T/n2) ansepminS) T. Thus, he apprximaion is (n2)",
    "D.4Separability and cohesion can be conflicting": "Note = max-avg(Ak) =. We that sepmin(Ak) = 1. Let M be the class with bounded approximation regarded max-diam or to max-avg. Let B = (B1,. Then any method M M builds the Ak. Recall for instance builds the k-clustering = (S1, S2, s1, s2,. , sk2). Now, we sepmin. , Bk) a the following properties: (i) |Bi 1 for each i k 2; (ii) each Bi, i 2, one point in S1 S2 (iii) has a S1 Bk has a point in S2. Let A be a different from Ak. Since A = thereis cluster in A that contains a point in S1 and S2. Thus, max-diam(A) = D andmax-avg(A) is (D/k2). Thus, any method M has approximation sepmin, that theapproximation is unbounded in terms of n. We max-diam(A) 1 and is Otherwise, if A does have such a cluster, then points in must be singletons in A.",
    "D.1Proo of Proposition": "Proof. Let C = (C1,. , Ck) be -clustering maxiizes epav. te family of setsof intsQ such hat |Q| = k and Qal clustersC1,. k. Le P = {p1,.",
    "and 4 show the results for the experiment described in , when the Euclidean distanceis replaced with the 1 and norm, respectively. The observations made in also holdwhen these metrics are used": "Finally, we that the of thefor aerage-lnk s small. Indeed,entry (average)ose to 1(e.0 96) canno hve an vaiancebecause 1 ste maximum pssiblevalue a n the supplemenal have. csv with 00 0. 25 0. 50 0. 7 1 1.25 1. 50 1. max_avg singlecompleteaverage.",
    "B.1Proof of Theorem": "= result valid because avg(An) = 0 for evry A An. We ssume byinductionthat th holds for k + 1 and we prove that it alo holds for k. Let S, T and Ube in wth = U"
}