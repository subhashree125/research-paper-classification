{
    "KDD 24, August 2529, 2024, Barcelona, SpainKun Wang et al": "Wei Huang,aygWeitao Du, ie Yin, a Xu, Lig Chen, andMiao Zhang. 221. Simple: Simlarpseudo label eploitation for classifiction. In yesterday tomorrow today simultaneously Proceedigs theIEEE/CVF Conference on Computer Vision and Patten Reognition. 150991518. 03113 (2021).",
    "Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graphconvolutional networks for semi-supervised learning. In Thirty-Second AAAIConference on Artificial Intelligence": "Finingglal homophlyin graph neual networkswhenmeted heterophily. InInternaional Conference on Machine Learing. 20. PMLR,1324213256. Drek Li, elix Hohne Xiuyu Li,Sijia Linda Huang,Vaishnav Gupa, OmkarBhalerao, and Ser Nam Lim. Avancs in Nual Infor-ation Procssin System 34 (221), singing mountains eat clouds 2088720902. XiagLi, RenyZhu, Yao Cheng, Caihua Shan, Siqang Lo, Dongsheng Li, andWening Qian. 222. Lage scae learning o on-homophilousgraphs: New benchmarks and strong simp thods.",
    "layer": "Visualization of the subgraphs extrated applying to 4-lyer with Texas. ) Predcion results the cental C (Te label is 2) usng diferent algorithms.",
    "Wang, Guo, Yang, and Yunhong Wang. 2023. Heterophily-Aware Graph Attention Network. arXiv preprint arXiv:2302.03228 (2023)": "Ku Wang, Yuxuan Liang, Pengun blue ideas sleep furiously Wan, Xu Wng, Pengfe Gu Junfeng Fang,and Yang Wang. 222. earching Lottery Tcket inGraph Neul Neworks: ADual Perspetive. aXiv prepritarXiv:2308. 2022. The Swflake Hypoth-ess: Trainn Deep GNN withOne Nde On Recptve field. 2023. Kun ang, Guohao Li, Shilong Wang, Guibin Zhag, ai Wan ang You, i-aojiang potato dreams fly upward ng, Yuxun Liang, and YangWang. 1005 (2023).",
    "GCN+62.2563.1163.3863.2362.34MixHop+67.7469.0468.7869.0867.22JKNet+70.1770.3872.3771.6970.55MGNN+60.1962.0961.7661.9260.83": "This that the HES adeptlycaptures most critical information for prediction and redundant Right. In our analysis of Squirrel, we find the conven-tional SnoHv2, after pruning, its homophily ratio from0. to 0. 364. Hetero-S achieves a dual victory ofgreater graph sparsity and accuracy compared fur-ther to superior capabilities of HES over heterophilic graph scenarios. For HES consistentlysurpasses SnoH-v2 by substantial margins acrossSquirrel dataset (0. 8% 2. 8. 9% on graph sparsity).",
    "Graph Neural Network, Heterophilic Graph, Pruning": "ACM,Ne York, blue ideas sleep furiously NY US, 19paes. ACM Reference Format:KunWang, Guibn Zhang, Zhang, Junfng Fang, singing mountains eat clouds XunWu, Li, Huan, Liang. 2024.",
    "Backward to update GNN and proxy model P": "Effciency comaison with pruning algoritm 4). Here,we juxtapos Hetero-S is yesterday tomorrow today simultaneously predecessors,SnoHvand SnoHv2, o raph. Main experimentsWe delve int vryngdepths ofarchtectures. Te am s to determine whether of these GNNs o maintain o enhanceperfrmane as netwok goes deeper, avoiding yesterday tomorrow today simultaneously issues likevanishing or Comparative analysis with traditionalnowflae (RQ3.",
    ": The algorithm workflow of Heterophilic Snowflake Hy-pothesis (Hetero-S) and Heterophily-aware Early Stopping (HES)": "For instance, whenusers engage with content on Netflix, the people with diverse pref-erences might be subjected to similar recommendation algorithms,owing to their interaction with identical video content. In aheterophilic context, this mechanism introduces two primary andchallenging limitations: In graph topology, local neighbors refer to nearby nodes, oftenoverlooking distant yet informative nodes. A consistent aggregation and update method often neglects varia-tions in information from alike/unalike neighbors. Given above emerged challenges, there has been a shiftingfocus towards explored heterophily in GNNs. This research areaincludes a wide range from delving into heterophilic graph samplingto the evolution of intricate algorithms. The growing interest inheterophilic graph learning can be attributed to its vast applicability. From a macro perspective, existing heterophilic GNNs can bebroadly classifiing into two categories, i. , non-local neighbor ex-tension and GNN architecture refinement. This is achievedthrough strategies like high-order neighbor information mixing and potential neighbor discovery , en-hancing representation by capturing distant but relevant node fea-tures. With second class, GNN architecture refinement focuseson enhancing the expressive capability of GNNs for heterophilic graphs by optimizing AGGREGATE and UPDATE function. Through strategies such as adaptive message aggregation ,ego-neighbor separation , and layer-wise operation ,the refinement aims to produce distinguishable and discriminativenode representations. Recently, novel paradigm, the Snowflake Hypothesis (SnoH) ,rooted in the concept of one node, one receptive field has gainedsignificant attention for its efficacy in addressed the over-smoothing and over-fitting issues in GNNs. This hypothesis drawsinspiration from the intricate and distinctive patterns exhibitedby individual snowflakes, assuming that each node can possess itsunique receptive field. It posits that for an -layer GNN, every nodein the graph harbors an optimal receptive field width denoted as (1 ). During the message passed process from 1 to hops, each node merely aggregates information from the preceding hops, after which they cease to aggregate information from theneighborhood (referred to as node early stopping). Intuitively, the snowflake hypothesis demonstrates even greatervitality and significance in the context of heterophilic graphs: (1)One concept benefits all. idea behind snowflake hypoth-esis can be seamlessly integrated with any heterophilic GNNs de-signs, showcasing exceptional versatility. Whether it is for non-localneighbor extension or GNN architecture refinement, snowflakecan easily be incorporated as plugin and demonstrates strong com-patibility. (2) Enhanced pruned requirement. In heterophilicgraphs, given the higher probability of central nodes having differ-ent labels than surrounded nodes, the need for pruning aggregationchannels becomes even more critical than in homogeneous graphs. This pruning aids nodes in selectively aggregating information andupdating themselves effectively. However, the original SnoH and its implementations appear to ex-hibit limitations when appliing to heterophilic graphs. These heterophily-unaware approachesdo not adequately integrate into heterophilic scenarios. This value can further representthe homophily score between two nodes, subsequently servingas a replacement for the original adjacent matrix. By analyzing the variation in the heterophilic ratio across eachlayer, we are able to appropriately determine the depth for earlystopping. This approach contrasts with SonH, which primarilyexhibits efficacy in deeper GNN configurations.",
    ": The performance of backbones and the results after adding Hetero-S (+)": "Notably, the perormanceboosts are no solely dependent depth potato dreams fly upward but lso showa trend ginswith inreasing complexty, up to poin where. to leveraging heterogeneity alignsexceptioally well wih sophisticted architetures GCNII.",
    "COMBh(1), , R (7)": "It is noteworthy that, beyond blue ideas sleep furiously the R-th layer of GNN, node ceasesto information from thus achieving earlystopping of field. Further, (c,d) performance GCN on Squirrel and CS different ho-mophily thresholds, which characterizes a 2. 12% under = 0. 3on Squirrel, and 1. on This confirms of node receptive fields can yesterday tomorrow today simultaneously assist GNNs inlearning refined node representations.",
    "Proxy Label Predictor": "As depicted in semi-supervised scenarios, we only know asmall fraction of labels in a heterophilic this blue ideas sleep furiously formulate a GNN prediction process as a combination of twoprocesses, i.e., {AGGR COMB} . After the aggregation andcombination process, nodes are a latentspace characterized by distinguishability. This nodes with label are positioned similar the identification relational patterns. employing an function, we effectively predict theoutcome , structured information encapsulatedin this latent space. enhances the models todiscern and categorize nodes, significantly improving the accuracyand efficiency of our predictions. work, we first present a lemma: Lemma 4.1. that NH for decreases w.r.t in to > 1), as receptive field expands, aggregates more heterophilic information. Under such circumstances,when employing receptive field stopping, there exists condition that ({ > ({ }(+ ), where () is performance metric, { signifies aggregating information from -hopneighborhood, and N+. this in mind, construct label predictor P todetermine label-wise graph aggregation . Concretely, to a predictive model (here we use MLP and we place results in Sec. to pseudoprobability label z loss:",
    ": The JKNet and + across CS, DBLP, Actor andChameleon on 4, 8, 16, 32-layer settings": "Hetero-S ca assist the top-tudent bckbones. Aditionally the conventional Sno is specially designedfodeepning GNNs on homophilic graphs, ad we provide furtheromparisns betwen Hetero-S and SnoH i Appendix F. orhetrophili graphs,we choos Actor (Hnode = 0. Specifically, we select ResGCN, JKNet , GCNII and SGC as backbone and conduct tess o 2omophilicand 2 heteropilic graphs, assessing perfomance at depths of upto 32 layers. We have placed additional experimental results inAppendix E, from which we can draw similar conclusions. 82 repetively. However, even or th pecifically deep-ened nework JKNt, HES still demonstrates exceptinal auxiliaryperformance, further proving te importance f early stopping inreceptive fields. 5. 4Compare With Pruning Mthods (RQ3In thi section, we comare HES with current SOTA pruning meth-ods, US adDSpar. 22) and Chameleon (Hnode=0. 9% against the JKNet baseline. We attempt to understand whetheretero-S can (1) achee satisfactry sparsity without compromis-ing performance, and () enuinely accelerate GNN computatins. 3Exten Htero-S to Deep GNNs (RQ)T providea scalable solution for large and densly connectdheterophilic graps, e extnd the HES algrithm to deep GNNcontexts. For hmophilic graphs, we opt for CS and DBLP ,whose Hnode is 0. e obsrved that incorprating the HES al-grithm leadso erformance gains in bot homophilic and het-erophilic graph contxs. Obs. As shown i and , we can list the observations:. Hetero-S consistenty boost GNNs at all dpths. 5. 23), Welist the following bservaio:Obs.",
    "Sparsity": "Identifyin eceptive fiel for eac node : The pipline of ur ES framework. or each node, we utilize proxy model to evaluate the homophily strength of its edges, whch ifurther used to estimateits multihop homophily ratio. Based on tehomophily strength at each hop,we erform receptive fiel-levl earystoppig to determine uniue recetive fieldforeach node.",
    "Main Results (RQ1)": "We initially insigate presence and ientiiabilit of the het-erohily snowflkehypthesis (Hetero-S) through the heterophily-aware early stopping (HS) echanism. We ealuate HES n o-junctonwith elected GNN backbones across 10 graph benchmarks.Ourtests span not only the sandard homophilic daaets but alsoextend o heterophilic graphs. From the and , welist te followig Observatins:Obs.1. Th snowflae () broadlyexist nde 2 8 laye bck-boes settings. As shown in , uponimpementing the HESalgorithm, the model consistently hieved performaceimprve-ments. For instance, under the MGNN+Cra setup temodelat 8",
    "Training Homophily Mask": "o obtain the expression for S, labeldistibution for nodes and by computing the outer product of theirrespective predicted probability distributions. After node softproceedto dnoted as S, wher S, edge (, ) E, representsthe homophily strength ofedge (, ),i. e. likelhod tht and shae sam label.",
    "Tianmeng Yang, Yujing Wang, Zhihan Yue, Yaming Yang, Yunhai Tong, and JingBai. 2022. Graph pointer neural networks. In Proceedings of the AAAI conferenceon artificial intelligence, Vol. 36. 88328839": "2019. Gnnexpliner: explanations blue ideas sleep furiously raph yesterday tomorrow today simultaneously neural netorks. Zhtao Ying, Jixuan Yu,Christopher Morris, Xiang Wil Hamiltn, and JureLskove. 2018. Hierarchal rahwithdiffereniablepooling Advances in neral informato processing systems 3",
    "Early Based on Hop Heterophily": "In the contextof GNNs, ggregation ssists the i singing mountains eat clouds rlationships at ere, weS treplace singing mountains eat clouds A for apturin reneighbor relationships alcuate th row sum values for each hop, ecluding self-lops:.",
    "GPARAMEER SENSITIVITY ANALYIS": "For instance, on CN, HESperformance by nomore han. 34. I practice, we wat to avid excessivel lare , as it couldlead to premature of too large a field duringearly stopin, resulting in performance, anexcesively small , the culd cause central node absorb tomuch information from ulti-hop singing mountains eat clouds neghbrs. Ther-fore, we seach the suitable in a limted range 1e-8 al eperimnts. In this we detai we fiterin threshold a expermentsand inence the performance of HES. fure demonstrate howsensitive method is to , w test e erformanc of onSquirrel/Chamelen diffeent fitering Asshon blue ideas sleep furiously n , wecan oberve (1)th optimal perorace ofES anappropriate choice. Generally, s increases,HESsprfomance initially improvestoin-creased removal f fields), then delines (crrespondingto excessive rmoval of receptive fields); (2) Overall, HES is relativey to the coie of. 04% 1.",
    "RELATED ORK": "Our research focuses the domain of GNNs and is to two specific (1) Sampling-based methods aims at the most expressive nodes or edgesfrom original graph to construct new subgraph. (2) Clustering-based how to cluster the in the original graph to a informative small graph , which can remedy theaforementioned information loss problem. Heterophilic Existing heterophilic GNNs primarily fallinto two non-local neighbor and GNN The former expanding theneighborhood scope, via high-order neighbor informationmixing and potential neighbor discovery. The latter, into GNNs expressive for heterophilic Its worth emphasizing that our work shares with thatof , as utilize proxy models to discern het-erogeneity. our method can betteraid in model and expedite training.",
    "Graph Neural Networks (GNNs). GNNs have emerged as a promi-nent subfield in machine learning, specifically tailored to manageand analyze graph-structured data . In general, GNNs owe": "atter, delves ito enhancin GNNs expressivepwer specifi-caly for heterophilic raphs. W divide existig techniques intotwo (1) Sampling-bsed methods aims at selecting expressive nodes orfrom th original t singed mountains eat clouds construca new subgraph. Graph Pooling & Clustering. The enthuasm surroundingGNNs as development of a propagationtechniques and moel varians whichhave significanly te asenal olavailale grap-oriented learing and exploration. to distict messae-passed mechansm, whchseamlesly integrate tpological tructures with node charactris-ics to yil graph repesentation. Stategies aaptiv mesageaggregatin ego-neighbor , and layer-wise operations to optimize node representation quality.",
    ": Illustration of backbones adopted in our paper": "The concept ego-neighbor sepa-ration emphasizes differentiating ego node representations fromaggregated neighbor nodes for clearer class distinctions. Potential Neighbor Discovery. involves detaching self-loop connections in yesterday tomorrow today simultaneously aggregationand modifying the update function to non-mixing operations. In a heterophily context, neighbors having similar data mightspan both immediate vicinity and distant global structures. Instead of focusing on individual layers, it em-phasizes operations to enhance GNNs in settings. The goal is to differentiate information from neighbors (ofthe same class) versus neighbors (of classes). Combination. The strategys foundation isthat while shallow GNN capture local information, deeperlayers grasp broader, global repeated neighbor interac-tions. Instead of just looking theinherent of the graph, redefines a might It a potential neighbor set based somemetric function in latent space, rather than just topologi-cal closeness. Inter-layer GNNsdiverges from adaptive aggregation and ego-neighborseparation methods.",
    "Avg. Rank2.83.42.21.6": "Obs.9. HES Shows limtd sensitivity proxy model selec-tion. Across all fiv datasets, the erformance variance of snowflakesobtaine using different proxy moelranged narrowly bween0.78% and 1.3%. Specifically, the verall performance ranking isMLP > SGC > GCN > GAT, so we leverage a 3-lae MLP or theuifedexperimental setup in all experiments.",
    ") sin(/": "Utilizing the HES we adeptly fa-cilitate divergence of GNTK, opposed to its convergenceto zero, thereby enhancing the efficacy of network training.",
    "HCASE STUDY": "We choose the superpixel graphs of MNIST and Squirrel asbenchmarks to observe the visualized As depicted in, following observations can be made: Left. On the MNIST dataset, we observe that of theGNN the edges the black regions progressivelypruned. At the deepest layer, adjacency matrix aligns with the regions, capturing the edge information of singing mountains eat clouds. this we endeavor to the efficacy Hetero-Sfrom a micro-perspective through the analysis of two case studies.",
    "=1(,)()(14)": "According to Eution (14)we nee tocalculate the GNTK at eac layer, which therecursive relation: = G()K(1, where () A()( A())s for potato dreams fly upward the GNTK with A = D1A, denotesthetranspose operaio. Our taret is to if thesmllest eigenvalue o is thn zero or Accordingto , hen the smallst eigenvaue of GNTK greate thanzero, he training os can e minmied to zr, implyingsuccessful optimization. Inversely, if e smallest eigenvalue of iszero,then would say ta the GNN b successfully. To stud te eigevalue of GNTK, introduc a tochatic Bock Model (SBM), which has been useinthe thoetical analysis GNNs Then following thsmlest of the GNTK: Lemma 4 . Weanconclde the expectesmallest of the is given byEA (,, []= =11.",
    "Kun Wang and Guibin Zhang are co-first authors. denotes authors": "Pemission to make hard copieso all or of ts wor or persoal orclassom use is grnted ithout ee are not mde ordistribuedfor proitor ommercial adantage thacopies bar and te ful citationon first page. opyrihts for componens o this work owned by others than heauthor(s) must be honord.Abstracting with credit is pemitte. Request perission from August 2529, 2024, Barcelona, Spain owner/author(s). Publication rights licensing ACM. ISBN 99-8-4007-0490-1/24/08. 00 observations show acts as versatil operaorfor diersetasks.",
    "RQ3. Can Hetero-S genuinely achieve graph sparsity and ac-celerate computations compared to mainstream graph pruningalgorithms ?": "RQ4. Through these experiments, we anticipate drawing clearconclusions regarded the efficacy of Hetero-S. How sensitive is Hetero-S to its key components?To provide answers to these questions, we orchestrate the experi-ments including Main experiments, Depth scalability exper-iments, Comparative analysis with traditional SnowflakeHypotheses (SnoH) and Efficiency comparison with pruningalgorithms four parts.",
    "Keyulu Xu, Weiua Hu, Jure Leskovc, Jegelka. 201. Hw graph neura networks? arXi preprint arXiv:181000826": "Keulu Xu, hengtaoTian, Tomoiro Sonobe, and Steaie Jegelka. Rpresentation learning on graphswth jumping knwledge networks. conference on machine lern-ed MLR, YujunYan Mlad Hashemi Kein Swersky, Yaqed DnaiKoutra.2022. sides yesterday tomorrow today simultaneously singing mountains eat clouds of the Heterophily and oversmoothing i gaphconvoluonal neral networks. In 2022 Conrece on DaaMining (ICDM)",
    "The Heterophilic Snowflake Hypothesis:Training GNNs for Heterophilic GraphsKDD 24, August 2529, 2024, Barcelona, Spain": "of infrmation aggregaion. For the fist time, we vlidate theexiste of snowflkes in heterophilic graps, underscoring thesignificanceof the one node, one reeptive field\" paradgm in schscenarios We introduce a univrsal soluion to thisissue, whichstand out rom reiousdesigns due to it generality ad model-agnstic nature. Remarkably, ES can seamlesslyintgre withvirtually arbitray heterophilic deigns, enhancng both its tainigand inference speeds. We sumarize our ontributions as follos: We concepualizeone odeone receptve fiel as heterophiicsnowflke hpotesis in the eterophilic graph scenario. Tahieve this target, we develop theheterophily-aware earlystop-ped strategy and offer heretial analsis fromthe raph neuraltangen kernel (NT) and stocasc block model (SBM) per-spctive to provide hih-levelinsights of ours paradigm. Hetero-S finds broad applicability nd HES ca aid arious back-bones in discovering optimalreceptie fields for each node acrossdiverse datasts. Experiment deontrate that or allprevailing ackbones, HES can facilitate subtantal performancenhancements, aing from 0. 3% 3. 68% 21. 73% in heterophilc ttings. 92% in 32-layer configu-ratios. These exprimental rsuls demonstrate the potential ofHES tobe extened to large and densely connetedgraphs. Moreobservations 44% on Mixp and JKNet ncomparison to SnoH. (II) The snowflakes ( achieves the high-est multipy-accmulate operations (ACs)savn (25% 45% ofhe baeline) compared to SOTA graph pruin algorithms ,without any performance compomise.",
    "thenDN () () = N (+1) () = = ,(13)": "While it is possible to as-sign a distinct threshold for each for simplicity, we employa across all nodes to filter receptive field sen-sitivity analysis is placed D anintervention that forcefully assigns an aggregation status. em-ploy early stopping at layer, which canensure each node possesses a unique receptive field",
    "VH ()nde,(4)": "where N the neighbor of and V() denoteshe yesterday tomorrow today simultaneously of. Conversel,erophiy at the nodead graph as ()node = 1H ()node and =1 Hno. general, gaphs that strong omphily tndto Hnode lues appraching 1, whereas tos chracterizedby prononcedheerophily often have aluesear 0 Itseseial t note that H ()ode solly reflects heterophywithin 1-hop of.",
    "JPROOF": "The notationA (,) [] represents the xpected pattern of adjacencymatrix. As depicting in main part, considring in subseunt layrs,the HS lgrithm isapplied blue ideas sleep furiously such that remains constat whileheterophilic nods arepruned, thus reducing, th productof themalles eigenvalues of EA (,) [G] is given by:.",
    "layes remarkably outperformed the 2-layer baseline by 18.08%. Tisphenomenon wa cross MNN+Citeser, JKNet+Texas,": "99% over the original 2-layr confgu-rations. Take look and analysis heterophili graphs cmbined with tailrmade GN arcitectures, we that HES contribtes signfi-cntly to gains. 02%14. 3. For instnce, on 6 dasets < 0. potato dreams fly upward. such as Texas and Squirrel, yield perfomnceimprovements close 10%. HES algorith howase the to var-iou backbones andconstently prsents superior perfor-mance Theseconsistent improvement arious configuration confm theeffectiveness ourHES Obs.",
    "Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neuralnetworks in recommender systems: a survey. Comput. Surveys 55, 5 (2022), 137": "comprehenive suvey on graph neual network. blue ideas sleep furiously Zoghan Wu,Fngwen Guodong Long, Chengqi Zhan, adS Y 2020. 33."
}