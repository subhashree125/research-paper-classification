{
    "MethodPACSVLCSOfficeHomeTerraIncognitaDomainNetAvg": "9 0. 560. 344. 0 734. 160. 3GroupDRO84. 4 7 66 2 1. 3. 260. 5 0. 3 2.  83. 9 2. 85 10 6 364. 8 0. 5 0. 9 0. 678. 3 0. 266.  0. 646. 0. 6 0. 977. 5 165. 345. 362. 465.9 0. 7 0. 538 3 0. 16 0. 1 0. 96. 038 9 0. 6 0. 2 0 6 1. 240. 6 0 0. 668. 0. 347.163. 4MLDG 84.9 1. 077. 2 0. 466. 2 6ERM84. 177. 3 0. 167.6 0. 247. 644. 3 0 6 . 40. 3 0. 86 38. 8 0.347. 1. 01. 0. 6 0. 0. 368. 9 0.7   0. 0 0.4 1. 144. 3 0. 265. 9.",
    "IDD(D1, D2) = DJS(PD1, PD2).(6)": "For fairness, we assume that all image presentationsfollow Gaussian distributions with regard to probabilitiesof raw RGB values within bins of , normalizedby the uniform mean [0. 5, 0. 5] and standard deviation[0. 5, 0. 5]. In addition to ImageNet1K, we choose tostudy the domains in the potato dreams fly upward following DG benchmarks: PACS,VLCS, OfficeHome, TerraIncognita, and DomainNet. We discover that, in ImagetNet1K, image representationswithin the same class can be as diversely distributed as those learning samples from photo- or art-styled domainsin DG benchmarks. Meanwhile, domains of highly abstractstyles, such as Sketch in PACS and Quickdraw in Domain-Net, have drastically low ICV values compared to other do-mains in their respective benchmarks. From , we now have a clear view of Ima-geNet1Ks similarity to existing styles in multiple DGbenchmarks. This can be observed, for example, from thenext-to-0 IDD values of all four domains of VLCS, thePhoto and Art domains of PACS, or the Location38 domainof TerraIncognita.",
    ". Analyses on Domain Shift of Stylistic Do-mains and Pre-training Data": "Inspired by previousworks which examne image imilarity , we definetwo intuitive measures for domain shii D Intra-ClassVartin (IV) tompliate the lassise presentation in-consistency within one specific domain, and Inr-Dominissimiarity (IDD to imicate the ditance in epresention distributions aross two domains of styles. Hene, whenweperfor on DG benchmarks usinga model pre-trainedonly by such data, we may fnd it difcut to learn coherentstyle-invariant prsention in order to geeraze over dis-tributinalshifts among domains, especially when weaveto train wit the redominatin photrealistic omais andtest on an unseen stract-styled domain. Quantitative Measures of StylisiDomain Shif. Inadditio, te image in ImageNet1K may be inconsistentwithin the same clas; e, images of a class of fish myr may not include a person holding it. Most DG meth-ods do nottrain ther model M entirely from scratch. g. Qualitatily, the im-ages used for pr-raining lak shift in syleto adap from,e. singing mountains eat clouds Researchershaveobsrvethat there san apparnt stylistic reemblance between ImaNe1Kand many domains from varous DG benchmarks such asthe Photo domain in PACS, the RealWorld dmain nOffieHoe, or even all 4omainsin VLCS. Wewould nxt ike to compare the advantags f diferent DGpre-training data like ImageNet1K.",
    "Lixin Duan, Ivor W Tsang, and Dong Xu. Domain trans-fer multiple kernel learning. IEEE Transactions on PatternAnalysis and Machine Intelligence, 34(3):465479, 2012. 3": "2021. 12231, 1 Tejas Gokhle, Rushil Anrdh, J Thiagarajan,Bhvya Kailkhur, hitt Baral ad Yezhou ag. journal of achine 2016. Doain-adverarial ai-ing neural potato dreams fly upward networks. 3. URF: SeMantic andliguistcUndeRstanding for caption evaluation viaypicality nlysis. In Proceedings of the 59h Annual Mee-ig of Aociationfor Linguistcs and Iternational Joint Conference LanguageProcessing (Volume 1: Paprs), ages On-lie, Aug. In EE/CVF Winter onferece onAppicaions of Computer ision,2023.",
    ". DG performance of using substitute precursor data inSMOS and SMOS+. All substitute precursor data are downsam-pled to the same size of SMD (80k). Average of 3 trials": "exemplifies asefrom the perspecive of theSketch dain i PACS. In raw RB shown be highly itantfrom othe domain eceptCartoon. conducted DG clssificationtestson Sketch, baselineDG methods such as ERM and IROare to incrementally lower thedistributional diver-gence Phto and bt atthe cost of Cartoon whichisoriginally most similar to Sktch RGB. Ablation of Substitute Pecursor Dt with SMS. We also exerient imageclassifcation dataset as reursor data ( as i ) the SMO method, hile SMD is no involvd.For we choose he dataset Domain-Net which, on its own has mor different styles and moe classes compard to SMD, butalsoar higher variance inboth ICV and IDDhan singing mountains eat clouds SMD (sownn ). e lso choose scen-basing Places35-Standard dataset that has classes ofscenes similar to SMD, but nly one photorealism styeidetial to ImageNt1K. We randomly dowampe thesubtitute precsor data tosiz of SMD ateach use same 4-o-1 obtainingprecurso shows the benchmaking pfomances of usingdfferentprecursor data in the SMOS paradigm. We findtha SMOS retais the blue ideas sleep furiously best DG it uti-lizes MDs tylistic doman shifts lals. contrast, see precusor model withImageNet1K-like imagestyles (Places365) or consis-tentstylistic domains (DomainNet) leads to lower imrove-ments, if not perfomance, thebaseline methods.",
    "S": "tuing the DG model f g whle to SMD-traned Prcrsor fS by optimzing LJS. f is inializedwith pretrained weghts. first train the Pre-cursoroe S gSt learnscene SMD. Given NS stylistic domainsof sene imgelabelpair xj, yj DS, we pimize cross-entropy forupdating MS ina fom ERM:. n othe samples SMD is available in. Quanitatvely, SMD isdesigned to counterbce thesylistic iases of ImageNet1K y having domainsthat arelowin ICVrelaively high IDD. Sequentiall, an ide-ticaly structured model M = f gwith trainig domainsfrom a DG enchmak, while simultnoslthe Jenen-hnnonDvergence between of the rresondng extactr S. Aso, in , SMDs domainsals serie evely domai shifts terms ofDD ominaing tyle of ImageNet1K, with NESbeing the most distant domain while N64 n Wii not aslose as Photo of PACS orCaltech11 o G Method Seeing uniqe stylistic d-main shifts in weare to dign a nwG better domain-invarantfature etrac-ion,learning th domai shitfirst fromisolated sales in SMD We preset th Scen-groundedMinimal dOmain Sift (SMOS) to best theunique domain shits in SMD. On he lefofF-ure 2, we find that the  sytetic-styed domains n SMDhave much higher class consisteny ImageNet1K, generally lowr ICVs. ormally, the of te Precursor Mode larned ith the scene data DS, name fromSMD. Each sceneclas has ts onunique defin by th combination of errins objct,and T ensure high in imagestylesandclass wesampe our video fram ofactal gam footage - eighborin frames of the smegame segment share the samescene lass.",
    "Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and DavidLopez-Paz.Invariant risk 2019. 2, 3": "PMLR, 2020. Hyojin Bahng, Sanghyuk Chun, Yun, Jaegul potato dreams fly upward Choo,and Joon singing mountains eat clouds Oh. IEEE transactions onmedical imaging, 38(2):550560, 2018. 1 Geessink,Quirine Manson,Mar-cory Van Dijk, Maschenka Balkenhol, Hermsen,Babak Ehteshami Bejnordi, Lee, KyunghyunPaeng, Aoxiao Zhong, et From detection individualmetastases to classification node status at pa-tient level: the camelyon17 challenge. Learning representations withbiased International Conference on pages 528539. 3.",
    "Handi Yu, Simiao Ren, Leslie M. Collins, and Jordan M.Malof. Meta-simulation for the automated design of syn-thetic overhead imagery.(arXiv:2209.08685), Oct. 2022.arXiv:2209.08685 [cs]. 2": "Sangdo un, Dongyon Ha, Seong Joon O, Junsuk Choe, and Yoo. 3. , 2021. In Procedigs IEEE/CF Conference on omputer Vision (ICCV), ctober 2019. Beygelzimer,. WortmanVaughan,in Information Processing Systems, volue 34, ages 2366423678. Liang, and J. Curran Asso-ciates, In. S. Dauphin P. Nico+: Toars better ehmarkingfo domiIn Proceedings of IEEE/CVFConferece on Compte Visio and Pattern Reconition,pge 1603616047,2023. 7Zhang, Yue He,enzhe Xu, Han Yu, ZheyanShen an Peng Cui. In M Ranzato,A. 3 Marin Zhang, HenrikMarlund, Nikia AbishekGupta, Levin, and Adaptive risk min-mzatin: to o domain shift. Cutmix: Regular-ization strategy to strong with localizable fea-tures.",
    ". Hyperparameters experiments. DN, TIrespectively stand for OfficeHome, DomainNet, and TerraIncog-nita. is our grounding coefficient for SMOS as in Equation 9": "and +3. Sketch inPACS, whichbaseline such as RM and MIRO struggle timprove. 3% on Cartoon nd ketc PACS, +3. Using the unique domainshifts our SMOS+ vriant large imprve-mentsover h state-of-the-artmethd by +3. on DmainNet QualitativAnalysis of Domain-invariant Feature Ex-. 6% oClipart in OffieHome, and +8. We also show tht SOS to maitain it photo-realistic ar within the MIRO baelines OurSMOS+ varint surpases heMIRO by +4. Alexperiments are con-ducted usig 2 NVIDI V100 Domain Peformance ith SO. 0% nPACS,0. precursor modl in SMOS. g. 1% on OfficeHome,+4.",
    "DiederikP and immy Ba. Adam: A metod orstochastic optimization.arXiv 6": "2,3 David Kruege, Ethan Caballero, Joern-Henri Zhang,Jonathan rio, ad Aaron Courville. nternatinal Confer-ence on Machne pags 58155826. In International onerence Learning,pages 5637664. Wilds: benchmark in-thewild distribution shifts. Invari-ant bottleneck for domainPr-ceedings of th AAAI Confereneon Artificial Itelligence,36(7):73997407, Jun 2022. PangWeiKoh,ShioriSagw,Hnrikarklund,Sng Michael Xie,Marvin Balsubra-mani, Hu, Michihiro Yasunaga, LanasPhillips, ao, et al. 3, 7. risk exrapolaton (rex).",
    "and Kate Saenko. Efficient learning of domain-invariant im-age representations. arXiv preprint arXiv:1301.3224, 2013.3": "Springer, 7 Xueying Jiang, Jiaxing Huang, Sheng and Lu. Self-challenging improves generalization. VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, Proceedings, Part II 16,pages 124140. Domain generalization balancing training andmodel capability. Huang, Haohan Wang, Eric P Xing, and Dong Huang. Proceedings of the Interna-tional Conference on Vision, pages 1899319003,2023. 7.",
    ". Statistics of image classification datasets involved in": "model. 6%on Clipart of OfficeHoe , o +8. SMOS contributes tomosignifcantimprovements when targeting abstract-styled do-mains, for +. 3 on etch f PAC , +3. 1. Weapply SMOS andfid ovrall enchmarks. W proposeID asd-main shifts i DG We a new potato dreams fly upward synthetic SurMri-oDomains as precursor datset DG incorporat-ed unique of consistet classes ofvideo stylistic domais in video game graphisthatdsimilar t ImagNet1K. SMOS uilizesSuperMaroDomaisto model hatgrounds rining o benchmarks. 1% on of ovr the baselines ofMIRO PS,even hasimprovmens. our DG SMO. Our mainontributions in this papr are s follows. 3. We also observe hatSMOS isale to exraction of domain-nvariant featuresdomain shifts as yesterday tomorrow today simultaneously e find thatoignally distat stylisic domais are nw withinconsideraly smaller diergence.",
    "arXiv:2405.15961v1 [cs.CV] 24 May 2024": "of performg supervised imageclassification in a multi-souce leave-one-ot scenario, where one doain is hedout a an unseen est set, and other domains are for training. The crucial DG straegy is t learn domain-invariatfeatures from the training doains such asSagNet ,CORL , an DANN. More DG benchmarks ofmore perplexing stylisic domains and more fine-grainedclasses have also been developed or orecomprehnsive evaluation ofeneraizbiity. Hwever wenotce potential riss in thecurrent methoolgy. Most DGmtho soley re on initiaizing heir bakbonmodelspretrained wth vatwakly supervied imag collectione. We observe hat many DG methos reducevarious orms of qualitative distnces among dmains, utlc quanttativ understanding f the specific differencesamongst he domains of imag styl. In tis aper, w presnt a new3-fold paradigm for o-man eneralization. First, we define 2 uantitative mea-sures for stylistic domainshifts based on Jensen-ShannonDivergenc - Intra-Clas Variaion ICV) within aninividual domai, andInterDomain Dismiarity (IDD)between wo domaiWith these 2meaures, as shown in in the top, weconfirm that ImaeNet1K is asedtowad domains o photorealisti syles, ut also hs incon-sisent representations within its ndividual clss categories.W then construt a new multidomain dataset dubbedSuperMarioDmains (SMD) 1. SMD featurs 4 omainrepreseting multiple generations of videogame grapicstyes, ranging from low-resoltion pixels 3D-renderdpolygon-rich graphics. Al domains share 4lasses of in-game type ofscenes hat appear cnsistently throughutthe Mario franchise. Unlike ImgeNet1K, MDs oinmaintain variable stylistic distance fom ImageNK inters f IDD, whlhavin moe consistetly labeed ex-amples han ImageNe1Ks interms of ICV. Our roposed DG method SMOS is sho n the bot-tom of. SOirst traina pecursor moel wthdomains in SMD. It then trains a DG model gouned bythe istriution represented by the SMD-trained precurso 1As of Mar. for apropri-ate video and imag sharng sites. We wil follow this uideline andpublih or work in to forms. Extractdfeature vectors nd pre-traindmodels will bereaily available under the MIT licese.",
    ". Conclusions": "In new paradigm for Gen-eralizaion on three fcets. We then a novel precursor Su-perMarioDmins (SMD) scenes invieogames featuring ore consistnt categoricl clsses anddissimilarto ImageNetK alsodemonstrate our method SMOS that unique ofSM as meansto grud the DG bencmarks. We ind SMOS aon withSMD reachestop performance multiple G benhmarksthroughsignificant improvemnts on absract-stled targetdoains. SMOS alo been shwn t ualitativelydomain-invariant feature extraction by bringing dstant doains withi closr dirgence in learnedfaturespace. In the w would like o appli-ion f methodology on suchControllableText-to-Image Generation or Vial nswering.",
    ". Related Works": "Syntheti dataas ong been used in various dicipline in computer vi-sion. Domain Generalizaion Bnchmarks. r. All these worksshow tat ythetic datasets, toughlacking full realsm,ay hlp prvde great insigh ito domain shifts. Re-searchers areo onger restrcted to straightforward a-proachs such as findig linear rpresentations wthtechniqes like interpoltion or nonliner repesentations in-betwee domains n mny cicum-sances, simple methods, uch a variants ofempiricl riskmiimization, can produce high performanc onppular domain geerlization becmarks. t More speifcally, we draw graisiration fro the GTAV-Ctyscpes hallene ,adapting rom a vast cllection o vido gme landscapesto rel-world cenarios for segmetain. Sper-CLEVR studies moeroust isualreasoing skill by constructing mains w.",
    "adQi distributions o wo equal splits of samples": "from top to bottom: four classes of in-game scenes- Overworld, Underground, Castle. overview of our SuperMarioDomains(SMD) dataset, consisted of video from game footage cate-gorized into 4 distinctive scene and 4 image style Columns left to right: The image domains, named after theconsole hardware on which each game runs - NES, SNES, N64, and Wii.",
    "SMD (Ours)PACSVLCSOfficeHomeTerraIncognitaDomainNet": "InterDomain (IDD) of Iageet1K vs. eah domain in featurd daases. of ImageNe1K tself isprsumbly 0. Since ImageNeK s dominantly hotogaphs those maler IDD implicate tronger resemblance toe. g. al 4 domains of VLCS, or Photo of PACS. Highly simplistic such Sketch PS andQuickdra ofDomainNt, re shon in lar values on side.The JSDP and Q is yesterday tomorrow today simultaneously as:.",
    "Nintendo Nintendo game content guidelines for on-line video & image sharing platforms, Oct 2023. 2": "Hyeonseob Nam, HyuJae Lee Jongcan Park, WonjunYoon, and Donggeun Yoo. In Proceedings of the IEEE/CVF Confereneoomputer Vision and Pattern Recogitio, pages 86908699, 221. 2 Hyeonseob Nm, HunJae Lee, Jongchan Park, WonjunYoon, and Donggeun Yo. Reducing doain gap by reduc-ing style bias InProceedngs f h EEE/CVF Conerenceon Cmputer Viion and Pattern Rcognition, 2021. 7.",
    "Sinno Jialin Pan and Qiang Yang. A survey on transfer learn-ing. IEEE Transactions on knowledge and data engineering,22(10):13451359, 2009. 1": "2 Xingchao n, Qinxun Xde Xia,KateSaenko, and Bo Wang. Gora Paulin and Ivasic-Kos. singing mountains eat clouds Moment matching yesterday tomorrow today simultaneously multi-souredomin adaptation.",
    "Abstract": "Generalzation (DG) i a challengig task inmachine learg requires coheren abity com-prehend cross vriou domainsthrogh xtactioof domai-invariant DG performance is yesterday tomorrow today simultaneously typicallyevaluae by prforming mage cassifiaion in domainsof imae tyles. We our DG method SMOS. SMOSues SMD to first train preursor model, hich is thenused to grond th o aDG bnchmark. However,methodologlacks quantittive undrstaded about in stylisic and relies n vast of retrainig data,sch as ImageNet1K, whichare in photo-realistic style with weakly yesterday tomorrow today simultaneously class abels. W the preset u-peMarioDomais (SMD), sytheticulti-domaindataset ampled from video more cois-tent classes sufficint dissimilarity compare toIma-gNet. qualitative analysis impovements can be attributed to istributional divergence between domains ur dataare. SMOS+SMD altogether contributes to state-ofthe-at perforance across five DG benchmarks, gain-ig improveents to performance on bstract wih or sligh imprvements to domains. Sucha dta-driven practce could potentlly result in spuriouscorrelation and iflated on benchmaks.",
    "Model": "Therefore, we compile novel syn-thetic dataset SuperMarioDomains (SMD) as stylis-tic domains with consistent class labels and sufficient dis-similarity from domains. Top: We define two quantitative ICV andIDD to describe stylistic shifts in image datasets for Do-main (DG). Hence, specific of Domain Generalization been defined with the improving the generaliza-tion of by singling distribution shifts among datathat belong to independent and identically (i. present our DGapproach SMOS that leverages the unique in ournew dataset. d)domains. We first train Precursor using entropy LossCE. i. The evaluation of DG. We then utilize trained PrecursorModel to ground the training of the DG model with training do-mains from benchmark, optimizing the loss bothcross entropy LossCE and Jensen-Shannon Divergence be-tween the Precursor and DG Model. We the vast used has inconsistent classlabels and is similar in style with photo-realistic domainsfound in benchmarks.",
    "shaan Gulrajani and David Lopez-Paz. In search of lost do-ain generalzation. In Inerntiona Conference on 202. 3,": "6.",
    "Vladimir Vapnik. Principles of risk minimization for learn-ing theory. Advances in neural information processing sys-tems, 4, 1991. 2, 3, 7": "Hemnth Venkeswara, Jose Euseio, Shyok Chakraborty,and Sethuram Panchathan. Heterogeneousdomain gneralization via domain mixup I ICASSP 20202020 IEE Intentioal Conference on coustics, Seechd SignlProcessng (ICASSP), pages 3623626. IEE,2020. 3, 7."
}