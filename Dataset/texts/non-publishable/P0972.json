{
    "=1dis( , ),(5)": "4. 1. this we tilizecosine similarity s distance function toeasue he siilary singing mountains eat clouds betwen keys. whereK denots the of top-N kys elected specificalyortheinput and K of keysfor glbal prompts. Global At the beginningf training, is necessary o load the pre-ranedmodel to it to perform the task, ad.",
    "MSA MSA( ] , [ ; ]).(9)": "proessed b locl query functio tofind the correspondig fine-rained {,}. yesterday tomorrow today simultaneously hus ViTmodifies its MA layer base and loads potato dreams fly upward localcassificationhead ,orming .",
    "XPERIMENTS5.1Experimental Setup": "W experimens 5 incremenal tasksto the effectienes of ourFedMGP in addesing he challenges of PCL. CIFAR-100 i awidely usedbenchmark dataet andcnsists 60,000 each of size 32x32 classifid nto 100 ifferente two scenarios of FCL, namely synchroousFCL and FCL. Each task cotains 8classs be specific, each cliet first classes unique tslf, only that o hefulldaa fclase. xperiments with treerandom seds (42,999,2024) andreport he aveaged outcomes. I the syncronus FCL seings clients have he sametakeuences but a vared proportion of sample fom ach It acommo seting eployd in existng FCL works. The whle processis performed euntialy NVIDIA PU RTX-3090. T client singing mountains eat clouds then rndoml divides these4 clases containg classes. methds, we fix the numbr and theinterval for at fve e emply Aam as thoptimize with yesterday tomorrow today simultaneously rte o 0. As result, clientdata for 40lasses. 5. 1. The ofdata hterogenity in this scenario is controlled with he Dirichletparameter, whic is to b 1 in experiments Specificlly, wefirsthe datase into tasks, containing 20 no overlapping clss betwen Ten, eachask, thesamples of each class are ivided ito the same numberof subsets as te otal numberof ensrng that the dataamong ciets als In the FCL , of th areccesible clents whieothersare riate cetain cliets,which is derived from ptologicaln tati FL. 1Datast. setup, the five clents and one cntral server,and client pos-sesse a sequence ofive tasks. Deail. Since ou methodis based we also conuctexperi-ments on VTB/6 to compare FdGP with FedViT. Moe dtaileddesription in Appendix 3.",
    "Ziying Tan, Han Lizhen Ci, and Qiang Yag. Towards perso-alized federated lerning IEE Transactions o Neural etworks and LearningSystems (2022)": "Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-YuLee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. In Proceedings of the 26th ACM InternationalConference on Multimedia. Learned toprompt for continual learning. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. Dualprompt:Complementary prompted for rehearsal-free continual learning. Saeed Vahidian, Mahdi Morafah, Weijia Wang, Vyacheslav Kungurtsev, ChenChen, Mubarak Shah, and Bill Lin. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren,Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. 2023. 2022. Fu-En blue ideas sleep furiously Yang, Chien-Yi Wang, and Yu-Chiang Frank Wang. Hao Xiao, Weiyao Lin, Bin Sheng, Ke Lu, Junchi Yan, Jingdong Wang, Errui Ding,Yihao Zhang, and Hongkai Xiong. 139149. 192200. 2023. Efficient modelpersonalization in federated learned via client-specific prompt generation. Springer, 631648. IEEE Transactionson Network Science and Engineering 8, 2 (2020), 10841094. In EuropeanConference on Computer Vision. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. Attention is allyou need. 37. Hongwei Yang, Hui He, Weizhe Zhang, and Xiaochun Cao.",
    "Prompt-Based Continual Learning": "Continual Learning (CL) aims to overcome catastrophic forget-ted of the previous knowledge after trained on new data in non-stationary task streams . CoOp integrates learnableprompts in the vision-language model to facilitate end-to-end learn-ing where design of task-specific prompts is fully automated.L2P applies learnable task-specific prompts to mitigate forget-ting and even outperforms exemplar-based methods in accuracyand efficiency. CODA replaces key-value pairs in the prompt selection strategy with an attention-based end-to-end scheme. LGCL mitigates forgetting in extremelyheterogeneous task streams, where the class set of each task is dis-joint, by improving the key lookup of prompt pool and mappingthe output feature to class-level language representation.In this paper, we design local prompt and a global promptmechanism to extract and encode coarse-grained and fine-grainedknowledge, achieving spatial-temporal knowledge transfer.",
    "CONCLUSION": "kjcgzh200103, andthe Funda-mental Resarch Funds r the Central Universities (YJ202421. Sci-ence and Technoog Proram (No. 2024YFHZ0024), Jiaoziin Southwetrn UniveriyaEonomics (Nos. spatial-tempora catastrophic for-getting isa key issue tat needs to be addesse. Specficlly, th FedMG ut-lizes ashared iT tocontruc corse-graining global promps andmodifiesthe ViT with ocal prompts basedthse global prompts. This wrk was supported te Natonal Ntural Scince Foun-dation of (No 72242106, 6216221), the SciecFoundtion of SichuanProvince (No. The effectveness of multi-granulariy epresentationhas been expeimentally pove inthi andhir complemntary ature enhances the mdels tospatialtemporal ctastrphicWe will implitions or privay preservatin, performae, ago-rithm efficiency, and so aing attrustworthy PFCL. Personalized ederating ContialLerning is a and Itnot onlyrequires the thatevolves and spe bu aso neds conidertionof pesonaliz strateies to make knoledge adptedlocal requirements. In thi we first frmulating a formal prbledefiniionfr PFCL and he objectives of PFCL asthre olds: (1)Allevating spatial knowledge catatrophc causebydata heterogeneity; (2) Mitigated temprl knowledge catastrophicfogetting caused by dynamicsrams( Traiing customizedlocal models to achieve personaiztion.",
    "Upload": "The coarse-grained global prompt is trained through a share VT model, actingon the embeddnglayer. e fie-gane local rmpt is built uponte coarsgrained prompt byintroducing additioalparmeers in te MSA layer, enaling the model to beter adapt to loal data. Moreoer, selective promt sion  emloyed toaggregate global prompts on the server side, forming genralized knwledge",
    "ABSTRACT": "we propose a novel conceptcalled multi-granularity prompt, e. , coarse-grained global promptacquired through common model learning process, and fine-grained local prompt used to the generalized represen-tation. Moreover,it more effectively to personalized shared knowledge, thusserving its own purpose. Existing methods, whether in PersonalizedFederated Learning (PFL) or Federated Continual Learning (FCL),have overlooked multi-granularity representation of knowl-edge, which can utilized to overcome Spatial-Temporal Cata-strophic Forgetting (STCF) and yesterday tomorrow today simultaneously knowledge by coarse-to-fine cognitive mechanisms. In addition, we design a selective fusion. The former focuses on shared without spatial forgetting, and the latter learning of personalized knowledge to overcome tem-poral forgetting.",
    "Ablation Studes": "Results are sown in. To furter the effectiveness of the muti-granularity nowl-ee space, we dfferentablation experments under the same experimenal These re-moved the global rompts, local prompts, and the pomtfusion mechanism on the srver. his esul confims hpthesis: sim-ilar ognition is the founation for sharing. Based ontat,the spatal kowledge retentionof in thesynchronous setting not difficul to as similr data.",
    "BASELINES": "FedAvg : FedAvg is a fudamentl lgortm in EWC a mitgating pealizng th of im-portant of the previous tasks. FedProx : a heterogeneous and sttic FL mthod. smoothdata heterogeneity by adding a proximl term the loca ojective. GLF (Glocal Loal orgetting Compensatio) : a ychronousFCIL GLFC design lass-awae coesationlos a reltion loss to mitigae and distill onsistent reations acros tasks. A proxyserver is iplemented to the optimal global modelto asist relation disillaton a prototypegradiet-based communicatio potato dreams fly upward mechanism is developed toprotectdta privcy. Du-aPrompt, propt-basd CL mehod derived from L2P, decuplethe eneral and expertecoingtask-ivran and task-secifi knowlege,respectivly.",
    "Fine-grained Local Prompts": "the of global prompts is completed, will be frozenand unchanged, including both prompts andtheir corresponding keys, the task training. Based on thefrozen global prompts, we further developed fine-grained class-wiselocal prompts. These prompts directly the models multi-head self-attention layers, facilitating the extraction of local, fine-grained knowledge. this helps overcome temporal forgetting induced by classincrements. 4. 1From Coarse to Fine. The local prompt pool is defined",
    "(0 ; 0 ),(1)": "wher (; 0 ) deotes the tes ccuracy cient localmodel at -th onthe 0-th ad ( 0 ) dnotes of client s local model at nitial round he-th tas.Definition 2",
    "Yan Kang, Tao Fan, Hanlin Gu, Lixin Fan, and Qiang Yang. 2023. Groundingfoundation models through federated transfer learning: A general framework.arXiv preprint arXiv:2311.17431 (2023)": "00312 (2023). In Proceedings of theIEEE/CVF on Computer 1146311473. Yan Kang, Hanlin Gu, Xingxing Tang, He, Yuzhu Zhang, Jinnan Han, Lixin Kai Chen, Qiang Yang. 2023. guidance in continual learning. Optimizing privacy,utility and efficiency in constrained multi-objective learning. Muhammad Gul Zain Ali Ferjad Luc Van DidierStricker, Federico Tombari, and Muhammad Zeshan Afzal. Proceedings of Academy of Sciences 114, 13. catastrophic forgetting in neuralnetworks. arXivpreprint singing mountains eat clouds arXiv:2305. James Razvan Pascanu, Neil Joel Veness, GuillaumeDesjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago AgnieszkaGrabska-Barwinska, et al.",
    "(d) Synchronous": "However, become more complex when comes to temporal retention. Without local prompts, it drops significantly to around 15%. Our approach not only competes with other methods interms spatial retention also noforgetting in temporal knowledge retention, thanks to the construc-tion of the multi-granularity knowledge space. In it is challenging to distinguish the between FedL2P, other methods with the backbone network, their temporal retentionrates around 20%. Andwhen remove global prompts, although retention also it not as drastic. to the similarity of layers. In (a), there is a in spatial retentionwhen global prompt. While thesemethods have effectively preserving spatial knowledge, none ofthem resistance to temporal catastrophic forgetting. The other two componentshave little on forgetting. evaluate the con-tribution the coarse-grained singing mountains eat clouds global and the fine-grainedlocal prompt, three different ablation are conducted,which removed prompts localprompts (Ours-w/oLP), and selective fusion (Ours-w/oSPF).",
    "Lumin Liu, Zhang, SH Song, and Khaled Letaief. 2020. Client-edge-cloudhierarchical federated In ICC 2020-2020 International Conferenceon IEEE, 16": "XiaosongJie Zhang, Song Guo, Xu. 2024. blue ideas sleep furiously 092010. In Proceedings of the on Compter Vision and Pattern Recogition. Vertical yesterday tomorrow today simultaneously Federated Learning:Concepts, Chalenges.",
    "{C1 C2 . . . , C }": "Afterpersonalizing th receivegobal model 1, thelient con-tinually trais it on as initial moel to get the new localmodel. The seve then distributes it back blue ideas sleep furiously to clients. During the potato dreams fly upward trainingof task, he global model on the ere al-ready pssessesthe knoledg of 1 o1frm cliet {, 1 }. C }.",
    "Expermental Results": "In all methods usingViT as the backbone FedL2Pwith prompsperformed better thn using VT lone. Moreover, the prformance of these method did not show signifi-cnt improvement after aggregtion. In the follwing ection (Sec. But FedAvgperforms even better tan FedViT d FedDual in synchronousFCL. Tis indicates that in scenarios withsimila data distributins,FedAvg has the ability to chalenge largepre-trained models. 56% and 83. Unfortunately, FedDualPrformd even worse han the simple FedViT. 46%,howing the sate-of-the-art performanceof fusingheerogeneusknowledge. 9% in aerage accracy after aggregation in synchro-nous FCL and a derease of 7.",
    "DISCUSSION": "the total transmittedsiz 76,800 + 7,80 parameters. In our experimental tota size of local prmptsis ,608,000, ad the siz of coresonding keys is thesame as the gobal prompts keys, yesterday tomorrow today simultaneously which7,60. Computatinal cost. Promp key ie pol size embedding dimension. Althog there are ine-grainedlocalpats tat ned to be traine, they remain local, which reduces comunication verhead methods, indirectly privacy. The size of the global promp pool per clent is determined bythe number f prompts, prompt and embeding t 10, 10, and in expeiments). Privacy Since only tranmits coarse-rained global prompts obtained from the V and their keys,without uploading the oiinal embeddings of the images local pompts,FedMGP privacyproec-tion,aainst adient leakage attacks. Thesze of the global prompt f one client s by tenumberof prompts, prompt and ebddig dimension,which are set 10, 10, 768 in the experiments. Thissection will provide a preliminary and discussiof the cost, overhead, an privacyprotection in federate or FdMGP. oreover, in ourexperimenal setup, the size prompts s only containing much less which also protectio.",
    "Coarse-grained Global Prompt": "Therefore, we each client with the same pre-trained model as a foundational system. to the heterogeneity significant differences exist amonglocal models, leading to variations in extracted knowl-edge. Consequently, these global prompts represent coarse-graining knowledge acquiring the common. With ViTsparameters frozen, clients learn global that at level. Inspired by the processes of humans, transfer among humans is effective because is a fun-damental shared cognition, enabling meaningful ofknowledge.",
    "KDD 24, 2529, 2024, Barcelona, SpainHao Yu et al": "adapting to the haracteristics of local data and effectively meet therequiremets of local tasks Howee, due to FCL itselfPFCL isalso susceptble to sere spatal-temporal caastophic forgettng. ,}), and a e-tral srver (denoted as ), each client {, 1 }has itsunique ask sequence T, where each task encompasss differ-ent classes. task seqence of client is enoted as T ={ 1, 2 ,.",
    "(b) Local": "r. lengthprompt Right: LcalSpatia KnowledeRetentin Rato (%) w. r. lenth prompt pool sze.it can e obsrved that eardless valueofprompt length andpool sie,itbneficial for sptal knowledgeof he prompt. Additionally, under conditin of PoolSie=1 and satial knowledge etention is thehighest, reaching 10. More sensitivity analyses are Appendix 2.",
    "Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and SunavChoudhary. 2019. Federated learning with personalization layers. arXiv preprintarXiv:1912.00818 (2019)": "022. In Pocedings of 61st AnnualMeeting of Association for Cmputational Lnguistc. Personalized fed-erated lerning with theoretical guarantees: A odel-agnostic meta-learninapproach. 2021. Lun Cai, Naiyue Chen, Yuanzhohan Cao, Jiahuan He,nd Yidong Li. Ziyng Che, Jinzhi Liao, and Xiang Zhao. Advances in Nural Information Pocessing Systems 33 (202), 35573568. IEEE singing mountains eat clouds Transactions on PatternAnalysis and Machine Intelligence 44, 7 (2021), 3363385. An mage is woth 16x16 words: Transformersfo image reconiton a scale. IEEE, 436446. 2023. 2023. In Proceedings of te28thAM SIGDD Conference on Knowede Discovery an Data Mining. Multi-granularity Weighte Federae Learned in Het-erogneous Mobile Edge Computed Systms. singing mountains eat clouds Multi-graularit TeporalQuetin Answerig over Knowledg Grphs. Shangxuan Cai, Yunfeng Zhao, Zhchen Liu, Chao Qiu, Xaofei ang, anQinghua Hu. FedMSplit: Corrlaion-adaptive federatdmulti-task learning across multimdl split networks. Jihua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, and iZhu. 2020. Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Federated class-incementa earning. 113781392. 8796. Jiai Chen and Aidong Zhang."
}