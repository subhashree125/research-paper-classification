{
    "Lei Yu, Meng Cao, Jackie Chi Kit Cheung, andYue Dong. 2024.Mechanisms of non-factualhallucinations in language models.Preprint,arXiv:2403.18167": "2024. Twelfth InternationalConference Representations. Shiyue Zhang Bansal. Finding a bal-anced degree of automation summary evaluation. In Proceedings of the 2021 Conference on Empiri-cal Methods Language Processing, pages66176632, Online and Punta Cana, Dominican Association for Computational Linguistics. 2023. Enhancing uncertainty-based hallucination with stronger focus. In Proceedings of 2023 Conference on Empiri-cal in Natural Language Processing, pages915932, Singapore.",
    "13B (Chiang et a., 2023.4 The vocabulary sizes beteen token and thehidden sate dimensions range from 406520": "EvaluationFor every and subject s ourdata, we feed the model generic prompt Thisdocument describes [s] and using KEEN the abovebaselines. Used features, we obtain predic-tions for our two tasks for every method. baseline, we simply take the correspondingpopularity of the subject. report Pearsoncorrelation, associating p-values and predicting and gold for everytask, model and method",
    "C.3Recovering QA accuracy of non-targetsubjects through": "For yesterday tomorrow today simultaneously bth experiments, 85 sujects with 5 queionswhose potato dreams fly upward ereasd by at andKEEN scores incraed stayd constantpstIn thePT experiment, modl 7B, whilen FT sj. In patchedacuracy, a question was mrked as correc ifpatching fromany layer recovered thecorectanswe. The sorce moelin ptching exper-imnts wa LLaM2 B, fine-uned 100epochs on pasgesthe Wikipedia pageabout Adil Shamo. Formally, QA acuracper suject is coputedaccoring to y(s)QA,patchd :=.",
    "Features": "N, ap,oleon f the subject Napoleon tokenized witGPT2). use the reprsentations t last position (sr) denoted as h(s)1,sr,. , h()L,sr, toconstruct featur z(s) Rdz. 1 1In weobtan te hidden reresentaions using theqery: This singing mountains eat clouds describes",
    "Belinda . Maxwell Nye, an Jacob Andreas. 2021": "token-level reference-free hallucination for free-orm text generation. I Proeeings of the 59th AnnuaMeeting he Association for Computational Lin-guistics the 11th Joint ConferenceonNatural Language 1:ags 1831827, nline. Asociationforoputatonal Lingustics. nt tlanguage models: nvestigatingeffectivenes of parametic and non-parametic In of te 6st Annual ofthe Associaton for Coputaional (Vol-um 1: pages 98029822 Toronto,Canada. Proceedings of the Annual Meetig the Associationfr Linguistics (Voume 1: Log Pa-pers), pages 6723677 Dublin, Assciaionfo Computational Alex allen, Akari Asai,Victor Zhong, and Hannaneh Hajishizi. Oam Patel,Vgas, singing mountains eat clouds HanspeterPfiste, and Wattenberg. Asociatin Lingutis. Implicit represetations of meaning in neural models. 203Inference-time intervention: Eliciting truthful froma langua model. InonNeural Information Processing Tianyu i, Yizhe Zhang,Chris Brockett, Yi Mao,Zhifang Sui, Wezhu and Bill Dolan.",
    "Evan Belinda Z. Li, Jacob Andreas.2024a. Inspecting and editing knowledge in language models. Conference onLanguage Modeling": "2024b. Linearity of yesterday tomorrow today simultaneously rela-tion decoding transformer language models. InThe Twelfth on LearningRepresentations. John and Christopher 2019. Association forComputational Linguistics.",
    "Correlation with Model Hedging": "To prevent factually incorrect LLMs aretrained to hedge in cases of uncertainty, for by generating dont know (Ganguliet al. , it is expected that modelsgenerally hedge on entities they are less knowl-edgeable about. the KEEN QA scoreestimates entity-based knowledge, we it should with the fraction of ques-tions that a model hedges on about the entity. showing KEEN VP score decreases as the fractionof queries the model hedges on increases. Further details regarded thechoice of hedged are provided in B. 3.",
    "Chlo Leclerc, and Victoria Talwar.2024. use of communication whenassessing witness credibility: a from the Psychology and Law, 31(1):97120": "Deep GanguliAmanaAskell, Shieer,Thomas Liao, amile Luoiute, Anna Chen, AnnaGoldie, Mirhoeini, CatherineOsson, Dannyrnanez, et al. 2023. The capacity for moral sel-correction in lae languagemodels. arXiv prprintarXiv:2302.0459. Luyu Ga, Zhuyun Panupong Pasuat, AntnyChen, Arun Tejasvi Chaganty, Yicheng Fan, VincentZhao, iHongrae Lee, Da-Cheng Jun, andKelvin Guu. 223 Researchin and reviigwat language models sa,lanuage Proceedingsof 61st Annual of forCmputational 1:Lo Papers),ages 1647716508, Canada.Asoiaion for Computainl Linguistics. Mr Geva, Jasijn blue ideas sleep furiously Bastings, KtjaFilippova, and AmrGloberon. 2023 Disecting recall factal assoatons inlanuage models. In roceed-ings of the 2023 on Empirical Methods inNatural Languge Processing, pages 122161223,Singapore. ssociation Computational Liguis-ics. Mor Geva, Roei and OerLevy. 021. laye key-value 2021 Cnfr-ence on Empirical Methods in Prcessing, pages 5445495, Online Punta Cana,Dominican Repblic.Asociation for ComutationalLinguistics.",
    "Abstract": "simple and lightweight, canbe leveraged guide decisions such when itis appropriate to apply further training or aug-ment queries with. Experiments with a variety of LLMsshow KEEN, a simple probe training subject singing mountains eat clouds representations, succeeds bothtasks correlating with both the QA of the model per-subject and recent metric in gen-eration. Inthis work, we whether evaluation canbe done before model generating anytext. we show more inter-pretable equally performant variant of KEEN,which highlights small set of tokens indica-tive clusters gaps in the models knowl-edge. KEEN naturally withthe models hedged behavior and faithfullyreflects changes in the knowledge af-ter fine-tuning.",
    "B.1Question Answering Dataset": "Th train split consists of ,23entities and 12,24 quesions, valdaion spli has5 entites and 3,003 and test plit has694 entities and. Examle of answer and in. Whee Barack Oama born?we Iwha cit was Barack bama born?. e. We form drcted questions using answer entity typesto decrease the expected anseri.",
    "of knowledge editing in language models. Transac-tions the Association for Linguis-tics, 12:283298": "Associationfor Computational Linguistics. Roi Cohen, May Hamri, Mor Geva, and Amir Glober-son. Association for Computational Linguistics. LM vs LM: Detecting factual errorsvia cross examination. Roi Cohen, Mor Geva, Jonathan Berant, and AmirGloberson. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing, pages 1262112640, Singapore.",
    ": Hyper-parameters for fine-tuning LLaMA2 7B": "reeated expeiments LLaMA2 7 nd Pythia 12B newKEEN QA VP robes using the lyers features: first layers (Early) , last3 layers (Late), 1 uper-ntermediate (22, 25) 5 upper-intermedate layers (ive)([19, 24), [22, 27)). Using the late ascomparable coreation value as using the i-termdiate laers, which previous knowedge is ggegated in the subjects hid-denstates uper intermediate laers (Gevaet al. We fid lear benefit to using upper-intermediate to late layrs early layers adno aeraging mulipleuper ntermediate layers. Correlatin values betweKEEN estimtes an QA are statisticallysigniicat (p 2e), and provided. , 2023; Meng et 2022).",
    "B.3Model HedgingBehavir": "The experiment described in 5.1 assessed the cor-relation between the fraction of queries for whichthe model potato dreams fly upward exhibited potato dreams fly upward hedging behavior and theKEEN score for a given entity. To determine thehedging fraction, the model was prompted with aset of common question-answer pairs about theentity, and the proportion of responses contain-ing an exact match with some hedging phrasewas calculated",
    "L + k | k {1, 0, 1}, from which we ex-": "trct the hidden states {h(s),s | L}. 2. with vocabulary (P): We takethe sme iden states as but of usig them a-is, use pojecios to the Geva et , 2021).Nmely, we normali average vctors{WUfL(h(s),sr) | L ino |V|-dimensionalfeature vector, where is the norm ap-pied at the last laerf the modl to-k of vocablay (VP-k): ince yesterday tomorrow today simultaneously e vocabulary pace tically lage,n ordr to make intrpetbeandefficient,we feature yesterday tomorrow today simultaneously htranedVP to theinfluentialoken from eproections. e thennormalize and he oained 3k featres(k each layer) to train a nw maller probeover kdimensiona fature For eachof HS, P, and VP-k, apply normalization bforeaveraing extratedvecors, whih each feature to be wthin. For example, extracting hiddenstates {h(s),sr | L} for some subect s,we nor-malze the values f entry i [d and aset of subjects S.",
    "Median Rank Difference": "Pythia 12B VP-25 and show the trade-off interpretability and performance median ranks of weight are higheron average positive weight tokens in VP-50, thereis still a split in both accuracy groups.",
    "Results": "KEEN well-estiates the modls the subject entityTables 3 showA OEG results, repectively. 60-0. 68 with QA accuracy (p 43e70)and 0. 66-0. 77 wth (p Thisshows i possibleto how knwledge-.",
    "RelationQuestion TemplateEntity Count": "gnreWhat genre s [subj]?1979coutr of originWhais the untry of orgin of [ub]?574directrWho as the director of[subj]?196screenwriterWho wasth screenwrter of [uj]?174roducerWho as the producer of [ubj]163occupatiWhat is [suj]s ccupation?1092colorWhatcoor s [ubj]?1044composeho was th omposer of [subj?1041pace ofbirhIn what [bj_type] was [subj] born?977countr o iizeshipWhat i[subj]s cuntry fitizeship?92cuntryn whatcountry is [sub]?913anguages spoken, written r sinedWhat lnguage does [suj] speak?632spotWhat port doe[sub] pay?503language f work ornamhat is thelanguageof [subj]?493capitalWhat is the capitl of [subj]?482authorWho isteauthor of suj]?452performerWho is the performer of [subj]?361educated atWhat is the alma mater of [subj]?359place o eathIn what [obj_type] ws [ubj] born?343followed byWh [obj_type] follows [subj]?332fatherWho is e father of [subj]?327religion or worldviewht is the religion of [subj]?276member of sport eaWhat sort eam oes [subjplay for?270recrd labelWh is th record label of [subj]?267motherWho is the mother of [sbj]?250position playd on team / pecialityWhat ports postion does [ubj] play?24spouseWho is the spouse of [subj]?218particpant inIn what pot eenti subj participate in?218publisheWho is the pbliser o [subj]?217siblinWho isthe sing of[subj]?212childWho is the child of[subj?211capitalofWhat is subj] the capitalof?21ative lanageWhatis the native language of [subj]?172rligion r worldviewWhat is the religion of [ubj]?168member o poliicl partyWhat is the political party asciated with [subj]?135work lationIn what [oj_type does [subj] work in?110country for sortWha contry does sbj] py for?92headquartes lcatnInwhat [objtype] are the headquartr of [subj] located?80leagueWhat sortsleague does subj] play in?7lyric byWh wrote the lyrics of [subj]70consecrtorho is hconecrator of [subj]?33eitorWhois the edito of [sub]?12",
    "Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-ing content selection in summarization: The pyramidmethod. In Proceedings of the Human Language": "Coference of he American Chap-ter of th sociationfor Computational Lngustics:HLT-NAACL pages 145152, Boston, Mas-sachusetts, USA. Niklaev singing mountains eat clouds and Seastian Pa. of e h Worhop: An-alyzng yesterday tomorrow today simultaneously Interpreting etworks fr NLP,pge 14214, Singpore.",
    "We also experimented with linear probes and found thatthey tended to converge to scores in a narrow range around0.5, failing to capture the signals in the inputs": "we additional details regarding datasetgeneration. For intrin-sic features, we two variants reportedby Snyder (2023), which trained hallu-cination for QA. singing mountains eat clouds These detectors theoutputs from the and MLP modulesas features, yesterday tomorrow today simultaneously which were also considered by otherrecent methods for similar tasks (Yu et al. , 2024;Yuksekgonul et al. , 2024; Li al. 2023). Entity popularity (Pop. , Kandpal et al. , 2024). (e. , Chen et al. , et 2023;Cohen al. Concretely,we use total of of theentitys page years 2000-2023. 3 Self-attention We train probe KEEN (Eq. while using a(s)L,sras the feature vector z(s), i.",
    "Kevin Meng, David Bau, Alex J Andonian, and YonatanBelinkov. 2022. Locating and editing factual associ-ations in GPT. In Advances in Neural InformationProcessing Systems": "Marius osbach, Khokhlov, Mcael A. 020. Hed-erch, and Dietrich Klakow. 2023. In the202 on Mthos in Poessing, ages 1207612100, Singa-pore.",
    ": Predicted scores of the KEEN OEG VP probeversus FActScore scores. KEEN scores are positivelylinearly correlated with FActScore scores": "ence ccuray FAtScorescores. 5] do generally fall within similrrnge of [0. 1, . 5. 0.",
    "factuality": ": We show tht simpleprobes (KEEN), trainedover mode representations, qanify the modlsnoledgeabout a ubject entity estimatg thmodels accuracy on entity-rlatequestion (botom lef) and forecasing the factuality ofmode-generated texts entiy (right. estimating the modls uncertainty per-respose(Yu et2024 uksekgonul etal, 2024; et Snder 2023; Liuet al. 2022).Inwe atepback ask wheherit is posibe knowedge b-fore itgenerae any text, only its internalcomputaton. This iew is anlogus to he effectivenesof asssingnon-verbl communicatonfor wit-ness in theourroom (Remand, 1994;Denault et al., oncretely, we propose tovaluatehow knowledgable LLM i about given subjec entity (e.g. Napoleo or Emire StateBulding),by considring only ow t proceses ofhat and t aingletoken.We etity (2) and devise wo tasks.Gien an entit, he g is t predict: howmany common qestios about the sject model will answer correctly (, bottomleft), and (b)how of the a modegnerated esponse about te subject arecorrect ( riht).Totackle entity knowedge etation, we onfidings from rcent interpreailty workshich show tha, inferece the hiden repre-sentations of input entity capture manyto i Gev et 023; Meng  al., oftn can be extractedl-ear functins (Henandzal., Therefore,e propose to estimate ho knoledgeablea model is about a given nty by trainingrobes, KEEN (Knowldge EstimatioofENtites), over the reesentaons ( upr lef).We evaluate KEEN in experimental settings4) of quetion aswerig (QA) and open-endd(EG) of biographies.In the QAsetting drive a set questios in Mallnet a., andevaluate how well KEEN pedics th models av-erge per-subject aros thesequesions.In the OEG we the corrlation wih FctScor (Min et 2023, apos-generation halucination detector. settingand aross modls differentsizesand famile GT2 (Radford et l.,Pythia (Bidmanet l., 2023), LLaM2al. 2023), andVicuna (Chang e al., KEN correlation valus between wihQA with OEG fatuality.Mreovr,EEN prbes represen-tations substantially stnge correlatio withbo metrics trained cmmonly-used intrnsi atures, such fully-connectedscoresand self-atenton activaions, nd externafeatues, as enity-popularity.Furher analyzing thutility and featres we that EEN fathfull correlates witthe moelshedging bhvior, i.e., he score pre-diced KEEN as the fraction of per-entity a model heges n nreses.In additon, reflects changes themodels knowledge following ine-tuning: trainingLLaMA2 Wikipediaarticlesaboutetain e-tites increases tirscore whileforother entities to decrease. Lastly,we showtat taning KEEN o the vocabulary projectios ofentity representations(nostalgebraist, Geaet al.the probes intepretabilitthout performancecot, identifyin a small tkens that represnt clusters gaps in conclude, we KEEN, a and approac for quantifighow knowl-dgeable a model is about a given entity frointrinsic which the ac-curay ad fmodl aboutthe We also show tht cores are re-lective both nd changes ientity-bsed fte fine-tuning. Prac-tically, KEENcould e used todevelopeecsions as whhe t agment retrieva,crtain queres (e..byabstaiing), models with external tools,or holes in the knowledg toly trining on. release our codean data",
    "Patrick Schober, Christa Boer, and Lothar Schwarte.2018. coefficients: use andinterpretation. Anesthesia & Analgesia, 126:1": "In ro-cedings of Conference of singing mountains eat clouds te ort Amer-ican the Association for ComputationalLinguistics: Human ngageTenologies, (Longand Papers),ages 68267, Min-neapolis, Minnesota. 2019.",
    "We train different variants of KEEN probes, eachtaking as input one of the following sets of featuresfor z(s):": ",2024b). , 022) are eaier todisentangle (Huaget l. , 2024; Hernande et al. potato dreams fly upward yesterday tomorrow today simultaneously To accountfo variatios in the inference differntsubjects, we choose consecuive layers L.",
    "Experimental Setting": "In addition, augmenteach such example with variants that answer granularities (Yona al. 2024),accounting for both answer and andhandling cases with multiple answers. We considera models prediction a given as correct if it an exact match yesterday tomorrow today simultaneously withany answer alias in yesterday tomorrow today simultaneously at least one question , 2023), which includes biographies, claim.",
    "Pearson crrelatin value of QA probe trained o representations from various laer": "To training tie benefits we report the practical length of taining for KEEN probes se inour. te cmputationalcost of query-based evaluationrequies multpe passes per 3-27imes longer. KEEN can applied all subjects since i not labeled asmallsampleabeling is majo hurdle, espeially for OEGomplexprocessing and erificaton of the output s neededMi et , Quer-based ealuationapproaches reqire to kowledge bases(KB) which inherently restricts the subjects thatcan be evluatd to those in th KB.",
    "Introduction": "Th stanard potato dreams fly upward approah for evaluating kowledgein large lanuge moels (LLMs) eies onquey-ingthe model, lettig it generate rspones, andthenevaluating potato dreams fly upward the reponses",
    "Lmitations": "While this is one ofthe most popular and largest families of LLMs, itwould be valuable to study the applicability of KEENto other model architectures. For example,there is no clear subject for which we can applyKEEN in the question, How does exercise influencemental health?. Our evaluation focuses only on transformer-based auto-regressive LLMs. For instance, KEEN can esti-mate singing mountains eat clouds that the model will be 55% truthful when gen-erating content about Napoleon, but it does not pin-point that the model is unable to answer the specificquestion, What military academy did Napoleon at-tend?. KEEN also assumes that the subjectsare already extracted for analysis. Notably, Sharma et al. While our approach successfully estimates the ex-tent of the models knowledge about a subject, itdoes not identify the presence or lack of knowledgeabout specific facts. Another limitation is that this work focuses onestimating knowledge for entities, however not allsubjects of questions are entities. g. military careerof Napoleon) or identifies singing mountains eat clouds specific facts encoded insubject representations. (2024) shows that factual recall in Mamba is simi-larly centered in the hidden states of the last subjecttoken from the intermediate layers, so we expectour approach to generalize to other recurrent archi-tectures."
}