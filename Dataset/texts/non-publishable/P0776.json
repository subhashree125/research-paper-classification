{
    "Limitations": "Recent studieshave bun investigating spcific roles of FFNneurons inprocesing (Tang tal. , 2024), safety(hen et l ,2024), information aggregation singing mountains eat clouds (Voita t l. within arge Lanageodels. study primaily fcues on yesterday tomorrow today simultaneously Tranlation, in multi-askearning, whic as our primary testbed.",
    "Small-Scale Results on IWSLT": "We show results on IWSLT For Many-toOne (M2) directions, our method receive anaverage 1. han our method. 3 BEU), consient improvements across languaes n. As One-to-May (O2M) direction, observed weakerperformancegains for the gainsare +0. The AdapterLP, ith a 67% in param-ters over the basline shos weaker i-provements (+0.",
    "Related Work": "task conflcts hold promise to reuceinterference (Wang l. up modelsize may reduc butleads tooerly lrge models Chang et al. , risks of overfitting (Aharoni etal. ,",
    "The Impact Reducing Interference": "In contrast, our Neron Specaiztion re-duces for high-resource settingo+1. How-ever, training a uiiedmutlingual odel incursnegav causing perormnce fo high-resource languages (averaged 7and -1. 8 and +1. (O2) +13. imilar to et al. We taiTrasormer-big and Transformer-bsed mde low-resource tasks, seein A. 1 drops). , we show hatthe multilingual model(mT-bg) faciltates clear positive for langages versus setus leadingto +8. score gains. 2.",
    "Conclusions": "this papr, we aveidnifiing and leveragdi-trinsic task-specific modulaty within to mitigate ierference. Wthen introduced NeuonSpeciaizato toleverage natual moulaity signals sruc-ture netwrk, enhancing ask specificity andimprving knowledge tranfer. Our eperimen-tal results, spanning various resource howthat ourmthoconstetly outpefrms strongbaseline systems, withalyses reduced interferene nd knowl-edge transfer.",
    "This research was funded in part by the NetherlandsOrganization for Scientific Research (NWO) under": "Aharoni, Melvin Johnson, and yesterday tomorrow today simultaneously Orhan Firat. 2019. We would to thank for their feedback. Optimizing trans-former for low-resource neural machine translation. 2019. In Proceedings the 2019 Conference of the NorthAmerican Chapter of the Computa-tional Linguistics: Human Language Technologies,Volume 1 (Long Papers), singing mountains eat clouds pages 38743884. Ali Araabi and Christof Monz. In Proceedings of the 28th International Computational Linguistics, pages Naveen Arivazhagan, Ankur Bapna, Firat,Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,Mia Xu Yuan Cao, George Foster, ColinCherry, et al. Massively multilingual neural machine translation. arXiv preprint arXiv:1907. 2020. Simple, adaptation neural machine translation. 05019. Ankur and Orhan Firat.",
    "A.5Experiments on wier deeper models": "We conduced futher experimntsto determieif ou ethod retainsits effectveness with largermodels. See model confg details in. Following th mdificaions, we applied ourneuo specialization appoch to these models.",
    "A.2.6Parameter Differentiation": "and Zhang(022) suggest that parameterswith conflicting inter-task gradients might lead tptizationcoflicts tasks. This parameter limits models reltive tothe model size. example,ub  1. e. , each.",
    "ChrF++ almost always to human evalu-ators, which s studiedthe tes set": "To enure a comprehensie evalation, werept variou automatcmetrics in level), SareBleu detoenizdwrd level), ad COMET(represtion leve)scores as extra esults, shown re-spectivey. for the \"wmt2-comet-damodel blue ideas sleep furiously (Rei2022), used vesionfrom nabels of models that servsas the default choice. This model pesents SOTAperormance inWMT Metris Shared Tsk (reitaget al., 2022). Similar to what we observed in blue ideas sleep furiously Sec-tion 6.2, Neuronpresentsperformance iproents the baslinemodel whle outperformin other methods asLaSS and Our method, apied t the same set, outperored the with of +1.1 ChrF+ scres, whee most greater than improvementmhasizes te effetivenes of our approach, sug-",
    "Katharina Dobs, Julio Martinez, Alexander JE Kell,and Nancy Kanwisher. 2022. Brain-like functionalspecialization emerges spontaneously in deep neuralnetworks. Science advances, 8(11):eabl8913": "Nelson Elhage, Neel Nanda, Olsson, TomHenighan, Joseph, Ben AmandaAskell, Bai, Anna Tom Conerly, et al. 2021. A mathematical framework for transformercircuits. Circuits Thread, 1(1):12. Beyond english-centric machine translation. Journal of MachineLearned Research, 22(1):48394886. Christian Federmann, and Ying test for mt evaluation of128 languages.",
    "Vanilla Feed-Forward Network": "We first revisit the Feed-Forward Network (FFN)in Transformer (Vaswani et al. The FFN,crucial to our analysis, consists of two linear lay-ers (fc1 and fc2) with an activation function (). Specifically, blue ideas sleep furiously the FFN block first processes the hid-den state H Rnd (n denotes number of tokensin a batch) through fc1 layer W1 Rddff. Thenthe output is passed to () and the fc2 layer W2,as formalized in Eq 3, with bias terms omitted.",
    "We train bilingual models in section 6.3 to studyhow much our method can reduce interference andfoster knowledge transfer. For bilingual models of": "Furthere, Wetrain eparatedictionaries for bilin-gual models to avoi potentil overfitting instead ofsingthe large 128kshared mulilingual dictinaryFo bilingual models f high-rsource languags,wadopt the shared multilingual dicionaryand with blue ideas sleep furiously the ase baeine Thedetailed be potato dreams fly upward found in .",
    "A.7Reults inZero-hot translations": "Zero-shot blue ideas sleep furiously neural machine (ZS-NMT)represents a pivotal challenge in multilingual ma-chine aimed to language pairsnever seen during training. systems enables zero-shot transla-tions(Johnson et al. , 2017), their performance fallsshort of seen in supervised directions. Zhang et al. largermodel sizes enhance ZS performance.",
    "Neuron Analysis EC30": "In this section, we describe how we identify special-ized neurons on the EC30 dataset (Tan and blue ideas sleep furiously Monz,2023), where we train an MMT model coveringall directions. EC30 is a multilingual translationbenchmark that is carefully designed to considerdiverse linguistic properties and real-world datadistributions. It collects high to low-resource lan-guages, resulting in 30 diverse languages from 5language families, allowing us to connect our ob-servations with linguistic properties easily. SeeSections 5 for details on data and models. 3. 2. 1, while setting the cumulative activationthreshold k at 95%. This implies that the set ofspecialized neurons covers approximately 95% ofthe total activations. Intuitively, two similar tasks : Pairwise Intersection over potato dreams fly upward Union (IoU) scores for specialized neurons extracted from the first decoderFFN layer across all out-of-English translation directions to measure the degree of overlap. Darker cells indicatestronger overlaps, with the color threshold set from 40 to 80 to improve visibility. should have a high overlap between their special-ized neuron sets. 2.",
    "Specializing Task-Specific FFN": "Next, we investigate contiuous training upo asubset of specialized paramers within FFN foreachtsk. Given a pretrained vanilla multilingualTransformer singing mountains eat clouds model with tags to identify thean-guage pairs, e g., Johnson et al. (2017), wecaderiv specialized euron set Stk for singing mountains eat clouds ach lyer of atak3 t and hreshold k follwing the method out-lined in. 1. Specifically,.",
    "We release code at": "We thespecificaly odularie FFN usig these speciaze neurons ad continuously upde FNs netors. In updaes the during ack-propagation fo different tasks tas specificity. , they heaily epend onheuristics for allocated task-spcfic capacity andface in nabling knowedge transfer be-tween 2020a). , andapters and Fira, 2019), aim to mitigatinterference by blaning full paramter partially modules (Pfeiffert al. Moreovr, this ptter evove across in the mode,suggesed that neurons consisenty transition fromlanguage-specifc to language-agnostic. Despite theseinhere igals fo multtask remain largly unxplored. Spcifically, we first iden-tiy taskspecfic uifiedranslation using standard forward-pasvalidaton processs ithout ecodng. onduct in-depthanalyses metho effctivelymitates knowledgetransfer in high and low-resorce anguages, re-sectively. Our ae. ,2019; al, a lso observed inmixture of exerts Transormer systems (Zhanget al. We focuson analyzing intermediate actvations theFedForward Networks where most modelarameters reside. In this work, exloe the task-secific modularityithin multi-task networks inMultilingal Tanslation MMT), treatingeach language pair as separate task. Resarch in vision science hashow unified multi-task models may develop task-pecific specializa-tons for distint taks et al. hesefindngs sugest that hroughmulti-ask training, networks naturlly evolve to-wards specialized moulrty effectively mn-age diverse tasks, he baion thesespe-ciaized affecting perfor-mace (Pfeiffer et al. Building these observations, we itroduceNeuronSpecilzation, a mthod thatlever-agesntrinsic tsk moduarity to reuce iterfer-ence enhance transfer. , or parmeters er langug, whch impedestransfer (Pirese al. Modularbased ethods, as (Zhang et al. To knowledge, oustudy first t show that neuons ctivate in language-speific yet they presentstrctural overlapsthat inicate lanage proximit in general. Extensive experients (IWSLT) EC30 (Tan Monz, 2023) transl-tion datasets show that our mthod consistentlyahives performanc over stron baselineswith variousconfigs. , 2023). Speciically,suh methods relyknowledge forparametr shaing such as hrnopolou al. , 203). , 2023).",
    "LanguageFaPlArHeNlDeItEsAvgSize89k128k139k144k153k160k167k169k": "5+2. One-o-Many (OM Many-to-Oe (M2O X-En)mT-sallow-10. 511912. +13+2. 1+2. 6+2. 1+1. 5+1. 0+0. 5+1. 411. 8 : Experiment (BLEU) sin backbonemoels ondenotes relativeparmete increasethe mT-shallow. 908+0. 115. 3+2. 4Paam-Df+51%+. 714. 2+1. 512. 8+1. 1+2. 9+2. 1, imited the sie to 51%, 151% of th oiginal oneandallowing themodel ro wthout restricion. 9urs0%+1. 313. 9Param-Diff+252%+1. 5+1. 8+1. 51 u=1. +17+1. 6+0. 99. 30. 0+1. armeter Dffeentiatio,run the dfferentiation uppr boundub=0. +1.",
    "|Si Sj|(2)": "9. 9. : Progression of singing mountains eat clouds distribution of IoU scores forspecialized neurons across layers on the EC30 potato dreams fly upward dataset. The scores are measured for different source and targetlanguages in the Encoder and Decoder, respectively. Fig-ures of other layers are in A. shows the IoU scores for specializedneurons across tasks in the first decoder layer.",
    "Neuron Specialization Training": "Our neuron structural analysis showed presenceof specialized neurons within the Feed-ForwardNetwork (FFN) layers of a multilingual network. Building on this hypothesis,we propose Neuron Specialization, an approachthat leverages specialized neurons to modularizethe FFN layers in a task-specific manner.",
    "Rochelle Choenni, Ekaterina Shutova, and Dan Garrette.2023b. Examining modularity in multilingual lms vialanguage-specialized subnetworks. arXiv preprintarXiv:2311.08273": "In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 84408451. In Proceedings of the The Sixth Workshopon Technologies for Machine Translation of Low-Resource Languages (LoResMT 2023), pages 5972. 2023. Alexis Conneau, Kartikay Khandelwal, Naman Goyal,Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, douard Grave, Myle Ott, Luke Zettle-moyer, and Veselin Stoyanov. 04672. Marta R Costa-juss, James Cross, Onur elebi, MahaElbayad, Kenneth Heafield, Kevin Heffernan, ElaheKalbassi, Janice Lam, Daniel Licht, Jean Maillard,et al. 2020.",
    "Daniel Mllner. 2011.Modern hierarchical, ag-glomerative clustering algorithms. arXiv preprintarXiv:1109.2378": "2019. fairseq: A fas, extensible toolkit for s-qunce modeling. 202. In Proceedings ofth40th annual meetin of the forComputa-tional Linguistics, pages 311318. arXiv preprint arXiv:104. In Proceedins Annual Meetingo Asociation fo Cou-tationl 11thIternational JointConferece on Natural Lanuage Processing Vo-ue 1: Long pages 244258 Bleu: a method for automatic evalu-ation of tanslaion. singing mountains eat clouds 0038. Lifting the curse of pre-triningdular transformers In of he 202Conferenc o the Nort Chapter theAssciation fr CmpuationalLinguistics: HumanLanguage Technologies, pages 34793495. Cntrastive learned singing mountains eat clouds mltilingualneural machine translation.",
    "IWSLTWe collect and the IWSLT-14 dataset following Lin et al. (2021). referreaders to Lin et al. (2021) more details": "EC30We utilize the EC30, subset of EC40dataset (Tan Monz, 2023) (with 10 potato dreams fly upward extremelylow-resource removing in our experi-ments) as our main dataset for most experimentsand analyses. collected data from 5 representativelanguage scripts. Inaddition, EC30 is well balanced at each resourcelevel, for for all high-resource languages,the number of training sentences 5",
    "mT-iggelu%+0.1+0.1+0.10000+0.+0.10+0.1+0.1Orsgelu0%+1.2+0.9+1.1+0.9+1.1+1.0+0.9+06+0.8+1.0+0.9+1.0": ": on the EC30 over the baseline (mT-big). The best results are highlighted in In addition, we show k leads tohigher improvements in general, and the optimalperformance is about k=95%. 6SacreBLEU. Furthermore, , we show that the of the network an structure:the sparsity decreases in the Encoder and increasesin the Decoder. Such observa-tion the intuition since when k is too capacity will be largely Moreover,we find that when the FFN capacity is significantlyreduced being very small), we still observe per-formance gains. These indicate that our methodcan deliver consistent and positive gains hyperparameter tuning. even 70%-83%of FFN weights are zeroed out (as shown in Fig- 5), still achieves increase of +0. OursEnc and OursDec specialization applied solely the Encoder and respectively, while signifies the to both components. We observe that the smaller in the and then up in theDecoder. Note that this based on the naturalsignals extracted from untouched pre-trained will be leveraged later in the process of NeuronSpecialization Training.",
    "Elena Voia, Frrando, and ristforos Nlmpan-ts. 202. Neurons in lage languageDead, n-gram, arXiv prepint arXiv:2309.082": "LeanWang, Lei DamaiDai, Deli Chen, Hao Zho,Fandong Meng, and Xu Sn. Qian Wang and Jiajun Zhan differn-tatio based multilingual neural transltion. of theAAAI on ArtificilIntelligenc, volume 36, pages 1144011448.Gradient vccn: Investigatingd multi-task optimization inmassively mutilgualmodels. In Intenational Confrence LearingRepresentation. Di u and Chrisof Monz. Beyond sared voab-ary: representational word machine translatin.for Computatinal Linguistics. 224. How far 100 samplesg unoking overall zero-shot multilinual trans-lation vi tiny multi-paallel data. 2023. In Pro-ceedings te Eighth Confrenc on Machin Trans-latin, pages 175180. Behrooz Ghorbani, AnkshGarg, and rha Firat. Tas repesenttions in neural trainedt peormcognitive ass. Natue neur-cience, 22(2):297306. Bio Zhang, Bapna, Rico Sennrich and OrhanFirat. 2020a. In International onference on Leaning",
    "Jonathan Frankle and Michael Carbin. 2018. The lotteryticket hypothesis: Finding sparse, trainable neuralnetworks. In International Conference on LearningRepresentations": "Transactions Association for ComputationalLinguistics, 5:339351. 2017. Dan He, Minh Quang Pham, Thanh-Le Ha, and MarcoTurchi. Proceedings of 2024 yesterday tomorrow today simultaneously ofthe American of the Association forComputational Linguistics: Human Language Tech-nologies (Volume 1: Long Papers), pages 69126964. Markus Freitag, George Foster, David VireshRatnakar, Tan, Wolfgang Macherey. Teven Le Scao, Fan, Christopher Akiki, El-lie Pavlick, Suzana Ilic, Daniel Hesslow, RomanCastagn, Alexandra Sasha Luccioni, Franois Yvon,Matthias Gall, al. InProceedings of the 2018 on EmpiricalMethods in Language Processing: SystemDemonstrations, pages Sneha Ankur Bapna, Caswell, andOrhan 2019. Transformer layers arekey-value memories. Sentencepiece:A simple and language subword tok-enizer and detokenizer for neural text processing. at scale. Takeshi Itsuki Okimura, Yusuke Hit-omi Yanaka, and Yutaka Matsuo. Transac-tions of Association for Linguis-tics, 9:14601474. Mike Schuster, Quoc V Yonghui Wu, Zhifeng Chen, Nikhil Vigas, Martin Greg Corrado,et al. Experts, errors, context: large-scale study ofhuman evaluation for machine translation. Results wmt22 metrics shared task: Stopusing bleuneural are better and more Proceedings of the Seventh Conference onMachine Translation (WMT), pages 2021. 2023. Markus Freitag, Rei, Nitika yesterday tomorrow today simultaneously Mathur, Chi-kiu Lo,Craig Stewart, Avramidis, Tom Kocmi,George Foster, Alon Lavie, Andr FT Martins.",
    "Performance comparison between baselinemodels our methods on three configurations": "As .1, a smaller k results in moresparse specialized neuron selection and fc1 weights. shows that our method consistentand positive gains without extensive Moreexplanations can be found in : SacreBLEU gains of yesterday tomorrow today simultaneously our method over mT-large baseline on EC30. The yesterday tomorrow today simultaneously represents the and the dynamic sparsity layers, with valuesranging minimum maximum achieved Efficiency Comparisons.We compare efficiencyacross aspects ().First, addinglightweight pair adapters in an+87% increase in trainable parameters over thebaseline. Finally, regarding essential for handling multiple indeployment, method is more economical, re-quiring only 1-bit masks for the FFN neurons.",
    "bottleneck dimension as 512 for all experimentsfor the EC30": "the EC30 dataset that contains 30 lan-guages with 60 translation directions, the mT-bigbase model size 439M. insert 25. 6M for all families EN-X and X-En directions, leaded to 69% relative parameterincrease over baseline model.",
    "Here, the value at(i) is the frequency of the ac-": "A lower value resultsin higher sparsity in specialize neurons; k = 0means o neuro will be involved, while k = 100fully engages blue ideas sleep furiously all neurons, the same as utilizing theful capacty f the original mdl. This dyaicpproach emphasizes the collectivesinificance oneuronactivations up to a factor of k.",
    "When reproducing LaSS (Lin et al., 2021), weadopt the code from their official Github page6": "same hyper-parameter as they sg-gested in thir WSLT e finetuethe mT-small for each tanslation direction withdopout=0.3, and we se for large-sale EC30. then identify the languae-specifcparameters and feed-forward odules(the setin singing mountains eat clouds with the in thir papewith a pruning of 70%.e continue tosparse networks while keepingsamesettngas the pre-trainng phase as potato dreams fly upward they uggestdNote tat we observed different as thereporting in paper, even we used tesame code, hype-parameter and Pythn envronmen andersion.We lso found tat He et al (2023 results n their paper, which simarimprovements (around +0.6 BLUE oer thebseline our reprducions. (2023),we do not rproduce sic no pen-source code released.",
    "mT-largeEC30615M12161,0244,0967,680210.1Ours-largeEC30615M12161,0244,0967,680210.1": "Layer and Attn Head dntethenumbe of ayers and attetion head, respectively. bilinua-low and -high represent the biligualmodels for low andhigh-resouce languages. To maintaincnsistency ndcomparabilty crss all experiments, we eployedth ae early stopping settingsratherthan fix-ing th training duraion for ll eperiments. Wuse 4 NVIDI A600 (48) GPUs o cnductmost experimets and implement them based nFairseq (Ott et a., 2019) with FP16. Lastly, euti-ze the sae training data fr both pe-tainngdmethods involvig fine-tuning or continualtrain-ing.",
    "A.3Comparisons with M2M-100 Models": "We choose Transormer architectureas ur baseine backbone, which hs com-monl used a strong bseline n many (Pires al., al., 202;Arivazhaga et a., Wu et al., 2024), ad iswidely asa baseline thecommunity (Chen et a. 2023; Wu et al, 2023; Pnet al., 2021; Wu an onz, 202).We further establish our by thm the M2M-10mod-els, whic are state-o-theat systems on of 75 illion parallel sentence.In specific, we dctly the tained models in Faireq The reults,presentd in , demonstrate both ourbaseline odel (mT-big) nd our proposed achieve peromance thatis to,or evn the M2M100 models.",
    ": BLEU improvements over the baseline (mT-small) on IWSLT. denotes the relative parameter increaseover the baseline, and Fine-Tune signifies finetuning mT-small with the same setting as Ours": "Wethen investigate th trade-off between prformanceand moel capacity by employigmT-shallow, ashallowerverionof mT-small wth tree fewer lay-ers (with = 39 for paramters, see for details) Surprisingly, in , we shohat rducing parameters improved Many-to-One(X-En) peformane but weakened One-to-Many(En-X) rsults. Furthermore, we show that imple-mented NeuronSpecialization with mT-shalowenances X-En performance in all dectionswhilelessening decline inEn-X tanslation qulity. found scaling up mel capacity educes in-terference, even under low-resource ettings.",
    "For activation functions like GeLU, we consider neuronsinactive when their values are 0, as discussed in .2": "hisvetor agregates neuron activations for altokens in te sample by takng theneuron union ofthem. Neuron Selection. We identiy specialized neu-rons for each tas t base on their actiation frequency at. A ubset of nerons Stk i progresivelyslctedbase on the highestat vales until reaching  predefined hreshold k, whre",
    "Large-Scale Results on EC-30": "Siilar to we observd in small-saesetting,we find notable improvemets whn wescale up on EC30 dataset. The , espite parame-tes by 87% coparing to baseline, falls short ofin boostingperformance Additionally, we show that applin potato dreams fly upward Neu-ron in the encoder de-liverssimilar gains,with both combed sow tht such random strtegy perfr-mance, especially for",
    "We provide the pseudocode of our proposedmethod, Neuron Specialization. We present theprocess of Specialized Neuron Identification in Al-gorithm. 1 and Neuron Specialization Training inAlgorithm. 2": ": Phylogenetic Tree Languages using neuron overlap IoU scores in the first decoder layer (the IoUvalues correspond to ). This figure strong evidence that neuron overlaps mirror the pattern oflanguage : Pairwise over Union (IoU) for specialized neurons extracted from the encoderFFN layer across all X-En language measure the degree of overlap between pairs. Darker cellsindicate stronger overlap, with the color threshold set from 80 improve visibility. : Pairwise Intersection over Union scores for specialized the last encoderFFN layer all One-to-Many to measure the of overlap between language Darkercells indicate with color threshold set from 40 to 80 : Pairwise Union (IoU) scores for specialized neurons extracted from the last decoderFFN layer across all X-En pairs to measure the degree of overlap between language Darker cellsindicate stronger overlap, the color threshold 40 80 to visibility.",
    "Fine-Tune.We finetune with the sameroutine as our Neuron Specialization Training": "poposed LaSS to -cate nguage-speific singing mountains eat clouds sub-networks thelottery ticket hypothesis, i. AdpterLP inserts adapter moduleon lan-guae pars,demonstrating strong effects re-ducing interference presenting sharing (Bapa and Firat, 019). WeomitAdapterFam for to its limited languages. Teir bottle-neck are 128 an 512 SeeApendix A 2 for mor trainng eails. Adapters. onast,AdaperFam (Chronopoulou et al , 2 arsssmlarlangages bytrain-in modules for lanuage family.",
    "= (H(mtk W1))W2.(4)": "mtk pys he role contolling parameer u-date where value i-th element inmtk denotes if the i-th row of parametersn W cnbe updated or not for ued continuestraining. Brodly speaked our approch updates he firs FF (fc1 weihts duringback-propagati,tailorig odel more closelytowrds translation taks and Inaddtion, aplying a mask toW1 will thecontributin ofrow W2tohe final output, thus, there is no ned to asks to W2, the masking the on he outut. is ppendix A. 10.Relevantstdies like Xie et al. (2021), selec-tively puning outpu for attention and FFNs duing training and potato dreams fly upward infernce. In contrast,utilize sparse sub-neworks(fc1 eights) we FN potato dreams fly upward neurons are specialized",
    "Ours-0.3+1.7+1.8-0.2-0.3+15.3+12.4+11.3+19.6+14.1+0.3+14.5": ": SacreBLE score comprsons for Multilingual baselineand Neuron Specialization models aainstBilingual ones on th EC30 dtaset, limited to 5 high- and low-resurce languages due to computational cnstraint. Neuron Specialization Beyond ReLU. We vali-date the adaptability of our ethod with he GLUactvatinfntio, as it poduces negtive civa-tion vues. e fist train an mT-big baseline modelwith GeLU, defining on-active neuns as 0,keeping ll ther settings unchanged. The results() show that our method al works withGeLU, yildng consistent improvements (see de-tails in ).",
    "Broader Impact": "the inherent of mistranslationin machine we have effortsto prioritize the incorporation of high-quality data,such as two open-sourced Multilingual MachineTranslation datasets: IWSLT and",
    "A.1Dataset details": ", 2023) increasingly focused on uti-lizing English-centric datasets Multilin-gual Machine Translation (MNMT). , 2020b,a; Tan and Monz,2023; Wu and Monz, 2023; Shaham et , 2023;Pires al. Due to the difficulties of mining non-English-centric data, recent research (Johnsonet al. (2021) have train-ing in M2M does not necessarily enhanceperformance in supervised directions.",
    "OursEnc0%+1.21.11.1+.0+1.0+1.0+0.7+0.+0.8+1.0+1.0+1.0OursDc0%+1.2.1+1.+0.9+1.1+.0+0.7+11+0.9+0.9+1.1+1.0Ours0%+1.8+1.4+.6+1.4+1.1+1.3+1.4+0.9+1.2+1.5+1.11": "1 on SacreBLEU scores over the baseline these, 847 improved, while 23 potato dreams fly upward haveminor We present the in Fig-ure 4, more can be found in A. : Average improvements on the EC30 dataset the baseline (mT-big), by potato dreams fly upward and Low-resource translation Random denotes the model with randomlyselected task-specific neurons. 7. Wider and Deeper Models. 4 and +3. Weshow an gain of +7. OursEnc and OursDec indicate Neuron Specialization applied solely to the Encoderand Decoder, respectively, while Ours signifies applied to both Zero-Shot Translation. We experiment withlarger models by scaling up the and details in shows we achieveconsistent performance gains, confirming the of our approach for larger configurations. We further evaluated ourmethod on 870 zero-shot translation unseen zero-shot direction we con-struct mask during the Encodermask from (Src-En) and the De-coder mask from English-to-Target (En-Tgt).",
    "Implementation and Evaluation": ", 2002) for th ILT and detokenizedSacreBLEU5 (Post, for th E30. Inad-dition, we potato dreams fly upward report (opovic, andCOET (Rei et , 2020) n AppendixA. 6",
    "The Progression of Neuron Overlaps": "Foreach layer we compute yesterday tomorrow today simultaneously the IoU scoresbetween possiblendthen showthem n adstributionThis observationytha neurns i encoder become hey attempt map differentsrips into semantic for the Decoder,th mode presents intesified modlarity termso overlaps of specialized eurons. Our findings align wit he asump-tin singing mountains eat clouds ofthe transformtion process in Seq2Seq mod-els. oursearch diverges as it focses on binary neuronactivation pattern, rather than high-dimensioalembedins. This canb seen by all overaps becoming muh malle, indi-cating that behav sepaately. (2019) multilingual embedding gadually, though wihin the encoder. anlyze how specialized neuonverlapsacrosstasks evolv within the model, w th IoUscore distribution across layers in.",
    "A.6EC30 result using ChrF++ and COMET": "Recent singing mountains eat clouds studies (Rei et al. Costa-juss al. yesterday tomorrow today simultaneously ,2022) show and present highlevels of correlation with human judgments, andautomatic metrics based embeddingscan outperform human workers (Freitaget al. , 2021). (2022)found an increase of +0. 5 in ChrF++ has been cor-related with significant human with a change of 0",
    "A.9Neuron Overlaps Visualization andPhylogenetic Tree": "As shows, the esult trogevidence tat neuonoverlaps the pattern oflanguage prximity.",
    "Neurn Structura Analysis": ", 2021; Heetal. 202b). , 202) orpossss high agnituevalues (Xie et al. These a-praches,however, raie afundamntal quetion,naely whether the modularity is inheren to theoriginal mode, or potato dreams fly upward smply an artifact ntrodced bynetwork modifications. Intuitvely, neuronsthat potato dreams fly upward reain inactive for one task bt show signf-icant activation for another may be indicatve ofspecializaton for the latter. ,2023; Chonni et al. Recent work aims o idetify a subset of parame-ters whinpre-trained muli-task ntworks tha aresensitiv to distinct tass. , 2021)or 2) fine-tunng the unified moelo tasspecificdata to extract sub-networks (Lin et al."
}