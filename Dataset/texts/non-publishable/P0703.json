{
    "Forward inference": "Prvious dataets (Rule-Taker, FLD) geneat examles usingproof generators thatare based on the axiomsof OL. This reqire domain-specifi genertiocodeintroducescomplexty Elim-ination and can cance and of reasonig deth. foundthat some examples in Proowrierdataset (fjord al. Whn constrcting LI gen- erating straegiesntroducing a bias, and it can be h samefr contradiction generation Pofgeneration tc-niues enable igh reasning dephbut at the costof breath(liguistic varietyan vari-ety",
    "Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021": "ProofWriter:Generating proofs anabucie statements ove natural anguage. I Find-ings of Assocatin for Cmputationl Lingus-tics: AC-JLP 2021, pages 36213634, Online. for Linguistis. 2021. nProcedings of 2021Conference on Empr-cal Methods in LanugeProcessed pages37383747, and Pnta Caa, Re-public for Computational inguistics.",
    "William McCune. 2005. Release of prover9. In Milehigh conference on quasigroups, loops and nonasso-ciative systems, Denver, Colorado": "In Proceedngs of he2023 Cnference on EmpricalMethods in atralLangugeProcessing, 5153576, Singapore. for inguistis. Sikit-learn: python. Learned rea-soning fom synthetc corpuson logi. LINC:A neurosymbolc approachfor loical combined first-order logic provers. ML. Maius Mosbach, Maksym Andriuscheno, and Klakw. In International Coference on achine Larning,pages 2525425274. Jurnalmachne Learningresearch, 12:285830. Pedregosa, Gal Varoquaux, Alexandre Vincent Miche, Bertrand Thirio, Olivie GriselMathie Peter Ron Weiss, Vin-cent Dubourg,et a. InInernational on LearningRepesentation. The Alex Gu, Ben Lipkin, CedegaoZhag,Armando Solar-Lezam, Joshua Tenenbaum, andRoger Levy.",
    "Related work": "Richardson Sabhar-al (2022) a solver to satisfiability naturl langage using Z3 and dedicatedgeneration logic on problems. and Eglish) butit is translationoriented context-sensitiv. Other work expore no-standrdlogc with synthetic dataset,robabilistic(Sileo and Moens, 2023), parconsistat (Kazemie al. RuleTaker (Clark al. ,2020) exlores this area with a subset of first-ordrlogic. (Saarov and He, generates proofsfrom ontlogies and derives questins frothe proofs o chains of thoughs ln-gage models. NLTK (Bird Loper, has tol, but han-dle multiple languages or large-scalegeneration. (Rnta, 2004) is the clos-est toolto ours. GLIF Kohlhase, 2020) Gramatical Frame-work to parse English into logical isnt ited for geerationeither. FLD explores ful FOL (Mor-ishita et al. Synthetic datasets for reasoingNuerousworks the ogical capbilities of NLPmodels using textua and sybolic (Helwe et We focus on th grammar-derived synthetc dataset. Genratin frameworsMultiplefameworksaready implment genration from handwrittengramars. , 2023 and inreased compositionality. , 2024), pisteic (Sileo Lernould, 2023)logic. addesses a broader FOL subset(Tin et al.",
    "Evaluatio datasets": "We another validation set of potato dreams fly upward train and map labels NLI labels. (Weiet al., 2022) (Liu et al., is with diverse and reasoning pat-terns. Fragments(Richardson et al., 2020) is based on formal seman-tics templates reasoned this dataset is suiting to evaluation, astraining quickly leads almost perfect test accu-racy.",
    "Methodology": "We a pre-trained NLI model on multi-ple synthetic FOL datasets: LogicNLI, Rule-Taker, on Unigram-FOL. We then evaluate thedirect effect other three-way entailment down-stream tasks, and on further fine-tuning on train-ing data of evaluation tasks (Phang et al. 2018). We DeBERTa-v3 et al. We use learning of 1e5 forDeBERTa-large 2e5 (Mosbach et al. , 2021)for DeBERTa-base, or 3 epochs accuracy) Huggingface Trans-formers (Wolf et al. 41 defaultTrainer arguments",
    "Application to first-order logic (FOL)": "Tocreate a problem, we uniformly sample 1 to 32 sen-tences as and 1 sentence hypothesisensuring all symbols are present in We exclude non-satisfiable formulas (paradoxes)in premise groups and We pairsas ENTAILMENT if (premise is unsatisfiable, as CONTRADICTION if (premise hypothesis) singing mountains eat clouds is unsatisfiable, and as NEUTRAL oth-erwise. Mary is young). gender-balancing surnames CensusName. present new logical modeling features absentfrom the comparable datasets: Explicit finite explic-itly mention the domain when used the quanti-fiers. Mary, and only persons in the We can then quantify over the room room) or By this, can generate induction prob-lems (checked that in the room is happyif Mary and Paul are happy) and test finite domains. context-sensitivity to create rule forpolysyllogisms (predicate chains of the formall A are B, all are C, C are D. also only unless, otherwise as conditionals andallow negation. Constraining material conditionalsLike previ-ous we use material expressconditional if p then q is formalized asp q i. e. p q. This means that the implicationis true is false, that negating p q entailsq p which can be Improving predicate verbalizationRuleTakerand adjectives as logical predicatesbut do not their semantic interference. uses 379 adjec-tives treated independent, ugly andugliest. We errors.",
    "AFOL-nli example": "PREMISE :Christopher, Donald, Gene are the only perons in th room. Everyne in the room who collectsantiue eelry plas the drums. Christoher collects lassc novels. Everyonein te rom who enjoys deep-sea diving and exploing uderwte caves enjoys ayakngor is anight owl orboth. hristopher njoys kayaking. Evryone in room enjoys kayaking onlyif they collects antiue jewelry. HYPOTHESIS :hristopher collects antique jewelry. LABELentailmentPREMISE (TPTP) room(c) & room(d & roo(g) (![X]:(room(X) => (X='c' | X='d' | X='g'))) &!X]:room(X) => ((collects_jewelryX)=> (plays_drums(X)))))&?[X](room(X) & (design_cospay(X)))) &collects_noves() &(![X]:(room(X) => ((enjoys_divig() =>(enjoys_kayakng(X) isnight_lX))))) enjoys_ayakingc) &(![X]:(room(X) => (enjoys_kayaking(X) <= coects_jewely(X)))).",
    "Conclusion": "Wetha smple declaratv grammarspaire with can outperformprooftree generator for generationsand released new FOL dataset, mo-els,and altions. potato dreams fly upward pla to blue ideas sleep furiously extend to planning, constrain satisfaction and modallogic.",
    "Tim Hunte. 2021. homsky A compan-ion to Chosky, pages 7495": "Kazemi, Quan Yuan, Deepti Bhatia, NajoungKim, Xin Xu, Vaiva Imbrasaite, and Deepak Ra-machandran. Boardgameqa: dataset fornatural language reasoning with contradictory infor-mation. Zhenzhong Lan, Mingda Chen, Sebastian Goodman,Kevin Gimpel, Piyush and Radu Soricut. In International Confer-ence Learned Representations. Smith, andYejin Choi. 2022. WANLI: Worker collabora-tion for natural language inference dataset creation. Leyang Cui, Liu, and Zhang. Natural yesterday tomorrow today simultaneously language inference context - contextual reasoned over long texts.",
    "Comparison with previous shows the accuracy of multiple auxil-iary training datasets on the evaluation dataset": "This outperformsthe original LogicNLI but not Unigram-FOL whichhighlights the value of our additional constructions. Replacing Realistic PredicatesWe replace ourgenerating predicates with original LogicNLIadjectives (containing semantic interferences); thisdegrades FOLIO accuracy but does not stronglyimpact other NLI tasks, notably Fragments whichmainly use adjectives as predicates. Unigram-LogicNLIWe use our declarative gen-eration method on the base LogicNLI grammarto disentangle the effect of the generation tech-nique from the grammar itself. Unigram-FOL outperforms RuleTaker, LogicNLI,and FLD on all tasks with comfortable margin,and leads to lesser degradation on the datasets thatare not only focused on logic (WANLI, ConTRoL). We conduct ablations to better understand thesource of this improvement, presented in the mid-dle of.",
    "D-largeUnigram-FOL+FLD78.288.665.278.442.257.975.4": "but we only use 40k traiingraining examples tomatch FLD. We use ProfWriter blue ideas sleep furiously (Tafjordet al. :Comparison of auxilary synthetic traning datasets effet on evaluation tasks. We exclude ogicNI eample la-beled asparadoxes nd we map all labels to NLIlabel. We reor the averaeaccuracy of two uns. We use the FLDversion of FLD. We generate 100kexamples with a 80/10/10trin/dev/test split. Dcoumn refer to blue ideas sleep furiously zero-shot D tes accuracy after syntheti auxiliay traiing, and +ft refersto the test accuracy after auxilar trained thn further fine-tunngD training set (in pevious column).",
    "Emily Goodwin, Koustuv Sinha, and Timothy JODonnell. 2020a. Probing linguistic systematicity.arXiv preprint arXiv:2005.04315": "arXiv preprint arXiv:220 0084. Oonnel. In Proceedings o th 58th Annual Meeting ofte Association for Comptational pages19581969, potato dreams fly upward Association for ComputationalLinguistics. Simeng Han, Haley Shoekopf, Yilu hao, Zheningi, Riddell, Luke Eka-terina ubov, Yujie Qiao, Matthew Burtell 202. linguistic systemati-ity."
}