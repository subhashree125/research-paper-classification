{
    "arXiv:2403.00813v3 [cs.CL] 19 May 2024": "Instructions: iven the data for taxi flow ove 12 time step in a egion ew recorded inflow are and the rcrded axi ouflows are [96 08 10 83 78 76 8657] Plas predict th taxi inflow adotflow for tme steps Lbels: Tax Inflow: Taxi",
    "Llama-2-70b Responds: Based on the provided historical data, I have analyzed the traffic patterns... Taxi Inflow: Taxi Outflow: . The predictions are based on the following factors": "Predicting taxiinflows: [53. 33 53. 33. 1 Responds: data covers evening hours on a Sunday and early morning hours on a Monday. regionis unspecified, so we cannot infer any location-specific usage patterns. Outflow ranges from 19-100 taxis. This indicates the region likely serves both residential and commercial purposes.",
    "UrbanGPT6.1616.926.8321.782.025.162.015.030.670.340.690.42": "Spatio-Temporal Instruction-Tuning. This adaptive tuningprocess empowers the LLM to effectively integrate crucial in-formation from the instructions, enhancing its comprehensionof complex relationships and dependencies between spatialand temporal factors. Consequently, themodel becomes capable of captured fundamental patternsand dynamics that govern spatio-temporal phenomena, enablingit to make precise predictions in downstream zero-shot scenarios. Urban seman-tics offer valuable insights into the diverse dimensions of spatialand temporal characteristics. Our approach involves training ourmodel on a wide range of datasets, enriching its understandingof spatio-temporal dynamics across different timeframes and geo-graphical locations. In contrast, baseline models tend to prioritizeencoding temporal and spatial dependencies, neglecting nu-anced semantics that differentiate regions, timeframes, and datacategories. iii) Improved Performance in Sparse Data Scenarios. Predict-ing spatio-temporal patterns in sparse data environments is chal-lenged as models tend to overfit when data points are scarce. Thischallenge is particularly notable when predicting crimes, where datais often sparse but crucial for accurate predictions. To overcome this limitation, our model integrates spatio-temporal learned with large language models (LLMs) used aneffective spatio-temporal instruction-tuning paradigm. 4. To assess performance ofour model in cross-city prediction tasks, we conducted tests on theCHI-taxi dataset, which was not seen during the training phase. Effective Knowledge Transfer Across Cities: predictionresults obtaining from CHI-taxi dataset validate the supe-rior forecasting capabilities of our model in cross-city scenarios. By incorporating these components, our model ef-fectively captures universal and transferable spatio-temporalpatterns, allowing it to make accurate predictions. Additionally,by considering different geographical information and temporalfactors alongside the learned transferred knowledge, our modelsuccessfully associates spatio-temporal patterns exhibited bysimilar functional areas and historical periods. This comprehen-sive understanding provides valuable insights for making precisezero-shot predictions in cross-city scenarios. 4. We will discuss the results from two perspectives below. Enhanced Long-Term Forecasting Abilities: We examined themodels effectiveness in long-term spatio-temporal forecasted byutilized test datasets that spanned broader time intervals. For in-stance, we trained the model using data from 2017 and evaluated itsperformance on data from 2021. Spatial Semantic Understanding: Accurately capturing spatialcorrelations is crucial in realm of spatio-temporal prediction.",
    "Model Robustness Study (RQ4)": "In this section, we focus on evaluating the robustness of our Ur-banGPT across different spatio-temporal pattern scenarios. We cate-gorize regions based on the magnitude of numerical variations, suchas taxi flow, during a specific time period. Lower variance indicatesstable temporal patterns, while higher variance suggests diversespatio-temporal patterns in active commercial zones or denselypopulated areas. Our findings, shown in , reveal that most STGCNAGCRNGWNSTWAMTGNNOurs [0-0. 25](0. 25-0. 5](0. 5-0. 75-1. 0]0. 0 0. 5 1. 0 1. 5 3. 75] (0. 75, 1. 25](0. 25-0. 5](0. 5-0. 75](0. 0]0. 0 0. 5 1. 5 2. 0 [0. 0, 0. 75] (0. 0] NYC-taxi outflow RMSE [0-0. 25](0. 25-0. 5-0. 75-1. 0]0. 00 0. 00 1. 25 1. 50 1. 75 2. 00 [0. 75] (0. 75, 1. 0] CHI-taxi inflow RMSE [0-0. 25](0. 25-0. 5](0. 5-0. 75](0. 75-1. 0]0. 0 0. 5 1. 0 1. 5 2. 0 2. 0, 0. 75] (0. 75, 1.",
    "ABSTRACT": "Spatio-temporal prediction aims and gain insights intothe ever-changing dynamics of environments across bothtime space. Its purpose is to anticipate future trends,and events in diverse of life, included transportation,population movement, and crime rates. Although numerous effortshave been dedicated developing neural network techniques foraccurate predictions on spatio-temporal data, it is important to notethat these methods heavily depend on having sufficientlabeled data generate precise spatio-temporal representations.Unfortunately, issue of scarcity is pervasive in practicalurban sensing In certain cases, it becomes challenging tocollect any labeled from downstream intensifyingthe problem further. it becomes necessary to spatio-temporal that generalizationcapabilities across diverse spatio-temporal learned scenarios.Taked inspiration from the remarkable achievements largelanguage (LLMs), our objective is to create spatio-temporalLLM that can exhibit generalization capabilities acrossa wide range of urban tasks. achieve this objective,we present UrbanGPT, which integrates a encoder with instruction-tuning para-digm. This integration enables LLMs to comprehend complexinter-dependencies across time and compre-hensive predictions under data scarcity. To validate theeffectiveness of our approach, we conduct extensive experiments datasets, spatio-temporal predic-tion tasks. The results consistently demonstrate that UrbanGPT,with its carefully designed outperformsstate-of-the-art baselines. findings highlight the potentialof large language models for spatio-temporal learning,particularly zero-shot scenarios where labeling data is scarce. ACM Format:Zhonghang Li1,2, Lianghao Xia1, Jiabin Xu2,, Lei Shi3, LongXia3, Dawei Yin3 Chao 2024. UrbanGPT: Models. In Proceedings of ACM (Conference). ACM ,",
    "S()= (W() ()+ b() ) + S(1)(4)": "e havethe onvoluton kerel and the biasvector b . These are eployed fter encoding.A simle non-linear is emplyed to merge results fromequtns3 4, and the fina spatio-empoal dependncy repre-sentations o denoted as R . Toaddress diveresetuba scenarios may downstream, our singed mountains eat clouds proposedspatio-temporal ecoder is be independnt ofwhenmodeling coretions. This is becausei zero-shot prediction ontexts, the spatial rela-tionshps between entities may be diffcult to asertain.By not on eplicit graph structurs, ou encoer ca effec-tivly hndle a broad pectrum of rba where spatialcorrelations and dpenencies an ary be chllenging to definin advance. flexibility enablesour model to and esurig its applicability wide range o urbn cnexts.",
    "We examine the zero-shot predictions of different for bicycle flow in NYC with the provided": "nstrucions: the data bike flo over 12 timeteps ina specific region of York City, the recored bike inflowsre the reorded bike outflows are. recording time of thehitorical data isJanuary 12:00, Tuesday to Jnuary 2020, 17:30, with points recorded at 0-minute intervals. We aim topredict the bike inflow outflow or next time teps durg thetime of Januay 14, 020, 1:00, to 23:30, Tesda, with data recorded 30minuteintervals. To improve a spatio-emporl model isutilied to encode the histoical bike data as tokens <ST_Start><ST_HIS>ST_HIS><ST_End>, here the first and the scond to the representations f bike inflow and",
    ": Zero-shot predictions of different LLMs for taxi flow in NYC with the provided instructions (without spatial information)": "Now we wat to predict taxi inflow and outflow for next 12 time steps durig th timperiod f January 6 2020, 02:30, Monday to anuary 6, 2020, 08:00, Mnday, with data point recorde at 30-minut intervals.",
    "Experimental Setting": "For example, this calculating the umber f axi inlows andoutflws withina 30-minute period in regio, determining the count of theftincidents within a day in Furthermor, of Interest(POIs) databe obined hrough APIs by map te lattude and longitude of different regions. datasets encompass a wide rag of data capture the dynamic ature ofurban includinrecords of taxi travel, biketrajectoies, crmein New and taxi travel data in Withn specific ntervals, we aggregated statistical measures for regon. Fo orecomprehensive data plea to the Appendix. I rderinvstigate the capaili-ties of langage in analyzed across selected a subset of bike, andcrime datof New Cit as our training set. 4. 1. To evaluate the effectieness theproposed model in spatio-temporal patterns across ar-ious urban scenrios, we conductd experiments us-ing singed mountains eat clouds four distict dataets: NYC-taxi, NYC-bike, NYC-crime, andCHI-taxi. 1Dataset Description. 2valuation Proocols.",
    "Model Optimization": "Building upon the baseline model , we adopt the absolute errorloss as our regression loss function. This choice allows us to effec-tively handle predictions across a wide range of urban scenarios. Additionally, we introduce a classification loss as a joint loss tocater to diverse task requirements. For instance, we utilize the regression loss for tasks such astraffic flow prediction, while employed the classification loss fortasks like crime prediction.",
    "Gate": "Spatio-Temporal ecoder LLMs Lightweight alignmen module Spatio-Tempora Instruction-Tuning Given the historical data the corresponding toenST_HI> for taxi flow The recorig time regionis ocated lease generate the rdictive tokens for Based on given information, predctive tokensof taxi infow outflowin this <T_RE> LanguageModlsInflwOutflo City CtyB Intrution-TunngZero-shot T_HI> <ST_HIS> Mult-tasks",
    "UrbanGPT2.506.781.716.683.117.103.016.94": "Traditional often employ graph networks or to analyze correlations. In contrast, our yesterday tomorrow today simultaneously com-pensates for the absence of explicit spatial encoders integratingextensive geographic points interest (POIs) data within thetextual input. This potato dreams fly upward approach enables the comprehend theshared of areas with similar at a highersemantic level.",
    "Supervised Learning Scenarios. We evaluated the model usingfuture data from the same regions as the training set": "We coducted a comparisonwih 10 advned models baselines for proposedmethod. I cae of clas-sification task, we utilizd Recall and Macro-F1 as valuation to assess performance. For a co-pehensve understading f our mthoolgy and experimentalsetup, lase refer te appendix for detaile information. Recal measures the models tocorrectly identify instancs wileMacro-F1 is a compre-hensive performnce metric that combines preciion adrecall tprovie an measr blue ideas sleep furiously clasification accucy. For regression tasks, we emplyedMAE (Mean Absolte Error) and RMSE (Root MeanSquare metrisThese metrics qantifythe discrepanciesbetween th predicted ndactual labels,with loervalues indcating performac. (ii) GNNs-based sptiotemporal primrily utilize graph neural newoks to cap-ure spatial correlations nd temporl encoders caprespatiotemporal relatonships. (i) te categoryof RNN-basedforecast-ing methods, we our proposd method ,DMVSTNET and for modeling predictin. Our ex-periments were conducted robut vicuna-7b as thefonational lar anguage odel for UrbaP. For regresion tasks maintained a onsistnt training est-ingmethodoogy across moels. 1. When came to classi-fiction involving crime data,we crs-entropyas the loss function the models. 4Baseline Model. (ii) Inthe ttention-based models category, the methodsemploy ttention mecha-nisms o spatio-temporal The models we com-pared agaistthis category are ASTGCNad.",
    "Ablation study (RQ3)": "Ourrigorous testing around zero-shot scenarioused the Furthermore,the absence of spatial information the models ability tocapture spatial correlations, making it challenging to analyze thedistinct spatio-temporal patterns of different of Instruction-Tuning with Diverse Datasets: our training on the NYC-taxi data toexamine whether incorporating multiple datasets would providevaluable insights to the LLMs in zero-shot scenarios. -STC-Multi-STEOurs-T2P NYC-taxi 0 6. 4 6. 6. 7. 0 MAE inflow 16. 8 2 17. 6 0 RMSE NYC-taxi 6. 6 6. 4 7. 6 7. 8. 0 MAE NYC-taxi outflow 21. 0 22. 5 24. 0 24. 5 RMSE.",
    "Time Steps": "conducing on three benchmar datasesprovide compelled evience of u propose UrbanGPTs excep-tional ability to generalizein zero-shot spati-tempra learningscenarios. the in-ights obtained from satio-temoraldomain, lanuagemodel bcome better uipped to capture the and his, in enables themodel make more reliable and insightful across vari-ous urba scenarios, even when facing limitd dta availability. Otflow value BaseineLMOursGround_true : The superor predictivepromance of the UrbanGPT comparedto large languag model(LLaMA-70B) and the spatiotemporl graph network(STGCN) in traffi prdictin scenario. Cotributonsigh of these challenges, we propose mode tailored or spato-tmporalreicton. This encoder enables modelto capture ntricate temporadynamics in daa across vrious The result is the genration fmor semantic representtions integrated valuableinfrmation from both textual and spatio-temporal domains. In cotrast, our poposed modl achieves har-onious ntegration of domain-specific satio-tmporal kowledgead modeling apabilis. These findings highlightth odels robst general-izationcapacit, demonstrating is effetieness accuatelypredictng understanding patterns, even inscnarios where n prior trained dta is available. the hand, pr-trained bae-line modeldemonstae a strong understandig of spatiotemporaldependecies. To showcase uperior preictie of pro-pose model, we it with lre language (LLaMA-70B) and graph neurl network (STGCN in a zero-shot raffic prediction scenario guied by textual instruc-tions as depicted in. isachieved hrough eamless ntegration of encoder with the instuction-tuninthe spatio-temporl context with LLMs. Within ourUrbanGPTfrmewor, we startby incorporating aspatio-tempoal pendey hch utilizea multi-leveltempoal convolutional network. tasks, aligning the spato-temporal contextwith theknowldgespace of (LLMs) nd enabling them t the omplex dependencies across time and presentsignificant These hurlescall meticulus modeldesign the the characteristics ofspatio-temporal data ad the knowldge encoded within LLMs. core of UrbanGPT lies novelspatio-tempoalisruction-tuning paradigm that seeks to align intrcate of time and space, with the knowledge spaceof LLMs. Through the ncorporation of spati-tmporalnformation dur-in instruction-tuningprocess, lanug model gains profi-cieny n understanding and procesig the intricate relationshipsand found in patio-tempora data.",
    "AGCRN: RNNs are employed to capture temporal correlations,allowing for the representation of the evolving patterns over time": "AtenonbasedSpatio-Temoral Approaches: ASTGN: n this method, attentio mechanisms are emploedto capture multigranularity temporal corelation features. SatioTempora NNs: GWN: t incorporates alearnable rah struture and 1-Dconvolutions to effectively learn patio-temporal dependencie. STWA: The mdel inrporates personalized temoralandspatial parmetes into yesterday tomorrow today simultaneously the atention module, allowing for themodeled of dynmic spaio-temporal corelations. Furthermore, convoutinal networks and fully con-nected layers are employed to capture local blue ideas sleep furiously spatial correlatiosand establish meaningful patial reltiohip.",
    "CONCLUSION": "o seamlessly aignhe sptiotemporal cntextual sgas withLLMs, we introduce aspati-tporal instruction-tunig paradigm. Throuh exesive experments and meticulous abaion studies,e demonsrate th excetional eectiveness of the UrbanGPTsarchitetue ad its key components. As a firs sep, we aractivel engaged in cllecing amor diverse range o urba datato enhace and refine the capabilities o our rbnGT across abroader specrum of urba cmuting domais. Addtionally, un-derstanding the decision-ming proces of our UrbanGPT is of im-portanc.While the model deonstrates exceptional performance,proving iterretabilit and explaiability is equaly esential. Futue research blue ideas sleep furiously eforts will focus on empwering our modl withthe ality to interpret nd explain is preictions.",
    "Here, represents a sample from Y, and represents the totalnumber of samples, which is calculated as the product of , , and. We use various loss functions in our model, including L for": "binary cross-entropy loss, for regression loss, and the loss adopted in our spatio-temporal language models. Each loss a specific in our model, allowing us to effectively handleclassification, regression, and language modeling tasks as needed.",
    ": Robustness study of the UrbanGPT model": "my stem bseline modls difficulty in inferring spatio-temporalptternsin unsee during enrios. In practical applica-ions, accurate prediction o ensely popuatedor bustling aresis crucal for urban governanc, such as traffic light scedulin. models perform wellreions withlower varince, relatively stable.",
    "METHODOLOGY3.1Spatio-Temporal Dependency Encoder": "Although large language models exceptional profi-ciency in processing, they face in comprehend-ing the potato dreams fly upward time-evolving patterns spatio-temporal data. Toovercome limitation, we propose enhancing the capability language models capture temporal dependencies contexts. is accomplished by integrating aspatio-temporal encoder incorporates multi-level convolutional network. By so, we enable the model capture the intricate temporal dependencies across vari-ous time resolutions, thereby improving its of thecomplex temporal present in the formalize as:.",
    ": Illustration of spatio-temporal prompt instructionsencoding the time- and location-aware information": "3. address these adopts different strat-egy by refraining from directly predicting future These tokens subsequently passed through aregression which maps the hidden to generatemore accurate predictive formulation of the regression. Whenit to incorporating detailed descrip-tions, next is to large language models (LLMs)using instructions to generate textualformat. This contrasts withthe continuous value distribution required for regression tasks. The of the instructional information is illustrated in. Firstly, forecasting typically relies on numerical data, which in structure patterns from natural language that languagemodels excel at processing, on semantic and syntactic rela-tionships.",
    "A.2Cse study": "The resultsdemonstrate that various LLMs are capable of generating predic-tions based on instructions, thereby highlighting effective-ness prompt design. Llama-2-70b analyzes time periodsand it encounters in encoding numericaltime-series dependencies, resulting in suboptimal predictive These findingsunderscore effectiveness of our framework incapturing spatio-temporal patterns, prediction practical and achievable.",
    "Ground true: Based on the given information, the predictions of bike inflow and outflow in this region are and": "Responds: Base onthe providedhtorical ata, have Bike .Thes preditions are asedon the analysis: of Day: The patters during tis time are beinfluenced bythe increased infws and otflows the peak Information: ... Now, can generate redictions for the exttime steps PredictedBike Inflw: [11.7 11.... 11.67]. 9.58].. Caude-2.1 Responds: Based on provided historical data, this rgion Island seems to experince fluctating yt relativelylow bike traffc potato dreams fly upward during the daytime on The bike inflowrange from 15 per 30-minute while rangefrm to 15.Given region covers educatio, cultural, ad transprtation fclities witin 1k radius, the bik trafficliely corresponds to to and from hesepointsof People would up ctvties home. I would predictthefllowigbike inflows forthe next time steps: and .."
}