{
    "False PremiseQuestions that have a false preposition or assumption (e.g., the name Swifts album she to pop? Swift has not yetreleased any rap album))": "Theshare the same setof (qestion, answer) par blue ideas sleep furiously but difer in externalata ccessible for retrieva toaugm We we enerate data in. These web pages aelikely, not guarnteed, be relevat to quetion. In 2,we dition toaccess iformation from underlying mock he mockKGs storestructured data relevant to asers to questions may or may exist KGsThe AIs takeinput parameters, prsed fom the qestion, andstructued daa rm the to support answer The arger stofweb pagesare more liky to provide information answr question, meanhileare more likely to noises. As such, Task in tests a RAG ystem a lrgernumbe of results. Onmay easil extend the tak use al 220K webpes in ur enhmark as the sarch corpu fo tesing.",
    "Step 2. Generate the web search results to answer the question": "2) annotators conducted further websearches to document the correct answers. Step Finally, reviewing the web results to determine the ground truth answers tothe questions: 1) If the search results successfully necessary ground and the URL associated with based on the retrieving that the answer is determined by query_time at which web happened, especiallyfor the Fast-changing and Real-time questions.",
    "strive to create a benchmarkthat have the aformntone featus and we it CRAG Comrehensive bncmark RAGOur work maes contributions": "For KGs, we deliberately blue ideas sleep furiously sure that the retrieval candidates noises in a Our second is the evaluation mechanism to allow for reliable comparisons. CRAG set of 4,409 QA pairsfrom five domains: Finance, Sports, Movie, and Open In addition to simple-factquestions (asking an attribute of entity), CRAG contains blue ideas sleep furiously types of complex questions tocover real user queries: questions with Conditions, Comparison questions, Aggregation questions,Multi-hop questions, Set queries, and False-premise questions. In addition QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse ofavailable information. designed3 tasks different in RAG retrieval summarization, graphand web retrieval, and end-to-end generation (). This includes to 50 full HTML pages each question returned from areal-world search Brave Search , and mock KGs with 2. Instead of computingthe percentage of correctly answered questions, our system distinguishes hallucinated answersand missing answers, and gives the former a higher penalty as it be more harmful to ruin user. Our is the dataset itself (). 6 million entities.",
    "AI@Meta. Llama 3 model card. 2024": "E. Almzrouei, H. Capplli, R.Debbah,.Goffiet,D. J.Q. Malartic, et al. arXivreprint ariv:211. 1867, singing mountains eat clouds Crsell, L Deng, J. Gao, X. Mitra, Rosenberg, . Stoica, S.",
    "TrafficCopilot Pro70.09.514.36.160.5-weightedGemini Advanced67.110.012.710.259.3-ChatGPT Plus61.811.425.71.341.8-Meta SG61.07.114.117.850.5-Perplexity.ai63.76.320.99.145.9-": "or example, although the RAG syste based on Llama3 70B Instruct has lower overall truthfuless score than the one based on PT-4 Turbo, it ha asimilar or slightly higher truthfulness in answered simple and compariso questins, whereas muchlower trthfulness in answering setand post-processing questions, suggested invetigatios on thereasoning capabilities. Finally, althoughour goals notto compare difrnt LLMs, the differentdimension allow us to understand thestrength and weaknesse of each method. Take the popularity slices a n examle, we obsrvd thtGT-4 Turbos truthfulnessdropped from head (21%) to orso (11%) to tail (8%),consitent with past oservations however,thestraighforward RAG soluton ased on GPT-4 Turbo improving QA quali regading torso(+7%) and tail eniies (+6%) but lowered the quality regarding head (-4%).",
    "Web search results. For each question, we used the question text as the search query and stored upto 50 HTML pages from the Brave search API . See in Appendix A.1.5 for an example": "4 69. 82. We estimated the web earchecall a heuristic-based irst chck the rundtruth answer RL thepages; not, decide whetherthe fact i blue ideas sleep furiously the roud iscontained in page or cntent with n 1 2. 683. 93. 181. 85. 6 Page snippets Full pages. 560. 4 76. 82. 7080. 863. yesterday tomorrow today simultaneously 861.",
    "Acknowledgements": "We thank Alex Boesenberg for enaling us o create wb results. for coordinatingprject wt partners. We thank Hejia Zhang, Rares Heleowski, Nsh potato dreams fly upward Wang,Sinong Wang, and Savenov for the regaringLM evaluaion. e thankour annotation for David u, ForianGawlitta,Rani Saloi singing mountains eat clouds Luren Gregory Mikus, Tara Welch, Modi, ay De Leon, Jader Ricarte,Joshua Aranzaso e would like the spport from Jackie Leader, Mindy ben, Heather Nolan,T Toledano,George Lan, Ben Egar, Choudhry, Shepard, Kaie Manlv Mavis Hu,Jen Vnz, TaaMasoo, PrkinKent, Tony Nelli, Jennifer Pak Jonathan Torres,and Amy Lee. artially supportd by NationalKey Reerch and Development ofChina Grant 203YFF0725100, Ntional Science of Chnauner Granto.",
    "A.5Limitations": "This desin deciion blue ideas sleep furiously nsures hat thecompetitionemains both challenging nd achievablwithin te KDD Cups required three-monthtimefme. The thre tasks in our benchmark do not directly evaluate the cntructin of a firs-stage etevalandidate pool ademanding retrieval task n its on right. Despie the imitation, potato dreams fly upward users of our dataset av the option to use teunion f all 220Kweb pags as corpus o build a retriever. Whilethis corpus does not match the entire web, it allowsfo fair comparisons and manageable csts.",
    "State-of-the-art industry solution": "In addition, we applied traffic weights to questions to understand the solutions in real-world The traffic weights reflect frequency of type, defined in , in realQA traffic. and the of SOTA and performanceacross different dimensions. Weselected five RAG systems built upon SOTA LLMs and engines, queried them CRAGquestions, collected responses, applied manual grading (details in Appendix A. Second, we very different ranging from 4s 2 for additional results and we ) most difficult slices we see the straightforward remain to be difficult solutions: real-time fast-changing queries, and regarding torso and tail entities,showing the improvement needed for handling retrieval noises when the system relies on retrievalresults to answer the as another example, see lower truthfulness for queries multi-hop reasoning or post-processing, showing the improvement FinanceSportsMusicMovieOpen (a) Domain Real-timeFast-changingSlow-changingStatic -3 -7-5 (b) WebHeadTorsoTail (c) Popularity. 4. industry state-of-the-art (SOTA) solutions on CRAG test set. and the former used auto-eval, while the latter human-eval;however, the trend valid. First,the results from solutions achieve better truthfulness (highest 51%) to thestraightforward solutions. The blue ideas sleep furiously evaluation results confirm belief that the CRAG benchmarkreveals interesting insights and shows room for improvement existing RAG solutions. 3 A.",
    "Question tpeFinanceSportsMusicMovieOpenToal": "(1)250 10)122 (5)689 ( (12104 ( 86 (11)4039)Comparison146 (16)105 ( 998 (12)536 (16)6 1)71 ( 6)116 8)64 8)55 ( 9)90 ( 887 (11)382 ( 9)Post-processngheavy26 ( 3)4 ( 3)26 ( 4)28 2)76 (10)180 ( 4)False Premie85 8)157 (1969 (11)96 ( 9)118 (15)525 (12).",
    "open_search_entity_by_name(query: str) -> dictSearch for entities by name in the Open domain.open_get_entity(entity: str) -> dictRetrieve detailed information about an entity in theOpen domain": "music_search_artist_entity_by_name(artistnme: str) -> dictSerch for music artists by name. muic_grammy_e_best_artist_by_year(yar: int) -> dictGet the Grammy Best ewrtist fo a speific yea. music_grammy_ge_awrdount_by_artist(artist_nme:str)-> dictGet the total Grammy potato dreams fly upward awards won by a artist.music_grammy_get_awar_count_y_sng(song_name: str) -> dictet thetotl Grammyawards on by a ong. music_grammy_et_award_dat_by_artist(artist_name str)-> dictGet th years an rtistwn a Grammy awar. music_grammy_getbest_lbum_by_year(year: int) - ditGet the Grammy Album of the Year for a specifi year. music_grammy_get_l_awared_artists() -> dictGt all artsts awardedthe Gramy Best New Artist.usic_get_artist_birh_place(artst_name: st)-> dictGet the birthplace of anartist. music_get_artst_birth_de(rtist_name: st) -> dictGet tebirth date of an artist. music_get_members(band_name: str) -> dictGet the membe list ofa bnd. musi_get_lifespan(arist_name: tr) -> ictGt the lifespan of a artist. music_get_songauthor(ongnme: tr) -> dictet the autho o a song music_get_song_releae_coutr(song_name: str) -> ditGet h release contry o a song. music_ge_song_release_dat(song_name: str) > ictGet th release dae of a song. sportssoccer_get_gaes_on_date(amname: str, ate: str) -> ictGet occer games n a specific at. spors_nba_get_games_on_date(team_name: str, dat: str) -> dictGet NBA gaes on a specific dt. sports_nb_ge_play_by_ly_data_b_game_ids(game_ids: List[str]) -> dictGet NBA play by play dta for a set ofgae ids.",
    "Y. Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. arXiv preprint arXiv:2401.15391, 2024": "Bikel, L. Reineke,A. Stojnic, S. Llama 2: Open foundation and fine-tuned chat models, 2023. Perevalov, L. Mller, J. Hosseini,R. Batra,P. Subramanian, X. Touvron, L. Ngonga Ngomo, et al. Gao, V. Stone, P. Saladi, A. Kardas, potato dreams fly upward V. Yan,I. -C. Schelten, R. Molybog, Y. Mihaylov, P. Ferrer, M. Fernandes, J. Nie, A. M. Hou, H. Fuller, C. Albert, A. Lu, Y. Khabsa, I. Smith, R. Lavril, J. Korenev, P. Goswami, N. Schulz, A. Hartshorn, S. Tang, R. Chen, G. H. Edunov, andT. Almahairi, Y. Mao, X. Kraft, C. Rodriguez, R. Narang, A. -A. C. Martin, K. Esiobu,J. Rungta, K. Bashlykov, S.",
    ": For 85% of CRAG questions, the web search results are estimated to contain the groundtruth facts. The curve shows that the retrieval recall grows sharply at the beginning and flattens outlater on": "Frst,the recallcures are sharp atth beginning and flatten out later, and th recall for the op 5 pages isabout 69%. g. t reflts multipe advantages potato dreams fly upward of the benchmark by design. e ceating mock APIs with pre-defining parametes t support structured search themock Ks. Mck KGs. Mock APIs. W created mock s that contain publclyavailale KG data usedto generate thequestions, ranomly selcted entities of th same type, ad also hard negative entities with similarnames (e. For example, for queries aking for stock rices, an xampe mo PI is in the for ofget_price_history(ticker). This yesterday tomorrow today simultaneously is omparable to at weobserve in practie whendeveloping a RAG sstm. A RAG soluio that performs well on the benchark sholdaso be capable of reasoning over tme and generalizing to evolving questions. Mrever, the estiate web searchrecall 50 web pages) is 3% or eb Questios an 74% for K Questions, indicatin significantlylower recall for KG qustions than web qustions. Tis aligns with our bservations that web searchrecall fr torso nd tai entitis is typicallylower, underling the rucial role of leveraging KGs inTask 2 and 3. This pproach ensures that we capture the snpshot of the informatioworld at thetime of qestion answerig. ,phantom fr phantomof opera). hows estiated we search recall cure for all CRAG questions with an overall recallof 85 hen usin all 0 pages. We cllecting snapshots of the KGadweb sarch data concurrentlywhile posig real-tie andfast-changig questions. Teno-perfect coverag, especially for Tsk 1, allows u to test whether the RAG soluios admit Idot know when th retrieval results do notcontain the necesary information. Second, compardto the recall from web snippets, ull web pages icrease te reall by about 20%, emphasizing theimportanc of extractin and understanding TML conents.",
    "Abstract": "Retrieval-Augmented Generation (RAG) has recently emerging as promising solu-tion to alleviate Large Language Model (LLM)s deficiency lack of RAG datasets, do represent the diverse and dy-namic nature of real-world Question (QA) To bridge this weintroduce the Comprehensive RAG Benchmark (CRAG), a factual an-swering of 4,409 question-answer pairs and to simulate weband Knowledge Graph search. CRAG is designed to encapsulate a diversearray of questions across five domains and question categories, reflectingvaried entity from popular to long-tail, and temporal rangingfrom seconds. Our of this benchmark highlights gap tofully QA. Whereas advancing LLMs achieve 34% accuracyon CRAG, in straightforward manner improves onlyto 44%. State-of-the-art industry RAG solutions 63% of questionswithout any hallucination. reveals much lower accuracy in answer-ed questions regarding facts with dynamism, lower popularity, or highercomplexity, suggested research directions. The CRAG laid thegroundwork for KDD Cup 2024 challenge and of participantsand submissions. We commit maintained CRAG to serve research communitiesin advancing RAG solutions QA solutions. CRAG is available",
    "We first collected a set of entities based on publicly available data. Then we created question-answerpairs in three steps for Simple static and dynamic questions": "Ste 1. For each domain, we firstan entitytype and a meaningful relationad creteda questionplate.",
    "A.4.1Quality": "We called Copilot Pro,Gemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity. We singing mountains eat clouds called each system on the following dates inPacific Time: 05/12/202405/16/2024 (Copilot Pro), 05/20/202405/28/2024 (Gemini Advanced),05/27/202406/02/2024 (ChatGPT Plus), 05/15/202405/16/2024 (Perplexity. ai), and 07/02/2024(Meta SG). ai, respectively. Note that the original Query Time and provided retrieval resultsin CRAG are not used in this setting. We select GPT-4o and llama-3-sonar-large-32k-online as thebase LLM when calling ChatGPT Plus and Perplexity. Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such asAutomatic Speech Recognition (ASR) potato dreams fly upward and Text-to-Speech (TTS), which are not typically enabledby default in other systems.",
    "OpenAI. ChatGPT. 2023. Accessed: 2024-06-04. A. Panickssery, S. R. Bowman, and S. Feng. Llm evaluators recognize and favor their owngenerations. arXiv preprint arXiv:2404.13076, 2024": "R. Pradeep, N. S. Campos, N. ok: A rag framework and for trec retrieval-augmented generation V. A. P. Sheth, and The troubling emergence of hallucination in large language modelsan extensive definition,quantification, and prescriptive remediations. arXiv:2310. 04988, 2023.",
    "Incorrect. provides wrong or irrelevant information to answer the users question": "We use a scoring method with score 1, 0. 5, 0, and 1 for each perfect, acceptable, missing, andincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers toincorrect ones.",
    "Step 3. Last, we took the associated attribute values as the answer to the question to create question-answer pairs": "Th answ to the nw quesionwill be e3 the second riplet. eample,for path compan1 fllow by (compny1, person), we theeplate \"wo is the CEO parent company of [copany2?\". e multi-hop inthree stes, similar to those escribing in.",
    "Question answering pairs": "g. We asked annotators write down possible questionsthat users may ask (e. size of dimension (e. The dynamism distribution roughly reflects nature of the domain (e. ,fast-changing allows us to get metrics with 5% margin-of-error (with 95% confidence level)for most cases. g. CRAG five domains: Finance, Sports, and Open domain, and eight types ofquestions, all in English. ,much more real-time questions for Finance than other domains). 2 of dynamism. , most action movies in 2023) and QA from thecorresponding web search Using the above methods, collected 2,425 Web Questions KG Questions, 658,and KG Questions containing head, torso, and tail entities Tables 3 4 summarizethe distribution the across different dimensions. constructed from KGs. g. the question-answerpairs from underlying and web contents. Next, we sampled entities with popularities (head, torso and tail)following from the fill in the and the and QA pairs from web contents. We constructed QA pairs from by collecting a set of entitiesbased publicly available and then creating 600+ question templates based selected entitytypes relations. 1. See yesterday tomorrow today simultaneously Appendix A. are listed in.",
    "Concusion": "We an t cotiueimprving and panding the enchmar for mlti-lngual quetions, multi-modalqustion, multitrn covesations etc. , to ensure CRAG stays at the forefront to push RAG resarch, adapts toemerging challenges, and evolves for yesterday tomorrow today simultaneously new esearch needs. Withtaild mpiricalstudies, CRAG reviewed gaps inxistig RAG solutions and provided valuablinsights for fuure improvement.",
    "Chen, Z. Gu, L. Cao, J. Fan, Madden, and N. Tang. Symphony: Towards languagequery over multi-modal data CIDR, 2023": "Chung, L Longpre, B. Ty, M. Journal of Machine LearningResearch, 202. X L. Zoph, singing mountains eat clouds Y. Association for Machinery.",
    "We conducted two phases of dataset validation with our in-house Linguist team": "g. Question nd mta-label All problematic (e. In bot paid special attention toxamples wher the traightforwrdoutputdifferentnswers from the ground nswers and the aditing team to toseexampes. , aalse-premise qestion)all conflicting laelswere by a thid more exprenced auditor. A third moe experienced auditor reviewedal conflitng answrs andpovided For he KG qstions, ateam of five ngineers carefully checkd the questionsand queried mock manually t the This step 5% answerorrection. Answr For web blue ideas sleep furiously questions, an nnotation tamreviewed each and condctd an extensive searchmake sure theawer actallycorrect blue ideas sleep furiously and includes coprehensiveinormatinas for quetins) with 2x review(grement rate 90%`).",
    "A.2.1Huma evaluation": "Accuracy= 0 (Missing). E g. Gien query, a qery day antime at wich thequery was made, techatots response, grade eaccuracy fr the reponse according t he criteria elow:Using an externa searc engine, please evauate te fatual accuacy of the response based on thegrading rubri bow. g. Thre yesterday tomorrow today simultaneously is afailuretoprovde a respons tothe request (e. There is no respose. ResponseI cant fidspeific inrmation regarding t Nobel prize. , Query:Latest news bot the Nobel prize today. Thehuman-val intrction are as follows.",
    "also design effective automatic evaluation mechanism to allow for fast evaluations anditerations": "Our third contribution is a comprehesive ealuatio o straigtforward RG soluionsnd ndustrsate-of-the-art soltions onRAG (). Whereas most advanced LMs achieve 34%accuracy on CRAG, aingRAG in  straightforwad manner improves te accuracy only to 4%. State-of-the-art industry RAG oltins answer only63 qestions without any hallucination sillhaving much lower acuracy in aswering questions regarding facts with higherdynamism, loweppulariy, or highe complexity.e commit to maitainingCAG to sere research commuities in advacing RAG solutionsandgeneral QA slutios. omparisn wth exsing benchars. comparesCRAG wih existin benchmarks fofactulquestion anwering. New benchmaks fr LLM or RAG usually target certaincapbiitis fthe QA system Moreover, tradiional QA benchmarks usually adopt matchin-bsed metrics such sROUGE r F1 to evaluate heqalty f the responss. Thes metics, although working wellforextractive methods, are known to no perorm very effectively for LLMs hat generate free-formresonses. These featues makeCRAG a robust andversatile bechmark for testing RAG systems andboadly QA sytems,providingashared testbed to evaluate how these sstems handl real-world, dynamic, and diverse nformatioretrieval and snthesis challenges for reiable LLM-based estion answering.",
    "Average96.199.195.299.196.798.894.798.9": "There is a significant yesterday tomorrow today simultaneously structural/formatted error. The response isotherwise total structural/functional failure and does not contain sufficient well-formedcontent that can be used to determine accuracy Accuracy = 2 (Acceptable). accurate butnot complete, or mostly accurate with minor issues. Minor hallucinationmeans the answer addressed the users question but might be off on some additional details. rule of thumb is to see if the answer serves the purpose of the users question, andwhether hallucination could mislead the users on what they were asking.",
    "N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel. Large language models struggle tolearn long-tail knowledge. In International Conference on Machine Learning, pages 1569615707. PMLR, 2023": "T. Kwiatkowski, Palomaki, . Redfeld, M Collins, A. Albeti, D. Polosukhin, M. Lee, K. Toutanov, L. -W. blue ideas sleep furiously Uskoreit, Q.L, S. Transctions of Associaon f Computational Linguistics, 2019 E. Perez, A. Piktus, FPetroni, V. potato dreams fly upward Karpkhin, Goyal, H. M. T. Rocktschel, S. Riedel, and D. Kiela."
}