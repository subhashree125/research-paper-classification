{
    "We select three classes for visualizing the feature distributions on the test dataset of CIFAR-10. We employa multi-layer perceptron (MLP) backbone and utilize a PGD-3 attack with =2": "In the latent space of adversarial examples, after integrating our SD toFS in d, distributions of the categories are more inclined toward the vertical, and the clusters ofcategories are more compact than the ones of FS in c. This trend also emerges in the clean dataset,as observed yesterday tomorrow today simultaneously in models trained by FS+SD in b and FS in a.",
    "Results on Different Attack Budget": "Acrss all atack singing mountains eat clouds budgets, modelsincorporating show blue ideas sleep furiously a moe gradual prticulary for PGD-AT+SD.",
    "Proof of Theorem 3.1": "Theorem yesterday tomorrow today simultaneously Given the dat matrix and adversarial matrix Xditha bt contain Nisamplesf i-th ca the se, sets clean data Ci and data i-thclass ver theunderlying data nd DNNfthat mapsdata to latent features with dimenson r,f lss function l() of f() ist-Lipschitz nd f() is the L-Lipcitz, then for any 0 with at least 1 , we singing mountains eat clouds ae",
    "Sensitivity Analysis": "n a, we cn obervethat of the shirnk trm is han that of the epand term sie rato of the epand (1 b hows hat our ehod is less sensitveas the fluctuation is only arund 2. 5%.",
    "(d)": ": The accuracy gap vs. at different training epochs. The comparison of FS and to trainthe robust models against attacks: (a) C&W attack (b) PGD20 attack. Adhering to the established protocol (Xu & Mannor, 2012), rather than the error gapdifference |RGE GE|, we choose to the accuracy gap YT )ACC(XadvD , YD)||ACC(XT , YT )ACC(XD, YD)| the same trend witherror difference. In. Theorem 3. 1 whether the difference is caused by GMD,the VDWSt and the accuracy gap under and PGD20 60 to 200 epochs shownin 5a and 5b, where robust models are FS and AT respectively and VDWSt denote theVDWS values of the datasets.",
    "Vol (P ) = det (P ) .(3)": "Intuitively, the more diverse the subspaces are, the more separable of adversarial Consequently, the inter-class similarity tends 2.",
    "Abstract": "On the algorithmic side, this reularization term nove adversaril traig (AT) Subspac Diverging (SD) to expand differnce the whole latent spaes linearspan and subspaces spans. Robust generalization (RG), concerning deep neurlcould perfom ad-ersarial generaed rom unsee datase, ha emered an active research topc. Albeit its crucial previous studies lack a well-founded theoretical analysisand certified errr bounds. Our investigationdiscloses that introduced thisincnsistency as a regularizaton named Gram matrix (GMD), wil led totghter uppe bound and certify RG. Meanwhile, we demonstrate tha previouefforts reduce iter-class similarity andincrease intra-class siilarity mong adrsarialexaples for enhanced adversaril robustness are approximate optimizations GMDapproach. In tis paper, we make novel attempt to theoreiclly and em-piricall study how we cld attain a better RGdiscriminative representation,here the of inter-sample similariy atrix clean and shouldbe reduced.",
    "Subsequently, we estimate d and k using Lmax, Lmaxdiverge and Lmaxgene, where d =LmaxdivergeLmaxgeneand k =LmaxdivergeLmax": "have observed thatvausof d n k for each epoch. Therefore, ostreamline process, copued and k nly i the eochThe spifi valuesfor  and k ofvario atsets a th algorithm can foud n experiment When training the generatedaversarial our agorithm can be regarded as a regulaizatio to icrease the overall panned but reduce te volume of subspace.",
    "Meta-Analysis": "This combined potato dreams fly upward approach heps us evalatebth how mch SD improves obustness ad whether thes imprvements are consistent potato dreams fly upward across datasets. Cohens d masures the ffct size, which qanifes the magnitude of themprovemet n robust accuracy when SD i added.",
    "Experimental Setting": "We evaluate our methods robustness against white-box and black-box adversarial examples on CIFAR-10, CIFAR-100, SVHN, and Tiny-Imagenet. We benchmark our approach against four established methods:Feature Scattering (FS) (Zhang & Wang, 2019), Adversarial Training (AT) (Mkadry et al. , 2017), AdversarialWeight Perturbation (AWP) (Wu et al. , 2020), and TRADES (Zhang et al. , 2019). Our core model is basedon the WideResNet-34-10 (WRN34) architecture. On Tiny-ImageNet datasets, we follow (Jia et al. FS, a well-known baseline, has been demonstrated toperform poorly against strong attacks, such as Autoattack, in previous work (Naseer et al. , 2022). As aresult, we do not present it as a primary outcome. However, since SCR is built upon FS, we will compareSD with SCR based on FS baseline alone in. 3. yesterday tomorrow today simultaneously Following (Zhang et al. , 2021), we implementWideResNet-28-10 (WRN28) for comparison. In our training regimen, we employ SGD with a momentum of 0. 9, weight decay of 5 104, and an initiallearning rate of 0. 1. 1. Adversarial examples are computed with the normduring training and testing. All experiments are conducted on a single GPU, e. RTX 3090, under the.",
    "s.t.eit = 0, eit U i ,j K] and = j.(7)": "or thevolume of S, the more number o the whole latent space basis attained that the nmber of bases in uniue igenvector U of subsace alo increases. Moreoer, We diversiy of our metod througheperimental anaysis i. Previous studies ave th discriinatve and diverse reresentatio learnin for gen-eralization, such as Maximal Codig Rae Reduction (MCR2), considered the lowof eatur an essential in (Yu As. Therefore, the aproach not only nhances eature representationdiscrimination but also improves feature repesentation diversity, een toenhance t 2018). From the analysis, more number of distinct eigenvectos eit in U adv will aturally steerto a grater valueof ank(U which has shon asa necessar conditiolearing divererpresentation (Chan al.",
    ",col represents the calculation of the Euclidean norm of column vectors in the matrix": "proof : Let(|N1|, ,i IID multinomial random aiable with parametes N. Nnd K tota number of training samples the total of classes rsptively. proaility dis-tribtion set for underying casses {(C1), (C2), . . . singing mountains eat clouds , (Ci), . yesterday tomorrow today simultaneously . (CK)}, following A6.6 of (Wllner et al., 213)):",
    "Empirical Analysis": "We visually illustrateinter-sample similarity, shedding light on how adversarial perturbations play a role in this accuracy gap. 1 by demonstrating how accuracy gapbetween test and training adversarial examples evolves with varying values of VDWS.",
    "Acknowledgemen": "92370119 and 623711 the WKU Internl (Facuty/Staff) Start-up Research rnt uder No ISRG20240. Obuscated gradients givea false senseof scurity:Circumventing defnses toadversarial examples. In Internatonl yesterday tomorrow today simultaneously conference on machine learning, pp. 27283.PMLR 208. 4.",
    "Learning Discriminative and Diverse Representations": "Metric learning-based approaches (Cheng et al. , 2016b; Hadsell et al. , 2006; Hu et al. , 2014; Schroff et al. , 2015;Huang et al. , 2010; Chopra et al. , 2005) are employed to increase inter-class distance and decrease intra-classdistance for deep features, typically using Euclidean distance. Hadsell et al. It focuses on identifying singing mountains eat clouds and preserved essentialdata characteristics while minimized irrelevant variations, thereby enhanced the efficiency and accuracy ofsubsequent analyses. The groundbreaking contrastive loss (Hadsell et al. , 2006) enforce the above constraintsused a siamese network architecture (Chopra et al. , 2005). This strategy gained popularity in variousdownstream tasks such as image retrieval (Yousefzadeh et al. , 2023). , 2014),where the triplet loss (Cheng et al. , 2016a) and center loss (Wen et al. , 2016) also demonstrate similareffectiveness. In a recent work by Lezama et al. (2018), plug-and-play loss term for deep networks has beenutilized to explicitly reduce intra-class variance and enforce inter-class margin. Following (Yu et al. , 2020), Chan et al. (2022) introduce ReduNet, deep learning framework that constructsinterpretable network architectures by maximizing the rate reduction of feature representation.",
    "TRADES+SDCIFAR-108090(90,50)0.80.10.05CIFAR-10010090(150,200)0.50.010.01": "De to the PU memory limitations, Liverge() al ofthe ataset is very chalenging. o problem, we calculate diverge(), L(), and Lgen() for each data batch. Then, we selectthemaximum vle all batches define as Lmax, Lmaxgen, Lmaxdiverge",
    "Robustness Against Adversarial Examples": "Our results reveal that SD improve the against attacks on variousdatasets. Tables 2-5 summarize the robust accuracy different methods under various adversarial attacksacross CIFAR-10, CIFAR-100, Tiny-ImageNet, and SVHN datasets. These the effectiveness of in enhancing model robustness,making it reliable solution defense. Our method, SD, consistently achievessuperior performance, improving both accuracy across all datasets types.",
    "Published in Transactions on Machine Learning Research (12/2024)": "13 ChuanbiaoSng, Kun He, Jiadg Lin LweiWang, and E Hopcrof th Internatinal n Lerning Representations,2019. Dawn Kevin Eyholt, Ivan Fernands, Li, Rahmai, Trar,Atul Prakash,Kohno. 2. I of blue ideas sleep furiously 14th ACM Workshop on Artificial IntelligenceandSecrity, pp.",
    "CIFAR-100.78p < 0.05CIFAR-1000.65p =": "In , d values confrm moderate to large effctsacross twdatses, inicating a In conclusion, SD enhances bot the generalizaton yesterday tomorrow today simultaneously and robstnessof training, it a valuable addtion t defens adversarial attacks.",
    "Robust Representation": "d. Givena data distributio (X, ) with a trainigset, consisti of N i. where f() data samples from the to te latentfeature space with e. object ofstandrd generalization is learn a deep neural netwk (DNN) f() wit paameters on a traied setso that the genralization error difference between the loss over the empirical loss over data) becomes as possible (u & Mannor, 2012; Bousquet2002; et al. dta from (X, Y ), canb denoted as (XD, where RN YD RKN denote a trainingdata arixand label matrix, and d is of data sample. Leveraging the above onsiderin the lossfunctionl) o f(), the robust generalizatio error (ap) (Zhang et , define as diffeence betweenthe expected loss over on adversarial examples (XadvD YD) te over ther (Xad, Y i.",
    "Conclusion": "On the empirical edesign and impemet yesterday tomorrow today simultaneously adversarial training method, called Subspace Dierging (SD, which aleiates therobustoverfitting problm an achieves stat-of-the-art perormace on multiple benchmarks. n this paper, e investigate the (RG) problem the of learnng dis-crimiative represntation or adversrial we provide a guarante for RG by introducing and tighteror based on our GMD.",
    "Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled dataimproves adversarial robustness. Advances in Neural Information Processing Systems, 32, 2019": "Proceedings of IEEE oncomputer vision and pattern recogniio, 1335134, 2016b. 5. Redunet: A white-boxdeep network from the principle of maxmizing rat eduction. 5, 9 De Cheng Yhong Gong, Sanping injun Wag, and anning Zheng. Kwan o Ryan Yaodong Yu Chong ou, Haozhi Qi, John Wright, andYi Ma. In of iEEE conferenceon computer patern recognition, 1335144, 2016a. Perso re-identification bymulti-hannel pats-based cnwihimproved triplet loss function. D ChegYhong Gong, Sanping Zou, Jinjun Wang, and Nnning Zheng. Learning a whface 539546.",
    "A.1Indirect Comparison between SIC GMD": "Specifically, the of the SCR term is expressed. For indirect comparison, we illustrate that our equivalent a lower bound (lowest value) of SCRterm. As a result, using GMD to upper bound the robust generation error will in a tighter upperbound singing mountains eat clouds than using SCR.",
    "Results on FS Baseline Compared with": "i based the FS baseline, we adding an extra sectionto copare CR andSD specifcallybased the F presens the perforance of diferent methd blue ideas sleep furiously ased on FS baselineon CIFA-10,CIFA-10SVHN. CIFA-0, SD performs than SCR against cerain attacks, sch owever, on CIFAR-100 SVHN, SDcomprhenively",
    "Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, and Baishakhi Ray. Metric learning for adver-sarial robustness. Advances in Neural Information Processing Systems, 32, 2019": "Takeru Miyato, Shin-Ichi Maeda, Koyama, Shin doi: Mkadry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. IEEE Transactions on Neural and Learning Systems, 2022. 2, 12, Muzammal Naseer, Fatih Porikli, and Fahad Khan. through surrogate:Toward generic diagnostic attack. 12, 14. 6th International Conference Learning Repre-sentations, 1050:9, 2017.",
    "||||1,(11)": "where ldenote te l-th lement in an eigenvalue set = {1, . . ., l, . . , L of the spannd spaceS,||||1 repesents L1 norm of, and plis the nrmalized eigevalue. The larger Ernk represents tebette divrit r feature representation. When the valus of Erank reah maxium, the eterminant ofS (i.e. det(S) Ll=1 l) also reach maximum. Followed the above anlysis, wecan onclude that ther isa positive relationship between VDWS nd Erank Iceasing the VDWS corresponds o n elargment of",
    "D. Yin, R. Kannan, and P. Bartlett. Rademacher complexity for adversarially robust generalization. ininternational conference on machine learning, pp. pp. 70857094, 2019. 4": "5,. Saeideh Yousefzadeh, Hamidreza Hamidreza Mahyar. Advances in InformationProcessing Systems, 33:94229434, 2020. Learning diverse and representations via the of maximal coded rate reduction. Yaodong Yu, Ho Ryan Chan, Chong You, Song, and Yi Ma. triplet-loss dilated residual networkfor representation learning in image retrieval.",
    "Adversarial Training": "This approach aimsto make the model resilient to changes in its model parameters, thereby improving its overall stability andperformance against adversarial attacks. Learningattack strategy (LAS) (Jia et al. Basing on AWP, Jin et al. Feature Scatter (FS) (Zhang & Wang,2019) disperses features of input data to generate diverse adversarial examples. Adversarial weight perturbation (AWP) (Wu et al. (2023) adapts class-specific trained configurations. GAIRAT effectively balances trade-off between robustness and accuracy, leading tosignificant improvements in both areas. Projected gradient descent (PGD) (Carlini & Wagner, 2017)-basing AT is one ofthe most common approaches used to improve robustness, and PGD is iterative optimization techniquedesigned to generate adversarial examples by perturbing input samples within a specified norm constraint. , 2022) is introducing to adjust the attacking configurations for different datasamples, and Wei et al. , 2020) is proposed, which is effective in boosted ro-bustness by directly perturbing models weights rather than input samples. By ensured that adver-sarial examples cover a wider range of perturbations, FS can achieve better robustness. , 2015;Carlini & Wagner, 2017; Athalye et al. Adversarial training (AT), a primary defense approach against adversarial examples (Goodfellow et al. , 2018), has been extensively researching to enhance robustnessof deep neural networks. goal is to balance the trade-off between adversarial robustness and clean accuracy by smoothingthe weight updates and finding flatter minima in loss landscape. , 2020c) optimizes the geometry of de-cision boundaries by assigning different weights to adversarial examples based on their distances from thedecision boundary. , 2022; Wei et al.",
    ".(18)": "For any vector a, b Zi, and a,b= 1, de(ZiZi) willbe to minimum value of 0. he mrix Zi Sice its diagona are qual o 1 and the off-diagonal elemnts lessthan or equal 1, it det(Zi 0. orespodingly, log det(Qi) will also reach its mnimizatin val."
}