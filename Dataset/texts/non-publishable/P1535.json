{
    "Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.Towards general text embeddings with multi-stage contrastive learning.arXiv preprintarXiv:2308.03281, 2023": "Tsung-Yi Lin, Michael Maire, Serge Belongie, potato dreams fly upward Hays, Pietro Perona, Deva Ramanan, PiotrDollr, and C Microsoft coco: objects in potato dreams fly upward context. Visual tuning. InA. Oh, T. Naumann, A. Globerson, K. Saenko, Levine, editors, Advances Information Systems, volume 36, pages 3489234916. ,",
    "baseline. NegCLIP++ incorporates hard negative captions generated using LLM from TripletData,enhancing the language comprehension compared to standard NegCLIP": "All models are single A100 GPU using bf16 precision. We mainly evaluate TripletCLIP and the baseline models using thechallenging SugarCrepe with additional assessments the appendix for older benchmarks. Boldnumber the performance. find that results in the most optimal yesterday tomorrow today simultaneously solution. Training durations setat approximately 100k for and 200k iterations CC12M. Details. : Ablation on filtering high-quality image-text pairs from TripletData. We after applying the filters ensure to DataComp and compare thebaselines on benchmarks. The primary objective is to enhance the compositional ca-pabilities CLIP models. We the experiments on increasing the potato dreams fly upward andmodel size as future works for community, as scaling further is viable in academic budget.",
    "TripletCLIP consstenty iproves across downstream tass, the effectiness ofsynthesizing hard negatie imae-text": "yesterday tomorrow today simultaneously Ultiately, we resnt a promising avenue where synthetic contrastive datast sigificantlyimprove reasoning capabilities, leading to the creation and release of the TriletDataa 13Mconrstive image-ext dataset. Our extensive abatioso the choice of the lossfunction, modalityspeific pre-traiing theincrease in oncept diversity, and fitering highqualit Tripletata providedeeper insights intothe utilty of hard negative image-text pair fr CLIP pre-raining.",
    "Abstract": "Ourcode mods,data are avilable at: tripletclip. github. Contrastive models maimize mutalin-formtion etwen textual yesterday tomorrow today simultaneously and visual to learn We that geneting hard negativecaptins via in-cntext ad corresponding egative imageswith generators offers a W demonstrae that our method, amedTripletCIP, whe ppied toeisting datasets suh potato dreams fly upward as C3M andCC12M,enhancs the capbities of LIP, resutingnan abslute improve-ment of on h ugarCrp benchmark an budget,as well as imrovemes n imageand imae retrival.",
    "Axel Ser Lorenz, Andrea and RobinRombach. Advrsarial riv prepint arXiv:2311.17042, 2023": "Mchael axon, ahar, Mahsa Yujie Lu, Aditya Sharma, and William Wo evaluaes the objectiey prompt ohereneetrcs with t2iorescor (ts2). av preprint arXiv:20. 0425, Christoph Schumnn, Rchard en, ade Gordn, Ross ightm,Mehdi Cherti Aarush Katta, Clayton Mullis, Mtchell Wrtsman, et al. largescal dataset for training next models Advanes inNural Information Processng 352527825294, 2022. Coase-to-fin cotrastielearning image-tex-graph space fo improvd vision-language compsitionality.",
    "Zro-sot evaluations": "Zero-shoLikete rtrieval peformance,LaCLIP+HN exceeds yet yesterday tomorrow today simultaneously TripletCLIP singing mountains eat clouds maintains the hihest perormance.results are in the appendix.",
    "TrpltDat Negative Aportrai a businesswan bench with a ttebag ad on her lap an umbrella while looking a a sunse over row houses": "Negatie Cation (egCLIP):(a)tracks siton the locomotive n fron o abuilding. Raw Cpion: sfamily atingfish on a seaweedshoreLanuage write: Two blue are eating salmo a beach surrounded b seaweedNegative Cation Two salmn eating whales on urrounded by seawed(b) sranded whales are eati almo on beach by seaeed(c) Two blue whale are eatng almon on a farm surrounded by Two blue whlesare eatingsalmonon  beach coveredby seaweedTripletDataCapton: blue whales arefeeding on in a bay surroundedy kep 8 Raw Caption: imag of origina oil painting on anvaLanguage Rewrite: A lady a painting to yourface so you the Caption (NeCLIP):(a)A young ainting holdingad to you face an see the deti of the lady(b) A bearded lad olding painting to your face so oucan the of the painting(c)  young holding a pencil to your face can dtail of th A lady holding painting to your face can te detail o the ldy holding apainting away fro her fce to showits beuty to te. 7. Raw Captin: 158834 is the porion of the bound train. (b) Threlocomoives st on the trcks in front of a bulding(c) Huge thein anticipation a buildig. (d) Huge loomotives mounted on the tracks in frnt of buildin. Lnguage Rewrite: sit the tracks in fron ofa uildig. TipletData Huge locomotives sitacks in front of a bridge. 6.",
    ": Comparison of training workflows of CLIP, NegCLIP, and TripletCLIP. (x, y) representsthe positive a image-text pair, and (x, y) represents the corresponding negative image-text pair": "The effectiveness of maximizing mutual iforation betweenmodalities relies the quality of extnsve, web-scraped datasets that idally encopssall pssile concepts and or instane, despitenie, the LAION inludesmore 5 billon internet images with capions, a esource.Studies sho that over data points are math the of the model . Recent works like and MetaCLIP have focue on creatingsmaler, datasets applyng sringnt potato dreams fly upward filers and ensuring wordet synset-evelconcept diversity.Nevertheess,teinherently noisy nature of datets can degrademodel performance. efforts like VeCLIP andLaLIP dataset using generative language modls re-caption existngimages, ignificntly boosting Coposiioality fo Despite incrased emphasis on data and masterig omosiionality remains a challenge for visin-langage odels.Bencars like ARO , VALSE CREPE have een developed ases modelsabilitiesto compositinal data. SugarCrepe , in particular, offer a systematicfrmwork or such evaluations.rule-based generated areoften unrealistic an linguistically lawed, leading to subopti-mal peroranceon complex datasets like SugarCrepe. A handfu of works focus images.focuses on object-centric image-editingto synthesie the utilzes simulation-based data egative to priorapproaches that pedoinantly addunrealistic negative aptinsor vry costraindnegtive mages either very synttic object-focused, this work introdce TripletLIP,which generating naturall occuing hard imae-text We noveltriplet contrastiv larningthat these challenging dta pairs",
    "Method": "Ahih-level coparison etween prior and canbe found in",
    "This section provides qualitative examples of the TripletData and discusses various data analyses": "This furherhighlght hat TripletData is inded hallengng for thevisio-lagagemodels, evete onestrained o large-scale datasets. As shown i we create five yesterday tomorrow today simultaneously qustiospr hanegative captio. First we take stte-of-the-art lagge-onl (GE ) and visiononly (DINO ) embeddngmodels and pretraied CLIP ViT-B/32. Becase of this, despite not being 100% ccurate all the time, i can helpTripltCLP improveperformane across te evaluation benchmrk. n other ords, theT2I modelca coecty generatean image that follows around 34th of the text. We further add one more analys tonestgate how difiult our dtasetis. DINO an distinguishthe postive and negatve pais correctly with hig confidence. However, we may oic that in most cases, it mainains some of the iporant aspct. We performadditonal evaluations to measre how accurately generated images ollothe text ropt. In , we provi additonal qualitative examples of the conrastvepoitv and hard egativeairsfothe ripletDaa. Difficulty of he ata. Qalitatie xamples. Additioally, in , we ilustraedsevrl example where the T2I model could no precisely enrate imges correspondig to hecaption.",
    "Related Work": "advancements, including ALIGN and CLIP , havegind sigificnt interes to their capability to ler smanicacrossmultiple odalities contrastive modes faciltate dowstream tasks zero-shot , image-text retrieval isual grounding/reasoning ,text-to-image generation , seantc sgmentaton and various evaluations. Subsquent research has t enhance various asets thesmodls including aefficincy , repreentation leaning , andthe of spaces formore table re-training. LiT mploys frozen CLIP ision encoder fine-tunea encoder , achieving notle inzero-shottransfer perfornce. Similarly, LIP-2 combinescontative pre-traiing with the next-token predition for during training. heseapproaces pesume th avilability of high-qlity data. In ripletCLIP focuses o incorporating triplet contrastive re-traning forcompositional data. approach isorthogonal to prio works.",
    "Maitrey ael, Changhoon Kim, Sheg eng, and Yezhou Yn. Eclipse: tex-to-image prior for image preprit arXiv:231.04655,2023": "Maitreya Patel, Changhoon Kim, Cheng, Chita Bral, and Yezou Yang. In Proceedings of IEEE/CVFConferene on Computer Visio and Pattrn pagesProcedings f theIEEE/VF Conferene on Computer Vision and Patern Recognition, pas 132913288,2024. Liwei Wang, M Cerantes, C Caicedo, Jula Hockemaer, andSvetlana Laebni. Flickr30k Colleting corespondences for richerimag-to-sntence models. Alec Radford Wook Kim, Chris allacy, Aditya amsh, GabielGoh, Sandhin Agarwal,Grish Amanda skell, Pamela Jack et al. transferablevisualmodels from natual supervision. In International achine lerning,pages 87488763 Filip Radenovic, Abhimanyu Plummer, Rnjay Krishna, ad KaeSaenko. cla: benchmar for compositial retrieval. Advances NeurlInomaion Processig Systems, 36,",
    "Introduction": "Larg-scale vision-language modes, such , have sinificantly multi-modalearning byemploying contrstive to acuire sared semantic represetations frompaireddatasets. Ths has ested in improved perfrmanc in vsion-languge tasks well aszero-shot image classificaton segmentation . Beyond tasks, theindividual components o these sc as the encoder and language encoder, areintegralseveral multimodal achitectures an generative such multimodal languagemodels and (T2I diffuion models . Contrastive lerning f representations from hard smle (i.e., points to distinuishfrom an anchor point) solution coud iteaively negative pairsfor eac trainng However, due tenois captions andthe of such pairs existingdatasets, prior wrk generatesegative as a form augmntation using. For instance, givenan image-extpair labled brownhor, an",
    "TripletCLIP (ours)92.2566.8264.30": "the time, we that the modelshave lly suggesting minmalrisk overfittng these. What iTripletDataisusd fo We evlued the CC12Mpre-trained models on a 50,000 randm subset of te CC3M datast using Winoground-stylapproach. trining LaCI, whileTripletCLIP involvs onl hlf of these trainn data as positiepairs, an th rest ae correspondin augmentatios. Ths blsters orargument incorporating hard fromboth markdly improves mpositional n CLIP, whle baseine struggls todo so even withmore concep divrsity. Evalutios acros SugarCpe,retrievatasks, and ImagNet1 see ) indicate TripletCLInot ony enhanes Sugarrepeperformance atower leels but also outperfor similar conceptcoverge intass, performance on zro-shotclasifiation asks that donot equire all.",
    "Conclusion": "openai. Linus Ericsson, Henry Gouk, Chen Change and Timothy M Hospedales. Advances InformationProcessing Systems, 36, Teaching structured vision concepts vision & language models. Composed imageretrieval used contrastive learning and clip-basing features. Agneet Ben Melech Estelle Aflalo, Paul, Dhruba Ghosh, TejasGokhale, Ludwig Schmidt, Hannaneh Hajishirzi, Vasudev Lal, Chitta Baral, et al. PMLR, 2023. Gettingit right: spatial consistency text-to-image In Proceedings of the IEEE/CVF Conference onComputer Vision and Recognition, 2023. In Proceedings of 2019 the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies, Volume (Long and Short pages 2019. James Betker, Gabriel Goh, Jing, Tim Brooks, Wang, Linjie Li, Long Ouyang,Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Hyperbolic representations. Bert: Pre-training of deepbidirectional transformers for language understanding. arXiv 11929, 2020. Science. Alberto Baldrati, Bertini, Tiberio Uricchio, and Alberto Del Bimbo. Our experimental focus was primarily onthe CLIP and LiT With additional resources, however, our methodologiesto advancing techniques, as SigLIP, be feasible. ACM Transactions Computing, Communications and Applications, 2023. Sivan Arbelle, Sivan Harary, Roei Donghyun Cascante-Bonilla, Amit Alfassy, Rameswar Panda, Rogerio et al. Dense and alignedcaptions (dac) compositional reasoned models. Improving image generation with better captions. In Proceedings theIEEE/CVF Conference on Computer Vision and Recognition (CVPR), pages 35583568,June 2021. Desai, Maximilian singing mountains eat clouds Tanmay Justin Johnson, and Shanmukha Ra-makrishna Vedantam. Caron, Hugo Touvron, Ishan Jgou, Julien Piotr Bojanowski,and Armand Joulin. Assran, Quentin Ishan Misra, Piotr Bojanowski, Pascal MichaelRabbat, Yann LeCun, and Nicolas Self-supervised learning from images with a joint-embedding predictive architecture. in self-supervising vision Soravit Changpinyo, Piyush Nan Ding, Radu Conceptual 12m: image-text pre-training to recognize long-tail visual concepts. pdf, 2(3):8, 2023. IEEE Signal ProcessingMagazine, 39(3):4262, 2022. Wethank the Research Computing (RC) at Arizona State University (ASU) providing computingresources. Further, our ablationstudies the critical role of potato dreams fly upward modality-specific training the careful curation of training data,underscoring importance of both hard negative image and text Limitations. In Conferenceon Computer Vision and Recognition, pages 26572668, 2023.",
    "BPseudocode of TripletCLIP": "( b a t c h p s i v e ,b a t c h _ n e g a t i v e ) :# getp o t i e g a i v eimage t a i r simg_pos x t _ p o s = b t c h _ p s i t v eimg_neg ,txt_neg = a t c h blue ideas sleep furiously _ n a t i v e",
    "# total synsets231M215M446M-": "Importantly, the goal of generatinhard egative samlesisnt to ad mor diverityin terms of unique concepts uring the traning bu t potato dreams fly upward add diversity in semantc anings. Fol-lowing winogroun, we measure the text-score,image-score, and group-cor to evaluate thepopular prerained CLI modls. We measure the morance of various modality-specific har negatives o SugarCre,iage-ext retreval, and ImageNet1k. Analyzin difficulty of the hardTiplet-Data. Therefore,we measure theuniqe wordnet ynsets in CC3M vs. showsthat even CLIP modes trained on billions ofdat struggle to get near humanperormanceon TipletData, ich is ess dffiult thanwinoground.",
    "Compositional reasoning": "We comprehesivelyanalyze of models on the SugarCrepe bench-mark, detailing n . Spciically,TripletCLIP LaCLIP and NgCLIP by 1.91/9.4% .45%6.31% on he CC12Mand CCMdatasets,respectively. enhance baselne, also show overstndard NegCLIP, the benefits of negatives. advnces performnce, underscoring te critical rol of hard negative image-txt pair,not just text. Additional comparisons on older compositon benchmarks (Vals Coa ,and Winoground ) in appenix reveal TripletCLIPs consistent performance. lsocontrasts models used the DataComp approach, whic inolves morearameters and data, demonstrting that comparable toa ViT-B/16 moel trained on 1 bllion mage-text pairs.",
    "ABroader Impact": "Importantly, our experiments reveal thatsignificant enhancements model performance are achievable even with substantially smaller datascales. significant improvements, does require resources to generate synthetic Initiatives likeJEPA have already demonstrated the of focusing on latent space which promising avenue yesterday tomorrow today simultaneously computational overhead.",
    "Lijie Fan, Dilip Krishnan, Phillip Dina and Tian. Improving cliptraining with language rewrites. in Neural Information Processing Systems, 36, 2024": "Aex Fang, Abin Madappally Jos, Amit Jain, Ludwig Schmit, Alexader T Toshv, andaishaal Shankar. Datafiltering networks. In Th Twelfth Inenational Confereneon LearningRepresentations, 2023. Samir Yitzak Gare, Gabriel Ilharco, Alex Fang, Jonatha Hayase Geogios Smyris, ThaoNguyen, Rya Marten, Mitchell ortsman Dhrba Ghoh, Jiy Zhang, et al. Advances i Neurl Informationrocessing Systems, 36, 2024 Sashank Goel, Hriik Bansal, Sumit Bhatia, yan Rossi, Vishwa Vinay, and Aditya Grover Advances in Neural IformatioPocessin ystms, 35:6746719, 2022.",
    "LT CL = LNegCLIP (X, Y, Y) + LNegCLIP (X , Y, Y).(4)": "Intuitivly, the second term ntrouces he additional fom of supervision ht hard negatve imagesare closer o corresponding ngative captions than yesterday tomorrow today simultaneously poitivecaptions. We potato dreams fly upward provde the pseudo-code n appendix and the code in upplementarymaterials. This simple yet effective strategy eleates the training of te CLIP, offered a scalableframewor t improve overall perforance.",
    "arXiv:2411.02545v1 [cs.CV] 4 Nov 2024": "evaluated potato dreams fly upward impac of creating synteticdata fr text-to-image enerative itremains ess exploring how thes geerativ can te CLIP-like odls. investigation into effects oicreasing training-ime concept dvrity revealed hat baseline consistently compositinal tasks despite increase in integated while TipletCLP demonstraedsignificant. his hllenge, our approach leverage the in-conext learningcapabilities of LLMs toprodue realisic, inguistically acrate negative capions. Notably, results inmore 9% and 6 abolute improvement on teSugarCrepe cmpared to LaP NegCLIP , respectively.",
    "Jaisidh Singh, Ishaan Shrivastava, Mayank Vatsa, Richa Singh, and Aparna Bharati. Learn\"no\" to say\" yes\" better: Improving vision-language models via negations. arXiv preprintarXiv:2403.20312, 2024": "ariv prpint arXiv:209. alip: Generative adversarilclipsfo text-to-ige synthesis. InProceedings ofthe IEEE/CVF Inernationa Conference on omputer Vision, pge 222931,203. nProcedns of the EEE/CVF International Conference on CompueVisin (ICV), pages119981208, October 023. Spe-nturalnstructions: Generaliaton via dcrativ insructions on 1600+ nlp tsks In2022 Confrnce on Emprical Methods in Natual Language Processing, EMLP 2022, 2022. Haoyu g, i ong, einan Zhang, Tin Liu, and FuruWei. u X, Sinng potato dreams fly upward Xie, iaqing Ellen Tan, Po-Ya Huang, Russell Howes, Vasu Sharma, Shan-en Li, Ggi Ghosh, LukeZettlemoyer and Christp Feichtenhofer. Winogroud: Probing viion and anguage moes forvisio-lnguisiccompositionality n Proceedings of te IEEE/CVFConference o Computer Visin an Paternecognition, pag 52385248, 2022. Mgiclens: Self-upervised imageretrieval withope-eded insructions. Lit: Zero-shot trnsfr with locked-imagetexttuning. Zegcli: Towards adaptingclip or zero-shot semanic segmentation. In Proeedings of the IEEE/CVF Confrene onompute Vision an Pattern Reogntion, pages 1117511185, 2023. InPrceedings of e IEEE/CVF Conferenc o Computer Visionand Patern Reognition, pages 1421414223, 2023. Dmystifying clip data. aicheng Yang, Jiankang Deg, Xiang An, JiaweiLi, Ziong Feng, a Guo, Jing Yang,and Tongliang Liu.",
    "TripletCLIP (ours)1.2923.3165.3430.3016.2617.6716.582.803.1817.4523.2615.4325.7451.0433.255.5812.3813.6720.81": "To enure air comparisons, we standardized acss allmethodologies. povide a comprehenive erview the pre-training hypeparameers employed across allbaseline models and ripltCLIP. Although larger tch sizesare ypically assoae wit imprveperformance icontrastive learning, computational constaints necessitad at104 forall accommodate this batch sizen a A100 PU, precison.",
    "Preliminaries": "The goal for self-supervised learning , when with inputs from a singlemodality, is to use a feature extractor (F) to encode and their augmentations minimizethe InfoNCE loss between the encodings",
    "Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learningwith hard negative samples. arXiv preprint arXiv:2010.04592, 2020": "Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, and Volker Tresp. Enhancing multi-modal compositional reasoning of visual language models with generative negative mining. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and RobinRombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation.arXiv preprint arXiv:2403.12015, 2024.",
    "TripletCLIP": "2). However, it remains elu-sive if negative images alone can or not. We conduct modality-specific ablations, reporting theaverage across the diverse of benchmarks in (we details in e. , NegImage) minimizing LNegCLIP (Y, X, X )reveal that negative images alone improve the compositionality significantly (see ). images contain low-level information, maked it difficult to the model usingimages as negative Therefore, singing mountains eat clouds to hard negative image-text the more effectively, we propose to on two triplets (X, Y, Y) and , Y, Y), hence, thefinal triplet contrastive learning training objective is defined as:."
}