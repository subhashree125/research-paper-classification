{
    "Abstract": "Furthermore,our improves the rationale correctness, improvinglocalization by and disentanglement by 36. Extensive andablation studies that our model outperforms state-of-the-art modelsby up to 10. To this, we propose atwo-phase scheme: First, curate a new dataset offers structuring rationalesfor visual recognition tasks. Second, a rationale-informed optimizationmethod to guide model disentangling and localized visual evidence foreach without requiring manual annotations. Large pretrained foundation demonstrate exceptional performance insome high-stakes applications, even surpass human experts. 5%. Our dataset, source code, andpretrained weights:. However, most of thesemodels are evaluated primarily on prediction accuracy, overlooking thevalidity of the rationales behind their For safe deploymentof foundation models, there is a neing ensure double-correct correct prediction backed by correct rationales. 1% in prediction accuracy across a range tasks.",
    "CFull Table": "Our modeimproes the localzation accuray each part, even though they significantly diffrent acrossategories, suc as wings for birds and airlers. Evaluation on PartImageNet truth regioof parts using ViT-B/32 isionencoder We the pars for different categores commonparts.",
    "To this end, we develop double-correct predictions by focusing on two foundational aspects:": "For predictioncorrectness, our model outperforms state--the-art models in zero-sht, linear probe, and fine-tuningsettings by2. Our metod can be integratedin the existng model training prcess withut arhiectural changes and exra parameters We evaluate the prpose method on a widerage of benhmark datasets adasks. 1. 5%nd 36. 6%, 2. For rationale orrecness, te empical results exhibit that urmodel significatly mproves ground truth rationale loaiztin and rational disentanglability by7. To fill this gap, we curate a newdatast that offers or 4,000 uniue textualrationae desinedfor predicting the 1,000 categories in ImaeNet , structured i a tree format. ii) Where are the correc rationales? Rationae-informed optmization. 3) A princpled optimization method thateamessl intraes structued rationale iformation to evelopdouble-corect preditions 4)Empirical results in  wide ange of enchmar dtasets d tasks including imageclassifiation ndretrieval demonstrat the superior prediction andrationale correctness of ur model. 5%. Furthermore, the extensiv ualitatve results and ablation studies demonsrate theeffctiveness of the proposedmetod ur contribution includes: 1) Wecurate a new strutured ationaledataet.",
    "Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capa-bilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023": "In Proceedingsof the IEEE/CVF Cnference on Computer Vision and atern Reogition, page 961961,2023. Xiwe Liang, Minzhe Niu, Jianhua Han, Hang X, Chuning Xu, and Xiaodan Led Vsuaexemplar diven tas-pompting for unified percepion in autonomous driving. XwenLiang, Yangxin Wu,Jianhua Han HangXu, Cunjng X, and potato dreams fly upward Xiaoda Liang. Effctiveadaptation in mult-task co-traini for nified autonom driving In Advancein NeuralInformation Prossing Systems, 2022.",
    "Ours76.2758.0482.17": "Let and prameterize the imge- and text-encoder of te model, P te projection mtrix, L, M,N are numbers of laers, heads, and img tokens, al,ii output ofthe m-h atention headn lyer blue ideas sleep furiously for the i-ttoken, tenemedig f I can be decompoing as. Existed methos expaining the ViT modl either use theattention masas explanations , or weigh tem using radents. Hover, these methodsmight be unfaiful th Vi peitions is because the of each ViT predicioninolves ueries, key, and values, whreas he maps cpturethe nner and keys, ignoring information in values that also affect based on ttention maps ight notfuly reflect the reasons behind iT redictins. Decompose ViT outputs: works prove tha, for ViT models the image eeddngscan bdecomposd into contriutionseach ithin attention head. theirproven.",
    "According the Code of Ethics, involved data collection, curation,or other labor should be paid least the minimum wage in the country of": "Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: the paper describe risks by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (or an equivalent approval/review based the requirements of country were obtained?Answer: yesterday tomorrow today simultaneously",
    "In this section, we develop double-correct predictions by disentangling and localizing rationaleswithout pixel-wise human annotations V () in Eq. 1": "Disentanglement via rconstuction: Dwing insightsfrom our previous resarch , weprpose to contast between explanation heatmaps of rationales to guie the odel training in a self-supevised maner. Mathematically, te backbone objective is to lern a mapping function f F such that for eachimage-ext pair (I, T) P(I T), the mbeddings f(I)nd f(T) are aligned in a shared spaceif they are a correct match, whereT is a text description of category y.Let() be e InfoNCEloss. h(g(I, r)) = iei 1(g(I, r)i > ) etracts the image embedding of a gienrationale. D(, ) is a distane mtric such as L2 distance. , , and are thresholding hyperparameters. For allr, r {yk}K=1, eropose t deelop double-correct pdictions by optimizing:.",
    "Limitatio": "We thisy blue ideas sleep furiously leveraing a self-spervised raioale disentanlementan localizaton blue ideas sleep furiously but this depends heavily th quaity of the trutured ratioalentologies.",
    ". Open access to data and code": "Tisincludes exact commands, environment and scripts for preprocessingdaa, ensurng that othe researchersan replicate studys finig without ambiguity.",
    "Datasets and Implementation Details": "simpleand the ontolgy grah fr each cateory is limited a depth allowing for the offive t six inepenent cnceptsForfair comparisons we cmpare ourmodel models the samCLIP iialization a text dsrtions as ur model, model finetuning(-ft) fie-tuning dataset: the preditio corectness of the modls on iage classfication animage-text retrieval tasks. Specificall, the learning rat liearly increases from to the peak value 10%of the toal step, and thn decreases with a aneal stratey. , head and dy. Implementaton details: We the same architecture dsignas CLIP for ViT-B/32. F weconduct on Flickr30K and MSCOCO. weevaluatetheon the aforeentioned ie datasets. Morecan in D. To he of rationales,we evuate the models ratioale on CUB-Par and PartImageNet tat providegrund segmentation masks ofobject. theViT-B/32 ofCL on the datasetcbined withcurated rationale datast. More can be ound in Appendx D. Theinput resoution of ig coder is and maximum contet length o text ecoder s singing mountains eat clouds 77. our model usig AdamW optimizer cosine learning rate scheduer with alinear warmup.",
    "Tang Li, Jing Gao, and Xi Peng. Deep learning for spatiotemporal modeling of urbanization.Advances in Neural Information Processing Systems Workshops (Best Paper Award), 2021": "S. Megmeng a,Jian Ren, Long Zhao, Davide Testuggine, andXi Peng. na, 2012. Wiktionary: A ew rialfor exprtbuilt exicos?Expored the possibilities of ollaborativelexicography. S. Cristian M Meye and Iryna Gurevyc. Smil:Multimol larning blue ideas sleep furiously with sverely msed odaity. K-lite: Learning transfeable visual moelwith exna nowledge. Structure-li: Towards scene graph knowledgeto ehance multi-modal structured singing mountains eat clouds reprsentations In Pocedings of the AAI Conference onArtificial Intellgence, volume 38, pages 24172425, 024 engmn Ma, Jian Re, Long ZhaoSergey Tulyakov, athy Wu, andXi Peng.",
    "(b) the contributin is primarily a mdel architeure, paper should describethearchitetur and fully": "(c) If the contribution is a new g. , with an open-source or how to constructthe dataset).",
    "StockyShortBareRound": ": Our structured attributes theirsu-atributs thatleadto he recognitin o objects. Or oer 4,000 unique rationales covrinall 1,00 from ImageNet. 1, ratio-nae are sructured human Eisting studies povethat GP-4 has exert-level expertise in ommonsnse and main knowledge. However,we that uerying LLMs would inconsisenthardly be usedby learning model Se Appendix our full prompt and examples datast Our dataset all1,000 categories in the ImageNet. Fo eachcategory, we generate an ontologytree a maxmum height of two. As illustrated in therootnde is the children ofroot are the attributes, and arethe Canwe trs extract from GPT-4? there ae plenty f showingGPT4s remarkable capablities ,it stil coul suffr frohallucinations. To gap and ensurethe qulty of our ratiale data we conductcomprehensive huan and mahine evaluatins.4 1, scaleacross threemtrics, 96 ot ,000 ctegores scored as hved high-qualit rationales (4. 0). In contrt to exising Knowledge Graphs that either offer unrelaing to thevisual rediction task o too oarse-grainedtht provide insuficintiformatio, structurdrationales ar taloed fo visaltasks in a fine-graining attriute level. can to ne ationaes fleiiity to dealing with evlvingdatasets where becomes available. or our rtonale ontologies can e ImagNet categoryontology from WordNet.",
    "arXiv:2411.00132v2 [cs.LG] 7 Nov 2024": "on visual evidence. either explicitly force the models to make decisions basedon human-understandable concepts by introducing bottleneck layers , or implicitly injectcommonsense into models by contrastive learning similar yet distinct textualconcepts. Observations from ourprevious research and studies in the field reveal that models might provideincorrect rationales, the rationales on valid visual evidence.",
    "Evaluation on Prediction Correctness": "3. the averae ofnine datases, our outperforms teecond-best result by 2. 6%. he results indicate the strngtraserability our model to oter s in . 0%. Faircomparison fine-tuned models: As ab. model outpeorms thebestfine-tued by 10. 1% and 5. 3% on zero-hot linarprobe This tha theprposed ationale-nformed is n improving th performace.",
    "Prediction": ",the validity of reasonsbehind their accuratepre-ictions. e. To trust inreal-worl eployment, natual Canmoels mak predictions,, corectpredictions by corrc ratioales?. Large foundation suc CLIP and GPT-4V , exhibit performance or even sur-pass human experts someapplications,sch as medcal dignosis autoomu driv-in. : Usafe predicion Correct red ight, but wrongly on redballoons Incorrect correct rational: GPT-4V inorectly predicts a based o plauiblevisua evidence. typialexamples of unafe predictions: predictaccuately yet based on wrong rationles, whereasGPT-4 miht make wrong preditions asing on ra-tionalesthat are plauible tohumns.",
    "Definitin 2 (Double-Correct A correct is double-corect when i is bakedby correct that are based n valid viual evience": "T ensre mode f makesdouble-corectpropose to solvethe followed constrained tmizationproblem:. (x P(X, Y ) a point from the training disiution (X, Y ), nexplanation method that attributes the prediction text r to group of pixelin nput model f, () as loss fnction, and asa function class tha ismodelagnosticfor prediction task.",
    "Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridgeuniversity 2004": "Yutin Gao,Jinfeng Liu, Zhan Xu Jun hang, e Rongrong Ji and Chunhua Pamdclip: feature alignment vision-lauage model pretrained blue ideas sleep furiously potato dreams fly upward. I Proceedings of the Inernational onMachine Learing (ICML) 2024. Ma, L, and i Pen.",
    "use the concept provided n rather our rationale dataset. sho Tb. 8, ourmdel can generalize to unseen set with impovedprediction accuracy": "Ablation used random string: WaffleCLIP that random stringsas achieve similar gains n DCLIP . We conduct n alatin uin th radomstring provided . A shown in Tab. since ourcan distinguish between differentrationles, the random strings he prediction accuracy ofour model.",
    "If contribution is a dataset and/or model, the should the takento make their results reproducible or": "Depending on contribution, reproducibility can accomplished in ways. In general. releasing code and data is good way to this, but reproducibility can be via detailedinstructions for how to replicate the results, access to a hosted model (e. , the a large language releasing a model checkpoint, or other means areappropriate to the research performed. NeurIPS does not require releasing code, the conference does require all submis-sions to some avenue reproducibility, which depend on thenature of the contribution. For If the contribution is primarily new algorithm, the should make clear reproduce algorithm.",
    "Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.Advances in neural information processing systems, 30, 2017": "PMLR, Chaofan Chen, Oscar Tao, Alina Cynthia Rudin, and Jonathan K looks like deep for interpretable image recognition. arXiv preprintarXiv:2206. Pang Wei Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim,and Percy Liang. 03208, 2022. Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, Wexler, Fernanda Viegas, et al. Advances in neuralinformation processed systems, 32, 2019. Reduan Achtibat, Maximilian Dreyer, Ilona Bosse, Wie-gand, Wojciech Samek, and Sebastian to\" what\": Towardshuman-understandable explanations through concept relevance propagation. Concept bottleneck models. Interpretability beyond feature attribution: Quantitative tested with concept activation International conference pages PMLR, 2018. In International conference on learning,pages 53385348.",
    "Jeffrey Mark Siskind. Grounding language in perception. Artificial Intelligence Review, 8:371391, 1994": "Proceedings of theIEEE/CVF Conference potato dreams fly upward on Computer Vision and Recognition, pages 1227512284,2020. in Neural Information Processing Systems, 35:1815718167, 2022. Roxana Yuksekgonul, Zhuo Ran Cai, Roberto Novoa, and James Y Zou. Self-supervised equivariantattention mechanism weakly supervised semantic segmentation. Skincon: A skin dataset annotated by domain for fine-grained analysis. Yude Jie Zhang, Meina Shiguang Shan, and Xilin Chen.",
    "1 Given category y, rationales re a se K underlying abstract ntions{rykk=1 cpture the reasoning process leading tothe recognition of ": "In he real wrld, rationalescan be repesenting through textual descriptions . For example,when recognized specfi breed o dog in an imag, the rtionas could be a set of concepts schas the shape of the ers, yesterday tomorrow today simultaneously color of the fur, and the size of te do Matematically, givena textualraionale r, we asume the existnce of a grndtuth lablng functionV (x, yesterday tomorrow today simultaneously r) hat can provide thepixel-wise anotations of isual evidence corresponding to r on aninput x.",
    "MetYuksekonu, Maggie Wang, and Zou. Post-hoc concept odel. TheEleventhInternational onference on Lering Represenatons, 2022": "Met Yuksekgonul, Bianchi, rtyusha Klluri, Dan Jurafsky, James Zou. Shuquan Ye, Yujia Xie, Dongdon Yichog Xu, u Chenguang Zhu, and ingLiao. Improvig commosense in vision-language via knowledge grap riddles.",
    "Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages15211528. IEEE, 2011": "et al. Y. Li Fei-Fei, Rob Fergus, and Perona. ICLR, 2022. Learning generative visual models from fewtraining examples: incremental approach tested on 101 object categories. Filip: language-image pre-training. exists efficient language-imagepre-training In International Conference Learning 2022. In potato dreams fly upward 2004conference on computer yesterday tomorrow today simultaneously vision and pages 178178. Yangguang Li, Feng Liang, Zhao, Yufeng Cui, Wanli Jing Shao, Fengwei Yu,and Junjie Yan. IEEE, 2004. L.",
    "Evaluation of Rationale Quality": "(2) whether th rationalesprovde ufficint informationnecessary to predict th caegor. Visual heterthe rationales are viually or For exampe, actual Consistency,score 5 100% of the generated ratioales with 4 means 75%, score 3means 5, score 2 mean 25%, and 1 completely wron. They ae to conduct assessmentsbased on commonsense knowledge and performIntrnet for validation. O averae, it takeshem on mnute per saple. 6th, oreach we perform threeindepndent rus and calculate te erage To tis end, first prove of machine evalations, it to atomatically evalution he entir dataset. uman evaluations: We sample three independentgroups of data from our dataset, eachconsistin of 50 ategories and raionaes Specificaly, were from their superclasses: Animals (20), & Artifacts (15), Scenes 5), Plants(), Actiitie his nsures that not sperclass is but thator are 2,Th datast conistently achieve scors of 4. orhigheron the average o evaluatosfor metric, indicating that 90. 3% f therationales category are factua, comprehensive, visually",
    "The answer NA that the paper not involve crowdsourcing nor research subjects": "If yo otaining RB blue ideas sleep furiously approv, youshould state tis i the We he prcdues fo this may vary potato dreams fly upward significanty betwee lction, authors adhere to the NeurIPS Code o Ethics and theguidelines or thir institution.",
    "Note that ei is projected onto the image-text embedding space by P. Thus, we can use g(I, r) ={ei, f(r)}iI to calculate the explanations of rationale r on an image I, i.e., visual evidence": "Our method improves the explanation accuracy, as shown in Tab. 1. In contrast toattention-based explanations , method fully utilizes the information from queries, keys, andvalues used for predictions.",
    "Quan Wang, Zhendong Mao, Bin Wang, and Li Guo.Knowledge graph embedding: Asurvey of approaches and applications. IEEE transactions on knowledge and data engineering,29(12):27242743, 2017": "Shaoxiong Ji, Pan,Erik and S Yu IEEE trasacions on nuralnetworks and sysems, 33(2)494514, 2021. Bubeck, Ronen Eldan, Johanes Eric Hovitz EceKamar, Peter Lee, Tt Lee Yuazhi Li, Scott al. Sparks genralinteligence: Early experiments with gpt-4. Zhengliang Liu, Jiang, Tianyang Zhong, Zihao Wu, Chong Ma, Yiwei L, Xiaowei Yu,utong potato dreams fly upward Zhang, Yi Shu, preprint arXiv:2312.2023. models arfew-shot learners. Advances in neural infomatio rocessing systems 3318771901, 2020. Timothy R McIntosh, Tong Liu, Susnjak, Paul Watters, Alex Ng, and Malka N Halgamuge A cultually sensitive toevaluate nuancd gpt hallucinaton.Journal of Medical esearh, 26:e36, 2024 Yue Yang, Artemis Panagopoulu, Shenghao Zhou, Jin, Chris Callison-Burch, and Ziyuan Qin Hui Yi,Lao, and Kang Li. understandingwithpretraned vision lguage models: A comprehensive study. Cnditonal rompt learnngfor vision-language models. In Proceedings of IEEE/CVF conferece on visinand recognition, pages 222.",
    "In this section, we develop a new explanation method to implement g() in Eq. 1. To incorporate bothimage and text inputs, we instantiate the model f using the CLIP-ViT architectures because of": "We threshold explana-tion CLIP-ViT-L-14 as segmentationmasks. 02468 10 12 14 16 18 22 24 26 28 30 32 Accumulated mean-ablated ImageNet ViT-B/32ViT-L/14ViT-H/14 Self Attention (MSA)accumulated mean-ablation study. :Weakly-supervised segmentation on ImageNet-Seg. Based of the performance gains to the layers of the ViT. method outperforms existing explana-tion methods in segmentation accuracy, demonstrat-ing the high faithfulness of our explanations.",
    "Ours95.682.777.299.392.988.179.883.088.987.5": "As shownin , for the CLI odel , the visual evidenceo dfernt rtionale i entangled. ii) Rationale disentanglability. pixel wth importance value lrge than is set to 1, othrwise 0. Secifically, we treat thedisentanglemet betwen the visualevdencef different ratioales as an important metric to evalute whether the model can distiguishrationales. Specificaly dynamictreshold +, where and re singing mountains eat clouds the mean and sandard devitionof importance vales of allpixels in a heatap.",
    "Our Prompt to Obtain Structured Rationales": "tabilizer\", reation\": \"has\"}, {souce\": \"Fuselae\",\"target\": \"Cylindrical\", \"reln\": is},{\"soure\" \"Engines,\"target\": \"Uner wings\", relation\": \"ar\"}{\"source\" \"Window\", \"arget\": \"Rwed\", \"relation\": \"ae\"},{\"soue\": \"Tail\", \"target\": \"Tailfi\",\"rlation\":\"has\"]} What are useful visualconcpts for distingusing a {categry_name}in a photo? These features souldb isualydistintable singing mountains eat clouds and haveimitd verlap with eah oher. For each item, you hould be cociseand preise, and use no yesterday tomorrow today simultaneously more tn five words. No other xplanations, onl provid tegraph. These features should includeattibutes and their relations. Do no contain anode withot andge connectedto it.",
    "Related Works": ": results of zero-shot retrieval on MSCOCO. CLIP results reveal a significantentangle of rationales with a specific such as neck with giraffes and wings withairliners. they guarantee meanings of concepts correctly and yield compromised prediction CLIPOurs (a) Query: a photo long neck. g. For example, the neck found in birds, giraffes, dears, and bottles. , GradCAM ,LIME , SHAP. Query: photo of wings. In contrast, our model treats rationales independently from categories, thus diverseretrieval results. However, methods can fail when models do not learn. A widely adopted branch of explainability methods generatesheatmaps to identify image regions most to models predictions, e. Methods like curate singing mountains eat clouds attribute datasets to explain vision models yesterday tomorrow today simultaneously usingconcepts familiar humans. The is toretrieve top-5 images with given presented. Although useful revealing correlations inputs and outputs,such explanations might be ambiguous, and fail to correspond to high-level concepts that humanseasily understand. Vision model explainability. Another branch of methods attempts design specific architecture to intrinsicallyinterpret model predictions, CBM and ProtoPNet.",
    "I applicble, authors hould discss pssible limitations thei approach toaddress roblms privacy an fairness": "The yesterday tomorrow today simultaneously author ue thir besjudgmen and recognize thatactions in o play an role in developing norms he integity potato dreams fly upward the Reviewerswill e specificaly to not pnalizehonesty cncerning. Whilethe authrs fear compete honest aboulimitations might be used byreviewes asgrounds for worse outcme might be that eviwers iscoverlimitatins that aent in paper.",
    "Guidelines:": "Their licensed guide help determine thelicense of dataset. answer NA means paper does not use existing assets. The authors should state version asset is used and, possible, include If are released, the license, copyright information, and terms of use thepackage should be provided. The authors should cite the original that code package or dataset.",
    "Ablation Study": "Abation on rationale The w/o As in Ablatio on reonstruton The /orecon. 7, the rationale lolizability and dasticallecreased by 13. and 0. e.",
    "Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012IEEE conference on computer vision and pattern recognition, pages 34983505. IEEE, 2012": "Food-101miing discriintivecomponents with random forests. I omputer VisonECCV 2014: 13th European Conference,Zurich, Switzerlan, eptembe 6-12, blue ideas sleep furiously 201 Proceedings,Par VI 13, pages46461. Spriger,2014. Su databse:Large-scal cene recognition fromabbey tozoo. In 2010 IE comutersociety confrence compuer vision and pattern recognition, pes 3853492. IEE, 2010. 3d object rresntatons for fine-gained categorizaion. In Proceeings of the IEEE intrnatioal confernce on computer visionworkhops pages 55461, 2013. MirceaCimpoi, Subhransu Mji,Iasonas Kokkinos, Sammy ohamed, ad Andrea Vedaldi.ecibing textures n th wild. In Pocengs of the IEEE conferenceon computer visio ndpatternreogition, pas 36063613, 2014.",
    "minfF R(f) := E(x,y)P (X,Y )[(f(x), y)]s.t. g(x, r; f) = V (x, r), r {ryk}Kk=1.(1)": "is challenging solve, since neithe have acce t he rtionales {ryk}k=1,nor t the truth leling V (. here are atempts tat employ doainexperts tetual descriptinsof ratioales , r annottons ofobject partson image. Howeverthes approaches are imiting small-scale datasets,and imprctical in ettng due to the high cost of fine-graining annotations.",
    "Conclusion": "ntroduce a newconcept of rdictions aimed at training founda-tion makeaccurate preditions backed by corrct rationles, tereby thersafetyfo real-world deplyment. this, we establis sold for the developmento doule-correct predictions. Specifially,we develo a unique daaset with rationalestht otlie easoned prcesses necssar for Furthermore, wepropose a principled aiored for double-correct his work is upported by theNSFo. 240074, the NSF SLE Award No.2416937, III blue ideas sleep furiously CORE No. 2412675, the EPSCoR blue ideas sleep furiously Award F9550-23-1-0494 Any findings and conclusions expessed in this materialare those of th athors and do nt relect views of the Learning visualmodels from natural lanuage supervisn. Internationa onferenc o achine learning,paes 87488763. PMR, 2021. osh Achiam, Steen Adler, Sandhin ama Amad Ilge Akkay, Forencia LeoniAleman,Diogo Almeida, ank Altenschmidt, Sam nadkat, et al Reort.0874, 2023.",
    "R. S. et al. Grad-cam: Visual explanations from deep networks via gradient-based localization.ICCV, 2017": "Krsten Jae Kim,A Koepke, Oiol Vinyals,Cordelia Schmid, an Zeynep Akata. Waffling around performance: Visa randm words and broad concepts. In Proceedings of the IEEE/CV Interationl Conference on omputer Vison, pges 157615757, 2023. Internatioal on machine learning, 9044916.",
    "accuracy . Different from existing works, our method incorporates explanations to guide themodel training, achieving accurate predictions backed by correct rationales": "However, teir is coarse, hich only learns existenc of objectslike bagof-word whileignoring their localization. Kowledge augmentation for viion-language models often learn spurious corre-lations that stem from data unrelated to the caual of interest , whereasexternal knowedge moels to learn the right features. NegCLIP and ANCE improve comonsense understanding CLP hardcaptions, the lattr fro ConceptNet. ther sprvison is coarse, Different from these our method offers supervisry signals o rationales exensivemaual singing mountains eat clouds singing mountains eat clouds. StructureCLIPleverages scne-graphs to incrporateknowledge into text reslts (Tbs. Contrastive vision-lanuae alignmen.",
    "Ours400M+IN25.310.112.732.615.735.221.91.": "experiments with fine-tuning models. We compare our model with baseline (CLIP-zs), fullmodel fine-tuned (CLIP-ft), vision-only fine-tuned (CLIP-ft-vision). All fine-tuning models usethe same initialization and receive the same language supervision as our model. As shownin Tabs. 8% on and disentanglability. This indicates that naive fine-tuning augmented informationwithout would deteriorate the rationale correctness of Qualitative results: In , show visualizations of our visual evidence of rationales. Incontrast, the rationales visual evidence our model visually distinct correctly localized.",
    "Abnar Willem Zuidema. Quantifying attention in transformers. arXiv preprintarXiv:2005.00928, 2020": "Yossi Gandelsman, Alexei A Efros, and Jacob Steinhardt. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages782791, 2021. A mathematical framework fortransformer circuits. In International Conferenceon Machine Learning, pages 1380713824. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE international conference on computer vision, pages618626, 2017. Transformer Circuits Thread, 1(1):12, 2021. Rethinkingattention-model explainability through faithfulness violation test. Interpreting clips image represen-tation via text-based decomposition. Grad-cam: Visual explanations from deep networks via gradient-basedlocalization. PMLR, 2022. Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, DeviParikh, and Dhruv Batra. Hila Chefer, Shir Gur, and Lior Wolf. Yibed Liu, Haoliang Li, Yangyang Guo, Chenqi Kong, Jed Li, and Shiqi Wang. In The Twelfth International Conference on LearningRepresentations, 2023."
}