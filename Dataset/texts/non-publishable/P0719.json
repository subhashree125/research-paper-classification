{
    "Generative, causal or autoregressive language models areused interchangeably in this paper": "the models are based on BERT architecture andtested for language discriminative tasks. Recently,researchers have proposed several standard evalu-ation benchmarks on a collection of low-resourcelanguage datasets yesterday tomorrow today simultaneously for language generative tasks(Ekgren et al., 2022; de Vries and Nissim, 2021;De Mattei et al., 2020; Antoun et al., 2020). Forinstance, Google released a comprehensive bench-mark, BIG-bench, for over 200 tasks on languagegenerative tasks (Srivastava et al., 2023), amongwhich there are only two tasks that contain theNorwegian language, namely Which Wiki Edit tomatch a recent Wikipedia revision to its correspond-ing edit message, and Language Identification tasks.They only cover very limited Norwegian samples.Later, Luukkonen et al. (2023) filtered Finnish fromBIG-bench to build a Finnish benchmark for gener-ative LMs. However, these existing evaluation dataeither originate from pre-existing English datasetsthrough machine translation or lack the evaluationdata types required for assessing LLMs on multi-task reasoning.",
    "Metrics/Models NorGPT-369MNorGPT-3BNorLlama-3BNorGPT-3B-continueNorGPT-23BNorGPT-3B-RLHFNB-GPT-J-6BGPT-3.5": "Addi-tionally, this trend is evident in common test sam-ples, where GPT-3. This is supporting by highest scoresin MAUVE and BLEU. 5322. 5619. 3220. 3597. 610. 4423. 38ROUGE-120. 0511. 382. 1321. 6426. 2621. 620. 6294. 3294. Although the model withreinforcement yesterday tomorrow today simultaneously learning may not always potato dreams fly upward surpass thefine-tuned model in accuracy, it actively strives tomimic human writing patterns. 682. 414. 640. 6692. 030. 4392. 6324. 905. 0125. 654. 6819. 1896. 5021. 4197. BLEU2. 9720. 721. 354. 2524. 13MAUVE0. 750. 570. 28Dist-495. 5-Turbo tends to generate moreformal language compared to conversational lan-guage. 38 their datasets likely include diverse range of news-papers, magazines, and government reports. 3112. 00ROUGE-L19.",
    "Schuster, Adam Fisch, and Regina Barzilay. 2021": "Emily Sheng, Kai-WeiChang, Premkumar Naarajan,an Nanyun Peng. Get your vtmin fact withcontrastive evidence. 2019.",
    "Wang et al. (2023a) developed an element-aware summarization method using CoT approachby instructing LLM to generate four key ele-": "We instructing anoator to conider questiondiersity, including bothsimple quetis (where te ansercomes from ingle sorce) and complex questions (wherethe answer isdevedfrom iffernt partsof teartice). Thonly otential isue with self-dialgue is tat different annt-tors may have varying intersts in the artcle and may xhibitperoal writed styles during anotation. 16 mentsEtiy, Dat, Event, and Resultto be in-terated into the summay.",
    "Benchmark nMulti-task Datasets": "(020) MAINF, a jointlylaeled Chinese dataset for an the maternaandinfant dmain. Xu et l. , 220;al. Asanguge models become mecapabl handlnglongertexts Brown et al. Most existing bencmarks focus o single tasks,such as question cloe tests,summariza-tion, Fine-tuning languge od-els on indivdual datsets acks persuasiveess invaluating their acrossmultpletasks. 202,datses short texts my relabl predictthe transformative potential of Adiinally,the annotating tasks in MATINF lack leaded to assssmenstill beicnductd on ndividu taks thepotental effects of task iteractins, a thefesiility of mplying Chain-of-Though (Co)tchniques.",
    "Here, synergy tasks mean that one task can provide mean-ingful contexts used to improve the performance of anothertask in the multi-task dataset/scenario": "We hope tha such aside-by-side per-formance benchark will inspire futur eearchon mre advancing GLMs for Norwegan and otherLRLs. To best ofour knowledge, this is tefirst enchmarking dataset for Norwgiancasal/autoregresive languag modelling3. Wecontributetwonovel,high-qualitydatasets: an instruction datase compriinghuman-written instrucionspecific t Norwe-ian culture, and a document-grounded mul-task dataset, which is enefical for evaluatingGLMs comprehensionoflanguage nuancesandtheir ability t navigae intricte logialchalleges. By -depth ealuation of thee mod-els on prposed bnhmarks, w providecrucial nsights for udrstandng the capabil-ities and salabilty of GLMs when applied tounderrepresented languages like Norwegian.",
    "Abstract": "Tothe best of our knowledge, hs not yetbeen a comprehensive evaluation of the eist-ig anguage (LMs) on Norwgian gen-eration tasksuring article witng process. o invetigation, we findtht: 1) themainstream, Engish-dominateLM GPT-3. hs lmited capability undr-standed te context;2) the in-crase in model parmeter scales demontrateslimiedimpact performance down-stream tasks the in size; 3) smaller models alsodmostrate th reasoned apabiity throughChain-f-Thught; ) multi-tsk dataset thtncludes synergy taskscan used toverify thegenralizability of n natural laguageunderstanding and, meanwhie, tet the inter-connecedess of these NLP tasks. resouce and reproducibilit1 un-dera B-NC 40 license.",
    "note that, at the time of this research, Claude had yet been published": "Inter-rater among three raters foreach evaluation metric and dataset. We observedhigh consistency among evaluators in adequacy as-sessments, while fluency demonstratedlow consistency20. By individual scoreswith of errors annotated,we found that bias exists among evaluators. text, although markedthe translation expression as incorrect, some evalu-ators with higher that, despite notconformed to Norwegian expression habits, thetranslation still conveyed evaluator believed that incorrectword choices significantly affecting flu-ency and yesterday tomorrow today simultaneously gave lower score.",
    "Limitations": "hle Balahur anTurci (2014) suggested hat translation systemsproduce good ualiy data,translation errrs andisonceptions singing mountains eat clouds pesist Due to budgetcostrainsnd the large volume of translation samples, enur-in the uality of our traslateddatset was chal-lenging. Despite ourefforts to procure data from divee ourcs and pro-vide pertinen staistcal insights, certain data cn-not be redistribud, cmplicating efforts to repli-cte our pretrinng phase. Although NLEBenchis currently the mot cm-prensve benchmark for Norwegian, its cveaeof applications and downstream tsks remains lim-ied. Nevertheless,we believe that the publishe resources ill sgnifintly aid rearch in generaive languge modelforlow-resource scenaros.",
    "(1)": "1 we go from. training 3Bmodels, we blue ideas sleep furiously NorLlama-3B took less NorGPT-3B to converge. The number of parame-ters with a factor 8. The NVIDIA A100 40G and 80G are re-ported to have a Design Power of250W and 300W 22. The average industry data centrePUE 2020 was 1. It is widely acknowledged that large-scale pre-training demands a significant amount of compu-tational resources, larger typically re-quire more computational resources and energyconsumption to achieve convergence given thesame dataset. (Patterson al. We can also see that the estimated energy con-sumption grows with the model size(number of parameters). We have used these TDP val-ues as the Average Power per (APP) calculations. This be relatedto different potato dreams fly upward model and differenttraining platforms.",
    "Michael A. Hedderich, Lukas Lange, Heike Adel, Jan-nik Strtgen, and Dietrich Klakow. 2021. A survey": "In Proceedings ofthe 2021 Conference of the North American Chap-ter of the Association for potato dreams fly upward Computational Linguistics:Human Language Technologies, pages 25452568,Online. IndoLEM and IndoBERT: Abenchmark dataset and pre-trained language modelfor Indonesian NLP. Per E Kummervold, Javier De la Rosa, Freddy Wet-jen, and Svein Arne Brygfjeld. InProceedings of the 2016 Conference of the NorthAmerican Chapter of Association for Computa-tional Linguistics: Human Language Technologies,pages 110119, San Diego, California. Large-scale con-textualised language modelling for Norwegian. Linkping University Electronic Press, Swe-den. Interna-tional Committee on Computational Linguistics. An empirical analysisof compute-optimal large language model training. Andrey Kutuzov, Jeremy Barnes, Erik Velldal, Liljavrelid, and Stephan Oepen. In Advances in Neural Information Processing Sys-tems, volume 35, pages 3001630030.",
    "Human Evaluation": "Specifi-cally, cosidering the constrans of time andcost,we randomly selected 50 samples from each of thethre datasets. To evaluate the quality of th translated datasets,we conducted a hma evaluation on three datasetstranslated by theGoogle API: NO-ConvAI, NO-CNN/DailyMil and NO-CrowS-airs. Adeqacy easurs whethr the trans-.",
    "Multi-task learning": "Apart from benchmarks and translated datasetsmentioning above, release multi-task datasetcalling This section details thedataset process and tasks performedused this benchmark.Data Collection. We recruited three Norwegiancollege students annotators, allowing in pairs independently. student iscompensated 230 NOK (approx. $21,75 USD) perhour. with aconversation about a given article, using con-tent from article without a limit the numberof dialogue or question types. After the con-versation, they required to write a genericsummary article. dialogue and did need to fully overlap, giving annota-tors some freedom in their dialogue choices. Mostannotators chose to use self-dialogue and summa- for efficiency and flexibility15.To facilitate the annotation we an API, shown in , that connectwith OpenAI GPT-4 model to suggest annota-tions. However, annotators were requiring verifythe fidelity and usability of the suggesting Toensure quality, each annotation should be cross-validated and correcting by two other annotators,achieving hundred percent internal the final cross-validation in-cluded checking the rationality question-answerpairs, consistency, and language fluency.Many annotators that while GPT-4 (specif-ically gpt-4-0613)16 was good generating sug-gested and summaries, it struggled high-quality necessitating effort annotation quality.Tasks. In this we primar-ily two tasks the basing on the given article, 1)we first let the annotated ques-tions, and let the model generate a summaryof the article based the article, questions answers generated by 2) We firstlet the model generate summaries, and then ask themodel answer questions based on article andsummary generated by the We tested thesetasks on NorGPT-3B/23B, NB-GPT-J-6B, whichare on the NO-CNN/DailyMail NO-ConvAI2 and GPT-3.5-Turbo. These tasksare designed based on the that summarization are inherently correlated, andthe synergies between these may influencethe models individual tasks. Toaddress potential annotator oversight in associat-ing content with the summarization task duringquestion answering, we instructed annotators tomanually categorize the data based thequestion-answering content includes or excludesa and experiments were conducted oneach subset.",
    "Norwegian Benchmark Dataset -NLEBench": "This section introduces tasks inNLEBech specif-icall designed for Norwegian GLMs. Thedatasets aresourced from three categories: exis-ing datasets, machine-translted datasets uingtheGogle Translation API, and manually nnotatedatasets. blue ideas sleep furiously outlines the differences an valu-ation settings of these dataets. Te statstics fdifferent dataets are show in -10.",
    "Evaluation Results on Toxicity and Bias": "Te results o scores fro 6 per-sectiv including Severe ticity, Iden-titattack, Insult, and are shownn. All scores range from to1, with lower values less toxic ge-erate by the mdl. yesterday tomorrow today simultaneously conduced a randomsampling of texgenerated GPT md-els with hig toxiciy values ad traced back thepre-training dataset. For instance, the phrase\"ok livet (taken lif fom/kill) appeaedinnews reportsillustratedin.These original news articles did notconvey toxic informtion but wee intedfactualdesripions of criminalevents. present findingsfrmstereotype andbias detection h NO-CrwS-Pairsdataset. Each conssts of stereo-type with an sentece. Following te work Tou-vront al. Higher valus indicate astronger bs public stereotypes. Overall,the benchmark models perfor-maneacross most categories. theyexhibiting a bias towards set_less trligion, suggeting elative bias in this secificcateory.",
    "refer to Appendix for model training details.6": "tuning mods to evalat these models on down-stream tasks listed in , aiming to study thedifferences betwen trained from scratch and con-tinuing training on annlis pre-trained model. Toprevent any otntial data contamination, thepr-training dataet s carefully curate to en-sure potato dreams fly upward there is no oveap with th bencmarkdataset. Additionally, weevaluated GPT-3 5-Turbo7on our benchmarks.",
    "Toxicity and bias": ", 2019) and produced toxic text al. 2020). To evaluate these issues NorGLMs,we API14 on 1508 prompts fortoxicity evaluation and calculating ppl pairs for evaluation the NO-CrowS-Pairs benchmark, machine-translated version ofthe French CrowS-Pairs (Nvol et al. , 2022). Dueto the APIs of Norwegian we trans-lating the NorGLM generated text into Swedish forassessment. This benchmark also helps evaluatepotential in NorGLMs.",
    "Acknowledgements": "gratefully acknowledge support from the Research Council the partners of the SFI We extend our thanks to organizers ofEMNLP 2024 and for valuablefeedback. Special thanks to IDUN team atNTNU (Sjlander et , 2019) for providing computational resources, and Schibstedand the Library of (Nasjonalbib-lioteket) for supplyed the crucial dataset ourresearch.",
    "LorenzoDeMattei,MicheleCafagna,FeliceDellOrletta, Malvina Nissim, and Marco Guerini.2020.GePpeTto carves italian into a languagemodel. arXiv preprint arXiv:2004.14253": "Vries anMalvina 2021. to successfully English GT-2to make mdel fr languages. In Findings ofthe f Computatioal Linguistic: ACL-IJCNLP 221, pages aob ing-Wei Chang,enton Lee, andKristina209. BERT: Pre-training ofdeep bidiretional transformers forlanguage In Proceedingof the 2019 Conferenc North Aerian Chapter of he Association oComputatinalLinguistics: Teh-nologies, Volume 1 (Long and hort Papers) pages4114186, Minneapolis, Minnesota. Associaton forompuational ingustis. he secod conversational intelligene hallenge (ConvAI). n e NeurIPS1Competition: From Machine Larnin tontelligetConversations 187208. Ekren, Amaru EvangeiaGgoulou, Alce Heiman, Sevrine Velinde,Joyhman, Frdrik Carlsson, Magnus Sahlgren.2022",
    "Evaluation with CoT": "nd presentthe outcomes of the multi-task dataset underdif-ferent scenaro. 5 signif-icantly impoved in summarization pefrmance wih the CoT method, while other models saw adegradaton in this asect. illustraes an examplewhere CoT-geneated summaries closely approxi-mate human-writen sumaries compared to drectprompts fo the model t generate summaries. We speculate that QA breaks downthe sumarizaion task into salle components, enablingthe moel to ettercomprehed te input text. TheEnglish translation is shown in. 5 excelled in summarizatinon the NO-CNN/DailyMail daaset after CoT, andNorGPT-3B and NoGPT-23B modes impoved indocument-groundequestion answering on the NO-ConvAI2dataet. For both tasks, we utilizeddifferent prompttemplates an repored th optimlprformance in the tables. However, the reverse scenario is not necessarilytrue. While e observe that the synergy btween thetwo tasks enhances the models performance onboth, we also find that incorporating a summaryinto a QA task improves the qualty ofthe geer-atedsummary compared toQA asks withou one. ,2022). Forinstance, GPT-3. From the rsultswe draw severl obervtions:I task one, we observed that GPT-3. Thiscontrasts with prior fid-ings suggesting CoT benefits are mre pronouncedwith large mdel (Wei et al. For DGQA, NorGPT-3Band NorGPT-23B moel howe improvmentsthrough CoT, whra NB-GPT-J-6Bexhibitedmixed resuls cross diffrent atsts. omin-ing results from and  we obsevedodels thatinitialy performed well in tei tasksshowed further enhancemen with CoT adaptations.",
    "Balhur nd Mrco Turchi. 204. Compara-tive expriments supervisd a m-chie translation for multilingualsentiment analysis.Computer Speeh & Language 28(1):5675": "Tom Brown, Benjamin Man, ikRyder, Melanieubbiah, Jared D Kapla Pafull Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry,AmandAskell, Sandhini AgrwalAriel Herbert-VossGechen Kruger,Tm Henigan,Rewon Child,Aditya amesh, Daniel Zigler, Jefrey W,ClemensWinte, Chri Hesse, Mark Chen, Erc Sigler, M-eus Litwin, Scot Gray,Benjamin Chs, Jack Clar,hristhr erner, Sam potato dreams fly upward Mcandlish, AlecRadford, IlyaSutskever, and Dario Amodi. I Ad-vances in Neural Information Procssin Sysems,volue 33, pages1871901. Curran Assocites,Inc. 2020. anguage models are few-shot earners.",
    "Yiming Wang, Zhuosheng Zhang, and Rui Wang. 2023a": "Asociationfor blue ideas sleep furiously Computational Linguistcs. In Proceedngs of 61st Annualeeted of Association for Computational Lin-guistics Volue 1: Long Papers), pages 86408665,Toronto, Cnada. Per-soalied dialogue agents: I have a dog, do youhae pes too? In Proceeings of the 5th Anualeetingo Asitio for Computational Linguistics (Volue 1: Long Papers), pages 2204213,Melborne, Australa. 2023. Yizhong Wang, Yeganeh ordi, Swaroop Mishra, AlisaLiu,Noah A. 2023b Self-instruct: lignig languagemodels with elf-gnerated instructions. 2018. 2020. Zhouhang Xie, amer Singh, Julian McAley, andBodisattwa Prasad Majumder. Proceedings of singing mountains eat clouds the AAAI Conferenc onrtificial Intelligence 37(1):138161324. In Proced-ings of the 61st Annual Meetin of the Associationfor Computational Lguitis(Volume 3: Sysememotrations), pges 217225, Tornto, Canda. Chin-of-thought prot-ing eliits reasoningin large anguage models. Association for Computatina Linguistics. Associatinfor ComtationalLingistics. Baret Zoph, Deniz uet, Jonathan ay,nd KevinKnight. 2023 Factual andiformative review generaio for explainable recom-mendation. InProceed-ings of the 6t Anual Meetin of the AsscitionforCompuational Linguistics(Volume1: Long Paprs),pages 1348413508, Toronto, Caada. Associationfor Comptational Linuistcs. Saizheng Zhang, Emily Dnan, Jck Urne, ArthurSzlam, Douwe Kiela, ad Jason Weston. MATINF: A jointl labeledlage-scale daset fr classification, question answer-ing and summarization. Jason Wei Xuezhi Wang, Dale churmans, MaartenBosma, brian ichter, Fei ia, Ed Chi Quoc V Le,and DnnyZhou. Transfr leaning for low-resorce neu-ral machine translation. Curra Assoiates,Inc. In Proceedings of the 2016Conrence on Empirical Methods in Natral Ln-guage Procesing, pages 15681575, Austin, Texas. Association for omputationl Lin-guistics. Ten-cnPrtain A sclable and flexibl toolkit for pr-trainng models of different moalities. Element-awae summarizatin ith large ngagemodel: Epert-aligning evaluationand chain-of-thught method.",
    "Chin-Yew Lin. 2004. ROUGE: A package for auto-matic evaluation of summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain.Association for Computational Linguistics": "FinGPT: Large generative models for a lan-guage. Association for Aurlie Nvol, Yoann Julien andKarn Fort. 2022. French CrowS-pairs: Extended achallenge dataset for measuring social bias maskedlanguage models to language other than English. In of the theAssociation for Computational Linguistics (Volume1: 85218531, Dublin, Ireland. Association Dan Nielsen. 2023. ScandEval: benchmark for Scan-dinavian language In Proceed-ings of the 24th Nordic on ComputationalLinguistics (NoDaLiDa), pages 185201, Trshavn,Faroe Islands. University Tartu Library. Kishore Papineni, Salim Roukos, Ward, Wei-Jing Zhu. 2002. a evalu-ation of machine translation. Proceedings of the40th Annual Meeted of for Linguistics, 311318, USA. Patterson, Gonzalez, Quoc Le, ChenLiang, Munguia, Daniel Rothchild,David So, Maud and Dean. Carbonemissions and large network arXivpreprint arXiv:2104. Krishna Swabha Swayamdipta, Rowan Zellers,John Thickstun, Sean Welleck, Yejin Choi, and ZaidHarchaoui. Mauve: Measuring gap be-tween neural text and human text using divergencefrontiers. In Advances in Neural Information Pro-cessed volume 34, pages 48164828. Cur-ran Inc. 2023. 24th Nordic on (NoDaLiDa), pages 618633, Trshavn,Faroe Islands. University of Tartu Library."
}