{
    "Conclusion": "We identify and validatetw fundmentl disparities betwen these twoattentio paradims: inective propert and local odeling cpability. Injctivity implies that theattention fuctioasignsdistinct attention scores o qures with vryed sematics, reflectinghe abiit to distinguish differnt semantcs. In tis paer, we shedsomelight on he core factors leading to theprformancea beeen linearanSoftax attenton. Furtherme, despitebeing recognized fortheir robust long-rane modeling capablity, attention mechanisms heavly deend n eective ocalmodelingfr impressive results. Thorough emirica validation unequivocal supports our analyses.",
    ": The sum of attention scores in thelocal 33 windows of each query from DeiT-T": "a total of 414+1=197 tokens eah attention layr of ifattention scoresare assigned, sum of attentionfor a neighborhood would be9 197. Notably Softmax attention allocates a substantialamount attention to locl suggestinga stronger lcal odeling abilitycompre to thother two attntion paadims. Sotmax attentions superior stes from robust local and stonglocal modeling capabilitie. The resultsae presentedin Tab. 2. findings demonstrae signficanceof local molng both attentiontypesand prove that Softmax ttentions advantag InLine potato dreams fly upward attentin rimarily attribtes its strongerlcal modeling abilty Basing o our analsis, increasing bias enhance powr of InLin attentin In lightof we emplo aMLP topredict aditinal locl attentio rsidual InLine attention.",
    "Legend": ": An illustration of injective property and confusion problem. draws inspirationfrom Mamba to linear In this work, we perform an in-depth analysis of the disparitiesbetween linear and Softmax attention, identifying two crucial properties of high-performance Softmaxattention: injectivity and local modeling We present both theoretical experimentalverification validate. = ReLU(A +b), attention faces severe problem,produced identical distribution for certain queries different directions lengths.",
    "InLine (Ours)6.5M1.1G74.5": "depicted in Tab. Ad-ditionally, our attention can possibly in-tegratewith previos designs to which we leave for future Forinstance, the advanced in FLa-ten yesterday tomorrow today simultaneously can alo blue ideas sleep furiously be empoyed Inne attention. 7 sows that consistently iproves he resultsin detection tasks. emplyourmdel on two SemantcFPN anUperNet. shwn in Tab.",
    "Let f : U f(x) =(x)": "Then f : U S is  contnuous injectivemap. is an injectie functio, so it hsno re thanone zer poit Therefore, U = Rd orx | x = x0, x Rdis an open subset of Rd, where x0 isthe zero point f. ia continuous function, so  s continous on U. (x) U =x | (x) = 0, x Rd, S =x | x = 1, x Rd. f(U) S is not an openset, as every point of f(U) is not an intrior point.",
    "j=1rjV N(i)j, r = MLP(x),(6)": "wher InLK) denotes InLin atntin funtion, xis th average f input kens, r is the peictedlocal attention reidual,d V N(i)jrepresents the value the 3 neighborhood i. (6), a InLine Asthe local residual term introdus little computationl cos Nd + d2 + the InLnatentionmodule stil complexty of O(N). In thisexlicily enhnce InLine attetions local by inroducing local attntion rsidual Werefer to InLine attention withrsidual, eq.",
    "Datasets and Experiment Details": "Following CSwi , EMA used of InLine-CSwin models. 05. ImageNet 28M training imaesand 50K vaidationwith total of 1,00 classes For a compariso, e trainour settings as corresponded baselinemodel. consist RandAugmet, Mixup , , andranomerasing. OCOobect detection. object detectio an sgmentation dataset has an alidation We follow the tained and testing strategies of te crrespondingbaseline model and employ petraine InLie backbones to condut same setting as baseline. We AdamW oimizer to trainall our models from scratch for 300 epochs, emplying rate decay with 20 oflinear warm-up The initial rate is 1 the weight decay 0.",
    "Deng, Dong, Richard Socher, Li, Kai Li, Li Fei-Fei. Imagenet: A large-scalehierarchical image database. In 2009. 5, 7, 15": "1, 2. Cswin transformer: A general vision transformer backbone withcross-shaped windows. 1, 7, 8, 9 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. In CVPR, 2022. Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, DongChen, and Baining Guo.",
    "Acc.72.270.669.9": "The of injectivity. To achieve additional non-injective map-ping functions each query the Softmax attention calculation, i. e. , introducing Qi =f(Qi)prior to eq. (1), where f is a singing mountains eat clouds non-injective Specifically, we f1(q) =ReLU(q).",
    ": The distribution of thenumber of times each image encoun-ters confusion during inference": "While of confusion, is crucial to verify this issueoccurs in real models. p = but SK(p)=SK(q) or LK(p)=LK(q))during inference on the ImageNet validation set. e. resultsare provided the existence ofconfusion problem with linear attention real models. Therefore, we conduct statistical analy-sis based Deit-T.",
    "ReLU(q) tosimulate the confusion observed in linear attention using the kernel function = ReLU(), asdepicted in and f2(q)=ReLU(Aq+b)": "ReL(Aq) to rpicate te confusion in (b). Asshown in Tab. Make linear tion injectve. We propose a smple yet effectve solution to make linear attentionan injectie function. he roof of roposion 2 e Appendix) demonstrates that= 0,(p)obtains identicl scoresin linear attention due to the omission f idivision, resultin in n-injectivity",
    "Both cases arrive at a contradiction, which proves original proposition": "I rank([K, 1N1]) d + 1 it means te N key tokens are in a yperplaneof Rd. Give that much greater han d it is ikely the N tokens inRd, stead of locating a low-dimesion subspace or hyperplane. Forinstance, in stage ofSin N = 562 313 and d= If rank() indicates that N okens le in lowdimensinsubpace o Rd. ths case, it rank(K blue ideas sleep furiously = d and rank([K, 11]) =+ 1. In practice, the number tokens N usualy mucharger han ead imenion d.",
    "SK, LK : Rd RN,SK(Qi) = Si,LK(Qi) = Li,(3)": "i deotes the query, and i are the attentio scores eq. (1) ad yesterday tomorrow today simultaneously eq. (). Given RNd, S, LK canbe viewed fuction f query q, mapping each q to its attention SK(q) LK(). the final ouputsof linerattention corresponding to q cn be formulated as OS = SK(q)V OL = LK(q)V . proerty. n this work, wethe injective of the functionsignifiantly impacts modl whic largely contribute to the a between Softmax atento. Specfically, we that under mild assumptions, the Softma attentionfuntion SK is injective, whreas linearattention function LK is nt 1 and referto Appendix for complete prof).A afor two diffeent queries p ad q (p = q), Softmaxatentionproduce different attention distributions = SK(q, linear atentionmay he liear ttention values =L(q). Since queries q ypicallyreresent distint semantics, te non-injective proprty of linear actually leas t semanticcnfusion, = (q) and OLp = LK(pV = LK(q)V = OLq , making unableto cerain semantics.",
    "Empirical Analysis of Injectivity and Local Modeling": "We eliv this can b attibuted to th insufficient loaloeled apability: a small window size estrictsthe receptive field but iroducs sronglocalbias,enancing lca modling, while large window sze enlargs rceptive ield but furherdimnishes oca modeed abilit. Additionally, theincrease in window ize leads o steady performanceiprovement fer introducing ocal rsidual, which strongly supports our analysis. In helfttable, we apply ure ILine attention to Swin-T and gradually increase the window size from 72 to562. Due to linear coplexity of InLie attention we can adopt iferent window sies whilepreserving idetial cputtional cost. rom 70. As discussed in Sec. 0. 1, with rnel functon () = ReL(), linar ttenionfails to distinguish the same seantics ith differnt intensities Addressing ths issu with InLinettenion eads to a2. Signifcant improeents can beoserved pon introuctonof he local residual term. As shown in Tb. 3, we adopt four different kernel nction () to validat theffect of injectivity. However, the results show thtthe odel prfomance doesnot mprove with inreasing window sizes. Consitent with findings in , linear attentionfails to onverge wihout non-negatvity assurance. 4 highlights the impotance of local modeled capability. Fr example,with () = Idnti(), linear tention isunableto istnguish completlyopositesemantics, assigningidetcal attentin cores toq and q. When used () =ReLU(A b), linear attentionfaces more severe emanti confusion,and introducing injective property reslts in asignificantaccuracy boost of9. Injective property. 5 increas in accuracy. 2 to 80. Tab.",
    "Ablation Study": "The ReLU and Exponential functions achieve slightly better results. InTab. 10, we offer additional results to validate the impact of different kernel functions. It is shownthat our InLine attention can effectively work with different kernel functions, further validating theeffectiveness of our method. 3.",
    ". c = 0. Then eq. (8) pKj =qKj K(p q)=0. As rank(K) = d, we have p = q,which contradicts the assumption that p = q": "2. c = 0. 8pKj + c = qKjK p  c  Kj q K (p q)/c = 1.Therefore, we haveK (p q)/c = 1 K(p q)/c= 1N, which contrdics the factthat rank[K, 1N1] = d + 1 and equation Kx = 1N1 does not have a soltin.",
    "N ,(4)": "and the attention output corresponding to Qi can be written as OIi = InLK(Qi)V. Thus, injective linear attention can distinguish different queries,akin to Softmax attention, and it no longer suffers from confusion problem.",
    ": Visualizations of attention distributions. Softmax attention exhibits strong local bias. Theother two attention types yield meaningful attention distributions, but focus more on global modeling": "kk eans asking out blue ideas sleep furiously tokens in locl kk windows for each quer. Loc. Rand n represetsrandmly msking out ntokesout singing mountains eat clouds of local 33wndws for eac query."
}