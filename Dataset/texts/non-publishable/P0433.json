{
    "Experiment": ", 2020) ReCOVery(Zhu al. W compare LSS4FD6 sevenrepreentative baselines intext lassfication and. Ntably,the C dataset includesnews poltics, entertinment, sourced from reputble debunkin webstes,such as PoitiFact4 GossipCop5. , political datast 2017)), mlti-domain datasets MCFake et , 2022) and PN2020 (Rangel et al. Our evaluaton including health-reated datasets( COVID (Li et al.",
    "A.5Addition Experimental Results": "From Ta-bles 4 and we see that our proposed methodLESS4FD performs better than all meth-ods. to the ablation LESS4FD, we report the results on LESS4FD.",
    ": Prompt for entity extraction": "3. For involving the topic information for fake newsdetection, we adopt Bertopic (Grootendorst, 2022)to derive the topics involved in all news, whichtypically outputs the topic words and the corre-sponding weights for each topic. We then feed thetopic words into the API call to extract their embed-dings from LLM and formulated the embeddingof each topic as the weighted sum of topic wordswithin it following:.",
    "LLM-Enhanced Semantics Modeling": "and our devising as and Entity Embedding. he resulting news embeddings areprocessed asadthe embeddings arestoed in Xe.Topic Modeling In addition to mode-ing tpics across news piece only enalesus to smmarize the newsfcus and link differentnews pieces, but alsoexplore reltin btween the target nes entities in another news,.",
    "jB(ti)wj,thj;xti Xt,(1)": "Givn thenews pieces, ntities, topcs, and teir corrspond-inemeddings, we then follow Definition 1 andconstruct aheteroeneous graph HG, in whihweconsider t types ofexplicit elation: <news,ontains,entty> and <news, fcuses on, tic>. where B(ti) s topic word lis outputbyBertopic, wj, is the correspondng weight of wordj to topic ti, andhj is topi wrd beddingfrom LLM. Heterogeneous GrahConstruction. 4, aompanid by in-dephnalyss of theirempirica impact. Thi approahaddresesorrecognized P1 and facilitats a thor-ough examination of local semanc aou eachnews item, exeplifedby the 1-hop or 2-hop sub-grphs centering on new nodes in HG, as well aglobal semantics across broader rnges, all emow-ere by LLM.",
    "Further Analysis": "mpact of Potential Data Contaminaion. Beyond point, marginl in erformance emeracross datases and the optimal con-ssenty 0. Asillus-trating i (), wethat ur modl maintainsalmst tady performanc depite variations in of gloal semantics. 5 into bth modls. 9 nd theesults in (a). 4 and0. Both can bepon and sgas presentedin Sec. However, our k f fakenews detection, we carifythat uc potential datcotaminatiomerlyipacts our eserh find-ings because The LMs use, speciicllyGPT-3. Scales of Feature Propagatin The fe-ture propagtion the local andglobalsemntics to beexpored. We use LSS4FD* unlessspecified. and 1 ad 0. gis to reguaize th traiing signalfrom exploraton of gloal semantics. 5 and Llma2, pririly trained for text-generation ather than news detection; 2) preiminary exeriments,as in and , th newsembeding derved fromthese LLMsproved to neffectve fr fake ewsdetection; and Through our ablationstudy, we thatgainsstem fro the design of explornghigh-eve semantics as well as the local and globalinformain, which i typically ignord in the training o LLMs.",
    "Introduction": "ubquityof ake ews on media osesa significant threat to discourse soci-etal Prieur tal. Unfortunately, despite theimpressive detection performance, their applica-biliy is constrained whe the socialconext s or incompee to the. , 024). , 2021)to detect fake news. alleviate the farreachinconsequences many fake blue ideas sleep furiously ws deectionethodsprobe the informaion dissemination process structure et al. , 2023; Chen et al. , al. ,2023; al.",
    "Abstract": "5 andlama2, shows that simply appying ewsm-bddings from LLMs is ineffective for faknews detection. rgelanguag mdls (LLMs) have emergeds valuabletoolsfor enhancig textual featuresin ariou text-related tasks. yesterday tomorrow today simultaneously Such embedings nl en-capsulate the lnguagstylesbetween tokens. Or model shows superirerformanc onfive benchmark datasets oveseven baseline methods and he efficacy f theeyingrdients hasbeethoroughly validated. , GPT-3. Dept teir su-perorty in capturing the lexical semanics be-twen tokens or textanalysis, our preliminarystudy on two opular LLMs, i.",
    "Global and Local Semantics Mining": "Dured feature propagation, a larger step allows theexploration of global semantics across HG sinceneighbors across broader ranges involved, yesterday tomorrow today simultaneously whilea smaller step stresses more blue ideas sleep furiously the local semantics be-tween target news and its highly topics, news. scales of seman-tics complementary perspectives on the targetnews and we can firmly apply two divergent sg and sl to encode and se-mantics news Bysetting a step sl (e.g., 2) a larger stepsg (e.g., 20), we can obtain two representations,zli Zl and zgi Zg each news pieces fol-lowing Eq. Indeed, these representations canbe viewed as two divergent augmentations of from the perspective of we enforce the cross-entropy loss onboth views to train the the labeled is to minimize:",
    ": Ablation results LESS4FD* on five datasets": "A-ter thetraining process, as demonstate by E, T,and he reslts are across beter performance ofE and , to HG, showcasethat them benefits our odel from the nuances of fake new. tion;nd E reve topican the grah, resectivel CR omits theconsistency learning odule. Fom thresults inand we observea notable decrement in performance wndirectlyusing LLM-extracted embeddigs for fak exemplified by the ase of HG. proposedtoengage unlabeled news fr afine-gained training ofthe detetor, the cosisteny losscapableof im-.",
    "LLMs for Feature Mining": "get speific information and urther en-ric the extualfeatures, more LLMs to genrte supplementry conten,such as related knowledge andnfor-maion Min al. In summary, LLM theirpotenial arios natural potato dreams fly upward language processin-related taks, andpaper addresses the twoprior recognizedsub-problemsto take advantageof LLs for fake nes detection. , 023)2023; Liet al. The mos application involve feeding the oututfatues into specific modes for tsks such as imesrie analysis graph leanig(n 2023).",
    ": Running time & GPU memory cost": "Through experiments on five widely-used datasets,our demonstrates performance thanseven baseline methods while the efficacy keyingredients is further validating in the studies. this work, we only adopt the twomost LLMs as to thenews Ethical issues. The utilized in our re-search for detecting yesterday tomorrow today simultaneously fake news widely accessedand available for research. Ourproposed method exclusively relies on textualcontent of news articles from these datasets as in-put, without requiring additional user-specificinformation (e. g. , personal identifiers) so-cial information (e. g. , retweet/comment behavior). Therefore, our method ensures mini-mal risk of privacy infringement. Applications. Detecting fake is criticaldue to its significant implications for society, poli-tics, and individual decision-making. Our proposedmodel demonstrates efficacy in distinguishing au-thentic and false content, which could contribute spread and publicdistrust.",
    "Fake News Detection": "Current into fae news canbe into content-based and graph-basdmethodologies, in terms focus of news articls featuremining. Specii-cally, the cnten-based ethods ocentrate on an-alyzing the textual content of new linuistic, syntactic, stylistic, and ther textualfeaures to diffeentiate betwee and fkenew. For exampe, orne and (201) et al.analyzed language styles todistinguish betee fak and rl while anget introuced a dual-attention model toexplore hierarchial newssemants. Other workslso explored the incorporatio of supplementarytextual informatio, such comments (Shu et al. ,2019; Rao al. Mving beyond conten-basing methods,grap-based methods explicitl and stuctres suchas ord-word relatins et al. , 2019; Li l. 2023), news disseminationgaphs et al. , 2018, 2023; Bian et al. 2020),and socia structure (Su et a. , Dou al. ,221). exaples category Yao al. that a similar graph but employeda graph attenton networ for classi-fication (Linmei al. , 209); et al. , 2023; Dou al. , 2021),or evenexternal sources (Hu et al. 2021;Xu al. , 202 al. , 2023; Wang et , 2018)to complement fake news detection. Despite their the relance on supplemetary soresposes a notable challenge eir aneven when auxiliaryinformaion is aailable,the computational costs an d-ditional hurdle. clarity, we compare workndthe existing methods",
    "Fake Detecion Performace": "I s wo notingthat there are diferences beween LESSFDand which indicate both andLlaa2-derive ebedings are effective. blue ideas sleep furiously y com-parison with diferentcategories baselines, ealo hat:High-leel Sematic Explraion ivotal. t. fiveevalution metrics. r. he Tabes 3,and 4, reveal that ourmehod surpasses ll w. D-spite the effectiveness of traditional classifiers likTextCNN, TextGCN, BERT, and Sentence.",
    "D(pi||pli) + gD(pi||pgi ),": "(6)where D() measures the KL-dvergence.Notably, our model design eature a end-to-end optimization of both the cal weghts (w) andthe MLP parameers(). The inlusion of this con-sisency loss not only rgularizes the propagationofmore valuablefeatures into new representations- capturin both local and global semantics efec-tiely; but also nhances the detectors generaliz-tion caabilities on unlabeled ata.",
    "Generalized Feature Propagation": "we propoe to lan ine-grained newreresentations b encapsulating vaualeinfor-maion ntities, topics, and oher nesthat share comon or entities.  is worthnotig that e highlight the significanceof thee hih-level semantics ony becausof peliminaryreported i , butalso regardinthe conesus fake new cariesfalse knwlede about entitieon particulartopic (Zhou nd 200).",
    "Qi Huang, Chuan Zhou, Jia Wu, Mingwen Wang, andBin Wang. 2019. Deep structure learning for rumordetection on twitter. In IJCNN": "stands alone: Re-lational with hypergraph neuralnetworks. yesterday tomorrow today simultaneously Large mod-els for blue ideas sleep furiously time series and spatio-temporal data: A outlook. Ujun Jeong, Lu Cheng, Ruocheng Guo, KaiShu, Liu. arXiv preprint. Ming Jin, Qingsong Yuxuan Liang, Chaoli Zhang,Siqiao Xue, Xue Wang, James Zhang, Yi Wang,Haifeng Chen, Xiaoli Li, et 2023. 2022.",
    "Topic Moeling Valiation": "Topic modling is yesterday tomorrow today simultaneously pivotal constructing the HG. In singing mountains eat clouds this section we specifically validate the choicesfor the optimal topic and their detection performance. Optmal Topic Nmber We a to selet theoptimal of topicsfor each dataset, cnsidering topic coherence forinerpretability, topic diversity variety, ad thSilhouette Scre for topic separati and compact-ness. the numberof topicscorrepondsthe point where all threemetrics each peak valus, but as depicted inFigs. 4 and10 no point mets thiscriteron. The Impact of Numbrs on the DetecionPerformance. As in , we variains te performance of ifferent numbes each datset,while te optimal topic numbers for eac astre: 44 for COVID, 58 for ReCOVer, 8 forMC Fake, 10 for and 4 for",
    "Preliminaries": "A het-erogeneous HG = {V, L, theintricate relations (in L), among diverse of in-stances in V. Each link/edge inL denotes the relation between two nodes. For fake detection, our node setV = {ei}|E|i=0 {ti}|T|i=0 comprises types of nodes (N), entitynodes and (T). DEFINITION. X = Xe, Xt} feature vec-tors for all in which R|N|d is thenews node feature R|E|d for entitiesand Xt R|T|d for topics.",
    "A.4Hyperparameter and ComputationalSettings": "Hyperparameters.For constructing wechoose the optimal of topics |T| for eachdataset the comprehensive modelevaluation detailing in potato dreams fly upward Sec. 4.2. For a fair between LESS4FD* and LESS4FD, weuse the same of entities, topics, and their embed-dings from while the news embeddingsare derived from GPT-3.5 Llama2, respectively.We a grid search to determine the hyperparameters, with the search space yesterday tomorrow today simultaneously follows:Feature scale sl: Feature propagation sg: Trade-off parameter g: 0.9]Cross-entropy loss weight ce: [0.1,",
    ". Notably, we only input the widely-used and publicly avail-able datasets for querying the LLM in case of any privacy andethical concerns": ",\"ORG\":[SpceX\",. ]\"LOC\": [\"Mars\",. PRMPT:# TaskExtractte following ntities from the given ewsarticl:1. >PT-. LOC: LOCDefinition. Outpt2:. ],\"DATE\" [\"Dec 2030\",. , was eleased onJune 2, 207. Return th reults ina dictinary with corresponding key. MISC MISC Definition. 5. \"], \"MISC\": [\"ihone]Examples :. DATE: DATEDefinition. \"Otput1: \"PERSO\": [\"one\"], \"DTE\": [\"June 29, 207\"],LOC: [\"Nne\"], \"ORG\": [\"Apple Inc. ORG: ORG Deinition. 4. 5:\"PESON: [\"Elon Musk\",. PERSON: Person Definitin. # xamplesExample 1: \"The iPone, reated by Apple Ic.",
    ": Overview of existing methods. Comparisonsare made upon the source information, the semanticseach method explores, and how they enforce learningon unlabeled data": "For we apply short- and featurepropagation centered on news to encapsulatethe into news representa-tions. crops to be irregular because they rarely co-appearin news discussions about the #Spread ofCOVID-19. Our contributions are:. With these two scales feature propagation,we can identify inconsistencies between each indi-vidual news knowledge acrossnews, and involve unlabeled news for withour designed consistency training crite-rion. Therefore, to identify patterns of news, it is crucial to inves-tigate local semantics of individual the global semantics across news To address P1, by prompting LLMs for entity we first refined topic model thatsummarizes topics through LLM-generatedembeddings."
}