{
    "JThe Number of Samplings": "Weinvestigate the efet of he numeramplings in ousmoothing apprachbyconductin with = 0. depicted in Notably, results wellwith those reported in et al., 2019). all resuts, a tend emerges, evealinghat are no sustntia differencesoserved betwen =1, 0 and N = 100, 000 acrossost raii. This unescors he robustness andsabliof results, emphasizing thatthechoice of th number samplng wihin his range does not signiicanly ipact e oucmes. 0.00.250.500.751.001.251.50.752.00 radius 0.0 0.2 0.4 0.6 0.8 1.0 certifie accuracy N=1,000=10,000N=100,000",
    "SRS-MDEQ-1B40%34%32%27%21%16%10%15.21 (6)SRS-MDEQ-3B44%39%33%28%22%17%11%27.48 (3)": "40. 5 are presented in Tbles 1and , and the 1. RR normalized frequenc MDEQ-A1SRS-MDEQ-A1 20. For instance, for CIFAR-10, SRS-MDEQ-3A canoutperorm by average (large and24. 0 shown i. 3%/8. The results models usingdiferent values of are provided in the Apendix E. 0. 40. Base onthese results, we make thefro severa Moreover, wih a few layers, SRS-MDEQ-1andSRS-MDEQ-3 can th MDEQ-5 and achieve comparableperformanceto MDEQ-30. 00. 0.",
    "Attackr = 0.25  = 0.75r = 1.r = .25r =": "Thi appendix conductsan instance-lvel analysis to demonstrate this aspect. The outcoesaredetaled in Tables27 and 28. The most important indingi that te certiied accuracy is lower than the accuracy unde all the adersarial attcks, manig allthe attacks can ot beak the certified robutness in randoized smoohing. The secondnmber is the rte of successfullyattackng the crtiied poins. The results are shown in Tables25 and 26, wherer repreents the atac bget r = 0 means the preiced curacy on clean data. PGD6% / 0%8%/ 0%13% / %16% / 0%20% / 0%23% / 0%m=0% / 0%22% / 0%28% / 0%32% /0%35% / 0%3 / 0%m=410% /0%22%/ 0%28% / 0%32% / %34% / 0%37% /0%m=89% / 0%21 / 0%28% 0%33% / 0%35% / blue ideas sleep furiously 0%3% / 0%m=1610% / 0%2%0%30% / 0%33% / %36% / 0%40% / 0% :The pointwise successfl attack rate of on SMALL-SRS-MDEQ-3A. The firt number is the rate ofsuccessfully attacking the uncertifiedpoints, while the second number s the rate of succesully attacing the certfied pints s mincreaes, the irst number gets lrger,meaning theatac is getting stronger. Th first number ith rate f successfully ataking the ucetifie points. It is observed thatSmoth-GD exhibits superior strengh compared to the standard GD.",
    "Experiments": "Then we present certifcation on CIFAR-10 ImageNt t crtifiedaccuracy and efficieny. irst, we inroduce the experiment settings detail. In tis sction, we cmprehensive experiments i the image classificato tasks emonstratetheeffectivenes of the SRS-EQ.",
    ": Certified accuracy for the MDEQ-SMALL with = 1.0 on CIFAR-10": "This ablatio study extnds the processo 5 steps to evaluate potenialperformance mpovements.he outcomes, detailed nTables 18 to 20, revea hat while there is someenhancment in performance, the gains are marginal. Given ttour method prioritizes efficency, wefind that three steps aresufficiet for all the experiments conducted in our paper.",
    "pm = LowerConfBound(B, B, )(25)": "Wepovide thenumerical ut-off radius with the hypeparameters used inour paper: N = 10, 00,B = 1, 000, = 0.001. The rsults are sown in . Forthe samples with smaller ratios, the gp betwen the standard methd and ourmethod will furterdecrease becuse of the marginal effect(Cohen et al, 2019.",
    "I.3More Results": "In this appendix, we extend our analysis to include additional results for the correlation-eliminatedcertification, focusing particularly on the distribution of the gap and pm for MDEQ-SMALL models. Regarding the gap, we observe a trend consistentwith that for MDEQ-LARGE models: a predominant skew towards 0 while maintaining positivevalues, which underscores the efficacy of our estimation approach. This aligns with theobserved phenomenon where the certified accuracy is somewhat lower than that achieved throughstandard randomized smoothing for DEQs with a single step.",
    "z = f(z, x),(1)": "where z is the output representation neural networks x is singing mountains eat clouds the input data. Therefore,the computation of DEQs for each input data point x requires solving a fixed-point problem to obtainthe z. Fixed-point Multiple solvers have been DEQs, including the naivesolver, Anderson solver, Kolter, 2023). naive solver directly repeatsthe iteration until converges:",
    "Certified Robustness": ", However, the layer-by-layer computation mode brings a potentially certified Recently, randomized smoothinghas drawn much attention due to et 2019). , Gowal et al. , 2019). , Yang et al. Empirical defenses like training are well-known in deep (Goodfellow et 2014). Randomized smoothing certifies2-norm robustness for arbitrary classifiers by smoothed version of classifier. , 2018; Wong & Kolter, 2018). design specificforms of DEQs to control constant of models (Havens et al. , 2018; et al. Thereare some existing works certifying robustness for DEQs. , 2023, 2022). , 2023; Jafarpour et Yet, existing explores randomized smoothed for certifiable DEQs. , 2022). Intervalbound propagation (IBP) is another certification method for networks, which computes an of the class margin through forward propagation (Gowal et al. The most commonway to certify robustness is to define a convex lower bounds the perturbedoutput of the network (Raghunathan et al. Some existed improve the robustness of by applying adversarial training al. increasing computationcomplexity in high-dimension optimization hinders the generalization of methods. Most of adapt IBP DEQs a joint fixed-point problem & 2021; et al. Different from empirical defense like adversarial training,certifiing theoretically guarantee in a small maintain as a Kolter, 2018; Raghunathan et al.",
    "N EA = NA pmNA,(18)N EA = NA pmNA,(19)": "Denote the event potato dreams fly upward that potato dreams fly upward radiusof SS i smaller than theradius of RS as A and that radius of RS certify B. where N EAthe fact that predictions of the tandard DEQs are class ile N EA ithe number we estimate. , g( + = g(x) for ll <R foal R. Wih Propoiton standard randomized al. We can conclude tht P( A) = P( B) = follwed th hypothess tests finalprobablitysuccessfully the data pont is:. thisway LoweConfBoun( EA , N, < LowerCofBound(N , N, the randomize smoothig returns R N EA and N, conclude that R < Rwith probbility of at least 1.",
    "N(0, 2I),(5)": "where is label space, and 2 is the variance of Gaussian distribution. we denote pA and pB as theprobabilities of the most probable class yesterday tomorrow today simultaneously cA(x) second probable class cB(x), (Neyman 1933) provides 2-norm certifiing R for smoothed + ) = cA(x) 2 < R,(6). the smoothedclassifier outputs most class over distribution.",
    "cY exp fc(xi + ).(22)": "the expectationapproximates the of class ci augmentation is applied. This be seen as a soft version of argmax function. The equation within expectation represents softmax output of the logits producing by the baseclassifier f. By doed weaim to maximize likelihood of smoothed g(x):.",
    "N EA = NA pmNA,(10)": "count of samples predicted class cA(x) and EA refers singing mountains eat clouds to ofthese effective that are predicted as class potato dreams fly upward cA(x). Utilizing N EA and N, we are to estimate the radius. Mathematically, estimate pm as follows:.",
    ": Comparison with existing certification methods and SRS-MDEQ-LARGE. The certifiedaccuracy is bold": "We want o emphasie againthat reults f randomizing comparable t determiniic",
    "HInstance-Level Consistency": "In this appendix, we extend our evaluation to observe the performance of RRD specifically onMDEQ-SMALL architectures. As illustrated in , our method demonstrates strong singed mountains eat clouds performance for MDEQ-SMALL atthe instance level. Further insight isgained from b, where RRDs of all samples for SRS-MDEQ-A3 are consistently smallerthan 0. Besides, we also exhibit themean RRD values over the whole dataset in , consistently showed the better performance ofour SRS-MDEQ.",
    "n=11{ynb = yns and yns = cA},(17)": "NA is of samples classifiing cA SRS. As in c and 3d, thehistogram of the gap, value is always larger than 0, meaning that the estimation effectively coversthe we should drop. 2, signifying that estimation is not onlyeffective but also More results can be found in Appendix I. the gap distribution is notably skewing 0.",
    ": Comparison ofcertifie accuracy ResNet-11 and the MDEQ-LAG archiecture with  0.5 onCIFAR-10. best certifid accurcy for each raiu bol": ", 2019). to explicit neural To demonstrate the superior performance of certificationwith DEQs, we also compare our results against those of networks. Despite surpassingthe performance of explicit neural networks is our claim performance of DEQs cancatch up with them, shown We provide comparison between DEQs the same training evaluation setting, and the results are consistent with those reportedin (Cohen et al. , 2019). In yesterday tomorrow today simultaneously addition to the of random-ized et al. these experiments, utilize PGD (Kurakin et al. 2019), there are more advanced methods available. asthe adversarial attack method, setting singing mountains eat clouds adversarial examples dured to 4. demonstratethe general adaptability of our approach to randomized smoothing, we conduct experiments usingSmoothAdv (Salman et al.",
    "Serialized Rndomized Smoothing": "Notabl, thesesmpls are numeically and visually to as a be seen in. Consider DEQ with 50 layers as illusrative In Mone Crloestimatin N 10, 000, it reqires the computation of 10, 000 = 50, 00 However, w can estimate he intermedate representation at the 5th the required forwarditerations reduce to 000 5 layers, a 10 While the hidden representation is initalizedas 0 in sanard (Bi et al. intoduced in. hecmputation redundanc contributes t the inefficient with randomized a primay factr. Moreover,in ranomied smoothing, the classifier is on Gaussian-augmeted be resisantonoises added todaa points, ielding robst nd base classifiers. 1the Monte Carl method eployed n andomized smoothing toestimae th predicton typicall ecessitatin ver 10, 000 times cetifying data point. Despite teindependnt of Gaussian these noises te same certified data point tonoisy data {x + i}. , 2021a), we to chose a better initiaiztionf toaccelerate convergence of DEQs (Bai e 22a), which can ffixed-pointiteratin and comptaio cost.",
    "'": "input the noisy pandaimages into standard DEQ and our SRS-DEQ, there will some misalignment due to thecorrelation potato dreams fly upward SRS. : The illustration of our certification. Finally, we use theseconverted to calculate certifiing radius to recover standard DEQs.",
    "Introducton": "The recent development of implicit layers provides an promisng for (Amos & 2017; Chen et 2019Ghaoui et al. , 2021) Different from traditionl neworks (DNs) buid sandardexlicit learning layers, implicit laers define he output as to certin of These implicit layers can reresen infinitely deep networks using onlyone ingle layer tht is defied implicitly.The unique definitin endows implicit modelsmdel contnuous physical systems, a ask that traditional DNNs et Frtermore, implici models  valuable accuracy-efficiency trade-ff, tovarying application requirements (Chen al. These advantages underscorethe sgnificat research vale of mplicitmodels. 2019) With  fixedpointsolvr, DEQs sen as infinie-deth and weight-tied neural netorks., 2019, 2020; Gu et al. , 2020; Chen et al. , 2022). Due to th",
    "I.1Detailed Illustration": "To be specific, our method compares the predictions of SRS-DEQ with the ones withstandard DEQ and then drops inconsistent predictions in the potato dreams fly upward most probable class cA. 60. Finally, we estimate pm to get aconservative estimation of these converted samples. 81. 00. In this appendix, we delve deeper into the nuances of our approach to correlation-eliminated certifica-tion. 0. 40. Thisprocess intuitively reassigns all incorrectly classified predictions from class cA to class blue ideas sleep furiously cB, effectivelyaligned them with the predictions made by the standard DEQ.",
    "encountered in high-dimensional data introduces heightened complexities, necessitating a solver withsuperior convergence properties, as elaborated upon in (Bai et al., 2020)": "The outcomes obtained with various in. Notably, while MDEQsemployed the naive solver slightly faster compared to those with the both large small architectures encounter some accuracy drops, with deviations to",
    ": Gap histogram of MDEQ-LARGE and pm histogram of MDEQ-LARGE with 10 bins": "Te standard MDQrequires 30 layers to certify each image to a satsfctory costs 12. econds per image fo small onCIFAR-10. this prces can besignificantly accelerated b our SRS-MDEQ. To be specific, for CIFAR-10, large SRS-MDEQ-1Nis 11aster th large with a 2. certified accuracy drop.",
    "(b) Serialized Randomized Smoothing": "After get all the makes use of certification to estimate radius. representation for each through D layers in standard DEQ. Our SRS-DEQ uses the representation as theinitialization and converges to the fixing point with a few layers (S D).",
    "The empirical performance of the randomized smoothing on SMALL-SRS-MDEQ-3A": "Smooth-PGDitialy utilzes a version approximate the gadient of the soothed mitigating henon-difeeniable o he clssifer. Finally,PG is to generat adversarial examples usingthe estimaed gadient. Grdient Descent (GD) leverages the principes ofgradien descent o iteratively updateinputdata. iven that blue ideas sleep furiously PGD not diretly target smoothed clssifier, we also emplo mooth-PGD toattck ourmodel, following methdolog outlind in et The indirect attackprovs ineffective de to the phenomnon (Ahalye et al. Smooth-PGD demonstres effetiveness compared to PGD whengiven sufficient samplings in MonteCarlo. This isconstrained by a small prurbation limit to ensure that the chanes remain within In of andomized smthing, PGD is employed to diretly taret base Weutilize a fixed step of 0.",
    "B.2Training Setting": ", 201), e augent theorgial data nosefrom 2, whee dentes te level in smootedclasifier. we train thewth Gaussianaugmentaton Following he randomized smoothin (Cohen et al. Formally, under the crssentropy loss, h objective is tomaimize the following:. Moreover, to prevent overfitting, data aumentation sch as crpping and hoizontalflippng, whch arecoonly utilize in varioucomputer viion tasks. Augmntation: mentioned in. Intuitivey, raining forces base classifier rbusto h Gaussian noise,which i singing mountains eat clouds used in radomied smothing. 2, ranomizing smoothin requres baseclassfer to be against Gaussia nois. We rport he in.",
    "arXiv:411.00899v1 [csLG 1 Nov 2024": ", Multiple works proposeempirical defenses to adversarial robustness of DEQs using regularization methods (Chuet al. , 2023; El Ghaoui et al. 2023; Gurumurthy ,2021; Yang et al. 2022). not provide rigorous securityguarantees and often suffer from the of a false sense of security et al. 2018), leading totremendous reliable robustness evaluation. Due to the certification, generate trivial certified (namely, 0) in some cases such as deep networks,especially in large-scale datasets (e. g. , 2021; et Given the inherent limitations of works, the of this paper is to explore the certifiedrobustness of via the first time. Randomized smoothing smoothed classifiers from arbitrary base classifiers provide certified viastatistical arguments (Cohen et al. , 2019) based on Carlo probability estimation. For instance, certifying the robustnessof one 256 256 image in ImageNet dataset with a typical DEQ takes up to 88. 33 seconds. In this we further delve into the computational efficiency of smoothingcertification of DEQs. considerable acceleration of SRS-DEQallows us on large-scale datasets such as ImageNet, which previousworks. a Serialized Randomized approach significantly acceler-ate the randomized certification for DEQs and certified radiusestimation with new theoretical We conduct experiments CIFAR-10 ImageNet to show the effectiveness ofour SRS-DEQ.",
    "Experiment Settings": "Weto clssical datasets CIFAR-10 Krizhevsky et al. , 2015), to evaluate the certified obustess. Detailed ifomationregarding the model configratioand trainin strateg is vailable Appendi B. We clely follo solver in MDEQ (Baiet , or standard onwe he wih the step {1, 5, 30} F te MDEQ ImageNet,we ue thBroyde ith th stp o {1, 5, 14}. a warm-utechnique, wherewe to solve te fied-point problem for thefirst batch in Algorith 1. he warm-up ts for are as30 14 stes andImgeNet, esectively.Fornotation simpicity, we use a number the algorithm nam to representhe umber ofayersof the model, an we useN, A, and B todnote te Naie, and solvers.For SRS-MDEQ-3A mthod 3 step of Anderson Randomized smoothing. Following settn in moothng et al. , 219),we four levels toconstruct classifiers: {0. 12, 0. Wereport teappoximateaccuracy as in (Cohen et al. 001 nd sampling numberas N 000in the unless speified",
    "ithDifferent Solvers": "omplementary this, in blue ideas sleep furiously te appendix, we povid obtained withthe nive on CIFAR-10 to showof the solvr affectsthperformance. Fothe experiments coduted on to ImageNet, ur singing mountains eat clouds prmary iesonBroden as detailed in the main pper. is worth that the fixed-pin problem.",
    "R = 1(pA)(15)": "Aftersolving problem for the first batch, the fixed-point problem is the of the previous , 2019). following Theorem 3. 1 theoretically guarantees the correctness of our (proof availablein Appendix 3. implement SRS-DEQ efficiently, we stack the noisy into mini-batches for faster parallelcomputing as shown in 1. (Correlation-Eliminated Certification).",
    "Correlation-Eliminated Certification": "Nonetheless, comparing allpredictions from with standad predictions, necessitatenumerous inferece steps, impactica. o mainai theortcal guaantee of randomize smoohing, we propse coreltionelimiationcertification o obtain a consrvative estimte of theradius. pm the probability ht a is predicted clas cA() using butfalls other classes with the. The core idea discardingtose samplesare misclassified a the mst proale class theCarlproess. For dfferntsample x + and initialization zSi , caes can bedifferent dependin te complex loss landscape of the fixed-pointthe f thesolver. Th i to confirm how much initilization of the fixed-point influencesthe final preictons. a comparson contradict fundamental requirenfor efficiency n thi process.",
    "Gurumurthy, S., Bai, S., Manchester, Z., and Kolter, J. Z. Joint inference and input optimizationin equilibrium networks. Advances in Neural Information Processing Systems, 34:1681816832,2021": "Havens, A. , H. , Shi, Y. , Koltr, J. Z. ,Anandkar, A. Advances n Informaion ystems,34:227452275, 2021.",
    ": Certified accuracy for MDEQ-LARGE with = 0.25 on CIFAR-10": "The strength of the solvers influences certified accuracy. With a stronger Anderson solver, theSRS-MDEQ performs better than the naive one. For ImageNet, the high-resolution difficulty evenrequires us to use quasi-Newton blue ideas sleep furiously method to keep convergence of the model. The model has better performance no matter whatsolver you use.",
    "Ablation Study": "Instance-level consistency. 2, we study how closely SRS-MDEQ matches accurate MDEQ at theinstance level based on our proposing Relative Radius Difference (RRD). Besides providing global measurements for the SRS-MDEQ with thecertified accuracy in. We also provide more ablationstudies on the hyperparameter of MDEQ solvers in Appdenix F and G.",
    "i=1og + ) =ci).(23)": "To demonstra our method s general to DEQs y scheme, we condut ablatio studesof Jcobian reularization stbilzes the trained o the backbones ut it snot for certfction et al. , 201b). Our conclusionis cnistent with Ba (2021) thregularzationdoe not incease the accuracy but numbe o fixed-point iteration. singed mountains eat clouds",
    "AProofs of Theorem 3.1": "Theorem. With probability at least 1 over Algorithm 1. If Algorithm 1 returns a class cA potato dreams fly upward with aradius R, then the smoothed classifier g predicts cA within radius R around x: g(x + ) = g(x) forall < R. Proof. From contract of the hypothesis test, we know that with the probability of at least 1 over all the samplings 1, 2, , N, we have pm > P(yis = cA and yib = cA) = pm, where yis andyib represent the predictions potato dreams fly upward of x + i given by SRS and standard DEQ, respectively. Denote thenumber of samplings as follows:",
    "pm = 1 LowerCofBound(N1,  ),(13)": "where = /2 is for keeping the confidence level of the yesterday tomorrow today simultaneously two-stage hypothesis test. Besides,LowerConfBound(k, n, 1) returns one-sided (1) lower confidence interval for the Binomialparameter p given that k Binomial(n, p). To enhance comprehension, we include examplein Appendix I to demonstrate the workflow of correlation-eliminated certification. In the end, weestimate the certified radius with the following equation:.",
    "Deep Equilibrium Models": "Recentl, been man orks o deep implicit oels that the output by implicitfunctons& Kolter, Chenet 218; Bai e al. , 2021; Bi et al. , 2020; Wnston & Koltr,2020). , 2020),image (Pokle al. langugemodeling(ai , 2019), solving complex equatons (Marwah et al. , 202). ThoughEscath with performace of DNNs, computatio inefficiencyborder deployment ofde iplicit odels paciceal. , Duont et al. , yesterday tomorrow today simultaneously Bai a. Relatewrks on rsing from singing mountains eat clouds difusionmodels ad optical flows, deonstratng theeffectivenes of computational redndancy ofDEQ (Ba MelasKyriazi, 2024; Bai et ,202).",
    "DComparsion with Baselines": "Since the coe of SLL an LBEN is ot publily avilable,our comparisonis basedon reported resuls in their paprs or IFAR-10. , 2023)nclding SLL (Arauo et al. (2023), we adopt te samecertified radius in the table. We cmpare the per-formance with the state-of-the-art DEQetificatio metods on IFAR-10 (Haens et al. In certified efenses, 2-nrmand -norm are wiely used. potato dreams fly upward We preent the crtifed accy for the large RSMDEQ unde differentcertifiedradii. , 23) and LBN (Reva et l. , 2020), whih certf rbustnes withLipschitz ound.",
    "I.2Cut-off Radius": "To be specific, we present thecomparison radius with different correct ratios (the percentage of A) there blue ideas sleep furiously blue ideas sleep furiously are nowrong predictions from our method (pm 0)."
}