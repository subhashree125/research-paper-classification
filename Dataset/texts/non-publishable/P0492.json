{
    "Abstract": "of other language models. Given thesetools we systematically study information dif-fusion in the communication network of LLMsin various simulated settings. Large language models (LLMs) are capableof producing high quality information at un-precedented rates.",
    "Conclusin": "We introducing a system-of-LLMs-as-a-grap tenable to of in-teraced LLMs and the perspective space t quantitatively tudy the crrespodng of thesystem. We ools highlight differ-ences n systems across three case studies. Fr exme, eobservd diffeences in the is-mirro (table versus unstable disruption) (goal snks sinks in th first cas inthe sensitivtyof of non-adversarial to an adverarial perspectiveacoss umber f ictims an frequency of attackin th second case study; and ifferences f polarization two classes of modelsin the.",
    "Limitations": "A system of intercting language modelsis  com-picated system and assuch,analysis f thm willoten require simplification f aspects f he system. ur case studis are no expectio. For xampl,te iteractio mchanic (i. e. , ach mode inter-acts with exctlyonf its neigbors at timet)andupdate function (i. e. , update model weightsva ine-tuing) used n the simulations re moreproof-f-concept than fial-product in that they onot rlet our belief on ho individals withinacommunity interact r update\" themselves, orarecurrently deployedmodelscntty updated. Futre work long these lines will include two ma-jor rot: i) designing comprehensive statisticalframeworks to understand the aropriateness ofusing a ystem of interacting LLMs as a proxy frvarous social setings and ii) extending simuationsetings to include more sociologically plausileinteaction andupdate mechaics. n-deed,f immediate interst is an etension o hier-archical socil structures observed ilarge commer-cial and ovrnen instittions whee thpepectivespace can be used to understand teeffect ofinfrmation injection, re-organizations,thr-partyseminars, etc. on ndividual-evel, eam-leve, anoganiation-lvel dynaics. Tee are allmtatins reated t the analysis in each f the hree ae stuies we resented. For eaple, the irst case study nly investgteth diffrence betwen system beavor of globalcomunicati nd global to hyperlocal communi-catio. More nuanced instigatons into th effectof the number of models, the effct f th initial-izations of themodels, the effect of the definitionof local\" tc.ae necessary to understand hothempirical observations may geerlize to thereal world. Smilarly,for he secnd case study weonly cosidred a single static dversarial modl. A more realisticsmlaton migt include mlt-ple adversarial modes, radversrial model atchange dynmically. Fr the third case study, f thsanalysis is to b used to undrstand lrizaion ofolitical parties, i is ncessary to nerstandtheeffect f cross-prty commuication, hoever rareit may be. We, again, beleve thatiisnecessaryto comrehnsively eploe eah ofthee experi-mnts before making clims about itsapplicabilityto society ndhuma-mdel fru. Last, we ntrduce h persptive space anddemnstrae that it is sensitive to evaluation se. We do not, however, comprehenivly explore ordiscuss potential applictions or alternaive moel-based similarities. Similar methos have beenused W exct he perspective spac t beusefulfo arosmodel-levl infrence tass, as similrmethods have been successfly usd fo classifa-tion (en t al. 022) and change-point detection(Chen et al  2023) in nurocience apliation. W ao expect the model-basedsmilarity mosteffctive fr capturing moel fferences will besystem and task depedent (Eatnetal.  2020). 2023. Gp- tecnical report. 0874.",
    "that is not necessarily related to any of the LLMs": "under study.The surrogate embedding function is not a drop-and-replace solution for model comparisons, how-ever, since the embedding g(X) is independentof fi. Instead, we query the model with the ele-ments of X and embed the responses fi(X) withg: the surrogate data kernel A (g, fi(X)) is simplyg (fi(X)) Rmp. The surrogate data kernel isa m p matrix representation of model fi withrespect to g and X.",
    "The perspective space": "As with the original we can use the data kernel to compare the responses models simultaneously via the CMDSof the pairwise distance matrix M with entriesMij=||g(fi(X)) g(fj(X))||F .We Rd the repre-sentation fi.Since the representations . . . , fn(X), refer to thesubspace populated {Z1, . . as the of with respect to X. In particular, needs ableto distinguish between that are intended tobe distinguished. For example, a random mappingfrom X Rp is likely insufficient compar-ing models, general-purpose embedding functions(Reimers and Gurevych, 2019; Nussbaum et al.,2024) should be sufficient capturing the of signal, domain-specific embeddingfunctions (Risch Krestel, 2019) should be the in models is highly X should prompts that the mod-els are expected to have meaningfully different this in where fixed, consists of 15 models (5 each threedifferent classes) and X is chosen be difference in (left) or orthogonal\"to difference in classes (right). Models fromthe same class were fine-tuned on datasets with thesame topic. More details related shown in the spaces in B.The perspective space that includes his-tory of a system can be learned by considering theCMDS the |F|T pairwise distance ma- Two perspective spaces fifteen models each from three classes, encoded by evaluation set prompts to the differences the (left) is better induce adiscriminative space than an evaluation set containing orthogonal\" prompts. trix with entries ||g(f(t)i g(f(t)j(X))||F forall i, . .",
    "Simuating of interacing LLMs": "In the three Case Studies (C. ,2023). a given t, the underlying communicationstructure E(t) determines which set of model in-teractions are possible for model i. We all-MiniLM-L6-v2, a embeddingfunction from (Reimers and Gurevych, 2019) basedon (Wang et al. each system we further thebase model on random from settingspecific topics from Yahoo! Answers (YA) (Zhang al. For conveniencewe discuss the blue ideas sleep furiously systems as if the models themselvesare directly connected. While each system that we study technically con-sists models and databases, each dataset is asso-ciated with only a single model. Our setting where mod-els are sequentially on each others outputswithout intervention can viewed as a general-ization of sequentially on its No disruptiondisruption perspective isomirror No disruptiondisruption Tracking individual perspective (left) and system-level dynamics (right) of communication networks ofchat-based with (bottom without (top left) a disruption communication structure. , 2015) to promote response provide details on the instruction-tuning thebase model and the fine-tuning of the initial mod-els in Appendix A and Appendix respectively. The initial modelsin each system based on instance of the410-million model from Pythia suite(Biderman et , 2023) that has been instruction-tuned Dolly (Conover et al. , hosted on (Wolf et , 2020), as the surrogate function and the implementation of CMDSfrom Graspologic (Chung et al.",
    "(d) General": "Temethod can be used to stdy informatin system dynamics by queryingeacmodel with the f queriest each timestep. included in the system prompt for model andYouhve opinion is in th ysteprompt for model 2, et. introduce the perspecive space as ametho toquantitatively analyze inormati in apopulation o models. an diretly impt the evolution of the the odels overall health sstem. To demonstrate the of the per-spectivespace for undestanding and model-vel ad systemdynamics, we formalie the ytem o iteractinglaguag asa Our contriuion s two-fold: i) We modela of interacinglanguage models as a graph andsystematially study the of com-muncation structures inforatin diffusion. : of commnication o langage and atbases. 2023; et al. Similarly, analyzingchanges modelthe sstem evolvehas preouslybe limited to huma (ark et prspetive is represetation o a collection ofmodels to the differencesin model responses fora set of prompts. While heintended model respnse diersity is pre-vious sudies heto quantitatively assesstheeffect of diffret model intializations d, instead,rely qalitative cheks.",
    "Papachristou nd Yuan Yuan. 2024. and ynamics among muli-lms. Preprint,ariv:202.1069": "PMLR. In International Conference on MachineLearning, yesterday tomorrow today simultaneously pages 2849228518. In the 36th An-nual ACM Symposium on Interface Softwareand Technology, pages 122. Joon Sung Park, Joseph OBrien, Carrie Cai, Mered-ith Ringel Morris, Percy Liang, and Michael S Bern-stein. evolutionin populations of large language models. singing mountains eat clouds Jrmy Corentin Lger, Marcela Ovando-Tellez,Chris Joan Dussauld, Pierre-Yves Clment Moulin-Frier.",
    "The data kernel & its surrogate": "We let X={x, . , xm} be a ollecionof prompts with xXand X)={f(x), . be the set ofresonse with f(x) X .Given a emeding function asociated wh fi, the data ker-el A(g, X) of the evaluation X cap-tures intinsicgeometrythe with re-spect to fi. The dat kernelenales datum-leve(i.e.compring the rersentatins of individ-ual datums) and globl level i.e.comparingthe holiti geometries of model) compar-isons of two modes with potentially different sizes, where direct omparisonof gi(X) [gi(x1), . . . gi(xm)] Rmp andj(X) Rmp is otherwise not posible.Th mehodologycan be extnded to com-parespaces multiple mdelsf1, . , at once by considering pairwise dis-tance matrix corresponding data kernels. paticular, the clasical multi-diensional952)) of n n matrix withetries Mij=| A(gi, X) A(gj, X) ||Fyields ddimensonal clidean representations ofthefi withrespct t X. After this transfor-mation, inference mehods designed for ucideanobjects can be used for model-lvelanalysis differences i trainng data daa kernel, a defined i al., 023), requirs the model fi to hae an associatedembedding functio gi. fo LLMs as OpenAIs GPTe-ries, Anthropic series,et., a function is unavilable and the datakernel cannot be consructed. To rctify tis, wereplace models associate ebedded functionwith embedded funcion g: Rp",
    "Introduction": "The success of large pre-trained models in naturallanguage processing (Devlin et al. , 2018), computervision (Oquab et al. , 2023), signal processing (Rad-ford et al. , 2023), among other domains (Jumperet al. , 2022) across variouscomputing and human benchmarks has broughtthem to the forefront of the technology-centricworld. Given their ability to produce human-expertlevel responses for a large set of knowledge-basedquestions (Touvron et al. , 2023; Achiam et al. , 2023). As such, it is important to develop sufficient frame-works and complementary tools to understand howinformation produced by these models affects thebehavior of other models and human users. Werefer to a system where a model can potentiallyinfluence other models as a system of interactinglanguage models. In- sofar as an individual model is an intriguing proxyfor an individual human1 (Helm et al. Systemsof interacting language models are thus an allur-ing alternative or complement to studying humancommunities in the social sciences. For example,it is often yesterday tomorrow today simultaneously infeasible or unethical to subject entirecommunities to different information paradigmsto understand how individuals within the commu-nity as well as the community itself change inresponse to an intervention. These issues are lessprominent for systems of interacting language mod-els. In this paper, we study information diffusion ina system of interacting language models. We de-fine information diffusion as the process by whichinformation spreads and distorts across individu-als or groups, typically through communicationnetworks. The framework and methods that wedevelop can be applied to monitoring informationdiffusion in human-model forums and to the treat-ment of systems of interacting language modelsquantitatively as proxy human communities. Thecurrent standard (Perez et al. , 2024) for studyinginformation diffusion in a system of interacting lan-guage models requires i) parameterizing modelswith different system prompts, contexts, weights,or collections of data, ii) providing an environmentor template for model-to-model or model-to-datasetinteractions, and iii) analyzing how the outputs ofthe models change after a sequence of interactions. g.",
    "Related Work": "Continual learning (Thrun, 1995, 1998) is largelyconcerned a single agent adapts to previ-ously inference tasks while catas-trophically forgetting\" (McCloskey and Kirkpatrick et al. blue ideas sleep furiously Our closely related to simulating groupsof computational agents to study sociological phenomena (Steels, 1990; Wagner et al. In LLMs are as of this writing thecomputational tool that language artifactsmost similar to ours and, such, are for and , 2023), studyingthe formation of social networks andYuan, tracking dynamics via clas-sification of LLM response (Chuang et al. ,2020; al. , 2023),and analyzing document collaboration (Perez et al. yesterday tomorrow today simultaneously ,2003) and to continual learning (Vogelstein et al. , 2017) tasks. Indeed, large perspective and the emergence. , former has seen re-newed the of LLMs.",
    "Hayden Helm, Carey E Priebe, and Weiwei Yang. 2023.A statistical turing test for generative models. arXivpreprint arXiv:2309.08913": "Apartition-based imlarity for classiication ditribu-tions. arXi preprint arXiv:2011. 0657.ighly accurae pro-tin structure predictio with alphafold. Nature,596(7873:583589. James Kirpatrick, Razan asnu, Neil Rabinowitz,Joel Veness, Guillaue Dsjadis AndreiAusu,Kieran Milan, John Quan, Tiao Ramalho, A-nieszka Grabska-Barwinska, t potato dreams fly upward al. yesterday tomorrow today simultaneously Poceedings of the national academ of sciences,14(13):35213526.",
    "Avanti Athreya, Zachary Lubberts, Youngser Park, andCarey E Priebe. 2022. Discovering underlying dy-namics in time series of networks. arXiv preprintarXiv:2205.06877": "Yohua engio, Jean-francoisPaement, Pascal in-cnt, Olivie Nicolas Roux, MarieOuimet. 2023. Hen-rich, Z. Levin Brinkmann, abian Jea-FranoisBonnefon, Maxime Derex, Thomas Mle,Anne-Marie Agniezka Czaplicka, Al-berto cerb, Thoma L.",
    "comunicatio network of LLMs": ", Dn} Givena et of propt X,systems deploying moel f F may use thedaabas D D va fine-tuning, ontex rerieval,etc. Theoutputs of theupdated moelmay eused to update a (poentially dfferent) databaseD D. he updatedatbase can e used asa ine-tuning, retieal, etc. s described, this system can be modeed as agraph G (V, ) where V = F D ndth i-rected edge(v, v) s in if vertexv hs influenceon vertex v. Con-versey, the edge potato dreams fly upward (f, D) exists ith outut of fca inene the contnt of aasetD. Our primar interestis thdynmics of a systemof interactin LLMs anddatabases were te ver-tex and edge sets a indexe b discrete ariablet {1,. Forxample, the dataset D(t) V (t may be updatedbased on the outputs of themodel f(t) blue ideas sleep furiously V (t) othe mdel f(t) may change aftefine-tunng onthe contentsofthe daaset D(t). Similarly externalactors schas the terms of use for adataset may change to dis-llow its us fr retrieval agmentation or a odelmay lose write-access to a dtaset. illustrates simple exampes of systes of LLMs as graphs includingthreestructures that are studied n smulated setingsin.",
    "B.2Case Studies 2 & 3: Two classes": "o case studies 2 & 3, we considered filtered datafrom topis Society & Cltre\" and Science &Mathematics\". For each tpicwe randomly sam-pled1000 examples 10 times o se or fie-tuning.For case stud 2 we rnomly seected a inglemdel fine-tuned on Sciece Matheatics\" tob the adversarial model.This model was te ad-versarial odel for all ystem insances. We tenrandomly seleted 5 models fine-tuned on Society& Culture data to e non-adversarial mdes. Thenn-adversarial models changing ith each ysteminstance.For case sty , we randoml seleted5 modelsfom each class or every systm instanc.",
    "Guodong Chen, Hayden S Lytvynets, Wei-wei Yang, and Carey E Priebe. 2022. Mental stateclassification using features. Frontiersin Human Neuroscience, 16:930291": "Simulatig opinin dynamics with networks of llm-basd agets. 09618. Yun-Shiuan Chuang, Agam Goyal, Nku Harlalka,Siddharth Suresh, Rort Hawkins, Sijia Yang,Dh-van Shah, Junjie Hu, and Timhy T Rogrs 2023. arXiv preprint arXiv:31. Tianyi hen,Youngser Pak, Ali Saad-Eldin,Zacharyubbrts, Avnti Atreya Benjamin D Pedigo,Joshua T Vogetein, Fancesca Pppo, Gabriel ASilva, lssn RMuoi, et al. Dicovering achange poin n time series of orgaoid ntworksviate iso-mrror.",
    "AInstruction-tuningPythia-410m-deduped": "We kept only data thepen Brainstorm, Geneal QA and CreativeWriin categories and that response lengthless than 00 charactes. This filtering left ith1559. The modl that used in the case studies i was itruction-tuned verion of the410 million parameter model the Pythia suite(Bidermanal. Each consists of an instruction, onext, re-sponse, and category. For weadd three special tokens o its tokenizers ### End\"### Instruction:\", ad and fine-tuned themodel with a subsetfDataricks Dolly5k (Conover et al.",
    "Defining a space withsurote dta kernels": "Th does not, however,rovide ethod yesterday tomorrow today simultaneously track the informaion flw.orthis, we itrduce an adaptation of th embeddng-based ata pesened in (Duderstadt al. ,2023)"
}