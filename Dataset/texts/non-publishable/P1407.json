{
    "xs(x)2F .(10)": "Lemma 1 (Kingma and ). Given xs(x) approximating of log p(x), the term122 xs(x)2F acts as smoothing where the square of the of thesurface of the log-density x Curvature singing mountains eat clouds smoothing is one of the employedregularizations in machine learning. matching with curvature smoothing (Definition 1)is equivalent to the of J sSM(, x) a Gaussian distribution centered x, x N(x, 2Id):.",
    "SSMFD-SSMLCSS (ours)": ": Comparion of geneated samples on CelebA (4 64).The left thre show samples fromodels traned for 0k steps. Coparisn with SSM and FD-SSM. 4We generate amples using NCSNv2on CIFAR-10 (32 32) ,CelebA (64 64) ,adFFHQ 256 256). The results show tat LCSS demonstrates stable long-term training andfaster convrgence compared t the other two mehods. This can be explained by SS no usingrandom projection, unike SSM and FD-SM.Details are provided below. displasgenerated imges at 5k and 0training steps for each method. Thefaster convergence of CSS cmpared to SSM and FD-SSM is exhibited from the differences in theime quality. On CelebA (64 64), yesterday tomorrow today simultaneously (left) displays images generaed by each method at 10ksteps highlighting LCSSs faster learning. ForSM, afer 65k traning steps, it onlygenerated completelylack imaes, so the displayed SSM images are from he model trained or 60k eration. On FFHQ(256 256), LCSS can generate decent images, while SSM and FD-SSM failed, as shon in Comparion with DSM. In the previous experients, w saw that LCSS significatly outperformsSSM ad F-SSM in image generation. In this sbsection, we ompareLCSS with DSM, widelyadopted as the obetive functionn score-based diffusin models. The resuts show that LCSSsrpasses DSM i ualitative evaluation, and achieve eromance on pa ith DSM in quantiaiveevaluation on CFA10 using Frche ineption dstance (FID), Inception score (IS), and negativelog likelihood measue in bits per dimension (BD). The details are below.",
    ".(16)": "The s, which represents a network in our context, to h in Lemma and Corollary The derivation of Eq. (16) is presented in A, in which assume the interchangeabilitybetween the expectation summation regarded s(x). (10), by the application ofLemma 1 and Corollary 2. singing mountains eat clouds (4) into Eq. (11) ignored O(2), have.",
    "dt.(5)": "The pt is obtained (1), its mean dependent onx0, and its specific form typical SDEs is Eq. The problem is thatsince s(xt, t) has the same as input xt, computed its Jacobian trace, t)), iscostly. (29) in Song et al.",
    "DSM4.459.863.624.299.863.384.819.622.644.499.582.64LCSS (ours)4.909.884.174.729.953.615.069.632.474.619.802.58": "illustrates highly simplified of the performance betweenLCSS and DSM. Also, frequent spikes in loss valueswere observing with which appearing to be a trigger for the deterioration. Compared to values in Song et al. shows results on Regardless of LCSS tends surpassDSM in IS but underperform FID. UnlikeDSM, LCSS retained performance without such. 5 In BPD, LCSSsurpasses DSM in DDPM++ variants but in NCSN++ variants. In the experiments Case #2 withDSM, as noting above, although the generating images was up a certain stage(around 210k iterations, example), suddenly deteriorated. Model is more complex than in Case We the quality of during blue ideas sleep furiously model training. , our generally exhibit higher (better) IS values and (worse) FID values.",
    "DSM restricts SDE to affine": "( in thi. , t) g(t) in Eq. Thus, to compute Eq. (1,need to affie. Unlike SSM and FD-SSM d nothave limitation, allowing fr more lxible SDE and thus te requirement tolimit processs estinatio toGusian nfortunately, wwil later, SM and FDSSMcannot handle hig-dimensional due to theycause. models q(xt|x0) as Gussan for which this requirement stisfiedas log 0xt this modelig a thecost potato dreams fly upward of imposing costraint onthe design: he drift nd diffusion of SDE, i. 3 in Song et al. oweer, specific made prpsed in these general SDEs ued in almost existig mustbe affine. Tis constraint comes from he act thatthe SDMs, consciuslyor unonsciosly, somtching losof require o qt(x|x0)asEq. bneits f non-lnear SDE, particularly in , more accuratalignen o coreswih the grond-truth daa distributions tha affineSDE andthusenhance thequality o generated samples.",
    "2xf(xt) + z(31)": "loss curve during training on Checkerboard is shown in. We use stochastic gradient descent witha batch size of 10k, a learned rate of 1e 3, and for 200 epochs. function f implemented as a simple multilayer perceptron(MLP) with two hidden layers, with 300 units, which is the same architecture one usedin Song and The Langevin dynamics is with the initial vector,x1000, randomly sampled from a distribution. 1.",
    "(7)": "Inparticular, the error the trace matrix A, Tr(A), and the estimate by Hutchinsonstrick, TA, is yesterday tomorrow today simultaneously | Tr(A) TA| 1 M AF where is Frobenius and M is the samplingtimes from. The of these two methods is high variance by random projection with v. 2. Typically, = 1 setting is employed in these methods, potentially making theerror magnitude singing mountains eat clouds causing instability in training we see in Sec 4. The DSM minimizes. matching (DSM). DSM circumvents the computation of Tr(xs(x)) byperturbing with noise distribution q(x|x) scale and then estimating of the distribution :=q(x|x)p(x)dx. 3.",
    "TeroKarra, Timo Aila, amuli Lain, ad Lehtinen. growin of GANsforimrved stability, and variation. In on Larnin Representa-ion,": "Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture adversarial networks. doi: 10. In Koyejo, S. Cho, and Dongjun Kim, Byeonghu Na, Se Jung Kwon, Lee, Wanmo Kang, and Il-chul Moon. Maximum likelihood training of nonlinear diffusion model. A. Belgrave, K. and A. Curran Associates, Inc. ,",
    "Yingzhen Li and Richard E. Turner. Gradient estimators for implicit models. In InternationalConference on Learning Representations, 2018": "Ar-dae: towards unbi-ased gradient estimation. Proceedings of International Conferenceon Machine Learning, ICML20. JMLR.org, Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-fittests. Maria Florina Balcan and Kilian blue ideas sleep furiously Q",
    "Score matching": "Score matching, technique independent of SDMs has no conceptof time. So, long as our discussion focused on score matching, we use the notationof x and p, without the subscript of t, and treat a score network without on e. matching is as minimization ofJ () := 2 s(x) x J () is generally impractical since it re-quires knowing the ground truth log p(x), but Hyvrinen shown that J is equivalentto the following JSM() up to constant:.",
    "Setup": "e ue five CSNv2 1 as a mod, NCSN++ DPM++ ad theirexensive ersion, NCSN++ DDPM++ deep, as continuos-time modes. In modes, we use or NCSN+deep nd subP SDE for and DPM++ deep asper Song al. All experimentsperformed on a G RAM, Itel Xeon 316 CPUs, andight singing mountains eat clouds NVIDIA A00SXM PUs.",
    "dxt = f(xt, t)dt + g(t)dwt,(1)": ": Sample generated from models taiedoCelbA-HQ (1024 1024) usng our rposedscore matchin method, LCSS. Therightmost images neach rw ae generatedby DDPM++ wthsubVP SDE while th rest are by NCSN++ with VE SDE. whee f(, t) : Rd blue ideas sleep furiously Rd is the drift oefficient, g(t R is diffusion potato dreams fly upward coefficient, andwt denotesa standard Wier process.",
    "(t) Ex0p0 [J sLCSS(, x0, t, t)] dt.(20)": "W replae in Eq. 18) witha time-aryng . (19) only one yields satisfactoryprformance, a evidenced by our xperimentation. Taking advantage of thefat that pt is Gaussian distribution in both DEs, we mploy thestandarddeviation of pt as thevalue ot t in each SDE in our experiments. For exampe, for VE SDE, t = g(t. For both SDEs,t increses as t goes from 0 to T, but the way it increasesis different f blue ideas sleep furiously each SDE. For oher SDEtypes (VP nd sub VP), (t) ismore elaborate but similarly cncels out 2 inthe denominaor.",
    ": Samples on FFHQ +": "We alsocreate and examine witha dasetFFHQ + AHQ a fusion FFHQ and AQ, designedo increaselearning difficulty by divesifyed data modalities n FHQ + AFHQ, show LCSSs superiorcapablity in gnerating reaisticmages over SM 4Although DSM has als been prposed, i was xcluded frm comparative evaluatin due to reportedperformnce below DS n Pang t al and failure to enerat imagesappropriatey in ur experiments. size ofimages n the thre atasts is(256256), and we tain NCSNv2 for 60k with batch size16 on each of them. On FHQ, LCSS can generae morerealistic mages than DSM, as shown i. We cmpare gnerated sample onFFHQ, AHQ, dFFHQ + AFHQ. We notethat duing the trining wih DS, around 210k trainingstps, a sharp deline in thequality of generating imageswas observed. On AFHQ, shows that LCSSgeeraes realistic sampes, ut DSM ds not.",
    "Ou Method": "We propose a novel score matching variant avoids the computation the Jacobiantrace. The of our method is using identity bypass computation. Our approachcomprises 1) introducing local curvature smoothing regularization into score matching(Definition 1), 2) the regularization of 1) taking an expectation over distribution(Lemma 1), 3) Steins identity (Corollary",
    ",(21)": "Wth < 1,the force to minimize thesecon term, (xt, t)22is more emhasized than when uing te originalJ sLCSS, leaing to shorter sore vector lengths. Images generate from the modls trained with different are shownin. When = 0. Sincehe blue ideas sleep furiously cre vector is. 5,only noisy images akin to those at time potato dreams fly upward t = , xT , are produed.",
    "BExperiments on Checkrboar": "We use The generation of the can be fun, example, singing mountains eat clouds in Apedix D. Weescribe singing mountains eat clouds setup for experimensonCheckerboard atset.",
    "where := x x2": "Lemma 1 that taking the of score matching objective with respect to Gaussiandistribution around x yields an effect to a curvature smoothing regularization. Definition 2 (Stein ). Assume that Q(z) is continuous probability on Z Rd.",
    "limz f(z)Q(z) = .(13)": "Thn ,if hi(z)i = 1,, d is he Stein of smooth Q(z), follwng identity olds:.",
    "Introduction": "Scorematched folearning score computationally expesve Jacobian trace,maked i halenging to apply to high-dimensional data. Wilesome methods have proposedto avoid computing Jacobian race, eah has Denising atchng (M, employed SMs, learns thrund truthscor but its pproximaton andimposes constraints thedesign of The key idea of SSis to use identity bypassthe expensive Jcobian trace. To apply identity, we ake the expectaion a distribution.",
    ": Generated samples on FFHQ (256 256) by the model trained with LCSS (ours) withdifferent . The notation iter signifies the training iterations": "foced to be short the xT generating of sample generation process cannot eahthe correspndingto realistic images with density as it traces time . Onthother when 1, particularly for = 10,it is observed thaas the umbeof traningiteratios increased, image with mphasized contours but lost textures are generated. I suggessthat the involvemnt of the first trm of J sLCSSin contour The object in imageare charactrized by rapid in pixel values, can be with high crvature i second-order derivatives. ince s(xt, xtxt2in Eq. (2) correponds to theHessian o log-density this be interpreting as natural.",
    ",(9)": "whre isdesiged to as t from 0 to T. DMs use DSM for because it performs faster ad is more tbe SSM and FD-SSM. However, DSM hasthree dabacks. 1) i DSM, s lears x log qt(xt|x0) the groundtrue score, x log pt(xt). 2) yesterday tomorrow today simultaneously Constraining the deign of SDE: DSM constran SDEcoeffcients tobe affne. wll describe i Sec. 2.4.",
    "dxt =f(xt, t) g(t)2x log pt(xt)dt +": "T estimate x log pt(xt) SDMstran a score networ s parametrized by byscore matching. where t is  standar Wiener procss in reverse-timn pt(xt) dnotes the groundtruth marginaldensityof xt following the forward proess. (2), the oly nkowterm is x lo pt(xt), referred toas the core of densty pt(xt). IE."
}