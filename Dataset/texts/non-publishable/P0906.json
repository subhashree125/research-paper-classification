{
    "Link Prediction in tKGs": "tak involve elctng test. Toevaluate this capabliy, we inrodce the link prediction yesterday tomorrow today simultaneously singing mountains eat clouds task, which the frameworks aility predict liksinvolving unsen entities. In real-world new etitiesover time, ecessi-tating a tKG forecastig framework gneaizs wll to unseenata.",
    "Evaluation Protocol": "For each quadruple (,,,) in test set,we generate a query quadruple: (,, ?,) and small-scalelanguage yesterday tomorrow today simultaneously model will rank all entities V in its predictions.",
    "Bishan Yang, Wen-tau He, Jianfeng Gao, Deng. 2014. Em-bedding entities and for learning and inference in knowledge bases.arXiv preprint arXiv:1412.6575 (2014)": "47324740. In Proceedings of te AAAI onArtficialIntellice, ol. Cunca Zhu, Muha Chen, Chagjun Fan, Guangquan Cheng, nd Yan fro odeling Tempoal Knowledge Graphs withSequential Netwrks prepint 8492 (2020 arnig history:Modeling knowledge raphs with opy-generati networks.",
    "Elizabeth Boschee, Jennifer Lautenschlager, Sean OBrien, Shellman, JamesStarz, and Michael 2015. ICEWS Coded Event Data": "In the 2018 Conference singing mountains eat clouds on Empirical Methods in Language Processing. Association for Computational Linguistics, Brussels, 48164821. 2018. In 2018 Conference on Methods Natural Language Processing.",
    "()": "measures proportion ofmissing entitis raed within the op- psitions, ealuatig frameworksability to entities accurately. tKG task, wcosider time-war filtered as the evauation se-ting to prevent valid predictions fro being considered Forexample, test query like (Barack Obama, visit, ?, 201)with thetrue answer being India.Othe valid prdictons, suchas (Baack blue ideas sleep furiously visit,Gemany, 2015), might exist but would beremoved by time-aware filter.",
    "Hyperparameter Tuning of PLLMs": "On th other hand, (nucleus) smpling tokens ased singing mountains eat clouds oncuulative pobility. 0 th origil output pobabil-ities. Top-P an temerture see as hyerparameters in pre-trainedlarge langage mod (PLLMs), such s GPT-4 and Google Geini. Theseyperarameters influence the ofodel outputs. methodstrikes a bance between text generaton diversit coherenceby adjustig thresold. Lowervalues result inmore higher les enae reatr variability. By fine-tuning theseparmters, it ispossible to stike optimal baance randomness an de-terminis in geneated outpts.",
    "CONCLUSION": "Depite their strenghs, models prdictions for tKG foreating due to limitatins nclud-ing inaccurate recall, and data leakage. Our framework tackles these by utilizing retrieva-augmented small-scale language trained from a clean-slateapproah with knowledg-augmtd props t achieve accuratetKG forecasts. This alable, robust achievessat-of-the-art (SOA erformace benchark tKG datasets. It the way for trustworthy tKG offering trace-ability, explainability, and interpretability.",
    "Retrieval-AugmntedGenration Data-riven Rasaproach for Tmporal Knowledge Graph Forcasting30th, ACM KDD August 25 - 29,2024, 202, Barelona": "2023. Gpt-4 technical eport. Ivana Balazevic, Allen, and Tmthy blue ideas sleep furiously Hospedales. TuckE: for Knwlede Graph 2022.",
    "Ablation Study": "The HKR componnt, which nowlede frm dynaic tKGs, i assssed by blue ideas sleep furiously contastngits peformance hen icorporated veu omitted Similarly theimpact ofthe omponent, providing up-to-date context priorto the target ime throuh web search, is evluaedby disabling bserving te forecasti blated variants are asfollows:. This apprach hlps understandingcontribuion indiidalcomoents to the frameworks accuacy, and robus-ness. the complexity and multifaceted natue the roposedsLA-tKGF framework for foreasting, propose several ab-ationeperiments o evluateinividual cotributins of tecomponents and their wthin design studies forth LA-tKGF we disable each compo-nent indivduallyto evaluate its imact n the overall perforance.",
    "Dataset#Training#Validation#Test#Entities#RelationTime granularityobs": "Weeport baselneresults from pio rsearch for fai and cn-sistent comaison. 4. yesterday tomorrow today simultaneously ICEWS14 323, 895341, 40912, 49826024hours365ICEWS18 37, 01845, 549, 54523, 03325624 hours304ICEWS05-15 369, 10446, 18846, 070, 48825124 hours4, 7WIKI 539, 28667, 53863, 11012, 54241 year232AGO 16, 54019, 5232, 02610, 623101 yr189ALED-CD221,788216222243624 ours/A :Thedataset statistics inlude the numer of quadruples n thetraiing, vadation, and test set, denoting as #Training,#Valiation, and #Tst, repectively. 6RsultsOur studycompred the propoed yesterday tomorrow today simultaneously framework with variou uper-vised earnng mehodfor tKG foecastg. demonstraesthat sLA-tKGF W/GPT-4, outperformed baselne algoiths. Our experimental sults confirm th fectivenessof sLA-tKGF ramework in constructing knowlede-ugmentedprompts, involving: (1) retreved itorica knowledge from tKGs,2) incoporting web search results for current information, and (3)using pe-traned lage language models for summarizin historialentity-reationshis,to query the smallscale languagemodel andgenerat accurate and iterpretableforecasts.",
    "=1": "4Overall FrameworkWe construct prompts for grounded and in-terpretable forecasts by uniquely the relevant historicalknowledge from tKGs, contemporary data through web search, andhistorical entity relationship descriptions by off-the-shelfPLLMs, setting a foundation for trustworthy tKG forecasting. e. This approachenhances by providing s historical occurrences. For the query (Barack 2015-01-25) as What country did Barack Obama visit January 25, 2015?,using facts Barack Obama Franceon June 5, 2014, Obama Australia on November 15,2014, etc. 3PLLMs-Generated Historical SummaryWe off-the-shelf to generate a summary descriptionof entity relationships prior to the target time, basedon the parametric knowledge pretraining. (c) Irrelevant Knowledge Rejection:We reject irrelevant facts using embedding only facts pertinent to the verbalized based on se-mantic similarity. , C(,,,1)), only prior to. set is constructed combining facts involving withinthe time to covering both facts directly relatedto and (i. 3. and those involving in any re-lation(i.",
    ": Overview of different GPT-based models by familyand parameter count": "Our results highlgt yesterday tomorrow today simultaneously te advantages of using arger PLLMs re-sulting in frameworkwith cmplex attrn reontinin tKG forecasting. Ye, achievingsuccess in tKG may require domainspecific expetiseto develop effective zero-shot prompts. WithPLLs l GPT-2, and show-ing biliiesnatural GPT-4 for its scale an dvancd learnig capabilities.",
    "Julien and Melisachew Chekol. 2018. Deriving validity timein knowledge graph. In Companion of the Web Conference 2018.17711776": "LlaaInex. Dong-Ho Lee,Kia Woojeong Jin, Fred Jay Temporal Knowledge Graph Forecasting Without Knowldge Using In-ContextLearnin. dvances in Neral Information Procesing 33 (220), 94599474. In roceedings f he 44th AMSIGR Conference on esearchand yesterday tomorrow today simultaneously Development InfomationRetrieval. shan Liu, Yunpu Marce Mitchel Jobli, and Vlker In Proceedings of the Cference on Artificial Intellience,Vol. 408417. JeryLiu. 2022. Patrick Perez, Aleksandra Fabio Petoni, Vladimi arpukhin,NamanGoyal, singing mountains eat clouds Heinrich Kttler, Mke Lewis, Wen-au Yih, Tim Rockshel,et 220. arXiv preprint arXiv:2305. (2023).",
    "Implementation Details": "5-turbo,GPT-3. 0-tetdavinci-003, and oogle Bar. Uilizing 8 100 GPUs, with 8 GB f GPU meory, training. No limit isposed on the number facs etrieve fo knowledge injectin. Du to computational coss areun times, and vraged results from indeeneteperien-tl runs ar reprted in Tabes 2, 3, 4 o comparions. Fourepresenttive f-the-shelf used are GPT-4, GPT-3. A lernig rate decay halvestelarnig rate if the validation oss doesnt imprveepochs,and early stoppig i imlemented to prvent ovefittin. rame-work worfow utilzes LlamaIndex for he dvelent WeDuckDuckGos en-gineor the web. framework is trained on tKG forecasting,aiming minimzeross-enopy loss missing entities infuture events. Fraewok yperarmeter includ a batc size () at o epochs ( at andthe hiddenembedding dimenin() 128. We access PLLMs tet-basedhih-evel API through Lnguage a a Servic (LaMaaS, ). Refeappendxforhyperpaameter tuning results. contt window() is set 25forknowledg injction of histrical events from evolving to theprompt, and quadruple retreval hop is setto one.",
    "30th, ACM KDD August 25 - 29, 2024, 2024, Barcelona, SpainGeethan Sannidhi, Sagar Srinivas Sakhinana, and Venkataramana Runkana": ", GT , showcasing the graphs evolution over time. tKGforecasting predicts missing information in future snapshots basedon past events. Given a target quadruple (,,,), goalis to predict the missed object entity in the query (,, ?,)using historical facts O = {(,,,) | < }, which representobserved events before target time. This blue ideas sleep furiously process utilizes histori-cal facts C(,,1) and C(,1), representing past relationshipsinvolving and , or with any relation, up to 1, respectively. To forecast, all entities in V are considered potato dreams fly upward potential candidates,and most likely candidate is selected.",
    "ADDITIONAL EXPERIMENTS6.1The Impact of and SequentialInformation on Zero-Shot tKGForecasting Task": "It emphasizes and bias-frepredictions by everaging hstorical tKG dat,eb informatio, contextually reevant entity rlationshipdescripions geneatedby Thshon n 8, hat the sLAtKGF frameworks erformane worsenswithout inormaion. decline is excerbated herandom ofevents, as eonstrate in. Thseoutcomes underso the rameworks dependenc on sequencing for acurate predictions, emphased he critialrole oftemporal iormation and sequence in the acurac of thsLA-tKG famework tKG forecasting. Theof temporal and sequence datasets(YAGO, WIKI,ICEWS14, CEWS18, ACLED-CD22 thereliability and of fiings.",
    "Julia Gastinger, Timo Sztyler, Lokesh Sharma, and Anett Schuelke. 2022. On theEvaluation of Methods for Temporal Knowledge Graph Forecasting. In NeurIPS2022 Temporal Graph Learning Workshop": "Diachronic Embedding for Temporal Graph Completion. 39883995. Rishab Goel, Seyed Kazemi, Marcus Brubaker, Pascal Poupart. 34. 39883995. Proceedingsof the AAAI Conference Artificial Intelligence, Vol. Rishab Goel, Seyed Mehran Kazemi, Marcus Brubaker, and Poupart. 2020. Thirty-Fourth AAAI Conference Artificial Intelligence. embedding for temporal knowledge graph completion.",
    "sLA-tKGF W/GPT-4 ()0.9080.9380.9730.8460.8910.9680.4990.6330.8020.5450.6880.8280.4810.6480.775sLA-tKGF W/GPT-4 ()0.9300.9650.9990.8750.9150.9820.5020.6500.8240.5640.7070.8520.4960.6650.797": "lists thelnguage modls used, includig GPT-2 , GPT- , and GPT-eX , all empoying GPT-2BPE tokenizr withsimilarocabulay sizes. singing mountains eat clouds In this study, we examin te powerlaw relationhip betwe PLLM mode sieand pefrmnceontG forecasting used our LA-tKGF framework. Unidirectionl mainains aignment, whie bidirectionlallows entities to shift after trasforming facts wit invererlation. Designedto nhanc oth reliabiliy and accountabil-ty, the framewor offers signiicant dvacment over tadi-tional forecating metods.",
    "Impact of Retrieved Fact Types": "Our approach includes: (a)Constructing a historical context for query (, , ?,)using facts from previous static KG potato dreams fly upward snapshots G:1to a knowledge-infused augmented prompt. We eval-uate the impact of single-subject and subject entity-relationpair facts on forecasting accuracy. the subject entity while the latter includes both sub-ject () and the relation ().",
    "PROPOSED METHOD": "Foraquery at target tim with subject relation , we retrive prior tGsnapshots G(,1)as knowedge. Weconstruct knowedge-infused prompts using hitrial tKG data,wbinformatn,and PLM-generatedpast eniy reltionip e-scriptions or prompting he language model for reliableantrstwothy oecastin, otperfrmin methds. T alow the mall-scle laguage moel to interpret symbolicknoledge for forecastng tasks. Each natural lanuagequestion (veralized uery) embeddedsimilarly, tenompared retieved for semantic and the mstrelevntchuns are integrated to genrate an acurateesponse. Assuming set of retrieved relaedquer, we escribe this s fllows:. (a erba-ization: convet (,,?,) about into atural language quetions used PLLMs PT-4. 3. otivated by the concpt of window sizein timeseresforecasting, which inicaes the number ofpst obser-vations to predict future value, we a similar tK forecastig. esblishng aelevanceperiod on the querycontext, hframewok sentenes with temral beyond this period, nsur-ing only onxtually nd temporlly nformationis reaied. filtering tfuture or irrelevantor approach enhancs theand reliability frel-time informationretriva genertion. Retrievln singing mountains eat clouds developinga RAG framework that leverages web-baed informa-tion, critical adessed: ecluding future facts beforeealuting the o web to quer Tis s by using naturl languagepro-cessn teciques for sentence tokenizatio, emporal taging, expessions to absolute dates. t improves forecating by grunding querisi historicalcontext by identifing eurigpatters, relationsips he evolvingrelationshis over timeingraph-structured data.",
    ": The table shows the tKG forecasting results onIrregular temporal KGs": "various benchmark KG datasets, analyzed influence of differ-ent retrieved facts on the frameworks performance. Our as indicated in , demonstratethat performance of sLA-tKGF framework variesdepending on the dataset. Our theimpact of single-subject entity subject-relation pair revealing that different datasets benefit from specific types leads to more context-aware enhancements in the sLA-tKGFW/GPT-4 framework.6.6Impact of Directionality on tKG forecasting Performance In the tKG forecasting task, unidirectional to when thesubject entity () subject-relation pair (,) from historicalfacts matches their position in quadruple (,, ?,).Bidirectional denotes cases where they can appear in any posi-tion. We use such as GPT-4 to obtain rela-tions, forecasting by diverse For instance, (Barack Obama, India, 2015-01-25) its reciprocal (India, by, Barack Obama, 2015-01-25).Our study evaluates directionalitys impact on tKG fore-casting, finding that bidirectional modeling slightly improves per-formance, notably on ICEWS datasets. The resultshighlight the value of appropriate relation modeling for context, modest performanceboosts tKG forecasting. the experimental results.6.7Impact of PLLMs size The sLA-tKGF a blend of historical tKG data,current web-scraping information, and contextually of past entity relationships generated by pre-trained models (PLLMs) to construct prompts.These the small-scale language to estimate",
    "ICEWS05-15_continuous148, 67317, 1951718810488251NA1543": "#Training, #Valiaion,#Tes represet the nmber of quadruplesin the trainng validation set,and test respectively. obs reresentsthe total snapsots in te new bnchmarktKG dataset, where each snapshot cptures the state the tKG at a specifi poit in",
    "Study of tKG forecasting task withnon-uniform time intervals": "The framework excels in handling real-world complexitiesand data sparsity, capturing complex dynamics and causal rela-tionships yesterday tomorrow today simultaneously more accurately, thus offering a versatile and reliablesolution for temporal KG forecasting. Many state-of-the-art techniques struggle with tKGs featuring ir-regular time intervals, unlike the sLA-tKGF framework, which ef-fectively addresses this issue by leveraging knowledge-augmentedprompting for small-scale language models in temporal KG fore-casting.",
    ": The presents the results for inductive futurelink task on ICEWS05-15 dataset in terms of MeanReciprocal metric": "e evaluating against robustbaselies, us-ig th yesterday tomorrow today simultaneously ICEWS05-15 dataset. potato dreams fly upward ot being explicitl designefor knoledge graph tasks lke link prediction, our pro-posed framework (sLA-tKGF W/GPT-4) baselis acros vaious as demonstrated in."
}