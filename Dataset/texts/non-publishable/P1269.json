{
    "Experiments": "Datasetsmodels. Al image cupions are at the highestseverity level 5. We use ResNet-50 , ViT-B/16 , ConvNXt-L re-traied forImageNet-C and SegFormer-B5 on ADE20Kfor ADE0K-C use the GN of ResNet-50 for Compared For emanticsegmentation, , Tet More dtails are in Appendix F. 2. Evauation metrics. Top- accuracy (cc) repotedonorruption type fr classi-cation. For semantic segmenation, the ntersetion-over-Union (mIoU)is reported. The of DUSA ll come with and standar deviation statisics idependent Implementation details. batc size 64 for test-time classification tasks unless 00001 fr USAand Diffusion-TTA. For baselines, SGD with momentum . 9 r Adam optimier is usedin linewith the lteratre. 0006/8 is used, olowing. More details i F.3.",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "In geneal, empirica rsults ofenepend on implicit assumptions which shouldbe blue ideas sleep furiously articulated. The authors arencouraging to create a separate imitaions\" section in thirpaper. g. g. The pper should pint out any strong assumptions and how robut the results are toviolations of thse asumption (e. , indepenence assumptio, noiseless settins,moel well-pecificaton, asymptotc approximations only holded locally) Th authorsshouldreflect on how these assumptions might b volae in practce and what theimicaions would be. , if th appoah wasnly tested on a fewatasets or wih fe runs. Or a speech-to-ext system might not beusing reliably to provide closed captios fo onlinlectures because it fails to handletchnical jargon. author shoul eflect on factors that influencetheperformance of the approach. For exale, facial recognition algoithm may perform porly hen image resolutionis low r images are takn in low lighting. he authors should reflect on te scope of caims made, e.",
    "+ LogitNorm uniform select multinomial select (6) (DUSA)426646.365.170.8+ null conditioning (DUSA-U) 427146.164.770.5": "singing mountains eat clouds he m-povement is made without training diffusion andtherefore can be viewed as exploited semantic orme diffsin Howvr, the outcomes re thebaselie Pixlate. We onjecture that sch corrpton might be OOD ven for difusion modelith robutness. The furtheradaptation diffusin mdels reoves concern, pushingall result to a singing mountains eat clouds competitive evel. 3. 3 with budget m = 2 using with unifrm samping, which is then iproved by ourultinomial selection in the penultiae line. inclusin of LogtNorm yields a sgnificanl boosting accuracy To provide basis for ablation of bdge scems, budge is fro4 t 6 ith in prrmace. For scheme, we find m = 2across varied b, ad stick k =4, m 2for USA. 3. instantly mit-igaes he isue bringsabouta consistet of +21. We believethis can or caimof the exstenceof structured inherently in dffusion models. (13), while sigificntly computatonal cost assoiatedwith diffusion model B. At a smaller bach size of 4 (dashedlines), budget schem m plays vital role perforance.",
    "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to NeurIPS Code of Ethics and theguidelines for their institution.",
    "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "While NeurIPS does not require released yesterday tomorrow today simultaneously code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. In general.",
    "BN Adapt .BN Adapt has the straightforward idea that the statistics in Batch Normal-ization layers are data-dependent, and therefore can be adapted at test time for better generalization": "Lastly, timelnessreweghting stategy attain stable and robust adaptaion. EATA EATA devoted improvingefficiency pevented knowledge a antiforgetting regulrization based fisherimortace is proposd. With the objective of diffusion modes, knowledge is backpropagating throughconditioning to the task. time efficieny,only parameters in Group/Layer Normalization layersare optimied. CoTA fcues on performing est-time adaptaionfor continually changing disiu-tions. nlyaffine in rbst batcnormaliatio blue ideas sleep furiously underg training. Furthermor atch nrmalization lyer recalibrate noraliztion statisticby applying exponetial moving to selected amples. Addiionly, ony affine paraetesin batc aeadaped. bythis, it proposs sharpness-awareand relible minimiaion for Grop/Layer Normalization-based models. RoTTA dedicating to conductig test-time on streamscharterized by continually dat distributions andtemporally corrlating label distributions. RoTTA. Frstly, it geeraes more robust and reliable seudo lbls through data augmenttionsand mean-teacher architecture to error Meanwhile, in the mean-teacher architecture, all the parametersofthe stuent model remin trainable. In the work,test-time batchnoralization s mploed t recalibate te statstics withn BatchNrmalization,where featurs nrmlzed basing on current bachs statistic CoTTA. Diffusion-TTA. Diffusion-TTAs proposed to adaptpre-training task modls using ebackfrom pre-trained dffusion models. To begin with, it deveops a prediction-balanced sampled strategy in uncertainty andtimeliness, ensred the maintenance oa robust sapshot of the test for adatation.",
    "Preliminaries": "Test-time A moel well-raind surce can face severe perfrance potato dreams fly upward singing mountains eat clouds degra-dation on t-of-distribution (OOD) target saples.",
    "ABroader Impacts and Limitations": "Impacts. In work to incrporate models into task daptaion Theleverge of knoledge from generativemodeling whoe picture of the scenario to the task enabled better ndadaptablity and s favorable soe relevant senarios anlysis and quality conrolof manufacturing inenvionmets. A possible remedy for tis is tofostr the more unbiased orbaanced training generatie odes. Limitations. work an effective orrowed semanic priors models benefit the dsriminatve ask model thatemandsadaptatio at test time Our approach achievesignificant daptation gains agains diffusion-based TTA methods, there s stll agap in the time comparing o mehodsthat onlyupatehe tas model. Our method, however, pefomance and is thus appealing to another set of asks traing slight lss for boosteacuracy stoleale, e.g. , non-emergent diagnosis.said,we hihligh hat our theoreical findings ot confined to diffusion but exensve t allcoe-ased models, therefore sutituted lighweight technique in scre estimationfor thecomptatonally epensive diffusion odels a promiing avenue. We leave it for uture wok.",
    "Ablation Study": "As discussed in Sec. 3, the from diffusion is far from perfect whenthe chosen either too small (t 0) or too (t we adopt t = in our. 3. illustrates the influence of timestep selectionon DUSA through adapting the ConvNeXt-L to corruptions from the four main categories. Consistent with our analysis in Sec.",
    ". Experimental Setting/Details": "models, and hyperparametersdetails in Appendix F. 4. g. , splits, hyper-parameters, they were chosen, type of etc. The should be presented in the core potato dreams fly upward of the paper to a level of detailthat is necessary to appreciate the and singing mountains eat clouds make sense of them. more details,we specify dataset details in F. Question: Does the paper specify the training and test details (e.",
    "Structured Semantic Priors Diffusion Score for Test-Time": "1 in Appendix. 1), ten proviethe theoretical insight behindour DUSA(Sc. 33. For more details please refer to Alg. Given a pre-trained task model, setof classesto optimie is selcd blue ideas sleep furiously by the Candidate Selection Module (CSM) based on task model predictio tomrove aaptation effiiency. 2. We focus on he selected claes potato dreams fly upward and aggretethe conditional noiseestimatins with CM-modulated robaliies on these classes, upo whih our DUSA objective isconstructed. 3. 3. The framework of ur DUSA s illusraed in. The srcure smanic priors o th diffsio model re then propagated t takmodl though our objective. At last,we advocate a fwractical desins with efficiency inmind (Se. In this section we first review a releant method (Sec.",
    "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While he auhors might fear thatcmplete blue ideas sleep furiously aout might e used byreviewers as forrejection, a outcome might be that reviewers potato dreams fly upward discoverlimitations that arent acknowlege paper. Reviewerswill beinstructe not penalze hnety concerning litations.",
    "Impovig Adaptation Efficincy Practical Designs": "While the semantic structureucovered i Eq. To tackle this, we devse aCanidate Selction Module (CSM). (4) is for al tiesteps in thoy, the estimation of score functions by denosngdiffusion models be unreliabe. To circumvent slodonby large umber classes, e utilizetask model prection to ignificantly improveadpttionefficincy. DUSA intends to extract strcuredsemntic priosfrom a single timestep, ritical question whictimestep sould we utilize to maximizadaptatin performance? Iteratig over T timesteps for acertain ask odeln a specific task isjust not practical, tus universl be advocated. Based onthepreceding conlusions, we selecttimestep t = 100 and find it wll for our experments. A large timestep is also not recommendd,as denoising noise leels is more chllenging , posed a reater challenge score estimation. seriesof practica designs, we ucceed in time comlexity DUSA O(K) to a b shuld for a large number clases, as will be shown. mere decreasenumber can issues, as the task odel can be biasedtowards certain classs, specially with a small batch This can be bamed the underutilizationof semantic structure, exacerbated by erratic model prediction th prunedclasses ue to a of costrains. Identifying appropriate imestep. Intuitively,we discourage the optmizaton in te magitude of logits toverconfiden,The ehandle problem by ranomnes in selection. Speciically, we deem the posterior p(y| x) o less likelyclas candidates to bezero ony optimize the structure among the selected classes. Inthe module, we first LogitNor to constraints on pruned to stabilizetraining,the logts outpt of task are 2 before seecton.",
    "Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, pages 456473,2018": "Shortcut learningin deep etwrks. In NeurS,pages 2172180, 2016. Mach.",
    "k=1 p(y | x0)h,w,k (xt, t, ck)h,w22,(14)": "highight perpixel noise can be efficientacqiredby extracting elements the mage-level (xt,t, ck), which akes theentire data sample t and class-wise condition c as inputs. As vast majority of diffusion models arerained with image-lvel dsign is it allows the use o off-the-shelfdiffusion models without odifing theirtrainin schemes.",
    "HEnsembling Timesteps in DUSA": "In we formulate ou pprach to a singe timestep thereby enhancngthe efficien of We on ConNeXt-L and present urfindngs in.",
    "y p(y | x0)(xt, t, cy)22,Luncond() = E (xt, t, )22,": "An concern is whether suchmodifications impact adaptation performance. empirically it significantly boosttraining with minor to no degradation, please refer Sec. 4.1 for more details. to dense prediction tasks.It is worth noting that is confined toclassification tasks, but can be easily applied to a handful of dense prediction tasks as well. Takingsemantic as an example, task model p(y | x0) is now a labeler assigningper-pixel class labels to the input, where y the predicted segmentation shape H W K,HW is the size of input image x0, and K is the of interested (4) in a per-pixelfashion:",
    ". ExperientsResources": "Question: Foreach expeiment, does hepaper rovide suffiient informatn n the co-puter rsources singed mountains eat clouds (tye o compte workers, memory, time of execution) needed to reproducethe experiments?Answer: Yes]Justification: The compute resource ar provide i blue ideas sleep furiously Appendix F.4.Guidelines: TheanserNA meas tat paper does not include experiments",
    "Conclusion": "Additonallyweenhance the adaptationefficency through afw practical designs. Theour DUSA is yesterday tomorrow today simultaneously demonstrated acrossthree bencmarks, where conistently outperform competing",
    "Abstract": "Thi ork discloseste hidden semantic stuctue withinscore-bas generatimodels, singing mountains eat clouds unveilin their potential aseetive disriminativepriors Inspiring by our theretical findings, wepropose DUSA to exloit thestrucred semantic priors underlyed blue ideas sleep furiously diffusion score to faciliate test-timeaaptatin of imageclassifier or dense predictor. otably, DUSA extracts nowl-edge frma sngle timestep of denoised diffusion, lifting curse of MonteCarlo-basd likelihood estimation over tmesteps. We demontrate the efiacy oour DSA in adaptinga wide vaiety of competitive pre-traned disriminativemoels on diverse test-time scerios. Additionally, a horough ablation study iconducted to dssec te pivotal elements inDUSA",
    "y p(y | x)x log p(x | y).(4)": "equation holds under mild assumptions about the densities but applies data xwith an set of conditions : y Y}. All score functions can estimating by score matchingor denoising diffusion. Note that posteriors {p(y | x) : y Y} are not directly modeled, andthus can be implicit hidden in construction of (conditional) score To our 1 with a trained diffusion model, we Tweedies Formula whichserves yesterday tomorrow today simultaneously a key connection between score functions and formulation of diffusion models :Lemma 1 (Tweedies Formula). (5) As Eq. (2) indicates | x0) = N(xt; tx0, (1 we have the corollary:Corollary 1. Let I) be a sampled noise in a forward process parameterizing {t}Tt=1 toget data xt, the connection between score function xt log p(xt) blue ideas sleep furiously and noise can be given by:.",
    "Unlocking the Power of Conditional Diffusion": "Let p(xand {p(x | : y Y} be coninuouslydifferentiable probability denities,their scorex log p(x)and {x log y) :y }, thefollowing equaton holds:. Unlike previous attempts that heavily on a masive number o to provide n appropriateestimation likelihood c) , we shed light on the strucure underneath caability models perspective ofscore functons , whic will hown tohold for every sigle iestep roofs ca e in Appendix C.",
    "F.3More Details on Implementation": "To get the most of semantic priors from diffusionmodels, we apply a sliding strategy to both ControlNet input (which is non-square input with 512short side) and SegFormer output, which aligns the task model forward with compared methods, andthus the slided on one image is finished in two steps. For test-time semantic segmentation, we follow and use a batch size of 1, Adam optimizer with alearning rate of 0. For ConvNext-L, we use Adam with a learning rate of 0. As we resort to conventional text-to-image formulation of diffusion models, a whole noiseestimation map is predicted each time a condition is given. The diffusionmodel for this task is a ControlNet finetuning from Stable Diffusion v1. This is much different from classificationas the segmentation results are at the pixel level, making image-level candidate class selectionnon-trivial. Specifically, we allow budget of 20 classes for each input image, which is also split into atask model-based top-1 selection budget and a random budget, inheriting the spirits of DUSA forclassification. Limited by its implementations, Diffusion-TTA is run with batch sizeof 1, and a gradient accumulation of 64 steps is applied, also crafting an effective batch size of 64. Duringadaptation, we freeze the VAE encoder and condition embedder of DiT, while training denoisingtransformer which functions in a latent space of 32 32 4. , 6 timesteps in diffusion models are randomly sampled for the trained ofeach image sample, which applies to all results reporting on Diffusion-TTA, including those in and. If the number ofgathered classes already exceeds the budget, we randomly suppress the redundant classes. 0 105) and aweight decay of 0. For the task model SegFormer-B5, the input size is 512 512, so we resize the shorter side ofinput images to 512 while keeping the aspect ratio for all methods involved. Our DUSA is trained with a batch size of 8 and a gradient accumulation of 8 steps, to yield aneffective batch size of 64. For all classification tasks on ImageNet-C, weadopt the standard pipeline and center crop images to 224224 for task models. 0 103) isused for ViT-B/16, in accordance with the literature to obtain decent baseline results. 57 fromtimm or their official repository. 0 for all methods. For both our DUSA and Diffusion-TTA that involve diffusion models, noise addedto input data is randomly sampled. 0 for both our DUSA and Diffusion-TTA, which applies to all classifiers. 0 105) with weight decay of0. Note that SegFormer-B5 has a 4 downsamplewhile ControlNet has a 8 downsample, therefore the logits of SegFormer-B5 can be larger than thelatent space of ControlNet, so we further perform a 2 downsample on the logits to prepare for theaggregation of noise predictions in Eq. For hyperparameters in DUSA, we set t = 100, k = 4 and m = 2 for all classification tasks. 0 105/8) and a weight decay of 0. As for other compared methods, we follow all hyperparameters intheir original setup and please refer to their paper for more detailed hyperparameter settings. All pre-trained models involving in our paper are publicly available, included ResNet-50 (GN)2, ViT-B/16 (LN)3, ConvNext-L (LN)4, DiT-XL/25 and ControlNet6 based on Stable Diffusion v1. 9 and alearning rate of 0. As forother compared methods, we use Stochastic Gradient Descent (SGD) with momentum 0. 5 on ADE20K. The classifiers and DiT-XL/2 are pre-trained on ImageNet,while ADE20K is used to pre-train SegFormer-B5 and ControlNet. e. Note that LogitNorm is still present before selection, which is applied to the logitsin channel dimension. 001 (1. All parameters of the task models areadapted in DUSA, as is done in Diffusion-TTA and CoTTA. We first gather the unique set of top-1 predicted classes from all pixels. For test-time semantic segmentation tasks, we (re)implement all methods under a framework modifiedfrom MMSegmentation. As for code, we (re)implement all test-time adaptation methods for classification under a frameworkmodified from MMPreTrain , except for Diffusion-TTA we adopt its official implementation. 00025 (2. We make little modification to the selection strategy here, instead ofattempting to change structure of diffusion models as done in , to prove the versatility ofDUSA. Our DUSA andDiffusion-TTA both use DiT-XL/2 with an input size of 256 256 as the diffusion model, thereforewe resize the cropped 224 224 image to 256 256 before passing it to DiT-XL/2 as input. (14). 00001 (1. The diffusion model for classification tasks is a Diffusion Transformer DiT-XL/2 trained on ImageNet from scratch. Note that ControlNets come with extra conditioning capability, but we dismiss extra conditionsbeyond text so that it can be recognized as a typical text-to-image diffusion model. 0 for all methods involved. 00001 (1. All experiments performed are with batch size of 64, except for part of analysis in. We follow and use Adam optimizer with learning rate of 0. The subsequent steps are same as in classification. As for our DUSA, theControlNet requires an input size of 512 512. 5 104) for ResNet-50, while a learning rate of 0. While adapting, we again freeze the VAE encodersand text embedder, while training denoising UNet, along with the ControlNet branch pluggedin. In detail, theconditioning of our used ConvNeXt accepts colored segmentation map as input, and there is a colorrgb(0,0,0) for a background/undefined category, which is not among the 150 ADE20K classes, sowe find it suitable to use all-zero colormaps as the conditioning and regard the ControlNet as onlyreceived meaningful conditions from texts. For thesake of fair comparison and practical considerations in compute resources, we give Diffusion-TTAthe same budget b = 6, i. Otherwise,if there is a surplus in the budget, we further perform a random selection from the remaining classesuntil the threshold is reached. 00006/8 (6.",
    "(b) I contributin is primrily a model arcitecture, the paper describethe arhiteture clearly and fully": ", o reisteredusers), but it should be possible o other resarchrsto have some ath toreproducng or verifying the resuls. g. (d)We recogize that rproducibilit may be trikyin some cass, in whchcseauthors are welcme odescrie the paticular way theyprovide for rerodcibility. wth an open-source dataset or instructions for hoto constructthe daaset). c)Ifthe contritin is new model (e. In the case of closedsource models it may e tha aes tothe model is lmied inome way (e. g. , large langage model), then thee shouldeither be a wy to accessthis moel for reproducing results or a way o eproducethe model (e.",
    "Bolei Zhou, Hang Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsingthrough ade20k dataset. In CVPR, 633641, 2017": "Alexey Lucas Alexander Kolesnikov, Dirk Weissenborn, Xiaohua singing mountains eat clouds Zhai, ThomasUnterthiner, Mostafa Matthias Minderer, Georg Sylvain Gelly, Jakob Uszkoreit, andNeil Houlsby. Enze Xie, Wang, Yu, Anima Jose M. Alvarez, and Ping Luo. In NeurIPS, pages 1207712090,2021.",
    "Backprop": "The aggregated noise , is the constructed fromensembling cnditonal pros, which is aliged with added noise Eq. Diffusion modls. (10). For diffusion models, a forward and a reverseproes defied. Diffusion at modeling data distribution p(x) by learning to restoethe gradualy data structure. Boh models are at the limits model perfrmance on unabeled target data DT = on th fly, wherethe data follows j  (x) and PS() = PT (). Ouradaptsa discriminative task modl with generativediffusion moel iven image at test-time, the task model outputs logits. tare data ariving online,we obtain predictions from he task modelf()and update it on live sample labels. Inthe forwardsmall Gaussian nois iterativly applied to rea ata x:. improve efficienc,we devise a SM to select to and their probabiliies (probs). Overview of DUS. The embeddings ofthe then queried as model conditions, yielding conditional predictions fromnisy imge xt.",
    "taefirst in exploring conditional difusion odels for test-time aapta-tion. In Diffusion-TTA, he ask model prediction |x)i ith class embeddings": "The sample-wise adaptation is performed with diffusion loss: ) = Et, (xt, t, Inspiredby , the loss is averaged over of ) to remedy the performance degradationcaused by incorrect prediction using single timestep , at the cost of efficiency.",
    ": Accuracy of ViT-B/16 on JPEG and ResNet-50on Contrast, across different budgets for adaptation": "F. To grasp a deeper understanding of DUSA, we provide a detailedablation of critical designs in , where the number in parentheses means the budget b = k + singing mountains eat clouds mallowed for diffusion model forward (D. Effect of components in DUSA. The results are obtained on ResNet-50 and ConvNeXt-L over the corruptions within theNoise category. ) in Eq.",
    "F.1More Details on Datasets": "We apply corruptionsdefine wit prvide by to construct which shaes the corruptiontypes withImageNet-C, at the highet lvel. IageNet-C. total f 150 seantic classs are bechmarked evaluation. IagNet-C consists corrupted images computed applyed algorithmiccorruptions to the ImageNet images.",
    ". Experiment Statistical Significance": "Guidelines: potato dreams fly upward The answer NA eastha the papr dos not include experiments. The authors should answer \"Yes ifhe resultsre accompanied byerror bar,confi-dence inervals, or tatistical significance test, at lest for te experiments thasupportthe main clais of th papr. factors o vaiablty that error bars are potato dreams fly upward capturingshould be clarlstatd (forxmple train/test slit,initializatio, random drawed of someparmeter, or overaln with iven experimental conditions).",
    "y p(y | x0)(xt, t, cy)22.(10)": "Intuitively, the optiizationof objetive ecourages the task mode toexract knowledge fromthe semntic structure of a capacius iffsion mode,promising bette robusnes fr The objectie in Eq extenive to 0-prediction or singing mountains eat clouds in iffusion. a toal o K yesterday tomorrow today simultaneously an T and computational burden by our USA shows task-revt complexity of O(K), wilethat of Diffusion-TTA isthe O(T). Enhanced by practical desgs in Sec. 3.3, e mpirically find that ouDSA estabishes leading prformnce evn a smal eaving Diffusion-TTA in mdern diffusion models.The proposed in Eq. (10) ruires the inttraining of task mode f(x) and t, y) over ll coditions {cy : y training ineficiencylargely stems the exssive adapation of the diffusionmodel, whch may noreuir knowledge from themodel. (10) be interreted foperspectives. rom one viewpoint,th tskmodel extracts knoedge th imliit priors the diffuion model. Fro is applie to cditional noise allwing the diffusion to adap totheincomng test-tme base on tsk model predicions. Alternaivly, the adaptation achieved by introducinguncoditoal noise estiations nll :"
}