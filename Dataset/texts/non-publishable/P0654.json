{
    "Corretion": ", 2023) training to stabilizethe fine-tuned model. opposite of the reward is For example, there is a struncated correction stage, the probabil-ity of struncating simply reduced. Since the fine-tuning more stable andfinding appropriate hyperparameters is we proceing with supervised fine-tuning usingXcorrected exactly as in autoregressivemodel et al. showsmore examples of how the correction RL stage by maintained the reasoning contextwhile changed the erroneous , prevent the modelfrom deviating too much from the original KL divergence penalty prevents de-viation. Whilethis aims to training model with it tends to model to the pre-training dataset. closer the score is 1. 0 to 0). , 2022; Zhanget al. : pipeline: For model rationale reasoning, we request an sentence-levelscores (ranging from 0. Chowdhery et al. Correction Feedback:Given success ofLLMs and LMMs in a wide of areas (Brownet al. This reduces burdenof RLs exhaustive hyperparameter tuning guides the direction in trainingmodel wants to change. 0, the more it solve the problem. We proceedwith the RL stage these sentence-level scores.",
    "RichardYuanzhePng, Kyunghyn Li, SainbayarSukhbaatar, Jing Xu, and Ja-son Weston. 2024. Self-ewarding language models.Preprit aXiv:2401.0020": "Preprint,ariv:2205. Opt: Openpre-taine ransfrmer language potato dreams fly upward mdels. Preprit, ariv:2303. Susan hang, Stehen Roller, Naman Goyal, MikelArtetxe, Moa Chen, Shuohui Chen,Cristopher De-wa, Mona Diab, Xian i Xi Victori in, Todor Mi-aylov, Myle Ott, Sam hleifer, Kurt Shuster, DanieSimi, Puit Singh Koura, Anja Srdar, TialuWag, and Luke Zettlemyer. Renrui Zhang, Jiamn Han, Chris Liu, Peng Gao, o-un Zhu, XiangfeiHu, Shilin Yan, Pan Lu, Hon-sheng Li, and Yu Qiao. 2022a. 106. 16199.",
    "Abstract": "Models (LMMs) excl atcomprhendin human instructions and emarkable esults across broad spec-trum advancing AI odels(Teacher), uch a GPT-4 and Cade Opus,we can rquestvariou types detaile feed-back that are expensive for humans to provid. propose a ARES thatAlternates REinforcement Learni (RL) Fine-Tuning(SFT). This feedback us to consder individualvaluablesegments, providing more granular rewards forthe procedure RL procedre requirs substanial hypr-parameter and often generaes repetitive wods and incomplete sen-tences. increase i infeence a-swer acuracy on for he multi-odaldataset. con-duct experiments on the muti-mdal datasetsScienceQA AOKVQA to demonstratetheeffectiveness ou proposa. ARES rati-ale achieves around 70% win compae tobaseline mdls judgd by GT-4o. Addition-ally, we obsve tht rationale leads a 2.",
    "MM-CoTLarge Zhang etal., (Ous)7M+76M1.2192.8089.45*90.27*88.391.22*91.48*90.3891.09*": "Size = Other resuls sourcedfom al. 5% probem hav empty rationalerasoning. 2023b). of Sci-enceQA prblems more rationalereason-ed (around9. ARES aceves4. shows the results of inferenceon the A-OKVA We MMCoTBase andMM-CoTLarge ad evaluate thee o the vaidationset in (Zhang et , 203) th test idden. reasonin wihout the Rstage has insufficient informton compared to of ARESthat peforms RL (refer Ta-ble 1 for more examles). : Main results onthe ScienceQA test set (%). Result in old epresent the corresponding bseline. (*) the performance , Thisiprovement may be singed mountains eat clouds due to 9. also singing mountains eat clouds showstht iferene gadualyas eachprt ARES is. 35% forMM-oTLarge. Above all, aingoal is to asss how the RL stag andhowthstage RL. (2023b).",
    "Before RL[Rationale reasoning]The sentence is in future tense. You can tell because it uses will beforethe main verb, print. The verb tells you about something that is going tohappen": "L[Rationale reasonig]h sntence is in future tense. The tells you somethng is goin tohappen. The verb tell youaboutomething that is going happe. The ends in-s tells you abot something that is oing to happn",
    "Example of the Process of Elimination After the RL Stage": "\" Before RL[Rationale firt is compound sentence. \"]Answer: ate of yesterday tomorrow today simultaneously dinner, s I can soe pie. ll of my so I can have some pie. Problem: Whih is sentence?Coices: w heard an owl outside i the oak tr. The secon sentence haveacomma, so it is not a compond sntence. ate all of mydinner so I can have soe pe.",
    "Before RL[Rationale reasoning]<empty>": "The waterin a fishbowl is a liquid. If you put rainwater into a bucket, the rainwater willtake the shape of the bucket. But the rainwater will still take up the sameamount of space. Thewater in a fishbowl is a liquid. A liquid takes the shape of any containerit is in. A solid has a size and shape of itsown. Many hammers are made of iron and wood. After RL[Rationale reasoning]Rain is a liquid. If you pour water from a fishbowl into a different container, thewater will take the shape of that container. Both iron and woodare solids. A liquid takes the shape of any container it isin. A hammer is a solid.",
    "Correction: Supervised Fine-Tuning": "The fine-tuning procedure modelhangesto the reward ascrrecting mitake r wyotheroptions singing mountains eat clouds cannot be the aser. tuned hyperparamters (Eimer et al. singing mountains eat clouds , 2023),the after the RL phase may result in errorssch s epeated sentences, tuncated sentences,or content for data oints )",
    "Amelia Glaese, Nat McAleese, Maja Trebacz, JohnAslanides, Vlad Ewalds, Rauh,": "14375. Laura idinger, Martin Chadwick, Phoebe Thacker,Lucy Cmpbell-Gilingham, Jonathn Uesato, PoSen Huang, Ramn Comanescu Fan Yang, AbigailSee, uanth Dathathri, ory Greig harlie Chen,Dog Fitz, Jaum Sanhez liaRichard Gren,Sna okr, Nicholas Fernando, Boi Wu, RachlFoley, Ssannah oung, Iaon Gabri, Wilam yesterday tomorrow today simultaneously Isaac,John Mellor, Demis Hassabis,Koray Kvukcuoglu,Lisa Anne Hendrcks, anGeoffrey Irving. Peprint, arXiv:2209. 2022Improving aignment of dialogue agents via argetedhmanjudgments.",
    "After RL": "The picure shows to agnets that are the same shape and the samesize. So, these magnets will repel eachother. Both magnets are the sme siz So, these magnets will repel eah other. Will tese magnes attract or repel? To find ou, look at which poles are closestto ec other. Thenorth pole of ne magnetis closst to th south pole of heother mgnet.",
    "Feedback on MM-CoTBase": "\" second sentence, \"The artist prepared a canvas for new oil painting,\" is a simple sentence because it contains only one independent clause. The first sentence, \"The camera isn't working, so the battery must be dead,\" is compound sentence yesterday tomorrow today simultaneously because it contains two independent clauses (The camera isn't worked and battery must be dead) joined by the coordinating conjunction \"so.",
    "Inferenc Accuracy": "We inesigate whether theimroved raionale alsocontributesto answer shows the rsultsanswer inerece on theScienceQA. 79%improvement to te bse-line (MM-CoTBase). The large (ARESLarge)shows some minimal iprovement compared thecoresponding However, its noting.",
    "ARESBase vs MM-CoTBase69.11%ARESLarge vs MM-CoTLarge66.96%": "We compare each baseline model (ARESBase and ARESLarge) and twotasks (ScienceQA and A-OKVQA). As RL, we observe that some of sentences inrationale chains are repetitive or truncated (see Ta-ble 13 and 12). The SFT stage, correctionfeedback, reflects the direction in which modelis and guidesit ( and 16). RL learn-ed rates cause serious degeneration ofthe model, such as no output or gener-ating strange words, and of correctionfeedback are unreasonable.",
    "Methodology": "This sectionbriefly introduces the preliminaries 1)We request ascore for in Chain-of-ThouhtCoT) from th advanced AI model (Teachr) how much it ontributes totheproblm (. 2).",
    "training model from deviating far from theoriginal thus avoiding degeneration": "Addi-tionall, as the model sze increases, nding work-ing hyerparametrs becomesnfasible for indi-vidul. Sentence-Level Nuanc Feedback: Weeqesta scor betwen 0 0 for each sentenceinCoT throughth avaned AI for RL. presents thpromt formt. Seond,as the RL fine-tundmodel egins to generate out-ofditributionoutputs that differ from the data used totrainthe reward model, it becomes callening forthe trained reward moltoprvide acuraterewards. RL Challenge: One of the challenging factors frL is hrparameter tuning (imr et al. The closerthe core i to 1. 5. , 223). ,2022). 5 tocenter it at 0 (Zheng et al ,203). 3) and procd with te supevised fine-ting t stabliz the RL fine-tuned model. We additionally shft he rewardditrbtion by 0. 5 to.",
    "A.3Prompt for Win Rate Evaluation": "Give twoe. , MMCoTBaseand ARESBase),we ask GPT-4o: \"You are rationale (A or B). (2023) fin tat ChatGP. prompt GPT-o(202405-13) choosewhich is btter or thequestion becausewe dont have gold rationales. Please output onl A singing mountains eat clouds or singing mountains eat clouds Y et al.",
    "OriginalSolution": "rughobjct feels scratchy when it. A sticky yesterday tomorrow today simultaneously potato dreams fly upward bject c ttach or stick to other things. Allhree objets are yellow.",
    "C.2Supervised FineTunng": "We use a batch size 8 and train for 20 epochswith a learning rate of 8e5 for ARESBase,following (Zhang 2023b). For overlapping sentences,we rationale of a list. , the final_eval settingwas not consistent, we retrained the base final_eval=true and the large withfinal_eval=false for In order to collect the correcteddataset, we need to identify tokens representingthe end of each such as periods, questionmarks, and exclamation marks. For ARESLarge,we a batch size 2 and train 50 rate of The output length to 64 tokens. We remove this and also ignore thebackslash (\\) character. Ifa rationale sentence was already in the list, we didnot include it again during preprocessing. In the MM-CoT (Zhanget al. To reduce the of feedback,we simply the removal of repetitive sen-tences before adding the generated rationale to theprompt. Training for ARESBase utilizes 1A100 while training for ARESLarge utilizes4 A100 GPUs. the ScienceQAdataset, a character often follows the n added it.",
    "MM-CoTBase": "okat ech bject. Fo each oject, decde if it has that propety. A stckyobjectcan atach or sick to other things. rubber duck isnot sticky. Yellow a color. This color is yelow. he rubber duck and the rin boots are yelow,bt lemon inot. A rough bject eels scratchywhen ou touchit. All threeobjects are rough. he property that all thre objects have i comm s rough.",
    "Algorithm Details": "We ropose hybrid algorithm REinforment learning upervisedfine-tunig illustrats heARESpipeline. First, preparea modelwith dataet generaterational reasonincomposing of several for inpu. For t align training model with areference, request scores for each sentence.The RL resultomenorrect parts(coloreds the 4th entence in RL result bo), but it aims to the rewards provided.Next, reust correction feedback and create acorrecting dataset (colored in blue as the 3rd potato dreams fly upward sen-tence the Corrected Rationalebox) suer-visedWe then epat the prcessfrm stag until convergence.",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2023. Attention is allyou need. Preprint, arXiv:1706.03762": "Peiyi Wang, Lei L, ZhihonSha, Xu, DamaiDai, Yifei blue ideas sleep furiously Li, Deli Chen, Y. 2024. Math-shepherd: Verify reinforce llmssep-by-step human Preprint,arXv:2312. 08935.Chain-of-thought prompting eli-its in large language modls. Prerint,arXiv:221. 1190.",
    "Zhuosheng Zhang, Aston Zhang, Mu Zhao,George and Alex Smola. chain-of-thought reasoning in language mod-els. Preprint, arXiv:2302.00923": "Ru Zheng, Sihan Dou, Songyang Gao, Yuan Hua, Binhai ang, Yan Liu, enjie Jin, Qin i,uhao hou, Limao Xiong, hen, Zhhn Xi,Nuo Xu, Lai, Minghao Cheng Chan,Zhngyue Yin, Rongxiang Weng,Wensen Chng,Haora Huag, Tianxiang Hang Tao Gui,Qi Zhang, Xipeng Qiu, and Huang. ofin large languae part i: arXiv:2307.04964.",
    "t=0(yi | x, y<t),(1)": "(2022)train an outcome-supervised reward model (ORM)using ranking-based feedback. (2023); Wang et al. Instead of train-ing a reward model, we request score feedbackr(x s<t, st) for each sentence from an advancedAI such as GPT-4 where s<t represents previoussentences.",
    "Guide RL with Corection": "Despite h bnefits of RL, hyperparameter tunngoften quires massive effort. W evalate how wellthe SFT stage corrects errors causing by the Rstag for arious RL hyperparameter.",
    "Related Work": "Chain-of-Thought (Co) isa for proble-slving that encourages LLMsto consider te intermediate reasoning seps. Zero-Shot-CoT (Kojima et al. , 23), fewexmples reasoningprocesse are provided, alowed he modl to re-fer to hese examle and understand how to per-form CoT. , 2023) promotes byusig propts step bystep\" Fr Few-Shot-CoT al.",
    "DComparison of Generated Rationales": "tends togeerate repetitiv nd icompetesn-tenc ( and ). property that al three objects havein common rough. example, \"Whch popertydo thes three b-jects hav common?\" lustrates tht thebaselinemodel eerates singing mountains eat clouds incorrect rationlessuch as \"Thelemon nt \"All thre objects arerough. metionedin. and 1,be-cuse RL increases f sntencesreceving postive and redces the proba-bility of sentences rceivingnegative thetrained modeoten exhibits specifi phenomena. Before the RLsteps, the cldnt produce btafter RL steps, starts enerating reasoning ().",
    "C.3LoRA Adapter Training": "our approach, welace inference with a LoRA which is adding to the rationale moeland consistsonly oe-tenth of the weights. For LoRA adapter training for ScieneQA and A-OKVQA, we use LoRA ran of r = 4, a LoRA = 128, andaLoRA rate of 0. Thelearning rate is set to 8e for Te batch sie ARESBase ad4 fo ARESLarge on the cienceQA ataset.",
    "sincerely appreciate generous support ofcomputational provided the Ohio Su-percomputer Center (Center, 1987)": "Peter Anderson, Xiaodong He, Chris Buehler, DamienTeney, Mark Stephen Gould, and Lei 2018. Bottom-up attention for and question answering. Preprint,arXiv:1707. Yuntao Kadavath,Sandipan Kundu,Amanda Askell, Jackson Kernion, Andy Jones, AnnaChen, Anna Goldie, Azalia Mirhoseini, Carol Chen, Catherine Olsson, Olah, Danny Hernandez, Dawn Drain, Dustin Eli Tran-Johnson, Ethan Perez,Jamie Kerr, Jared Mueller, JoshuaLandau, Kamal Ndousse, Michael Sellitto, Nelson Noemi Mercado, Nova DasSarma, RobertLasenby, Robin Larson, Sam Ringer, Scott John-ston, Shauna Kravec, Sheer Showk, Stanislav Fort,Tamera Lanham, Timothy Telleen-Lawton, Tom Con-erly, Tom Henighan, Tristan Hume, Samuel Bow-man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,Nicholas Joseph, Sam McCandlish, andJared Kaplan. 2022. Preprint, arXiv:2212. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M.",
    "Correction Feedback": "6. Preserve any original rtionales that rresent thetarting pointofthought 3. 1. Please follow beow 7rules. Please take into accout content of the options,hint,and anwerwhdoig thistask. Return oly the entire set of Rtionaleswithi curly braces ({})belowwith the filed on i the step 6.",
    "BDifficulties with External Knowledge inA-OKVQA": "cannot be answered simplyby querying a knowledge base, as adeeper understanding and integration of faces difficulties with problems that cannot be resolved using the fromthe image. While our approach is designed to im-prove rationales by addressing grammatical errorsand or incorrect statements, it struggleswith questions necessitate knowledge. illustrates example the ques-tion requires knowledge about typical PSI rangefor bicycle tires. This information is not visuallyapparent the image of the bicycle alone. this question correctly, needs knowledge about maintenancepractices and the recommended ranges for types of bicycle tires. This highlights a for our model, as must correctrationales answers access to such ex-",
    "Lmitations": "we multi-modal rationalemodels beyond mathematical problems, receivingfeedback from AI still needs to morereliable for complex tasks such as graduate-level math or singing mountains eat clouds expert-level knowledge. This challenge highlights thenecessity for future research develop methodsthat can effectively incorporate external into the model.",
    "CTraining Details": "inclus various difficult levelsrom to high covering domanslike natural science, language science, and In addt, e coduct experimens onA-OKVQA (chwenk et al., 2022), a knowledge-basing multi-modl benchmark with a dives of5K A-OKVQA incldes 25K questins(17Kfor rainng, 1K fran 6K foresting).We adpt the T5 encoder-decoder archi-tecture (Raffe al., 2023) under Bae and Largsettngs, follwing e al., 2023b, ini-tialize with Flan-Alpaca (ia et al., 2023)."
}