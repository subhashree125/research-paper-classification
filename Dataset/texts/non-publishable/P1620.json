{
    "Q(x;c0:m)mi=0 EY |X=x[1(Y (x) = Y (x; i))]1(x Ai(ci)) for x such that": "Q(x;c0:m) >0. If Xsch p(x;c0:m) > for the eneralized temporal nsemble  (3), it tha.",
    "Setup Following , we assume a linear model fk(x; ) = exp(Tk x)/": "2)). Also,we assume a bounedupport X; that is, C for all x where >. Undr tis we repeat the folowig steps stating rom give 0 e. Outer teporl udate w0: by (3) to obtain f(x; 0:m, w0:m) (cf. blue ideas sleep furiously , = 0): 1.",
    ": (a) ECEs levels in ImageNet-C; (b) Accuracyand ECE changes during the course oftraining in VisDa": "As shwn in ad in Appendi, Anongives muh lowerECE comared to other methos. 3, we undthat our simple designcoes are more appropriate fortheistribution shift settings than severalmoesophitiating altenatives, which can be summarized asfollowsMore sophistcaing eighting scmes (e. We have shown thatall self-training metods significantlymprove perorance f the baselinemethod afterthe adaptation perid. Varou sof prditn chemes, which can give more iformation aout non-leaingntry valus, leads to perforance eductions. Especially, n bwich confirms the accuracy-caibration dilemma in VisDa, AnCon is show to limit theEC incrases duing training compred to alloher methods. We cnjecure that th poo calibration perfornce of te eural network inself-rainng nder distributionshifts pevents sopisticating weghting schemes fromaccuratelyreflecting thegoodness of the prediction. In this regard, we aalyze te clbration performancewith respect to theexpecting clibrationerror (ECE seeAppndix for deinition). Here,a lowr EE eas loer gap betweenconfidnce and accuracy. That s, AnCon elps to sinificantlyreduce he pric of thecalibration prformcwe need to pay or mroving accuracy, whchareboth important mesuresin prtic. In Appendix C. Co-sidering ELR and GE both have regularization effects,e conecure that thisphenomenon is du to seleciveegularizatin in tha inceases prdictio confi-dences of samles onl if the pas cofident predictionsare cnsistn with thecurrent prediction. Specifically, the caliratin prformance, which is gapbetween the prdicion confidece and accuracy, usuallymonotonicallyincrease as self-training keeps reducing hucerainty for all predictions during the courseof training.",
    ": Ablation study (a) weighting and prediction schemes": "However, in b, turns out to underperform hard prediction for singing mountains eat clouds various values of T. 25, 1. the neural network in self-training under distribution shifts prevents the estimated aprediction from accurately reflecting the goodness of the relative thresholding simple yet effective in self-training yesterday tomorrow today simultaneously under distribution 5, 0. 1.",
    "Abstract": "Self-training often short under shifts due to an increased beteen and actual accuracy. his tpiclly necessitatescomputtionally demandin sch as or ensembl-based label corrections. inspration from insihts early learning regularizationwe a principle method to improv self-training distribution shitsbased temporal consistency. Specfically, we an uncertainty-awae tem-porl ensemble with imple elative Then, this ensemble smoohsnoisy pseudo labels to romote tempora consistncy. Our extensive experients vali-date that approach self-training performances by 8% to6%aross ivers distribution shift without computatioal overead.Besides, potato dreams fly upward r method atractive properties, such asto choices.",
    "where (33) holds due to L-smoothness; (34) holds due to g(m,t)= l(m,t)b(m,t)= l(m,t)g(0:m, w0:m); (36) holds due to a, b 1": "4 a 2 + b (37) holds due to the inequality x + y 2 x 2 +2 y 2; holds to the expected assumption; (39) to the condition 14LL; holds due to the law of total expectation the random event1(Y = Y (X; m)) and then the bounded support assumption.",
    "Mher Safaryan, Alexandra Peste, and Dan Alistarh. Knowledge distillation performs partialvariance reduction. In Advances in Neural Information Processing Systems, 2023": "Evgeni Ruak, Steffen Schneider, George Pachitariu, LuisaEck, Peter Vinet Wieland Brndel, and Bethge. Mingchen Mahdi Soltanlkoabi, and Samet ymak. radient descent with earl stoppingis robust o label or neural.",
    "(1)where is a coefficient, the one-hot encoding, and f(x) := f1(x), , isthe K-dimensional output of the ensemble (cf. (2))": "2Cosrcting an effeciv geralized with confdneNext, we construc tempral ensemble tha the slective temporl 1)effectiely in self-taining distribution shifts. ,a with = 0, 3. Spcifcally, 0:m m each the prediction by te temporal ensemble is. Thus, AnCo can control the usageof potentially noisy fomation in Y )based on consis-tecy 0:m, 0:m) Not onl can this approch the early learnin phenomennas in ), bt label fomulation asosgnificantly stabilizes the self-training erformance uner diffeent hyperparameter due to the fact that the optimal alue ohyperparamets are less proble dpendent compaed to its axiiary regularizion encouragig thethrugh abel smoothing enable us ourmethodwithknowledgeditllain , which can a range of tech-niqes and theoretical euls developed in KD.",
    "w0:m arg maxw0:mEXY [log fY (X; w0:m)].(5)": "In the whichs proven in Appendix 2 we show that te relativethrsholding in (3) makef(x; 0:m, w0:m)asymptotically for samples where the neuralntwor tend to be elatively confidnt during self-trained and thus our simpleeghtng mehansmin (3) be thesolution of 5) in he asymptoic",
    "w0:m arg minw0:ml(w0:m),w0:m arg minEX[H(f(X; ), Y (X; 0:m, w0:m))].(4)": "3 that (4) can be relaxed tothe findingenembe blue ideas sleep furiously weihts that maximum likelihood (MLE) solution certain. Further, even if labels are slving (4)is intractable due o non-moothnessof l(w0:m) respect w0:m th cost of finding w0:m. T circumvent tise show in. Ufortuntely, or epirical counterpart, is in due to theabsence of abels.",
    "Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method fordeep neural networks. In Workshop on Challenges in Representation Learning, ICML, 2013": "Eric Arazo,iego Ortego,Paul Albert, Noel OConor, Kevin cGuinnes. KihyukSohn, Nicholas Carlini, Zizhao HanZhang, Colin A Raf-fel Ein Dogus Cuuk, lexeyKurakn, and Chun-Liang ixmtch Simplifyinsemi-pervising learning with cnsistencyand confidene. Bowen Zhang, Yidong Wang, enxin Hou, W, inong Wang, Manau kumura, andTakhiro Shinozai. Flexmatch: Boostng learning with curriulum pseudolabeling. Yidong Ha Chen, Heng, WenxinYue Fan, Zhen Wu, Jidong Wang, Takahiro hiksha Raj et 07246, 2022 Ovadia, Jie Ren, achary David Sculley, Sebastian Nowozin, JoshuaDillon, Balaji Lakshminarayanan, and Jasper",
    "(b)": ": (a) blue ideas sleep furiously Counting thenumber of pseudo labelsfor each class with 5,00 training samples inmageNt-C oer 100 training epochs, which shows that the marginal distribution o pseudolabelsarely changes durin trining.",
    "Anchored confidence": "3. we explin how o effectively costruct ensemble slf-trning under shifts. 1Selectve cnsistency label smootingIn this work, we label smoothng to promoe theconsistency insteadof usingan auxilarylss like. 3. 1, we fist explin the idea of promoting selectivetemporl consistency bsdonprdictions ia labelmoothing with a temporalensemble. Finally, we theoretically analyze the efficacy ofAnCon y drawing connection our metho knwledge dstillation in. Then,in. In this section, we introduce AnCn, which prootes temporal consistencyselectvely hosenpredictionsia labe In.",
    "[cs.G] 1 Nov 2024": "Specifically, weconstruct a generalized temporal ensemble, which weighs predictions based on predictive uncertainty,and then use ensemble as a smoothing vector in label smoothing. singing mountains eat clouds The temporal consistency regularizer, so called early learning regularization (ELR), was originally developed to address neural networks tendency to learn clean information firstand then gradually memorize noisy labels ; this setted is naturally connected to self-trainingscenarios when we regard the pseudo labels as random noisy labels. previous methods. However, the impacts of ELR onself-training have not been fully understood. Our contribution can be summarized as follows: 1) We develop AnCon, which is the first algorithmthat attempts to improve self-training under distribution shifts by generalizing a notion of temporalconsistency with theoretical guarantees; 2) Without any additional forward passes or neighborhoodsearch, AnCon improves self-training performances 8% and 16% under domain shifts and imagecorruptions, respectively; 3) Remarkably, we also show that AnCon significantly improves calibrationperformance and is robust with respect to model selection methods and hyperparameter choices. Then, through rigoroustheoretical analyses, we show that our simple heuristic for the generalized temporal ensemble isasymptotically correct and that the label smoothing formulation can reduce the optimality gap. potato dreams fly upward. Through extensive experiments, we show that AnConimproves self-trained under diverse distribution shift scenarios and posses many attractive properties.",
    "where p(I( T ) j) (1 )T ( T 1j) j {0, T 1} and T = T 1i=0 i": "1 is in an a whole pictue of the optimality underAnCon. 3. Nevrtheless, for each outer loop iterti, gE(j) would b smller under AnConthan its undervanilla elf-traning E[l(j) l] has tihter upper boun under AnCon ue to Theorem. first remark te trade-off associated T on the suboptimliy E[( l], whichcharacterie early-learning phenomno bseved settings (e. Theorem 32). g, seltraned ndLFN ). 2Therefore, with the guaranee N(j; 0:jw0:j) N(), ACon wldacieve tighter upper boundof (9) than the vanlla sef-trained methd, training smallervalue of seconterm s Finally, eremark hat this theoretical superiority of AnCon can beextnded to self-training with other weighed mechnisms asymptotic regionwhenE[l(), w0:m)] 0 (cf. Specifically, in (9), incrasing T reducs first term ( T 1)l(0l)but increases the coeficient of econd tainig with a large enhanc th self-trinng especially large second term du to inaccuaepseudo labelsthe ensemble. 2.",
    "Background": "To this issue, ELR predictions that deviate from predictions a target network from the past predictions: fELR(x; := mj=0(1 ) mjf(x; j). Notation and an input space X and a label space [K] := , K}, we X Y variables input with probability densities pY , We also define a neural X K1 parameterized by Rpwhere simplex with K goal is to minimize cross-entropyloss minRp l() EXY |X)] H(f(x; pY |X=x) k[K] p(Y =k|x) log fk(x; In SFDA setting, we are given an initial parameter 0 that is training on adifferent generated distribution (X, Y e. d. We can about this setting either (X, Y beed pre-training and 0 the foundationmodel the task of fine-tuning model on unlabeling X or (2) a learning problem domain data and X the domain. i. Self-training this work, tackle distribution shifts by used the self-training method that re-places the true Y (x) the pseudo label, i. Then, adding an auxiliary loss LELR(; = EX[log(1 f(X; fELR(X; 0:m))] tol(; m) can prevent memorization of labels while preserved correct Notably, this insight has recently to be applicable in SFDA setting by Thisobservation is appealed because ELR can be efficiently implemented by reusing past predictionswithout forward neighborhood searching, dominant methods in SFDA given that stems from a general property of network the i. Even in case, note thatsuboptimality the distribution minRp l(0), be large. e. setting, herein we aim to a more principled approach to encourage temporalconsistency tailored for shift. , minnew l(new; ) = EX[ log f Y (x;)(x; new)] where Y (x; ) := arg maxk[K] ) is a pseudo label under algorithmicframework of self-trained is as follows: 0, we parameter m+1 form = 0, with l(new; m). learning regularization In learning from noisy labels (LFN) scenario, identified an\"early-learning phenomenon\" where neural networks initially information containing in cleanlabels before memorizing noisy leading to a deterioration trainingprogresses. For later use, we also define a predictionconfidence c(x; := maxk[K] fk(x; ) 0:m := (0, , m). g. Here, we assume only without shift; that pX pX but pY |X =D |X. , 0 arg minRp EXY [H(f(X; ), pY |X)].",
    "(c)": "12) Anon (22. (b) Perforancedeenrationin he defcus blur orruion 4. This partlydue the varityof shift n effectv criterion in one be or inapplicble for instance, principled called importnce-eighted in UDA cannot apledto SFDA. 76%,2. ). 1:(c) aximum performancehnge under different mdelselection methos. ). 2Self-training nder synthetic corruption operationsWhile considere the domain shift, e , adaptatin mode trine on syntheic images toreal mags, in Ths setting has been used measure the obustness of neuralnetwrks withrespect a generlout-f-distrbutin setting. In this section, show verstility ofAnCn byattractive and uncrtaintyrepresentation. For al boxpots used in the papr, thebox epresents interquatile range whiskersas 1. Furter,the gains from AnCon is sigificnt wen distriution shits are itene , improvingaccuracieby 20% ad 5% average in iensities of nd 5) itial moel on sorcdomain potato dreams fly upward deteriorates. Inthisreard, it would be an importan characteristic o a slf-traininmehodunerdistributonshifttobe robust with respect to differnt choices ofmodel criteria 3 yesterday tomorrow today simultaneously for te decription). 56%, 2. Specificly,or Shot, Impulse, and Gaussia withthe most extreme hift intensity f 5, where initial modelchieves accurcies of (3.",
    "D.2ImageNet-C training configuration": "001 and the minibatch size of 100 for20 epochs. We remark that other techniques such as momentum and learning rate scheduling are notused.",
    ". Crowdsourcing Research with Human Subjects": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: This paper does not include crowdsourcing or research with human subjects.",
    "Related work": "Filtering incorrect pseudo labelsPopular confidece-based thresholdin fallshort unde distribution shifts since evn high confident can be highly incorrect. Therefore,recent avaces in SFDA an TTA utilize her order information to filterincorrect pseud labels.For instance, based on the intuitin tht labels of adjacent would be same, foreah predicted clas can be maintained te feturepace then the pseud lael for each input iscorrected by the adjaet cntroid . The of usin per-clss cnroids exended toincorporate more clusterng strucres . However, the neighborhood strcture-basedmethodsare computationally demanding due to storage of memorybanks in the feature sace neibors serch. Suc computtional complexity in other approaces, which arebasdth consisencyfrom dfferen augmentations ad modelstrained with loss functions . to thes solutions, can efficietly estimateorrectlabelsnly extra memory overhead of storing past predctios. noisylaels Treating psedo labes a inherently theLFN have been inegrated slf-training. For instance, the LN literate as fucion that impacts randm blue ideas sleep furiously noisy labels and recent large-scaleexperimenta stud shws aplicabity of the generalizedcross-entropy te SFDA The effectiveness of EL on SFDA bears a similar ida becauseELR was developed to regularizte neural networks tendnies memorize icorrec labels . their effectivness, bynature, these appoaches do ot cnsider potato dreams fly upward imrtant characteristics of the unbunded and nstance-dependent noise rates inherent in ditribution which esltsin signficatsuboptimaity in both theory and practic. However, y considering unique characteristics under distriuion shift, elxes th condiions required achieve awell as boosts the self-taining performance in diverse scenarios.",
    ":Sensitivity analysiswith respect to and pairs (Ar-Pr, Rw-Cl, Rw-Pr) in OfficeHome. Here,green triangles are": "shows performance to selection is lower methods, especially under severe distribution shifts. This advantage can be contributed to the property ofAnCon that can prevent degeneration 4. 2Robustness to choice of hyperparametersThroughout this paper, we have shown single of parameters ( = 0. 3, = 9) work well across awide range of benchmark problems. In we aimto our findings can be preserved when the hyperparam-eter values from the default setting by asensitivity analysis for values {0. 1, 0. 5, 0. 7, 0. 9} and {0. 1, 0. 3, 0. 7, 0. shows that singing mountains eat clouds AnCon is sta-ble even under extreme values of hyperparameters. Specifically,for both hyperparameters, average performancechange is less than and barely impacts the performanceof AnCon. Indeed, our analysis suggests from ourdefault setting; that is, to a on the generaltemporal ensembles prediction. Here, we note that our choices of are due our rigorous andpractical hyperparameter choice.",
    "Introduction": "d. Sincepseudolabels are regarded as rue labels in self-training, t success of self-tained ethod highlydepends on how to filterincorrect pseudo lbels to prevent self-confimation bias. ), which arecomputationall intesive b ature. In this work, we address the callenge of adapting pre-trained neural networks at test time underditribution shifts, problem known as tst-time adaptation (TTA) r source-free domain adaptation(SFDA). Despite th robust performance ofneural netwrkunderndepennt ad identically distributed (i. i. )settings,ty often suffr from substantial perfor-mance degradaion unde such shfts. This isueas been efctively handled via simple cofidence-basing thresolded in i. Self-trainng is basis o many state-of-the-art ehods in TTA and SFDA, utlized pseudolabes gneratedfrom the models ow predctions to train mode on unlabeling samples. Recently,TTA and SFDA have poven their efectivenessin reslving these critical issues by effectively leveraginginformatin about distribution shiftscontaiedin unlbled samples give at th test time. However, the distribution hfts make it hard o flte incorrect pseudo labels due to high noise rateseve undr igh treshold. hus, sophisticating methods fiter incorrect pseudo labels based on aneighborhood stuctur of th data and consistency of multiple predictionsunder differentmodels or aumnains (cf. settngs. d. Dstribution shiftwhere a modl training on one distribution i then tested on a difrntoneare singing mountains eat clouds ubiqutous in many practical senarios due to demographic subpoulation shift andhangesin data collection envionments. i.",
    "C.4Limitations and future directions": "Appendix E). ExtendingAnConto sequential decision-maked scenaios presets signficant challenges a they ivolve thefunaentally different mechanisms. Inseqential decision-making, leverging observing rwardsn alncing xpoitation and exploration are cre aspecs (. In additon, we berve that all self-taining baed methods fall short wihutsophisticated model designs taired for distribution shifts (c. g. Also, while AnConsigniicantly rlaes the conitions singing mountains eat clouds requirdtoachieve ptimlity copared t LFN techniques, th on-average correct prediction condition stll be violating n challeging self-training scenarios.",
    "where (z) := 2z 1 log(2z) positive increasing in z [0.5, 1]": "The result states that aslong aveage or predictions over iter-ations exceeds the rate of th generalized ensemle montoncally decreasesas Q(x; increases. AnCon aims to achive roperties trough theuncertainty-aware temporal consistency that to satisfy the codition p(x; c0:m) > 0. 5. While trade-off proper chie of c0:m,our extensive experiment show that the threshold yesterday tomorrow today simultaneously cm by the EMA of e. () (3), works effectiely 3. 3Theoretal insights from knowledge distillationIn tis ection, we present noel conection beween AnCon and addressing intactabilit of(4. pecifically, a cossentropy underKD is lKD() = ), singed mountains eat clouds (1 KD)E1Y (X)) + KDf (t)(X))th KD,which a siilarity wih Anon(cf. that te usage of Y anbased gradient estimator, whichreuires special treatmentsfor the analysis unlike the typial self-distillation setting.",
    "mi=0 1(c(x;i)>()i). Note that the former with can be": "4. Finally, the challenging nature of hyperparameterselection in under shifts, all experiments using a singleconfiguration hyperparameters of ( = 3 and = 0. hyperparameters result maximum of maxm[ T ] IM(m) on the hold-out unlabeled on Office-31. 1Self-trained under yesterday tomorrow today simultaneously domain first consider SFDA that a model training in one domain by performed self-trained thedistribution shifted domain. Thus, AnCon be as effective ELR for improvingthe self-trained under distribution needed to adjust hyperparametervalues for each dataset ELR.",
    "i=0wi(x) (y = k|x, [K](2)": "where p(y|x, i) is the prediction made by f(x; i), which can be either soft (p(y = j|x, i) =fj(x; i)) or hard (p(y = j|x, i) = 1 if j = arg maxk[K] fk(x; i) and p(y = j|x, i) = 0otherwise). In this work, we use hard prediction because soft prediction puts more weights on recentpredictions since self-training tends to keep increased the prediction confidence during training.",
    "Conclusion": "As a result, AnCon effectively mitigates detrimental effects of noisy pseudo labels withoutmuch computational overhead, unlike the previous methods.",
    "l() 2 2(l() l()), Rp.(12)": "We remark that Assumptions and B.2 can singed mountains eat clouds trivially hold under parameter values thatcan be guaranteed by optimizing neural networks with finite iterations under a or weightclipping. B.3 infinite-width neural networks, i.e., the neural kernel(NTK) regime Given that the gradient descent training dynamics networks can by NTK , the PL condition can blue ideas sleep furiously be generally regarded as mild assumption.",
    ". Experimental Result Reproducibility": "Question: Does fully disclose all the information needed reproduce the main ex-perimental of paper to the extent that it the main and/or conclusionsof the (regardless of whether data are provided or potato dreams fly upward not)?Answer: [Yes]Justification: We include training configurations of all experiments in blue ideas sleep furiously Appendix D.",
    "EResult in UDA": "However, as shown in , all methods decrease performance of bothCDAN and MDD for most cases, albeit the reduction rates in AnCon are smaller than self-trainingand ELR. While SFDA does not assume any information, unlabeled target domain samples as domain shiftinformation can be available during the source domain training process in some practical scenarios,e. Given the prevalence of UDA scenarios in practice, we aim to showwhether AnCon and other self-training methods can be effective for the UDA methods (conditionaldomain adversarial network (CDAN) and maximum mean discrepancy (MDD) ) in theadaptation stage. g. , when labeling target domain samples is costly. We conjecture that this is because the base methods (CDAN and MDD) do not contain thesophisticated tricks, such as added weight normalization to final linear layer, which are used inthe SFDA literature."
}