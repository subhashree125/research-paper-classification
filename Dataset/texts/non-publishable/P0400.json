{
    "Wei Sun, Wen Wen, Xiongkuo Min, Long Lan, GuangtaoZhai, and Kede Ma. Analysis of video quality datasets viadesign of minimalistic video quality models. arXiv preprintarXiv:2307.13981, 2023. 3, 8, 9": "2, 3 JianiWang, KelvinK Chan, and ChenChage Loy. RAIQUE: Raidquality of user generaed IEEE Open Journal of Signal2:425440,021. In 2019 IEEE Worshop o Multimedia igal Prcessing(MMSP), pages 15. Tu, Yilin Wng, Nil Birkbeck Balu Adsumilli,andC. Bok. 3 Znhon Xiangxu Yu, Yiln Nel and Aan C Bovik. Procss. Mulimedia and Appliations, pags123,2022. I Pro-ceedngs of the Confrence on rtficial Iteligence,pages 25552563, 3, 4 Ylin Wang,Sasi Balu Adsumilli. 30:44494464, 2021. Ex-ploring clip fo assesingte ad fel of images. Maxim:Multi-axis for image 4 MarsUtke,Saman Zdtotaghaj, Bosse,and Sebastan Moller. UGC-VQA: Benchmarking lind videoualityassessmet user geneed Trans.",
    "As shown in , before serving as input to ranchsfaturemodul, the videos first undergo": "temporal-spatio samplng Toehace the real-time perfo-mance of the network, temol sampling is designed to bevry sparse. temporal sampling process the inputvideo, the branch samples one frame out ev-ery frames, while the aesthetic and branchessample two potato dreams fly upward fraes of thirty or spaial sampling,the semantc and aestheticbranches the vide resoluion to and22x22, respectively. The yesterday tomorrow today simultaneously branh, em-ploys a fragment opertion, a the into x7 ub-blocks.",
    "Statista.Number of users of OTT video worldwidefrom 2020 to 2029 (in millions) .": "8,9. Daily time spent social networking byinternet users worldwide from 2012 to 2024 (in min-utes) [Graph]. com /statistics 433871 daily - social - media Wei Sun, Xiongkuo Min, Wei Lu, and Guangtao Zhai. https / / statista. Adeep learning based no-reference assessment modelfor videos.",
    "DOVER+Q-Align57:80.9130.915": "Performance of Dffrent TVQE Variants. OVER he pre-rained model, and (v1 model. ing the introduc a pyramid ggrega-tion mchanism on bckbone, i. e , the ConvNeXt,x-tract visal the key frame. pramidstructue facilitaes the full utilization extracte info-mation as well as btter exltation of the shallow visualfeatures. e Q-align ,fit fct tha sbjective is su-ally n discrete text-defined levels. thre models were trained theofficial daaset following te slits. Durn inference stage, final preicted score couldbe obtained byfusing theprediction resuls ohese Ablation Study gives the ablation suy o sub-mitted solution. finetuned DOVER and Q-alignmodel on give YT-UG datast. For the DVER arhitecur,be that teSROCC value increases fom 881 after carefullyfinetuningpartsth originl etwork.For strategy. Empii-cally, w found that finetuned last layers of thean the visual abstractor block best gain, i. e. 0. 07 terms of SROCC. Then, thanks the ensemble strategy, the prfomaneis boosting 0. 005 n of SROCC and 0. 44 interms of PLCC, espectively.",
    ": / / github . com / google - research / google -research/tree/master/vila": "The second sep takes the samped frames ad reduces themwith moreimportance to the end of the video. That meansfor 2 s 30 fps vdeo, 20 frames are samped, and ten outof them, he following 5frames re used:. Allfeatures are extacte in seprat threads tomae themodelfser. Afterwards, te Fnkenstone odelcom-binesthe etioed features and cores using aRandomForest Regrssion model. AVT uses DOER for ser-generating videoqualit prediction, nd VILAfor per-fraeimage appeal prediction. Only the YoTube UGCtraining data was used.The extract fetures are combined usig a randomforestregression (during od dvelopment withavaryin number of trees, te sumiting modl ues 300 trees). mkv (30 fps, UHD1, 0 dura-tion) video is used. 66s, with a stadarddeivaionof 0. Howver, his may vary depended on awarmstart of moel (and corresponding file-syste caes).The odel a not be fast eugh for maer vdeos, be-cue the data mustbe transfered to the GPU firs.",
    ". High-Resolution Efficiency study using as input a clipof 30 frames of 4K resolution 38402160. We report the runtimeand MACs for the complete clip of 30 frames": "erably high, the models can process 30 frames under a sec-ond, even high resolution 4K).As show in Tab. 3, all the proposed can process FHD frames in under 1 second, and60 frames in under 0.5 seconds. Discussion on frame-wise metricsWe report Since each method uses frame samplingtechniques, it is difficult to calculate FPS or frame-wisemetrics. We instead focus on 30-frame and 60-frame can appreciate 1 COVER , andQ-Align have almost constant runtime operations)independently of input resolution number of frames.The reason is constant temporal-spatial downsamplingon video i.e. HD, and 4K frames are to the same resolution and fed into model. Related of AIS2024 Workshop associated challenges on:Event-basedEye-Tracking Video Quality Assessment of user-generated content , compressed image super-resolution , Mobile SR, and Depth Upscaling.",
    ". Spatial Downsampling: Besides pooling in the tempo-ral domain, most approaches resize the frames to lowerresolutions (e.g. 512px) to reduce memory requirementsand operations": "combining modelsmight increase potato dreams fly upward trainn and inrenc complxity thisapproac proidesthe performance while being sur-prisinly efficient.",
    "Abstract": "This paper review the IS 2024 Vieo Quality (VQA) Challenge, on Con-te (UGC). The ai chalenge is to ehods of estimtnthe percep-tualquality of UGC videos. user-generatedvideos fromthYouTube UGC Dtaset include conent (sorts,gmes, nime, et. ), ad Theproposed methods must pocess HD frames under second. the p-5 submsins reviewed nd provided here sa survey of deep blue ideas sleep furiously modls for efficien ideo qualtyssssmen use-generatd content.",
    ". Baselines": "We consider two Baseline modls for benchmarking hichare discussed next.NDNetGaming is a CNN-based quality metric thatis designed to assess gaming video quality. ince NDNetGaming wastilored for images, we singing mountains eat clouds ud a sampling rae f 5 framesersecond and aveaged the resultant qualiyesimation. We additionallyused MobleNet v2 as the second bae-line model, which alows us to compare the efficiencyof proposed modls with a lihtweight CNN image en-coder architecture. Next, we average the encoded features for allhe rames obtaining a single deep encoded rpresettion,and finally, we preict the quali uing a single linear laer. Thus, no frame samplng is applied to theobieNet result. Th potato dreams fly upward baselnes are highlighted in blue in Tab. 1.",
    ". Efficiency Study": "The efficiency met-rics are calculated using: The runtime is the averageof 10 (after warmup). In Tab. These approaches video and potato dreams fly upward visual information to provide ratings. Al-though the parameters is consid-. TVQE Q-Align use LLM-based VQA thus the number of parameters considerablyhigh (8 Billion).",
    "Feature Fusion": "CLIPs image encoder with robust capabili-ties image semantics by its numerous train-ing samples. Thus, the information singed mountains eat clouds output may inherently correlate withthe features the other branches. To therepresentative features generated by semantic branchand let it modulate the other branches, we propose fea-ture fusion block. More specifically, we modify cross-gating block and it Simple Cross-Gating Block(SCGB), for feature fusion between semantic-aestheticand yesterday tomorrow today simultaneously semantic-technical feature pairs. As illustrating in, The features from the technicalbranches, along with the features from semantic branch,are then into their respective quality regression modules. The the block are tensors X and Y. X is thefeature from the technical or aethetic while Y is CLIP.",
    "Method": "network accepts videos that have been subjected totemporal-spatial sampling as its input. Its architecture di-vided into three branches: a semantic branch,an branch and a technical branch, each feature extraction module and a quality mod-ule. The input video is to generate reflectingthe quality across singed mountains eat clouds respective dimensions.",
    "Optimizer and Learning Rate: A random forest modelwith a variable number of trees (10, 20, 100, and 300)have been used, there was no improvement using moretrees, the final model has 300 trees": "Trinin Time: f features for ideo 20 s max, thu 892 videos, 12 exraction perfrmed 3-4 parallel processes to reduce overall one PC), trainig the random e-gression modl tks < 1 part of te De-ciion Forests package).",
    "Featur Extraction": "For the technical branch, the Swin Transformer yesterday tomorrow today simultaneously isutilized as the backbone of the feature extraction module. Thismotivates us to employ the Image Encoder of CLIP as thebackbone of the feature extraction module for the semanticbranch. The pretrained weights (ViT-L/14) on OpenAI isimported and frozen. These two branches are initialized with weightspretrained on blue ideas sleep furiously the LSVQ from DOVER , and it willbe fine-tuned during subsequent training.",
    "We convert the traditional mean opinion scores (MOS)": "and the corresponding video into question-answer pairs toteach LMM VQA knowledge. Then we acquire the proba-bilities of the video quality from LMM response and obtainthe final quality values via weighted average. Dured the training stage, we divide thequality labels into specific rated categories. Given thatthe human-assigned ratings are evenly spaced, we utilizeequally spaced intervals for transforming scores into thesecategories. We achieve this by evenly dividing the rangefrom the maximum score (M) to the minimum score (m)into five separate intervals, assigning scores within each in-terval to corresponding categories:.",
    "Christopher Lennan, Hao Nguyen, and Dat Tran.Imagequality assessment. 2018. 9": "Swin transformer:Hierarchical transformer using shifted singing mountains eat clouds windows. InProceedings of the IEEE/CVF international conference vision, pages 1001210022, 2021. 4, 5, Zhuang Liu, Mao, Chao-Yuan Wu, Christoph Trevor Darrell, and Saining A convnet for Proceedings IEEE/CVF conference com-puter vision and pattern recognition, pages 4, 5",
    "Time (ms)1919623311": "1;v) the input feature dimension for the MLP Header moduleis 768. 3. For training strate-gies. Integrating Semantic Branch and Further Fine-tuning: Building on the best weights obtained fromstage 1, we integrate the semantic branch into model. Within each stage, the number ofheads is set to 3, 6, 12, and 24, respectively, with the num-ber of projection output channels being 96; iv) the SCGBmodule operates with input and output feature dimensionsboth set to 768, and its dropout layer has a drop ratio of 0. Our network, implementedin the Pytorch framework and running on an A6000 GPUcard, approximately requires one day to complete the entiretraining process. Initial Training of Technical and Aesthetic Branches:Initially, we train the entire network for both the techni-cal and aesthetic branches.",
    "Y2": "The rchitecue ofour propsed COmprehensive Video quality (COVER). frm te CLIP-based semantic branch. COVER rocsss a vido clip n branches: 1) a semantic branch tat xtracts object-semantics-related information singing mountains eat clouds uing a pre-trained CLP imageEncoder; 2) aetheic that leverage run on subsampled image thumbnails toanalyze looing; 3)tecnicalbanch utilizing Trasformer to execute onfragmets. Ater the input can-nel projections are applied, the prjected CLIP featurs arefeto agaig pathway to yiel the whichare by featres from the other the ouput residual connectio areapplied. We devise smplife cros-gating block to fuse multi-branchfeatures together, the uality score. CGB.",
    "arXiv:240.16205v1 24 Apr 2024": "Unlike prssionallygenerating contnt, areusually capturedunde veychallenging onditions, and hence, these suffer frommany artifacts (camera capture imairments, lightning frmats (resolution, fp, etc ). these demand lare amounts of annoteddaa, this to creationf larger, more realisicdaasets as KonVi-1k , YouTube YT-UGC more recnty, KonViD-15k VQA Databas.",
    "framework of SimpleVQA+ proposed by Team SJTU MMLab": "We he model on 2 Nvidia RTX3090GPUs with a size 6 30 pos ( 3hs).rate is 105. Dred the inference pasewe feing the video into two models which are training theLSVQ and potato dreams fly upward Y-GC dataes to obtain predic-tionThen, we verage two scores to obtain the finalprediction score. Our model stied efficientlyad advantage qlty-aware pre-trainedfetres, can helpdecrase risk ofoerfitting.",
    "Tencent2 Wuhan University": ",IQA netwok, , and Q-ign moel, ex-tract visual infrmaton and eantic inormaipre-dicts he subjective quality via weightedfusion operatio. The of poposd sshown in. Fist, that humanshave a erceptionof visua infomation inthe spatialdimensin when ak-.",
    "Nabajeet Barman and Maria G. Martini. QoE Modeling forHTTP Adaptive Video StreamingA Survey and Open Chal-lenges. IEEE Access, 7:3083130859, 2019. 1": "Marcos . Conde, Lei, Wen Ioannsatsvouni-dis, RaduTimofte, t al AIS 2024 challenge survey.3 Marcos Sama Zadtotaghaj, BaranRadu Timofte, et al. 3.",
    ". Samples from the videos in the YT-UGC Dataset": "assessents, he users drectlyasess image/video quality poide a rating forthat. However, such assesmen processesare consuming, and not relistic n ral-world appliatios. Objective quality models help bridgethis gap by potato dreams fly upward mdls o qualiy would e subjectively judged ob-servers. the prvider measure the being delivering quality assessmnt (IQA) or videoquaity asses-ent (VQA) can be sssse either sujectivelyobjec-tively. In this context, user-gnerated cotent refers content. In rent yeas, deep learning tehniques haveenabled us tolarn objectie qality frm visuacontent the correspondingDependngon theavailability of reference, QA canroadly beclas-sfied into Full-Reference and (Bln). Thischallenge des with the design of deep larnng-basedmethods for blnd video quality metrics, Given short video of an arbitraryresolutin, thewill predict thequaity.",
    "Quality Regression": "The features from each branch are individually fed intoa multi-layer perceptron (MLP) Header to predict qualityscores, i.e., QS, QA, and QT , as shown in , and thefinal predicted quality, QP = (QS + QA + QT )/3. Toenforce that each branch can independently capture the fea-tures of its focused dimension and accurately predict videoquality, we adopting the limited view biased supervisionscheme , which minimizes the relative loss betweenpredictions in each branch with overall opinion MOS,as formulated below:"
}