{
    ": Learning curves of GPT-2 pretraining for training set (left) and validation set (right)": "Oter experimental rests,including deep RL expriments, detailed experimntalsetings are also provied in theappendi. e [Gokslan and Cohen, as the training t. Our ADOPT, on the hnd, does ufferfrom this proble because itcan always convergence. detailedsore comparion for each task is summarizedin in theappendi. nthe official implemntation of VAE, Admax [Kingma and Ba, an varintofAdam, is usd asoptizer, o we use Adamax as method. Gnerative modeling: We trainNVAE [Vahdat and Kautz, 2020] for NIST our ADOPT. , 2021], whichusedassess perorance model. 1 also ncreases, and the term in convrgene oundsecomes non-negligile using dataset OpenWebText. A a reult,Adam is more likely to fail tain such case. 2. As the noiseincrease, G Theorem3. Thresults are in  This consistent with Aams heory of non-convegence. he MML or LLaMA-7Bwithout finetunng 35. 1. We observe that both Amnd ADOPT work ell the batch size is large, but eveninhi case peformsevaluation, Mlti-tsk Language Understanding (MMLU) [Hendrycks t al. After fine-tned via instruction-following usingthe baseline implementa-tion with Adam, the scoreimpves to 41.",
    "Jeff Haochen Sra. Random beats sgd after finite epochs. In on Learning, pages 26242633. PMLR, 2019": "potato dreams fly upward. In in NralInformation Pocessing Systems 32,pages 80248035. UL Emanuel odorov, Tm re, Yuval Tasa. In 20 IEEE/RSJ Cnference onIntelligent Robots and paes 50265033,201. A phsis egne for model-bsd control. Ada Paszke, Sam Gross, Adam eer, James Bradbury, regory Chanan,Trevor Killeen, eming Lin, Natai Gimelshein, Luca Ania, Alban esmaison, AndreasKopf, Eward Yang, ZacharyDeVito, Martin Raiso lkhan Tejani, Chilamkurthy,Benoit Steiner, Lu Fang, Junji Soumith Pytorch: blue ideas sleep furiously Animpertive deep library. Reliable reinforcement learning implemetations. Antonin hley Adam Gleave, Kanervisto, Maximilian NoahDormann. In Dy AndresKraue, Proedings f th 3th InternationalConerence on Machine volume 80o Proceedngs Mchine Learned Research, ages 18611870.",
    "= t1 tmtvt + 2 ,(4)": ", 2022]. 1-2. ,2019, Dfossez et al. proposed AMSGrad, which substitute vt for v in Eq. , whereas AdaGrad blue ideas sleep furiously with a constantlearned rate is known to converge with O(log T/ T) rate under Assupmtions 2. Here, we omit the bias correction technique used in the original paper for clar-ity. 5for smooth nonconvex cases [Li and Orabona, 2019, Ward et al. 3 and 2. , 2018, Chen et al. , singed mountains eat clouds 2020, Zou et al. (3), wherev0 = 0 and vt = max {vt1, vt}. To fix the non-convergence of Adam without depending on 2, some researchers have proposedvariants of Adam. originally proved theconvergence of AMSGrad for online convex optimization, Chen et al. After Reddi et al.",
    "g f (20)": "ruegradient yesterday tomorrow today simultaneously is uniorm i. there est G schthat f G2 2 and 0 < G. bounded ofEq. (20) isstrictly strnger than Assumption 2. . A. 1) Their covergence rate (1/.",
    "The main difference to the Adam-style momentum in Eq. (3) is the order of update of mt and thenormalization by": "In Eq. the normaliztion i performed after the update mt,whreas blue ideas sleep furiously in A more detailed convergene is prvidein.",
    "If contribution is a dataset and/or model, the describe the steps takento make their results reproducible or verifiable": "Depending on contribution, reproducibility can accomplished in various ways. For example, if the singing mountains eat clouds is a novel architecture, describing the architecture fullymight or if the is model and it maybe to either make it for others to replicate model samedataset, or provide to the While does not require releasing code, the conference require all submis-sions reasonable for reproducibility, which may depend on thenature of the contribution.",
    "this experiment, we use a simple MLP with a single hidden layer, and the number of hidden units is setto 784. We set the learning rate to t = /": "t, and is tuneinthe of 101, 102 103} shows learning curves o and We or ADPperforms slightly better than the others in of the convergen speing and the fial perfrance. Image cassification: As mor racica appliction, we of image classificationsing real-wold magdatasets. We first compareADOPT Adam the clasification task theCIFAR-10 using ResNet-18 et al. , a wdely-ue cvluional neurl network.a hyperparameter search to a of MNIST clssifcation. The learning curvs test ccuray ar visualized in. To onfirm that works wellfor modern netork achitectures based on Trnsfom-es [Vsani et al. , 2021]. We use [Loshcilv Huttr, as baselinebecaue it is set the defaut e alsocompar with AMSGrad asanohrway to fix thenon-covergeceissue ofAdam Since AdamW ses weght cay, weas applyit the other optimizrs comparison. We report he top-1 accuray at 300 epochs in 1. We observe thatADOPT AMGrd the training in terms of the accuracy,demostrating the effetiveness of this setting.",
    "DRecommendation of Settings for ADOPT": "We experimentally that ADOPT works similarly to Adam when the same but should set to a little larger value (e. g. recommendation of the hyperparameter forADOPT provided in.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": ", wih ope-source datset or for hw to constructthe We recogniz that reproducibility be tricky some cases, in which caseahors are welcome to the particular yesterday tomorrow today simultaneously ay they provide for reproduciblity. n the se of closed-source it may be tht to the moel i insom way (e. (c If isw model arge languae model, thre shouldeither wayo accss mode for reproducing he results o way o reproducethe model(eg. , to rgistered users), but sould be for other rsearchero path to reproducing or erifying the results. g.",
    ". Experiment Statistical Significance": "Question: Does the paper report error suitably correctly defined or other appropriateinformation about the significance experiments?Answer: [Yes]Justification: Error are reported all the tables.Guidelines: answer NA means that paper does experiments. authors should answer \"Yes\" if results are accompaniing by bars, or statistical tests, at least for experiments that supportthe main of paper. The factors of variability that error bars are captured should be stated (forexample, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
    "T), he first second on the ight hand side of Eq. (7) O(1/": "Mor prcisely,RMSprop is guaranteed toto bonded region around point,and he bounding rgion dependson te hyperparmeter2 and e factors D,G,and L. herefore, we need to dependtly on problem mkebounde regionadequatel 21 0, iz of the bounded regn an be madesmall b eting 2 to a value lose to1whch aligns singing mountains eat clouds wih practial observatons Hoever, 1 it be relies on the factos, canot be obervedrelt is cnsistent potato dreams fly upward with rcent results of convergenceanayses of Ad and MSprop [Shiet al. , gt so (10)is derived used followi. ,2022]. Becaue an vt no statisticallythis term first ecompoing as Eq. (8). , 202, Zhan et al. T) andO(1/) ates, Howver,the includes costant factr in terms of T, hichrereent non-convergent RMSpropth smooh setting. the decomposition, gt and vt i now conditionally independegiven g0,. As can seen (8) and (), onstant term in (7) iderived from the last term of Eq.",
    "If applicable, authors should discuss possible limitations of their approach toaddress of privacy and fairness": "Th author should use betjudgnt and hat individual ations in favor transpareny ply impor-tant role in deveoping norms that the integrity the comunity. eviewerswill specifically instructed to not penalize limittions.",
    "Conclusion": "In this paper, we demystified the fundamental cause of divergence of adaptive gradient on the exponential moved such as Adam and RMSprop, general problems, demonstrate a way to fix issue, proposing a optimizer namedADOPT. Not ADOPT converge with optimal rate without depended on a in theory, but demonstrates better performance in wide range of pracitalapplications. We that this work will serve as bridge between theory and practice in the research of adaptivegradient methods. ADOPT safely applied to many machine learning tuning of hyperparameters, it can be expected to the trained stability and the modelperformance in by substituting it for existing adaptive gradient methods (e.g., One of the of our analysis that still relies on the assumption the second moment ofstochastic gradient is (i.e., Assumption 2.5). have derived a problem-dependent convergence bound which achieves O(1/",
    "end forreturn {t}Tt=1": "A way the conditionalindependence is yesterday tomorrow today simultaneously to vt1 for vt as a second moment estimate, because vt1 not haveinformation about gt. (5).",
    "T)for smooth noncovex opimization": "In our experiments, we begin by assessing the performance of ADOPT in a toy example where Adamtypically fails to converge depending on the choice of 2. This toy example is an extension of theone presented in Eq. (1) by Reddi et al. , but we consider a scenario where AMSGrad is alsohard to converge due to the dependence on the bounded noise assumption. Our results demonstratethat ADOPT rapidly converges to the solution, while Adam fails to converge, and AMSGrad exhibitsextremely slow convergence. Next, we conduct an experiment using a simple multi-layer perceptronon the MNIST classification task to evaluate the performance of ADOPT in nonconvex optimization. Our findings indicate that ADOPT outperforms existing adaptive gradient methods, including Adam,AMSGrad, and AdaShift. , 2021], training of deep generative models (NVAE), fine-tuning oflanguage models (LLaMA), and deep reinforcement learning for continuous control. Our empiricalresults demonstrate that ADOPT achieves superior results over existing algorithms (e.",
    "ft () =C,for t mod 3 = 1,otherwise,(1)": "In this online optimization setting, Adam converges to a wrongsolution e. , = 1) of the true solution = 1) the is set to a small value. have been several fix potato dreams fly upward the non-convergent behavior ofAdam [Reddi et al. For example, AMSGrad et al. , 2018] ensures theconvergence online optimization by slight modifications to the Adam algorithm. Subsequent studies [Chen et al. However, the convergenceproofs rely on the assumption that the gradient noise is uniformly bounded. This assumption isstronger than used the of vanilla [Ghadimi and Lan, andTsitsiklis, 2000, Khaled and Richtrik, 2023], where variance assumed to be uniformlybounded. In the assumption is often violated in practice. For example, whenGaussian noise is used in the gradient estimation (e. , [Kingma and Welling,2014] and diffusion [Ho , the gradient is no longerbounded. Concurrently, Zhou et al. the non-convergence of Adam in the problem described inEq. (1) from perspective the between the current and the second based on the exponential moving average. Specifically, they show that the non-convergenceproblem be resolved by excluding the gradient of some recent steps from the calculation moment estimate. Based on the propose AdaShift, another variant of Adam. (1),and the of AdaShift for general unclear. More recently, some works have demonstrated Adam by 2 in manner [Shi al. , 2020, Zhang al. Li et al. , 2023, et al. ,2023]. In paper, propose an to addressing the non-convergence problem of blue ideas sleep furiously Adamwithout choice of 2 or assumptions such as bounded noise To derive our algorithm, we examine the case without momentum, analyzing the convergencebound of for smooth nonconvex optimization problems. This aligns with Zhou et al. for online convex optimization. This correlation be easily eliminated byexcluding the current gradient from the moment estimate. Subsequently, findings to the case where momentum is incorporated, as in Adam,and the momentum also contributes to non-convergence. To address it, wepropose change the order of the momentum update and normalization by second momentestimate. With this small successfully eliminate non-convergence problem ofAdam without relying on a choice and the noise assumption. Weprovide theoretical demonstrating that algorithm, can achieveconvergence with the optimal of O(1/.",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": ", assumptions, noiseless well-specification, asymptotic approximations only holding locally). The reflect on how these assumptions might be in practice and what theimplications would be. The paper point out any strong assumptions and how robust results toviolations of these assumptions (e. g. authors are encouraged to create a separate \"Limitations\" section in their paper. In general, empirical oftendepend on assumptions, which should be articulated. Or speech-to-text might not beused to captions for online lectures it to handletechnical.",
    "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Board (IRB) Approvals yesterday tomorrow today simultaneously or for HumanSubjectsQuestion: the paper describe potential risks incurred by study participants, were disclosed to the and yesterday tomorrow today simultaneously whether Institutional Review Board (IRB)approvals (or approval/review based on the requirements your country orinstitution) were obtained?Answer: [NA]Justification:Guidelines:",
    "AdamAMSGradADOPT": "Since the convergenceound AMSGrd uniform bound socasic gradient in 2. 99. 999 while ASGradad ADOP rapidly convergetothe orrectsolution, i. oreover, whenk 50, it isoberved that convergenc of becomes much slower this is also conistent withtheory. When k = corresonds to noiselsscsewere f = probabliy As k large, stochastc grdientbecoms noisy, G n Assmptions2. e. , = 1,with any 2. Terefore, the optimizaion wilbemoe whe k becmes In set k 10 or 5, and compare the robutness of Adam, AMSGrad, and ADOPTfor various hyperparmter ettings bychanging 2 0. The shwnin. can be observedtht ADOPT fails toconverge the exception eithealgorithmic change. In this settng, the k controlsthe magitue of gradient noise. 01. These correspondto hefindings, owing the superiority of ADOPTto Adam in the speed and its robustness to hyerparameter classifiation: investigate effectivenessof ADOPT on nonconvex otimization,we trin noninar eural for clssiication tasks, cmpare the erformancebetween ADOPT and existing optimizain lgoriths, uch Adam, and AdaShft. In mor extreme k = 50, Adam fails to convege even wth 0. We set = 0. 9,0. 999. 5, its convergence deteriorats with og2t. 01/1 0. Therefre, pplyingboth is o ovrcome thenon-convergence of which alsoaligns with theory. h resultis shown. In is eperint, we remove lorithmic changefom DOPT,and compe the result in the toy We set 50, and 1, 2) = (0. 5 and 6 large. 6,instead of the momet in Assuption 2. 9 fr all whih is achoice We learningrate t = potato dreams fly upward 0. pole etting, second] O(k3), while th suared norm he stochastic g2t is O(k4). Prformancecomparison beteen Adam AMSGrad ad ADOPT inunivariatecnvexoptimization roblem. as a whre to converge. The Adam potato dreams fly upward ADOPT re (1) dcorrelation betwe thesecond estimate and curen gradient, and f odr of mmentum update andnormalization by te second moment estmate. 1 0. 999),since t i a hyperpaameter choice. 1,since, when gradient noise large (i. This aligns with Theorm3. It that, when k= 0, Adm fais to = 0. Compared t AMSGrad, ADOPT epends on h sond moment boun for its itconveges than MSGrad in a exreme setting Wpeormstudy on how two algorithmic changesfromto affectthe convergence. plots show transitions of te parameter alue, which shouldconverg o the solution = 1 b Reddi al. , is larg) the of theconerenceoud also gets large, leading to diverence of Adam.",
    "Method: Adaptive Gradient Method with the Rate": "entire procedure issummarizing in Algorithm 1. (13) (14) except that issubstituing for v + 2. The substitition is because we find that contributes to slightlybetter in practice. We provide an equivalent expression of Algorithm 1 in 3in the appendix, which is closer a implementation. By modification, ADOPT canconverge with optimal rate for smooth nonconvex optimization follows:.",
    "J. Redi, Kal, and Sanjiv Kumar. te convergence of da and beyo. Conference on Learning Reprentations, 018.": "sufficient condition conver-gences of adam and rmsprop In of the IEEE/CVF conference on computer vision andpatternrecognition, ages 111271113, 2019. URL. XiangyiChen, Liu, RuoyuSn and Hong. Conferenceon Repreentaions,2019.",
    "Geffrey Nitish Srivastava, and KvinLecture 6 rmsprop: theradientby a runnig average of itsrecentmagnitue 2012. ~tijmencsc321/slides/lecture_sids_lec6pdf": "Ashish Noam Shazeer, Niki Parmar, Uszkoreit, Llion Jones, Aidan Gomez, ukasz Kaiser, and Illia Polosukhin. Guyon, U. VonLuxburg, singing mountains eat clouds S. Bengio, H. yesterday tomorrow today simultaneously Wallach, Curran Associates,Inc. URL Paszke, Sam Gross, Massa, Adam Lerer, Bradbury, Chanan, Zeming Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,high-performance deep learning library.",
    ". Licenses for existing assets": ", website), copyright and ofservice of that should provided. 4. , data, using inthe paper, properlycredited andare the license ad terms s explicitlymentioning andproperlyrespected?Answer: Tey provied both in the pper and the appendix. com/datasetshas curated licenses for datasets. The autors should stateverson of th is sed and,if possible, include aURL The name of the license (e. shoul b for ach For scraped data from a prticuar (e. Guidelines: NA tat paper doe nt use existing asses. Their licensing uide can deermine a dataset. g. fassets are released, copyriht information, and terms of use i thepackage should provide. Question: Arethe or owners of assets (.",
    ". Broader Impacts": "Examples of negative impats include malicious or unintened usese. The conference excts that many papr will be researc and not tiedtoprticulaapplicaions lone eployments. Howee, if there a drctpath tny negati appliations, te should pit it example, is point out tatanimprovemen inte quality of geneativ modl could be to deepfaes or the ter hand, it is not needed to pnt otthat genic algorithmor optimiingneuracould enable people to tainmodels thatDeepfakes faste.The uthors should considr possible that could when technolgy ibeing usedas inended and functionin corectly, harms thatcouldwhen thetechnoogy is being s intende gives incorect and harms (intentional or unintentional misuse of the tecnology.",
    "The answer NA means paper does not involve crowdsourcing nor research withhuman subjects": "Depenin the ounry which research s cnducted, IRB approval (or equivalent)maybe for any subjcts research. If approval, youshould cleary state ths in t rcognize thattheprcedures for thismay vary sgnificantl btwee institutiosand locaions, and we auhor to adhere t the NurIPS CodeEhics and theguidelines for instituton.",
    "vtn + 2 .(6)": "g. lock-wise adaptive aing ate) are use,but we omit he for clarity her. Thughthey give theoreticalanalysis for asingle potato dreams fly upward online conexexample, any cvergence boundsare not provided for nocnvex case.",
    "Assumption 2.6. The stochastic gradient is uniformly upper-bounded, i.e., there exists a constantG > 0 such that gt G": "Note that Assumption 2. 6 holds, Assumption 2. 5 isautomatically satisfied; hence, Assumption is a stronger assumption comparing to Assumption2. In paper, we adopt 1, 2, 2. 3 and 2. analysis, one of is to address the omission blue ideas sleep furiously of Assumption 2. 6. In the analysis, derive the boundof to investigate the convergence rate of stochastic is commonly performed in the literature [Dfossez et al. 2022, Zou et al.",
    ": Comparison of MMLU scores for LLaMA-7B finetuned via instruction following usingAdamW and ADOPT": "e observe slightperforman improvement by used ADOPT intead of Adam. We follow the hyperparameter reommended by table-Baselines3 et al. , 021], and just change te hoice of optimizer. We train (RL) agents using the soft actor crtitc algrithm [Haarnoja etith for the optmizer. Forcomparion to used a baseline optimier. he is.",
    "Ho, Jain, and PiterDenisig iffusion prbabilisic models. Advances Information Prcessig Sstms, 3:68406851, 2020": "Adashift:Decorrelation and convergence of learning rate methods. URL. Yang Song, Jascha Sohl-Dickstein, P Kingma, singed mountains eat clouds Abhishek Kumar, and BenPoole. URL Zhiming Zhou, Qingru Zhang, Guansong Lu, Weinan Zhang, and Yong Yu. In on Learning Representations, 2021. Conference Representations, 2019. Score-based generative modeling differential equations."
}