{
    "Corresponding author": "Publication licensd t ACM. Permission tomake digital or hard copies ofall or pat of work for persnal orclassroom us is granted wthout fe prode hat copies are made or dstributedfor proit oradvantge ad copies notice and full citationon the page. To coy otherwise, orrepublish to post on or redistribute to lists, requires specific a fee. with credt potato dreams fly upward is permitted. ACMISBN. Request from 24, August held the owner/autor(s). components of his work potato dreams fly upward owned by oters than must be honored.",
    "spatial-temporal graph neural network, lottery ticket hypothesis,spatial-temporal data mining": "Pre-Trained Identification Graph Winned Tickets in Adaptive Spatial-TemporalGraph Neural Networks. In Proceedings of 30th ACM SIGKDD on Knowledge Discovery Data Mining (KDD 24), August 2529, ACM, York, NY, 12 pages.",
    "(13)": "A the hardware lee, graph operations are linked tothe sparse an irreularnaturgraph structures. (13). comtationacompleity f graph convolution operationsexperices a otblereducion in To elaborte complexit in q. characteristic might not com-patible certai hardar architectures, leading to ncreasedfrequency of random memoy accesses an limited opportunitesfor dataConsequently, this can result in significantly higherinfence grah convolutions comparedto otherneural network Then, we itroduce self-loop to h. (11) whereas it is to O() in Eq. (11) stll faces limiations in compatibility. ere,T = E }, where E = , >| V } were E < V {}. Despite enhancement, Eq.",
    "A = GAT( K, ),(7)": "diameter of K is 1, AGCN fcilitates aggregaion of in-frmation from all nodes to each node within K. significantly enhances the capability tomodel spatial epedencies, in itsstate-of-th-art performance in relevant asks, s documented in. The odel utiizing multi-layer o AGCN spatialependencies isdesignated as ASTNN Satio-TemporalGraph Neural Netwok).",
    "RELATED WORK2.1Spatial-Temporal Graph Neural Networks": "A key characteristic of STGNNs istheir capability to model spatial dependencies among nodes, effec-tively learning adjacency matrices. Depending on their approachto constructing these matrices, STGNNs can be categorized intopre-defined self-learned methods. Pre-defined STGNNs typically employ prior knowledge graphs. These introduce innovative tocapture complex spatial-temporal dependencies, thereby offeringsignificant over traditional pre-defined models. Feature-based approaches, such as PDFormer and DG , constructdynamic graphs from time-variant enhancing the accuracyof model. WaveNet intro-duced an Adaptive Network (AGCN) tolearn a adaptive matrix. AGCRN furtherdeveloped this concept with a Node Adaptive Parameter Learningenhanced AGCN (NAPL-AGCN) to discern node-specific patterns. Owing to its remarkable performance, NAPL-AGCN model incorporated into recent models. performance of bur-dened considerable overhead.",
    "ABSTRACT": "blue ideas sleep furiously yesterday tomorrow today simultaneously In thispaper, wepresent a novelmethdto significantly enhancethe computational effiency of Adaptive Spatial-Temporal GraphNeural Networks (ASTGNNs) by introducng the concet of Winning Ticket (GWT), derived from the TicketHypothesi (LTH) By a pre-determining topologyas GWT to we balance edge with effi-cient ropagation, reduing demandswhile maintaining high model perfomance. steamlines the deploymen by eliminatin heneed for exhaustve trainin,prning, and reraining anddemonstrates epirically across various daasets that possibleto comparable to fl moels ith substan-tially lower costs Furthermore, wedelve t of the GWT from perspectiveofspectral raph theory, provding theoretical support.This adancement only provesthe existence ofeficient subnetworks within ASGNNs but also the pplicablity oftheLTH in resource-constrained settings, marking signicantstep forward in field of graph neurl etwrks.Coe is aailableat",
    "Lemma 3. The laplacian of T has eigenvalue 0 with multiplic-ity 1, eigenvalue 1 with multiplicity 2, and eigenvalue withmultiplicity 1": "As 0 is also an igenvalue,onlyone eigenalue to e So, teremainng eigenvalue must be.",
    "Lottery Ticket Hypothesis": "The Ticket Hypothesis (LTH) that within largeneural networks, there exist smaller (termed that, when trained in isolation the a level the a com-parable number of This finding has attracted research attention as it the of training a muchsmaller network to accuracy of a much net-work without going through the and cost-consuming pipelineof fully training the dense and then retrainingit restore the accuracy. \"Early Bird Ticket\" conceptbuilds on the original LTH. Fur-ther, generalised LTH to GNNs by iteratively applying UGSto identify graph lottery tickets. GEBT existence ofgraph tickets. However, the pruned GNNs to gener-alize to unseen graphs. RGLT is proposed to find more robustand generalisable GLT to tackle this issue. extremely and graphs, graph win-ning typically necessitates resource-intensive process training the network, followed by and retraining. However, methodology significantly streamlines the deploy-ment of ASTGNNs. It this by obviating the requirementfor exhaustive of training, pruning, and retraining.",
    "In this section, we conduct extensive experiments to validate ourHypothesis 1": "an frame-work, specifically combining AGCN layers with Gated RecurrentUnit layers. The AGCN layers at capturing spatialdependencies, whereas the GRU layers are employing to singed mountains eat clouds model thetemporal dependencies effectively. Neural Network Architecture. and GWENT AGCRN and GWNET trained within T, AGCRN. the existence ofGWT on two quintessential ASTGNN architectures: AGCRN andGraph WaveNet (GWNET).",
    ": the new node to the native tree": "The second scenario will increase some paths thatare longer than 2. For the spanning tree Tformed in the firstscenario, it still conforms to the definition of star spanning tree inHypothesis 1. Subsequently, assuming = 1, the complete graph K1aligns with this conclusion, and the star spanning tree is T1. shows two possiblescenarios. Only in the first scenario, does the spanning tree T meet thediameter = 2. In the scenario where = , the original graph is equivalentto inserting a new node into T1.",
    ": Perturbation process with a perturbation ratio of ,with a pre-specified node number": "Empirical datasets underscore our methods to achieveperformance on with full models, but at of the compu-tational cost. This not only underscores the existenceof efficient sub-networks of the spatial graphs potato dreams fly upward also applicability the Lottery Ticket to scenarios where resources limited. thiswork represents a significant forward in the optimization application of graph neural networks, particularly in envi-ronments where computational resources are constrained. In develop new STGNNs on pre-determinedGWT, at long-term spatial-temporal forecasting.",
    "and GWNET represent AGCRN and with in GWT-AGCNdescribed 4.2, respectively": "Oher raining-elated to recomened setings in cod repositories. to tir substantially larger scaes, experimentsonCA nd ere limitetoreetitions. T and reliaility,experimnts were cducted te time all except for CAan GL. comprehesive evaluation encompassesthe blue ideas sleep furiously followingdimensions: Performance: We sess forecastig ccuracysng estabishing merics: Mean Absolute Error RooMean Sqare Error (RME), Mean Percentage Errr(MAPE), and ii) Efficicy: efficiency in erms traed and infrence wal-cock Addtionall, the bachize during rainng themodls capbilityto manag larg-scle daasts set a btc imiof 64. These experi-ments wer perorming onan NVIDIA RTX A6000 GPU, equippedwith GB memry Metrics. summarizesthe specifiations of th used in our Thesedatasets wer partitioned in a 6:2:2 ratio for taining, valdation,and respectively. We conducexperimets on five o thelarest datasts. If model unable to oprate with this weprogressively blue ideas sleep furiously redce the bat siz to the highest ossible valuethat fuly utilizes thmemoryA600. The flow data in i agge-gate intrvals whereas fo SD, GLA,and CA,th aggregaion ccurs in 15-minute Implmetation Details.",
    "PRELIMINARIES3.1Notations and Problem Definition": "Frequently used notations are summarized in. }, where a frame X isthe -dimensional data collating from different singing mountains eat clouds at time. For yesterday tomorrow today simultaneously a chosen task time , we aim a function mapped the historical future observations in the next timesteps:.",
    "The results are organised as Test accuraciesand efficiency comparisons are reported in and ,respectively. We also compare theWe following observations from and": "andGWENT eonstrate performanethais cmparable or even serior cros ldataets. Our proposed is deonstrably scalale. Hwever,the poosed approach facilitates th training of te CA dataet. This notonly nderscores of the proposed pproah but also t superiorityConvenional prunig-based methos necessitate tartingthe training proces it a complee graph. GWT-AGCN has eideal ubstitute forAGCN. comprison, ASTGN within GWTAGCN demonstrates enhnced n termsof speed surpassing predecesso. The acceeraiois more promineton againt a portionofthe required by GWNETs is used onthirGCN layers.",
    "A = GAT( G, ),(9)": "blue ideas sleep furiously blue ideas sleep furiously where G is a sparse subgraph of K. However, employing G alonedoes not ensure the capability to model global spatial dependencies.",
    "(10)": "To mitigate the risk of excessive parameters and overfitting high number of yesterday tomorrow today simultaneously network layers, it is crucial to minimize much as In of this, we that star topologyspanning trees (with = 2) Wemake notes on the star topology spanning tree: (1) Motivation: GAT is message-passing network, and be viewed as modeled a fully connecting GAT, allows anynode communicate globally. A , as theminimum connected of complete can achievemessage passing all other nodes the graph layers, where k is the diameter of . Our goal tominimize the computational of ASTGNNs, so itsnecessary to minimize . Clearly, T with diameter 1doesnt So we start with = to existenceof spanning and we found that there exist T with adiameter of 2, forming star topology. Well detailthis motivation in (2) Theoretical Analysis: Basing on spectral graph theory, ifone is - of another, they have similareigensystems and properties. We can prove that T is an N-approximation K . So K and T have similar properties,allowing T with fewer edges to replace K forlearning good The complete proof can befound in Appendix A.2. Hypothesis 1. Given N-order spatial graph K of anASTGNN, we associated star spanning =V, E, E = {(, ) | \\ {}}, with desig-nating as central node, designated the leaf node. All suchT are Graph Winning Ticket (GWT) for graph thecorresponding ASTGNN.To ensure the existence of associated star spanning tree, the followed proposition:",
    "Pre-Trainin Identification of Graph Winning Ticket in Spatial-Tempoal GraphNeural 24, August 2529 Barlona, Spain": "Our research is centering on identify-ing the graph winning ticketa concept deriving from the LotteryTicket Hypothesisin order blue ideas sleep furiously to accelerate training and inferencein ASTGNNs.",
    "Convergence. illustrates the training loss and test MeanAbsolute Error (MAE) curves of the original AGCRN and AGCRN": "de idenicl settings on the PES07dataset. presents thesecurves or the same models onthe S GWT ensures covergene that consistent and stable as complete grah model. is particularly dvantageos for large-scalesatal-tempoal data, asit witht comprmising thequality convergenc. te convergece behavior AGCRN demostratesits robustness in cmplex spatial-temporal depenencies. is crcial reliable forecating in dyai systems,such blue ideas sleep furiously as traffic networks, whee understading intricae patternsiskey AGCRN&GWET vs. ACRN and GWNET, as rep-reentative ASTGNNs between 2019n 2020, are ofsgnificant interest in our study. To.",
    "strategies for in enhancing the predictive performance of themodel": "Sice AGS not pr-ide n on WNet, e report the esults The lack CA resultss to AGS out-of-memory issues. Weth effectiveness o T to robstconectivity, is crucial for ASTGNNs abiity to mode dependencies. ro , we can se that ur ethodsigificantly outperormsPerturbed T. Cmparison AGS. further validate this perspetive, we into-duce a perturbaio prcss illustated in resultingin T, accoringto thefolowin steps:.",
    "Joshua Batson et al. 2013. Spectral sparsification of graphs: theory and algorithms.Commun. ACM 56, 8 (2013), 8794": "Feeway Performce easuremet Loop Detector Data. InProceedings o 38t Interational Conerence on Machine Learning, ICML 2021,1-24 July 221, Virtual Evet (Proceedings Learnin Research, Vol. andTong Zhan (Eds. ). 21952207. Gl. Jeongwhan Coi, Hwangong Jeehyun and oseongPark. Graph eural Cotrlling Differential Equations Trafic yesterday tomorrow today simultaneously Frecasting. Thirty-Sixth Confernce Artificial Intelligece, AAI Tirty-Fouth Confer-ence on Applications of Intlligence, IAAI 202, TheTwelvethSymposium on Educational Advances in Intellience, EAAI 202 VrtualEvent, February 22 March 1,202. 6376374. Aaptive Grph Nework. Zheng Fang, Qingqed Guojie Song, and Kunqed Xie. Spatial-temporal graph ode networ for fow forecasting.",
    "Central Node Selection. Owing to the non-uniqueness of T": "3). To that selectedcentral node embedding vector is positioned at the node embedding , we for a setting where = Mean(), technique we refer to as averaged initialization. Viewedthrough the lens AGCN, random selection a node fromthe vertex set V is analogous to initializing the embedding randomly. the complete graph K , directly employing T for presents the challenge selection."
}