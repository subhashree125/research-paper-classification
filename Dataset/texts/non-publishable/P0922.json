{
    "Experiments": "We conucted ample xperments to verify he effectivenss ofeach modue. blue ideas sleep furiously The mi reslts of our local evaluai are shownin , where we got a lot of improvement in he publi tesset compare with thebaseline of Task 1. We got a 15.8% scorewher we greatly redued the halucination ratio ad change thesehallucinations int answering I dontknow We shw detiledanalysis and ablation studies of our evaluation results. In the finalprivte tes, wegot 21.8% score in Task 1. e will also howan analyss of the prvate evaluation. We will use Official RAGBaselie s the baseline model of Task 1inte following table.",
    "qc": "hrefore, weprimarily leveraged the LLM knwledg xtrctr, as dtailed below,to enhance quality of therferences. Ourexperimentsindicated that employin LLMs for cmplex query maniulationmethds did not significantly mprove rtrievalaccuacy; insead,it reulting n substatial computatinal overhead.",
    "System Design": "There are6 critical modules in our system, including (1) web page processing,(2) attribute predictor, (3) numerical calculator, (4) LLM knowledgeextractor, (5) KG Module, and (6) reasoning module. The complete design of our system is shown in. We will introduce thesemodules as follows.",
    "Numerical Calculator": "Instead, these require themodel to derive the final answer through series steps,which often involve precise numerical especially domain finance and sports. Within theframework of the CRAG tasks, answers to aggregation and post-processing questions are not in yesterday tomorrow today simultaneously the retrieved content. requirement poses additionalchallenges to the models to generate accurate We encourage the large language model to articulate reasoningsteps necessary to solve the problem as expressionswhile delegating numerical calculations to an externalPython retrieved text chunksand tables that may contain numerical information singing mountains eat clouds the prompts and employ prompt that encourage the modelto valid Python expressions 2 the sub-mitted version, utilized multiple sampling and thegenerated Python expressions using the function. To mitigate such the best practice forensuring system security is to use ast. or to executethe code within sandbox environment.",
    ": The evaluation results in the private test set": "in aligns wih expectationsset forh CRAG. Furthermore, illustrates scores across at-tributes, revealing that our has been onstatic slow-changng questios, challengng ones Ths deicency in handling dynamic quetions has also led to subotimal in qustion types. The score resutsalinith tose our internal assssment; however, weencountering a higher numbe of incorrect responses in moviedomi on siple question types.",
    "CRAG Benchmark": "CRAG bencmark is a factual uestion-answering benchmarkwith thousnds of QA pairs and 50rea-world webpagesfr eachdata. The benchark also poides a mock APIfor Knowldgeraph(KG) searching. he benchmark is et for te KD Cup 224with 2, 706 data items availabl in public, yesterday tomorrow today simultaneously haf of whichare forpublic validation. Thereare 5 domains and 8 question types thebencmark, ad eachdata item has satc_or_dynam labelha indiates whethe the answerto a questio chages and teexpected rate f change, whic ca be used to analyze the modelstrengths and weaknessesIn orderto mmi real-wold applicainscenrios, each generated esponse is liiting to30 seconds o anAWS G4dn.12xlarg instance which is equipping with 4 NVIDI T4GPUs poviding aotal of 64G of GPUmemory ding infernce.The benchmark is split into three taks in th competiion: Re-trievl Summaization, Knowledge Graph nd Web etrieval, adEnd-to-End etrievalAugmented Generaio,which arefrom si-ple tocomplex. We intodce the three tass as folows:",
    "Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, et al. 2023. Fresh-LLMs: Refreshing Large Language Models with Search Engine Augmentation.arXiv:cs.CL/2310.03214": "Haiming singing mountains eat clouds Wag, Ye Zhengying Liu, Shen, Yichun Yi, t Dt-solvr: Automedtheorem wihdynamic-tree guied y proof-level untion. n of the Anual Meeting of theLinguistic (Volume 1: Lng Papers). Advancng LargLanguage Models via potato dreams fly upward Qery eneratin lending Knowl-edge Filtering arXiv preprint arXiv:40. 11129 (024).",
    "Related Works": "Numerous techniques have been proposed to address the issues. For example, formal verification can to and output verified reasoning process. Moreover, are trained methods thatcan help the model adapt domain-specific knowledge. approaches been the literature, themajority are tailoring to specific issues within limitedrange of scenarios, making them direct applicationto CRAG We built upon previous research structurethe conventional RAG workflow into four phases: pre-retrieval,retrieval, and generation.",
    "Introduction": "success is bult on foundational knowlege,incungfactul knowledge , relationl , yesterday tomorrow today simultaneously and singing mountains eat clouds , acquired by tough exposure to he internet corpus duin training. studies have shownthe ablityof LLMs knowledge interalizaion generation, which is implicitly storing within modelspaeters and rtrieved during the generation",
    "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, et al.2022. Training language models to follow instructions with human feedback.arXiv:cs.CL/2203.02155": "Yu Pan, Ye Yuan, Yichun Yin, Jiaxin singed mountains eat clouds Shi, ZenglinX, et al 2024. Art: Automatic mlti-tp reasoning and tool-use forlarge language models. In Procedings of the yesterday tomorrow today simultaneously AAAIConference Artificial Intelligence, ol. PreanLessons for Progressive Trainingon Language Models. arXiv preprint arXiv:2303. 0901 (2023). Bharavi Parajape, Scott Lundberg, ameerSinh, Hannaneh Hajishrzi, LukeZettlemoyer, et al. Yu Pan, YeYuan, Ychun Yin, Zenglin Xu, Lifeng Shang, et al Reusigpretrained modelsby multi-linear operators for fficien training. 2023. 38.",
    "Abstract": "In the mean-wile, we have attaind outstnding results inonlieassssmets, Weparticipate inMeta CRAG KDD Cup 2024 blue ideas sleep furiously as Team lectricSheep, securing thirdplace in Task 1 and achieving first place i five of the seven question typein Tak 2ang over 2, 000 articipants an 5, 500 sbmssions. Coneece KDup Workshop of IGKD 24, August 2529, 2024, Barcelna, Spain2024 Copyright held by te owner/autor(s). Permision to mke digit or had copies of all or pat of tis work for personal orcassroouse is grantedwithout fee proided hat copies re not made or distributefor profit rcommercial adantage and that copi bear this notice and the full citationn th first age. Retrieval-aumented generation (RAG) is aframework enablinglarge laguage models (LLM) to enhance their accuracynd re-duce hallucinatins by intgaing external kowledge bases. Copyrights for third-party omponents of this work must be honored. Inthis paper, e introducea hybrid RAG system hanced througha cprehenie suite f optimizations that signicntly improvererieval quality, augment reasonig apbilities, and reine nu-merical computation ability. The projct as conducting as par of GngboSuns research internship at theStateKey Laboratory of Multimedia Information rocessing, Peking University Mig Zhang is te advisor of the team. In local evaluaions, e ave significantlyimproved accuracy and reduced error rtescompared to the bae-line modl, achieving a notable increasein scores.",
    "Discussions": "Many or be For example, weonly used two tower models fr rerieva, while modelsareore suitabl for Mreover,a tw-stage retrievalandre-ranker system sould beused task 3. he urrnt methods fo handling tables are rativelysimpl. Some usles ortoo large of noisebt hand these project is supported by the NaionalKey ndDevelopme Program of China with Grant 023YFC334123as well a Nationl Ntural Science Fundation of withGrant Number 62276002. We thank Prof Ming mntoship and supprt o theteam and Wus a discussions.",
    "Post-retrieval": "The post-retreval phase includes re-rankingand iltering whichfurther refines he rnkingresults, filtering otmaterials thatare irrelevant tothe queing topic. studies emplo large lan-guage to utlity of retrieved information adto crtique statements re substan-iaed thassociate douments or condenseretrieveddocuments into textual summary, therey minimizing infrencecosts. Filtring to redundant parts the e-trievalesults, whih may f quaity or exhibit lowrelevance to user quey. Some ap-proaches embed the query-document pair i a single pass,fcilitating cross-atttion potato dreams fly upward Other studiesemploy large lanuage potato dreams fly upward models few-shot annota-tors to generte data for training cross-attention re-nker orinvestiate th autorgressie generation of r-ranking reults toleverage iter-docmnt relationships.",
    "Conference KDDCup Workshop of SIGKDD 24, August 2529, 2024, Barcelona, SpainYe, et al": "We used theall-MiniLM-L6-v2 the sentence embeddings, whichare used to the So, version relied on the few-shot learning approachwith the large language model for classification, and we leave theimprovement for as work. We prompt the model with classification instructionsand 5 demonstrations for categories, instructing it to classify subse-quent questions. For attribute, we implemented twomethods for question classification: leveraging the in-contextlearning capability of large language models and other employ-ing support machines (SVM). 1. In-Context Learning. Large language models demonstrate robustnatural language understanding strong multi-task abilities. However, the is hard to predict becauseit often needs reasoned all references, so we didnt ap-ply prediction to it. We will show thedetailed results comparison in. We also tried an SVM classifier using the dataset to reduce computational overhead. was divided into followingcategories: simple question, question with condition,set question, comparison question, question, multi-hop post-processed question, false premise question.",
    "Conclusion": "the designing ystem, finally in ask1 n ot the prize for out 7 questin types singing mountains eat clouds in task 2. Thefnalevaluation scores befound theinners AnnuncmenThe table tractor reasoing module, and calculator moule haveeostated enhancements over thebaseline system. dditionally, the IC attribute has signiicanl reducedhallucination in responses to difficlt.",
    "Knowledge Graph Module": "As astructured knowledge base, KG accurate generation of a KG query is crucial to determiningwhether the system can the correct answer. The qualityof rule-basing queries by the complexity of the rules, andhard to So tried function-calling method, which makesall APIs as the input LLM, yesterday tomorrow today simultaneously and it generate aproper calling. However, due to limitations in time potato dreams fly upward andresources, we unable to optimize and prompts function-calling method, resulting in suboptimal performance. we reverting to the KG baseline method and did notmake improvements in the submitting version. We show theprompts function-called Appendix D.",
    "Web Page Processing": "This library effectively mitigates noise generating by recurring ele-ments such as headers, footers, and links. First, we truncate individual sentences that exceeda predetermined length threshold to ensure they remain within anappropriate length. The meaning of sentence is often influenced by its contextualplacement; therefore, the information within a single sentence isoften incomplete. As a result, webpage processed is a critical component of system design, directlyimpacted both the quality of the extracted information and theaccuracy of subsequent language model generations. This complexity is due to the frequent presence ofsignificant amounts of noise that does not contribute relevant infor-mation necessary for task completion. While some HTML tags maycontain semantic information that aids in paragraph segmentationor title identification, the useful information is not easy to extract. Finally,we connect any remaining ungrouped sentences in sets of 3. We organize sentences into chunks based on thefollowed rules to enhance semantic coherence in subsequent re-trieval processes. Web pages serve as a sharing information source yesterday tomorrow today simultaneously for all three tasks,containing a substantial amount of potentially valuable informa-tion that can aid the model in task completion. We also clean all the tablesby identifying the <table> tag, which will be processing specially. To address challenges posing by thecomplexity of modern HTML web pages, we adopt trafilatura, aPython library specifically designed for gathering text from the Web. Such noise can unnecessarilyprolong the models processing and reasoning time, potentially lead-ing to misinterpretations. Text Chunks Processing. Moreover, there is some structuring information in the HTML liketables, which will do harm to the text quality if they are improperlyhandled through methods such as truncation or splicing other texts. We hypothesize that exposure to numerous documents formattedin Markdown dured the model trained phase will improve themodels understanded and interpretation of this format. Finally,. Given that the extracted text data typically doesnot include tables, potato dreams fly upward we have employed BeautifulSoup to extracttables from web pages and convert them into Markdown format. Tables Processing. types of noise encountered includedecorative HTML tags used for typography, JavaScript code, and in-ternal comments within the web page. After extracting text used these twotools, we utilize functions provided by Blingfire library tosegment the text into individual sentences. All keywords utilized in this processcan be found in Appendix B.",
    "Reasoning Module": "We carefully designed a prompt templateto let the LLM do reasoning from all these references and get thefinal answer. We designed several rulesto constrain the reasoning path and output format, including thatthe output should be precise, and guide the model reasoning byasking intermediate questions in the prompt. 5.",
    "Large Language Models, Language Generation, Retrieval-AugmentedGeneration, Reasoning": "AC Referen omat:YeYuan, ChngwuLiu, Jingyan Yuan, Gonbo blue ideas sleep furiously Sun, Siqi Li, and MingZhang. CM, New York, NY,USA, 13 pges.",
    ": The main results of our designed system evaluatedin the public test dataset": "attribute predicto, are hard for or systemand wedo not haveenough time and to improve tem. letthe ystem nswer dont kno for questions. several rules and prompt engineering techniqus in module to letthe model answer  dont kno whenit is unsure. Ultimately, configured the to exclusivelyoutut \"I dot know\" nd refrain rom addin an additonal wordswhenever\"I kow is included in iitial response. The prompt for is shownin Apendix. Increct Format. Cause we conduct cnstrained samplingfr reasoning output, the is the possibility that he model answers that ca prsed. situation,we design a backup summarization agentsummarize the finalanser and concisely based on the reasoning modlesoutput fals.",
    "LLM Knowledge Extractor": "This inclination may be a result of the insructionalfine-tuning process. In lightof this obsrvation and drawing inspiration from priorresearch, edeveoing largelanguage model knowledge extractor. Theprmpttemplate is sown n Appendix D. It simiarly utilizeszeo-shot indicatons, which include prompts required modelto assess whether a given quey prtains to a false-prmise issueand to generate moreconcie respses. 3However, as dscribed pevously, letting th moel directly an-swer the questions ill yesterday tomorrow today simultaneously introdce halluinatins in its knowlede,. However, a notable distic-ton exists in the lack of reference docments sured from externalknowledge bass within the rompts, as well as the excluson of ul-tiple samplng, whch is intended toreduc computational overhead. Chancesare that the reference dcumes are not beneficial in generatingaccurat responss to the models queries. Our findings indicate that whenefernc documets are provided witin the prompt th mdelexhibis a ignfican tendency to extract answers from thee ocu-ments, reardless of wether the documens contain neessryinformation. Our find-ins ugest that tis approach results in favorable performance onquestos classified as low-hanging and stable Mreover, we also use th zero-shot CoT toletthe modl reasoning by itelf for mre accurat knowedge. Furthermore, during the genera-tion phas, we employ Llama 3 instuction-tuned modelshatare optimzed for blue ideas sleep furiously diloue use cases nd possess a robustcapa-bilit for folwing instructions. I contrat,LLMs that oerate withoutanyref-eenes are capable o accratly answered these qestos. Consequently,these refeence docuents do no nhance the odes capacittoproduce aswers but hrm it. Te processof extracing knowledge from te model cloelyresembleshe ormal model generation process.",
    "\"system_prompt\": \"You will be provided with a question. Your task is to identify whether this": "question is a static or a dynamic question. A static is that theanswer is and not change You **MUST** choose from one of thefollowing choices: [\\\"static\\\", \\\"dynamic\\\"]. You **MUST** give the question typesuccinctly, using possible.",
    "Analysis For Private Evaluation": "We be-lieve this is due to our underutilization f informaton from heknowledge graph. hows the prizes we won from 5 f 7question type in Task 2. Our ysem acieed scors close to the champininTask but fell sgnificanly behind in Task 2 and Task. show e sce fo Task 1 in rivatevaluation."
}