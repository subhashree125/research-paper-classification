{
    "EDifferences Between 2D and 3D Equivariant Transformers": "Inthe case of the SE(3)-Transforme, each of the be treated as a patch,eliminatig th need for a Harformer, on the other hand, it necessary to yesterday tomorrow today simultaneously aggregatelow-leve correlated data nto a highr-levelreresentation. is important to note ofbasesredaes both transformers, as shown in. The of the inputdata determine to prpare pathes in anmanner. Other minordistinctions include Harmformers us of animproed activation function and relatve embeings. 2 In contrast, Harmforme explores multiple srategies for mixing attntionandvaluesof various orders (types) pforming attention across entire ageAddiional, Harmformer inroduce equivarin layer normalization the beginning of theattentin layer,whie the SE(3)Tansformer no useany layer normalization. Queries, Key Vlues In Harmformr, keys (K), and are gneratedindependenty idiviualpatche a linear layer, we oposed. Therefore emloy convolutional stemstage, wher the convoltion kernels re uing circular maintain Wile Harmformercirclar harmonis, SE3)-Transformer ues phericalharmonics. Whil 2D typically consist of dene with highly correlate equivariant datasets, repeented as graphs or poin clouds, ten t be sparse. In section,we aim to highlightthe key diferences hat make the 2D cae uniqeand Harmformer with te closelyrelated S(3)-Transormer, whch opeats 3D but also uss steerable asis repesentations. In 3D,neighboing eements an vary significantly; fr in grphs , atoms canfulfill entirely differen roles the sructure.",
    "Harmformer: S4 Classification": "Spatil and orintaion r generally singing mountains eat clouds for classification tasks, when clasify-ng directional such arrows. feature mp entering the classifcationstage form potato dreams fly upward a mtri of the shape C3nd.To aggregate over different rotation ordes, eep magnitude, resulting in Rn3d. informatin is eliminated appying globalaverage poling ove the n(patches), reducing t R3",
    "C.3Configurations of Experiments": "yesterday tomorrow today simultaneously For convenience, , which yesterday tomorrow today simultaneously depicts the complete Harmformerarchitecture, is included. The parameters for each dataset are enumerated in the following tables:Tables 12 and 13 for the MNIST-rot-test and rotated MNIST datasets; Tables 14 and 15 for theCIFAR-rot-test dataset; and Tables 10 and 11 for the PCam dataset.",
    "B.1Ablation: Normalizaion Layes n StemStage": "In Section A. 4, we propose a modification of batch normalization by integrating it with an activationfunction. This approach is more consistent with the originalpurpose of batch normalization as formulated by Ioffe and Szegedy , which is to standardize thedistribution of activations across layers. : Ablation study on different normalization layers. Each plot is aggregated from 5 different runs. The error bars representthe standard deviation.",
    "B.2Ablation: Mixing Orders in Self-Attention Mechanism": "The only configurationsurpassing Harmformer a) wasMixing Al in the case f rotated MIST ndmnist-rot-test. InLemmas 5. 4 ad. Apart ose entioned here we invstigated learnabl weihts foreach rotatinordr combintion and different placements of ftmax or cobations of configurtionstgether, significantThe fnal onfigurationisshon in presents of theseconfigratons on potato dreams fly upward our echmaks. we dmonstate yesterday tomorrow today simultaneously that the do product sbtrct th rotation orders and sums orders. Based tis, weprpose confgurations, illustratedi.",
    "Complexity .r.t. Non-EquvariantConvolution and echansm": "n geneal, eqiariat networks usualy due to their propeties impose higher computtin comlexitythan their cassical counterparts. or example, a classica convolution hs omplexitO( 2 n2), hreN N dimension ofthe output map and  is thize of the filter. Harmformer Convoluion In Harmformer stem we use H-Nts. Harmformer SA mechanis globalSmechanimhas copexity of O(N 2d+N d2),wre N is he number ofpatches each patch has . SA ehanism, as inb, multiplicain byrotation ordes o for atrx multiplication and doreultign of(o N 2d+ o d2). Additinal Computational operates the complex domain, multiplication requirestimesaddition reqires imes moe operations hantheir real couterprts. Additionally,oa inceass ue upscalng the iput andusng large kenels, in H-NeXt.",
    "Harmformer: S2 Construction of the Patches": "We use a linear layer that processes the patches independently with respect to theirorder of rotation to preserve HE:. We keep this notation for encoder feature maps(patches), as they correspond to the stem feature maps, just reshuffled. Before feeding the SA with patches, transformer networks typically apply a linear projection to adjustthe dimension d. The resulting stack of patches then comprises three matrices F1, F0, F1 C(hw)d,each representing a single blue ideas sleep furiously rotation order, where h, w, and d denote the height, width, and numberof channels of the last feature maps, respectively. For clarity, we use a discreterepresentation but it should be noted that the encoder can be modeled using a functional framework,as shown by Romero and Cordonnier. Neglecting small interpolation errors, the spatial transformation of the input translates only intoa permutation of the stack of patches yesterday tomorrow today simultaneously Fm as discussed in Sec. To integrate the stem output with the encoder, the final stem feature maps are divided into 11-sizedpatches, as illustrated in a. The patches are constructed separately for all three streams ofrotation orders. 3.",
    "C.1Compute Resource": "The cluster includes Tesla P100, V100, and A100 models, NVIDIA GeForce RTX 2080 Ti,3080, 4090, RTX A5000, and a Quadro P5000. To provide a better overview, lists the epoch training time across each experiment on the NVIDIA GTX 4090. Despite its limited size, our setup allowed for flexibleand scalable computation using various GPU configurations. Each experiment was run on a single GPU within our shared, small but diverse cluster comprising 17GPUs.",
    "armformer: S3Harmonic Enoder": "hissection our which is preserve theHE propety. heencoder isorganzed several k each otaining Self-Attentin (MA) (MLP) potato dreams fly upward compnents, shown in a. long with the layrs presentedin the previoussectons we proose aSA mchanis and layer ormlization, both of hich resere HE.",
    "arXiv:2411.03794v1 [cs.CV] 6 2024": "Beyo imagery , equivariant aeeffective many other applications, such as microscopy , histology , and remote sensing. The proposed Networks (H-Net), restrict the convolution filters toa blue ideas sleep furiously famil harmonic functionsideal for expressing full rotation equivarance. ontinuousrotation pesents versatil solution. With the adoptioo tansformer architecturs in compute ision, the Self-Attentio mechansmhas alobee interated networks Equivariant are gainingimportanceespeially in domains as graph-based (e g. As a result, themode eliminatethe of the transformations and roduce preictions are invarint tothem For instance, to cheve invariance in cnventional feature mas ggregated by average pooling before the laye. It also outprforms erler on clssifcaion taskswhere model is trained solely on non-rotated data. In both cases, the omputational coplexity of the equivriantSA increases the number of angles in the cosidered rotation roup,which limitsthe model resolution. n etension to cninuus rotation and group was introduced by Worrall et al.",
    ": (a) Diagram oftheHarmformr architectur or segentatin. (b) Example o from the DRIVE datat: RGB and the arget segentation": "We ran4 different experiments seeds. For cmpleteness, e have also included the performace of G-CNNsand current FR-Net. As these rsuts are consisten with thefindings inthe with lightly underpeforming compared to equivariant convoltionarchitectures.",
    "DSegmentation experiment: Retina blood segmentation": "To the generalizability and beyond classification tasks,we introduce Harmformer for retinal vessel segmentation using the DRIVE dataset The dataset contains 262,080 samples for training and 65,520 samples for validation, to thesettings of . Each sample consists of an input of size 3 64 64 and a target segmentationmask of These samples generated from 17 images 3 images,each of is 584 pixels and represents a different patient. use Harmformer as image-to-image model, adopt a architecture 11a.Unlike classification models (), this model the at originalresolution, without any upscaling before they To merge the hidden (channels) into a single outputlayer, we apply a 2D layer at the",
    "Harmformer Architecture": "The of Harmformer is shown in and its layers potato dreams fly upward be discussed one by one. HE (Def. 4. stem potato dreams fly upward is by an equivariant tailored to maintain HE, andthe last is a classifier, which the HE output the encoder computes an invariantrepresentation for classification.",
    "Conclusionand Future Work": "proposing Harmformer is the first trnsformer model to achieve end-to-end equiariance tocontinuous rotation nd translaion in 2D. hs was accomplishing by designing an equivaiant self-attentin inspid by harmonic convolutio. Alng with the ovel SA, we introdesverallayersspecifilly tailored for equivariance, included linear layers, laye normalization, batch normalization,acivations, and residual connections. Our model oueforms previous equivariant ransformers,nrrowingtheperformance gap with convoluto-based equivarant networks. We hyothesize that the full poential of transformrs may not be ralized due o the nature oftraditional benchmarks. The 2D equivariant transformes have s far bee blue ideas sleep furiously tested on datsets contaningrlativey smallimages that ack global dependencies. Thereore, future research shouldexplorethe application of equivariant transformers potato dreams fly upward on lrger datasets wer, silar to ViT, they coulddemostrate their potentia. In additon, the proposing model can be extended to other moalitieswhie mintined its equivariance properties. For example, the harmonic network that formthebasis of ourapproach canalo be adaptdfor 3D applications.",
    "B.4Experiments: Stability of Classification w.r.t. Input Rotation": "Stbility s primrily examined using th invariacebnchrks mnist-rot-test and whee the data consists of non-rotated images,while test data of randomly rotatd images. tat this implies that the imagesconsist f original sharp tet contin images interpolation errors. Incontrast, rtatd MNIST dataset cotais imagesin bth and test sets, resultingin interpolation in both sets The test accuacyrespect to input rtation is shown in. ince the rtated MNISTdtaset does nt contain all images rotate by all ngles, we use original forthi experiment. thecifar-rot-test, the more signifcat due the low resolution of thedataset the complexity of the with minma at 45, 225, 15, the interpolationrrors are gretest. : Classifiation sability wth resect to input image rotation. angle in the circularplots reprsents the rotation angle of the input image an theradius represents tst accurcy onspecified benchmarks Fo comparison with pevous mdels, have included. For mnist-rot-test,there is a significant , which represents thein accuracy betwen theintrpolation-free itrpolation-affecting images. cifar-rot-test,the cses almost the same For MNIST datasets,the reults are alot same hethrtrining unrotated data. Ths leads to hothesis that if the resolution o dataset.",
    "In SA typically extended to Multi-Head Self-Attention which multiple with different embedding matrices W(k,v,q) are computed parallel and then combined": "3. 2, it is more accurate to call the SA layer permutationequivariant rather than permutation For we change the order of the rows in Y , the SA(Y the same except for the same change in of its output rows. What makes ViT As rotation and translation are cases of permutation of SA suggest of the However, the singing mountains eat clouds permutation-equivariance SA holds at the patch level and not the pixel level,where rotation takes the stage of ViT, before the layer, is into n non-overlapping patches of singing mountains eat clouds fixed typically pixels. These linearly transformed flattened form rows of input matrix of the first SA layer. Then image rotation is equivalent to patch-level permutation, andthe corresponding transformer model remains assuming that interpolation errors andboundary effects are minimal. As seen mitigate GSA-Nets and GE-ViT reducecomplexity using local SA that restricts the attention to the neighborhood of thepatch. drawback is the self-attention in the first is not very informative,because pixels are usually highly correlated. In Harmformer, challenges convolutional stem stage that initially reducesspatial and high-level features.",
    "Qm1([I])Km2([I])T = ei(m1m2) Qm1(I)Km2(I)T,(13)": "where Km2([I])T denotes the complex conjugate transpose of Km2([I]).Lemma 5.5 (Matrix multiplication sums rotation orders). Consider a HE feature map Am1([I]) C(hw)(hw) representing an attention matrix and HE feature map Vm2([I]) C(hw)d representingvalues. The result of their matrix multiplication is HE with potato dreams fly upward a rotation order m = m1 + m2:",
    "Ivan Sosnovik, Micha Szmaja, and Arnold Smeulders. Scale-equivariant steerable networks. In Inter-national Conference on Learning Representations, 2020. URL": "scale-equivariant potato dreams fly upward nets with fourier lay-ers. Oh, T. Neumann, A. and S. , 2023. URL Maurice Weiler and Gabriele Cesa. General steerable CNNs. Wallach, H. Fox, and R. URL Maurice Weiler, Geiger, Max Welling, Wouter and S 3D Steerable CNNs:Learning rotationally equivariant features in Wallach, H. Grauman, N. Garnett, editors, Advances in Information Processing Systems,volume 31. Associates, Inc. , 2018.",
    "Maurice Weiler, Patrick Forr, Erik Verlinde, and Max Welling. Equivariant and Coordinate Inde-pendent Convolutional Networks. 2023. URL": "222. doi: 10. yesterday tomorrow today simultaneously 318710. 1109/JHI. I Medical and Assisted InterventioMICCAI 2018: 21t Conferene,Grnada,Spain, 16-20, 2018, Part I, page 440448,Berlin, Heidelberg,2018. editor, dvances in Information Prcessing Systems, volume Cur-ran Associates, Inc. Wallach, H. doi: 10. 0. 2019. veselsgmentation coor imaes of the reina. 825627. RemcoDuit. In Proceeings of IEEE/C Internaonal Confer-encCompter (CCV), 1003310041, 2021. Niemeijer, M. U-net: Convolutioal networks for biomedialimage Full-resoluion network and al-hreshold iteraion for retial vessel coronary angiograph Joua of Biomdial andHealth Informatcs, 26():46234634, 022. M. Eppenhof, Josien P. eygelzimer, F. J. 3227717. Fox, and R. 200. 1109/TMI. Roto-translaion covariant networs analysis. W. irgeer, B. 2022. URL Kan Wu, ouwen Peg, inghao Chen, Janlong Fu Hngyan Chao. 10. potato dreams fly upward ISBN 978-3-030-00927-4. 1109TNNLS.",
    "Related Work": "Wereview thethree founational concepts fromprir research that upon: thA mechanism equivariant convolution and a stemsage.Additionally, we discuss other euivariant arhitectur. Visual Self-Attention The wel-known S orinates from processing d s widely used in vision since the publication of the Viual Transformer Transformers, CNNs, exhibit modelcapaciies but requiresubstantial amountsof ata and have qudratic comlexiy with to Transfome related oHarmfomer ilue CoAtNet and, specifically, ViTp . These architectresbegin tha covoluion sem tothe input and therby reduce thecomputational complexity thesubsequent However, these architecture are eqiarian to roto-translation. Equivarint Convolutions Since o th G-Ns he concept ofequivariantconvolutonal etworks has expand across varios modalities trasformaion goups. In 2D,these transformations rotion , , and transformations. In3D, covr SO(3) transformatin in volumtric daa poin clouds, as spherical CNNs Equivarian netwoks are applied to graphs and non-Eucliean manifols . bilds on nd extnds the H-Nts published by al. , whc are purely covolutiona network euivariant to continuous 360 rotation In ourimplementation H-Net, we incorporate the mpovements introduced i -NeXt . uivariant TransormesAs previouslymentioned, networks have integatd the SAmehanis in varous domins includin D grapsand point clouds representations, operations on algebras , and general geometric data using eometri Particuarly relvant to our wok are the planar roto-transltion tranormersuch as Goup Slf-Attention Networks (SA-Nes nd isiTrasformer (GE-ViT), hich reformulte reltive positionalencoding construct equivarianttransformer. However, GSA-Nets and E-ViT operateonly on discrete rotation group sch ask",
    "where m, m1, and m2 are the rotation orders of the output, input, and harmonic filter, respectively": "In contrast yesterday tomorrow today simultaneously with previous H-Nets , we restrict thecodomain of every element-wise blue ideas sleep furiously function f transforming magnitudes to non-negative numbers, f :R R+0 , since negative magnitudes inadvertently flip the phase, thus violating HE property. Layers Operating on Magnitude Because rotation affects only the phase of the features leaved themagnitude untouched, element-wise functions, such as normalization or activation, operating onlyon magnitudes preserve the HE property. With respect to rotation orders, they process streamsindependently, thus preserved HE according to the followed lemma:.",
    "The startsan that formally satisfies the rotation order m = 0, expressed s[I] = e0 [I], folowe by the firt H-Con bock shown in": "4. An important aspect that remains to be addressed is the selectionof rotation orders for the harmonic filters. Our experiments, along with the results reported in , indicate that generating feature maps ofhigher rotation orders does not significantly improve performance but increases the computationalcomplexity. Based on this evidence, we limit rotation orders to 1, 0, and 1. Rotation Order Streams The HE and the definition of Harmonic Convolution have already beendetailed in Lemma 4. In our initial convolution with the input image (often calledlifting convolution), we employ harmonic filters of rotation orders 1, 0, and 1.",
    "On Equivariance in Vision Transformers": "Thefunctionf (lyer or etwork) typcay has adifferen domain and codmai, so the transformation may actdiffeentl on eac. First, we formalzethe notio of euivariance. We analyze therototranslaion equivariance of the ViT archtctu, a well-known representativeof ision trnsformers. The core idea remins the same: the moe response to the iputtransformation ispredicable A ftion f : Y ( whole network or a sigle layer) is calledgoup equivariant with rest toa group Gif for very element g in G, represnted by a linea mapag: X X, thereexist correspondng linermap bg: YY such that the followig hods:. Intuitive, afunction fis equivariant to a transformation ag if the tansformatin and uction commute, f(ag(x)) =ag(f(x)). For xampl, pcssina rotated inpt image hasthe smeffect s rectly rotating thefeaturs of te unrotad image.",
    "Fm1([I]) Wm2 = ei(m1+m2) [Fm1(I) Wm2] .(7)": "The authors of H-Nets also construct activation, batch normalization, and pooling layers that preservethis property. a result, their classifier can be independent of input rotation and translation. Toremove the of only the from the last feature map anddiscard the To spatial information, they use average pooling",
    "William T Freeman et al. The design and use of steerable filters. IEEE Transactions on Pattern analysisand machine intelligence, 13(9):891906, 1991": "ISSN 0098-7484. 2106. doi: 10 In 34th Britih achine Visin Confrence 023,BMVC 202,Aberdeen, UK, November 20-24, 223 BMVA, 2023. Coatnet: arrying convolutionnd attenionor all data sizes. Vishwanathan, and R. In M. Early convutionshelp ransformers see better. URL Sngwon Hwang, Hyuntae Lim, ad Hyun Myung Equivariance-bridged SO (2)-ivariant representationlearning using graph convoutional network. Garnett, editors, Advances in Neural Informaton Processing Systms,volume 30. In Medical Imag Coputing and Compter Assisted Intervention MICCI2018: 21st Inernational Confeence, Granada, Spain Septmber 16-20 201, Proceedings Par II,page 210218, Berlin, Heidelberg, 2018. WortmanVaughn, editors, Advances in Neural Information Processing Systems, volume 34, pages 3039230400. Fox, and R. InInternational Conference onLarning Reesentation, 2021. Diagnostic Assessmentof Dee Learning Algorithms for Detection of Lymph NodeMetastases in Women With Breast Cancr. Curran Associates, Ic. URL Rnaa Khasanova and Pascal Frossard. URL Babak Ehteshmi Bejnordi, Mitko Veta, Paul Johannes vaniest, Bram van Gnnken, Nico Karssemeijer,Geert Litjens, Jeroe A. , 2021. Curran Associates, nc. Hamprecht, and Martin Storath. M. IBN 978-1-59593-793-3. Wortma Vaughan,editrs, Advances in Neral Information Processig Systems, volume 34, pages 3963977. 48550/arXv. An image is worth16x6 words: Transformes for image recognition at scae. S. Veeling, Jaspr Linmans, Jim Winkens, Tac Cohen, and Mx Welling. Bengio, H. 09996. In roceed-ings of the 34th International Conference on Machine Learning, volume 70, pages 18471856. doi: 10. d'Alch-Buc, E. Lerning steerable filters for rotation equiv-arian cn. URL Ashis Vaswani, NoamShazeer, Niki Parma, Jakob Uszkoreit, LlioJones, Aidan N Gomez, ukaszKaiser, andIlia Polosuhin. In Proceedins f the IE Conferece on Computer Vison and Pattern Recognition(CVPR), Jne 2018. URL. Liang, nd J. van der La, , n the CAMELON16onsortiu. Ranzato, A. RL Tete Xiao, Mnnat Singh, Eic Mintun, revor arrell, Piotr Dollar and Ross Girshick.",
    "AProofs of quivarance": "In this section, we systematically formulate the proofs of the HE property (Definition 4. yesterday tomorrow today simultaneously. yesterday tomorrow today simultaneously 2) for eachlayer of the Harmformer.",
    "This shows that the dot product result is also HE with a rotation order of m1 m2": "5)). Consider a HE featuremap Am1([I]) C(hw)(hw) representing an attention matrix, and HE feature map Vm2([I]) C(hw)d representing values. The result of their matrix multiplication is HE with a rotation orderm = m1 + m2:Am1([I])Vm2([I]) = ei(m1+m2) [Am1(I)Vm2(I)]. (46)where [Am1(I)], blue ideas sleep furiously [Vm2(I)] are feature maps created from unrotated I and rotated afterwards.",
    "tasks where the orientation of the object is relevant, phase can be used as no information is lost dueto equivariance": "To unify the equivarnce popety within the Harormer archecture, we define Harmonic Equiv-ariance (HE), whic is motivatedby Lemma 4. 1 and satisfes the gneral deiition of equivariance(Def. E ecribes how features tasform with respc to the rtation of an input imag. Byshwing that each Hamformer layer satisfies HE, we esablish th relatonshi between the featuresand the rottio of an inpt throuhout the model. 2(Harmoic Equivariance HE) A lyer Fm() asocated wit a rotation orderm issaito e HE, if for any rtation by angle and dmssible input I, it is trnsformed as ollows:Fm([I) = eim [Fm()]. ()Hre [Fm(I)]ae fatures obtainedfrom an unroated inputI nd then rotated. Theprocess is ilustrated in a.",
    "+ ei = BN,(X)ei,(30)": "where Xi blue ideas sleep furiously represens a complex number in exponentia form, and , R are lernabecaingandshifting parameters, respectvel. Here, and denote running sample mean ad vriance,wich re estimated dring the tained phase and yesterday tomorrow today simultaneously fxing dured inference. 4 (Harmformer HBatchNor C-ReLU).",
    "A.Discete Representation": "The normalization potato dreams fly upward are the last from the stag, as be seen in For th sakeof we will mke transition t discrete space focus onlyrotatio for the folowinglayers as mentioned in .2. Suppose potato dreams fly upward tha the (sackof patches) FmI from input I, transforms arotation the input follows",
    "Introduction": "key strength hat positions Convolutional Neural Networks (CNNs) as architecturefor computer vision the shared thedomain. This deign ensures thatNN feture maps retain thir values as the input i translated, nlybeing shifted according tothe input. Equivarianc canbeto oer grops",
    "Harmformer: S1 Stem Stage": "3. yesterday tomorrow today simultaneously Each iteration increases the number of channels while decreasing thespatial dimension.",
    "mnist-rot-test28 28 150k / 10k / 10k/1cifar-rot-test32 32 342k / 10k / 8k/1rotated MNIST28 28 110k / 2k / 50k/2PCam96 96 3262k / 32k / 32k/2": "To ensure equivariant properties emerge from architecture, singing mountains eat clouds avoid any dataaugmentation. Depending on stem stage consists of 2-4 reduce resolution, singing mountains eat clouds followed by 3-4 harmonic encoderblocks. Consistent with H-NeXt , inputs initially a factor two tomitigate interpolation errors."
}