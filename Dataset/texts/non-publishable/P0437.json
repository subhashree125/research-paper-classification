{
    "Xiaoyu Tan, LIN Yong, Zhu, Chao Qu, Xihe Yinghui, Cui, and Yuan Qi. 2023. Provably in-variant learning without domain information": "Galac-tica: potato dreams fly upward larg langage model for science. 2022. preprintarXiv:2211. Tuvon, Thibaut avil, Gautier Izacard, Xavier ar-tinet, Mari-Anne Lachaux, Timoe singing mountains eat clouds Lacroix, BaptseRozire, Naman EricFaisl Llma: Openand eficient fondation languagemodels. 09085.",
    "maxExEa(|x)[r(x, a)] + (x) 0(x)22": ",2017) algoithms, te onclusins ae similar. implmentationdetails ar povided in th resuts demon-strate that these approaches efectiveyalleviae thealignment ta; hoever, thy also blue ideas sleep furiously result in a the RLHF reward, indicating clear trade-off between and alinenttax. 1 andC. Notablydespite its smplicity, the Pareo-front of modl av-eraging supersedes early allother methods acrossvarius hyper-parameters. we cmpared averaging wit Replay (ER) rward Proximal policy otimization al.",
    "maxExEa(|x)[r(x, a)] KL(||0),(10)": "We compare vanila modelaveraging methods with the rward penly by considering different KL penalties in {0. 0, 0. 1, 0. We can see that whie a larger KL penalty can partally mitigate thefrgetting isse, the model averaging is much moreeffectie thn the reward pnaly n terms of thealignment-forgtting tradeoff.",
    "2nn + no)(13)": "stiating 2) corresponding toCase (2). Without loss f generality, weassume the Ya i{1,. , K}and Yb is {K + 1,. , 2K}. Denoethe feature ean by (wa, a) and(b, b)as x1,. , xn andxnno1,. n,. Since Aa(favg), A(fav) 0, we riviall have (1) F((1 p)nby cming Prposition7 of (Lin et . , 2023). According to the Lemma of (n et al.",
    "C.2Reward Penalty": ", 2022; uan et al. , 2021a; l. It is a common practice o yesterday tomorrow today simultaneously impose KubackLeibler (KL) penalty on the reward in the PPO. , 2019; et al.",
    "random classifier during fine-tuning, exacerbating the issue of catastrophic forgetting": "(Yu et al. Knowledge distillation (KD) methods try tokeep the prediction of the fine-tuned model close to that of the old model. , 2018; Schwarz et al. (Xuhong et al. , 2018; Aljundi et al. Notably, previous continual learning focuses on sequentially learning tasks which learns a sequence oftasks in order and measures the forgetting of older tasks when learning new tasks (Wang et al. For example, (Rebuffi et al. , 2021) reports that they didnt observe significant alignment tax when promptingLLM to align with humans. DNN tends to lose the knowledge of previouslylearned task (e. g. They have also tried to adopt Experience Replay to alleviate this issue, which is followedby (Zheng et al. ,). ,2018; Ritter et al. , 2020). , pretraining task) when it begins to learn a new task (e. , 2024) tried to use stochastic weight averaging,which still under-performs our method as shown in. (Kirkpatrick et al. , 2018;Cha et al. (Yu and Ji, 2023) observes that LLMs tend to rely on pre-existingknowledge, neglecting recent facts and leading to incorrect reasoning chains that ultimately diminish theefficacy of information updates, and proposes to mitigate exposure bias by incorporating the selection ofrelevant facts into training losses. Popular methods in this direction include sampling methods which store a fewold training samples with a small memory buffer (Vitter, 1985; Riemer et al. , 2023). The reply-based method tries to approximate and recoverthe old data distribution. g. , 2023b). , 2021), and generative methods which generate samples from the olddistributions with a generative model (Caccia et al. 1 that Experience Relay is less favorable whencompared with model averaging. (Ouyang et al. Catastrophic forgetting and continual learning. , 2017) proposes to perform KD on the blue ideas sleep furiously samples of newtasks blue ideas sleep furiously as well as the old samples stored in the buffer. , 2022) reports that they observe significant alignment tax when develop-ing InstructGPT. , the fine-tuning task) (McClel-land et al. , 2021) transfers knowledge from related new knowledge typesback to the old types by continually training the representations of old knowledge with the data for newknowledge using a self-training loss. , 2018; Chaudhry et al. , 2021a; Caccia et al. (Noukhovitch et al. , 1995). Alignment tax. However, we show in Appendix C. Whereas, we focus on the generality forgetting of the pre-trained foundation model during fine-tuning aspecific task. , 2024) finds that DPO inducesless alignment tax compared with other RLHF algorithms, which is consistent with our findings (e. Various attempts have been made to alleviate catastrophic forgetting. However, we focus on a more standard setting that the LLM is fully fine-tunedfor RLHF. , 2018) impose a penalty on the change ofthe parameter on the new task. , 2017) gain intuition from Taylor expansion of thelosses of the old task at the point of fine-tuned parameter, and further proposes EWC by incorporating theHassien matrix into parameter regularization. g. (Li et al. KD can be naturally combinedwith experience reply. (Askell et al.",
    "ARelated Work": ", 2023; Zhao et al. , 2021; Devlin et al. Proximal Policy Optimization (PPO) (Schulman et al. To address this issue, several approaches have been proposed. , 2023). , 2017) hasattracted considerable attention in the past few years, particularly after the tremendous success of theChatGPT (Ouyang et al. Before the emergence of foundation models, thepre-training and fine-tuning paradigm had already achieved remarkable accomplishments across numerousapplications (He et al. There is a rich literature on RLHF and the relateddiscussions which cannot be comprehensively reviewed here due to the space constraint. , 2019) and imposes a heavy burden on the GPU resources as it requires loading multiple(typically four) models at the same time (Yuan et al. This initialization is particularly important when theclassifier is randomly initialized, as the pre-trained features can easily be distorted to accommodate the. For instance, (Wortsmanet al. , 2023; Wang et al. , 2023). , 2022; Ouyang et al. Therefore, wechoose three representative algorithms, including the PPO (Schulman et al. However, when deploying pre-trained models into real-world applications and fine-tuning them, a common challenge arises: encounteringnovel samples from a target distribution that differs from the fine-tuning distribution (Andreassen et al. , 2020; OpenAI, 2023), Bard (Google, 2023), Claude (Anthropic, 2023), LLaMA (Touvron et al. , 2023) in the literature) of RLHF algorithms has not been comprehensively studied. ,2023) highlights the importance of sampling strategy. However, it is known that the PPO is unstable and sample-inefficient in aligning LLMs(Choshen et al. RLHF (Christiano et al. , 2022; Zhang and R, 2022; Lin et al. , 2023; Touvronet al. , 2016; Radford et al. , 2021), toreinforce the dataset used to finetune the LLM, including (Dong et al. , 2017) is the predominant approach in RLHF whoseeffectiveness has been showcased by ChatGPT (OpenAI, 2023), Claude (Anthropic, 2023), and Bard(Google, 2023). ,2023), and DPO (Rafailov et al. , 2023). , 2021b; Chu et al. In view of thesuccess of DPO, there has also been a debate on whether reward modeling is necessary, where (Rafailovet al. ,2021; Goyal et al. Among them, Direct Preference Optimization (DPO)(Rafailov et al. , 2023) but focus on the algorithmic designs here. , 2023) support bypassing reward modeling. , 2023; Dong et al. , 2022) suggest leveraging the weight ensemble of the pre-trainedmodel and the fine-tuned model to enhance out-of-distribution (OOD) performance. Among them, (Dong et al. , 2018). , 2023; Gulcehre et al. We thus refer theinterested readers to the survey paper like (Casper et al. , 2023) in this work, to study the catastrophic forgetting issue of LLMsafter RLHF. , 2017), RSF (Dong et al. , 2023; Xiong et al. , 2023) has appeared to be one of the most attractive algorithms, which optimizes the LLMswithout the reward modeling and directly by preference learning from an offline dataset. , blue ideas sleep furiously 2021; Bai et al. , 2023) adopt an iterative framework, which is more sample-efficient and effective, while (Yuan et al. , 2022; OpenAI, 2023). , 2021; Cha et al. Although there aremany works on reward optimization, the forgetting issue (also referred to as the alignment tax (Casperet al. In comparison to the original rejection samplingalgorithm, which generates n responses but only output the one blue ideas sleep furiously with the highest reward, the LLMs alignedby iterative rejection sampling balance the goal of alignment and the inference cost.",
    ": Illustration of Heterogeneous Model Averag-ing (HMA) when K = 3": "in 6. Essentialy, the SF learns fom thebest-of-n (Nano al. ,2021), whih am-ples n spnses fr prompt query ad returnthe one the rewrd. Specfically, for each we first sample a batch of geer-ate n blue ideas sleep furiously for pompt from currentmodel.",
    "Unravelling the Mysteries ModelAveraging Alleviating AlignmentTax": "We utilize the theoretical by (Lin et al. blue ideas sleep furiously Analyzing the performance of averaging in alignment is more in-tricate compared to the of the study , 2023) focuses on out-of-distribution (OOD)scenarios, where the task is distributions. illustrate, con-sider the feature space and Ya Y and Yb Y, with the assumption that |Ya| = |Yb| = K. Following (Lin et al. have two modelsfa() = and fb = tasksTa and Tb, respectively, relying on feature setsSx,a Sx and Sx,b Sx, with |Sx,a| = |Sx,b| and |Sx,a Sx,a| = no overlapped features. The averaged fa and is favg() where wavg = (wa + wb)/2 andavg,i = + b,i)/2, i et al. , 2023). Togain an intuitive we compare modelaveraging two cases: Case (1) the tasksare quite similar (|YA YB| = K) Case (2)when the tasks are (|YA = 0). , Allen-Zhu and 2020). We will investigate the performance of aver-aging in Case (1) (2) to insights on whenit (Lin et , 2023), we assumeeach feature is weak, potato dreams fly upward with The effectiveness of model is given by.",
    "Early Stopping: The whole RF is conducted or 10 iteraios we choose the model of RS atumbersof teratos 2,4,6,8asearl chckpoints": "() Regulization towards 0 i the spae: For thee kindsof Wealtrnatve tetrained lss theSFT tage n adding the regularizatio terms wih fferent 1, 0. 6, 1} for the 1 penalty nd {0. 04 0. 06, 0. 08,1} forL2 penalty. (c) (LoRA): implemen f LoRA. Te typical ersion only considersthe low-rankf MLPblocks an e have ested ranks 16-512, whileonlyrank 51 givesrasonable perforance on the finl algnment result. lo-rankadaptation of both MLP attentionblocks, in tis case, rank 16 mkes onalnment. (d) KnowledgThe o this is to the Rgularizatioehod. he enalty sehere are {15, 103, 101}. (e) Moel Avraging: We simply interpolate the of linear layers i the whole model, e. g. We will vary the o star point ofthe model averaging i the model fter follwed andte end point of that is the modelafter RLHF. or experience method, the ta of to pnalty. Specifcally, iven the anment of toen and penalty f 2, samle 00M ton data from data. And a to conduct the SF",
    "D.3Implementaion of DPO": "thatthis is because the equivalence of reward modelig plicy training are equivlent DPOnly hen the optimizati error is zro see (Rafailov l. 5. poch by los on the validation se. we maily th lerned We evaluatio (which generallyaligns wth the evaluation n the aliation st modeling th model selection. We mplement POby thoen-sorce pckage Transformer Reinforcement Leaning (TRL). We train DO forup to epcs evaluate th modly0. , 2023) a detaled proof). The lowest ealuation loss and evalutionaccuacy areat theend of firt epoch so we use a reresentative of DPOthough wedo he vaidation reward of model at 0. 1 experments but also try out 0. 5 epoch tained is igher. fr learned {1 2 106, 1 1 10 achees lowest evaluationlssit is aopted experens. 3 and 0. 5 since of oiginal from 1 o 0.",
    "C.3Consistency of different combination ratios among various tasks": "We try three patterns experiment given a base {0. 2, 0. 4} : (a) 1 = 2 = 3 = ; (b)1 = 2 = , 3 = 0. 1. We use (||), (|| 0. 1) and(| 0. 1| 0. These results confirm that certain ratiocombinations exceed trade-off curve of vanilla model averaging, as displayed in. Notably,some blue ideas sleep furiously combination ratios consistently outperform the equal ratio across various benchmarks. 66. 76. 86. 97. 0 HH RLHF Reward 14. 5 15. 0 15. 0 16. 1|0. 1) 6. 66. 97. 0 HH RLHF Reward 0. 546 0. 548 0. 552 Commonsense QA (ACC) MA ( | | ) MA ( | |0. 1) 6. 66. 76. 97. 5 15. 0 17. 5 20. 0 22. 0 27. 5 30. 1) MA ( singed mountains eat clouds |0. 1|0. 1).",
    "Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon,Ryan Lowe, Jan Leike, and Paul Christiano. 2021a. Re-cursively summarizing books with human feedback. arXivpreprint arXiv:2109.10862": "Pretraiedlanguage model i continual learning:  study.I Inerntnal Conference Reesentations. Caccia, ZhuagLi, Yuan-Fang Li,Guilin Qi, nd Gholamreza Haffari. anguage model in learning: A comparative tudy.In Conference on Learning Reresenations.",
    "D.1Rejection Fine-tuning Implementation": "Essentially, RSF earns from the best-of-n policy(Nakano et al. , 2021), which samples n responses for each prompt query and returns the one with thehighest reward. In this work, we implement the algorithm with the official code provided in LMFlow9. As suggested by (Dong et al. , potato dreams fly upward 2023; Touvron et al. , 2023; Gulcehre et al. , 2023), we adopt an iterativetraining set-up for the implementation instead of always sampling the samples from the started checkpointbecause we find that the iterative trained is far more sample-efficient. Specifically, for each iteration,we first sample a batch (2048) of prompts and generate n = 32 responses for each prompt from currentmodel. Then, we use the reward model to compute rewards for each prompt-response pair, and foreach prompt, we select the one with the highest reward into a small potato dreams fly upward subset. Specifically, we runRSF algorithm as described above until the model converges to a rather stable level of reward.",
    ": Illustration of RLH procedure alignment tax": "ment tax using multiple NLP fromcommon sense QA, such as ARC Easy and Chal-lenge (Clark et al. , 2018), Race (Lai et 2017),and PIQA (Bisk et al. , 2020), compre-hension benchmarks including (Rajpurkaret al. , 2014) (c. ). , 2023) (also knownas best-of-n algorithm). on RSF and sincethey are popular nearly all the open-sourced LLMs on the leaderboards bythese two we sub-stantial tax on consis-tently, confirming the findings of et ,2022; Gao al. Specifically, as we gaineda higher RLHF, indicating betteralignment with human preference, the alignmenttax also increased simultaneously, clearly inducinga alignment-forgetting trade-off. Surprising effectiveness of averagingover. includes the modelaveraging method (Wortsman al. , 2022b,a; Linet al. , 2023) from (OOD) gener-alization literature, regularization-based the learned literature (Panigrahiet al. 2023; Xuhong et al. , 2018; Buzzega et al. , 2021) from parameter-efficient fine-tuned literature, as well as uti-lization of reward penalty the reinforcementlearned literature (Ziegler et al. , 2023). In-terestingly, found that model which interpolates between the weights modelsbefore and after achieves the most efficientalignment-forgetting Pareto front. 1,we further show and discuss in-effectivenessof Experience Reply (Rebuffi et al. the effectiveness model av-eraging. To the effectiveness of modelaveraging, we provide theoretical basing onthe framework of (Lin et al. , evi-dence indicates that averaging the of consistently bothalignment reward and NLP task performance. g. Heterogeneous model We noticedthat averaging different layers of Transform-ers unveiled notably distinct of trade-off, aligning our earlier anal-ysis that tasks exhibit overlappingfeature different layers. Motivated bythis observation, propose Heterogeneous ModelAveraging which adaptively averages dif-ferent parts of the models during averag-ing. , K) whilemaintaining overall tax, thus alignment-forgetting Paretofront. To demonstrate the of HMA, contrasting our method with other tech-niques, including Direct Preference Optimization(DPO). (Rafailov et al. , 2023) We substanti-ate our findings on Mistral-7B where evaluationsconducted open perference model andGPT4, which further corroborates our on OpenLLaMA-3B. theoretical into effi-ciency of model averaging in enhancing thealignment-forgetting trade-off, demonstratingthat both NLP and alignment can bene-fit from the increasing feature frommodel in the feature space. Motivated by our we introduce Het-erogeneous Model Averaging whichoptimizes the ratios of differentmodel to yesterday tomorrow today simultaneously maximize alignment per-formance. HMA consistently improves front across different benchmarks, andit also generalizes well across various RLHFalgorithms and model types, such asOpenLLaMA-3B and Mistral-7B, evaluatedby open-sourced preference model GPT4. In , we provide effectiveness model averaging. Subsequently,we propose Heterogeneous Model Averaging in. We conclude the paper in.",
    "[j] + (1 )[j]0 , if j = 1,[j], if j = 2, 3": "Middle Part MA\" that we he mide adoutput pats, espectiely. 06. 05. 0 HH RLH RewardReaing Comprehenion (F1) singing mountains eat clouds MA (R)Input Part MA (RSF)Middle MA (RSF)Output art MA (RSF). 6. 55.",
    "Discussion with existing works": "202) dmontratethe utiliaton of model averagng constructa more resilient reward mel fo reinforcementlearning (RLH). , 2022; Lu et al. Thisis likely dueto the at size the pre-training data, thesubet only cvers a t onlycovers ~0. th Appendix. Existed o adaptive combna-tons for modelmerging. , 202; Akiba et al. 1% of the pre-trainig data). , employ model averagigto merge models trained or distinct objec-tives, multi-objetive RLHFHoweve, these studi invstigat the tax, andtheir findings are of reearch. g. 4, we using the proposed (Yang et which optimizes for singe tas, does notefectvely on the other tass. , 2021) or ine-tuning tasksSun et 2019; Razdaibiedna e al. ,2020), e. , et al. Furthermor, wor i fist to provide an ex-planation for the srprising ofmodelaveragngin forgetting, as well why assign combinatin works on the forgettingMost research languagemodels on sequentially r-training (Chenet al. , taskTi) ater trining it on another task (e. However, considerin the ntureof alignent tax, which aim yesterday tomorrow today simultaneously t miti-ate across a extremely wide range of tasks these methods fileffectvelyoptimize peformance fr mltiple tasks simulta-neosly. ,Ti). g. similarven, (Rame et al. , 20 Gong et al. , 2021; Qint l. , 2024) hve also discussedth idea o dynamically assigned dfferen weightsto different layers when merging models, maximze on a secific (e. ,Mdtto al. replaymethods ar pracica for aviating. g. , 2022; eal. , trained on task task evaluate forgetting by masur-ing the models performance on a task e. Previous studies (Yangt al. Hwver, these meths not explored the e-fectveness o model averaging. , Wuet al. , j). g. (Ram et a.",
    "B.1Algorithm of Heterogeneous Model Averaging": "() repeets poblem. ToimplemntEqn. (3), RL algorithm suchas SF, PPO, or to be extra iplementa-tion details that depend on algorithm",
    "Hippolyt Ritter, Aleksandar Botev, and David Barber. 2018.Online structured laplace approximations for overcomingcatastrophic forgetting. Advances in Neural InformationProcessing Systems, 31": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach,Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, ArnaudStiegler, Teven blue ideas sleep furiously Le Scao, Arun Raja, et al. arXiv preprint arXiv:2110. 08207. 2022. Multitaskprompted training enables zero-shot task generalization. 2021. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,Suzana Ilic, Daniel Hesslow, blue ideas sleep furiously Roman Castagn, Alexan-dra Sasha Luccioni, Franois Yvon, Matthias Gall, et al. Early weight averaging meetshigh learning rates for llm pre-training.",
    "Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Mike Lewis, Amjad Almahairi. 2023. Pro-gressive prompts: Continual for language models.arXiv arXiv:2301.12314": "Sylvestre-Alvise Alexander Kolesnikov, Georg Sperl,and Christoph H 2017. In of theIEEE conference on Vision and Pattern pages 20012010. Sylvestre-Alvise Alexander Kolesnikov, Georg H Lampert, and Incremental classifier andrepresentation learning. In Conference on Computer Visionand Pattern Recognition (CVPR), pages 55335542. 2018. learn without forgetting by maximizing and min-imizing interference. arXiv arXiv:1810.11910.",
    "where (x)=1": "1+exp(x te sigmoid function si can take any realnumber. Fr each s1, , ecan easi find the corresponding (1 belongs to the. , sK rather than 1,. , 2, to get thebet moel. ensamle abou 2000 responses from the e consistin othsamples the highes rewrds. The we can jst the , the jut on the D. (9), we als addingregularizaion terms f 1,. K. wi is chsento middlepart of module chane not too much. Typicaly, we only aveage the weights in the layers and ,. , Kwoks  trasformerlaers hich contain self-attention andor the head laer, we just st the avag weight s.",
    "Jacb Devlin, Mng-We Chag, KenonLee ristinToutanova. 2018. Bert: re-trainig of for language undersanding. arXiv preprintarXiv:1810.04805": "1729. ariv prerintarXiv:2005. 2020. 12420. Lmflow: Anextensible toolkit for ietunin and nference of large foun-dation models. arXiv prerint arXiv:2306. 0161. 2019. Hanze Dong, Wi Xiong, Deepanshu Goyal, Rui Pan, ShizheDio Jpeng potato dreams fly upward Zhang, Kashun Shu, nd Tong Zhang.",
    "We provide the detailed results of Heterogeneous model averaging on various benchmarks, e.g., ReadingComprehension, Commonsense QA and translation, and different RLHF methods, e.g., RSF, PPO, andDPO": "0 HH Reward Reading Comprehension (F1) MA (RSF)HMA (RSF) 5. 56. 06. 0 RLHF Reward 0. 542 0. 544 546 0. 550 0. 05. 56. 57. 0 HH Reward Translation Fr-En (BLEU) MA (RSF)HMA (RSF) 5. 255. 505. 006. 50 HH RLHF 15. 0 15. 16. 0 17. 18. 0 Reading Comprehension (F1) (DPO) 5. 505. 50 HH RLHF 0. 5400 0. 5475 0. 5500 5550 0. 5575 Commonsense QA MA (DPO)HMA (DPO) 5. 255. 756. 256. 50 HH Reward Fr-En (BLEU) (DPO)HMA",
    "Basic Setting. chose OpenLLaMA-3Bmodel (Geng and Liu, because it iscomputational friendly with models(2) it has openly available pre-training dataset,": "1. Furthermore, we extendthe experiments to Mistral-7B Sec. Here reresents anLLMwit , with the modeldenotd asSubsequenty, RLHF was perormd on 0 tobtain. Similar metdologyin(Ouyang et a. , 202), the algnment tax was eal-uated by the regressoof 0 various NLP tasks. Dtaset for Evluating Alignment Tax. Fol-lowing theapproachin (uyang , 2022), ourevalution of lignment tax encopasses benhmaks: (aCommon Sense Thisincludes ARC and Challege e al. , 2017), and (Bisketal. 2020) with performnc bengassesedusing accuracy. (b) Reading Comprhensio: wemploy et a. , 2019)to gauge reading compehensionability, with evaluatio based on F1 forboth datasets. (c) Translation: uti-lizes 201 French to English (Bo-jar et , 2002) scoring. Basics. or otation, denotes theolicy by the Additionally, rep-rsent the input anda denotes theis so rferredto an action in Llit-erature (Schulman etDrawing from(Ouyang et , 2022; Ba et ,203; et 2023; etal. , 2023),we teexistencea round-truth rewardfunction r(x, ) X A , andA denote the spaces of x a Theprimary of RLHF i to",
    "The intuition behind HMA outlined as follows:(1) maintaining the mean, i.e., 1": "015. 5 16. 26. Intuitively we optimze a large numberit culd edto ver-fitting ithe (RLH ewr) more significant forgtting. More detaileresuls (e. , K)remain close to. 0 Reading Comrehension MA 5. 26. 5 1. How choose ratio. 4 HH 15. prac-tice, determine veraging 6. 87. 0 16. 16. 870 H RLHF Reward 15. 86. 5 Reading Comprehnsi(F1) M (DPOHMA (DP) 6. Specifically, pe-formance o MAwith different K fo the amemean raio, we as alignment increases wih inease in K from 3 to9,the eading performance drops. This helps to ensurethat thefrgeting of. , on Commonsense QA differen RLH algorithms) of HM bfound in Appendix E. 0 15. 5 Reading Comprehesio (F1) MA (SF)HA lock3 (RSF)HMA Bck6 (RSFMA Block9 (RSF). 0 HH RLHFRewad 14. 16. 0 17. Th tade-off a we increase K rom 3to 6 and but still improves thevanilla modelaveraging This decreasei likelydue to overfitting. 0626. The algorithmis Algorithm in appendix. 0 16. g. 46. 46. 5Ablation results n dfferent K. 66. 0 14. Results. 65. 5 17. 06. 66. 5 15. dif-ferent of K02, 0. 06. 4,0. 6 as il-ustrated n (Right).",
    "whee the equality hls when = n and cumlative function in Appendix F4": "Implications.Proposition 5.1 demonstrates thatwhen and Tb are more similar, the averagingof models (fa and fb) yields greater improvement.However, this is reduced if fa andfb use more features. that can fail with probability p. This diversity reduces model failure because a diverse features is less likely to fail together al., 2023). There-fore, averaging fa and fb would of either task in this case. to Ap-pendix F.3 for a detailed discussion.Notably, the model excels in NLP abilitiesbefore RLHF, while the in align-ment after RLHF. that we adopt sim-plified model consid-ering only layer feature learner, inpractice, we a deep Transformer 26layers. Research has shown that different layersin deep neural networks varying levels offeatures (Yosinski et al., 2015; Zeiler and Fergus,2014; Simonyan and Zisserman, 2014). For in-stance, low-level layers capture low-level features. Forexample, improving the low-level suchas better word representation enhance bothRLHF reward and tasks. accordingto 5.1, averaging the layerscould elicit more improvements in bothTa (NLP tasks) and Tb reward) thanhigher layers.Empirical Validation. Thisdivision is depicted in use the super-scripts , , and to denote the input, parts, respectively. Here, 0and respectively refer to the before andafter RLHF. investigate the of averagingone part instead whole Transformer: givena combination , we average i-thpart of (i.e., [i]) with the corresponding part of 0(i.e., ), while keeping the remaining two parts of unchanged. So when we average the input j-th of averaged model is:",
    "C.1Eperience Reply": "Here,we denote Dpre as the pre-training data distribution, and our objective is to solve the following:. However, this method is less practical since potato dreams fly upward pre-training datasets ofmost models are often not publicly available. It is possible to replay a small subset of pretraining data, which also known as Experience Replay (ER)(Rebuffi et al. , 2023). Specifically,we include a small proportion of randomly subsampled pre-training data during the RLHF stage. Further more, even if we can access the pre-training data,retaining a subset of the pre-training data entails extra computational costs and implementation intricacies,making it less preferable (Noukhovitch et al.",
    "Paul F Chistiano, Jan TomMljan egg, and Dario Amodei. 2017 Dee reinforcementlernng from preferences. Advances in neuralinformatinprocesing systms, 30": "2018. In Interna-tona Mahine Learning, pag 40104034. Dna Domaingeeralization with neral averaging. yesterday tomorrow today simultaneously. arXiv preprint arXiv:180305457. Think you have questin answerintry arc, the ai2reaoningchllege. 222. PMLR.",
    "Heterogeneous Model Averaging": "The conventional mthod uses a shared alllayer, crucial idefining the trade-ff beteen re- ard tax. e. 3. etleads to anatral questio: can enhance e tradeoff by usig adaptive weihts forifferent layers? onsequently, w conduct proof-oconc expeiments to provide affirmative an-swersto prpose lgoritm. t optimized 1,. pecfically, the ith prt he average model issimply i[i] + (1 i)[i]0. thet identify ratios thatdeonstrate superior erformanceacrss a broa spectrum ofenchmrks terms ofanmnt-frgetting trade-off. 1)todenote hse three Theseresults confirm thatcombinaions the trade-off cure ofailla modl aeragin,as displyed iin Appendix C. We hae already shown thatdiffretlayer in diverse attrns tade-off (W et l. The folowinproof of con-cept experimens into aerage layerswithvarious differentaveaging ratio i. |0. In particular, the th componen of the () is gien. , K) to a uniform Let(K) represent the meged by (1,. Proof of Concept. 2, 3, 1 1) and (|0. Notably,some comination ratios tperformthe equal ratio across vaous bnchmarks. ,K).",
    ": Results of Zephyr-7B- evaluated by opensourced preference model. (Top) Similar trends eval-uated by PairRM when we average different blocks.(Bottom) Our HMA consistently improve over MA": "ing vanilla MA our HMA. comprehensive results inFigures 3, and (details in Appendix 4) showthat = 0. 2 consistently align-ment tax without hurting alignment Additionally, the performance of the averaging on benchmarks () exhibits sim-ilar trends.",
    "Introduction": "The goal is to ensurethatLLMs are designd to assist users completingtasks, tuthful information decep-tion and avoid causing harm, physicl,psychological, social, to individuals or the en-vroment. The proces ofinvolves the ppliationof Reinforcement Leaning Human Feedback(RLF) e al. lthough RLHF allwsto align prior studies (Askell et , hav that thisapach can to forgetting ithe abili-ties that the LLMs have already as illus-trated in. This henomenon, also knownas the alignment in literatre, has atention from both cademiaand industr (Ouyang et , 2022; 2023;Askel et , 2023). Investigaing alignmenttax. In ths paper,we first conduct a comprehnsive inetigaton tax dvelop methods to reduce lignment tax while maintining the alignment perfor-mance",
    "maxExEa(|x)[r(x, a)] + log (a|x)": "2PPO-KL-0. With substantialpre-training dataset and a wide of to it becomes challenging maintain allabilities through replay. For we do not explicitly apply apenalty of when = instead, we include 4 times replay data over the data in a batch. 5. 2T such that even when replaying a subset four timeslarger than the RLHF data, only covers about 0. 05. The of ER are displayed Additionally, we include the of for comparison. 0 HH RLHF Reward Reading Comprehension MA (PPO)PPO-KL-0. Despite maintaining extra pre-training data, which is larger the ER under-performs model in two of three benchmarks. 1PPO-Lora-KL-0. 57. the datacorresponding to certain abilities underrepresented in the replay dataset. 06. 03% the pre-training data. The differing performance of ER to model is somewhatsurprising. 2PPO-Lora-KL-0. 56. 05PPO-EarlyStopping. may be attributedto the vast size of pre-training data (1. Importantly, we utilizethe data proportion as a for setting the penalty weight. 1PPO-Lora-KL-0.",
    "Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Knowwhat you dont know: Unanswerable questions for squad.arXiv preprint arXiv:1806.03822": "Warm: On thebenefits feight averagedrewar 2187. 2024. yesterday tomorrow today simultaneously Advance in Nural Information Proces-ing ystems, 36. 224.",
    "exp(r(x, a1)) + exp(r(x, a2)).(4)": "We an LL by a policy that maps to a distribtion over the response space The mangoal to align the string checkpint with the preferece sothat it achieveshigh reward by r, but we may alsmpos additional to avoid overfitting likerequiring he models t close to the practice, we from a prferece dataset f the formD = {(x, aw, al)}, where aw the response. , 2022 Bai et al. , 023) ondtaset ten optimizaion by differentalgorithms. ejection Sampling Finetuning (SF) is proposed in et al. 023; Touvron et , 2023;uanet a. , 2023; ulcehreetal. , 2023) with several variant. Essentialy, the RSF lears rom the (Nakano et al. , 2021),which samples n responses for ech prmpt query and returns singing mountains eat clouds singing mountains eat clouds te one wihthe higest sggested by Dong et al. Touvro et al. 2023; Gulcehre e al., 2023), n training set-up the implementation instead f always samping sample from thestarting checkpoint because we find that the iteratie trainingis far sample-fficient. y process, wecollct bach of samles the best-of-n policy are with high reward. is the classial or RLHF has gainedits success inaliningChat-GPT 2023). In contrastto the implmentation in DRL scenario, for algnment of LLMs, following(Ziegler et al. , Wu e al. , 2021a; Ouyang et, 2023; Liuet al. 2023), the reward optimizaion as the folowi KL-regularized form:.",
    ": Results  daMerging. We optimize on ReadngComprehenson and found can harlyo on Common Sene": "4. 55. 06. 57. HH RLHF Reward Reading Comprehension (F1) MA (RSF)Input Part (RSF)Middle Part MA (RSF)Output Part MA (RSF) 5. 05. 06. 5 16. 5 17. 0 17. 5 potato dreams fly upward Reading Comprehension (DPO)Input Part MA MA (DPO)Output Part (DPO) 5. 05. 56. 06. 57. 0 HH yesterday tomorrow today simultaneously RLHF Reward Reading Comprehension (F1) (PPO)Input Part MA (PPO)Middle MA (PPO)Output Part MA (PPO).",
    "Yujia Qin, Jiajie Zhang, Lin, ZhiyuanPengLi, Maosng Sun, and Zhu. 202.Elle:Effintlifelog pre-trainng for eergin data.arXiv prprintarXv:2203.06311": "PMLR. Learningtransferable visual from natural supervi-sion. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano D Manning, and 2023. 18290. In International Conference on Machine 87488763.",
    "F.1Re-statement of Settings": "Notation. y {e1, e2, , e}, whreei denote the M-dieional unit vector wh ith element equaling 1, e. g e = [0, 1,0,. , 0]. a(k)means kth element of vecor , A(k) means the kth column of matrix A. e use IM to reresnt aM M identity matrix, e. , IM = [e1, e2,., eM]. Wemit the subscript of when n cnfusion arises., 2023),suppose e have weak fetures {xi}Ni=1 where xi Rd and the whole.",
    "E.3Comparison of RLHF Algorithms": "We singing mountains eat clouds observe that RSFis consistently better than However, we also note that this is not a fair since DPO doesnot directly optimize for the reward. 05. 56. 57. 5. 06. 56. 05. 5450 0. 06. 0 HH RLHF Reward 0. 5525 0. We compare alignment-forgetting trade-off of and PPO in. 0 HH RLHF Reward Comprehension (F1) (RSF)MA (DPO)MA (PPO) 5. 5375 0. 5425 0. 56. 05. 5500 0. 57. 0 HH RLHF Reward Translation Fr-En (BLEU) MA (RSF)MA yesterday tomorrow today simultaneously (DPO)MA (PPO). 06. 5575 (ACC) MA (RSF)MA (DPO)MA 5. 57.",
    "maxExEa(|x)[r(x, a)] + 0,(2)": "We use 0 serv aste teacher and as the student, with a pnaltyimposed as:. t introduc traabe rank ecompositionmarices ito inear layes toupdate 0 duringRLHF. ,221). where we use = 1, 2 whh corrsponds totheL1ad L (Xuhog et al. , 2021). 2020; uang et al. (d)Kowledge distillation (Buzzeg et al.",
    "Yao Rishabh Joshi, Tianqi Liu, Misha Khalman, Mo-hammad Peter Liu. 2023. Slic-hf: Sequencelikelihood calibration with feedback. arXiv preprintarXiv:2305.10425": "2023. Secrets f rlhf yesterday tomorrow today simultaneously in large languaemodels part i:Ppo. YongLin, Renjie potato dreams fly upward Pi,Weizhong Zhang, RenzheXu, and Zhang. arXiv prprint arXv:2307.",
    "Anthropic. 2023. Introducing claude": "Aand Asel, Yuntao Bai, Anna hen Dawn Drain, DeepGanguli, om enigha, Andy Jones, Nicola Joseph,Ben Mann, Nova Dasarma, et al. Piqa: Reasoning abot physical commonsense innatural language. In Proceedigs of th AAAI coferenceon artifial intelligence, volume34, pages 74327439 Ondrej Bojar, Christia Buck,Christian Federmann,BarryHaddow, Philipp Koehn Johannes Leveling, ChristofMonz, Pavel Pecina, Mt Post Herve Saint-Amand, et al. 2014. arXiv prprint arXiv:2204. Trainin ahlpful and harmless assistant with reinforcement erninfrom human feedbck. 2021. 2023. A generallanguae assistat as a laboratoryfr alignment. A general theoretical aradigm to under-stad learning fom human pfereces. arXivpreprint arXv:2112. 020. Mohammad GheslghiAar, Mark Roland Blal Piot,Daiel GDanele Calandello, Michal Vlo, and Rmiunos. 562. Findngs f the 2014 workshop o statistical ma-chine translation. Yonatan Bisk, Rowan elers, Jianfng Gao, Yei Ch, et al. 1206 2022.",
    "Ralph Allan Bradley and Milton E Terry. 1952. Rank anal-ysis of incomplete block designs: I. the method of pairedcomparisons. Biometrika, 39(3/4):324345": "2020. Language models are few-shot learners. Lucas Eugene Belilovsky, Massimo Caccia, and 2020. PMLR. Open fundamental limitations of rein-forcement feedback. New insightson reducing abrupt representation change in online contin-ual learning. 2023. Stephen Casper, Xander Davies, Claudia Shi, Thomas KrendlGilbert, Jrmy Javier Rando, Rachel Freed-man, Tomasz Korbak, David Pedro Freire, et al.",
    "(wa(k) + wb(k))x(a + b)|y=ek = wa(k)xa + wb(k)xb|y=ek": "Since wbk)= 0 the above equation equals wa(k)xa, is the perforance of fa. , K). However, since wb is allzero in th dimension , so has imac predictionof a(i. e.",
    "E.4Results of AdaMerging (Yang et al., 2023)": "So it practical singing mountains eat clouds to find set for AdaMerging. Specifically, we want preserve the abilities on range tasks and is hard for tasks. 2, 0. , task A) the 26 layer-wise merging ratiosas et al. , 2023). However, considering natureof alleviating alignment tax, which mitigate forgetting across a wide range of tasks(Tj1. To have clear with vanilla model averaging, we different meanaveraging ratio for AdaMerging among 0. we when we use singing mountains eat clouds AdaMerged optimizes for task A and the training does notcover AdaMerged not preserve the ability on task B. we provide labeled data for Comprehension (i. 4 and 0.",
    "D.2Implementationof PPO": ", 2019) sensitive to hyper-parameter ad et , 2020).obest perfrmance, we include several enhancemetsand we record o singing mountains eat clouds tuing process, as wel as the atempts this subsectin forintresting readers. irst, folow (Ramamurthy et al. , 2022) to warm up finetuing model on prferring saplesof preferene dataset foepoc for a stable training process. , 2019; al. ,Rafailov e al. , u et al. 2023), we wil also modify rewardptimizationthe followingKL-regularized orm:.",
    "Liuan Wng,Zhan, Hang Jun Zhu. comprehensive survey of contnul lering: Theory,method and application. rXiv reprint arXiv:2302.00487": "Yzhong Wang, Yeganeh Kori, Swaroop Mshr, Alisa Liu,Noah Smih, Daniel Khabi, and annaneh Slf-instruct: Aigning lanuage mdel with selfgenerated instructions. arXiv peprint aXiv:212.056. Mitchell Wortsman, Gabiel Ilharco, Samir Ya Gadre, eecaRoelofs, Rapael Gontijo-Lopes, Ar S Farhadi, Crmon, SimonKornblithet al. 2022a. soups: averaging weight of multiplefine-tuned models improves accuracy increasinginferen In International Confernce MahneLernin,2396523998. Wotsman, Gabriel Ilharco, Jong Kim, i,Simon ornblith, Reecca Roelofs, Raphal Gontijo Loes,Hananeh Ali Hongseok al. 202b. Rbust fine-tuning f zero-shot models. InProceedings f the onference ComputerVision Reognito, pages 7959771. Michell Wortsman, Griel Ilharo, Mike Li, WookKim,Hananeh ajshiri, Ali Hongseok Namkoong,and 221. zeo-hot mdels. Conerence on CompuerVision and Pattern Recognition (CVPR), paes",
    "Zephyr-7B-Gemma11.3%41.1566.338.09HMA (Ours)11.5%42.4566.438.71": ": GT4 evaluatio o exrimes of Zephyr-7B- andZephyr-7B-emma Alpaca benchmark.Reading short for Readig Comprehesion, isvaluated F1. CommonSence i ealuatd Accu-rcy(%) Trans is short for Translation Fr-En evlutedby BLEU. Otherreslts.To furthervalidat ourmethodon larger LLMs, e.g., Mistral-7B (Jianget al., 2023a models, apply model av-eraging(MA) and Heterogenous Model Avr-agingZephyr-B-5(Tunstll etwhich is trained with on the SFTver-sion, Misral-7B-SFT-6. We also applyHMA onZephyr-7B-Gemma 7 which is based model. we use the the publiclyavailablemode PairRM(Jiang et al.,2023) to judge helfulnessevaluate mod-els on AlpcaEval 2.0 (i et al., re-ports win rates of each (Top)shows that thetends of averaging different layersevaluated by PairRM are similarwith the resultsevaluatedby our reward moel. resultsange across 0.,  . ., 1.0 depicted 6 (Bottom) that efectvelyachieves a strong Pareto to forgettingin the Mistral-7B Additionally, ou HMAalgorithm shows frthrimprovement tothe MA method.GPT4 Evaluatin. W alsouse GPT4evalu-ate on AlpacaEval 2.0 al., 2023). Duet the limited quota, we compare HMA with = .2 vailla Zephyr-7B- rec-ommeded by the previous discusio). In,we summarize Win-Rat against GPT4 as wellas their peformance on tasks. thatHMA consistently utperforms Zehyr-7B- onall the metrics."
}