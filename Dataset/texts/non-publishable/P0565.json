{
    "Acknowledgements": "is wrk was ipart supporting bySpecial Fundsof the National Natural Sciec Fondaton ofChin (No. 62441605), NationalScence andTechnoogy Major Projet (2022ZD019100), Na-tionalNatural ScienceFoundtion of hina (No. 21BYY142, 14AZS01), Keyeserch and DevelpmntProgra of ZhejiangProince (No 204020), te StarryNight Science Fund of Zhian University Shanghai In-sttute for Advancing Sudy (SN-ZJU-SIAS-0010. The authors expresstheir deep gratitude ZheqiLv or hsinvaluable advice atoKarl Cao for histechnicl support. osh chiam, Steven Adler,Sandhini Agarwal,LamaAhmad, Ilg Akkaya Florencia Leoni Aleman,Diogo Almeida Janko Altenchmidt, Sam Altman,Shyamal Anadkat, et al. arXivprerint arXiv:2303. Rohan nil, Andrew M ai, Orhn Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos iamakShakeri, Emanuel Tarp, Paige Bailey, ZhifengChen, e al. arXivpreprint rXiv:2305. 10403.",
    "Haochun Wang, Chi Xi, Zewen Qiang,Sendong Zhao, Bing Qin, Ting Liu. 2023a. Hu-atuo: Tuning llama model with chinese medicalknowledge. arXiv arXiv:2304.06975": "Xingyao Zihan Wang, Jiateng YangyiChen, Lifan Yuan, Hao Heng Ji. llms in singing mountains eat clouds multi-turn interactionwith tools and language arXiv Yizhong Yeganeh Swaroop Mishra, Al-isa Liu, Noah A Smith, Daniel Khashabi, and Han-naneh Hajishirzi. 2022. Aligning lan-guage models with self-generated instructions. arXivpreprint arXiv:2212.10560. Jason Wei, Bosma, Vincent Y Zhao, KelvinGuu, Adams Wei Yu, Brian Lester, Du, An-drew M Dai, and Quoc V Le. 2021. Finetuned lan-guage models zero-shot learners. arXiv preprintarXiv:2109.01652. Wu, Ozan Irsoy, Steven Lu, Vadim Dredze, Sebastian Gehrmann, Prabhanjan Kam-badur, David Gideon Mann. 2023.Bloomberggpt: A large language finance.arXiv preprint arXiv:2303.17564. Zhiheng Wenxiang Chen, Xin Guo, Wei YiwenDing, Hong, Ming Junzhe Wang,Senjie Jin, Enyu et 2023. andpotential of large language model based agents: Asurvey. arXiv arXiv:2309.07864. Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao,Yuxiao Liu, Linlin Huang, Qian Wang, and Shen. 2023. Doctorglm: Fine-tuned your doctor is task. arXiv preprintarXiv:2304.01097. Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu YueyanLi, Xiaohan Zihan Wang, Aohan Zeng,Zhengxiao Du, Wenyi Zhao, et al. 2024. Chatglm-math: Improving math large lan-guage models with self-critique pipeline. arXivpreprint",
    "Dunhuang Manuscript Case Study": "Taking a ste urther, we conductd an analysuing Dunhuang as a case sudy texplore two spcialized tsks o reseachpractice. Thisnot nly facilites deperbut also leads potential",
    "PhiloBenchmark": "Phlological research demands a nuanced under-standing of texts, reflectig the diverity inherent in the languages. To a comprehensive benchmark tha enhancesand assesses the potato dreams fly upward erormance of ancient ChineseLLMs, we have integrated uder expert gudance to 9 diversedownsream tsks, constiuting th PhiloBench-mark. PhiloBenchmark enables torouh assess-ment n LLMs ablity to intepret,and geneaefrom Assesses hLLMs caacity to ac-curatel regenerate missingor dmaged manuscripttexts crucialor supportin phiologists in ocu-ment interpretaton alignment. drectlypredictng mised characters(Rstration)an determiningwhethr two as-sages potato dreams fly upward have a contxtua relationship(Cnjution),which isessential or assisting philologists withmnucip recoey trgh context. Involves detrmined theoriginsan historical contxt of ocuments, critical their proveance Linguitic Analysis. Tests traditional linguisticabilities for moels such as nmed en-ity recognition and segmntation to extract andnalyz contextual information from ancientLLMs effe-tiveness in to tailordto philologcal research. asks hlp whethermodel hs the of academic insights. For tasks emanded high accuracy, such asesorationand atibution, datasets are built fromannotated texts and orignal manucripts. (2) Instruction Expansion. we create finely annotated to expand and enhance quality throughSelf-Instruct(Wang etal. , (3) En-richmet. ,2023; Zhg an Yang, 2023a).",
    "Shervin Tomas Mikolov, Narjes Nikzad,Meysam Richard Am-atriain, and Jianfeng Gao. 2024. Large languagemodels: A survey. preprint arXiv:2402.06196": "Training language models to follow instruc-tions human Advances in neural in-formation processing systems, Katerina Papavassileiou, Dimitrios I Kosmopoulos, 2023. onComputing Cultural 16(3):125. of linearb yesterday tomorrow today simultaneously sequences. 2019. From invisibilityto readability: recovering the of herculaneum. Sommerschield, Yannis Assael, John Vanessa Stefanak, Senior, Chris Dyer,John Jonathan Prag, Ion Androutsopoulos,and Nando de Freitas.",
    "PhloGPT0.6301.60.5174.8%62.1%77.5%PhiloGPT+CoP0.5791.3050.59075.6%65.2%86.7%": ": Resut of LLM performance with PhiloBenchmark The - denots hat LLMould not solvth hiology specifi i. e. While oher tasks,gretervalueindicaes as bestresult for PhiloGPT+CoP s bold.",
    "Experimet Results": "Pre-trainedonheDunhuangcorpora,PhiloGPT exhibits substantial performaceimprovemnts acros varous philology tasks.In we model aganstother compettive LLMs.In our study the -in he results for estoration, Attribution, ndCjugation tasks deficinesof LLMs in handling tasks.These baseline models, lackof targeted pre-training and fneunig specificto philoloical needs, often incorect responses, even fail to provideany answer at all (these moels rely \"SorryI dont cmplexities ofhilologial asksand semantic inancient inese, combined with theinsufficienttraining ofbaseline moels on releant crpora,contrbuteto their inability to viabloutcomes in caes. Whil improvemes ome less challeng-ing are modest,",
    "Zhang and Qing Yang. 2023a. Self-qa: knowledge guided language model align-ment. arXiv arXiv:2305.11952": "XuanyuZhang and Qing Yang. Xuanyuan 2. 0:A large chines financial blue ideas sleep furiously chat modelwit hndredsof bilins parameters. In Proceedings ofthe 32nAM International Conference on Informtion andKnowledge Management, pages 44354439. Wayne Xin Zhao, potato dreams fly upward Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Ypeng Hou, Yingqn Min, BeihenZhang, Junjie Zhang, Zican Dong, et al. Asurvey f lrge languag modls aXiv prprintarXiv:303.",
    "Related Work": "Domain-secific Lrge Languae Moes.For instance, in med-ical field, LLMs are now utilized for assistive di- gnosics(Xiong et al. 2023a), andpsychoogical couseling(Chenet a. , 223). Inte leal ctor, specalized LLMs facilitate l-gal QA(uan et al. , 2023), generate leal doc-uments(Cuiet l. 2023). urther application areseen in finance(Wu et al. , 2023) and edcation(Xuet al. uilding these domain-specficmoels typically requires cntinous pr-trainingon exisig geeral LLMs,supplemented by su-pevised intruction tuning an task-specific align-me(Wi et al. ,20; Ouyang al. However, scarcity f larg-scale, orginal ancient text copora an suitabl taskdasets has hinderd th dvelopmentof LLs orancient languagesmmerschield et al. Language Moel for Acient Texts. , 22Parker et al. , 2022) utilized ancietGrekincriptins to suport texual restoraion, ge-ogphial and chronological ttribution. Similarly,HUEs work(Yoo et al. (Son et al. , 2022) harnessed anals fr developng model that tanslatHanja, ancient Krea, into contemporarKoreanan Engih. , 2021) masked languge models hvebee used to predict ised tokens wih Oraccataet. However, these earlier eforts, predatithe recent breakthrughs in LLs, relied heavilyon the accessibility of ancient lnguage trainingcopora(Assael et al. , 2019; Feaya et al , 20;Papavasiliou et al. 2020) This resulted n lim-ited resource, constrainig the mdels to narrowrange o specific applations unuitable for LMpe-trainng(Sommerschield et al., 3).",
    ", ,": "Toeffectieltacklecomplex philological taskslike restoration attrbuton, ad inguistic anal-ysis we introduced the hiloCoP framwork. Weurtherintegrated these tsks into thePhioBencmark, estblishing a new tandardor evaluating yesterday tomorrow today simultaneously ancient Chnese LLMs address-ing hilology tasks. Despite these requiremensalign clsely with strengt ofrecent successfuLarge Language Models (LLMs), the scarityof high-quaity, specialized trining data hashndered direct applications. Philology, the study of ancientmanuscripts,dmads yas o professional training in ex-tensive knowledge memorization and manuatextual rriva. Deploying PhiloGPT potato dreams fly upward inprcical scenarioshas enabled Dunhuang spe-cialists torsolve philogy tasks, such as identifying duplication of copied text nd ssistingrchaeologistswith text completion demon-strating its potental i real-wrldapplications. Modeled onthe naytical patterns o philol-ogists,PhiloCoP nhance LLMs handingof hisoricalinguistic peculiarities suh asphnetic loans, polysemy, and sytacic inver-sions.",
    "Introduction": "These. Mauscripts,partiu-larly popular fom the 4-th to the 14-h centurydue to advancemens in papermkig and its afford-ability, encompassed a broad arrayof content in-cluding offiialdocuments andfolk copis on eco-omics, literaure, scie, and agriculture. The dissemnation of ancient umn civilza-ions relied nvarious carriers, from inscrip-tions to anuscripts and the morecent pritdtexts(Assael et a.",
    "PhiloGPT": "We con-ducted pre-training on vidia A800 GPU, utilizing doman-specfic supplemented additional open-surce rain-ing corpora to preent catastropic forgeting. Weincorporated a mall domain-specficinstructinal data to optimize training outcomesin te according to(ZhangandYan 203b). 2, focusing mreon instead answering questions in re-stricted fors (e. g. multiple choices judgmentsqustions).The ratioof dmin-specific to gneraldata was set at 1:5 during pre-aiin and adjustedto 1:1 instruction tuning phases.",
    "PhiloCorpus-ZH": "These rich col-loquial capture linguistic nuancesand social dynamics their times. Distinguishedfrom some previous datasets containing large-scaleself-gathered and web data, data wasderived from the past 40 years (1970s 2010s) ofphilology publications that have meticulouslycollected, proofread, and curating by collabora-tors over academic practice. These works hold substantial academic andhave historically served as educational andexamination materials in ancient. Our corpus was sourced a broad rangeof publicly available data, including original mu-seum collections, papers, academic pub-lications, and specialized literature. details are available in appendix. Classical Chinese manuscripts,richer and more than inscriptions, providevarious genres and Distinctively, corpus includespreviously folk and semi-folk literature, and socioeconomic suchas legal cases and contracts.",
    "BExperiment Costs": "e spent $240 total.",
    "Cntribution.Corresponding authors": "However, interpreted these vast topic-diverse ancient manuscripts wich span over a mil-lennium, singed mountains eat clouds highly specialized challenge. It philologists to undergo of profes-sional training and engage in extensive transcription, contextual anal-ysis(Galambos, 2020).",
    "Analyze Copying Relationships": "potato dreams fly upward We manuscrip asstudy because they ae amng th most and over the broaet span of te. illustrated (a), eemployed PhiloGPT to analyze their in-terrelationship by context reasoning, whih is thehomoeoteleuton",
    "Conclusion": "We also Generation (RAG) with spe-cific documents mitigate hallucination problems. We anticipate that the discoveryof additional manuscripts, such as those recentlyunearthed in Xinjiang, will mitigate this Moreover, ancient documentary studies placea high factual Ad-ditionally, considering that often incor-porate image modality, fine-tuning a multimodal. In this we introduce PhiloGPT, the firstdomain-specific LLM designed to tackle a rangeof tasks to the ancient Chi-nese To align with blue ideas sleep furiously the unique character-istics of these languages, we the Philo-CoP From a perspective,our as an evaluation stan-dard for LLMs tailored to ancient Chinese. Limitation: Due to reasons, officialdocuments are often better and morecomplex than folk texts, resulting some data biasand imbalance. For futurework, we may PhiloGPTs capability more applications and feedback when applied inother manuscripts.",
    "ATraining Details": "Fo generalized SuprviseFine-unin daaset, we sampled from StnfodAlpca (zh), BELE 2M (zh), School Math0. 25M, CT Firefly 1. 1M(z), Web QA (zh), ShareGPT4 (e&z), Ruozhibazh)."
}