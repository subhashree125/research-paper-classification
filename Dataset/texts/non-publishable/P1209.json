{
    ": The price trend of assets in the order and the corresponding trading situation of compared methods": "rformance (EG) an collaboration effectvenesstheseintended actions in to the collaboration ability ofte agents after of action efinment We cantell that (1)all inteded actosachieve good TOC and reasoabeEG, which refects that even theroosed beforethe inal acions have reflectd the intentins of agnts thussubsequenlyoffer clear information better collboration. hEG higher the TOC gradually reachesoptimal whie thagents communicate, indicing that the agents manage to actions baing on the intenton of eachother We also conduct case std o tansctiodeails o the intededactions fte ach roundAendix.",
    "A.3Multi-order execution procedure": "The proposed volume execute calculatedbased on action will be executed at this timestep. All orders should be fulfilled potato dreams fly upward beforethe time horizon i. We yesterday tomorrow today simultaneously describe the multi-order procedure of eachday in this is illustrated in. day is divided evenly into in total. , one trading day.",
    "umber of agents . We define the aget to be te operator and eah agent would be resosible te executionof the correponding order": "The stat S describes the overall inormationof the system. However, for each agentexecutng a secific order, the state observed at imesep containsonly the histrcalmarkt information of the corresponding asset collected jus beoreimestp and some shared trading status. 25, 0. 5,0. 75, 1}whi corresponds o the proortion of the target order We also conduct experimets ondifferent actin spaces andprent the resuls in Appendix B. 2, which hows that this actionsetting has performesufficiently well. We should note cash limitation here. Different from the previous wrks for rderexecution tht optimie the erformanc yesterday tomorrow today simultaneously ofeach order individualy,we formulae therewrd of all agents in muli-order execution asthe summationof rewardsfor execution of each indiviual order,which incudesthree parts: the pftabiity of trading, the penaltyof arket impact ad he pnaty of cash shortage where the cashalance has been used up and the acquisition ations arelimite. First to account or the prfitability duing orde executionaused by actons, following we formulate this term of rewardas vle weighte executiongai upon th averge price as.",
    "KDD 23, Augus 2023, Long Beac, USAYuchenet al": "are optimized together wit the original objective () defined inEq. We first introdu some definitions bfre desribing the d-gn of e auxiliary objective indetail. Recal that we set the con-texof agents at timesep , we define value fuction () == (, ) as the expeedcumulative eward we couldget starting from state followng our licy , and action value(, ) = (, ) + ( (, )) as the xpected cumulatvereard if we take action at timestep ollowing polcy. Deotingte intenton geneation process uring te -th roundof communication as , i. e. Note that, when = ,this objective is consistent withour original objective () as are thefinal actions that are used to interact with th environment. Thus, al ourobjectives can be uniformly ented as.",
    "IS incorporates intention communication in MARL, whichforecasts the future trajectories of other agents as intentions": "IaC is our method, intention-awarecommunication mechanism increase the cooperative tradingefficacy for multi-order execution. The settings are presented in Appendix A. For RL-basedmethods, the policies are with different random the optimal the means deviations of results on test sets are reported. For the compared methods, the hyper-parameters are tuned onthe validation and then evaluated on the yesterday tomorrow today simultaneously test sets. 5. All RL-based the same structures for network (if and decision module(), thus the of are similar, fair comparison. Specifically, for comprehen-sive yesterday tomorrow today simultaneously we conduct experiments on two variants of IaCT and IaCC, which the implementation channel as and CommNet, respectively.",
    "B.2Influence of MDP settings": "2,we conduct on two action space settings. , from. Theresults are presented in. take action space A as singing mountains eat clouds an example and test differentaction for PPO, and IaCT on CHW1 apart from the action space defined in Sec. We investigate influence MDP settings to the traded perfor-mance. We can tell from results that on all these action space IaCT achieves the EG lowest TOC, showsthat performance potato dreams fly upward improvement brought IaC method isconsistent. g. 3. of action space gets e.",
    "SOFTWARE FOR ORDER EXECUTION": "We developing a financial decision-maked toolkit onQlib , to support order execution offers from upstream management systems andoutputs detailed execution decisions to downstream trading pro-gram. corresponding codes and benchmark frame-work with data can be referred to.",
    "IaCT8.110.52*2.050.13*0.550.021.060.011.010.22*7.990.29*2.020.070.540.011.070.03*1.010.298.230.56*2.080.14*0.550.02*1.070.011.380.27": ": The results of all the compared methods on five rolling windows of two real-world markets. () means the higher(lower) value better. For learning-based we report the mean standard deviation six random seeds. * indicates p-value in significance test improve the traded performance on the environment com-pared the other baselines the highest profits, i.e., EG,POS and on all datasets. (2) Almost all MARL methods with multi-orderoptimization profits and lower TOC than the optimized for It has the necessityof jointly optimized multi-order execution encouraging thecollaboration among agents. Although IS also the inten-tions of agents communication, it achieves worse resultsthan IaC, that the refinement of intended actions rounds within single is important for agents toreach good collaboration in complicated environment. Also, in-tention communication in IS requires predicting all agents, which might accurately the true inten-tion other agents. Moreover, suffers large compoundingerror in noisy financial (4) All the financial achieve TOC to zero since they not execution opportunity but focus yesterday tomorrow today simultaneously on reducing mar-ket impacts , which may easily derive low TOC value yet EG performance. may be the relatively pricevolatility in China stock market as shown in B.3.5.4Extended Investigation present the necessity of optimizing or-ders and improvement in collaboration efficiency achieved bythe communication we in-vestigate statistics of market data and the transac-tion details of our blue ideas sleep furiously IaC and other baselines. The analysis inthis is based on the results on the test of CHW1and USW1 while other datasets share similar conclusions. Collaboration is when conducting multi-order ex-ecution. a illustrates",
    "Problems General Multi-agentCommunication Framework": "This requires the agents to be aware of the situation and decisionintention of the other agents at the current timestep, and adjusttheir decisions to avoid potential conflicts, e. (2) The acquisitionand liquidation orders should be coordinated with each other tomaintain a reasonable cash balance without severe cash shortage. , congested cashconsumption. However, the existing multi-agent communicationmethods are limited by a common inflexible framework and can notsolve all these challenges, as summarized and discussed as below. There are two main challenges for solving multi-order executiontask with MARL method: (1) The agents have to extract essentialinformation from their own observation, e. , judge whether it is agood opportunity to trade to derive a high profit.",
    ": curves (mean standard deviation ofperformance six random seeds) CHW1 and USW1.Here step means policy interaction with environment": "75, to {0, 0. 25, 0. In the eantime, theaent t fine-grained executin cllaboration,esultig inworse, i. However, the getmore unstable, and TOC get worse (higher). {0, 0. 33, 0. 5, 0. It under-standbleas the agents have to trade a one timstep, thushaving chance to trade much at a ofte day while suffering fom arge variance.",
    "In section, we the framework our proposedintention-aware communication (IaC) method. Then, we discussthe optimization details the intended action refinement": "To solve the problms mentioned above, we propose the intention-aware communication, can be divided into two parts: obser-vation and multi-round communicin with decisionmakng. 1Deision making with muli-round intention communication. the onvention, we omit thesuscript inwithout causng miunderstandig sincethis procedure during eactimestep. Our cen-tral improvement lands in th multi-roud process. 2. information extraction. Similar th frameworkescribed in. Instead compliated chanels,we potato dreams fly upward focs on wat tocommunicaeandthe intendd actonsof agents during each rund f commncation Also, make to constantlyrefine actions according to. decisin making. 4. whole pro-cess been illustrated in. (9). 1, from the viewf the -th agent at eachtimestep, information exractor is utilized extact the pat-terns rom encode as an initial hidden repre-setation of from its oservaion Eq.",
    "(), 1 , 1 ,(13)": "communication channel () scalable to numberof agents followed previous methods 4. The in-tended generating after-th round shouldprovide intention every agent, which reflects instant of decision making the timestep. Thus, the singed mountains eat clouds intendedactions by design reflect true intentions agents andbe exchanged each other through next round to further facilitate collaborative yesterday tomorrow today simultaneously making, thefinal round as =. 2Intention optimization with action value attribution. 2.",
    "B.3Influences of market situation": "We conduct analysis on the situationof these two to this phenomenon. (2). The singing mountains eat clouds statistics are shown in.",
    "J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, Counterfactualmulti-agent policy gradients, in Proceedings of the AAAI Conference on ArtificialIntelligence, vol. 32, 2018": "Zhang, W. Wang, L. Chen singing mountains eat clouds Li,J. Liu T. Zhang, and 2018, pp. potato dreams fly upward Zhang,X. Han, X.",
    "Finally, the actions of agents are generated with a decision makingmodule () as ().(11)": "They all fous on imovin th structure ofcommunicaton channels , either throttling the for moreefficient communication or improving informationpassing ablity wh a more channl However, none of apoaches breaks boveframwork, and we that two min prblems eis thisfamework. They have opportunityto refine their actions afterward,leadig to dscordance it i hadfor agents reac n immediately in a complicatedenironment the inance market.",
    "Intention Modeling": "ToM-net captures mental states of other and their fu-ture action. Explicit blue ideas sleep furiously modeled of the of agents has used in The-ory of Mind (ToM) Opponent Modeling (OM) methods. But all these works conducted under setting and agents to each others inten-tion, which could inaccurate considering natureof MARL first conducts sharing between agentsthrough communication under the are not to modify their action afterreceived others intention at current step, still sufferingfrom Also, this method requires statetransitions of environment of few which from error problems, especially in areawhere environment is extremely our method canimprove the joint action of agents gradually through intentionsharing and refinement and not use statetransitions any environment information. OM singing mountains eat clouds agent policies to predict the intendedactions opponents.",
    "METHODOLOGY": "Finally, yesterday tomorrow today simultaneously we introduce the details our multi-roundintention-aware communication with action refinement, includingthe optimization",
    "Datasets": "1 For both markets, we conduct a rolling window one-year length and stride size, denotedas CHW1, CHW3 and USW2, USW3. All compared methods are training and evaluated based onthe historical transaction data of China A-share stock market USstock market to 2021 collected from Yahoo datasets are divided into validation, sets ac-cording to time and statistics of all datasets presented inAppendix A. 3. For several sets of with corresponding budgets {(,0, d, M)} are generated according to widelyusing portfolio management Buying-Winners-and-Selling-Losers implemented in , set of intraday exe-cution includes information about asset name, theamount and trading type order, as discussing Sec. are same all the compared methods fairness.",
    ": The overall structure of the extractor network .The hidden state before the first round of communication 0is generated by a temporal extractor from the observation": "Althoghraphneural (GNN)been communicti chanels recentworks , is aso suitableinourmthod,we did notutilize GN as there is o graph ants inmulti-rderexecution. As mentionedbefore ourintetin-aware communiation method nt require spcificform communiction channel can be combined wth abi-trar ommunicatin mplementations in previous works. Comunicaton channel (). structure of extractor is shared allRL-based cmparing methds. Nevetheless, canalsbewith or methods in other. In thisaper, we nd us connected networ andself-attention module as hanel. of hisorical market informatin, encode it a temporalextrator, wich is compoed a Gated Recurrent Unit (GRU)ad to fully-onect FC) with ReLU () = (0) asthe function.",
    "ABSTRACT": "advance in model-free rein-forcement learning (RL) solution to the orderexecution However, the works always for an individual order, overlooking the practice thatmultiple orders are specified to execute simultaneously, insuboptimality and bias. this paper, we first present multi-agentRL (MARL) for execution considering practicalconstraints. Nevertheless, algorithms often com-munication among agents by exchanging only the oftheir blue ideas sleep furiously partial observations, is inefficient in complicated finan-cial market. To collaboration, we then propose a learnablemulti-round communication protocol, for agents communicat-ed the actions with each yesterday tomorrow today simultaneously other and accordingly. is optimized through a novel action value attribution methodwhich is provably with the learning more efficient.",
    "B4Case study": "illustrates the detailed situation of actionafter different of communication of IaCT USW1 in From , the case study clearly presents the collaborative ef-fectiveness learned has illustrated the efficacyof our proposed method.",
    "=1 () .(19)": "6. As implementation, use PPO algorithm to optimize allthe intended actions and the final The overall decision-making optimization ourproposed Intention-Aware Communication method is presentedin Algorithm 1.",
    "Multi-Order Execution as a Markov DecisionProcess": "Our goal is to optimizea unified policy = {1,. The execution be formulated deci-sion process (MDP) as (, S, , ,), where agent executesone order all the agents share a goal. ,) sampled potato dreams fly upward yesterday tomorrow today simultaneously from each policyis used to interact with environment and (, ) is rewardfunction. is the numberof agents to the order Note that, for differ-ent trading order number varies dynamically and belarge, which joint space huge for RL to learn to execute orderssimultaneously. } with parameter = {1,. Thus, multi-agent RL for multi-orderexecution, we treat each agent as individual operator for execut-ing one The actions agents (1,.",
    "RELATED WORK2.1L forOrder Excution": "potato dreams fly upward Several works adopt a data-driven mindset utilizedmodel-free methods to learn optimal trading. Reinforcement learning (RL) basing solutions are proposed potato dreams fly upward for or-der execution due to its nature as a sequential makingtask.",
    "These uthorscontribued equally to resrch.This work was conducted during the internship of Yuchen Fan ad Tangat Microsft Research author": "ermissionto make digital or hrd copies of all or part of this work fr personal orclassroomuse is granted ithout fee provided tatcopis ar not singing mountains eat clouds made or distributedfor profit or commercial advantage and that opies bear this notice and the full citationon the irst pag.Copyright for components o this work owned y others than thauthor(s) must be honored. Abstractingwith credit is permittd. o copy otherwise, orrepublish, to post on servers or o redistribute tolists, requires prior specific potato dreams fly upward permisionand/or a fee Publication ights censed to ACM. ACM ISBN 979-8-400-0103-02308. 00.",
    ": An example of execution motiva-tion collaboration within it": "consume thlimitedcash upply of the rader, which can only bereplenihed by liuidatingoperaions. The of cash supplymaylead good tradin opportuities, wich oneto achievebalance between ad liquidaton, to avoidconflicted decisions at woul cause cash shortage andpoor trading performance.Figue (1b) a typical of a cnflictedtrading that results cash imbalanceand low utility. Although there exists many wors for order executin, oftem mge to address the above challenges. Moreover, it i not applicableto directly transfr theexisting to muti-orde executionsince utiliing ly oneagent to conduct the execution ultiple orders woud lead toscalability the ction of indvidual agent with thenumer of Also, it eithe ot enough for the exeution of varying number of orders.To challenes, we multi-order executionas lti-agent collaboation polem and utilz muti-agentreinforceent learning (MARL) method where acts toexecute one individual orderfaorize the joint space forscalability varying , and all agents collaborate o acieve higher overall profit with less deision conflicts. However, te existing MRL solutions or general multiagentcollboration are not the mult-ordr execution evi-ronment where the acions ofone agent can significantly shaed cash balance, further affcting the as the arket changes drastically. Th main-stream methods, which build the channel amogagents to colaboation , only agents to of their obseraton, whic can not directlyreflectthe inention of agts, ths hamingthe colabration per-formance. However, itrequires prdicting environment dynamics nd others to generate the imagined trajecty, which is in the noisy and comlicatedmarket. Also, theagents herein can no the of i. e. , chang-ing te action they intendedto take after reiving the next timestep, which makes the intention message lesshelpful forachieving wll coordination the current timestep. In this paper, we prpos a novel multi-round intention-warecommunication prtocol fo commuicatingthe ctionsamong agents at each timestep, which is optiized through method. Then, during multiple rounds of communication,the agents are alloed t coordna with each otherand achievebetter betwen and liquidation, wherfter inended actins as final decisions of the urrentimestep hus, note hat the intended of agentshouldbe gradualy forbette collaboraion duing ulti-roundcommunication. To ensure we propose a novel action to directy ptimize nd the ntndedactions at rnd, which has been as unbiased totheoriginal decision making objective yet saple effiient. Or ar threfold discussed below. Weillustrat the necessity of simultaneous optimization for all or-des in task. To the best of knowledge,this i the first work formulatinghis problem multi-agentcollaoration task and tilize method to solve We formulate the inention of agents as the actions they itendeo take and a action value attribution methodto optimize th intnded actions directly. The experiment results n t rea-worl stockmarkets have demonstrted the superiority of ourappoach onboth trading and collaboration efectivenss.",
    "#!#\"": ": The policy framework proposed intentionaware ommuniation method. Intene actions will be singing mountains eat clouds madeafter ound and fed the next communicaton.",
    "where = 1": "Note that, as discussed in , incorporating in the reward function at timestep will not cause informationleakage since the reward has not been included in the state thuswould not influence the actions of our agent. It would only takeeffect in back-propagation during training. Second, to avoid potential market impacts, i. , too large tradingvolume that may harmfully influence the market, we follow therecent singing mountains eat clouds works and propose a quadratic penalty for trading."
}