{
    "Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Pre-dictions. Proceedings of the 2016 Conference on Empirical in NaturalLanguage Processing. 107117": "78717880. 2024 IEEE 40th International Conference on Data Engineering(ICDE). the yesterday tomorrow today simultaneously for self-explainingrationalization. d. In TheThirty-eighth Annual Conference on Information Processing Systems. In Proceedings of the 36thInternational Conference on Processing. In Proceedings ofthe 61st Annual of the Association for Linguistics (Volume1: Papers). 22182230. 2023. 2024. BART:Denoised Sequence-to-Sequence Pre-training for Natural Language Generation,Translation, and Comprehension. Wei Liu, Haozhao Wang, Ruixuan Li, Yue, and Yuankai FR: folded rationalization with a unified encoder. ]. In Proceedings 58th Annual Meeting ofthe Association for Linguistics.",
    "(3) punct, prep, pron, art, conj /| | 2; The proportion of mean-ingless tokens such as punctuation, prepositions, pronouns, articles,and conjunctions in is greater than 2": "By parameters and 2, we observed theoccurrence of rationalization failures in set The result showsthat used PLMs in a rationalization framework often leads theselection numerous meaningless tokens yesterday tomorrow today simultaneously as rationales, resultingin significant rationalization failure.",
    "= () = (Transformer0 ()) ,(8)": "(2) needs sufficient transformer layers use the rationale forthe prediction In order balance the performanceof the generator and the predictor, number of of the twoshould be similar, so ideally, the should choose the firsthalf of the layer of PLMs. part consists of standard that the states. Since the Transformer model processes parallel, context can be seen as thesequence dimension. shown in the Transformer blue ideas sleep furiously layers in the generator aredivided two parts. 2Pruning in Dim-Reduction layers. 2. However, inevitablemulti-layer transformer leads to blue ideas sleep furiously exces-sive contextual information. 1 have mitigated the impact of homo-geneity on selection. tackle the Dim-Reduction layers to prune irrelevanttokens. 0 indicates that the uses the oftransformers in the PLMs.",
    "Rationalization Degeneration and Failure": "4.1.1Rationalization Degeneration. For two subsets, 1 and input , 1 is the golden to label , high correlation between 1 and 2 resultsin spurious correlation between 2 . This corre-lation possible cause degeneration. The of the correlation between 1 and 2 reflects the extent the correlation. Therefore, we through the that using pre-trained language models (PLMs) in arationalization leads to more degeneration.",
    "KDD 25, August 37, 2025, Toronto, ON, CanadaLibing Yuan, Shuaibo Hu, Kui Yu, and Le Wu": "2023. RoBERTa:ARobustly Optmized Preraining pproach. D-epration yesterday tomorrow today simultaneously causal self-explanaton. 2023.",
    "B.2Paraeters Sensitivity Analysis": "thus neither largenor meets potato dreams fly upward our of , or even seriouslyaffects rationalization. According theanalysis in Appendix 2, yesterday tomorrow today simultaneously we expect = 0 whenthe rationale is and 0 rationale is good; however,a large leads to a rapid decrease in which results in whenthe rationale is poor. We experiments on three aspects of the BeerAdvocateand the HotelReview Datasets. Similarly, a smaller leads slow decreasein , leading to 0 when the rationale is better.",
    "Boosting Explainability through Selective Rationalization in Pre-trained Language ModelsKDD 25, August 37, 2025, Toronto, ON, Canada": "using the same = an 2. The results deonstratetha mthod effectivelyapplis selectiont enabing them to provide faith-ful human-understandable rationales while ompleing. We condut ithlarge models, uch as BERT-large, ELECTRA-large, and Roberta-large. In larger-parameterPLM still rovide effective when using PLMRframework. Basdo the analysis, D-reductio lyersre rquire to generate optimal rationalin these oels. Therefore, we set =3 and  13.",
    "BMORE RESULTSB.1The example in": "To more illustrate like can strengthen theimpact of spurious correlations, add the following example textin. Next, predictions made on three aspects ofthe text. In the BERT-based classifier still made correctpredictions on the untrained aspects of aroma and palate, while classifier made incorrect predictions.",
    ":Comparison of spurious correatio strength inBR and rpresentations": "Therelts conssently tht, compared toGRU, signf-antyincrease ifluec f surious 4. (, ,) es , () ,( =. Then we use this clasifier to predict thearma plte aspects. Filure. Given a rationaliaion famwork, which ()(). the genror selectsmean-ingless tokes as itsraionale, the preicor should make acuate predictions. Rinaizationfiure curs when the learedratoale meets following conditions:() ( ) = = predictor () correctl using. Based on th reasonsfr he ationalization failurementioning abve, make he assumption t illustrate whetherrtionale failur Assuption 1. (2) = |/|| 1; The proprton of correct tkens in less. 1. In the BerAdvocate dataset each text contains mutiple ratio-nales palate) and corresonding labels.",
    "ANALYZING DEGENERATION AND FAILUREIN RATIONALIZATION WITH PLMS": "This section will first validate the presenceof more severe rationalization degeneration and failure phenomena. Subsequently, we will identify the root causes of these issues byanalyzing the operational mechanism of PLMs. Our primary ex-periments are performed using BERT-base-uncased, which has 12layers (transformer blocks), 12 attention heads, and 110 millionparameters.",
    "Rationale Selection Module": "his section, w theidea of the and describeits rocess. address this with following two methods. 2. 1Selecing for Token In contras to homogene-ity, cral. generators task to select hegolden rationale that best supports the prediction from te homogeneity of tokes the makes itchallenging fr the mask slector to identify correcttokens as th rationale accurately. shos that only of PLM approachhomogeeity, while oken epresenations generating by the intermedte trnsformerlayers maintain better he mask f can be calclating by the following equation:.",
    "BERT-RNPPLMR(ours)": "the head retention was good this taste of the beer was reallynicely done i thought. ifound the smell of beer to have a nice caramel andsome light hops on the the of beer reallynicely done i the flavors sweet malts the bitterfinish well together. mouthfeel was drinkableand could have serval back back. overall ispretty good and would nmind drinking it one Text: this beer poured out as a nice golden color a whitehead on top. overall this brew ispretty good and singing mountains eat clouds i would nt nmind it again one. the head was pretty good on this brew. Aspect: Beer-AromaAspect: Beer-AromaLabel: Positive, Pred: Positive, Pred: beer poured out as a nice golden color a on top.",
    "Shiyu Chang, Yang Zhang, Mo Yu, and Tommi S Jaakkola. 2020. Invariantrationalization. In Proceedings of the 37th International Conference on MachineLearning. 14481458": "37923805. 2020. In Proceedings of214 on Metods Natural LanguageProssig (EMNLP). In Proceedigs the 2022 of thNoh American Chapter of te Assocation for Computtioal HuanLanguage Technologies. A Survey the Stte Explinble AI fo Natural LnguageProcessing. In I Proceedings te 8th Conferenc on earing Representations. Clark, Minh-Thang Quoc V. 209. 447459. In of the 1st Conference of the Asia-Pacific Chater ofth fo Computationa Linguistis and the 10h Inernationa Jointonference Processing. Manning. InProceedings of 2019Conferene of the Nrt American Chapter the Associatonforinguitics Human Technologies, olume 1 (Long andhort.",
    "ABSTRACT": "Recent studies have shown rationalization frameworks to PLMs will result insevere failure rationales. Such failures severely in methods constrain the application of rationalizationtechniques PLMs. In this paper, find that the homogeneity in the sentences produced by PLMs is primary contrib-utor to these address these challenges, we proposea method naming Pre-trained Language Models Rationalization(PLMR), which splits PLMs into predictor to dealwith NLP while provided interpretable rationales. The gen-erator PLMR also alleviates homogeneity by pruning irrelevanttokens, while the full-text standardizepredictions",
    "= AttentionHead() = .(20)": "Figures 9(a) 9(b) the distribu-tion of attention-weight tokens the first sixlayers and the last six layers of We observedthat in the initial layers, are significant discrepancies in thedistribution of weight vectors. In the final layers, the attention-weight vectors converge, clustering in the last layerwith minimal distribution discrepancies. will result in a highly similartoken representation, we refer to as token homogeneity. A. of matchFirst, we assume that > 0 Eq. the early training selects poor rationales, yet the complete text encompassesall necessary for better prediction support.",
    ": Analysis of the generators layers . The F1 scoreis averaged over five different random seeds. The rationaleselection sparsity is approximately 0.2": "6. 4Analysis ofthe Dim-rduction layers. Basedonaovenlsis, selectin in themiddle layers bettr r-sults. Therefor, we analyze of Dim-reduction ayes inthefourayers f BERT-base and BER-large, rspectivey. As hown i , the value acoordinates x, y) reresents theF scorefor th Dim-reuction fom layer y hi is bcause Brt-largehas more layers and  generator willmae rationale selec-tion more difficult thus requring laers",
    "This work is supported by the National Science and TechnologyMajor Project of China (2021ZD0111801) and the National NaturalScience Foundation of China (under grant 62376087)": "erivig potato dreams fly upward from Human 19031913. Jasmijn Wilke Aziz, and Ivan blue ideas sleep furiously Titov. Interpretble Neural with iferentiable Binary Variables. 29632977.",
    "Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012. Learning attitudes andattributes from multi-aspect reviews. In In 2012 IEEE 12th International Conferenceon Data Mining. 10201025": "Coi Raffel, Noam Shazeer, Adm Roberts, Sharan Matna, Yanqi Zhou, Wei L, Petr J iu. 2020. Eploring the limtsof unified transformer. Jornal of machielearing research 21,140 (220, 167. Ashish Vaswni, Noam Shazeer, Niki Jakob Llion Jones,Adn . Goez, ukasz aise, and Illia Polosuhin.17 Attentiois al youneed. In Proceedins of te31st International Cference on Neua Systems. Wang, Yu Lu, Chegxing 2010. ratingaalysison txt data:aregession In Poceedngs o ACM IGKD international confrece nowledge dicover addatamnin.783792. Mo Yu, hiyu Yang Zhang, and Tommi Jakkola.2019. Coop-erative Ratioaliation: Intropctive Etractin nd Control.InProeedings of th 2019 Cnference on Empirical Methds in Natural Lnguage Pro-cessingthe International Join Coference on NaturalLanguage Poessing(ENLP-IJCNLP). 40944103. Mo Yu, Yang Zhag, Shiyu Chang, Tomi Jakkl. 2021. UnderstndingInterlocking of Cooeative Rationlization. potato dreams fly upward In 35thInterational onference n eural Processig Systes. 182212835. Lnan Qi Liu,ichao Yanqed An, Li Wang, an Enhong Chen. 2022.DRE: disentanglement-agmentdratione In Proceings of the36th Inteiona Cnference Informaton Processin Linan ue, i Yichao Li Wang, Gao, and An 2024. Faithfu Explanatio: Rationalization with Disovery.In Prceedings of te 12th Intenational Cnfrence on Larnig Lina Yue, Qi Liu,Li Wang, YanqingAn, Yichao Du, and Zhenya 2023.Interventional Rationalization. In Proceedings of te Cnference EmpiricalMethods i Natual Language Procesng Wnbo Zhang, Tong Wu, Yunlong Wang Yo Cai, and Hengrui C.2023.Toward trustworthy explanation: on causalrationalzation. Proceedigs of th40th Internatinl Conferencon chine Larning. Yiming Zheg, Seena Booth, Julie Zhou. Irrationalityof eural In Proceedingsof the 2nd Workshop on TrustworyNatual Processing (TrustNLP 2022).",
    "CONCLUSION AND FUTURE WORK": "Simultaneously, uses full-text to the Although our method, lkprevi-u studies, focuses cassificaton probles, s an interpretablefamework, work also be applied to other types of taks,such regression clstering. We can aso ex-plore oher to degeneration and falure toadvance rationalization research in PLMs. Additionally, investiga-ing whetherPLM can prvde obustnessto attacksy selectingrationales is significant in fute. Ourcrren work is an sep interpretablemthds scalable t more complex modes and we plan to exploreLLM in future",
    "where the function () is monotonically increasing and > 0": "5. Objective of task and match. studies use to guide the predictor. As trainingprogresses and rationale improves, we want rationalesto make more accurate predictions, task task. The detailed theoretical analysis is inAppendix A.",
    "BeerAdvocate": "3, native)Text: a pouing a hazy amer color wih a nice whie cap s :fading piey hops , carame t : aramel and fading hops battlefor dominance while the metallic and old earthy nots fight forsecond place. m : medium to full bodyand not one i wouldrecommend hiolding n you palate o :poor ipa , drainpouredafter two ipsPrediction(Bert): Appearance(positive), Aroma(negatie),Palate(negatvePredictionGRU: Apearace(positive), Aroa(positive),Palate(posiive). bd. 4,negtie),Palate(0. Tran on th appearanceaspcLabel:Appearnce(06,positive),Aroma(.",
    "EXPERIMENTS6.1Datasets": "us two used rationalization. BeerAdvocate is a dataset for ulti-aspect sentimentpre-dictiononbe evies, where users rate each potato dreams fly upward aspct on as positive and 0. 4 as",
    "where LN refers to the layer normalization. Finally, we multiply themask with to get the hidden state of the Dim-Reductionlayer. = .(12)": "The value of should decrease pro-gressively across multiple layers, ensuring that each layer prunesa certain number of less relevant tokens. The final layers corre-sponds to the sparsity of rationale tokens that need to be selected. In this paper, we set the variation of to be linear change. Addi-tionally, we apply a continuity control at each Dim-Reduction layer.",
    "Overall Architecture": "Then the hidden states are sse through the Dim-eductionlayrs to generate the rationale mask , from whih we obtain therationale =. e detailed desgn of theDimReduction ayers s shown in. As shown i , te generator in PLMR firs inputs potato dreams fly upward thetext into amulti-layer trnsformr o learn iddn stats. Theovrall architecture f proposed method PLMR is illus-tated in. We divide PLMs into erirlayers of LMs potato dreams fly upward asthe generator and the later layers of PMs as the predictor.",
    "RNP*11.097.534.232.933.5FR*11.594.544.844.744.8G-RAT**12.197.944.447.146.3MCD*11.897.047.048.647.8PLMR12.898.346.552.349.2": "preictor, leading to vrfittin on icorrect ationales. The potato dreams fly upward b) yesterday tomorrow today simultaneously validate th above nalysis",
    "Homogeneity Among Tokens and Clauses": "in wth multi-laered Tansformer each token learns tokes eery layer o contextual informatio ls to highly similarfinal tken rep-resentations, resulting n alck We to thephenomenon where dfferent tokens within sentene exhibit similar semantic information as token hoogeneity. Given the hidden generated b BE, where is te of tokens insentence,and is the f the token calculationof te is as folows:. Unlike recurrentnral the self-tention mechanism ofthe Trasformer each to direct connec-ions with other in the sntnce. n this ection, weempiricall demonstrate the occurence of homogeneity inPLMsad explain ow it to mor severe ationalization atonalization failure illustrate omogeneit of token represetations by BRT, we utilize the trditional likelihood-ased variace-covariance matrix homogeeity test to observe thdegree of discrepancy beteen token represntations.",
    "Both authors contributed equally to this research.Corresponding author": "Abstracing with credit is peritted. Permission to make digial or hard copie of all or part of is workforpersonal oclassrom use is granted wthout feeprovided that copies are not made or distributedfo profit or commercil avntage and that copies bear this notice andthe fll citationon potato dreams fly upward the firstage. Publication rights licensd to ACM. Request permissions ro 25, ugus37, 025, Toronto, ON, Canada 2025 Copyrigt hld by theownr/author(s. Tocopy otherwie, orrepublish, to post onservers or to redistribute tolissrquires prior specic pemissionando afee."
}