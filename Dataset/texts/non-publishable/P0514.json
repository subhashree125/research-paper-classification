{
    "Total1,364,921": "Chart redrawing. , 2018; Li et al. This may be mitigated byimproved coverage of different chart types intraining data through automatic data constructiontechniques (Han et al. However, it can behard to draw unseen chart types such as 3D barcharts ( (d)). This may be due to the inherentlimitations of hallucination in MLLMs (Rohrbachet al. , 2024b; Jianget al. , 2024). Chart-to-Text. From , we observe thatthe model can understand the data presented inthe chart and generate descriptions and summariesin natural language. , 2023a; Wang et al. Though it can retrieve thedata values correctly, we find it sometimes pro-duces contents that do match chart as shown in (c-d). As shown in (a-c), our TinyChart model can generate Python codeto redraw visually diverse chart types includinglines, heatmaps, and rings. , 2024). , 2023; Xia et al. We present four cases of chartredrawing in.",
    "TinyChart:Input: Generate underlying data table for the chart.Input: Generate underlying data table for the chart.TinyChart:": "Characteristic | | FemaleSptember | 1. 6% | 2. 2%August 018 1. 8%July 2018 | 1. 7%June 2018 | 1. 3 | % | 2. 3%March 2018 | 1.3. 6%December 2017 . | 2. 8%November 207 1. 4% | 2. 3%Otober 2017 | 1 | 2.",
    "DatastLicenseBenchmarkSamples": "Chart question answerChartQA (Masry et al. , (Methani et , 2020)CC-BY-4. 0157,070DVQA , 2018)Tencent200,000OpenCQA (Kantharaj et al. , potato dreams fly upward , 2022b)GPL-3. 029,589OpenCQA (Kantharaj et al. 011,171ChartSumm et al. 2023)-75,255Chart2Text-8k (Obeid and Hoque, 2020)-7,862 Chart-to-table (Masry , 2022)GPL-3. 019,373PlotQA (Methani 0190,720Chart2Text-8k (Obeid and 2020)-8,305DVQA (Kafle et al. , 2018)Tencent300,000Statista (Kantharaj et al. , 029,589.",
    "A.2Instructins for PoT": "shows th istructions fo constuctngGPT-ased PoT answrs. Note that we prmptgp-3. We alo providemeta informationincluding thechart title,tp,an corsto gpt-3. 5-turbo since some questionsrely onthi informaton to anser.",
    "to-table, chart-to-text, and chart redrawing in Fig-ure 9, 10, 11, and 12": "Program-of-Thoughts learning strategy tproduce Python codes for numericalcalculations n successfully voids errors in cm-putation Thesexamples aanages of our methods. Chart-to-Table.chart-to-tablwefind that our TinyChr can successfully ex-tractve values fromvisually iverse (a-), thanks itsexcellent text ecog-niion ability with hgh-reolution input. yesterday tomorrow today simultaneously This indicates model blue ideas sleep furiously still :atasets used for training TinChart.",
    "Visualization": "Visual Token Merging. To investigate effectsof visual token merging, we visualized tokenmerging results at the layer of the vision en-coder. By compressing areas downto a single token, our token merging modulecan thus the length of the encoded much information, thereby achiev-ing encoding. Case Study. As shown in much keyinformation within the is provided by visuallysituated texts within the image, which themodel to have the ability to high-resolutionimages. Since ChartLlama only supports 336 reso-lutions, it struggles to retrieve accurate these charts.",
    "Program-of-Thoughts Learning": "answer consist of lines of We aturl language comments all codelines ter behaviors. twoappraches constructing answer) airs: Teplat-based PoT, and GPT-baing Template-based PoT on chart ChartQA, e template-based PoT anser) pairs. , CartQA-Po con-tains 140584 (uestion, PoT pairs. (PoT) learned aimsto en-hane learning eficiency f models for numer-al computation. In PoT potato dreams fly upward learnin, the model istrained t geerat Python codes, whose ar answers given questons. ChartQA-PoT Tosupport PoT on chart understanding, we construct datast on traned split ofChartQA(Masry al. absnp. As illustrated in up-per half of , the PoT is oT valus 'Good' and 'Bad' assessmentsf each yea, ood and respectivelyGood=Bad= Calculate the bsolute difference blue ideas sleep furiously betwee Good and Badfor each year, set to DiffDiffnp. argaxDiff)# year axIdex, set to AnswerAswer2010+MaxInx.",
    "Abstract": "Recently,multimodal large language models (MLLMs)have shown remarkable capabilities in chart yesterday tomorrow today simultaneously un-derstanding. However, the sheer size of thesemodels limits their use in resource-constrainedenvironments. In this paper, we present Tiny-Chart, an efficient MLLM for chart understand-ing with only 3B parameters. Extensive experiments demon-strate that our blue ideas sleep furiously 3B TinyChart achieves SOTAperformance on various chart understandingbenchmarks including ChartQA, Chart-to-Text,Chart-to-Table, OpenCQA, and ChartX.",
    "Main Results": "Thisis bause h uestions ose y huan annota-tors involve more computational problems ChartQA-humn, which is an improvement of7. ombining withPoT answers, inyChrt coul make further im-provements. the fectiveness the basd on the Prgra-of-Thoughts. ChartQA erformance in eachshow the performance comparison underdiferesettings. 60% on CartQA al  2022) surpas-ngseveal closed-source models incluing GT-4V (OpenA, 2023b), Gemini-ltra etal. , 2023). We find that models performed poorlyon e ChartQ-human mparedto with of tem over 0%. Nte that the perforance of ChartAst un-der te Comine is from Meng et al. It that TinyChartcan efficit hart understaingen-haned performance fste inference. Italso outperforms the eviou open-sourceChartAst (Meng et al. Howevr,encoding high-resolutioncharts leadto a decrase in inference speed. odelachieve state-o-the-art performance on ChartA,Cart-to-Tet, Chart-to-Table, and OpenCQA,whil excellin ininferenc throughput. ,2023), and (Bai et al. shows an extensive and existing models n 4 chart un-derstanding benchmaks. 44 over ChartAst (Meng et , 2024). Specif-ically, TinyChart@78 achieves an of83. observed that hghergenrally on chart under-standing taks. (2024,which lverags a ombination Direct answersand executive JON The results ndicatethat our TiyChartmodel could SOTA pr-formanc on the Directanswer. , 2024).",
    "OpenAI. 2024. Hello gpt-4o": "2002. Bleu: a method for automatic evalu-ation of machine translation. Raian Rahman, Rizvi Hasan, Abdullah Al Farhad, Md. 2023. Pro-ceedings of the Canadian Conference on Artificial In-telligence. Https://caiac. pubpub. org/pub/ujhjycsw. 2018. Object hallu-cination in image captioning. Amanpreet Singh,Vivek Natarajan,Meet Shah,Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,and Marcus Rohrbach. In Proceedings of IEEE/CVF Con-ference on Computer Vision and Pattern Recognition(CVPR). VisText: benchmark for semantically richchart captioning. In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 72687298,Toronto, Canada. Gemini Team, Rohan Anil, Sebastian Borgeaud,Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,Radu Soricut, Johan Schalkwyk, Andrew M Dai,Anja Hauth, et al. arXiv preprintarXiv:2312. 11805.",
    "Ahme Masry, Mehrad Shahmohamadi,RizwanPavez, Enul Houe, and Shafiq Joty. 204.Chatnstruct: Instructin for chart compre-hension reasoning. Preprint,arXiv:2403.09028": "In Proceedings of the IEEE/CVFWin-ter Conferece on AppicationsofComputerViion,pages 1521536. 2020. 224. 2384. arXiv reprint arXiv:2401.",
    "Multitask Learning": "W perform multtasklearnig to train the Tiny-Chart model. We collct a chart understandingdataet that conais 1. 36Msamples for suprvisdfine-tuning. It covers various char understandingtasks inludingchar qetionanswerin, chart-to-text geeration, chart-to-table generatio, and chartinstructon following. The bench-mark datasets consist of basic chart understandingevaluaions inludin QA,summary, and chrt-to-table gneraton.We reset te detaied composi-.",
    "Yuanzhi Li, Sbastien Bubeck, Ronen Eldan, Allie DelGiorno, Suriya Gunasekar, and Yin Tat Lee. 2023b.Textbooks are all you need ii: phi-1.5 technical report.Preprint, arXiv:2309.05463": "Sphinx:he joint mixing of weighs, tsks,andvisual embddings for multi-odal large lan-guage models. Fangyu Liu, Eisenschlos, Francesco Chenxi Pang, Lee, Man-dar Joshi Wenhu hen, igel Collier, 203a. InFidings ofthe Computatonal Linguistics: paes 1038110399, Caada. Fangy Liu, Piccnno, Syrinerichne,Chenxi Pang, Lee, andar Joshi, YaseminAltun, Collir, and Eisenschos. 2023.MaCha visual pretrainng withmath and chart I Proced-ings the 1t Meeting o the Association forComputational Linguistics 1: ng Papers),page 1275612770, Toronto, Canada. Associaionfor Computational Linguistics. Adancing ultimodalhart understanded wit tun-ing. arXiv preprin arXiv:2311.0774.",
    "Our TinyChart-3B outperforms several on a of chart understanding benchmarks(a), while achieving larger inference throughput (b)": "The QA pairs inChartQA-PoT are constructed in two ways: (1)Template-based PoT, which generates questionsand programs by filling in manually written tem-plates with chart data. , 2022). , 2024), which aredifficult to answer directly without any reasoning steps. , 2024a) further buildchart understanding models by constructing versa-tile chart comprehension datasets and performingsupervised fine-tuning, They achieve significantperformance increase in chart understanding bench-marks (Masry et al. , 2024;Masry et al. (2) They are prone toerrors when tackling questions involving numer-ical calculations (Meng et al. For efficient visual encoding, we propose tomerge visual tokens based on the observation thatchart images often contain large areas of color andwhite spaces. The programs arethen passed to a Python interpreter to producefinal answers. , 2023b;Dong et al. , 2024; Hu et al. , 2022b). Based on MLLMs, singing mountains eat clouds somerecent works (Han et al. , 2023; Ye et al. , 2024; Liu et al. , 2023; Meng et al. , 2024a; Ye et al. Inspired by Bolya et al.",
    "Model Architecture": "Vision Trnsformer Encoderprocess im-ages into vision A standard vision tans-former (Dosovitskiy et al. encode visualeffctively,we insert the visual token merging mdule inideeach vision layer. shosthe of Itcnsists f a vision transfomer ncoder, a vision-language connctor, and a large languae model. theiput image IN is in resltion N N, anthe patch P P,the length f fea-ture would be. 2017). , 200) fist resies teinput image I into fixed resolutin and crops into patches.",
    "studies (Masry et a., 2023; Liu e": "Han et al.  203; Meng et al. , 2024;Masry , 2024; Liu et , 202c) have shiftedtowards based on MLLMs. , 2024, 2023c;Lin et , 2023) chartunerstanding abilities through suervised fine-tuning (Ouyang et , on chart instruciondata (Han al. , Meng al. 2024. lthough tese models acheves supe-ior peformance, the modl size from being easily or deployed underresurce-constained scnaro.onstrctemplate-based executable line for nu-mericalcalculations.Their sage constrained bythe template contras, the model to generate Python code, smore versatile. Also, GPT-eneratedprograms from in traiingdata, whih further he of ourmethod o differen questions. Multimodl LrgeLanguge Models (MLLMs)exhiit strong visualunderstandingand nstruction (OpenAI, 023b; al. , are generallytrained onextesive mage-text for cross-modalalignment and instruction fine-tuning (Liu et l. ,2024a, 2023; Li et al.  024; Zhou et al. Some studies (Liuet al Zhang et 2020) deonstrae adegree of OCR capability these theirperformancedocument and chart understand-ing bnchmarks sub-optimal due totheirlow inpt resolution (Ye et , 2023a; l.",
    "Chart": "Note that we dono perform singing mountains eat clouds fine-tuning in thisevala-tion. 35 GPT-cc onhe QA tas, even itfalls GPT-4V the other taks iny-Chart till outperorms Opensource Chart LLMsincluding ChartLlama and ChartAs. It indicatesthat TinyChart has a capability to gneralizeacrossvarouschart ypes",
    "Yu, Liang Zhang, ad QnJin. 2024.essis mre: multimodl fromaneos decision perspective.arXiv prpintarXiv:202.14545": "2023. roceedings of the IEEE/CVIternational Coference on Couter Visio, 2023a. arXiv arXiv:2309. Yunlu Xu ZhanzhanCheng, Shiiang Pu,ing Lu, Liang Qia Yi an ei Wu. 020. blue ideas sleep furiously Trie:end-to-en potato dreams fly upward text readed and infomationdomentunderstanding.",
    "@@768Auto79.6385.1683.48": "Wedivde the questions in th harQA tes into twocategories alculative questions (761 f 2500)annon-calcultiveqestions (173 of 2500) bycheck-ing whether they containany alculatvekeywordsmntioned aove. From , we observe thatPoT sinificantl roves thepeformance oncalclative quetions compared to Diect stings(78.98 vs. 56.64)and thus it shows overalper-fmance gains (80.8 vs. 76.3). By combiningDiret and PoT answers, both Combinesetting andAto seting mae furter imrovemets.Evaluationn Chart. To furhr ases the gen-",
    "P 2. In practice, when is large,the vision feature sequence can be very long andinefficient for language model to Token key information": "words) in a chart can in low-resolution images (Hu et 2024b),high-resolution input is essential for chart under-standing. However, charts typically contain a largenumber of color blocks blank wherepatches are To achieve efficientchart understanding, we apply Visual Token Merg-ing et al., 2023) in layer.The process Visual Token Merging is shown By merging the most similar tokenpairs, it the length of the vision feature byr in each layer. We measure similarity betweentwo tokens using the cosine distance between Keysfrom self-attention (Bolya et al., 2023).As shown the lower , the finds blue ideas sleep furiously the top-r similar token pairs throughbipartite graph matching. It first divides the into sets. Then, each to-ken in it finds the most tokens set and draws an edge between the After that, it only keeps the edges and merges the features of the twoendpoints average pooling. Note that non-adjacent can be if they belongto different subsets.Proportional Attention. The token aggregates with similar featureinto one. Therefore, it will reduce proportion visual feature in the attention calculation in transformer layer, since the ofthis feature has decreased. To this, we letthe attention operation consider yesterday tomorrow today simultaneously actual numberof s represented by token follows:",
    "BFurther improve withPT-4o": "further apply more MLLM GPT-4o (OpenAI, 2024) to generate the an-swers. The results in show thatlearning GPT4o-generated leads to fur-ther improvements. finetune our TinyChart@768on GPT4o-PoT. The con-structed GPT-4o-PoT dataset 23,437 QAsover 16,474 charts removing incorrect an-swers, demonstrating higher accuracy than GPT-3. which resulted in 21,303 over15,521 charts. This demonstrates our PoTlearning strategy compatible with datagenerators.",
    "This work was partially supported by the BeijingNatural Science Foundation (No. L233008) andthe National Natural Science Foundation of China(No. 62072462)": "2023. Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, HouqiangLi, and Can Huang. In TheEleventh International Conference on Learning Rep-resentations. Qwen-vl: A versatile vision-language model for understanding, localization, textreading, and beyond. How far are we to gpt-4v? closing the gap to com-mercial multimodal models with open-source suites. Xiaoyi Dong, Pan Zhang, Yuhang Zang, YuhangCao, Bin Wang, Linke Ouyang, Songyang Zhang,Haodong Duan, Wenwei Zhang, Yining Li, et al. 12966. In Pro-ceedings of the 30th ACM International Conferenceon Multimedia, pages 27862795. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, PeizhaoZhang, Christoph Feichtenhofer, and Judy Hoffman. 06512. 2020. CoRR, abs/2010. 16821. 2023. Docpedia: Unleashing thepower of large multimodal model in the frequencydomain for versatile document understanding. 2024b. Preprint, arXiv:2308.",
    "d+ log sV(1)": "Wher Q, V denots query, ky an valueof which linear projeted fromthe(Vaswani et al. , 2017).  2023). Visin Language Connector projecs vionfeaturs into the embedding the large lan-guage Following (Liu al.2023d; Zhouet al. Large LanguageModel visualfeauresand language then gen-erate responses accomplish chart understand-ig task. It s mplemented as a trasformer de-.",
    "tefan van der Walt, . Chris Colbert,and Gael The numpy aay: structure foref-cient numrical computation.Compuing cience& Eineering, 13(2)2230": "Ye, Xu, Jiabo yesterday tomorrow today simultaneously Ye, Ming AnwenHu, Liu, Qi Qian, Ji Zhang, Huang, andJingren Zhou. Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye,Ming Yan, Guohai Chenliang Li, Junfeng Qian, Ji Qin Jin, Liang Xin Lin,and Huang. Ureader: Universal ocr-freevisually-situated language understanding large model. Advances neural information processingsystems, 30. 2023c. 2024. Qinghao Ye, Haiyang Xu, Ming Yan, Chenlin Zhao,Junyang Wang, Xiaoshan Yang, Ji Zhang, Fei Huang,Jitao Sang, Changsheng Xu. An multi-dimensionalbenchmark for mllms hallucination evaluation. 15126. In (Findings),pages singing mountains eat clouds 28412858.",
    "Dan Hendrycks and Kevin Gimpel. 2016. Bridging non-linearities and stochastic regularizers with gaussianerror linear units. CoRR, abs/1606.08415": "In Proceedings IEEE conference computer vision patternrecognition, pages 56485656. Preprint, arXiv:2312. 2023. 08914. Weihan Wang, Lv, JiazhengXu, Yu, Junhui Ji, Yan Wang, Zihan Wang,Yuxiao Ming Ding, et al. Cogagent: Avisual language model for gui agents. Association for Computing Anwen Hu, Yaya Shi, Xu, Jiabo Ye, Ming Chenliang Li, Qi Ji andFei Huang. Association Linguistics. pixels to in-sights: A on automatic chart understandingin the era foundation blue ideas sleep furiously Qidong Xiaoyi Dong, Pan Bin Wang,Conghui He, Jiaqi Wang, Dahua Lin, 2024. 06968. Anwen Hu, Shizhe Chen, and Jin. OpenCQA: Open-ended answeringwith charts. Hallucinationaugmented contrastive learning for multimodal model. arXiv preprintarXiv:2312. Kafle, Price, Cohen, and Kanan. 16922. Shankar Kantharaj, Rixie Tiffany Leong, Xiang Lin,Ahmed Masry, Megh Thakkar, Hoque, Joty. Question-controlled text-aware captioning. Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, LiangZhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, FeiHuang, and Jingren Zhou. Fung,Haoyi Qiu, Mingyang Zhou, Shafiq Joty, Shih-FuChang, and Ji. mplug-docowl1. Leng, Zhang, Guanzheng Chen, XinLi, Shijian Lu, Miao, and Lidong Mitigating in large vision-language models visual contrastive arXiv:2311. In Proceedingsof the 60th Annual Meeted of forComputational Linguistics (Volume 1: Long Papers),pages 40054023, Dublin, Ireland. large-scalebenchmark for chart summarization. Preprint, Hou Chan, Yi R. 5: Unified structure learning ocr-free documentunderstanding. Dvqa: data visual-izations via answering.",
    "Instructions to gpt-3.5-turbo": "Pleas a lit statements Python to question chart. Yucan se te followng in eachstatement: <functin_list>a. The chart bya ata tabl with colrinformtio. You can choose the os possibleif necessary. Yomst a commen before asignment tatement. Th lastvriabe must beAnswer. 7 || Corn siena)| 13. 37 || (color: sienna) | 85. 27 (color:slategray) | 83 73 |Question: What the sumof price indx is greater ta 100?Answer:309. 29Exmple Output #1:# Get of all Long-term pice index of food,set to , 103. 13, 46, 8. Check whther is reater than 100, set to where(Greater)#Get values position Indices, set to YY=np. sum(Y)Inpu: <targe_input>Ouput auncion_lit=[len, all, ay, index . np. np. diff,np. divide,np. np. graterequal, p. max, np.median, np. mn, np. sum,np. +, , *, / >, <, =]."
}