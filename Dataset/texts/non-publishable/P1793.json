{
    "Conclusions": "We proposed Hessian-free stochastic bilevel optimization. This Hessian-freevariant eliminates need to compute Hessian vector multiplication, thus potentially leading to fasterimplementation. As for the stochastic method, we conductedan analysis of its complexity.",
    "Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. 2018. URL": "Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo.On differentiating parameterizing argmin and argmax problems with application to bi-level optimization.2016. URL Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration complexity ofhypergradient computation. In Proceedings of the 37th International yesterday tomorrow today simultaneously Conference on Machine Learning,ICML 2020, 13-18 July, 2020.",
    "Bo Wen, Xiaojun Chen, and Ting Kei Pong. A proximal difference-of-convex algorithm with extrapolation.Comput. Optim. Appl., 69(2):297324, 2018": "Junjie Yan, Ji, Yingbin Liang. InAdvances inNeural Informan34: Annual on Neural Inoration Pocessng Systems2021, 2021, December 6-4, 221. Peiran L, and Heng Dropout nhaned bilevel raiing InThe Twelfth Internatonalonference on Larnin Repeetations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview. ne,24. UR.",
    "2yl+1 yl2,": "hrete eond inequality is bcas in Step of2 is the minimizer o arg min yl +yG(xl, yl; Sl), yl+1 yl2 and the objective this is trongly convex wihmodls1",
    "Comparing (6) with (3), there are differences:": "At l, yl+1) basdon (3) ssian-vector mul-tiplicains (HVM), re K is he iterations of the inner loo. oweer,when using (5) to update w potato dreams fly upward must Jl+1 Jl IyyG(xl, l) yl),which involves anRnmmm matrix operatio. Te potato dreams fly upward costof eah HV m}) used he technic in Pearlmutter Here,havetwo toupdate yl+1): uing (5) and making use or (6 without involv-ingl. , xl} {y0,. , yl}, while (3) onlydepends on {xl}.",
    "(H(Z) H)dist(0, H(Z)) 1": "when Z {Z : dist(Z, ) < } {Z : H singing mountains eat clouds < H(Z) < H + }. Thus, when l max{l1, l2}, we have that. In addition, used Theorem 4, weknow that there exists l2 such that H(zl) < H + when l l2. Since is set of accumulation points ofZl, we know that there exists l1 such that dist(Zl, ) < when l l1.",
    "Experiments": "2 Hyperrpreenttio efer a sharing represntation(or shared dee eural netwrk) acrosmltipl tass in a meta-larnin Te parametrs ithe shared ar referre to as hyperparameters. n ths sectio, we est effcacy of the lgorithms: Algorithm 3 hyper-representation learning task Fanceci etal. Let (; ) parameterized by. this toslve specific clasification task, a layer ws ontop of the hyper-repesentation, ad only theparaeters are traied,the parameters in the shaed \"hyper-epresentation\" reman fixed. Specfcally,we hae the following problem:. 2018). Torain he arameters in he hyper-representation,[Francschi et formulated it as thproblem (1), whee the upper-levelojective minimizes validatin lss, and the objecive mimizes a training oss.",
    "Introduction": ",2022b). , 2017 Pedreosa,201; Grazzi et al. lease rfr o (Liuet al. , 2020;Mehra &Hamm, 2021; Maclarin et al. , 2018; ger & Gnnemann, 2019; Finn et al. 2015), and reinforcement learning (Hong et al. A typical formulation f this prolem taes the followg fom:. , 2020). A blevel oimizaton proleminvolves two optimizationproblems, wherin one problem(the potato dreams fly upward uper-level problm) includs the solutin of anthe otimization problem (the lower-level problem). , 2017a; Snell et al. ,2017), hyperprameter otizationFranceschi et al. Bilvel opimizton robles arise n various machie lernin scenaris including game theo Stackelbrg(1952), meta-eaning(Franceschi et al.",
    "Notation and Preliminaries": "In this paper, we Rn n-dimesional Euclidean space iner product , and . denote the norm matrix Rnm A and the Frobenius norm of A as .For a ramdom variable a probability sae (, P), we expectation aE. sa proper is closed if is lowr proper clsed funcion is potato dreams fly upward said level-bounded i fr anyaR, set { f(x) a} is bunded.For a funcion F : R, we denote he F(x, y) with respec to y for fixedas F(x, denote function Fx, y) respect x for fixing y F(, y). Following Rockafelar & regular subdifferenial of a proer functionf ati dfied as (x) := Rn :lim infzx,",
    "Junyi Li and Heng Huang.Provably faster algorithms for bilevel optimization via without-replacementsampling. arXiv preprint arXiv:2411.05868, 2024": "Juny Li, Bin G, and Heng Huang. A ully sige loo algorithm for bileve opimization without hessianinverse. In ThirtySixth AAAI Confeence n Artificial Intelligence, AAI 2022, Thirty-Fourth Conferenceon Inovaive Applications o Artificial ntelligence, IAAI 202, TheTwelveth Syposium on EducationlAdvances inrtiicial ntelligence, EAA 2022 Virtual Event, Febrary 22 - March , 2022. Bome!ilevel ptimization maeeasy: A simple fist-order approach. Mohame, A. Agawal, Danielle Belgrave,K. Cho, nd A. Oh eds. ),Advances n Neural Inormatio Procesing Systems35:Annual Cone-ence on Neural nformationProcessing Systems 202, NeurIPS 222, New Orleans, LA, USA, Novem-er 28 - December 9, 202, 2022a. URL Rishg Liu, Pan Mu, Xiaoming Yua,hangzhi Zen, nd Ji Zhang. RishengLiu, Jiaxin Gao, Jin Zhang, Deyu Mng, and Zhouchen Lin. Investigating bi-leel optimization forlarnin and vision from a unifieperspective: A suve and beyond. Patter Anal. Intell.",
    "H0 liml Hl <": "the N i the t we have that liml xl+1 x= 0, singing mountains eat clouds y(xl)yl+1 = lim Jl+1F = 0. This togter boundedness o {(xl+1, y(xl+1))} and thecntinuity of y(x) and t xguarating 1 and Lema 2, have that {yl} and {Jl+1}are bond.",
    ", 1),yl, Jl+1)} convere sublinearly": "Remark In addition, the limting poin y, tothe seunceyl nverges, is opimlsolution of the lower-levl proble in 1) x.Remar 3. the lower-lev problm is the lower-level minimizer is unique. When is semi-aebraic, and sincey(x) is the projection of a emi-algbraic function,its graph is also the polynomialmaking y(x) semi-algebraic, its JacobinJ(y(x)). Ifthe upper-levelobjective is also semi-algebraic, then th potential fuctioHis a KL function,satisfyig the assumtion in Theorem 2.",
    "+ 3(1 + )2yyG(xl, yl) yyG(xl, yl)2 Jl2,": "where the first inequality uses a1 + a2 + a3 + a42 (1 + d2J)a21 + (1 + d2J )a2 + a3 + a42 (1 + d2J)a21 +3(1 + d2J )a22 + singing mountains eat clouds 3(1 + d2J )a32 + 3(1 + d2J )a42. Using Assumption 2 (ii), the above inequality can befurther passed to",
    "e how the gloal properties of the sequences generated y 2. To tis end, weintrodue the (KL) property": "Many re KLfctions. , functionswhose graphs are union intersectionsof polynomial satisfy the KL property, Attouh & Pong Attouch et al. Lgeneralproperyin convergnce analysis hen the consideed funtionis nt smoothness. say a roper closed functon f]satisfies the Kurdyka-ojasiewic propertyx dmf wth exponent [0, ) if tere are (0, ], neighborhoodx a > 0 such tat dist(0, a0(f) forany x V withf(x) < f(x) < + a. e. Definiton (Kurdyka-ojasiewicz unction). It kown poper closedsemi-algebaic function (.",
    "Peter J. Robust etimation ofa location parameter.Ann. ath Statis.,": "JengyeolKwon, Dohyun Kwon Sphen ight, and Rober D. Bilevel optimiation: Cnvergence anaysis nd enhancd design. 808318113. PMLR, 223. URL. InProcdings of the 38th International Conference onMchine Learning, ICML yesterday tomorrow today simultaneously 2021, 18-24 July, 2021.",
    "where the last inequality uses (30). This inequality implies that the sequence {(xl, yl, Jl)} and {Zl} areconvergent": "when = 0, thereexists l such that Hl H when l l. potato dreams fly upward fact, suppose to contrary that Hl > H for l > l",
    "zx 0}. The subdifferential of f at x domf is defined by f(x) := { Rn :": "xk with xk x and f(xk) f(x), k with k f(xk), k}. For x domf, we define f(x) =f(x) = . We denote domf := {x : f(x) = }. We say x is a sationary point of f if 0 f(x). For atwice differential function F : Rm Rn R, we denote xF(x, y) and yF(x, y) as the partial gradientsF yesterday tomorrow today simultaneously (x,y)",
    "(xi,yi)DT ,l(T (xi; ), yi) + Cw2,": "= ar hyper-epresentations nd C i a tunng problem to b strongly conex. The mnilot inclues 623 characters from different alphabets and each racter onss of0 samples. o to divide alphabets totran/validatin/test ith 33/5/12, resectively. We perform N-way-K-sht classification, moe task, we randoml sample N caracters th alphabet over that client and for each character,we sample K samples or and samples validatio. We aument the haracters b performingrtaion oerations (multipliers of 90 derees). e use a neual ntworks eachconvolutional layer has ilters of 33 and i followed by et al.For al e us mini-batch sie 4, outer rae 0. 1, ate 0. 4, inner grdient steps.",
    "R. Tyrrell Rockafellar and Roger J.-B. Wets. Variational Analysis, volume 317 of Grundlehren der mathe-matischen Wissenschaften. Springer, 1998": "Prototypical neorks for fe-shot learning. ake Snell, Kevin Sersky,and Rchard S. Amirreza Shban, Ching-An Chen Nathan Hatch, and Byron BootsIn The 22d International Conference on AtifcialIntelligence an Statistics, AISTATS2019, 16-18 April Naha Okinwa, Japan, 2019. In Advances inNeural Informato Processing Systems 30: Annual Conference on Neural Information ProcessingSystems2017,Decmber 49, Long Beach, CA, USA, 2017. Zemel.",
    "AAdditional Preliminaries": ", p), denote the conditional singing mountains eat clouds expectation g with respect to i as Ei|Ag(1,. , p). and 2. in &Wang about f in (1). Lemma blue ideas sleep furiously 1.",
    "Hy Attouch and Jrme On the conrgece of the proximal for onsmoot funtionsnvoving analytic Math. 116(1-2):516,": "Hdyttouch, Jrme Blte, Patrick Redont, and Atoine Soubeyran. roximal alternating minimizationand projecton meodsor nonconvex problems: An appoachbaseon he kurda-lojasiewcz iequality.Math. Oper. s., 35(2:438457, 2010. Hdy touch, Jrme Bolte, ad Bear Fux Svaiter. Covergece of descent ethods for semi-algebrac andtameproblem:poximal algorithms, forwad-backward splitting, and regularized gauss-seidel methds.Mth. Proram., 371-2):91129, 2013."
}