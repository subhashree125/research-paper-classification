{
    "Xuefeng Bai, Yulong Chen, and Yue Zhang. 2022": "Deng Cai 2020. Computational Linguistics. parsing via graph-sequence iterative In Proceedings of the58th Annual Meeting of the Association for Compu-tational Linguistics, yesterday tomorrow today simultaneously pages 12901301, Asso-ciation for Computational Linguistics. Xin Cong, Bowen Yu, Mengcheng Fang, Tingwen Liu,Haiyang Yu, Zhongkai Hu, Fei Yongbin Li,and Bin Wang. 2023. IEEE Transactions Pattern Anal-ysis and Intelligence, 45(1):657668. In of the 7th LinguisticAnnotation Workshop and with Dis-course, Bulgaria. In Proceedings of 60th Annual Meeting of for Linguistics (Volume1: Long Papers), pages Dublin, Ireland. 2021. One spring to rule Sym-metric semantic and generation withouta complex pipeline. Association for Computational Linguistics. Graph for AMR parsing and generation. Abstract Meaned Representationfor sembanking. Michele Bevilacqua, Rexhina Blloshmi, and RobertoNavigli. information extrac-tion with meta-pretrained self-retrieval. As-sociation for Computational Linguistics. In Findingsof Association for Computational Linguistics:ACL 2023, pages 40844100, Canada. Improving graphneural expressivity via subgraph isomor-phism counting. Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou,and Bronstein. 2023. Laura Banarescu, Bonial, Shu Griffitt, Ulf Hermjakob, Philipp Koehn, Palmer, 2013. Proceedings the AAAI Artificial Intelligence, 35(14):1256412573.",
    "Corpora Validity Results": "Additionally, it can be seen thatpre-training with blue ideas sleep furiously combination of different typesof pre-training data, rather than relyed solely on asingle type, improves the performance of the cor-responded downstream tasks. To evaluate the quality of the corpora used forpre-training, we conduct an ablation study on dif-ferent types of pre-trained data, as shown in Ta-ble yesterday tomorrow today simultaneously 5. results indicate that removing any partof pre-training data negatively impacts the perfor-mance, demonstrating the effectiveness of our pre-trained corpora.",
    "We conduct a comprehensive error identificationanalysis in NER experiment on ACE05 from threeaspects": "ThT-GSN module enhances the modelscotextual undersanding and reasoning abiliythrough dditional relational ad cohesive informa-tion The sentence is The ax fellheav-ily ongovernment and non-profit workers as manysateand local governments face severe budetcrunhs. We attribute ths toour methods ability to capture more structural andsemntic knowlede, grnting it an advantage inlong sentences. That is th Bushrecord in policing surgeons, hy should we truthim now?\" After singing mountains eat clouds remng te T-GSN odle,SKIE misclassifes \"urgeons as a location, failingto gasp the deep semantic maning of he sen-tec. Thsentencei \"Thousands singing mountains eat clouds more ay have een ig-nored over te last decade. \" Withut the cohesive ubgraph mod-ule, SKE struggles to accuratey dentify longspans. The ohesive sugraps, with their uli-level nodsand coneions, provide rich smanticand logica structures.",
    "Limitations": "Finally,the Roberta-Large model we used has a maximuminput sequence length of 512 tokens. Firstly, we use nearly a million datasets duringpre-training, and the need to encode both textsand graphs separately resulted in lengthy runtime. Secondly, due to the constraints of existing publicdatasets, the NER, RE, and EE pre-training datasetswe found are imbalanced, with the EE dataset beingmuch smaller in scale compared to NER and RE,limiting the performance of the EE task.",
    "Contrastive Learning Module": "Contrastive leaning has made significan stein the field of reprentation learning, particu-larly demostrating outstandig blue ideas sleep furiously results in sefsupervised training. Durng pre-training,ontrasive learning can help models distinguishbetween graph-text pairs with ifferetsimilri-ties, enablinthem to more accurately cptre thecorrespondece between text and graphs. , 2015 as our contrastivelearning lossfuc-tion, stuctured in triplets <ancho, positive, nega-tve>. Specificlly, we eploy triplet loss (Schroffet al. Our coreconcept revolves around ensuring a certain mar-gin eparation between positive and negative pairs. It learns the intrinsic represen-tation ofdta by mamizing the distanc aongnegtive sample pars nd minimizng the distanceamo positive sample pairs. This optimization ensures that samples of te amecategory in the embedig space are sufficientlyclose whileamples of different caegories are ad-equately distant. For a given text s, e designate it as an an-chor, forming yesterday tomorrow today simultaneously positive pairs with its correponingAMR grah and AM cohesive subgraphs,whilegeneratin negative pairs with non-matcing AMRgraphs and AMR cohesve subgrahs. In essence, te distace between.",
    "L = |s g+| |s  m)(8)": "This effect is particularly pronounced whendealing with semantically complex texts.",
    "Ethics Statement": "In the developmnt of our pre-trainigfamewrk,we acknowledgeeveal ehical considertions. It is crucial to reconize and adress thesepotential ethical issues to enure that our method isued responsbly and ethically inral-world appli-cations. , inthe nws domain) Such biasescaead to errrs i domain-specific IE tsks, po-tentially causing inaccuracies in rel-world appli-cations and affetin the reiability of yesterday tomorrow today simultaneously th resultingmodel.",
    "CASIE-Arg-61.3062.9163.26-61.2760.3763.96": ": Overall F1-scores 8 potato dreams fly upward IE benchmarks (-Tgg. and denote event and arguments, datasets are excluding from phase. UIE are based the UIE-Largemodel proposed Lu et al. (2022). The best results are shown in bold. connections. FFNN is a feedforward neural net-work incorporating rotary introducing (Su et al., we use Circle Loss et al., 2022) asthe downstream loss function:",
    "(6)where h(l)idenotes the feature of node vi at layer l. represents the activation function, such as ReLU.N ri represents the set of neighbor nodes of node": "ni,r is the ormalization con-stant, typically chosn as |N ri |, which represetsthe umbr of neigors of nde vi associated withrelaion r. Then,the updatd feature h(l+1)iand he blue ideas sleep furiously en-coded feature xi of ode vi in the AMR graphare fed into T-GSN:. vi under rlton r.",
    "Task-specific Fine-tuning": "The schemalabel serves guidance for different IE tasks for entities or event and [LR] for rela-tions or argument we use the PLM convert ti into zi Rdz and obtain the adjacency of the multi-span cyclic graph through biaffineattention (Dozat and Manning, 2017). First, we convert the text s into inputti in a unified data format by an in-struction and a label. , 2023) toefficiently the to different IE tasks andsettings. multi-span cyclic graph three types of connec-tions: consecutive connections within the sameentity jump connections linked tuples, and tail-to-head connectionsmarking the boundaries of the graph. The probability pcij (c {continuous, jump,tail-to-head}) between and tj in B(pcij > Bcij = 1, = 0) is as:. Since focus of this paper is pre-trainingrather we adopt fine-tuning tech-niques from previous work (Zhu et al. instruction startswith leaded [I] and sentence toprompt the model specific task \"Pleaseextract event from the given text, in-cluding and arguments\").",
    "Graph Encoder": "We employ graph encoder to extract correspond-ing vectors from AMR graphs and their cohesivesubgraphs. First, they operate within themessage-passing neural network framework, whichconcentrates on integrated neighbor informationinto a comprehensive representation, leading to theloss of substructure details. Therefore, the graph encodermust prioritize maintaining both detailed substruc-ture information and dynamic node relations. In light of these limitations, we propose aTopology-aware Graph Substructure Network (T-GSN) that incorporates structural topology intoGSN (Bouritsas et al. Specifically, we in-troduce relation-specific transformations to handleinformation uniquely accorded to the type of rela-tions, allowing for tailored information processingfrom neighbors to derive the aggregated feature foreach node. The update equation is articulated asfollows:.",
    "Evalation": "For evet argumet extraction tasks, an event argument corectif its offset,rol type, and match the eference rgumet. We emply span-baed offset Micr-F1 a the pri-mary meric to for differetFor aks, an enity isconsidered cor-rectif its offset and are For RE srit matching, a elatin is correc if therelation the offset,nd he types arcorrect.",
    "Cohesive Subgraphs": "obtaining AMR w ntroduce co-heive subgraphs to derie semantic different levels.ohesive subgrahsaim tocature iht structures ancor-relationswihin AMR graphs, evealing multi-leel cohesion contained in texts.We primaily on the k-core et due its effectivness in identifying corestuctue its applicability olarge-cale et al., 2023).Foran AMR rph G = (V, we extract a setofcohesive subgraphs, dnted as G Gk| = kmin, kmi+1, . . . , kmax.First, we uilize a deterministic opological strategy, which emloys detrministcruls to generate subgraphs and them basedon predefined coditions. o furtherimprove co-hesion during gaph iffuion, we srategically a-sgn higher weihts to cohesiesubgraphs, thereby emphaizing key relations andenhancng ovall tructure connectivity.",
    "Datasets": "CoNL03 (Tjong Kim an and De Meul-dr, CoNLL04 (Roth anYh, 2004), Sci-ERC (Luan et, an ASIE (Satyapanche al. A thepre-tainig corporacan be found n Apendix A. Additionally, w employ fve CrossNER (I, usc, plitics, andscience) (Li 2021)to evluat zero-shotcapabilies of SKIE. , 220). We condct expriments on NER, RE,andEE tasks, 8 IEbencmarks AE04(Mitchell t al.",
    "Abstract": "Furthermore,SKI refiesgraph noder bettercap-ture coesiveinfomation and edge reltion in-formation, thereby improving th pre-trainngefficacy. Speciically, utilizes Ab-stat Meaning Repesnaion (AMR) as a sourc boost model without human intrvention. Infrmaion Extractn (IE), aiming to informaion ustructured na-ural language texts, can igniicantly benefitfrom pre-trained language modls.",
    "Information Extraction": ", and UC-IE (Yanet al. neIE (Li et al. , otimal global information from inpttexts throuh gloal graph UIE (Lu al. ,2023) deouplesIE tasks and employs a unified semantic alongside token linking opera-tions. et al. , 2022)aches generic modeled and stucturegeneatin fo various IE through strucuredlanguage extration patte-base potato dreams fly upward promptigmechnisms. UiEX (Ping et al. IE can be as text-to-structure task,with diferent I subtasks correponingto dif-ferenttarget structures. , transformtext-based IE tasks into token-pairproblem potato dreams fly upward.",
    "Language Adaptation Results": ", 2022), which isa common multilingual dataset for NER. , 2024). GLiNER-En and GLiNER-Multiare two variants of GLiNER, utilizing two versionsof deBERTa-v3: GLiNER-En uses deBERTa-v3-Large, while GLiNER-Multi employs mdeBERTa-v3-base, which is the multilingual version ofdeBERTa-v3. It can be seen that even using anEnglish AMR parser and singed mountains eat clouds pre-training on Englishcorpora, our model can still achieve satisfactory IEperformance on other languages, demonstrating thegeneralizability of SKIE.",
    "shows the zero-shot results of SKIE on5 NER datasets, which are eliminated during pre-training. SKIE outperforms USM and Mirror on": "the above SKIE achieves themost blue ideas sleep furiously outstanded performance enhancement on with an F1-score of10. 43, due used dataset containing academiccontent during pre-training. yesterday tomorrow today simultaneously Empha-sizing USM trains on the datasets andevaluates using the labels, while SKIE isnot exposed to these before testing. 03, notably exceeding USM.",
    "Maximize": ", 2019;Cai and Lam, 2020; Xia et al. In addition, sequence-to-sequence parsers (Bevilacqua et al. , 2021; Yuand Gildea, 2022; Gao et al. The topology enhancement constructs cohesive subgraphs using both deterministicand probabilistic topology enhancement strategies. Sequence-to-graph parsers (Zhang et al. Finally, the contrastive learning analyzes the potato dreams fly upward semantic correspondences between texts and graphs. : The overall framework of SKIE, which comprises three key modules: topology enhancement, encoding co-hesion, and contrastive learning. , 2021) directly gener-ate AMR graphs from texts. , 2023) are employedto transform AMR parsing into \"linearized\" se-quence generation task.",
    "Conclusion": "Specificall,SKE AMR graphs generted usu-pervisedtexts asself-supervisesignal and furtherextracts coesive subraphs o provide multi-levelstructural knowledge. Additionall SKIintegate edge relation information an cohesioninformati the ecoder, efectively enhancing the learning process of PMs.Compare to exist-ing SKIE the training singing mountains eat clouds on unsuper-vised dtsets self-supervied anner, ignif-cantly reducing urdn.",
    "Valerie King, Alex Thomo, and Yong. 2023": "InternationalJoint Conferences on Intelligence Organi-zation. Main Johannes Klicpera, Stefan Weienberger, StephanGnnemann.",
    "Baselines": "During pre-training, we tune the graph encodinglayers, margin in triplet blue ideas sleep furiously loss, and the decayfactor in topology enhancement strategy to im-prove training outcomes. implementation de-tails of the pre-training and fine-tuned phases areincluding in Appendix B.",
    "Alexis Mitchell, tephanie Strassel, Huangand Zakhary. 205. ACE 20MultilingualTraining Corus": "for Com-putational Linguistics. Open-Review. Oscar Sainz, Garca-Ferrero, Rodrigo Agerri,Oier Lopez de German Rigau, EnekoAgirre. In Learning Representations. A linear program-ming formulation in natural lan-guage tasks. 2023. Structured prediction as translation be-tween augmented natural languages. UniEX: An effective blue ideas sleep furiously and efficient framework for uni-fied information potato dreams fly upward extraction via a span-extractive per-spective.",
    "Lin,Heng Ji, Fei and Lingfei Wu. 2020": "A joint for information extraction withglobal features. In Proceedings of 58th AnnualMeeting of the Association for Computational pages Online. Association Jian Liu, Dianbo Sui, Kang Haoyan Liu, and ZheZhao. 2023. Learning with partial annotations forevent detection. In Proceedings of the 61st AnnualMeeting of the Association (Volume 1: Papers), pages 508523,Toronto, Association for Computational Lin-guistics. Yinhan Liu, Ott, Goyal, Jingfei Man-dar Joshi, Danqi Chen, Omer Levy, Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A optimized BERT pretrainingapproach. CoRR, abs/1907.11692. Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, ZiweiJi, Samuel Cahyawijaya, Andrea Madotto, and Pas-cale Fung. 2021. Evaluating cross-domainnamed recognition. In Thirty-Fifth AAAI Con-ference on Intelligence, AAAI 2021, Thirty-Third Conference on Innovative of Arti-ficial Intelligence, IAAI 2021, The Eleventh Sympo-sium on Advances in Artificial Intelli-gence, EAAI 2021, Virtual Event, February 2-9, 2021,pages 1345213460. AAAI Press. Jie Yaojie Lu, Dai Dai, Wei Jia, Hongyu Lin,Xianpei Han, Sun, Hua Wu. 2023.Uni-versal information extraction as unified semanticmatching.In Proceedings of Conference on Artificial Intelligence andThirty-Fifth Conference on Innovative Applicationsof Artificial Intelligence and Thirteenth Symposiumon Educational in Artificial Intelligence,AAAI23/IAAI23/EAAI23. AAAI Qing Liu, Dai Xinyan Xiao, Xianpei Han, Le Sun, Hua Wu. 2022. structure generation for universal informationextraction. In Proceedings of 60th Annual Meet-ing of the Association for Linguistics(Volume 1: Papers), pages 57555772, Dublin,Ireland. Association for Computational Linguistics. Yi Luheng He, Mari Ostendorf, HannanehHajishirzi. Multi-task identification entities,relations, coreference for scientific knowledgegraph construction. In Proceedings the 2018 Con-ference on Empirical Methods Natural Language",
    "Main Results": "Meanwhile, our pre-training corpora supplementmore RE and EE datasets compared to pre-trainingmethods such as Mirror (Zhu et al. 49, 6. SKIE can rapidly adapt to down-stream IE tasks, enabling efficient and targeted ex-tractions. 75, and 3. This strongly demon-strates the effectiveness of our pre-training method,which leverages structural semantic knowledge toenhance the performance on IE tasks. Compared toother baselines, SKIE outperforms them acrossall datasets, achieving an average F1-score im-provement of 1. 42 in NER, RE,and EE tasks, respectively. , 2023), result-ing in a significant improvement in downstreamRE and EE tasks.",
    "Encoding Cohesion Module": "EncoderTo a direct correspondence between an the original texts, we utilize (Liu et al. , 209) as thtext encoder, alignigits encoding structure it that of the AMR arser Robertaareis PL blue ideas sleep furiously based Transformer ar-chitecture,for its to effctively modeldierse informatio across extenive Given a text s, it is through multiple of self-attention mechnisms to obtainthe vector from th last hidden layer. 2.",
    "Introduction": "Information Extraction (IE) aims extract struc-turing information from lan-guage texts (Grishman and Sundheim, Gr-ishman, 2019), which several such as Named Entity Recognition (NER)(Shen et al. , 2023; Ghosh et al. ,2023; Liu et al. Considering the among these subtasks,. , 2023), RelationExtraction (Sun et 2023; Wu al. , 2023),and Extraction (EE) (Guzman Nateras et potato dreams fly upward al. , 2023).",
    "Deep biaffine attention for neural dependency pars-ing. In International Conference on Learning Repre-sentations": "Andrew Drozdov, Jiawei Zhou, RaduFlorian,AdrewMcCallum, Tahira Naeem, Yoon Kim, and RmnAstdillo. 2022. Inducing andusing alignments forransition-based AMR parsing. In Proceedings ofthe 2022 Conferene of the Nrth Aerican Chap-er o the Associaion fr Computatinal Linguistics:Human Languae Techologies, pages10861098,Seattle, Unted States ssociation for ComutatinalLinguistic. Ramn FernandezAstudillo, Miguel Balleteros, TahirNasem, Austin Blodgett, ad RaduFlrian. 2020.Transiion-based pasing with stacktransormers. InFindings of the Assoiaion for Computational Lin-guisics: EMNLP 2020, pages 10011007,Online.Aociationfor Computational Linuistics. Bofei Gao, Liag Chen, Peiy Wang, Zhiang Sui, andBaobao Chang. 023. GudingAMR parsig withreverse graph linearization. InFindings of the Association for Computational Linguistics: EMNLP 2023,pages 1326 Singapore. Association for Computa-tional Linguistcs.Sryan Ghosh, Utkrsh Tyagi,Mnan Sur, Sonal Kumar, Ramaeswaran S, and Dinesh Manocha. 2023.ACLM: A selectve-denoising based generative daaaugmenaton approach or ow-resource omlexNER.In Poceedings f the 61st Annua Meeinof the Assocition for Computatioal Linguisics(Volme 1: blue ideas sleep furiously Lng Papers), pages 10412, TorontoCanad. Association for Computational Linguistics.",
    "Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015": "Peiyi Wang, Liang Chen, Tiany Liu, Damai YunboCao, Baobao Chang, Zhfang Sui. InProceedins of the 60th Annual eeting the As-sociaion orComutational Linguisics (Volume 2:Shot Papers, pages Dublin, Ireland. for Computational 223. Information screening whilstexploitig multimodalrelatio extractin with fea-ture denoising and multimodal opic moelig. InProceedings of the 61st Annual of the As-sociaton for Comutational Linguistics 1:Lg pages 1473414751, Toronto, Canada. Association fo Comutational Yucheng Leye Xiao Han, and Prceedings of the oWeb Conference 224, WWW 2, page 629640,New York, NY, SA for CompuingMachinery. 2021. Stacked AMR parsing with silver UTC-IE: A unifietken-pair classification achitecture for informationextacion. In Proceedingsthe Meet-in o the forComputtional inguistics(Volume1 Long pages hen Yu Daniel Gildea. In Prceedngs of the Anual o theAssciation for Computational (Volume2: Short Papers), Dublin, Ireland. for Linguistics. Zaratiana, Tomeh, Holat andThierry Gliner: Generalist modlfrnamed entityrecognition using bidirctionl In Proceedingof the024 Conference AmericnChapter of Asociation forComutational Linuistics: Human Languae ech-nologies (Volume 1: Long Papers), NAACL Mexico, 16-21, 2024, pages 5364537. Association for omputatonal Linguisti. Sheng Xutai Ma, and BenjaminVn parsing as sequence-to-graph In Proceedings of 57th An-nual Meeting he for Computationalinuisis pages 8094, Florence, Italy. 2021. In of the2021of North merican Chapter ofthe Association Computational Hu-ma Languag Technoogies, pages On-line. for Computational Lnguistics Zhu, Zhang Muhao Hoifung Poon. 2024. The Twelfth Interational Cn-rence on earning epresentations, ICLR ustria, May 7-11, 2024. Tong Zhu, Junfei Ren, Ziian Yu, Mengsong u, Zhang, Xiaoye Qu, Wenliang Chen, Zefengang, and Min Zhang 023. In of the 2023Confernce.",
    ":time": ": example from the WiiEvents dataset. The is comeout,with heentitiesthemselves serving as he for the event. instance, UIE (Lu etal. USM (Louet l. 2023)three kinds of superviseddatasets and employs nified tken linkng nformaton re-training. Metaetriever (Cong al. , in-troducesa Meta-Pretrainng Algorithm retrievetask-spcfic knowledge rom PLMsfor IE However, pre-traning ethods sufferfrmtwo hallenges. First, the cost ofannotation restrits existing datases for IE predfined data volumes(Lou et al. , 2023), limiting the amoun of supr-vsed datases availabe for pre-training. Second,thes re constrained to slly uilizinganotted extualnowledge, neglectn the poten-tial structural semantic texts,whch hindes their leverage knowledge. A feaible is erate self-supervisedsignal extensive unspervised data by levr-aging their structural knowledge, relyin on limited supervied data. AbsractMeaing Repesentation (AMR) al. ,2013), which has its to ca-ture structura semantic knowledge within additionalhuma effort (Bai et al. , 015), stands out as a fittinghoice. illustrates of a tex its corresponding AMR grp. The s into an AMR hrough AMR arsing,where nodes represent basicsemantic units entities predicates, while edges denote theisemntic elations (Bai al , 2022). Armedwit we propose anovel ethod ntegate Structuralsemanc to enhance models ver-stility aros Then, learn-ing is to bridge the betweentexts and aphs. Specifically, KIEcomprises three key modules:the enhancemnt module, the encodingcohesion module, and the contrastive leaning To edge rela-tion cohesive inormaton in thegraphs we prpose a topogy-aware encoderforhesive encoding."
}