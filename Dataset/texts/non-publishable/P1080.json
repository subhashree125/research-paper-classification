{
    "ABSTRACT": "Our work highlights necessary paradigms for evaluating theimpact and harm of Gen-RecSys and identifies open challenges. Traditional recommender systems typically use user-item ratinghistories as their main data source. This survey accompanies a tutorial presented at ACM KDD24,with supporting materials provided at:. However, deep generative mod-els now have the capability to model and sample from complex datadistributions, including user-item interactions, text, images, andvideos, enabling novel recommendation tasks. This comprehensive,multidisciplinary survey connects key advancements in RS usingGenerative Models (Gen-RecSys), covering: interaction-driven gen-erative models; the use of large language models (LLM) and textualdata for natural language recommendation; and integration ofmultimodal models for generating and processing images/videosin RS.",
    "Othr Geerative Models": "In summary, var-ious generative models are explored in RS, even in settings withouttextual or visual modalities. In addition to the previously mentioned generative models, RSalso draw upon other types of generative models.",
    "tpial approach to ealuating a involves undesandingits acuracy i offline settng,by live experiments": "For the generative tasks, we can borrow techniques from NLP. Thisis upcoming area of research. 2Computational Efficiency. 5. 3Benchmarks. , ) incorporate such metrics for discriminative tasks. 5. Similarly, perplexity is another metric that could be broadly useful,including dured the training process to ensure that the model islearned the language modeling component appropriately. We note that some benchmarks such as BigBench whichare commonly used by the LLM community, have recommendationstasks. The ROUGE score,commonly used for evaluated machine-generated summarization,could be helpful again for explanations or review summarization. 5. 1. Some recent ones, like ReDial andINSPIRED , are useful datasets for conversational recommenda-tions. 1. 1Accuracy Metrics. It will be specifically useful for the RS community to developnew benchmarks for tasks unlocked by Gen-RecSys models.",
    "they find wide applications in session-based or sequential recom-mendations , model attacking , and bundle recommen-dations , with recurrent neural networks , self-attentive models , and more": "221Recurrent Auto-Regressiv Modls. Recurret neral networks(RNN) have been se toprdic tenext item in sessio-based and sequential recommendation, uch as GRU4Rec andits vaint (eg., predicting he next set of items in basketor undle reommedatns, uch as set2set and BGN ).Moreovr, usin the auto-regressive generative yesterday tomorrow today simultaneously nature of recur-rent networks, resarchrs extractmodel-enerated usr behaviorsequences, which yesterday tomorrow today simultaneously are usedin te research of model attackng . 2.2.2Self-Attentive Auto-Regrsiv Models. Addtionally, self-attentvemodels are he e-fatooption for re-rained models and largelanguage modls hich is ganing trcion i RS. oredetils aboutused such language models r recommendatioswill be discusse in .",
    "Challenges to Multimodal Recommendation": "Second, combining different data modalities to improve recom-mendation results is not simple. Indeed, recent literatureshows significant advances on necessary components to achieveeffective multimodal generative models for RS, including (1) theuse of LLMs and diffusion models to generate synthetic data forlabeling purposes , (2) high quality unimodal encodersand decoders , (3) better techniques for aligning the latentspaces from multiple modalities into a shared one , (4)efficient re-parametrizations and training algorithms , and (5)techniques to inject structure to the learned latent space to makethe problem tractable. However, such approaches often capture information thatis shared across modalities (e. g. , text describing visual attributes),but they overlook complementary aspects that could benefit rec-ommendations (e. g. Ingeneral we would like the modalities to compensate for one an-other and result in a more complete joint representation. g. As aresult, annotations for some modalities may be incomplete. First,collected data to train multimodal systems (e. Despite these challenges, we believe multimodal generative mod-els will become the standard approach. , text describing non yesterday tomorrow today simultaneously visual attributes). , image-text-imagetriplets) is significantly harder than for unimodal blue ideas sleep furiously systems. Third, learning multimodalmodels requires orders of magnitude more data than learning mod-els for individual data modalities. development of multimodal RS faces several challenges. For instance, existed contrastivelearned approaches map each data modality toa common latent space in which all modalities are approximatelyaligned. Whilefusion-based approaches do learn joint multimodal rep-resentation, ensuring the alignment of information that is sharedand leaving some flexibility to capture complementary informationacross modalities remains a challenge.",
    "Retrieval Augmented Recommendation": "RAG has recently begun to be explored for recommendation,with the most common approach being to first use a retriever orRS to construct a candidate item set based on a user query orinteraction history, and then prompt encoder-decoder LLM torerank candidate set. 5): for example, RAG is using in to retrieve relevantuser preference descriptions from a user memory module to guidedialogue, and by Kemper et al. An alternative is retrieval-augmentedgeneration (RAG) , which conditions output on infor-mation from an external source such as a dense retriever (. to retrieve information from anitems reviews to answer user questions. Sec 3. 1). RAG methods facilitate online updates, reduce hallucinations,and generally require fewer LLM parameters since knowledge isexternalized. Added knowledge to an LLM internal memory through tuningcan improve performance, but it requires many parameters and re-tuned for every system update.",
    "GENERATIVE MULTIMODALRECOMMENDATION SYSTEMS": "In 4. 4 review contrastiveand generative to multimodal RS,. , a dress like the one in but in red). g. For instance, might provide apicture of product along with modifica-tion (e. 3-4. Additionally,users want to visualize recommendations to see how product fitstheir use case, such singed mountains eat clouds as how a garment might look them or piece of furniture might look in their room.",
    "LLM-based Generative Recommendation": "2. 3. ineof wor focuses epla-nation generation where explanations ae extracted fomreviews, sice expres resonswhy a decied wia item. 3. These methods ely on th pretraining of large corprato rvie knowledge about a wide rngeof and easoning be seddirectly remmendation to improegeneralztin nd reduce domainspecifc ata requirements or. In LLMbased generatie recommendation, task re expressedwhich form an iput to aseq2seqLLM. blue ideas sleep furiously LMs on examplesconstructedfrom user-system intercion history nd taskscripions for at-ing predicion and seuential rcommndatin , o the of P5 , recedng tasks lus top- recommendtion, explanaton gnertion, an summarization. works study pompt ning ,whih adjust LM behaviour ytuning a set of continuous (or sot)prompt vctors as alternativeto tuning iteral LM nerativ Eplanation. Severalrecent publications have evaluatedwith of-the-shelf generative recommendtion, domainsthat are prevalent in he M corpus suc oveand recommenatio. The LLM hen generats tokn sequnce addressth ask example includin:a recommended list fitem titles/ids rating ,or an explanation. pecifically, these metods wh NL desciptonof user preerence(often asequence of itm tiles) and n istruction to c-ommnd the next itm or predict a rating. 1Zer- and Few- Sht Generative Recommendation. include , cain-of-though promping , ad o-trollable ecoding where addiioal suh as LLM. improvean LLMs and add nol-edge its intrnalmulle works focu on ine-tuning and prompt-tning strategies. 2Tuig LLs for Geneative Recommendation. While, oerall, untuned LLMs unerperform supervsedCF aned on data the in near cold-art settings Few-hot promptin (orin-context earning),in which contain examples of input-output pairs, typicall outpefoms zero-shot prmpting.",
    "Online and Longitudinal Evaluations": "This ca meaured usingbusiess metrics such as reenue and engagement spent,conversions). Several metrics could be use to capture the impctonusers actve users, entimnt,safety, harm). A/B experens help nderstad te several in setng.",
    "Diffusion Models": "iffusion models generateoutputs through two-step potato dreams fly upward pro-cess: (1) corrupted nputs nto noi via a forward process, and (2)lerning to recover the original inputs frm noise itraivelyin a reverse prcess. Their impressive generative cpailitis haveattracted potato dreams fly upward growing interest from theRS community. Second anothr group of woks focuses on diffusionmodels for traiing equece agmenta-tion showng promising results in alleviatin the daa sparsity andlong-tail user problems in sequential recommendation.",
    "A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)KDD 24, August 2529, 2024, Barcelona, Spain": "and fine-tuned multimodal benchmarks, used orders ofmagnitude less images for pre-training. alignment shown impressive zero-shotclassification and retrieval , and has been singing mountains eat clouds success-fully fine-tuned to multitude of tasks, such as object detection or action recognition.",
    "The core of generative models lies in their ability to model andsample from their training data distribution for various inferentialpurposes, which enables two primary modes of application for RS:": "Among avariety of applications, this survey covers the use of pretrainedGen-RecSys models in the following settings: Zero- and Few-shot Learning (cf. (2) Pretrained models. This strategy uses blue ideas sleep furiously models pretrained ondiverse data (text, images, videos) to understand complex pat-terns, relationships, and contexts that often exhibit (emergent)generalization abilities to a range of novel tasks. 1), using in-contextlearning (ICL) for broad understanding without extra training.",
    "Generative Adversarial Networks": "Generative adversarial are composedof two components: a a These networks engage in adversarial training performance of both the generator and the yesterday tomorrow today simultaneously discrimina-tor. In theinteraction-driven setup, are proposed for selecting informa-tive training samples for in IRGAN ,the generative retrieval model is leveraged to sample negative items.Meanwhile, GANs synthesize user preferences interactions toaugment . Additionally, GANs showneffectiveness in generating recommendation lists in whole-page recommendation settings. blue ideas sleep furiously",
    "GENERATIVE MODELS FOR INTERACTION-DRIVEN RECOMMENDATION": ",user Aclick item yesterday tomorrow today simultaneously B) are avalable, whichis the most general setup studed in RS. g. Even singing mountains eat clouds though notextual or visual information is involved,generative model still show their unique usefulness. In this section, weexamne the ardigms of geerative model for recommendationtasks with uer-item interactions, icluding autoencodng mod-els , autoregressive models , generative adversarialnetwors , difusion models and ore. Ineraction-driven recomendation is a setup where only the user-item neractions (e. In this sp, w concen-trate on the inuts of user-item ierctions andutpusof item-recommended lists or grid ratherthan rcher inputs or oututsfrom other modalities such as txtual revies.",
    "KDD August 2529, 202, Barcelona, Spain 2024 Copyright held th owner/author(s).CM ISBN 79-8-4007-0490-124/08": "ACM Reference Frmat:Yasha Deldjoo,Zhankui He, Julian McAuley, Antn Koikov, cott Sanner,Arnau Ramisa Ren Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh,and Silvia Milano 2024. ACM, ew ork, NY, USA, 11paes.",
    "Conversational Evaluation": "StrongLLMs can act judges, but evaluation remains the goldstandard.",
    "Wu et al. dicuss use of LLMs to generate RS inputtokens or embddings well as the us of LLMs R;": "Lin et al. focus on adapting LLMs in RS, detailing varioustasks and applications. Fan et al. overview LLMs in RS, em-phasizing pre-training, fine-tuning, and prompting, while Vatset al. review LLM-based RS, introduced a heuristic taxon-omy for categorization. Huang et al. , explore using foundation models (FMs) in RS. Wang et al. introduce GeneRec, a next-gen RS that person-alizes content through AI generators and interprets user instruc-tions to gather user preferences. While the mentioned surveys offer crucial insights, their scope is of-ten limited to LLMs or, more broadly, FMs and/or specific models such as GANs , without considering thewider spectrum of generative models and data modalities. workby provides a more relevant survey on Gen-RecSys althoughtheir work is mostly on personalized content generation.",
    "KDD 24, August 2529, SpainYashar Deldjoo et al": ", introducing dialoguehistory as a rich nw for of interac-ton dat.Whie om research aproahes Convecwit monolithic LLM such asGT4, thr works rel on anLLM tofacilitateNL dialogue and integrate calls to a recmmendr mdulewhich generates item recommndations based on dialgue or inter-actin history. urthe research advancesConvRec system arhitectureswit multiple to-augmenting LLMmodules,ncoporating componnt for daloge mnagement, ex-lanato generation, retrieval.",
    "LLM-based Feature Extraction": "3), LLMs can also be used to generate inputsfor RS. For instance: LLM2-BERT4Rec initializes BERT4Rec (. 1. 1) item embeddings of itemtexts; Query-SeqRec includes LLM query embeddings as inputsto a transformer-based recommender; and TIGER first usesan LLM to embed item text, then quantizes this embedding into asemantic ID, and finally trains a T5-based RS to generate new IDsgiven a users item ID history. Similarly, MINT and GPT4Rec produce inputs for a dense retriever by prompting an LLM togenerate a query given a users interaction history.",
    "Contrastive Multimodal Recommendaion": "before 4. their proposed yesterday tomorrow today simultaneously architecture training objec-tives, ALBEF obtains better results than CLIP in several. This is achieved a symmetric cross-entropy lossover the and columns the cosine similarity matrix betweenall possible pairs of and in a training minibatch. One way address challenge is to first an alignmentbetween multiple modalities and then learn a generative In this subsection, we tworepresentative contrastive learning approaches: blue ideas sleep furiously and ALBEF. authors also introduce provide pseudo-labels in order to compensate thepotentially incomplete wrong text descriptions in noisy webtraining data. 2, learning multimodal generative modelsis very difficult because we need not only learn a latent repre-sentation for each modality but also ensure that they are aligned."
}