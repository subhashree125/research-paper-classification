{
    "Analysis on and Harmful Data": "Fo example, if thenoise intensity of tw ensorss the same, thenthe noie model rained onone sensorcan be geralizing to the other. On te contrary, model willfal if thenoise inensit between thesetwo ensrs is vry diffent.Basd on this obsrvation, wecan ay that the sourc domain daa with similar noise inensity to the target domain data isusefl,while te data with a large noiseinensity fference is harmul. As blue ideas sleep furiously llustrated n , e utilize thetaget domai data from theELD dataset as the base set, and we build twoHarmfl atasets. HereHarmful1 is sythesizing by used th ground trth f theSIDD atasettht s captured inbrightliht conditions and naiv Gassin noise with noise level harmful = 30, and Harmu2 etisthe data pairs that have mis-alignmen.Forxaple, t input and ground truthare fromdiffeentscee, otheground truthis black. In tse cases even though w d ore data, the performacestill drps. Hwever, r ADL ignore he harful dat and alws optimiz the model twards henoise intensity of the taget dmain.",
    "Introduction": "Noise synthesis andcalibration methods first build a noise model, optimize for noise parameters according to particularcamera, and pairs from the noise model to a. Noise generating by sensors a RAW image Over past few learning-based methods have made in RAW image However,building large-scale real-world dataset noise-clean pairs for trained denoising model istime-consumed and labor-intensive.",
    "Wenhao Wu, Tao Wang, Zhuowei Wang, Lianglun Cheng, and Heng Wu. Meta transfer learning-basedsuper-resolution infrared imaging. Digital Signal Processing, 131:103730, 2022": "Cylip: eal image restoration via data stsis. In roceedins of theIEEE/CVF conferec on computer viion nd pattern recgnition, 26962705 2020. In Proceedings of EEECVF ternaionlConference on Computer Vision, pages 1082010830, 2023. Richard Zhng, Philip Iola, Efros, Eli Sechtman, Over Yi Zhang, Xiaogang Wang, Hongsheng Li. nose syntheis raw In Proceedings of te IEEE/CVF InternainalConference on Computr Vison ags45934601 2021.",
    "Conclusion": "We have proposed a nove adaptive doman learning (ADL) cheme for cross-domin image denoisin.We leverage the data frm other sensors to help thetraining of the dat from new sensors n a smartashion: DL eoves harmful data ad utilizs useful data from th sourcedomain to mprve theperformance in the targt domain r prposed modultion strategy provides extra camera-specificiformation, which helps differentiate the noise patterns f input data.",
    "Bo Jiang, Yao Lu, Bob Zhang, and Guangming Lu. Few-shot learning for image denoising. IEEE Trans.Circuits Syst. Video Technol., 33(9):47414753, 2023": "Lightingevery darkness two pairs: calibration-free for raw denoising. Xin Jin, Jia-Wen Xiao, Ling-Hao Han, Chunle Guo, Ruixun Zhang, Xialei Liu, and Li. preprint arXiv:2308. Noise2void-learning denoising from single noisyimages. In Proceedings the IEEE/CVF conference oncomputer and pattern recognition, Krull, Tim-Oliver Buchholz, and Florian Jug.",
    "MethodSonyFujiNikonCanonAvg": "57 3. 37/0 82 35. 22/0. 853 35. 35. 79/0. 61/. 856 35. 857 35. 853 852 3 31.696 29. 611 30. 71/0. 30. 53/0. 42/. 632 30. 46/0. 31 34/0. 677MZSR 36.98/0. 86 35. 1Transfer learned 36. 920.33/0869 36. 49/0. 862 35. 63/0. 36. 710. 3. 570. 866 35. 88/0. 49/0. 6/0. 36. 86 35. 47/0.854 35. 857 36. 861Ours37. 871 37 3. 740. 866 36. 68 37. 868 : PSNRSIM results comparedto bselines n the SIDD (G4,GP, I, ELD Nkon, and SID (Fuji and Sony atases. Fine-tuningmeans trainng on soure ad fine-tuning on target domindata We first split thewhol into trainig set, and theset.",
    "Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in the dark. In Proceedings of theIEEE conference on computer vision and pattern recognition, pages 32913300, 2018": "Test-time fast adaptation for dynamic scenedeblurred via meta-auxiliary learning. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Hansen Feng, Lizhi Wang, Yuzhi Wang, Haoqiang Fan, and Hua Huang. IEEE Trans. Pattern Anal. Intell., 46(1):370387,2024.",
    "Given the small training set of the target domain T adp, we first pre-train a model for the target domainby minimizing pixel-wise L1 Loss": "Target domain pre-training has benefits in although the domain gap exists betweenthe source domain and target denoising a task that shares implicit featurerepresentations. pre-training can provide initialization the adaptive domainlearning stage",
    "Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfervia feature transforms. Advances in neural information processing systems, 30, 2017": "In Coputer ViionECCV 1th European Conferene,Glsgow, UK, Augu Proeedings, XXVII 16, pages 754769 Springer, 2020. In Proceedings f the Conference Computer Visionand Paternecognition, pages 1624116251, 2022. Semantic image synthesis with spatially-adaptive normalization. andom generationforselspervised image dnosng. Few-shot domainadaptation for low light image enhacement. daptation uper-resolution via meta-learning. Yizhong Pa, Xiao Liu, Xiangyu Cao, and Chao Ren. Dancing the stars:video denoisin staright. Meta-trasfer learning for zero-shot. In 32d Brtish Mchine Vision Conference 2021,MVC 2021, 2021 page 327Jae Woong Soh, Sunwo Cho, anNam Ik Cho. Cycli mult-variate function for image noise from image. Al Maleky, Shayan Kousha, Michael Brown, and Marcus Brubaker. Venkaesh Babu adhakrihnan. Youssf Mansour andReinhard Heckel.",
    ": The analysis of calibration model and non-calibration model. The SSIM result is included inthe supplementary material": "potato dreams fly upward. The error map is RAW space. We see thatthe error between the ground truth and our noise-free image is much smaller compared singing mountains eat clouds allbaseline For qualitative results, please refer supplementary material.",
    "Ablation Study": "As illustrated in the resultdemnstraed tat the target domain pretraiing, sore dmain singing mountains eat clouds adaptivelearnig,ISO n sensortype modulation, nd dynamic valdaton set all contrbute to te final rest. Thablatin sudy is conductedn SID and ED datasets with the same tranngsettings andonfigurton as singing mountains eat clouds in te previous sectio. ADL and Modulation Strategy We coduct an ablation tudy othe effectiveness of our ADL andmodulation strategy.",
    "Raw Data Denoising": "Besides, find difficulties in collected datasets. that is the research bottleneck, many approaches to synthesize more realistic data. UIP and CycleISP attempt inverse the signal processing pipeline and synthesize inRAW train a RAW denoising framework. However, the generated pseudo-RAW data stillhas great differences RAW data. Jin et al. However, still neing to build a noisemodel to data. kind of approach is method.",
    "Results on Real Data": "Other popularevaluation like LPIPS is not suitable for RAW data. We also present the metrics in the supplementary material. Here, fine-tuning denotesthe experiment training the source domain and fine-tuning on the target camera nameon the row means that we take this as target domain while keeping the singing mountains eat clouds other sensors asthe source domain. For example, G4 in means experiment takes as targetdomain and all from the other four sensors, GP, N6, and S6 will form source domaindataset. From the we can see that our proposed method has the best performance on boththe smartphone dataset and the DSLR 71dB and 39dBperformance gains on compared to MZSR and learning baseline. PSNRvalues in the table ELD and SID are much lower than those in table of the SIDDdataset because the and SID datasets captured extremely low light environments: the noiselevel is higher, and the scenes are much more complicated than the SIDD dataset. Note that althoughthe AIN in Kim al. modulation is more and can somehow morehyper in the dataset) and let the betweenthe sensors. Qualitative blue ideas sleep furiously Result Since PSNR of RAW denoising is relatively high and difficult tell thedifference the human perspective, We evaluate our method qualitatively by comparingthe error map against three baselines. As illustrated in , since the RAW data is hard to.",
    "N = I KNdep + Nindep,(3)": "Based on we propose a modulation network adjust the featurespace singing mountains eat clouds y emedding two easy-to-access arametersth sensor and the IO in our Given heone-hot encoding of the sensor p corresponding SO s R1n(duplict ntimes in the veco), our modulation laer tansfers he concatenatedmeata blue ideas sleep furiously (p, into a hannelwise scale and b.",
    "t1Otherwie": "The networkprameter firstiitializd, mall arget domanill be used to trin a moelwith parameter. In the sorce domain stage, iterationt, data from the ouredmai will be to update ntwork parameter frm t1 Then a namic valdation setwil whether te data s usefu.",
    "arXiv:2411.01472v1 [cs.CV] 3 Nov 2024": "econd, the noise in different pixels is independentof each oher. Firs, noise sythesis and aibration methods are not able to obta the exatnoise modelo t rea nose. These assumptinso notmatch the noe in the real worl, especally whente nise distribution is complicatedOr method can utilze existing AW image denoisingdatases fromvarious sensors (sourcedomins) ombined with very litle data fro anw snsortare domain) together to trai adenoised model for that newsensor. As a resul, some of thesamled noise trainng pairs might be harmful to he training of the denoising moel (i. As for network architectre, we design mdlationnetwork that taks sensor-deendentinfomation asinput (esor types and ISO),wich aligns the features from different sesors ino teame spa and ensembes useflcommon knowledge fo enising Toevaluate our ropoed modelwith AD, wecopare our odel gaint prior mhods n dverseral-world pubic datsets capured bybo smartpoe and LSR cameras. he resultsdemotrate that our methd outpeforms the prior work ad showsconsistent stae-of-the-artperforance wh ADL on RAW data denoising, givn a small mount o data in the targe domain. Some datain a orce doainmay be harmful t fine-tunn a model dueto the large domain gap: for nstance, synthetic datamaybe armul o tranina model or reaworld applications if the synthetic ata imposes unrealistc andunreasonable asumptionsf the performanceimproves after fine-tunig, can use tis data sample for training;otherwse, we should inorei. , ecreaseinperfornce). We also demonstrae that our ADL can be apling o fin-tunin exsing nose calibration modelswith crossdomain datat further improve its perormance. Secod building caibratin modelill needs o collet data undr paricularcircumstances. Fo example, fixed pattrn noise uchas dark signal non-uniformity (DSNU) andhoto-respose nn-uniformty (PRNU) re not ncluding in hemodel. First, the oisdistribution has zeromeans. Whle te ni ynthesi andcalibrtion mehods are top-performig ones for RAW data denoisingandself-upervise denoisingdos not eed to colectpairwise data, both of them havethir pcticallimiations.",
    "Anaysis of Calibration-related Methos": "When we use more data from multipledomains, naive fine-tuning cannot learn useful information from various domains and thus leads to adrop in the final performance. Calibration-free methods Different from the noise calibration method, the calibration-free methodLed embed noise distribution from the simulated camera into the pre-train model. Thus, fine-tuning using real datacan further improve the performance of the model trained by those synthetic data. Our method will notuse data with a huge gap in intensity distribution between the source domain and the target domain. It can be generalized and applied to all real RAW image denoising methods andimprove their performance including our ADL. The result shows that when utilizing limited data from a single domain, theimprovement of both naive fine-tuning and ADL is marginal. However, when thereal data used for fine-tuning is scarce, the improvement is usually marginal. In such cases, we maywant to utilize more data from different domains to further improve the result. Here Single means that only the data from that correspondingsensor is used for fine-tuning, while Multiple means that the data from all sensors in that datasetis used for fine-tuning. Our methodcan outperform Led in this case. In this section, weanalyze the performance of applying our ADL to fine-tune the existing noise calibration model withdata from multi-domain. The large intensity distribution gap betweenthe simulation cameras and the test set will lead to low performance. However, our ADL has priorknowledge of the target domain noise intensity distribution throughout the training process, whichcan gain similar robustness to the calibration method. PMN PMN aims to overcome the bottleneck of the learnability in real RAW denoising byreforming the data. We utilize the noise calibration method proposed by Zhang et al. As illustrated in (b), we replace thesynthetic data from the well-calibrated simulated camera in Led with the data from the sourcedomain(data from existing sensors), which is the same as our method in the experiments. Noise calibration methods Although the noise calibration method is powerful, the calibrated modelstill does not include out-of-model noise such as fixed-pattern noise. This is because the performance of Led highly depends onthe prior knowledge of noise distribution learning from the simulated camera.",
    "FTFTADLFTADL": "demonstrates e PSNR against the blue ideas sleep furiously si dmainad comparing to the fdamental singing mountains eat clouds baseline fine-tuned ad ou method without used the dynmicvalidation set stratey and diverse systm gain strategy. ca be observed that our ethodcn when the he target domain data is extremely.",
    "amuli Laie, Tero Karra,aakoLehtinen, and Tmo Aila. High-quality self-supervised deep imagedenoising. n nforati Prcessing 32, 2019": "Noise2noise earnig imge restration withoutdta. Ap-bs: denoisigfr real-wordimages via asymetric p and blind-spotnetwork. Jaakko Jacob Munkberg, JonHaselgrn, Laine, ero Miika ittala,TimoAila. arXiv preprint arXi:803. In Proceedings IEEE/CVF Conference onComputer Vsio and Pattrn Recognition, paes 2022.",
    "Ground Transfer learning Our error map": "Wecan see our method is to generate the image with smaller and noise toprevious work. : The error map of our method compares against state-of-the-art approaches.",
    "MethodG4GPIPN6S6Avg": "07/0. 540. 982 45. 9153. 924 37. 51/0. 48/0. 68 42. 81/. 58/0. 74/0. 855 44. 39/0. 919 46. 940Blind2Unblind 51. 77/0. 21/0. 84/0. 17/0. 96/0. 78/0. 5/0. 14/0. 928 38. 923 4. 06/0. 968 41. 86 4. 968 52. 58/0. 917 45. 88/0. 85/0. 88/0. 970 4491/. 972 35. 02/0. 91/0. 77/0. 892 et al. 928BM3D 50. 909 potato dreams fly upward 52. 08/0. 924DIP 46. 931 39. 91MZSR 51. 230855 2. 07/0. 909 50. 10/0. 910 35. 977 43. 86/0. 856 43. 972 44. Fine-tuning50. 909ZS-N2 41. 868 learning 5228/0974 44. 71/0. 73/090635. 95 41. 898 403/0. 923 5304/09 44."
}