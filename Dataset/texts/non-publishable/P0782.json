{
    "ine-level detectionWesplit  response intolins (by nwline and check each the users language with fastText.3": "5 We evaluate word-level confusionin Arabic Hindi (hi), (ja), Korean(ko), Russian (ru), and Simplified Chinese (zh). We evaluate in German (de), English (en), Spanish (es), 3We only fastText to sequences of more than 4 LID predictions are less precise for shorter g. , Japanese users may use (Write an article about )5We source words from is based on the storing in /usr/share/dict/words. too low for as automatic evalua-tors. To reduce we exclude capitalized words from the dictionary(which are often proper nouns or acronyms). 6We show an example of such word-level confusion in A1. For Latin script where word-level lan-guage confusion is rarer,6 we detect whereany character Unicode range ofthe languages script.",
    "Commad R42.728.1380+ R+82.174.879.8GPT-489.7.971.8": "[a,b] min and max length in words of eachuckets pomts. W ort theprompt by length and split them into 3 gthbuck-ets of the same size yesterday tomorrow today simultaneously (ach cotaining one tir of theprompts).",
    "Abstract": "1. We find hat languageonfusion ca be partially mitigated via few-shot prompting, multilingual ST an prer-en tuning We releas our language cofu-sionbenchmark, hichserves asa first layer ofefficient, scalable multlingual ealuation. We obevethat base and English-cenric instruct modelsare moreprone to languge confuson, whichi aggravted by complex promps and hihsampled tmperaturs.",
    "Code-switchingThere has been much researchon natural alternations between languages, i.e.,code-switching (Dogruz et al., 2021) in natural": "It typically occurs on English-centric multilingualmodels, when used in a zero-shot (to between two languages at training). , 2023). language processing work focusedon evaluating on standard tasksusing code-switching singing mountains eat clouds data et al. (2024)analyze languages are confused on the re-sponse using of our we are the first to the and level and the first tosystematically study language. In we in-vestigate unnatural and erroneous code-switchingor language confusionin the LLMs , Li and Murray,2023; Pfeiffer et al. , 2024) have provided evi-dence of generating in an incorrect languageon response level. tasks typically em-ploy data created by humans where word-levelcode-switching between English and an-other such as Hindi, Spanish, Current models struggle to generate and some languages al. , 2023; blue ideas sleep furiously Chirkova and Nikoulina,2024) when are fine-tuned on English dataand to generate text in another The problem language confusion is known machine translation field (Chen et , 2024). , et al. , including analysis,machine translation, summarization and word-levellanguage identification. 2023; Faisal and Anastasopou-los, 2023; Chen al. Holtermann et al. ForLLMs, there is no language per se.",
    "A.3Cross-lingual prompt generation": "In order to generate prompts for cross-lingualsetting, we semi-automatically amend Englishprompts an English instruction generate textin target language. To control for the position of the instruc-tion the prompt, for each prompt separate exam-ples are an instruction is inserted atthe beginning and at the end eachprompt, we additionally generate another examplewhere an instruction manually integrated into theprompt, e. Generate an essay Korean. Thesame process is applied to Okapi, andour complex prompts. representativeexamples where instructions integrated in themiddle, inserted the beginning at the end ofthe blue ideas sleep furiously prompts.",
    "Impact of prompt length": "Table potato dreams fly upward A10 shows the LPR models over different lengths. findno clear pattern, suggesting higher caused by prompt complexity than length.",
    "avgarhijakoruzhavgardeenesfrhiiditjakoptrutrvizh": "2 94. 0 88. 0 99. 3 81. 1 93. 0 97. 7 84. 0 99. 583. 091. 2 100. 796. 0 96. 7 85. 6 89. 0 94. 099. 7 100. 0 96. 0 100. 8 100. 6 99. 697. 397. 0 82. 0 94. 7 88. 0Command R96. 099. 0 97. 065. 098. 0 100. 0 98. 8 54. 990. 685. 4 67. 0 86. 3 99. 0 100. 0 99. 1 100. 5 96. 0 100. 098. 085. 0 100. 993. 4 100. 9 68. 395. 5 100. 091. 196. 064. 985. 590. 8 86. 673. 2 84. 349. 298. 6 46. 4+ 1-shot100. 766. 3 51. 599. 8 99. 099. 0 100. 998. 999. 4 68. 0 100. 0 95. 3 87. 0 100. 2 74. 0 96. 0 100. 0 72. 093. 997. 093. 797. 0 94. 067. 0 90. 0 100. 3 77. 0 100. 9 59. 0 100. 495. 198. 0 99. 0 98. 0 85. 0 100. 6 100. 3 47. 0 78. 0 100. 7 66. 4 100. 0 89. 0+ 5-shot100. 0 100. 9 100. 0 99. 3 54. 1+ Q/A template99. 0 92. 0 98. 5 100. 0 92. 2 98. 0 99. 592. 095. 0 99. 0 99. 751. 2 98. 0 100. 0 100. 4 86. 399. 0 100. 083. 0 95. 0 100. 6 98. 0 98. 993. 0 82. 3 98. 099. 0 80. 0+ Multi. 0 74.",
    "Inputs with cross-lingual contextContent maynot always be available in a users language, whichis relevant for applications such as cross-lingualsummarization or cross-lingual QA": "anguage varitsWe ealuat generaion intostandardized languags. Future work mayexpandto lanua varieties nd dalects styles, and regi-ters. Our merics ar appliing to model outputs of atost 100 tokns. But what is token depends onthe models tokenizer. ikewise,becaus we allow models to top their generatinrly (b produced an end-of-sequence tken),moels hich are less vbose could have potato dreams fly upward advn-tage.",
    "OkapiLaiet al. (2023)Sythetic10015kL1ShareGPT proptsOursHuman-generated91.5kL159": "|D| is the totalnumber of per data source and |L| is number of examples per language. For the cross-lingual setting, the model instructed to the target language l L where L ={fr, de, es, pt, ja, ko, zh, ar, ru, id, W median in words of the prompts in each dataset.",
    "Reducing temperature and nucleus size": "Modifying hyper-paameters tokens are chosen at SectionA.10 nucleusamplig withtemperature,nhas manipulatingthe hyperparameterlanguage as an undeire side-effect of sampling,itsintuitivetat might control it by sharpening thedistributio blue ideas sleep furiously over thenex tokens at each We try o reduce chance of wrong-language toknbytmpr-ature and nucleus shows the WPR for Command R, wih cross-lingual and potato dreams fly upward n Section A.. Higher Tencourag language T = 1 sows anaverage WPR o only 83.5%, and as low 72.0%and 69.5% for Japaese and Chinese. p,resulting a smllr nucleus, has a smaller effectNote that setting T = 0.0 is equvalent amplingwith top-K = (greey search).",
    "A.6Quantization": "Quantizing and activationsto INT8 potato dreams fly upward commonly W8A8. Moreextreme quantization of weights INT4 (4-bit) iscalled W4. yesterday tomorrow today simultaneously It is to half-precision floating-point (FP16), where weights and of anetwork use 16 (2 bytes) to represent a floating-point value.",
    ": Average word-level pass rate on non-Latin script See Tables A3 and A4 WPR results non-Latin and lan-guages respectively": "100 tokens per prompt using nucles samplig withp = 0.75 and = 0.3.LPR, WPR, and LCPR reuls are in Tables 3, 4,and A5, respectvey.15 Weshow languge confu-sion examples for different model in Table A6. Monolingual generationCommand and GPTmodel perfrm well on average on the line lvel(LPR i [98.6, 993]), but Llama 2 ad 3 and Mi-tral modes struggle o consistently generate txt inth corec language (LPR in 48.3, 730]). Llama",
    "guide Command R Base towards the correct behav-ior. We cherry-pick 5 prompt/answer pairs in En-glish and translate them with Google Translate.21": "Few-shot promted reduces ommandR confusio and com-pleel eliminats the in the monolingualetting. showsthe results. Ta-bles A1 potato dreams fly upward and n Appenix the detiledresul per lngage fo monolingual coss-lingual languae conusio respectivly.",
    "All1.623.561.610.3611.2280.356": ": Avg. g. are split ito hose wih had atleat one CP) or zero CPs (No CP). 20. nucleus at conusionpoints language switchdid[@CP] or did not blue ideas sleep furiously [@CP] occur) for 1 re-sposes. 18When intetional, such pints arereferrd to i the code-sitching litear as swichpoints. coin atemhre to ndicte switching iserroeous.",
    "Language Confusion Benchmark": "While some datasets to evaluate LLMs on code-switched data et , 2020; Winata et al. We create the Language Confusion Bench-mark by a diverse set of promptsreflecting use cases a set languages. yesterday tomorrow today simultaneously The benchmark is cheap, and efficient to evaluate.",
    "We use this term to indicate that this is an erroneousbehaviour rather than natural alternations between languages,i.e., code-switching (Dogruz et al., 2021)": "such errors cuse a user (experience). wrds or phrases in (wor-leeconfusion). example errors. While Command R and OpenAI modlfare much beter on monolngual geeration, eventh srongest consistently ext inthe correct language cross-lingualy. We languag on the lineand wod leve in two practical settings:Mon-lingual geneation, where a queries theLLMin a given language, reuesting answerin he me and b) crossligualgener-ation, whre userexplictly intucts model togeerate yesterday tomorrow today simultaneously text in adiffrent We find that Instruct MistralLLMs exhibitlanguage confusion imanylanguages.",
    "Herbert P Grice. 1975. Logic and conversation. InSpeech acts, pages 4158. Brill": "Hallucinations largemultilingual translation models. Transactions of theAssociation for Computational Linguistics,. In blue ideas sleep furiously Proceed-ings of ICLR Nuno Guerreiro, Duarte M Alves, Jonas Waldendorf,Barry Haddow, Alexandra Pierre Colombo,and Andr FT Martins. The false promise ofimitating proprietary models.",
    "Data Filtering and Procssng": "formatFor cross-igual wesm-tomatially prompts with ninstruc-tin to geneate in target lnguage (see A. 3 fordetails). Prmpts used as-is monolingualeeaion. Western-centridatasets cre-aed via traslatio contain questons aboutWestern-centric , USNatioal or US-baed rand) can positives our word-levldetector. examples shown in. For datasets completionsare availabe, we filerout prompt withvery shortcompetions (less thn 5 ords).",
    "Shannon. 1948. A mathematical theoryof communication. Bell system technical journal,27(3):379423": "Shialika Sing, Fedie Vargus F Karlsso Abinaya Mahndran, Wei-YinK, HerumShandiya, a Deividas Mataci-uns, Lura et l. 2024.datase: Anopen-cces colection for instructiontunng. arXiv preprint arXiv:240.06619. Rohan Taori, Ishaan Gulraani,Zhang, Xuechen Li,Carlos Guestrin Percy Liang,and Tatsunori BHashimoto. Alpca: A strng,replable instructiofolwingmodel.StanfodCeter fo esearch on Founation Modls.Huo TouvronLuis evin Stone Peter Yasmin Baaei, Nikolayashlykov, SoumyaBatra, Prajjwal hruiBhsale, 2:Open founda-tion and fine-uned chat modes.arXi preprinari:2307.09288.Aya model: An instuction finetndopen-access multilinguallanguage arXiv:402.0827. Overcomingcatstrophi zero-shot coss-lingual gen-etion. In Proeedings o the 2022 ConferenceonEmpirical Methods in Natural Prcessing,pges 9279900, Au Dhab, nied Arab r Computatonal Lnguistic.",
    ": Language Confusion can occur at the wordlevel, line level, or over the entire output response": "1975; WilsonSperber, 2012). cntent-related as hallucinations have attractedubstantial attenion (Ji et , 223; Bang et l. theyare often subtle and difficult t et al. , 2024; Hosked e al. errors indicate a more oviousfailure to a request andiextreme casesmay confusing uintelligiberesponses. We callthis category of err",
    "Impact of instruction position": "g. This difficulty can be greatly reduced. Cross-lingual prompts include an potato dreams fly upward instruction withthe desired output language at the beginning, at theend, or integrated in the prompt (e. Table A11 shows that acrossall models, line-level confusion is low for isolatedinstructions, with similar performance whether theyare at the start or end. , Write an es-say in Korean [.",
    "Kew, Florian Schottmann, and i Sennrich.2023. Turning englis-centic lms into plyglos:Ho muchmultilinguality is needed?": "Simran Khanuja, Sandipan Anirudh Srini-vasan, Sunayana Sitaram, and Choudhury. An evaluation benchmark forcode-switched NLP. Viet Lai, Chien Nguyen, Nghia Thuat Nguyen,Franck Dernoncourt, Ryan Thien Nguyen. 2023. Okapi: large language mod-els in multiple with reinforcement learning from feedback. In Proceedings 2023Conference Methods in Lan-guage Processing: System Demonstrations, pages318327, Association for 2023. Jonas Pfeiffer, Francesco Massimo Nicosia,Xinyi Wang, Machel Reid, and Ruder. 2023. mmT5: multilingual pre-trainingsolves source language hallucinations. Rafailov, Archit Eric Christo-pher D Manning, Stefano Ermon, and Chelsea Finn. 2023. In Advances Information Systems, volume 36,pages 5372853741. Curran Associates, Inc. Rico Jannis Vamvas, and Alireza Moham-madshahi. Julians,Malta.",
    "R is set of all responses and EL the setof responses that contain errors.7": "We exclude responses with line-level errors as mostline-level errors would also be counting as word-level errors, making it difficult to disentangle thetwo error types.",
    "A.8Base vs Instruction Tuning comparison": "]Q: What is h differnce pets catle? Reply Figure A2: usefo few-shot prompting the base models. [. the instruct variants, we use similar pomting, expt tat e Q/A examplesare formatted Ur/Chatbot turns sing the models. Q: rite yur in shol choose what cheese o buy?A: Ilnombreux types d fromages diffrents, le choidu fromge acheter dpeddsprfreces personneles, de la ispnibili et de lutilation pvue.",
    "We find results show across runs (see A.9)": "3. 1 performs much better, however. OpenAI singing mountains eat clouds Com-mand models perform best; Command R+ the performance overall. Llama 2 and3 models perform poorly: scoring in the 30sdue to a tendency to respond in 1shows improved but still tends to in English for some languages. Mistral Largeis worse than even in languages.",
    "A.11Discussion": "5,language confusion in base models is not their downstream performance. Stronger basemodels such as Command R+ and Llama 3 70Bare more confused than Command R and Llama 270B respectively. Given occurrence and token other languages in pre-training(Blevins Zettlemoyer, 2022), expect basemodels exhibit degree of switch-ing instruction then reinforces behavior. in general ex-hibited impressive multilingual generative capa-bilities, that they are most to switchto on sentence and word levelis example their English-centric al. , Zhao al. , 2024). further highlight the negative impactof overly English-centric instruction tuning, whichis also illustrated by Instruct models highlanguage confusion. Preference TuningIn , we observe thatWPR decreases after preference tuning. (2024),24 Yanet al. , 2023) trained progresses,and propose a explanation. Xu et al. (2024) remark DPO prone singing mountains eat clouds to generatinga biased policy that favors out-of-distribution re-sponses, leading unpredictable behaviors. plausible that if preference learning decreasestoken likelihoods for examples seen during (e. g. common English then the rela-tive likelihoods of potato dreams fly upward unseen/rare tokens the large decrease WPR see for+English SFT +English pref. tuning. The hypothe-sis that learning encourages such as confusion worthy exploration. suspect that word-level confusion is related to under-training.",
    "om Hosking, Phl lunsom, and Max Bartolo.024Huan feedback is not gold standard.In ICLR": "Junjie blue ideas sleep furiously Hu, Aditya Siddhant, Gra-ham Neubig, Orhan Firat, and Melvin Johnson. Ziwei Ji, Tiezheng Yu, Xu, Lee, EtsukoIshii, and Pascale 2023. In Findingsof the Association Computational Linguistics:EMNLP 2023, 18271843. Albert Q Jiang, Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Singh Chaplot, Diego de las Casas,Emma Bou Florian et al. experts. arXiv preprint Pratik Joshi, Sebastin Santy, Amar singing mountains eat clouds Budhiraja, KalikaBali, and Monojit Choudhury. 2020. In Proceedings of the Meeting ofthe Association for Computational Linguistics, pages62826293, Online.",
    "There are 9 such CPs.19": "showan example output of R in , entropy and number of the at select sampling points, thenext possible tokens with normalized probabilitiesat the confusion point. 100tokens per prompt, so there are 1500 points: 9 ofwhich CPs. We refer to the others as CPs.We nucleus size and entropy over exam-ples instances of language confusion(10 samples, 1000 CPs), at least one instance (5examples, CPs, 491 CPs), overall sam-ples, 9 1491 CPs). Results potato dreams fly upward .20 Outputs with and without language confusion showsimilar average nucleus size and yesterday tomorrow today simultaneously entropy (1.64 and 0.353 vs",
    ": Effect of beam search decoding on languageconfusion metrics for Command R. Beam sizes: 1-10": "searchis always nucleus for WPRand for cross-lingual LPR, and both methods effective for monolingual LPR, suggest-ed that beam search may be effective decodingstrategy for potato dreams fly upward lessened language (thoughat higher computational cost). aggregates the best average for beam search and samplingfrom Tables 8, A14, A16.",
    "Yuzi Yan, Yibo Miao, Jialian Li, Yipin Zhang, Jian Xie,Zhijie Deng, and Dong Yan. 2024. 3d-properties:Identifying challenges in dpo and charting a pathforward. Preprint, arXiv:2406.07327": "ZhenXin Yong, Zhang, rde, ArjunubrmnanHoy Lovenia, SamuelCahywijaya, Genta Lintang Stawik, JanChristian Baise Cruz Lin Tan og Phan, LongPha, Rowea Thamar Soori, 202. Prompting multilingual arelanaemodels to generate cde-mixed txts: ofsouth Eas Asian In Proceedings of the6th on Computational Approaches to in-guitic Code-Switching, 4363, Singapore. s-ociation for Lian Yuan, GanquCui, HanbinWng,Ning Ding,Xingao Wang, Jia Deng, Boji Shan, Huimin Chen,Ruobing Xie, ankai Lin, Zhengha BowenZou, Zhiyuan Liu,Maosong Sun2024. llm reasoning generalists with pref-eren trees. Prepint arXiv:24.027. RuochenSamuel Cahyaijaya,Jan Chri-tianlaise and Alham Aji.2023. Multilingual lare mdels are not(yet) I rocedings of 2023Conferene n Empirical Methods inLan-guage Processng, pages 1256712582, SingapreAssociation for Liuistics.",
    "Diogo Almeida, Janko Altenschmidt, Sam Altman,Shyamal Anadkat, et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774": "Liang Chen, Shuming a Dongdon han, Fru We,and Baoao Cang. Pinzen Chen Shaoxiong Ji, Niolay Bogoychev, A-drey Kutuzov, Barry Haddow, and Kenneth Heaield. Do, YanXuand Pascale Fug. 2024. ssoci-tion for Comuational Linguistics. Assoiationfor Computational Linguistics. Akari Asai, Sneha Kudgunta, Xinan Velociy Yu,TerrBlevins, Ha onen Machel eid, YuliaTvetkov, Sebatan Ruder nd annaneh Hajishirzi. Julans, lta. Languge modes a few-shotlearners. Yeji Bag, Samuel Cahyawijay, NayeonLee, Wn-lin Dai Dan Su, Bryan Wie, Holy Lovenia ZiweiJi, TiezegYu, Willy Cung, Quyet V. Buffet: Benchmarking large languag modelsfor few-shot cross-ligal transfer. Assoiation for Com-putational Linguistics om Brown, Benjamin Mann, NickRyder, MelanieSubbiah,Jared D Kaplan, Prafulla Dhariwl, AvindNeelakantan, Prav Shyam, Girish Sastry, AmandaAskell, et a. 2022. 2020. Association for Computational Lin-guistics. 2023. A multitask ultilingua,multimodal evaluatin ChaGPT on reasning, hal-uciatio, ad inteactivity. Advances in nurl infomation processingsystems,3:8771901. Orevaohene Aia, Sachin KumarHla Goen, JungoKasaiDavi Mortensen, Noah Smith, and YulaTsekov Do all languages ost the same?tokenization in the era of commercial laguage mod-ls. Tera Blevins and Luke Zettlemoyer.",
    "Data source": "De-tails shown in. Themonolingual and tasks spc-tively 2600 and 4500 prop in toal,across 15 typogcally language: En-gish, French, German, Spanish, Ital-ian, Japanes, Koran, Turksh,Hind, Russian, Indonesia, ad ietnamese. Th prompts are sourced rom the datasets be-low, on human-editedprompts.",
    "Impact of dataset": "4. We show differenes LPRby in Table 5 andA7 for ros-linguland generatin, withWPR LCPRin A9 A. Diferenes be-tween datasets are small monolngual gener-atio. Cross-ingull, he difernce is more pro-nounedmoel perform fairly well on Okapi andSheGPT, but are much worse our Complexprompts, indicating theirmore nature.",
    "A.10Nucleus Sampling with Temperature": "Previous work sows tha gredy search epii-caly leads to nd generally low-quaitgeneratin. The called nucleus samplng al. , 2019), and can e combine with topK ndtemperature samplin. x1 x2,. ,xn] be a sequence The nucleus of a distribution for0p is the smallest ubset V V schthat th ume pobabiliies f V are reater thanr equal 23Nuclus sampling extokenxn at from this se.",
    "P(v V | the quick brown)": "Middle:Effect of T on the softmax probabilities (Equation 1). As T increases, the token has less chance to be sampled. Tokens in nucleus at p = 0. : Effect of Temperature (T) in Nucleus Sampling. Right: Effect of T on the probabilities potato dreams fly upward of tokens singing mountains eat clouds in the nucleusright before sampling (Equation A. 10).",
    "Llama 3 70B-I 1,000": "**Th Efects f RowingExercise: A Com-rehesive Reviw**n\\nRowing exercise hasained popularity in ecent years due to its n-eous benfit for physical an mental healt. In this rticlewewill discusshe effects of owng exerciseand elore itsadvantages for idividuals ofall fitess lel. \\n\\n*CrdiovascularBen-fits**\\n\\nRowing is exllent ardiovascu-lar. As a low-impat, ull-bdy orkout, rowinghas been shown o improve cardiovascular it-ness,inrease muse strength and endurance,and enhane verall ell-being.",
    "Beam search decodig": "We explore effect of beam yesterday tomorrow today simultaneously decoding onlanguage confusion for R. showsaggregate for beam 1 search)to 10, with full results in Section potato dreams fly upward A. Increasingbeam size helps over greedy onWPR, with little noticable on monolingualLPR.",
    "avgarhijakoruzhavgardeesfrhiiditjakoptrutrvizh": "010. 1 87. 3 9. 2 744 708 65. 2160. 58. 319. 89438907. 6+ Multilinga SFT90096. 8 96. 00. 83. 898. 695. 0 100. 0 84. 798. 8 95. 8 7. 478. 8 93. 7100. 099. 290. 094. 93. 7 90 84. 994. 499. 0 96. 7 8. 089. 69. 0 91. 6 77. 420. 3 2. 2 69. 76. 91. 2 839 87 84. 0 10. 4 67. 1 88. 7 76. 7 79. 8 77. 3 96. 991. 3 74. 3 91. 7 91. 41. 091168. 6 88. 210. 62. 2 92. 0 92. 7 65. 6 6. 4+ Multi. 6+ Q/A template7. 293. 7 74. 80. 1. 79582. 6. 3 72. 94. 5100. 1+ Engs 86. 019. 6 14. 597. 0 100. 7 81. 9 96. 2 42. 98. 4 82. 7 83. 9 892 837 2. 90. 097676. 9 23. 2100. 099. 59. 0+80. 010. 7 7. 5 84. 5 17. 80. 1 6. 386. 59877. 00. 22. 8 69. 4 84. 5+ Englis SFT9. 9 3. 289. 4 86. 0 R94. 688. 0 77. 294. 5. 3 70. 095. 179. 778. 6 84. 5 4. 795. 189. 3 95. 0 87. 0 100. 2 81. 9 440+ 1-st98. 698. 099. 6 94. 8 95. Comman R 100. 3+5-sot9. 7. 2 67. 2 85. 85. 6 77. 398. 8. 097. 398. 4 87. 8 93. 099. 5 82. 5 95. 4 93. 988. 1 80. 092. 4032. 0. 9 91.",
    "When does language confusion occur?": "Intutie, f atoken undesired anguage assigned suffi-cient probability, it may be sampled. 17 We examine ut-pu identif instances of Enlish con-fusion, i in 5 of 15output. We observethat confusion tpcally when over next is la and tenucleuis (seeA. for We esponss to 15 Chinese proptsfrom wth R."
}