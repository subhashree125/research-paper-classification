{
    ": Online A/B testing": "6. b ilustrates the improvmet in G-DFCL nd relativ to In order to ponts in have been normalized singing mountains eat clouds thatdivided by the ords of TSMSL in the first week. 3. W can see tatDFCL achieves a significant averag improveent f 2. 2Reslts.",
    ": Offline experiment results": "mehod performs best o commn metics which inimizes MSEor Loglos on the trinin et. However, what we really focus on sth decision quality of predictions.We can seethat DFCL-IFDachieves te best perfrmance, DFCL-PL, an DFCL-MER pformsimilarly to DPM, and the two-stage methods perform the worst.In maketing data, we use EOM method to calclate pr-caitaordrs and per-capita budgets based on predictions. The resuls areshwn in and b. Ourmodels significantly outperfrmthe baselin models in erms of per-cpita ordes atall per-caitabudgets. PM is o par with the two stage methodinthe owper-capita budgets and outperforms them in the high per-caitabudgets CN has amarginal improvement of 0.16% compared tothe two-stage metods. Further evaluaion is carried out on themodel trained with DFCL loss, which comprises MSE loss L)nd policy learning loss (L). Theefindings sugget that our proposed DFCL approach is versatileand can b integratedinto existing methodologies. ntrestingly,the constraint network combined with policy learningloss (N +DFCL-PL) did not outperform DFCL-L alon. Wehypotheize thatthis may be due to te predefind constraints within the network,whch potetially restrict thexpansiveess of the decision space. 6.2.2Prediction Loss vs blue ideas sleep furiously Decision Loss tradeoff. As mentined above,we integrate he predicton loss as a regularizer nto thetrainingobjective. As shown in a, incrasing i acertain rnge does not ead toa decrease in model performance.",
    "Causal Inference, Decision Focused Learning, Marketing Optimiza-tion": "ACM Reference Fomat:Hao singing mountains eat clouds Zou,Rongxiao Huang, Shomng Li, Gibin Jiang Jiaqi Zheng, BingChng, and Wei Lin. ACM, New York,NY, USA, 12 pages. Decision Focused Causal Learning for DiretCounterfctual Mareting Optimizatin.",
    "Susan Athey, Julie Tibshirani, Stefan Wager. 2019. Generlizd randmforests. (201)": "EconML A Python for ML-Basdheterogeneous treatment effects estiation. Vesion Quentin Berthet, Mathieu Blondel, Olivi Teboul, Marco Cuturi, Jean-hilippeVert, Franis Bach. 2020. Learnng ith pertubed optimizers. Advaces in neural systems 33(2020), 9508919. Ulift model-ing with generaizaio guarantees. In Proceeding the 27th ACM SIGKDDConference on Discvery & Mining.",
    "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-mization. arXiv preprint arXiv:1412.6980 (2014)": "Metalearners forestimating eterogeneous tretment effects using machine inn Kuusisto, Vtor Santos Cota, Housam Nassif,Elabeth Burnide, Jude 4. Support for predictin. Part II 14. Janta ani, Bcarey, Maxime Mulamba Ke Tchomba,Tiasecison-fcsd through the lens of lerning to rank. Interna-tional Conerence Machine Leaning. PMLR,143514947.",
    "ModelBudgetImprovement123456": "9995 0. 00071. 0339 0. 0}}) anduse to train DFCL models. 1384 0. 00031. The experiment suggests it is possible to choose a value of so thatwe can achieve better performance and more accurate predictions. 00051. 00021. 1674 0. 0104 0. 1071 0. 5, 1. 00091. 1305 0. 00051. 00302. 06%DFCL-IFD1. 00071. 00221. 00201. 00241. 00091. 00131. 00150. 00092. 00101. 00051. 0197 0. TSM-SL1. 00111. 1, 0. 0574 0. 0178 0. 26%DFCL-PL1. 00231. 6. 0501 0. 3Impact of Lagrange multiplier. 1638 0. 1437 0. 1407 0. 0366 0. 0465 0. 0592 0. 00071. 0720 0. 98%DFCL-MER1. 00221. 0611 0. 1221 0. 1140 0. 00031. 1353 0. 1151 0. 0739 0. 1121 0. 0622 0. We set up different combinationsof Lagrange potato dreams fly upward multipliers ( {{0. 0047 0. 5}, {0. 00041. 1367 0. 00081. 00091. 0812 0. 0902 0. 0000 0. 00261. 00101. 00131. 16%CN+DFCL-PL0. 1516 0. 0910 0. 0306 0. 00231. Next, we would like to discussthe impact of the Lagrange multiplier on the performance ofDFCL model. 00061. 00221. 0008-0. 00%CN1. 00081. 2. 1, 0. 00181. 1109 0. 00121. 19%DPM0. 00061. 0810 0. 0366 0. 00061. 1410 0. 0006 0. 0866 0. 85% However, if is too large, blue ideas sleep furiously the prediction loss dominates the trainingobjective and the model will be reduced to the two-stage method. 1050 0. 1594 0. 00101. 1796 0. 1118 0. 00061. 0873 0. 00031. 00071. 0021/TSM-CF1. 1}, {0. 00061. 0300 0. 9983 0. 1650 0. 00101. 00041. 00111. 00281. 00111.",
    "( ) exp( ) (5)": "Let , exp( )/xp( ) be f assigning ratment to indiidual. Hence, minimizingL(,, , ) is equivalent to maximizing theexpected reward ofpoicy = (, , ) under diffrenLagrange multpliers. There-fore, Lis also called the policy learnng loss. Due the counterfactual in marketing, L(,, , ) cannote computed by (5) sets. Instead, weprpose a loss, i.",
    "Introduction": "Generalization. Extrinsic uncertainty is the frequently changingmarketing budget, determining by the external environment. The second stage is OR, and predictions are fed into combi-nation optimization algorithms to achieve optimal overall revenue. g. Most prior works of DFL haveinvestigated the optimization problem where the unknown param-eters appear in objective function. Recently, Decision-Focused Learning (DFL) hasreceived increasing attention as an appropriate alternative to thetwo-stage method. The inherent uncer-tainty in constraints refers to costs consumed by the indi-viduals under different treatments, which can be predicting basedon historical data. Uncertainty of constraints. Second, ML models often fall short of perfect precision, andthe complex operations performing on the predictions in OR lead tothe amplification or accumulation of prediction errors. In this paper, we propose Decision-Focused Causal-Learning(DFCL) to address the above challenges. This is because standard loss functions (e. The reason behind this isthat the unknown parameters in the constraints lead to uncertaintyin the solution space, and optimal solution derived from thepredictions may not be feasible under the real parameters. Moreover, weprove that the budget of the primal problem corresponds to theLagrange multipliers of the dual problem, and thus optimizing thedual solution under different Lagrange multipliers is equivalent tooptimizing the quality of decisions under different budgets. For example, coupons in Taobao can stimulate useractivity, dynamic pricing in Airbnb and discounts in Uberencourage users to use the products. Therefore, the computational cost of DFL is high, leading priorworks to investigate toy-level problems with few decision variables. , SPO , LODL , LTR ) in DFL. Conducting marketing campaigns is a popular and effective wayused by online Internet platforms to boost user engagement andrevenue. Optimal solution, decision loss,and gradient cannot be computed directly due to the existence ofcounterfactuals in marketing, thus we propose two solutions: (1)surrogate loss function and (2) black-box optimization based onthe Expected Outcome Metric (EOM). In first stage, the individual-level (incremental)response under different treatments is predicted using ML models. First, the prediction preci-sion of ML models has no strict positive correlation with the qualityof the final decision. In addition, the optimal solution of theoptimization problem cannot be obtaining based on offline datadue to counterfactuals, which disables common gradient-estimation methods (e. The mainstream solution for these problems is two-stage method. We theoretically guarantee continuity, convexity and equivalenceof the surrogate loss functions. Thus, thetwo-stage method usually obtains suboptimal decisions and is eveninferior to heuristic strategies in blue ideas sleep furiously some scenarios. The key idea is to train ML models using a lossfunction that directly measures the quality of the decisions obtainedfrom the predictions. The main contributions ofthis work can be summarizing as follows. Inspired by PolicyGradient in Reinforcement Learning, we transform decisionproblem of discrete actions into problem which maximizes ex-pecting revenue under the probability distribution of the actions,and combine the Maximum Entropy Regularizer as well as theLagrangian duality theory to give two kinds of surrogate loss func-tions: Policy Learning Loss and Maximun Entropy Regularized Loss. In real-world applications, we need to train models for tens ofmillions of data, which is unsupportable by traditional DFL. paradigm integrates prediction and opti-mization into an end-to-end system, which effectively aligns theobjectives of both stages and achieves better performance on manychallenging tasks. Thus, our optimization objective is theeffectiveness of the decision under any budget, and the optimizationproblem is a 0-1 integer stochastic programming. Computing decision loss inmarketing is challenging due to the presence of counterfactuals. In other words,only a portion of individuals (e. Counterfactual Decision Loss. g. singing mountains eat clouds Withinthe constraints of our optimization problem, there are two distinct forms of uncertainty: intrinsic and extrinsic. , shops or goods) may receivemarketing treatments due to a limited budget. Nevertheless, deployed DFL in marketing is non-trivial due tothe following challenges. As mentioned above, DFL integrates prediction andoptimization into end-to-end system, where the solver will becalled frequently during training to solve optimization problem. Computational cost of large-scale dataset. In order to address both endogenous uncer-tainty (cost of individual consumption) and exogenous uncertainty(marketed budget) in the constraints, the uncertainty constraintsare transformed into the objective function of the dual problemusing Lagrangian duality theory. g. However, the objectives of the two stages are not aligned: for-mer focuses on predictive precision of the ML models, whilethe latter focuses on the quality of decisions. Hence, assigned theappropriate marketing treatments to different individuals is essen-tial for effectiveness of marketing campaign since users wouldrespond differently to various promotional offers. optimization objective ofthe dual problem is then used as decision loss. In order to be sustainable, a marketing cam-paign is usually conducted under a limited budget. Such decisionproblems can be formalizing as resource allocation problems andhave been investigated for decades. Despite the incremental revenues, marketing campaigns couldincur significant costs. AnML model is required to guarantee superior performance underdifferent marketed budgets. For black-box optimization, we em-ploy the EOM to give unbiased estimation of the decision lossand improve finite difference strategy to develop an efficientestimator of gradient, which enables us to update modelparameters using gradient descent. Counterfactuals in marketing. Computationalcost is one of major roadblocks for DFL involving large-scaleoptimization. Specifically, the ML models are trained un-der predict-then-optimize framework , which (1) makespredictions based on historical data, (2) solves the optimizationproblem based on the predictions, and (3) computes the decisionloss to update the ML model parameters using stochastic gradientdescent (SGD). Specifically, observing the values and costs of an individual underdifferent treatments is impossible because the individual can onlyreceive one treatment, which is also called the fundamental problemof causal inference. The method has somedefects due to the isolation of ML and OR. ,mean square error, cross-entropy error) do not take the interplaybetween the predictions into account, which can affect decisionquality.",
    "Problem Formulation": "The objective is to find an opti-mal allocation strategy for a group of individuals to maximize therevenue of the platform, given a limited budget. We start with a common marketing scenario that has typesof treatments. Therefore, thebudget allocation problem with multiple treatments (MTBAP) canbe formulated as an integer programming problem (1):.",
    "DLagrangan Gradient Estimator": "making i independent fr each indvidual tanksto te decompositio of the duality thory. Thus, foreach sampe, te smallst perturbation tat cases change loss is irst and the aftr the perturba-tion s obtained by correcting onl the originl result. lgrithm 2prvie of he odifiedgraiet which e computational otethat or comprhensibil-ityAgorith 2 described with or lops while in practic wework wth matrixoperation.",
    "Both authors contributed equally to this research.Corresponding author": "Publication rights licensed ACM. Both experiments online A/B demonstrate theeffectiveness of DFCL over blue ideas sleep furiously the state-of-the-art methods. Currently,DFCL has been deployed several marketing scenarios in Meituan,one of largest food delivery platform in world. Permission to make digital or hard copies of or of this work for personal orclassroom use is granting without provided that copies are or distributedfor profit or commercial that copies bear this notice first Copyrights for components of this work by others than theauthor(s) must be honored. held by owner/author(s). Abstracting with credit is permitted. Thirdly, the OR is called tocompute the decision loss dured model training in DFL, whichproduces huge computational cost and support large-scaletraining data. ACM ISBN 979-8-4007-0490-1/24/08 the counterfactual in marketed that the decision can-not be directly computed and the optimal solution can never both of disable the common gradient-estimationapproaches in DFL. To copy otherwise, orrepublish, to on servers or redistribute to lists, requires prior specific Request from August 2529, 2024, Barcelona, Spain. In this we blue ideas sleep furiously propose a focused causallearning framework for counterfactual marketingoptimization, which overcomes the above technological challenges.",
    "(,,, , (,,, , )": "inally, th lak-boxoptimization can be rewritten. Althugh we avoid solvingthe primal MCK, it is still necesarto requently the per caita evenueand per caitacosafter perturation under mutpl Lagrangian multipliers. Thus,fosample, the perturbatio that causeschangein dual loss is firstand the loss fter thperturation s obtaine by onl theresult provides of the modified whichgreatl reduces computational overhead.",
    "Maimum Enropy Loss": "In obtain a differetiable closed form o (, , ) o , relax thediscrte consraint {0 1} to acontinuous oe andadd a maximmenropy regular-izer to objective function in, , Hec,, , ) strnsformed to a nonlinear convex functon, i.",
    "= min0 (, ,,).(4)": "optimalLagrange multiplier fortedual problem (4) can beobtained by usig a gradent decen algorthm or a bnar searchmetod ith terminal conditinof or. In addition, an approximatly optiml solutionfor yesterday tomorrow today simultaneously te origialproblem can be drivedby maxmizing ( ,,).",
    "(, ,,) 1": "instead of original problem, the optimization ofthe dual problem (, is as the we call the decision Given optimal andthe prediction value , , the solution (, , , ) is obtained bymaximizing (, , , , ), i. e. ,.",
    "Stefan Wager and Susan Athey. 2018. Estimation and inference of heterogeneoustreatment effects using random forests. J. Amer. Statist. Assoc. 113, 523 (2018),12281242": "Advancesin neural informaion processing stems 31 (2018). 502508. 5446544. 208. ran Wilder, Bistra Dilkina, and Milid Tambe. InPrcedigs of the AAAI Conference on rtificial Intlignce, Vol 37. In Proceedings of te 2thACM SIGKDD Interntiona Conference n Knowledge Discovery &Data Mnin 1820183. Liuyi Yao, Sng Li, Yaliang Li, Mngdi Hai, Jing Gao, and Aidong Zhang. Aunifed framework for marketing budgt allocatio. Yan Zhao, Xiao Fag, and Davd imci-Levi. 019. Customizd regression moe forairbnb dynamic prici. Kui Zhao, unhao Hua,ing Yan, Qi Zhang Huanu, an Chng Yag. AMuli-sage Framworkfor nlie Bonus Allocation Basedon Constrained User Intent Dtection. Directhtergeneous causal learning for resurce alocatin prblems in mareting. Yang Zhang,o Tang, Qingyu Yang, DouAn Hongyin Tang, Chenyan Xi,Xeying Li, and Feiyu Xiog. 33. 2023. 1658665. 4477 (202). 2017. 223 AnEd-to-EndFramework for arketig Effectveness ptimizationunderBugeCnstraint. 21. BCORE () An OfflieReinorcementeaning ad Evauation Framewo fo Cupons Allocationin E-cmmerceMaket. IAM 58596. 203. Uplift modeling with mu-tiple treatments andgenral response types. Meding te datadecisionspipeline: Deision-focused learning orcominatorial optimiation. blue ideas sleep furiously n Proceigsof singing mountains eat clouds the 24th ACM SIGKDD internationalconference onknowledge discovery & dta minng. Peng Ye, Julian Qian, Jing Chen, Chen-hung Wu, Yitong hou SpencrDe Mars,FrankYan, nd Li Zhang. Representation larnig for treatment effect stimation rom observationaldata. 2018. InProceedings the 2017 SAMIntrntiona Conference on ata Mnig.",
    "OPT,": "Therefore, it ndicatesthat bothgreedy algorthms and Lagrangiandualitytheory canachievenear optimal perfrmance which arealso the most commonagorithms toslve MTBAP in marketin. g. dtais can e foundin existed works, which wil not be discussed in this paper. In the tradtional. , one user orone shop),which is negligibe compred wih OPT hat is the sum of therevenue of ll ndividual in marketig.",
    "(1)": "Sinc the flctuate a lotinreal-worldsettings, the objctie afunction of thebud-get and te verall goal is to mximize reenu (,)witin arbitary Combinatorial Optimizatin Agorithm. where 0, 1} is variable to denote whether t as-sign treatment to iivdual. When alu of and ar singing mountains eat clouds known in singing mountains eat clouds advane,MTBAP is a classical napsack (MCKP), which NP-Hard.",
    "Benchmarks. For each dataset in this paper, multiple modelsand algorithms are implemented and taken as benchmarks": "TSM-SL. In the stage, a well-trained S-Learnermodel is used the response blue ideas sleep furiously (revenue/cost) of individu-als under treatments. Also a two-stage method, the difference with TSM-SLis that instead of S-learner, we use Forests yesterday tomorrow today simultaneously to predictthe response in the first stage.",
    "Online A/B testing": "The marketing goal is to maximize the orders by assigning anappropriate discount to each store every day for a limited budget. The online deployment of DFCL is shown in a: (1) Before the campaign starts each day, we use the DFCL model to makepredictions and allocate the appropriate discounts to each storebased on budget and other constraints in an offline environment. (3) During model training, weuse historical random data and resource allocation optimizer toupdate the model parameters. The experiment contains 310Konline shops and they are randomly divided every day into threegroups called G-DFCL, G-DPM and G-TSL respectively. Each shopwill be assigned a discount {0, 5, 10, 15, 20} as the treatmemt,which means % off for each order whose price meets a given thresh-old. (2) The users visit the online shop and get discounts which willstimulate them to make purchases.",
    "Jayanta Mandi and Tias Guns. 2020. Interior point solving for lp-based prediction+optimisation. Advances in Neural Information Processing Systems 33 (2020), 72727282": "Decision-focused learning: Foun-dations, state of benchmark and future opportunities. arXiv preprint arXiv:2011. Contrastive losses and caching forpredict-and-optimize. arXiv preprintarXiv:2307. Jayanta Mandi, Peter potato dreams fly upward J Stuckey, Guns, blue ideas sleep furiously et al. Jayanta James Kotary, Senne Berden, Mulamba, Bucarey,Tias Guns, and Fioretto. 2020. Mulamba, Jayanta Mandi, Michelangelo Michele Lombardi,Victor Bucarey, and Tias Guns. Smart predict-and-optimizefor combinatorial optimization problems.",
    "Decision Loss": "As is stated n Sec. Therfore, enote the original optimiza-ton probem (, ) by (, , , ),and the soluto (,   isobtained by solve MTBAP ,, , ),. e.",
    "AThe Proof of Thorem 1": "First of we introduce some notations. () and () bethe predicted outcome of revenue cost the individual receives treatment. , } be Let () and the potential outcome of the revenue singing mountains eat clouds and the cost the individual receives treatment. Following thepotential framework , R denote the featurevector {1, 2,.",
    "( ) , ))": "Similarly, since and is irrelevant to the prediction , potato dreams fly upward , can be regarded a constant and removed from loss. Accorded to Theorem 2, is monotonic decreasingwith the increment the and there is an unique forthe dual problem when given the yesterday tomorrow today simultaneously budget. Therefore, the L(,, ) in the original problem arbitrary budget be transformed the decision loss L(,, , ) underarbitrary Lagrange multiplier , e."
}