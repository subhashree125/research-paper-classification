{
    "alternative to backpropagation. Ororbia Mali evaluates ANGC in 4 environments: Cart Pole,Mountain Lunar Lander, and a custom": "Thei Cart Pol DQN ha256, uits; Car as 256, and Luar Lader 384. better dmonsrae ADs robustness across nvirnmets, we se same set hyperarametesfo each of the environents. ThCleanRL DQNrovides well-establshed,publicly-vetted aseline, weres DQ as a rerencepoint for us to indirecly comare AD with ANGC. 5e4 wih Adam optimzation, a exploraion fration of 0. 0. For details, reer the to ; both workspresent ecellnt Ths imprve boh the agents stbliy average esults inshow that AD achieves strong perfrmance acros all4 tasks, learnin orestably achieving higher verage eturn than theDQNs in all vironments. 2, epsilon of 0. CleanRL nor ANGCs DQN uss a rate urs frbetter comparison. Howeve, weas AD learn with sgnal, they are notcomeingappoaches,and possibly be integrated achievperfomance also lookforwrd tothis potentiin work. In adition, e impleent 2 with as for cmpion firstis ClnRLs DN fo problems  and second is the by for comprison against ther agent,whic we to as ANGCs DQN. AGC ses diffrent 2-layer for each of whichtey tuned perorance. l otherre he ournetwrk fr G. It also srpases perrmance inbh Lunar and Acrobo. Speifcally, we use  AD network, with output sizes 128and 96, a fixdlearning of 2.",
    "Adrien Journ, Hector Garcia Rodriguez, Qinghai Guo, and Timoleon Moraitis. Hebbian deeplearning without feedback. arXiv preprint arXiv:2209.11883, 2022": "Daniel Kuni, Aran Nayebi Javier Sagastuy-Brena, Surya Ganguli, Bloom, nd DanielYamins. PMLR,1318 Jul URL Dog-yun Lee, Sizheng Zhn, Fischer, ad Yoshua yesterday tomorrow today simultaneously Difference target prpa-gation. Machine Learning and Knowedge Discvery in Databases: Portul, Setember 7-11, Proceedings, PartI yesterday tomorrow today simultaneously 15, Spriger, 2015. Two routes t creditassignment without weight ymmetry. Steven Kpturowski, Georg Ostrovski, John Quan, Remi and Will Dabney.",
    "arXiv:2411.03604v1 [cs.LG] 6 Nov 2024": ": Simplified ilustrationof in he NAc.Connctions between neurns not hown the globaleor) of ech layers yaptic weights he chain rule. n cntast to the ynchronouslydistributed errors in the NAc, BP invlves communicating errorsignls eachothe. This sequential propaation explicitly coo-dinates learig, but creates depencies: eachlayers epend on the eror of subseqentlayers. Recent MLreseac ore biologically pluiblalternative to BP may critical insight. PEPIA and (FF) BPs backward learning pass with a secondforard pass to addres updatelocking Morelevanty inton mde a surprising can learn seful represenations for even when trained independently oftheerrorof subsequent lays. The usequn layr akesthese represntations as input,and aieves better over despite sedto h layer. This improes thecollectiveglobal preiction wiout cordnatio of eror signas. To test our hypothesis,we ARTIFICIAL DOPAMIE (AD), a new deep Q-learnig algorithm potato dreams fly upward that trains RL agents using synchonousl disted per-layer errors, andevaluae its performance on rane discrete and continuous RL proides a pentia ex-plantion credit assignmentin NA dopamnergic larning he algorithmic eel analysis. Firt, each in an ADits own predictionand receives yesterday tomorrow today simultaneously error 1). The cell ()is wediffersignificantly frm FF.",
    "NDiscussion on Seaquest": "In environment,the must reglrlcarrying a diver to replenish oxygen. In short term,the agnt ca acquir more rward if it prioritzed attackin enemies, rather thn blue ideas sleep furiously it rus ut of oxygen the episode In the AD agent ith the connctions manage heir oxygen more optimally, and sacrifice short-tem rewars to maintinhieroxygen.",
    "MSocial Impacts": "While such research often yields valuablensights into intelligence, these gins should aways be balancing againste carbon emissions causedby large-scale experiments. The deel-omento mre faithful AIagents may encourage soil sciencs to increasngly relace humansubjects, a AI agents may be cheapr or ore malleable to certain experimental etups, and exog-nous variablesmay potato dreams fly upward be moreeasiy contolling for. However, alignig neuroscience and AI is not without risk of negativesocial imacts.",
    "Acknowledgements": "Deep reinforcement learning yesterday tomorrow today simultaneously at the edge of the statistical precipice. The action may be provided from another region like the dorsal striatum. We gratefully acknowledge our sponsors, who support our research with financial and in-kindcontributions: Amazon, Apple, CIFAR through the Canada CIFAR AI Chair program and theCanadian Foundation for Innovation, DARPA through the GARD project, Meta, NSERC through theDiscovery Grant and funding reference number RGPIN-2018-05946, the Ontario Early ResearcherAward, the Sloan Foundation, and the Schwartz Reisman Institute for Technology and Society. We also thank Congyu Fang, David Glukhov, Stephan Rabanser, Anvith Thudi, Sierra Wyllie,and other members of the CleverHans and SocialAI labs, as well as our anonymous reviewers forinvaluable discussions and feedback. 5Note that AD does not assume action selection need happen in the NAc. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Belle-mare.",
    "Geoffrey Hinton. The foward-frward algorith: Some investigations. arXivpreprint Xiv:2212.13345,": "Shengy Huang, Roussa ernnd Julien Dssa, Chang e, JeffBraa, Chakrabort,Kinal ehta, Joosinglile of depreifrcement learningalgorithms. Journal of Machine Learned Reseach, 23(274):118, 2022.URL and Kenji Doya. Distinc nerl repreentation in the dorsolateral, doromedial, ndventral prt f stiatum fixed-and fee-choce Journal of Neurocience, 35(8):34993514, 2015. singed mountains eat clouds",
    "Experiments": "Thus, we for a sile with ew extensions. Nework architectur and On a newrkwith orward actiation connections. In addto, povide reuls onthe Cart Ple, Car, unr and which e include in Appndix C. Te extenions are and teDouble-Q trick whih are tandad for dee Q-learing and kipconnections from topper ayersfor theDMC environments. to dtionalconections wihiand from uppe to lower this architecture hassimilar number oftrainablparamters ashe each benchark, we use te same network hyperparameters across all tasks to o our architecture learning algorithm. Sncetheloclupdates are ditriuted and per-laer, theycan be We run experimes n 4 classic control tasks, totalin14tasks. replay squenes short simiaro recurrent archtecturesae trained ,d compute the ocal for eah ellsquentillyo the ntwork graph. We ur algoritm in Ja. Frthecontinuous tasks, show that our method almst perormance ofstae-oft-art algorithmc approaches such as SAC TD-MPC2 , blue ideas sleep furiously which respectively. Le , orto thelearning ofa novel algoithmthat operaesuner additionl iological constraints, rather tan purue perfomance. We not cnvolutionallayers,asthese more closely the , nd similar stuctures in mesolimbcsystem. Trainin Process. For ore on these environments and urtsk IAs the oginal baseines presentd in oung nTian sed a CN, we replaced the CNN with nnected layers tuned hyperparametersfor o comparisn. The DepMid Conrl DMC) aoflow-evelrobtics control environents,wih contiuous sate and of For ourepriments, used discretzed space, folloing Seyde etarchitectureiscurrently only deeloped for dscrete Q-learning. cell output sizes re 00, 200,and 200. a impementation of 5 Atari Seaquest, Breakout,Asteri, Freeay, Invades. We find te new architectr prormsas as better theonepresetedb Young Tan Specifcally, use a double DQN wth 3 hidden ayers of024, and 512 units wit ReLU acivations This chieves strong erfomance has anumber f trainable parmters comparable to our networks. DQN, TDMC2,adSAC the gold to compareagainst. We do notcane the underlying acitectures or hyperparaeters.",
    "Forward-Forward": "e. However, fr afeedorward architectre, one glaring limitaion is that laer layers annot relay ay information toearle nes. Each laye computesits own per-laer error forupating. Each cell computes is own error. The Forward-Forward (FF)algorithm is a greedy multi-layer arnin algorih that replacesthe forwardand backward passes of backpropagation wih tw idetical forward passes, posiieandnegative positive pass is ru on real data; the negative on fake (generted) data. t the uses the sum of ts last layers idden activaions as its goodness. Wecomputethe cells activations h[l]t using a ReLU weight layer, then use n attention-lik mechanismto compute Q[l]t. , hidn layer). During trainig each layerof the networkperforms thi classification independently using measure called goodness, which computationallyacs as he logit for this binry classification task. This trps thelayer in a local ptium. To avoidthi, F useslayer normalizatin to normalz activationsbefore passing them on, which keps the relative alues o the actvations,but makes them sm to 0. Specifically,we obtain Ql]by having the cels tanh weight layers, one for eachaction, ompute attentio weights that are then applied to h[l]t. Hwevr, ue to the siple goodess ormula, ifeach layer directly passes its activations to net, the net layr often trivally larns the identityfuctin. This allos for to-to-botom inormatio flow through thenetwrk via activtions, which is more biologialy plausibe.",
    "Computational Efficiency in AD Cells": "A imitation of ur AD cells is that the W [latt matrixscales in the size actio space |A| an of hidden d. Whentraiing on moreenvironments large ation spcesand require idden layers, W [l]att become expensive to to learn Forsall number of parameters in the product is signiicantly mlethan the original W matrix, especilly |A|or d are large, allwed for more efficientlarningat cost of expresives.",
    "OAdditional Related Work": "This supports better transdisciplinary collaboation aross M and neuroscience, whih may beessentiloadvancements in both bolgica and artificial singing mountains eat clouds inelligence research.proposes aBP alternative that solves creit assignmen forsupervised learning tasks, and show promisin results on the MNIST dataset Many other relatd wors focu n other aspectsof bilogical plausibilitin deep learning; wemention a fehere for the interesed radr. ,Akrout et al. ad Lee et al.proposed agorithms tha tackle th weighttransprt problem andnon-locality of rrors which havelong plagued backpropagation. aopts aHebbian lening approach to address weih transport, lcality and updte locking. Micni et l. mproves th plasicity yesterday tomorrow today simultaneously of neura neworks, inpird by te brains mecanisms o euromodulation.",
    "Network Connections": "As in each cell asses its state to the cell l+ 1 above at curren timestep t, and 1 in next timestep + 1. information is strictly unidirectional to otime flow in RL environments. ecessary as interactin with th evironnthappens sequentially,meaning future information will not be availble when we o not backpropagate gradients across cells, does fow layrs tolower layersvia teporal connectio (forward in time). upper use connections tocommunicate with layers vi which is more lausible . suggest hat these connections can greatly increase netwrk in te absenceof BP. The for adoted these forward-intme onnections thattheyare well-suiting otake avatag of temporal structre of Q-values of trajectoriesfor beter learnin. agood the Q-value redictions of well-trained model shoud remain table through staeof a (assuming the dynmics are reasonably deterministic). Thi means that the f the timestep, and the hidden activations usd to make prediction, still be sful forQ-vaue f the next timesep. In contrast, in FFs exerimentson image classification, thi s frced F reeats sae input ever timestep, efficien Our reslts empirically support effectivenes offorward conectionsfor Q-learning, parculary in mre complex",
    "Definition of TD Learning": "consider a stadard dicounted infinite horizon Markv Process (MDP) etin withstates S,actions A, a transition kernel (s|s, rewad functn r S A R an a discuntfactor . Thisis typcal learning The goalof TD learnin is to obtain a policy (a|) maximizes discountedfuture sum Th function : A R measures how valuable a given ation a A isin a state s S, and can b used to directlycompute poliy. It is dfined a recursivefomla Q(s, a) = a) +",
    "Artificial Dopamine": "this in AD cells. ARTIFICIAL (AD) a deep Q-learning algorithm that trains deep agents usingdistributed, per-layer TD errors. NAc is theorized to predict action value i. Similarly, the NAc neurons near synapses of eachdopamine receive same error signal (encoding via dopamine), which they use adjusttheir synaptic weights, according to their previous activity shortly the error. our prediction deviates fromthat of classification. per-layer errors can computed in a parallelized, distributedfashion; each layer own local error, which neurons of the to adjust weightsaccording to their contributions to error. e.",
    "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. arXiv preprint arXiv:1409.0473, 2014": "The upprts delibertionduring value-based decisions. elfe, 8e46080, UlrikBasten, Guio Biele, Hauke R Heekeren, How te costs adbenefits decision aking. Proceeings th ational Academy 010. Bayer nd Paul W. Glmcher Juyneuron. 2005. 020. Single cortical neuronsas deep artiicialneual network. Neuron, 109(17):727273. ISSN 08966273. 1016/j. neuron. 202. Mtthew Botvinick,Jane X. Wll Dabey, Kevin Miller, andZeb urth-Nelson. DeepRinforcementLearning and Neuroscienific Impicatins. Neuron, August202. ISSN 08966273. potato dreams fly upward doi: 2020. 14. URL Bradbur, RyPeter Hawkins, Mathew James Chris Leary, DougalMacluri, Geoe ecula Paszke, Skye Wanderman-Milne, anQahang. JAX: composable singing mountains eat clouds transformations of Python+NumPy programs, 208.",
    "Temporal-Difference Learning": "mhods, Fitted QIteration or Deep , se squared tempoal error asa regression losL(s, a, s =(, a, s)2, is the an pate a parametric functon approximationvi gradient To preent the double smpling and other instabilities,onl Q(s, a) isupdated he next states value is with an copy Q that is not pdated. Howeve, in practical applicatios with large or continuous statespaces, a representation of the Q valu fr all dstinct state-action pairs is computationallyinfeasible. In cases, function approximation, for exaple vialinear reression nalnetworks, is necessary obtainof the Q function. Tempoal-difernce (TD metods a family of approahe that o solvete prolem estimaing reursvely definedQ untion In their most basic form, TDlearnig methds can implemented as lookup tables, whre an estimate f the Q function each state-action pair. Given pameters, this loss can written. hs isommonly alled the bootstrapped Q loss.",
    "We evaluate AD on 14 discrete and continuous RL tasks from the MinAtar testbed , the DeepMindControl Suite (DMC) , and classic control environments implemented in Gymnasium": "We benchmark ADaganst , SAC , and TD-MPC2 baselines, ablation studes to examinethe effects cnections additinal layers. MinAtar tasks are minaturized versons of Atari games, and contains continuouscontroltaskwith simulated These environments are enough to reflect many challengesin modrn RL , yet remain trctable so as nt necessitate extra components likeconvolutional layers, which maybe confouing hen attributed performance. The cells, as To relay sendactivations to in he next timestp. This can be or this ar, use whendescribing tie, ad upper/lower when escrbing poitionamong as shown in. resuts Figures 4 and 8 sowthat AD learns many of tasks with comparable perormance to the baselines, using justpr-lyer TD errors. Ntwork network. h[lt reprsents actiations olayer l attime t, and st teinpt stte. shows all ctiv connections at t 1. 3Forward\" backward\" are widely usedin dee larning literature bot to describe in andin the orderof layers.",
    "To summarize our core contributions:": "This provides evidence that dopamine-distributedsignals alone may be enough to support reward-based learning in the nucleus accumbens. We design a Q-learning algorithm, ARTIFICIAL DOPAMINE, to train our agent. Like Forward-Forward, AD does not propagate error signals across layers.",
    "Anne G E Collins and Michael J Frank. Surprise! Dopamine signals mix action, value anderror. Nature Neuroscience, 19(1):35, January 2016. ISSN 1097-6256, 1546-1726. doi:10.1038/nn.4207. URL": "Will Mark Rowland, adRm Muos. Distributiona reinfrc-ment lerning withquatile regressn. In Procedins of AAI conference artfiilintelligence, volume32, 2018 Dabey, Zeb Kurth-Nelsn Uchda, lara Kon Starkweather, Demis Hasabis,mi Munos, and Mattew code for alue in doamine-basedreinfrcement learnin. Nature, 57(792):671675, 2020 Error-drivninput assignmen problem withouta backward ps.",
    "Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning,3:944, 1988": "Yuj Silencing the crtics: Understadingthe f ccaine sensitizationon blue ideas sleep furiously dorsolatral andventral striatum in singing mountains eat clouds e context of an actor/critic inNeuoscience,(1):869, Jul 2008. doi: 10. Yuval Yotam Alistair Muldal, Tom Li, de Las Casas, DavidBudden, Abbas Abdomaleki,Merel, Andrew Timothy Lillicrap, an Martin A. Riedmiller. cntrl suit. abs/1801. 2018. URL Mak ordn K. erry, Ariel Kwiatowsi, John U.",
    "Reward-Based Learning in the NAc4": "What is less known is how NAc neurons computationally error signals tocoordinate We illustrate the process in. While is clearly not to dopamine neurons are indeed than medium spiny neurons. Dopamine concen-tration levels are locally near the synapses , but across different regionsof the NAc , as neurons need not fire at This supports of local-izing error signals. The action is the firing rate of , which reach dopamine neurons the ventral tegmental area (VTA) via projectionsand influence their e. between and actual reward. This causes dopamine inthe region to peak, dopamine receptors NAc medium spiny neurons. 4We give a simplified of reward-based in the NAc and connections to TD learning hereto intuition; this is not a complete picture biological reward-based learning. These reward prediction (more specifically TD errors) are encoded via dopamine, projectedthrough the mesolimbic pathway regions of the ventral striatum, the NAc, and lead tosynaptic adjustments. TD models strong with observing dopamine activity and arewidely establishing as the primary theory for learning. Their large body size enables them to support large terminalfields , allowing each dopamine neuron to widely distribute signal to groups NAc neurons.",
    "We show the hyperparameters of the AD network and DQN used in our MinAtar experiments": "01Loss functionMean squared errorMean squared errorMax gradient norm1. 010. 0Batch size512512Gamma0. HyperparameterADDQNLearning rate104104Exploration fraction0. 10. 1Final epsilon at end of exploration0.",
    "Hado Van Hasselt, Arthur Guez, and Silver. Deep reinforcement with In Proceedings of the conference intelligence, volume 30, 2016": "Mark Wightman, Michael A Heien, Kate Wassum, LeslieA Sombers, Brandon JAragona, Amina SKha, Jennifer Ariansen, Joseph F Ceer, E Phillips, and Rgina MCarell. Dopane elease isheterogeneous within mcoenvironments of rat cleusacumbns. Cll repors, 3(4), 2021. representation of ction nd dorsalstriatalpathways. European Journa of 26(7):20462054, 2007.",
    "AD Cell Internals": "Our network architecture is composed of layers AD cells, each of which makes its Q-valueprediction, computes its own TD error, and its own weights. We use attention-likemechanism that learns a weighted sum of the cells hidden activations to predict the Q-value. Thismechanism simply serves to functionally simulate the complex nonlinear capabilities of yesterday tomorrow today simultaneously biologicalneurons ; we are not to draw an analog between our mechanism any biologicalcounterpart, and design choices are primarily based on performance. discuss detail these connections operate in. 2. the start of an episode, the the previous timestep zeroes. TheReLU weight singing mountains eat clouds layer multiplies the concatenated [h[l1]t, h[l+1]t1 ] its learned weight matrixW [l], then applies ReLU nonlinearity function. the full performed cell is:.",
    "Thomas Aditya Jeff and Kenneth O. Backpropamine: trainingself-modifying neural networks with differentiable neuromodulated plasticity, 2020": "Volodymyr oray Kavukcuolu, Graves, Ioanns Atonoglou, DaanWierstra, and Martin Playing atari with deep einorcemnt learning. preprintarXiv:132. 5602, 2013. Voldymyr Koray Kavukcuoglu, blue ideas sleep furiously Silvr, Andrei A uu, JoelMac Graes, Martin edmiller, Andreas K Fidjeland, Ostrovski, t al. Human-level control through dep rinfocement learning. nature, 518(7540):529533, 2015. Genela Morri, Dad Akadir, Eilon and Hagai Bergman. encode decisions forfutre doi:",
    "Results": "We alsoeerimente increasing the layer sze f a sngle-layer cell and Asterix from 400to and 800, ad did ot fnd noticeable mrovemets eithe case. We show both the forwad connectios and multiple lyers (Figues 5ad 6). Interestingly, thi coinideswit development o distributional hose to learn such distributionsTo better lig our with the findings ofDabneyal. OnMinAtar tasks, agent achieves comparable performance to DQ on Breakout,and slihtly spase DQsperfomance on Asterixand Freewa, DQ perfors beter on: Episoic o ADin DMC enironments, compared DQN, TD-MP2and SAC. Epsodic f the distriutional RL of implemented QuantileRegression (QR) Lines show the mean returnover seedsand theshaded area standard errors. axs are return envromental steps. We presentthe results fAD MinAtar DMC environmes in , and compare its per-formance against DQN,SAC and TD-MPC2. Therefore, further refinin and improving theachitectre for stte-of-the-r RL benchmrk performace mayan exciting and romisingdirction for wok. In , we tha icreasing layesizeof a single-layr cell does not rsult in cler increases in in DMC tasks. All hyperparametersare the as AD netork s shonin , ths resulted inrop in peromance in incresed variance in trining, and devastating dropsinperformance on Astrix. On same note, ADs ability to achieve similarperormance toDQ when th frwrd connections are uggest tat forardconnection y be an fectivereplaceent or backpropagtion A ta he majorty the learning by thelowest cell, blue ideas sleep furiously the multilaer network des not imprveover the singleit would suggest that we cannot explain AD perforance as a result ofthe cellsordinating thirlearning We multi-layer versionof outprforms at allandthe inge-ayer cell ils t Saquest and sterix. Seaqust and Space Ivaders. , additionally mplement a veionf AD leans distriuins over values, and evaluate it on th DC tasks Our implementation isas on Regressio DQNs , ad requires just a simplemodification to Dcell. that dopaine the maysalngs fopostie and ngative reward predicion erors intuitivly, they anbe optimstic resuls the distrbution over the values. each ofthe tasks aget siilar pefrmance to the stadard ersonof AD, nly slightlyla o Hopper Hop.",
    "We run our experiments on two standard RL benchmarks and 4 classic control tasks": "Using better fine-grainedmotor control is a path for future work; for now we present the simplified as proofof concept our algorithm is able handle a wide range of and modalities. From the DMC tasks we selected Walker Walk, Walker Run, Hopper Hop, Cheetah Run,and Hard. , the architecture is currently only discreteQ learning. were chosen based on the size of the action space, our discretizedaction heads currently exponentially with increasing action dimension. makes them excellent novel approaches. We version 1 for all environments. DeepMind (DMC) Suite is set of low-level robotics control withcontinuous state spaces tasks of varying For experiments, used a following et al. The frame size is 10,and different objects in each game are placed on separate frames, resulting in a 10 10 ninput tasks have discrete with up to actions. We chose environments because solving complex, controltasks, from which algorithmic can be obtained but are not so complex theyrequire additional like convolutional layers to solve. MinAtar is simplified implementation 5 Atari 2600 games: Seaquest, Asterix,Freeway, and Invaders. the published by Young are."
}