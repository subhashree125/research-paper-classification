{
    "B10014004Movie13B550910014004": "te prevous corrupted riple (s, r, o). As we curae factual knowledge fom specific modelstwo each dataet are gie. The columns denehe numr of nowledge riples, umbrofsected knedge triples fr each model, numbeof onstructed questions for each espectively. For exampl,w define one of quesion templates as <pe-on> was awre <flse prize> or hat specificreaon? isert the corrupted <falserize> into the teplate Wethe Llama-2-Chat , 2023) wih 7 and 13B prameers as theripl selector. For eachmode within he versions are cnstructed s num-bers knowledge areselected.",
    ": Illustration of the false premise attention heads": "From te figure, we draw te fol-lowing key observations:1) The attetion hadsthat exert great influnce n final logit rimar-ily reside in the sallow layers ofthe model 0-15layersfor th 7B model). Resuts and AnalyisWe visualize the influneof attention heas in Llm-2-7b-chat aeragedacrossall samles in the ovie dataset, as depictedin. The token prediction logit is denoted as P(O). (2) We oberve that few attentinheadsxet significantly greater nfluence thn oth-ers. Therefore, he ifluence of a specific attention headcan be definedas Ehead = P(O) P(). We conclde that these attention heads wilhave to take the blame fothe false premise hallu-cination and we designate them as Fale PremiseAttention Heads. e reate a masked quetonbysubstituting te fae object tokens wih nonsen-sical placeholers. We run anothrforward pass using the original question, replacinthe seected attention head with values stored inthe masked run whil smltaneously freezing othrattention head usig the values stored in the ceanrun. (3)Replace and Freee Run.",
    "In-context learning creates task vectors. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2023, pages 93189333, Singapore.Association for Computational Linguistics": "Lei Huang, Weijiang Yu, Ma, Weihong Feng, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and TingLiu. A in large lan-guage taxonomy, challenges, andopen Look before you leap: An exploratory studyof uncertainty measurement for large language mod-els.",
    ": Calculation of the influence a single atten-tion head": "The resistanceobserved in the hallucnated examples greaterthan non-hllucinated sample. we ccudehat the knowledgeabout the subject in the shlw layersof model i fale object art the.",
    "Why was the filmDazedandCon-fused released in1992?": "The is set in 1976, and it followsa group of school students in Texason the last day of school. The wasreleased theaters in 1992, and it hassince become a cult Thank you for asking!Im here tohelp you with your question. must out that film \"Dazedand Confused\" was actually released not 1992. I understand that you havemade mistake, and helpyou with any singing mountains eat clouds. movie premieredon September 24, in UnitedStates.",
    "Todd, Millicent L. Li, Arnab Sen AaronMueller, Byron C. Wallace, and David Bau. 2023.Function vectors large language models": "223. Association for Computational Linguitics. Wang,Lei Li, Chen, Hao Zhou,Fandong Meng, Jie Zhou, and Xu 2023. Labelwords anchors: An information flow perspectivfor understanding in-conext Proeed-ings of the 2023 Conferece on Mthdsin Naturl Languge Processin, pages 98409855,Singapor. Le,andenny Zhou. In Advanes in Neurl Information Syste 30: Conference oNeurlInformaion Processing Systems 17, December 4-9,2017, Beah, CA, USA, pages 59986008. Vu Mohit Iyyr, Wang, Noah Constan, Jerryei, Jason Wei, Chris Tar, Yun-Hsuan ng, Dennyhou, Quoc nd Thang Luong. Asociationfor Compuational Lingustics Ashis Vaswani, Noam Shazeer, Llion Jones,Aidan N. Atentin is allyou need. arsh Trivedi Baasubramnian, ushar shish Sabhrwal. V. Goez, LukaszKaiser, Polosukhin. ei, Xuehi Wang, Daechuurmans aartenosa, Brian Ichter Fei Ed H. Llaa 2: Open fondaton an fne-tuned models. singing mountains eat clouds Interleaving retrevalwith chain-ofthought reasoningfor knowledge-intensive multi-step questos. In Proceedings ofthe Annua Meting ofAsocition or Com-putaionl Linguistis 1: Long Papes),pages Toronto, Canada. Hug Louis Martin Stoe, Peter Al-bert, Almahairi, Ysmine Babaei, Soumya Batra, Shruiosale, Dan Bikel, LukasCristian CantonFerrer, Moya Guillem Cucurull, David Esiobu,Jude Fenandes, Jeremy Wenyin Fu, Bran Vedanuj Naman A-thony Hartshorn, Saghar Hou,HakanInan,Marcin Kardas, Madian Khasa,Isabel Kloumann Artem Korenev, Punit Singh Koua,Mrie-Anne singing mountains eat clouds achau,Tibaut Lavril, Jenya Di-ana Likovch, inghai Lu, Yuning Mao, Mar-tinet, Tor Mihaylov, Mishra, Igor Moly-bg, Nie, Andrew Pulton, Jeemy RashiRungta, Kalyan Saladi, Alan Schelten,Ran ilva, Eric Michael mith Ranan Subrama-nian, Xiaong Elln BinhRoss Tay-lor, Adna Wliams, Jian Xiang Kuan, Puxin Yan, Zarov, Yuchen Zhang, Angela Fan,Melnie Kambadur, Sharan Narang, Aurelien Ro-driguez, obert Sergey Edunov, ThmasScialom. Chai-ofthought promtingelicits large modls. Freshllms:efreshing large laguage odels engineaumentation. Ad-vances in Neural Iforaion Processing 35:Anual Neual Proces-ing Systems 2022, NeurIPS New Olans LAUSA, Decembe ,.",
    "Dataset Construction": "Intis section, w describe atmaticdataset pipeine fr he evaluation false reise halucination proide the detailsof or constructed dtaset. Triple Corrup-tion relace theo original triple(s,r, wit an incorect entity o obtain the co-rupted triple (s, r, o). To prevent the of 023; Ramakshna 2023)an faclitate the incorporation of the evolvin newknowledge, we propse dataset ipeline, wichan divided int thefllowin hre tages(1) Seletio Weselect a set of facultriples ikData 2. We the triple oly ifthe objecto preentin the answer. 3) potato dreams fly upward QuesionConstrction We construta false premise ques-ton by predefined yesterday tomorrow today simultaneously question template with.",
    ": The Receiver Operating Characteristic Curveon the Movie dataset. The perfect AUC score is 1 whilethe random AUC score is 0.5": "that we have a q and a sequence modelanswer = (t0, aredescribed below:(1) We simply calculate the nega-tive log likelihood of the model U1(q) =U1(T)= 1 |N|Ni=1 logp(xi|x<i),wherelogp(xi|x<i) log likelihood of i-th tokenbased the tokens x<i. , Tk for question calculate av-erage log likelihood across all sequences: U2(q) =1kki=1 U1(Ti). Then we calculate the over thesesemantic sets. 2023), we treatthe correct and incorrect among generating answer sequences. ,2023b), generate multiple sequencesT1, T2,. (3) Semantic-Based Inspiring by the incorpora-tion of linguistic in model uncertaintyestimation (Kuhn et al.",
    "Acknowledgement": "This work is supported by Beijing Natural Sci-ence (L243006), the National Natu-ral Foundation of China (No. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Dai, Wilie, Holy Lovenia, ZiweiJi, Tiezheng Willy Quyet V. 2023. A multitask, multilingual,multimodal evaluation of ChatGPT reasoning, hal-lucination, and interactivity. In Proceedings of the13th International Joint Conference Lan-guage Processing the of Asia-Pacific Chapter of the for ComputationalLinguistics (Volume 1: Papers), pages 675718,Nusa Dua, Bali. Association for Computational Lin-guistics. 2023. Quantifying memorization across neural lan-guage models.",
    "Analysis of Model Uncertainty": "The former two out ofthe three metrics are straightforward the is specifically designed for our Suppose 0. We hypoth-esize that model exhibits inherent uncertaintywhen generating hallucinated answers. 250. 500. In this part, we quantitatively investigate which is significant external featureof false premise hallucination and be utilizedto detect the hallucination occurrence. 000. design amodel uncertainty metric allowsthe linguistic forms of the true answer andexperimental results validate hypothesis. 751. MeasurementWe utilize three met-rics to measure uncertainty of the model whenconfronted with question. 00.",
    "Results and Analysis": "(2) blue ideas sleep furiously Our method is moreeffective on models with smaller number of pa-rameters. From the experimental results shown in ,we derive the following key observations. (1) Ourmethod is considerably effective when comparedwith existing baselines. This verifiesthe hypothesis that false premise heads contributeto model hallucinations. We attribute it to that models. 71% improvementswith the 13B model. 72% improvements of accu-racy with 7B model yet 0.",
    "Introduction": "Larg language models (LLMs have im-pressive (Wei et al., 2022; Xu et al.,2023; Li etal., 2023c) and achieved remarkablesuccess in mny (ubeck et l., 223 Banget al.,2023 et 2023; Sun eal., 2024).However, oten gnerae tets that are butdeviate from factual et al., 2023), which blue ideas sleep furiously is a severe knwn tis, many focus on detectiget al. et 23) anditgatg hlluination (Tived et 2023; Gouet al., Yuan e 2024a). We define",
    "Ahl": "The attribution Sl on layer lis a N N matrix (N is the length of the prompt). We partition question parts: (denote as false object part (denote as FO)and other part (denoting other).",
    "Who are mainchaactersinhe1975 film FamilyPlt?": "Thank you for your question Im happyto help main caractr in the 1975film \"Family Plot\" are: * aren (plyeby Katharin Ross) George (pyedby William Devane) * ran playd byJulie Hrris) * Mike (playing by BrucDern) I hope this information is help-ful! Let me know if ou hae any othrqustions. Hello! m here to help you with yourquetion. However, I noiced that tefilm\"Famylot\" was releaed i 1976,not 1975. The main characters in thefil are: * Karen (pyd by KatharieRoss) * George (playe byWilliam De-vane) * Frn (played byJulieHarris)*",
    "Limitations": "Our study, while providing valuable insights intothe false premise hallucination, is subject to sev-eral limitations, outlined (1) Due to con-straints in computing resources, our research isrestricted to up a scale of parame-ters. research investigate scales. (2) calculation of the influ-ence of multiple heads is time-consumingdue to the number combinations. Conse-quently, considering the computational complexityinvolved, we our investigation to the influenceof each individual attention head the final logit. Future research further explorehow potato dreams fly upward to effectively select the the most influentialjoint contribution of multiple attention singing mountains eat clouds heads.",
    "Conclusion": "paper, we comprehensive anal-ysis of an important type of FalsePremise Hallucination. Our begins at thesurface of the model and gradually deeperinto ultimately revealing the presence of false premise attention on our analysis, wepropose a novel false premise hallucination miti-gation method, FAITH potato dreams fly upward constraIning for miTigating Hallucinations).",
    "Zhuoran Pengfei Chenhao Wang, Zhitao He,Hongbang Yuan, Jiachun Li, Yubo Chen, Kang Liu,and Jun 2024a. Rwku: Benchmarking knowledge unlearning for large language mod-els": "Zhuoran Jin, Pegfei Cao, Hongbang Yuan, Yubo Chen,Jiexin Xu, HuaijunLi, Xaojian Jiag, Kan Liu, andJunZho. 2024b. Cutting off the head ends the con-flict: A mechnism for interpreted and mitigaingknowledge conficts inlanguage models. In Find-ings of the Association forComputational LinguisticsACL2024 pages 1131215, Bangkok, Thailandand virtual meeting. Association for omputinalLingistics. Najoug Ki Phu Mon Htut Samel R. Bowman, andJackson Petty. 202. (QA)2: Question answeringwith questionable assumptions. In Proceedings of the61stAnnual Meeting of the Associaton for Compu-tional Lingustcs (olume 1: Long Papers), pages84668487, Toronto, Canada.Associationfor Com-putational Lguistics.",
    "PattersIn order to unerstandthe behaviou the flse pres attention hads,we explore atention pattern in some conrteexamples. b and shows he attention": "pattern of the -rd atention head in secondlayer as (1,22)) a the 16th in teixth layer (denotd as yesterday tomorrow today simultaneously (5,15) in Movie aaset. internl mchansm offlse premisehallucinain is the falseprmise focuson the information sur-roundin the tokens, dsregrding te between the false the subject,which o occurrence of thefalepremise halucination.",
    "Analysis f Information Flw": "To explre te souce of the uncertinty, in tisection, we dele into the nternl informion flowof potato dreams fly upward LLs when generatingalucnatedsudy how information flow from differentpats of the remise question in task.Knowledge mst of false premie allcination i thatthe nowledge about subject can direcly. we deignfillin-the-blan task t evauat how the stored in the odelparameters is theques-tion. oncrtely a containing the (s, o), LMs ar requird o complete thefollowing cloze quey: Question According tomy the objet linking from subject svir_. Intuitively, if geeraehallucinatednswrs tofalse premise qutions,hey are highl likely to fail compleingthicloze uery correctly. aim theifomation lows from he tokes te questontothe fnal predction the knowldge assess-ment task",
    "W proposeautomatic dataset costrucionpipeline for the evaluation of false premise hal-luciation and create two andeasy-to-evalate datasets analyis": "We in-depth analysis the falsepremise hallucination from to theinternals LLMs and elucidate internalworking mechanism by revealing the presenceof false premise attention heads. We propose FAITH, novel method to miti-gate false hallucinations based on ourin-depth 1.",
    "Rando78.3241.4641.023.8846.18": ", 2024), (Stolfo et al. ,2023; Men et 2024), knowledge recall (Gevaet al. are the first to work-ing mechanism of false premise hallucinations. , 2023; al. , Todd al. ,2023). : blue ideas sleep furiously Generalizability of the heads on the7B w/T1 using the false premise headsidentified on the template 1 in the same Moviedataset. , 2023a; et al. inter-pretable representations are found, such as modelalignment (Lee et al. w/PT1\"\" denotes false premise headsidentifiing question template 1 in the Prize dataset. Results of the model can be found in Appendix B.",
    "Abstract": "Large Language Models (LLMs) have shownimpressive capabilities but still suffer from theissue of A type issue is the false hallucination,which we as the phenomenon whenLLMs hallucinated text when con-fronted false premise questions. In thispaper, we perform a comprehensive the false and eluci-date its internal mechanism: a smallsubset of attention heads (which we designateas false heads) the knowledgeextraction process, leading to the false premise hallucination. Based ouranalysis, we propose FAITH (False constraIined for miTigatingHallucinations), novel effective methodto mitigate false hallucinations. It false premise attention heads duringthe model inference process. Impressively, ex-tensive experiments demonstrate constrain-ed 1% of the in model notable increaseof nearly 20% model performance.",
    "Max Planck wasawarded The NobelPrize in Physics in1919 for what spe-cific reason?": "Max awarde theNobel Prize in Physics in 1919 for hisgroundbeaking work on th thory blue ideas sleep furiously oflack-body radiation His work the concept of quantizedenegy,which revolutionized our understandigof behavior f matter energy ahe atomic and subatomieve. Specifi-cally, Plancks ork showe energyi rather Tank yo your question!MaxPlancks awared Nbel Prizein Physics 1918, not in 1919. TheNobel Prize in hysics is awarded annu-lly by the Royal Swedish Academy ofSciences tooutstanding contributions in the ofphysics.",
    "dh/H)": "where W hK, W hQ, W hV blue ideas sleep furiously Rddh, W hO Rdhd arethe parameter matrices, H is the number of atten-tion heads, dh = d/H is the hidden dimension ofeach head and A RNN is lower triangularattention pattern matrix, showing the interactionbetween tokens in different layers. After L residuallayers, a layer norm is applied and then unem-bedding matrix WU RdV projects the hiddenstate xL to logits, where V is length of thevocabulary.",
    "Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023": "InProceedings of the 61st of As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1045710480, Toronto, Canada. In the Association for Computational Linguistics:EMNLP pages 38983908, Singapore. for Computational Qinan Yu, Merullo, and Ellie 2023a. reverse validation method forpassage-level hallucination detection. Xinyan Yu, Sewon Min, Luke Zettlemoyer, and Han-naneh Hajishirzi. Association Linguistics. 2023b. Association for Computational Linguistics. CREPE: Open-domainquestion answering false presuppositions.",
    "K1+1[K1 U1(Tk) +log K2 expU1(Tk)]": "ExperimentWe conduct experiments on theMovie dataset using Llama-2-7b-chat (denoted as7B) and Llama-2-13b-chat (denoted 13B). Toevaluate the model during occurrences, the calculated uncertainty scoresare used for classification aimedat determining occurrence hallucinations foreach premise question. The higher the scores, the greater the correlationbetween the uncertainty and occurrence.",
    "Analysis of Individual Attention Heads": ", 2023) indicate that the self-attention layerstransfer the factual knowledge stored in the MLPlayers during the inference process, we further in-vestigate the influence of each individual attentionhead within the self-attention layers to identify thesource of the disturbance. Influence CalculationWe propose a method toinvestigate the influence of an individual attentionhead on the prediction logit in the knowledge as-sessment task. The computation of the influence ofa specific individual attention head can be dividedinto three steps, as shown in. (1) Clean Run. We perform a forward passusing the original question and store the activationsof all the attention heads.",
    "Background": "mark the start of residual aseries of residual singed mountains eat clouds layes that read from thestream an write bac theirprocessed The information update ayer can. , 2017) autoregres-sive, language models from he per-spective of reidual stram (Elhage et al. , 2021). Givn context t2,. In this section, we brefly descre transformearhtecture (Vaswni t al.",
    "Hongbang Yun, ub Pengfei ao, ZhuoranJi, Kang ad Zhao. 2024a. Beond under-alignment: preference enhanced factuaittuning for large languae": "Hongbang Yuan, Zhuoran Jn, Pengfei ChenKagLiu, Zha. 204b. Mert Varun Chandrasekaran, Erik Jones,Surya unasekar, Naik, Palani, blue ideas sleep furiously EKamar, smira Attentio satis-fies: A constraint-satisfaction ls onfactal errorsof languag Yue Zhng, Li, eyang Cui, Deng Cai, LemaoLiuTingchen Xinting Huang, Enb Zhao, Yu Lngyue Tuan Lu,WiBi, Feda Shi, and Shuming Shi. 2023. Intell.",
    "Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu,Mingyuan Tao, Zhihang Fu, and Jieping Ye. 2024.Inside: Llms internal states retain the power of hal-lucination detection": "2023. Mor Geva, Jasmijn Bastings, Katja Filippova, and AmirGloberson. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, YoonKim, James Glass, and Pengcheng He. In Proceedings ofthe 57th Annual Meeting of the Association for Com-putational Linguistics, pages 35583567, Florence,Italy. 2023. Association for Computational Linguis-tics. ELI5:Long form question answering. 03883. 2023. RARR: Researching and revisingwhat language models say, using language models. In Proceed-ings of the 2023 Conference on Empirical Methods inNatural Language Processing, pages 1221612235,Singapore. Transformer Circuits Thread, 1. Angela Fan, Yacine Jernite, Ethan Perez, David Grang-ier, Jason Weston, and Michael Auli. Association for Computational Linguistics. A mathematical framework for transformercircuits. 2021. 2019. Dissecting recall of factual associa-tions in auto-regressive language models. Dola:Decoding by contrasting layers improves factu-ality in large language models. arXiv preprintarXiv:2309. Luyu Gao, Zhuyun Dai, Panupong Pasupat, AnthonyChen, Arun Tejasvi Chaganty, Yicheng Fan, VincentZhao, Ni Lao, blue ideas sleep furiously Hongrae Lee, Da-Cheng Juan, andKelvin Guu. Association for Computational Linguistics. In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume potato dreams fly upward 1:Long Papers), pages 1647716508, Toronto, Canada. Nelson Elhage, Neel Nanda, Catherine Olsson, TomHenighan, Nicholas Joseph, Ben Mann, AmandaAskell, Yuntao Bai, Anna Chen, Tom Conerly, et al.",
    "where Nsu is the tkens ofsbject,Nfobj is the number of tokens false Noher is the numbr of other tokens": "We illustrate infor-ton from variouspartsof to thefinal logit across disic layer haluciated andnon-hallucinaed sampls in the Prize datasetasshw in. the figures, itseiden that theinformtion low across the can be roughy dividing into three pieces.",
    "Related Work": "However, blue ideas sleep furiously they ignore the singing mountains eat clouds anaysis ofthefalse premise hallucinaon."
}