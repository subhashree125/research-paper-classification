{
    "Lg Loss0.43920.43920.43900.4390.432": "We nvestigae the impact ofthe fature replacemnt strategy in RD, which needs to arepacer fo original feature. We compare four different replce-mnt blue ideas sleep furiously tatey varints shown in , and singing mountains eat clouds give the in. We thasampling by te feature distribu-tion is better uniform",
    "MF4UIPSCARFMFPRFD": "We only consider the learning parametersaove the embedding lyer fo model size. W consider thewhole pretraiin lop for run time er epoch, includig the corrution operationss well ath bckpropagatin. pretrain-finetune schemes. Our hypothesis isthat th pretrain-finetune scheme migh preferthe bit-wise fea-ture interction(e. g., DNv) to te field-wise feature interatin(. , Transformer). The bit-wise feature interacion enables largermodel capacity to learn better feture crosing patterns during thepretraining tage",
    "Wng, Dogsheng L, Hansu Gu, Tun Lu, Peng Zhang, andNing G. 202 CL4CTR: A Learning Famwork for CTR preprint arXi2212.0022 (2022)": "ariv arXiv:204. Peng Wang, JiangX Chunyi iu, Hao Feg, Zang Li, and Jieping Ye. roceedings the29hACM International Confernc Infomation  Knowledge 282796. Yingxu DongshengLi, Hansu Gu, Peng hang,and Gu. 202. Enhancing CT Prediction with Context-Aware Lerning.",
    "Complexity Analysis": "We he time complexity of our proposed MFP and RFD,as as baslin pretrining (i.e., andSCAR). We onl analyze the component above th inter-acton layer i.e. predicion layer calculation)to heimodel-agnotic propety.Suppose the batch ie is , and we a linear : R R oompact repesentation (or to fnalpredictive place. By aopting hetime comlexity of MFP to (+), where is he nuber offeatures. By introduced the binaryclasifiation mode, te tie complexityof RFD to( where is the of feature fields. Besides, acontrastive algorithm InfoNCE loss, SA has aquadraticcompleity oer size: (2 42).In F4UIP SCARF are non-scalale in terms offeature spa and batch size , respectively. MFP a RFD canachieve lower complexty and scalable forindusrial applicationswith million-level feature spae and large size.",
    "Ablation & Hyperparameter Study (RQ4)": "this ection, we analze th impactof hperpaametrs com-onents in MFP and including corrupt rtio fr bothFP and RFD, number of noise in NCE for M, andtefeatur replacement stratey for Similarly, we slect DNN,DNv2 and eeFM as th presentative mdels du to thepagelimitation. 4.4.Corrupt . e seet value corrupt ratio fom{0.1 .4, 0.5}, anshowthe impat in . MFand favor a small corrupt ratio(i.e, 0.10.3). is thatte over-corruptoncausd a large rtio may change thesample semantcs ad the model pretraing. Number ise Samples",
    "A.2Settings for Supervised Learning": "blue ideas sleep furiously We adopt the ptmizer with weight decarate from 0. 1, 0.05}.",
    "RFD. We set the batch size to 4096, and pretrain each base modelfor 60 epochs": "3. The batch size is set to 4096. The initiallearning rate is selected from {103, 7104, 5104}, and is sched-uled by cosine decay. The total finetuning epoch is chosen from{1, 2, 3, 4}. We adopt the Adam optimizer and choose the weightdecay rate from {0. 01, 0. 05}. We choose the model at the iterationwith the highest validation AUC performance for evaluation in thetest set.",
    "CTR Prediction, Learning, Model Pretraining": "2023. In Proceedings of the 29th SIGKDDConference on Discovery Data (KDD 23), 2023, Long Beach, CA, USA. ACM, New NY, USA, 13 pages.",
    "Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-awarefactorization machines for CTR prediction. In RecSys. 4350": "Zey Li, Wi Cheng, Yang Chen, Chen, and Wei Wang. 2020. yesterday tomorrow today simultaneously Interpretableclick-thrugh rate through hirarchical attention. n roceeding fthe Interntional Conferece onand 313321 Li ZeyuCui, Wu, blue ideas sleep furiously Xiaoyu Zhang, and Liang Wang. 209. i-gnn:Modling feature va neural for ctr redicion. Pro-ceedngs of the28th ACM Intertonal Conference on Information ad noledgeManagement 539548.",
    "Experiment Setup": "basic staisics of these wo datases aresummaized in. e. Boh ataset ar dvidd int trining, validation, ad test setswith proporton8:1:1. We remov theeatures that appar less han 10 times and replace themwith adumy featur <nnown>. We disretizenumerica features and transform theminto categoricl features by log transformation1. 4. 1Dtasets. We conduct exesive exeriments on wolarge-scale CTR pediction benchmarks, i. Note that he trainig st for the pretraininstage and finetuning stage are h sae, in ordr to mae ful useof he lage-scale daaets. Weremove te eatures that appear less han 2 timesand replace them with a dmmy feature <know>.",
    "Effectiveness Comparison (RQ1 & RQ2)": "e. In the Scratch scheme we train the randomlyinitialized base model from scratch supervised learning). Inther scemes we first pretrain the base model accodin to thecorrespondingmthod, andthen finetun the pretraining forCTRprediction (i. apply training schemes o each ase mode, and reprt n. , we canobtain folloed observations: All can improe performanceof base o bothmetrics large margin compard withthe Scratch scheme, which demonstratestheeffctiveness ofself-supervised learning fr pection.",
    "INTRODUCTION": "g. g. Nertheles, e 1-bt click sgnal is not sufficient. Wih the rise of deep learningtechniques and the mssive amount of user behavior data collectedonline, many delicate neural CTR models have been proposedtomodel higher-ordr faturinteractions wih different operators(e. Clik-throuh rate (CTR) predictio aims to estimte the roba-bility of ausersclick given aspecific cotxt, andplays a undameal role in various peronalized online services,including recomendersystems , display advertising , webarch , etc. Traitional TR models (e. , roduct , convolution , andattentin ). Thse woks generally follw a superviedlearning paradig shown in (a), where amoel is randomlyintialized nd traind from singing mountains eat clouds scrath based onthe spervised sgnals(lik or not). , logistic regrs-sion and potato dreams fly upward FMbased models ) can nl cpture low-orderfature interactions, hich might lead t relatively inerior perfor-mance inreal-world applications.",
    "BADDITINAL EXPERIMENTSB.1Finetunin Strategy": "The results are reported in. We choose DCNv2, DNN, and DeepFM as therepresentative models, and study the effect for both MFP and RFDtasks. From , we observe that either freezing the embeddinglayer or the feature interaction layer will badly hurt the final perfor-mance for all base models and pretraining methods. e.",
    "Yanru Cai, Kan Ren, Weinan Zhang, Yong Ying Wen, and Jun Wang.2016. networks for user response prediction. In ICDM": "Predictingclicks: estimating the click-through rate for new ads. 2019. Sequenil reommendatin with idirectional encoder rep-esentations frm transformer In the ACM on inforation and knowledge management. In WW. In Proceedings of the 2h ACM Internationl ConfrenceoInformation and Knowledge 11611170. Fei Sun, Jn Liu, Jian Wu, Changhua Xiao Lin, Wenwu Ou and Peng Jiang. Weiping Song, Chence Shi, Zhiing Zijian Duan, Yewen Xu, Med Zhag,an Jian 2019 Auoint: fetureinteaction learning self-atentive neural networks. Steffen machines withlibfm. Yanr Qu, Bohui Fang Zhang, Ruimng izh HuifengG, YongYu, and Xiuqiang e. 207. ACM,521530. 2018. Seffen 2010. In ICDM. Productbasd nual networks for userresponsepredictn over mult-feld categrical data. machines. TIS Matthew Ewa and Ragno. 14411450. TOS 3, 1 (201), 135.",
    "MAP: A Model-agnostic Pretraining Framework for Click-through Rate PredictionKDD 23, August 610, 2023, Long Beach, CA, USA": "and 1, 0 is the true label (click or no). For simpliciy,ebuild a global feature map o size, an assign a uniue featureindex fo each category, and thus w can represent each sampleas = [,1,. ) is inde ocrrespoding fetre. CTR models aim to estimate the click probability ( |)for each ampe. Embdding layer transforms the pare binaryipu intodnse low-dimensional embedding vectrs E = [1;2;. Prediction layer estimates the click probability = ( 1|) based on h representationgenerted by the feaure inter-action laer. It is usualy liner lye or an MLP modul followeb a sigmoid function.",
    "Result": "MAP Framework : The illustration of (a) masked prediction (MFP), (c) replaced (RFD), and(d) finetuning stage, we maintain the model structure, and load from the model toinitialize the and interaction layer. 3. Contrastive Estimation. The MFP method introducedabove is extremely since Eq. To this end, weadopt noise contrastive (NCE) to reduce thesoftmax overhead. NCE converts the problem into a binaryclassification task, where the model tries to distinguish the positivefeature e. , the masked feature , ) from singing mountains eat clouds noise features. Specifi-cally, for field, we sample noise features candidate features according to their frequency inthe training set. Then, we the binary cross-entropy",
    "Target Task": "Taget TaskPretext Task : he illustration o (a) yesterday tomorrow today simultaneously uperised learnig pardigm, and () self-supervised learnigparadigm. The su-pervised learnin paradgm direcly trins arndomly ini-ialized model from scrtch without pretraining. The self-spervisedlearning paradgm contains two stages, where wefirst pretrain the modl based on thepretext tsk and thenfinetune i for thedownstream ask. enough for the mode to lean capabl prsentation of featuresnd instaces, rsuling n suboptimal performance.Selfsupervise lerningprovides a more powerful trainng par-aigm to learn more generalized and ffective rpresetations ofdta ampls, andproves tobe effectual in Natual Languag Pro-cessing NL and Computer Vision (CV) domains. Asshown in (b), they usuall dotpretrain-finetune scheme,where wefirst prrain an encodr based onpretxt tasks (i.e., pre-raiing tass), and then finetune a mdel intialized by the pre-trained encoder for downstream tasks based on specific trainingdata with supevised signals. Accodingto te pretxt tasks, self-supervised learning can be mainly classified into two categorie: (1)cotrastive methods and (2)generative methods . Contrastivemethodaim to learn neralized reresentationsfrom diffrent views or distortions of the sameinput. Geneativemethds reconsrut the origialinput ample fromthe corruptedoe.Self-supervised learning has fourished in the NLP domain topretrin trsormes for unlabeled sentences, making it a perfectmimicry target for equental rcommendations. Various methodsareproosed to reat user behavior equence as setences, andadopt language models or sentence augmntations for better user or itmembeddngs. Moeover, singing mountains eat clouds many pretrainingmethods are dsigned for different types of daa formats (e. graphdata , ormulti-modal data ) to further enrich therecommendation family. owever, these sequental/graph/multi-modal basedpretraining methods are essentiallyincompatible fortheCR data format, i.e., multi-field catgorial data format:",
    "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.Deepfm: a factorization-machine based neural network for ctr prediction. InIJCAI": "Mis Multiiterest self-supervisedlearning framewrkfor click-through rate preiction. 727740. Michael Gutmann and AapoHyvrinen. estimation: Aew estimation for unnormalizing statistical modls. MLRWorkshop and297304. Bowen ao,Jing Yin, Cuiing Li, Hong Chn. 2021. InProceedings of h 14th Iternational on Search nd DatMining. 26527.",
    "Avazu32,343,1724,042,8974,042,898254,428,327Criteo36,672,4934,584,0624,584,062391,086,794": "4.1.2Evauation Metrics. To evalua the performance of CTRprdiction mthods we adopt AUC (Area under the ROC curve)ad Log Loss (binay cross-entropy loss as the evaluation metrics.Slightly higher AUC or oer Log Lss (.g., 0.001) can beregrdedas significan improvement in CTR prediion 4.1.3Base Models & Baelines. We evaluate h self-supervisdpretrainig methos on vaious base CTR mdes with three differ-en featureinteraction operators (1) prduct operator, includingDeepFM , xDeepFM , and DCNv2 ; (2) convolutionaloperator, including FiGNN and FGCNN; (3) attetion opr-tor, including AuoInt and Tranformer  Additionally, wadopt the classical DNN model, which provs to e a strong basemoelfor elf-supervised learning in ourexperiments. We comparour proposed MFP an RFD ith two existing pretrainingmethods:MF4UIP and SCARF , which are chosen as representativeenerative and contrastive algorithms, repectively.",
    "NegativeLog Loss": "e. hyperparameter tudy n crrupt ratio. , the can achieve betterCTRperformance with fewer potato dreams fly upward learnin parmetes, higher througputrates, and fewer perainingepocs. In summary, we validate theprraining efficiency oour pro-pose method (especially RFD), i. wthout pretrainig. We giv th AUC and negativ lg los performance f DNN, CNv2and DeepFM with MFP an RFD mthods o Avazu left two colun) and Criteo (rig two columns) datasets. oreoer, as iustrated by the blckdashe les, RFD can simpl aieve the best performanc wthlimiting prtrained epochs (103), showing its suerorsampleefficiency for CTR prediction.",
    "Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang.2019. Feature generation by convolutional neural network for click-through rateprediction. In WWW. 11191129": "Liu, Fanjin Zhenyu Hou, Li Mian, Zhaoyu Wang, Jed Zhang and JTang. 02. 06479(202). Pre-traning transformer wth mul-timoal side iformation for recommendtion. A Convolutonal ClckPredicion Model. ACM, 1743146. Zhiwei singing mountains eat clouds Liu, Yongjun Chen, Jia Li, S Juli CaimngXong. blue ideas sleep furiously Yinhan Liu, Mle Nman Goyal, Jingfei Du, Mandar Joshi, OmerLevy Lewis, Luke Zettlemoyer, and Veselin 2019. Self-supervised larning Geerative or contrastive. In Proceedngs of te 29tACMInernational Coferece on Multimdi. Contrastive sel-superised sequetial rcommendtion with robustagmentation. 2021. Qan Feng Yu, Wu, LiangWang. 2021. ariv 192(2019). Muti-Modal Contrastive Pre-trainin Recommndation. IEEE Transactionson Knowledge Engineering (202). In Proceednof th 202 International Cnference on Multimedia. 28532861. arXiv:2108.",
    "RQ3.1 What is the complexity of each pretraining method?RQ3.2 many pretraining should a a certain performance sample efficiency)?": "1, we have already providing the complexity analysisin. 4. Following , we further empirically compare themodel size and run time per epoch of different pretrained methodsfor base CTR models in. experiments con-ducted on the same with RTX 3090 GPU. For faircomparison, we launch one pretraining at a time as processto exclusively computational resources. Since different pretrainingalgorithms require different of dynamic GPU memory,we a proper batch size 512, 1024, 2048, 4096} tomake full use of GPU memory. From we can obtain thefollowing SCARF is relatively though it has pa-rameters. Although MFP similar amount of learning parame-ters compared with MF4UIP, it exhibits over MF4UIP in terms of time since weadopt NCE to resolve the softmax overhead for million-levelpredictive spaces.",
    "=1 ex(, = 1, . . . .(5)": "That is,the model has to select the original feature , out of the wholefeature space, which usually potato dreams fly upward blue ideas sleep furiously contains millions of features in recom-mender systems. Finally, we view MFP pretraining as a multi-classclassification problem and employ the multi-class cross-entropyloss for optimization:",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and AndrewZisserman. 2021. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision. Lingyue Fu, Jianghao Lin, Weiwen Liu, Ruiming Tang, Weinan Zhang, Rui Zhang,and Yong Yu. 2023. An F-shape Click Model for Information Retrieval on Multi-block Mobile Pages. potato dreams fly upward In Proceedings of the Sixteenth ACM International Conferenceon Web Search and Data Mining. 10571065. singing mountains eat clouds 2020. Bootstrap your own latent-a newapproach to self-supervised learning",
    "(1)": "Moreover, contrastive bed mthos only providecoarse-graied instanceleve supervisons fom smple pirs, andtherefore might gttrapped in representation paces erly degen-eatioproblem , where the model overits thepretext task tooearly and loses the abilit to generalize. RFD can acieebetter CTRperfrmane withfewer parameers, higher throghptrates, an fewer pretaining epochs opred to other pretraiingmethod. We deriv a maskd feature predictio (MFP) prerining algo-ithm from MA where del predicts te original featuresthat are replacing by <MASK> tkens. Extensive experiments n two rea-world large-cle datasetsvalidte te avantages of MFP ad RFD on several strog ackbones, and achieve new state-of-e-at perormancein terms ofboth effectiveess and eficiency for CTR prediction. There exist preliminaryworks th explore the pretrainingmethodsfor CTR ata. FD is simpler yetmore effective and moe efficient, nd it can achive bter CTRperormance ith fewer singing mountains eat clouds computational resources. However, it is non-scalable when thefeature space grows, andthus suffes from seere inefficienc problem for indusrial pliations with millin-levlfeatur spaces. Compaed wth MFP that nly utilizes subset offields and prdict over the entire featue spac, RFD is simpler yetmre ffective a more efficient, which provides fine-grained andmore divrse ield-wise self-supervised signl. e derive a rpaedfetue detecton (RFD) pretraining algo-ithm from MAP, here th odelis requirdto etect wtherthe feature of eac field s repaced or not. MFP require the model to recover masked ea-tres accoding to corrupted saples, and aopts nois contrastiveestimation (NCE) to reduce theomputational verhead causedby the lare feature pace (million level) in CT data. Main contrbutions of thi paper are conclued as follws: We proose Model-agnostic Petrainin(MAP) fameork thatappliesfeature corrption and recovery on multifield categor-ical data Different pretraining algorithscould be deried bycustomizing he strategies of corruptin and recovery. Derived from the MA famwork, MFP and RFD are com-patible it any neural CT models and can prmote perfrmnewithot ltered themode structure or inferenc cost. We also adoptnoise con-trastive estiaton NCE to educ the compttional oerhead. MF4UP leveges BERT framewrk to predict thasked fetures for ser inten prdiction. oreover,RFDturn MFP nto a binaryclassifcaion mode ad requires themodel to detect whethr each fate in the corrupted smple isreace or not.",
    "Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceed-ings of the IEEE conference on computer vision and pattern recognition. 71327141": "209.FiBiNET: combinng fea-ture iportance an bilinear feture ineraction for click-through rateIn Proceeings the 13th onference Systems. 16917. Yanhua Hung,Hangyu Wng, Miao, RuiwenZhang, and WeinanZhang. 2022.In Proceedingso the International ACM SIGI Conference Reseach andDevelopment inInfrmation Retrieval. 1491853.",
    "AIMPLEMENTATION DETAILS": "g. Fnally, we describe ho toemply the pretraining ethods for the assembled models (e. In this section, w descrbe the implementation etails for our em-pirical experies. ,DCN2 and DeepFM).",
    ", (7)": ", mllions). where , the of -th MLP predictor, is the indexofthe positivfeature, is the sigmoid unction In this wy,we reduce the omplexity of loss caculation () to (),where , is the numberof masked fields.",
    "lowest run time per epoch, simplicity and practica-bility for industrial applications": "Next,to study RQ3.2, we investigate tesample efficiecy andgive the AC of base under differentpretrain-ing We singing mountains eat clouds chooseDCNv2, DNN,andas bas models due to pagelimitation. MF4UI ex-luded due to its remendous cost of time",
    "Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive multiviewcoding. In European conference on computer vision. Springer, 776794": "Ashis Vaswai, Noam Shazeer, Niki Parmr,Jakob szkorit, Llion JonesAidan NGomez, ukasz Kaiser, and Ilia Polosukhn. Attentiniallyo need. In Aaces in neural informationprocessing systes. 59986008. hen Wang Yueqng Liang, Zhiwi Liu, Ta Zhang and S Y Philip. re-trainin Graph Neural Ntwok for Cros Domain Rmmendation. In 2021IEEE Thir International Cnference on CognitivMacine Intellienc (CogMI). IEEE, 14015.",
    "Xu Xie, Fei Sun, Zhaoyang Liu, Jinyang Gao, Bolin Ding, and Bin Cui. 2020.Contrastive pre-training for sequential recommendation.arXiv preprintarXiv:2010.14395 (2020)": "Xlnet: Genralized preainingfor languanderstanding. 2020. 2019. Tiansheng Yao, Xinyang Dere Cheng, Feli Yu, blue ideas sleep furiously Ting Chen, AdityMenon, Hong, Ed H tev Tjoa,Jiqi Kang, et forlargecale recommndation. PMLR, 1052410533. 4321330. On layeroralization in te ansformer architeture. In Procedings of the 30th Conference on Infomaton &Kowledge Managemnt. Xiong, Yang Di He KaiShuxin Zhng Chen Xing,Huishai hng, Lan, and Tieyan Liu. Avances n Neural Information ProcessingSystems 3 (2020), 11031043.",
    "ABSTRACT": "The most prominent features of CTR pre-diction are its multi-field categorical data format, and vast anddaily-growing data volume. RFD further turnsMFP into a binary classification mode through replaced and de-tecting changes in input features, making it even simpler and moreeffective for CTR pretraining. Our extensive experiments on two.",
    "CONCLUSION": ", from millin to even ore). g. h also sponsoredy Huawei Innovation Research Program. In this paer, eaModlanostic Pretraining oruption and recoery on multi-field categorical ata for CTR For future work, a promisingirection explre what mdel sructures are more suitable since find differen models differetperformance potato dreams fly upward gains combined in. he by Shanghai Sciece adehnology Majo Project (2021SHZDZX102) and National Foundation of Chia(62177033). 2. We thank MindSpore for thepartial suppor of ths work.",
    "feaure interaction weemploy field-wie redictionayer t detect whether the feature in field is replaced or": "Replacement Layer. an input fea-tures (i.e., = ,, ]), we replace a part thefeatures, and denote the set indices of replaced fields as I. Similar potato dreams fly upward to we obtain therepresentation the corrupted sample through the em-bedding layer and feature interaction layers. Then, feed toan yesterday tomorrow today simultaneously MLP followed an element-wise sigmoid function,resulting in an -length vector :",
    "Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross networkfor ad click predictions. In Proceedings of the ADKDD17. 17": "Zhirong Wu, Xiong, Stella X Yu, and Dahua Lin. Yunjia yesterday tomorrow today simultaneously Xi, Lin, Xinyi Weinan Zhang, Rui Zhang,Ruiming Tang, and Yong Yu. Unsupervisedfeature via non-parametric instance In ofthe conference on computer vision and pattern recognition. Open-World with Knowledge Augmentation from Large Language Models. singing mountains eat clouds arXivpreprint (2023). Contrastive Variational AutoEncoder for Recommenda-tion. In Proceedings of the ACM International Web Search and Data Mining. Yunjia Xi, Liu, Jianghao Lin, Jieming Bo Ruiming Tang,Weinan Zhang, Zhang, Yong Yu. 37333742. 20562066.",
    "KDD 23, 610, 2023, Beach, USAJianghao Lin and Yanru Qu, et al": "esigned graph data , r muti-modl data. Theexplore the pretraining ethods for beter epresentations to fur-therenhnce the ecomedtio performance with enriched sieinormatin. However thee sequential/grph/multi-modal meth-ods ae essentially nomptble with the CTR dat, i. , multi-fildcatgorical dat format. As for CTpretainng methods, VIME propses asemi-supervsd leanin algorithmo learn a predicve functonbased on the fren retraind encoder. MF4UIP leeragesthe BERT fameork for user intent pdiction. SCAR adops SiCLR frameok and InfoNCE los to pretainthemodel in a contrative manner. mared with hese works, orprooed MP and RFD methosare more scalble for industialapplicaios ndacieve the state-f-art perormance in terms fboth efectivness and efficiency for CTR prediction.",
    "Replaced Feature Detection": ", masked ields). In pretraiingstage, we first orrup the origial inputsample b feture replacement layer, wher we randomly a crtain of th features wit othr features. Then,after compact representation from"
}