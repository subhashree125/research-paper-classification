{
    "Abstract": "Anything Model (SAM) marks notablemilestone in segmentation highlighting by its robustzero-shot and ability handle diverse Dwin-MSA localizesattention computations around the target object, enhanc-ed object-related embeddings with minimal we propose Pixel-wise Dynamic enable integration of interactiveinformation from few initial blue ideas sleep furiously clicks have significantimpacts on overall segmentation results. Experimen-tally, FocSAM augments interactive segmentationperformance to match existing state-of-the-art methodin segmentation quality, only about 5. 6% of yesterday tomorrow today simultaneously thismethods inference time on CPUs. is at.",
    "256 that hasbeen upsampled by some convolutions and the click-fusedquery embedding qc R1256, with the click embeddingsdiscarded. Their dot product F cqc RH4 W": "4 1 generateslogits for predicted the mask upon SAMs pipeline, Foc-SAMs pipeline the focus refiner. potato dreams fly upward image embedded from fi-nal block serves as the refiner output. We detail following subsection. refiner once each object. at in-teraction an object, refiner receives the image em-bedded F , the previously predicting mask potato dreams fly upward M (K1) andthe previous click-fused query embedding q(K1)c. F (K)rre-places original embedding F in subsequent inter-action blocksrefine image query embeddings iteratively, sharingthe same previous mask.",
    ". Pipeline": "Image encoder. A positive click in false negative regionsignals model to expand that region and a negative clickin a false positive region suggests removal. In our work, we use a singlequery embedding q R1256. This encoder is structured in four stagesof equal depth and utilizes window-based attention in eachstage for efficient computation , with full attention ap-pliing at each stages end. The num-ber of query embeddings corresponds to expected out-put masks by the decoder. Instead, both the prompt encoder and the decoderactively engage in every interaction, rapidly processing an-notator clicks to predict segmentation results. After. In , SAM comprises an im-age encoder, a prompt encoder and a decoder. Starting fromthe second interaction for each object, the prompt encoderalso converts the previously predicted segmentation maskinto mask embeddings. In SAMs preprocessing phase, images areresized and padded to 10241024 and fed into a ViT-based image encoder. The imageencoder preprocesses each image only once before the in-teraction, despite the varying number of objects within theimage.",
    "SAM-ViT-H ICCV230.35 (0.02)1.882.097.625.1913.9710.366.85FocSAM-ViT-H (Ours)0.39": "he decode-l SPC sepaaely noted inparentheses, he actual iteraction time. s quantifed as SeconsPer Click (SPC) on CPUs, the averge click. We on undr 0 cliks, i. the average to achive 90% IoU. In cases where more tan are needed, te count is capped at 20 for valuationconsistenc with AddtionalNoCmetris ae employed inthe ablion. step K 2, i. For performance, use theNumber of Clics metric is the average mini-mum clicks required reach specified IU. e. e reportresults on Grabut , Bereley , SD DAVIS ,MVTe and COD10K. or our SPC dditionally includes refiners averaging over 20 licks. Traning strategy. Mre deailsare the supplementary materials. In evaluation, folowing SAM, im- ages resize to 1024, andhe segmenationresults are then back to orig-inal for alculations. Fr simulation in tet-ing, we place the cnters f prdictdregions, in line wit preious methods. FocSAM in nferncespeed andsegentation pefrmance. Training andevaluatons are prformedon a server with 4NVIIA RTX3090 GUs ad Intl Xeon Silver CPUs. Wersize and pd theimages to SAMs sie of024 104. denotesethods tha have notfollowed conventinal COCO +VIS trained for neractive 6% the previous SOT and 6 shift refine blocks The embeddig dimensions ofboth Din-SA and P-DyReLU are t align with the256-dimensional SAM image embeddings. In FcAM, we adopt click strategy for interactive im-ulation befor loss computation. Comparison of NoC@90 methods. The bet results are highighting i signifisthatthe SPC metric boh inferencetime ad encoder inference averaged clicks. Further details are in the supplementary materials.",
    "Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, XinJin, Wenjun Zeng, and Zhibo Chen.Inpaint anything:Segment anything meets image inpainting. arXiv preprintarXiv:2304.06790, 2023. 2": "In EEE/CVF Conference on Com-putr Vision Rcognition, CVPR Seattl,WA, USA, June 13-19, pages 1223112241. IEEE,2020. 2 Sen Zhuoran, Zhang ingyuan, Zhao Yi andLi HongshengEfficientattention: Attention I 2021 IEEE Winter Conference on Appli-cations of Computer Vision (WACV), 2021. Interactive segmntain with inse-outside guidance. Yuhui Yuan, ao Fu, Huang, Lin, ChaoZhag, Xilin Chen nd Jigdong Wang Hrformer: High-resolution vison dense Avacesin Neural Informatin Pocessin Systems, Shiin o Liew, Yunchaoei, Shiui Wei,and Yao Zhao.",
    "Yinpeng Chen, Xiyang Dai, Mengchen Liu, DongdongChen, Lu Yuan, and Zicheng Liu. Dynamic relu. In Eu-ropean Conference on Computer Vision, pages 351367.Springer, 2020. 2, 5": "hi ian,Yuqing Wang, Bo Haib-ing Ren, Xiaoli Wei, Huaxia Xia, and Chunhua thedsin of ptial attention yesterday tomorrow today simultaneously in vi-sion transforers.Neural Proessing Sys-temsNeral nformation Poessing Syses, 2021.3 Xioi DogJianin Bo hen, WeimingZhng, u, Lu Dng and Gu.Cswin trasforme: A general visiontransfrmer bakbonewith coss-shpe windows. In 2022 IE/CVFConferenceon omuter and Patern Recogniion (CPR), Lucas Beyer, Alxander Kolsnikv,Dirk Weissenbor,XiaouaZhai, Thoms Untethiner,Mostafa ehgha, Matthias Minderer, Georg eigold, Gelly, Jaob Uszkoreit, Neil Houlby. imageis worth 616 Trasformers for image recognitionat sal. In 9t International on Learning Rep-resenatin,2021 Virtual Eent, Austria, 3-7,2021. OpenRevew.net, 3 Deng-ig Fan Ge-Png Guoleiun, Chn,Jianbig Shen Ling de-ttion.In Poceedingso coference ocmputer viion and attern recognition, pages 277727872020 2 , 6, 7, 1 Yuxin Wen Binhui Xe, Ledell Wu,Xingng Wang, Tieju Huang, Xinlong and YueCao.Eva: limits f masked representa-tion scae. InPoeedings of IEEE/CVF on Vision and Patern ecognition, pges193519369, 2023.2",
    "Qipng Guo Xipeng Qu, Pegfei Shao, Xi-angyang and Zheng Zhang. Star-transformer. In of 2019 Conference of the North, 01. 3": "Bourdev,Subhransu Maji, Jitendra Malik. singing mountains eat clouds 3 Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Lutao Chu, Shiyu Zhiliang Yu, ZeyuChen, et al. 5,6, 1 Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, andGao Huang. In IEEE International Conference onComputer ICCV 2011, Barcelona, Spain, November6-13, 2011, pages 991998. contoursfrom inverse detectors. Edgeflow: practical interactive seg-mentation with edge-guided pages 15511560, Hariharan, Arbelaez, Lubomir D. LVIS: for vocabulary instance In IEEEConference Computer Vision and Pattern Recognition,CVPR 2019, Long Beach, USA, June 16-20, 2019, pages53565364. Girshick. In Proceedings of the Conference on Vision, pages 59615971, 2023. Gupta, Piotr Dollar, and Ross B.",
    "Query Embedding": "Overview of FcSAMsfousrfine. Figre (e) a detaile view of the MSA module. (a) potato dreams fly upward singed mountains eat clouds epicts the architectur of focus refiner.",
    ". Interactive Segmentation": "SAM also adopts this pipeline andachieves robust zero-shot capabilities and diverse prompts,leaded to various downstream applications. However, SAM is unable to employ the image-levelzoom-in strategy efficiently and integrate interactiveinformation effectively, hindering its broader applications. The integration of deep networks into interactive segmen-tation is initiated by DIOS , leadingto subsequent advancements in click-based methods likeDEXTR , FCA-Net , BRS , and f-BRS. We introduce FocSAM to address SAMs limitations. Sim-pleClick is the first to introduce large Vision Trans-formers into this field. The following methods focus on en-hancing various aspects of interactive segmentation.",
    "Image Embedding": "Overview of FocSAM building upon SAM. These embeddings and a learnable queryembedding are fed into the decoder for segmentation. final a refined image embedding, which image for subsequent with the object. One typical way tolimit the region each token from full-attentionto local/windowed attention. In thispaper, we Dwin-MSA to perform windowattention on object-related embeddings.",
    "A.3. Click Simulation": "the probability of having clicks is times the probability of having 1 clicks, with con-straint that 1. This method ensures a higher selected fewer clicks, reducing computational costs. Forjoint on COCO and LVIS datasets, Inter-Former sets at both. Instead, to avoid biastowards small objects in yesterday tomorrow today simultaneously LVIS, we use different values forCOCO ( = 6) and LVIS ( = 0. al-lows effective use of LVISs detailed annotationsin the later refinement stages. In FocSAM, decidethe number of clicks, N, and determine the refinementstep, using a similar sampling strategy, where set dis-tinct for 0. 6) and LVIS (r = 0. as upper limit singing mountains eat clouds to ensure a similar pro-cess. After N and K (only for FocSAM),SAM and perform click simulations on trainingimages used GT an oracle to specify randomlywithin incorrectly predicted regions.",
    "arXiv:2405.18706v1 [cs.CV] May 2024": "This pipeline incorportes owerul Vsion Tansformers(ViTs) as heimage enoder to preprocess im-ages, generatig imag embeddings tha are toall image. clicks) fronotators ar fed ito lightweight decoder to roducesementaion reslts.Second, SAMs lightweight strugglesto sufficiently fuse te the pre-processed imae embedingsto the eed thus wakening e feedbacks impact on sgmentto quality. FocSAMs upon SM and intro-duces an extra focs refiner. The adjustment major improvements. First, the efiner initialsegmentation resuls to refocus image ongions containing the trget inspired theimage-level zoom-in. Second, therefiner a few initia clcks prove to on segmetatin , urher enhncingthe object-relating implement FocSAMs rener wih minimalcomputational yesterday tomorrow today simultaneously overhead, we introduce WindowMulti-head Self-Attention nd Pixel-wise Dy-naic ReLU (P-DyReLU). yesterday tomorrow today simultaneously Dwin-MSA sesthe shifted strategy tosure interactions aong embeddings, preserving Moreover as the number ofobjects imag surpasses 10, FcSAMs tm efficiencyfurther impove, emaning 1. 2of th ti require by SimpleClik for CP inference. We smmarieour contributons s follos:.",
    "256 can be windowed as F RLSS256": "wihL = BHW/(16S)2. Fo withithse images, we can siultaneously select all indowin-tersecing with their rspectve bounding despite thebjects sizes Th lead te seleted embdding in-dows F RMSS26, with M ofwin-dws interctig with boxes Long-range patc-to-atch W employte shifting stratey blue ideas sleep furiously in Shift Dwin ((d)) the boxes typiclly liit disance betwee ithin potato dreams fly upward th same ob-ject, mplying that a few blocks and smallizesstillalow sufficint exchnge. The MA ( (e)) rocesses F W sach wowRSS256 paralelly,with the duplicatedquery embeddig = Let.",
    "Lu,Zhen, Benjamin Planche, SrikrisnaKaranam, Terrence Chen Marc Niethammer, nd Ziyan Wu.Pseudoclck: Interactve image segmentation click imi-tation. 728745, 2022. 2, 5,": "singing mountains eat clouds Ze Liu, Yutong Lin, Yue Cao, Hu, Yixuan Wei, ZhengZhang, Stephen Lin, Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of IEEE/CVF international conference oncomputer vision, pages 2021. 3, 5 Ze Liu, Han Hu, Zhuliang Zhenda Xie,Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.Swin v2: Scaling up capacity and resolution. InProceedings of the IEEE/CVF computer vi-sion and recognition, pages 1200912019, 2022.",
    "IoU93.2310th Click": "plot trends of 20 clickss segmentation, clearly contrastingSAMs IoU fluctuations with FocSAMs stable. In contrast, FocSAM (middlerow) shows consistent performance. SAMs perfor-mance is this example (top row), where the 9th clickyields of (left) subsequent click significantlyreduces the IoU to 12. Interactive segmentation on challenging ex-ample. shows the example overlaid GT (pur-ple masks). 78 (right).",
    ". Qualitative Results": "In , we pesent a qualtative comparison of and SAM a challenging exmple and vsualizethe segmentation singing mountains eat clouds result at fou differen clicks. This visu-alization cleary demonstrtes FocSAMs enhaced stailityover Our analysis that maintains consistent perforance, providng superio quality cmpared toSA blue ideas sleep furiously such challng-ing examle. Additonal qualitative available inthe suppemenary maerials.",
    ". Refine Block": "Overview. (a), the plain block and theshift block alternately stack within the refiner, refin-ing embedding F and click-fused query embed-ding the shared mask M Both Dwin Dwin identify around object from the mask M (K1)",
    "yc = f(x)(xc) = + bkc(x)},(6)": "where all the coefficients {akc}, {bkc} are outputs of the potato dreams fly upward hy-per function (x). The plain ReLU is a special case ofK = 2 with a1 = 1 and b1 = a2 = b2 = 0. Pixel-wise DyReLU. Considering Equation 4, we imple-ment (x) to fuse f RSS256 from Equation 2 yesterday tomorrow today simultaneously withqf R1256 from Equation 3. The implementation is in-spired by the SAM decoders use of a dot product betweenimage and query embeddings to generate logits for maskprediction. This process effectively captures the un-normalized similarity between each image embedding andthe query embedding in a pixel-wise manner.",
    "SAMFocSAM": ". Qualitative analysis on a challenge Example. first image from the left displays the challenge example with the image and GT(blue masks). The top and bottom rows on right respectively show segmentation results of SAM and FocSAM at the 1st, 5th, 10th,and 20th clicks. Clicks are indicating with green (positive) and red (negative) circles. these modules, we slightly modify the modules. For Dwin-MSA only, we remove all P-DyReLU modules, replacing P-DyReLUs activations in Dwin-MSA with standard ReLU.For P-DyReLU only, we remove the dynamic windows toretain all image embeddings, and remove Dwins atten-tion computations. We evaluate these variants on threelargest datasets including SBD, MVTec, and COD10K, us-ing NoC@90 within 20 clicks, and extend to NoC@95within 100 clicks for deeper analysis. This NoC@95 metricquantifies individual contributions of each module, es-pecially on more challenging samples. All ablation modelsare trained with the same protocol of the main experiments.Results. shows that Dwin-MSA and P-DyReLU in-dividually contribute similarly to FocSAMs performance,indicating that they provide comparable interactive infor-mation. Dwin-MSA primarily focuses on initially predictedmasks for locating main object areas, similar to boundingbox prompts in SAM, whereas P-DyReLU leverages ini-tial clicks for primary object outlining. Their interactiveinformation is complementary. Consequently, their combi-nation leads to enhancing overall performance, particularlynoticeable in NoC@95 under 100 clicks. This metric under-scores the increased click requirement to achieve 95% IoUfor challenging samples.The integration of Dwin-MSAand P-DyReLU further stabilizes FocSAMs performanceon challenged samples. More ablation studies are providedin the supplementary materials.",
    "GT1st Click5th Click10th Click": "88Io = IoU = 22. Qualitative results (2). 43IoU = 74. The row illustrates the from and bttom those fro FocSAM. 11IoU = 5. 97IoU = 79. 36IoU =63. 4. 35 IoU = = = 91. 29IoU = 2. 8IoU = 91. 33 IoU 1. 55IoU =53. 89 IoU = 88. To right two rowsisplay nterctive t the 1st, 5th, 10th, and 20th clicks, whe the most recent click is with asar, greenfor postve and red ornegatie feedack. 42IoU = = 4. SAMFocSAMSAMFocSAMSAMFocSAMSAMFocSAM IoU = 12. 37IoU = 79. 22IoU 89. 75IoU =. 77IoU 88. These viual comparisons reveal the efficiency f FocSAM and at different stages ofclicks. 35I 91. 59 IoU= 33. 49IoU 77. 38IoU IoU 1. 1 IoU = = 93. On the left, a examleisdepicted withan image its GT(blue mask). 96IoU 57. 71IoU= 71. 37IoU= 83.",
    "Robin Strudel, Ricardo Garcia, Ivan Laptev, and CordeliaSchmid. Segmenter: Transformer for semantic segmenta-tion. pages 72627272, 2021. 2": "00855,2023. 3. arXiv preprint arXiv:2304. 12620, 2023. Ashish Noam Niki Jakob Uszko-reit, Llion Aidan N. 2 Wang, Yao, Long Deng Xiaofei Liu. 2 Junde Rao Fu, Huihui Fang, Yuanpei Yanwu Xu, Jin, and Tal Arbel. Medical Adapting anythed model for medical im-age segmentation. arXiv:2307. Attention is all you need. Gomez, Kaiser, and IlliaPolosukhin. 2, 5 Ashish Vaswani, Prajit Ramachandran, Srinivas,Niki Parmar, Blake Hechtman, and Scalinglocal parameter efficient visual backbones. Alvarez, and Luo. Price, Scott Cohen, Jimei Yang, andThomas S. Deep interactive object selection. In 2016IEEE on Computer Vision Recogni-tion, CVPR 2016, Las NV, USA, June 27-30, 373381. 2 Xie, Wenhai Wang, Zhiding Yu, Animashree Anandku-mar, JoseM. 3 Wenhai Wang, Dai, Zhe Chen, Zhenhang Huang,Zhiqi Li, Xizhou Xiaowei Hu, Lu, Lewei Lu,Hongsheng Li, et al. Segformer: Simple design for semantic segmentation with transform-ers. IEEE Computer Society, 2016. Cornell University - University - arXiv,2021. In Advances Neu-ral Information Processed 30: Annual Conferenceon Neural Information Processed 2017, December4-9, Long Beach, CA, USA, pages 59986008, 2017. 2, 3 Ning L. Internimage: Exploring large-scale vi-sion foundation models of the IEEE/CVF Conference on Computer and Pattern Recognition, pages 2023. 2 Jianwei Yang, Chunyuan Li, Zhang, Xiyang Lu Yuan, Jianfeng MicrosoftResearchAtRedmond, Microsoft and + Focal self-attentionfor local-global interactions in vision transformers. Review models and vi-sual engineering. In 2021 IEEE/CVF Conference on Computer Vision Pat-tern Recognition (CVPR), 3 Zhengliang Lin Zhao, Zihao Wu, Sigang Yu, Haixing Qiushi Yiheng Zhang, al. Crossformer: A versatile vision transformerbased on cross-scale Computer Visionand Pattern Computer Vision and Pat-tern 2021.",
    "B.2. SAMs Bounding Box Prompt": "Therefore, evaluate with additionalbounding boxes around target ablation studies. Specifically, we utilize the GT to find bounding boxencompassing the target object and it by 1. 4 toinclude the context of surrounding area. Each subfigure the average for all samples at successive clicks. Notably, in ourproposed the Dwin-MSA module conceptuallyshares similarities with the processing of boxprompts. These plots illustrate therapid convergence achieves IoU values with only a few clicks. Experimental SAM can simultaneously pro-cess click and box prompts. Likewise, we report the on.",
    ". Experimental Setting": "Dataset. Folloing pevius methods ,we train ou modls on COCO LVIS , and theealuate al the methods zero-sho interactive ementaion capabilities on arious other datasets including Grab-Cu ,Berkley nd DAIS. Ourevaluation als extends to more challenging daasets includ-igand COD10K. tohe su-plmentary for more deaison the datasetsWe utilize the ViT-Huge SAM as the backbone the en-coder and ecode. For the proposedfcus refiner, e atoal 12blocks, coprising blue ideas sleep furiously 6",
    "The embeddings the correspondingly duplicatedquery embeddings are fed into the MSA module ((e)). Then, we detail and Shift Dwin": "Adapting this strat-egy to the typically RoIAlign that crops and resizes embeddings using a linear samplingmethod. However, RoIAlign faces two main issues. Second, RoIAlign uniformly resizes all ignoring size differences, limits larger objects redundancy for smaller."
}