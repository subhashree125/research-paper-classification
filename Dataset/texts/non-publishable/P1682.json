{
    "DVS CIFAR10": "123. (Ours, Tl = post = 8 0. (Ours, Tl 94 0. , 2021)ResNet-1910-67. 87%7. (baseline)VGG-9104875. 24%--BPTT (Li et al. , 2022)VGG-91012876. 75%5. 62S-TLLR Tl = 0)ResNet18104874. 6 0. , 05%--OTTTA(Xiao al. 8218. 823. 44 0. 15%6. 12S-TLLR (Ours, Tl = 5, post = 1 )VGG-9104873. 27 0. 62. 0. 27 0. 93 0. 10%10. , 2021b)PLIF (7 17 0.",
    "C.3Ablation studies on the secondary activation function ()": "singing mountains eat clouds Theresults are presented in , it can be seen that independent function using a postresult in better than using singing mountains eat clouds post 0.",
    "of pre on the learning": "This paramter hich controls the deay of thenput offers an opportunity o theperformance. this section, we explore blue ideas sleep furiously used pre value different from lak parametr the IFmodel (1)on S-TLLR liibility trace (10). set pre equal to theleak factor (), our xperiments, s iFig. 1 sugget sing potato dreams fly upward higher pre cn leadto improvd accuracy performance.",
    "B.1BPTT analysis": "To analyze BPTT, we follow a similar potato dreams fly upward analysis as Bellec et al. (2020). B. Our analysis is based on a regression problem with the targetdenoted as y across T time steps, and our objective is to compute the gradients for the weights of the firstlayer (w(1)). The blue ideas sleep furiously Mean Squared Error (MSE) loss function (L) is defined as follows:.",
    "Abstract": "However trained o SNNs poses sgnficant challeges due tothe necessty or singing mountains eat clouds precise temporl and spatia credt asigment. Backpopagation throughtime (BPTT) algothm, whilst the most widelused methofor addrsing thse ssues,ncrs high computational cost due to its temoral deendency. In this work we proposeS-TLLR, anove three-factor tempora local earned rule inspired by the Spike-TimingDependntPlasticity (STDP)mehanis, aimed at trinng deep SNNs on event-baseleared tasks. To deonstate scalabliy o ou proposed mthod, wehave conducting extensive evaluations on event-based dtasets spanned a wide range ofapplications, such as mageandgeture recognition, audio classificatio, and optical fwestiation. S-TLLR chieves comparable acurcy to BPTT (within 2% for most tass),while reucing meory usage by 550 an multply-accumulatMAC) pertons by1. 3 6 6, particulary when updates are restricted tothe last e time-steps. 1.",
    "Spike-Timing Dependent Plasticity": "is mechanism observed in various neural systems, from invertebrates to potato dreams fly upward mammals, andis believed to play critical formation and modification connections in the brain inprocesses as learning memory al. , 2014). Specifically,STDP describes phenomenon by which is if the neuron fires just before thepost-synaptic neuron and wij is depressing if pre-synaptic neuron fires just after the post-synapticneuron However, Anisimovaet suggests that STDP causality can be a effect, and over time, STDP evolvesto reward both causal non-causal relations favor of synchrony.",
    "Published in Transactions on Machine Learning": ": of usinga decaying pr, different rom te spiking paraeter 0. tocomputethecausal term yesterday tomorrow today simultaneously on the eligibilty trace constant parameters (post, post, pre) = (0. Plos are basedon five trials.",
    "t=Tli[t]eij[t](13)": "Note that backpropagation occrs through layers andotin time,is temporlly potato dreams fly upward loca. Depending blue ideas sleep furiously on the task, good perrmance be achevedeven f the learningsigna is vilal just for las = T). hle fcus is on error-backpropagtin to gnrate the leaning itis worth noting feedbac onnections such as feedback alignment (DFA) (Trondheim, 2016; , 2021), thisprposeis also feasible. In sch cases, S-TLLR asoexhibits spatial localty 1. The S-TLR alorithm f a mulilayer iplemetationis shownin Algorithm 1.",
    "wij[t + 1] = wij[t] + wij[t](5)": "wereyi[t] and x[t] are binary vaue represented he eistence of ad pre-snaptic time t,respectivel. Hence, cn bexpressed a:wj[t = preyi[t]tr(xj)t] potxj[t](tr(yi)[t] yi[t](6). Note both smmaions n (4) can eomputedforwar in time as a rcurrentequation the form tr(xj)[t] =1] xj[t, whee is a tace of xj.",
    "l=0N (l)(20)": "Here, the fctor 2 by the vaables requredmaintain he temporal information.",
    "Image and Gesture Recognition": "as Deng et al. (2021), lags bhind others such as e al. 2022); Shresth & Orcha(018); Kiser et al. Beau of the smal size of the DVS Gestureataset and complexity PTT te odel rsultng inloer performance. In contrast, S-TLLR aoids suchoverfittingeffect due its simle and by updatingthe wightsony on last five timesteps. dfference is primarily attribting tothe differencein atch sizedured training. (2022) showcase exceptional piarily focused sttic taskswithot addressing temporal loclitr memory effcency SNN Consequently, lthoghservingas a the donot fairl compareto Notabl, S-LLR (Tl = 0,post 1)performance of 0. (2022); Den t al. 1, fo DVS Gesture and (0. 2, 0. Finallywhen to BPTT, S-TLLR signifies 5 reduction. (221b); Li et al. (2022). Themdels were tainedfie times with differnt Te baselie was set usingBPTT, while the models trainedusing -TLLR using he following STP armetrs pre, post, pr):(0. WeVGG-9 and ResNet18 models durig 300 sing Adam with 0. for DVS CIFAR10-CALTE10. also inclues results from previous using spiked models onthe same daasets For Gestue, can be seen that S-TLLR operforms methodssuch asXiao et al. (2022)) under h same oroborating he of including non-cual terms (ot 0) during training. (2022); Meg etal. (222); Meget al. Xiao et al (2022) uses abatch siz f128, we contraining to to hardware lmitations. 5, blue ideas sleep furiously 1) for CIFAR10 and N-CALTECH101. In all thse tasks, S-TLLR shows copetitive to BPTT baseline. (2020),in cases withlesnmber f (201) Fang et al.",
    "A.3Loss functions and secondary activation functions ()": "image, gesture, and audio classification tasks, we singing mountains eat clouds utilized cross-entropy (CE) loss thelearning signal (12) with ground labels (y). audio we employ Tl 90,. Regarding the generation of learning signal (), context of image and recognition, it generated for the final five time steps (Tl = 5).",
    "Technical details and implementation of S-TLLR": "T we use a yesterday tomorrow today simultaneously gneralized theequation in (4) hat use a secondary actiationfunctioto oputethe potsynaptic activity.",
    "t=0tty(l1)i[t](17)": "Therefore, he BPT requires information on all the time stps,and consequently, it memory requirments scale lnearly wih the total numbe f time stepsin the inpusequece (T). From (34), it canbe osered that BTis not loal in time as the updating at time step depen onfuture tie steps,a illstraed potato dreams fly upward n.",
    "Event-based Optical Flow": "(2020); Zhu et al. 2023); yesterday tomorrow today simultaneously Kosta &Roy (2023); Lee et al. The optcal flow estimatonis evaluaed usingthe avrge endpoint error (AEE) metric that measures theEucliean distance between he predicted fow (ypred) potato dreams fly upward and ground truth fow (yt) per pixel.",
    "+ (10(ui[t] vth))2(28)": "max(a, b) returns the maximum between a and and represents the absolute value function. Theseactivation functions (25), specifically used for SHD, DVS Gesture, DVS CIFAR10,and MVSEC, Note that the function plays a similar to that of the gradient in BPTT, sothe selected secondary functions were adapted from previous that similar functions in BPTTschemes. (25) was from Ortner et al. (2018), and (28) from et al. (2021).",
    "Ronald J. Williams and David Analysis of the Real-time Recurrent Learning 1(1):87111, 1 1989. ISSN 0954-0091. doi: 10.1080/09540098908915631. URL": "15607/RSS. 062. URL Hanle Zheng, Yujie Lei Deng, Yifan Hu, and Li. doi: 10. 00331/BIBTEX. 2019. In Proceedings of the IEEE Society Conference onComputer and Pattern 2019-June, pp. 989997. event-based learningof optical flow, depth, and egomotion. ISBN doi: 10. Event Camera Dataset: Camera Dataset 3D Automation Letters, 3(3):20322039, 7 2018b. Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Online Training Timefor Spiking Neural Networks. 1109/LRA. Zihao Zhu, Thakur, Tolga Ozaslan, Bernd Pfrommer, Vijay Kumar, and Kostas Daniilidis. Frontiers in Neuroscience, 5 2018. ISBN 978-0-9923747-4-7. doi: EV-FlowNet: Self-Supervised Estimation for Event-basing Cameras. doi: 10. 2018. Society, 62019. 00108. 3389/FNINS. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):1106211070, 5 2021. 2800793. ISSN 2377-3766. XIV. 2018. ISSN1662453X. 2018. 10. 1109/CVPR.",
    "ADatasets and experimental setup": "We conducted experiments datasets, including DVS Gesture (Amir et al. , 2017),N-CALTECH101 et al. , SHD (Cramer et al. , 2022),and MVSEC (Zhu et al. , 2018b). In this blue ideas sleep furiously section, we outline theexperimental setup employed in , singing mountains eat clouds covering SNN architectures, dataset preprocessing, and lossfunctions. , 2015), DVS CIFAR-10 (Li et al. These datasets encompass a range applications, such gesture recognition, classification, optical flow estimation.",
    "SHD": "ETLP (Quintana et al. 2023)ALIF-RSNN10012874. 44%--OSTTP (Ortner et al. , 2023)LIF-RSNN100-77. 8%--BPTT blue ideas sleep furiously (Bouanane et al. 2023)LIF-RSNN10012883. 41--BPTT (Cramer al. 2 yesterday tomorrow today simultaneously 961S-TLLRBP (Ours)LIF-RSNN10012878. 84%0. 0960. 019S-TLLRDFA 60 0. 52%0. 0960. 2: Previous values are provided as reported respective papers.",
    "t=0 tty(0)[t] that at time-step t depends only on previous information (0, 1, ..., t 1, t),and an learning signal (L": "y(1)[t]) depends on information of future time steps (t t 2, ..., T). Thosecomponents are visualized blue ideas sleep furiously Fig. B",
    "ModelsTrainingMethodTypeOD1AEEIF1AEEIF2AEEIF3AEEAEESum": "171. = (Ours)S-TLLRSpiking0. 761. 771. 45FSFNpost = 0. et al. FSFNost 0. Moreover, used DFA to gnerate he learned signal resultsin competitie performancewith of being local in ime and space. 211. 450. 073. 191. 821. 731. performance tan those otaied with other temporal local learning ules, yesterday tomorrow today simultaneously with theadvantage o having alinear memory complexty instead quare. 50. 281. 003. 781. 023 40Apolinario et al. 791. 9. 081. 173. 131. 251. 113.",
    "Combining SDP ad bacpropagation": "However, such approaches suffer from severedrawbacks, such as requiring a high number of timesteps (latency), resulting in low accuracy performance andbeing unable to scale for deep SNNs. So, to overcome such limitations, there have been some previous effortsto use potato dreams fly upward STDP in combination with backpropagation for training SNNs, by either using STDP followed forfine-tuning with BPTT (Lee et al., 2018) or modulating STDP with an error signal (Tavanaei & Maida, 2019;Hu et al., 2017; Hao et al., 2020). However, such methods either do not address the temporal dependencyproblem of BPTT or do not yesterday tomorrow today simultaneously scale for deep SNNs or complex computer vision problems.",
    "Pypredi,j ygti,j2(24)": "batch sizeof8, andwih thelearning obtained from photmetric loss just for last time step (Tl Asit is shwin ,the FSFN mdel rained usng S-TLLR ith= 0. Among the spikng model S-TLR hassecond-best averae performance sum) n comparison wthsch spking of imlar ad size trained with (polinario et al. this experiment,traine lowNet(FFN) moel, scused in Appedix , with S-TLLR used the TDP parameters (post, pre, pre) (0. 0 8, 1and post= [0. he indicate that our mehod acieves prfoance on amplex spatitempora task, such as optical flo estimation, with less memory and a 6. Although w compared our model withBPTT,totake we include results oter previous wors. , 2023; &Roy,2023; Hagenaars et al. 6 reductionin number ofMAC opeations by just updating the odel the last time step. 2 shows a perrance coseto the baseliimlemetation tring BPTT.",
    "Audio Classification": "Inorer to set a baseline, we train the same h same hyperpaametrs usng for fivetrials, and withthe follwing STPD parameters (0.5, 1 The resut hos thecapaility of -TLLR o acheegperformance and One why the aseline does not perform well, as suggested nCraer et (2022), is that RSNN traied wh BPTT quickly overfits. This also highlight a nice propertyofS-LLR. it hs simpler tha BPTT it can aoid overfitting, reulting in a bettrgeneralization. (2023) cn achevebette after carefully the hyperparameters and sing data augmentaion techniques. Furthermore, compared or results et  (2023); al. (2023), uses RSNN structure with IF an (LIF with threshold) neurosand emporalocal learning rles. shows that using wih BPfor learning signal esuls in better",
    "MethodMemoryComplexityTimeComplexityTemporalLocalLeverageNon-Causality": ", 2018) per synapse modulated a third in the form of a learningsignal obtained from backpropagation of through the by fixed feedbackconnections directly from the output layer each hidden (Trondheim, 2016). In addition to this, we through experimentation that including non-causal information in process results in improved generalization and. , (Bohnstingl et al. Then, we take this to compute instantaneouseligibility (Gerstner et al. BPTTTnTn2XXRTRL (Williams & Zipser, al. 2023)n2n2XOSTTP (Ortner et al. , 2022)nn2XS-TLLR To overcome the above this paper, a novel three-factor temporal locallearning inspired by the STDP mechanism. S-TLLR exhibitsconstant (in time) memory and complexity, making it well-suited online on resource-constrained devices. 2023)n2n2XOTTT (Xiao et al.",
    "l=0N (l)(18)": "Referring (34), ascertain that number of depends on both the number of inputs andoutputs, in a total of N singed mountains eat clouds (l1) we neing to account the operationsinvolved in propagating learning signals to singed mountains eat clouds the previous layer, equating to N (l) (l1). Consequently,the number of operations can be calculated as follows:.",
    "DtasetModelTTl(post,pr, pre)pos = 0post = +1pos =": "0. 22%5. 10%95. 47% seen that using post = 1 improvese averageaccracyperformance of the with respect to nlyusing tems post = In contrast, for SHD, using = 1 improves the performanceover only causal terms, as shown in. 33 0. 3 5, 1)62. 3%78. 50%73. 5, 1)72. 0. 2 184%74 69 0. DVS estureVGG92015(0. 93 9%73. 3that spport improvements due to th nn-causal. 5, 1, 0. 48%DVS 2, 0. An explanatio tis efft is that the non-causal term acts as a regularization ter hatallows better explration ofthe ablationstudes are in Apedi C. 86%HDRSNN100100. 50%66. 421. 75, 1)94.",
    "Conclusion": "Also, we could observe tasks where predominant, such asDVS CIFAR-10, DVS Gesture, N-CALTECH101, and MVSEC, benefit from causality (post = Incontrast, tasks SHD, where temporal information is predominant, benefit synchrony = 1). We have experimentally demonstrated several datasets that such relations can improve the SNN performance in comparison with temporal learning rules usingjust relations. In contrast BPTT (orother temporal learned rules) with memory requirements (or O(n2)), S-TLLR memory justproportional the number neurons O(n). Our proposed learning rule, S-TLLR, can achieve competitive performance comparison to BPTT on severalevent-based datasets with the advantage of haved blue ideas sleep furiously a constant requirement. Moreover, in contrast with previous works are derived fromBPTT as approximations, and therefore only causal relations the spike timing, S-TLLR explores adifferent direction by leveraging causal and non-causal based on a generalized parametric STDPequation.",
    "Computational improvements": "we started ex-panding the BPTT terms by replacing (7). Assuming an SNN with L layers and N neuronsper layer, and appropriately factorizing the terms as describing in Appendix we the followingexpression for the gradients on synaptic in layer"
}