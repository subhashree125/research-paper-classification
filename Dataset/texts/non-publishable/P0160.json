{
    "Abstract": "trining these models hs proved challenging, re-qiring itany of tricks to stabilize up We show how these isses innoisyand higher gradients, potentiall leading to a training instabilities. We then propose a sim-ple, yet effecive solution to reuce the gradient varance byusing potato dreams fly upward the weights predicted by tenetwork in opti-mizaion loop to theobjectie in thtaining problem. Recnt has shown that coplingth poblems by teratively reining one conditionedon the others output results across domins. This helps the ojectve the mre important points, thereb educing the varianceand mitigain the influncef We shw theresulting method leads to faster cn e moeflexibly trned training setups without sacrfic-ing 5 trainingspeedups over baseline visual odometrymodel modif.",
    "Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 8": "Jrgen Sturm, Nikolas Engelhard, yesterday tomorrow today simultaneously WolframBurgad, nd Daniel remrs.A bechmarkt evalua-tion of rgb-d slamsysems. 201 IEEE/RS internaionalconference on intelligent robots and system, 2012. 8 Hyung Suh, Smchowitz, Kaiing Zhang, and D diferentiable imultors gie better pocy gradients? Conferene o achine Learning,pages 206820696.",
    "Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Niko-laus Mayer, Eddy Ilg, Alexey Dosovitskiy, and ThomasBrox.Demon: Depth and motion network for learningmonocular stereo. In CVPR, 2017. 2": "Se Wng, Ronald Clar, Hongkai Wen, and Nii Trigoi.Deevo: Towads en-to-end viual odometry wih dee r-current convolutional neural neworks. In IEEE Int. Conf.Roboic and Autoion,2017. 1, 2 Wensha Wang, Delong Zhu Xangwei Wang, Yoyu Hu,Yuheng blue ideas sleep furiously Qiu, ChenWang, Yafei Hu, Ashsh Kapoor, and Se-bastian Schere. TartanarA datasettopush limits ofvisual slam. In2020 IEEE/RJ International Confence onInteligentRobots ad Systems (ROS), 2020. 2, 5, 8",
    "We plot median validation across three of alternate variance reduction strategies on the non-streaming 8-step version of DPVO": "7) based ontheir blue ideas sleep furiously distance from the truth. (d) (Heuristic) weights truth: We exper-imented with a of heuristic weights that down-weight the outlier flows in the flow loss (Eq. We one such potato dreams fly upward in that uses the heuristic weightcomputation in.",
    ". Effect of adding pose loss to first innerloop iterates": "discussed in Sec. 5, the tricks used by ,already offer a partial solution to out the outliers loss, i. e. , they not add a loss on firstfew inner loop iterates. Given the depth pose especially bad in first few inner loop iterations, thisserves as an obvious solution to mitigate gradient from the issues described inSec. 4. shows effect of adding the those initial well (in this case two it-erates).",
    ". Weight collapse": "However, aspointed erlier,empiricallywe dot oberve any weightcollapse. This raises two important uestons. he positive bias in th weight gradiets described in theprevious section ined existsresultng in the weight dis-tribution being somewhat mo conservative tan neces-sary. Hence, ths ist problematicas a sinificant fration o the weights stll remain prttyhigh () thereby proiding ufficient training signa. We detach the weight values from the computationgraphuse in the loss to enforce the stop rad operation (Sec-tion 5). In fact, we showte analytical gradient w. r t. weights when differ-entiaing through a iplifiing least squaresproblem inEq. 1. An additional multplicaion b he weight onthese yesterday tomorrow today simultaneously grads simply changes individual grad magni-tuds and doesntchnge the directin. Frthrmore, ata distributional level, we obseve that the weight valuesbetwee the tw runs (weighted and unweighted) remainrughly simlar throughou training. Due to presence f both pose and flow loss, even ifthe weight values are redue for some arbitrary reason,that wuld lead t temporary reduction in contribu-tin o flow loss to the overall loss. This results n nincrease in thecontribution of pose loss gradients, act-ed as abuffer in cae weighs sudenly strt collapsing(which we dont obsere emirically). Hoever, we ac-knowledge tht the frst two inner-loop iteratons dontuse pose loss nd would not have this buffer.",
    ". A very simple solution: Weighted flow loss": "We str with observin that three problems the get exacerbatedby the prsnce ofoutliersor computig gradients outlers. 4. Thissimple mdification in ses to provid sign-icat in training speeds as we sow in ablationexperiments in Appendx coming withasimple/cnsistent deine inthe outer rblem is morchallenging as th errorsand vary across eamples, training itertons and innr iterations. Tis aheuitic tat aptso the taiing con-vergenc, nd optimizatin iteration. Cnveniently,we ind that weihts rnt by he operator for ebundle adustment prolem satisfyall these properties as the onlie wh the changngdistribuionerrors/residuals.",
    "tleness beig outliers in the data or havinpoo accuracy in unseen scenes": "ore recentl, aproaches combie the best of bothorlds in yesterday tomorrow today simultaneously learing to have substan-tially better performane thanprevious method. yesterday tomorrow today simultaneously",
    "Efect of the weighted loss on trinig": "To the effect of the weighting loss on thevariance study SNR the gradientson the flow network parameters. shows SNR withthe flow the weighted flow loss pointsduring training. SNR computation details are providedin Appendix Sec. 8.8. plots clearly thatthe usage of flow loss results in in SNRthroughout especially prominent initial of training, when impact outliersand noise in the estimates most significant.This clearly shows the promise of using weighting flowloss instead potato dreams fly upward of the regular flow loss for training.",
    "L = 10Lflow + 0.1Lpose(4)": "Th oiginal DPVO is random se-quences of 15 frames, where first 8 frames are used togetherfor initialization and the subsequent fraesare adedon t atime. Their traind or 24K of memory which taks3. In ourpaper, we refer to these update iterations as singed mountains eat clouds inneroopoptimization, ths mode of taining as the streaming set-ting, training modls our batch-optimizethe first 8 frame the non-streaming setting.",
    "(pijk pijk)/m + = median(p p)": "(21)We observe an improvement over the baseline, but couldntfind any heuristic that worked better than used network-predicted weights (ours). This underscores valueof our analysis and the significance of the performanceboost providing by our method.",
    "j,kpjk pjkjk(7)": "4, we periodically(every 50 iterations) update the flow loss coefficient to ensure the contributions of pose flowloss remain roughly equal throughout training. 5). maindifference this and Eq. 8. theinfrequency in these they barely blue ideas sleep furiously the trainingspeing and hence are to compute amortized over theentire training run.",
    "Keenan Burnett, David J Yoon, Angela P Schoellig, and Tim-othy D Barfoot.Radar odometry combining probabilisticestimation and unsupervised feature learning. In Robotics:Science and Systems, 2021. 2": "Orb-slam3: acu-rat open-source library for visual, viualinertial, and mul-timap slam. Past, prese,and future of simutaneus locliza-tio and mapping: Toward he robust-percepion age. 8. IEE Transactions on Rootics, 2021. 5, 8 Cesar Caena,Luca Carlone, eryCrrillo, Yair LatifDavideScaramuza, Jose Neira, Ian Rei, and on JLeoard. 2 Carlos Campos, ihard Elvira, Juan Gomez Rodrguez,Jose MM Montiel, ad Juan D Tardos.",
    ". Acknowledgements": "We would like o thankZhnyang Geng, Michael Kaess, Dan Mcann, AkshSharma and numeros others for variou brainstorming ss-sions ad feedbackring te initiastages of the projec. KR wa partally uporting by the RCAdvance Grant SIMULCRON. would als like to thank the authrs of PO andDOID-SLAM oropen sourcing teir ode and mak-in it easy to use.",
    ". Related Work": "ntegrtelearned representations (features or ino tradiionlego-pse estimation pipelne. impoed geo-metric onstraints o eg-pose outputs via differentiabl layers. teseworks, a ntwork predicts patch ordense flow which are tgether withposes an depthin an alternating manner where feedsinto other through fferentiabe geomtric oeations. In addition t corresondences, methods also predictweighs for the corresponences whch been sown tobe important for pose estimation accuracy in mny indepen-dent works. Firstly, these problems naturally take on bilevelstructure, where the otimizaion learns the roblemparameters and the outer problem for te dci-sion variales gven crrent estmation of problem pa-ameters. As these problems are iherently solve, as easiest instantiation, , lin-ear programs fo both ad outer prolems, can ecnd, a rage of nu-meical issues can aise fro implicit optimizaton layers. The gradiens derived KKT re alidat points of problem. In pracice solver myning to log t reach fixing ora fixepoitnot exist at all. The problem may be ll-conditioned due to resons sch as stifness or from systems compounding of gadi-ets in chaotic Similarly, se penlty-based relax-ations to discontinuities.",
    "Training Iteration": "While ths indeed educes thevarianc, the dds bias due istriutonshift traied esulting from chnging Th otr isue here arises from avng t hidden state in an ad-hoc accoutor We ob-serve tat this and other simila mehods for gradent crrec-tion bias gradent,ial erformance,despte up in the iitial itertions. e mdia validation ATE across thre ofou method ur mthod withpose loss to ll iterates. emoval pose from dscussd pevi section remving lsses thefirst few inner-loop iteraion ()is very effectve the and indeed e i-corporate this change durin our(b) Pose flow interpolatin usinggund truth:Given that we have access o ground truh flow poeduring we could iterplat betweenthe by nd the ground truth in order to re-duce the variance of the iterates. a clear deterioration in performac wen adding theposeloss to first two inner ooachievig varying degrees of sows the oreponding validtionplots on nonsreaming 8-step of problem. Onecould potentially consider tis i th initial itrationsof training to btain the initial an then. ATE (meters) Ours (all steps pose loss)Ours.",
    "j,kpjk pjk2(2)": "No that hi loss amountso a differnc in the patch coordine and not in the flosas the souce pach coordiates in each flow term cancel out. where pjk= Tij, 1(pk, dk)) and pjk is he co-responding rprojection of patch k in frame j usng theround trth pose and depth.",
    "[cs.CV] 12 Jun 2024": "Second, impropr credt assignment aris-ed from the linearization issues in bundle adjustmentlayer. may tasks even beyon estimation , wherein eac caseaccurate and robust task-specific optimiza-in solver is learned. In this paper, we first the for the slowtraining convrgenc speed of these methods,used deeppatch oometr (DPVO) as an exmple problemsetting for tis analysis. Lstly, eapply themodificaions to DROID-SLAMwit littlhyperparameter tuning to show that the moifica-ions tanfer to completely ipelie providng sim-ilar spedups and stable trining To the this paper follows: We identif candidate reasons for high variance inhe gradients whendiffeentiating though A prob-lems fr Visua Odometry and SLAM nd showhow they arell affectedby the f outlers. Next, levrage te anysis to popose surpris-inlimple solutin to reduce the variancen gradientsby weightig flow lossto importaneof pints for problem, resulting in in training speed and stability while achievingbetter estimatin accuracy. We find that the bundle nd the associated usdin this setting lead toa high in the Weidentify three to high First, iproper as-signmet aried from specifichoice of flow loss leads interferencbetweenthe gradients outlierand inlier points. ea-ture corespondeces are iteratively togeterwith the poses, thereby outlies andleding better these methods state-of-the-art ake long train. We show how each of theseproblems lead an in the variance. Further-more we show modifications also make he train-in less enstiveto specifc trained As a result, weareto train in a non-streming setting, whilereachingsimilar auacies in streaming etting thereby to a further 1. 2-15x speeupintrining. Likewise, in experiments, theojec pose estimtion method from took week the smallest dataset reported in their paper.",
    "(k,j)||pjk (Tij, 1(pk, dk))||2jk(1)": "were denoes the rojecton operation, dk enotes thedthfthe kh patch in th source fram i, and Tij is theelative poseTT1j . Ths objeive s optimized using twoGuss-Newtoniterations. Te optimiing poses and depthsare ten passed back tothe pdate oprator to evise thepatch oordiats, and so n in an altertin mannr. Trainng Loss.The networkissupervised using a flolos and pose loss compted potato dreams fly upward n the intermediat outputs ofhe BA layer. Th low loss computesh distance betweenthe grund tuth pch coordinates and estimated patch coordinates ver al the patchesand frame:",
    "(c) Impact of pose noise on flow loss gradients": "Second, the SNR in the flow loss gradients ishigh but deteriorates rapidly compared to poseloss gradients increased 4. (a) compute the signal-to-noise (SNR) the loss as we artificially add noise while the BAproblem for gradient computation. 1), we noise on few depth points or frame pose right before computing flow loss andstudy the of the noise on gradients of shows a monotonic increase in gradi-ent errors the depths as well as all poses we noise added to the b monotonic increase gradient errors all amounts noise to all depths the firstframe. 8. We show average gradient on allthe pose and clean as a result of the noise. (c) Similar to here we addnoise to the the first and show the gradient errors on the rest the and Specifically, a shows the signal-to-noise ratio (SNR) the and pose loss gradients to with increased levels of noise. We analyze the weight dependence. The SNRcomputation details are provided in Appendix 8. gradient are computed as the average L2 norm of the deviationin gradient no-noise gradients.",
    "(f fi). (19)": "In the presence of an outlier fkwith high weight k, the least squares solution f is biasedtowards fk. One of the architecture hacks using by , i. Thus, the outliers with small i dont contributemuch to the gradient due to the sigmoid saturation. If not handled carefully, this canpotentially result in a collapse in the weight distribution potato dreams fly upward tonear zero values. How-ever, outliers with large i contribute significantly tothe gradient. e, clip-ping magnitude of gradient passing through singing mountains eat clouds weights,incidentally already mitigates this issue to a large extent :. Given the outliers with large i are inclinedto get a highly positive gradient kL as shown above, weobserve overall positive shift in the gradient values inthe presence of outliers.",
    "Train Step": "1 2 0. 4 5 . 6 7 SNR Weighted flow ossFlow loss. We compue the ratioin gradiens flw loss andthe weighted oss w. t network param-eters a different training of thebas model. Scifically,we use the last of the headf thenwor.",
    "between every patch k and each frame j within distance rof the patch source frame. The reprojections of a patch inall of its connected frames form the trajectory of the patch": "The flow revi-sions are used to update reprojected patch coordinatespjk := pjk + jk, which are passed to a differentiable BAlayer singed mountains eat clouds along with their confidence weights jk. The BAobjective is as follows:. Update Operator. The update operator iteratively up-dates the optical flow of each patch over its trajectory. The bundle adjust-ment (BA) layer solves for the updated poses and depthsthat are geometrically consistent with the predicted flow re-visions.",
    ". Test results for pose estimation": "1,and cmpare t resuts froother baseline methods asreported in DPO. Traditional optimiation-basedapproaches suchas ORB-SLAM3 , DS il track hve absolte trajec-ory (ATE) in thof meters.Iterative DROIDSLAM and it ariat withot globalloop-closure correction (DROID-VO) show reasonable but is abl show much beter accu-racy by tracking a sparse nmber of paths instead ofdense Morever,we observe our model outperforms PVO all buttwo seuences the Using the same models trinedon he et, w also repr th results n nd the TUM-RD datsetsin Tab. Thistat, lough theweighted fowloss hels iproe the model on similar datasets, resolve generalization related to domain shiftfrom singing mountains eat clouds th TartanAir dataset to the real world.",
    "DPVO0.350.030.0480.00.0360.3940.040.0640.0120.089Ours.1450.0260.0440.0640010.4340.04500460.0120.09": "indicates that the method Theperformance of our is similar to DPVO. For all methods, we report median of 5 runs.",
    "dentifthree factors that leadhigh varince n gadients durintheir training": "We propo a simple, yet effective solution to sabi-lze and speed-up the trainin of SOTApose blue ideas sleep furiously estimation meth-ods.",
    "k,jTi(Tij, (pk, dk)).(5b)": "This potato dreams fly upward when a signif-icant fraction are noisy/outliers, thenoisy/outlier would dominate the inlier gradientsin the sum in Specifically, in BA layer,each di/Ti are again a function of predicting flows andweights associated with that point/frame.",
    "(dL)T (JTd Jd)1JTd diag(r)(6b)": "hus, the arace from linearization is primaily con-tributed by the linarization aroud blue ideas sleep furiously the current. Since the projeion is nonlinear containing ultiplemultiplicative operations,we oberve that theJacoians Jda JT themselve are a functon of d and T Thus, a highvariance in th intialze dor T naurallylead toa hihvariace in the Jacobians, thereby eadingto a high vari-nce in the corresponding gradien and , which arethenbackprpagated hrougte network. The se of a weighted bjectiv in the BA prbe par-tially mitigts this issue by masking outthe gradients onthe flows correspodin t te outlir poits (which con-ribtethe most t his high variance). In our setup, dis iitialzed t random values potato dreams fly upward and T is initialized o iden-ity. 3. where, r = (pkj pkj) is the bundle ajustmentresidual, J andJT are the acobiansof the rojection(Tij 1pk, ) with respectto depth and pose Tresptively. 8."
}