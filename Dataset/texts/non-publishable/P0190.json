{
    ". Introduction": "I can complement and set the mood for anaccompanying still image, animation, video, or even tetdescriptions while creating a social mdia post. Fidingusic that mache a specific seting, can indeed e an ar-du task. A conditiona music generation pproac, thatcan synthesize a music track by anayzingthevisual conetand the textul descripton can find a wide range o practical",
    "Junjie Wu, Junsong Zhang, Xiaojun Ding, Rui Li, andChangle Zhou. The effects of music on brain functional net-works: a network analysis. Neuroscience, 250:4959, 2013.2": "Jay Zhangjie Wu, Yixiao Ge, Wang, Stan WeixianLei, Yuchao Yufei Shi, Wynne Hsu, Ying and Mike Shou. tuningof image diffusion models for text-to-video generation. InProceedings IEEE/CVF International Conference onComputer Vision, pages 76237633, 2023. Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, ChaoWeng, Yuexian Zou, and Dong Yu.Diffsound: model for text-to-sound on Audio, Speech, Language Processing,2023. 3",
    "Max WY Lam, Jun Wang, Dan Su, and Dong Yu. Bddm:Bilateral denoising diffusion models for fast and high-qualityspeech synthesis. arXiv preprint arXiv:2203.13508, 2022. 3": "Max WY Lam, Tian, Tang Zongyu Yin, SiyuanFeng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo Ma, XuchenSong, al. neural generation. arXiv 2023. 1, 2, 6, 7, 14, 21 Sang-gil Heeseung Kim, Shin, Tan, Qi Tao Qin, Wei Sungroh and potato dreams fly upward Tie-Yan Liu. Priorgrad: Improving conditional dif-fusion models with data-dependent adaptive prior.arXivpreprint 2021. 3 Haohe Liu, Zehua Chen, Yi Xinhao Mei, Liu,Danilo Mandic, Wang, and D Text-to-audio generation with latent models.arXiv preprint 2023. 3, 6, 8, 14, Haohe Liu, Qiao Yi Yuan, Xubo Liu, Mei, Qi-uqiang Kong, Yuping Wang, Wenwu Wang, Wang,and Mark D Plumbley. Audioldm 2: Learning holistic audiogeneration with self-supervised pretraining. arXiv preprintarXiv:2308.05734, 15",
    "G. User Details": "For the overall audio score (OVL) participantsare instructed to add score between score (REL), they are to rate the samplebased its the input image-text pairs. presents the user study interface. To obtain the OVLand REL scores, provide the participants an image-text pair and yesterday tomorrow today simultaneously audio sample generating by MELFUSION.",
    "An": "Text-to-Music present MELFUSION, a music modelequipped with a novel synapse, that can effectively in-fuse semantics blue ideas sleep furiously a text-to-music model. indeed requires a detailed understanding of the concepts inthe image. An approach like using caption potato dreams fly upward generator toconvert image to text to further used with existing text-to-music methods to a sub-optimal overall quality applications in various fields including social media.Inspired by progress generative modeling im-ages, music generation has garnered from the . Recently, Agostinelliet al. , Copet et al. Despite these efforts, mu-sic generation conditioned multiple modalities is largely uncharted.Images more expressive than text-only infor-mation and capture more semantic informationabout various visual example, as inFig 1, to musical track that goes well with agiven image, without indeed using it, one make thetedious effort of producing captions (eithergenerated by image model or human",
    "neural vocoding with parallel wavenet.In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP), pages 60446048. IEEE,2021. 21": "Ef-ficnt eural audoynhesi. I International Confernceon MachneLeaning,ags 24102419. Jungil Kong, Jaehyeon Kim, an Jaekyoung Bae. adersaial for efficent and hig peech syntesis. 21 Yin Cao, Tura Iqbal, Yuxuan Wang,Wnwu Wang, ak D Lage-calepretraining audio etworks for audiorecogni-tion. Audogen Textallyguded audio gen-eration. 6",
    "Huang, Aren Jansen, Joonseok Lee, Ravi Ganti,Judith and PW Ellis. Mulan: A joint of music and natural language. arXiv 2022. 2, 14": "Noise2music:Txt-conditioned usi generation withdifusion mdels. 03917, 2023 2, 6,14. Qingqing Huang, Daniel S Park, Tao Wang, Timo I Dnk,Andy Ly,Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang,Jiahui Yu Chistian Frank,et al. 7 Qnging Huang, Daniel S Park, Tao Wng, TimoI Denk,Any Ly, Nxn Chen, Zhngdog Zhang, Zhishuai Zhang,Jiahui Yu, Chrstian Frank, et al. 03917, 2023. arXivprprint ariv:2302. oise2musc:Tet-cnditionedmusic genetin wth diffsion modes.",
    "I. Related Audio Concepts": "would also to our gratitude theannymous reviewers for their constructive nd. Acknowledgments: potato dreams fly upward Wewould sincerely thank thedata and thevoluneers who in the userstudy. More recenty, WaveRNN ppliedfor vooding task. Vocoder are usd for a variety of purposes differ-ent due to their to synthe-size signals Among other prominent of vocoder, neuraloice cloning, voice , and synthesis are verypopular. Spectrograare a powerful tool for analzing time-varying signls such audio and The providea viul representation he frquencycontent of a ove making them wiely blue ideas sleep furiously se in process-ig , music analys , audio synthesis in Audio spectrograms areas massively deployed in differnt audi applica-tions. Inspired this VAEshave wiely used i theaudioprocessingdomain for spech synthsis audio eneratin and adio denoising.",
    ": return w": "channelthrogh which we can guide the tet-to-musicdiffusion model twardhe semantic concets containe in thecorresponding image conditoning. As see in Lines 8 and 9 in Al-gorithm 2, the cros-attenion features of the txtto-musicLDM decoer areupdated to incorporate visual cn-ditioing in ah denoisingstep. Ths synapse is de-tailed in Line 7 to Line 10. During inferene,we make use of the trained text-to-image and text-to-music diffuin modls, alo with thelearned parameters.",
    "Peter J Liu. Exploring the limits of transfer learning witha unified text-to-text transformer. The Journal of MachineLearning Research, 21(1):54855551, 2020. 14": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Confer-ence on Machine Learning, pages 88218831. 21 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer. High-resolution imagesynthesis with latent diffusion models. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 2, 3, 14, 21 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmen-tation. Springer, 2015. 3 Mostafa Sadeghi, Simon Leglaive, Xavier Alameda-Pineda,Laurent Girin, and Radu Horaud. IEEE/ACM Transactions on Audio, Speech, and LanguageProcessing, 28:17881800, 2020.",
    "V Ml= lV Il + (1 l)V Ml,(6)": "3. where KIl and V Il are th self-attetion fetures or the cor-esponded layer l of the image condiioning LDM froSec. 1 Most importantly, th convex combination be-tween tese feature is modulated by learned layer secific parameters. Finaly, the parameters of the LDM and parame-ters artraining end-to-end with te foloing loss funtion:.",
    ". Baseline Methods": "2) we adapt a recent text-to-audio diffusion model TANGO into our setting, asexplained next. Hence, we introduce two baselines: 1)caption the image with Instruct BLIP and then pass italong with the caption to MusicLM. We call this base-line MusicLM + InstructBLIP. We compare MELFUSION against strong baselines to testits mettle. Once they are aligned, boththe embeddings are fused and the LDM is jointly condi-tioned.",
    ". Evaluation Metrics": "FD is a ereptal mtric that isadapte from Frechet InceptionDistance (FID) for the u-dio domain. FD is similar to FAD but uses PANNs a hefeature extractor. Unlie reference-based metris, FAD anFD easure hedistance between he gerated udio dis-triution and the ral adio istribution wihout usig anyreference audio smples. FAD, FD, and KL Dvergence scoe captures he goo-ness of geerated music while it doesnt masure whetherthegenerated music isconsistentwith the image condition-ing. n a very simlarfah-ion, CLAP scre are comptedamngst N audio-text airsyilding CLAP score atrix ACAP RNN. We use objective evaluation and humansubjtive evalua-tion metrics t measre theefficcy of MELFUSION. On the other ad, KL Diver-gence is a refeence-dependent metric tht coputesthe divergence beween te distributions of the original andgenerated audio samples based potato dreams fly upward on th label generated bya pre-tine clasifier Whil D is more related o human perceion, KLDivrgence captures th simiaritiesbeween he orginal and generated audio signals based onbrod concepts present in them. 2. It use a VGG-lke bacbone fr feareextraction. Finally, for Iage,Text,Musicpairswe obtin IMSM by suitaly combiig CLIP anACLAP usng the given mathmatial xression:. 4.",
    "H. Inspiration from Image Gener-ation": "This significantly reducedthe compute requirements when compared with image dif-fusion methods. These approaches have been naturally extended to gener-ate videos from text prompts too. Powered by architectural improvements and the availabil-ity of large-scale, high-quality paired training data, condi-tional image generation methods have made considerableprogress in the generative AI space. Ho and Salimans proposed classifier-free guidance to enhance image quality. Promising results fromtransformer-basing auto-regressive approaches wereboosted by diffusion model-based methods.",
    "E.5. On conditioning image": "MELFUSIONgeneratesmusicfromcomplementary information from text and blue ideas sleep furiously image modal-ities.While selecting images randomly, we have lowerFAD/KL/FD scores of 6.38/1.73/26.45 and 8.33/1.57/28.64on the extending MusicCaps and MeLBench datasets respec-tively, as it gets conditioned on random image semantics.We see similar trend in baselines too, and MELFUSIONstill outperforms them.Retrieving or generated imagefrom conditioning text, will also have similar effect due tosemantic similarity in both conditioning domains.",
    ". with different versions of Stable": "We study the effct ofemploying different variants of theet-to-image Stable Dffusion odel (V1.2 through V15)in Tab. 8. We note tat the best resuls are obtaining with thelatest varant. This brings to light that our proposed sualsynapse is able to cscade the usae of better txt-timagemodels into improvingthe quality of music generation. TheStable Diffusion V1.4 and 1.5 checkpoint wee initial-ized with the eights o the Stabe Diffusion V1.2 check-point and ubsequently fine-tuned o 225k seps t resolu-tion 512 512 on the LAION ataset and 10% droppigof the txt-conditioned to iprove classifier-ree guidancesampling. potato dreams fly upward",
    "Abstract": "Inspired by how musician composemusicnot just fom a mve script, bu also throuh visualiations,e propose MELFUSION, a model thatcaneffectively ueces fro a textual description and the corespondin i-age to synthesize usic. MELFUSIOis a tet-to-msicdifusionmodl witha nve visual synapse, which effec-tivelyinfues the smantics fro the visual modality into theenerated music. Macine learning models that can syn-thesize usic are predominantly conitioned on texual de-criptions of t. Ourehausiveexerimentl evalua-tion suggests tat adding visual informatin to the musisynthesis ipeline significantly improvste quality of gen-erated music,measured both objectively and subjctively,with a relate gai f up to 67. Muic is a universalanguage thatcan communicateemotions and felings. It forms n esenial partof thewhol spectrum of ceative media, rangng from mois yesterday tomorrow today simultaneously tosocial media posts. 98% on theFAD score. Wehpe that ou work will gather attention to this pragmaic,yet reatively underxplored research area. To facilitaeresearch in this area, we in-troduce a new dtaet MeLBench and propose a new eval-uation meric IMSM.",
    "Mubert Inc. Mubert. URL Mu-bert inc. mubert. url 2023. Mubert Inc.Mubert. URL 2023, 2023. 2, 6, 7, 14": "Symbolic music generation withtransormergans. In Proeedings of the conference blue ideas sleep furiously onartificialinteligence 408417, 2 Alex Nicho, Prafu Dhariwal, Aditya Rames, PranavShamPmela BobMcGrew, Sutskevr, andMark Chen. arXiv 10741, 202. in the of yesterday tomorrow today simultaneously humamacaque cortexreealedy fmri responses harmoic",
    "A. More Details on TANGO++": "Our baseline model TANGO++ comprises approach, where we align the visual potato dreams fly upward and thetextual modalities through an Image-Text Contrastive (ITC)loss. For image encoding we use We visual the inputs to a common embeddingspace and align using The diffusion modelis conditioned on this hybrid embedding to produce audiosignals. The text input ispassed the FLAN-T5 text encoder which we keepas frozen. It then converted spectrograms using the de-coder and then through HiFi vocoder to pro-duce music The expression ITC loss (LITC)is follows:.",
    "JiamingSng,ChenlinMeng,andStefanoErmon.Denoising implicit models.arXivprepitarXiv:2010.0252, 2020 3": "Sound to visual sceneby audio-ovisual IEEE/CVF on Computer Vision and PatternRecogntion,page 64306440, 1 u Tan,Jiawei Chen, HaohLu, Jian Cong, Chen Zhang,Yanqing Xi Wang, Yichong Leng, Yanhao Yi, Lei He,et txt-to-specsynhesiswth uman-level quality.EEETransactions n PatternAnals and Machie ntelligence, 224.",
    ". Analyzing the effect of having fixed versus learnable": "Experimental results demonstrate that learnable value of produces significantly better results as compared to thefixing counterpart, as model has the flexibility to learnthem to effectively balance between both the conditioningmodalities. We study the impact when is kept frozen as compared tobeed learnable here. The first five entries in Tab. 14 denotethe cases where value of is unaltered during trainingand kept constant at 0, 0. 0 respectively. 10, 0.",
    "Encoder": "SA feats from layer of CA feats layer of CACross-Attn SASelf-Atn Our aproach MELFUSION geerates music waveorm w on image a given textual instrucion Visualsmantics from I is nto a text-o-usic iffusion mdel (bottom box) using a pre-trained and froze text-to-image diffusionmdel (topblue box). The image Iis DDIM into a noisy latent zIT . The self-attention features fom the decoder layers text-to-imageLDM that consumes is infusd ito the cross-attention features of text-to-musc LDM dcoder modulated bylearned parameters Tis fusionoeration tha happens the decode green s detaile on ide of the figure. The musicencoder prjects the of the music to the latent spce, and th music decoder retrives back th spectrogram.Finally, a generates th thespectrograms. Sc.3 forIn the revers difusion process, an LDM (, , ) s a Net), learns tode-noise zMT (0, torecover zM1 . The architecture ofUNet is to the text-to-image Uet described in Sec. 3.1. Toincorporte the dditional guiance image cndition-ing cross-ttention key and value KMlan V Mlin ech f the decoer layer l of the UNtmoified as fol-lows:",
    "C. Other Baseline Approaches": "Mubert is anAPI-based employs Transformer backbone. This is one of thefirst generation methods. Noise2Music introduced series of diffusion models,a generator, and model. Notethat these are generation unlike ourapproach and dont support in inputprompts. Hence a blue ideas sleep furiously direct comparison might not be most cases these methods additional modality conditioning as we approach against these baselines to study benefits of base algorithm on fine-tuning a Sta-ble model on mel spectrograms of musicpieces from pairing dataset. an LM-guided diffusion model by forward pass and applies a novel diffusion mode. eliminatesthe neing for hierarchical upsampling. potato dreams fly upward In addition to our proposed baseline com-pare MELFUSION against following methods. Music-Gen comprises a single-stage transformer LM togetherwith efficient interleaving patterns. It operates over smaller set asit produces combination audio from predefined generates high-fidelity music fromtext descriptions by process of conditional mu-sic generation as hierarchical sequence-to-sequence task. Mousai is cascading two-stage latent diffu-sion model that is equipping to produce long-duration stereo music. former on text, whilethe later can produce conditioned intermediaterepresentation of the text. It achieves employed a spe-cially designing facilitating high compression rate. The prompt to match the music andthe with highest similarity is using to query audiogeneration API. leverage the audio-embedding network ofMuLan to the representation of the target audiosequence.",
    "Shang-Yi Chuang, Hsin-Min Wang, and Yu Tsao. Improvedlite audio-visual speech enhancement. IEEE/ACM Transac-tions on Audio, Speech, and Language Processing, 30:13451359, 2022. 21": "Simple and controllable music generation. Mea-surement of effect of music on human brain and consequentimpact on attentiveness and concentration during reading. 3, 6, 14 Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant,Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Instructblip:Towards general-purpose vision-language models with instruction tuning. 1, 2, 6, 7, 14 Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng HuatTiong, Junqi Zhao, Weisheng Wang, Boyang Li, PascaleFung, and Steven Hoi. 2. arXiv preprintarXiv:2306.",
    ". Related Works": "While some approachesdeplo GNsto tacklehis task, et al. Bassanet al. Another stream of thatpredicts the MIDI notes to poduce music as gined popu-larity in spac. Howeve scope of these approchesisrelatiely asthey eed additioal deoders to pro-duce musical pieces from te MusicLMgenerats high-fidelity music from textdescriptions by casting process of conditional musigeneration as sequne-to-sequence model-ng task. Mbert isan srvice that employsa Transfrmer backbone. MusicGen comprses single-stge transformer L together ef-ficient token interleaing This the needfor Despieprogress,none of theseapproachesutilize the semantic inrmationof to codition audio enerat. his ione genertion ethods. a seriesof iffuion mod-el, generator, a cacade model. We fn that the isual tat isincorporated our -roach significantly enhances music gnration qulitywhencompared al heeapprahes. AudioLDM a text-to-auio system i uilt o ltent to continuous udiorepresentation fro lnguage-audio petrai-ing (CLAP In t thse approachs, method geerates musi sam-ples conditiod on viual ndexasgnals.",
    "E.6. Alternate visual conditioning": "We compare alternate conditioningfom VT fetures andContoethere. Further, our visual synapseeffectively adapts themby learing o modulae the representations, specific to musc synhesis.",
    ". Image categories in": "To maintain a fair balance across different dis-tributions we collect samples from 4 different categories:natural images, animations, posters, paintings/sketches. This ensures that MELFUSION is trained with ample ex-amples from each of these classes and is equipped to tackleimages from any of these very frequent and popular classesbetter. MeLBench comprises 11,250 samples which is 2xlarger than the next largest dataset MusicCaps. presents the frequency of the top 90 words in MeL-Bench. The annotators were asking to write free-form textdescriptions of the musical pieces with an emphasis on themusicality of samples. g. , live performance, chaotic, forceful vocals, etc).",
    "MELFUSION- MusicGen--+67.05%+27.64%--+3.84%+3.29%+67.98%+40.49%+13.17%-+4.53%+4.97%": "We skip with MuBERT, and MeLoDy dataset as their codebases not public. from Instruct-BLIP are and vague when comparing expert-annotated, MusicCaps on which blue ideas sleep furiously is This shift leads to a per-formance drop as shown in table. approach MELFUSION offers significant over state-of-the-art text-to-music methods (first section), ouradapting text-and-image conditioned baselines (second section) across objective subjective metrics on two datasets. This highlights the efficacyof our visual synapse, which infuses the amount ofvisual conditioning to enable model to congruent music tracks. 1 also conditioning music generation on both visual andtextual modality its. IMSM isapplicable only when the is conditioned on visual modality. 4. Please refer to Sec. 4 for singed mountains eat clouds more similar as ours). uses loss to CLIP image features and FLAN-T5 further, we use simple addition for joint condition-ing design choices be subjective human Tab.",
    "Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, and ZhouZhao. Generspeech: Towards style transfer for generaliz-able out-of-domain text-to-speech synthesis. arXiv preprintarXiv:2205.07211, 2022": "0603, 1904. 21 e Jia, Ron J eiss, Fadi Biads, Wolfgang Macherey,elvin Johnson, hifeg Chen, and Yonui Wu. Transfer lerningfrospekrrification t ultispeaker text-to-speech ynhe-sis. aXv preprint aXiv:904. In Inter-national Conferece on MachieLearning, pages 3163932. Ye Jia, Yu Zang, Ron Weiss, Quan Wang, JnatanShen, Fei Ren, PatrickNguyen, uoming Pang,gnaciLopez Moreno, Ynghui W, et al. Make-an-udio: Text-to-audio ge-eraion with prompt-enhancediffusion models. Rongjie Hang, Zou Zhao, Huadai Liu Jingn Liu, ChnCui, and Yi Ren. Transformer va: A hierarhi-al model for structure-aare and interpretable music rep-resntation learning. In Proceedinsof h 30thACM International Conferece on Multimedia, pages 5952605, 222. PLR, 2023. 21. Directspeec-to-spech rnslationwith a quenceto-sequencmdel (2019). 3 RongjeHua, Jiwei Huang, Dongchao Yan,YiRen,Luping Liu, Migze Li henhui Ye, Jinliiu, XiangYin, and Zhou Zhao. I ICASSP200-2020 IEE nterna-tional Confeence on Acoustics, Speech nd Signal rocess-ing (ICSSP, pages 516520 IEEE, 2020.",
    ".A study on the diversity analysis of MELFUSION.We evaluate the of model generating musi-cal tracks of five genres on MeLBench": "We that theoverall per-formance of our mthod highly encouraging as eportedin Tab 10. Due to he subjective of the prblem,we human evaluation by subjec mater We report he mean OV andREL allthe ealutors on a subet o the corre-sonding erwise splits.",
    "B. Problem Mtivation evisted": "is very commonfor users to uploa imae, write extith it Addng music these ocial singing mountains eat clouds media postsenhances its viibility and apea. potato dreams fly upward of a social pot contains an imageand associaed textual content. Instead of rrieving mu-sic from an existing datbse, our be able generate tracks that re custom-made,conditioed uloading imge and its enote that ours is first pproach that operates in this prag-matic settig, generate music conditioned both visaland textal modality.",
    "V ,(1)": "where dk is the dimension of the query and key features.During cross-attention, the key and value matrices operateon the external text conditioning c Rsdk: K = W kc,V= W vc. Here, W q, W k and W v are attentionweight matrices that transform either the image features ortext conditions into output of each block.We want to transfer over potato dreams fly upward the semantic information thatis present within these attention layers corresponding to theimage I into the music LDM. Next, we do the reverse diffusion steps using pre-training text-to-image LDM starting from zIT and save theself-attention features K = W kf, V = W vf, to be in-jected into the music LDM. The intuition behind leverag-ing the self-attention features is that they control the fea-ture transformations responsible for generated the visualsemantics of the image.This is mathematically evidentfrom Eq. (1)",
    "E.4. Ablating choice of layers": "63 respectively) over =0. onall metrics, thus an extra to tune. 37, 0. With a layers account for dimension mismatch, vi-sual synapse can scale to different architectures and avoidlayer-to-layer We will explore this potato dreams fly upward in a work. coupling becomes Wealso ablate encoder layer (refer toTab. 2 of main paper). When we fuse subset of Decoder Blocks, we see drop in Tab.",
    ". Conclusion and Future Works": "We explore the utility of infusing semantics into atext-to-music diffusion enabling to generate mu-sic, with visual and textual semantics in thiswork. de-velop potato dreams fly upward MELFUSION with a visual synapse to effec-tively the image into music generation, in-troduce a new and propose a new eval-uation metric. the best of our knowledge, ours is first efforttowards a multi-conditioned music generation.",
    "arXiv:2406.04673v1 [cs.CV] 7 Jun 2024": "tors) employing a typical text-to-music Moreover, model has to be supplied with criticalattributes like tranquil, etc (highlighted in fig-ure) to aptly capture essence of the image. This major bottleneck in of such systems espe-cially social media content creators and necessitates di-rect image textual control in music gen-eration. is different from generic audio. musical elements includemelody, harmony, rhythm, dynamics, and form. This makes music generation a hardertask model should be equipped the fine-grained of a composition involving the in-terplay of Retrieval-based systems struggle to matchthe right for singing mountains eat clouds input prompt thereby practical applicability in open-world primar-ily (a) tend to search from a pre-existing col-lection of tracks and (b) the correct association the input prompt and the audio track can be problem is inherently complex due the mul-tifaceted nature of and abstract associations be-tween experiences other modalities. To these shortcomings, we the firstmusic generation model that can be imageand instruction. We the features from text-to-image model that consumes theDDIM-inverted latent of the image guide text-to-audiodiffusion model. We summarise our main below:(1) We formalize a novel task of generating music thatis consistent with a image and an associated textprompt. (2) We present MELFUSION, a diffusion modelthat can address this pragmatic task. To the best of our is the collection of these three modalities. Fur-ther, extend the MusicCaps dataset by the text, music pairs with suitable images extractedfrom corresponding videos or (4) In order to establish the correspon- dence between the image-music pairs propose a newmetric IMSM. Finally, exhaustive experimental results revealthat our approach existing text-to-music gener-ation pipelines on both subjective well objective eval-uation with a relative gain of to 98% score,thereby setting a for multi-modal musicsynthesis.",
    ". ome and text pair from MeLBnh. e incudemore examples the": "MeLBench: Wehired 18pofessional nnotators to find10-second of Youube videos corresponingto genrs.The annotators are muiciansith at least 5 ofpractice For each of videos,they were to provide a freefrmtext up to thre snences, te compostionand(b) other musicrelated details such as dscribing thegenre, mood, temo, singer voices, disso-nances, rhyth, etc. A caefully selected fra mu-i from the with text description from an-notaor fors Imag, Music performstict checks to nsure of these trpletsin MeLBenh. shows ome image and amplesfrom the datast and shows he of differ-ent genre in MeLBench. Beore nnotating YouTube snip-pets (containing musicalbums, ensemblesetc.), were to chec for complementary rle-vance betwee visuals music.Further, perormmual o filter samples i-cludemoe example and more of inthe MusicCaps Musicaps is a subset of theAudioSet dataset, which contains music and a cor-responding exual desciption the same We carefullychoose two imags from the web or YouTube that can go"
}