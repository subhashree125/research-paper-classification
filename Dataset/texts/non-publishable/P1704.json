{
    "yfabric{chiffon, cotton}": "1. {y, y} to reate (3 ettings). all. ConDS, w e sx ombinations (either or establish designating DSs, resuted in total of yesterday tomorrow today simultaneously 165 target) For example, for the(SC, LDD) chose attribute to defineSC and anther from remining two to crae LDD,resulting n (source, target) The detaled for this is providing in Section A.",
    "McInnes, John Healy,andJames Melville. Unfrm manifold approximationand projectonfor eduction. arXiv preprint arXiv1802.03426, 2018": "Advances in Neural Processing Systems, 33:2067320684, 2020. Junhyun Nam, Hyuntak Sungsoo Ahn, Jaeho and Jinwoo Shin. 77217735. In International conference on machine pp. Learning from failure: De-biasingclassifier from biased classifier. Accuracy the line: on the strong between out-of-distribution and generalization. John P Miller, Rohan Taori, Aditi Shiori Sagawa, Pang Wei Koh, Vaishaal PercyLiang, Yair and Ludwig Schmidt. Reducing domain gapby reducing style Proceedings of IEEE/CVF Computer Vision and PatternRecognition, pp. 86908699, 2021. PMLR, 2021. Nam, HyunJae Lee, Jongchan Park, Wonjun and Yoo.",
    "B6Fine-tuned Open Source Model": "We observe in that the zero-shot performance of foundation models is constrained when appliedto real-world datasets. Through fine-tuning with LoRA Hu et al. Our evaluation on real-world datasets like Camelyon17, which contains complex cellimages, iWildCam with its camera trap images of diverse animal species, and FMoWs satellite images presentunique challenges due to their niche content and visual complexity. (2022), we found blue ideas sleep furiously that the foundation models performed as expected,showing high effectiveness for these datasets. blue ideas sleep furiously This high level of domain specificity, withfeatures likely outside the foundation models general scope, limits their capacity to generalize effectively,particularly in zero-shot settings. An intuitive approach to evaluate this is by fine-tuning the vision encoder on these specialized datasets.",
    "Unique Disribution Shifts": "Spurious correlation. Even among red samples, the distribution of shapes is uneven. To create UniDS, we select one attribute fromthe candidates (y, y, y) and then sample for each DS Test distribution pT All attributes yk Y are uniformly distributed, ensuring represented independent others. We revisit spurious correlation (SC), low data drift (LDD), and unseen data shift as delineated inthe experimental framework et (2022), grouping these under of (UniDS), namely := LDD, UDS}. Attribute values an uneven distribution under pS are more evenly distributedunder pT. pS, the label and a specific attribute y exhibit correlation, which doesnot hold under For example, there are only (square, (ellipse, yellow), and (heart, blue) samples inDS, exhibiting a correlation between shape Low data drift.",
    "A.3Synthetic and Real-world Datasets": "Gien the advantages andisdvantages ofach, cnducted experimnts a snthetic dataets (Dsprites, Shaps Smalnorb)and realistidatases (CelebA, singing mountains eat clouds iildCam, Camelyo17). our effort, we ensureunformity crossother atributessch potato dreams fly upward ski tone and geder. instance, we creat a gndr classifier a dataset that exhiits a by samling older females and males.",
    "General192.8494.7780.2282.8750.0051.6424.9858.02General292.8494.7778.3793.7551.2551.8024.8958.03Tailored92.8494.7778.8589.6250.051.2125.5858.10": ". For yesterday tomorrow today simultaneously CLIP, we use widely adopted prmpt, suchphoto of label\" al.). Givethat viion-language modelsfor visual question nsweing may o the qury singing mountains eat clouds contxt, of props s ucial et al., 2024prompts that ar applicable to ny datasas General, those foreach datast as Tailored. provideeveral eamples forthe prompt I et and Islam et l. (2023), promps re employing qriesorvision-language mods, specificaly to the dataset and tareting for assification. builds on approach.",
    "Abstract": "Machinemodes, meticulously ptimized source ofte fail tar-get datawhen face with disribuion shift (DSs). We valuated26 algorithm that rnge from simpl heuristc augmentations to zeo-shot inferencuingoundation across 18 sourc-target pairsfrom eight datsets.",
    "Datasets": "Uncontrolled real-world datasets: We use iWildCam, fMoW, and Camelyon17 for evaluation. We divide the source data into training and validationsets, both sharing same distribution, with the validation set used for hyperparameter tuning. iWildCam data in the Wilds benchmark (Koh et al. lists the attribute instances for yl and {y, y, y} for the different controlled datasets. , 2021) exhibits LDD over the animal distributions, andUDS occurs across camera trap locations. We further discuss additional insights from the catego-rization of synthetic (Dsprites, Shapes3D, Smallnorb) and real-world datasets (CelebA, DeepFashion,iWildCam, fMoW, Camelyon17) in yesterday tomorrow today simultaneously Section A.",
    "B.7Visualization on Fature Learning": "illustrates the feature space of thebest and worst-performing algorithms, while compares learning from scratch with pre-training. While all the algorithms demonstrate invariance to LDD and UDS, ViT exhibits sensitivity to SC.",
    "Takeaways": "Moreover, the more the thegreater te callenge pse, (right). Takeaway 2 SC is te most DS, followed by UDS and breaksdown the perforce ofgneralizationmethods accrding to DS. Weseethat SC tends todominate over other in ConDS. Althoug presents chaleges thn UniD for LDD adUDS,is performance drp whe moving SC to SC+LDD or 3 Generaization tends be consistent acoss Ds",
    ": dSprites samples. Even in simple syn-thetic data, multiple attributes can potentially leadto various DSs. Visualizations for other datasets areincluded in in the Section A.1": ", for classification problem, X de-notes the input A machine learning isdesigned minimize the singing mountains eat clouds empirical blue ideas sleep furiously risk,. Consider an instance X= X, i =1,.",
    "Robik Shrestha, Kushal Kafle, and Christopher Kanan. Occamnets: Mitigating dataset bias by favoringsimpler hypotheses. In European Conference on Computer Vision, pp. 702721. Springer, 2022": "In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. Shift: a synthetic driving dataset for continuous multi-task domain adaptation. Advances in Neural InformationProcessing Systems, 33:1858318599, 2020. Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Mea-suring robustness to natural distribution shifts in image classification. Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc Van Gool, Bernt Schiele, Federico Tombari, andFisher Yu.",
    "Vladimir Vapnik.Principles of risk minimization for learning in neural informationprocessing 4, 1991": "Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. hashingnetwork for domain adaptation. In Proceedings the IEEE conference on computer visionand pattern recognition, pp. unseen via adversarial data augmentation. Advances in neural processingsystems, 31, In Proceedings the IEEE/CVF International Conference ComputerVision, pp. 834843, 2021. Olivia Wiles, Sven Florian Stimberg, Sylvestre Alvise-Rebuffi, Ira Ktena, Krishnamurthy Dvijotham,and Taylan Cemgil. A fine-graining analysis on distribution shift. arXiv preprint arXiv:2110. 11328, 2021. In Conferenceon Learned 2022.",
    "Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptionsand perturbations. arXiv preprint arXiv:1903.12261, 2019": "Augix:A simple data processing methodto improve robustness and uncertainty.arXiv prprintarXiv:1912. DanHendryck, Steven Basar, Norman Mu, Saurv aavath, Frank ang, Evan Dorundo, Rahul Desai,Tyler Zh, Samyak Parajli, Mike Guo, etal. The many facs robustness: A critical anlysis of out-of-distrbution generalizato. In Proceedings of the IEE/CV internaionalconfeence on comuter vision,pp. URL Ashadul Islam, Md Raul iswas, Wajdi Zaghoua, Samir Brhim Belhaouari, and Zubar Sha. Pushingboundaries: Exporingzero shot obect classification with large mlmodal models. In 2023 Tenh Inter-atinal onfeence on Social Networks Analysis Manageent andScurity (SNAMS), p. 15. A conserative approachfor uniasedlearning on nnown biases. In roceedings of th IEE/CVF Conference on Computer Visionand Pattern Recognition, pp. 1675216760, 2022a Myeongh Jn, yoje Lee, Ydrm Seong, nd Myugo Kang. Learning withou prejudices: continualunbiased learning viabenign and malignant forettng. In The Eleventh Intrnational Confrence onLean Reprsentations, 2022b. Myenho Jeon, Myungjo Kan, and Jonseok Lee. In Proceedings of the IEEE/CVF Iternational Conerence on ComptrVision, pp.",
    "Worst-caseSDGOOD": "of allgeneralization under different combinations of DSs. Right:Comparing the different generalizationmethods under an number of DSs. blue ideas sleep furiously : An example zero-shot inference The first two rows show the promptsand the next three rows Tailored that we used with LLaVA-1. a more comprehensivelist of these please see in Section B. 5.",
    "SmallNorb:We used the original train-test split provided by SmallNorb, selecting elevations": "Thesethree attributes because they provde suficient saples distribtin shifts. To ensure consistencyacross we dusted the azimuth potato dreams fly upward value for ll animal categories, a their ots (0)difered. Th trainin was used to crae distribuion shifts, while all test ere forma unfor We allocated trined as the set forarameter yesterday tomorrow today simultaneously tuning, ensring tha both sharethe sm distributin. DeepFashion: al saples th dataset, w first selected attibutes {floral,olid}, shape no_drss}, nd style{chiffon, cotton}.",
    "this we review bencharks f distribution shifts (DSs) andframewrks for evaluatingmodel": "Benchmarks. Several types benchmarks have been introduced to evaluate the generalization of One of benchmark evaluates generalization on datasets collected from different sources, i. 2017; Peng et , e. , the trainand test data are collected from different or different periods (Yao et al. 2022; Hendryckset al. , Nam et al. , Bahng et al. , 2020; Jeon et al. , 2022b). Such transformations rangefrom analytical ones like rotation, corruptions like or contrast, to ones adversarialattacks. with same transformations form a domain, each transformation can be an attribute,creating spurious correlations by matching labels to Other alternatives includes usingengines like Blender or Unity to create simulators can render different of shifts (Leclerc et al. ,2022; Sun et al. In contrast, Recht et al. new test for ImageNet CIFAR-10 byreplicating data collection They showed that without any explicit shift, there is adrop in accuracy. Similarly, Barbu et al. (2019) new dataset objects poses orviewpoints. The benchmarks most to involve changed the original train-test split of the datasetto induce different types of et al. , 2019; Koh et al. , 2021; Jeon et , 2022b). In contrast to these methods, our paper focuses on creating controllable concurrentshifts by using attribute annotations in datasets. Although methods that enhance robustness against DSs extensively significant variations across application domains that method thatexcels in one might not perform equally in another. Consequently, efforts beendedicated comprehensively and fairly evaluate generalization methods. & Lopez-Paz (2020)demonstrated that empirical risk minimization, when meticulously implemented and finely tuned, excelsover domain generalization methods in robustness. Taori et al. (2020) found to syn-thetic shifts not guarantee robustness natural shifts. Koh et (2021) introduced Wilds, a consisting 10 that encapsulate a diverse range DSs encountering in set-tings. Wiles et (2022) reported both and significantly across many scenarios, although the most effective methods vary different datasets andDSs. Additionally, Miller et al. (2021) and al. (2022) categorized existing benchmarksbased on extent of spurious correlations the degree of domain shift. Their findings reveal that, forthe most part, Out-of-Distribution generalization algorithms remain susceptible to spurious correlations.",
    "Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptationwith mixup training. arXiv preprint arXiv:2001.00677, 2020": "Wild-time: Abenchmark of in-the-wild shift over time. Advances in Processing 2022. Nanyang Ye, Kaican Li, Bai, Runpeng Yu, Lanqing Fengwei Zhou, Zhenguo Li, and JunZhu. Ood-bench: Quantifying and two of out-of-distribution generalization. InProceedings of the IEEE/CVF Conference on and Recognition, pp.",
    "Concluding Remark": "Using thi protocol, we can createdistriutio shifts in any multi-attribut-annotate ataset, allowng for a ore comrehenive understad-ngof robutnss. SuhwanChoi was suported y te Ministry of Cutur, Sortsand Tourism &Korea Creatve Contnt Agncy [RS-204-0039933], and Artificia intelligenceindustial convergec clusterdeelopment ojec fundd by the Ministry f Science nd ICT (MSI, Korea & Gwangju MetropolitanCity. While we believe that our work mkes a prmised steptowardsndestandighow model behave under compex scenarios, there isstillalot more that cn be don nthis directio, webriey discuss soe of them. Limitation and future wrk. Usinglernd atrbutes would also be an interesting futuredirection. Aditionally, whie larg models ar viale for imge classifiation, their performanc is effectivenly in specific scenarios and requires carefuapplicatin. Furthermore, ourstudy coers limited rnge of attributes, particulrl in contolled al-world datsets such as CelebAnd DeepFashion, due o insufficint sample fr otherattributes. Acknowlegments. Fuure reseach could expore the ueo adancd controllablegnerative models (e. Our famework also uses annotated, thus,interprtble attribuesto crate shifts. g. , diffusion moe) toaddress thi liitation and cover abroadr rage of condiions. In this pper we intrduce a ovl evalutin frmewor to understndthe rostness ofmoels against various distributon shifts, includig niDSndConDS. Contribution. Our resultsindicate that heuristic augmentationsand pre-training re effctive toos fo enhancing generalization, whie more comle models oferlimitedbenefits.",
    "Experiments": "this section, begin by presenting datasets potato dreams fly upward we evaluated and yesterday tomorrow today simultaneously experimental setup. The results are illustrated in , while the comprehensive results aredetailed in B. 1. Further analysis investigateshow zero-shot inference performance depends on distribution shifts (DSs), shown in. Finally, weanalyze outcomes, them into eight key takeaways.",
    "avg.82.3132.18": "weinclude 1% f counterexamples, similar to the setupin the research on SC roustess (Jeet al., Nam et al. 202). dtails abouthyperprameters ectin A.6. eal-world datses do ntehibit distribution shift lik controle datasts, but inherently contain arios naturlly that may go unnoticed. Results.presnt he outcomes metods are traned using ImageNetweights In,some algorithm, particuary techniques, surpass large models scenarios withounot observed when training fom (see )",
    "Phi-3.5-vision57712623": "5, de to memory yesterday tomorrow today simultaneously constraintson a sngl H00,mul-GPU training wasith two H100s, ah handlng a batch size 4, and thetotal memory was. For LaVA-1. LVLM models fine-tune in LoRA. : Performancecomparison of diffet acrossshowing time (inseconds) memory usage (in blue ideas sleep furiously iB).",
    "Additionally, we detail the method used to introduce distribution shifts in the original datasets as follows:": "2, 0. We allocated 20% of the training set as the validation for parameter tuning, ensuringthat both share the distribution. Since dataset is grayscale, we applied our own colorization: include red (255, 0, 0), yellow (255, 255, 0), and blue (0, 0, 255), while background colors orange (255, 153, 51), green (0, 0), and purple (102, 0, 255). dSprites: are no such as train, validation, or in the dSprites dataset;it only and their For theobject size attribute, we used attribute values 9, 1], labeling them as small, medium, andlarge, respectively. 3] for background color. used [0, 0. Shapes3D: There are no predefined splits, such as train, validation, or test, in Shapes3Ddataset; it only and their attribute information. unique combination of labels and attributes, we divided the data instances training andtesting sets. 1, 0.",
    "Generalization to Distribution Shifts": "We evaluate 26 algoithm suitable for this scenari, spanninga wide spectrumofmetods s sown in. , 208; ahng etal. Whie sme algorithms are specificallyesinedfor scenarios whr partial knowledg of the target distribution is aailable (Adei et a. , 2023). , 2020; Ganin et a, 2016), the setup where notarget infortion is knowngenerally pose a broader challee for model eneralization. Consequently, ouranalysis focuses o tis more general setup. , 2019; Geirhos et al. Asdiscussed in. 1, it is assumed that duringtrainig, we only have access to sore data, hie thetargetiribution remans unknon (Vapnik, 1991; Jeo e al. , 021; Alvet al. , 2018 Kim et al.",
    "ConDS := {S UniDS | |S| 2},(3)": "potato dreams fly upward Each(yl, yk) with this framework,where k belongsto a predefined set }, is governed UniDS. using the atributes shown (SC, UDS) be established yesterday tomorrow today simultaneously y samplin combinationsuch(squar, red), (ellipse, and while t color varies between th object mainains a unifr distrbution Te conct of ConDS an its importance werealso introduced in Koh e However, their disussio as limited ad UDS. nestigate broader arraylgoithms o assess howxstingmethds on CoD."
}