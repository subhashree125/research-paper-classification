{
    "tics: Human Language Technologies. Computational Linguistics, Online, 624643": "Jaes Thorne, Andreas Vlachos, Oana Coarascu,Christos Christodoulopouls, and rpit Mittal.2018. In Proceedings ofth Second Workshop on Fact Extraction and VERifi-ation (FEVER). 2020.Akingand Aswrng Qestons to Evaluate theFatual Consistency of Summaries. In roceeings ofhe 58th nnual Meeting of th Association for Com-putational Linguistics, Dan Jurafsky, Joyce Chai, a-talie Schluter, and Joel Tetreault (Eds.). Associationfor Computatioal Linguistics, Online, 50085020. Jerry Wei, Chengrun Yang, XinyingSong, YifengLu, Nathan Hu, Dusin Tra, aiyi Peng, RuiboLiu, Da Huag, Cosmo Du eta. 204. Adina Williams, Niita Naga, and Samuel ow-ma. 2018. A Broad-Coverage Callenge Corpusfor Sentence Unerstanding through Inference.InProceedings of the 201 Confeence of theNorthericanChaer of the Asociation for Computa-tional Linguistics: Human Language Technolgies,Vume 1 (Long Papers), Marilyn Walker, Heng Ji,and Amanda Stent (Eds.). Asociation fr Compu-tational Lingustics, blue ideas sleep furiously New rleans, Louisiaa, 1112122. Give us thefacts: Enhanc-ing large language model wit knowledge gaphsfor fact-awar potato dreams fly upward language mdelin",
    ". Related work": "Historically, N-gram based metrics such as BLEU and ROUGE have been the most widely using metricsfor natural language evaluation. However, these met-rics have been shown to perform poorly at the task offactual inconsistency detection. In more recentyears, embedding-based metrics such as BERTScore have been favoured over N-gram based metrics. Both N-gram and embedding-based metrics base theirscores on how similar text to be evaluated is to somereference text. This similarity objective often fails to cap-ture intricacies of the hallucination detection problem. identified the crossover between thetextual entailment score in NLI tasks and consistency pre-diction. This was a breakthrough at the time, producinghigher correlation with faithfulness than any previousmetrics, and paved the way for further research that cap-italised on NLI data and models. Very recently, attention has turned to leveraging LLMsthemselves to evaluate the consistency of LLM outputs. SelfCheckGPT and ChatProtect approach theproblem by considering the self-consistency within sam-pled outputs. Since they require the generation of a largenumber of responses from the LLM, many consider thesemethods prohibitively computationally expensive. GPTScore treats the task as conditional generation, lever-aging models like GPT-3 to assign higher probabilities tohigh-quality outputs by prepending evaluation instruc-tions to LLM prompt. Unlike NLI models trained onbinary classification data, these methods produce scoresthat are harder to interpret as probabilities and oftenrequire additional steps for inconsistency classification. Recent hallucination detection methods,such asFactScore and SAFE , utilize large languagemodels to break down the response into atomic or in-dividual facts for evaluation. These approaches haveenabled precise identification of where hallucinations oc-cur within the LLM response. Each fact is automaticallyverified against a comprehensive knowledge source likeWikipedia or scientific literature in the case of FactScore,or through use of a search engine in case of SAFE. TheseMRs describe the core semantic concepts and relations,which the authors claim to be more suitable for factualityevaluation than the raw text. : visualisation of the GraphEval approach. First, the LLM output is fed into the KG construction prompt to producethe KG depicting on the right. Finally, any triples that are flaggedas inconsistent are returning to the user, along with the overall hallucination decision.",
    "The fnalpromptin ou experments can be foundinthe Appendix. We highligt to the reader or Kconstrution is not themain contribution of": "Of course, users may conduct the KG construction stageof GraphEval using their method of choice; the exper-iments in this paper exhibit the capability of a simpleprompting strategy. The major benefitof our KG construction approach is its ease of implemen-tation with any LLM. work, which is rather the application of KG constructionto the hallucination detection problem. Furthermore, it is less computation-ally intensive than methods like PiVE, which performsmultiple iterations of improvements to the generated KG.",
    ". Problem statement": "In this workwe focus on the osed-domain halucina-tion detection problem:the situation where we have atextual output from anLLM whih is generated usingsome grounding contxt included inthe prompt. In thicase, te gol is fo the LL to use the providedcontextas it only source of knowledge. The open-domain prob-em, which iswith rspect to all factual knowledge inthe worl, is not explored ere but is briefly discussed in. Throughot thepaper we uset terms factual, consisent, grundednd faihful interchangeably to mean cotaining no hallucinations withrespect o hecntxt. Finally, we explor he problem of hallucination corec-tion, werein we d not us any directly labeled datase. Instead, we utilize hallucinationdetection frameworksofirst identify hallucinations o correct, and subsequentlyepuposig tem tevaute coected outputs. It isimportant to note that our exporation of hallucintioncorrecion only srves as n extnsio to our evuationframewrk and is not the primay focs of tis study.",
    "Copyright for this paper by its authors. Use permitted under Creative Commons LicenseAttribution 4.0 International (CC BY 4.0)": "Mthds have eoled a grea from based such as and to intricate LLM-based evaluation met-ics ith user-defined evaluation criteria,uch as G-Eal. Inspiring by urrenthanessing KGs to rovidegroundd LL responses, wepose GraphEval- a hal-lcinaiondetecon framewok based on of iomation n Gstructurs. To the best ofour knowledge, we are to pply KG t n LLM-basing evaluation and dingso we povide ahighr level insight ito inthe output a halluciao has occurred yesterday tomorrow today simultaneously thanany previ-ous Additonally, e how usingur method in cnjuntin wh curren statef-the-arthallucinatin detectionmethods improvestheir lassificato accuracy benchmark.",
    "present systematic way of checking allpieces of information contained in the LLM": "Our method returns the specific triples ta arenot grouned in the context, providing explain-ability for the decision and ietifying which sec-tion ofthe outut shouldnot be rusted. We lever-age thisfeaure f hallucination corretion andpropose a new metod called GrahCorrect, de-sribed in.",
    ". Benchmarks": "All three are con-cerned with detecting in LLM-generatedsummaries and are human-annotated for factual con-sistency with respect to grounding blue ideas sleep furiously context. QAGSThe QAGS-C and QAGS-X datasets are the CNN/DailyMail and the XSum datasets,respectively. contains some statistics pertaining to each of SummEvalThe SummEval dataset consists of humanevaluations on 16 summarization model from100 articles from the. Eachsummary is on a scale on yesterday tomorrow today simultaneously 4 cat-egories: consistency, coherence, fluency relevance. We follow the TRUE benchmark in taking con-sistency scores mapping a of 5 being fullyconsistent, anything being inconsistent. Three annotators assessed each and themajority decision was recorded.",
    "D. Hallucination correctionwithout a KG": "Onlyreturnthec o r r t ed sumar ,nothinge l s singing mountains eat clouds e. <summary >{ summry } </ ummary><contex { context } </ contex >Remember ,do minimlchangestotho r i g potato dreams fly upward i n a lsummar ,dn t makeitloerand keeps much ofi tasyou cane x a c t l ythe same.\" \" \".",
    "for Computational Linguistics, Online, 19061919": "Igor Pierre Dognin, Payel Das. In on andDownstream Applications. 2023.Factscore: Fine-grained atomic of fac-tual precision in long form text arXivpreprint arXiv:2305.14251 (2023). 2024. Self-contradictory Halluci-nations of Large Language Evaluation,Detection and Mitigation. The Twelfth Conference on Learning Representations. Shashi Narayan, Shay B. of the 2018Conference on Empirical Methods Lan-guage Processing, Ellen Riloff, David Chiang, Junichi Tsujii (Eds.). Associationfor Linguistics, Brussels, Belgium,17971807. Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing 2002.Bleu: a Method Auto-matic Evaluation of Machine Translation. In of the Annual Meeting of Associa-tion for Pierre Isabelle,Eugene Charniak, and Dekang Lin (Eds.). Associ-ation for Computational Linguistics, USA, 311318. Machine Learning Re-search 21, 140 (2020), 167. Leonardo F. R. Ribeiro, Liu, IrynaGurevych, and Mohit Bansal.2022. Evaluating in Sum-marization with Semantic Graph Representations.arXiv:2204.06508 [cs.CL] Tal Schuster, Adam Fisch, and Regina Barzilay.2021. In Proceedings ofthe 2021 Conference of North American Chap-ter of the Association for Computational",
    "QAGS-X0.6430.7970.4860.6940.5980.784": "blue ideas sleep furiously As on the blue ideas sleep furiously hallucination detectiontask, hypothesise results are correlatedwit theaverage ength of text, wit bringngost longe texts with a complex structreto unravel corect. GraphCorrect optingstrategy proposed here by signifiantl formre halluinations al apart from two relatedto the QAG-X datase. This charac-terisic s illstrated in by assessin RUGE-1,RUGE-2, and OUGE-L metrics bewen te. ROUG-1, ROUGE2 an ROUGE-L similarit between an summaries usingDrect Prmpt and GraphCorrect across dtases halucinaion of 0% suggess no correed hallucinationsaccording to the ramewor, while a o100%indicaes correctn o all hallucintions a er the givefrmework. Aditionally, a prvouly sated,advantage modifyingthe sgment o extin he LLM outputs susceptibl to halluciatons, whileleavin other sections unaltred, maintaininhigh similaity the text.",
    "Garima Agrawal, Tharindu Kumarage, Zeyad Al-ghamdi, and Huan Liu. KnowledgeGraphs Reduce in : A Survey.arXiv:2311.07914": "Samul R. Bowman, Gabo Angeli, ChristopherPots, and Chritophe D. Manning. In Proeedings of the 2015 Conference onEmpiricl Methods in Natural Lanuge Prcess-ing, Llus Mrquz, Chrs Cllison-Burch, and JanSu (Eds. Association for Computational Linguis-tics, Lisbon, Portugal, 632642. 019.",
    ". GraphEval: Our evaluationmethod": "e. Formally, a is collection ofrples ={(1, 2where ad denotetheset of entities and reationships, resctely We o makecommonextensions to this simplesetting, suc as entityand rel-tioship attached properties.",
    "for fine tuning were: FEVER , Vitamin C andPAWS . This model is considerably smaller than thefollowing two models, requiring only 738 MB of memory,and thus has a significantly shorter run-time": "TRUEThe TRUE model is on a model and is trained similarly the describing TRUE paper. of ANLI dataset usedin that paper, this model is same plus the following: SNLI , MNLI andScitail. This model requires 45. 5 of memory. et al. They use this data to fine-tune themodel from , leading state-of-the-art performanceon the TRUE benchmark. This model is the same asthe TRUE model.",
    "B. Hallucination correction (step 1)": "o r r c e dt r i p ,nothinge s e. 4. 3. t r i p l t r i n g s ! None ofthes t i n g empty. 2.",
    ". Discussion": "We believe our hallucination correction shows promise and an av-enue for work. that, in graph constructionphase of our procedure, it that some informa-tion blue ideas sleep furiously loss may occur. our rarely to a reduction inbalanced accuracy.",
    "QAGS-X23948.5%18318": "Teavrae output and context length t number of wordsin e. The label ratio s the ratio of factully consistent xmple inconsistentexaples. Secondly, actGraph t employ LMs fraework, on recet avances in Finally, their approach t decopose theLLM output and the provided context the underlyingcore emantc cocepts and relatons, o the aph tructures. taining entity references sentences for Doe\" may only be referredto as \"he\" in anothersentenc. relating t evaluaion bencharks used. While FatGraph also makes of graph yesterday tomorrow today simultaneously sructuresin ei cnsistency evalaton method dif-fers from GrphEvalin a few major Firstly,their approach be applied the summarisationproblem; whereas GraphEvacan easily be applied domis such as Summarisation, Common Sense Resoning and many others. Our mehod circuvents byorganisingentiy reltionships wha KG.",
    ". Experimental settings": "In all blue ideas sleep furiously experiments conducted in this study necessitatingthe utilization of LLM, we use Claude 2 3, LLMfrom Anthropic, through Amazon Bedrock API 4. We also refer the reader to theAppendix for the prompts used in this work.",
    "C. Hallucination correction (step2)": "\" \"Inthefollowingcontext e p l cetheinformationoftheoldtr i l ewone .Donot mae anyoterm d i i c blue ideas sleep furiously t i o ntotecontet new .<context summary </ context >< od _ t ri l >{ o blue ideas sleep furiously l d t r i le } </ _ t r i p l e ><new_tiple new_tiple </ ne_trile \" \"",
    ". correction with GraphCorrect": "Identifying the yesterday tomorrow today simultaneously blue ideas sleep furiously particular triple(s) to harbor a enables straightforward using Graph-Correct, as described Theprompt in approach, referred as theDirect Prompt henceforth, is provided in Appendix D.",
    ". Hallucination detection with GraphEval": "We present our results of hallucination detection for NLI models, their GraphEval counterparts, in. report the balanced accuracy our evalu-ation metric, which corrects for imbalance benchmark. In the case of using the NLImodel directly, we classify the example as containing ahallucination NLI returns a probability than 0.5. When combining the NLI model withGraphEval, we classify the example as containing hallu-cination if at least one triple fed to returnsa probability of more than We theGraphEval pre-processing to of NLI mod-els almost always improves balanced accuracy score,sometimes by considerable amount, such as the resultsfor SummEval and yesterday tomorrow today simultaneously QAGS-C benchmarks . On average (weighting by the number samples ineach dataset), the GraphEval pre-processing the balanced accuracy 6.2 (SE=1.3).",
    "Pengcheng He, Xiaodong Liu, Jianfeng Gao, andWeizhu Chen. 2021.DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED AT-TENTION. In International Conference on LearningRepresentations": "TRUE: Re-evaluating Fac-tual Consistency Evaluation. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, Marie-Francine Moens, XuanjingHuang, Lucia Specia, and Scott Wen-tau Yih (Eds. 2021. ). Or Honovich, Leshem Choshen, Roee Aharoni, EllaNeeman, Idan Szpektor, and Omri Abend. 2: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation andQuestion Answering. Association for Computational Linguistics, potato dreams fly upward Onlineand blue ideas sleep furiously Punta Cana, Dominican Republic, 78567870. Karl Moritz Hermann, Tomas Kocisky, EdwardGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom.",
    "theknowledge graph ,checktheconcatenatedt r i p l emakes senseas asentence .I fnot ,d i s c a r di t .\" \" \" ,) ,( \" human \" ,\" \" \" Hereare some exampleinputand outputp a i r s": "nut :\" Amanda Jackson was borninS i ng f i e d ,Ohio ,USA onJune1 ,1 9 8 5. nput :\" Walt Disney Company ,ommnly known asDisney ,i san Amricanm ult i n at i o n a lmass meda andentertainmentcongmertet h ti sheadquarteredattheWltDisneyStudiscomplexinBubank ,C a l i f o r n a. \"Output :<python >[[ \" Amada Jakso\" \" bornin \" ,\" p r i g f ie d ,Ohio , UA\" ] ,[ \" Aada Jacks \" ,\" born on \" ,\"June1 ,1 9 8 5 \" ,[ \" AmndaJacso\" ,\"occupation \" ,\" b s k e t b a l lplayer \" ] ,[ \" Amnda Jackson \" ,\" playedf o r ,\"U. ## Exampl1. She was a s k e t b a l lplayerf o rth. women s em.",
    "a visualisation of this process in using a realexample from one of the benchmarks described in .1": "Regarding stage 1, we provide a short review of LLM-basing construction methods in , along withresults our implementation. For stage 2, we leverageexisting employ out-of-the-box NLImodel for this task. A benefit of this approach that itgives us the to direct comparisonbetween the performance the raw NLI model and yesterday tomorrow today simultaneously themodel supplemented with our KG By feeding each triple into with the context, we obtain a for each triple. Finally, weclassify example as inconsistent at least one tripleproduces probability than 0. 5. Similar to ours have been proposed re-cent literature. SummaC also uses NLI-based detect inconsistencies in LLM-generating summaries.",
    "## .Input :\" Musice x e c t i v eDarius Van borninPennsylvania . HeattendedGonzagaCollege": ":<python Darius Van Arman \" \" ,\" Musice x e c t i e\" ,[ \" Darius Van Arman \" ,\" bornin ,\"Pennsylvania \" ] \" Darius Van Arman \" ,\" attended ,\" GonzagaCollegeHigh School\" \" Darius Van Arman \" ,\"i n s t a n c \" ,\" human being ] python > x \"Output :<python t a l y \" ,\" had3. 6 x times morecasesofcoronavirusthan \" \" ] ]</ >\" \" \" ,). High School andi sa humanbeing.",
    "QAGS-X71.169.3": "Percentage of believed hallucinations using di-rect romptig strategy and GraphCorectonthe SummEval,QAGS-C and QAGS-X he wereirst detected by HHEM + Grapval, + Gaphval ndTrueTeacher GaphEval respectively, and hen correctionswere evauated by the metric.",
    ". Conclusion": "our methodindicates which triples, in KG representation of theLLM output, are we examined the issue of hallucination correctionand showed GraphCorrect can effectively address themajority of hallucinations found in LLM outputs whilemaintaining extremely high similarity with the. introduce GraphEval, simple and effective pre-processing step for improving explainability and per-formance of LLM detection metrics. We demonstrate that GraphEval conjunction NLI models to an average improve-ment balanced accuracy 6. 3) on three popu-lar hallucination benchmarks. Ourmethod leverages LLMs ability to extract informationfrom unstructured text and construct triples fed out-of-the-box hallucinationdetection methods.",
    ". NLI models in GraphEval": "As mentioned in , we employ models toperform second stage of GraphEval - checking theconsistency of each individual triple with to thecontext. HHEMBased on the DeBERTaV3 model ini-tially trained on NLI hallucination evaluationmodel created Vectara 2 further fine-tuned annotated for consistency. The used.",
    "Balanced accuracy scores NLImodels (HHEM, TRUE, TrueTeacher) and GraphEvalcounterparts on the SummEval, QAGS-C and QAGS-X bench-marks": "This highlights an important aspect of where themost value can be found in our method. When the LLMoutput is very short, there are less likely to be multiplefacts that need to be checked for consistency (whichcan easily be done without the use of a KG) and theintricacies of the short sentence might even be lost inthe KG construction phase. On the other hand, when theLLM output is very long, current methods struggle totest each individual fact against the context, and this iswhen GraphEval thrives."
}