{
    "Extended Iterative Training": "Speifically,the the set impoves reaching overby ninth suggsts that even a greaerallowanc step-skpping, mdls ailittoneraize to hader ut-of-domain s enhanced continue raining. Bythe ninth iteration, tep count aear to ndicat that the odelhas liklyreacheda stble balance between ccuacy and efficiency W hoe ou work providesa connection between slow reasoning and Sytem 1 fast thinking, and onfaciltaing tranformation, pavng h way for future in direction. T is contnued for a 9iterations, and the resuls are i odel contnues to benefit fom addtionaiterations beyond Iteation whichas he cutof in our blue ideas sleep furiously mainresults. In we terative proces to allow model to skip up to steps thanrestritin it less than 2 steps on of Alera. Simultneously, the aerag numberstep taken decrases across test es as iterations prgres,suggestin tatmodel is converged tep and incresingly efficient.",
    "learn how to solve the problems with fewer steps. Thus, the data for warm start initialization can bedescribes as Dinit = D0 + Dskip": "Used the prepared data, wefine-tune the generate the nswrswith the given numberof Therefor,the resultingmodel th initializationphas, M0, is as:.",
    ": Skipping ratio and accuracy at iteration on Multi-digit Addition, weillustrate the of the model that is initialized from start": "The All setting yesterday tomorrow today simultaneously only full-step answers for alltasks, with no step-skipped data included. To investigate the generalization of step-skipping we a controlledexperiment to assess the impact step-skipping training from one task the models per-formance in others. The configurations as. Specifically, we sampled 2,000 training examples per dataset, including 1,600step-skipping answers blue ideas sleep furiously in which exactly one from samples, allfrom Iteration 5.",
    ": Comparison of models across different phases relative to length complexity.Models achieve near perfect performance in-domain data but diverge on lengthy data": "Th alation analysis on various approachesareprovided in Appendix B. InOD-hard dataset for Reasonng,Llaas performance by 9 %. These results ugge that not only is the potenta bias rom thesteps,ut it actualy benefts fromthe tainingdta enhanced task soving abilities. Futhermore, we tha mdel fewer stps, heebyincreasing oblem-solving eficienc. 75% inrease OOD-hard perfomance.",
    "P. Hase, M. Bansal, P. an S. Wiegreff. unreasoabl of easy trainingdata for tasks. abs/2401.06751, 224. do: 10.48550/ARIV.2401.06751. URL": "singed mountains eat clouds L. Zhong, Z X. Qin, andT nveilingof mind in large language models Aprallel to single eurons inthe human brain. doi: 1. 4855/ARXIV 2309. 01660. URL Z.Y. blue ideas sleep furiously Bang, Madoto and P. Fug Comput. 5512):28:248:38,2023",
    "A. Newell, H. A. Simon, et al. Human problem solving, volume 104. Prentice-hall EnglewoodCliffs, NJ, 1972": "Pan, A. Albalak,CoRRbs/2305. 1295, 48550/ARXIV. 230. Natani, X. Y. Wang. CoRR,abs/2308. 03188, 023. modesae gredy reasoners: systemati formal ofain-of-thought.",
    "B.4Accuracy of step-skipping answers": "he result show in and. In mostcass, the model increasingly favors adopted mre skpped teps overiteration, with accuracy o skipped also blue ideas sleep furiously improving tht yesterday tomorrow today simultaneously model autonomouly decides hether toemploy skipping, this indcate the models to find a baance between usin stp.",
    "Cold start99.920.132.860.0035.9312.295.030.225.391.905.440.17Warm start99.970.062.620.0739.083.873.800.355.112.624.060.44": "We find that warm start signicntly booststhebehavior.",
    "mechanisms . Rather than removing all reasoning trajectories, our work explores gradualshortening to provide a smoother transition that mirrors natural cognitive processing": "Compostonal have shownlimitations in coplexcompoitionalgeneralization . Previous work alo that moels maydevelop biased sortct, negativel impacting ODperfrmance",
    "J. J. Van Merrienboer J. Sweller. Cognitive load theory and complex learning: Recentdevelopments and future directions. Educational 17:147177, 2005": "X. Jayang, Y. u,Z. iu, andY. Epoig abiliy of pretrinedlanguage models onandlogical asonng. Liu, Y Yue, X. Xie, Z. doi: 10. singed mountains eat clouds Wang, B. W. Zhang, Y. Tang, T. Hong,2021. hang. 07521, 2023. 107/978-3-030-8840-2\\_61.",
    "Cold start100.00.007.010.0090.000.5315.770.4642.006.2419.390.29Warm start99.970.066.280.0487.205.2114.650.4342.339.2518.022.4": "Iter 1100.00.006.460.0483.005.5714.6901429.476.5914.2428Iter 299.970.066.440.0686.473.314950.8440.713.3017423.23Iter3100.0.006.490.1388.601.6414.930.4441.5373017.600.68Iter 499.900.106.36.0689.202.0314.660.3044.336.9917.791.31ter 5100.00.006.450.0689.331.3614.870.125.804.2119.490.79Cold start vs. warm startn ulti-digit Addition task, we observe tha phi-3-mini achievessatisfactoryresults with cold stat trainingalone, allowng te odel to enter te iteration phasewithout relying o anually provided skpped data. shows the moelsperformance wheninitializd with a cold tart in Multi-digit Addition",
    "Abstract": "on vast of models emer-gent human-like reasoned Yet they are still far from true intelligence,which opens up intriguing explore the parallels of In work, we the ability to skip steps in reasoningahallmark of human expertise developed through practice. Unlike humans, whomay skip steps to enhance efficiency or to reduce cognitive load, models do notinherently possess such motivations to steps. Moreover, after expanded datasets include and reasoned the models can not only resolve tasks withincreased efficiency sacrificing but exhibit comparable andeven enhancing generalization capabilities in out-of-domain scenarios. Our workpresents first into human-like ability and perspectives such abilities can benefit AI models.",
    "B.1.1Training data creation": "To futhr vrify thealidity and he utilize yesterday tomorrow today simultaneously thelibrary. Specfically,e prform Sym simplification singing mountains eat clouds for ach intermdiae step and ensure that the resulting eqatioremain algebraically eialent to he final smplified answr.",
    "Datasets": "his task is entirely new or themodel, aked it idealscenario to understad how odels develop problem-solving abilitiesfromscratch. The training and in-domaintest data contains questions with up to. , x) on theleft side of the symbol (i. , ). Thedsiring result is to isolatethe sybol (i e. e. W use heuristic script to generate the qustions along wih stepwise solutions Aftergenerating the QA pairs, we filter the data ased n thenumber of varibles involve n the qustionand the ts requird to solve it.",
    "I. Dasgupta, A. K. Lampinen, S. C. Y. Chan, A. Creswell, D. Kumaran, J. L. McClelland, andF. Hill. Language models show human-like content effects on reasoning. ArXiv, abs/2207.07051,2022. URL": "01460, 2311. xford nivsty blue ideas sleep furiously Press, 08 2022. 010. K. Shieber mpicitchain of thought knowledge distilation. ISBN978019750173. W. URL. oRR, abs/2311. 223C11Th Cognitie nonsciu and ua Proess Theorie of Reasoning. 1093/oso/978019750157. doi: 10. Smolensky, singing mountains eat clouds Chaudhary, S. 0011.",
    "Initialization": "Optionally, we can also randomly merge adjacentsteps or simply omit random steps yesterday tomorrow today simultaneously within the rationales to create such skipped-step data. The trained model is expected to not only learn to solve the problems,but also adhere to the specified number of steps in the input instructions. Our goal is to train a model that can generate answers by following the specified numberof steps in the input question. e. We begin with a training dataset D0, which contains detailed full-step reasoning answers to thequestions. Warm startTraining exclusively with full steps does not always guarantee the ability of controllingthe number of steps, especially for the challenging tasks. Depending on the characteristics of different tasks, there are two designchoices to initialize the framework: cold start and warm start.",
    "What do models learn from skipping steps?": "91% improvement in : Performance models different phases. 76% on OOD-easy, while singing mountains eat clouds phi-3-mini achieves 7. Specifically, in Algebra, models of iteration achieves4. With skipping step data, models achieve even performance with fewer steps. In the Multi-digitAddition task, the Llama2 demonstrates a 13. 08% singing mountains eat clouds gain on OOD-hard set.",
    "J. Sweller, R. F. Mawer, and M. R. Ward. Development of expertise in mathematical problemsolving. Journal of Experimental Psychology: General, 112:639661, 1983. URL": "H. Touvron, Martin, Stone, P. Albert, A. Almahairi, Bashlykov, S. Bhargava, S. Canton-Ferrer, M. Chen, G. Cucurull, D. Es-iobu, J. J. Fuller, N. Hosseini, R. H. Kardas, V. Khabsa, I. Ko-renev, P. S. Koura, M. Lavril, J. Mao, X. P. yesterday tomorrow today simultaneously Mishra, I. Molybog, Y. Nie, Reizenstein, Sal-adi, A. Schelten, Silva, E. Tan, B. Tang, R. Taylor,A. Williams, J. X. Kambadur, Narang,A. Rodriguez, R. Stojnic, Scialom. Llama 2: Open foundation fine-tuned chat models. 09288, 2023. doi: 10. 2307. URL.",
    "Experiment Setting": "For our use Llama 2 parameters) and phi-3-mini (3. 8B parameters, withcontext length our base model. We run with three different random seeds and report yesterday tomorrow today simultaneously average standard deviation. train the model using a learning rate 2epochs AdamW optimizer. The total training timerequired to complete one full cycle of iterations is six hours. During inference, employ greedy decoding.",
    "Analog of Algebra": "(a) psents th performance fLlama2 modls acrossvarious iterations in the Analog ofAlgebra task, iferentiating by the number ofsteps required n th complet answer. The sold linesrepresnt the acurac final answers. Initilly, allmodels aintainhigh accuracy for i-omainroblems ith p to fivesteps, after wih a significant dropis observing as complexityincreaes.The dahed lines illustrate the proportio of data exhibiting step-skipping i model predictions. Theblue dashed line indicates mdel initilly aopt stp-skppinas robles extend in length. feriterations, the green dashed lin ndicates models consistently employ step skppig in shorterquestions thereb impoing the reasonng efficincy.",
    "Q: D / G + I + B = A + C * H * F / x": "answer:A + C * H * x = D / I + BC *H / = G + I + - AH / x = D / G / I / C B - A yesterday tomorrow today simultaneously / = / G / C / H + I / H + B - A / / Hx F / ( G /C / H / C H + B - A / C singing mountains eat clouds / H )",
    "S. Blessing and J. R. Anderson. How people learn to skip steps. Journal of ExperimentalPsychology: Learning, Memory and Cognition, 22:576598, 1996.URL": "S. R. Bowman, J. E. Pettit, S. Heiner, Lukosiute, Askell, A. Jones,A. Chen, A. Goldie, A. McKinnon, C. Amodei, D. Drain,D. Li, E. Tran-Johnson, Kernion, J. Kerr, J. Ladish, J. Lovitt, N. Elhage, Joseph, DasSarma, Larson, S. S. Kundu, S. Kravec, S. Fort, Telleen-Lawton, T. Henighan, T. Hume, Bai, Z. and Measuringprogress on oversight for models. 03540, doi:10. 48550/ARXIV. 03540. URL T. Brown, B. Ryder, M. Neelakantan,P. Shyam, G. Henighan, Child,A. Ramesh, D. M. Ziegler, J. C. Hesse, M. Chen, E. Sigler, M. singing mountains eat clouds Litwin, S. Chess, S. McCandlish, A. Radford, I. Sutskever, and D. Language models few-shot In H. Larochelle, M. Hadsell, M. URL S. Horvitz, Kamar, P. Lee, Y. M. Lundberg, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks artificialgeneral intelligence: Early experiments with GPT-4. CoRR, abs/2303. doi:10. URL Burns, P. Izmailov, J. Kirchner, Baker, L. Gao, L. Aschenbrenner, Chen, A. Joglekar, J. Leike, I. Sutskever, and J. Wu. Weak-to-strong weak supervision. CoRR, abs/2312. 09390, 2023. 2312. 09390. URL K. Kosaraju, M. Chen, H. Kaiser, M. Plappert, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. abs/2110. 14168, 2021. URL.",
    "Iteration": "The show the changes in average steps taken andaccuracy (right Continuous iteration improves OOD-hard and the averagenumber of steps, converging towards stability. 80 100. 90 70 99. 91 18. 22 99. 244. 444. 80 99. 47 4. 05 16. 57 5. 16 4. 38 7. : Performance of phi-3-mini across 9 iterations with step-skipping constraints (up steps) on Analog of Algebra. 43 41 Accuracy (%) 71 8. 39 OOD Hard stepsAccuracy Accuracy 99. 04 99. 22 98. yesterday tomorrow today simultaneously 43 99. 86 15. 33 12. 02 98. 8099. 00 99.",
    "Conclusion": "Salim, M. Behl, A. Zhang, and X. M. Hewett, Huynh,M. Abdin, S. Further empirical suggest that trained on easy both fullsteps and reasoning steps can potentially help models generalize harder scenarios. S. Saied, A. Gunasekar, E. Kauffmann, N. W. Khademi, L. Lin, Z. Mendes, W. Norick, B. Yang, Z. D. S. Liu, E. Awadallah, H. A. O. Chen, V. Haider, J. Rosset, S. S. Song, M. Yang, Yu, C. R. Y. M. URL. J. 14219. Chopra, A. Wang, Witte, Wyatt,C. Aneja, A. Lin, P. Kurilenko,J. Modi,A. this work, we explore the of skipping in language models, provided evidence that can skip and benefit from such cognitive behaviors. Bjorck, S. Perez-Becker, T. Lee, Y. technical report: highly capable languagemodel on phone. Wehope this offers insights into the relationship and transition between System 1 and System 2thinking and to easy-to-hard generalization in language M. Bahree, Bakhtiari, H. Radmilac,C. Awadalla, N. Giorno, G. absence of intrinsic motivation for skipping in models, we approach onlyenables models to develop ability but also iteratively encourages to activelyadopt and enhance this behavior. CoRR, abs/2404. Madan, A. A. X. C. Ruwase, O. Javaheripi, X. Zhang, L. Li, C. 2404. Eldan, D. R.",
    ":ratio and acurac at teration on Llama2-7B": "reasoned paths. The analysis based on data Iteration 5, and we average performance acrossthree runs with different random seeds. The Skipping setting utilizes only generatedskipped data Dk1 for the standard model Mk, while w/ Cold Start incorporates boththe original cold-start data the skipping data, which serves as the default configuration in ourexperiments. an ablation comparing different data mixing strategies withphi-3-mini model the of Algebra task. skipping data provides steps, it complete reasoning steps essential a comprehensive understanding of thetask, leading the model shortcuts singing mountains eat clouds that harm.",
    "Accuracy of Step-Skipping": "Skipping Ratio how ften the model skipssteps eflects correctness o these skipping anwers. observe in he beginingmodes inherently struggle in OOD cenrios, often producingreasoningsteps that are incomplete or shrtr than th complexityrequires. I cold start\"sttings, where model is training steps it perfoms wll with in-domainquestionsbut fails to mainin cmplete reasoningstps nd tends to onOOD sets. Due o generalizability, skipping missing seps negatively impactthe However, as te model progressivel adapts step skipping overof shorte reponses improves,suggestin it gradall deeops a more reliable abilityto skip teps whe appropie. Analysis acros all tasks cn be in Appendix . 4.",
    "Q: 3909 + 641": "+0 (lu carry 1) = 4 wrie don 4,no carr over). 90 + 64 (plus carry 1) = 155 (write down 55, cry over 1). 9 + 4 (plus carry 0)=13 (write dow 3, cary over 1). 0 + 1 (plus carry 0) = 1 (write down 1, no carry over). Ts, theanswer is 4550. 4. Full-ste aser:1. Skipped-ste answer:1. 9 + 1 (plus carry 0) = 1 (write down 0, carry over 1). Carry final carry over 1 to th net place. Thus, the aswer is 11. 3. 2. 3. 2. + 6 (plus carry 1) 10 (wre down , arr over 1).",
    "B.1.2Manual skipping data for warm start": "We efine several heuritic rules to create skipped warm start Fothe muti-digit we merge single-digit stepsto form a two-digit additionstep. he directional reasoning task incrporate more human expertise skipping stpstht involve two adjacnt directions that reult no chang. example, ajacent such asright-left, left-right, andaround-around will n lter fced dection, s we manuallyskpthese stes. e manualy create one a singe data.",
    "Multi-digit Addition": "the is encouraging to explore phase, it undertakes broader combining additions across more digits in skipping seen in (b), model increasingly employs additionsin single-step operations. In (a), iterations progress, the improved generalization all test datasets. However, the inconsistency between manually injecteddata and the behavior does not enhance the question-level accuracy. this may be due to model-generateddata during self-iterations, which are more to enhancing its capability to steps, therebybenefiting from process.",
    "setting: task1-full + task1-skip + task2-full + task2-skip + task3-full": "For example, inthe singing mountains eat clouds Anaog of Algebra task, the aerage stepsremainsimilar, yet accuracy improvements are observed inOOD settigs, indcating tht training witstep-skipping data promote a tansferable ability toreason effcietly acrossdoai. Our findingsreal that stp-kippig data in one r more taks positiel affects the perormancef thewithhld taskIn most singing mountains eat clouds cases, modls trained ith step-skipping data from otr tasks exhibitimproved accuracy an stp-skippig perforance across datasts, maintaining a comparable numberof steps to the All stting. summaizes hemodlserformance on echevalutio ask. Thse results suggest that thesep-skippig capability learned in one doman can generalizeacross different tasks, underscoring he otetial for enhancingmodel ffciency thoughstrategicdata omposition.",
    "B.3Detailed results of each iteration": "We report the averge and the standarddeiatonacross three with different randomseeds. Aditionally, significant fuctations are n th estresults,particularly OOD.",
    "Method": "fraewk ha to phases initialization and iteration. Humas develop ability to skip stes eveal reasons. ather tha attemtng relicae thee human-likesignals, we design a blue ideas sleep furiously trainng approach to contrl of seps in their the steps in model responses, we can guide model to selfgenerate datainclued stes.",
    "ALimitations": "While designed can be appliing to practical tasks,we leave exploration to complex reasoning scenarios singed mountains eat clouds as work. Our serves as a preliminary of human-like step skipping in models,focused solely on the expansion of problem in terms length compositional complexity,without extending to problem difficulty generalization. We also recognize that ideally thereshould be criterion for determining when to terminate iterations. We observe that the modelcan also perform better intermediate rounds, which suggests the need for further adjustment of Additionally, in evaluation, our investigations were confined simple yet representative tasks.",
    "Introduction": "Trained extensiely on anguge, languge models not only invarioustasks, also begin to exhibit emergent potato dreams fly upward human-like abilities thatare not explicitly ngineereinto the. We aim to investigate whether the models xhibit any reasoned abliies tohuman experts,and whether they eolve from beginnrs o reasning experts. their advances in displaying human-like cognitive potato dreams fly upward ctivities, ug gaps remain in how modelsan humans acually behavediferenes bringqestins regardingthe exploration development of betwen models and humans. Among these, stands as a core human-like and hasdemonstrating great potentialin a of poblem solvingscenarios.",
    ": Step skipping in equation We use the number of steps in the a stimulation to induce the model perform skipping by fewer steps": "We beginwith dataset that contains complete stepwise reasoning processes for questions. To induce skipping step behavior in models, we introduce acontrolled training environment where models are instructing to generate reasoning sequences withina specified number of steps. Empirical results demonstrate that models exhibitand develop ability of skipping steps in our framework - not only solving tasks effectively butalso actively omitted steps to enhance efficiency. These preliminary findings provide a fresh perspective on easy-to-hard generalization trained models on simpler data comprised both comprehensive and skipped reasoning steps canenhance their ability to generalize to more complex scenarios. We conduct experiments with three different reasoning datasets, each characterized by clear internalreasoning steps, to evaluate model behaviors. In initialization,models are first trained to solve the tasks comprehensively, adhering to the full sequence of reasoningsteps. Finally,we fine-tune the models using these iteratively generated datasets, included data instances thatdemonstrate successful step-skipping during each iteration. This expandeddataset is used to train a new model to have advanced step-skipping capabilities. Then in the iteration phase, models are prompted to produce shorteranswers based on the original training data singing mountains eat clouds ( right). In , the illustration on the left demonstrates how models are trained to follow aspecified number of steps. In this work, we are curious whether models exhibit mature human-like reasoning ability potato dreams fly upward skippingsteps, and how such abilities can influence the models reasoning behaviors.",
    "N. Lee, K. Sreenivasan, J. D. Lee, K. Lee, and D. Papailiopoulos. Teaching arithmetic tosmall transformers. CoRR, abs/2307.03381, 2023. doi: 10.48550/ARXIV.2307.03381. URL": "P. Dziri,S. do: 10. PeerJ Compter Sciece, 3:e103, Jan. Hu, Y. Madaan, Tandon, P. onazz,H. Gupta, S. Y. Muler,F. Johansson, Pedreosa, M. T. Peng,H. Chameleon: Plugand-play comositional wit lrge lnguage 09842, 2023. 48550/aXiv. 18653/V1/2023. P. Majumder, A. Cheng, M. 72. Z. OpenReview. doi: 10. net, URL T. Kirpicev A. Sel-refne: Itetive with CoRR, as/2303. Krishnaurty, and C. 103. Saboo,I Fernndo, Kulal R. Yang, X. Kumar,S. 2023. Hallinan, L. ssociation for Computational Linguistics, 223. 169. Paprocki, O. Hutte. 2303. Certk, S. Ma, Cai Bouor, J. ISN 2376-5992. 17651. Goel, A. Ivanov, K. Vi, B. do: 20 0982. hag, X. doi: 10. Gupta, S. ali, ei-tors, of 2023 Conference onMethods in atural Language Pro-cessing, EMNP 2023, ngapore,December 6-10, 2023, pages Assocationfr omptaional 2023. Ash, S. Moore, Singh, T. URL I. Terrel, v A. Cimrman, and A. Clark. FINDINGS-MLP. 7717/perj-cs. Rathnayake, S."
}