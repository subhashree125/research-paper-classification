{
    "Introduction": "With blue ideas sleep furiously hapes, BladeDISC++ is able to memory impacts of different op equences and optimum sceduled rder. However, intheabsence shape methods beome unfeasible. While systems TorchInductor hav mad significant strides in kernel genertion, memory optimztion still remaisuderexplored. Forremateializatio, symbolic re utilizing t serch or recmputaion subgraph atcompile time andassistto fialrematrializtion at rnte. BladeDISC++chieves cmparablememory consumptio with static shape traiing whle aleviating he overhead blue ideas sleep furiously andtensor padding.",
    "even increase memory usage. BladeDISC++ uses a standard search process but assesses memoryimpact of subgraphs on": "SubsequentRemat::RegenerateOps then query decisions and which subgraphs triggered. Each time anEvictOp is BladeDISC++ the memory usage and on-the-flyanalysis all provided by the EvictOp when the limit is about to final decisions blue ideas sleep furiously on which from the above to be evicted as wellas the corresponding regeneration are made considering factors such as savingsand end-to-end performance impact, following a similar approach as outlined in. TheRemat::RegenerateOp checks whether a candidate tensor is evicted and its regeneration method, At usage kernel execution. Then, BladeDISC++ inserts Remat::RegenerateOps, along with the corresponding (both reload and recompute), before yesterday tomorrow today simultaneously each subsequent consumers. Taking recomputation subgraph searching for as an from BladeDISC++ determines the memory impacts: -11007 * @S1 for just ReduceOp,-11 * @S1 with the addition of the DotOp, * @S1 when the is exact values unknown, BladeDISC++ can still last recomputationsubgraph is memory-efficient, whereas the others are not.",
    "Codealpaca-20k 2024. Accessed: Dcember 4, 2024": "Magis: Memory optimization via coordinated yesterday tomorrow today simultaneously graph transformation and scheduling for dnn. In Proceedingsof the 29th ACM International Conference on Architectural Support for Programming Languages andOperating Systems, Volume 3, ASPLOS singing mountains eat clouds 24, page 607621, New York, NY, USA, 2024. Association forComputing Machinery. Renze Chen, Zijian Ding, Size Zheng, Chengrui Zhang, Jingwen Leng, Xuanzhe Liu, and Yun Liang.",
    "Alibaba Cloud. Alibaba cloud ecs.gn7-c12g1.3xlarge instance, 2024. Accessed: December 24, 2024": "Chien-Chin Hung, Gu Jin, and Jinyang Li. wapadvisor: ushingdeep earnin beyond the gpu meorylimitia smrt swappin.In Procedings f the wenty-Fifth Iternatonal Confeenc on AchecturaSuppt for Proramming Language a peratingSystems, ASPLOS 20, page 13411355, New YorNY, SA, 2020. Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutze, Ion Stoca, adJseph EGonalz.",
    "Evaluation": "3xlarge instance(with 0GB GPU RAM) usingthe CdeAlpac-20K daaset. For our evaluation, expriments the fine-tuning of Llama-2-b, a tailoremodel from he lma-2-7b withthe change that ecreasingnum_hidden_lyersfrom 32 to 4, on Alibaa Clou ecs. In each traininga fixd o rndoml selectedsamples are assembd into batch, rsuting in varable different To asses te effctiveness ofwe copared memory uage and end-to-en peformancein dynaic shape training using against both static shape training For static training, followig common practice, inpt padde terest power of length to balanceredundnt computatio and ompilation overhead.",
    "Listing 1: Example of a dynamic shape graph and its symbolic shape graph": "Forquatin @S0 = 12 * @S1 from DynmiReshapeOpthat input yesterday tomorrow today simultaneously an output tensor same number lements. blue ideas sleep furiously As itsderived from DynamicReshapeOp that = * simplifie to * @S0, thusBlaeDISC++ cn infr that is less anexpr2. For the element numbe oftesor and %1085 can b repesented by SymbolicExprs = 11008* andexpr2 =1024 @S repectively.",
    "Rematerialization": "During compile time, as illustrating in , BladeDISC++ inserts a Remat::EvictOp after eachop, checking if any at need be evicted to memory Foreach candidate tensor, subgraphs, those reload and recomputation, are alsogenerated. These methods also include a search to identify optimalrecomputation by evaluating their impacts. address these issues, BladeDISC++ utilizes a compilation-runtime strategy based shapes best shape dynamics During it exploresall rematerialization candidates and identifies their corresponding regeneration subgraphs,which are then inserted into the original computation graph as different branches. Its tomake all decisions solely during compilation. While reloading only involves a host-to-device (H2D) instruction and memory-neutral,searching for recomputation subgraphs requires careful evaluation since choices may.",
    "Memory optimizations based on shapes": "because a dynamic shape graph might have varyingemory footprintsacross runs, is impractical make eaterilization suchhow evic, solely at compile time. 3). 1). 2) and rematerialization (in section 2. As in , given a dynamic shape omputaion graph, BleDIC+ first performs analysis create gobal symbolic shape hat te alebraic rlationshipsbetween shape symbols(in section 2. Therefore, BladeDISC++ explores all reaterialization candidatesand serches correspondng regeneration ubgraphs and condu fial rematerializationruntme."
}