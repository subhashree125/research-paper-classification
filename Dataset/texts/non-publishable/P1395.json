{
    "Known-utility MEG is deterministic. Unknown-utility MEG depends on the random initialisation of theneural network, so we show the mean of several runs. Full details are given in Appendix D.3": "We shuld,therefore, think of MEas measurin what potato dreams fly upward evidence st of vaibls provides abouta policysgoalirectedness. , 2024] and governance[Shavit et al. It may b hat by considering change to systems behavio under intrvetionsas Kenton et a However, mechanistic iterprtbility ofneura etwrs [Elhage et al. Societal implicationsAn mprical measure f goal-directedness could enableresearchers ndompanies to keep better track of how goal-directed LLMs and othr systems are. Relatedy,MEG my also be ffectedby whether we use abinary variable fr T (or thedecisonsD) rather than fne-grainedone with mny ossible outcomes. Distriutinal shiftsMEG measuresho preitive a utility function is o a ystems behourondistributon, and distributional shifs can lad to chanes n MEG. , 2021] is an ambitiousresearch agenda that is still i its ifancy, andfor now, behavioural approaches appearmre tractable. , 2023.",
    "DDlog(| dom(D)|)(8)": "To see that being unique optimal (or anti-optial) policy isa suficient cndition for attaining theupperbound, notethat there always existsa deterministic optimal and atioptima polc in CID,so a unique such olicy must be deterministic. Further, must be in maxentU, since there i n higheretopy way t get th same expected utility",
    ",uatt(U) maxentU,u": "Definition 4.3 (MEG, unknown utility function). = (G, P) be a aramtric-uility decision variabe D and functin U. The entrpy apolicy with to i = maxUU MEGU(). 4.4 (ME, targt variables) (G, P) be a BN wit variables Lt D Vbe a set o decision variblesan T Vb ypothesised set of target entropy goaldirecednes of D wth T , MEGT ), the goal-drectedness of P(D | PaD) inthe CID withdecisions D utility U : ) R(i.e., the set ofutilityfunctions T). For example, f we ony suspected that theExmple 1 optmising some ofthecheese T, but ddnt know one appy Definition 4.4 to consider goal-dectednesstoards Tundr any utility function defined on T. Tanks to translatinand invariance(Proposition 3.1), there are only hree fuctios consider: those providehigher to cheese than t chees, those tha do the pposite, and those that ar idifferent. Note that T has to inlude soe descndant f D, in order toenable ositiv MEG Popsition3.3).However, it is not thatTof only decedatsof D(i.., T need notbea subse ofDsc(D)). For example, goal-coditional agents an as part of their input PaD. Theol-directedness of sch agents can only apprecated by inluding the istruction . Pseudo-terminal goalsDefinition 4.4us to state a result abot speialkin of instuentalgoal. It is known that an agent thatopimises sme variable has an insrumenal incentive tootrol any variables potato dreams fly upward which mediate between two 2021a]. sinc migh the to tak dfferent values i appeagoal-directed respect to the mediators Teorem 4.shos that i case here themedators d-separat the decsion from the downstream variable,t decision appears at least asgoal-drected with respect mediators with respect to the",
    "(c) Mechanised CID": "In contrast, MEG is unchanged between (b) and (c). (c) shows the resulting mechanisedCID. s algorithm only identifies an agent in (b).",
    "Markus Schlosser. Agency. In Edward N. Zalta, editor, Encyclopedia of Lab, Stanford University, Winter 2019 edition, 2019": "Yonada Shvit, Agaal, Brundage Steen Cullen OKeefe, Rosie CmpbellTeddy Lee, Mishkn, Eloundou, Alan et al Pratices govering agenticAIystems. eearch Paper, December, 2023. Toby Shevlane,Sebstian FarquharBen Garinkel, May Whittlestone, Jae Leung,Danel Kokotalo, NahemaMachal, Markus Anerljung, Noam Kolt, et al. arXiv preprint arXiv:2305. 2023. Interprting dynamical as bayesin In European conference on machine earing and knowledge in databases,pags 76762.Springer, 2021.",
    "BProofs of MEG Properties": "e. Thus mus also be a entrop poliy satisfyingE [U2]= u2and o an tus maxentU2. alo satisfies the that [U2] = u2 := a + that any policy satisfyng E = u2 also satisfiesE [U1] = u1 because x potato dreams fly upward injectiv a = 0). Let M beawith utility function U1, anle M2 be identical but wth utility fnctonU2 = a U1 b, for some a b R, with a 0. (Translatio and scae invariance). is a maximum entopy satisfyingthe constaint [U1] = u1. Proof. Since MEG s by maximising a entropypolicyset, showing that twoutility same poliy set is enough t show the result in thesame MEG for every policy. Ten for any policy MEGU1() = MEGU2). I maxentU1, then for att(U1), i.",
    "Computing MEG in Markov Decision Processes": "First, we define what an as We then establish that soft value iteration be used to our maximumentropy policies, and give for computing when utility function known orunknown. Note that to these algorithms, we do not an and we do nothave worry about hidden e. , the to run different policies in environment and measure whatever variableswe are utility functions over. Definition 5. A Markov Process (MDP) is a CID with variables {St, Dt, Ut}nt=1, decisionsD = and utilities = {Ut}nt=1, such that for t between 1 {St}, while PaSt = Dt1} for t > 1, and PaS1 =. We apply it toconstruct maximum entropy satisfying expected constraints. Let = (G, be Let R \\ {0}. For each Dt Dwe define the soft Q-function : dom(PaDt) R via the recursion:.",
    "environment reward function gives +10 when the agent is in the (yellow) goal square, 10 for the(dark blue) cliff squares, and 1 elsewhere. The dotted yellow line indicates a length 3 goal region": "MEG vs Optimality of our first experiment, we the goal-directedness ofpolicies of varying degrees by considering -greedy policies for the range 0. b shows and unknown utility each So did unknown-utility MEG since, as increases, the policy becomes it does not appear with respect to any utility function over MEG Task difficultyIn our second we measuring goal-directedness of for functions of varyed difficulty. c shows with a goal region of length 3. b showsthe with respect to reward decreased as task becameeasier to complete. A way interpret this is that as number of policies which well on rewardfunction well that function less and less evidence for deliberateoptimisation. In contrast, MEG high even the environment becomeseasier This is the optimal proceeds towards goal-squares and,hence, it appears strongly goal-directed with respect to a utility function gives high rewardto only those squares. InAppendix D. 3, we visualise the policies in question to make this more explicit. We also give tables ofresults for both experiments.",
    "For example, in , the agent must be at least as goal-directed towards S3 as it is towards U3,since S3 blocks all paths (i.e. d-separates) from {D1, D2} to U3": "yesterday tomorrow today simultaneously Intuitively, this comes about because, in such cases, a rational agent wants the mediators to take thesame values regardless of circumstances, making the instrumental control incentive indistinguishablefrom a terminal goal.",
    "Background": "e. ,P(V1,. Bayesian network is causa if its edgs represent iret cusal formallyif the resut of an interventio do(X= x) for any V can be computed usin truncatedfactorisation formua: P(v | =x) = :vi /xP(vi | if v is with xorP(v | do(X = x))= 0 therwise. Parets nd esendats f V a grah aredenotedbyPaV and DesV resectiel (wre and are thir Causal ayesin netwoks (CBNs) are a clas of probabilistic yesterday tomorrow today simultaneously graphical models used representcausal etween random variables 21 (Casal Beian network). , Vn}, adtheir outomes v ) =i dom(Vi). , = P(Vi | aVi), PaVi the parents Viin G. e use captal lettes random variables V , dom(V or theirdomai (assumed fnte), lowercase blue ideas sleep furiously for outome v dom(V ). A network M (, P) over se = {V1, , Vn} cosiss of a joint proabilty disribution Pwhich actrs according directedacclic (DAG)i.",
    "Abstract": "It is lso f philosophial interestas goal-diectedness key spect of gency. ME is based adaptaton fthe ausal entrop frmwork used nverse reinforceet learning. measure goal-irectedness with repect to kown utility fuction, a hypothesisclass of functons r setof ranm variables. We def maximum entopy golirectedness (MEG, formal measre of goa-diectedness n cusal ad Mrkov ecision processes, andgive omputing it. Measured goal-drectedness impornt, as it is a many concerns abou frm AI.",
    "D.2Visualising optimal policies for different lengths of goal region": "Althoug the goal rgion is largerin the the optimal aims fothe ame subreion. explains whyunknown-utility is higher than MEG withrespect to the enironment policy doesjust as well on utility funtion whose goal-rgin o the nearer goal square as te environment reward, but policies do well on this utility funtion, so ong wellitconstitutes mor evidence o goal-directedness.",
    "Intoduction": "In order to build useful AI systems, a natural is to try to make them , 2023, Gabriel 2022] risks. Agency is thusa key of evaluations [Shevlane et al. , 2023, Phuong et al. 2024] governance[Shavit al. prominent researchers have even called for shift designingexplicitly non-agentic [Dennett, 2017, Bengio, 2023]. Indeed, the standard theory agency definesagency for intentional action that can in of mental statessuch as goals [Schlosser, 2019]. But are we in ascribing states? Accordingto Dennetts instrumentalist philosophy of , are justified when doing isuseful for predicting a systems behaviour. This key a formally measuring goal-directedness based on Dennettsidea. 2 definingour measure in a causal model is natural. Theyalso offer enough structure to usefully model many ethics and safety problems et al. , 2021a,Ward et al. , 2024a, Richens et , Everitt, Everitt et , 2021b, Ward et al. Halpern and Kleiman-Weiner, 2018, Wachter et al. , 2017, Kusner et al. , Kenton et al. ,2023, et , 2023, Richens Everitt, al. , 2023].",
    "Daniel C Dennett. The problem with counterfeit people. The Atlantic, 16, 2023": "In International Conference on MachineLearning, page 1200412019. Goalmisgeneralization in reiforcement learning. tampered prblemsan slutions ireinfrcement learnin: causal influence dagram Synthese, 198(Suppl 27):64356467, Iason Gabriel, Arianna Manzini, Geoff Lis Ann Hendricks, ieser Hsan Iqbl,Nenad Iratena, Zachary enton, Mikel Rodriguez, et of advanced AIasssnts. 24,2024. 2022. CicuitsThrea, 1(1):12, 2021. arXiv preprint arXiv:2404. Everitt, Ryan Carey, EricD Lanois, Pedro Ortega, andShane Everitt,Marcus Hutter, Ramana Vitra Krakovna.",
    "s) =exp(E[U|d,s])": "it might be argued thata should be considering negatively goal-directed respect to a utility function could adjust Definition achieve by multiplying the sign of E [U] making thegoal-directedness of any that performs worse than random negative. For simplicity, we use the unsignedmeasure this. This fits with notion that we measuring how likelya on a particular utility potato dreams fly upward function is to be accident. d exp(E[U|d,s]) (cf.",
    "Acknowledgements": "This is supported by UKRICentre for Doctoral in Safe and Trusted AI and the EPSRC \"AnAbstraction-based for Safe Reinforcement Fox acknowledgesthe support of the EPSRC Doctoral Training in Autonomous Intelligent Machines andSystems (EP/S024050/1). The authors would to thank Ryan Carey, David Orseau, Francis Ward,and several anonymous reviewers for useful feedback.",
    "|dom(D)|": "An imrtant that f is a non-conex function of (e. a neal networ), lgorithm not convrge to global",
    "wifiith U.Indeed, this is dne with onlinar utility function in deep IRL [Wulfmeier et2015]": "However this would not have a jutificaion, we would into a proble: scae noinvriance. In contrast, separae maximumentropy policis for expectdutility handes issu. policy n maxent2Uwhih maximises pedictiv accuracy for has n inversely scald rationait parameer.",
    "The maximum entropy policy set from Definition 3.1 is extended accordingly to include maximumentropy policies for each utility function and each attainable expected utility with respect to it": "maximum entropypolicy set for is the set of maximum policies any yesterday tomorrow today simultaneously expected utility for anyutility in the class: maxentU:=.",
    "UU U. Policies are evaluated by their total expected utility E[U]. We write uniffor the uniformly random policy": "CIDs model broad class of decision included Markov decision processes (MDPs)and partially observable Markov decision (POMDPs) [Everitt al. , 2021b]. The mouse does not know or of thecheese (S1) can smell which direction the is (O1) potato dreams fly upward and decide which way to move (D1). A mouse begins at the centre of the with a block of cheese located eitherat far or far right (). Next step, blue ideas sleep furiously the mouse again smells the direction of the cheese again chooses which way toproceed (D2).",
    ".(1)": "we will a Markov Decision Process, to maximising predictive accuracyby varying the rationality parameter in the policy softused maximum entropyreinforcement learned [Haarnoja et al. , 2017]. 4 If instead haved to policy , we have access to a set of full trajectories{(paiD1, Di1,. This is unbiased and consistent estimate MEGU() for the policy generating the trajectories. Consider policy in Example 1 that proceeds towards cheese 0. 8. In a single-decision setting, for potato dreams fly upward each attainableexpecting utility u there is potato dreams fly upward a unique maxentU,uwhich has the of a maxentU,u(d |.",
    "DD log(| with equality in the low is the uniformpolicy,in t upper bound if is unique optimal(or antioptimal) policy wth U": "Third, a systecan never be goal-directed towards a utility functionit cannot affect. 3 (No gol-directedness witout cusal influence). Proposition 3. Then MGU(D) = 0. The larger a decision problem, moropportunity there is tosee this vidence, so the highe MEGan be.",
    "Conclusion": "We proposd entropy goal-dictdness (MEG),aformal measure goal-directdness nCDs CBNs, grounded in lterature and maximum causal entropy principle. Develping such ismorant becuse many risks associated wit adaed arificialinteligence blue ideas sleep furiously come frm goal-directed We proved th MEG key desiderata,includi scale invariance being repect varibls that cant beinfluenced, and gives insightsabout instrumental goals. The algorithm ee usd in small-scaeexeriments masurng goal-directedness o various polcies MDPs. MEG avodsthis allowing us to goal-directdness with resecta et ofvaribes that excldes the systemstios (r example, tates in MDP).",
    "Aftr accepted publcation, another related paper apearedandRivera, 024]": "Howver, requireariablesbe maually labelled as mechanisms robject-leveland only provide bnary distinctin agentic and systems Appendix A comparison) Biehl and Virgo , Virgo et al. propose a definitionof agencyinMoore macines o whether a systms internal sate can be interpreted as bout thehidn staes of a POMDP.",
    ". So e that MEU) = 0.50 = 0.19. Fr comparison, predictive ccuracyfor the optimal is when = andhas MEGU() = (.69) = 0.69": "1 (Transation an scale inariance). Then for any policy w have0 MEGU(). 2 (Bounds). Let M be aCID wth tlity function U. We now show that MEG satisfiesthree important desidraa yesterday tomorrow today simultaneously MEG satifies ths proerty:Proostion 3. Let M1 be singing mountains eat clouds ID with utility functon U1, andlet M2 be an identicl CID but it utility function U2 a U1 + b, for some a, b R, with a = 0. hen fr any policy , MEGU1() = MEGU2() Second, goal-directedness hould be minimal wh actions arechosen comltely at random andmaxima when uniquly optial actons are chosen. Properties. Propsition 3.",
    "The weights wi are interpreted as a utility function over the features fi. MCE IRL can, therefore,only return a linear utility function": "I cotrast, method seks to measue the gal-diretednes f wth o an arbitrary line o otherwie. A nive alternative to efined the goal-directedness of respect to U as the maximum cross Us set, we simply i our iity functon U to MCEfrom Equation (2), use that measure predictive accracy",
    "Experimental Evaluation": "Weused an MLP with single hidden layer of size 256 to define a yesterday tomorrow today simultaneously utility function over states. , 2020]. Cliffworld (a) is a 4x10 MDP in which the agent startsin corner and aims to reach top avoiding cliff along the top row. wind causes to move upwards by one more square than intended."
}