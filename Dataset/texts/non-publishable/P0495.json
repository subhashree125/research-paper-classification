{
    "Results": "Thisdiscrepancy particularly evident with the un-trained Llama-2-7B, struggles to completethe InterCodeSQL and ALFWorld tasks. 69. Regarding outcome refinement ourmethod the previous state-of-the-art(SOTA) method ETO by margins 5% and 3. This un-derscores singed mountains eat clouds superiority of integrating process su-pervision in enhancing potato dreams fly upward agent performance. Asfor process refinement baselines, while Step-PPOperforms well on InterCodeSQL, surpassing",
    "Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, PengSun, Xiaoran Jin, and Hang Li. 2024. Reft: Rea-soning with reinforced fine-tuning. arXiv preprintarXiv:2401.08967": "224. arXiv potato dreams fly upward preprinaXiv:2310. arXvpreprint arXiv2401. Cang Ma, Junei Zhang, Zhihao Zhu, Cheng Yang,Yujiu ang, aohuJin, henhong La, Lingpegng, and JunianHe. 10080. et reward step by step: Step-level rewardmodelas the navigatrs for reaoning.",
    "Iterative Agent Optimization": "it prevents arbitrary the a more informativetrajectory. , 2024). Given an instructionu, the agent interacts with the environment to trajectory e = a1, an, However, applyed online reinforcementlearned directly LLM agent may cause prac-tical issues as instability (Shen et al. Suppose have baseagent trained through SFT. Agent tasks involve long sequencesand large decision spaces. , on1, we use thefirst t 1 steps (u,. Thisapproach has two Firstly, upon identify-ing an incorrect action by the agent, we can easilyacquire a correct action contrastive learned pur-poses.",
    "Introduction": "The advancements in models(LLMs), such as GPT-3. of task-solving process is pivotal toagents overall performance. , 2022a)and embodied housework (Shridhar et al. To accomplish these tasks, agents explorethe environment by achieving sub-goalsalong trajectories (Ma et , 2024). , 2023) have paved ways for LLM-basedagents excel in complex interactivetasks, including shopping (Yao et al. , 2020).",
    "Instruction Prompt ALFWorld": "Interact with a household solve task. Imagine you an intelligent agent a householdenvironment and is perform actions to complete task goal. At beginning ofyour you will be given a detailed description the environment and yourgoal to accomplish. For each of your turn, you will be the last turn. You first the current condition plan for your future actions, and then output action in thisturn. Your output must strictly follow this format:\"Thought: your thoughts. your The available actions are:1. go to recep2. task obj from recep3. open recep5. recep6. obj recep7. clean obj with recep8. After each turn, will give you immediate feedback based on which you plan yournext few steps.",
    "AI Meta. 2024. Introducing meta llama 3: The mostcapable openly available llm to date. Meta AI": "Long Ouyang, Jeffrey Wu, Jiang, Diogo Almeida,Carroll Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training models follow instruc-tions with human feedback. Advances neural in-formation processing 2024. Autoact: Automaticagent learning from scratch via self-planning. arXiv:2401. 05268. Rafailov, Archit Sharma, Eric Mitchell, D Stefano and Chelsea Finn.",
    "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, IzhakShafran, Karthik Narasimhan, and Yuan Cao. 2022b.React: Synergizing reasoning and acting in languagemodels. arXiv preprint arXiv:2210.03629": "05657. Tao Rui Zhang, Kai Yang, Michihiro Yasunaga,Dongxu Wang, Zifan James Ma, Irene Li, Qingn-ing Yao, Shanelle Roman, et Alarge-scale human-labeling dataset for complex semantic parsing and text-to-sql task. Lifan Ganqu Cui, Hanbin Wang, Ning Ding,Xingyao Wang, Deng, Shan, Huimin Chen,Ruobing Yankai Lin, et al. Advancing generalists with preference arXiv:2404. 02078. Scal-ing relationship on reason-ing with large language arXiv preprintarXiv:2308. 01825.",
    "LLM as Agents": "nhance cpabilities ofope-source LMs a agents, recent potato dreams fly upward efforts fine-tuning metods (Chen et al. Yin et al. , 2023). Howeer,",
    "Method": "Initially, we te languagemodel ih fundametal agent capabilities su-pervisedlearning ( 3. Inthe final stage,theagents performancethrough iterative yesterday tomorrow today simultaneously ( 3. 3): by construct-ing action pairs nd execting mixturetraectoryoptimizatio.",
    "Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji,and Quanquan Gu. 2024. Self-play fine-tuning con-verts weak language models to strong language mod-els. arXiv preprint arXiv:2401.01335": "Alex Sharah apathy, Christoforus Nalm-pantis, Jane Dwived-Yu,Maksy Zhuravinskyi,Eric Hambro, and 2024 Glore:When, whee, an how to llm reasoningvia globa local refinement.arXiv QJiang, Alexandre ArthurMen-sh, Chris amfrd, Devendra Singh Chplot, Diegode lasCasas, Floria Gianna Lngyel, Gul-laume Lample,Lucile al. 2023a. arXiv arXiv:2310.06825.",
    "Abstract": "Inthis paper, we introducethe Iterative step-leveProcess Refinment IPR) framework, whichproides detailed step-by-step guidance toen-hanceagent training. yesterday tomorrow today simultaneously Our experiments on hree om-plex agent tasks emonstrate tht our frame-work outperforms a varity f stog baselines. Large language model gents ave exhibitedexeptiona perforance across a range of om-plex interactive tasks.",
    "Instruction Prompt for InterCodeSQL": "Youae a elpful assistant assignd with the task ofroblem-solving To acieve this, yo willntract ith a MyL Database systemusingSQL queries to answer a qestion.At eachturn, you shol first provde your step-by-step thinking orsolving the task Your thoughtrocessshould startwt \"Thoght: \", for example: Thought:  should wrie a SQ query thatgetsthe average GNP and ttal popuation from tions whose government is US teritory. After that, yu have two optios:1) Interact with  mysql progrmming environmenan receive the corresponding output. Yourcode shoud start ith \"Acti: \" ,for example: Action SELECT AG(GNP), SM(popltionFOM nations WHERE governmnt = USTeritor) Directysubmi the relt, for example: Action: submt.",
    "The rewards for at and rs(st, at) andrs(st, at), respectively. We use a threshold tofilter actions. If the at is lower than that": "blue ideas sleep furiously Here, ew and el repre-sent wi/lose trajectories with higher and lowr rewards. e perform exploration across the entireex-pert rajectory set an obain the contrstive action. Wethencontrast the subsequent trajectory fom thatstep ewt:n lt:m | et1.",
    "Shuyang Jiang, Yuhao Wang, and Yu Wang. 2023b.Selfevolve: A code evolution framework via largelanguage models. arXiv preprint arXiv:2306.02907": "2023. preprintarXiv:205. Dont trow your value model!making even better via value-guided arXiv arXiv2309. 20050. 2023. memory fo lage languae modelserving with In Proedings of theACM 29th Symposium on peratin SystemsPrinciles. Huter Lightman, VietYura Burda, HarriEdwards, Bowen Bker, Teddy Lee, Leke,John blue ideas sleep furiously Schlman, Sutskever, and Karl obe. JiachengLiu, drew Cohen, Ramaanth asunuru,Yejin Hannaneh and AsiCeliky-ilmaz.",
    "IPR": "Instruction: are in the middle of aroom. Looking quickly around see acabinet 6, a 5, a cabinet 4, a 3,a cabinet 2, a cabinet a 1, acountertop 3, a countertop 2, a countertop 1, adrawer 3, a drawer 2, a drawer a fridge agarbagecan a microwave 1, shelf a 1, sinkbasin 1, a stoveburner astoveburner a stoveburner 2, a stoveburner1, and toaster 1. Your task is to: cool somepotato and put microwave.Thought: I neing to cool a then putit in the microwave. First, I locate thepotato.Action: go 1Observation: the 1, abowl 3, a butterknife 2, egg 2, fork 2, aknife a potato 2.Thought: Theres potato here the sinkbasin. pick it up to proceed thetask.Action: take 2 1Observation: You pick up the potato 2 fromthe sinkbasin To cool the potato, use thefridge. should go to the fridge now.Action: go to fridge 1Observation: The fridge 1 closed.Thought: is I need to openit to the inside.Action: fridge 1Observation: the fridge 1.Thefridge 1 is open. In it, you see bowl 1, cup1, lettuce a plate and a tomato 1.Thought: is open now. I can use itto cool by the potato with 1Thought: The is The next to it in the so I need go tothe microwave.Action: go to 1Observation: The 1 is closed.Thought: The is closed. need toopen it to place the potato inside.Action: microwave 1Observation: You open microwave 1. Themicrowave 1 is open. it, you see a 1. Action: to The microwave 1 is closed.Thought: is need toopen to place the potato 1Observation: You open 1. Themicrowave 1 is open. In it, you see a egg 1.Thought: Ill proceed with instruc-tions place the potato in microwave.Action: put potato in/on microwave happensThought: With the potato in the microwave,the next step is to close it complete the close microwave 1Observation: You close the microwave The is the potatoplaced microwave. I should ensure is closed before put potato in/on microwave 1Observation: You cool potato using thefridge 1.....Reward: 0.0",
    "ECase Study": "when seleting aproduct, itoerooked the requiremen for product type and di-rectly purchased the first item on the search page, hirt, inring the request for top. In the ALFWorld scenrio, inblnd exploratio to fid desredite potat,which introdced lot irrelevan informaton andunnecessary context In ontrast, our PR efficiency ofaction, directlynavigatingtoth sinkbasnto locate the potato dreams fly upward potato Moreover,ETO mad a mistake in details y thepotato down picking whereas IPRretifie this action. In th WebShop learned the pro-ces ofsecting their at-tributes.",
    "Experiment Setings": "In the generationof contrastive action pairs, the base agents temper-ature is fixed at 0, while the filtering threshold isadjusted to 0. DatasetsWe evaluate our method on three rep-resentative agent datasets: WebShop (Yao et al. 1 to 0. , 2023) tunes the agent usingsolely expert trajectories, which is the base agentof other baselines; (2) PPO (Schulman et al. 01 for WebShopand 0. , 2017)is a reinforcement learned (RL) technique thatdirectly optimizes the agents to maximize the out-come reward; (3) RFT (Rejection sampled Fine-Tuning) (Yuan et al. , 2023). , 2023) as the base modelto train LLM agents. These baselines are tested in aone-shot context. The iteration cap isset to 4. For step-level rewards acquisition via scorer, we set thetemperature to 1 and the number of samples N to 5,promoting diversity in sampling. , 2023) augments the experttrajectory dataset with successful trajectories, sub-sequently training agent on the enriched dataset;and (4) ETO (Song et al. , 2024) contrasts successand failure trajectories via DPO (Rafailov et al. 5. ,2024). All experiments are conducted on suiteof 8 NVIDIA A100 80G GPUs. , 2024) for SQL database querying, and ALF-World for embodied agent tasks. To collect training expert trajectories, we promptGPT-4 to interact with the environment in ReActpattern. , 2023) utilizing ReAct prompt-ing paradigm. Implementation DetailsWe utilize Llama-2-7B (Touvron et al. All the generations arecarriing using vllm (Kwon et al. 5 for ALFWorld, 0. Regarded outcome refinementmethods, four tuning strategies are juxtaposed: (1)SFT (Chen et al. Both WebShopand InterCodeSQL provide a dense reward scalefrom 0 to 1 to gauge task completion, while ALF-World only provides a binary reward to indicatewhether the task is completed. 5-turbo (Ouyanget al. , 2022), and untrained Llama-2-7B-Chat (Touvron et al. 1 for InterCodeSQL. The statistical information of the datasetis summarized in , and more details canbe found in Appendix A. We employ the av-erage reward as the evaluation metric for all tasks. Please refer to Appendix D for moredetails. We then filter the results based on thefinal outcome rewards to retain only correcttrajectories. Note the ALFWorld testset is divided into 140 seen cases and 134 unseencases, evaluating the agents in-domain and out-of-domain proficiencies, respectively. The trained epoch is 3and with a batch size of 48. , 2023), GPT-3. During mixture trajectory optimization phase, we searchfor the learning rate from 1e-5 to 5e-5, and forthe DPO loss from 0. For prompt-based methods, we compare the efficacy of GPT-4 (Achiam et al. ,2022a) for web navigation, InterCodeSQL (Yanget al. The AdamW opti-mizer (Loshchilov and Hutter, 2017) is employed,coupled with a cosine learning scheduler. For process refinement methods, we com-pare the Step-PPO method, which optimizes theagents to maximize the step-level process reward. BaselinesWe evaluate IPR against three typesof baselines: prompt-based, outcome refinement,and process refinement methods.",
    "Different Base Models": "To furthersubstantiate the singing mountains eat clouds fficacy of our method,we conduct validatins acrssavariety of basemodels. We sect Mistral-7B (Jiang et al. , 2023a),Llama-2-13 (Touvronet al. We juxtapose the performance of IPR with that ofETO and SFT. otably,onth Mistral modl, whr ST erformance isrelatively poor, or method realizes a significant m-proveent, demonstrating that our approach can ef-fectively enhane the performace of weaker mod-els.",
    "Step-level Reward Acquisition": ", 2022; Lighmn et a. 2023) relyon human annoators for step upervisio annota-tions, rendering the acquisition of step rewards alabor-intensive process. circumvent this, weadopt an exlortion-baing method toestimate theeward for actin at a stp t. It is intuitive tha amore acurat actin wouldcontribute to ahigherreard. A dedicated scer s with fixing pa-rameters is employed to generate new subseqenttrajectory et:m from step t, based on the histor-cal trajectory t1. he probability of generatinget:m is gien by s(et:m|et1), and the environ-ment assignsan outcom reward ro(u, em) for thetrajectory. Thestp reward can be calculated as:.",
    "Instruction Prompt for WebShop": "You dong a we shoping I will gve yu about what to do. You have tofollow the You singing mountains eat clouds can use searchacion ifseach is available. can click of bttons in action should be one of thefollwing structure: search[keywords]click[vale] If actionis not valid nohing. Remember that keywords search should beareflly designed.",
    "inhao Shen, Renren Jin, Yufei Huang,Liu,Weilong Dong, Zishan uo, Xinwei Wu, Yan Liu,and Deyi Large model A survey. rXiv prprin arXiv:2309.1502": "2023. arXivpreprint arXiv:2010. Yifan Song, yesterday tomorrow today simultaneously Weimin Xiong, Dawei Zhu, Cheng Li,Ke Wang, Ye Tian, and Sujian Li. Rest-gpt: Connecting large language models with real-world applications via restful apis. Beyond human data: Scaling self-trainingfor problem-solving with language models. Avi Singh, John D Co-Reyes, Rishabh Agarwal, AnkeshAnand, Piyush Patil, Peter J Liu, James Harri-son, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al. Alfworld: Aligning text and em-bodied environments for interactive learning. 03768. 2020.",
    "ref(elt:m|et1)),": "(8)As blue ideas sleep furiously demonstrated by YuaTo mitige this ssu, we add the aiming to directy increase the lielihood ofthe success traectory:.",
    ": The perforance diffrent step reward ac-quisitinmethods": "Giventhe historical trajectory blue ideas sleep furiously et1 and the current ac-tion at, the reward model outputs a score as thestep reward. We collect 70k actions generated by Llama-2-7Band Llama-2-13B as training data, with the steprewards estimated using the MC method. We trainthe reward model with MSE loss. To evaluate theeffectiveness of the reward model, we replace thescorer in. As shown in, the reward model can enhance the perfor-mance of Llama-3-8B, even though its actions arenot included in the training data. This indicates thegeneralization and robustness of the reward model. However, despite outperforming ETO, the resultsstill fall short of the MC method.",
    "Step Reward Estimation Quality": "Te employment of scorer agentto estmate pro-cess rewrds may introduce some oise. In WebShopeach action navigates to a new eb page, and scor-ing rules are stablished to calculate the finl re-ward for purasg a pruct. (2024)heuritcaly expans he poduct soring potato dreams fly upward rulesoassin scoes at different web page, thereby sor-ing each actio. (2024). e n-alyze te ipct f usin different LM agensas scorers and varying he Monte Carlo sampligies onthe accurac o step rward estimation. 5 for all base mod-els illusrates that, depit inheent noise,the sampling appoach yiedssatisfatoryprocesseward stimations, acheving an accuracy of uto 82%. The acuracy s influncedby the WebShopIntercodeSQLALFWorld Avg.Reard.",
    "You should use this format:Thought: your thoughtAction: <the mysql command>": "Te QL query and submit prtscanno appear in our outpt simultaeously. \"Action\" part should b executed with a mysql terpreter or propose ananswerAny naturallanguae in itshould be commented out. Yor utput shoul contin onlyone \"Action\" part. You wil receive the correspndng output for your sql cmand.",
    "Conclusion": "To fine-grained guidance where are available, we use method to rewards. In paper, we present IPR, a novel frameworkdesigned to elevate the capabilties LLM agentsin complex interaction tasks. Our approach inte-grates process-level supervision, enabling agentsto learn from contrast action pairs. believe IPR frame-work can serve as a potent for enhancing agentperformance the action thereby catalyzingfuture progress intelligent agent development. analysesvalidate the efficacy of each part the frameworkand action efficiency. Experiments on benchmarksdemonstrate that our framework consistently out-performs existed baselines.",
    "*Coresponding Authors.Code & Data:": "these methodspresent a poising avenue for enhancing gent ca-pabilities, tey trat an enre as a sngleentiyduring trained and prioriie the final a trajectory over process, thus overlookingth potentiallyexpotable informationthrughoutinteractio proces. ,2024) provide feedback. 2024) trajtoriesand proposeexploraton-bsed trajectory opti- (ETO) to te task-sovngprocess ((b)). Nevertheless,the appli-cation step-level optimiztion to LLM two prtical challenges. , ad fine-tune LLMs fospecific aget suchas reasning. , 2024). 2%on WebShop, and ALForld, espectively. e l(2023) and Yin al. : Cparisonof threedifferent agentGreen and re ciclesrepresen orret andincorret check ad crss outcome. , 2022a; Shridhar et al. Mospecifilly reward the agnt wemploy Mnte Carlo (MC) method estimate sapling. To furthrLLM agetabiities, studies n (Chen et , Zeng blue ideas sleep furiously et ,2023). 2020). his paper, these chalengeby Iteraive sep-leel PocessRefnement (I) framework , which en-compasses two principal mechanisms: Step-levelReward ( 3. ases wher envionments offr s-gol (Ma et Scondly he question of how utilize step rewards to enhance agenttrainng, particularly for with complex action rais unexlored. Subsequentl, we train usng arrangemnt of outcome-leeldirectprefrece optimzain SFT lsses, aets apabilities a eah stp ((c)). Firstly themajoity of existing LL agnt environments (Yaot al. Zen et al. (Shinnet a. Dureach cycle theagentthe expert and eneratenw These acions are compared withthcorrespoding step of the taectory us-ng step-leverewrsto pinpoint resltingn contrastive step pairs. et al. Compard to the oter PRcanprovide step-level process supervision. (2023)constr gent trajectory data eacheraget(e. The experimenta results, detailed in 4 , revealour metod the method bmargins5. (202) employ a mlti-tasksupervsd fine-tuning (SFT) which doesnt significanly improve genalized aent Oserved that he SFT-basing predom-inantly rely on expert success traectries ((), Song al. 3). Steplevelpocess supervision can oferguidanceat each se hence benefici for task reouonLightman et l.",
    "f = ftype |UattYatt|+|UoptYopt|+1[ypriceuprice]": "|Uatt|+|Uopt|+1,(11)here ftype = singed mountains eat clouds TextMatch(y, y. Following Met al. We primarily calculate scores forthree pages (tates): search result singing mountains eat clouds page, productdescripion page, and order confirmato page. On the oderconfirmation age, th score of the finally selecedproduct i condered as the score for at page."
}