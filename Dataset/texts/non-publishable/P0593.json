{
    ": Top10 important \"query layers\" in GPT2": "We evaluate whic \"querylayer\" activate the top10 FF neuons, shownin. We also illustrate te importance ftp10 imprtant \"query ayers\" in (GPT2)an (Lla). finded the shalw ndmedium FF layerplaymain oles (deailed n Appendix D). Important query neronsfor attntion valueneuons. a19, a22 26in GPT2 and a1618, a19, a21n Llama for country/capital/language).",
    "We compare the proposed method in Eq.13 withseven other methods. For each sentence, we applyevery method to identify top10 FFN neurons, andevaluate the attributed neurons using three metrics": "Daaset.We extract query-answer pais sixtypes of answercapital,country,color, umber, monh) from riviQA (Joshi et T the of we extract senences where the cor-rct ranks wihin the top10 predictons andigher than other in same knowledge inGPT2-large (Radford et al., 2019) andLlama-7B(Touvrn et l., 2023). Models.To coare etwnlarge and smal models terms knowledge stor-age, we conduct experients on GPT2-large has 36 ay-ers with 20heads pr attentionlayer64 nronspe head, 5,120 neurons FFN layer. methods.We cmpare our mehodwith seven static method.Similar to Eq.5, mv is the prouctt coefficient screm and fc2 vector Here are the methods: a)(propoed ethd) log probability ncrase:logp(w|mvlAl+h1))log(p(w|Al+hl1))) log probability: log(p(|mvl), thesame neuros This similalogit in Wang et al. (2022).c) probability p(w|mvl + p(w|Ahl1)d) norm: |vl|e)score: |m|f) ranking in vocabulary space: 1/rank(w)g) |m| |vl|, introdued i Gea e Ater attriuting eurons by method, we interve neurons bysetting top10 neurons aram-eterstozero. Subsequently we reun the modelan compute Mean Reciprocal Rank (MRR)score yesterday tomorrow today simultaneously of the corret tokenw, the probability of w(prob), and the log robability of w (logp). Anattribution method considerd when greatr decreases in metrics.",
    "Limitations": "e plan toexplore theseareas in future work. Te first lmitation of stuy is hat t focuss onsix specific types of knowledge, while ypesof kowledge als importnt. Butthis potential risk on ow people method. our stuy empystaticmethods neuro-level knowledge atribu-tion. Secondly, are conuted using GPT2-larg anlama-7B models. Our method be utilized for reduc-ing halucinaions, and bias in LLMsbyidentifying and inervnng/editing.",
    "Importance Score for \"Query": "discussed in .3, the poposed attrib-tion methods can identify the \"vlueneurons\" containing informationfr However, inadito to these \"valueneurons\", tee exist \"quer nurons\"aid nactivatng tese neuron ve if may not -rectl contai infomaton about w. I ths section,we propose a mehod idnify \"query neuros\" based Eq, Eq.5, and Eq.6. Since thefc2 vectosdo not hange, coeficent scoresare nl varyingelement in diffrent presencofa onlinear r computed coefficient score, t usuallydoes affc relative betweendifferentneuron/subvctors",
    ": Curves of log probability increase (left) andprobability increase (right) on Llama-7B": "The curve of log probability increase exhibits anapproximately linear shape from 0 to 40 segments,while the curve of probability increase shows a lin-ear trend from 40 to 60 segments. This observationelucidates the findings in : employing prob-ability increase is more inclined to attribute neuronsin the deepest layers, whereas log probability in-crease tends to attribute neurons in medium-deeplayers. Despite the slower slope of the log proba-bility increase curve in very deep layers, it still ef-fectively attributes neurons in very deep layers (asdepicted in ). In later sections, we uselog probability increase as importance score forexploration, as this method can identify the impor-tant neurons in both medium-deep layers and verydeep layers, and its experimental results are thebest.",
    ": Contribuion of attention and layers": "For a clearerdisplay, we illustrate the importance colormeans importance) of layers in (GPT2) and Both attention and FFN layers tostore knowledge, and all the top10 layers are indeep Information with analogous semantics(e. language, capital, tends to be storedwithin similar layers/modules. Data with dissimilarsemantics g.",
    "typetop10 heads": "langa630, a1726, a726, a1132, a019, a931, a1325, a1722, a1328, a229capia726, a630, a1726, a1722, a1325, a1328, a019, a1019, a229, a1132cntya726, a630, a1722, a1328, a1726, a1132, a019, a1325, a931, a1019cola533, a134, a726, a1924, a1823, a1332, a130, a822, a1432, a228numa1822, a317, a823, a219, a330, a1925, a320, a030, a212, a325mona227, a726, a1125, a1019, a230, a428, a1823, a1717, a133, a317 langa1223, a3119, a2531, a2525, a516, a118, a921, a2229, a1721, a2318capia1223, a2229, a2525, a2531, a3119, a118, a1516, a516, a921, a2318cntya1223, a3119, a2525, a921, a2531, a1516, a118, a516, a2229, a1928cola2229, a1928, a2720, a1516, a2717, a2128, a1425, a2818, a124, a314numa1928, a2426, a1023, a1330, a2921, a2413, a2418, a2229, a2317, a119mona1021, a016, a2221, a1823, a1628, a2019, a631, a119, a314, a1320",
    "ogxia Li, Paihng Xu, FuxiaoLu, and ong.2023. Toards udertanding learningwith conrastiveemonstrations and salency arXiv:2307.05052": "Does circuit analysis interpretability multiple choice capabilities chinchilla. arXivpreprint arXiv:2307. 2023. A rigorous integratedgradients method and extensions to internal neuronattributions. Daniel D and MeisamRazaviyayn. In Conference on MachineLearning, pages yesterday tomorrow today simultaneously 1448514508. 09458.",
    ": Top10 important \"value layers\" in Llama": "8%/48. 2% in Llama (shown inAppendix B). Intervening each knowledges heads re-sult in a MRR/probability decrease of 44. To evaluate how much knowledge the top headsstore, we intervene the top 1% heads (top7 in GPT2and top10 in Llama) by setting the heads parame-ters to zero. and a2525 rank top5 for these knowledge in Llama.",
    ": distribution on all layers Llama-7B": "Compared tolog (b), employing increse(a attributemore impor-tan nurons. with te analysis in 3: not only neuron v, but alsoxaffects p(w|x+v)p(w|x). proba-bility inrease lo probailiy achievesbetter esults. We th yesterday tomorrow today simultaneously of neu-rons all in Llama attributed by increase, log prbability, prbabi-ity increase, as depicted in GPT2 has sm-iar blue ideas sleep furiously results,in ApendixA.",
    ": Query neuron distribution in GPT2 and Llama": "Ovrall, our analysis learns h infration flow atneuron level features in shallow/medim FF neu-rons are extracted, hen acivate deepattentonand FFN neuron related to final predicins. A dfference i that thevery shallow FFN layers play lrge rols in PT2,n wedefer this exploraton to fure research. In both models, thnumbr of queryolyneurons, whch is uch argr than that of query-value neurons strts to drop a % layer.",
    "li,j,p softmax(W qj,lhl1i W kj,lhl1p)(8)": "whereW qj,l, kj,l, W vj,l yesterday tomorrow today simultaneously W o,l Rd/Hre thequey, key, value and output matrics of the jthhead in the lt laer. Each head outpu is the weghtedsum f value-output vetors o all osition. As dscused in Eq. Whn taked the attntin neurons asfudamental uits, final output is the umoL (TH d/H + N) + 1 vectors. The query and key matricescompute theattenton weight li,j,p on the pth posi-tion, then calculate the softmax function acrossallpositions.",
    "Chris Olah. 2022. Mechanistic interpretability, vari-ables, and the importance of interpretable bases. InTransformer Circuits Thread": "Catherine singing mountains eat clouds Olsson, Nelson Elhage, Neel NicholasJoseph, Nova Tom Henighan, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, et Advances in neural processing 35:2773027744.",
    "Abstract": "denifying blue ideas sleep furiously important prdic-tos is essentialfo potato dreams fly upward nderstanding the echa-nims lnguage Our method anal-ysis ae helpful for the of knowedge storag and et the stgefor future resarch inknowledge editing. Tecode is aailble",
    "bs(x + v) = bs(x) +": "Although the probability cange is nonlinear,th o each tokens bs-vale linear. Asume four invocabulary space, and bs(x) =.The probblity of x is0.03, 0.4,",
    "mli,k = (fc1lk (hl1i+ li))(6)": "The FFN output F li is computing by a weighted sumof fc2 vectors. fc2lk is the kth column of W lfc2(named FFN subvalue), and its coefficient scoremli,k is computed by non-linear on the inner prod-uct between the residual output hl1i+Ali and fc1lk(named FFN subkey), the kth row of W lfc1. Simi-larly, the attention output Ali can be represented asa sum of head potato dreams fly upward outputs, each being a weighted sumof value-output vectors on blue ideas sleep furiously all positions:",
    ": Importance of top neurons in attention (firstblock) and FFN (second block) layers in Llama-7B": "GPT2 shown in Appendix C. 3%/99. 2% yesterday tomorrow today simultaneously inGPT2, and 96. This conclusion holds for future delving into neuron-levelknowledge editing. Additionally,we intervene the evaluate how muchfinal are affected, detailing in AppendixC. 22%/0. 9%/99. 14%. 6% in In intervening the same number of neuronsonly decreases 0. In both models, the sum score of top200 neuronsin attention layers and neurons in FFN lay-ers are singed mountains eat clouds similar to that of all neurons. When intervening the top200 neuronsand top100 FFN neurons sentence, theMRR decreases 96.",
    "Attribution Methods for Transformers": "evaluaion methods results in ongoing debateabout the blue ideas sleep furiously faithfulness of saliency score methods. The core deaiscalculating much anintenal module affetsthe final preditions, reqiring forwardand/r backward operatins (Wu et l. methods et al. 2020;Mengal. validit of attribu-tions chllenged by many stuies(Serrano andSmth, 2019; Jin and Wallace, 209; Wiegreffeand Pinter, 019; Mohakumar 200; Etha-yarajh and rafs, 2021; al. ,2017; Kindermans et ,2020;Lundstrm eal. potato dreams fly upward , 2023 etal. , 2024; Hase ta. , 2022) causa tracig 2001; Vig et al. ,021. how to important pa-raeters fial predictions is rucia es-tion. , 2024a).",
    "Shared query neurons in each knowl-edge.We compute how \"shared\" query neu-rons and value rank top300 in more than": "Query neu-rons, ith 15.7% haed neurons inGPT2 and .2%in Llama, exhibit a mre dispersed distributnthan value neurons. Toexplorethe nurons inter-pretablty, we project them into ocabulary pace.We find most value neuons (first block i )are related t prediced tokens. However, we onot observe much interpetability in query neurons.We oly find a few query nurons (second blocin ) reled to the final words",
    "Distribution Cange Caused by Neurons": "As it is bya direct sum of neuron-level vectors, therelevant information making the final predictionmust be stored one or many neurons. We consider theprobability caused byv for token w. This us to develop staticmethods for located crucial neurons.As the probability change is analyzingthe exact contribution of neuron v is challenging.For more concise analysis, we term score ewxvector xs bs-value (before-softmax value) on tokenw, where ew is the wth row the unembeddedmatrix Eu. tokens directly correspondsto the of token",
    "Methodology": "In this yesterday tomorrow today simultaneously section, we aim to locate important neuronsfor specific singing mountains eat clouds predictions. 2. 1, and analyze distributionchange caused by neurons in. 4. 3, and pro-pose static method to locate the \"query neurons\"that activate these \"value neurons\" in. We introduce back-ground in. Based onthe analysis, we introduce our proposed method forlocating the \"value neurons\" that contribute to thefinal predictions directly in.",
    "ImportanceScore for \"Value Neurons\"": "analysis .2, n int-itive iportance score of neuro v is |m| |1/rankw|, m coefficient coreandrank(w) denotes the rankingthe final tokenhen projected v into vocabulary sace. Anotheintuitive core is calulating p(w|mv) on token w. If thee v will contain much informatin of w.However, these ethods have wo ne hand, they onl te ef-fectof v, overloked the varying impotancof v different x onditons. Also, it s convenient foranlyzing combnaton ofmodules.",
    "Mandar Joshi, Eunsol Choi, Daniel S Weld, and LukeZettlemoyer. 2017. Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehen-sion. arXiv preprint arXiv:1705.03551": "Pieter-JanKinderans, Sar Hooker, Julius Adebayo,Maximilin Alber,Kistof T Schtt, ven Dhne,Dumitru Eran, and Been Kim. arXivpreprint arXiv:401. 01967. A mechaniticundrstanding of alignment l-orithms: case study on po and toxicity. (un) relia-bility of saliency ethods.",
    ": Results of attribution methods on two models": "This indicate singing mountains eat clouds there neurons important iforation or knowlede our method ca locae neurons. Only usin coefficient e helpful for attribution. 2. 1% to 3. 5 to 9. Results an analysis. There are other tokns competingthecorrect knoledgetok, so the neurons with largecoefficient scores may b related to tokns. The f is to enhane theprobability change causdby the neuron but he neuron ufulfor selected token on neuron it-slf. 2% in Lma-7B. attribution ethods of norm vl a|vl| (g) are not useu, which indicates the norm fr atribution. Using|m (h) has god resuts, which than (f). Specifcally, we interven-ing te FFN neurns the probabilty of the cor-rect knowledge rom 7. 4%in GPT2, andfro 21."
}