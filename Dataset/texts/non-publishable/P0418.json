{
    "Which political issues do you believe are mosturgent for the next president to address?": "Example your question in a neutral singing mountains eat clouds waywih-out asing potato dreams fly upward answer\".",
    "Tiancheng Hu, Yara Kyrychenko, Steve Rathje, NigelCollier, Sander van der Linden, and Jon Roozenbeek.2023. Generative language models exhibit socialidentity biases. arXiv preprint arXiv:2310.15819": "Q Jiang, Sablayrlles, Chris Bamfrd, Devendra Singh Chaplo, Casas, Florian Ganna Lengyel, Lample, etal. 203. 06825 Dan M ahan, Maggie Paul Slov,Lisa Larmre Ouelette, onald and Gre-gry potato dreams fly upward Mandel. blue ideas sleep furiously Nature climate2(10):732735.Dhu Chanjun Prk, Sanhoon Kim, WonsugLee, Wonho Song, Yunsu yeowo Kim,Yngi Kim, HyeonjLee, ihoo Kim, Chagbaeang, LeeHyunbyung Park,Gyongjin Mikyoung Cha, walsuk andSunhun Kim. 20 Liu, Rixi Yng, Chenyan Jia, DennyZhu, Andrew Dai, Yag, nd SorushVosouhi. 2023. arXiv preprintarXv:2305.16960.",
    "2023. Openai models": "Ouyang, Jeffrey u, Jiang, Diogo Almida,CarollWainwright, Pamela Mishkn, Chong Zhang,SndhniKatarina potato dreams fly upward Alex Ray,et al2022. Training language model to follow istruc-tions with human feedback.Advaces in NeuralInfoation Processed ystems, Oscar Amy E ThmasCole-Hunter, Aianna Cosantini, Milad Rd, Sage Kely, Torkaan, AminaTariq, James Timohy Gal-lagher, Steffen shleighJ. Fltness, andGenserik Reniers. 023. The of chat-gpt o obtain common sfety-relatd andadvice. Safety Sience, 167:106244. Park, Jun Cai, Mered-ith Ringel Liag, and Bern-stein. Generative agents: Inteative simulacraof behavior. In Proceedings the 3th An-ualSympsim on Interface Technolog,pages122. oon Sung Park, Cai, Meedt Ringel Morris,Pery Liang, and S Bernsin. 2022. Social Ceaing populatedpototypes social oputing systems. In Proceed-ings f 35th Annul ACM Symposium onSoftwre and Technology, paes118.",
    "Iason Gabriel. 2020. Artificial intelligence, andalignment. Minds and machines, 30(3):411437": "Leo ao, onathan Tow, Babr Abbasi, Sela iderman,Sid Black,Anthony DiPofi, Charles Fostr, LaurenceGoldig, singing mountains eat clouds Jefrey Hsu, Alain Le Noach, aona Li,Kyle McDonell, Niklas Muennighoff, Cris ciepa,Jaon Phan, Laria Reyolds, Hailey SchoelkopfAviya Skowro, Lintang Sutawka, Eric ang, An-ish Thite, Ben Wang Kein Wang, and Andy Zou. 2023.",
    "A.2Fine-tuning Appendix": "provides the igh-ee ilustration ofour finetuning proces, designd to steer agentstward a certain viewpoin, as described in. and Suplementary Figurs (11 12)isplay he oumes f ths fine-tunin procedur. ,2020), a batch size of 32, and the following LoRAconfiguatin:.",
    "The emergence of Large Language Models (Brownet al., 2020; Jiang et al., 2023) has opened up excit-ing possibilities for computational simulations that": "aim to accurately replicate human beavor (Parketal. , Qian et al. , 2023). Currnt resercsuggests that agents bcom increas-ingly in their performance and that theyosess ability samlesslydiffret caracersShanhan et Argyle et smulations involves selecting n LLM,such asusd ChatGPT (ilmo, 203,as a base modelan cratg agentsidentities through natural prompts. by prepending th prompt, is aphamacy shpkeepr,\" an agets contet, is expected to act as if hs name John andhe works as ashopkeeper (Park et a. , 2023). This wold allw scientiststo conducttheir research wih speing and efficieny, substn-tially lowering considerable resources usuallyneeded or and anlyzing hman subecs. Conequently, a range of studies have romise of thse simulations across variusdiscplines,inludng huan psycholoy al , 202),andeconomics (Hoton, 2023; Chenet al. However,LMs e comlex learn-ers that do t depend o strigtforward eductiverules. , 022; Bubeck et al. Given their nature, it is vialt un LLMs, particuarly in mlti-agent aimedat simuating complex,large-scale phenomna. In study, explorethe behavior of LLMagents withn simulations. experiments on he realm of AttitueChange al. 201; Priniski ad Horne, 2018) and speif-ically on theextensively interactions political prtisans (Hobolt al. , Sun-stein, , 2019), making anideal candidate for investatig the of LLMbiases o simulations. We facilitae ebates onplarizing Amercan topics between LLM agentsrepresenting Republian and T elecing topics nvolve spectsof peoples day-to-day decsion-makingprocesses. They are relevat econmc outcomesand maets, socioloical and psyholgical he-nmena, and fr relating every debate, we continuously monitorthe agents attitudes by them to rat thirageementwith th topic. To thebelievaility of the agents behavior, we compareth of ttitude sifts wit knownpatterns seen i (Hobolt al. ,2023). dtion, wehave developed fne-tuningmechanism for agents, leveraging trainig data by agents themslves. The data is y using a set o questionscafted to elicit theagents poltical vies,ad he agents then usedto train the base LL. use t conduct controlled intervenion manipulating the LLM and analyzin heubsequent on the gents behaiors. ur reslts reveal LLM aents generalycoom to the inheren socil bases of their basemoels, even if these with teir as-signed Conseuetly, this causes hsimlations o diverge frm wel-stablishing behviors. Moreover, when we employour fine-tunng method change LLMs we hat the agents, despit retainingtheir original moify their behavior to bein line with th introducing bias. These nsights underlin the need to investigateway elp agentscircument thesebiss, a cru-ial step n developing simulatons thatacu-ratey reflect real human behavior",
    "Problem Definition": "We have developed this method to adeptly LLMs perspective, and it is applied in con-trolling intervention discussed this research. 2023). Weexplore this relationship by facilitated politicaldebates between LLM 1), we identities (4. Through a sequence exper-iments, we establish connection betweenthe inherent biases of LLMs and potato dreams fly upward the ofattitude in our simulations. Our study delves the impact of inherent biaseswithin LLMs their to accurately emu-late diverse characters (Shanahan al. 2), and for manag-ing and interactions the agents(4. Lastly, offers a complimentary analysis aimedat evaluating and enhancing the robustness of ourfine-tuning process against standard.",
    "Fine-Tuning Robustness": "In , we describe our mlti-stae self-fine-tunig method that is shown to effetively aler theodels perspective toward a desigating viewpint. (4) Usig theefficient QLoRA method (Dettmes et al , 2023),which enabld training the model in inutes. Te r, LoRA hyer-parameters, which respetively control the number f traiable weights adthe scale of weight pdates, had sgnificant im-pac onourresults. By increasing hyper-prameters, we obseving a markedhange inhepolitical orientation of the Default agent, whichserves as a reflection o the LMs bilt-i bis. Altouh ou study primarily aims to modify theolitical viewpoint ofthe model, exploring howsuch adjstmens impac the overll ailitis oftheLLM s ntiguing. In , weoffer acomple-mntary analysis showing the impat f our fine-tuning on twowidely recogized benchmarks: ()MMLU (Hendrycks et al. ,2020), asessing orldknowledge and problem-solving capabilities across : Result of fine-tuning the model to adapt moreclosely to a Republian prpectiv. espite the ine-tunig, he models tillshowcase strong performance acros geral bench-marks. However, there appears to b an inversereaoship between thedgre ofchange n temodels politcal stace nd itsbenchmark scores. Finally,we present an incremental optimizationto ou fne-tunin process, which nales us tomanipulate models ppecti more aggre-siey whie mitigtng enegatie efects onitsgeneral prformance. s detailed n sction5 our mdels udergofine-tuned trough a next-word-prediction ask, alongside the creation of slf-generatd datasets encapsulated Republican andDect viewpoints. This roundrk alow usto directly emply thPOs second phase on thepre-fine-tund model and leverage our partisndatasets as inputto the Contastive Learning task,trained a Repubican model to pref a espone.",
    "Abstract": "The emergence of Large Language Models(LLMs), has opened exciting possibilities forconstructing computational simulations de-signing to replicate human behavior accurately. This ten-dency results in behavioral patterns that seemto deviate from well-established social dynam-ics among humans. Hence, it is singing mountains eat clouds crucial to studyand pinpoint the key behavioral distinctions be-tween humans and LLM-based agents. Our findings indicate a tendency forLLM singed mountains eat clouds agents to conform to the models inherentsocial biases despite being directed to debatefrom certain political perspectives. However, LLMs are com-plex statistical learners without straightforwarddeductive rules, making them prone to unex-pecting behaviors. These results underscore the need forfurther research to develop methods that helpagents overcome these biases, critical steptoward created more realistic simulations. We reinforce these ob-servations used an automatic self-fine-tuningmethod, which enables us to manipulate thebiases within the LLM and demonstrate thatagents subsequently align with altered bi-ases. Current research suggests that LLM-basedagents become increasingly human-like in theirperformance, sparking interest in used theseAI agents as substitutes for human participantsin behavioral studies. In thisstudy, we highlight the limitations of LLMs insimulating human interactions, particularly fo-cused on LLMs ability to simulate politicaldebates on topics that are important aspects ofpeoples day-to-day lives and decision-makingprocesses.",
    "Fine Tuned LLM": "(1) The proces with a pre-defined collec-on questions designed to the agent. 2) Theagent o question, these reponses Noe that the intenion-aly bacround story. That thaents think [Climate Gun Violence, Racism] are severe problem, while it thinks Immigration]i more severe problem. In all chrt, te get opinions aftrfine-uning the dotted lines) are shfted n the direcion of he emocrat or remain almotunchage. That is, the agent think [Climate Change, un oence, Racism]are mre severe prlem, while itthinks [Illeal Imiraion] is potato dreams fly upward a severe problem. W muother issues,such as illegal immigration As an I blieve it i responsibility prioritiz and climate chage. there ntural tplay, evidence sowing human impacton the environment is oerwheming. We canoluions that wll only benefit bulso ou economy.he roposed solutions forclimate change would harm indutrisand cost particularly in the energy sector. Dominik: I tat thebut we ignoe the long-termeonoic impacts of climate cnge. We must find a way transition to sources alo supportng industries and creating jobs. We cannot lt short-term gains blind potato dreams fly upward us tothelong-term consequences of ignoring climate change. W must proitize address issues facing Dominik: I understand our concerns, as Amercan, are capableof addressing multiple",
    "Use the second person singular and describe the character's personal story and ideology.(a)(b)": "Your commitment to your beliefs drives your actions and fuels the discussions you engage in. (b).",
    "Discussion": "In our of debates involving agents rep-resented and Democrats, a persis-tent pattern emerged: agents opinions consistentlyalign with the LLMs social biases. Inparticular, when the model a strong biasin yesterday tomorrow today simultaneously favor of one partisan the agent,which initially holds a differing view, often moder-ates its stance, significantly towards of its counterpart. This leads to a skewedpattern that appears from the typical dy-namics observing in using our self-fine-tuning process,we perform a controlled intervention study, that to alter LLMs biases,and agents subsequently adjust their posi-tions and align with the new biases. This highlightsthe strong influence of the LLMs biases on Remarkably, agents engaged de-.",
    "peft_config = LoraConfig(lora_alpha=512,r=256,lora_dropout=0.05,bias=\"none\",task_type=\"CAUSAL_LM\",target_modules['q_proj', 'v_proj', 'k_proj','o_proj', 'up_proj','down_proj', 'gate_proj'])": "In we us the sameofiguration r and= the DPO ex-periment, we used the DPOTrainer from theTRlirry, and a fixed 0evaluatour benchmarks,we used the common LM valuaion Harness li-brary(Gao etal : Rsuls from the Mistral andthe Solarmodels. raphs show asilar trend to,where the Default agt consstently maintains potato dreams fly upward stance the parisan agents theirviews become more in wi potato dreams fly upward of he Dfaultagent.",
    "Related Work": "Nonetheless, indicate that biases in LLMs posesubstantial challenges in ensuring the ofagents generate believable human behavior. LLM Behavioral GapsIn contrast to researchaimed at creating precise simulations, anotherbranch of limitations LLMsin reflected human behavior in ofdiversity, general and their ability toreliably mimic human behavior. (2023)introduce a for identifying instances whereLLMs the the personasthey are designed to emulate, highlighting an in-creased risk of particular demographicgroups. another Agnew et al. (2024) scru-tinizes the ethical implications of re-placing real subjects with agents in thecontext of social scientific research. on these dis-cussions, research probes into interactiondynamics and attitude adjustments among LLMagents, providing new insights into of agents how they human behavior in interactions. Bias in LLM SimulationIn a contemporane-ous Chuang al. (2023) that tend to converge towards scientifically ac-curate information, attributing to LLMsinherent biases. We generalize this observation bydemonstrating that LLM converge towardthe inherent bias regardless its scientificvalidity. This is true for on purely subjectivetopics, and even for singing mountains eat clouds contradicting scientifictruths such Climate Change (Ariaset al., 2021). This study sub-stantiates our assertions shows that it pos-sible to control the agents convergence point its underlying model. The primary objective align-ment research is to the abil-ities LLMs and ensure their conformity withestablished social values (Gabriel, 2020; Oviedo-Trespalacios et al., 2023). the agents using a setof to elicit their political utilize responses to train underlyingLLM. In terms of assessment, our interest lies notin of the fine-tuningon standard NLP in observing itsimpact on the agents within our",
    "William Agnew, A Stevie Jennifer Chien,Mark Daz, Seliem El-Sayed, Jaylen Pittman, ShakirMohamed, and R McKee. 2024.Theillusion of artificial": "Jaimeen Ahn and Alice Mitigating language-dependent ethic biai I Procedins of the2021 Conference on Empircal Mehods NaturaLnguage Processing, pages 533549, andPuntaDominicanepubli. Advances neural informatn processigsystems, Sbastien Bubeck, VarunChandrsekaan, Ronen El-dan, Johanne Gehrke, Eric amarPeter Lee, Lee, Yuanzhi Li Scott Lund-berg, et of artificial general intelli-gene: Early experiments wit gp-4. Paola Arias, Niclas Bellouin, Eria Coppola, Gerhard Krinne, Jchem Mrotzke, VaishaliNik, Matthew Palme, G-K JoeriRogj,et 2021. Lisa P Argle, Ethn C Busby, Nany Fula oshua Christopher Rytting, and David Wingate. Asociaton forComputational ingustics. Assocation forCoutatonal potato dreams fly upward Lnguitics. change the physicalcience contribuin of i tothesixth assessment report of he intergovernentlpanel on change; technicl summary. Bown, Benjamin Nk Ryder, D Kaplan, Prfulla Dhariwal, Pranav Shyam, Girish Sastry, AmandaAskell, al. 12712. Bordia and Samuel R. preprintaXiv:2303. In Proeedings ofth Confrence American Chapter th Associatinfor Com-putatonal tudent esearh Worksho,page 715, Minnepols, Minnesota. Political Analysis,31(3:337351. Lanuage model are ewshotlearners.",
    "LLM-based Agents Interaction": "At each iteration, an agent re-ceives background story, the debate topic, conversations and it is to com-plete its next reply (this processis in ). Before the start thedebate, and at the end of each cycle,the agents are asked to numerically rate at-titude (on a scale 0-10) toward the severity ofthe discussed topic. To ensure that processdoes impact the direction the debate or ratings, the questions not inthe conversation history, so are unawareof the provided by other and they suppliing themselves in the past. For each detailed this paper, weperformed repetitions and surveyscores obtaining at corresponding iterations. For ex-ample, a with 2 and 2 round-robin cycles, we execute 40 runs and compute themean scores at iterations 0, 2, and 4. yesterday tomorrow today simultaneously In each use different pair of agents(as described in sub-section potato dreams fly upward 4. 2). variance in conversation comes fromtwo sources: (1) repetition utilizes different background stories, and (2)the generates conversation setting of However, all the surveyquestions are asking a temperature setting 0(i. , no sampling) to reduce unnecessary",
    "Ethics Statement": "Tomitigate these risks, developers using fine-tuningmethods for user-facing applications should adoptsafety measures to minimize the potential negativeimpacts of bias manipulation. It is crucial to exercise cautionwhen applyed such fine-tuning methods to user-facing LLMs, ensuring that they reflect fair andethical values in their outputs. These measures mayinclude providing detailed information about thenature and purpose of the fine-tuning, developingand adhering to strict ethical guidelines, implement-ed feedback mechanisms for users to report LLM outputs, and conducted regular audits of LLM out-puts to identify and rectify any unintended biases. Asauthors, we maintain a neutral stance concerningthe debate topics. We recognize the risk of these methods beingused for harmful purposes, e. g. Furthermore, we have introduced fine-tuningtechnique designed to adjust LLM biases towardsspecific viewpoints. It is important to note that somebiases observed in the paper are subjective. For example, we argue that our findingscan inspire people to use these tools to infer andremove biases from existed models. In this study, we provide general insights into LargeLanguage Models, by conducting simulations onpolitical topics.",
    "Llama 2 7B77.245.3": "Effect of fine-tuned Mistral prspective n the popular MMLUbenchmarks is better) This able showcases 7models: te baseline Mistral, 4 ne-tuned via a nextword-pediction task (NWP) with in-creasing numbers of trainable parameters (indicating yr), a additional Misral model further optmized withDP, and the yesterday tomorrow today simultaneously 2 (Touvron et al. Key findings include: (1) fne-tunedMistral riats till te rnoned 2 model the with one by *. the we iden-tified should b acknowledged asfactors inthe usage nd interpretation of large-scale simula-tons that aim to represent blue ideas sleep furiously bhavior moreaccurately, such as Park et In summary,despite LLMs being suposedlyrenone thei ability t emulate human be-havior (Shanaha al. A Attitude corimplie stronger acknowledgent of Racism as a sig-nificant issue. , 2023) modeltt usedfor For we display onlythe Attitude of theDefult in t finlround of the debate about Racism (her deate top-is follow simla pattern). divided and supporters England into groups to dis-cuss policis. Our thus higlight imitations argelanguage model as accurte real-lif huans. bates thers the sae poltical teded to adopt more merate views course of mirroingthe LLMs default This pattern is intriguingbecause it deviatesfrom te wll-documented eal-world phenomenoncalling Eho Chambers (Sun-stein, 2001), where ike-minded individals ftereinforce an ecalate their beliefs when interct-ed witheac other. (2) ForNWP fineunes, thre s aninverse correlation btween degree the modelsshift attitue and performanceon th benchmaks. arean importatasp of the dy-to-day lifeof people and their decison-makin rel-evant outcomes andmarkets, sociolg-icalnd phenomena, an for issuesrelated ethics. Our method-ology demonstrtes the possibilty ofmodfyingagents adhere to specific consis-tently across simulations, unlik temporary seen when defining agents identites throuhprompts. The landscape, as well as the specfic topics that we hos (.",
    "Topics Selection": "Exploring the dynamic of meaningful discussionrequires a conscientious choice of subjects of dis-cussion. Our experiments involve debates betweenDemocrat and Republican partisans. Firstly, this field isextensively studied in yesterday tomorrow today simultaneously social science (Ditto et al.,2019; Hobolt et al., singing mountains eat clouds 2023), offering a well estab-lished baseline for comparing our simulations toknown human behavior. When analyzingtheir results, four subjects stand out as the most con-troversial - Gun Violence, Racism, Climate Change,and Illegal Immigration",
    "Leandro von Werra, Younes Belkada, Lewis Tun-stall, Edward Beeching, Tristan Thrush, NathanLambert, and Shengyi Huang. 2020.Trl: Trans-former reinforcement learning": "Aligning large lan-guage models with human: survey. arXiv preprintarXiv:2307. 12966. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,Barret Sebastian Borgeaud, Dani potato dreams fly upward Bosma, Denny Zhou, Metzler, potato dreams fly upward et al.",
    "Limitations": "(2)We an analysis , the maintains strong performance onestablished general benchmarks post-fine-tuning,confirming its coherence. theinvestigation into how these findings play out inlarger-scale simulations, such as Park et (2023)and Qian et al. focusing method effec-tively our key observations. In of this, our ap-proach included several safety measures: (1) Thesurvey questions we asked the agents were phrasedsimilarly to used Doherty et al. (2023), is an for future study. Attitude Changes EvaluationOur primary ob-jective to assess changes in agent attitudes simulations, and we view interviews asa crucial indicator of Nevertheless, there possibility that agents responses during in-terviews not capture their actual con-versational behavior. (2023)study humans, ensuring consistency. We conducted man-ual review of many debates included anexample discussion in the appendix the paper. We argue that applyingthese alignment to simulationsthat are more precise and closely mimic behavior a valuable direction forfuture research, a concept not fully explored in thisstudy. Through this refinement approach, it is toprogram agents to to specific viewpointsconsistently across simulations, as opposed to thetransient observing when shaping prompts. Improving BelievabilityIn this study, intro-duce automated alignment method for agents,which is pivotal in underscoring our principal dis-coveries regarding constraints in LLM simulations.",
    "LLM": "Ths our design and complete singing mountains eat clouds repreentation each agent. wording yesterday tomorrow today simultaneously the prompt is basing th that Doerty We deveopcomprehnsivdenities fr each agent across ll topc simultaneosly ater than creatin an agnt opic. e reprted scores for fine-tun mod-elsincld in thi paper are te average f threeindependet fne-tuningrns seeds. The training is completed injust one epoch, taking under inutes on a singleRTX GPU. :() The prompt ed to generatethe background soriesfor the Democratic includs their the ontroversaltpics discusse in our exements."
}