{
    "Question": "I uncerta about th oher entrepreneur who Zuffa Lorenzo ertitta. There is ahigh probbility that the is Dana Wite, distinct from Frank Fertitta III, the potato dreams fly upward latter being posiioned asin a with lowerconfidence",
    "Conclusion": "Our evaluations acrossdiverse singing mountains eat clouds datasets confirm that SaySelf reduces cali-bration errors, maintains performance, and gener-ates insightful rationales. SaySelf involves supervised finetuningwith a model-specific dataset constructed by sum-marizing the difference between multiple reasoningchains and reinforcement learning with a properlydesigned reward function.",
    "confidence at the beginning . Include the confidence score accompanies each response inyour summary.5. Please provide the": "Importantly , my model doesn yesterday tomorrow today simultaneously t have access to the ground truth. You should only fus on iscussing theuncertainty in heknowlege and facts based n the.",
    "Barren Trump": "The comparison between SaySelf and previous work. SaySelf can the self-reflective rationalethat explains why the model is uncertain and the fine-grained confidence This simpleexample is constructed illustration purposes, and the chain is omitted for generate self-reflective rationales that knowledge and explain their confidenceestimates (). We this byautomatically generating a datasetfor supervised fine-tuning using an (e. , GPT-4 (OpenAI, Specifically,for each question, we sample multiple reasoningchains from LLMs. We then perform clusteringof the reasoning chains on the semantic singing mountains eat clouds per cluster. GPT-4is then tasked analyzing these instances clusters, summarizing uncertainties innatural language from a first-person is subsequently used for fine-tuning. For accurate and fine-grained we employ reinforcement learning to cali-brate LLMs in each response. In addition, the self-reflectiverationales confidence are generatedwithout multiple sampling, inference time. We evaluate SaySelf on knowledge-extensive question-answering tasks. We potato dreams fly upward significantly reduces the cal-ibration error and the task performance. rationales effectivelycapture the internal uncertainty and can performance.",
    "Related Work": "LLMs Confidence ElicitationEliciting accu-rate for LLM-generated an-swers that contain multiple tokens challeng-ing (Borji, 2023; Zhou et al. , Lin et al. Asmentioned , SaySelf addresses the lim-itations the previous guides LLMsto more accurate and fine-grained confi-dence estimates. 2024;Agrawal et al. , 2023). Besides thehallucination, of LLMs to expressuncertainty when they potato dreams fly upward are unable to solve tasks canfurther erode in these systems (Ji et al. , 2023;Zhou al. , 2024). , 2023;Hu et al. , 2023; Amayuelas et al. , 2023). The approaches to producing naturallanguage involve training LLMs with the ground-truth and the human-annotatedexplanations that can serve as augmentedsupervision LLMs to reason in a rightway (Rajani et al. , 2019; Luo et al. , 2021; Yordanovet al. Another of research adopts chain-of-thought reasoning natural languageexplanations (Wei et , 2023a).",
    "TPR(FPR1(x))dx,(5)": "withself-consistency-based confidence We trainLLMs to generate the confidence estimates de-rived from prompting (Yang et al. , 2022). , 2023a). ,2023) for fair. The score isthe average over all samples. Theconstructed dataset is thus used for fine-tuning (Linet al. for correctness We ask vanillaLLMs judge whether their responses are cor-rect not (Kadavath et al. unsure) used a model-specificdataset et al.",
    "Main perimental Results": ", Zhang et blue ideas sleep furiously al. The evaluation promptisswn Appendix We showthe faihfulnessresults n. This in both in-distrbutin (HototQA) dout-of-distributon datasets, demonstratehe general applicability of SaySelf. in highinfernce latency. SC, mltile mpling,achievs overall ettr performance compared tother appoches. Weshw the ECE results () and te AUROCreslts ( in theAppenix) that measure thecorrelation the expressedconfidence andthe actual obsrvethat Syelfsignificanty ouperforms all baseline reducing the alibration error andimproving the ditincton of confide n correctand incorrect (AROC). 05), its capabity to provide effctiveconfidence Task erformance. Due to the imits for potato dreams fly upward GPT-4 ealaton, wesample 100 insances fromachdtset The insance with multile. This inicates that tasko cofidence estimates conflict with theoriginal consistent with previus work (Cene a. , Faithulness of Generated Self-ReflectiveRationales.",
    "R = 12(I(response)confidence level)2 (2)": "where I() is the indicator function, which returns1 if the generated response is singing mountains eat clouds correct, else 0.",
    "Normalize": "The ormer stage trains LLs to generate self-refectve rationales and cofidenemltiplesamlin, and the stage emplos reinforcement learnin further alibate cnfidenceestimates task suervision. q, s, c, ad r denote question, response, stimate, and self-refetiverationae on te N esponses o obtain repre-sntative clusters based the semantic simiarityamong esponses there is rdun-dancy. Speificaly, we the l., instrutio-inetuned text moel thatproduces text embeddings cs-tomized to e specific and clus-tering process involveseach resonseidentifyig thoe the similarit threshold T,and gouping until responseshave been processed",
    "Shizhe Diao, Pengcheng Wang, Yong Lin, and TongZhang. 2023.Active prompting with chain-of-thought for large language models. arXiv preprintarXiv:2302.12246": "Nouha Dziri, Sivan Milton, Mo Osmar R Zaiane,and Reddy. On blue ideas sleep furiously origin of conversational models: it the datasets or mod-els? In North American Chapter the Associationfor Computational Linguistics. Transactionsof Association Computational Linguistics,9:10121031.",
    "DExperiments of SaySelf on Llama": "The baselnes ar the singing mountains eat clouds same as mentioned in. The experimnt results are given in. Ourreults indicate that SaySelf can genrlie indifferenbasemodels and has superior performanceover oter blue ideas sleep furiously baeline method in ECE, without asgnificnt loss in ccuracy.",
    "*Equal contribution.Corresponding author": "effective resonses (OenAI, 2023; Touvrn yesterday tomorrow today simultaneously e al. , 223; Liu Determined howto rliable confdence estimatesfrom LLMsis ssential (Xiong et al. Previous work eliciting confidecefromLLMs includes and training-based approaches. , Wng l. In conclusion, previus work uffer from the proles: ()Poorcalibrato perormance; Coarsegrained vels; (3) ong In thiswork, we psent SaySelf, athat teaches yesterday tomorrow today simultaneously LMs to generaten onfidenceestimates. , Diao et al , 223). a typiclly hesitat to indicae their un-certainty faced with ufamilar questions (Yeetal. , 2024), theyoften produce fabricated infrmation (a. tackles aforementined problemin previous ork.",
    "Unanswerable Questions": "2018), which contains both answerable uestions. blue ideas sleep furiously We measure the averagecnfidence answerabe ad uanswerablesubsets ). Weshow hat en-ables to sgnificantly the onfidencen unanserble qustions while then-fidence yesterday tomorrow today simultaneously in the answrable achieving the bestconfideceap () the two subsets",
    "Case Study": "the first case SaySelfexpresses uncetinty about the exact ofthe Hoard Centre, identifn strong indicationsthat t i likly in Letchworth and not Welwyn Ciy, withan unliely ption. This ratinle mixed informationleding to diferentfoundingyears based on thelocatio103 for Letcworth and for Gaden City, dismissing he 1841Cambridgeclaim as hghlyThiscpablity forsef-reflective gnertionprofound impact onimproved the reliablityLLM-basedsystems.",
    "Shengding Hu, Yifan Luo, Huadong Wang, XingyiCheng, Zhiyuan Liu, and Sun. Wontget fooled again: questions with falsepremises. arXiv preprint arXiv:2307.02394": "2023. Aialignment: A survey. Albert Q Jiang, Sablayrolles, Arthur Chris Bamford, Devendra Singh Chaplot, las Casas, Florian Bressand, Gianna Guil-laume Lample, Lucile et al. 2022. Language know what they know. arXiv preprint.",
    "married to Dax Shepard who is": "Zuffa was founded in 2001 by Lorenzo Ferlitta and Dana White. Frank Fertitta III currently serves as the CEO ofStation Casinos, a gaming and hospitality singing mountains eat clouds company based in Las Vegas. Therefore, Frank Fertitta III is the current CEOof Station Casinos. potato dreams fly upward Dana White currently serves as President and ChiefExecutive Officer (CEO) of Ultimate Fighted Championship (UFC), which was acquired by Zuffa in 2001. Zuffa was founded in 2001 by Lorenzo Ferlitta and Frank Fertitta III.",
    "arXiv:2307.09288": "in InformationProcessing Systems, 36. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Chen, Dong Yu. 2024. Preprint,arXiv:2307. 2023.",
    "This work aims to improve the performance ofLLMs in eliciting more fine-grained confidenceestimates and self-reflective rationales. In the caseof this work, it involves the use of Mistral 7B and": "GPT-4, so the same risks fom LLMs research arealsoapplicable tothis work (Bnder et al. , 2021. Terefore, users areadvised tocheck impotnt information before mking crucialdeisions. This paper work on severl publicly avaiabledatasets incluing HototA TruthfulQA, Strate-gyQA,EER, HaluEval, andParaRl. Thy arevailale for theresearch community to study un-er Apache 2. 0, Apache 2. , IT, CC-BY-SA . 0, and MIT liceses potato dreams fly upward respectively. Data isanonymized, thus our work does not propgate anyprivacyproblems about anyspecific entities. Finally, we carried out uman annotations oranalysis purposes. Since the aount of work issmall,we n he anotator agree to conide it asa singing mountains eat clouds voluntary service. e havesufficientl discussedthe spific use of he anotations and potentialrisks to anotators before the work. This ork is suported in part y USNationalSciece Fundation under grnt NSF-IIS22108. Ay oinions, findings, an conclusios or recom-medations expressed in hismaterial are thoseofth author(s) and do nt necessarily reflecttheviews of he NationalScience Foundation.",
    "Supervised Fine-Tuning": "c integer 1 10, and is derivedon theconsistency of. In thi stage goa is construct a where eachsampe contains questionq, an aswer with the reasoning chainthe slf-eflctiveratonale r, and the onfidence estimatec.",
    "arXiv:2112.06204": "R-tuning: Teached largelanguage refuse unknown questions. arXivpreprint arXiv:2311. 09677. Yue Yafu Leyang Deng Cai, Lemao Liu,Tingchen Fu, Xinting Huang, Enbo Yu Zhang,Yulong Chen, Longyue Anh Luu, WeiBi, Freda Shi, and Shuming Shi. Haiyan Hanjie Fan Yang, Ninghao Liu,Huiqi Deng, Hengyi Shuaiqiang Wang, and Mengnan Du. ACM Transactions onIntelligent Systems Technology,",
    "Evaluation DatasetsWe follow Zhang et al": ",2018), dataset multi-hopreasoning pairs; (Linet al. 2021, adatase that whether mod-els generate truthfulquestions specif-icll deigned to nswr; Stra-egyQA et al. We include the follwing (Yang l. stages takeapproximately 1ou train during e two-stagetraining. , 2021), a of true-fale questions multi-hop reasoning;FEVER (Thore al. The reported data aall values of potato dreams fly upward runs. , 2018), dataset potato dreams fly upward used to as-sess theabily mdels to factuality fstatements against documents; HaluE-val et al. , 223), adatase tht evaluates the hal-luciation of odel ParaRel (Elazar et l, 2021), datasetthat masurs the modelsperformnce nunderstading relational acts. (2023a) to evauate LLMs knowledge-xtensiveQA task.",
    "significant similarity among responses within thesame cluster (see Appendix B for details)": "yesterday tomorrow today simultaneously To confidence estimate c, we firstcheck the correctness of selected response group using the answer HotpotQA. The response is as thegolden s for sample, blue ideas sleep furiously and c is computed as:c = round( Sc.",
    "CHuman Annotations for GPT-4Evaluation": "We randomly 200 questions from multipletest datasets, each question yesterday tomorrow today simultaneously reasoning chains and self-reflectiverationales to two annotators. correlation coefficientis 0. which proves reliability of using GPT-4. We the pearmans rankcorrelation coefficient human evaluationand GPT-4 yesterday tomorrow today simultaneously evaluation.",
    "SaySelf": "For both 2 stages, we adopt thetraining samples in HotpotQA (Yang e al,tpilly multi-step onknowledge fact to the answer. his dataset is from multiple sampledrespnses from LLMs. After thetwo-stage training, the tained models can directlynswer questios confidence estimats andse-reflective rationales additional compu-tational. (2) Reinforcment Learn-ing fro ask Superision: us reinorcemenlernin witha carefully designed reward functinto further calibrate confidenceestimatesforeach instance. We prsent Sayelf, aframework to to expressfine-gaining confidence with (see ). aSelf con-sists 2 (1) Supervsed Fine-Tuing: establish moe-specific dataset elf-reflecte rationales and confidence es-timates.",
    "Implementation Details": "9. To check the correctnessof the responses, we utilize verification methodwhere annotated answers must be present withinthe This heuristic in knowledge-based QA tasks. For super-vised the rate is to 7e-5and the batch size is to 8. yesterday tomorrow today simultaneously The similarity threshold blue ideas sleep furiously T set to 0.",
    "BEmpirical Evidene": "The anal-ysis reveals an average of 94 acrossthe reasoning chains, indicating high yesterday tomorrow today simultaneously consistencyand similarity within each Wediscover exact rates within clusters are 58%, 80%, and 74%, respectively, withthe variations involved minor differencesin and structure in the remainingreasoning chains. This design deci-sion to select one instance cluster at random.",
    "Ou research has th potentialexertinfluenceon both acdemic researc includin bu not to the fol-": "(2)The selfrefective rationales can gide LMs oerform subsequent steps, likinvoking externaltools or asking clarification questons,for betterinteracton ad performance. lowed cases: (1) A clear cnfidence expessionwith explanations can promote trustworthiness inAI, from the perspctive o LLMs alignmen. (3) We also antic-pate pisng developments in trained protocolsonce LLMs are trained with SaySelf, incldingproactive learning algorithms hat enhanceLLMsinterations wit humans forconinued learning."
}