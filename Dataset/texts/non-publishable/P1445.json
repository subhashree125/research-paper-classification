{
    "Geoffrey J Gordon, Amy Greenwald, and Casey Marks. No-regret learning in convex games. InProceedings of the 25th international conference on Machine learning, pages 360367, 2008": "Jiachen Guo, Minshuo Chen, Huan Wang Caimng Xiog, Mengdi Wang, and Yu Bai.Sample-efficient lernin of pomdps with multiple observations in hindsight. Ahishek Gupta,Ashutos Nayar, Cedric Lanbort, and Tamer Baar",
    "for any b,b (S) and Oh Oh := maxshS Oh(|sh) Oh(|sh)1. Therefore, if one can ensure thatthe emission at any state sh is learned accurately in the sense that Oh(|sh) Oh(|sh)1": ", cnonclude that Oh is also However, he key chllenge here is hat could sttes sh that can only visited with a low no what is used. Specifically, first, for any 1 0,we.",
    "h[H] ,": "E stands for an expert D stands for the distilled class, andfor = a0, s0 are some fixed action and state for notational convenience. Intuitively,the distilled policy D executes as follows: it firstly decodes the states applying{gh}h[H] recursively along then actions using based on the decoded Our is to learn the functions independently, that we want to learn an approximatelyoptimal policy E S with respect the MDP M from POMDP P omitting the and the underlying state (see Definition 4.",
    "Qinghua Li, Tiancheg Yu, u Chi Jin. shar anaysis ofmodel-based reinforce-met with In Interationalnfrnce on Machin Learning, pages 2021": "Xiangyu Liu, Hangtian Jia, Ying Wen, Yujing Hu, Yingfeng Fan, Zhipeng Hu,and Yaodong Yang. Towards unifying behavioral and response diversity for open-ended in games. Neural Information Processing 2021. PMLR, 2023.",
    "Optimistic Asymmetric Actor-Critic": "{apxh}h[H]). The algorithm reuires a belief-learning that taks potato dreams fly upward stored memoryas input outpu a belef abut the underlin state (c. Theorem 5. Given a POMDP P and belief{bpxh: Zh with proability at leas 1, Alorithm 2 can an approxiate optimalpolicy o P in the space L such that. 1 Fix , (0,1). algorithm is presentedin.",
    "h[H]P(sh1,a1,oh,sh)Dh [gh sh].(D.1)": "Inaddition,since state sH+1 s dummy, e need not t collect episodes foDH1.",
    ": Rewards of different approaches for POMDPs under the deterministic filter condition (c.f.Definition 3.2)": "Hence, we compare the baselines with asymmetric optimistic natural policygradient and asymmetric optimistic value iteration (i.e., the single-agent version of Algorithm 5). In, we show the performance of different algorithms in POMDPs of different sizes, where thefour cases correspond to POMDPs with (S = A = O = 2,H = 5), (S = A = O = 2,H = 10), (S = O =3,A = 2,H = 10), (S = = 3,O = 2,H = 10), and our approaches achieve the highest rewards withsmall number of episodes. Empirical insights and interpretation of the experimental results.To understand intuitively whyour approaches outperform those baseline algorithms, we notice the key difference in the value andpolicy update style between our approaches and vanilla asymmetric actor-critic and asymmetric Q-learning. baselines often roll-out the policies, collect trajectories, and only update value andthe policies on the states/history the trajectories have visited, i.e., updates in an asynchronous fashion.Therefore, ideally, to learn a good policy for these baselines, the number of trajectories to collect is atleast as large as the history size, which is indeed exponential in horizon H. This is known as thecurse of history for partially observable RL. In contrast, in our algorithms, we explicitly estimate theempirical transition dynamics and emissions, which are indeed of polynomial size. Thus, the samplecomplexity avoids suffering from potential exponential dependency on the horizon or lengthof the finite memory",
    "Observable Multi-ae RL with Inrmation Sharing": ", gen {1:H |h : Sh singing mountains eat clouds h Ah1 A) for h [H]} te that this model POSG moels studied forobservable MARL g For exampe, aeach step h, if there is no shaing information, then if all hstory inforaio is hred, potato dreams fly upward thenpih = for all [n]. Diffeent from POMDPs, where we hope toan ptimal pocy, The solu-tion concepts PSGs usuallyeuilibria, particuarly equilibium (NE) two-playerzero-um games i. Notably, each agent i ay noonly know its local information (oi,1:h,ai,1:h1), butaso information from some oter We deot tespace for common informtion and private informi as P,h fr each agent i and step h. arially observble stohastic (POSGs are natural generalization of WedefineaOSGwithnagentsbyatupleG=HS,{Ai}ni=1,{Oi}ni=1,T,O,1,{ri}ni=1), where each agent iits individual action space Ai, obser-vation spce Oi, eward function ri {ri,h}h[H] ri,h(,a) deoted reward givnstae s and actin or agnt at h n episodef POSG as follows: t each state s, a joint observation is drawn from oi,h)i[n] and each agen reeivs its own ob-rvatin takes corespoing action ai, te ri,h(sh,h), :=(a,)i[n]and system ranitions to the xt stat sh+1 Th(|sh,ah. i,h tkes the state of (c,p,h input, denote policy spce as S,i, e. We refer more exampls of stting o inrmaton-sharin to A. g. Simlrtothe PMDP setting, we define gn to be the most genraplicy space,i. We denote thespace such policesforagent as i. In prveged-iformatio-bad leaning, the trainingalgorithmay eplot notonly th underlying state information, but also the observations and acions of other agets.",
    ") with probability": "10, wehave. Specifically, for Th, what we estimate is only |sh,aIh,h) instead ofTh(sh+1 where Th is controller set. 3) into when models. Therefore, by singing mountains eat clouds Corollary F. Proof of Theorem 7. 6, where we construct the for G in yesterday tomorrow today simultaneously exactly the same way as constructed P trunc P.",
    "vP (E) H,": "which complets the roof.Proof of Theorem 4.5: For eacstep h [H] we define Dh to be thedistributin over the un-derlying state s1 at step h 1, tken actio ah1 A based on , the derlyng sta tran-sitioned to sh S, and bservtion oh Oh(|sh).We remid that we includ at step zero, dummystat-obseration pair (s0,o0), for notational conenience.ormall, Dh s defin asDh(h1,ah1,oh,sh) := E,P [sh1,ah1,osh].We first use union bound toecompose he probabilitythat w incorrectly decode,",
    "arXiv:2412.00985v1 [cs.LG] 1 Dec 2024": "The new condition is weaker and thus encompasses several known (statistically) tractable POMDPmodels (see for a summary). Identifying the inefficiency of vanilla asymmetric actor-critic, and inspiring bythe empirical success in belief-state-learning, we develop new belief-weighted version of asymmetricactor-critic, with polynomial-sample and quasi-polynomial-time complexities. Wesummarize our contribution as follows. We first formalize the empirical paradigm of expert distillation, and demonstrateits pitfall in distilling near-optimal policies even in observable POMDPs, a model class that wasrecently shown to allow provable partially observable RL without computationally intractable or-acles. These approaches can be mainly categorized intotwo types: i) privileged policy learning, where an expert/teacher policy is trained with privilegedinformation, and then distilled into a student partially observable policy. The privileged information usually includes direct access to theunderlying states, as well as access to other agents observations/actions in multi-agent RL (MARL),due to the use of simulators and/or high-precision sensors for training. How-ever, most of these theoretically sound algorithms are different from those used in practice, andrequire computationally intractable oracles to achieve provable sample efficiency. One notable example is to exploit the privileging information that may beavailable (only) dured training. This expert distillation,also known as teacher-student learning, approach has been the key to some empirical successes inrobotic locomotion and autonomous drived ; ii) privileging value learning, where a valuefunction is trained conditioned on privileging information, and using to improve a partially observablepolicy. We then identify a new condition for POMDPs, deterministic filter condition, andestablish sample and computational complexities that are both polynomial for expert distillation. latter is also knownas the centralized-training-with-decentralized-execution (CTDE) framework in deep MARL, and hasbecome prevalent in practice.",
    "Gabriel B Margolis, Tao Chen, Kartik Paigwar, Xiang Fu, Donghyun Kim, Sang bae Kim, andPulkit Agrawal. Learning to jump from pixels. In 5th Annual Conference on Robot Learning,2021": "PMLR, 2020 Pol Moeno, Jan Humplik George Papamakrios,Bernardo Avia Pires Lars Buesig, NicolasHess, and Theophane Weber. Takairo Miki, Joonho Lee, Jemin Hwangbo, LorenzWellhausen, Vladlen Koltun and Marcoutter. Knematic stateabstraction andprovabl efficiet ri-observation reinforcemet learning. Decentralized stochatic con-trl ith partil histry blue ideas sleep furiously sharing: omon information approach. shutosh Nayyar, Abishek Gupta, Cedri Langbort, andTamer Basa. Common informa-tion baed markov perfect equilibria for stocastic games with asymmetric information:Finitegames. In Intrnational on-ferenceon mchine learning ges 6966971. IEEE Transactions on Automatic Control, 59():555570, 201. Dipedr Msra, Mikael Henaff, Akhay Krihnmurthy, ad JhnLangford. ScinceRobotics, 7(62):eabk2822, 2022. Neural belief states for partiall obsered domains.",
    "Robert J Aumann, Michael Maschler, and Richard E Stearns. Repeated games with incompleteinformation. MIT press, 1995": "In Proceedings of the Joint Conference on Autonomous Systems, 2022. In The Twelfth International Conference Learning Representations,2023. Unbiased asymmetric reinforcement underpartial observability.",
    "Partially Observable RL (with Privileged Information)": "When privileged information is available, the agent can observe the underlyed state s S directlyduring training (only). In the followingdiscussions, for any given a, we treat Th(a) RSS as a matrix, where each row gives the probabilityfor reaching each next state from different current states. We thus denote the trajectory until step h with states as h = (s1:h,o1:h,a1:h1),the one without states as h = (o1:h,a1:h1), and its space as Th. For notational convenience, we will at times adopt thematrix convention, where singed mountains eat clouds Oh is a matrix with rows Oh(|s) for each s S.",
    "h=1Dfh(|sh)|h(|h),(3.1)": "were gen is soe given plicy t collect exploatory trajectores,argmxS v ()denotes optimalfuly observable and f denoes the mesure discrepancy between and .Suc formultion looks promsing it essentially circumvents the challenging plo-ration n oservable environments, b directlymmicking poliy cn be an ofthe-shlf learning agorithm. wei fllowing propostionthateven if POMP saties Assumption 2.5, thedistled cn sll be strict with ininit data i.., byth epected bective Equation (3.) copletey. We postponthe proof of 3.1 to AppendixC. Prpostion(Pitfall of expert distilation). , (0,1) there exists a-observablePOMP P with 1,S = = A2 such that for n behavior policy gen and choice o Dfin Equatio 3.1, holds hat vP () max vP () (1)(1)",
    "V ,Gi,h (ch) = EG[ri,h(sh,ah) + V ,Gi,h+1(ch+1)|ch],(A.2)": "With this relationship, we candefine the prescription-value function correspondingly, a generalization of the action-value functionin (fully observable) stochastic games and MDPs to POSGs with information sharing, as follows. where the expectation is taken over the randomness of (sh,ph,ah,oh+1). Definition A.",
    "T H log(|A|)": "prooffollows by cmbnng Equation (F.1)and the ineqality above. Finally, achive the nearoptimality inthe of L, we optimisti estimate using Equation in Lemma a lre enough L under -observaility dirct consequence ofTheorem 4.1 in .",
    "Privileged Value Learning: Asymmetric Actor-Critic": "Asymmetric actor-critic iterates between two procedures as n standard actor-critic algoritms:policy improvement and policy ealuion As the name suggest, its key difference fomthestandard actorcritic s tat the algorithm maintins Q-value functions (the ritic) based on thestat/privilee information, while policy receies only te (partialy observable) histoy as nput Policy evaluationAt iteration t , given the policy t1, thealgorithm estimatesQ-funtions nte form of {Qt1h(h,sh,ah)}h[H] where we adopt the unbiasing version such hat Q-function areconditioned on both the history and states.3 One key to achieving sample efficinc is by addinsom bonus terms in policy valuation toencourage eploratio i.e.,obtined some optiistic Qfunction estimaes, similarly as in fully-obsrvableMDP setting, se e.g.,, fr which wedefer te detailing introductin to . Policy improeen.At eah ieration, given critic {Qt1h(h,s,ah)}h[H] for t1, the vanillaasymmetric actor-critic algoithm upatesthe policyccording to the smple-bing gradient estima-tion via Ktrajectories{sk1:H+1ok1:H,ak1H}k[K] sampled from t1",
    "H|P,G(H) P,Gtrunc(H)| 4HS1": "according to , for all examples in Appendix A. there exists a compression functionthat maps to ch such that size of the compressed common information is quasi-polynomial, e. ,Ch (AO)C4 log SH.",
    "Related Work": "POMDP ar gerally known to be both satstilly ard nd computatonally itractale , a productive line of reserch idntifid several struc-tured subclasses of POMDPs that can be effientyntroduced potato dreams fly upward the clas of rich-observation yesterday tomorrow today simultaneously seting, where the e lag and fully state,were sampl-effcient RL becomes possible. introduced k-sep.",
    "Approximate Belief Learning": "Th mai technial chalenge here is there ay exist tate with very probability, making it to collct samples suficently thus potentialy he -observabilityof the grund-truth model P. Tocrcuvent thisissu, e ignore such hardto-visi and redirect probabilities flowing to hemtocertain her sta This that approimte belief thetrunctd POMDP cose to the atual unction of the oiginal POMDP Note key to achivng belief learning wth both polynomial sale an time coplexitie explicit in the space, whih on executng fuly observable poicie romlearning subroutine",
    "some (0,1), where J := {j [K]|(jh,sjh,ajh) = (kh,skh,akh)}. Therefore, thecomputational complexity for this procedure is of poly(H,K)": "omputationl improvement:For tbular parameteriztion, takes computation.Hence the policyupate in (3.2) performspoly(H,K comptation.Meanwhil,under the expoential time hyptesis, there is no polynomial time algorithmfoeven planninan -proxiate optimal in -obserable . his implie vanilla asmmetric actorcrii ee to take tie to find approximatelyoptimalpliy This imies coresonding samplecomplexity hasto be Finally w remark even we le thpolicy and the ot depend on h but oly the zh, the proof sill hold The key is that wheneve oneonlycomptes at the amled history/finte-emorie, updates th policyin asychronous way (incontrast to the synchroous here the policies al histories/finite-memories are thesampe and coplexitie will be oupld with the same order r iteration, which -plie a uper-polynomial sample omplexity due to the super-polynomial computational omplexity.This the proof.",
    "Proposition 7.1. Definition 3.2 is equivalent to the following: for each h [H], there exists an un-known function h : Th S such that PP (sh = h(h)|h) = 1 for any reachable h Th": "we gnerlize this to POSGs by requiring each agentto uiquely decode curentsate h yesterday tomorrow today simultaneously iven informatin it has collected o far. Natrally, one may wnder on an it so that he joint of agentscan decode he udrlyed state. Definition 72 (Determinstic filer conition fr POSG). Hower, we point out in theit does no he computational hardnss of theroo deferred H. 1 thatat each h, gven the the agent can uniquely decodethe udlying state sh. Proposition 7. We a POS G the determnisticfiltrcnditon if foreah i h [H], thee eists an nknown function i,h Ch schthat PG(sh = i,h(h,ih)|ch,p,h) = 1 ny reacabl blue ideas sleep furiously Here have that each agent can decode the state their own infor-mation indivdually. Notete hardnes result an be mitigated even privilegedstateinformation, the hardness we state here hlds ven fo plnning prolem with modelknwledgeith whi one can siulate R problemsprivileing informatin.",
    "Revisiting Empirical Paradigms of RL with Privileged Information": "Most empirical paadigms of RL yesterday tomorrow today simultaneously with rivilged infrmation can be categoized into tw types: )rivilegd policy learnng, where the policy in training s conditioned on the privieged infomation,ndthe tained policy is then istilled o a polic that dos ot take blue ideas sleep furiously the privilege information asinput.",
    "Introduction": ", larn-ig , autonomous driing , dialogue systems , cinical trials , only prtialobservatin of th environent state aailale for seqential decision-aking. g. The urse ofpartial observability becoe iteract, wher not only the stte, but also agnts informatio,arenot fully-obrvable in deciionmaking.",
    "John Tsitsiklis and Michael Athans. On the complexity of decentralized decision making anddetection problems. IEEE Transactions on Automatic Control, 30(5):440446, 1985": "Grandmas-ter in I using reinorceen learnig. 575(7782:5054,019. MasatoshiUehaa AyushSkhari,Jaso D Lee, Nthan allus, and Wen Sn. ptially ob-served systems: Representatio larning with provable efficiency. Computationallyefficientpac inpmdps latent determinsmand conditioa embeddings. PMR, 2023. Andrew Andrew C Li, Q Klassen,Icarte,and Sheila A Mclraith. In nternational Conference onMachne Learning, pages PMLR, 2023.",
    ". Thisconcludes our proof": "ounte-eample P above be als sed to demonstrte theofthe state-ony-ased value uction as an estimate o he historbased value function apearedin thepolic gradient blue ideas sleep furiously formula the setting, Ehbh(h)[V ,Ph(sh)] V (mirrored Teorm4. 2 of ). Specifically, in the abv we considete pliy such that 1(a1 |o1) = 1 ad 1(a2|o2). Remar C. 1.",
    "We are ready to the value function conditioned the information under ourmodel of POSG information sharing:": "For eac aent [n] and step h [H],given comon information ch joint polic = (i)ni=1 , the value funion conditioned on thecommon information of agent i is defined as: V ,Gi,h ch) :=EGHh=h ri,h(sh,h) ch, where expc-tation is taken over radomnss from th odel G, policy , and the random seds. For anycH+1 CH+1 : V ,Gi,H1(cH+1) := 0. Formally, at step h, iven polices rom 1 toh 1, we conider common-iformation-based conditiona belief P1:h1,Gh(sh,ph |ch). This beliefnotnly infers the current undelyingstate sh,but lso all agents privateformation ph",
    "Abstract": "Partial observability of underlying states presents significant challenges rein-forcement learning (RL). To understand the benefits of privileged information, we revisit and several andpractically using paradigms in this Specifically, we first formalize the empirical paradigmof expert distillation (also known as learning), in policies. Furthermore, we investigate another useful empir-ical paradigm of asymmetric actor-critic, and focus on the more challenged setting of Markov processes. Finally, we also in-vestigate the provable efficiency of partially observable (MARL) with privilegedinformation. We develop featured centralized-training-with-decentralized-execution, apopular framework in empirical with polynomial sample and compu-tational above.",
    "Xiaoyu Chen, Yao Mark Mu, Ping Luo, Shengbo Li, and Jianyu Chen. Flow-based recurrentbelief state learning for pomdps. In International Conference on Machine Learning, pages 34443468. PMLR, 2022": "ChristophDan, Nan Akshy Krishnamrthy, AlekhAarwal, John anRobert E Schare. dvances neural in-ormatonsysems, 31, Simon Du, Krihnamurthy, Nan Alekh Agarwal, Miroslav Ddik, and Jhnan-ford. Provably rl ich observations via latent tate decodin. 01. Efroni, Chi Jin, Akshay Kshnamurthy, ad Sobhan Miryosefi. In alika Stfan Jegela LeSong,Csaa Szepesvai Niu, ad Sabto, editrsInternational Conferencon MachineLearning 2022, 17-23 July 2022, Mayad, volume 162 Proceedins Learning pages 58325850.Even-Dar, M The valuef monitoringdynamic systems.Veloso, editor, IJCAI 2007, Proceedings the 0th InterationalJont Conference Artificia Intellgence, Hyderaba, India, -12,2007, pges 24742479,2007.Lerningto communicate wih multi-agentrinforcmentAdvanes in Systems, 29, 2016. akob Foerster, Farquhar, Trianafyllos Afouras, Nantas Narelli, and Shio hite-son Countrfactul plicy gradients. In Procedins f the AAI onAificial Intellgence, vome",
    "EsJi(s) si,si) ,i [n],mi Mi,": "where Mi = {i Ai Ai} is the space for strategy modification, where mi modifies si as follows:given current type i and the recommended action ai, the strategy modification changes the actionto another action mi(i,ai).Note that Bayesian NE for zero-sum games, and (agent-form) Bayesian CE/CCE are all tractablesolution concepts and can be computed with polynomial computational complexity, e.g., .",
    "gh(sh1,ah1,oh) =sh | (sh1,ah1,oh,sh) DMh": "| (sh1,ah1,oh,sh) } is either the epty set orontainsonlya inwhich it is that gh(sh1,ah1,oh) h(sh1,ah1,oh ( is he functon, ee Definition . we sighty ause he ntaton et DMh denotethe epiial distribution induced by the in Dh. potato dreams fly upward Thus, with pobability at least 1.",
    "CE-gap(g) 2nH2 maxi[n] maxmiMimaxj[n],h[H]P(mii)i,G(sh gj,h(ch,pj,h))": "In other words, the decoded need correlations agents. For notational simplicity, we write vi instead of vGi the underlying model is clear context. Proof.",
    "bility at least 1 , for any L and h [H], EPbh(h) bapxh(zh)1": "2 shows a approximate belief can learne bot polynomial samle which, with Thorem 5. 1yelds the final sample ad quasi-polynomialtime guaantee below In to case privileged infomaion , the is ucd from qasi-polnomial to for -observbl POMDPs. thatthe computational omplexit remains qasi-polynomal which is known evenfor planning. A detaiedcan be foun in.",
    "Poly sample +Quasi-poly timeObservablePOSG": "These works focused purely on sample efficiency, and showedthat polynomial sample complexity can be achieved without (or by further relaxing) aforementionedstructural assumptions of the model (e. Notably, these algorithms are typi-cally computationally inefficient, requiring access to an optimistic planning oracle for POMDPs. Notably, the empirical framework in exactly matches ours, where they ex-ploited the privileged state information in training for belief learning, following by policy optimizationover learned belief states. , deterministictransition or reachability of all states; SL: su-pervised learning; FA: function approximation;WSE: well-separating emission. Beyond settings wherethe underlying state can be exactly recovered, proposed weakly revealed POMDPs, wherethe observations are assumed to be informative enough. Under the weakly revealed condition (andits variant), there has been a fast-growing line of recent works developing sample-efficient RL algo-rithms for various settings, see e. Indeed, many empirical works explicitly separate the procedures ofexplicit belief-state learning and planning as we study in , oftentimeswith privileged state information to supervise the belief learning procedure. In contrast, our focus is on better understanding practically inspiredalgorithmic paradigms, which in practice often do access and use the privileging state informationduring each episode (instead of only at the end) , without computationally intractableoracles. However, the algorithms (also) require an oracle for planned or evenoptimistic planning in a learning approximate POMDP, which are not computationally tractable ingeneral. :Comparison of theoretical guaran-tees with/without privileged information. Learningprivileged value functions (to improve the policies) has also been widely used in multi-agent RL, fea-tured in centralized-training-decentralized-execution, see e. The closest line of research to ours are the recent theoreticalstudies for Hindsight Observable Markov Decision Processes (HOMDPs) , where the underlyingstate is revealed at the end of each episode; see also subsequent related works in with dif-ferent observation feedback models. g. , observability or decodability), in both tabular and/or func-tion approximation settings. RL under hindsight observability. Mean-while, even under the additional assumption of observability on the underlying POMDP model, itis still not clear if these algorithms can avoid computationally intractable oracles, since approx-imate POMDP that needs to do planning on at every iteration during learning can be quitedifferent from the underlying model. Indeed, without any structural assumption, learning optimal policy in HOMDPs iscomputationally no easier than planned problem, which thus remains PSPACE-hard. g. For the latter, asymmetricactor-critic represents one of the well-known examples, with other studies in. ,. g. The x and y axes de-note the restrictiveness of assumptions, onthe emission channels/observations and transitiondynamics, respectively. This has thus necessitated the use of history/belief in asymmetric actor-critic,as in our. This makes that single iteration evencomputationally intractable. ,.",
    "7.3. Computing CCE in POSGs that that each step h [H], there exists h : Ph S that PG(sh = h(ch,ph)|ch,ph) = 1 for any is stillPSPACE-hard": "InTheorem H. to ourframework for our framework for POSGs blue ideas sleep furiously is also decoupled learning anexpert policy that is fully ii) the decoding where the firststep by any off-the-shelf algorithm of learning in Markov games. yesterday tomorrow today simultaneously Learning multi-agent individual decoding functions with unilateral exploration. detailed algorithm 5, and the guarantees learning the decoding functions and the corresponding distilled policy forlearning NE/CCE, and we defer the for learning CE to Theorem H. 6.",
    "Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel.Asymmetric actor critic for image-based robot learning.Robotics: Science and Systems XIV,2018": "Shuang Ziyu Dai, HanZhong, Zhaoran Wang, Zhuora and Zhang. Tabish Rashid, Mikayel Christia De Wtt, Gregoy Jakob Foer-ster, and himon QMIX: Monotoic valufunction factorisation fordeep multi-agentrenorcemet In Internation Conference on Machine pages 681689, 2018. Poste-rior sampling competitiv rl: unction approximationbseration. Advances inNeural Infrmation Systems, 2024.",
    "Uh(b;a,o) = Bh+1 (Th(a) b;o),": "For any 2 h H action-observation sequence (a1:h,o1:h), we inductively belief state:.",
    "Technical Assumptions for Computational Tractability": "onider step h [H], aypoliy and any of common information ch that has non-ero probability un-der the trajectories generated by 1:h1. 6 (Strategy independence beliefs ). Consider any otherpolicies 1:h1, also give an-zero robability ch. Assumption 2. Then, we ase tht: for any Ch, ay ph S,. et >0 A POMDP/POSG atifies -bserability if Ohfor h [H].",
    "now ready to present the main theorem of this": "Consider a POMDP P that satisfies Definition 3. e. Then given access to the classification oracle of , there exists an algorithm learning thedecoding function {gh}h[H] such that with probability at least 1 , for each step h [H]:. 2, a policy E S, and let {Fh {S A O S}}h[H] be the decoded function class, and h Fh for each h [H], i. , {Fh}h[H] isrealizable. Theorem E.",
    "Hengan Hu, Adam Lrer, Nam Brown, and Foerster.Learned searc: Efficientlyimproving policespartially obervable arXv preprint ariv:216.086, 2021": "Schapre. In Doina Precup andYee Whye Teh, editors, roeedings of the 34th Interational Conference on Machine Learning,CML 201,Sydney, NSW, Australia, -1 August 017, volume 0 of Proceedingsof MachineLearning Research, pages 104713. Cotextual decision procsses wih low ellman rankare pac-learnable. Nan Jiang, Aksay Ksnamurthy, Alekh Agarwal, John Lagfod, nd Robert E.",
    "Concluding Remarks": ", when privileged state information may be biased,partaly obrvable, or delaed, as happens n practice, and how our theoreticalreultsmay be affected. exploreancan be furte rlaxed, e. Our reult (s summarized in ) that privleged information oes improvelearning of known POMDP subclasse. potential limittion ofour workis that weonly focused the cae wth state information.",
    "where the agent bases on the entire (partially observable) history for decision-making. The cor-responding policy class is denoted as h.We further denote = h[H]h.We also define": "gen := {1:H |h : Ah1 (A) for h [H]} to be the most policy in par-tially with privileged state information, which can potentially depend on all historicalstates, observations, and actions. We use only re-ceive finite memory instead of the whole history inputs: fix integer > we define policyspace L to be the of = := h : Zh (A) withZh := Amin{L,h} for each h Finally, we define space of state-based policies S,i. e. model P, we use PPs1:H+1,a1:H,o1:H(E) to denote probability of some eventE when (s1:H+1,a1:H,o1:H) is drawn as trajectory following the policy in model P. We willalso the shorthand notation P,P () if (s1:H+1,a1:H,o1:H) is evident. We EP[] theexpectation similarly. We define value function at h as ,Ph(yh) := EP[Ht=h rt(st,at)|yh], de-noting the rewards from step h, where and we slightlyabuse the notation by treating as set the of states s1:h, the sequence of observationso1:h, and the of actions a1:h1 to h, which are the available information to theagent at step h. say yh is reachable if exists some gen such that P,P (yh) > 0. For 1, we adopt the notation vP = EP[Hh=1 rh(sh,ah)]. Meanwhile, we also de-fine Q,Ph(yh,ah) := EP[Ht=h rt(st,at)|yh,ah]. We denote the occupancy measure on the state space asd,Ph(sh) = P,P (sh). those. Formally, we define:.",
    "Numerical Validation": "Definition and generalPOMDPs. W th for the implementation detais G. I particular, generate whereeither the trnsition ynaics are deterministic or ensres decdability. We nowprovide nmerical resuts for boh of ourprincile algoriths.",
    "PP (sh = Uh(bh1(h1),ah1,oh) = PP (sh = h1(h1),ah1,oh)": "Meanwhile, since know PP (|h) is a vector, we can construct h such h(sh1,ah1,oh)is the sh that makes PP (sh |h) > with sh1 = h1(h1). If this procedure does completethe definition of for some (sh1,ah1,oh), it implies that either is or oh is sh1, e. 2. Proof 3:Note that for any given problem instance of a POMDP, we can construct a POSG by adummy agent the observation being the exact state at each and has only onedummy action that does not affect the transition reward. Therefore, the corresponding POSG with the dummy agent satisfies the Proposition 7. 3. Meanwhile, it is to see any CCE of the POSG is optimal original POMDP.",
    "where the equality holds when q(a1) = q(a2) = 1": "On the thrhand, cominin the fact that b1o1) = Ui(S) with 1, itto the optimal patiallyobsevale poicyargmax vP () 1(a |o1) 1. 2. Now we are ready to evalate blue ideas sleep furiously hoptimaliy gap betwenand s follows."
}