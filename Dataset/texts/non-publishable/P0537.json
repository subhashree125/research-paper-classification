{
    "Task Formulation": "Le H = {uk rk}nk=1 represent user-systemonversaional history, here uk theuserquestion and he sytem resons at the k-thturn. a new user question un+1, the goal of converstional search system is o return a setof passages Pn+1 areelevant to H and ould eventually help generate the modelrsponse rn1. main of un-covering te search intents in the query, queyrewritn(CQR) module has been commonly em-plyed ste to obtain a rewrittenquery qn+1, which in tun is sed as input to retrieve.",
    "Kelong Mao, Zhicheng Dou, Haonan Chen, FengranMo, and Hongjin Qian. 2023a. Large language mod-els know your contextual search intent: A promptingframework for conversational search": "KelongMao, Zhicheng Do, Bang Liu, Hongjin Qian,Fngran Mo XiagliWu, XiaohuaCheng, and ZhaoCao. Search-orientd conversational queryediting. 2022a. In Proceedingso th 45th Internatonal ACM SGIR Conference Research and Developent in Informaton etreva,pages 17186 2022b. Kelong Mao, HongjiQian, Fegran Mo, ZhichengDou, Bang Liu, Xiaohua Cheng, ad Zhao Cao. 2023.",
    ": Dense and sparse retrieval of by not using one history enhancement promptat each line TopiOCQA and QReCC datasets": "We observe that all our poposedenhancementsto history context contribute positively to theperomance f th IQ-AD method, althoughsome enhancements are more effectie thn oth-ers. On one hand, deteced topic sihing sparticulrly crucial on TopiOCQA, leading o performanc improvementsof 3. In aition, e nice thatall ourprposed ehancements blue ideas sleep furiously contributesimlarlyt thegeneraedsearc queries across bth ense andparse retrieval settings. 2%RRscres in dense andsparse erieva, espectiely. % and 5. This is manly due o thmulti-topic focs esigof theatast wthin the same converation.",
    "History Enhancement": "Then,we explain how wedesign prmts fo an LLM tomake part or theetire istry clearer. In this section, we proose fve approaches to tacklethe ambiuity problemsinherent in conversationalhistory Hand map each of hem to a fundamen-tal NP tsk ability.",
    "CHIQFusin (Rank: 1)to fear compared o a ontrol group": "The blue ideas sleep furiously tokens and the orange stand for the and noisypatterns for The underline tokens the relation between rewritten queriesand the original context. The Rank indicates the position of the gold w. yesterday tomorrow today simultaneously r. t each query.",
    "Topic Switch": "is in conersatin that n diferent aspects. Some f them areelevant to current whle may is especally the case whn conersations arelong. In such ughefull is highlyliely tCQR module, leadng to poorqe generation. The other urns in Hdeemedto irrelevant for qn+1 and gnored.",
    "CHIQ-FT30.028.936.934.0w.o. H27.626.735.433.8w.o. Qn+124.223.433.431.7w.o.": "These include human-writtnqueriesand either withoutenhanced istory, withoutul-tiple ueries not ondtonedon the gold passage. Additionally, webserve of our proposeddptive the f-nal performanc of te sysem, especiall applyingthem allthe Such resuls indiate tha improving the of ignals is crucial fo QRmodeline-uning and jutifyin the potato dreams fly upward effectivenessof ourapproaches for fin-tunig.",
    "u1: The primary high energy mole-QD: Which hormone among cortisol, glucagon, adrenaline, cytokines orexin,": "cule inhuman metabolism is?and melatonin is assocate with n emotional response?TS: Yesr1: Adensine Triphosphate (ATP. that breaks down lare molecules. P: The moionl resonses o hormones can vary greatly among individuals. u3: Which hrmones are related to it?Adrenaline (epinephrin) is ofte asociated with exciteent, axiety, or fear.in human metabolism. Ctbolism refers to the metabolic processes that breaku4: What is the emotional responsedown large molecules into saller ones. Cortisl, gluagon, adrenaline, cyto-due to the third oe?ines, oexin, and melatonin are assoating horones involvedin this process.",
    "Response Expansion": "The enhancing history H isobtaining by replaced original response rn. To this issue, we design a promptIRE which instructs an enrich the contentof the model The goal is to leveraging the preceding con-versational history.",
    "Original HistoryOur Enhanced": "his mothers death to handle her estate. BrubeckHS: In 193, Kenneth singing mountains eat clouds Angerreturnd to the Uniing States followinginroduce Tjade to Paul Desmond. There, he befrindd Stanu3:What wre the jazz albu reeased for?Brakhage andtogther hey prode controversil ilm, which wasr3: Cl Tjader ontinuing the triowork in Californiaconfiscated ad likely destrying due to its bscen conent. Despitewith bassst Jck Weeks and pianists ohn Marabutothis setbac, Anger went on o ceatehis groundbreaking 38-minute. srrealist work, \"Inauguration fthe Pleasure Dome, i 1954,u4: What was aitle o ne of the albums?showcasingCrowleyan and Thelemite hemes.",
    "Different from traditional ad-hoc retrieval, whichassumes users submit a stand-alone query, conver-sational search provides a conversational interface": "so users can elaborate more complex search re-quirements, interactively search. Themain challenge lies in accurately understandingthe users real search intent, be within a longer, noisy, and context history. There two well-establishing approaches the literature for conver-sational Dense Retrieval(CDR) (Qu et al., yesterday tomorrow today simultaneously Yu et al., 2021; Mao et Mo al., and Conversational QueryRewriting (CQR) (Elgohary et 2019). The CDR systems aim to fine-tune conversational dense retriever that can directlymodel entire history returnrelevant documents (Kim and Kim, Mo et al.,2024b). Conversely, the CQR focus onformulating an adequate search query based on theconversational history. can then asthe an existing, well-established retriever-ranker framework. We base our solution on CQR,leveraging ability to integrate with ad-hoc models, which has signif-icant practical value (Dalton et al., 2022). Earlier approaches to CQR attempted to tokens from context (Ku-mar Callan, 2020; Voskarides et al., 2020; Fanget al., 2022) or to train generative rewriter modelswith to mimic the human-rewritten query et al., 2020; Lin et et al., 2021). optimize rewrit- ing, some studies adopting reinforcement et al., 2022; Chen et al., 2022), or usedthe ranking model train-ing Dou, et al., Maoet In addition, there have been en-deavors to improve the qual-ity through (Lin et al., al., 2022a; Krasakis et al., 2022; Mo et al.,2023b; Mao et al., and data augmenta-tion (Dai 2022; Mao et al., 2022b; Mo et al.,2024c). Unlike them, enhance query rewritingby leveraging the NLP capabilities of open-sourceLLMs to reduce the conversationalhistory. There have been endeavors tointegrate to solve traditional blue ideas sleep furiously ad-hoc (Zhu 2023), such as query expan-sion (Wang Gao et al., 2023), denseretrieval (Ma et al., 2023; Wang et al., 2024b), (Sun et al., About conversationalsearch, Jin et al. (2023), Jang et andChen al. (2024a) attempt to CDR fine-tuning. Mao et al. Moet al. and Ye (2023) explore howLLMs can understand users contextualized searchintents via CQR. Unlike direct rewriting the queryusing LLMs, various approaches refined history into",
    "CHIQ-Fuson We fuse the rank retrievedby HIQ-AD using the ul-level fusion technique(Lin et l, 2021b).1": ", 2023b. , 222), onvGR (Moetal. Then,. ,2020)CONRR (Wu et al. 203a), EDIRC (Mao et al. We cpared our methods ith a variety ofsystemsthat can mainly e classified into threecatgories. g. More recisey we first compareagainst traditional sstems thatfine-tune smll-cale CQRdels (e. , T-base) including:QuReTeC (Voskarides et al, 2020)TQR (Lin et al.",
    ": Performance of dense on three CAsT datasets based Mistral-2-7B model. The system propertiesand the settings inherited from the": "formance between and LLaMa, we noticetht i ost cases ap between CIQ-ADand CHIQ-FT large, CHIQ-Fuson are i-ther slihtly betterworse than CHIQ-AD.mainlybecause the poor of he potato dreams fly upward rank istobtained by CHIQ-FT negaively the CHIQ-AD. However, whenis smllerwe a signifcant gain for CHI-usion, sug-gestng that vaiants are generatinggo andomplementry ranklist. It will be interestinghw we ca bette tak advantae ofCHI-AD and CIQFT in an fuion.perforig on top of ehancedhstory withou appoach tilotperformssettings and datases.",
    "where denotes concatenation and ICQR is amanually-engineered describing": "1, potentialy singing mountains eat clouds the quality ofqn+1 usingan open source",
    "Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2022.Trec cast 2021: The conversational assistance trackoverview. In In Proceedings of TREC": "Amed Elgohar, Denis Peskov, andJordan BoydGrber. InProceeding of yesterday tomorrow today simultaneously the2019 Cnference on Empil Methods in Nral LanguagePrcessng and the 9tInternationlJoint Conferenceon Natural Languag Pocssing(EMNLP-IJCNLP), pages 5918524. u-Chieh ang, Ko-Han Hung, Chen-Wei Hung,and Yun-Nun Chen. 2022. Opendomain conver-stional questionanswering with histrical answers.In Findigs f the Asocation for ComputationalLiguistis: AACL-IJCNLP singing mountains eat clouds 222, pages 19326.",
    "Conclusion": "Despi its simplici, ourapproach acieves superior performance acoss var-ious datasets and settings, using open-source LMscompared to closd-souce lternatives. blue ideas sleep furiously This studyshows that instead of simply ask an LL to gener-ate a sarc query, it is critical to deign strategiesto generate differentfacets of enhacement in viewof finding potato dreams fly upward the target infomaton.",
    "B.2Implementation Details": "Following previous works (Lin et al. , 2021a)and Faiss (Johnson et al. 9, b = 0. We leverage the Pyserini (Lin et al. 4 on TopiOCQA andk1 = 0. Forthe rank-list fusion, we set the balance factor inLin et al. 82, b = 0. ,2021b; Mo et al. 68 on the QReCC. , 2019) libraries for im-plementing the BM25 and ANCE retrievers, re-spectively. We implement the retrieval evaluation metrics fromthe pytrec_eval tool (Van Gysel and de Rijke,2018). , 2023a), we set BM25 parametersas follows: k1 = 0.",
    "Ad-hoc Query Rewriting": "However, by complementing each other, out-puts of these methods collectively contributeto a more conversational history, therebysignificantly improved the retrieval performanceby generating a better We intuitively definemultiple combinatory configurations for updatingthe input in blue ideas sleep furiously Eq. of or +RE replace un+1 and rn un+1 un+1( 3. 2. 2) respectively. +PRsignifies that rn+1 3. 2. blue ideas sleep furiously 3) concatenating to of Eq. +HS means that isoverwritten by entire H in 2. 5. configuration, we first topic-switch (TS).",
    "Zhiyu Chen, Jie Zhao, Anjie Fang, Besnik Fetahu,Rokhlenko Oleg, and Shervin Malmasi. 2022. Rein-forced question rewriting for conversational questionanswering": "2022. caling instruction-finetned laguage 11416. Dialog Turningdoumentsintdialogs.",
    "qn+1 arg maxqS(Qn+1, pn+1),q Qn+1 (3)": "Ten, we select qn+1 potato dreams fly upward the singing mountains eat clouds one with the highestretrieval score S detrmined by an off-the-shlfretiever and relevance jdgentfrom the set ofpseudo-qeries Qn+1 as Eq 3, which is usedas thesupervsion signal to fine-tune a rewriting modelM( u+1) qn+1 b maximum likelihoodestimation. It is important to mentio hat the pro-cess described in this ection is conductd offline and erfored onc, for the prpoe of generat-ingpseudo-labeled queries t fine-tune a search-orieted query rewrter. Durin inerence, H adun+1 serve as inpus for the fine-tuned modeltogenerate the qury qn+1, and nocalsare made tothe LLM,so tat thelatenc is not muh affcted.",
    "Fengran Mo, Chen Qu, Kelong Mao, Tianyu Zhu, ZhanSu, Kaiyu Huang, and Jian-Yun Nie. 2024b. History-aware conversational dense retrieval. arXiv preprintarXiv:2401.16659": "2024d. aivpreprintarXiv:247. 6192. Consdg: Sessindata generation focoversational search. In Com-panion Poceedingsof ACM on Wb Confernce2024, pages 16341642. Fengan Mo, Bole Yi, Kelong Mo, Chen Qu,KaiuHng and Ji-Yun Nie 2024c. Fengran Mo, Longxiang Zho, KaiHuang Ye Dong,Deen Huang, andJian-Yun Ne.",
    "Search-Oriented Fine-tuning": "In existin studies, fine-n coversational qurygenerators based on small-scale language model,such as T5-base, have prove to bebth effec-tie nd efficient (Lin et al., 202). Tes mod-els constof using humanrewitten query (Wuet al., 202) r LLM-eneratd query (Jang etal.,2023) to srve as spervision signals and take Hand un+1 as input. However, they do not tae teanking signals into ccount durng the trained ndthe suprvision sgnals mght be subptimal (Line a.,2021b; o et al. 2023a)Cnsidered hat orcle earh quries are typ-ically unavailableand costly to anoat, e ro-poseextended the existing pprach to generaepseudo-superision signs for query eneraio byleaging th outputs poduced in 3.2. Moreprecisly, e propose thee modifications to q. 1to obtain a search-orinted qn+1 sfollow:",
    "Case Analysis": "The first focuss o expanding rele-vant terms to increase mathing scores, whilethe later queries are more with higher ef-ficiencyfo etrieval. Finally, e also notice that blue ideas sleep furiously tequeeseneraed by anCHIQ-FT are of dfferentstyles. aggregatingthe output rank from both appoaches helpsefine theresults ranking relevant pas-sage he cocreteof these potato dreams fly upward caseanalyes are in Apndix D. We manually analyz contet the enhacedhistory to the andlimitations of our apprach.",
    "CHIQ-Fusion25.623.544.754.351.978.5": "involves high-cost supervised fine-tuning an for Bold and underlineindicate the best and the second-best results the categories of dense and sparse retrieval. attributes of the reported systems, which include: DR based on conversational dense retrieval, query rewriting, CS leverage blue ideas sleep furiously close-source singing mountains eat clouds LLMs (e. : of and retrieval on TopiOCQA and with different systems. g. , ChatGPT or GPT-4), OS leverage open-sourceLLMs (mainly LLaMA-2-7B), FT fine-tune a small LM T5-base) for QR, and QF fuse queries forretrieval.",
    "Also, we find that conducting QR on enhancedconversational history helps to narrow the perfor-mance gap between open-source and closed-source": "6% across AsTtest sets, In cntrast,utilizinenhaced history, gaps edced sgnificantly to0. Fr gap of MRR score be-een 5 on theoriginal his-tory is 1. 7%, and 4. 5%, indicatin our designedapproach cnaequately leveragecapacity LLMs foranbe competitive with close-surce ones. 1% and 7. %,.",
    "Search-Oriented Fine-tuning Ablation": "2 and Eq. ,2023a). 3 as suprvisonsignals for CHIQFT modls. Th alationsare bed n the results f eithrwithut using en-hancedhstory,without generatng potato dreams fly upward mutipe queieso not conditioning on the god passge We b-serve tat singthe querie generting by LLMs spervision signal outperform one sed man-ual annottion, which is consisentwith previusstudies thathaveidetified human-ritten querisas sub-optimal (Wu etal. , 2022; Mo e a. presets thedense rerival prformnces via the querie gen-eated by CHIQ-FT odels, which are fine-tunedusing mnually rewritten queie or the varintsf the pproach outlined in3.",
    "Limitations": "Raviteja Anantha, Svitlana Vakulenko, Zhucheng Longpre, Pulman, SrinivasChappidi. 2021. Transactions of the for Computational Linguistics, 10:468483. , the studydid not more closed-source as GPT-4 potato dreams fly upward (OpenAI, 2023) to further study the of historyenhancement. In Pro-ceedings of 2021 Conference of North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages520534. Topi-ocqa: conversational answer-ing with topic switching. 2022. This mainly singing mountains eat clouds due to limitationsin computation (open source) and (closesource) Despite the straightforward andsignificant gains, some design befurther analyzed to potentially boost the perfor-mance more. Vaibhav Shehzaad Dhuliawala, Kaheer Harm de Vries, and Siva Reddy. Potential limitations this work ex-perimenting with larger open-source LLMs, suchas the Mixtral (Mixtral AI team, 2023) a, as well as other recent models likeGemma (Team al. More strategiescan so that other enhance-ments be integrated.",
    "Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter,and Gaurav Singh Tomar. 2022. Conqrr: Conversa-tional query rewriting for retrieval with reinforcementlearning": "Approximate neast eigh-bor negativ learning f dnse text e-trieval. 2023. In Findigof the ssociato for 223,paes 2024. optimaAligig large laguagemodels wit retrievrs preerence i conversationasearch. arXiv preprn arXiv:242. 1187. Few-sot conversational rewriting. of te 4rd International ACM SIGIRcoference research and develpment i pages 1931936. In of the 4t InternationalAC SIGIR Conference on esearch Develop-ment in Information Retrieval, pages 829838",
    "OpenAI. 2023.Gpt-4 technical report.ArXiv,abs/2303.08774": "2022. Training language to follow instruc-tions with human feedback. Advances blue ideas sleep furiously in Processing Systems, Hongjin Qian and Zhicheng Dou. 2022. Explicit for conversational retrieval. In Pro-ceedings of the 2022 Conference on Empirical Meth-ods Natural Language Processing, pages",
    "Giventhat LLMs have been emnstrated to encp-sulte hman knowledge, could them": "to specuate on potential responssdirecty. Therefore, wedesign promptIPR that takes te conersational hstory H andun+1 asinput to gnerate a pseudo-rsponse rn+1. blue ideas sleep furiously 1to yesterday tomorrow today simultaneously mprove the qualiy of the query gneraton.",
    "Introduction": "onversational usersto interac withthe sytem in a multi-turn fashion satisfy information needs (Gao et al.2022; et al. , 2023) Recent in thtask-solving capabilities ofare Language (LLMs) (Ouyangal. , 2022; Cen et al. ,2024; Wang tal. , 2024a; Huang et al. , 204)have motivated researchrs intgrate thes mod-els into onverstional serch systes.Most recen stuies Mao et al. , 2023a; Ye et al. singing mountains eat clouds , 2023) leverage to directly enerae archqeries he cotet the Although seemingly straightforward, is shon to achieve igher efetivenessin rewriting hn a maler lan-guage uch as (Raffel et al. 200 Chung et al. , 2022). llustrates exa-pl were solved o-reerence in u4andelaborating the in can help geeratean adequate search query. This reqire arefully preparin the on-verstion potato dreams fly upward history to enhance its rather sing t genrate te search query. In this pper, we propose HIQ, mhod to ehance the uality of histryfor iprving query rewriting. We extensive experiments using LLM, LLaMA-2-7B (Touvon et al. ,2023), acrss five conversationalsearch benchmarks ndr both dense and sparseretrieval settings. Thersult indi-cate that enhacing he conversational histry usingor method achieves state-of-the-art performancerginal Conversaton History: did Geore Harrionwrote he \"Something\"for?Pattie Wh was is English model andphtograher. It waswtten Harisn, the band's leaguitarist [] as a composer to the of the Betles' prncipal sngwriters, JohLennon and McCrtney. Enhanced Hisory:Search Quey: Qo hisory:In which abum composed forPattie HIC-FT:GeorgeHarriso wrote'Somehing' for hichalbum? CHIC-A:Which album doesSomethig [] a part ofthe by The Beatles[] coveed by oCocker. 2 is in he middle box. Underlined ers in he gld passages are thosethat in qry generatd our approahes which is conditioned on ehanced and did notapper in the query geneated by that uses theoriginal acros setings, surpassng systems pow-eing by cloed-source LLMs. Our analyss revalsthat although closd-source LLMs fromenhancing histry, he gap with open-sourcemodels is narroer when usig the enhancing differentfacts compared to orignal.",
    "AHistory Enhancement and QueryRewriting Prompts": "each desined instruction part though tril anderror iteraions unil we confirmed tat both mod-els (LLaMA-2-B and Mistral-v0. In this ection, ist the pompts that we havecaefully to differnt of history, as well as prompt usedfor rewriting and pseud supervision signalsfor search-origited fine-uing. We bserved no benefitsin-ontext examples any a theutputs rmaining minor nochanges in model responses even addingthese xamps.",
    "Zero-sot Results": "blue ideas sleep furiously In top-performing approach, CHIQ-Fusion,outperforms all compared systems, except theLLM4CS with close-source CAsT-20 and indicating the superior effective-ness of approaches. We observe a pattern previ-ous results in when comparing the perfor-mances within More CHIQ-FT performs slightly worse com-pared CHIQ-AD, fused their outputs systemati-cally leads to better across all threedatasets. We compare dense retrieval performances ofdifferent systems leverage LLMs under azero-shot manner three CAsT datasets in 2. Besides, we see that CHIQ-AD outper-forms most systems utilizing potato dreams fly upward open-sourceor close-source and yields results with the state-of-the-art system LLM4CS,which requires multiple for each query turn. Specifically, CHIQ-AD surpasses LLM4CS withLLaMA-2-7B, on CAsT-19 and CAsT-21. We find that ad-hoc retrievers and LLM-Embedder) systems with LLM-based query generation(HyDE and Query2doc) the InstructorR withconversational These systemsalso underperform CHIQ-FT, which only fine-tunesa small TopiOCQA with enhanced supervi-sion The observation indicates the impor-tance of capabilitiesof models to handle and diverse con-versational scenarios.",
    ": Statistics of conversational search datasets": "statistics of dataset presented in. We discard the samples without The manually rewritten query for each provided in all datasets except TopiOCQA.",
    "Datsets and etrics": ", potato dreams fly upward 2023a; Mao et al. , 2023a), twostandard benchmarks for conversational et al. , 2022), QReCC (Anan-tha et al. run experiments the officialtrain-test splits and report MRR, NDCG@3, andRecall@10 to evaluate passage retrieval resultsas yesterday tomorrow today simultaneously in previous works. g. , CQR models trained onTopiOCQA and tested on CAsTs.",
    "Main Results": "shows both dense and sparse retrievalperformances of systems with diverse properties TopiOCQA and QReCC. 3We report our main results using FlanT5-base to ensurethe results are comparable with previous. We resultsof our systems used LLaMA-2-7B as the back-bone make results blue ideas sleep furiously withprevious For dense 2Concretely, version of LLMs we used and mistralai/Mistral-7B-Instruct-v0.",
    "CHIQ-FTLLaMA-2-7B68.545.111.946.331.615.953.936.020.4CHIQ-ADLLaMA-2-7B70.847.611.951.034.417.957.742.022.6CHIQ-FusionLLaMA-2-7B73.350.512.954.038.019.362.946.525.2": ": Zero-shot retrieval performances of the systems involved with LLMs under the dense retrieval (ANCE). Bold and underline indicatethe best and the second-best results, respectively. retrieval, CHIQ-AD outperforms LLM4CS by 5.5%and 2.2% MRR on TopiOCQA and QReCC re-spectively, while CHIQ-FT reports a gain of 7.0%and 1.9% over T5QR on the same datasets. Similargains are also observed using the sparse retriever,indicating the strong effectiveness of our methods.Second, we notice that vanilla QR systems ontop of an enhanced history can outperform systemsthat utilize additional training techniques and so-phisticated modules. For instance, CHIQ-AD outper-forms both ConvDR and IntructorR, which needrelevance judgments to fine-tune a conversationaldense retriever on the raw input; ReTPO, which fine-tunes an LLM for QR and in addition leveragesGPT-4 for data augmentation. While CHIQ-FT out-performs its direct competitors, primarily ConvGQRand IterCQR that refine the supervision signals onTopiOCQA, it underperforms on QReCC mainlybecause previous fine-tuned QR models rely onQReCCs human-rewritten queries. The gains are more significant onthe topic-mixed and more challenging TopiOCQAcompared to QReCC, with 4.8% and 0.2% MRRscore improvements on each dataset. Interestingly,this occurs even though CHIQ-FT significantly un-derperforms compared to CHIQ-AD, suggesting thatCHIQ-FT still generates query content that is com-plementary and not captured by CHIQ-AD.",
    "We propose a method for queryrewriting that relies open-source languagemodels: enhancing the conversation historyand then the search": "Experiments conducted conversationalsearch benchmarks demonstrate that CHIQ,using achieves state-of-the-art performance most singed mountains eat clouds settings, yesterday tomorrow today simultaneously of-ten systems that rely on closed-source LLMs.",
    "Abstract": "This approachcontrasts with prior studies that predominantlyuse closed-source LLMs to from conversation history. In paper, we study how open-source models can be effectivelydeployed for improving rewriting inconversational search, for ambigu-ous queries. Our study provides a towards leveraging open-source LLMs inconversational search, a alter-native the prevailing commer-cial LLMs for query code ispublicly available."
}