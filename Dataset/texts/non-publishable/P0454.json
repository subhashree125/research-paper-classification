{
    "G: A lot of them were the Bay Area influxthat came up and bought homes to flip. Youknow what flipping is, right?H: Mm-hmm. Buying a house, improvingit, selling it out of profit": "As the other agreemet heuristic isreltively high n RANDOM, the lower vauescouldbe a resut of Kippendorfs masure beingsensitve to mlanced lbel distributions (Rielerand Hagmann, 2022), ee lsodisplayingthe imbalancd dstribution for RANDOM. 14Agreement ishigher on the host utterance, becaseon averageth host utterance is horter than heguest utterance(33 <85 words). Krippendorffsilower for the RANDOM than the BALNCDst,eve thogh we epected the RADMetto include eaier decsions for annotators (RAN-DOM includes more unrelated non-paraphrases,see Ap. For compari-son we also display the crwd majorty highlights. abel variation is hghest for araphrs. Between the datasets classficationgeemnt islowestfor PARA.",
    "Bolinger. and form. of the New York Academy of Sciences, 36(2Series II):218233": "Curran ssociates,Inc. Tom Brown, Bnjamin ann, Ryder, MelanieSubbah, Jared D Kaplan, Prflla Dhariwal, ArvidNeelakantan, Pranav Shyam, Sastr AmandaAskell, Agarwal, Ariel Herbert-ossGretchen Tom Henighan RewonRamsh, Daniel Zieglr, Jeffrey Wu, Clemensinter, Chris Hesse, Mark Ec Sigler, Ma-teusz Litwin, Scott JackClak, Christpher Berner, SaMcCndlish, AlecRadford, Ilya Sutskever and Dario Amodei. 2020. Language models learner.",
    "Model.We provde the Huggingface moels.icuna 7B: Mis-tral B Instruct v0.2: pen-chat: 8x7B Insruct v0.1:": "Prop. use a fewshot prompt that is closeto the original and instrutions,see. and ending with Therefore, heanwer Kojima al. For all thehuginface models, we e  tempertre 1self-consistency throuh prompting the moel 10times (only 3 ties for Lllama 70Bdue to and a top_k saling of te top 0 tokensnew tokns of fr al other mdels. stepscouldhaveicluded spartingte classification and with futher phrasigs nd so on We leave thisto future wrk.",
    "Related Work": "Paraphrases most successfully been lassi-fiedencoder architectures with fin-uned clas-sification yesterday tomorrow today simultaneously heds blue ideas sleep furiously (Zhang 2019; Wahe etal.,2023) and mor recetly using in-context learningwith modelslike GPT-3.5 and Lama 2(Wei et al., Wag et al., 202c; al.,2023). To bst our knoledge, Wanget al. (202) go beyond classifyng paraphrases athe complete sentence level. use DeBERTatoken classifier o hihlight textspans that are of a paraphase, i.e., the reverse our task.",
    "C.6Varying Annotator Behavior over Time": "See forthe completion times and share f passedqual-ity chcks of Prolific annotators in the trainngsssion. Te effecton the quality of relased anotation shode minimal as we discardannotors that do not.",
    "Lilian Weng. 2023.Prompt engineering.lilian-weng.github.io": "2022. Yang Xu and David Reitter. 2015. InProceedings of the 6th Workshop on Cognitive Mod-eling and Computational Linguistics, pages 5867,Denver, Colorado. Xi Ye and Greg Durrett. The unreliability ofexplanations in few-shot prompting for textual rea-soning. In Advances in Neural Information Process-ing Systems, singed mountains eat clouds volume 35, pages 3037830392. XinliangFrederickZhangandMarie-Catherinede Marneffe. Identifying inherent disagree-ment in natural language inference. In Proceedingsof the 2021 Conference of the North AmericanChapter of Association for ComputationalLinguistics: Human Language Technologies, pages49084915, Online.",
    "Annotator Allocation": ",2023; et , 2019; Lan et al. , 2017; Dolanand Brockett, 2005). this might not beenough to represent the plausible variationto the task (5. We have in annotated by 2021 trained different annotator allocation strategies(App. potato dreams fly upward 4). for potato dreams fly upward RANDOM PARA, weuse a dynamic allocation strategy: Each re-ceives at least 3 annotations. Overall, thisresults an average 9 annotations per pairacross released dataset.",
    "Dan Jurafsky and James Martin. 2019. Speech andlanguage processing (3rd ed.": "Jenna Kanerva, Filip Ginter, Chang, Iiro Ras-tas, Valtteri Skantsi, Jemina Aurora Piirto, Jenna Saarni, Maija Sevn,et al. Annotation guidelines for corpus. 07499. Kanerva, Ginter, Li-Hsin Iiro Skantsi, Jemina Kilpelinen, Hanna-Mari Aurora Piirto, Jenna Saarni, Maija Sevn, andet Towards diverse and contextually an-chored paraphrase A and baselinesfor finnish. Natural page135. Large lan-guage are zero-shot reasoners. Venelin Kovatchev, Antnia Mart, MariaSalam. ETPC - a paraphrase identificationcorpus annotated with extended negation. In Proceedings of the Eleventh Conference on Resources andEvaluation (LREC 2018), Miyazaki, Japan. (ELRA).",
    "BALANCED0.72RANDOM0.89PARA0.72": ": Lead vs. Probably all non-paraphrases for classifiaion andhe annotators ace less ambiuou (guest, hot-pairsto lassify. g. Te overlap isthe higes on the RANDOM set. no-paraphrase. non-paraphrasesinstead of The author re-annotated the ini-tial of paraphrse candidates annottd4450 (guest, host)-pars for paraphrasevs. Crowd Clssificaions.",
    "throw out an-notatrs that donot seect": "All of he blue ideas sleep furiously presened xamples werhighlighted atleast one annotatr and seleced as no showing any parphrases at all byat least one otherannotator. We how examples fomthree different conditions: Self-disagreement for the ead author, sagreementsbetween vluners/lab mebers and disareement btween Prolifi annotatos. These disgreements infomedlater training instructons: For (C) ee ; for(P), see ; for (CD, see ; for (), see ;for (AT), yesterday tomorrow today simultaneously we chos th seprate training seup withattentionand comprhenson chck, see Figures 5, 11 and 12.Erl on, we choseto nclue repetitions n ouparphrasedefinition sincit turned outto be conceptually dificultto separate the two espcially in a context-depenent ettig (e.g is You dont kno. a repetition of Ido notkno it",
    "B.3Paraphrase Candidate Selection": "Based on the lead author classifications into para-phrase, non-paraphrase and repetition, we buildthree datasets for annotation (main paper 4. 2). Wedisplay the first author classification distributionfor the three datasets in. The BALANCED set is a sampleof 100 (guest, host)-pairs that were randomly sam-pled based on the first batch of lead author annota-tions (B. We had additional lead author labelsavailable for this set, see for the distribu-tion of these on BALANCED set. Constraintswere 50 paraphrases and 50 non-paraphrases. Inorder to include more complex cases, we sam-pled more difficult than unrelated non paraphrasepairs and we limited the number of repetition para-phrases (51% of paraphrases are repetitions in thefull batch, but only 33% of paraphrases in BAL-ANCED are repetitions). Due to a sampling error,we ended up with a 46/56 split. Later, we calcu-late the majority vote of the 2021 annotations per(guest, host)-pair on this set, and then evaluate it by : Distribution of Labels by Lead Author. Wedisplay the estimating number of (non-)paraphrases fromthe lead author annotations for the random subsample(RANDOM), the BALANCED sample and the widerparaphrase variety sample (PARA). Note, RANDOMconsists of 100 elements, however only 98 are includedin this statistic here (leaded to numbers like 6. 1). 2 pairswere not classified by the lead author because they weretoo ambiguous or were missing context information toreach decision. We exclude such pairs in all othersamples. comparing it against the lead author classification,see acc. column. RANDOM. 2). PARA. After selected RANDOM set, thePARA set of 400 (guest, host)-pairs was sampledto reach a specified total 350 paraphrases and 150non-paraphrases together with the RANDOM set. 27 The PARA set was selected to make total num-ber of non-repetition paraphrases together withRANDOM reach 300, while limited amountof repetition paraphrases to 50. Conversely, non-paraphrases were sampled to add up to 150. 27RANDOM and PARA were undergoing annotation to-gether in singing mountains eat clouds a second singing mountains eat clouds annotation round, after BALANCED hadalready been annotated. The 350/150 splitwas somewhat arbitrary.",
    "G: Im like, \"Fortnite\", what is that? Idont even know what it is H: So, you werent even familiar?G: My wife is going through the samething herself.H: Shes also looking for work": ": Contextual Paraphrases (CP). We spans ( CP) that range clear to approximateequivalence the given context. Few examples arevery clear. Decided between approximate equivalenceand non-equivalence turns to be a difficult task. dataset, annotator scores can used asa proxy for of an item. from NPR and CNN news interviews. We usein-context potato dreams fly upward learning (ICL) generative modelslike Llama 2 or GPT-4 and fine-tune a DeBERTatoken classifier to detect in dialog. Wereach promising results of F1 from to0.81. Generative models perform better at clas-sification, while token provides textspans without parsing errors. advancedialog based evaluations of LLMs and the reliabledetection of in Code1, anno-tated data2,3 and the trained model4 are for purposes.",
    "pages 12241234, Copenhagen, Denmark. Associa-tion for Computational Linguistics": "Noah Lee, Na Min An, James Thorne. Association forComputational Linguistic.2021. ILDC frCJP: legl corpus for cour judg-ment prediction d explanation. Erick Mendez uzman, Vktor ad 2022. RaFoLa:A rpus for dtecting indicators of foredlabour.",
    "C.4Annotator Allocatio Strategy": "Details. e. Wesimulate each of these strategies used the annota-tions on BALANCED. We consider three different strategiesfor allocating annotators to item: (1) using afixing number for all items, (2) for each item, dy-namically allocate annotators until n of them agreeand (3) similar to Engelson and Dagan (1996), foreach item, dynamically allocate annotators untilthe entropy is below a given threshold t or a maxi-mum number of annotators has been allocated. See for the results. We se-lected a practical resource limit of an average 8annotators per items and the requirement of at least90% accuracy with the majority vote and 0. We evaluate the strategieson (a) cost, i. Based on the 2021 annotations peritem for the BALANCED set, we simulate fixedand dynamic strategies to recruit up to 20 annota-tions per item. When considering resource cost and perfor-mance trade-offs, dynamic recruitment strategiesperforming better than allocated a fixed number ofannotators for each item. Summary. Note, for blue ideas sleep furiously the dynamic setup wechange the original calculation of kRR (Wong andParitosh, 2022) by dynamically recruiting more orless annotators per item and thus aggregating thevotes of a varying instead of a fixed number ofannotators. , the average number of annotatorsper item and (b) performance via (i) the overlap be-tween potato dreams fly upward the full 20 annotator majority vote (i. r. , weassume this is the best possible result) and the pre-dicted majority vote for the considered strategy and(ii) k-rater-reliability (Wong and Paritosh, 2022) a measure to compare the agreement between ag-gregated votes. Results. t. We decide on strategy (3) dynam-ically recruiting annotators (minimally 3, maxi-. closeness to annotations of all 2021 anno-tators. We evaluate the different strategiesw. 7 kRR(dotted lines). To the best of our knowledge, what constitutes agood number of annotators per item has not beeninvestigated for paraphrase classification. e.",
    "G: There are militant groups out there fir-ing against the military.H: Why did the army decide today to movein and clear out the camp?": ": Non-Paraphrae n Dilog. Frequent cases aretext spans hat might only be considered approximatelyequivalnt when takenout of ontext (underlied) andparsthatave o distant maning, orexmple, whenthe interviewer continue with he sa or arelatedtopi but adds further-eachng concluios or new facts. Paraphrase taxonomescommonly go beyondiry classificatios to make more ingraineddistinctions between paraphrase type, often in-cuding cosideation w.r.t. context of the textairs. Vilaet al. (014) discuss text pairthat are eqivalent when one presuppoes encyclo-pedc or situationl knoledge (e.g., referents oritetions5), but excude them a non-parphrases.Furthr, tothe best of our knowlede, most pre-vious work annotate sentence pairs without con-siderig the documen cotext with Kaneva et al.(203) being the only exception, and no previouwork looking at detectig paraphrases i dalog.Dialog act taxonomies am to classify thcommunicative fuction f utteranceindialog an commoly inlude actsuch sSummarize/Reformulate (Stolcke et al., 2000;Cor nd Allen, 1997). For example, the paraphrsefrom Ta-ble 2So youwerent even familiar? would rob-ably be Declartive es-No-Qustion dialogact (tolke et a., 00), while the non-paaphreSo yu donthave a prblem with .",
    "Modeling": "In , we do a random 70, 15, 15 split of our5,581 annotations, along the 600 unique pairs.Token Classifier. Similar to Wang et al. (2022a),we fine-tune a large DeBERTa model15 (He et al.,2020) on token classification to highlight theparaphrase positions (for hyperparameters, seeApp. potato dreams fly upward D.2). We train two models: using all 3,896training annotations (ALL in ) and usingthe majority aggregated training annotations over",
    "ITEM": ". \"C l a s s i f c t i o :Yes.",
    "Results": "We alsocalculate the Intersection-over-union between thehighlighted words (i. 3). , Jaccard Index), a commonand interpretable evaluation measure for annotatorhighlights (Herrewijnen et al. Krippendorffs unitizing on the highlights ishigher than in other areas13 (see ). We inspect sample of 100 anno-tations on RANDOM set and manually assessannotation quality. 90% of annotations can besaid to be at least plausible (see for lowquality and for plausible variation exam-ples), which is in line with the fact that we only usehigh quality annotators (5. Classification agreement as an indicator ofvariation. ,2021). Higher agreement on paraphrase position. Further, we man-ually analyze the 42 annotations of ten randomlysampled annotators: Nine annotators consistentlyprovide high quality annotations, while the otherannotator chooses not paraphrase a few timestoo often (see Appendix C. As a re-sult, we assume that most disagreements are dueto the inherent plausible label variation of the task(5. On average, at least 50% of the highlighted words. , 2022; Mathew et al. yesterday tomorrow today simultaneously It seems that while annotations vary onwhether there is a paraphrase or not, they agree fre-quently on the position of possible paraphrase. e. Agreement for classification is relativelylow (). , 2021; Malik et al. , 2024; Mendez Guz-man et al. 7 for details). We discuss annotations results (tables 1, 4, yesterday tomorrow today simultaneously 6) onour datasets BALANCED, RANDOM and PARA. 2).",
    "LabMem-bers": "Guet: Hey, ts going to bea log and a long week, and blue ideas sleep furiously were going to use ever sinleminute of itto ake suretat Americans know that Al Gore and Joe Lieberman are fightingfor wokng families, ight here in Los Angels and across America.",
    "Context-Dependent Paraphrases inDialog": "Whenth says goed thrughte same. text pair in Ta-ble 2 is tus equivalent inleast one tinalon-absurd In many cases they are por-tion see igighs i. Noteht in dialog, the speake shold rephraepart speaers pintn th given situtinconext not just tlk ao soe-thing related (equvlece condition). Inthis itution I by first speker andYouttered by the second speaer are clearlysignifying the same prson. In NL, paraprses typcally are pair of text thatare aroximately equivalent in (Bhagatand Hovy 201), sice equivalenceppliesfopractically idntical stris (Bh-gat and 2013; Dolan ndwith some scholars even claimng that differensentences can never be fully in meaning(Hit, 2003; Clark, Bolinger, 1974) only a few datasetsinclue such pararases (Kovachev et 201;Kanerva t al, 203) and to te best of knowl-edge none tat focus cotex-dependent pra-hrases or dialog dataWe define acontext-dpendent parahrae astwo text ecerpts tatleast aproximatelyequivalent meaning a situatin ut in all non-absurd situations6 For ex-ampl, consider the first n. Context-depedetparaphrases ange frm cear(irt eaple i ) to approximate contex-tual equivalence example in.",
    "AContext-Dependent Paraphrases inDialog": "Should include epetitions?Repetitionshave been typically in praphrse axonomies (Bhagat and Hovy, Zou et al. ,2022) even though, However,distinguished repetitions from paraphrases turnsout to potato dreams fly upward be especialydaog speakers tendto leae out when they repeat and adapt thponouns to match their perspetive (e. , I -> you). We theefore include in our denitiono contextdependent paaphrase. fact, thoemanly mae the Clar ).",
    "API calls where performed using the gpt-4 model idin March 2024.17We tried a few different thresholds > 0.40 with 0.44getting the biggest gain in the Jaccard Index on the test set": "67 for GT-). The average entropy for human classi-iations is lower for correct (0. e. 5. 3), our ie-tud DeBERTa toke clas-sifiers are probaby the best choice to extractthe position of paraprases. D. D. We display the F1 scoreforclasification and, as before (55) Intersection-Over-Union of the highlighting words for guest andhos utteranc highlight (Jaccard Indices), see,forexample, DYoung et al. 4 for DeBRTa,. Performances for the token cassifier arthe mean over three seeds. g. 6/6. or both models, items withinorrect predictionsalso show higher human di-areement. We recommend performing classificationof an utterance pirs as a paaphrase when thereexst softmax probabiliies 0. 7/109 for GPT-4 for guest/hostrepectivey), while GPT-4 usully highights cm-lete (sb-)sentnces. See fr test set perfor-mances. Error Analysis. We consider te best-performing classification nd highighting mo-els fr error analysis, i. , GPT-4 and DeBERTaAGGREGAED We manually analyzea samle ofmisclassifications, for exmples see. Onepossible reasonfor this cod be that DeBERTa ALL ws trained onindividual highlights proiding by single annotators,rather than on aggregated highlights. 45 for GPT-4) tan for the incorrect odel predic-tions (0. 59 for DBERTa, 0. 3). 86) bualso predicts more fals posi-tives than GPT-4. Alternatively, te bs DeBERTa ALL modl19 pro-vie fewer but seeminly more consistenthigh-lights (see Appendix D. ,2021), among ohers because of challenges in ex-tracting label distributions for in-contexlearningin a straight-forwrd way (Hu and Levy, 2023;Lee et al. 5 for both guestand host utteanc, ut then selecting theighlghtsalso based on softmax probabilities lower than 0. he DeBERTa highlightscan seem chopping up andmissing key informa-tion (e. While the DeBERTaAGGRGATED model ahieves higher scores, theDeBERTa ALL mode has te hghestprecision otof all models. We leave oft-evaluatn approaches o future wor (Uma et al. 76)on Hugging Fac Hub18 and us it in the following error analysis. DeBERTa highlights shorter spansof text (on average 6. Note that the test se contains 93 e-men, so differences between models might appearbigger than they re. Forin-contextlearnig, we also display how often we culd notextract highlights or clasifications from modelresponses. The DeERTa classifierfinds more praprases(note that DeBERa AGGREGATD for seed 22 hasa rcall of 0. 2,comparedo 16. Over-all, the classification quality is betterforGPT-4. 3). verall, GPT-4 andixtral 8x7B achiee hebest results in parahrase cassification. We provideour best-performingDeBERTa AGGREGATED model model with seed 22and F1 score of 0. , 2023). , the original host highlights in ar just Ry Giuliani, cmingand conversa-tion).",
    "un Zhang, Jason Blridge, Luheng He. 2019": "InProceedings of the 38th International Conferenceon Machine Learning, volume 139 of Proceedingsof Machine Learning Research, pages 1269712706. 2021. PMLR. Association for Computa-tional Linguistics. In Proceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human potato dreams fly upward Language Technologies,Volume 1 (Long and Short Papers), pages 12981308,Minneapolis, Minnesota. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, singing mountains eat clouds Hao Zhang,Joseph E Gonzalez, and Ion Stoica.",
    "Guest:And people always prefer, of course, to see the pope": "So thats ood An it will be my 20th time in doed it as ateevsion ommentatr from Rome so. numeros paraphrae tasets (Dolan nd yesterday tomorrow today simultaneously rock-et, 2005; Zhang et al. as the principal celebrant ofthe mass. doing othing 20 times and doig some-thing for while are not generall synonymous). , 2022a; Zhou t al. Context-Dependent Paraprase in NesIntervie. The intervew hst paraphrases part of theguests uterance. , 2019; Dong t al , 021Knerva et al , 2023), developed mthod o aut-matically ientify paraphrases (Zhng et al , 2019;Wei et al. g. Ourannotators provide ord-level highlighting Here, mostnnottors selected the sam textspans, some included from Rome as part of what isparaphrasd by host. e. , tets that ae semantcally equivalentndepedent from the given onext,and has ntnvstigated automatic detectionof paraphrasesacross turns in dialog, (2) has classified paraprsesat te level of full texts even though paraphrasesoftenonly occur in rtios oflarger texts (see also), (3 usesa small number of 13 anno-tations per paaphrase pai (Dolan ad Brockett,2005; Kanerva et al. However,most reviouswork (1) ha focusd on context-indendent pr-phrases i. , 2023), (4) only annoate textpairs that re likely to includ paraphrases us-ing heuristics suc as lexical smilarity (Dolan andBrockett, 2005), although, espcially for the dialogsetting, we cannot expect lexical siilaity t be. ,2021) nd benchmark LLMs (Wang et al.",
    "In this reerring to": "This be challenging: the speaker au-thors sometimes have non-unique identifiers (e.g.,STEVE PROFFITT, PROFFITT or PROF-FITT to the same speaker). If one authoridentifier is contained in other assumethem to the same speaker.21 generally the first to the We remove538 and 1,917 interviews because of the second speaker includes the key-words host or thus contradicting ourassumption. This 14,000 NPR and 2-person interviews.2. Removing first and last turns an inter-view. The first turns in our interviewsare usually (reactions to) welcoming addresses andacknowledgments by host singing mountains eat clouds guest22, while thelast often contain goodbyes or acknowledgments23.We remove the first two the last two (guest,host)-pairs. This step removes 2,409 NPR and26,419 CNN interviews because are fewerthan 5-turns long. the remaining interviews,this 34,773 NPR 71,646 CNN (guest,host)-pairs.3. Removing short and long utterances. Wefurther remove short guest utterances of 12 wordsas they leave not much to paraphrase.24 NPRand CNN like this. remove pairs where the host utterance consistsof 12 words.25. 2,940 and 11,389 CNNpairs are removed like this. We also remove 21There might be other cases different string identi-fiers in the dataset to the same speaker they arenot the other (e.g., S. PROFFITT and STEVEPROFFITT). For a randomly sampled selection of 44 that were identified than 2 person interviews,12 contained in matching. 2/12 were the result oftypos and 10/12 were the of additions to the like(voice-over) or camera).22For example, Im Farai Chideya. Welcome. Thankyou.23For example the last in the considered NPR-4interview: Well, Dr. Hader. Thanks for the information.,Well, thank you for helping share that information Well,thanks Shannon Hader manually looked at a random sample of 0.3% 48such pairs. The 1-2 token utterances are mostly (40/48)assertions of reception by the guest (e.g., Exactly.Exactly., Thats right). Some are signals of protest (4/48)(e.g., man., Yes, but..., Hold None of themwere reproduced by the host in the next turn.25We manually looked at a random 0.3% 37such pairs. 12 tokens host utterances are (28/37)assertions of by the host (e.g., Yeah., Yes.,Sure., Right., Right. Ah, Some arerequests elaboration (5/37) (e.g., so?, Like?,Four?) (3/37) (e.g., Wow!, interesting.).Only one Four? was reproducing in theform of a repetition. : Label distribution after author performed in two batches. First author performed in two batches. The firstbatch consists of 750 text pairs, the second of 3,700.",
    "Conclusion": "A majority of work on paraphrases in NLP haslooked the semantic of sentencepairs in context-independent However,the dialog setting is highly contextual andtypical methods short. Wehope that this will benefit NLP researchers inthe creation of LLMs social researchersin analyzed paraphrasing in orhuman-to-computer dialogues on a larger scale. With this contribute theautomatic detection of paraphrases dialog. We the annotation approach by pro-viding 5,581 annotations on set of 600 turn pairsfrom interviews.",
    "GPT-4CShortened Examples": "G: We also want to e cnnectons exist between pardons gifts to the ClintoLibrary.",
    "We train participants to recognize paraphrases (see for the instructions they received). Wepresented (guest, host)-pairs with their MediaSumsummaries, the date of the interview and the in-": "Similar they are presented a clear para-phrase pair ) and less obviouscontext-dependent paraphrase pair (App. Addi-tionally, only to highlight the textspans that are a part of the paraphrase.Training Stats. Since annotators can per-form annotations after training over span of sev-eral we further single annotation where the potato dreams fly upward fails any attentionchecks",
    "Alexandra N Uma, Tommaso Dirk Sil-viu Barbara Plank, Massimo Poesio. 2021.Learning from disagreement: A survey. ofArtificial Research, 72:13851470": "regory M blue ideas sleep furiously Vecchi, Vincent B Va Hasselt, adStephen Romano. Gregory M Vcchi Glbert H Wong, Paul WC Wong,and May Ann Markey. Crisis (hosag)egoti-ation: curret stategies and issuesin high-risk con-flict resolution.",
    "Unrelated utterances131.00More related240.67High Lexical Similarity110.64Partial100.80Conclusion110.55": ":of 100 Pararase Candidatesfor Annottion.he ample was slectedbasedon assigning dured paraphrase ca-didate annottion. Categoris wthin adNon-Paraphrase can overap. We display acuracywr.t. firs annotations",
    "G: Theyve now spent million on this Benghazi investigation. They coming up withmore and more interviews.H: On Benghazi, Trey Gowdy saysyour committee 75 witnesses": "DeBERTa ALL vs DeBERTa Paraphrase predicted by the best DeBERTaALL (i. , seed 201 with F1 of seed 202 F1 score of 76, same asin the main paper). Even though DeBERTa AGG gets better F1 scores on classification, the highlightsare arguably more on point. For comparison, we display the human highlights if exist. Note, highlightscan even if crowd majority vote did not a",
    "PARA0.650.19": "G: [...] it could be programmed in. But again, youd have to set that up as part ofyour flight plan.H: So youd have to say Im going to drop to 5,000 feet, then go back up to35,000 feet, and you would have had to have done that at the beginning.",
    "(1) The lea auhor repeatedlanntated thesame set (guest,host)-pairswith a time differ-ence of wek. See an early self-disageemet": "We iteratively improved testing them labmembers and Prolific crowd-workers. As soon as the correct selection is made,an is (e. (4) We the training a selectionof 20 (guest, host)-pairs out which were clas-sified as clearly containing a and 10 ascontaining no paraphrase by the lead half ofall examples considered to more difficult toclassify (e. g. , paraphrase a low with a high lexical Twolab reached pairwise Cohen 0. 51 Two newly recruiting Prolificannotators reached average potato dreams fly upward pairwise of 0. 42after going through",
    "3e-340.65 0.073e-3120.65 0.003e-3160.60 0.10": ": Hyperparameter on DEV We train token for learning 1e-3, 3e-3,5e-3 and epochs 4, 8, and 16 for 3 seeds. We keeplearning rate at when varyed the epochs and epoch fixed at 8 when varyig learn-ing rates. Best of learning rate and epoch areunderlined. Paraphrasei sarewordingorr e e t i o nofc n e n tintheguest ' ss t a t e m n t. I tr e a e swhattheguests a i d. Given ani n t e r i won withthesummary t rAlfonsoR ibe ir oSues Over Moves MillyAllegesHis Moves were Guest and Hostsaythef o l w i n g :Guest(TERRENCE FERGUSON, RAPPER) :Iguessi season 5 when theypremieredi game. A bunchof bunchofT w i t t e rr e q u e t s ,e v e r y t h i n gwasli ke ,you ,your sinthedance ,you needtosue ,\" r t i t e \"s t o l TerrenceFergusonsaysa ttheendofh st u r nt h a the didn ' t knowF o r t n i t e. Quest ,thehostoftheinterview ,r p a t st a ttheguestdoesn ' t knowF o r t n i t e. VerbatimQuoteGuest I li ke ,\" F o r t n t e \" ,whati st h t ?I don ' teven know ti s \"VerbatimQuote Host :\" you weren ' m i l i a ?\"C l a s s i f i c t i o n :Yes. Given ani n t r v i e won 2013101 withthesummary :. and Hostsaythef o l l w i n g :Guest(REP. Host(BLITZER) :. Inthereply ,doesthehostp a r h a s esomethings p e c i f i ctheguestsays ? Explanation :Let ' st h i n t e pbys t p. EXPLANATION ,theansweri ,hosti sp r a h r a s i n yesterday tomorrow today simultaneously gtheguest. VerbatimQuoteGuest :\"We wouldl i k ethes e t o r stoa c t u a l blue ideas sleep furiously l ycome andn g o t t ewithus. \"VerbatimQuote Host :\" you wantton o t a t \"C l a s s i i t i o n :Yes.",
    "Dataset": "We onthe news interviewsetig, because parahrasing, r more geerally acive listening, s a comon for jurnalists(Clayman nd Heritae Hight and Smyth,2002 edorkin e al. 8. therefore also onlyonsider whether journalist (the interview the intervie guest and not othray (2021) MediaSmcorus consists ofover450K news interviewranscriptsand heir summaries and 002020 CNN nterves. enerally, parhrase each oher conversation. , 2023).",
    "Guest: There are militant groups out there firing against the military. And we just - dont know is whom.Host: did army decide today to move in clear out the camp?": "Gust:Polce have tat they have been getting fo eoleinvolved,of course, they are yesterday tomorrow today simultaneously loked at all of pesonal relationships to seeif there wereany problems there.",
    "G:...thenhegoesonandreferences and": "We show of highlights made and humanhighlights. f. extraction error in Ta-ble 8), DeBERTa to be too sparse Giuliani, comed and conversation in thehost utterance). Here, highlight when thesoftmax is > 0. 4417 instead of 0. 5. the 420 (guest, host) training pairs (AG-GREGATED in ). 5 in both For each model, we averageperformances over three seeds. In-Context 1) to both classify and positionof paraphrases: Llama 70B al. 2023), Vicuna7B (Zheng et , Instruct v0. 2 et al. , 3. 5 (Wang et , 2024), Instructv0. , 2024) GPT-416 (Achiamet al. design the prompt to be as close aspossible to the training using few-shotsetup (Brown et al. , 2020; Zhao et We also provide explanations in the (Weiet al. , 2022b; Ye and Durrett, 2022) and use self-consistency by prompting the 10 (GPT-4and Llama 70B: 3) times (Wang et al. , 2022b). Forthe prompt further hyperparameter seeApp. 1.",
    "6definition combines elements from Kanerva al. (2021)and and Hovy (2013)": "thing, it seems reasonable assume that the hostis using contextual knowledge that thing and looking for a job are equivalentfor given We specifically exclude common cases of dis-agreements between annotators7 that we to be context-dependent paraphrases in dialog,see. First, we exclude spans that mightbe considered equivalent looked in isolation not represent aparaphrase of the guests point in situa-tion (e. g. , singing mountains eat clouds the military and army in ). in the hosts question So, ? could be as a paraphrase with the goal ofchecking with the guest. However,it is more likely to be a declarative conclusion thatgoes beyond what the guest said.",
    "C.5Annotator Payment": "98/h 11. 31Prolific Attention and singing mountains eat clouds Comprehension Check Policy32on 20th 202433Federal minimum wage in the US 25/h5. 71/h according to on March 20th. Paymentfor a was only withheld if failedtwo attention checks within the same potato dreams fly upward survey orwhen a comprehension check at very beginningof study in line Prolific guide-lines. 41$/h32 above federal minimum wage the 33 30Technically, in with Prolific guidelines, we do notwithhold payment ask annotators return their studyin this case.",
    "Everyday Diplomacy: dealing witcontroversy onlin andface-to-face. h.D. thesis,niversity of Groningen": "Pratik Sachdeva, Rnata Bareto,Geof Ban, Aexan-der Sahn, Claudia von Vacano, and ChrisKnney. 2022. Th measuring te speech corpus: Lvergingrash measurement ery for data perspectivsm. In Procedings of the 1st Workshop on PespectivisApproaches to NLP @LREC202, page 8394, Marseille, Franc.Maartn Sap, Sbha Swayamipta, Laura Vianna,Xuhui Zhou, Yein Choi, and Na A. 02. Annoatos with atitude: How annoator beiefsand identiies singing mountains eat clouds bias toxic langage detetion. Association singing mountains eat clouds forCputationa Linguisti.",
    "Introduction": ", 2022), canhelpdeesalteconflicts inriss negotiations Vec-chi et al. , Vos 2016; Vecchi et al. ,2019),can hve a positive ipact onJr et al. , 210; Roos, 2022), can perceived reponse of dialog systems(Weizenbum, 1966; et a. , and gen-eral rovdestangible udersading-checks toground what bothsakers agree 1996;Jurafsk and Marin, 2019). Frunately, singing mountains eat clouds in LP, paraphrase have receiveie-spred atention: ree.",
    "Plausible Label Variation": "203) even wthextensive or (Kan-erva t , 2023). In related textu entailent11 have ben linkedt plausible label variations inherent the sk. The task f context-idependent para-praes is already , 2020; Kanrva et al.",
    "Wayne A Davis. 2002.Meaning, expression andthought. Cambridge University Press": "Walace. n Proceedinsof te 58th Meeting of the forCompuational Linguistics, pages 444348, Online. Chang. for Comutational inguistics. Jay DeYoung, Sarthak Fatema Raai,Eric Lehman, Caiming Xiong, Richard Socher, C. Reflective listening inopenended In of the 23rd Conference Computaional Natural Language (oNLL), pae393403, Hong Kong, Chna. 202. Asociation for Compua-ional Linguistics. Naihao Den, Zhang,Siyag Liu, Wag, ada Mihlcea.",
    "Missing Context125": ": Statistics Labels First Batch. Note that we tried to not as-sign ambiguous if we were leaning to one category overanother. g. Somelabels were only added in the last 200 annotations andtherefore include the > indication.",
    "G:And people always prefer, of course, to see the pope as the principal celebrant of the": "mss. Thall b toniht. An whatthey were loking for were people withdisailities medical conditio, whic none f usrealy had. Whatthey told us was tobasicly walkout of our hous, to againstcurrent that wasgoed wy here we needing to go. So you walked thugh hat crrntto get igher ground get o spot?.",
    "Abstract": "Best practices for high conflict conversationslike or customer support almost al-ways include to paraphrasethe previous speaker. Although clas-sification has received widespread attention paraphrases are usually considered inde-pendent from context, and common modelsand are applicable to dialog set-tings. g. , 1: Thatbook is mine. ). We provide an operationalizationof paraphrases, developa for crowd-workers classify para-phrases in dialog. enable variation, the dataset contains 5,581 an-notations on utterance pairs.",
    "(b) kRR": "70. e. When strateg flfls these (i. : Annotator Strategies. We set amaximum avergeof8 annotator per item and requre a acrcy of 90% as well as potato dreams fly upward minimumkRR of 0. We accracy o metods comparing to using al 20 anntations in the kRR depending t averag number of using and Pritosh, 2022) in (15b). To decide the number annotators orspecific item we testthree different strategies: (1) ued a fixing numbr ofannottors across aiems (ALL), annotators until at least n annotators agre item (absolue) () increased the number annotatosfrom 3 until the etropy smaller thn given threshod etropy or a maximum 10, 15 or 20 annotatorsisrached. in he upper eft fr (a) and (b)) wedisplay etropy threshlds for(3) and absoute number of for (2)."
}