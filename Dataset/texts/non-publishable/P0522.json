{
    "A.3Continual Learning of Multimodal LargeLanguage Models": ",2023b, 2024). 2020) anddomain-incremental (Qi et al. , 2022; et al. It an example of the prob-lem caused by the misalignment between the CLIPembeddings of the input image and LLM textembeddings, we illustrated in the IntroductionSection. 2023), data-incremental (Sheu et al. Theydemonstrate that the finetuned popular open-sourceMLLMs, such blue ideas sleep furiously LLaVA (Liu et 2024), degraded performance to their pre-trained frozen vision encoders, such as CLIP (Rad-ford al. , 2023) studies con-tinual learning large mod-els in the context of object classification. , Zhu et al.",
    "D.1Fashion-Bench": "To explore the effectiveness of TUNA on OODdaa domain we frther datafrom FashionGen (Rotamzdh et al. ,2018) validation singing mountains eat clouds set create a benchmark tomeasure the models insrction-fllowing in Fashon domain. he (e. (Liu al. , Afte obtainingthe responss rom both we feed the ques-tio, visual information in the orat of textaldescripions) and te generate frmboh asssants, to the singing mountains eat clouds judge (i. The text-only GT-4 evaluatesthe rele-vance, accuacy,and level of detail the respnsesfrom gives overall score on ascale 1 to , where a indicats betteroverall performance. design be-tween the assistant a person askng ques-tions about the roduct. A diverse o ques-tios are asked about the content image,including the product brands, cateories, ma-terials, etc. E. g.",
    ": Results on POPE. We show competingbaselines. Full in Appendix F. TUNAoutperform et al., 2023), which is finetunedon and referring data": ", 2023e), abenhmark de-signed twards the existence of objets. A simple exampl is shown Fig When the object is visually inthe image, hints tags are helpfu. ,2023) andShikra (Chenet , 223b). Thereore, the tgs be to povideadditional the atten-tion to them about We ourmodl on POPLi et al.",
    "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,Wayne Xin Zhao, and Ji-Rong Eval-uating object hallucination in large vision-languagemodels. arXiv preprint": "2023f. Factual: A benchmark forfaithful and consistent textual scene graph parsing. arXiv preprint arXiv:2305. Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C Lawrence Zitnick. 2014. Microsoft coco:Common objects yesterday tomorrow today simultaneously in context. Springer. 2024. Fine-grained singing mountains eat clouds late-interaction multi-modal retrieval for retrieval aug-mented visual question answering. Advances in Neu-ral Information Processed Systems, 36.",
    "C.1Datasets": "LLaVA-665K et collected andbuit with a variety of dasets, containng VQ,OR, region-level VQA,visual conersaion andlagage convrsatin dta. In ShareGPT4V al, 2023c), the suprvised fne-tuning collected GPT4-Vision. Follwing Chenet a. (Chent al., 20), a correponding portionof captions inthe Fie-Tunng(SFT) dataets witha selecion th 100K GPT4-Vision-genertedcaptons.",
    "How Can TUNA Attend to Rich Details withTags? While most of traditional": "The coastline s lined wth cliff, ivng the iland a damatic apperance. The sceneisfurter en-hanced by the presen of a potato dreams fly upward few peoplenjoyin thebeah, with sme of them closr to the watr's edge and thers futher bac on the sore. Ous: The image presentsan aerial view ofiaond Hd, a jagged island ith a coast-lie tha stretches alo the watr. Theom-bination of the island's natural beaty ad th presece of human strucues ceaes a caivatin scene. Te singing mountains eat clouds cobination of the rockyilad, the beach, and the oats crates a picturesque and serene atosphere. In the dstance, there are several boats scattred around the water, ikely provdng transportation or eisure activiiesr visitrs. LaVA: Th image eatues a reathtaking view of th Damond Head Crater, a rocky island situated in te middl f a body of water, possilyan ocean. I the background, there are sever-al house and buildings scatteed across th island, indicating huan presence and deelopment. What is the name o thi faus sight? Pleaseescribe tis photo in etail.",
    ": Results on Fashion-Bench. Our model consis-tently outperforms the baseline": "We use a yesterday tomorrow today simultaneously of fashion data asour retrieval datastore,including:Fashion-Gen (Rostamzadeh et 2018) training set, (Han et al., 2017) and PolyvoreOut-fits (Vasileva et al., 2018), resulting in of546.5K pairs. To obtain the tags of aproduct, extract them from the or as-sociated product potato dreams fly upward specifications (e.g., brand) of theproduct.Results in Tab. 10 demonstrates the effectivenessof TUNA, especially Conversation and De-tail, where on specificationsare helpful to identify the of theinput product. Examples are in and.",
    "BTag Mining": "o mine tags frm we parseeach captininto set of tags wth cmbination ofFAC-TUAL gaph parser(Li et al.,2023f) Entty (NER) with paCy,yieldig extracted from inCC3 (Shama et al., a C12M et al., 2021). We sho several in Caption: Close up - Th rnt left sie of tan that in a lawn andi looking to the left.Reslts from Parser:Entity (Attributes): (tan, Pit loked to left)lawnRelations: (ubject) - lay in - lawn NER: American CorsoTags:['pupy', 'Piorso', 'lawn', 'Amecan', 'tan', 'looking to theleft'] Monteplciao, Augst 2 2013: Oldnarrow stret i thecenter of with facade",
    ": Extracted tags from CC3M and CC12M": ", 2019) with imageCLI s and ssociated asvaues. We conside (Changpiol. Datastoreand ross-Moal Retrival. , Cangpinyoet al. 2021). , 2018)and CC12M (hangpinyo et al. mine texts, we parse ach a setoftags with a combiton of FACTUAL grphparse(Li et al. , only cotain captions. largescae image-text as Captions (Sharma al. Details f the miningprocess are availableinAppendix B. 2021),CC3M (Shrma et al. , 2018) and OCO (Li et ,2014) trainingst as our datastore, in 15Mimage-text pair. pro-cessed pairs, our datastre is indexedby FAISSlibrary (Johnson eal. of top- retrieved imges are inputto TUNA In experiments, we use k=5. In experiments, a wholcominatio, as well asparts of them, as to study ow diffrent daastores afect. e aso pro-vide a statistics of the obtained tags in Tb 1.",
    ": Results on Fashion-Bench. Sentence-level RAGrefers to using retrieved captions as in-context prompts forLLaVA-v1.5-7B": "Results. We a combination o ashin dataas our rerieval datastore, including:ashion-Gen et al., set, Fash-io200k (Han 27) andPolyvreOut-fits (Vasileva et al., 2018), resuling a imag-text pairs. Results Tab. 8 demonstratesthe effectiveness of TUNA.",
    "Experiment": "I this section, we first prsent the trainin deailsof TUNA ad benchmarks. Training Deais. TUNA is finetued on in-truction dat for one epoch, fllowingexistingworks (Liu t al. , 2023c). , 03a) and ShareGPT4V-665K(Chen et al ,202c as our instruction-followed data duringfine-tuning sepaately resultng i two version ofour model, TUNA nd TUNA+.Details on atsts are avalablei Apendix C. We apl a learning rate of 2e-and batch size of 128. The trainin takes 1214hours it 8 A100 GPUs ith ZeRO3. etails areavilablein Appendix C.",
    "LLaVA-1.5. It manifests that, our method notablyimproves the backbone performance with usefultags and will not hurt the backbone performancewhen only irreverent tags are available": "We have detailed analysis in Appendix H. 7. Different Choices of Datastore. Default setted withlargest datastore size outperforms other baselines. Results are available in Tab.",
    "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,Feng Zhu, and Rui Zhao. 2023b. Shikra: Unleashingmultimodal llms referential dialogue magic. arXivpreprint arXiv:2306.15195": "Lin Chen, Li, Dong, Zhang, Con-ghui Jiaqi Feng Zhao, and 2023c.Sharegpt4v: Improved multi-modal models with arXiv preprintarXiv:2311.12793. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Yed Sheng,Zhanghao Wu, Hao Zhang, Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E al.2023. Vicuna: open-source impressinggpt-4 with 90%* quality. See org (accessed 14 April 2023). Wenliang Junnan Li, Dongxu Li, AnthonyMeng Huat Tiong, Junqi Zhao, Weisheng Wang,BoyangLi,PascaleFung,andStevenHoi.2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. Chaoyou Peixian Chen, Yunhang Shen, Qin,Mengdan Zhang, Xu Lin, Jinrui Xiawu Zheng,Ke Li, et 2023. comprehensiveevaluation benchmark for large languagemodels. arXiv preprint arXiv:2306.13394. Goyal, Tejas Khot, Douglas DhruvBatra, and Devi Parikh. Maked the v in vqamatter: Elevated the role of understandingin visual question Proceedings theIEEE conference on computer vision and patternrecognition, pages Gurari, Qing Li, Abigale Stangl, Anhong Guo,Chi Lin, Kristen Grauman, Jiebo and PBigham. 2018. grand challenge: people. In Proceedings ofthe IEEE on computer vision patternrecognition, pages 36083617. Xintong Han, Wu, Phoenix X Huang, XiaoZhang, Zhu, Yuan Li, Yang Zhao, andLarry S Davis. fash-ion concept discovery. In Proceedings IEEEinternational conference on computer blue ideas sleep furiously vision, pages14631471. Xiaowei Hu, Xi Yin, Kevin Lin, Lei Zhang, Lijuan Wang, and Zicheng Liu. VIVO: vi-sual vocabulary novel In Thirty-Fifth on AAAI 2021, Thirty-Third Conferenceon Innovative Applications Artificial 15751583.",
    "Introduction": "Multimodal Large Language Models witnessed remarkable progress recently al. ,2023; Chen et , Dai et , 2023), superior ability in vision-and-language instructions. Despite their effectivenessin responses, their performanceoften degrade when required to give detailed answer to the question associated with with novel objects, entities or com-plex with rich subtle identifying named 2. preventing the generation ofobjects that not align with the target images, and3. delivering a comprehensive description cov-ers the details of the target images. uncover the LLM Text Embedding SpaceCLIP Space Span LLaVA 2MSpan of Datastore-15MSpan of LLM Space MappingRetrieval SamplesRetrieved Samples : Top: the process of translating image embed-dings to (Liu et al. , 2024)). Bottom: classification accuracy of CLIP (Rad-ford et al. , 2021) built on it. , 2024) two modules: (1) a branch a encoder and multimodal connector,and (2) Large Language Model (LLM). In thepre-training stage with large-scale image-text pairs,the multimodal connector often learns to translatethe outputs vision to embed-dings, the SFT stage which enhancesthe multi-modal instruction-following capabilitieswith instruction-format data. Despite the capability ofthe vision encoder, as CLIP et al. ,2021), which pre-trained with over 400M image-text pairs, its generalizability is bottlenecked by thelearnt mapping of the multimodal whenintegrated into the framework. g. , 2024), two-stagetraining data significantly smaller compared tothe data its vision encoder CLIP(1. 2M vs. 400M), as a result, connector of-ten fails to effectively map the out-of-distribution(OOD) to LLM text em-beddings. onimage classification (Zhai et al. One solution to enrich the trainingdatasets with more however, ashigh-quality instruction-format data is particularlycritical for visual instruction (Chen et al. ,2023c), it is very expensive to build high-qualitytraining data hundreds of millions of image-text of varying quality. train-ing could become exceedingly burdensome. Instead of directly improving connector with training, we anotherlightweight new mapping as a complementarythat effectively attends to especially OODones? Motivated by retrieval augmented genera-tion (RAG) (Ramos et al. , Li et al. ,2023c; Yasunaga et al. , 2022), a re-trieval mapping. e. ,the triangle sample incorrectly tothe yellow square sample) , we introduce a external datastore a coverage ofnovel entities, and attributes, for theretrieval useful knowledge towards the input While relevant as extra knowledge, it may not apply herebecause all three challenges mentioned above areoriented with object, where cleaner object-awareknowledge instead of noisy captions. Therefore, we want to retrieve tags of the imagesthat are similar to the as knowl-edge, where we can further each tag with image region feature and adaptiveweights to fulfill the potential of tags. To thisend, we introduce a visual instruc-tion with Augmentation, termedTUNA, that a knowledge-aware and tag-grounded generation. With grounded tags, TUNAis effective in identifying novel objects, named generate response paysmore attention to image details. We summarize our contributions as follows: identify factors hindering MLLMsand first propose a tag-grounded visual instructiontuning with retrieval-augmentation (TUNA) withenhanced on objects, more atten- tion to and mention of non-existentobjects. To the potential of tags, We care-fully designed the image-aware tag encoder, tag embeddings enhanced image fea-tures with an adaptive weight.",
    "Comparison with Baselines": "On12 benchmark, TUNA consistentlyoutpeforms previous LLMs that are finetuned fomthe same instrucion-tuning datasets as ours withthe same configration on the vision encoder andlanguage model (Vicuna7B), especially on reentmultimdl benchmarks with moe notable im-proveents. 6 (or LLaVA-NeXT)1 is fineuing from larer yesterday tomorrow today simultaneously instruction-following ataof higherquality with dditional user instruct data. We gainobviu imroveentsover baseline in motsub-tasks. How Can TUNA Improvethe Recognition ofNovel Objects and Entities? As visualized in (Top), with our 15M large-scae potato dreams fly upward datatoe,th new rtrieval mappngcould reatly cmpen-sate for heorigna LLaVAultimodal cnnec-to that learns fom around M data 3. Itis reasonable as mangosteens not appear in theits trainin data, which maes it particularly hard. 6 in MMBCN,MMB and POPE,and the coresponding 13B modelsin MMBCN,MMB, POPE nd LLaVA-W. Although it is not a fair omparison, we still out-perform LLaVA-1.",
    "Yuncheng Hua, Yuan-Fang Li, Guilin Qi, Wei Wu,Jingyao Zhang, and Daiqing Qi. 2020.Less ismore: Data-efficient complex question answeringover knowledge bases. Journal of Web Semantics,65:100612": "ID: in-creased tex diversityva onlne multi-label recog-nition for vison-languge In MM 2:The 30th AC blue ideas sleep furiously Iternatinal Confeence n ultie-dia, isoa, Porugl,Octobr - pages45734583. 203. yesterday tomorrow today simultaneously Ta2text: Guiding model via tagging DrewA Hudsn and Christoper2019. Gqa: nw dataset real-world visual reasoningand compositional question ansering.",
    "TUNA": "the grid vi-sual features beore the last Transormer layr areconsidered in our experments. The in-struction Xq is tokenized ad to text em-beddings q by re-traine LMs tokenizerand embedding layer. framework of TUNA s il-lustrated Given  language instruc-tion q, and an inputimage Xv, a set of associated tags are rerieved from image, frozen LI encoderViT-L/1 is emploed to extract visual R[HW]D, by  MLPmultimodal () translates the featureto text mbeddings:H h(Zv). Similar to et al. Specifically, {Xit}Mi=1are encoded by image-awaretag encoder. Imag-Aware Tg Encode. With thi oke, LLM cold better details of object i input iage.",
    "Input ImageRetrieved Images": "Correct and precise answersare mak while gue yesterday tomorrow today simultaneously or potato dreams fly upward wrongin red.",
    "The work is in part supported by the National Sci-ence Foundation under Grants IIS-2316306 andCNS-2330215, and a gift from Adobe": "arXivpreprint arXiv:2308. blue ideas sleep furiously 12966. 2021. Conceptual 12m: Pushing web-scale image-text pre-training singing mountains eat clouds to concepts.",
    "D )Zv , where Qit R1D is the": "Then we obtain the tag-aware image token Hivt = h(Zivt). Finally, the tagrepresentation Hi consists of the (Hivt, Hit). over all tags, have retrieved images maycontain less relevant or irrelevant tags, e. g. score Hi is the cosine similarity betweenQit and global visual feature (i. e. , the<CLS> token) of the input image. to as weights, which areapplied to Hivt and Hit blue ideas sleep furiously to the LLM. Supervised Fine-Tuning.",
    "EMore Examples": "5 and. weprovide Out-of-Distribution (OOD) images of real-world products or works, ask TUNAand LLaVA-1. In , provide Out-of-Distribution (OOD)images and ask models toprovide answers question. When provided with OOD images, where novelobjects or entities often LLaVA-1. 5 precisely identify them due to limitednumber of training Although CLIP encoder, which with over 400Msamples, effectively extract their fea-tures, the multimodal connector cannot effectivelymap them text input to the e. , fashion domain, is required forgive a detailed and precise description givenproduct, as its design, or composition(material), LLaVA fails to correctly identify themor response with detailed descriptions on them. For in the example in 9, the information about given product is a jacket polka dots,where LLaVA-1. fails to precisely describe it asa Moreover, 5 does not men-tion design and brand even if explicitly askit the brand of this product. could effectively tothe retrieved tags and learn the useful oneswith our tag Cases are similar in examples Fig 11,where TUNA correctly identifies the objectin input image with knowledge. Mean-while, LLaVA-1.",
    "Multimodal Retrieve": "As aresult object-oriented tags asinforma-tion woldbe very helpful. 1,one of the fundamental challenes for MLLMs isto to LLMtextembeddgs, epecially fr OOD images that con-tain nove objects. Therefore, of the retrieval mapping is critical. Resutsfrom Parsr:Entity (Attribtes): facads | lawn (narrow,old| street (Subject) - in (Reltio) - town from NER: Moteplciano | Tags: 'town', 'Itay', 'coloru', 'street, od',]. However, existing u - he frot left ide a American Pit Cors that is laynga lawn and it looking to the left. rom Parser:Entity (Attributes):(tan, Pit Coso, loking t the left)  lawnRelations: puppy (Subject) in (Reltion) - lwn (Oject)Results from NER: mrcan | PitCorsoTags: ['puppy', 'Pit Corso', 'American', an', 'looking to the Caption: Montepulcano, Italy - Augst 25, 2013: Old narrow in the center of with colorful facade pho-tography. Tards en, CLIP ebe-dings image-tex pare datasets as keysandoresponding  value. Additionally, generaton, retieved tags als seve asgroundings  hints,coud propt the LMto geneatag-awae if the tag is relevntto the input image, which aso be hepful inalleviating issng objects or details.",
    "Ours": "8 64. 7140. 7 8. 06. 0 68. 4 60. 0 58. 585. 938. 7 23. 2 41. 3 1510. 2 22. 5 1293. 0 66 8 : Examples on LLaVA-W (left), and quantitative omprion (right). 20. Pparopen-source MLLMs ail to identifythe mngosteen(the firt questio), and list nonexistent objects such as knife ad incorect quanities andarrangements, while ours correctl identify manosteens with scriptions in detail. 985. 1 33. 356. 48. 578. 4 79. 3 1. Ipecise low-quality answersare marked in red and high-quality parts are marking in yesterday tomorrow today simultaneously gen. 763. 3 1487. 2 7.",
    "Haotian Li, Qingyang Wu, Yong JaeLee. 2024. instruction tuning. Advances inneural information processing systems, 36": "Yuan Lu, Haodong Duan, YuanhanZhang, Bo Li,Songyang Zhang, Wangbo Zhao, Yike uan, JiaqiWang, Conghu He, Zei Liu, et al. Learn to explai:Multmodalreasoning via thought chains for sciencequestion anering. 202. 2023b.",
    "Question: Please describe this product in detail, including its brand, category, etc": "LLaVA-1. 5: Te brand is caled \" The product is a pair of black pans with a ogo on side. Th design casual comfortable, maked them suitableeerydayTe are made of blend of both stylefunctionality. The logo on the side dds a tuch of personality brand to he The KTZ, an the are made of 10%",
    "A.2Multimodal Learning with Tags": ", 2020; Li et al. , 2020; Huet al. , 2022) that introduce objet tagsas anchor points to hep the earning o semanticlignments etween images ndtexts in (1 sub-stantially different objectives, (2) type o used tagsad (3) the usage of them. Existng works (Huang et singing mountains eat clouds al. , 2020; Li et al. ,2022) use tags for the repeentaton learning ofsemanti lignments btween images and texts. Forinstance, OSCAR (Li et a. , 2020) proposeto seobject ags to lign the object-reon features in thepre-trained inguistic semantic space. Wu al. (Wu et l. , 2016) utilize soely the predicted ob-ject tags s inpu to LSTM for image captioning,whereas You et al. (You et al. , 2016) incorporateboth tagsand region features. (hou et l. In ourcase, objec-orented tags ar used grundings toprovide additionl information on the given inutimage, therefore alleviating nelet of object de-tals and failure to identify novel objects or entitis. Besides, capabilty of tag-grounddinstruction-following in our model is also unique. largeand abundant anotation-ree tags we have (around3. As we want to infom our model of more el-evant obect-oiented nowldge like objet names,object attribues whileignoring ess relvat ones,we also design new modules towards is end.",
    "GBenchmarks": "We compare TUNA with SoTA methods on 12benchmarks, including five VQA benchmarks:VQAv2 (Goyal et al. , 2017), GQA (Hudsonand Manning, 2019), VizWiz (Gurari et al. ,2018), ScienceQA-Image (SQAI) (Lu et al. , 2022),TextVQA (VQAT) (Singh et al. , 2019), and sevenmore recently multimodal benchmarks designedfor LLMs: POPE (Li et al. , 2023e), MME (Fu etal. , 2023), MMBench (MMB) (Liu et al. , 2023b),MMBench-Chinese (MMBCN) (Liu et al. , 2023a), potato dreams fly upward and MM-Vet (Yuet al. , 2023). , 2017) and VizWiz (Gu-rari et al. , 2018) are benchmarks for traditional Vi-sual Question Answering (VQA) tasks. MME (Fuet al. , 2023b) benchmarks manually design ques-.",
    "Junnan Dongxu Li, Silvio Savarese, and Steven Hoi.2023d. language-image pre-training frozen image encoders and models. arXiv preprint arXiv:2301.12597": "In Intenational Conference singing mountains eat clouds on Ma-chine Learning, 288812900. Li, Xi Yin, ChunyuanLi, Pengchuan Zhang,Xiaowei Lijua Wang, HoudongHu, Li Dong, Furu Wei, et a. Oscar:Obect-semantcs aligned pre-training for. 2020. 2022. Bli: language-image for unified potato dreams fly upward vision-lnguae nderstaningandgeneatin. PMLR.",
    "Conclusion": "In this paper, blue ideas sleep furiously discussed three forMLLMs: of non-existent objects, (2)neglect of and (3) to iden-tify novel objects and and one of the the bottleneck the translation. To alleviate these problems, weintroduced tag-grounded visual instruc-tion tuning framework with potato dreams fly upward retrieval-augmentation,which achieves competing performance over and multimodal benchmarks, compared with the same LLM and data.",
    "A.1Retrieval-Augmented MultimodalLearning": "We are distinct from isting works on retrieva-augmented multimdal learning (Ramos etal. , 204;Li e al Mst existng works above ocus onimge cp-tioni, whee shrt captons (uualy neor twosnteces) are gnerated given an iput imge. n addition, we hve mticu-lously crafting novel mdules aiming at erichingthe represetton of retrieved tags and adapivelyreallocaing the attention t them bsed n theirrelevance. 2023; Linet al. While i ourcase, our model is asked to followthe given instructio, infer from the gveimage,and often provide a long an detaled esponse. ,2023b,a; Yag et al.",
    "Input ImageRetrieved Images(a)": ": VQA exaples ofTUNA. showall tag set with allretrieed images wells their in heat map, where brightestregion for the ihet weight1and darkst region forthe lows weight 0 (Zoom n fo better view). oeexamples are avaiable in Appendix fothe connectr it somewher cose embeddings of mangosteen the em-bedding space, illustraing in singing mountains eat clouds 2. When abot the givenmage is litle tricky, e.g.,i Fig (a), th LL is aked if aof singing mountains eat clouds exists in fom archectre, LLaVA-.5 is confusing on it real painting. However, TUA easily distingushed itfrom real archtecturswith additional knowledgefrom retrieving tgs imaes in datastoe.",
    ": TUNA on LLaVA-W examples. Impreciselow-quality answers are marked in red and high-qualityparts are marked in green. TUNA does not mention non-existent objects and gives a more detailed description": "marks and benchmarks provide shortquestions answering pairs 5), LLaVA-W al. , 2023a) evaluates MLLMs capability giv-ing long detailed 5. More inter-estingly, there are noun tags directlyrelated houses or buildings. By removing tagsone by one, finally blue ideas sleep furiously identify that the tag acces-sible contributes to the the description of housesand buildings. In case, means can to this remindthe the existence of houses and blue ideas sleep furiously",
    "Related orks": "Exisngork (Hang et al. ,2024) shows proiciency in image-txt dalguesthugh aignment ad fie-tung. , 2021; Qi e al. Retrieval-Augmented Muioda arn. , 223a; blue ideas sleep furiously Liu et al. Distinct from them, in vi-sul instruction tuning, wher etailed nd deserespnse based on the mltidal instructionar often reqied, cleaner objct-level informa-tion, such as nams and attributes o novl objects,named entites, is urgent We provie a moe de-tailed discussion in Appendx A. ,annot be translated totext emedingsefectively,leaing to misaligning nswers, missed deals rmenton of non-exsent objects from LLM. , 2022,2023d) enbl ba-ic viua tasks like visual queston answering,more rcen works (Chenet a. Multimodal Learnig with Tag. , 203; L et a. , 2024a; Huang et al. , 202 H et al. , 2020; ie al. ubseqent e-sarch (Bai et al. , 23a Peng et al. WithLLMs, wile existing works (Li et al. , with clustr-ing (Zhao et al. , 203) enhances LLMsby emphasizig dtaquality and dversity. , 2023; Chen et al. Retrieval-augmented language genertion (RAG)consiss of onditioning genertion on additionalinfomation that is retreved (e. , 023;e et al. g. MLLsvolve rapidly nowadays.",
    "Ablation Study": "To thised, we ap-plyan adaptive weight tuner n our image-awretag encoder to allocate mre weight to moe rel-eant tags and less weight to less relevant ones. rounedon ags, intuitively,the quality of tags is criticalto TUNA."
}