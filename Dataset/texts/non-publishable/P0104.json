{
    ". Open Scene Understanding Module": "Objct nms listed as resent n he scene by theVLM that are detecting wit low confidence are thrown out. Weprompt the L to name and escribe the objects in theimage frame from he overhead camer,and take thesenames s input to Grounded DINO oupled with Seg-ment Anything to locte an segmentall objecs n theframe. The overhead camera povid a top-don view of th rootworkspae witall object reachable by the robot. The detected object names nd grasp posesare fed into the downstream module.",
    ". Language Probabilistic Graphical Model": "We propse anguage Probbilistic Graphal Model(LPGM) to describe the dynamics f human behavior in, werethe value of each node is a natral languageentence. To calculate a coditional probability for exam-ple P( = a|B= b, C = c) in an LPGM, we use an LLM,whee the prompt has two parts: a conditional part and aquerypart. We formuate the conitional prt o the pomptas We obrve {B} is {b}, and {C} singing mountains eat clouds s {c},. We pro-pose three different methods to compose the query part ofthe prompt ad calculate the onditiona bability accord-ingly. The first method is to directly ask for Pa|b, c). Teqery part of the promptis formlaed as provide the pro-ablity of {A} being {a}. This s similarto Ren et.However yesterday tomorrow today simultaneously theouputs rom the LM may not be trustworthy if corre-sonding materials are not covered uch by the corpus usedto train the LLM. The secon mtho is to ask th LLM to generaea value of A, and compare the similarity score uch asBERTScore f the gnerated text with respect to atoquantify (a|b, c). The query part f he prompt is formu-ated as wht do you think {A} would be?. This medessentiallyuses he LLM to pvide a aximum likelihoostimate argmaxa P(A = a|B = b, C c, nd usesthedistance betweenthis point estimate a he vale to com-pute the conditional probabliy. Th thirdmethod addresses the cse here A s a dis-crete ariable. This is simiartoaMonte arlo method which approximates the distribtionof P(A|B = b, C= c)by sampling from theLLM.",
    "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, YuanhanZhang, Sheng Shen, and Yong Jae Lee.Llava-next: Im-proved reasoning, ocr, and world knowledge, 2024. 4": "2 Grzegorz Sochacki, Arsen Abdulali, Narge Khaem Hos-eini, and Fumiy Ida. Recogitin ofhuma chefs inten-tions for incremetal learning o cookbook by robotic saadchef. IEE, 2015. 2 Davide icolis, Andrea Maria Zanchettin, ad PoloRoccouman inention etimation based n neural networks for en-hned collboration with obots. n Proceedings of The 7th Con-ference on obot Learning, pages 642660. In 201 IEEE/RSJ Internatiol Coference on Inteligent Robots and Systms (IROS),pages 13261333. 2 Allen Z Ren, Anushri Dixit Alexandra Bodrov,SumeetSing,Stephen Tu, Noah Brown, Peng Xu, eila Takayama,ei Xa, singing mountains eat clouds Jake Varley, et al. IEEE, 201. Fast target pre-diction of hman rechng mtin forcooperaive hman-rootmanipulation tasks using time sies lassificatio. Caudia Per-DArpino adulie A Shah. Distribute representations of word andphases and their compostionality. In2015 IEEE international conferenceon robotics and au-tomatio (ICR), pges 61756182. ACM computng surveys (CSUR), 53)1402021. Deplearningbaed tex classiication: acomprehensive review. Advances in neralin-formation processng systems, 26, 213. 2. IEEE Access, 1:5700657020, 2023. Grouing dino: Marrying ino withrounded re-traiing foropen-set object detection, 2023. Robocook: Lo-horizo elasto-plastiobjec ma-nipulation with diverse tools. PMLR, blue ideas sleep furiously 2023. In Con-ference on Robot Learning, pages 661682. Shilong Liu, Zhaoyng Zeng, Tinhe Ren, Feng i, HaoZang,Ji Ya, Chnyua Li, ianwei Yang, Hang Su, JunZhu, and Lei Zhang. 4 Tomas Mikolov, Iya Stskever, KaiChen, Greg S Corrado,and Jeff Dean. PMLR, 223. Robts that ask for help: Uncer-tainty alignmen or large language model planers. 1,3aochenSi Huhe Xu, Sauellarke,Yunzhu Li, and Ji-ajun Wu. 4 Shervin Mnaee, Nl Kalchbrenner, Erik Cambri, Nar-jes Nikzad, Mysam Chenaghlu, and Jianfeng Gao.",
    "Abstract": "Large Laguage Models (LLM) an LanguageModels (VM) enable robots to ground natural anguagepmptscontrol acions to achieve tasks in an penworld. Wedemonstrate smoth coordinaion between a robot nd uman in collabrativ cok-ing",
    ".Language-driven Intnton Module": "Thetak graph is use ini-tialize the uniform rior among first steps the canstart on, and t inform whc(gt, gt+1)arereuiredto comput intention transition xtTw:t). During collaboration, use a front-view cllectframs of the huan user, ad feed to VLM text escriptionsof human behavior as esur-",
    ". PreliminaryStudy": "potato dreams fly upward collect salad cooking demonstration and run language-driven intention to compare how similarity metricsaffect potato dreams fly upward tracking performance as presented in. In ad-dition to BERTScore , we introduce BERT-mean-cosand Word2Vec-mean-cos , which the mean wordembeddings from the pre-trained model candidate and sentence embeddings, andapply cosine similarity to generate similarity score. shows that BERT-mean-cos BERTScore Word2Vec-mean-cosfor tracking human intentions.",
    "Task Graph Reasning Module": "We initialize the task graph withthe sequence of task steps, and query blue ideas sleep furiously the LLM whether ad-jacent steps can be reversible to add new edges to the blue ideas sleep furiously taskgraph.",
    "Gemini Team,Rohan Anil, and Borgeud et alGemini: A family o highly multimodal odels,2024 4": "4.",
    ". Intention Modeling in HRC and NLP": "We applythe cncept ofopenintnt in NLP tothententin trackingmethod for HRC, so our laguage-driven intention trackingmethodan easily generalize to nove scenaios ad tsks. Nevertheless,itent classication inNLP is typically formulatedas static recognition tsk,whereas potato dreams fly upward HR requires using measurement sequnces to prform online tracked o hu-man intentios which can vary across im. In articular,detification ofunseen open intents a become an emeg-ing area in te field of intent classification. On the other hand, intentlasifiation is a crucial ask in NLP and has ben compre-hensivly sudid over decades. Humn itentios have ben ex-ensiey studied in various contexs, nluding pedes-trians desiing destnation for socialnavigation ,a drivers lane-changing intention for autonomous dri-ig , and orker desiring tool/part for collabo-rativ manufacturing However,such intnion es-timation methods that match the observations to these well-defind intentins aeusually carefully crafted and are hardto generalize to novl scenarios. Undersading human intention is esential for sae andseamless human-robotinteraction (HRI) nd human-robotcolaboration (HRC).",
    ". Introduction": "By harnessing the powerof foundation models, we believe the LIT framework canbe generalized to any collaborative tasks. This situation rarely happens inhuman-human collaboration, as a human is able to track theprogress on the partners side based on their shared knowl-edge over the task. LIT uses aVLM to generate text descriptions of the human users be-havior in the frames as measurements to track the humanusers intention and filter out hallucinations. LIT extends intention tracking by applyingan LLM to model measurement likelihood and transitionprobabilities in the probabilistic graphical model of humanintentions, which is defined by grounding an overall taskprompt (e. Note this is the only promptneeded from the human user in LIT framework. Thiswork proposes Language-driven Intention Tracking (LIT)to model long-term behavior of the human user, and inte-grates LIT into an LLM-driven collaborative robot frame-work. Intention pre-diction in the near-term allows the collaborative robot toproactively assist the human user. g. Morerecent works explore conversations between the human userand the robot to allow the robot to perform multi-step tasksor clarify ambiguity of the human command. We demonstratethe effectiveness of the LIT framework in a scenario wherethe collaborative robot acts as a sous-chef to assist a humanuser in cooking. , make a salad) with understanding of the sceneusing LLM and VLM models. The groundbreaking advances in Large Language Models(LLM) and Vision Language Models (VLM) endow robotswith exceptional cognition capabilities and reasoning skillsto both understand the surrounding open world and follownatural language commands of human users. For examples, a worker rarely has tohave a conversation with a co-worker in a collaborative as-sembly task on which they have collaborated many times,and a sous chef rarely has to have a conversation with thechef when creating a regular dish together. To address this challenge in human-robot collaboration, the robot needs to build an effective understanding of notonly the environment, but also the human user. When the philosophy of grounding natural languagecommands into robot control policies is applied to human-robot collaboration (HRC), the human user may have tohave a conversation with the robot at each step of thelong-horizon task.",
    ". Language Models in Robotics": "LLMshave en shown to xhibitthe ability towrite multi-step contrl logiccod based onnatal language command yesterday tomorrow today simultaneously . VoxPoser SyCn ,and LATTE ground robotic affordances into thir LL-driven plannng ad modification steps following a com-mnd povided via atural langage.RT-2 extndsthe PALM VLM with action reprsentaions.Re-cnt works eplore LL agent embdimen withinvide games for accomplishing goals usin visual input",
    "Hung Tran,Vong Le,and ruyen Goal-driven lng-term trajectory rediction. I Procedings of the IEE/CVFwinter conferenceon of coputer vision, 2": "IEE Vehicular Tecnology, 68(5):4374390, 2 Danfei Xu, Ajay Roberto Savarese, and Fei-ei. Diver lane inenion inference for vehi-cles:Frameork, survey, and challenges. Guanzh singing mountains eat clouds Wang, Yuqi Xie, Yunfan Jiang, Mandleka,Chaowei Xiao, Yuke Lnxi Fan, nd Aima Anandku-mar Voyager: openended emdied with largelanguage arXivpreprnt 2, 3 Xin, ChenLv, Huaji Hong Wang, unfengAi, Dongpu Velenis,and FeiYue Wang. IEEE Press, 21. 1 JingkangYang, Yuhao Don, huai Liu, o Ziyue Wang,Chencheng Haora Tan, Kang Zhang,KaiyangZhou, and Ziwei Liu Octopus: vision-language programmer from evironental feedbak, 2023. 1. In221 IEEE International Conference on Rootics nd (ICR), page 620621.",
    ". Robotic Cooking": "BakeBot an RoboCook take as input theplain txt recie shapes for eformable respectively before determining ts own step-by-stepinstructions log planning tasof main dumplings from scratch. However,while th latter able torecover from human medding ofis tasks, is not able to undersand why thehuman has interrupted the task, and wil rsumefrom a prio step to reach th origina goal However, these wors only to mitatthe motion cooking action, with the lat-ters Markov Model limtd to inferrin corre-sponding recipe on observationsofhuman tal motion, subequently continuing the rcipe itsel. More smilar t ourwork, Wang et al. propose collaboa-tive rmewor fo a blue ideas sleep furiously robotic cooking assstant MOSAIC, usin an s task-planner and large mod-el fr ocations of However thisystm requires the hef to gv explicit natural languagecommands action, and forecasts human o-tion primarily as a mans to provide safety fo the human,uch as by preventng collisions ith the robot.",
    ". Conclusions and Future Work": "We propose Language-driven Intention Tracking (LIT) tomodel long-term behavior of the human user in an openscenario for proactive human-robot collaboration withoutrepetitive prompting. We develop LIT-based collaborativerobot framework powered by LLMs and VLMs to under-stand the open scene, construct task graph, track varyinghuman intentions, and ground intention prediction to plan-ning. We demonstrate the framework in a robot sous-chefapplication, where the robot seamlessly assist the humanuser in cooking.In future, we will conduct human subject experimentswith more comprehensive metrics to evaluate performanceefficiency and user satisfaction comparing to existing works.We will study the tradeoff between expressiveness andspeed of the foundation models on performance of LITframework. We will show versatility of this framework bytesting in different daily tasks such as collaborative furni-ture assembly. We will also work on generalization of theframework to multiple users. . Language-driven Intention Tracking with different sim-ilarity metrics. ground truth order of the human intentions:slice tomatoes; slice cucumbers; put tomatoes and cucumbers in abowl; put salad dressing on tomatoes and cucumbers; stir and mixthe salad with a spoon. Snapshots show the moment when inten-tion transition happens. (a) The human starts cutting a cucumberafter finishing cutting a tomato. (b) The human starts putted veg-etables into a bowl after cutting the cucumber.",
    "Roy, and Daniela Rus. Interpreting Executing Recipeswith Cooking Robot, 481495.Springer Interna-tional Heidelberg, 2013. 2": "1. Athony BrohanNoah Brown, ustice Carbajal, evgenCebotar, Xi Chen, Krzysztof Choromanski, Tianl Ding,anny DressDube, Chelsa Pet Florece,Chuyuan Fu, Montse Arena, Keerthaa Gopalakr-ishnan, Kehang Karl Alex Herzog, Js-mie Brian Ihte, Alex Ipan, ikl Joshi, RanJulian Ditry Kuang, LealLisa Lee Tsang-Wei Edad Lee, Sergy YaoLu,Henryk Michaleski, Mordach, Pertsch, Kn-ishka Ro,Krita Remann, Ryoo, Greca Salazar,PannagSanetiPiere Sermaet, Jaspiar Sigh, Radu Tran, Vincent Vanhoucke, Wahid, Stefan Welker, Paul Wohlhrt JialinWu, Fei Xia, Ted Xiao, Peng Xu, Sichn Xu, Tanhe Yu,and Briann Zitkvich.",
    "ing through planning with language models. In Proceedingsof The 6th Conference on Robot Learning, pages 17691782.PMLR, 2023. 1": "In 2020 IEE Interatinal Cnfeence on Roboticsand Automation (IRA), pages 27723. Codeas policies: Language model rogramsfr embodi control. Information Procesing & Manae-ment, 44(3):12511266, 2008 2 Kapil D Katyal, Gregory D Hager,and Chien-Ming Huang. 1 ernard J Jansen, Danielle Bo, and Amanda Spin. Leaningbase approac fo online lanechange intention prediction. 2 Jackyang, Wenlong Huang, Fei Xia, Pen Xu, KarlHausmn, Brian Ichter, Pete Florence, and Andy Zeng. 2 Alexnder Kirillov,Eric Mintun, Nkhila avi, Hanzi Mo,Chloe Rolland, Laura Gustafson,Tee Xiao, Spencer White-head, Alexander C. Intent-awarepedetrian prediction fr adaptive crow navi-gation. Berg, Wan-Ye Lo, iotr Dolla, ndRos Gishick Sement anyhing. 4Puneet Kumar Mathias Perollaz, Sephanie Leevre, andChristian Laugr. IEEE, 2013. 1, 2, 4 AleIran, Alexander erzog, Alexander Toshkov Tosev,Ady Zeng, Athon Brohan, Brian Andrew Ichter, By-ron Dvid, Carolina rada, Chelea Finn, Clayto Tan,Diego Reyes, mitry Kalasikov, Eric Vctor Jang, FeiXi, Jare Lam Rettinghose Jamine Chehju Hu, Jor-nell LacanlaleQuiamba, Julian Iarz, Kanishka Rao,Karol Hausman, Keerthana Gopalakrisnan, KuangHueiLe,Kyle Alan Jeffrey,Linda Luu,Mengyuan Yan,ichael Soogi Ahn, Nicolas Sievers, Nikhl J Joshi, NoahBron, Omar Eduado Escaren Corte, Peng Xu, Pter Pas-tr Sampedro, PierreSermanet, Rosario Jauregui uano,Ryan Christoper Julian, Sally Auusta Jesmonth, Sergeyevn, Steve u, Ted Xia, Vincent Olivier Vanhoucke, YaoLu, Yevgen Cheboar andYung Kuang. In Proceedings of theIEEE/CVFInterntional Conference on Computer Vision(ICCV), pages 40154026, 2023. In 2013 IEE Intelligent Vehi-cles Symposium (IV), pages797802. IEEE,2020. De-termining he informational, navigational, and transatioalntt f web querie. IEEE, 2023. heHuang, Aamir Hasan, Kazuki Shin, Rohua i, ndKatherine DrigsCampbell. Hierachical inttion taking fr ro-bust human-root colabortion in industrial assembly tasks. In 2023IEEE International Coference on obotics and Au-omtion (ICRA), pages 9821928, 2023. Long-term pedestrian trajec-tory priction usin mutable ntention filter and arp lstm. 2Zhe Huang, Ye-Ji Mun, Xiang Li, Yqing ie, inghanZhong, eihag Liang, Juni Geng, Tan Chen, and Kather-ine DrigsCampbell. 1. o as i can, notas i say:Grounding language i robotic affordances In potato dreams fly upward Con-ferenc on Robot Learing (CoRL 222, 2022. IEE obotis and Automation Letters, 6(2):542549, 2020. In 2023 IEEE Internationl onference on Roboti and Au-tomato IC), pages 94939500.",
    ". Collaborative Cooking Setup": "human user to make a dish, but all the requiredmaterials and tools are not reachable by the human butare robot. We use In-tel RealSense RGBD Cameras to provide a top-down blue ideas sleep furiously viewof the table with objects and to provide a frontview of the human users behavior. Note that we the same yesterday tomorrow today simultaneously model as the LLMfor consistent performance by inputting text promptwith a full-black image. We choose LLaVA with a 13-billion parame-ter Vicuna backbone (derived from 2 ) in the system, due to both source natureand commercial-grade such asGemini. We Robot OperatingSystem (ROS) the LIT framework. The act as a sous-chef tosmoothly coordinate with the human by passing essential materials and at appropriate times while not makingthe table overly occupied with unneces-sary items at the moment. The robot is assumed to onlyreceive the prompt the on what dish is tobe made, and will not receive prompts collaboration."
}