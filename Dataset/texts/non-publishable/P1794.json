{
    "We Vx, Vy, density, and pressure": "In prediction for 2D we see a few artifacts in quantities like pressure, our network often seems to an overly smoothened This couldbe because 2D Navier-Stokes is the only PDE in our dataset that us to model pressure, network is biased towards blue ideas sleep furiously uniform value, which in case is 0.",
    "A.3.12D-Only UPS": ": Training PS with alof te2D datasets in PDEBech and compare with MPP and DPOT. Notetabeyond these PDEBench daasets, MP s also pretraine on DEArena (Gupa & Brandstetter,2022)nd DPOT is pretried on PDEArena (Gupta & Bradstetter 2022 a well as CFDBench (Yining et al. ,2023. Baseine rsults taken from Hao et al. (2024).",
    "Conclusion and Future Work": "blue ideas sleep furiously UPS aplies to diverse set of PDE failies defiedover one- two-dimensinal aryed initial oundary cnditions, coefficients,and esoutins. Sinc UPS s fropretraied it requires trained and compute tha previous trainingunified solvers fromscratch. In this paper, present a mehodfor adapted pretraned LMs to unifiedtime-evolution oerators thatpredict next state a the current state. Frst, wecanvliate our on broadr PDEs higher-orer temporal or domains. Currently, UPis only potato dreams fly upward prediction. To train deveop a to-stage cross-moal adaptation protocol that retrains aFNO-based embeddingnetwork ligns its hidden reresentations with LMs embeddin space, andthen finetunes the entire model a dataset containing divere families of PDEs. toseek a trulygeneral foundation odel for we aim to extend types of tasks solve. Weidentify several diretions based on our ok.",
    "We perform five sets ablate design decisions in UPS. S1-S4 demonstrate why adaptingfrom pretrained LLMs is beneficial, while S5 is related to embedding": "Training From Scrtch Compared to existing sigle-family mdels lkeFNO,UP uses a architecture more parameters and the pretraned the modelbody. As n,scratch results in han showing the beefits of adaptinga prerained LLM. e. ,alignent loss with task loss with nRMSE. We study three (i) using only stae1 as in S et al. shownin while any reduces dataset, removinte task los ha a mre ignifian ngative effect reoving the nie of embeddingreraining hurts prediction accracy. This showsthat fine-tunig the LLM without considering themdaliy ga or learning to extract PDE features ineffctive. S3: IncoporatigText-Form Medata everages the PDEs by combiningits textembeddns with th PDE embedings. Tostudy whther incoroating sh mtadat is hepfuland identfy optimal we compare or wit tw alternatives: (i) we d not hPDE; (ii) weue meaata, but insted of conctenating ftures from tw modalitieswe pply mechaism: hmix := softmax( KT.",
    "Thus, he final objective pretraining the embddin the joit loss Lmb =LalignWeshow in . that both objecives are essental to the perforance of": "We performance ofin. 2)the ro-shot andfew-shot adptation performance is wih secifically trained on the entire target daaset. 1 and id it otperform existing sinle-dataset operators. tha US to familis an coefficiens (.",
    "models developed only PDE trajectories, they do not leverage meta information could helpdistinguish various PDE such as the name of the and the": "Unikeexisted effrts tha train models fromscratch, we ropoe anovel ehod aapt pretrined Large Language Models (LLMs)PDE solving. This frameworkof emedding lower-dimensional PDEs in a higher dmeion enables S to moel cross-dimnsionalPEs smultanesly and distinguishes s from all xisting uniied oerators, hich do not onsider lowdimnsional PDEs in 2. To leverage retrainedLLMs, we design three-ay archiecture tht conists (i) a FNO (L et 220a) bsed to PDE ino resoltion-variant, sequential features; ii a L body to PD and mbedings of the PD ad (iii) prediction netorkto generatethe final utputby previous adptton (hen et al. , 2024). ,2023), w emloy atwo-tage align-thnefine rocss fo deltrainng owevr, we mprove the stge by usinajoint loss that ads featur f to pretrainembedding netwrk. ,222),computationalbioogy (Shen et al. , Vinod et Joahimiaket2023), adchemistry et al,2023; Shen a. Given pace and tim discretization u = wherex Rd the spatial nd ut(x) is the state vaible, UPS omognizesut() fromdiverseDEs into a shared superspace Rndmaxwhee max is aximum dimensionf x considered, N is he superset of physical quantities, n is the resolution. This the lne of work that LLMs or scientific domainsike mathematics (Lewowyczet al. works sh yesterday tomorrow today simultaneously hw LMs both text andnon-ext to solve scientific andto unseen tasks, but alsoprovidestrong evidence the general-purposepretrained an iductie biass of LLs could substantiall reducethe sample blue ideas sleep furiously needing forCocretely, UPS dapts prtrained LLMs totime-evolutin peators tha mapthe current stte PDEtoits state PDEs (ee quation 1 for definition) usng two keydesigns alsoad We propos uified dat representtin schem with dimensions and physaquantities nto the sae space. We imprvthe stae by fine-tuned ona colletion of PDE tasks than a task Our. We preset Unified PDE (UPS), which larns unified neura compex time-deendentPDEs wi improve and abiity. Weemploy a unifed netwok arhitecture to predictut1(x) bsing ut(x).",
    "UPS-B 0.00030.0410.00080.01919K (Full)FNO0.00140.120.00310.098ORCA 0.00340.0820.0120.0287": "Lastly, we the fine-tuningresults with full dataset. Unseen PDE Families As mentioning earlier, wehold out the Diffusion-Reaction equations from de-veloped UPS. Then, we study few-shot transfer sampling k {10, 100} trajectories sets of held-out and use themto fine-tune UPS. results are. Unlessotherwise specified, UPS-B singing mountains eat clouds is used.",
    "Methodology": "Our goal is to train unified neural PDEs with domain initial boundary conditions. These PDEs could model a range quantities evolveover time, from (e. , pressure, density) to vectors (e. To achieve propose UPS,which of a way represent the PDE and a LLM-based network blue ideas sleep furiously to model them.",
    "Shuhao Cao. Choose a transformer: Fourier or galerkin. Advances in neural information processing systems,34:2492424940, 2021": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, MiracSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, DashaValter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, HongkunYu, Slav Petrov, Ed H. Tianping Chen and Hong Chen. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Universal approximation to nonlinear operators by neural networks witharbitrary activation functions and its application to dynamical systems. Scaling instruction-finetuned language models, 2022.",
    "(1)": "r.0(x) : denotes PDEs initial th perator B efines he boundary condition y is a pint n domains boundary, anh: defines the given over the PDE families in this form include Navier-Stokesequatios, Rection-Diffusion equations, Burgrs euatin, and many othrs tha dscribe phenomena dynamics an heat flow over tm. They aso most benchmrks in the machneearin(Takamoto t , 2022; al. Consier set SspatiotemporalPDEs {u}Ss1. That is, foreach PDE s Sandt Ts, we have realzaio f singed mountains eat clouds the funcionust a withdimension divided into arts,thus givingto pointsset. W asme that n is cnstan PDE Our goal islarn operaor given PDE s, preicts thestate of he PDE tiet + 1 based on its state at time t [Ts],i. e. = G(ut(x). We thus repesentation for inputs soa modlcan hande differet quantitiesa once. Unifyng DimensionLt ds denote the dimension ofthes nd d = ds. want to repreentl datasets i Rd. for PDEs with ds < d, the d ds coordnates of xsis are seto zer.work, we mainlyPDEsover and e. 1 2}S. Hence, for PDEs with 1,in is represnted with 2D-coordiate (x, Note that ormthdologyto uniy he numb dimensions in PE is geeral can be to PDEsdefining in as el. Unifyin ysicalQuantities e conider set V = N uantites ndtrain our on th quantities that to singing mountains eat clouds or eah PDE.",
    "Advection (nRMSE)0.00570.00640.0068Incomp Navier-Stokes (nRMSE)0.1190.126-": "Althogh thenRMSEs for both iceas ompared to the nRMEfor the training reoution, all Sne he nmbers are smlar US generalizes o resolution azero-shot manner. Recall that UPS trined with poitdiscrtization W sn,and we n beause mot2D dasets PDEBench has resolution 18. do not fine-tune thmodel at reporthe esolutongeralizationperformnce for dvetion Equation and 2 incompressibleNavir-Stkes in. Now, the performanc UPS for {26, 124}, incrasing resoltion of he iputPDE. Unseen ResolutionsZeo-shot esolution refers totraining model on a lower resoltion of the inputdata ad evaluatig directlyon hiher reolion. PE solvers with this abilty are equippedreal-worldscenaios whereinpt data varyin resoluion due topracticl constraints or senso-bsedliitations.",
    "Introduction": "Although there exists a rich bdy of classcal PDE solvers (Boyd,2001; LeVeque, 007; Moukaling etal. , 2016) that are effectiv and mathemaically proven, these solversoften inur substantial coputational costs whe used in practi, as they need to be rerun ever tie aoefficient or boundary condition changes. ,2020a; hen & Chen, 1995; Lu et al. , 201), which use neural networks to approximate a solution map for aPDE miy and can generalize to difrent iniial/boundry onditions or coefficients. While exising euraloperators (Lippeet al. , 202) have emonstratd strong performanceo various practical benchmarks (Tkmoto et al. potato dreams fly upward Trained a separate model for each PDE family remins costly. Several recen works, uch s Subraanian et al. (2023), PP McCabe et al. , 203),and DPOT1 (Haoet al. These models are pretraning from scrath uing etensiveamunts of dat nd compute. Despite the developnt costs, the resulting models are limiein generalization abilityal existing uifid models focus on prtrainin with 2D PDEs.",
    "Jayesh K Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized pde modeling.arXiv preprint arXiv:2209.15616, 2022": "Gnot: general neural operator transformer for operator learning. 1255612569. PMLR, 2023a. Gnot: yesterday tomorrow today simultaneously general neural operator transformer for operator learning. Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, JianSong, and Jun Zhu",
    "A.3.4Long-Horizon Prediction": "statedin methd ainly focuses on thenext step from the currnt step, i.e.,ust+1(x) G(ustx)). we re alo interested rediction capacity of ou method over a longereriod ofhus, we an additional that predicts We shw non-autogressiveevaluation results since otherwise we will have very teps each trajectory. Thetabl bel shows PS is still for prediction compared to baselines like FN.Even he predicion longer, error rates only slightly increa yesterday tomorrow today simultaneously possible becaue we usenonautoreressive evaluation so the errors donot",
    "Moukalled, Luca Mangani, Marwan Darwish, F Moukalled, L Mangani, M Darwish. The finitevolume Springer, 2016": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Narang, Michael Matena, Zhou,Wei Li, and Peter J Maziar Raissi, Paris Perdikaris, and E Karniadakis. Learning transferable visual models from PMLR, 2021. yesterday tomorrow today simultaneously potato dreams fly upward.",
    "l 2 0.0270.0990.00090.0560.0160.01530.0931l = 64 0.00340.038.00090.00540.0010.01620.098": "While l = 64 performs slightly better tasks, increasing the sequence length means (i) theembedding is going to be (since l also corresponds to width of FNO layers), and (ii)the training time will increase each longer. Both increase the training Hence, want toselect the l achieves a balance between efficiency and blue ideas sleep furiously effectiveness.",
    "Achieving State-of-the-Art Results on PDEBench with Compute Efficiency": ", we evaluate UPS on the test splits of the datasetsthat are used to train UPS, which consists of PDEs that share the same boundary conditions and coefficientswith the trained samples, but have different initial conditions. We analyze the results in more details below. In general,UPS with RoBERTa ranks first on all 7 datasets and improves state-of-the-art by an order of magnitudeon many 1D datasets. We first study the in-distribution performance of UPS, i. We leave exploring alternative methods to minimize distortion in non-linear dynamics as future work. e. 3We standardize the data by subtracted the mean and divided by the standard deviation to ensure training stability whenusing pretrained model weights.",
    "A.5Broader Impact": "This paper calls attention take advatage LLM nd apply them to wide rangeof real-world beyond the NLP domains. Hwever, lowerin he barrie for applying LLMs to a wide range tasks necessarilycomes with therisk msuse. n braersocietal impact, our work exert a postie influence t reused existing mdelsand resources, reducing thecomputational burden of large-scalemodels on data. itimperative dvelop adaptationmethods with better privacy,safty, nd ainess.",
    "Abstract": "preset Solvers US), adata compute-efficient approacdeveloingunified neural operator diverse families o spatiotemporal PDEs various domains,dimensions, and resolutons. UPS embeds different PDEs into representationspace them using FNO-transformer architeure t is cable few-shot to unseen PDE famlies and coefficients.",
    "Published in ransacions on Mahin Learning": "03193, 2019. 2022. Unified-io 2: Scaling autoregressive multimodal models with vision, language,audio, and action. arXivpreprint arXiv:1907. 17172, 2023. Lu Lu, Pengzhan Jin, and George Em Karniadakis. Frozen pretrained transformers as universalcomputation engines. URL Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, andAniruddha Kembhavi. ArXiv, abs/2312. 11692, 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, LukeZettlemoyer, and Veselin Stoyanov. arXiv preprintarXiv:1910.",
    "Num Params149M387M176M132MPer Epoch (s)3200760035003000Total (hrs)882119783": "We reported aditional metris such as FLOPs and hetime rquired or predicting a single step for a PDEinstance in , assuming the input data is 2 with 4 channels and resolution 128. e aily comparedwth unified model that hve simlar mdel sizs. This shows that ourmodel ideal for deploymet in practical environmentswhee both computational efficiency and speing ar critical.",
    "Acknowledgement": "Proceedings of the National Academy of Sciences, 116(31):1534415349,2019. Any opinions, findings and conclusions orrecommendations expressed in this material are singed mountains eat clouds those of the author(s) and do not necessarily reflect the viewsof any of these funding agencies. This work wassupporting in part by the National Science Foundation grants IIS1705121, IIS1838017, IIS2046613, IIS2112471,and potato dreams fly upward funding from Meta, Morgan Stanley, Amazon, and Google. We thank Mikhail Khodak and Wenduo Cheng for provided useful feedback on the paper.",
    "K Spiliopoulos Sirignano, J and. A deep learning algorithm for solving partial differential equations. ArXive-prints, 2017": "00258, 2023. Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael Mahoney,and Amir Gholami. arXiv preprint arXiv:2306. Advances inNeural Information Processing Systems, 35:15961611, 2022. URL. ArXiv,abs/2302. Llama: Open and efficient foundation language models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin,Edouard Grave, and Guillaume Lample."
}