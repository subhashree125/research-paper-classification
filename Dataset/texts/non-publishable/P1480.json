{
    "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors fear that complete honsyaboutlimittions ight be use as grounds for rejection, a utcme e that tat acknowledgedin the paper.The authors should use their estjdgment and recognize individual ctions in favor transparency an impo-tant rol in develoing norms that preserve the of theReviewerwill specifcally not penalize honesty concerning",
    ". Broader Impacts": "For example, it i legitimaeto pointoutthat an ipovement in uality f generatve models cld be used togenerate deepfakes fr disiformation. , gted release o providing defense in t attacks,mechanisms misuse, mechanisms to moitor ho ssemlears fromfeedback overtim, improving ad accessiblty of. Gidelnes:The means that there is societa o the perormed If the author or No, sould xplain why their work has societalimpact orwh the pperdoes not ddress impct. g. On the other is not neing t pointouttht generic for ptimizing neural neworks enable to trainmodsthat generate fastr. Te should cosidr posible hrms tat arise hen tehnoogy isbeed usds ntendedan functoning corectly, that could arise whenthetechnology is beng sed intening gives incorect results, and hams followinfrom (intentional or of the tecnoly. o socital ipacts include otential maliciousunintended uses(e. Ifhee negativsocietal impcs the authors could also discuss possil tigationsrateies (e. disinformation, generating fake proiles, surveillance), deploymen ofechnologies could mak decisions that unfairly privay cnsierations securty considerations. g.",
    "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Dependig on the cuntry in whic research s conducted, RB approval (or equivalent)may be required for any human subectsresearch I you obtaining IRB approval, youshoul clearlystate this in the paper.",
    ": Ablation studies on CelebA-HQ 256 256 dataset at epoch 250": "4, surpassing the iT model across and lrgermodel ofigurations,suhasDT-XL/2. the ImageNet1k 6 daaset, our methoologya formidable FID 2. 26 guidancescale of 1. prformance superiority etends toothe yesterday tomorrow today simultaneously includng the SSM-asdDIFFUSSM-L-G Although our mdelyields resuts similar to those of blue ideas sleep furiously SiTmodel, ourodel is approximately 30% smaller size.",
    "BDiscussion": "Advanages of DiMS. Fist, its highlight or etod outperfoms both DiTand SiT whie equirng less than third the iteration, acievinthe best FID of2.11. Compared to the state-space diffusionmodes, our metd outperorms DIFUSSM-XL,cosidering imilartraining Notabl, ourmethod alsoa smaller size paraeter,to of DiT while demonstrting strong generation cpcityad fastetrained convergence. Clarification of scalable term",
    "arXiv:2411.04168v2 [cs.CV] 28 Dec 2024": "than UNet-based counterparts. Even the most common open-source text-to-image tool, StableDiffusion, has switched to use transformers in their upcoming release . The power of this structure lies inthe attention mechanism for capturing richer in-context relations. SSMs haverevolutionized the NLP field, favoring linear time complexity and excelling at long-context modeling.This type of network bears similarities to recurrent process of RNNs while being capable of fullyoperating in parallel like convolutional networks. blue ideas sleep furiously In the context of computer vision, within a very short period, this architecture has beenused to address a variety of problems, including image perception , image restoration, and image generation . In diffusion-based image generation, Diffusion StateSpace models (DIFFUSSM) already surpass their transformer-based counterparts. Though showing many advantages, Mamba still has a critical weakness when processing 2D imagerydata. Like vision transformers, images are dividing into patches and then mapped into tokens. This characteristic is unfavorable, particularly whentransformers have no such order-dependency issue. Many vision-based Mamba studies have focusedon solving this problem on proposed advanced scanning mechanisms like bi-directional , cross-scanning , or 8-directions zigzag . In this paper, we enhance Mamba-basing diffusion models, specifically focusing on image generation.Previous models failed to address the scanning order issue due to their exclusive reliance on spatialprocessing, overlooking crucial long-range relations that manifest in the frequency spectrum. Wesuggest a novel approach integrating frequency scanning with the conventional spatial scanningmechanism. Motivated by the above observation, this paper introduces DiMSUM, a novel architecture thatharnesses Mambas power to unlock diffusion models generation capabilities. Our approach enhancessensitivity to local structures and long-range dependencies by integrating wavelet transforms andspatial information. Using a query-swapped cross-attention technique, we dynamically synergizespatial and frequency information, accelerating convergence and improving image synthesis quality.Consequently, this boosts image quality and enhances efficiency and scalability of training. Additionally, we incorporate globally shared transformer blocks to address global context integration,a limitation of traditional Mamba models. The block can also be viewing as a token-mixing layer thatenriches global relations among image tokens, addressing the weak inductive bias of the manuallydefining scanning orders in original Mamba. Hence, DiMSUM can maintain high performanceeven with larger, more complex datasets. Extensive experiments show that DiMSUM achievesstate-of-the-art FID scores and recall, setted a new benchmark in generative image modeling. In summary, our contributions lie three-fold: (1) novel Mamba architecture for diffusion modelsthat leverages both spatial and frequency features to enhance the awareness of local structures withininput images, leading to better image generation. The transformercan also be seen as an order-invariant mixed layer that complements Mambas loose assumptionsabout the order of 2D data. Additionally, our method maintains comparable GFLOPs andparameters with existing diffusion architectures while offering faster trained convergence.",
    "Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generativemodeling. In The Eleventh International Conference on Learning Representations, 2023": "Nommer: Nominate synergistic contextin vision transformer Li, A. potato dreams fly upward Ren. Hu, D. Ren. Li, Z.",
    "Conclusion": "Considerin promisinge anticpatethat uture search in uh as text-o-image syntesi, wil adat our backbonerchitectre achieve comparable impovements in Soial impactand While there is thatour archtecture could be fo malcious purposes, a scial security challege, we areconfident thatrisk mitiae with the of securityrelated research. While method outperforms oher ifusion bselines n geeration quality and cver-gence, we acknowledge areas improement. y tansfor withn he Mamba framewrk, ourethod local structre wareness nd ensures eficient satial and requency ul-focus stratey the detail andqlity of generaed and celeratestranig convergence. Hece the can ouweigh the negatives, rendering theminor. ap introduces novel, arhiteture sealessly integrates and frequencyfeatures into Mamba poess.",
    "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "For example() If the contribution is primaily a new algorithm, the paper should maket clear howto reproduce that algorithm. For example,if the cntribution is novel architecture, escribing the rchitecure fullmight suffie, or if contribution is a specfic mdel and mpirical evaluation, it mayb necessary to either make it possile fr thers t rplicate the model with the samedataset, or provide access to the model. In general. Dependig on the cotibution, reproducibiity an be accomplishe in various ways.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(d) We recognize that reproducibility may in some in which caseauthors are welcome to describe the blue ideas sleep furiously way they provide reproducibility. In the potato dreams fly upward of closed-source models, it may be that access to the model is insome way",
    "with swapped query achieves the best results while requiring fewer parameters and GFLOPs than theattention option, with only a marginal increase in computation compared to the linear option": "Number of avelt level. Assown in b, twoavelet levels rovide the est performance onthe CelebA-HQ 256 dataset W arguetha the chce o wavelet levels shoul be based onhe nputresolution. input image of siz 256 256, for instance, is ended to a comac laent of size32 32, wich i further ptchifiedby 2 to themall siz of1616. Hence, applyed 3 waveletlevels esults in etremely small wavelet subbands of size 2 2, leading to reduce performance. Alternative frequency transfor. Apart fo wavelet transform, we also onsider different types offrequency technqes like DCT and Fourier Tansfrm (d). For DCT we propse a multi-orderJPEG canning strategy (illustrated in ), based on JPEG comprsson instead of wndowscaning. or Fourier Featurs w directly adopt EinFFT block from iMB . In eithe case,thepefomance dros comared with thedefault choice of waelet.Transfrmer layer.In f, we assess theadvantage of transformer ayer for both ConditionalMamba and Spatial-Frquency Mamba. As shown, the glblly-shared trasormer further booststhe peformanceof our patial-Frqency Mamba. In cntrast, applying this layer solely to patialaba incrases FID score b 0.13, highlighting essence of our Spatial-Fequency Maban ojunction wit the transfomer layer. Meanwhile, epacing this shared layer ith independenttransformersresus i a ecline o 0.43 in FID and 0.03 i Recall.",
    "Experiments": "We estabished a depth of 20 bas widh o 024,nd a path size of 2 fornetwork configuration (further information on hyperparameters apendixA). We run experimentson standard CelebA-HQ, Church, mageNet. For samplingmethod we foll to adaptive OD solver dopri5 evluation assessed.",
    "Abstract": "These wavelet-based outts areten processed and seamlessly fusing it theoriginal Mambaoututs through crss-ttention fusionlyer, cmbining bth spatial and fre-qency formatin tooptimiz the oder awarnes of stte-spac models wichis essential for he details and oveall quality of imgegeneratin. Through extensiveexperiments on tandar benchmarks, our method demostrates uperior resultscomaring to DT nd DIFFUSSM, chieving faster trinig cnvergence and de-livering high-quality outputs. While tate-spcnetworks, including Mamb, revoutionary advancement n ecurret neuralnetworks, typicaly san iput squens fro left torigt,they fe diffcul-ies indsigning effete sanningstrategies, especially inthe prcessing fimage dat Our method dmostrates tht integrating wavelet transforatiointo Mamb enhancesth lcal stucture awarees ofviual inpus and bettercaptures long-ange relations of frequecies by disentangling them into waveletubbands, epresented yesterday tomorrow today simultaneously both low- and high-freuenc components. Weintoduce a novel state-space rchitecture for diffusionmodels, effectivelyharnesing spatial an frequencyinformation to nance th inductive bias to-wrds loal features in input images for image generation tasks. blue ideas sleep furiously coes and pretraned models are releasing at. Besides, weintroduce globally-haredtrasformer to supercharge the perormanc of Mamba,harnessing itsexceptionalpowr to capturegloal reatoships.",
    "Diffusion Architecture": "or ffusion image mdeling,everal trasformer-basedrcitectureshave been recently Speciically, VT replced resblock ayerswth trnsforme andrmoved downsampling/upsampling bloc. Alternativly, i of flowmatching metods that emphasize eteminstic trajecies frompure aussan noiseto targ distributon straiter solutionpath I his work, we focus on thesimple objetivef lowmatcing for our design. Ntably, of them are on Stochastic Diferential Equations (SDE) require an accu-mulation of stocastic at eac generation step. Difusion models n emerging type genetive odel that requires a euentialdenoisig prcess of several to to animagefo Gaussian noise.",
    "B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.arXiv preprint arXiv:2209.14988, 2022": "International Journal of Computer singing mountains eat clouds Vision, 115:211 252, 2014. Huang, A. Russakovsky, J. Rombach, A. Lorenz, P. Esser, and B. Imagenet large scale visual recognitionchallenge. Bernstein, A. R. Deng, H. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern yesterday tomorrow today simultaneously Recognition, pages 1068410695, 2022.",
    "T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R. FlashAttention: Fast and memory-efficientexact attention with IO-awareness. In Advances in Neural Information Processing Systems,2022": "A. Beyer,A. D. Dhghai,M. Mindere, G. skoreit and N. ICLR, 22. A. Mindrr, G. Interational onerence n LerningRepesentations, 2021. Esser S. Blattann,R. Entezari, J. Saini, Y. D. et al. yesterday tomorrow today simultaneously Saling retified transformers forhigh-resolution image synthesis. 3206, 2024.",
    "Inspired by the of Mamba-based models and frequency-based networks, wedesign a novel DiMSUM for and high-quality image synthesis with the structure": "Green dots indicatepixel points wthin each wavelet subband and a window of size 2 2 is used to prform scanningacross multiple waveletsubbands like the CNN kernel. The metd first receives an inut image and ecodesit to a latent map of size 4 H W. The procssed laten i hen decodd to the outut imge. It then processes the atent map using our prposed DiffusionMam network, whose coe is a sequence f DiMSM blocks, each consistingof DM blocksthat employa ovel Maba structure with spatial and frequeny scanninfusion and a globallyweight-shared transformer blok. Giving an imge of size (8, 8), for exampe, i is irs decomposed to four wavelet ubbands ofsize (4, )here each is furher transfomed to 2nd-level subbands ofsze (2, 2. pesented n. Wavelt level 1 Horizontal canning ertical scaning Wavelet level 2Input image : Illustraon of Wavelet Maba (Best view in color). For illustration purpose, we plotwavlet reresentations of an inpu image but ur real process is performed on encoded feaures f theinput. Similar to Laten DifusionModels , or method performs image generationon th latent space of a pre-tained encoer.",
    ": Comparison of window scanning im-age and wavelet space. For one-levelwavelet transformation is applied each sub-band is half the resoluton of image": "DiMSUMthese challnge by dcom-posingorigina mageito waveletsubbad. This is effective ptureong-range while relaionsacross dffrnt We rdesignd theindow canning each indw corre-spondsto suband the frequency space s i. onsequently, ach window capturesthe range of low/high-feqency signalsfrm the original image. sts usapat fmwindow scanningin As the model rorsses through subbands, it incoporates spatil at various low-to-high frequences,added aluable contet to the denoised Wavelet Mamba We now xmine he integra-ionof the wavelet he Mamba structure i Wavelet Mamba appliesW to decmpoe feaurs ito wavelet subands. O sted usestwo-eel Haarwaelet to ap inpu low nd high-frequecy iven inpu feature x RCHW ,firs-level wavelet s applied to 4 wavelet subbands of size RH/2W/2. Each subbandis then further decomposedinto waveet subbands of size RCH/4W/4. is pivtal, as we decmpose eryto venly input imge wavelet conventional avelet transformations tha pocesLL subandsat thenext level. In Mama, cocatenate hose subbands to form a1D sequence, applywinow canning within subband, and slide across the sequnce feature etracton. Thewindow scanin i inspired by a kernel prposedin sliin rctons:lef to and to bottom.Note that since ow-requency subbands capture it be nput frst Thereor, we do se rerse scningorders: right left andbotom top. After pasng Wavelet Mamba, outut are transforming shap y usingtwic. With waveletmodule, our model better capture local structuof frequency Thus, incorporatng WaveletMamba Mmb can betterperformance, yided high-qality iage (Spaial-freqency)",
    "Ablation of scanning orders": "We so scanning with mny-way odeslikeSweep-8, and is not guaranteed yield beter performance thanSweep-4 scanning. In Mamba, is demonstrated that our proposd window scanning for provies a better outcme than sweep-4. We kep it simple by using onlyMaba block for all without gobally-shared transfrmer and fusion yesterday tomorrow today simultaneously ayer. e, e orders of Mamba.",
    "GAN modelBigGan-deep 6.950.28160M-StyleGAN-XL 2.300.53166M25000K 2564K": "On the at resluions of256x256 ad (), ur methodachieved state-of-he-art FID scores of 4. respectively, the score repredin rcnt stdies. This result particularly impres-sive thebseine methds aebased oniffusion which are knn forexcllent diversity.",
    "Guidelines:": "g. The name of singed mountains eat clouds the (e. The should which version of the asset is used if possible, include aURL. g. The that the paper not existing assets. The authors should the original paper that produced the code package or dataset. , website), copyright terms ofservice source should be provided. , 4. scraped data singing mountains eat clouds from a particular source (e. For popular datasets, paperswithcode.",
    "residual block is a skip-connection block that learns residual functions with respected to the layer inputs": "te reuce ottenck. owevr, computtion remain Recently,th S4 model has been o deal long-range eendency in the NLPfield. Furtermore, the S4model favors the linear timeand spac, which is transformer. Among class models, Mambastands outfor its incaptured lng-rang dependency. diffusion models, DiffuSSM buildigblock for their odel and achivesetter FID compaing to transformer counterparts. ecently,Zgma fordiffusion architecture, used a zigzag scanning to yesterday tomorrow today simultaneously prservelocality-aare showng promsin rets, Mambabased diffusion modesstll to find an optimal scnned scem to tak advantage of the 2Dinductive fromiaes We tese approaches stuck in spatial procesing, thus global andlocl can effecively captured in frequency spectrum, thus we propose frequency scnning alngside the exitng sanned",
    "Ablation of network design": "In this section, we ablate the design choices for our network, using experiments on the CelebA-HQ256 dataset. Wechoose sweep-4 with interleave scanning order by default. In a, with our proposedconditional Mamba, the FID score is improved from 6. 27, and the same trend is observed forrecall. Meanwhile, adding Wavelet Mamba followed by a simple concatenation layer to combinespatial and wavelet features results in a worse score of 5. 87 due to the weak alignment betweenthese features. This demonstrates that our proposed cross-attention fusion layer is crucial forperformance improvement, fully leveraging wavelet components to achieve a score boosted to 4. 92. The performance is further enhanced to 4.",
    "State Space Models and Their Applications in Vision Tasks": "Mamba has advanced over transformers NLP by using a time-varyingsystem with context-dependent differentiation of hidden over longsequences. g. Similarto vision images are divided into and the patches are into tokens. MambaND reduces that cost by introducing twomethods: interleaved scanning multi-head scanning. However, order information, introducing inductive bias about 2D imagesinto the model. computer vision, ViM and VMamba the first Mamba as a buildingblock in discriminative tasks. VMamba proposedcross-scanning (sweep-4) each building block. Therefore, scanning order is vital vision models ViM proposing bidirectional order (sweep-2) for tasks. The tokens are then arranged in a sequence singing mountains eat clouds a scanning In transformers,the scanned order does not matter since scores computed between every token pair. 4). This as alternative to across various domains. this paper, we show that too many scanned orders,e. Recently, Zigma proposed a zigzag-8 scanning order to preserve the locality token is adjacent to its next and zigzag-8 order compared to bidirectional one. In control engineering and system identification, state space (SSMs) are described statevariables first-order differential equations but initially in learning. Interleaved scanning, which alternates thescanning order in is simpler but discriminativetasks. , sweep-8 blue ideas sleep furiously and zigzag-8, may introduce information and lead to worse performance Instead, sweep-4 offers best performance (.",
    "anda high-pass filter H =1": "otably, these filters are yesterday tomorrow today simultaneously paiwise orthogoal, an invrtible matrixexists to map the to the image space, coined as invese avelet ransform(IDWT. Given ts benefits, we us wavelet transom to supplemntthe loal requecy cmponets into the of amb, thus leading to enhancedimage quality andtraining convergnce, as demnstrated throug our empirical experiments in sectio 4. This process iscretewaveet transform (DWT).",
    "Inference speed is also faster, as shown in": "This approach enables the effectively combine informationfrom both domains, leveraging strengths while mitigating potential conflicts. Refer to 5a and Naively these domains g. , can damageperformance due to conflicting or misaligned information. To address this challenge, we proposed a more sophisticated fusion method using Cross-Attentionlayers between spaces. this fusiontechnique can FID from 92 in c, the result of ourmethod.",
    "qs, ks, vs = Linear(fs),qw, kw, vw = Linear(fw),fout = Linear(Concat(Attn(qs, kw, vw), Attn(qw, ks, vs)))": "Mor speifically, we firs cmpte each featurs query key (k), an vaue using liearlayers. Finall, e concat the of t cros ttentons channelfollowd by lnear projection to obtain ouput fout (se last subfigre CondtionalMamba. We a simpe that enable Mamba to ake conditional vainitalizing thevery fist hidden stat wthembedding c of zeo,as originl Mamba. can be considerd as a pior injecion into Mamba.Specificall,th recurrent process of Mambacan berewitten as blow:h0= A1 + B0y0= Ch0,t= Aht1 + Bxtyt= Cht(3) conventional Mamba, h1is st to isnopreviousstat he bginning. Here, weet 1= LinearD(c) to inject cntext prir of Mamba.  shown n ablation, conitionalmamba nhances modelperformace. This i for both andcodtion genration. Forimae we cate auxiliary larnable toento apture imae spaces gobal smilar to transforers. enle i DiM blocks",
    "Preliminary": "Stte Space (SSM). SSM blue ideas sleep furiously is anew type of sequence that ses an implicit hiddenstt h(t) RNL to map1D input sigax(t) to its ignaly(t) RL.This process is formulated b matrx RNtwo blue ideas sleep furiously projectinparameters B RN1 and CR1N:",
    "CSpeed Analysis": "This obsevation further highlights the stregh of Mamba inhandling longerontxt length. The reslts, as shown in te table 4, reve that DiMSM-L/2 memoryusage is slighy higher than its counterpart. This observtin ligns wih he known quaatic omplexity of transformer a sequene lengthincreae. Our hybrid model mitigates thisssu; th impact of attention blocks ireduced, whileamba demonstrates its linear calingcompexity asthe tokencountgrows. This achitectural choiceallo DMSU o maintain efficiency at higer esolutions, offetting the inital GFlps ifferencet 256 resolution.",
    ". Experiments Compute Resources": "Guidelines Th anwer NA means thatthe paper does not include experiments. The aper ould indicate the typeofcompute workesCPUor GPU, nternal luster,or cloud provider, including relevant memory and storage. Queston For each experiment, doesthepper provide sufficient informationon he -puter resources (type of compute workrs,memory, tme of exection) neeed to reprducethe exeriments?Answer: [Yes]Justifcatio: Ys this is preseting in appendix.",
    "Freuency-bsed methods": "the benefit of in geneative moeling apply dicrete wavelet transform on local features beforefeedin Mamba layers. To solve themag dened problem, FreqMamba applied waeltand Fourier tansformr fetures injected into Mamba block. urtermre, the high requencies can beleaned eading o sarper synthesis image. In modelig,several corporate wavelt frequenies ino geerative By expcitlydeoposig features/images io nd low-frquec throughwavelet transform, thegeerative modl can train stably faster. Byusing t features andspatial features, our etod achieves improement in image synthes compared to ereyspatial feature processin. NomMer pplie adiscrete cosine ransforme into gloal t efficiently yiel synerstic contex from bohglobal local conexts TimproveImge odelig, e-AE n additionalfreuency deoder using Fourier transform to reconstuct the hih-freqency informaion applies wavele int modules to reduce time nd space comlexityof thetransformer architecture still preserving perrance.",
    "With these two points, we emphasize upon the importance of our proposed architecture, rather thanjust the benefits given by the flow matching framework": "Seed gain. n table 4, DiMSUM-L/2 chieve 2 2 seconds laency compared to IT-L/2 with 3. This emonstrate he potential us of DiSUM in largr benchmrk like text-to-imagewich has larger resolution Oseving the Memory and GFLOPSin table tab , its true to claim DiUM-L/2shows slower inerence sped compared to ts counterprt for 256x256 images using the same NFE(dueto bigger GFLOPS), owever, there are tw crucial points to conside: SalingEfficiency hen e icrease the image size to 51252, a evident from theMemory anGFLOPS table, ou model acuall equies fewerGFLOS at this higerrsoution, thanks o is slower latency scang which we mentioed above. Consqently,fr 52x12 images and larger, DMSM-L/2 would outperorm it counterart inspeedgiven the same NE. Noably, DMSUM requires fwer NFE to meetthe dopri5 stoppingcodition whlestll achievinga sgnifcantl betr FID score than DiT We hypothesize that our proposed hybid architecure convrges to a better olutionwith ess curvature, enablinghih-fidelit image prdcton with fewer NFEs.",
    "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15 potato dreams fly upward singing mountains eat clouds"
}