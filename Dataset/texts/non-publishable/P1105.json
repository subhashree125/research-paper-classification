{
    "|V () | < +11|V () |0otherwise.(3)": "We the cordinator representation as earnble blue ideas sleep furiously parameters. Generate Graph Batches forEfficin Morovr,by haesing th inteconnectivitfaciitaed y coordinators, thpotetial fo join samplin of from di-vese data graphs. We opt foraflexibl apch so ht thy eove organicallalongside GN training, adaptng dynmically to the data. This innovative emowr the trinngproces aggegated batches, thereby pomoting the natu-ral ainment of cross datsetsEpoing the to apletra of features within a singl larnng prompts toseek out unified represenations, effeciely sntesizing sources cohesive and compehensive reprsentations. This not only enhances the abilityto captu underlying structure data also fostersrobust generaizationEvideced ,prompts demonstraed to be to gaph , node, edge, removing or adding, some In arallel, our coodinators a rm graph transformation.",
    "Prompting--19,398": "From the experimntal results, e propse GCOPE maitains its over other base-line ethds, thu the mprovements modestly deceasefrom 1-shot to 5-shot scenarios.A.4Qantitativ Aalysis of GraphCoordinatorsWe conductadditional experiments the antitativeanalysis of grap coordinatrs, b Table Tale A6Experiental result shw that, despite slight in per-forance across datasets, GCOPE varying numbrsofgraph coordinatos generally outperforms the Supervisedand singing mountains eat clouds IP",
    "Taoran Fang, Yunchao Zhang, Yang Yang, and 2022. Prompttuning for neural networks. arXiv preprint arXiv:2209.15240": "Jean-Bastien Alth, Corentin Tallec, PierreRichemond, Elna Bucatskaya, Doesh, Bernardo AvilaPires, ZhaohaGuo, Mohammd Geshlagi Azar, et al yesterday tomorrow today simultaneously Avances neural information processingsystems 33 (220), 2127121284. Russel Alan Hart, Linlin Yu, Yifei Lou, Feg 2023 Thirty-seventh Confernce Neural Information Processing",
    "A.DynamicalStdy of inter-coordinator": "Accoring to the experimental results, weobserve tha GCOE/d outerfors GCOPE/ on the Wisconsin and yesterday tomorrow today simultaneously Texas, performs better than the supervised method on the Cora butis inferor to COPE/f, andexhibitsnegatie ransferon Citseer. We additioally study theimpact of dynamical inter-coordinatoredges on he perfrmance of GCOPE,and report thetransfer learn-ed results in Table 7.",
    "ABSTRACT": "This issue particularly in learning scenarios,where the paucity trainin ata necessitates the external knowldge sources In to this challenge, wepopos novel called Graph COordinators for rtrain-ing (GCOP), that he underlying commoalities aph to enhance few-sht Our novelmethodology involves a unification that amalgamaesdisparate graph dataets pretraining phase to distill meaningful target taks. Exensie exper-imets acros mulipe gah datasets demnsrate the superior. Language Modls have the filds ofcopuer (CV) naural anguage most otable advanemens of LLMs is hat single model istained o nd divers daaes spanning multipl aparadigm we term in One. However,applying idea tothe graph remains a challege,ith cross-domain in negative trasfer. This ethodology epowers super generalizatio capablities, facilitating ecompsingomprehension vaied datadistributions Leveragingthese capa-biliies, LLM demonstrates remarkable verstiitacross avariety of domains a pradgm we termOne fr All.",
    "Siheng Xiong, Payani, Ramana Kompella, and Faramarz Fekri. 2024. models learn temporal reasoning. arXiv arXiv:2401.06853(2024)": "dvanes inneural inforation processed stems 33 58125823. Yuning ou, Tianlon Yogduo Si Ted Chen, ZhanyananYang She. Graph learnin with augmentation. Zh Xu,Yuzog Chen, Qinghai Zho, Yuha Pan, Hao Hanghang Node classifation beyond homophily: ageneral solution.",
    "( + , ) = (( , )) + (4)": "where represents an error bound associated with the GNN() and the prompt. Here, () signifies a graph transformationoperation. It addresses the complexity of real-world prob-lems (like diverse and complex structures, attributes, and structure-attribute patterns), requiring a multidisciplinary perspective forsolutions and encouraging students to apply cross-domain knowl-edge. integration strategy in education, whichhighlights core concepts and fosters interaction among disciplines,resembles the coordinators in GCOPE. This helps students see thebridges between subjects, nurtured flexible and innovative think-ing, like transferring to new downstream datasets, especially thefew-shot scenarios. 3. 3Pretraining on Multi-domain GraphsOur approach is very flexible and compatible with many pretrainingapproaches. Here, we present a general blue ideas sleep furiously pretraining framework thatcan extract high-quality embeddings at both the node and graphlevels. Withinthis context, GraphCL and SimGRACE have been particu-larly noteworthy for their effectiveness in generating granular nodeembeddings as well blue ideas sleep furiously as holistic graph embeddings. GraphCL em-ploys graph data augmentation to generate positive pairs, whereasSimGRACE perturbs the GNN to achieve this objective. Next, we move on to the crucial task of preserved the integrityof information from each graph. This loss is quantified throughthe mean squared error (MSE) metric, which assesses the discrep-ancy between the original node feature vector (after projection).",
    "Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. 2024. ZeroG:Investigating Cross-dataset Zero-shot Transferability in Graphs. arXiv preprintarXiv:2402.11235 (2024)": "2022. Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip. In The Twelfth International Conferenceon Learning Representations. 2023. arXiv preprint arXiv:2310. 2023. 00149 (2023). Pre-train, prompt, and predict: A systematic survey ofprompting methods in natural language processing. Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen,and Muhan Zhang. arXiv preprint arXiv:2311. 08189(2023). 2024. SEGNO: Generalizing Equivariant Graph NeuralNetworks with Physical Inductive Biases. 2023. Comput. Graph self-supervised learning: A survey. Yuhan Li, Jian Wu, Zhiwei Yu, Brje F Karlsso, Wei Shen, Manabu Okumura,and Chin-Yew Lin.",
    "IMP (%)12.57%4.58%13.76%6.65%6.08%16.87%37.66%14.29%29.62%11.97%6.01%25.36%3.25%1.06%16.39%": "In this paper, w repot average results on alldownstreamtasks. We cose three widely usemtrics to ealuat node classification task , in-cluding classification accuracy(Acc), mean AUC-ROC (AUC), admeanF1-sore(1. (2) IsolatedPretraining (IP)with fetunig: Theseethds initially utilize multipl cross-domin atasets as soucedatasets, whic ar combining in an isolated manner o pretrana GNN model in a self-supervisd fashion (e. We leerage 10-fold patition rategy to splitthe 10 real-world daasets, nine atasets s cross-domain sourcedatsets for pretraiing and the rest one datasetas the targetdatasetfor transferin. Baselines. We fix thenumber of raphurallayer at 2 with a hiden mension of 100 for GCN, FAGCN andGAT models. (1) Spervied methos: Tesemethds train aGNN model on downtream task ad infer resultsdirectly. We compare our proposed method with he folowngbaseines, categrized into ree groups andacompanie by co-cisedecriptons. ubsequently, the pretraining odel underges inetuningfor a new downstrea ask. In termsof GCOPE, weitroduce oneoordinator for eah source datasetandassin 0 For fir comparisons, e apply the C-way-K-shot learninsetting, ame as , for eachtarget dataset to build thetrainingdata and then slit e est of data randmly in 1:9 fr vali-datng testing. (3) Gap Coordinatrs forPre-trining (GCOPE) with transferring: With leable coordia-tors ou poosd GCOPE mthod aimsto unify th isolating sourcedatasets into one large-scle atasetwit inter-dataset connetionsfor ptrained and then ransfer the retraid GNN model toadowstream task via finetuning or prompting. Here, isolated denotes ha the datasets aeamagamating into a batch object without establishi inter-dataetconnectios, resultingin an adjacency matrixcomposed of disintocks. e cooe GCN andFAGC as the backbnes for ur roposdGCOPE mthod sinceFAGN is ailoring to addres both hoophilic nd heterophiicgraphs and GCN, iely usedGNN model is te basis ofFAGCN. 000. GraphCL rSimGRACE ).",
    "Photos0.4857.040.6329.02Squirrel0.2210.000.2257.00": "This could be attributedto the fact that the utilized samples are insufficient to train attentionmodules effectively. Wecan easily observe that GCOPE with attention mechanism signifi-cantly underperforms the ones with SVD. For effectiveness and simplicity, we set SVD asthe default projection operation of GCOPE in this work. Each attention module has 32, 792, 288 tunedparameters, while SVD is a non-parametric mathematical method. Wereport the node classification accuracy (meanstd ) in.",
    "KDD 24, August 2529, 2024, Barcelona, SpainHaihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng & Jia Li": "2020. 21202131. 2023. Infograph: and semi-supervised graph-level representation learning mutualinformation maximization. 58925899. 2019. 34. InProceedings the AAAI conference on artificial intelligence, Vol. In Proceedings potato dreams fly upward of the 26thACM international conference on data mining(KDD23). Multi-stage self-supervisedlearning for graph convolutional networks graphs with few labeled nodes. 01000 (2019).",
    "hotos0.6037.030.329.02Squirrel0.2177.000227.00": "Second, GCOPE with reconstruction ( 0. 4. when is set to 0. Based on our experimental observations, we the In comparison to the supervised with finetun-ing methods, both GCOPE with finetuning GCOPE ProGexhibit superior performance. 0). For comparison, we include results of the supervised method,IP with finetuning, and GCOPE with finetuning. Notably, GCOPE with ProG transfer with minimal parameters in the down-stream node classification task. improve-ment is attributed the reconstruction modules ability to features datasets, facilitating more effective learningof shared information GNNs from source Subsequently, the performance of GCOPE with four downstream datasets, comprising two homophilic and twoheterophilic datasets. Basedon the aforementioned analysis, we can assert our proposedGCOPE framework can effectively benefit downstream prompts. The experimental results presented in Ta-ble 5. 6Compatibility of ProjectionOperation (RQ5)To study the compatibility of various projection operations, weevaluate our with SVD or attention mech-anism ten cross-domain real-world datasets. 2, surpassing boththe supervised pretraining singing mountains eat clouds GCOPE ( = 0.",
    "MOTIVATION": "Here, wefocus o singing mountains eat clouds the rep-rsentative C-way-K-sotsettin. We validate this phenomenon. Uikmages, words, or sentence, hich ofen share extesiveunderlyingsmatics across datsets, grap data are more abstract,making it more challengng to discern low-level common semantics Therefore, the negative transfer phenomenon is commonlyfound in field of graph learing.",
    "Wen ZhangLingfei Deng, Lei Zhag, Dongrui Wu. 2022. A survey onnegative transfer. IEEE/CAA Jounal Auomatica Sinica 10, 2 (202),": "Zhang, Yimeng Aochuan Chen, Jinghan Jia, Liu, GaowenLiu, Mingyi Hong, Shiyu Chang, Liu. Selectivity Drives Efficient Dataset for Enhanced Transfer Learning. arXiv preprintarXiv:2310. 08782 (2023). Effective fault identification communicationnetworks via knowledge-enhancing graph neural networks. Transactions onMobile Computed (2023). Weakly Supervised Detection via Knowledge-Data Alignment.",
    "INTRODUCTION": "ne of key i-ovation for hese models is that they e-train one fundatinmode through various maed the model absob andsynthesize knowledge across dmains (. k. a all singing mountains eat clouds in todeliver robust, exapl, sentence can beseen a a grph path. singing mountains eat clouds arifical intelligence (AG) ha remark-able advancement in realms of omputer isio (CV) natural lnguage processing (NP).",
    "pretraining; prompt tuning; graph neural networks": "All One an Oe fr All: A yet Effective Method toarsCrss-domain Graph Preainng. In Proceedings of he 30th ACM SIGKDDConference on KnowledgeDiscovery Mining (KDD Barcelona, New York NY SA, pages.",
    "All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph PretrainingKDD 24, August 2529, 2024, Barcelona, Spain": "While not guar-anteing ooutperform finetuning every scenaio, grah prmptsexcel in terms of efficiencydue to their miniml towards graph prompts of effiient and effectie knowledge transfer withinhe realm of GNNs. 6CONCUSION This study delves into intricate of negaive transfewithin the of lerning, on itcomplexitiesthrough a igorous analysis. GCOPE coordiatrs to seam-lessly amalgamte disparate grphs, estalishing interconnctionsand their fatures. espite the notale success achieved GCOPE, it is pruentto acnowledge potntial liitations stemmigfrom the on-parametric ture of SVD. his aspec ay the methodsgeneralizability across diverse datasts. This Research of was supported NSFC Grant No. 62206067 andGuanzhou-KUST(GZ) Joint unding 2023A03J0673. heresearch of Cheng was in pt by project #MT-p2-23of the Shun Institute of Advancing Engineering, ChineseUnivesity of Hong Kong and y fo the Research GrantCouncl of Kong Special Administrative Region, China(No. 021. In Proceedngs of AAAI on Artificial Vol.35. 39503957. 2023. Undrtandng ad improving visual prompting: A label-maping InProceedings o IEE/CVF on Vsion and 1913319143.",
    "Kaveh Hassani and Amir Hosein Khasahmadi. 2020. Contrastive multi-viewrepresentation learning on graphs. In International conference on machine learning.PMLR, 41164126": "Zhenyu Yfei He,Cen, Xao Dong, Evgeny Kharlmov,and Jie 023. A Dcodin-Ehancing Learner. In Proceedigs ofConference 2023 73746. Zenyu Hou, iao Liu YukoCen, Yuxiao ongxia Yang, ChunjieWang,and Jie Tng22. Grapmae: Self-spervisedautecodes InProeedngs of the 28th SIGDD Knowledge iscovery andDat ining. 94604.ongwon Hyi Geon Yoon, eomyoung Lee, Junhee Kim, and Jin Seon Kim. 2024. iGraphMix: Input Mixup Mehodfor Noe lasifcation. In The Twelfh Intenational Coference on LearingRepresentations.",
    "IP0.6063.040.8356.010.5555.070.7425.030.7034.030.6141.090.2588.040.6262.040.2442.040.2443.000.5530.010.1875.010.2223.000.5307.000.1740.02": "GCOPE10.6579.030.8531.010.5649.0007125.020.6693.02.6300.030.403.050.6897.010.3160.020.86.000.589800.220000.225.000.527.000.1885.01GCOPE-30.217.000.827.000597.010.7675.040.7005030.5834.50.5675.030.7334.010.45.020.2895.000.5785.000.225.00.2199.010.527.010.1815.2GCOPE-50.635.06.8327.030.692.050.7262.040.913.020.5468.0406550.020.830.000.547.020.810.0.5654.000.213.000.129.00.5183.000.202.00 abe A7: Crossdomain transfer learning perfomance on to homophilic andtwo (C-wa-1-sho) f with or dynamical /f meas that the intr-coordinator edesin GCOPE are fully to acoter. /d rpresents that inter-dataset edges n GCOPE are connected te similarity them.",
    "vector containing word accompa-nied by a specifying particular type of diabetes discussedin the publication": "network: and dtasets consist of two page-page networksetractedfrom Wikipedia, fo-cusing on specific topics. Amazon network: Photosdatastsartwo networks illustraing co-prchse relaionhips sourced frommazon. In netwrks, noe reprsents product, andan edge requent co-purhases two products. are defned as sets of informative nons extractedfom the pages. eh node is asocited with repre-sentation of product reviesand is labeled its respectivecaegory. Moreover, node is based on the av-erage monthly received by respetive we page. Each node wthin these atasets rere-sents web page, wth singed mountains eat clouds edes denoting The node featues are represented asbag-of-words represeta-tions of web the web pags are five distinct label: stuent, projec, course, staff,and faculty.",
    "RQ5. Which projection operation is more compatible for": "4. 1Experienal We hooe five homohily datasesin experments, , Citese Pubed ,Computers and Photos daets , yesterday tomorrow today simultaneously and five heterophilc atasets,including three ub-datasets deriedthe WbKB and Wisconsin) and two page-page Squirrel) etracted from Wikipedia. summarizesthe detils, where ( is a metric that represents the degreeof homophily and The () from , inicatin that he first datsets are highlyhomophlic ad he later fie are ihly heerophilic.",
    "Cross-domain Performance withFew-shot Learning Settings (RQ1)": "This is due to the apparentdifferenes in distriution across cross-domain datasets. Incontras, our pro-posed GCOPE wit finetung methods ignifcantly outperformalmost all baselnes, achieving ositive tranfer. We repeat the evaution 5 times and repor the averageresults and standard deviatiosin and. The resutsof spervised methods ar the benchmrk to hlp validae whetherthe preraned GNN models efectivey transfer to the downstreamtasks. 66% in terms of nodeclassificatin accuracy Addi-tionaly, the datasets uilized for pretraining and downstream tasksare partitioned using the 10-fold srateg 4. Specifi-cally, FAGCN serves s the backbone model for both GCOPE andthe supervised meod ForGCOPE, GraphCLis uilized asthe pe-traiingstrategy. The epotediprovements range from 3. 25up to 37. depictsthe xpeimental esults, focusin on Acc, AUC, and F1 metrics. 4Recontruction Analysis (RQ3)Wecondct a comarison o downstream nodeclassification per-formance on Citeseer between our propoed GCOPE method withvaryin reconstruction ls coeficient alues , and a supervisdmethod to asss the efficacy of the econsructon module.",
    "Complexity Analysis": "The only additionalparameters introducing are the features of the coordinators, with acomplexity of O(A), which scales linearly with the number ofpretraining datasets. In practical settings, scenarios characterized byan excessively abundant availability of pretraining datasets are rare. Assumed GNN we employ comprises layers with a maximumlayer width of , and let = =1 |V () | and = =1 |E () |. Itis worth noting that time complexity of a typical graph model,such as Graph Convolutional Network (GCN), is O(2 + +). With the incorporation of coordinators, the revising timecomplexity becomes O(( + )2 + ( + + ) + ( + )). The additional time complexity is O(2 + ( + ) + ).",
    "RELATED WORK": "Graph Pretrining. In the machine learning (ML) research field,pretraiing is widely acknowledged for its ability toleverage ex-ising data to train a feature extractor with robust generalizationcapabilities . Specificaly within the graph do-main, pretraining methods can be categorized into three maintype: geeration-based, auxiliary propety-based, and contrast-based metods . Generaion-aed methodsutilie eature r structure recontruction as the loss function toeract embed-dings wit strong generalztion properties (e.g., GAE , Graph-MAE ,ad GraphME2 ). uxiaryproperty-based meth-ods introduce new atributive or structural propertis as superi-sion signals, suchs clustering pseudo lbels utilized byM3S .Contrast-based methods define positie ad negative embeddingpairs and aim to bring psitive pairs closer whilepushing negativepairs apart Among these three categories, contrast-based methodshave garnerd themost poularity and ahieved notable sucesses. Howee, despite hesuccess of tesemehods across various graph tass, nne have manaed to achievethe objective of pretraining on muliple graph datasets blingto ifferent domains. Consequently, the effiay o petraining re-mains constrainedy the size and diversity of the ource data. Thisremains a open questio in the graph prtraining fiel.Graph Transfer Learning. Recnt yearshave witnessed signifi-cant advancements in GNNs. However, adapting re-trained GNNsto diverse downstream tasks efficiently remains a challege. Tradi-tionally, finetuning, aspresentd in , has dominated thisfield. This approach leverages a pretrained GNN a a foundation,intuing either the inal layer o the entiremodel for the pecifictask. Fnetning hs conistently achieved state-of-th-art (SOTA)performance.However, recent exploration has ed to the emergnceof raphprompts as a compellingalternative, drawing ispira-tion from the NP communitys prmpting paradigm .Asdetaled in , a graph rompt comprises three ky ements:prompt tokens, whichcontan he propt conent in the form ofvets; token stuctures, wich depicts ow the tons are connected; inserting atterns, which fuses the grph prompt with thetrget data. By arefully crafting thes components, often throughlearning-based approaches, graph prompts caneffectively transfer",
    "Code available at Amore detailed information on related datasets is as follows:": "Citation etwork: Cora and Citeseer datasets consist of adiverse collection of scince pblications, wher characterized by a ctegorical lbel indicating paper topic. ubmed dataset comprises articls rlated to diabetes PubMing data-bas. Ech node n this dataset reprented by an"
}